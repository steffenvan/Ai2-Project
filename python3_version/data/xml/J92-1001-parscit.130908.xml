<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9965315">
Using Multiple Knowledge Sources for
Word Sense Discrimination
</title>
<author confidence="0.960681">
Susan W. McRoy.
</author>
<sectionHeader confidence="0.5043645" genericHeader="abstract">
Artificial Intelligence Program
GE Research and Development Center
</sectionHeader>
<bodyText confidence="0.999242454545454">
This paper addresses the problem of how to identify the intended meaning of individual words in
unrestricted texts, without necessarily having access to complete representations of sentences. To
discriminate senses, an understander can consider a diversity of information, including syntactic
tags, word frequencies, collocations, semantic context, role-related expectations, and syntactic
restrictions. However, current approaches make use of only small subsets of this information.
Here we will describe how to use the whole range of information. Our discussion will include how
the preference cues relate to general lexical and conceptual knowledge and to more specialized
knowledge of collocations and contexts. We will describe a method of combining cues on the basis
of their individual specificity, rather than a fixed ranking among cue-types. We will also discuss
an application of the approach in a system that computes sense tags for arbitrary texts, even when
it is unable to determine a single syntactic or semantic representation for some sentences.
</bodyText>
<sectionHeader confidence="0.990242" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999890210526316">
Many problems in applied natural language processing — including information re-
trieval, database generation from text, and machine translation — hinge on relating
words to other words that are similar in meaning. Current approaches to these ap-
plications are often word-based — that is, they treat words in the input as strings,
mapping them directly to other words. However, the fact that many words have mul-
tiple senses and different words often have similar meanings limits the accuracy of
such systems. An alternative is to use a knowledge representation, or interlingua, to
reflect text content, thereby separating text representation from the individual words.
These approaches can, in principle, be more accurate than word-based approaches, but
have not been sufficiently robust to perform any practical text processing task. Their
lack of robustness is generally due to the difficulty in building knowledge bases that
are sufficient for broad-scale processing.
But a synthesis is possible. Applications can achieve greater accuracy by working
at the level of word senses instead of word strings. That is, they would operate on
text in which each word has been tagged with its sense. Robustness need not be sacri-
ficed, however, because this tagging does not require a full-blown semantic analysis.
Demonstrating this claim is one of the goals of this paper.
Here is an example of the level of analysis a sense tagger would provide to an
application program. Suppose that the input is (1):
</bodyText>
<note confidence="0.6463446">
* Correspondence should be addressed to the author at Department of Computer Science, University of
Toronto, Toronto, Canada M55 1A4 or mcroy@ai.toronto.edu.
C) 1992 Association for Computational Linguistics
Computational Linguistics Volume 18, Number 1
Example 1
</note>
<bodyText confidence="0.969483">
The agreement reached by the state and the EPA provides for the safe storage of the
waste.
The analysis would provide an application with the following information.
</bodyText>
<listItem confidence="0.997656545454545">
• agreement refers to a state resulting from concurrence, rather than an act,
object, or state of being equivalent.
• reach is intended to mean &apos;achieve,&apos; rather than &apos;extend an arm.&apos;
• state refers to a government body, rather than an abstract state of
existence.
• safe in this context is an adjective corresponding to &apos;secure,&apos; rather than a
noun corresponding to a container for valuables.
• The EPA and the state were co-agents in completing some agreement
that is instrumental in supplying a secure place to keep garbage, rather
than there was some equivalence that extended its arm around the state
while the EPA was busy filling safes with trash.
</listItem>
<bodyText confidence="0.996538866666667">
Preliminary evidence suggests that having access to a sense tagging of the text im-
proves the performance of information retrieval systems (Krovetz 1989).
The primary goal of this paper, then, is to describe in detail methods and knowl-
edge that will enable a language analyzer to tag each word with its sense. To demon-
strate that the approach is sufficiently robust for practical tasks, the article will also
discuss the incorporation of the approach into an existing system, TRUMP (Jacobs 1986,
1987, 1989), and the application of it to unrestricted texts. The principles that make up
the approach are completely general, however, and not just specific to TRUMP.
An analyzer whose tasks include word-sense tagging must be able to take an in-
put text, determine the concept that each word or phrase denotes, and identify the
role relationships that link these concepts. Because determining this information accu-
rately is knowledge-intensive, the analyzer should be as flexible as possible, requiring
a minimum amount of customization for different domains. One way to gain such
flexibility is give the system enough generic information about word senses and se-
mantic relations so that it will be able to handle texts spanning more than a single
domain.
While having an extensive grammar and lexicon is essential for any system&apos;s do-
main independence, this increased flexibility also introduces degrees of ambiguity not
frequently addressed by current NLP work. Typically, the system will have to choose
from several senses for each word. For example, we found that TRUMP&apos;s base of nearly
10,000 root senses and 10,000 derivations provides an average of approximately four
senses for each word of a sentence taken from the Wall Street Journal. The potential
for combinatoric explosion resulting from such ambiguity makes it critical to resolve
ambiguities quickly and reliably. It is unrealistic to assume that word sense discrimi-
nation can be left until parsing is complete, as suggested, for example, by Dahlgren,
McDowell, and Stabler (1989) and Janssen (1990).
No simple recipe can resolve the general problem of lexical ambiguity. Although
semantic context and selectional restrictions provide good cues to disambiguation,
they are neither reliable enough, nor available quickly enough, to be used alone. The
approach to disambiguation that we will take below combines many different, strong
</bodyText>
<page confidence="0.997836">
2
</page>
<note confidence="0.955746">
Susan W. McRoy Using Multiple Knowledge Sources
</note>
<bodyText confidence="0.986513">
sources of information: syntactic tags, word frequencies, collocations, semantic con-
text (clusters), selectional restrictions, and syntactic cues. The approach incorporates a
number of innovations, including:
</bodyText>
<listItem confidence="0.985797166666667">
• a hybridization of several lexicons to help control which senses are
considered:
- a static generic lexicon
- a lexicon linked to collocations
- a lexicon linked to concretions (i.e., specializations of abstract
senses of words)
- lexicons linked to specialized conceptual domains;
• a separate processing phase, prior to parsing, that eliminates some
ambiguities and identifies baseline semantic preferences;
• a preference combination mechanism, applied during parsing and
semantic interpretation, that uses dynamic measures of strength based
on specificity, instead of a fixed, ordered set of rules.
</listItem>
<bodyText confidence="0.997552111111111">
Although improvements to our system are ongoing, it already interprets arbitrary text
and makes coarse word sense selections reasonably well. (Section 6 will give some
quantitative assessments.) No other system, to our knowledge, has been as successful.
We will now review word sense discrimination and the determination of role re-
lations. In Section 3, we discuss some sources of knowledge relevant to solving these
problems, and, in Section 4, how TRUMP&apos;s semantic interpreter uses this knowledge
to identify sense preferences. Section 5 describes how it combines the preference in-
formation to select senses. Afterward, we will discuss the results of our methods and
the avenues for improvement that remain.
</bodyText>
<sectionHeader confidence="0.800424" genericHeader="method">
2. Cues to Word Sense Discrimination
</sectionHeader>
<bodyText confidence="0.999879">
The problem of word sense discrimination is to choose, for a particular word in a
particular context, which of its possible senses is the &amp;quot;correct&amp;quot; one for the context.
Information about senses can come from a wide variety of sources:
</bodyText>
<listItem confidence="0.998563">
• the analysis of each word into its root and affixes, that is, its morphology;
• the contextually appropriate part or parts of speech of each word, that is,
its syntactic tag or tags;
• for each sense of the word, whether the sense is preferred or deprecated
— either in general, because of its frequency, or in the context, because it
is the expected one for a domain;
• whether a word is part of a common expression, or collocation, such as
a nominal compound (e.g., soda cracker) or a predicative relation (e.g., take
action);
• whether a word sense is supported by the semantic context — for
example, by its association with other senses in the context sharing a
semantic category, a situation, or a topic;
• whether the input satisfies the expectations created by syntactic cues
(e.g., some senses only take arguments of a particular syntactic type);
</listItem>
<page confidence="0.991436">
3
</page>
<note confidence="0.653771">
Computational Linguistics Volume 18, Number 1
</note>
<listItem confidence="0.99338075">
• whether it satisfies role-related expectations (i.e., expectations regarding
the semantic relations that link syntactically attached objects);
• whether the input refers to something already active in the discourse
focus.
</listItem>
<bodyText confidence="0.999028777777778">
Of course, not all these cues will be equally useful.
We have found that, in general, the most important sources of information for
word sense discrimination are syntactic tags, morphology, collocations, and word as-
sociations. Role-related expectations are also important, but to a slightly lesser degree.
Syntactic tags are very important, because knowing the intended part of speech is of-
ten enough to identify the correct sense. For example, according to our lexicon, when
safe is used as an adjective (as in Example 1), it always denotes the sense related to
security, whereas safe used as a noun always denotes a type of container for storing
valuables.
Morphology is also a strong cue to discrimination because certain sense—affix com-
binations are preferred, deprecated, or forbidden. Consider the word agreement. The
verb agree can mean either &apos;concur,&apos; &apos;benefit,&apos; or &apos;be equivalent&apos; and, in general, adding
the affix -ment to a verb creates a noun corresponding either to an act, or to its result,
its object, or its associated state. However, of the twelve possible combinations of root
sense and affix sense, in practice only four occur: agreement can refer only to the act,
object, or result in the case of the &apos;concur&apos; sense of agree or the state in the case of the
&apos;equivalence&apos; sense of agree. Furthermore, the last of these combinations is deprecated.
Collocations and word associations are also important sources of information be-
cause they are usually &amp;quot;dead giveaways,&amp;quot; that is, they make immediate and obvious
sense selections. For example, when paired with increase, the preposition in clearly
denotes a patient rather than a temporal or spatial location, or a direction. Word as-
sociations such as bank/ money similarly create a bias for the related senses. Despite
their apparent strength, however, the preferences created by these cues are not abso-
lute, as other cues may defeat them. For example, although normally the collocation
wait on means &apos;serve&apos; (Mary waited on John), the failure of a role-related expectation,
such as that the BENEFICIARY be animate, can override this preference (Mary waited on
the steps). Thus, collocations and word associations are strong sources of information
that an understander must weigh against other cues, and not just treat as rules for
sense-filtering (as in Hirst 1987 or Dahlgren, McDowell, and Stabler 1989).
The selection of a role relationship can both influence and be influenced by the
selection of word senses, because preferences partially constrain the various combi-
nations of a role, its holder, and the filler. For example, the preposition from prefers
referring to the SOURCE role; transfers, such as give, prefer to have a DESTINATION role;
and instances of colors, such as red, prefer to fill a COLOR role. Approaches based on
the word disambiguation model tend to apply constraint satisfaction techniques to
combine these role preferences (Hirst 1987). Preferences based on role-related expecta-
tions are often only a weak cue because they are primarily for verbs and not normally
very restrictive.
Although generally a weak cue, role-related preferences are quite valuable for the
disambiguation of prepositions. In our view, prepositions should be treated essentially
the same as other words in the lexicon. The meaning of a preposition either names a
relation directly, as one of its core senses (Hirst [1987] also allows this), or indirectly,
as a specialized sense triggered, for example, by a collocation or concretion. Because
the meaning of a preposition actually names a relation, relation-based cues are a good
source of information for disambiguating them. (References to objects in the discourse
</bodyText>
<page confidence="0.996973">
4
</page>
<note confidence="0.870102">
Susan W McRoy Using Multiple Knowledge Sources
</note>
<bodyText confidence="0.999400034482759">
focus can also be a strong cue for disambiguating prepositions, but this cue appears
fairly infrequently [Whittemore, Ferrara, and Brunner 19901.)
The problem of determining role relationships entangles word sense discrimination
with the problem of syntactic attachment. The attachment problem is a direct result
of the ambiguity in determining whether a concept is related to an adjacent object,
or to some enveloping structure that incorporates the adjacent object. Most proposed
solutions to this problem specify a fixed set of ordered rules that a system applies un-
til a unique, satisfactory attachment is found (Fodor and Frazier 1980; Wilks, Huang,
and Fass 1985; Shieber 1983; Hirst 1987; Dahlgren, McDowell, and Stabler 1989). Such
rules can be either syntactic, semantic, or pragmatic. Syntactic rules attempt to solve
the attachment problem independent of the sense discrimination problem. For exam-
ple, a rule for Right Association (also known as Late Closure) says to prefer attaching
a new word to the lowest nonterminal node on the rightmost branch of the current
structure (i.e., in the same structure as the last word processed) (Kimball 1973). Seman-
tic rules, by contrast, intertwine the problems of discrimination and attachment; one
must examine all combinations of senses and attachments to locate the semantically
best one. Such rules normally also collapse the attachment problem into the conceptual
role filling problem. For example, a lexical preference rule specifies that the preference
for a particular attachment depends on how strongly or weakly the verb of the clause
prefers its possible arguments (Fodor 1978; Ford, Bresnan, and Kaplan 1982). Pragmatic
rules also intermingle sense discrimination and attachment, but consider the context
of the utterance. For example, one suggested rule says to prefer to build structures
describing objects just mentioned (Crain and Steedman 1985; Altmann and Steedman
1988).
The accuracy of systems with fixed-order rules is limited by the fact that it is
not always possible to strictly order a set of rules independent of the context. For
example, Dahlgren, McDowell, and Stabler (1989) propose the rule &amp;quot;If the object of
the preposition is an expression of time, then S-attach the PP&amp;quot; to explain the preference
for assuming that &amp;quot;in the afternoon&amp;quot; modifies adjourn in Example 2:
</bodyText>
<subsectionHeader confidence="0.869424">
Example 2
</subsectionHeader>
<bodyText confidence="0.999167666666667">
The judge adjourned the hearing in the afternoon.
Although they admit this rule would fail for a sentence like John described the meeting
on January 20th, where the NP has a lexical preference for a time modifier, lexical pref-
erences are not always the determining factor either. The existence of a conceptually
similar object in the context (such as &amp;quot;the morning trial&amp;quot;) can also create an expectation
for the grouping &amp;quot;hearing in the afternoon,&amp;quot; as in Example 3 below.
</bodyText>
<subsectionHeader confidence="0.921168">
Example 3
</subsectionHeader>
<bodyText confidence="0.9997606">
The judge had to leave town for the day. He found a replacement to take over his
morning trial, but couldn&apos;t find anyone else that was available. He called the court-
house and cancelled the hearing in the afternoon.
Moreover, pragmatic effects are not always the determining factor either, leading many
people to judge the following sentence as silly (Hirst 1987).
</bodyText>
<subsectionHeader confidence="0.773422">
Example 4
</subsectionHeader>
<bodyText confidence="0.980925">
The landlord painted all the walls with cracks (Rayner, Carlson, and Frazier 1983).
</bodyText>
<page confidence="0.967246">
5
</page>
<note confidence="0.329864">
Computational Linguistics Volume 18, Number 1
</note>
<bodyText confidence="0.99988175">
The presence of different lexical items or different objects in the discourse focus may
strengthen or weaken the information provided by an individual rule. Another possi-
bility we will discuss in Section 5 is to weigh all preference information dynamically
(cf. Schubert 1986; McRoy and Hirst 1990).
The system we will be describing in Section 4 will use many of the cues described
above, including syntactic tags, morphology, word associations, and role-related ex-
pectations. But first, we need to discuss the sources of knowledge that enable a system
to identify these cues.
</bodyText>
<subsectionHeader confidence="0.891478">
3. Sources of Knowledge
</subsectionHeader>
<bodyText confidence="0.999994454545455">
To identify preference cues such as morphology, word frequency, collocations, seman-
tic contexts, syntactic expectations, and conceptual relations in unrestricted texts, a
system needs a large amount of knowledge in each category. In most cases, this just
means that the understander&apos;s lexicon and conceptual hierarchy must include prefer-
ence information, although processing concerns suggest moving some information out
of these structures and into data modules specific to a particular process, such as iden-
tifying collocations. TRUMP obtains the necessary knowledge from a moderately sized
lexicon (8,775 unique roots), specifically designed for use in language understanding,
and a hierarchy of nearly 1,000 higher-level concepts, overlaid with approximately 40
concept-cluster definitions. It also uses a library of over 1,400 collocational patterns.
We will consider each in turn.
</bodyText>
<subsectionHeader confidence="0.999934">
3.1 The Lexicon
</subsectionHeader>
<bodyText confidence="0.999890259259259">
Development of TRUMP&apos;s current lexicon followed an experiment with a moderately-
sized, commercially available lexicon (10,000 unique roots), which demonstrated many
substantive problems in applying lexical resources to text processing. Although the lex-
icon had good morphological and grammatical coverage, as well as a thesaurus-based
semantic representation of word meanings, it lacked reasonable information for dis-
criminating senses. The current lexicon, although roughly the same size as the earlier
one, has been designed to better meet the needs of producing semantic representa-
tions of text. The lexicon features a hierarchy of 1,000 parent concepts for encoding
semantic preferences and restrictions, sense-based morphology and subcategorization,
a distinction between primary and secondary senses and senses that require particu-
lar &amp;quot;triggers&amp;quot; or appear only in specific contexts, and a broad range of collocational
information. (An alternative would have been to give up discriminating senses that
the lexicon does not distinguish; cf. Janssen [1990].) At this time, the lexicon contains
about 13,000 senses and 10,000 explicit derivations.
Each lexical entry provides information about the morphological preferences, sense
preferences, and syntactic cues associated with a root, its senses, and their possible
derivations. An entry also links words to the conceptual hierarchy by naming the
conceptual parent of each sense. If necessary, an entry can also specify the composition
of common phrases, such as collocations, that have the root as their head.
TRUMP&apos;s lexicon combines a core lexicon with dynamic lexicons linked to spe-
cialized conceptual domains, collocations, and concretions. The core lexicon contains
the generic, or context-independent, senses of each word. The system considers these
senses whenever a word appears in the input. The dynamic lexicons contain word
senses that normally appear only within a particular context; these senses are con-
sidered only when that context is active. This distinction is a product of experience;
it is conceivable that a formerly dynamic sense may become static, as when military
terms creep into everyday language. The partitioning of the lexicon into static and
</bodyText>
<page confidence="0.998736">
6
</page>
<note confidence="0.781992">
Susan W. McRoy Using Multiple Knowledge Sources
</note>
<bodyText confidence="0.99801196">
dynamic components reduces the number of senses the system must consider in situ-
ations where the context does not trigger some dynamic sense. Although the idea of
using dynamic lexicons is not new (see Schank and Abelson [19771, for example), our
approach is much more flexible than previous ones because TRUMP&apos;s lexicon does not
link all senses to a domain. As a result, the lexical retrieval mechanism never forces
the system to use a sense just because the domain has preselected it.
3.1.1 The Core Lexicon. The core lexicon, by design, includes only coarse distinctions
between word senses. This means that, for a task such as generating databases from
text, task-specific processing or inference must augment the core lexical knowledge,
but problems of considering many nuances of meaning or low-frequency senses are
avoided. For example, the financial sense of issue (e.g., a new security) falls under the
same core sense as the latest issue of a magazine. The &apos;progeny&apos; and &apos;exit&apos; senses of
issue are omitted from the lexicon. The idea is to preserve in the core lexicon only the
common, coarse distinctions among senses (cf. Frazier and Rayner 1990).
Figure 1 shows the lexical entries for the word issue. Each entry has a part of
speech, : POS, and a set of core senses, : SENSES. Each sense has a : TYPE field that
indicates *primary* for a preferred (primary) sense and *secondary* for a depre-
cated (secondary) sense. The general rule for determining the : TYPE of a sense is that
secondary senses are those that the semantic interpreter should not select without
specific contextual information, such as the failure of some selectional restriction per-
taining to the primary sense. For example, the word yard can mean an enclosed area,
a workplace, or a unit of measure, but in the empty context, the enclosed-area sense
is assumed. This classification makes clear the relative frequency of the senses. This is
in contrast to just listing them in historical order, the approach of many lexicons (such
as the Longman Dictionary of Contemporary English [Procter 1978]) that have been used
in computational applications.
The :PAR field links each word sense to its immediate parent in the semantic hier-
archy. (See Section 3.2.) The parents and siblings of the two noun senses of issue, which
are listed in Figure 2, give an idea of the coverage of the lexicon. In the figure, word
senses are given as a root followed by a sense number; conceptual categories are desig-
nated by atoms beginning with c-. Explicit derivations, such as &amp;quot;period-ic-al-x,&amp;quot; are
indicated by roots followed by endings and additional type specifiers. These deriva-
tive lexical entries do &amp;quot;double duty&amp;quot; in the lexicon: an application program can use
the derivation as well as the semantics of the derivative form.
The : ASSOC field, not currently used in processing, includes the lexicographer&apos;s
choice of synonym or closely related words for each sense.
The : SYNTAX field encodes syntactic constraints and subcategorizations for each
sense. When senses share constraints (not the case in this example), they can be en-
coded at the level of the word entry. When the syntactic constraints (such as io-rec,
one-obj, and no-obj) influence semantic preferences, they are attached to the sense
entry. For example, in this case, issue used as an intransitive verb (no-obj) would favor
&apos;passive moving&apos; even though it is a secondary sense. The lo-re c subcategorization
in the first two senses means indirect object as recipient: the ditransitive form will
fill the RECIPIENT role. The grammatical knowledge base of the system relates these
subcategories to semantic roles.
The : G-DERIV and :S-DERIV fields mark morphological derivations. The former,
which is NIL in the case of issue to indicate no derivations, encodes the derivations
at the word root level, while the latter encodes them at the sense preference level.
For example, the : S-DERIV constraint allows issuance to derive from either of the first
two senses of the verb, with issuer and issuable deriving only from the &apos;giving&apos; sense.
</bodyText>
<page confidence="0.996674">
7
</page>
<figure confidence="0.66645">
Computational Linguistics Volume 18, Number 1
( issue
:POS noun
:SENSES
(( issuel
:EXAMPLE (address important issues)
:TYPE *primary*
</figure>
<equation confidence="0.91904553125">
:PAR (c-concern)
:ASSOC (subject) )
( issue2
:EXAMPLE (is that the october issue?)
:TYPE *secondary*
:PAR (c-published-document)
:ASSOC (edition) )))
( issue
:POS verb
:G-DERIV nil
:SENSES
(( issuel
:SYNTAX (one-obj io-rec)
:EXAMPLE (the stockroom issues supplies)
:TYPE *primary*
:PAR (c-giving)
:ASSOC (supply)
:S-DERIV ((-able adj tr_ability)
(-ance noun tract)
(-er noun tr_actor)) )
( issue2
:SYNTAX (one-obj io-rec)
:EXAMPLE (I issued instructions)
:TYPE *primary*
:PAR (c-informing)
:ASSOC (produce)
:S-DERIV ((-ance noun tract)) )
( issue3
:SYNTAX (one-obj no-obj)
:EXAMPLE (good smells issue from the cake)
:TYPE *secondary*
:PAR (c-passive-moving) )))
</equation>
<figureCaption confidence="0.748973">
Figure 1
</figureCaption>
<bodyText confidence="0.986393222222222">
The lexical entries for issue.
The derivation triples encode the form of each affix, the resulting syntactic category
(usually redundant), and the &amp;quot;semantic transformation&amp;quot; that applies between the core
sense and the resulting sense. For example, the triple (-er noun tr_act or) in the
entry for issue says that an issuer plays the ACTOR role of the first sense of the verb
issue. Because derivations often apply to multiple senses and often result in different
semantic transformations (for example, the ending -ion can indicate the act of perform-
ing some action, the object of the action, or the result of the action), a lexical entry can
mark certain interpretations of a morphological derivation as primary or secondary.
</bodyText>
<page confidence="0.974032">
8
</page>
<note confidence="0.530701">
Susan W. McRoy Using Multiple Knowledge Sources
</note>
<equation confidence="0.89692025">
NOUN_ISSUE1:
PARENT CHAIN: c-concern c-mental-obj c-obj
c-entity something
SIBLINGS (all nouns):
regard1 realm2
premonition1 pity1
ground3 goodwill1
draw2 department2
care1 business3
puzzlel province2
pet2 parameterl
feeling2 enigma1
concern1 cause2
baby2 apprehend-ion-x
NOUN_ISSUE2:
PARENT CHAIN: c-published-document c-document
c-phys-obj c-obj c-entity something
SIBLINGS (all
week-ly-x
tomel
source2
romance2
paperback1
omnibus1
</equation>
<bodyText confidence="0.8631148">
month-ly-x
magazinel
guidel
feature4
dissertationl
copy2
column1
brochure1
bibliographyl
anthologyl
</bodyText>
<equation confidence="0.941300272727273">
nouns):
volume1 transcript1
thesaurus1 supplement2
softwarel seriall
publication2 profile2
paper3 paper2
obituary1 novel1
memoir1 map1
library1 journal1
grammar1 gazette1
facsimile1 epic1
</equation>
<figure confidence="0.8357869">
directoryl digest1
constitute-ion-x1 comicl
cataloguel calendar1
book1 blurbl
bible1 atlasl
tragedy2
strip4
scripture1
period-ic-al-x
pamphlet1
notice2
manual1
handbook1
fiction1
encyclopedial
dictionary1
column2
bullet ml
biographyl
article1
</figure>
<figureCaption confidence="0.982093">
Figure 2
</figureCaption>
<bodyText confidence="0.931004">
The parents and siblings of two senses of issue.
</bodyText>
<subsubsectionHeader confidence="0.552189">
3.1.2 The Dynamic Lexicons. Unlike the core lexicon, which lists senses active in all
</subsubsectionHeader>
<bodyText confidence="0.999922307692308">
situations, the dynamic lexicons contain senses that are active only in a particular
context. Although these senses require triggers, a sense and its trigger may occur just
as frequently as a core sense. Thus, the dynamic—static distinction is orthogonal to the
distinction between primary and secondary senses made in the core lexicon.
Currently, TRUMP has lexicons linked to domains, collocations, and concretions.
For example, TRUMP&apos;s military lexicon contains a sense of engage that means &apos;attack.&apos;
However, the system does not consider this sense unless the military domain is active.
Similarly, the collocational lexicon contains senses triggered by well-known patterns of
words; for example, the sequence take effect activates a sense of take meaning &apos;transpire.&apos;
(Section 3.3 discusses collocations and their representation in more detail.) Concretions
activate specializations of the abstract sense of a word when it occurs with an object of
a specific type. For example, in the core lexicon, the verb project has the abstract sense
&apos;transfer&apos;; however, if its object is a sound, the system activates a sense corresponding
</bodyText>
<page confidence="0.971066">
9
</page>
<note confidence="0.313565">
Computational Linguistics Volume 18, Number 1
</note>
<bodyText confidence="0.999881727272728">
to a &apos;communication event,&apos; as in She projected her voice. Encoding these specializations
in the core lexicon would be problematic, because then a system would be forced to
resolve such nuances of meaning even when there was not enough information to do
so. Dynamic lexicons can provide much finer distinctions among senses than the core
lexicon, because they do not increase the amount of ambiguity when their triggering
context is inactive.
Together, the core and dynamic lexicons provide the information necessary to rec-
ognize morphological preferences, sense preferences, and syntactic cues. They also
provide some of the information required to verify and interpret collocations. Sec-
tions 3.2, 3.3, and 3.4, below, describe sources of information that enable a system to
recognize role-based preferences, collocations, and the semantic context.
</bodyText>
<subsectionHeader confidence="0.996385">
3.2 The Concept Hierarchy
</subsectionHeader>
<bodyText confidence="0.984752777777778">
The concept hierarchy serves several purposes. First, it associates word senses that
are siblings or otherwise closely related in the hierarchy, thus providing a thesaurus
for information retrieval and other tasks (cf. Fox et al. 1988). In a sense tagging sys-
tem, these associations can help determine the semantic context. Second, it supplies
the basic ontology to which domain knowledge can be associated, so that each new
domain requires only incremental knowledge engineering. Third, it allows role-based
preferences, wherever possible, to apply to groups of word senses rather than just
individual lexical entries.
To see how the hierarchy&apos;s concept definitions establish the basic ontology, con-
sider Figure 3, the definition of the concept c-recording. c-recording is the parent
concept for activities involving the storage of information, namely, the following verb
senses:
book2 cataloguel clockl compilel
date3 documentl enter3 indexl
inputl keyl logl recordl
In a concept definition, the :PAR fields link the concept to its immediate parents in the
hierarchy. The :ASSOC field links the derived instances of the given concept to their
places in the hierarchy. For example, according to Figure 3, the object form derived
</bodyText>
<figure confidence="0.905370142857143">
(c-ent c-recording
:DESC (the storing of information)
:PAR (c-action)
:PAR (c-simple-occurrence
:ROLE-PLAY (r-object r-patient))
:ASSOC ((r-object c-information))
:PREF ((r-patient c-information)))
Figure 3
The conceptual definition of c-recording.
(c-ent c-clothing
:DESC (cloth materials for wearing)
:PAR (c-phys-obj)
:RELS ((*modified-by*
(c-fabric-material c-made-of-rel))))
</figure>
<figureCaption confidence="0.978879">
Figure 4
</figureCaption>
<bodyText confidence="0.787678">
The conceptual definition of c-clothing.
</bodyText>
<page confidence="0.834308">
10
</page>
<figure confidence="0.926512">
Susan W. McRoy Using Multiple Knowledge Sources
(c-ent c-color-qual
:DESC (qualities of the color of an entity)
:PAR (c-phys-prop-qual)
:RELS ((*transform* (c-state c-color-rel))
(*modifier-of*
(c-phys-obj c-color-rel))))
Figure 5
The conceptual definition of c-color-qual.
(c-ent c-made-of-rel
:DESC (a relationship between an object and what it is made of)
:PAR (c-phys-prop-rel)
:PREF ((r-statevalue c-phys-obj)
(r-stateholder (or c-phys-obj c-whole)))
:RELS ((*held-by* c-phys-obj)))
</figure>
<figureCaption confidence="0.98227">
Figure 6
</figureCaption>
<bodyText confidence="0.98686545">
The conceptual definition of c-made-of -rel.
from enter3 (i.e., entry) has the parent c-information.
The :ROLE-PLAY fields mark specializations of a parent&apos;s roles (or introduce new
roles). Each :ROLE-PLAY indicates the parent&apos;s name for a role along with the concept&apos;s
specialization of it. For example, c-re cording specializes its inherited OBJECT role as
PATIENT.
The :RELS and : PREF fields identify which combinations of concept, role, and filler
an understander should expect (and hence prefer). For example, the definition in Fig-
ure 4 expresses that fabric materials are common modifiers of clothing (e.g., wool suit)
and fill the clothing&apos;s MADE-OF role. TRUMP&apos;s hierarchy also allows the specification
of such preferences from the perspective of the filler, where they can be made more
general. For example, although colors are also common modifiers of clothing (e.g., blue
suit), it is better to associate this preference with the filler (c-color-qual) because col-
ors prefer to fill the COLOR role of any physical object. (Figure 5 shows an encoding of
this preference.) The hierarchy also permits the specification of such preferences from
the perspective of the relation underlying a role. For example, the relation c-made-of
in Figure 6 indicates (in its : RELS) that physical objects normally have a MADE-OF role
and (in its : PREF) that the role is normally filled by some physical object. Figure 7 gives
a complete account of the use of the : RELS and :PREF fields and how they permit the
expression of role-related preferences from any perspective.
</bodyText>
<subsectionHeader confidence="0.999512">
3.3 Collocational Patterns
</subsectionHeader>
<bodyText confidence="0.889496166666667">
Collocation is the relationship among any group of words that tend to co-occur in a
predictable configuration. Although collocations seem to have a semantic basis, many
collocations are best recognized by their syntactic form. Thus, for current purposes,
we limit the use of the term &amp;quot;collocation&amp;quot; to sense preferences that result from these
well-defined syntactic constructions.1 For example, the particle combination pick up
1 Traditionally many of these expressions have been categorized as idioms (see Cowie and Mackin 1975;
Cowie, Mackin, and McCraig 1983), but as most are at least partly compositional and can be processed
by normal parsing methods, we prefer to use the more general term &amp;quot;collocation.&amp;quot; This categorization
thus happily encompasses both the obvious idioms and the compositional expressions whose status as
idioms is highly debatable. Our use of the term is thus similar to that of Smadja and McKeown, who
partition collocations into open compounds, predicative relations, and idiomatic expressions (Smadja
and McKeown 1990).
</bodyText>
<page confidence="0.995778">
11
</page>
<table confidence="0.960969636363636">
Computational Linguistics Volume 18, Number 1
Perspective Preference
holder filler relation
holder NA :RELS ( (*modif ied-by* :RELS ( (*holder-of* (role)))
(filler) (relation))) :PREF ( ((role) (filler)))
:PREF (((role) (filler)))
filler :RELS ( (*modif ier-of* NA :RELS ( (*modif ier-of *
(holder) (relation))) (holder) (relation)))
relation :RELS ( (*held-by* (holder))) :PREF NA
:PREF ((r-stat evalue (holder)) )
((r-st at eholder (filler)))
</table>
<figureCaption confidence="0.99847">
Figure 7
</figureCaption>
<bodyText confidence="0.871938">
The use of :PREF and :RELS.
</bodyText>
<listItem confidence="0.9918271">
1. 249 profit take
2. 205 take place
3. 157 take act
4. 113 say take
5. 113 act take
6. 99 take advantage
7. 94 take effect
8. 88 take profit
9. 77 take step
10. 76 take account
</listItem>
<figureCaption confidence="0.919033">
Figure 8
</figureCaption>
<bodyText confidence="0.996327227272727">
The top ten co-occurences with take.
and the verb-complement combination make the team are both collocation-inducing
expressions. Excluded from this classification are unstructured associations among
senses that establish the general semantic context, for example, courtroom/defendant.
(We will discuss this type of association in the next section.)
Collocations often introduce dynamic word senses, i.e., ones that behave composi-
tionally, but occur only in the context of the expression, making it inappropriate for the
system to consider them outside that context. For example, the collocation hang from
triggers a sense of from that marks an INSTRUMENT. In other cases, a collocation simply
creates preferences for selected core senses, as in the pairing of the &apos;opportunity&apos; sense
of break with the &apos;cause-to-have&apos; sense of give in give her a break. There is also a class
of collocations that introduce a noncompositional sense for the entire expression, for
example, the collocation take place invokes a sense &apos;transpire.&apos;
To recognize collocations during preprocessing, TRUMP uses a set of patterns,
each of which lists the root words or syntactic categories that make up the collocation.
For example, the pattern (TAKE (A) (ADJ) BATH) matches the clauses take a hot bath
and takes hot baths. In a pattern, parentheses indicate optionality; the system encodes
the repeatability of a category, such as adjectives, procedurally. Currently, there are
patterns for verb-particle, verb-preposition, and verb-object collocations, as well as
compound nouns.
Initially, we acquired patterns for verb-object collocations by analyzing lists of
root word pairs that were weighted for relative co-occurrence in a corpus of articles
</bodyText>
<page confidence="0.995589">
12
</page>
<note confidence="0.913451">
Susan W. McRoy Using Multiple Knowledge Sources
</note>
<bodyText confidence="0.999858913043478">
from the Dow Jones News Service (cf. Church and Hanks 1990; Smadja and McKeown
1990). As an example of the kind of data that we derived, Figure 8 shows the ten
most frequent co-occurrences involving the root &amp;quot;take.&amp;quot; Note that the collocation &amp;quot;take
action&amp;quot; appears both in its active form (third in the list), as well as its passive, actions
were taken (fifth in the list).
From an examination of these lists and the contexts in which the pairs appeared in
the corpus, we constructed the patterns used by TRUMP to identify collocations. Then,
using the patterns as a guide, we added lexical entries for each collocation. (Figure 9
lists some of the entries for the compositional collocations associated with the verb
take; the entries pair a dynamic sense of take with a sense occurring as its complement.)
These entries link the collocations to the semantic hierarchy, and, where appropriate,
provide syntactic constraints that the parser can use to verify the presence of a collo-
cation. For example, Figure 10 shows the entry for the noncompositional collocation
take place, which requires that the object (r ail*) be singular and determinerless.
These entries differ from similar representations of collocations or idioms in Smadja
and McKeown (1990) and Stock (1989), in that they are sense-based rather than word-
based. That is, instead of expressing collocations as word-templates, the lexicon groups
together collocations that combine the same sense of the head verb with particular
senses or higher-level concepts (cf. Dyer and Zernik 1986). This approach better ad-
dresses the fact that collocations do have a semantic basis, capturing general forms
such as give him or her (some temporal object), which underlies the collocations give
month, give minute, and give time. Currently, the system has entries for over 1700 such
collocations.
</bodyText>
<subsectionHeader confidence="0.991671">
3.4 Cluster Definitions
</subsectionHeader>
<bodyText confidence="0.999986115384615">
The last source of sense preferences we need to consider is the semantic context.
Work on lexical cohesion suggests that people use words that repeat a conceptual
category or that have a semantic association to each other to create unity in text
(Morris 1988; Morris and Hirst 1991; Halliday and Hasan 1976). These associations
can be thought of as a class of collocations that lack the predictable syntactic structure
of, say, collocations arising from verb-particle or compound noun constructions. Since
language producers select senses that group together semantically, a language analyzer
should prefer senses that share a semantic association. However, it is unclear whether
the benefit of knowing the exact nature of an association would justify the cost of
determining it. Thus, our system provides a cluster mechanism for representing and
identifying groups of senses that are associated in some unspecified way.
A cluster is a set of the senses associated with some central concept. The definition
of a cluster includes a name suggesting the central concept and a list of the cluster&apos;s
members, as in Figure 11. A cluster may contain concepts or other clusters.
TRUMP&apos;s knowledge base contains three types of clusters: categorial, functional,
and situational. The simplest type of cluster is the categorial cluster. These clusters con-
sist of the sets of all senses sharing a particular conceptual parent. Since the conceptual
hierarchy already encodes these clusters implicitly, we need not write formal cluster
definitions for them. Obviously, a sense will belong to a number of categorial clusters,
one for each element of its parent chain.
The second type of cluster is the functional cluster. These consist of the sets of all
senses sharing a specified functional relationship. For example, our system has a small
number of part-whole clusters that list the parts associated with the object named by
the cluster. Figure 12 shows the part-whole cluster cl-egg for parts of an egg.
The third type of cluster, the situational cluster, encodes general relationships
among senses on the basis of their being associated with a common setting, event,
</bodyText>
<page confidence="0.991898">
13
</page>
<table confidence="0.980801520833333">
Computational Linguistics Volume 18, Number 1
( take
:POS verb
:SPECIAL
(( take50
:S-COMPOUNDS
((vc (or (member c-verb_advise2-obj
c-act-of-verb_blamel
c-act-of-verb_losel noun_profit2)
c-giving)))
:EXAMPLE (take delivery)
:PAR (c-receiving) )
( take51
:S-COMPOUNDS ((vc (or (member noun_effort1)
c-temporal-obj c-energy)))
:EXAMPLE (the job takes up time))
:PAR (c-require-rel) )
( take52
:S-COMPOUNDS ((vc (member noun_newsl
noun_burden1 noun_load2 noun_pressure3
noun_pressure2 noun_stress1 noun_stress2
c-act-of-verb_strain1)))
:EXAMPLE (he couldn&apos;t take the presssure)
:PAR (c-managing) )
( take58
:S-COMPOUNDS ((vc (or (member noun_office2
noun_advantage1 noun_charge1
c-act-of-verb_control1 noun_command2
noun_responsibilityl) c-structure-rel
c-shape-rel)))
:EXAMPLE (they took advantage of the situation)
:PAR (c-contracting) )
( take59
:S-COMPOUNDS ((vc (member noun_effect1)))
:EXAMPLE (the new rules take effect today)
:PAR (c-transpire) )
( take60
:S-COMPOUNDS ((vc (or c-task)))
:EXAMPLE (he took the assignment)
:PAR (c-deciding) ))
Figure 9
Some compositional collocations involving take.
( take-placel
:CTYPE vc
:TAIL noun_place
:PREF ((r-*tail* (and (fillerp number singular)
(fillerp limit null))))
:PAR (c-transpire)
</table>
<footnote confidence="0.5302795">
Figure 10
The entry for the noncompositional phrase take place from the collocational entry for take.
</footnote>
<page confidence="0.985334">
14
</page>
<figure confidence="0.851625363636364">
Susan W. McRoy Using Multiple Knowledge Sources
(c-ent cl-business
:PAR c-cluster-obj
:CLUSTERS (c-business-group c-business-manager-human
c-business-org c-business-qual c-business-human
c-profession c-employment-action cl-financial)
Figure 11
The definition of the cluster cl-business.
(c-ent cl-egg
:PAR c-cluster-obj
:CLUSTERS (noun_albumin1 noun_white4 noun_egg1 noun_yolk1))
</figure>
<figureCaption confidence="0.939495">
Figure 12
</figureCaption>
<bodyText confidence="0.846187">
The definition of the cluster cl-egg.
</bodyText>
<equation confidence="0.929497">
(c-ent cl-courtroom
:PAR c-cluster-obj
:CLUSTERS (c-law-action c-law-obj verb_judgel noun_juryl
verb_defendl noun_lawyerl noun_attorney1
noun_crime1 noun_plaintiff1 noun_justice1
noun_justice2 verb_prosecute1 noun_bail1
noun_plea1 verb_object1 noun_fine1 noun_jaill
noun_prison1 noun_court1 noun_testimony1
verb_testify1 verb_try3 verb_swear2 noun_oath1
noun_truth1 noun_bench2 verb_perjure1))
</equation>
<figureCaption confidence="0.678188">
Figure 13
</figureCaption>
<bodyText confidence="0.999186652173913">
The definition of the cluster cl-courtroom.
or purpose. Since a cluster&apos;s usefulness is inversely proportional to its size, these clus-
ters normally include only senses that do not occur outside the clustered context or
that strongly suggest the clustered context when they occur with some other member
of the cluster. Thus, situational clusters are centered upon fairly specific ideas and
may correspondingly be very specific with respect to their elements. It is not unusual
for a word to be contained in a cluster while its synonyms are not. For example,
the cluster cl-courtroom shown in Figure 13 contains sense verb_testify1, but not
verb_assert1. Situational clusters capture the associations found in generic descrip-
tions (cf. Dahlgren, McDowell, and Stabler 1989) or dictionary examples (cf. Janssen
1990), but are more compact because clusters may include whole categories of objects
(such as c-law-act ion) as members and need not specify relationships between the
members. (As mentioned above, the conceptual hierarchy is the best place for encoding
known role-related expectations.)
The use of clusters for sense discrimination is also comparable to approaches that
favor senses linked by marked paths in a semantic network (Hirst 1987). In fact, clus-
ters capture most of the useful associations found in scripts or semantic networks,
but lack many of the disadvantages of using networks. For example, because clusters
do not specify what the exact nature of any association is, learning new clusters from
previously processed sentences would be fairly straightforward, in contrast to learning
new fragments of network. Using clusters also avoids the major problem associated
with marker-passing approaches, namely how to prevent the production of stupid
paths (or remove them from consideration after they have been produced) (Charniak
</bodyText>
<page confidence="0.959709">
15
</page>
<note confidence="0.507288">
Computational Linguistics Volume 18, Number 1
</note>
<bodyText confidence="0.887799055555556">
1983). The relevant difference is that a cluster is cautious because it must explicitly
specify all its elements. A marker passer takes the opposite stance, however, consid-
ering all paths up, down, and across the network unless it is explicitly constrained.
Thus a marker passer might find the following dubious path from the &apos;written object&apos;
sense of book to the &apos;part-of-a-plant&apos; sense of leaf:
[book made-of paper]
[paper made-from wood]
[tree made-of wood]
[tree has-part leaf]
whereas no cluster would link these entities, unless there had been some prior evidence
of a connection. (The recommended solution to the production of such paths by a
marker passer is to prevent the passing of marks through certain kinds of nodes [Hirst
1987; Hendler 19871.)
From the lexical entries, the underlying concept hierarchy, and the specialized
entries for collocation and clusters just described, a language analyzer can extract the
information that establishes preferences among senses. In the next section, we will
describe how a semantic interpreter can apply knowledge from such a wide variety
of sources.
</bodyText>
<sectionHeader confidence="0.606829" genericHeader="method">
4. Using Knowledge to Identify Sense Preferences
</sectionHeader>
<bodyText confidence="0.999960166666667">
There is a wide variety of information about which sense is the correct one, and the
challenge is to decide when and how to use this information. The danger of a combi-
natorial explosion of possibilities makes it advantageous to try to resolve ambiguities
as early as possible. Indeed, efficient preprocessing of texts can elicit a number of cues
for word senses, set up preferences, and help control the parse. Then, the parse and
semantic interpretation of the text will provide the cues necessary to complete the task
of resolution.
Without actually parsing a text, a preprocessor can identify for each word its
morphology,2 its syntactic tag or tags,3 and whether it is part of a collocation; for
each sense, it can identify whether the sense is preferred or deprecated and whether
it is supported by a cluster. These properties are all either retrievable directly from a
knowledge base or computable from short sequences of words. To identify whether
the input satisfies the expectations created by syntactic cues or whether it satisfies
role-related expectations, the system must first perform some syntactic analysis of the
input. Identifying these properties must come after parsing, because recognizing them
requires both the structural cues provided by parsing and a semantic analysis of the
text.
In our system, processing occurs in three phases: morphology, preprocessing, and
parsing and semantic interpretation. (See Figure 14.) Analysis of a text begins with
the identification of the morphological features of each word and the retrieval of
the (core) senses of each word. Then, the input passes through a special preprocessor
that identifies parse-independent semantic preferences (i.e., syntactic tags, collocations,
and clusters) and makes a preliminary selection of word senses. This selection pro-
cess eliminates those core senses that are obviously inappropriate and triggers certain
</bodyText>
<footnote confidence="0.99190625">
2 This is at least true for English, although whether it is possible for morphologically complex or
agglutinative languages such as Finnish remains to be seen.
3 A similar caveat applies here. (See Church [1988] or Zernik [1990] for statistical approaches to tagging
English words.)
</footnote>
<page confidence="0.989144">
16
</page>
<figure confidence="0.991671421052632">
Susan W. McRoy Using Multiple Knowledge Sources
Morphology
Preprocessor
Tagging
Identification
of
collocations
Identification
of
clusters
--&gt;
--&gt;
--&gt;
Parser
A
V
Semantic
interpreter
-&gt;
</figure>
<figureCaption confidence="0.99237">
Figure 14
</figureCaption>
<bodyText confidence="0.946443888888889">
The system architecture.
specialized senses. In the third phase, TRUMP attempts to parse the input and at the
same time produce a &amp;quot;preferred&amp;quot; semantic interpretation for it. Since the preferred
interpretation also fixes the preferred sense of each word, it is at this point that the
text can be given semantic tags, thus allowing sense-based information retrieval.
In the next few subsections we will describe in greater detail the processes that
enable the system to identify semantic preferences: morphological analysis, tagging,
collocation identification, cluster matching, and semantic interpretation. Afterward we
will discuss how the system combines the preferences it identifies.
</bodyText>
<subsectionHeader confidence="0.99998">
4.1 Morphological Analysis and Lexical Retrieval
</subsectionHeader>
<bodyText confidence="0.99995952631579">
The first step in processing an input text is to determine the root, syntactic features,
and affixes of each word. This information is necessary both for retrieving the word&apos;s
lexical entries and for the syntactic tagging of the text during preprocessing. Morpho-
logical analysis not only reduces the number of words and senses that must be in
the lexicon, but it also enables a system to make reasonable guesses about the syntac-
tic and semantic identity of unknown words so that they do not prevent parsing (see
Rau, Jacobs, and Zernik 1989). Once morphological analysis of a word is complete, the
system retrieves (or derives) the corresponding senses and establishes initial semantic
preferences for the primary senses. For example, by default, the sense of agree mean-
ing &apos;to concur&apos; (agree 1) is preferred over its other senses. The lexical entry for agree
marks this preference by giving it : TYPE *primary* (see Figure 15). The entry also
says that derivations (listed in the :S-DERIV field) agree1+ment and agree2-1-able are
preferred, derivations agreel±able and agree3+ment are deprecated, and all other
sense-affix combinations (excepting inflections) have been disallowed.
During morphological analysis, the system retrieves only the most general senses.
It waits until the preprocessor or the parser identifies supporting evidence before
it retrieves word senses specific to a context, such as a domain, a situation, or a
collocation. In most cases this approach helps reduce the amount of ambiguity. The
approach is compatible with evidence discussed by Simpson and Burgess (1988) that
</bodyText>
<page confidence="0.98861">
17
</page>
<figure confidence="0.997365060606061">
Computational Linguistics Volume 18, Number 1
( agree
:POS verb
:G-DERIV nil
:SENSES
(( agreel
:SYNTAX (one-obj no-obj thatcomp comp subj-equi)
:EXAMPLE (she agrees with me * they agreed to use force *
they agreed on 3 percent * they agreed that he was right *
I agree it is true)
:TYPE *primary*
:PAR (c-agreeing)
:ASSOC (concur believe)
:S-DERIV ((-ment preferred noun tr_act tr_object tr_result)
(-able secondary adj tr_ability))
)
( agree2
:SYNTAX (one-obj)
:EXAMPLE (winter agrees with me)
:TYPE *secondary*
:PAR (c-abstract-relation)
:ASSOC (benefit)
:S-DERIV ((-able preferred adj tr_ability))
)
( agree3
:SYNTAX (no-obj)
:EXAMPLE (the two accounts do not agree)
:TYPE *secondary*
:PAR (c-equivalence-rel)
:ASSOC (correspond)
:S-DERIV ((-ment secondary noun tr_state))
)
)
</figure>
<figureCaption confidence="0.973596">
Figure 15
</figureCaption>
<bodyText confidence="0.96284475">
The lexical entry for the verb agree.
&amp;quot;multiple meanings are activated in frequency-coded order&amp;quot; and that low-frequency
senses are handled by a second retrieval process that accumulates evidence for those
senses and activates them as necessary.
</bodyText>
<subsectionHeader confidence="0.998529">
4.2 Tagging
</subsectionHeader>
<bodyText confidence="0.999825375">
Once the system determines the morphological analysis of each word, the next step in
preprocessing is to try to determine the correct part of speech for the word. Our system
uses a tagging program, written by Uri Zernik (1990), that takes information about
the root, affix, and possible syntactic category for each word and applies stochastic
techniques to select a syntactic tag for each word. Stochastic taggers look at small
groups of words and pick the most likely assignment of tags, determined by the
frequency of alternative syntactic patterns in similar texts. Although it may not be
possible to completely disambiguate all words prior to parsing, approaches based on
</bodyText>
<page confidence="0.998527">
18
</page>
<note confidence="0.89559">
Susan W. McRoy Using Multiple Knowledge Sources
</note>
<bodyText confidence="0.998175142857143">
stochastic information have been quite successful (Church 1988; Garside, Leech, and
Sampson 1987; de Marcken 1990).4
To allow for the fact that the tagger may err, as part of the tagging process the
system makes a second pass through the text to remove some systematic errors that
result from biases common to statistical approaches. For example, they tend to prefer
modifiers over nouns and nouns over verbs; for instance, in Example 5, the tagger
erroneously marks the word need as a noun.
</bodyText>
<subsectionHeader confidence="0.707755">
Example 5
</subsectionHeader>
<bodyText confidence="0.999019909090909">
You really need the Campbell Soups of the world to be interested in your magazine.
In this second pass, the system applies a few rules derived from our grammar and
resets the tags where necessary. For example, to correct for the noun versus verb
overgeneralization, whenever a word that can be either a noun or a verb gets tagged
as just a noun, the corrector lets it remain ambiguous unless it is immediately preceded
by a determiner (a good clue for nouns), or it is immediately preceded by a plural
noun or a preposition, or is immediately followed by a determiner (three clues that
suggest a word may be a verb). The system is able to correct for all the systematic
errors we have identified thus far using just nine rules of this sort.
After tagging, the preprocessor eliminates all senses corresponding to unselected
parts of speech.
</bodyText>
<subsectionHeader confidence="0.999916">
4.3 Identification of Collocations
</subsectionHeader>
<bodyText confidence="0.999475">
Following the syntactic filtering of senses, TRUMP&apos;s preprocessor identifies colloca-
tions and establishes semantic preferences for the senses associated with them. In this
stage of preprocessing, the system recognizes the following types of collocations:
</bodyText>
<listItem confidence="0.999208333333333">
• verb+particle pairs such as take on;
• verb+preposition pairs such as invest in;
• verb+particle+preposition combinations such as break in on;
• verb+complement clauses such as take a bath, their passives, as in actions
were taken, and hyphenated nominals, such as profit-taking;
• compound noun phrases such as investment bank.
</listItem>
<bodyText confidence="0.96842475">
To recognize a collocation, the preprocessor relies on a set of simple patterns, which
match the general syntactic context in which the collocation occurs. For example, the
system recognizes the collocation &amp;quot;take profit&amp;quot; found in Example 6 with the pattern
(TAKE (DET) PROFIT).
</bodyText>
<subsectionHeader confidence="0.785456">
Example 6
</subsectionHeader>
<bodyText confidence="0.9989916">
A number of stocks that have spearheaded the market&apos;s recent rally bore the brunt of
isolated profit-taking Tuesday.
The preprocessor&apos;s strategy for locating a collocation is to first scan the text for trig-
ger words, and if it finds the necessary triggers, then to try to match the complete
pattern. (Triggers typically correspond to the phrasal head of a collocation, but for
</bodyText>
<page confidence="0.829906">
4 Magerman and Marcus (1990) do complete stochastic N-gram parsing.
19
</page>
<note confidence="0.532418">
Computational Linguistics Volume 18, Number 1
</note>
<bodyText confidence="0.99989375">
more complex patterns, such as verb-complement clauses, both parts of the colloca-
tion must be present.) The system&apos;s matching procedures allow for punctuation and
verb-complement inversion.
If the triggers are found and the match is successful, the preprocessor has a choice
of subsequent actions, depending on how cautious it is supposed to be. In its aggressive
mode, it updates the representations of the matched words, adding any triggered senses
and preferences for the collocated senses. It also deletes any unsupported, deprecated
senses. In its cautious mode, it just adds the word senses associated with the pattern
to a dynamic store. Once stored, these senses are then available for the parser to use
after it verifies the syntactic constraints of the collocation; if it is successful, it will add
preferences for the appropriate senses. Early identification of triggered senses enables
the system to use them for cluster matching in the next stage.
</bodyText>
<subsectionHeader confidence="0.999142">
4.4 Identification of Clusters
</subsectionHeader>
<bodyText confidence="0.999994733333333">
After the syntactic filtering of senses and the activation of senses triggered by col-
locations, the next step of preprocessing identifies preferences for senses that invoke
currently active clusters (see Section 3.4). A cluster is active if it contains any of the
senses under consideration for other words in the current paragraph. The system may
also activate certain clusters to represent the general topic of the text.
The preprocessor&apos;s strategy for assessing cluster-based preferences is to take the
set of cluster names invoked by each sense of each content word in the sentence
and locate all intersections between it and the names of other active clusters. (For
purposes of cluster matching, the sense list for each word will include all the special
and noncompositional senses activated during the previous stage of preprocessing, as
well as any domain-specific senses that are not yet active.) For each intersection the
preprocessor finds, it adds preferences for the senses that are supported by the cluster
match. Then, the preprocessor activates any previously inactive senses it found to be
supported by a cluster match. This triggering of senses on the basis of conceptual
context forms the final step of the preprocessing phase.
</bodyText>
<subsectionHeader confidence="0.96132">
4.5 Semantic Interpretation
</subsectionHeader>
<bodyText confidence="0.981177705882353">
Once preprocessing is complete, the parsing phase begins. In this phase, TRUMP
attempts to build syntactic structures, while calling on the semantic interpreter to
build and rate alternative interpretations for each structure proposed. These semantic
evaluations then guide the parser&apos;s evaluation of syntactic structures. They may also
influence the actual progression of the parse. For example, if a structure is found to
have incoherent semantics, the parser immediately eliminates it (and all structures
that might contain it) from further consideration. Also, whenever the semantics of a
parse becomes sufficiently better than that of its competitors, the system prunes the
semantically inferior parses, reducing the number of ambiguities even further.&apos;
As suggested above, the system builds semantic interpretations incrementally. For
each proposed combination of syntactic structures, there is a corresponding combi-
nation of semantic structures. It is the job of the semantic interpreter to identify the
possible relations that link the structures being combined, identify the preferences asso-
ciated with each possible combination of head, role (relation), and filler (the argument
or modifier), and then rank competing semantic interpretations.
5 A similar approach has been taken by Gibson (1990) and is supported by the psychological
experiments of Kurtzman (1984).
</bodyText>
<page confidence="0.989045">
20
</page>
<note confidence="0.874535">
Susan W. McRoy Using Multiple Knowledge Sources
</note>
<bodyText confidence="0.8600865">
For each proposed combination, knowledge sources may contribute the following
preferences:
</bodyText>
<listItem confidence="0.943597666666667">
• preferences directly associated with the head or the filler, determined
recursively from their components, beginning with preferences identified
during preprocessing.
• preferences associated with syntactic cues, such as the satisfaction of
restrictions listed in the lexicon. For example, a word may allow only
modifiers of a particular syntactic form, or a modifier may modify only a
</listItem>
<bodyText confidence="0.909859461538462">
certain syntactic form. (For example, the sense meaning &apos;to care for,&apos; in
She tends plants or She tends to plants occurs with an NP or PP object,
whereas the sense of tend meaning &apos;to have a tendency&apos; as in She tends to
lose things requires a clausal object.)
• preferences associated with the semantic &amp;quot;fit&amp;quot; between any two of the
head, the role, and the filler, for example:
filler and role e.g., foods make good fillers for the PATIENT role of
eating activities;
filler and head e.g., colors make good modifiers of physical objects;
head and role e.g., monetary objects expect to be qualified by
some QUANTITY.
The conceptual hierarchy and the lexicon contain the information that
encodes these preferences.
</bodyText>
<listItem confidence="0.631753">
• preferences triggered by reference resolution. (Currently, our system does
not make use of these preferences, but see Crain and Steedman [1985];
Altmann and Steedman [1988]; Hirst [1987].)
</listItem>
<bodyText confidence="0.9707695">
How the semantic interpreter combines these preferences is the subject of the next
section.
</bodyText>
<sectionHeader confidence="0.785672" genericHeader="method">
5. Combining Preferences to Select Senses
</sectionHeader>
<bodyText confidence="0.999921411764706">
Given the number of preference cues available for discriminating word senses, an
understander must face the question of what to do if they conflict. For example, in the
sentence Mary took a picture to Bob, the fact that photography does not normally have
a destination (negative role-related information) should override the support for the
&apos;photograph&apos; interpretation of took a picture given by collocation analysis. A particular
source of information may also support more than one possible interpretation, but to
different degrees. For example, cigarette filter may correspond either to something that
filters out cigarettes or to something that is part of a cigarette, but the latter relation
is more likely. Our strategy for combining the preferences described in the preceding
sections is to rate most highly the sense with the strongest combination of supporting
cues. The system assigns each preference cue a strength, an integer value between +10
and -10, and then sums these strengths to find the sense with the highest rating.
The strength of a particular cue depends on its type and on the degree to which
the expectations underlying it are satisfied. For cues that are polar — for example,
a sense is either low or high frequency — a value must be chosen experimentally,
depending on the strength of the cue compared with others. For example, the system
assigns frequency information (the primary-secondary distinction) a score close to
</bodyText>
<page confidence="0.997056">
21
</page>
<note confidence="0.63633">
Computational Linguistics Volume 18, Number 1
</note>
<bodyText confidence="0.999975882352941">
zero because this information tends to be significant only when other preferences are
inconclusive. For cues that have an inherent extent -- for example, the conceptual
category specified by a role preference subsumes a set of elements that can be counted
— the cue strength is a function of the magnitude of the extent, that is, its specificity.
TRUMP&apos;s specificity function maps the number of elements subsumed by the
concept onto the range 0 to +10. The function assigns concepts with few members a
high value and concepts with many members a low value. For example, the concept
c-object, which subsumes roughly half the knowledge base, has a low specificity
value (1). In contrast, the concept noun_hammer 1, which subsumes only a single entity,
has a high specificity value (10). Concept strength is inversely proportional to concept
size because a preference for a very general (large) concept often indicates that either
there is no strong expectation at all or there is a gap in the system&apos;s knowledge. In
either case, a concept that subsumes only a few senses is stronger information than a
concept that subsumes more. The preference score for a complex concept, formed by
combining simpler concepts with the connectives AND, OR, and NOT, is a function of
the number of senses subsumed by both, either, or neither concept, respectively. Simi-
larly, the score for a cluster is the specificity of that cluster (as defined in Section 3.4).
(If a sense belongs to more than one active cluster, then only the most specific one
is considered.) The exact details of the function (i.e., the range of magnitudes corre-
sponding to each specificity class) necessarily depend on the size and organization
of one&apos;s concept hierarchy. For example, one would assign specificity value 1 to any
concept with more members than any immediate specialization of the most abstract
concept.
When a preference cue matches the input, the cue strength is its specificity value;
when a concept fails to match the input, the strength is a negative value whose magni-
tude is usually the specificity of the concept, but it is not always this straightforward.
Rating the evidence associated with a preference failure is a subtle problem, because
there are different types of preference failure to take into account. Failure to meet a
general preference is always significant, whereas failure to meet a very specific pref-
erence is only strong information when a slight relaxation of the preference does not
eliminate the failure. This presents a bit of a paradox: the greater the specificity of a
concept, the more information there is about it, but the less information there may
be about a corresponding preference. The paradox arises because the failure of a very
specific preference introduces significant uncertainty as to why the preference failed.
Failing to meet a very general preference is always strong information because, in
practice, the purpose of such preferences is to eliminate the grossly inappropriate —
such as trying to use a relation with a physical object when it should only be applied
to events. The specificity function in this case returns a value whose magnitude is the
same as the specificity of the complement of the concept (i.e., the positive specificity less
the maximum specificity, 10.) The result is a negative number whose absolute value
is greater than it would be by default. For example, if a preference is for the concept
c-obj ect, which has a positive specificity of 1, and this concept fails to match the
input, then the preference value for the cue will be —9.
On the other hand, a very specific preference usually pinpoints the expected entity,
i.e., the dead giveaway pairings of role and filler. Thus, it is quite common for these
preferences to overspecify the underlying constraint; for example, cut may expect a tool
as an INSTRUMENT, but almost any physical object will suffice. When a slight relaxation
of the preference is satisfiable, a system should take the cautious route, and assume
it has a case of overspecification and is at worst a weak failure. Again, the specificity
function returns a negative value with magnitude equivalent to the specificity of the
complement of the concept, but this time the result will be a negative number whose
</bodyText>
<page confidence="0.997517">
22
</page>
<note confidence="0.930603">
Susan W. McRoy Using Multiple Knowledge Sources
</note>
<bodyText confidence="0.976994166666667">
absolute value is less than it would be by default. When this approach fails, a system
can safely assume that the entity under consideration is &amp;quot;obviously inappropriate&amp;quot; for
a relatively strong expectation, and return the default value. The default value for a
concept that is neither especially general nor specific and that fails to match the input
is just —1 times the positive specificity of the concept.
The strategy of favoring the most specific information has several advantages.
This approach best addresses the concerns of an expanding knowledge base where
one must be concerned not only with competition between preferences but also with
the inevitable gaps in knowledge. Generally, the more specific information there is,
the more complete, and hence more trustworthy, the information is. Thus, when there
is a clear semantic distinction between the senses and the system has the information
necessary to identify it, a clear distinction usually emerges in the ratings. When there is
no strong semantic distinction, or there is very little information, preference scores are
usually very close, so that the parser must fall back on syntactic preferences, such as
Right Association. This result provides a simple, sensible means of balancing syntactic
and semantic preferences.
To see how the cue strengths of frequency information, morphological preferences,
collocations, clusters, syntactic preferences, and role-related preferences interact with
one another to produce the final ranking of senses, consider the problem of deciding
the correct sense of reached in Example 1 (repeated below):
Example 1
The agreement reached by the state and the EPA provides for the safe storage of the
waste.
According to the system&apos;s lexicon, reached has four possible verb senses:
</bodyText>
<listItem confidence="0.986903">
• re ach 1, as in reach a destination, which has conceptual parents
c-dest-occur (&amp;quot;destination occurrence&amp;quot;) and c-arriving;
• reach2, as in reach for a cookie, which has conceptual parent
c-bodypart-action;
• reach3, as in reach her by telephone, which has conceptual parent
c-comm-e vent (&amp;quot;communication event&amp;quot;); and
• reach4, as in reach a conclusion, which has conceptual parent
c-cause-to-event-change.
</listItem>
<bodyText confidence="0.98557825">
Figure 16 shows a tabulation of cue strengths for each of these interpretations of
reach in Example 1, when just information in the VP reached by the state and the EPA is
considered. The sense reach3 has the highest total score. From the table, we see that,
at this point in the parse, the only strong source of preferences is the role information
(line 6 of Figure 16). The derivation of these numbers is shown in Figures 17, 18,
and 19, which list the role preferences associated with the possible interpretations of
the preposition by for reach3, and its two nearest competitors, reachl and reach4.
Together, the data in the tables reveal the following sources of preference strength:
• The &apos;arrival&apos; sense (reachl) gains support from the fact that there is a
sense of by meaning AGENT, which is a role that arrivals expect (line 3 of
column 3 of Figure 17), and the state and the EPA make reasonably good
agents (line 5 of column 3 of Figure 17).
</bodyText>
<page confidence="0.993611">
23
</page>
<table confidence="0.991151869565217">
Computational Linguistics Volume 18, Number 1
Cue Type Cue Strength
reachl reach2 reach3 reach4
c-dost-occur c-bodypart-action c-comm-event c-cause-to-event-change
Frequency 1 1 —1 1
Morphology 0 0 0 0
Collocation 0 0 0 0
Cluster 0 0 0 0
Syntax 0 0 0 0
Roles 41 38 46 41
Total 42 39 45 42
Figure 16
Score tabulations for reached in the VP.
reachl
Preference Type Role Preference Strength
byl by3 by4 by5
SPATIAL-PROXIMITY DIRECTION AGENT INSTRUMENT
Relation—Filler 0 0 0 —2
Filler—Holder 0 0 0 0
Relation—Holder 0 0 5 0
Syntax 1 1 1 1
Strength of PP 37 35 35 35
Total 38 36 41 34
</table>
<figureCaption confidence="0.73982">
Figure 17
</figureCaption>
<bodyText confidence="0.881018375">
Role-related preferences of reachl for the preposition by.
• The &apos;communication&apos; sense (reach3) gains support from the fact that
there is a sense of by corresponding to the expected role COMMUNICATOR
(line 3 of column 3 of Figure 18) and the state and the EPA make very
good agents of communication events (communicators), in particular
(line 1 of column 3 of Figure 18), as well as being good agents in general
(line 5 of column 3 of Figure 18); however, reach3 is disfavored by
frequency information (line 1 of column 3 of Figure 16).
</bodyText>
<listItem confidence="0.94937075">
• The &apos;event change&apos; (conclude) sense (reach4) gains support from the fact
that there is a sense of by corresponding to the expected role CAUSE (line
3 of column 3 of Figure 19) and from the fact that the state and the EPA
make good agents (line 5 of column 3 of Figure 19).
</listItem>
<bodyText confidence="0.84515375">
Although the system favors the &apos;communication&apos; sense of reach in the VP, for the
final result, it must balance this information with that provided by the relationship
between agreement and the verb phrase. By the end of the parse, the &apos;event-change&apos;
sense comes to take precedence:
</bodyText>
<listItem confidence="0.9706435">
• The system completely eliminates the &apos;destination&apos; sense from
consideration because it is significantly weaker than all its competitors.
</listItem>
<page confidence="0.994848">
24
</page>
<table confidence="0.976187739130435">
Susan W. McRoy Using Multiple Knowledge Sources
reach3
Preference Type Role Preference Strength
byl by3 by4 by5
SPATIAL-PROXIMITY DIRECTION COMMUNICATOR INSTRUMENT
Relation—Filler 0 0 5 0
Filler—Holder 0 0 0 0
Relation—Holder 0 0 5 0
Syntax 1 1 1 1
Strength of PP 37 35 35 35
Total 38 36 46 36
Figure 18
Role-related preferences of reach3 for the preposition by.
reach4
Preference Type Role Preference Strength
byl by3 by4 by5
SPATIAL-PROXIMITY DIRECTION CAUSE (AGENT) INSTRUMENT
Relation—Filler 0 0 0 0
Filler—Holder 0 0 0 0
Relation—Holder 0 0 5 0
Syntax 1 1 1 1
Strength of PP 37 35 35 35
Total 38 36 41 36
</table>
<figureCaption confidence="0.92402">
Figure 19
</figureCaption>
<bodyText confidence="0.966788">
Role-related preferences of reach4 for the preposition by.
The main cause of this weakness is that (in our system) the role that
agreement would fill, DESTINATION, has no special preference for being
associated with a c-de st-event — many events allow a DESTINATION
role.
</bodyText>
<listItem confidence="0.879610166666667">
• The &apos;communication&apos; sense loses favor because it does not gain much
support from having agreement as either PATIENT or RECIPIENT. The final
score of this sense is 52.
• The &apos;event-change&apos; sense gains support from having agreement fill its
AFFECTED role, enough that the final strength of the &apos;event-change&apos; sense,
55, ultimately surpasses the final strength of the &apos;communication&apos; sense.
</listItem>
<bodyText confidence="0.999947571428571">
By summing the cue strengths of each possible interpretation in this way and
selecting the one with the highest total score, the system decides which sense is the
&amp;quot;correct&amp;quot; one for the context. The strengths of individual components of each inter-
pretation contribute to, but do not determine, the strength of the final interpretation,
because there are also strengths associated with how well the individual components
fit together. No additional weights are necessary, because the specificity values the
system uses are a direct measure of strength.
</bodyText>
<page confidence="0.993728">
25
</page>
<note confidence="0.690186">
Computational Linguistics Volume 18, Number 1
</note>
<sectionHeader confidence="0.997728" genericHeader="method">
6. Results and Discussion
</sectionHeader>
<bodyText confidence="0.999977545454545">
Our goal has been a natural language system that can effectively analyze an arbitrary
input at least to the level of word sense tagging. Although we have not yet fully
accomplished this goal, our results are quite encouraging. Using a lexicon of approx-
imately 10,000 roots and 10,000 derivations, the system shows excellent lexical and
morphological coverage. When tested on a sample of 25,000 words of text from the
Wall Street Journal, the system covered 98% of non-proper noun, non-abbreviated word
occurrences (and 91% of all words). Twelve percent of the senses the system selected
were derivatives.
The semantic interpreter is able to discriminate senses even when the parser cannot
produce a single correct parse. Figure 20 gives an example of the sense tagging that
the system gives to the following segment of Wall Street Journal text:
</bodyText>
<sectionHeader confidence="0.451549" genericHeader="method">
Example 7
</sectionHeader>
<bodyText confidence="0.999986823529412">
The network also is changing its halftime show to include viewer participation, in an
attempt to hold on to its audience through halftime and into the second halves of
games. One show will ask viewers to vote on their favorite all-time players through
telephone polls.
Each word is tagged with its part of speech and sense number along with a parent
concept. For example, the tag [changing verb_3 (c-replacing)] shows that the in-
put word is changing, the preferred sense is number 3 of the verb, and this sense falls
under the concept c-replacing in the hierarchy. This tagging was produced even
though the parser was unable to construct a complete and correct syntactic represen-
tation of the text. In fact, when tested on the Wall Street Journal texts (for which there
has been no adaptation or customization aside from processing by a company-name
recognizer [Rau 19911), the system rarely produces a single correct parse; however,
the partial parses produced generally cover most of the text at the clause level. Since
most semantic preferences appear at this level (and those that do not, do not depend
on syntactic analysis), the results of this tagging are encouraging.
This example also shows some of the limitations of our system in practice. The
system is unable to recognize the collocation &amp;quot;hold on to&amp;quot; in the first sentence, because
it lacks a pattern for it. The system also lacks patterns for the collocations &amp;quot;vote on&amp;quot;
and &amp;quot;all-time players&amp;quot; that occur in the second sentence, and as a result, mistakenly
tags on as c-temporal-proximity-rel rather than something more appropriate, such
as c-purpose-r el. These difficulties point out the need for even more knowledge.
It is encouraging to note that, even if our encoding scheme is not entirely &amp;quot;correct&amp;quot;
according to human intuition, as long as it is consistent, in theory it should lead to
capabilities that are no worse, with zero customization, than word-based methods for
information retrieval. However, having access to sense tags allows for easy improve-
ment by more knowledge-intensive methods. Although this theory is still untested,
there is some preliminary evidence that word sense tagging can improve information
retrieval system performance (Krovetz 1989).
To date we have been unable to get a meaningful quantitative assessment of the
accuracy of the system&apos;s sense tagging. We made an unsuccessful attempt at evaluat-
ing the accuracy of sense-tagging over a corpus. First, we discovered that a human
&amp;quot;expert&amp;quot; had great difficulty identifying each sense, and that this task was far more te-
dious than manual part-of-speech tagging or bracketing. Second, we questioned what
we would learn from the evaluation of these partial results, and have since turned our
</bodyText>
<page confidence="0.987413">
26
</page>
<note confidence="0.629174">
Susan W. McRoy Using Multiple Knowledge Sources
</note>
<figure confidence="0.73016224">
[the det_1 (c-definite-qual) ]
[network noun_2 (c-entertainment-obj c-business-org c-system) ]
[also adv_l (c-numeric-qual) ]
[is *aux* ]
[changing verb_3 (c-replacing) ]
[its ppnoun_l (c-obj) ]
[halftime noun_1 (c-entity)
[show c-act-of-verb_show1 (c-manifesting) I
[to *infl* ]
[include verb_2 (c-grouping) ]
[viewer c-verb_view2-er (c-entity) ]
[participation c-result-of-being-verb_participate1 (c-causal-state) I
[*comma* *punct*
[in prep_27 (c-group-part) I
[an det_1 (c-definite-qual) ]
[attempt c-act-of-verb_attempt1 (c-attempting) ]
[to *infl* ]
[hold verb_4 (c-positioning) ]
[on adv_1 (c-range-qual c-continuity-qual) I
[to prep_1 (c-destination-rel) ]
[its ppnoun_1 (c-obj) I
[audience noun_1 (c-human-group) I
[through prep_1 (c-course-rel)
[halftime noun_1 (c-entity) ]
[and coordconj_1 (c-conjunction) I
[into prep_5 (c-engage-in) ]
[the det_1 (c-definite-qual) ]
[second c-numword_two1-th (c-order-qual) ]
[halves noun_l (c-portion-part) ]
[of prep_8 (c-stateobject-rel) ]
[games noun_1 (c-recreation-obj) ]
[*period* *punct* I
[one noun_1 (c-entity) ]
[show c-act-of-verb_showl (c-manifesting) ]
[will *aux* ]
[ask verb_2 (c-asking) ]
[viewers c-verb_view2-er (c-entity)
[to *infl* ]
[vote verb_l (c-selecting) ]
[on prep_4 (c-temporal-proximity-rel) ]
[their ppnoun_l (c-obj)
[favorite adj_1 (c-importance-qual c-superiority-qual) ]
[all det_1 (c-quantifier) ]
[*hyphen* *punct*
[time noun_l (c-indef-time-period) ]
[players c-verb_play1-er (c-entity)
[through prep_1 (c-course-rel)
[telephone noun_l (c-machine)
[polls c-act-of-verb_polll (c-asking) ]
[*period* *punct*
</figure>
<figureCaption confidence="0.6385995">
Figure 20
A sample sense coding.
</figureCaption>
<bodyText confidence="0.9852675">
attention back to evaluating the system with respect to some task, such as information
retrieval.
Improving the quality of our sense tagging requires a fair amount of straight-
forward but time-consuming work. This needed work includes filling a number of
</bodyText>
<page confidence="0.988468">
27
</page>
<note confidence="0.601528">
Computational Linguistics Volume 18, Number 1
</note>
<bodyText confidence="0.9999057">
gaps in our knowledge sources. For example, the system needs much more informa-
tion about role-related preferences and specialized semantic contexts. At present all
this information is collected and coded by hand, although recent work by Ravin (1990)
and Dahlgren, McDowell, and Stabler (1989) suggests that the collection of role-related
information may be automatable.
Our next step is to evaluate the effect of text coding on an information retrieval
task, by applying traditional term-weighted statistical retrieval methods to the re-
coded text. One intriguing aspect of this approach is that errors in distinguishing
sense preferences should not be too costly in this task, so long as the program is fairly
consistent in its disambiguation of terms in both the source texts and the input queries.
</bodyText>
<sectionHeader confidence="0.965468" genericHeader="conclusions">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.999936619047619">
Having access to a large amount of information and being able to use it effectively
are essential for understanding unrestricted texts, such as newspaper articles. We have
developed a substantial knowledge base for text processing, including a word sense-
based lexicon that contains both core senses and dynamically triggered entries. We
have also created a number of concept-cluster definitions describing common semantic
contexts and a conceptual hierarchy that acts as a sense-disambiguated thesaurus.
Our approach to word sense discrimination uses information drawn from the
knowledge base and the structure of the text, combining the strongest, most obvious
sense preferences created by syntactic tags, word frequencies, collocations, semantic
context (clusters), selectional restrictions, and syntactic cues. To apply this information
most efficiently, the approach introduces a preprocessing phase that uses preference
information available prior to parsing to eliminate some of the lexical ambiguity and
establish baseline preferences. Then, during parsing, the system combines the baseline
preferences with preferences created by selectional restrictions and syntactic cues to
identify preferred interpretations. The preference combination mechanism of the sys-
tem uses dynamic measures of strength based on specificity, rather than relying on
some fixed, ordered set of rules.
There are some encouraging results from applying the system to sense tagging of
arbitrary text. We expect to evaluate our approach on tasks in information retrieval,
and, later, machine translation, to determine the likelihood of achieving substantive
improvements through sense-based semantic analysis.
</bodyText>
<sectionHeader confidence="0.937613" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994239923076923">
I am grateful to Paul Jacobs for his
comments and his encouragement of my
work on natural language processing at GE;
to George Krupka for helping me integrate
my work with TRUMP, and for continuing
to improve the system; to Graeme Hirst for
his many comments and suggestions on this
article; and to Jan Wiebe and Evan Steeg for
their comments on earlier drafts.
I acknowledge the financial support of the
General Electric Company, the University of
Toronto, and the Natural Sciences and
Engineering Research Council of Canada.
</bodyText>
<sectionHeader confidence="0.924687" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98965415">
Altmann, Gerry, and Steedman, Mark
(1988). &amp;quot;Interaction with context during
human sentence processing.&amp;quot; Cognition
30(3): 191-238.
Charniak, Eugene (1983). &amp;quot;Passing markers:
A theory of contextual influence in
language comprehension.&amp;quot; Cognitive
Science 7(3): 171-190.
Church, Kenneth W. (1988). &amp;quot;A stochastic
parts program and noun phrase parser for
unrestricted text.&amp;quot; In Proceedings, Second
Conference on Applied Natural Language
Processing. Austin, Texas, 136-143.
Church, Kenneth W., and Hanks, Patrick
(1990). &amp;quot;Word association norms, mutual
information, and lexicography.&amp;quot;
Computational Linguistics 16(1): 22-29.
Cowie, Anthony P., and Mackin, Ronald
(1975). Verbs with Prepositions and Particles.
Volume 1 of Oxford Dictionary of Current
</reference>
<page confidence="0.997493">
28
</page>
<note confidence="0.949244">
Susan W. McRoy Using Multiple Knowledge Sources
</note>
<reference confidence="0.998427844262295">
Idiomatic English. Oxford: Oxford
University Press.
Cowie, Anthony P.; Mackin, Ronald; and
McCraig, Isabel R. (1983). Clause and
Sentence Idioms. Volume 2 of Oxford
Dictionary of Current Idiomatic English.
Oxford: Oxford University Press.
Crain, Stephen, and Steedman, Mark (1985).
&amp;quot;On not being led up the garden path:
The use of context by the psychological
syntax processor.&amp;quot; In Natural Language
Parsing: Psychological, Computational, and
Theoretical Perspectives, edited by
D. R. Dowty, L. Karttunen, and
A. M. Zwicky, 320-358. Cambridge,
England: Cambridge University Press.
Dahlgren, Kathleen; McDowell, Joyce; and
Stabler, Edward (1989). &amp;quot;Knowledge
representation for commonsense
reasoning with text.&amp;quot; Computational
Linguistics 15(3): 149-170.
De Marcken, Carl G. (1990). &amp;quot;Parsing the
LOB corpus.&amp;quot; In Proceedings, 28th Annual
Meeting of the Association for Computational
Linguistics. Pittsburgh, PA, 243-251.
Dyer, Michael, and Zernik, Un (1986).
&amp;quot;Encoding and acquiring meanings for
figurative phrases.&amp;quot; In Proceedings, 24th
Annual Meeting of the Association for
Computational Linguistics. New York, NY,
106-111.
Fodor, Janet Dean (1978). &amp;quot;Parsing strategies
and constraints on transformations.&amp;quot;
Linguistic Inquiry 9(3): 427-473.
Fodor, Janet Dean, and Frazier, Lyn (1980).
&amp;quot;Is the human sentence parsing
mechanism an ATN?&amp;quot; Cognition 8:
417-459.
Ford, Marilyn; Bresnan, Joan; and Kaplan,
Ronald (1982). &amp;quot;A competence-based
theory of syntactic closure.&amp;quot; In The Mental
Representation of Grammatical Relations,
edited by Joan Bresnan, 727-796.
Cambridge: The MIT Press.
Fox, E.; Nutter, T.; Ahlswede, T.; Evens, M.;
and Markowitz, J. (1988). &amp;quot;Building a
large thesaurus for information retrieval.&amp;quot;
In Proceedings, Second Conference on Applied
Natural Language Processing. Austin, Texas,
101-108.
Frazier, Lyn, and Rayner, Keith (1990).
&amp;quot;Taking on semantic commitments:
Processing multiple meanings vs.
multiple senses.&amp;quot; Journal of Memory and
Language 29(2): 181-200.
Garside, Roger; Leech, Geoffrey; and
Sampson, Geoffrey (1987). The
Computational Analysis of English: A
Corpus-Based Approach. London: Longman.
Gibson, Edward (1990). &amp;quot;Memory capacity
and sentence processing.&amp;quot; In Proceedings,
28th Annual Meeting of the Association for
Computational Linguistics. Pittsburgh, PA,
39-46.
Halliday, Michael, and Hasan, Ruqaiya
(1976). Cohesion in English. London:
Longman.
Hendler, James A. (1987). Integrating
Marker-Passing and Problem Solving.
Norwood, NJ: Lawrence Erlbaum
Associates.
Hirst, Graeme (1987). Semantic Interpretation
and the Resolution of Ambiguity. Cambridge:
Cambridge University Press.
Jacobs, Paul S. (1986). &amp;quot;Language analysis in
not-so-limited domains.&amp;quot; In Proceedings,
Fall joint Computer Conference. Dallas, TX.
Jacobs, Paul S. (1987). &amp;quot;A knowledge
framework for natural language
analysis.&amp;quot; In Proceedings, Tenth
International Joint Conference on Artificial
Intelligence. Milan, Italy.
Jacobs, Paul S. (1989). &amp;quot;TRUMP: A
transportable language understanding
program.&amp;quot; Technical Report CRD89/181,
GE Research and Development Center,
Schenectady, NY.
Janssen, Sylvia (1990). &amp;quot;Automatic sense
disambiguation with LDOCE: Enriching
syntactically analyzed corpora with
semantic data.&amp;quot; In Theory and Practice in
Corpus Linguistics, edited by Jan Aarts and
Willem Meijs, 105-135. Amsterdam:
Rodopi.
Kimball, John P. (1973). &amp;quot;Seven principles of
surface structure parsing in natural
language.&amp;quot; Cognition 2: 15-47.
Krovetz, Robert (1989). &amp;quot;Lexical acquisition
and information retrieval.&amp;quot; In First
International Lexical Acquisition Workshop,
edited by Uri Zernik.
Kurtzman, Howard S. (1984). &amp;quot;Studies in
syntactic ambiguity resolution.&amp;quot; Doctoral
dissertation, Department of Psychology,
MIT. Bloomington, IN: Indiana University
Linguistics Club.
Magerman, David M., and Marcus,
Mitchell P. (1990). &amp;quot;Parsing a natural
language using mutual information
statistics.&amp;quot; In AAAI-90 Proceedings, Eighth
National Conference on Artificial Intelligence,
Menlo Park, CA: AAAI Press/The MIT
Press, 984-989.
McRoy, Susan W., and Hirst, Graeme (1990).
&amp;quot;Race-based syntactic attachment.&amp;quot;
Cognitive Science 14(3): 313-354.
Morris, Jane (1988). &amp;quot;Lexical cohesion, the
thesaurus, and the structure of text.&amp;quot;
Technical Report CSRI-219, Computer
Systems Research Institute, University of
Toronto, Toronto.
Morris, Jane, and Hirst, Graeme (1991).
</reference>
<page confidence="0.9432">
29
</page>
<reference confidence="0.990686012658228">
Computational Linguistics Volume 18, Number 1
&amp;quot;Lexical cohesion computed by thesaural
relations as an indicator of the structure
of text.&amp;quot; Computational Linguistics 17(1):
21-48.
Procter, Paul, editor (1978). Longman
Dictionary of Contemporary English. Harlow:
Longman Group Ltd.
Rau, Lisa F. (1991). &amp;quot;Extracting company
names from text.&amp;quot; In IEEE Al Applications
Conference (CAIA).
Rau, Lisa E; Jacobs, Paul S.; and Zernik, Uri
(1989). &amp;quot;Information extraction and text
summarization using linguistic
knowledge acquisition.&amp;quot; Information
Processing and Management 25(4): 419-428.
Ravin, Yael (1990). &amp;quot;Disambiguating and
interpreting verb definitions.&amp;quot; In
Proceedings, 28th Annual Meeting of the
Association for Computational Linguistics.
Pittsburgh, PA, 260-267.
Rayner, Keith; Carlson, Marcia; and Frazier,
Lyn (1983). &amp;quot;The interaction of syntax and
semantics during sentence processing: Eye
movements in the analysis of semantically
biased sentences.&amp;quot; Journal of Verbal
Learning and Verbal Behavior 22: 358-374.
Schank, Roger C., and Abelson, Robert P.
(1977). Scripts, Plans, Goals, and
Understanding. Halsted, NJ: Lawrence
Erlbaum.
Schubert, Lenhart (1986). &amp;quot;Are there
preference trade-offs in attachment
decisions?&amp;quot; In Proceedings, National
Conference on Artificial Intelligence
(AAAI-86). Philadelphia, PA, 601-605.
Shieber, Stuart M. (1983). &amp;quot;Sentence
disambiguation by a shift-reduce parsing
technique.&amp;quot; In Proceedings, 21st Annual
Meeting of the Association for Computational
Linguistics. Cambridge, MA, 113-118.
Simpson, Greg B., and Burgess, Curt (1988).
&amp;quot;Implications of lexical ambiguity
resolution for word recognition and
comprehension.&amp;quot; In Lexical Ambiguity
Resolution, edited by Steven L. Small,
Garrison W. Cottrell, and Michael
K. Tanenhaus, 271-288. San Mateo, CA:
Morgan Kaufmann Publishers.
Smadja, Frank A., and McKeown,
Kathleen R. (1990). &amp;quot;Automatically
extracting and representing collocations
for language generation.&amp;quot; In Proceedings,
28th Annual Meeting of the Association for
Computational Linguistics. Pittsburgh, PA,
252-259.
Stock, Oliviero (1989). &amp;quot;Parsing with
flexibility, dynamic strategies, and idioms
in mind.&amp;quot; Computational Linguistics 15(1):
1-18.
Whittemore, Greg; Ferrara, Kathleen; and
Brunner, Hans (1990). &amp;quot;Empirical study of
predictive powers of simple attachment
schemes for post-modifier prepositional
phrases.&amp;quot; In Proceedings, 28th Annual
Meeting of the Association for Computational
Linguistics. Pittsburgh, PA, 23-30.
Wilks, Yorick; Huang, Xiuming; and Fass,
Dan (1985). &amp;quot;Syntax, preference, and right
attachment.&amp;quot; In Proceedings, Ninth
International Joint Conference on Artificial
Intelligence. Los Angeles, CA.
Zernik, Uri (1990). &amp;quot;Tagging word senses in
corpus: The needle in the haystack
revisited.&amp;quot; Technical Report 90CRD198,
GE Research and Development Center,
Schenectedy, NY. In Proceedings, AAAI
Symposium on Text-Based Intelligent Systems.
Stanford, CA, 25-29.
</reference>
<page confidence="0.99881">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.803785">
<title confidence="0.9998035">Using Multiple Knowledge Sources for Word Sense Discrimination</title>
<author confidence="0.999797">Susan W McRoy</author>
<affiliation confidence="0.924987">Artificial Intelligence Program GE Research and Development Center</affiliation>
<abstract confidence="0.992915">This paper addresses the problem of how to identify the intended meaning of individual words in unrestricted texts, without necessarily having access to complete representations of sentences. To discriminate senses, an understander can consider a diversity of information, including syntactic tags, word frequencies, collocations, semantic context, role-related expectations, and syntactic restrictions. However, current approaches make use of only small subsets of this information. Here we will describe how to use the whole range of information. Our discussion will include how the preference cues relate to general lexical and conceptual knowledge and to more specialized knowledge of collocations and contexts. We will describe a method of combining cues on the basis their individual than a fixed ranking among cue-types. We will also discuss an application of the approach in a system that computes sense tags for arbitrary texts, even when it is unable to determine a single syntactic or semantic representation for some sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gerry Altmann</author>
<author>Mark Steedman</author>
</authors>
<title>Interaction with context during human sentence processing.&amp;quot;</title>
<date>1988</date>
<journal>Cognition</journal>
<volume>30</volume>
<issue>3</issue>
<pages>191--238</pages>
<contexts>
<context position="14846" citStr="Altmann and Steedman 1988" startWordPosition="2311" endWordPosition="2314">te the semantically best one. Such rules normally also collapse the attachment problem into the conceptual role filling problem. For example, a lexical preference rule specifies that the preference for a particular attachment depends on how strongly or weakly the verb of the clause prefers its possible arguments (Fodor 1978; Ford, Bresnan, and Kaplan 1982). Pragmatic rules also intermingle sense discrimination and attachment, but consider the context of the utterance. For example, one suggested rule says to prefer to build structures describing objects just mentioned (Crain and Steedman 1985; Altmann and Steedman 1988). The accuracy of systems with fixed-order rules is limited by the fact that it is not always possible to strictly order a set of rules independent of the context. For example, Dahlgren, McDowell, and Stabler (1989) propose the rule &amp;quot;If the object of the preposition is an expression of time, then S-attach the PP&amp;quot; to explain the preference for assuming that &amp;quot;in the afternoon&amp;quot; modifies adjourn in Example 2: Example 2 The judge adjourned the hearing in the afternoon. Although they admit this rule would fail for a sentence like John described the meeting on January 20th, where the NP has a lexical</context>
</contexts>
<marker>Altmann, Steedman, 1988</marker>
<rawString>Altmann, Gerry, and Steedman, Mark (1988). &amp;quot;Interaction with context during human sentence processing.&amp;quot; Cognition 30(3): 191-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Passing markers: A theory of contextual influence in language comprehension.&amp;quot;</title>
<date>1983</date>
<journal>Cognitive Science</journal>
<volume>7</volume>
<issue>3</issue>
<pages>171--190</pages>
<marker>Charniak, 1983</marker>
<rawString>Charniak, Eugene (1983). &amp;quot;Passing markers: A theory of contextual influence in language comprehension.&amp;quot; Cognitive Science 7(3): 171-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, Second Conference on Applied Natural Language Processing.</booktitle>
<pages>136--143</pages>
<location>Austin, Texas,</location>
<contexts>
<context position="51914" citStr="Church 1988" startWordPosition="7895" endWordPosition="7896"> a tagging program, written by Uri Zernik (1990), that takes information about the root, affix, and possible syntactic category for each word and applies stochastic techniques to select a syntactic tag for each word. Stochastic taggers look at small groups of words and pick the most likely assignment of tags, determined by the frequency of alternative syntactic patterns in similar texts. Although it may not be possible to completely disambiguate all words prior to parsing, approaches based on 18 Susan W. McRoy Using Multiple Knowledge Sources stochastic information have been quite successful (Church 1988; Garside, Leech, and Sampson 1987; de Marcken 1990).4 To allow for the fact that the tagger may err, as part of the tagging process the system makes a second pass through the text to remove some systematic errors that result from biases common to statistical approaches. For example, they tend to prefer modifiers over nouns and nouns over verbs; for instance, in Example 5, the tagger erroneously marks the word need as a noun. Example 5 You really need the Campbell Soups of the world to be interested in your magazine. In this second pass, the system applies a few rules derived from our grammar </context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, Kenneth W. (1988). &amp;quot;A stochastic parts program and noun phrase parser for unrestricted text.&amp;quot; In Proceedings, Second Conference on Applied Natural Language Processing. Austin, Texas, 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.&amp;quot;</title>
<date>1990</date>
<journal>Computational Linguistics</journal>
<volume>16</volume>
<issue>1</issue>
<pages>22--29</pages>
<contexts>
<context position="36160" citStr="Church and Hanks 1990" startWordPosition="5547" endWordPosition="5550">ttern (TAKE (A) (ADJ) BATH) matches the clauses take a hot bath and takes hot baths. In a pattern, parentheses indicate optionality; the system encodes the repeatability of a category, such as adjectives, procedurally. Currently, there are patterns for verb-particle, verb-preposition, and verb-object collocations, as well as compound nouns. Initially, we acquired patterns for verb-object collocations by analyzing lists of root word pairs that were weighted for relative co-occurrence in a corpus of articles 12 Susan W. McRoy Using Multiple Knowledge Sources from the Dow Jones News Service (cf. Church and Hanks 1990; Smadja and McKeown 1990). As an example of the kind of data that we derived, Figure 8 shows the ten most frequent co-occurrences involving the root &amp;quot;take.&amp;quot; Note that the collocation &amp;quot;take action&amp;quot; appears both in its active form (third in the list), as well as its passive, actions were taken (fifth in the list). From an examination of these lists and the contexts in which the pairs appeared in the corpus, we constructed the patterns used by TRUMP to identify collocations. Then, using the patterns as a guide, we added lexical entries for each collocation. (Figure 9 lists some of the entries fo</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Church, Kenneth W., and Hanks, Patrick (1990). &amp;quot;Word association norms, mutual information, and lexicography.&amp;quot; Computational Linguistics 16(1): 22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony P Cowie</author>
<author>Ronald Mackin</author>
</authors>
<date>1975</date>
<booktitle>Verbs with Prepositions and Particles. Volume 1 of Oxford Dictionary of Current Idiomatic English.</booktitle>
<publisher>University Press.</publisher>
<location>Oxford: Oxford</location>
<contexts>
<context position="33131" citStr="Cowie and Mackin 1975" startWordPosition="5081" endWordPosition="5084">t the expression of role-related preferences from any perspective. 3.3 Collocational Patterns Collocation is the relationship among any group of words that tend to co-occur in a predictable configuration. Although collocations seem to have a semantic basis, many collocations are best recognized by their syntactic form. Thus, for current purposes, we limit the use of the term &amp;quot;collocation&amp;quot; to sense preferences that result from these well-defined syntactic constructions.1 For example, the particle combination pick up 1 Traditionally many of these expressions have been categorized as idioms (see Cowie and Mackin 1975; Cowie, Mackin, and McCraig 1983), but as most are at least partly compositional and can be processed by normal parsing methods, we prefer to use the more general term &amp;quot;collocation.&amp;quot; This categorization thus happily encompasses both the obvious idioms and the compositional expressions whose status as idioms is highly debatable. Our use of the term is thus similar to that of Smadja and McKeown, who partition collocations into open compounds, predicative relations, and idiomatic expressions (Smadja and McKeown 1990). 11 Computational Linguistics Volume 18, Number 1 Perspective Preference holder</context>
</contexts>
<marker>Cowie, Mackin, 1975</marker>
<rawString>Cowie, Anthony P., and Mackin, Ronald (1975). Verbs with Prepositions and Particles. Volume 1 of Oxford Dictionary of Current Idiomatic English. Oxford: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony P Cowie</author>
<author>Ronald Mackin</author>
<author>Isabel R McCraig</author>
</authors>
<title>Clause and Sentence Idioms.</title>
<date>1983</date>
<booktitle>of Oxford Dictionary of Current Idiomatic English.</booktitle>
<volume>2</volume>
<publisher>University Press.</publisher>
<location>Oxford: Oxford</location>
<marker>Cowie, Mackin, McCraig, 1983</marker>
<rawString>Cowie, Anthony P.; Mackin, Ronald; and McCraig, Isabel R. (1983). Clause and Sentence Idioms. Volume 2 of Oxford Dictionary of Current Idiomatic English. Oxford: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Crain</author>
<author>Mark Steedman</author>
</authors>
<title>On not being led up the garden path: The use of context by the psychological syntax processor.&amp;quot;</title>
<date>1985</date>
<booktitle>In Natural Language Parsing: Psychological, Computational, and Theoretical Perspectives, edited</booktitle>
<location>and</location>
<contexts>
<context position="14818" citStr="Crain and Steedman 1985" startWordPosition="2307" endWordPosition="2310">s and attachments to locate the semantically best one. Such rules normally also collapse the attachment problem into the conceptual role filling problem. For example, a lexical preference rule specifies that the preference for a particular attachment depends on how strongly or weakly the verb of the clause prefers its possible arguments (Fodor 1978; Ford, Bresnan, and Kaplan 1982). Pragmatic rules also intermingle sense discrimination and attachment, but consider the context of the utterance. For example, one suggested rule says to prefer to build structures describing objects just mentioned (Crain and Steedman 1985; Altmann and Steedman 1988). The accuracy of systems with fixed-order rules is limited by the fact that it is not always possible to strictly order a set of rules independent of the context. For example, Dahlgren, McDowell, and Stabler (1989) propose the rule &amp;quot;If the object of the preposition is an expression of time, then S-attach the PP&amp;quot; to explain the preference for assuming that &amp;quot;in the afternoon&amp;quot; modifies adjourn in Example 2: Example 2 The judge adjourned the hearing in the afternoon. Although they admit this rule would fail for a sentence like John described the meeting on January 20th</context>
</contexts>
<marker>Crain, Steedman, 1985</marker>
<rawString>Crain, Stephen, and Steedman, Mark (1985). &amp;quot;On not being led up the garden path: The use of context by the psychological syntax processor.&amp;quot; In Natural Language Parsing: Psychological, Computational, and Theoretical Perspectives, edited by D. R. Dowty, L. Karttunen, and</rawString>
</citation>
<citation valid="false">
<authors>
<author>A M Zwicky</author>
</authors>
<pages>320--358</pages>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge, England:</location>
<marker>Zwicky, </marker>
<rawString>A. M. Zwicky, 320-358. Cambridge, England: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen Dahlgren</author>
<author>Joyce McDowell</author>
<author>Edward Stabler</author>
</authors>
<title>Knowledge representation for commonsense reasoning with text.&amp;quot;</title>
<date>1989</date>
<journal>Computational Linguistics</journal>
<volume>15</volume>
<issue>3</issue>
<pages>149--170</pages>
<marker>Dahlgren, McDowell, Stabler, 1989</marker>
<rawString>Dahlgren, Kathleen; McDowell, Joyce; and Stabler, Edward (1989). &amp;quot;Knowledge representation for commonsense reasoning with text.&amp;quot; Computational Linguistics 15(3): 149-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl G De Marcken</author>
</authors>
<title>Parsing the LOB corpus.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>243--251</pages>
<location>Pittsburgh, PA,</location>
<marker>De Marcken, 1990</marker>
<rawString>De Marcken, Carl G. (1990). &amp;quot;Parsing the LOB corpus.&amp;quot; In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics. Pittsburgh, PA, 243-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Dyer</author>
<author>Un Zernik</author>
</authors>
<title>Encoding and acquiring meanings for figurative phrases.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, 24th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>106--111</pages>
<location>New York, NY,</location>
<contexts>
<context position="37648" citStr="Dyer and Zernik 1986" startWordPosition="5786" endWordPosition="5789">at the parser can use to verify the presence of a collocation. For example, Figure 10 shows the entry for the noncompositional collocation take place, which requires that the object (r ail*) be singular and determinerless. These entries differ from similar representations of collocations or idioms in Smadja and McKeown (1990) and Stock (1989), in that they are sense-based rather than wordbased. That is, instead of expressing collocations as word-templates, the lexicon groups together collocations that combine the same sense of the head verb with particular senses or higher-level concepts (cf. Dyer and Zernik 1986). This approach better addresses the fact that collocations do have a semantic basis, capturing general forms such as give him or her (some temporal object), which underlies the collocations give month, give minute, and give time. Currently, the system has entries for over 1700 such collocations. 3.4 Cluster Definitions The last source of sense preferences we need to consider is the semantic context. Work on lexical cohesion suggests that people use words that repeat a conceptual category or that have a semantic association to each other to create unity in text (Morris 1988; Morris and Hirst 1</context>
</contexts>
<marker>Dyer, Zernik, 1986</marker>
<rawString>Dyer, Michael, and Zernik, Un (1986). &amp;quot;Encoding and acquiring meanings for figurative phrases.&amp;quot; In Proceedings, 24th Annual Meeting of the Association for Computational Linguistics. New York, NY, 106-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Dean Fodor</author>
</authors>
<title>Parsing strategies and constraints on transformations.&amp;quot;</title>
<date>1978</date>
<journal>Linguistic Inquiry</journal>
<volume>9</volume>
<issue>3</issue>
<pages>427--473</pages>
<contexts>
<context position="14545" citStr="Fodor 1978" startWordPosition="2270" endWordPosition="2271">nal node on the rightmost branch of the current structure (i.e., in the same structure as the last word processed) (Kimball 1973). Semantic rules, by contrast, intertwine the problems of discrimination and attachment; one must examine all combinations of senses and attachments to locate the semantically best one. Such rules normally also collapse the attachment problem into the conceptual role filling problem. For example, a lexical preference rule specifies that the preference for a particular attachment depends on how strongly or weakly the verb of the clause prefers its possible arguments (Fodor 1978; Ford, Bresnan, and Kaplan 1982). Pragmatic rules also intermingle sense discrimination and attachment, but consider the context of the utterance. For example, one suggested rule says to prefer to build structures describing objects just mentioned (Crain and Steedman 1985; Altmann and Steedman 1988). The accuracy of systems with fixed-order rules is limited by the fact that it is not always possible to strictly order a set of rules independent of the context. For example, Dahlgren, McDowell, and Stabler (1989) propose the rule &amp;quot;If the object of the preposition is an expression of time, then S</context>
</contexts>
<marker>Fodor, 1978</marker>
<rawString>Fodor, Janet Dean (1978). &amp;quot;Parsing strategies and constraints on transformations.&amp;quot; Linguistic Inquiry 9(3): 427-473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Dean Fodor</author>
<author>Lyn Frazier</author>
</authors>
<title>Is the human sentence parsing mechanism an ATN?&amp;quot;</title>
<date>1980</date>
<journal>Cognition</journal>
<volume>8</volume>
<pages>417--459</pages>
<contexts>
<context position="13545" citStr="Fodor and Frazier 1980" startWordPosition="2114" endWordPosition="2117"> a strong cue for disambiguating prepositions, but this cue appears fairly infrequently [Whittemore, Ferrara, and Brunner 19901.) The problem of determining role relationships entangles word sense discrimination with the problem of syntactic attachment. The attachment problem is a direct result of the ambiguity in determining whether a concept is related to an adjacent object, or to some enveloping structure that incorporates the adjacent object. Most proposed solutions to this problem specify a fixed set of ordered rules that a system applies until a unique, satisfactory attachment is found (Fodor and Frazier 1980; Wilks, Huang, and Fass 1985; Shieber 1983; Hirst 1987; Dahlgren, McDowell, and Stabler 1989). Such rules can be either syntactic, semantic, or pragmatic. Syntactic rules attempt to solve the attachment problem independent of the sense discrimination problem. For example, a rule for Right Association (also known as Late Closure) says to prefer attaching a new word to the lowest nonterminal node on the rightmost branch of the current structure (i.e., in the same structure as the last word processed) (Kimball 1973). Semantic rules, by contrast, intertwine the problems of discrimination and atta</context>
</contexts>
<marker>Fodor, Frazier, 1980</marker>
<rawString>Fodor, Janet Dean, and Frazier, Lyn (1980). &amp;quot;Is the human sentence parsing mechanism an ATN?&amp;quot; Cognition 8: 417-459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Ford</author>
<author>Joan Bresnan</author>
<author>Ronald Kaplan</author>
</authors>
<title>A competence-based theory of syntactic closure.&amp;quot;</title>
<date>1982</date>
<booktitle>In The Mental Representation of Grammatical Relations, edited by Joan Bresnan,</booktitle>
<pages>727--796</pages>
<publisher>The MIT Press.</publisher>
<location>Cambridge:</location>
<marker>Ford, Bresnan, Kaplan, 1982</marker>
<rawString>Ford, Marilyn; Bresnan, Joan; and Kaplan, Ronald (1982). &amp;quot;A competence-based theory of syntactic closure.&amp;quot; In The Mental Representation of Grammatical Relations, edited by Joan Bresnan, 727-796. Cambridge: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Fox</author>
<author>T Nutter</author>
<author>T Ahlswede</author>
<author>M Evens</author>
<author>J Markowitz</author>
</authors>
<title>Building a large thesaurus for information retrieval.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, Second Conference on Applied Natural Language Processing.</booktitle>
<pages>101--108</pages>
<location>Austin, Texas,</location>
<contexts>
<context position="29085" citStr="Fox et al. 1988" startWordPosition="4489" endWordPosition="4492">information necessary to recognize morphological preferences, sense preferences, and syntactic cues. They also provide some of the information required to verify and interpret collocations. Sections 3.2, 3.3, and 3.4, below, describe sources of information that enable a system to recognize role-based preferences, collocations, and the semantic context. 3.2 The Concept Hierarchy The concept hierarchy serves several purposes. First, it associates word senses that are siblings or otherwise closely related in the hierarchy, thus providing a thesaurus for information retrieval and other tasks (cf. Fox et al. 1988). In a sense tagging system, these associations can help determine the semantic context. Second, it supplies the basic ontology to which domain knowledge can be associated, so that each new domain requires only incremental knowledge engineering. Third, it allows role-based preferences, wherever possible, to apply to groups of word senses rather than just individual lexical entries. To see how the hierarchy&apos;s concept definitions establish the basic ontology, consider Figure 3, the definition of the concept c-recording. c-recording is the parent concept for activities involving the storage of in</context>
</contexts>
<marker>Fox, Nutter, Ahlswede, Evens, Markowitz, 1988</marker>
<rawString>Fox, E.; Nutter, T.; Ahlswede, T.; Evens, M.; and Markowitz, J. (1988). &amp;quot;Building a large thesaurus for information retrieval.&amp;quot; In Proceedings, Second Conference on Applied Natural Language Processing. Austin, Texas, 101-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
<author>Keith Rayner</author>
</authors>
<title>Taking on semantic commitments: Processing multiple meanings vs. multiple senses.&amp;quot;</title>
<date>1990</date>
<journal>Journal of Memory and Language</journal>
<volume>29</volume>
<issue>2</issue>
<pages>181--200</pages>
<contexts>
<context position="21188" citStr="Frazier and Rayner 1990" startWordPosition="3295" endWordPosition="3298">esign, includes only coarse distinctions between word senses. This means that, for a task such as generating databases from text, task-specific processing or inference must augment the core lexical knowledge, but problems of considering many nuances of meaning or low-frequency senses are avoided. For example, the financial sense of issue (e.g., a new security) falls under the same core sense as the latest issue of a magazine. The &apos;progeny&apos; and &apos;exit&apos; senses of issue are omitted from the lexicon. The idea is to preserve in the core lexicon only the common, coarse distinctions among senses (cf. Frazier and Rayner 1990). Figure 1 shows the lexical entries for the word issue. Each entry has a part of speech, : POS, and a set of core senses, : SENSES. Each sense has a : TYPE field that indicates *primary* for a preferred (primary) sense and *secondary* for a deprecated (secondary) sense. The general rule for determining the : TYPE of a sense is that secondary senses are those that the semantic interpreter should not select without specific contextual information, such as the failure of some selectional restriction pertaining to the primary sense. For example, the word yard can mean an enclosed area, a workplac</context>
</contexts>
<marker>Frazier, Rayner, 1990</marker>
<rawString>Frazier, Lyn, and Rayner, Keith (1990). &amp;quot;Taking on semantic commitments: Processing multiple meanings vs. multiple senses.&amp;quot; Journal of Memory and Language 29(2): 181-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Garside</author>
<author>Geoffrey Leech</author>
<author>Geoffrey Sampson</author>
</authors>
<title>The Computational Analysis of English: A Corpus-Based Approach.</title>
<date>1987</date>
<publisher>Longman.</publisher>
<location>London:</location>
<marker>Garside, Leech, Sampson, 1987</marker>
<rawString>Garside, Roger; Leech, Geoffrey; and Sampson, Geoffrey (1987). The Computational Analysis of English: A Corpus-Based Approach. London: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>Memory capacity and sentence processing.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>39--46</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="58168" citStr="Gibson (1990)" startWordPosition="8874" endWordPosition="8875">inferior parses, reducing the number of ambiguities even further.&apos; As suggested above, the system builds semantic interpretations incrementally. For each proposed combination of syntactic structures, there is a corresponding combination of semantic structures. It is the job of the semantic interpreter to identify the possible relations that link the structures being combined, identify the preferences associated with each possible combination of head, role (relation), and filler (the argument or modifier), and then rank competing semantic interpretations. 5 A similar approach has been taken by Gibson (1990) and is supported by the psychological experiments of Kurtzman (1984). 20 Susan W. McRoy Using Multiple Knowledge Sources For each proposed combination, knowledge sources may contribute the following preferences: • preferences directly associated with the head or the filler, determined recursively from their components, beginning with preferences identified during preprocessing. • preferences associated with syntactic cues, such as the satisfaction of restrictions listed in the lexicon. For example, a word may allow only modifiers of a particular syntactic form, or a modifier may modify only a</context>
</contexts>
<marker>Gibson, 1990</marker>
<rawString>Gibson, Edward (1990). &amp;quot;Memory capacity and sentence processing.&amp;quot; In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics. Pittsburgh, PA, 39-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English.</booktitle>
<publisher>Longman.</publisher>
<location>London:</location>
<contexts>
<context position="38277" citStr="Halliday and Hasan 1976" startWordPosition="5888" endWordPosition="5891">s approach better addresses the fact that collocations do have a semantic basis, capturing general forms such as give him or her (some temporal object), which underlies the collocations give month, give minute, and give time. Currently, the system has entries for over 1700 such collocations. 3.4 Cluster Definitions The last source of sense preferences we need to consider is the semantic context. Work on lexical cohesion suggests that people use words that repeat a conceptual category or that have a semantic association to each other to create unity in text (Morris 1988; Morris and Hirst 1991; Halliday and Hasan 1976). These associations can be thought of as a class of collocations that lack the predictable syntactic structure of, say, collocations arising from verb-particle or compound noun constructions. Since language producers select senses that group together semantically, a language analyzer should prefer senses that share a semantic association. However, it is unclear whether the benefit of knowing the exact nature of an association would justify the cost of determining it. Thus, our system provides a cluster mechanism for representing and identifying groups of senses that are associated in some uns</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday, Michael, and Hasan, Ruqaiya (1976). Cohesion in English. London: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James A Hendler</author>
</authors>
<title>Integrating Marker-Passing and Problem Solving.</title>
<date>1987</date>
<location>Norwood, NJ: Lawrence Erlbaum Associates.</location>
<contexts>
<context position="44956" citStr="Hendler 1987" startWordPosition="6832" endWordPosition="6833">osite stance, however, considering all paths up, down, and across the network unless it is explicitly constrained. Thus a marker passer might find the following dubious path from the &apos;written object&apos; sense of book to the &apos;part-of-a-plant&apos; sense of leaf: [book made-of paper] [paper made-from wood] [tree made-of wood] [tree has-part leaf] whereas no cluster would link these entities, unless there had been some prior evidence of a connection. (The recommended solution to the production of such paths by a marker passer is to prevent the passing of marks through certain kinds of nodes [Hirst 1987; Hendler 19871.) From the lexical entries, the underlying concept hierarchy, and the specialized entries for collocation and clusters just described, a language analyzer can extract the information that establishes preferences among senses. In the next section, we will describe how a semantic interpreter can apply knowledge from such a wide variety of sources. 4. Using Knowledge to Identify Sense Preferences There is a wide variety of information about which sense is the correct one, and the challenge is to decide when and how to use this information. The danger of a combinatorial explosion of possibilitie</context>
</contexts>
<marker>Hendler, 1987</marker>
<rawString>Hendler, James A. (1987). Integrating Marker-Passing and Problem Solving. Norwood, NJ: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Semantic Interpretation and the Resolution of Ambiguity. Cambridge:</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="11503" citStr="Hirst 1987" startWordPosition="1801" endWordPosition="1802">/ money similarly create a bias for the related senses. Despite their apparent strength, however, the preferences created by these cues are not absolute, as other cues may defeat them. For example, although normally the collocation wait on means &apos;serve&apos; (Mary waited on John), the failure of a role-related expectation, such as that the BENEFICIARY be animate, can override this preference (Mary waited on the steps). Thus, collocations and word associations are strong sources of information that an understander must weigh against other cues, and not just treat as rules for sense-filtering (as in Hirst 1987 or Dahlgren, McDowell, and Stabler 1989). The selection of a role relationship can both influence and be influenced by the selection of word senses, because preferences partially constrain the various combinations of a role, its holder, and the filler. For example, the preposition from prefers referring to the SOURCE role; transfers, such as give, prefer to have a DESTINATION role; and instances of colors, such as red, prefer to fill a COLOR role. Approaches based on the word disambiguation model tend to apply constraint satisfaction techniques to combine these role preferences (Hirst 1987). </context>
<context position="13600" citStr="Hirst 1987" startWordPosition="2125" endWordPosition="2126"> fairly infrequently [Whittemore, Ferrara, and Brunner 19901.) The problem of determining role relationships entangles word sense discrimination with the problem of syntactic attachment. The attachment problem is a direct result of the ambiguity in determining whether a concept is related to an adjacent object, or to some enveloping structure that incorporates the adjacent object. Most proposed solutions to this problem specify a fixed set of ordered rules that a system applies until a unique, satisfactory attachment is found (Fodor and Frazier 1980; Wilks, Huang, and Fass 1985; Shieber 1983; Hirst 1987; Dahlgren, McDowell, and Stabler 1989). Such rules can be either syntactic, semantic, or pragmatic. Syntactic rules attempt to solve the attachment problem independent of the sense discrimination problem. For example, a rule for Right Association (also known as Late Closure) says to prefer attaching a new word to the lowest nonterminal node on the rightmost branch of the current structure (i.e., in the same structure as the last word processed) (Kimball 1973). Semantic rules, by contrast, intertwine the problems of discrimination and attachment; one must examine all combinations of senses and</context>
<context position="16108" citStr="Hirst 1987" startWordPosition="2526" endWordPosition="2527">s are not always the determining factor either. The existence of a conceptually similar object in the context (such as &amp;quot;the morning trial&amp;quot;) can also create an expectation for the grouping &amp;quot;hearing in the afternoon,&amp;quot; as in Example 3 below. Example 3 The judge had to leave town for the day. He found a replacement to take over his morning trial, but couldn&apos;t find anyone else that was available. He called the courthouse and cancelled the hearing in the afternoon. Moreover, pragmatic effects are not always the determining factor either, leading many people to judge the following sentence as silly (Hirst 1987). Example 4 The landlord painted all the walls with cracks (Rayner, Carlson, and Frazier 1983). 5 Computational Linguistics Volume 18, Number 1 The presence of different lexical items or different objects in the discourse focus may strengthen or weaken the information provided by an individual rule. Another possibility we will discuss in Section 5 is to weigh all preference information dynamically (cf. Schubert 1986; McRoy and Hirst 1990). The system we will be describing in Section 4 will use many of the cues described above, including syntactic tags, morphology, word associations, and role-r</context>
<context position="43551" citStr="Hirst 1987" startWordPosition="6613" endWordPosition="6614">t verb_assert1. Situational clusters capture the associations found in generic descriptions (cf. Dahlgren, McDowell, and Stabler 1989) or dictionary examples (cf. Janssen 1990), but are more compact because clusters may include whole categories of objects (such as c-law-act ion) as members and need not specify relationships between the members. (As mentioned above, the conceptual hierarchy is the best place for encoding known role-related expectations.) The use of clusters for sense discrimination is also comparable to approaches that favor senses linked by marked paths in a semantic network (Hirst 1987). In fact, clusters capture most of the useful associations found in scripts or semantic networks, but lack many of the disadvantages of using networks. For example, because clusters do not specify what the exact nature of any association is, learning new clusters from previously processed sentences would be fairly straightforward, in contrast to learning new fragments of network. Using clusters also avoids the major problem associated with marker-passing approaches, namely how to prevent the production of stupid paths (or remove them from consideration after they have been produced) (Charniak</context>
<context position="44942" citStr="Hirst 1987" startWordPosition="6830" endWordPosition="6831">akes the opposite stance, however, considering all paths up, down, and across the network unless it is explicitly constrained. Thus a marker passer might find the following dubious path from the &apos;written object&apos; sense of book to the &apos;part-of-a-plant&apos; sense of leaf: [book made-of paper] [paper made-from wood] [tree made-of wood] [tree has-part leaf] whereas no cluster would link these entities, unless there had been some prior evidence of a connection. (The recommended solution to the production of such paths by a marker passer is to prevent the passing of marks through certain kinds of nodes [Hirst 1987; Hendler 19871.) From the lexical entries, the underlying concept hierarchy, and the specialized entries for collocation and clusters just described, a language analyzer can extract the information that establishes preferences among senses. In the next section, we will describe how a semantic interpreter can apply knowledge from such a wide variety of sources. 4. Using Knowledge to Identify Sense Preferences There is a wide variety of information about which sense is the correct one, and the challenge is to decide when and how to use this information. The danger of a combinatorial explosion o</context>
</contexts>
<marker>Hirst, 1987</marker>
<rawString>Hirst, Graeme (1987). Semantic Interpretation and the Resolution of Ambiguity. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul S Jacobs</author>
</authors>
<title>Language analysis in not-so-limited domains.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, Fall joint Computer Conference.</booktitle>
<location>Dallas, TX.</location>
<contexts>
<context position="4297" citStr="Jacobs 1986" startWordPosition="668" endWordPosition="669">an there was some equivalence that extended its arm around the state while the EPA was busy filling safes with trash. Preliminary evidence suggests that having access to a sense tagging of the text improves the performance of information retrieval systems (Krovetz 1989). The primary goal of this paper, then, is to describe in detail methods and knowledge that will enable a language analyzer to tag each word with its sense. To demonstrate that the approach is sufficiently robust for practical tasks, the article will also discuss the incorporation of the approach into an existing system, TRUMP (Jacobs 1986, 1987, 1989), and the application of it to unrestricted texts. The principles that make up the approach are completely general, however, and not just specific to TRUMP. An analyzer whose tasks include word-sense tagging must be able to take an input text, determine the concept that each word or phrase denotes, and identify the role relationships that link these concepts. Because determining this information accurately is knowledge-intensive, the analyzer should be as flexible as possible, requiring a minimum amount of customization for different domains. One way to gain such flexibility is gi</context>
</contexts>
<marker>Jacobs, 1986</marker>
<rawString>Jacobs, Paul S. (1986). &amp;quot;Language analysis in not-so-limited domains.&amp;quot; In Proceedings, Fall joint Computer Conference. Dallas, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul S Jacobs</author>
</authors>
<title>A knowledge framework for natural language analysis.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, Tenth International Joint Conference on Artificial Intelligence.</booktitle>
<location>Milan, Italy.</location>
<marker>Jacobs, 1987</marker>
<rawString>Jacobs, Paul S. (1987). &amp;quot;A knowledge framework for natural language analysis.&amp;quot; In Proceedings, Tenth International Joint Conference on Artificial Intelligence. Milan, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul S Jacobs</author>
</authors>
<title>TRUMP: A transportable language understanding program.&amp;quot;</title>
<date>1989</date>
<tech>Technical Report CRD89/181,</tech>
<institution>GE Research and Development Center,</institution>
<location>Schenectady, NY.</location>
<marker>Jacobs, 1989</marker>
<rawString>Jacobs, Paul S. (1989). &amp;quot;TRUMP: A transportable language understanding program.&amp;quot; Technical Report CRD89/181, GE Research and Development Center, Schenectady, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvia Janssen</author>
</authors>
<title>Automatic sense disambiguation with LDOCE: Enriching syntactically analyzed corpora with semantic data.&amp;quot;</title>
<date>1990</date>
<booktitle>In Theory and Practice in Corpus Linguistics, edited by Jan Aarts and Willem Meijs,</booktitle>
<pages>105--135</pages>
<location>Amsterdam: Rodopi.</location>
<contexts>
<context position="5863" citStr="Janssen (1990)" startWordPosition="914" endWordPosition="915">nt NLP work. Typically, the system will have to choose from several senses for each word. For example, we found that TRUMP&apos;s base of nearly 10,000 root senses and 10,000 derivations provides an average of approximately four senses for each word of a sentence taken from the Wall Street Journal. The potential for combinatoric explosion resulting from such ambiguity makes it critical to resolve ambiguities quickly and reliably. It is unrealistic to assume that word sense discrimination can be left until parsing is complete, as suggested, for example, by Dahlgren, McDowell, and Stabler (1989) and Janssen (1990). No simple recipe can resolve the general problem of lexical ambiguity. Although semantic context and selectional restrictions provide good cues to disambiguation, they are neither reliable enough, nor available quickly enough, to be used alone. The approach to disambiguation that we will take below combines many different, strong 2 Susan W. McRoy Using Multiple Knowledge Sources sources of information: syntactic tags, word frequencies, collocations, semantic context (clusters), selectional restrictions, and syntactic cues. The approach incorporates a number of innovations, including: • a hyb</context>
<context position="43116" citStr="Janssen 1990" startWordPosition="6547" endWordPosition="6548">ustered context or that strongly suggest the clustered context when they occur with some other member of the cluster. Thus, situational clusters are centered upon fairly specific ideas and may correspondingly be very specific with respect to their elements. It is not unusual for a word to be contained in a cluster while its synonyms are not. For example, the cluster cl-courtroom shown in Figure 13 contains sense verb_testify1, but not verb_assert1. Situational clusters capture the associations found in generic descriptions (cf. Dahlgren, McDowell, and Stabler 1989) or dictionary examples (cf. Janssen 1990), but are more compact because clusters may include whole categories of objects (such as c-law-act ion) as members and need not specify relationships between the members. (As mentioned above, the conceptual hierarchy is the best place for encoding known role-related expectations.) The use of clusters for sense discrimination is also comparable to approaches that favor senses linked by marked paths in a semantic network (Hirst 1987). In fact, clusters capture most of the useful associations found in scripts or semantic networks, but lack many of the disadvantages of using networks. For example,</context>
</contexts>
<marker>Janssen, 1990</marker>
<rawString>Janssen, Sylvia (1990). &amp;quot;Automatic sense disambiguation with LDOCE: Enriching syntactically analyzed corpora with semantic data.&amp;quot; In Theory and Practice in Corpus Linguistics, edited by Jan Aarts and Willem Meijs, 105-135. Amsterdam: Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John P Kimball</author>
</authors>
<title>Seven principles of surface structure parsing in natural language.&amp;quot;</title>
<date>1973</date>
<journal>Cognition</journal>
<volume>2</volume>
<pages>15--47</pages>
<contexts>
<context position="14064" citStr="Kimball 1973" startWordPosition="2198" endWordPosition="2199">that a system applies until a unique, satisfactory attachment is found (Fodor and Frazier 1980; Wilks, Huang, and Fass 1985; Shieber 1983; Hirst 1987; Dahlgren, McDowell, and Stabler 1989). Such rules can be either syntactic, semantic, or pragmatic. Syntactic rules attempt to solve the attachment problem independent of the sense discrimination problem. For example, a rule for Right Association (also known as Late Closure) says to prefer attaching a new word to the lowest nonterminal node on the rightmost branch of the current structure (i.e., in the same structure as the last word processed) (Kimball 1973). Semantic rules, by contrast, intertwine the problems of discrimination and attachment; one must examine all combinations of senses and attachments to locate the semantically best one. Such rules normally also collapse the attachment problem into the conceptual role filling problem. For example, a lexical preference rule specifies that the preference for a particular attachment depends on how strongly or weakly the verb of the clause prefers its possible arguments (Fodor 1978; Ford, Bresnan, and Kaplan 1982). Pragmatic rules also intermingle sense discrimination and attachment, but consider t</context>
</contexts>
<marker>Kimball, 1973</marker>
<rawString>Kimball, John P. (1973). &amp;quot;Seven principles of surface structure parsing in natural language.&amp;quot; Cognition 2: 15-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Krovetz</author>
</authors>
<title>Lexical acquisition and information retrieval.&amp;quot;</title>
<date>1989</date>
<booktitle>In First International Lexical Acquisition Workshop,</booktitle>
<note>edited by Uri Zernik.</note>
<contexts>
<context position="3956" citStr="Krovetz 1989" startWordPosition="610" endWordPosition="611">fers to a government body, rather than an abstract state of existence. • safe in this context is an adjective corresponding to &apos;secure,&apos; rather than a noun corresponding to a container for valuables. • The EPA and the state were co-agents in completing some agreement that is instrumental in supplying a secure place to keep garbage, rather than there was some equivalence that extended its arm around the state while the EPA was busy filling safes with trash. Preliminary evidence suggests that having access to a sense tagging of the text improves the performance of information retrieval systems (Krovetz 1989). The primary goal of this paper, then, is to describe in detail methods and knowledge that will enable a language analyzer to tag each word with its sense. To demonstrate that the approach is sufficiently robust for practical tasks, the article will also discuss the incorporation of the approach into an existing system, TRUMP (Jacobs 1986, 1987, 1989), and the application of it to unrestricted texts. The principles that make up the approach are completely general, however, and not just specific to TRUMP. An analyzer whose tasks include word-sense tagging must be able to take an input text, de</context>
<context position="75595" citStr="Krovetz 1989" startWordPosition="11726" endWordPosition="11727"> These difficulties point out the need for even more knowledge. It is encouraging to note that, even if our encoding scheme is not entirely &amp;quot;correct&amp;quot; according to human intuition, as long as it is consistent, in theory it should lead to capabilities that are no worse, with zero customization, than word-based methods for information retrieval. However, having access to sense tags allows for easy improvement by more knowledge-intensive methods. Although this theory is still untested, there is some preliminary evidence that word sense tagging can improve information retrieval system performance (Krovetz 1989). To date we have been unable to get a meaningful quantitative assessment of the accuracy of the system&apos;s sense tagging. We made an unsuccessful attempt at evaluating the accuracy of sense-tagging over a corpus. First, we discovered that a human &amp;quot;expert&amp;quot; had great difficulty identifying each sense, and that this task was far more tedious than manual part-of-speech tagging or bracketing. Second, we questioned what we would learn from the evaluation of these partial results, and have since turned our 26 Susan W. McRoy Using Multiple Knowledge Sources [the det_1 (c-definite-qual) ] [network noun_</context>
</contexts>
<marker>Krovetz, 1989</marker>
<rawString>Krovetz, Robert (1989). &amp;quot;Lexical acquisition and information retrieval.&amp;quot; In First International Lexical Acquisition Workshop, edited by Uri Zernik.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard S Kurtzman</author>
</authors>
<title>Studies in syntactic ambiguity resolution.&amp;quot; Doctoral dissertation,</title>
<date>1984</date>
<institution>Department of Psychology, MIT. Bloomington, IN: Indiana University Linguistics Club.</institution>
<contexts>
<context position="58237" citStr="Kurtzman (1984)" startWordPosition="8884" endWordPosition="8885">As suggested above, the system builds semantic interpretations incrementally. For each proposed combination of syntactic structures, there is a corresponding combination of semantic structures. It is the job of the semantic interpreter to identify the possible relations that link the structures being combined, identify the preferences associated with each possible combination of head, role (relation), and filler (the argument or modifier), and then rank competing semantic interpretations. 5 A similar approach has been taken by Gibson (1990) and is supported by the psychological experiments of Kurtzman (1984). 20 Susan W. McRoy Using Multiple Knowledge Sources For each proposed combination, knowledge sources may contribute the following preferences: • preferences directly associated with the head or the filler, determined recursively from their components, beginning with preferences identified during preprocessing. • preferences associated with syntactic cues, such as the satisfaction of restrictions listed in the lexicon. For example, a word may allow only modifiers of a particular syntactic form, or a modifier may modify only a certain syntactic form. (For example, the sense meaning &apos;to care for</context>
</contexts>
<marker>Kurtzman, 1984</marker>
<rawString>Kurtzman, Howard S. (1984). &amp;quot;Studies in syntactic ambiguity resolution.&amp;quot; Doctoral dissertation, Department of Psychology, MIT. Bloomington, IN: Indiana University Linguistics Club.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David M Magerman</author>
<author>Marcus</author>
</authors>
<marker>Magerman, Marcus, </marker>
<rawString>Magerman, David M., and Marcus,</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Mitchell</author>
</authors>
<title>Parsing a natural language using mutual information statistics.&amp;quot;</title>
<date>1990</date>
<booktitle>In AAAI-90 Proceedings, Eighth National Conference on Artificial Intelligence,</booktitle>
<pages>984--989</pages>
<publisher>AAAI Press/The MIT Press,</publisher>
<location>Menlo Park, CA:</location>
<marker>Mitchell, 1990</marker>
<rawString>Mitchell P. (1990). &amp;quot;Parsing a natural language using mutual information statistics.&amp;quot; In AAAI-90 Proceedings, Eighth National Conference on Artificial Intelligence, Menlo Park, CA: AAAI Press/The MIT Press, 984-989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan W McRoy</author>
<author>Graeme Hirst</author>
</authors>
<title>Race-based syntactic attachment.&amp;quot;</title>
<date>1990</date>
<journal>Cognitive Science</journal>
<volume>14</volume>
<issue>3</issue>
<pages>313--354</pages>
<contexts>
<context position="16550" citStr="McRoy and Hirst 1990" startWordPosition="2593" endWordPosition="2596">d the hearing in the afternoon. Moreover, pragmatic effects are not always the determining factor either, leading many people to judge the following sentence as silly (Hirst 1987). Example 4 The landlord painted all the walls with cracks (Rayner, Carlson, and Frazier 1983). 5 Computational Linguistics Volume 18, Number 1 The presence of different lexical items or different objects in the discourse focus may strengthen or weaken the information provided by an individual rule. Another possibility we will discuss in Section 5 is to weigh all preference information dynamically (cf. Schubert 1986; McRoy and Hirst 1990). The system we will be describing in Section 4 will use many of the cues described above, including syntactic tags, morphology, word associations, and role-related expectations. But first, we need to discuss the sources of knowledge that enable a system to identify these cues. 3. Sources of Knowledge To identify preference cues such as morphology, word frequency, collocations, semantic contexts, syntactic expectations, and conceptual relations in unrestricted texts, a system needs a large amount of knowledge in each category. In most cases, this just means that the understander&apos;s lexicon and </context>
</contexts>
<marker>McRoy, Hirst, 1990</marker>
<rawString>McRoy, Susan W., and Hirst, Graeme (1990). &amp;quot;Race-based syntactic attachment.&amp;quot; Cognitive Science 14(3): 313-354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
</authors>
<title>Lexical cohesion, the thesaurus, and the structure of text.&amp;quot;</title>
<date>1988</date>
<tech>Technical Report CSRI-219,</tech>
<institution>Computer Systems Research Institute, University of Toronto,</institution>
<location>Toronto.</location>
<contexts>
<context position="38228" citStr="Morris 1988" startWordPosition="5882" endWordPosition="5883">epts (cf. Dyer and Zernik 1986). This approach better addresses the fact that collocations do have a semantic basis, capturing general forms such as give him or her (some temporal object), which underlies the collocations give month, give minute, and give time. Currently, the system has entries for over 1700 such collocations. 3.4 Cluster Definitions The last source of sense preferences we need to consider is the semantic context. Work on lexical cohesion suggests that people use words that repeat a conceptual category or that have a semantic association to each other to create unity in text (Morris 1988; Morris and Hirst 1991; Halliday and Hasan 1976). These associations can be thought of as a class of collocations that lack the predictable syntactic structure of, say, collocations arising from verb-particle or compound noun constructions. Since language producers select senses that group together semantically, a language analyzer should prefer senses that share a semantic association. However, it is unclear whether the benefit of knowing the exact nature of an association would justify the cost of determining it. Thus, our system provides a cluster mechanism for representing and identifying</context>
</contexts>
<marker>Morris, 1988</marker>
<rawString>Morris, Jane (1988). &amp;quot;Lexical cohesion, the thesaurus, and the structure of text.&amp;quot; Technical Report CSRI-219, Computer Systems Research Institute, University of Toronto, Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Computational Linguistics Volume 18, Number 1 &amp;quot;Lexical cohesion computed by thesaural relations as an indicator of the structure of text.&amp;quot;</title>
<date>1991</date>
<journal>Computational Linguistics</journal>
<volume>17</volume>
<issue>1</issue>
<pages>21--48</pages>
<contexts>
<context position="38251" citStr="Morris and Hirst 1991" startWordPosition="5884" endWordPosition="5887">r and Zernik 1986). This approach better addresses the fact that collocations do have a semantic basis, capturing general forms such as give him or her (some temporal object), which underlies the collocations give month, give minute, and give time. Currently, the system has entries for over 1700 such collocations. 3.4 Cluster Definitions The last source of sense preferences we need to consider is the semantic context. Work on lexical cohesion suggests that people use words that repeat a conceptual category or that have a semantic association to each other to create unity in text (Morris 1988; Morris and Hirst 1991; Halliday and Hasan 1976). These associations can be thought of as a class of collocations that lack the predictable syntactic structure of, say, collocations arising from verb-particle or compound noun constructions. Since language producers select senses that group together semantically, a language analyzer should prefer senses that share a semantic association. However, it is unclear whether the benefit of knowing the exact nature of an association would justify the cost of determining it. Thus, our system provides a cluster mechanism for representing and identifying groups of senses that </context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris, Jane, and Hirst, Graeme (1991). Computational Linguistics Volume 18, Number 1 &amp;quot;Lexical cohesion computed by thesaural relations as an indicator of the structure of text.&amp;quot; Computational Linguistics 17(1): 21-48.</rawString>
</citation>
<citation valid="true">
<date>1978</date>
<booktitle>Longman Dictionary of Contemporary English.</booktitle>
<editor>Procter, Paul, editor</editor>
<publisher>Harlow: Longman Group Ltd.</publisher>
<marker>1978</marker>
<rawString>Procter, Paul, editor (1978). Longman Dictionary of Contemporary English. Harlow: Longman Group Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa F Rau</author>
</authors>
<title>Extracting company names from text.&amp;quot;</title>
<date>1991</date>
<booktitle>In IEEE Al Applications Conference (CAIA).</booktitle>
<contexts>
<context position="74233" citStr="Rau 1991" startWordPosition="11512" endWordPosition="11513">polls. Each word is tagged with its part of speech and sense number along with a parent concept. For example, the tag [changing verb_3 (c-replacing)] shows that the input word is changing, the preferred sense is number 3 of the verb, and this sense falls under the concept c-replacing in the hierarchy. This tagging was produced even though the parser was unable to construct a complete and correct syntactic representation of the text. In fact, when tested on the Wall Street Journal texts (for which there has been no adaptation or customization aside from processing by a company-name recognizer [Rau 19911), the system rarely produces a single correct parse; however, the partial parses produced generally cover most of the text at the clause level. Since most semantic preferences appear at this level (and those that do not, do not depend on syntactic analysis), the results of this tagging are encouraging. This example also shows some of the limitations of our system in practice. The system is unable to recognize the collocation &amp;quot;hold on to&amp;quot; in the first sentence, because it lacks a pattern for it. The system also lacks patterns for the collocations &amp;quot;vote on&amp;quot; and &amp;quot;all-time players&amp;quot; that occur in</context>
</contexts>
<marker>Rau, 1991</marker>
<rawString>Rau, Lisa F. (1991). &amp;quot;Extracting company names from text.&amp;quot; In IEEE Al Applications Conference (CAIA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa E Rau</author>
<author>Paul S Jacobs</author>
<author>Uri Zernik</author>
</authors>
<title>Information extraction and text summarization using linguistic knowledge acquisition.&amp;quot;</title>
<date>1989</date>
<journal>Information Processing and Management</journal>
<volume>25</volume>
<issue>4</issue>
<pages>419--428</pages>
<marker>Rau, Jacobs, Zernik, 1989</marker>
<rawString>Rau, Lisa E; Jacobs, Paul S.; and Zernik, Uri (1989). &amp;quot;Information extraction and text summarization using linguistic knowledge acquisition.&amp;quot; Information Processing and Management 25(4): 419-428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yael Ravin</author>
</authors>
<title>Disambiguating and interpreting verb definitions.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>260--267</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="78325" citStr="Ravin (1990)" startWordPosition="12092" endWordPosition="12093">sking) ] [*period* *punct* Figure 20 A sample sense coding. attention back to evaluating the system with respect to some task, such as information retrieval. Improving the quality of our sense tagging requires a fair amount of straightforward but time-consuming work. This needed work includes filling a number of 27 Computational Linguistics Volume 18, Number 1 gaps in our knowledge sources. For example, the system needs much more information about role-related preferences and specialized semantic contexts. At present all this information is collected and coded by hand, although recent work by Ravin (1990) and Dahlgren, McDowell, and Stabler (1989) suggests that the collection of role-related information may be automatable. Our next step is to evaluate the effect of text coding on an information retrieval task, by applying traditional term-weighted statistical retrieval methods to the recoded text. One intriguing aspect of this approach is that errors in distinguishing sense preferences should not be too costly in this task, so long as the program is fairly consistent in its disambiguation of terms in both the source texts and the input queries. 7. Conclusion Having access to a large amount of </context>
</contexts>
<marker>Ravin, 1990</marker>
<rawString>Ravin, Yael (1990). &amp;quot;Disambiguating and interpreting verb definitions.&amp;quot; In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics. Pittsburgh, PA, 260-267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Rayner</author>
<author>Marcia Carlson</author>
<author>Lyn Frazier</author>
</authors>
<title>The interaction of syntax and semantics during sentence processing: Eye movements in the analysis of semantically biased sentences.&amp;quot;</title>
<date>1983</date>
<journal>Journal of Verbal Learning and Verbal Behavior</journal>
<volume>22</volume>
<pages>358--374</pages>
<marker>Rayner, Carlson, Frazier, 1983</marker>
<rawString>Rayner, Keith; Carlson, Marcia; and Frazier, Lyn (1983). &amp;quot;The interaction of syntax and semantics during sentence processing: Eye movements in the analysis of semantically biased sentences.&amp;quot; Journal of Verbal Learning and Verbal Behavior 22: 358-374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>Robert P Abelson</author>
</authors>
<date>1977</date>
<booktitle>Scripts, Plans, Goals, and Understanding.</booktitle>
<location>Halsted, NJ: Lawrence Erlbaum.</location>
<marker>Schank, Abelson, 1977</marker>
<rawString>Schank, Roger C., and Abelson, Robert P. (1977). Scripts, Plans, Goals, and Understanding. Halsted, NJ: Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lenhart Schubert</author>
</authors>
<title>Are there preference trade-offs in attachment decisions?&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, National Conference on Artificial Intelligence (AAAI-86).</booktitle>
<pages>601--605</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="16527" citStr="Schubert 1986" startWordPosition="2591" endWordPosition="2592">se and cancelled the hearing in the afternoon. Moreover, pragmatic effects are not always the determining factor either, leading many people to judge the following sentence as silly (Hirst 1987). Example 4 The landlord painted all the walls with cracks (Rayner, Carlson, and Frazier 1983). 5 Computational Linguistics Volume 18, Number 1 The presence of different lexical items or different objects in the discourse focus may strengthen or weaken the information provided by an individual rule. Another possibility we will discuss in Section 5 is to weigh all preference information dynamically (cf. Schubert 1986; McRoy and Hirst 1990). The system we will be describing in Section 4 will use many of the cues described above, including syntactic tags, morphology, word associations, and role-related expectations. But first, we need to discuss the sources of knowledge that enable a system to identify these cues. 3. Sources of Knowledge To identify preference cues such as morphology, word frequency, collocations, semantic contexts, syntactic expectations, and conceptual relations in unrestricted texts, a system needs a large amount of knowledge in each category. In most cases, this just means that the unde</context>
</contexts>
<marker>Schubert, 1986</marker>
<rawString>Schubert, Lenhart (1986). &amp;quot;Are there preference trade-offs in attachment decisions?&amp;quot; In Proceedings, National Conference on Artificial Intelligence (AAAI-86). Philadelphia, PA, 601-605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Sentence disambiguation by a shift-reduce parsing technique.&amp;quot;</title>
<date>1983</date>
<booktitle>In Proceedings, 21st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>113--118</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="13588" citStr="Shieber 1983" startWordPosition="2123" endWordPosition="2124">is cue appears fairly infrequently [Whittemore, Ferrara, and Brunner 19901.) The problem of determining role relationships entangles word sense discrimination with the problem of syntactic attachment. The attachment problem is a direct result of the ambiguity in determining whether a concept is related to an adjacent object, or to some enveloping structure that incorporates the adjacent object. Most proposed solutions to this problem specify a fixed set of ordered rules that a system applies until a unique, satisfactory attachment is found (Fodor and Frazier 1980; Wilks, Huang, and Fass 1985; Shieber 1983; Hirst 1987; Dahlgren, McDowell, and Stabler 1989). Such rules can be either syntactic, semantic, or pragmatic. Syntactic rules attempt to solve the attachment problem independent of the sense discrimination problem. For example, a rule for Right Association (also known as Late Closure) says to prefer attaching a new word to the lowest nonterminal node on the rightmost branch of the current structure (i.e., in the same structure as the last word processed) (Kimball 1973). Semantic rules, by contrast, intertwine the problems of discrimination and attachment; one must examine all combinations o</context>
</contexts>
<marker>Shieber, 1983</marker>
<rawString>Shieber, Stuart M. (1983). &amp;quot;Sentence disambiguation by a shift-reduce parsing technique.&amp;quot; In Proceedings, 21st Annual Meeting of the Association for Computational Linguistics. Cambridge, MA, 113-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg B Simpson</author>
<author>Curt Burgess</author>
</authors>
<title>Implications of lexical ambiguity resolution for word recognition and comprehension.&amp;quot; In Lexical Ambiguity Resolution, edited by</title>
<date>1988</date>
<pages>271--288</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>San Mateo, CA:</location>
<contexts>
<context position="50069" citStr="Simpson and Burgess (1988)" startWordPosition="7611" endWordPosition="7614">(listed in the :S-DERIV field) agree1+ment and agree2-1-able are preferred, derivations agreel±able and agree3+ment are deprecated, and all other sense-affix combinations (excepting inflections) have been disallowed. During morphological analysis, the system retrieves only the most general senses. It waits until the preprocessor or the parser identifies supporting evidence before it retrieves word senses specific to a context, such as a domain, a situation, or a collocation. In most cases this approach helps reduce the amount of ambiguity. The approach is compatible with evidence discussed by Simpson and Burgess (1988) that 17 Computational Linguistics Volume 18, Number 1 ( agree :POS verb :G-DERIV nil :SENSES (( agreel :SYNTAX (one-obj no-obj thatcomp comp subj-equi) :EXAMPLE (she agrees with me * they agreed to use force * they agreed on 3 percent * they agreed that he was right * I agree it is true) :TYPE *primary* :PAR (c-agreeing) :ASSOC (concur believe) :S-DERIV ((-ment preferred noun tr_act tr_object tr_result) (-able secondary adj tr_ability)) ) ( agree2 :SYNTAX (one-obj) :EXAMPLE (winter agrees with me) :TYPE *secondary* :PAR (c-abstract-relation) :ASSOC (benefit) :S-DERIV ((-able preferred adj tr_</context>
</contexts>
<marker>Simpson, Burgess, 1988</marker>
<rawString>Simpson, Greg B., and Burgess, Curt (1988). &amp;quot;Implications of lexical ambiguity resolution for word recognition and comprehension.&amp;quot; In Lexical Ambiguity Resolution, edited by Steven L. Small, Garrison W. Cottrell, and Michael K. Tanenhaus, 271-288. San Mateo, CA: Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Frank A Smadja</author>
<author>McKeown</author>
</authors>
<marker>Smadja, McKeown, </marker>
<rawString>Smadja, Frank A., and McKeown,</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kathleen</author>
</authors>
<title>Automatically extracting and representing collocations for language generation.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>252--259</pages>
<location>Pittsburgh, PA,</location>
<marker>Kathleen, 1990</marker>
<rawString>Kathleen R. (1990). &amp;quot;Automatically extracting and representing collocations for language generation.&amp;quot; In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics. Pittsburgh, PA, 252-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliviero Stock</author>
</authors>
<title>Parsing with flexibility, dynamic strategies, and idioms in mind.&amp;quot;</title>
<date>1989</date>
<journal>Computational Linguistics</journal>
<volume>15</volume>
<issue>1</issue>
<pages>1--18</pages>
<contexts>
<context position="37371" citStr="Stock (1989)" startWordPosition="5746" endWordPosition="5747">or the compositional collocations associated with the verb take; the entries pair a dynamic sense of take with a sense occurring as its complement.) These entries link the collocations to the semantic hierarchy, and, where appropriate, provide syntactic constraints that the parser can use to verify the presence of a collocation. For example, Figure 10 shows the entry for the noncompositional collocation take place, which requires that the object (r ail*) be singular and determinerless. These entries differ from similar representations of collocations or idioms in Smadja and McKeown (1990) and Stock (1989), in that they are sense-based rather than wordbased. That is, instead of expressing collocations as word-templates, the lexicon groups together collocations that combine the same sense of the head verb with particular senses or higher-level concepts (cf. Dyer and Zernik 1986). This approach better addresses the fact that collocations do have a semantic basis, capturing general forms such as give him or her (some temporal object), which underlies the collocations give month, give minute, and give time. Currently, the system has entries for over 1700 such collocations. 3.4 Cluster Definitions T</context>
</contexts>
<marker>Stock, 1989</marker>
<rawString>Stock, Oliviero (1989). &amp;quot;Parsing with flexibility, dynamic strategies, and idioms in mind.&amp;quot; Computational Linguistics 15(1): 1-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Whittemore</author>
<author>Kathleen Ferrara</author>
<author>Hans Brunner</author>
</authors>
<title>Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>23--30</pages>
<location>Pittsburgh, PA,</location>
<marker>Whittemore, Ferrara, Brunner, 1990</marker>
<rawString>Whittemore, Greg; Ferrara, Kathleen; and Brunner, Hans (1990). &amp;quot;Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases.&amp;quot; In Proceedings, 28th Annual Meeting of the Association for Computational Linguistics. Pittsburgh, PA, 23-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
<author>Xiuming Huang</author>
<author>Dan Fass</author>
</authors>
<title>Syntax, preference, and right attachment.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, Ninth International Joint Conference on Artificial Intelligence.</booktitle>
<location>Los Angeles, CA.</location>
<marker>Wilks, Huang, Fass, 1985</marker>
<rawString>Wilks, Yorick; Huang, Xiuming; and Fass, Dan (1985). &amp;quot;Syntax, preference, and right attachment.&amp;quot; In Proceedings, Ninth International Joint Conference on Artificial Intelligence. Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Zernik</author>
</authors>
<title>Tagging word senses in corpus: The needle in the haystack revisited.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, AAAI Symposium on Text-Based Intelligent Systems.</booktitle>
<tech>Technical Report 90CRD198,</tech>
<pages>25--29</pages>
<institution>GE Research and Development Center,</institution>
<location>Schenectedy, NY.</location>
<contexts>
<context position="51351" citStr="Zernik (1990)" startWordPosition="7809" endWordPosition="7810">ot agree) :TYPE *secondary* :PAR (c-equivalence-rel) :ASSOC (correspond) :S-DERIV ((-ment secondary noun tr_state)) ) ) Figure 15 The lexical entry for the verb agree. &amp;quot;multiple meanings are activated in frequency-coded order&amp;quot; and that low-frequency senses are handled by a second retrieval process that accumulates evidence for those senses and activates them as necessary. 4.2 Tagging Once the system determines the morphological analysis of each word, the next step in preprocessing is to try to determine the correct part of speech for the word. Our system uses a tagging program, written by Uri Zernik (1990), that takes information about the root, affix, and possible syntactic category for each word and applies stochastic techniques to select a syntactic tag for each word. Stochastic taggers look at small groups of words and pick the most likely assignment of tags, determined by the frequency of alternative syntactic patterns in similar texts. Although it may not be possible to completely disambiguate all words prior to parsing, approaches based on 18 Susan W. McRoy Using Multiple Knowledge Sources stochastic information have been quite successful (Church 1988; Garside, Leech, and Sampson 1987; d</context>
</contexts>
<marker>Zernik, 1990</marker>
<rawString>Zernik, Uri (1990). &amp;quot;Tagging word senses in corpus: The needle in the haystack revisited.&amp;quot; Technical Report 90CRD198, GE Research and Development Center, Schenectedy, NY. In Proceedings, AAAI Symposium on Text-Based Intelligent Systems. Stanford, CA, 25-29.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>