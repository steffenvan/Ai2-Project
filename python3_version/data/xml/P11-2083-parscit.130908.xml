<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001581">
<title confidence="0.982467">
Clustering Comparable Corpora For Bilingual Lexicon Extraction
</title>
<author confidence="0.892643">
Bo Li, Eric Gaussier Akiko Aizawa
</author>
<note confidence="0.605762">
UJF-Grenoble 1 / CNRS, France National Institute of Informatics
LIG UMR 5217 Tokyo, Japan
</note>
<email confidence="0.961415">
firstname.lastname@imag.fr aizawa@nii.ac.jp
</email>
<sectionHeader confidence="0.998218" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999710214285714">
We study in this paper the problem of enhanc-
ing the comparability of bilingual corpora in
order to improve the quality of bilingual lexi-
cons extracted from comparable corpora. We
introduce a clustering-based approach for en-
hancing corpus comparability which exploits
the homogeneity feature of the corpus, and
finally preserves most of the vocabulary of
the original corpus. Our experiments illus-
trate the well-foundedness of this method and
show that the bilingual lexicons obtained from
the homogeneous corpus are of better quality
than the lexicons obtained with previous ap-
proaches.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978377777778">
Bilingual lexicons are an important resource in mul-
tilingual natural language processing tasks such as
statistical machine translation (Och and Ney, 2003)
and cross-language information retrieval (Balles-
teros and Croft, 1997). Because it is expensive to
manually build bilingual lexicons adapted to dif-
ferent domains, researchers have tried to automat-
ically extract bilingual lexicons from various cor-
pora. Compared with parallel corpora, it is much
easier to build high-volume comparable corpora, i.e.
corpora consisting of documents in different lan-
guages covering overlapping information. Several
studies have focused on the extraction of bilingual
lexicons from comparable corpora (Fung and McK-
eown, 1997; Fung and Yee, 1998; Rapp, 1999;
D´ejean et al., 2002; Gaussier et al., 2004; Robitaille
et al., 2006; Morin et al., 2007; Garera et al., 2009;
Yu and Tsujii, 2009; Shezaf and Rappoport, 2010).
The basic assumption behind most studies on lex-
icon extraction from comparable corpora is a dis-
tributional hypothesis, stating that words which are
translation of each other are likely to appear in simi-
lar context across languages. On top of this hypoth-
esis, researchers have investigated the use of better
representations for word contexts, as well as the use
of different methods for matching words across lan-
guages. These approaches seem to have reached a
plateau in terms of performance. More recently, and
departing from such traditional approaches, we have
proposed in (Li and Gaussier, 2010) an approach
based on improving the comparability of the cor-
pus under consideration, prior to extracting bilingual
lexicons. This approach is interesting since there is
no point in trying to extract lexicons from a corpus
with a low degree of comparability, as the probabil-
ity of finding translations of any given word is low
in such cases. We follow here the same general idea
and aim, in a first step, at improving the compara-
bility of a given corpus while preserving most of
its vocabulary. However, unlike the previous work,
we show here that it is possible to guarantee a cer-
tain degree of homogeneity for the improved corpus,
and that this homogeneity translates into a signifi-
cant improvement of both the quality of the resulting
corpora and the bilingual lexicons extracted.
</bodyText>
<sectionHeader confidence="0.923888" genericHeader="method">
2 Enhancing Comparable Corpora: A
</sectionHeader>
<subsectionHeader confidence="0.738543">
Clustering Approach
</subsectionHeader>
<bodyText confidence="0.999298333333333">
We first introduce in this section the comparability
measure proposed in former work, prior to describ-
ing the clustering-based algorithm to improve the
</bodyText>
<page confidence="0.994013">
473
</page>
<note confidence="0.5841365">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 473–478,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.989714333333333">
quality of a given comparable corpus. For conve-
nience, the following discussion will be made in the
context of the English-French comparable corpus.
</bodyText>
<subsectionHeader confidence="0.998005">
2.1 The Comparability Measure
</subsectionHeader>
<bodyText confidence="0.996380272727273">
In order to measure the degree of comparability of
bilingual corpora, we make use of the measure M
developed in (Li and Gaussier, 2010): Given a com-
parable corpus P consisting of an English part Pe
and a French part Pf, the degree of comparability of
P is defined as the expectation of finding the trans-
lation of any given source/target word in the tar-
get/source corpus vocabulary. Let σ be a function
indicating whether a translation from the translation
set Tw of the word w is found in the vocabulary Pv
of a corpus P, i.e.:
</bodyText>
<equation confidence="0.9970985">
1 iff Tw ∩ Pv =6 ∅
l σ(w, P) =0 else
</equation>
<bodyText confidence="0.999941666666667">
and let D be a bilingual dictionary with Dve denoting
its English vocabulary and Dvf its French vocabulary.
The comparability measure M can be written as:
</bodyText>
<equation confidence="0.998729">
M(Pe,Pf) (1)
� e σ(w, Pf) + � f σ(w, Pe)
wEPenVv wEPfnVv
#w(Pe ∩ Dv e) + #w(Pf ∩ Dvf)
</equation>
<bodyText confidence="0.9999612">
where #w(P) denotes the number of different
words present in P. One can find from equa-
tion 1 that M directly measures the proportion of
source/target words translated in the target/source
vocabulary of P.
</bodyText>
<subsectionHeader confidence="0.9998715">
2.2 Clustering Documents for High Quality
Comparable Corpora
</subsectionHeader>
<bodyText confidence="0.999950052631579">
If a corpus covers a limited set of topics, it is more
likely to contain consistent information on the words
used (Morin et al., 2007), leading to improved bilin-
gual lexicons extracted with existing algorithms re-
lying on the distributional hypothesis. The term ho-
mogeneity directly refers to this fact, and we will say,
in an informal manner, that a corpus is homogeneous
if it covers a limited set of topics. The rationale for
the algorithm we introduce here to enhance corpus
comparability is precisely based on the concept of
homogeneity. In order to find document sets which
are similar with each other (i.e. homogeneous), it
is natural to resort to clustering techniques. Further-
more, since we need homogeneous corpora for bilin-
gual lexicon extraction, it will be convenient to rely
on techniques which allows one to easily prune less
relevant clusters. To perform all this, we use in this
work a standard hierarchical agglomerative cluster-
ing method.
</bodyText>
<subsectionHeader confidence="0.955997">
2.2.1 Bilingual Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.996768666666667">
The overall process retained to build high quality,
homogeneous comparable corpora relies on the fol-
lowing steps:
</bodyText>
<listItem confidence="0.973580666666667">
1. Using the bilingual similarity measure defined
in Section 2.2.2, cluster English and French
documents so as to get bilingual dendrograms
from the original corpus P by grouping docu-
ments with related content;
2. Pick high quality sub-clusters by threshold-
ing the obtained dendrograms according to the
node depth, which retains nodes far from the
roots of the clustering trees;
3. Combine all these sub-clusters to form a new
comparable corpus PH, which thus contains
homogeneous, high-quality subparts;
4. Use again steps (1), (2) and (3) to enrich the
remaining subpart of P (denoted as PL, PL =
P \ PH) with external resources.
</listItem>
<bodyText confidence="0.987602">
The first three steps aim at extracting the most com-
parable and homogeneous subpart of P. Once this
has been done, one needs to resort to new corpora
if one wants to build an homogeneous corpus with
a high degree of comparability from PL. To do so,
we simply perform, in step (4), the clustering and
thresholding process defined in (1), (2) and (3) on
two comparable corpora: The first one consists of
the English part of PL and the French part of an ex-
ternal corpus PT; The second one consists of the
French part of PL and the English part of PT. The
two high quality subparts obtained from these two
new comparable corpora in step (4) are then com-
bined with PH to constitute the final comparable
corpus of higher quality.
=
</bodyText>
<page confidence="0.965235">
474
</page>
<subsubsectionHeader confidence="0.720392">
2.2.2 Similarity Measure
</subsubsectionHeader>
<bodyText confidence="0.999673666666667">
Let us assume that we have two document sets (i.e.
clusters) C1 and C2. In the task of bilingual lexi-
con extraction, two document sets are similar to each
other and should be clustered if the combination of
the two can complement the content of each single
set, which relates to the notion of homogeneity. In
other words, both the English part Ce1 of C1 and the
French part Cf1 of C1 should be comparable to their
counterparts (respectively the same for the French
part Cf2 of C2 and the English part Ce2 of C2). This
leads to the following similarity measure for C1 and
C2:
</bodyText>
<equation confidence="0.63851">
sim(C1, C2) = Q · M(Ce1, Cf2 ) + (1− Q) · M(Ce2, Cf 1 )
</equation>
<bodyText confidence="0.999447">
where Q (0 &lt; Q &lt; 1) is a weight controlling the
importance of the two subparts (Ce1, Cf2 ) and (Ce2,
Cf1). Intuitively, the larger one, containing more in-
formation, of the two comparable corpora (Ce1, Cf2 )
and (Ce2, Cf1) should dominate the overall similar-
ity sim(C1, C2). Since the content relatedness in the
comparable corpus is basically reflected by the re-
lations between all the possible bilingual document
pairs, we use here the number of document pairs to
represent the scale of the comparable corpus. The
weight Q can thus be defined as the proportion of
possible document pairs in the current comparable
corpus (Ce1, Cf2) to all the possible document pairs,
which is:
</bodyText>
<equation confidence="0.995249">
#d(Ce 1) · #d(Cf 2 )
Q =
#d(Ce1) · #d(Cf2) + #d(Ce2) · #d(Cf1)
</equation>
<bodyText confidence="0.999978">
where #d(C) stands for the number of documents in
C. However, this measure does not integrate the rel-
ative length of the French and English parts, which
actually impacts the performance of bilingual lexi-
con extraction. If a 1-to-1 constraint is too strong
(i.e. assuming that all clusters should contain the
same number of English and French documents),
having completely unbalanced corpora is also not
desirable. We thus introduce a penalty function O
aiming at penalizing unbalanced corpora:
</bodyText>
<equation confidence="0.994715">
1
O(C) _ —#d(Ce)−#d(Cf) |(2)
(1 + log(1 + min(#d(Ce)),#d(Cf )))
</equation>
<bodyText confidence="0.999664666666667">
The above penalty function leads us to a new simi-
larity measure siml which is the one finally used in
the above algorithm:
</bodyText>
<equation confidence="0.998124">
siml(C1,C2) = sim(C1,C2) · O(C1 U C2) (3)
</equation>
<sectionHeader confidence="0.997096" genericHeader="method">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9999395">
The experiments we have designed in this paper aim
at assessing (a) whether the clustering-based algo-
rithm we have introduced yields corpora of higher
quality in terms of comparability scores, and (b)
whether the bilingual lexicons extracted from such
corpora are of higher quality. Several corpora were
used in our experiments: the TREC1 Associated
Press corpus (AP, English) and the corpora used
in the CLEF2 campaign including the Los Ange-
les Times (LAT94, English), the Glasgow Herald
(GH95, English), Le Monde (MON94, French), SDA
French 94 (SDA94, French) and SDA French 95
(SDA95, French). In addition, two monolingual cor-
pora Wiki-En and Wiki-Fr were built by respectively
retrieving all the articles below the category Society
and Soci´et´e from the Wikipedia dump files3. The
bilingual dictionary used in the experiments is con-
structed from an online dictionary. It consists of
33k distinct English words and 28k distinct French
words, constituting 76k translation pairs. In our ex-
periments, we use the method described in this pa-
per, as well as the one in (Li and Gaussier, 2010)
which is the only alternative method to enhance cor-
pus comparability.
</bodyText>
<subsectionHeader confidence="0.996067">
3.1 Improving Corpus Quality
</subsectionHeader>
<bodyText confidence="0.9998539">
In this subsection, the clustering algorithm described
in Section 2.2.1 is employed to improve the quality
of the comparable corpus. The corpora GH95 and
SDA95 are used as the original corpus P0 (56k En-
glish documents and 42k French documents). We
consider two external corpora: P1T (109k English
documents and 87k French documents) consisting of
the corpora LAT94, MON94 and SDA94; P2T (368k
English documents and 378k French documents)
consisting of Wiki-En and Wiki-Fr.
</bodyText>
<footnote confidence="0.9994675">
1http://trec.nist.gov
2http://www.clef-campaign.org
3The Wikipedia dump files can be downloaded at
http://download.wikimedia.org. In this paper, we use the En-
glish dump file on July 13, 2009 and the French dump file on
July 7, 2009.
</footnote>
<page confidence="0.99572">
475
</page>
<table confidence="0.998925">
P0 P10 P20 P1 P2 P1 &gt; P0 P2 &gt; P0
Precision 0.226 0.277 0.325 0.295 0.461 0.069, 30.5% 0.235, 104.0%
Recall 0.103 0.122 0.145 0.133 0.212 0.030, 29.1% 0.109, 105.8%
</table>
<tableCaption confidence="0.999942">
Table 1: Performance of the bilingual lexicon extraction from different corpora (best results in bold)
</tableCaption>
<bodyText confidence="0.999990210526316">
After the clustering process, we obtain the result-
ing corpora P1 (with the external corpus P1T) and
P2 (with P2T). As mentioned before, we also used
the method described in (Li and Gaussier, 2010)
on the same data, producing resulting corpora P10
(with P1T) and P20 (with P2T) from P0. In terms
of lexical coverage, P1 (resp. P2) covers 97.9%
(resp. 99.0%) of the vocabulary of P0. Hence, most
of the vocabulary of the original corpus has been
preserved. The comparability score of P1 reaches
0.924 and that of P2 is 0.939. Both corpora are
more comparable than P0 of which the comparabil-
ity is 0.881. Furthermore, both P1 and P2 are more
comparable than P10 (comparability 0.912) and P20
(comparability 0.915), which shows homogeneity is
crucial for comparability. The intrinsic evaluation
shows the efficiency of our approach which can im-
prove the quality of the given corpus while preserv-
ing most of its vocabulary.
</bodyText>
<subsectionHeader confidence="0.99965">
3.2 Bilingual Lexicon Extraction Experiments
</subsectionHeader>
<bodyText confidence="0.999934923076923">
To extract bilingual lexicons from comparable cor-
pora, we directly use here the method proposed by
Fung and Yee (1998) which has been referred to
as the standard approach in more recent studies
(D´ejean et al., 2002; Gaussier et al., 2004; Yu and
Tsujii, 2009). In this approach, each word w is rep-
resented as a context vector consisting of the words
co-occurring with w in a certain window in the cor-
pus. The context vectors in different languages are
then bridged with an existing bilingual dictionary.
Finally, a similarity score is given to any word pair
based on the cosine of their respective context vec-
tors.
</bodyText>
<subsectionHeader confidence="0.860851">
3.2.1 Experiment Settings
</subsectionHeader>
<bodyText confidence="0.999990681818182">
In order to measure the performance of the lexi-
cons extracted, we follow the common practice by
dividing the bilingual dictionary into 2 parts: 10%
of the English words (3,338 words) together with
their translations are randomly chosen and used as
the evaluation set, the remaining words being used
to compute the similarity of context vectors. En-
glish words not present in Pe or with no translation
in Pf are excluded from the evaluation set. For each
English word in the evaluation set, all the French
words in Pf are then ranked according to their sim-
ilarity with the English word. Precision and recall
are then computed on the first N translation candi-
date lists. The precision amounts in this case to the
proportion of lists containing the correct translation
(in case of multiple translations, a list is deemed to
contain the correct translation as soon as one of the
possible translations is present). The recall is the
proportion of correct translations found in the lists
to all the translations in the corpus. This evaluation
procedure has been used in previous studies and is
now standard.
</bodyText>
<subsubsectionHeader confidence="0.333879">
3.2.2 Results and Analysis
</subsubsectionHeader>
<bodyText confidence="0.999987541666667">
In a first series of experiments, bilingual lexicons
were extracted from the corpora obtained by our ap-
proach (P1 and P2), the corpora obtained by the
approach described in (Li and Gaussier, 2010) (P10
and P20) and the original corpus P0, with the fixed
N value set to 20. Table 1 displays the results ob-
tained. Each of the last two columns “P1 &gt; P0”
and “P2 &gt; P0” contains the absolute and the rel-
ative difference (in %) w.r.t. P0. As one can note,
the best results (in bold) are obtained from the cor-
pora P2 built with the method we have described in
this paper. The lexicons extracted from the enhanced
corpora are of much higher quality than the ones ob-
tained from the original corpus . For instance, the
increase of the precision is 6.9% (30.5% relatively)
in P1 and 23.5% (104.0% relatively) in P2, com-
pared with P0. The difference is more remarkable
with P2, which is obtained from a large external cor-
pus P2T. Intuitively, one can expect to find, in larger
corpora, more documents related to a given corpus,
an intuition which seems to be confirmed by our re-
sults. One can also notice, by comparing P2 and
P20 as well as P1 and P10, a remarkable improve-
ment when considering our approach and the early
</bodyText>
<page confidence="0.998081">
476
</page>
<bodyText confidence="0.999128266666667">
methodology.
Intuitively, the value N plays an important role
in the above experiments. In a second series of ex-
periments, we let N vary from 1 to 300 and plot the
results obtained with different evaluation measure in
Figure 1. In Figure 1(a) (resp. Figure 1(b)), the x-
axis corresponds to the values taken by N, and the y-
axis to the precision (resp. recall) scores for the lexi-
cons extracted on each of the 5 corpora P°, P1t, P2t,
P1 and P2. A clear fact from the figure is that both
the precision and the recall scores increase accord-
ing to the increase of the N values, which coincides
with our intuition. As one can note, our method con-
sistently outperforms the previous work and also the
original corpus on all the values considered for N.
</bodyText>
<figure confidence="0.9998205">
(a) Precision
(b) Recall
</figure>
<figureCaption confidence="0.999821">
Figure 1: Performance of bilingual lexicon extraction
</figureCaption>
<bodyText confidence="0.472448">
from different corpora with varied N values from 1 to
300. The five lines from the top down in each subfigure
are corresponding to the results for P2, P21, P1, P11 and
P° respectively.
</bodyText>
<sectionHeader confidence="0.997766" genericHeader="discussions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999959434782609">
As previous studies on bilingual lexicon extrac-
tion from comparable corpora radically differ on
resources used and technical choices, it is very
difficult to compare them in a unified framework
(Laroche and Langlais, 2010). We compare in this
section our method with some ones in the same vein
(i.e. enhancing bilingual corpora prior to extract-
ing bilingual lexicons from them). Some works like
(Munteanu et al., 2004) and (Munteanu and Marcu,
2006) propose methods to extract parallel fragments
from comparable corpora. However, their approach
only focuses on a very small part of the original cor-
pus, whereas our work aims at preserving most of
the vocabulary of the original corpus.
We have followed here the general approach in
(Li and Gaussier, 2010) which consists in enhancing
the quality of a comparable corpus prior to extract-
ing information from it. However, despite this latter
work, we have shown here a method which ensures
homogeneity of the obtained corpus, and which fi-
nally leads to comparable corpora of higher quality.
In turn such corpora yield better bilingual lexicons
extracted.
</bodyText>
<sectionHeader confidence="0.997073" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9391065">
This work was supported by the French National Re-
search Agency grant ANR-08-CORD-009.
</bodyText>
<sectionHeader confidence="0.988997" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.524751235294118">
Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for cross-
language information retrieval. In Proceedings of the
20th ACM SIGIR, pages 84–91, Philadelphia, Pennsyl-
vania, USA.
Herv´e D´ejean, Eric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In Pro-
ceedings of the 19th International Conference on
Computational Linguistics, pages 1–7, Taipei, Taiwan.
Pascale Fung and Kathleen McKeown. 1997. Find-
ing terminology translations from non-parallel cor-
pora. In Proceedings of the 5th Annual Workshop on
Very Large Corpora, pages 192–202, Hong Kong.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the 17th international con-
</reference>
<figure confidence="0.9995265">
0 100 200 300
N
Precision
0.8
0.6
0.4
0.2
0.0
P2
P2�
P1
P1•
P0
0 100 200 300
N
Recall
0.4
0.3
0.2
0.1
0.0
P2
P2�
P1
P1•
P0
</figure>
<page confidence="0.977051">
477
</page>
<reference confidence="0.998586409090909">
ference on Computational linguistics, pages 414–420,
Montreal, Quebec, Canada.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon induc-
tion from monolingual corpora via dependency con-
texts and part-of-speech equivalences. In CoNLL 09:
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning, pages 129–137,
Boulder, Colorado.
E. Gaussier, J.-M. Renders, I. Matveeva, C. Goutte, and
H. D´ejean. 2004. A geometric view on bilingual
lexicon extraction from comparable corpora. In Pro-
ceedings of the 42nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 526–533,
Barcelona, Spain.
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In Proceedings of
the 23rd International Conference on Computational
Linguistics (Coling 2010), pages 617–625, Beijing,
China, August.
Bo Li and Eric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
pages 644–652, Beijing, China.
Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual terminology mining -
using brain, not brawn comparable corpora. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 664–671,
Prague, Czech Republic.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics, pages 81–88, Sydney, Australia.
Dragos Stefan Munteanu, Alexander Fraser, and Daniel
Marcu. 2004. Improved machine translation perfor-
mance via parallel sentence extraction from compara-
ble corpora. In Proceedings of the HLT-NAACL 2004,
pages 265–272, Boston, MA., USA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, pages
519–526, College Park, Maryland, USA.
Xavier Robitaille, Yasuhiro Sasaki, Masatsugu Tonoike,
Satoshi Sato, and Takehito Utsuro. 2006. Compil-
ing French-Japanese terminologies from the web. In
Proceedings of the 11st Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 225–232, Trento, Italy.
Daphna Shezaf and Ari Rappoport. 2010. Bilingual lex-
icon generation using non-aligned signatures. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 98–107, Up-
psala, Sweden.
Kun Yu and Junichi Tsujii. 2009. Extracting bilingual
dictionary from comparable corpora with dependency
heterogeneity. In Proceedings of HLT-NAACL 2009,
pages 121–124, Boulder, Colorado, USA.
</reference>
<page confidence="0.998014">
478
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.657164">
<title confidence="0.999861">Clustering Comparable Corpora For Bilingual Lexicon Extraction</title>
<author confidence="0.974034">Bo Li</author>
<author confidence="0.974034">Eric Gaussier Akiko Aizawa</author>
<note confidence="0.919069">UJF-Grenoble 1 / CNRS, France National Institute of Informatics LIG UMR 5217 Tokyo, Japan firstname.lastname@imag.fr aizawa@nii.ac.jp</note>
<abstract confidence="0.989772533333333">We study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora. We introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and finally preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lisa Ballesteros</author>
<author>W Bruce Croft</author>
</authors>
<title>Phrasal translation and query expansion techniques for crosslanguage information retrieval.</title>
<date>1997</date>
<booktitle>In Proceedings of the 20th ACM SIGIR,</booktitle>
<pages>84--91</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="1067" citStr="Ballesteros and Croft, 1997" startWordPosition="148" endWordPosition="152">ng-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and finally preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches. 1 Introduction Bilingual lexicons are an important resource in multilingual natural language processing tasks such as statistical machine translation (Och and Ney, 2003) and cross-language information retrieval (Ballesteros and Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al.,</context>
</contexts>
<marker>Ballesteros, Croft, 1997</marker>
<rawString>Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal translation and query expansion techniques for crosslanguage information retrieval. In Proceedings of the 20th ACM SIGIR, pages 84–91, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herv´e D´ejean</author>
<author>Eric Gaussier</author>
<author>Fatia Sadat</author>
</authors>
<title>An approach based on multilingual thesauri and model combination for bilingual lexicon extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1--7</pages>
<location>Taipei, Taiwan.</location>
<marker>D´ejean, Gaussier, Sadat, 2002</marker>
<rawString>Herv´e D´ejean, Eric Gaussier, and Fatia Sadat. 2002. An approach based on multilingual thesauri and model combination for bilingual lexicon extraction. In Proceedings of the 19th International Conference on Computational Linguistics, pages 1–7, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Kathleen McKeown</author>
</authors>
<title>Finding terminology translations from non-parallel corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Annual Workshop on Very Large Corpora,</booktitle>
<pages>192--202</pages>
<location>Hong Kong.</location>
<contexts>
<context position="1550" citStr="Fung and McKeown, 1997" startWordPosition="218" endWordPosition="222">ng tasks such as statistical machine translation (Och and Ney, 2003) and cross-language information retrieval (Ballesteros and Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words acro</context>
</contexts>
<marker>Fung, McKeown, 1997</marker>
<rawString>Pascale Fung and Kathleen McKeown. 1997. Finding terminology translations from non-parallel corpora. In Proceedings of the 5th Annual Workshop on Very Large Corpora, pages 192–202, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>414--420</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="1570" citStr="Fung and Yee, 1998" startWordPosition="223" endWordPosition="226">ical machine translation (Och and Ney, 2003) and cross-language information retrieval (Ballesteros and Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These </context>
<context position="12653" citStr="Fung and Yee (1998)" startWordPosition="2086" endWordPosition="2089">arability score of P1 reaches 0.924 and that of P2 is 0.939. Both corpora are more comparable than P0 of which the comparability is 0.881. Furthermore, both P1 and P2 are more comparable than P10 (comparability 0.912) and P20 (comparability 0.915), which shows homogeneity is crucial for comparability. The intrinsic evaluation shows the efficiency of our approach which can improve the quality of the given corpus while preserving most of its vocabulary. 3.2 Bilingual Lexicon Extraction Experiments To extract bilingual lexicons from comparable corpora, we directly use here the method proposed by Fung and Yee (1998) which has been referred to as the standard approach in more recent studies (D´ejean et al., 2002; Gaussier et al., 2004; Yu and Tsujii, 2009). In this approach, each word w is represented as a context vector consisting of the words co-occurring with w in a certain window in the corpus. The context vectors in different languages are then bridged with an existing bilingual dictionary. Finally, a similarity score is given to any word pair based on the cosine of their respective context vectors. 3.2.1 Experiment Settings In order to measure the performance of the lexicons extracted, we follow the</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee. 1998. An IR approach for translating new words from nonparallel, comparable texts. In Proceedings of the 17th international conference on Computational linguistics, pages 414–420, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikesh Garera</author>
<author>Chris Callison-Burch</author>
<author>David Yarowsky</author>
</authors>
<title>Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences.</title>
<date>2009</date>
<booktitle>In CoNLL 09: Proceedings of the Thirteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>129--137</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="1693" citStr="Garera et al., 2009" startWordPosition="245" endWordPosition="248">se it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These approaches seem to have reached a plateau in terms of performance. More recently, and departing from such traditional appro</context>
</contexts>
<marker>Garera, Callison-Burch, Yarowsky, 2009</marker>
<rawString>Nikesh Garera, Chris Callison-Burch, and David Yarowsky. 2009. Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences. In CoNLL 09: Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 129–137, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gaussier</author>
<author>J-M Renders</author>
<author>I Matveeva</author>
<author>C Goutte</author>
<author>H D´ejean</author>
</authors>
<title>A geometric view on bilingual lexicon extraction from comparable corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>526--533</pages>
<location>Barcelona,</location>
<marker>Gaussier, Renders, Matveeva, Goutte, D´ejean, 2004</marker>
<rawString>E. Gaussier, J.-M. Renders, I. Matveeva, C. Goutte, and H. D´ejean. 2004. A geometric view on bilingual lexicon extraction from comparable corpora. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 526–533, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audrey Laroche</author>
<author>Philippe Langlais</author>
</authors>
<title>Revisiting context-based projection methods for term-translation spotting in comparable corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>617--625</pages>
<location>Beijing, China,</location>
<contexts>
<context position="16762" citStr="Laroche and Langlais, 2010" startWordPosition="2807" endWordPosition="2810">an note, our method consistently outperforms the previous work and also the original corpus on all the values considered for N. (a) Precision (b) Recall Figure 1: Performance of bilingual lexicon extraction from different corpora with varied N values from 1 to 300. The five lines from the top down in each subfigure are corresponding to the results for P2, P21, P1, P11 and P° respectively. 4 Discussion As previous studies on bilingual lexicon extraction from comparable corpora radically differ on resources used and technical choices, it is very difficult to compare them in a unified framework (Laroche and Langlais, 2010). We compare in this section our method with some ones in the same vein (i.e. enhancing bilingual corpora prior to extracting bilingual lexicons from them). Some works like (Munteanu et al., 2004) and (Munteanu and Marcu, 2006) propose methods to extract parallel fragments from comparable corpora. However, their approach only focuses on a very small part of the original corpus, whereas our work aims at preserving most of the vocabulary of the original corpus. We have followed here the general approach in (Li and Gaussier, 2010) which consists in enhancing the quality of a comparable corpus pri</context>
</contexts>
<marker>Laroche, Langlais, 2010</marker>
<rawString>Audrey Laroche and Philippe Langlais. 2010. Revisiting context-based projection methods for term-translation spotting in comparable corpora. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 617–625, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Li</author>
<author>Eric Gaussier</author>
</authors>
<title>Improving corpus comparability for bilingual lexicon extraction from comparable corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>644--652</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2343" citStr="Li and Gaussier, 2010" startWordPosition="349" endWordPosition="352">f and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These approaches seem to have reached a plateau in terms of performance. More recently, and departing from such traditional approaches, we have proposed in (Li and Gaussier, 2010) an approach based on improving the comparability of the corpus under consideration, prior to extracting bilingual lexicons. This approach is interesting since there is no point in trying to extract lexicons from a corpus with a low degree of comparability, as the probability of finding translations of any given word is low in such cases. We follow here the same general idea and aim, in a first step, at improving the comparability of a given corpus while preserving most of its vocabulary. However, unlike the previous work, we show here that it is possible to guarantee a certain degree of homog</context>
<context position="3851" citStr="Li and Gaussier, 2010" startWordPosition="587" endWordPosition="590">ity measure proposed in former work, prior to describing the clustering-based algorithm to improve the 473 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 473–478, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics quality of a given comparable corpus. For convenience, the following discussion will be made in the context of the English-French comparable corpus. 2.1 The Comparability Measure In order to measure the degree of comparability of bilingual corpora, we make use of the measure M developed in (Li and Gaussier, 2010): Given a comparable corpus P consisting of an English part Pe and a French part Pf, the degree of comparability of P is defined as the expectation of finding the translation of any given source/target word in the target/source corpus vocabulary. Let σ be a function indicating whether a translation from the translation set Tw of the word w is found in the vocabulary Pv of a corpus P, i.e.: 1 iff Tw ∩ Pv =6 ∅ l σ(w, P) =0 else and let D be a bilingual dictionary with Dve denoting its English vocabulary and Dvf its French vocabulary. The comparability measure M can be written as: M(Pe,Pf) (1) � </context>
<context position="10494" citStr="Li and Gaussier, 2010" startWordPosition="1736" endWordPosition="1739">, the Glasgow Herald (GH95, English), Le Monde (MON94, French), SDA French 94 (SDA94, French) and SDA French 95 (SDA95, French). In addition, two monolingual corpora Wiki-En and Wiki-Fr were built by respectively retrieving all the articles below the category Society and Soci´et´e from the Wikipedia dump files3. The bilingual dictionary used in the experiments is constructed from an online dictionary. It consists of 33k distinct English words and 28k distinct French words, constituting 76k translation pairs. In our experiments, we use the method described in this paper, as well as the one in (Li and Gaussier, 2010) which is the only alternative method to enhance corpus comparability. 3.1 Improving Corpus Quality In this subsection, the clustering algorithm described in Section 2.2.1 is employed to improve the quality of the comparable corpus. The corpora GH95 and SDA95 are used as the original corpus P0 (56k English documents and 42k French documents). We consider two external corpora: P1T (109k English documents and 87k French documents) consisting of the corpora LAT94, MON94 and SDA94; P2T (368k English documents and 378k French documents) consisting of Wiki-En and Wiki-Fr. 1http://trec.nist.gov 2http</context>
<context position="11767" citStr="Li and Gaussier, 2010" startWordPosition="1940" endWordPosition="1943">can be downloaded at http://download.wikimedia.org. In this paper, we use the English dump file on July 13, 2009 and the French dump file on July 7, 2009. 475 P0 P10 P20 P1 P2 P1 &gt; P0 P2 &gt; P0 Precision 0.226 0.277 0.325 0.295 0.461 0.069, 30.5% 0.235, 104.0% Recall 0.103 0.122 0.145 0.133 0.212 0.030, 29.1% 0.109, 105.8% Table 1: Performance of the bilingual lexicon extraction from different corpora (best results in bold) After the clustering process, we obtain the resulting corpora P1 (with the external corpus P1T) and P2 (with P2T). As mentioned before, we also used the method described in (Li and Gaussier, 2010) on the same data, producing resulting corpora P10 (with P1T) and P20 (with P2T) from P0. In terms of lexical coverage, P1 (resp. P2) covers 97.9% (resp. 99.0%) of the vocabulary of P0. Hence, most of the vocabulary of the original corpus has been preserved. The comparability score of P1 reaches 0.924 and that of P2 is 0.939. Both corpora are more comparable than P0 of which the comparability is 0.881. Furthermore, both P1 and P2 are more comparable than P10 (comparability 0.912) and P20 (comparability 0.915), which shows homogeneity is crucial for comparability. The intrinsic evaluation shows</context>
<context position="14501" citStr="Li and Gaussier, 2010" startWordPosition="2399" endWordPosition="2402"> to the proportion of lists containing the correct translation (in case of multiple translations, a list is deemed to contain the correct translation as soon as one of the possible translations is present). The recall is the proportion of correct translations found in the lists to all the translations in the corpus. This evaluation procedure has been used in previous studies and is now standard. 3.2.2 Results and Analysis In a first series of experiments, bilingual lexicons were extracted from the corpora obtained by our approach (P1 and P2), the corpora obtained by the approach described in (Li and Gaussier, 2010) (P10 and P20) and the original corpus P0, with the fixed N value set to 20. Table 1 displays the results obtained. Each of the last two columns “P1 &gt; P0” and “P2 &gt; P0” contains the absolute and the relative difference (in %) w.r.t. P0. As one can note, the best results (in bold) are obtained from the corpora P2 built with the method we have described in this paper. The lexicons extracted from the enhanced corpora are of much higher quality than the ones obtained from the original corpus . For instance, the increase of the precision is 6.9% (30.5% relatively) in P1 and 23.5% (104.0% relatively</context>
</contexts>
<marker>Li, Gaussier, 2010</marker>
<rawString>Bo Li and Eric Gaussier. 2010. Improving corpus comparability for bilingual lexicon extraction from comparable corpora. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 644–652, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>B´eatrice Daille</author>
<author>Koichi Takeuchi</author>
<author>Kyo Kageura</author>
</authors>
<title>Bilingual terminology mining -using brain, not brawn comparable corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>664--671</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1672" citStr="Morin et al., 2007" startWordPosition="241" endWordPosition="244"> Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These approaches seem to have reached a plateau in terms of performance. More recently, and departing from s</context>
<context position="4922" citStr="Morin et al., 2007" startWordPosition="786" endWordPosition="789">al dictionary with Dve denoting its English vocabulary and Dvf its French vocabulary. The comparability measure M can be written as: M(Pe,Pf) (1) � e σ(w, Pf) + � f σ(w, Pe) wEPenVv wEPfnVv #w(Pe ∩ Dv e) + #w(Pf ∩ Dvf) where #w(P) denotes the number of different words present in P. One can find from equation 1 that M directly measures the proportion of source/target words translated in the target/source vocabulary of P. 2.2 Clustering Documents for High Quality Comparable Corpora If a corpus covers a limited set of topics, it is more likely to contain consistent information on the words used (Morin et al., 2007), leading to improved bilingual lexicons extracted with existing algorithms relying on the distributional hypothesis. The term homogeneity directly refers to this fact, and we will say, in an informal manner, that a corpus is homogeneous if it covers a limited set of topics. The rationale for the algorithm we introduce here to enhance corpus comparability is precisely based on the concept of homogeneity. In order to find document sets which are similar with each other (i.e. homogeneous), it is natural to resort to clustering techniques. Furthermore, since we need homogeneous corpora for biling</context>
</contexts>
<marker>Morin, Daille, Takeuchi, Kageura, 2007</marker>
<rawString>Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi, and Kyo Kageura. 2007. Bilingual terminology mining -using brain, not brawn comparable corpora. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 664–671, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Extracting parallel sub-sentential fragments from nonparallel corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>81--88</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="16989" citStr="Munteanu and Marcu, 2006" startWordPosition="2845" endWordPosition="2848"> with varied N values from 1 to 300. The five lines from the top down in each subfigure are corresponding to the results for P2, P21, P1, P11 and P° respectively. 4 Discussion As previous studies on bilingual lexicon extraction from comparable corpora radically differ on resources used and technical choices, it is very difficult to compare them in a unified framework (Laroche and Langlais, 2010). We compare in this section our method with some ones in the same vein (i.e. enhancing bilingual corpora prior to extracting bilingual lexicons from them). Some works like (Munteanu et al., 2004) and (Munteanu and Marcu, 2006) propose methods to extract parallel fragments from comparable corpora. However, their approach only focuses on a very small part of the original corpus, whereas our work aims at preserving most of the vocabulary of the original corpus. We have followed here the general approach in (Li and Gaussier, 2010) which consists in enhancing the quality of a comparable corpus prior to extracting information from it. However, despite this latter work, we have shown here a method which ensures homogeneity of the obtained corpus, and which finally leads to comparable corpora of higher quality. In turn suc</context>
</contexts>
<marker>Munteanu, Marcu, 2006</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2006. Extracting parallel sub-sentential fragments from nonparallel corpora. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 81–88, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Improved machine translation performance via parallel sentence extraction from comparable corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL 2004,</booktitle>
<pages>265--272</pages>
<location>Boston, MA., USA.</location>
<contexts>
<context position="16958" citStr="Munteanu et al., 2004" startWordPosition="2840" endWordPosition="2843">ction from different corpora with varied N values from 1 to 300. The five lines from the top down in each subfigure are corresponding to the results for P2, P21, P1, P11 and P° respectively. 4 Discussion As previous studies on bilingual lexicon extraction from comparable corpora radically differ on resources used and technical choices, it is very difficult to compare them in a unified framework (Laroche and Langlais, 2010). We compare in this section our method with some ones in the same vein (i.e. enhancing bilingual corpora prior to extracting bilingual lexicons from them). Some works like (Munteanu et al., 2004) and (Munteanu and Marcu, 2006) propose methods to extract parallel fragments from comparable corpora. However, their approach only focuses on a very small part of the original corpus, whereas our work aims at preserving most of the vocabulary of the original corpus. We have followed here the general approach in (Li and Gaussier, 2010) which consists in enhancing the quality of a comparable corpus prior to extracting information from it. However, despite this latter work, we have shown here a method which ensures homogeneity of the obtained corpus, and which finally leads to comparable corpora</context>
</contexts>
<marker>Munteanu, Fraser, Marcu, 2004</marker>
<rawString>Dragos Stefan Munteanu, Alexander Fraser, and Daniel Marcu. 2004. Improved machine translation performance via parallel sentence extraction from comparable corpora. In Proceedings of the HLT-NAACL 2004, pages 265–272, Boston, MA., USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="996" citStr="Och and Ney, 2003" startWordPosition="140" endWordPosition="143">ns extracted from comparable corpora. We introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and finally preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches. 1 Introduction Bilingual lexicons are an important resource in multilingual natural language processing tasks such as statistical machine translation (Och and Ney, 2003) and cross-language information retrieval (Ballesteros and Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et a</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>519--526</pages>
<location>College Park, Maryland, USA.</location>
<contexts>
<context position="1582" citStr="Rapp, 1999" startWordPosition="227" endWordPosition="228">tion (Och and Ney, 2003) and cross-language information retrieval (Ballesteros and Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These approaches s</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 519–526, College Park, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Robitaille</author>
<author>Yasuhiro Sasaki</author>
<author>Masatsugu Tonoike</author>
<author>Satoshi Sato</author>
<author>Takehito Utsuro</author>
</authors>
<title>Compiling French-Japanese terminologies from the web.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11st Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>225--232</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="1652" citStr="Robitaille et al., 2006" startWordPosition="237" endWordPosition="240">etrieval (Ballesteros and Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These approaches seem to have reached a plateau in terms of performance. More recently, </context>
</contexts>
<marker>Robitaille, Sasaki, Tonoike, Sato, Utsuro, 2006</marker>
<rawString>Xavier Robitaille, Yasuhiro Sasaki, Masatsugu Tonoike, Satoshi Sato, and Takehito Utsuro. 2006. Compiling French-Japanese terminologies from the web. In Proceedings of the 11st Conference of the European Chapter of the Association for Computational Linguistics, pages 225–232, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphna Shezaf</author>
<author>Ari Rappoport</author>
</authors>
<title>Bilingual lexicon generation using non-aligned signatures.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>98--107</pages>
<location>Uppsala,</location>
<contexts>
<context position="1743" citStr="Shezaf and Rappoport, 2010" startWordPosition="253" endWordPosition="256">gual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These approaches seem to have reached a plateau in terms of performance. More recently, and departing from such traditional approaches, we have proposed in (Li and Gaussier, 2010)</context>
</contexts>
<marker>Shezaf, Rappoport, 2010</marker>
<rawString>Daphna Shezaf and Ari Rappoport. 2010. Bilingual lexicon generation using non-aligned signatures. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 98–107, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Yu</author>
<author>Junichi Tsujii</author>
</authors>
<title>Extracting bilingual dictionary from comparable corpora with dependency heterogeneity.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL 2009,</booktitle>
<pages>121--124</pages>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="1714" citStr="Yu and Tsujii, 2009" startWordPosition="249" endWordPosition="252"> manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These approaches seem to have reached a plateau in terms of performance. More recently, and departing from such traditional approaches, we have propos</context>
<context position="12795" citStr="Yu and Tsujii, 2009" startWordPosition="2111" endWordPosition="2114">rthermore, both P1 and P2 are more comparable than P10 (comparability 0.912) and P20 (comparability 0.915), which shows homogeneity is crucial for comparability. The intrinsic evaluation shows the efficiency of our approach which can improve the quality of the given corpus while preserving most of its vocabulary. 3.2 Bilingual Lexicon Extraction Experiments To extract bilingual lexicons from comparable corpora, we directly use here the method proposed by Fung and Yee (1998) which has been referred to as the standard approach in more recent studies (D´ejean et al., 2002; Gaussier et al., 2004; Yu and Tsujii, 2009). In this approach, each word w is represented as a context vector consisting of the words co-occurring with w in a certain window in the corpus. The context vectors in different languages are then bridged with an existing bilingual dictionary. Finally, a similarity score is given to any word pair based on the cosine of their respective context vectors. 3.2.1 Experiment Settings In order to measure the performance of the lexicons extracted, we follow the common practice by dividing the bilingual dictionary into 2 parts: 10% of the English words (3,338 words) together with their translations ar</context>
</contexts>
<marker>Yu, Tsujii, 2009</marker>
<rawString>Kun Yu and Junichi Tsujii. 2009. Extracting bilingual dictionary from comparable corpora with dependency heterogeneity. In Proceedings of HLT-NAACL 2009, pages 121–124, Boulder, Colorado, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>