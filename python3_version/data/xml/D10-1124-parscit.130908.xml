<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.997007">
A Latent Variable Model for Geographic Lexical Variation
</title>
<author confidence="0.998295">
Jacob Eisenstein Brendan O’Connor Noah A. Smith Eric P. Xing
</author>
<affiliation confidence="0.900061333333333">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998798">
{jacobeis,brendano,nasmith,epxing}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997331941176471">
The rapid growth of geotagged social media
raises new computational possibilities for in-
vestigating geographic linguistic variation. In
this paper, we present a multi-level generative
model that reasons jointly about latent topics
and geographical regions. High-level topics
such as “sports” or “entertainment” are ren-
dered differently in each geographic region,
revealing topic-specific regional distinctions.
Applied to a new dataset of geotagged mi-
croblogs, our model recovers coherent top-
ics and their regional variants, while identi-
fying geographic areas of linguistic consis-
tency. The model also enables prediction of
an author’s geographic location from raw text,
outperforming both text regression and super-
vised topic models.
</bodyText>
<sectionHeader confidence="0.999524" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978754716981">
Sociolinguistics and dialectology study how lan-
guage varies across social and regional contexts.
Quantitative research in these fields generally pro-
ceeds by counting the frequency of a handful of
previously-identified linguistic variables: pairs of
phonological, lexical, or morphosyntactic features
that are semantically equivalent, but whose fre-
quency depends on social, geographical, or other
factors (Paolillo, 2002; Chambers, 2009). It is left to
the experimenter to determine which variables will
be considered, and there is no obvious procedure for
drawing inferences from the distribution of multiple
variables. In this paper, we present a method for
identifying geographically-aligned lexical variation
directly from raw text. Our approach takes the form
of a probabilistic graphical model capable of iden-
tifying both geographically-salient terms and coher-
ent linguistic communities.
One challenge in the study of lexical variation is
that term frequencies are influenced by a variety of
factors, such as the topic of discourse. We address
this issue by adding latent variables that allow us to
model topical variation explicitly. We hypothesize
that geography and topic interact, as “pure” topi-
cal lexical distributions are corrupted by geographi-
cal factors; for example, a sports-related topic will
be rendered differently in New York and Califor-
nia. Each author is imbued with a latent “region”
indicator, which both selects the regional variant of
each topic, and generates the author’s observed ge-
ographical location. The regional corruption of top-
ics is modeled through a cascade of logistic normal
priors—a general modeling approach which we call
cascading topic models. The resulting system has
multiple capabilities, including: (i) analyzing lexi-
cal variation by both topic and geography; (ii) seg-
menting geographical space into coherent linguistic
communities; (iii) predicting author location based
on text alone.
This research is only possible due to the rapid
growth of social media. Our dataset is derived from
the microblogging website Twitter,1 which permits
users to post short messages to the public. Many
users of Twitter also supply exact geographical co-
ordinates from GPS-enabled devices (e.g., mobile
phones),2 yielding geotagged text data. Text in
computer-mediated communication is often more
vernacular (Tagliamonte and Denis, 2008), and as
such it is more likely to reveal the influence of ge-
ographic factors than text written in a more formal
genre, such as news text (Labov, 1966).
We evaluate our approach both qualitatively and
quantitatively. We investigate the topics and regions
</bodyText>
<footnote confidence="0.999248">
1http://www.twitter.com
2User profiles also contain self-reported location names, but
we do not use that information in this work.
</footnote>
<page confidence="0.838565">
1277
</page>
<note confidence="0.818992">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1277–1287,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999888875">
that the model obtains, showing both common-sense
results (place names and sports teams are grouped
appropriately), as well as less-obvious insights about
slang. Quantitatively, we apply our model to predict
the location of unlabeled authors, using text alone.
On this task, our model outperforms several alterna-
tives, including both discriminative text regression
and related latent-variable approaches.
</bodyText>
<sectionHeader confidence="0.997489" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999972264705882">
The main dataset in this research is gathered from
the microblog website Twitter, via its official API.
We use an archive of messages collected over the
first week of March 2010 from the “Gardenhose”
sample stream,3 which then consisted of 15% of
all public messages, totaling millions per day. We
aggressively filter this stream, using only messages
that are tagged with physical (latitude, longitude)
coordinate pairs from a mobile client, and whose au-
thors wrote at least 20 messages over this period. We
also filter to include only authors who follow fewer
than 1,000 other people, and have fewer than 1,000
followers. Kwak et al. (2010) find dramatic shifts
in behavior among users with social graph connec-
tivity outside of that range; such users may be mar-
keters, celebrities with professional publicists, news
media sources, etc. We also remove messages con-
taining URLs to eliminate bots posting information
such as advertising or weather conditions. For inter-
pretability, we restrict our attention to authors inside
a bounding box around the contiguous U.S. states,
yielding a final sample of about 9,500 users and
380,000 messages, totaling 4.7 million word tokens.
We have made this dataset available online.4
Informal text from mobile phones is challeng-
ing to tokenize; we adapt a publicly available tok-
enizer5 originally developed for Twitter (O’Connor
et al., 2010), which preserves emoticons and blocks
of punctuation and other symbols as tokens. For
each user’s Twitter feed, we combine all messages
into a single “document.” We remove word types
that appear in fewer than 40 feeds, yielding a vocab-
ulary of 5,216 words. Of these, 1,332 do not appear
in the English, French, or Spanish dictionaries of the
</bodyText>
<footnote confidence="0.999896">
3http://dev.twitter.com/pages/streaming_api
4http://www.ark.cs.cmu.edu/GeoTwitter
5http://tweetmotif.com
</footnote>
<bodyText confidence="0.99821">
spell-checking program aspell.
Every message is tagged with a location, but most
messages from a single individual tend to come from
nearby locations (as they go about their day); for
modeling purposes we use only a single geographic
location for each author, simply taking the location
of the first message in the sample.
The authors in our dataset are fairly heavy Twit-
ter users, posting an average of 40 messages per day
(although we see only 15% of this total). We have
little information about their demographics, though
from the text it seems likely that this user set skews
towards teens and young adults. The dataset cov-
ers each of the 48 contiguous United States and the
District of Columbia.
</bodyText>
<sectionHeader confidence="0.996864" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999971846153846">
We develop a model that incorporates two sources
of lexical variation: topic and geographical region.
We treat the text and geographic locations as out-
puts from a generative process that incorporates both
topics and regions as latent variables.6 During infer-
ence, we seek to recover the topics and regions that
best explain the observed data.
At the base level of model are “pure” topics (such
as “sports”, “weather”, or “slang”); these topics are
rendered differently in each region. We call this gen-
eral modeling approach a cascading topic model; we
describe it first in general terms before moving to the
specific application to geographical variation.
</bodyText>
<subsectionHeader confidence="0.999836">
3.1 Cascading Topic Models
</subsectionHeader>
<bodyText confidence="0.999763583333333">
Cascading topic models generate text from a chain
of random variables. Each element in the chain de-
fines a distribution over words, and acts as the mean
of the distribution over the subsequent element in
the chain. Thus, each element in the chain can be
thought of as introducing some additional corrup-
tion. All words are drawn from the final distribution
in the chain.
At the beginning of the chain are the priors, fol-
lowed by unadulerated base topics, which may then
be corrupted by other factors (such as geography or
time). For example, consider a base “food” topic
</bodyText>
<footnote confidence="0.997736333333333">
6The region could be observed by using a predefined geo-
graphical decomposition, e.g., political boundaries. However,
such regions may not correspond well to linguistic variation.
</footnote>
<page confidence="0.996056">
1278
</page>
<bodyText confidence="0.999923578947368">
that emphasizes words like dinner and delicious;
the corrupted “food-California” topic would place
weight on these words, but might place extra em-
phasis on other words like sprouts.
The path through the cascade is determined by a
set of indexing variables, which may be hidden or
observed. As in standard latent Dirichlet allocation
(Blei et al., 2003), the base topics are selected by
a per-token hidden variable z. In the geographical
topic model, the next level corresponds to regions,
which are selected by a per-author latent variable r.
Formally, we draw each level of the cascade from
a normal distribution centered on the previous level;
the final multinomial distribution over words is ob-
tained by exponentiating and normalizing. To ensure
tractable inference, we assume that all covariance
matrices are uniform diagonal, i.e., aI with a &gt; 0;
this means we do not model interactions between
words.
</bodyText>
<subsectionHeader confidence="0.999734">
3.2 The Geographic Topic Model
</subsectionHeader>
<bodyText confidence="0.99699784375">
The application of cascading topic models to ge-
ographical variation is straightforward. Each doc-
ument corresponds to the entire Twitter feed of a
given author during the time period covered by our
corpus. For each author, the latent variable r cor-
responds to the geographical region of the author,
which is not observed. As described above, r se-
lects a corrupted version of each topic: the kth basic
topic has mean µk, with uniform diagonal covari-
ance U2k; for region j, we can draw the regionally-
corrupted topic from the normal distribution, ηjk ∼
N(µk, U2kI).
Because η is normally-distributed, it lies not in
the simplex but in ][8W. We deterministically com-
pute multinomial parameters β by exponentiating
and normalizing: βjk = exp(ηjk)/ Ei exp(�(i)
jk ).
This normalization could introduce identifiability
problems, as there are multiple settings for η that
maximize P(w|η) (Blei and Lafferty, 2006a). How-
ever, this difficulty is obviated by the priors: given
µ and U2, there is only a single η that maximizes
P(w|η)P(η|µ, U2); similarly, only a single µ max-
imizes P(η|µ)P(µ|a, b2).
The observed latitude and longitude, denoted y,
are normally distributed and conditioned on the re-
gion, with mean νr and precision matrix Ar indexed
by the region r. The region index r is itself drawn
from a single shared multinomial ϑ. The model is
shown as a plate diagram in Figure 1.
Given a vocabulary size W, the generative story
is as follows:
</bodyText>
<listItem confidence="0.96721">
• Generate base topics: for each topic k &lt; K
</listItem>
<bodyText confidence="0.525708153846154">
– Draw the base topic from a normal distribu-
tion with uniform diagonal covariance: µk ∼
N(a, b2I),
– Draw the regional variance from a Gamma
distribution: U2k ∼(c, d).
– Generate regional variants: for each region
j &lt; J,
∗ Draw the region-topic ηjk from a normal
distribution with uniform diagonal covari-
ance: ηjk ∼ N(µk, U2kI).
∗ Convert ηjk into a multinomial
distribution over words by ex-
ponentiating and normalizing:
</bodyText>
<equation confidence="0.9951525">
βjk = exp (ηjk) / EW exp(����
jk ),
</equation>
<bodyText confidence="0.998974">
where the denominator sums over the
vocabulary.
</bodyText>
<listItem confidence="0.900693294117647">
• Generate regions: for each region j &lt; J,
– Draw the spatial mean νj from a normal dis-
tribution.
– Draw the precision matrix Aj from a Wishart
distribution.
• Draw the distribution over regions ϑ from a sym-
metric Dirichlet prior, ϑ ∼ Dir(a,y1).
• Generate text and locations: for each document d,
– Draw topic proportions from a symmetric
Dirichlet prior, θ ∼ Dir(a1).
– Draw the region r from the multinomial dis-
tribution ϑ.
– Draw the location y from the bivariate Gaus-
sian, y ∼ N(νT, AT).
– For each word token,
∗ Draw the topic indicator z ∼ θ.
∗ Draw the word token w ∼ βTz.
</listItem>
<sectionHeader confidence="0.999386" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.999966625">
We apply mean-field variational inference: a fully-
factored variational distribution Q is chosen to min-
imize the Kullback-Leibler divergence from the
true distribution. Mean-field variational inference
with conjugate priors is described in detail else-
where (Bishop, 2006; Wainwright and Jordan,
2008); we restrict our focus to the issues that are
unique to the geographic topic model.
</bodyText>
<page confidence="0.969282">
1279
</page>
<figure confidence="0.743445571428571">
0
W
N,
Y
Z
r
D
K
o2
α
0
A
µ
�
V
�
µk log of base topic k’s distribution over word types
σk variance parameter for regional variants of topic k
ηjk region j’s variant of base topic µk
θd author d’s topic proportions
rd author d’s latent region
yd author d’s observed GPS location
νj region j’s spatial center
Λj region j’s spatial precision
zn token n’s topic assignment
wn token n’s observed word type
α global prior over author-topic proportions
ϑ global prior over region classes
</figure>
<figureCaption confidence="0.9741665">
Figure 1: Plate diagram for the geographic topic model, with a table of all random variables. Priors (besides α) are
omitted for clarity, and the document indices on z and w are implicit.
</figureCaption>
<bodyText confidence="0.999820538461538">
We place variational distributions over all latent
variables of interest: θ, z, r, ϑ, η, µ, σ2, ν, and Λ,
updating each of these distributions in turn, until
convergence. The variational distributions over θ
and ϑ are Dirichlet, and have closed form updates:
each can be set to the sum of the expected counts,
plus a term from the prior (Blei et al., 2003). The
variational distributions q(z) and q(r) are categor-
ical, and can be set proportional to the expected
joint likelihood—to set q(z) we marginalize over r,
and vice versa.7 The updates for the multivariate
Gaussian spatial parameters ν and Λ are described
by Penny (2001).
</bodyText>
<subsectionHeader confidence="0.98628">
4.1 Regional Word Distributions
</subsectionHeader>
<bodyText confidence="0.999272">
The variational region-topic distribution ηjk is nor-
mal, with uniform diagonal covariance for tractabil-
ity. Throughout we will write hxi to indicate the ex-
pectation of x under the variational distribution Q.
Thus, the vector mean of the distribution q(ηjk) is
written hηjki, while the variance (uniform across i)
of q(η) is written V(ηjk).
To update the mean parameter hηjki, we max-
imize the contribution to the variational bound L
from the relevant terms:
</bodyText>
<equation confidence="0.9914046">
L[(η���
�� )] = hlog p(w|β, z, r)i+hlog p(η(i)
jk |µ(i)
k , σ2k)i,
(1)
</equation>
<bodyText confidence="0.966511928571428">
7Thanks to the naive mean field assumption, we can
marginalize over z by first decomposing across all Nd words
and then summing over q(z).
with the first term representing the likelihood of the
observed words (recall that β is computed determin-
istically from η) and the second term corresponding
to the prior. The likelihood term requires the expec-
tation hlog βi, but this is somewhat complicated by
the normalizer EWi exp(η(i)), which sums over all
terms in the vocabulary. As in previous work on lo-
gistic normal topic models, we use a Taylor approx-
imation for this term (Blei and Lafferty, 2006a).
The prior on η is normal, so the contribution from
the second term of the objective (Equation 1) is
</bodyText>
<equation confidence="0.992342">
1
−2(σk) h(η(i)
</equation>
<bodyText confidence="0.9921822">
jk − µki))2i. We introduce the following
notation for expected counts: N(i, j, k) indicates the
expected count of term i in region j and topic k, and
N(j, k) = Ei N(i, j, k). After some calculus, we
can write the gradient ∂L/∂hη((i))
</bodyText>
<equation confidence="0.949246">
jk i as
N(i, j, k) − N(j, k)hβ(i)
jk i − hσk 2i (hηjk i − hµ(i)
k i),
</equation>
<bodyText confidence="0.970226266666667">
(2)
which has an intuitive interpretation. The first two
terms represent the difference in expected counts for
term i under the variational distributions q(z, r) and
q(z, r,β): this difference goes to zero when β(i)
jk per-
fectly matches N(i, j, k)/N(j, k). The third term
penalizesη(i) jkfor deviating from its prior µ(i)
k , but
this penalty is proportional to the expected inverse
variance hσ�2
k i. We apply gradient ascent to maxi-
mize the objective L. A similar set of calculations
gives the gradient for the variance of η; these are
described in an forthcoming appendix.
</bodyText>
<page confidence="0.944486">
1280
</page>
<subsectionHeader confidence="0.997227">
4.2 Base Topics
</subsectionHeader>
<bodyText confidence="0.999986181818182">
The base topic parameters are µk and σ2k; in the vari-
ational distribution, q(µk) is normally distributed
and q(σ2k) is Gamma distributed. Note that µk and
σ2k affect only the regional word distributions 7 jk.
An advantage of the logistic normal is that the vari-
ational parameters over µk are available in closed
form,
where J indicates the number of regions. The ex-
pectation of the base topic µ incorporates the prior
and the average of the generated region-topics—
these two components are weighted respectively by
the expected variance of the region-topics (σ2k) and
the prior topical variance b2. The posterior variance
V(µ) is a harmonic combination of the prior vari-
ance b2 and the expected variance of the region top-
ics.
The variational distribution over the region-topic
variance σ2k has Gamma parameters. These param-
eters cannot be updated in closed form, so gradi-
ent optimization is again required. The derivation
of these updates is more involved, and is left for a
forthcoming appendix.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="method">
5 Implementation
</sectionHeader>
<bodyText confidence="0.999849119047619">
Variational scheduling and initialization are impor-
tant aspects of any hierarchical generative model,
and are often under-discussed. In our implementa-
tion, the variational updates are scheduled as fol-
lows: given expected counts, we iteratively update
the variational parameters on the region-topics 77 and
the base topics µ, until convergence. We then update
the geographical parameters v and A, as well as the
distribution over regions 0. Finally, for each doc-
ument we iteratively update the variational param-
eters over 0, z, and r until convergence, obtaining
expected counts that are used in the next iteration
of updates for the topics and their regional variants.
We iterate an outer loop over the entire set of updates
until convergence.
We initialize the model in a piecewise fashion.
First we train a Dirichlet process mixture model on
the locations y, using variational inference on the
truncated stick-breaking approximation (Blei and
Jordan, 2006). This automatically selects the num-
ber of regions J, and gives a distribution over each
region indicator rd from geographical information
alone. We then run standard latent Dirichlet alloca-
tion to obtain estimates of z for each token (ignoring
the locations). From this initialization we can com-
pute the first set of expected counts, which are used
to obtain initial estimates of all parameters needed
to begin variational inference in the full model.
The prior a is the expected mean of each topic
µ; for each term i, we set a(i) = log N(i) − log N,
where N(i) is the total count of i in the corpus and
N = Ei N(i). The variance prior b2 is set to 1, and
the prior on σ2 is the Gamma distribution 9(2, 200),
encouraging minimal deviation from the base topics.
The symmetric Dirichlet prior on 0 is set to 12, and
the symmetric Dirichlet parameter on ϑ is updated
from weak hyperpriors (Minka, 2003). Finally, the
geographical model takes priors that are linked to the
data: for each region, the mean is very weakly en-
couraged to be near the overall mean, and the covari-
ance prior is set by the average covariance of clusters
obtained by running K-means.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="method">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999885333333333">
For a quantitative evaluation of the estimated rela-
tionship between text and geography, we assess our
model’s ability to predict the geographic location of
unlabeled authors based on their text alone.8 This
task may also be practically relevant as a step toward
applications for recommending local businesses or
social connections. A randomly-chosen 60% of au-
thors are used for training, 20% for development,
and the remaining 20% for final evaluation.
</bodyText>
<subsectionHeader confidence="0.992857">
6.1 Systems
</subsectionHeader>
<bodyText confidence="0.984557222222222">
We compare several approaches for predicting au-
thor location; we divide these into latent variable
generative models and discriminative approaches.
8Alternatively, one might evaluate the attributed regional
memberships of the words themselves. While the Dictionary of
American Regional English (Cassidy and Hall, 1985) attempts
a comprehensive list of all regionally-affiliated terms, it is based
on interviews conducted from 1965-1970, and the final volume
(covering Si–Z) is not yet complete.
</bodyText>
<equation confidence="0.989190333333333">
(i)b2 EJj (η(i)
jk ) + (σ2k)a(i)
(µk ) = b2J + (σ2k)
V(µk) = (b−2 + J(σ−2
k ))−1
,
</equation>
<page confidence="0.949776">
1281
</page>
<subsectionHeader confidence="0.725495">
6.1.1 Latent Variable Models
</subsectionHeader>
<bodyText confidence="0.986221552631579">
Geographic Topic Model This is the full version
of our system, as described in this paper. To pre-
dict the unseen location yd, we iterate until con-
vergence on the variational updates for the hidden
topics zd, the topic proportions 9d, and the region
rd. From rd, the location can be estimated as yd =
�J
argmaxy j p(y|vj, Aj)q(rd = j). The develop-
ment set is used to tune the number of topics and to
select the best of multiple random initializations.
Mixture of Unigrams A core premise of our ap-
proach is that modeling topical variation will im-
prove our ability to understand geographical varia-
tion. We test this idea by fixing K = 1, running our
system with only a single topic. This is equivalent
to a Bayesian mixture of unigrams in which each au-
thor is assigned a single, regional unigram language
model that generates all of his or her text. The de-
velopment set is used to select the best of multiple
random initializations.
Supervised Latent Dirichlet Allocation In a
more subtle version of the mixture-of-unigrams
model, we model each author as an admixture of re-
gions. Thus, the latent variable attached to each au-
thor is no longer an index, but rather a vector on the
simplex. This model is equivalent to supervised la-
tent Dirichlet allocation (Blei and McAuliffe, 2007):
each topic is associated with equivariant Gaussian
distributions over the latitude and longitude, and
these topics must explain both the text and the ob-
served geographical locations. For unlabeled au-
thors, we estimate latitude and longitude by esti-
mating the topic proportions and then applying the
learned geographical distributions. This is a linear
prediction
f(2d; a) = (�zTd alat, 2Td alon)
for an author’s topic proportions zd and topic-
geography weights a E R2K.
</bodyText>
<subsectionHeader confidence="0.659787">
6.1.2 Baseline Approaches
</subsectionHeader>
<bodyText confidence="0.97908625">
Text Regression We perform linear regression
to discriminatively learn the relationship between
words and locations. Using term frequency features
xd for each author, we predict locations with word-
geography weights a E R2W:
f(xd; a) = (xTd alat,
Weights are trained to minimize the sum of squared
Euclidean distances, subject to L1 regularization:
</bodyText>
<equation confidence="0.97800725">
(xTd alat _ �lat
d )2 + (xd Talon _ �lon
d )2
+ Alat||alat||1 + Alon||alon||1
</equation>
<bodyText confidence="0.99995553125">
The minimization problem decouples into two sep-
arate latitude and longitude models, which we fit
using the glmnet elastic net regularized regres-
sion package (Friedman et al., 2010), which ob-
tained good results on other text-based prediction
tasks (Joshi et al., 2010). Regularization parameters
were tuned on the development set. The L1 penalty
outperformed L2 and mixtures of L1 and L2.
Note that for both word-level linear regression
here, and the topic-level linear regression in SLDA,
the choice of squared Euclidean distance dovetails
with our use of spatial Gaussian likelihoods in the
geographic topic models, since optimizing a is
equivalent to maximum likelihood estimation un-
der the assumption that locations are drawn from
equivariant circular Gaussians centered around each
f(xd; a) linear prediction. We experimented with
decorrelating the location dimensions by projecting
yd into the principal component space, but this did
not help text regression.
K-Nearest Neighbors Linear regression is a poor
model for the multimodal density of human popula-
tions. As an alternative baseline, we applied super-
vised K-nearest neighbors to predict the location yd
as the average of the positions of the K most sim-
ilar authors in the training set. We computed term-
frequency inverse-document frequency features and
applied cosine similarity over their first 30 principal
components to find the neighbors. The choices of
principal components, IDF weighting, and neighbor-
hood size K = 20 were tuned on the development
set.
</bodyText>
<subsectionHeader confidence="0.995065">
6.2 Metrics
</subsectionHeader>
<bodyText confidence="0.9834025">
Our principle error metrics are the mean and median
distance between the predicted and true location in
kilometers.9 Because the distance error may be dif-
ficult to interpret, we also report accuracy of classi-
</bodyText>
<footnote confidence="0.990870333333333">
9For convenience, model training and prediction use latitude
and longitude as an unprojected 2D Euclidean space. However,
properly measuring the physical distance between points on the
</footnote>
<figure confidence="0.578449333333333">
xdT alon)
�
d
</figure>
<page confidence="0.809457">
1282
</page>
<table confidence="0.999902444444444">
Regression Classification accuracy (%)
System Mean Dist. (km) Median Dist. (km) Region (4-way) State (49-way)
Geographic topic model 900 494 58 24
Mixture of unigrams 947 644 53 19
Supervised LDA 1055 728 39 4
Text regression 948 712 41 4
K-nearest neighbors 1077 853 37 2
Mean location 1148 1018
Most common class 37 27
</table>
<tableCaption confidence="0.9628955">
Table 1: Location prediction results; lower scores are better on the regression task, higher scores are better on the
classification task. Distances are in kilometers. Mean location and most common class are computed from the test set.
Both the geographic topic model and supervised LDA use the best number of topics from the development set (10 and
5, respectively).
</tableCaption>
<bodyText confidence="0.9997215">
fication by state and by region of the United States.
Our data includes the 48 contiguous states plus the
District of Columbia; the U.S. Census Bureau di-
vides these states into four regions: West, Midwest,
Northeast, and South.10 Note that while major pop-
ulation centers straddle several state lines, most re-
gion boundaries are far from the largest cities, re-
sulting in a clearer analysis.
</bodyText>
<subsectionHeader confidence="0.83989">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.969505833333333">
As shown in Table 1, the geographic topic model
achieves the strongest performance on all metrics.
All differences in performance between systems
are statistically significant (p &lt; .01) using the
Wilcoxon-Mann-Whitney test for regression error
and the k2 test for classification accuracy. Figure 2
shows how performance changes as the number of
topics varies.
Note that the geographic topic model and the mix-
ture of unigrams use identical code and parametriza-
tion – the only difference is that the geographic topic
model accounts for topical variation, while the mix-
ture of unigrams sets K = 1. These results validate
our basic premise that it is important to model the
interaction between topical and geographical varia-
tion.
Text regression and supervised LDA perform es-
pecially poorly on the classification metric. Both
methods make predictions that are averaged across
Earth’s surface requires computing or approximating the great
circle distance – we use the Haversine formula (Sinnott, 1984).
For the continental U.S., the relationship between degrees and
kilometers is nearly linear, but extending the model to a conti-
nental scale would require a more sophisticated approach.
</bodyText>
<footnote confidence="0.720097">
10http://www.census.gov/geo/www/us_regdiv.pdf
Number of topics
</footnote>
<figureCaption confidence="0.998599">
Figure 2: The effect of varying the number of topics on
the median regression error (lower is better).
</figureCaption>
<bodyText confidence="0.999924166666667">
each word in the document: in text regression, each
word is directly multiplied by a feature weight; in
supervised LDA the word is associated with a la-
tent topic first, and then multiplied by a weight. For
these models, all words exert an influence on the pre-
dicted location, so uninformative words will draw
the prediction towards the center of the map. This
yields reasonable distance errors but poor classifica-
tion accuracy. We had hoped that K-nearest neigh-
bors would be a better fit for this metric, but its per-
formance is poor at all values of K. Of course it is
always possible to optimize classification accuracy
directly, but such an approach would be incapable
of predicting the exact geographical location, which
is the focus of our evaluation (given that the desired
geographical partition is unknown). Note that the
geographic topic model is also not trained to opti-
mize classification accuracy.
</bodyText>
<figure confidence="0.985289461538462">
0 5 10 15 20
Median error (Ivm)
1100
1000
400
900
800
700
600
500
Geographic Topic Model
Supervised LDA
Mean location
</figure>
<page confidence="0.859624">
1283
</page>
<table confidence="0.999719896551724">
“basketball” “popular “daily life” “emoticons” “chit chat”
PISTONS KOBE music” tonight shop :) haha :d :( ;) :p lol smh jk yea
LAKERS game album music weekend getting xd :/ hahaha wyd coo ima
DUKE NBA beats artist video going chilling hahah wassup
CAVS STUCKEY #LAKERS ready discount somethin jp
JETS KNICKS ITUNES tour waiting iam
produced vol
CELTICS victory playing daughter BOSTON ;p gna loveee ese exam suttin
BOSTON PEARL alive war sippin
CHARLOTTE comp
+
Boston
N. California THUNDER SIMON dl 6am OAKLAND pues hella koo hella flirt hut
KINGS GIANTS mountain seee SAN fckn iono OAKLAND
pimp trees clap
New York + NETS KNICKS BRONX iam cab oww wasssup nm
#KOBE AUSTIN omw tacos hr af papi raining wyd coo af nada
#LAKERS #LAKERS load HOLLYWOOD th bomb coo tacos messin
HOLLYWOOD HOLLYWOOD fasho bomb
imm MICKEY
TUPAC
Los Angeles
CAVS premiere prod stink CHIPOTLE ;d blvd BIEBER foul WIZ salty
CLEVELAND joint TORONTO tipsy hve OHIO excuses lames
OHIO BUCKS od onto designer officer lastnight
COLUMBUS CANADA village
burr
+
Lake Erie
</table>
<tableCaption confidence="0.985674777777778">
Table 2: Example base topics (top line) and regional variants. For the base topics, terms are ranked by log-odds
compared to the background distribution. The regional variants show words that are strong compared to both the base
topic and the background. Foreign-language words are shown in italics, while terms that are usually in proper nouns
are shown in SMALL CAPS. See Table 3 for definitions of slang terms; see Section 7 for more explanation and details
on the methodology.
Figure 3: Regional clustering of the training set obtained by one randomly-initialized run of the geographical topic
model. Each point represents one author, and each shape/color combination represents the most likely cluster as-
signment. Ellipses represent the regions’ spatial means and covariances. The same model and coloring are shown in
Table 2.
</tableCaption>
<page confidence="0.995896">
1284
</page>
<sectionHeader confidence="0.982567" genericHeader="method">
7 Analysis
</sectionHeader>
<bodyText confidence="0.953910783333334">
Our model permits analysis of geographical vari-
ation in the context of topics that help to clarify
the significance of geographically-salient terms. Ta-
ble 2 shows a subset of the results of one randomly-
initialized run, including five hand-chosen topics (of
50 total) and five regions (of 13, as chosen automat-
ically during initialization). Terms were selected by
log-odds comparison. For the base topics we show
the ten strongest terms in each topic as compared to
the background word distribution. For the regional
variants, we show terms that are strong both region-
ally and topically: specifically, we select terms that
are in the top 100 compared to both the background
distribution and to the base topic. The names for the
topics and regions were chosen by the authors.
Nearly all of the terms in column 1 (“basketball”)
refer to sports teams, athletes, and place names—
encouragingly, terms tend to appear in the regions
where their referents reside. Column 2 contains sev-
eral proper nouns, mostly referring to popular mu-
sic figures (including PEARL from the band Pearl
Jam).11 Columns 3–5 are more conversational.
Spanish-language terms (papi, pues, nada, ese) tend
to appear in regions with large Spanish-speaking
populations—it is also telling that these terms ap-
pear in topics with emoticons and slang abbrevia-
tions, which may transcend linguistic barriers. Other
terms refer to people or subjects that may be espe-
cially relevant in certain regions: tacos appears in
the southern California region and cab in the New
York region; TUPAC refers to a rap musician from
Los Angeles, and WIZ refers to a rap musician from
Pittsburgh, not far from the center of the “Lake Erie”
region.
A large number of slang terms are found to have
strong regional biases, suggesting that slang may
depend on geography more than standard English
does. The terms af and hella display especially
strong regional affinities, appearing in the regional
variants of multiple topics (see Table 3 for defini-
tions). Northern and Southern California use variant
spellings koo and coo to express the same meaning.
11This analysis is from an earlier version of our dataset that
contained some Twitterbots, including one from a Boston-area
radio station. The bots were purged for the evaluation in Sec-
tion 6, though the numerical results are nearly identical.
term definition term definition
af as fuck (very) jk just kidding
coo cool jp just playing (kid-
dl download ding)
fasho for sure koo cool
gna going to lol laugh out loud
hella very nm nothing much
hr hour od overdone (very)
iam I am omw on my way
ima I’m going to smh shake my head
imm I’m suttin something
iono I don’t know wassup what’s up
lames lame (not cool) wyd what are you do-
people ing?
</bodyText>
<tableCaption confidence="0.987788">
Table 3: A glossary of non-standard terms from Ta-
ble 2. Definitions are obtained by manually inspecting
the context in which the terms appear, and by consulting
www.urbandictionary.com.
</tableCaption>
<bodyText confidence="0.999709444444445">
While research in perceptual dialectology does con-
firm the link of hella to Northern California (Bu-
choltz et al., 2007), we caution that our findings
are merely suggestive, and a more rigorous analysis
must be undertaken before making definitive state-
ments about the regional membership of individual
terms. We view the geographic topic model as an
exploratory tool that may be used to facilitate such
investigations.
Figure 3 shows the regional clustering on the
training set obtained by one run of the model. Each
point represents an author, and the ellipses represent
the bivariate Gaussians for each region. There are
nine compact regions for major metropolitan areas,
two slightly larger regions that encompass Florida
and the area around Lake Erie, and two large re-
gions that partition the country roughly into north
and south.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.9999943">
The relationship between language and geography
has been a topic of interest to linguists since the
nineteenth century (Johnstone, 2010). An early
work of particular relevance is Kurath’s Word Geog-
raphy of the Eastern United States (1949), in which
he conducted interviews and then mapped the oc-
currence of equivalent word pairs such as stoop and
porch. The essence of this approach—identifying
variable pairs and measuring their frequencies—
remains a dominant methodology in both dialec-
</bodyText>
<page confidence="0.964269">
1285
</page>
<bodyText confidence="0.999942215686275">
tology (Labov et al., 2006) and sociolinguis-
tics (Tagliamonte, 2006). Within this paradigm,
computational techniques are often applied to post
hoc analysis: logistic regression (Sankoff et al.,
2005) and mixed-effects models (Johnson, 2009) are
used to measure the contribution of individual vari-
ables, while hierarchical clustering and multidimen-
sional scaling enable aggregated inference across
multiple variables (Nerbonne, 2009). However, in
all such work it is assumed that the relevant linguis-
tic variables have already been identified—a time-
consuming process involving considerable linguistic
expertise. We view our work as complementary to
this tradition: we work directly from raw text, iden-
tifying both the relevant features and coherent lin-
guistic communities.
An active recent literature concerns geotagged in-
formation on the web, such as search queries (Back-
strom et al., 2008) and tagged images (Crandall et
al., 2009). This research identifies the geographic
distribution of individual queries and tags, but does
not attempt to induce any structural organization of
either the text or geographical space, which is the
focus of our research. More relevant is the work
of Mei et al. (2006), in which the distribution over
latent topics in blog posts is conditioned on the ge-
ographical location of the author. This is somewhat
similar to the supervised LDA model that we con-
sider, but their approach assumes that a partitioning
of geographical space into regions is already given.
Methodologically, our cascading topic model is
designed to capture multiple dimensions of variabil-
ity: topics and geography. Mei et al. (2007) include
sentiment as a second dimension in a topic model,
using a switching variable so that individual word
tokens may be selected from either the topic or the
sentiment. However, our hypothesis is that individ-
ual word tokens reflect both the topic and the ge-
ographical aspect. Sharing this intuition, Paul and
Girju (2010) build topic-aspect models for the cross
product of topics and aspects. They do not impose
any regularity across multiple aspects of the same
topic, so this approach may not scale when the num-
ber of aspects is large (they consider only two as-
pects). We address this issue using cascading distri-
butions; when the observed data for a given region-
topic pair is low, the model falls back to the base
topic. The use of cascading logistic normal distri-
butions in topic models follows earlier work on dy-
namic topic models (Blei and Lafferty, 2006b; Xing,
2005).
</bodyText>
<sectionHeader confidence="0.995518" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999982227272727">
This paper presents a model that jointly identifies
words with high regional affinity, geographically-
coherent linguistic regions, and the relationship be-
tween regional and topic variation. The key model-
ing assumption is that regions and topics interact to
shape observed lexical frequencies. We validate this
assumption on a prediction task in which our model
outperforms strong alternatives that do not distin-
guish regional and topical variation.
We see this work as a first step towards a unsuper-
vised methodology for modeling linguistic variation
using raw text. Indeed, in a study of morphosyn-
tactic variation, Szmrecsanyi (2010) finds that by
the most generous measure, geographical factors ac-
count for only 33% of the observed variation. Our
analysis might well improve if non-geographical
factors were considered, including age, race, gen-
der, income and whether a location is urban or ru-
ral. In some regions, estimates of many of these fac-
tors may be obtained by cross-referencing geogra-
phy with demographic data. We hope to explore this
possibility in future work.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.74046425">
We would like to thank Amr Ahmed, Jonathan Chang,
Shay Cohen, William Cohen, Ross Curtis, Miro Dud´ık,
Scott Kiesling, Seyoung Kim, and the anonymous re-
viewers. This research was enabled by Google’s sup-
port of the Worldly Knowledge project at CMU, AFOSR
FA9550010247, ONR N0001140910758, NSF CAREER
DBI-0546594, NSF IIS-0713379, and an Alfred P. Sloan
Fellowship.
</bodyText>
<sectionHeader confidence="0.993238" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999875125">
L. Backstrom, J. Kleinberg, R. Kumar, and J. Novak.
2008. Spatial variation in search engine queries. In
Proceedings of WWW.
C. M. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
D. M. Blei and M. I. Jordan. 2006. Variational infer-
ence for Dirichlet process mixtures. Bayesian Analy-
sis, 1:121–144.
</reference>
<page confidence="0.784537">
1286
</page>
<reference confidence="0.999860209302326">
D. M. Blei and J. Lafferty. 2006a. Correlated topic mod-
els. In NIPS.
D. M. Blei and J. Lafferty. 2006b. Dynamic topic mod-
els. In Proceedings of ICML.
D. M. Blei and J. D. McAuliffe. 2007. Supervised topic
models. In NIPS.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet allocation. JMLR, 3:993–1022.
M. Bucholtz, N. Bermudez, V. Fung, L. Edwards, and
R. Vargas. 2007. Hella Nor Cal or totally So Cal?
the perceptual dialectology of California. Journal of
English Linguistics, 35(4):325–352.
F. G. Cassidy and J. H. Hall. 1985. Dictionary of Amer-
ican Regional English, volume 1. Harvard University
Press.
J. Chambers. 2009. Sociolinguistic Theory: Linguistic
Variation and its Social Significance. Blackwell.
D. J Crandall, L. Backstrom, D. Huttenlocher, and
J. Kleinberg. 2009. Mapping the world’s photos. In
Proceedings of WWW, page 761770.
J. Friedman, T. Hastie, and R. Tibshirani. 2010. Regular-
ization paths for generalized linear models via coordi-
nate descent. Journal of Statistical Software, 33(1).
D. E. Johnson. 2009. Getting off the GoldVarb standard:
Introducing Rbrul for mixed-effects variable rule anal-
ysis. Language and Linguistics Compass, 3(1):359–
383.
B. Johnstone. 2010. Language and place. In R. Mesthrie
and W. Wolfram, editors, Cambridge Handbook of So-
ciolinguistics. Cambridge University Press.
M. Joshi, D. Das, K. Gimpel, and N. A. Smith. 2010.
Movie reviews and revenues: An experiment in text
regression. In Proceedings of NAACL-HLT.
H. Kurath. 1949. A Word Geography of the Eastern
United States. University of Michigan Press.
H. Kwak, C. Lee, H. Park, and S. Moon. 2010. What
is Twitter, a social network or a news media? In Pro-
ceedings of WWW.
W. Labov, S. Ash, and C. Boberg. 2006. The Atlas of
North American English: Phonetics, Phonology, and
Sound Change. Walter de Gruyter.
W. Labov. 1966. The Social Stratification of English in
New York City. Center for Applied Linguistics.
Q. Mei, C. Liu, H. Su, and C. X Zhai. 2006. A proba-
bilistic approach to spatiotemporal theme pattern min-
ing on weblogs. In Proceedings of WWW, page 542.
Q. Mei, X. Ling, M. Wondra, H. Su, and C. X. Zhai.
2007. Topic sentiment mixture: modeling facets and
opinions in weblogs. In Proceedings of WWW.
T. P. Minka. 2003. Estimating a Dirichlet distribution.
Technical report, Massachusetts Institute of Technol-
ogy.
J. Nerbonne. 2009. Data-driven dialectology. Language
and Linguistics Compass, 3(1).
B. O’Connor, M. Krieger, and D. Ahn. 2010. TweetMo-
tif: Exploratory search and topic summarization for
twitter. In Proceedings of ICWSM.
J. C. Paolillo. 2002. Analyzing Linguistic Variation: Sta-
tistical Models and Methods. CSLI Publications.
M. Paul and R. Girju. 2010. A two-dimensional topic-
aspect model for discovering multi-faceted topics. In
Proceedings of AAAI.
W. D. Penny. 2001. Variational Bayes for d-dimensional
Gaussian mixture models. Technical report, Univer-
sity College London.
D. Sankoff, S. A. Tagliamonte, and E. Smith. 2005.
Goldvarb X: A variable rule application for Macintosh
and Windows. Technical report, Department of Lin-
guistics, University of Toronto.
R. W. Sinnott. 1984. Virtues of the Haversine. Sky and
Telescope, 68(2).
B. Szmrecsanyi. 2010. Geography is overrated. In
S. Hansen, C. Schwarz, P. Stoeckle, and T. Streck, ed-
itors, Dialectological and Folk Dialectological Con-
cepts of Space. Walter de Gruyter.
S. A. Tagliamonte and D. Denis. 2008. Linguistic ruin?
LOL! Instant messanging and teen language. Ameri-
can Speech, 83.
S. A. Tagliamonte. 2006. Analysing Sociolinguistic Vari-
ation. Cambridge University Press.
M. J. Wainwright and M. I. Jordan. 2008. Graphical
Models, Exponential Families, and Variational Infer-
ence. Now Publishers.
E. P. Xing. 2005. On topic evolution. Technical Report
05-115, Center for Automated Learning and Discov-
ery, Carnegie Mellon University.
</reference>
<page confidence="0.992271">
1287
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.943771">
<title confidence="0.999965">A Latent Variable Model for Geographic Lexical Variation</title>
<author confidence="0.999994">Jacob Eisenstein Brendan O’Connor Noah A Smith Eric P Xing</author>
<affiliation confidence="0.9999505">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999497">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.996846722222222">The rapid growth of geotagged social media raises new computational possibilities for investigating geographic linguistic variation. In this paper, we present a multi-level generative model that reasons jointly about latent topics and geographical regions. High-level topics such as “sports” or “entertainment” are rendered differently in each geographic region, revealing topic-specific regional distinctions. Applied to a new dataset of geotagged microblogs, our model recovers coherent topics and their regional variants, while identifying geographic areas of linguistic consistency. The model also enables prediction of an author’s geographic location from raw text, outperforming both text regression and supervised topic models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Backstrom</author>
<author>J Kleinberg</author>
<author>R Kumar</author>
<author>J Novak</author>
</authors>
<title>Spatial variation in search engine queries.</title>
<date>2008</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="34600" citStr="Backstrom et al., 2008" startWordPosition="5609" endWordPosition="5613">ntribution of individual variables, while hierarchical clustering and multidimensional scaling enable aggregated inference across multiple variables (Nerbonne, 2009). However, in all such work it is assumed that the relevant linguistic variables have already been identified—a timeconsuming process involving considerable linguistic expertise. We view our work as complementary to this tradition: we work directly from raw text, identifying both the relevant features and coherent linguistic communities. An active recent literature concerns geotagged information on the web, such as search queries (Backstrom et al., 2008) and tagged images (Crandall et al., 2009). This research identifies the geographic distribution of individual queries and tags, but does not attempt to induce any structural organization of either the text or geographical space, which is the focus of our research. More relevant is the work of Mei et al. (2006), in which the distribution over latent topics in blog posts is conditioned on the geographical location of the author. This is somewhat similar to the supervised LDA model that we consider, but their approach assumes that a partitioning of geographical space into regions is already give</context>
</contexts>
<marker>Backstrom, Kleinberg, Kumar, Novak, 2008</marker>
<rawString>L. Backstrom, J. Kleinberg, R. Kumar, and J. Novak. 2008. Spatial variation in search engine queries. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="12132" citStr="Bishop, 2006" startWordPosition="1907" endWordPosition="1908">nerate text and locations: for each document d, – Draw topic proportions from a symmetric Dirichlet prior, θ ∼ Dir(a1). – Draw the region r from the multinomial distribution ϑ. – Draw the location y from the bivariate Gaussian, y ∼ N(νT, AT). – For each word token, ∗ Draw the topic indicator z ∼ θ. ∗ Draw the word token w ∼ βTz. 4 Inference We apply mean-field variational inference: a fullyfactored variational distribution Q is chosen to minimize the Kullback-Leibler divergence from the true distribution. Mean-field variational inference with conjugate priors is described in detail elsewhere (Bishop, 2006; Wainwright and Jordan, 2008); we restrict our focus to the issues that are unique to the geographic topic model. 1279 0 W N, Y Z r D K o2 α 0 A µ � V � µk log of base topic k’s distribution over word types σk variance parameter for regional variants of topic k ηjk region j’s variant of base topic µk θd author d’s topic proportions rd author d’s latent region yd author d’s observed GPS location νj region j’s spatial center Λj region j’s spatial precision zn token n’s topic assignment wn token n’s observed word type α global prior over author-topic proportions ϑ global prior over region classe</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>C. M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>M I Jordan</author>
</authors>
<title>Variational inference for Dirichlet process mixtures. Bayesian Analysis,</title>
<date>2006</date>
<pages>1--121</pages>
<contexts>
<context position="17704" citStr="Blei and Jordan, 2006" startWordPosition="2856" endWordPosition="2859">rgence. We then update the geographical parameters v and A, as well as the distribution over regions 0. Finally, for each document we iteratively update the variational parameters over 0, z, and r until convergence, obtaining expected counts that are used in the next iteration of updates for the topics and their regional variants. We iterate an outer loop over the entire set of updates until convergence. We initialize the model in a piecewise fashion. First we train a Dirichlet process mixture model on the locations y, using variational inference on the truncated stick-breaking approximation (Blei and Jordan, 2006). This automatically selects the number of regions J, and gives a distribution over each region indicator rd from geographical information alone. We then run standard latent Dirichlet allocation to obtain estimates of z for each token (ignoring the locations). From this initialization we can compute the first set of expected counts, which are used to obtain initial estimates of all parameters needed to begin variational inference in the full model. The prior a is the expected mean of each topic µ; for each term i, we set a(i) = log N(i) − log N, where N(i) is the total count of i in the corpus</context>
</contexts>
<marker>Blei, Jordan, 2006</marker>
<rawString>D. M. Blei and M. I. Jordan. 2006. Variational inference for Dirichlet process mixtures. Bayesian Analysis, 1:121–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J Lafferty</author>
</authors>
<title>Correlated topic models.</title>
<date>2006</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="10184" citStr="Blei and Lafferty, 2006" startWordPosition="1560" endWordPosition="1563">on of the author, which is not observed. As described above, r selects a corrupted version of each topic: the kth basic topic has mean µk, with uniform diagonal covariance U2k; for region j, we can draw the regionallycorrupted topic from the normal distribution, ηjk ∼ N(µk, U2kI). Because η is normally-distributed, it lies not in the simplex but in ][8W. We deterministically compute multinomial parameters β by exponentiating and normalizing: βjk = exp(ηjk)/ Ei exp(�(i) jk ). This normalization could introduce identifiability problems, as there are multiple settings for η that maximize P(w|η) (Blei and Lafferty, 2006a). However, this difficulty is obviated by the priors: given µ and U2, there is only a single η that maximizes P(w|η)P(η|µ, U2); similarly, only a single µ maximizes P(η|µ)P(µ|a, b2). The observed latitude and longitude, denoted y, are normally distributed and conditioned on the region, with mean νr and precision matrix Ar indexed by the region r. The region index r is itself drawn from a single shared multinomial ϑ. The model is shown as a plate diagram in Figure 1. Given a vocabulary size W, the generative story is as follows: • Generate base topics: for each topic k &lt; K – Draw the base top</context>
<context position="14710" citStr="Blei and Lafferty, 2006" startWordPosition="2353" endWordPosition="2356">i) jk |µ(i) k , σ2k)i, (1) 7Thanks to the naive mean field assumption, we can marginalize over z by first decomposing across all Nd words and then summing over q(z). with the first term representing the likelihood of the observed words (recall that β is computed deterministically from η) and the second term corresponding to the prior. The likelihood term requires the expectation hlog βi, but this is somewhat complicated by the normalizer EWi exp(η(i)), which sums over all terms in the vocabulary. As in previous work on logistic normal topic models, we use a Taylor approximation for this term (Blei and Lafferty, 2006a). The prior on η is normal, so the contribution from the second term of the objective (Equation 1) is 1 −2(σk) h(η(i) jk − µki))2i. We introduce the following notation for expected counts: N(i, j, k) indicates the expected count of term i in region j and topic k, and N(j, k) = Ei N(i, j, k). After some calculus, we can write the gradient ∂L/∂hη((i)) jk i as N(i, j, k) − N(j, k)hβ(i) jk i − hσk 2i (hηjk i − hµ(i) k i), (2) which has an intuitive interpretation. The first two terms represent the difference in expected counts for term i under the variational distributions q(z, r) and q(z, r,β):</context>
<context position="36209" citStr="Blei and Lafferty, 2006" startWordPosition="5879" endWordPosition="5882">the topic and the geographical aspect. Sharing this intuition, Paul and Girju (2010) build topic-aspect models for the cross product of topics and aspects. They do not impose any regularity across multiple aspects of the same topic, so this approach may not scale when the number of aspects is large (they consider only two aspects). We address this issue using cascading distributions; when the observed data for a given regiontopic pair is low, the model falls back to the base topic. The use of cascading logistic normal distributions in topic models follows earlier work on dynamic topic models (Blei and Lafferty, 2006b; Xing, 2005). 9 Conclusion This paper presents a model that jointly identifies words with high regional affinity, geographicallycoherent linguistic regions, and the relationship between regional and topic variation. The key modeling assumption is that regions and topics interact to shape observed lexical frequencies. We validate this assumption on a prediction task in which our model outperforms strong alternatives that do not distinguish regional and topical variation. We see this work as a first step towards a unsupervised methodology for modeling linguistic variation using raw text. Indee</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>D. M. Blei and J. Lafferty. 2006a. Correlated topic models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J Lafferty</author>
</authors>
<title>Dynamic topic models.</title>
<date>2006</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="10184" citStr="Blei and Lafferty, 2006" startWordPosition="1560" endWordPosition="1563">on of the author, which is not observed. As described above, r selects a corrupted version of each topic: the kth basic topic has mean µk, with uniform diagonal covariance U2k; for region j, we can draw the regionallycorrupted topic from the normal distribution, ηjk ∼ N(µk, U2kI). Because η is normally-distributed, it lies not in the simplex but in ][8W. We deterministically compute multinomial parameters β by exponentiating and normalizing: βjk = exp(ηjk)/ Ei exp(�(i) jk ). This normalization could introduce identifiability problems, as there are multiple settings for η that maximize P(w|η) (Blei and Lafferty, 2006a). However, this difficulty is obviated by the priors: given µ and U2, there is only a single η that maximizes P(w|η)P(η|µ, U2); similarly, only a single µ maximizes P(η|µ)P(µ|a, b2). The observed latitude and longitude, denoted y, are normally distributed and conditioned on the region, with mean νr and precision matrix Ar indexed by the region r. The region index r is itself drawn from a single shared multinomial ϑ. The model is shown as a plate diagram in Figure 1. Given a vocabulary size W, the generative story is as follows: • Generate base topics: for each topic k &lt; K – Draw the base top</context>
<context position="14710" citStr="Blei and Lafferty, 2006" startWordPosition="2353" endWordPosition="2356">i) jk |µ(i) k , σ2k)i, (1) 7Thanks to the naive mean field assumption, we can marginalize over z by first decomposing across all Nd words and then summing over q(z). with the first term representing the likelihood of the observed words (recall that β is computed deterministically from η) and the second term corresponding to the prior. The likelihood term requires the expectation hlog βi, but this is somewhat complicated by the normalizer EWi exp(η(i)), which sums over all terms in the vocabulary. As in previous work on logistic normal topic models, we use a Taylor approximation for this term (Blei and Lafferty, 2006a). The prior on η is normal, so the contribution from the second term of the objective (Equation 1) is 1 −2(σk) h(η(i) jk − µki))2i. We introduce the following notation for expected counts: N(i, j, k) indicates the expected count of term i in region j and topic k, and N(j, k) = Ei N(i, j, k). After some calculus, we can write the gradient ∂L/∂hη((i)) jk i as N(i, j, k) − N(j, k)hβ(i) jk i − hσk 2i (hηjk i − hµ(i) k i), (2) which has an intuitive interpretation. The first two terms represent the difference in expected counts for term i under the variational distributions q(z, r) and q(z, r,β):</context>
<context position="36209" citStr="Blei and Lafferty, 2006" startWordPosition="5879" endWordPosition="5882">the topic and the geographical aspect. Sharing this intuition, Paul and Girju (2010) build topic-aspect models for the cross product of topics and aspects. They do not impose any regularity across multiple aspects of the same topic, so this approach may not scale when the number of aspects is large (they consider only two aspects). We address this issue using cascading distributions; when the observed data for a given regiontopic pair is low, the model falls back to the base topic. The use of cascading logistic normal distributions in topic models follows earlier work on dynamic topic models (Blei and Lafferty, 2006b; Xing, 2005). 9 Conclusion This paper presents a model that jointly identifies words with high regional affinity, geographicallycoherent linguistic regions, and the relationship between regional and topic variation. The key modeling assumption is that regions and topics interact to shape observed lexical frequencies. We validate this assumption on a prediction task in which our model outperforms strong alternatives that do not distinguish regional and topical variation. We see this work as a first step towards a unsupervised methodology for modeling linguistic variation using raw text. Indee</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>D. M. Blei and J. Lafferty. 2006b. Dynamic topic models. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J D McAuliffe</author>
</authors>
<title>Supervised topic models.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="21229" citStr="Blei and McAuliffe, 2007" startWordPosition="3460" endWordPosition="3463">with only a single topic. This is equivalent to a Bayesian mixture of unigrams in which each author is assigned a single, regional unigram language model that generates all of his or her text. The development set is used to select the best of multiple random initializations. Supervised Latent Dirichlet Allocation In a more subtle version of the mixture-of-unigrams model, we model each author as an admixture of regions. Thus, the latent variable attached to each author is no longer an index, but rather a vector on the simplex. This model is equivalent to supervised latent Dirichlet allocation (Blei and McAuliffe, 2007): each topic is associated with equivariant Gaussian distributions over the latitude and longitude, and these topics must explain both the text and the observed geographical locations. For unlabeled authors, we estimate latitude and longitude by estimating the topic proportions and then applying the learned geographical distributions. This is a linear prediction f(2d; a) = (�zTd alat, 2Td alon) for an author’s topic proportions zd and topicgeography weights a E R2K. 6.1.2 Baseline Approaches Text Regression We perform linear regression to discriminatively learn the relationship between words a</context>
</contexts>
<marker>Blei, McAuliffe, 2007</marker>
<rawString>D. M. Blei and J. D. McAuliffe. 2007. Supervised topic models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<date>2003</date>
<booktitle>Latent Dirichlet allocation. JMLR,</booktitle>
<pages>3--993</pages>
<contexts>
<context position="8695" citStr="Blei et al., 2003" startWordPosition="1319" endWordPosition="1322">er factors (such as geography or time). For example, consider a base “food” topic 6The region could be observed by using a predefined geographical decomposition, e.g., political boundaries. However, such regions may not correspond well to linguistic variation. 1278 that emphasizes words like dinner and delicious; the corrupted “food-California” topic would place weight on these words, but might place extra emphasis on other words like sprouts. The path through the cascade is determined by a set of indexing variables, which may be hidden or observed. As in standard latent Dirichlet allocation (Blei et al., 2003), the base topics are selected by a per-token hidden variable z. In the geographical topic model, the next level corresponds to regions, which are selected by a per-author latent variable r. Formally, we draw each level of the cascade from a normal distribution centered on the previous level; the final multinomial distribution over words is obtained by exponentiating and normalizing. To ensure tractable inference, we assume that all covariance matrices are uniform diagonal, i.e., aI with a &gt; 0; this means we do not model interactions between words. 3.2 The Geographic Topic Model The applicatio</context>
<context position="13278" citStr="Blei et al., 2003" startWordPosition="2114" endWordPosition="2117">al prior over author-topic proportions ϑ global prior over region classes Figure 1: Plate diagram for the geographic topic model, with a table of all random variables. Priors (besides α) are omitted for clarity, and the document indices on z and w are implicit. We place variational distributions over all latent variables of interest: θ, z, r, ϑ, η, µ, σ2, ν, and Λ, updating each of these distributions in turn, until convergence. The variational distributions over θ and ϑ are Dirichlet, and have closed form updates: each can be set to the sum of the expected counts, plus a term from the prior (Blei et al., 2003). The variational distributions q(z) and q(r) are categorical, and can be set proportional to the expected joint likelihood—to set q(z) we marginalize over r, and vice versa.7 The updates for the multivariate Gaussian spatial parameters ν and Λ are described by Penny (2001). 4.1 Regional Word Distributions The variational region-topic distribution ηjk is normal, with uniform diagonal covariance for tractability. Throughout we will write hxi to indicate the expectation of x under the variational distribution Q. Thus, the vector mean of the distribution q(ηjk) is written hηjki, while the varianc</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent Dirichlet allocation. JMLR, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bucholtz</author>
<author>N Bermudez</author>
<author>V Fung</author>
<author>L Edwards</author>
<author>R Vargas</author>
</authors>
<title>Hella Nor Cal or totally So Cal? the perceptual dialectology of California.</title>
<date>2007</date>
<journal>Journal of English Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="32487" citStr="Bucholtz et al., 2007" startWordPosition="5287" endWordPosition="5291"> just playing (kiddl download ding) fasho for sure koo cool gna going to lol laugh out loud hella very nm nothing much hr hour od overdone (very) iam I am omw on my way ima I’m going to smh shake my head imm I’m suttin something iono I don’t know wassup what’s up lames lame (not cool) wyd what are you dopeople ing? Table 3: A glossary of non-standard terms from Table 2. Definitions are obtained by manually inspecting the context in which the terms appear, and by consulting www.urbandictionary.com. While research in perceptual dialectology does confirm the link of hella to Northern California (Bucholtz et al., 2007), we caution that our findings are merely suggestive, and a more rigorous analysis must be undertaken before making definitive statements about the regional membership of individual terms. We view the geographic topic model as an exploratory tool that may be used to facilitate such investigations. Figure 3 shows the regional clustering on the training set obtained by one run of the model. Each point represents an author, and the ellipses represent the bivariate Gaussians for each region. There are nine compact regions for major metropolitan areas, two slightly larger regions that encompass Flo</context>
</contexts>
<marker>Bucholtz, Bermudez, Fung, Edwards, Vargas, 2007</marker>
<rawString>M. Bucholtz, N. Bermudez, V. Fung, L. Edwards, and R. Vargas. 2007. Hella Nor Cal or totally So Cal? the perceptual dialectology of California. Journal of English Linguistics, 35(4):325–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F G Cassidy</author>
<author>J H Hall</author>
</authors>
<date>1985</date>
<journal>Dictionary of American Regional English,</journal>
<volume>1</volume>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="19656" citStr="Cassidy and Hall, 1985" startWordPosition="3181" endWordPosition="3184">beled authors based on their text alone.8 This task may also be practically relevant as a step toward applications for recommending local businesses or social connections. A randomly-chosen 60% of authors are used for training, 20% for development, and the remaining 20% for final evaluation. 6.1 Systems We compare several approaches for predicting author location; we divide these into latent variable generative models and discriminative approaches. 8Alternatively, one might evaluate the attributed regional memberships of the words themselves. While the Dictionary of American Regional English (Cassidy and Hall, 1985) attempts a comprehensive list of all regionally-affiliated terms, it is based on interviews conducted from 1965-1970, and the final volume (covering Si–Z) is not yet complete. (i)b2 EJj (η(i) jk ) + (σ2k)a(i) (µk ) = b2J + (σ2k) V(µk) = (b−2 + J(σ−2 k ))−1 , 1281 6.1.1 Latent Variable Models Geographic Topic Model This is the full version of our system, as described in this paper. To predict the unseen location yd, we iterate until convergence on the variational updates for the hidden topics zd, the topic proportions 9d, and the region rd. From rd, the location can be estimated as yd = �J arg</context>
</contexts>
<marker>Cassidy, Hall, 1985</marker>
<rawString>F. G. Cassidy and J. H. Hall. 1985. Dictionary of American Regional English, volume 1. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chambers</author>
</authors>
<title>Sociolinguistic Theory: Linguistic Variation and its Social Significance.</title>
<date>2009</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="1439" citStr="Chambers, 2009" startWordPosition="193" endWordPosition="194">ic consistency. The model also enables prediction of an author’s geographic location from raw text, outperforming both text regression and supervised topic models. 1 Introduction Sociolinguistics and dialectology study how language varies across social and regional contexts. Quantitative research in these fields generally proceeds by counting the frequency of a handful of previously-identified linguistic variables: pairs of phonological, lexical, or morphosyntactic features that are semantically equivalent, but whose frequency depends on social, geographical, or other factors (Paolillo, 2002; Chambers, 2009). It is left to the experimenter to determine which variables will be considered, and there is no obvious procedure for drawing inferences from the distribution of multiple variables. In this paper, we present a method for identifying geographically-aligned lexical variation directly from raw text. Our approach takes the form of a probabilistic graphical model capable of identifying both geographically-salient terms and coherent linguistic communities. One challenge in the study of lexical variation is that term frequencies are influenced by a variety of factors, such as the topic of discourse</context>
</contexts>
<marker>Chambers, 2009</marker>
<rawString>J. Chambers. 2009. Sociolinguistic Theory: Linguistic Variation and its Social Significance. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Crandall</author>
<author>L Backstrom</author>
<author>D Huttenlocher</author>
<author>J Kleinberg</author>
</authors>
<title>Mapping the world’s photos.</title>
<date>2009</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>761770</pages>
<contexts>
<context position="34642" citStr="Crandall et al., 2009" startWordPosition="5617" endWordPosition="5620">ierarchical clustering and multidimensional scaling enable aggregated inference across multiple variables (Nerbonne, 2009). However, in all such work it is assumed that the relevant linguistic variables have already been identified—a timeconsuming process involving considerable linguistic expertise. We view our work as complementary to this tradition: we work directly from raw text, identifying both the relevant features and coherent linguistic communities. An active recent literature concerns geotagged information on the web, such as search queries (Backstrom et al., 2008) and tagged images (Crandall et al., 2009). This research identifies the geographic distribution of individual queries and tags, but does not attempt to induce any structural organization of either the text or geographical space, which is the focus of our research. More relevant is the work of Mei et al. (2006), in which the distribution over latent topics in blog posts is conditioned on the geographical location of the author. This is somewhat similar to the supervised LDA model that we consider, but their approach assumes that a partitioning of geographical space into regions is already given. Methodologically, our cascading topic m</context>
</contexts>
<marker>Crandall, Backstrom, Huttenlocher, Kleinberg, 2009</marker>
<rawString>D. J Crandall, L. Backstrom, D. Huttenlocher, and J. Kleinberg. 2009. Mapping the world’s photos. In Proceedings of WWW, page 761770.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Friedman</author>
<author>T Hastie</author>
<author>R Tibshirani</author>
</authors>
<title>Regularization paths for generalized linear models via coordinate descent.</title>
<date>2010</date>
<journal>Journal of Statistical Software,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="22332" citStr="Friedman et al., 2010" startWordPosition="3635" endWordPosition="3638">line Approaches Text Regression We perform linear regression to discriminatively learn the relationship between words and locations. Using term frequency features xd for each author, we predict locations with wordgeography weights a E R2W: f(xd; a) = (xTd alat, Weights are trained to minimize the sum of squared Euclidean distances, subject to L1 regularization: (xTd alat _ �lat d )2 + (xd Talon _ �lon d )2 + Alat||alat||1 + Alon||alon||1 The minimization problem decouples into two separate latitude and longitude models, which we fit using the glmnet elastic net regularized regression package (Friedman et al., 2010), which obtained good results on other text-based prediction tasks (Joshi et al., 2010). Regularization parameters were tuned on the development set. The L1 penalty outperformed L2 and mixtures of L1 and L2. Note that for both word-level linear regression here, and the topic-level linear regression in SLDA, the choice of squared Euclidean distance dovetails with our use of spatial Gaussian likelihoods in the geographic topic models, since optimizing a is equivalent to maximum likelihood estimation under the assumption that locations are drawn from equivariant circular Gaussians centered around</context>
</contexts>
<marker>Friedman, Hastie, Tibshirani, 2010</marker>
<rawString>J. Friedman, T. Hastie, and R. Tibshirani. 2010. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Johnson</author>
</authors>
<title>Getting off the GoldVarb standard: Introducing Rbrul for mixed-effects variable rule analysis.</title>
<date>2009</date>
<journal>Language and Linguistics Compass,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>383</pages>
<contexts>
<context position="33950" citStr="Johnson, 2009" startWordPosition="5516" endWordPosition="5517">(Johnstone, 2010). An early work of particular relevance is Kurath’s Word Geography of the Eastern United States (1949), in which he conducted interviews and then mapped the occurrence of equivalent word pairs such as stoop and porch. The essence of this approach—identifying variable pairs and measuring their frequencies— remains a dominant methodology in both dialec1285 tology (Labov et al., 2006) and sociolinguistics (Tagliamonte, 2006). Within this paradigm, computational techniques are often applied to post hoc analysis: logistic regression (Sankoff et al., 2005) and mixed-effects models (Johnson, 2009) are used to measure the contribution of individual variables, while hierarchical clustering and multidimensional scaling enable aggregated inference across multiple variables (Nerbonne, 2009). However, in all such work it is assumed that the relevant linguistic variables have already been identified—a timeconsuming process involving considerable linguistic expertise. We view our work as complementary to this tradition: we work directly from raw text, identifying both the relevant features and coherent linguistic communities. An active recent literature concerns geotagged information on the we</context>
</contexts>
<marker>Johnson, 2009</marker>
<rawString>D. E. Johnson. 2009. Getting off the GoldVarb standard: Introducing Rbrul for mixed-effects variable rule analysis. Language and Linguistics Compass, 3(1):359– 383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Johnstone</author>
</authors>
<title>Language and place.</title>
<date>2010</date>
<booktitle>Cambridge Handbook of Sociolinguistics.</booktitle>
<editor>In R. Mesthrie and W. Wolfram, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="33353" citStr="Johnstone, 2010" startWordPosition="5427" endWordPosition="5428">t may be used to facilitate such investigations. Figure 3 shows the regional clustering on the training set obtained by one run of the model. Each point represents an author, and the ellipses represent the bivariate Gaussians for each region. There are nine compact regions for major metropolitan areas, two slightly larger regions that encompass Florida and the area around Lake Erie, and two large regions that partition the country roughly into north and south. 8 Related Work The relationship between language and geography has been a topic of interest to linguists since the nineteenth century (Johnstone, 2010). An early work of particular relevance is Kurath’s Word Geography of the Eastern United States (1949), in which he conducted interviews and then mapped the occurrence of equivalent word pairs such as stoop and porch. The essence of this approach—identifying variable pairs and measuring their frequencies— remains a dominant methodology in both dialec1285 tology (Labov et al., 2006) and sociolinguistics (Tagliamonte, 2006). Within this paradigm, computational techniques are often applied to post hoc analysis: logistic regression (Sankoff et al., 2005) and mixed-effects models (Johnson, 2009) ar</context>
</contexts>
<marker>Johnstone, 2010</marker>
<rawString>B. Johnstone. 2010. Language and place. In R. Mesthrie and W. Wolfram, editors, Cambridge Handbook of Sociolinguistics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Joshi</author>
<author>D Das</author>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Movie reviews and revenues: An experiment in text regression.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="22419" citStr="Joshi et al., 2010" startWordPosition="3649" endWordPosition="3652"> relationship between words and locations. Using term frequency features xd for each author, we predict locations with wordgeography weights a E R2W: f(xd; a) = (xTd alat, Weights are trained to minimize the sum of squared Euclidean distances, subject to L1 regularization: (xTd alat _ �lat d )2 + (xd Talon _ �lon d )2 + Alat||alat||1 + Alon||alon||1 The minimization problem decouples into two separate latitude and longitude models, which we fit using the glmnet elastic net regularized regression package (Friedman et al., 2010), which obtained good results on other text-based prediction tasks (Joshi et al., 2010). Regularization parameters were tuned on the development set. The L1 penalty outperformed L2 and mixtures of L1 and L2. Note that for both word-level linear regression here, and the topic-level linear regression in SLDA, the choice of squared Euclidean distance dovetails with our use of spatial Gaussian likelihoods in the geographic topic models, since optimizing a is equivalent to maximum likelihood estimation under the assumption that locations are drawn from equivariant circular Gaussians centered around each f(xd; a) linear prediction. We experimented with decorrelating the location dimen</context>
</contexts>
<marker>Joshi, Das, Gimpel, Smith, 2010</marker>
<rawString>M. Joshi, D. Das, K. Gimpel, and N. A. Smith. 2010. Movie reviews and revenues: An experiment in text regression. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kurath</author>
</authors>
<title>A Word Geography of the Eastern United States.</title>
<date>1949</date>
<publisher>University of Michigan Press.</publisher>
<marker>Kurath, 1949</marker>
<rawString>H. Kurath. 1949. A Word Geography of the Eastern United States. University of Michigan Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kwak</author>
<author>C Lee</author>
<author>H Park</author>
<author>S Moon</author>
</authors>
<title>What is Twitter, a social network or a news media?</title>
<date>2010</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="5011" citStr="Kwak et al. (2010)" startWordPosition="731" endWordPosition="734">s research is gathered from the microblog website Twitter, via its official API. We use an archive of messages collected over the first week of March 2010 from the “Gardenhose” sample stream,3 which then consisted of 15% of all public messages, totaling millions per day. We aggressively filter this stream, using only messages that are tagged with physical (latitude, longitude) coordinate pairs from a mobile client, and whose authors wrote at least 20 messages over this period. We also filter to include only authors who follow fewer than 1,000 other people, and have fewer than 1,000 followers. Kwak et al. (2010) find dramatic shifts in behavior among users with social graph connectivity outside of that range; such users may be marketers, celebrities with professional publicists, news media sources, etc. We also remove messages containing URLs to eliminate bots posting information such as advertising or weather conditions. For interpretability, we restrict our attention to authors inside a bounding box around the contiguous U.S. states, yielding a final sample of about 9,500 users and 380,000 messages, totaling 4.7 million word tokens. We have made this dataset available online.4 Informal text from mo</context>
</contexts>
<marker>Kwak, Lee, Park, Moon, 2010</marker>
<rawString>H. Kwak, C. Lee, H. Park, and S. Moon. 2010. What is Twitter, a social network or a news media? In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Labov</author>
<author>S Ash</author>
<author>C Boberg</author>
</authors>
<date>2006</date>
<journal>The Atlas of North American English: Phonetics, Phonology, and Sound Change. Walter de Gruyter.</journal>
<contexts>
<context position="33737" citStr="Labov et al., 2006" startWordPosition="5486" endWordPosition="5489">rie, and two large regions that partition the country roughly into north and south. 8 Related Work The relationship between language and geography has been a topic of interest to linguists since the nineteenth century (Johnstone, 2010). An early work of particular relevance is Kurath’s Word Geography of the Eastern United States (1949), in which he conducted interviews and then mapped the occurrence of equivalent word pairs such as stoop and porch. The essence of this approach—identifying variable pairs and measuring their frequencies— remains a dominant methodology in both dialec1285 tology (Labov et al., 2006) and sociolinguistics (Tagliamonte, 2006). Within this paradigm, computational techniques are often applied to post hoc analysis: logistic regression (Sankoff et al., 2005) and mixed-effects models (Johnson, 2009) are used to measure the contribution of individual variables, while hierarchical clustering and multidimensional scaling enable aggregated inference across multiple variables (Nerbonne, 2009). However, in all such work it is assumed that the relevant linguistic variables have already been identified—a timeconsuming process involving considerable linguistic expertise. We view our work</context>
</contexts>
<marker>Labov, Ash, Boberg, 2006</marker>
<rawString>W. Labov, S. Ash, and C. Boberg. 2006. The Atlas of North American English: Phonetics, Phonology, and Sound Change. Walter de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Labov</author>
</authors>
<title>The Social Stratification of English in</title>
<date>1966</date>
<institution>City. Center for Applied Linguistics.</institution>
<location>New York</location>
<contexts>
<context position="3520" citStr="Labov, 1966" startWordPosition="511" endWordPosition="512">ocation based on text alone. This research is only possible due to the rapid growth of social media. Our dataset is derived from the microblogging website Twitter,1 which permits users to post short messages to the public. Many users of Twitter also supply exact geographical coordinates from GPS-enabled devices (e.g., mobile phones),2 yielding geotagged text data. Text in computer-mediated communication is often more vernacular (Tagliamonte and Denis, 2008), and as such it is more likely to reveal the influence of geographic factors than text written in a more formal genre, such as news text (Labov, 1966). We evaluate our approach both qualitatively and quantitatively. We investigate the topics and regions 1http://www.twitter.com 2User profiles also contain self-reported location names, but we do not use that information in this work. 1277 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1277–1287, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics that the model obtains, showing both common-sense results (place names and sports teams are grouped appropriately), as well as less-obvious insights about slang. </context>
</contexts>
<marker>Labov, 1966</marker>
<rawString>W. Labov. 1966. The Social Stratification of English in New York City. Center for Applied Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Mei</author>
<author>C Liu</author>
<author>H Su</author>
<author>C X Zhai</author>
</authors>
<title>A probabilistic approach to spatiotemporal theme pattern mining on weblogs.</title>
<date>2006</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>542</pages>
<contexts>
<context position="34912" citStr="Mei et al. (2006)" startWordPosition="5661" endWordPosition="5664">ble linguistic expertise. We view our work as complementary to this tradition: we work directly from raw text, identifying both the relevant features and coherent linguistic communities. An active recent literature concerns geotagged information on the web, such as search queries (Backstrom et al., 2008) and tagged images (Crandall et al., 2009). This research identifies the geographic distribution of individual queries and tags, but does not attempt to induce any structural organization of either the text or geographical space, which is the focus of our research. More relevant is the work of Mei et al. (2006), in which the distribution over latent topics in blog posts is conditioned on the geographical location of the author. This is somewhat similar to the supervised LDA model that we consider, but their approach assumes that a partitioning of geographical space into regions is already given. Methodologically, our cascading topic model is designed to capture multiple dimensions of variability: topics and geography. Mei et al. (2007) include sentiment as a second dimension in a topic model, using a switching variable so that individual word tokens may be selected from either the topic or the senti</context>
</contexts>
<marker>Mei, Liu, Su, Zhai, 2006</marker>
<rawString>Q. Mei, C. Liu, H. Su, and C. X Zhai. 2006. A probabilistic approach to spatiotemporal theme pattern mining on weblogs. In Proceedings of WWW, page 542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Mei</author>
<author>X Ling</author>
<author>M Wondra</author>
<author>H Su</author>
<author>C X Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="35345" citStr="Mei et al. (2007)" startWordPosition="5730" endWordPosition="5733"> but does not attempt to induce any structural organization of either the text or geographical space, which is the focus of our research. More relevant is the work of Mei et al. (2006), in which the distribution over latent topics in blog posts is conditioned on the geographical location of the author. This is somewhat similar to the supervised LDA model that we consider, but their approach assumes that a partitioning of geographical space into regions is already given. Methodologically, our cascading topic model is designed to capture multiple dimensions of variability: topics and geography. Mei et al. (2007) include sentiment as a second dimension in a topic model, using a switching variable so that individual word tokens may be selected from either the topic or the sentiment. However, our hypothesis is that individual word tokens reflect both the topic and the geographical aspect. Sharing this intuition, Paul and Girju (2010) build topic-aspect models for the cross product of topics and aspects. They do not impose any regularity across multiple aspects of the same topic, so this approach may not scale when the number of aspects is large (they consider only two aspects). We address this issue usi</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Q. Mei, X. Ling, M. Wondra, H. Su, and C. X. Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T P Minka</author>
</authors>
<title>Estimating a Dirichlet distribution.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="18604" citStr="Minka, 2003" startWordPosition="3021" endWordPosition="3022"> can compute the first set of expected counts, which are used to obtain initial estimates of all parameters needed to begin variational inference in the full model. The prior a is the expected mean of each topic µ; for each term i, we set a(i) = log N(i) − log N, where N(i) is the total count of i in the corpus and N = Ei N(i). The variance prior b2 is set to 1, and the prior on σ2 is the Gamma distribution 9(2, 200), encouraging minimal deviation from the base topics. The symmetric Dirichlet prior on 0 is set to 12, and the symmetric Dirichlet parameter on ϑ is updated from weak hyperpriors (Minka, 2003). Finally, the geographical model takes priors that are linked to the data: for each region, the mean is very weakly encouraged to be near the overall mean, and the covariance prior is set by the average covariance of clusters obtained by running K-means. 6 Evaluation For a quantitative evaluation of the estimated relationship between text and geography, we assess our model’s ability to predict the geographic location of unlabeled authors based on their text alone.8 This task may also be practically relevant as a step toward applications for recommending local businesses or social connections.</context>
</contexts>
<marker>Minka, 2003</marker>
<rawString>T. P. Minka. 2003. Estimating a Dirichlet distribution. Technical report, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nerbonne</author>
</authors>
<title>Data-driven dialectology.</title>
<date>2009</date>
<journal>Language and Linguistics Compass,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="34142" citStr="Nerbonne, 2009" startWordPosition="5541" endWordPosition="5542">ivalent word pairs such as stoop and porch. The essence of this approach—identifying variable pairs and measuring their frequencies— remains a dominant methodology in both dialec1285 tology (Labov et al., 2006) and sociolinguistics (Tagliamonte, 2006). Within this paradigm, computational techniques are often applied to post hoc analysis: logistic regression (Sankoff et al., 2005) and mixed-effects models (Johnson, 2009) are used to measure the contribution of individual variables, while hierarchical clustering and multidimensional scaling enable aggregated inference across multiple variables (Nerbonne, 2009). However, in all such work it is assumed that the relevant linguistic variables have already been identified—a timeconsuming process involving considerable linguistic expertise. We view our work as complementary to this tradition: we work directly from raw text, identifying both the relevant features and coherent linguistic communities. An active recent literature concerns geotagged information on the web, such as search queries (Backstrom et al., 2008) and tagged images (Crandall et al., 2009). This research identifies the geographic distribution of individual queries and tags, but does not </context>
</contexts>
<marker>Nerbonne, 2009</marker>
<rawString>J. Nerbonne. 2009. Data-driven dialectology. Language and Linguistics Compass, 3(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B O’Connor</author>
<author>M Krieger</author>
<author>D Ahn</author>
</authors>
<title>TweetMotif: Exploratory search and topic summarization for twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<marker>O’Connor, Krieger, Ahn, 2010</marker>
<rawString>B. O’Connor, M. Krieger, and D. Ahn. 2010. TweetMotif: Exploratory search and topic summarization for twitter. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Paolillo</author>
</authors>
<title>Analyzing Linguistic Variation: Statistical Models and Methods.</title>
<date>2002</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="1422" citStr="Paolillo, 2002" startWordPosition="191" endWordPosition="192">reas of linguistic consistency. The model also enables prediction of an author’s geographic location from raw text, outperforming both text regression and supervised topic models. 1 Introduction Sociolinguistics and dialectology study how language varies across social and regional contexts. Quantitative research in these fields generally proceeds by counting the frequency of a handful of previously-identified linguistic variables: pairs of phonological, lexical, or morphosyntactic features that are semantically equivalent, but whose frequency depends on social, geographical, or other factors (Paolillo, 2002; Chambers, 2009). It is left to the experimenter to determine which variables will be considered, and there is no obvious procedure for drawing inferences from the distribution of multiple variables. In this paper, we present a method for identifying geographically-aligned lexical variation directly from raw text. Our approach takes the form of a probabilistic graphical model capable of identifying both geographically-salient terms and coherent linguistic communities. One challenge in the study of lexical variation is that term frequencies are influenced by a variety of factors, such as the t</context>
</contexts>
<marker>Paolillo, 2002</marker>
<rawString>J. C. Paolillo. 2002. Analyzing Linguistic Variation: Statistical Models and Methods. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
<author>R Girju</author>
</authors>
<title>A two-dimensional topicaspect model for discovering multi-faceted topics.</title>
<date>2010</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="35670" citStr="Paul and Girju (2010)" startWordPosition="5784" endWordPosition="5787">similar to the supervised LDA model that we consider, but their approach assumes that a partitioning of geographical space into regions is already given. Methodologically, our cascading topic model is designed to capture multiple dimensions of variability: topics and geography. Mei et al. (2007) include sentiment as a second dimension in a topic model, using a switching variable so that individual word tokens may be selected from either the topic or the sentiment. However, our hypothesis is that individual word tokens reflect both the topic and the geographical aspect. Sharing this intuition, Paul and Girju (2010) build topic-aspect models for the cross product of topics and aspects. They do not impose any regularity across multiple aspects of the same topic, so this approach may not scale when the number of aspects is large (they consider only two aspects). We address this issue using cascading distributions; when the observed data for a given regiontopic pair is low, the model falls back to the base topic. The use of cascading logistic normal distributions in topic models follows earlier work on dynamic topic models (Blei and Lafferty, 2006b; Xing, 2005). 9 Conclusion This paper presents a model that</context>
</contexts>
<marker>Paul, Girju, 2010</marker>
<rawString>M. Paul and R. Girju. 2010. A two-dimensional topicaspect model for discovering multi-faceted topics. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W D Penny</author>
</authors>
<title>Variational Bayes for d-dimensional Gaussian mixture models.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>University College London.</institution>
<contexts>
<context position="13552" citStr="Penny (2001)" startWordPosition="2160" endWordPosition="2161">al distributions over all latent variables of interest: θ, z, r, ϑ, η, µ, σ2, ν, and Λ, updating each of these distributions in turn, until convergence. The variational distributions over θ and ϑ are Dirichlet, and have closed form updates: each can be set to the sum of the expected counts, plus a term from the prior (Blei et al., 2003). The variational distributions q(z) and q(r) are categorical, and can be set proportional to the expected joint likelihood—to set q(z) we marginalize over r, and vice versa.7 The updates for the multivariate Gaussian spatial parameters ν and Λ are described by Penny (2001). 4.1 Regional Word Distributions The variational region-topic distribution ηjk is normal, with uniform diagonal covariance for tractability. Throughout we will write hxi to indicate the expectation of x under the variational distribution Q. Thus, the vector mean of the distribution q(ηjk) is written hηjki, while the variance (uniform across i) of q(η) is written V(ηjk). To update the mean parameter hηjki, we maximize the contribution to the variational bound L from the relevant terms: L[(η��� �� )] = hlog p(w|β, z, r)i+hlog p(η(i) jk |µ(i) k , σ2k)i, (1) 7Thanks to the naive mean field assump</context>
</contexts>
<marker>Penny, 2001</marker>
<rawString>W. D. Penny. 2001. Variational Bayes for d-dimensional Gaussian mixture models. Technical report, University College London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sankoff</author>
<author>S A Tagliamonte</author>
<author>E Smith</author>
</authors>
<title>Goldvarb X: A variable rule application for Macintosh and Windows.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Department of Linguistics, University of Toronto.</institution>
<contexts>
<context position="33909" citStr="Sankoff et al., 2005" startWordPosition="5509" endWordPosition="5512">erest to linguists since the nineteenth century (Johnstone, 2010). An early work of particular relevance is Kurath’s Word Geography of the Eastern United States (1949), in which he conducted interviews and then mapped the occurrence of equivalent word pairs such as stoop and porch. The essence of this approach—identifying variable pairs and measuring their frequencies— remains a dominant methodology in both dialec1285 tology (Labov et al., 2006) and sociolinguistics (Tagliamonte, 2006). Within this paradigm, computational techniques are often applied to post hoc analysis: logistic regression (Sankoff et al., 2005) and mixed-effects models (Johnson, 2009) are used to measure the contribution of individual variables, while hierarchical clustering and multidimensional scaling enable aggregated inference across multiple variables (Nerbonne, 2009). However, in all such work it is assumed that the relevant linguistic variables have already been identified—a timeconsuming process involving considerable linguistic expertise. We view our work as complementary to this tradition: we work directly from raw text, identifying both the relevant features and coherent linguistic communities. An active recent literature</context>
</contexts>
<marker>Sankoff, Tagliamonte, Smith, 2005</marker>
<rawString>D. Sankoff, S. A. Tagliamonte, and E. Smith. 2005. Goldvarb X: A variable rule application for Macintosh and Windows. Technical report, Department of Linguistics, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Sinnott</author>
</authors>
<date>1984</date>
<booktitle>Virtues of the Haversine. Sky and Telescope,</booktitle>
<volume>68</volume>
<issue>2</issue>
<contexts>
<context position="26183" citStr="Sinnott, 1984" startWordPosition="4247" endWordPosition="4248">ic topic model and the mixture of unigrams use identical code and parametrization – the only difference is that the geographic topic model accounts for topical variation, while the mixture of unigrams sets K = 1. These results validate our basic premise that it is important to model the interaction between topical and geographical variation. Text regression and supervised LDA perform especially poorly on the classification metric. Both methods make predictions that are averaged across Earth’s surface requires computing or approximating the great circle distance – we use the Haversine formula (Sinnott, 1984). For the continental U.S., the relationship between degrees and kilometers is nearly linear, but extending the model to a continental scale would require a more sophisticated approach. 10http://www.census.gov/geo/www/us_regdiv.pdf Number of topics Figure 2: The effect of varying the number of topics on the median regression error (lower is better). each word in the document: in text regression, each word is directly multiplied by a feature weight; in supervised LDA the word is associated with a latent topic first, and then multiplied by a weight. For these models, all words exert an influence</context>
</contexts>
<marker>Sinnott, 1984</marker>
<rawString>R. W. Sinnott. 1984. Virtues of the Haversine. Sky and Telescope, 68(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Szmrecsanyi</author>
</authors>
<title>Geography is overrated.</title>
<date>2010</date>
<booktitle>Dialectological and Folk Dialectological Concepts of Space. Walter de Gruyter.</booktitle>
<editor>In S. Hansen, C. Schwarz, P. Stoeckle, and T. Streck, editors,</editor>
<contexts>
<context position="36871" citStr="Szmrecsanyi (2010)" startWordPosition="5981" endWordPosition="5982">ents a model that jointly identifies words with high regional affinity, geographicallycoherent linguistic regions, and the relationship between regional and topic variation. The key modeling assumption is that regions and topics interact to shape observed lexical frequencies. We validate this assumption on a prediction task in which our model outperforms strong alternatives that do not distinguish regional and topical variation. We see this work as a first step towards a unsupervised methodology for modeling linguistic variation using raw text. Indeed, in a study of morphosyntactic variation, Szmrecsanyi (2010) finds that by the most generous measure, geographical factors account for only 33% of the observed variation. Our analysis might well improve if non-geographical factors were considered, including age, race, gender, income and whether a location is urban or rural. In some regions, estimates of many of these factors may be obtained by cross-referencing geography with demographic data. We hope to explore this possibility in future work. Acknowledgments We would like to thank Amr Ahmed, Jonathan Chang, Shay Cohen, William Cohen, Ross Curtis, Miro Dud´ık, Scott Kiesling, Seyoung Kim, and the anon</context>
</contexts>
<marker>Szmrecsanyi, 2010</marker>
<rawString>B. Szmrecsanyi. 2010. Geography is overrated. In S. Hansen, C. Schwarz, P. Stoeckle, and T. Streck, editors, Dialectological and Folk Dialectological Concepts of Space. Walter de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Tagliamonte</author>
<author>D Denis</author>
</authors>
<title>Linguistic ruin? LOL! Instant messanging and teen language.</title>
<date>2008</date>
<journal>American Speech,</journal>
<volume>83</volume>
<contexts>
<context position="3369" citStr="Tagliamonte and Denis, 2008" startWordPosition="480" endWordPosition="483">luding: (i) analyzing lexical variation by both topic and geography; (ii) segmenting geographical space into coherent linguistic communities; (iii) predicting author location based on text alone. This research is only possible due to the rapid growth of social media. Our dataset is derived from the microblogging website Twitter,1 which permits users to post short messages to the public. Many users of Twitter also supply exact geographical coordinates from GPS-enabled devices (e.g., mobile phones),2 yielding geotagged text data. Text in computer-mediated communication is often more vernacular (Tagliamonte and Denis, 2008), and as such it is more likely to reveal the influence of geographic factors than text written in a more formal genre, such as news text (Labov, 1966). We evaluate our approach both qualitatively and quantitatively. We investigate the topics and regions 1http://www.twitter.com 2User profiles also contain self-reported location names, but we do not use that information in this work. 1277 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1277–1287, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics that the mo</context>
</contexts>
<marker>Tagliamonte, Denis, 2008</marker>
<rawString>S. A. Tagliamonte and D. Denis. 2008. Linguistic ruin? LOL! Instant messanging and teen language. American Speech, 83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Tagliamonte</author>
</authors>
<title>Analysing Sociolinguistic Variation.</title>
<date>2006</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="33778" citStr="Tagliamonte, 2006" startWordPosition="5493" endWordPosition="5494">the country roughly into north and south. 8 Related Work The relationship between language and geography has been a topic of interest to linguists since the nineteenth century (Johnstone, 2010). An early work of particular relevance is Kurath’s Word Geography of the Eastern United States (1949), in which he conducted interviews and then mapped the occurrence of equivalent word pairs such as stoop and porch. The essence of this approach—identifying variable pairs and measuring their frequencies— remains a dominant methodology in both dialec1285 tology (Labov et al., 2006) and sociolinguistics (Tagliamonte, 2006). Within this paradigm, computational techniques are often applied to post hoc analysis: logistic regression (Sankoff et al., 2005) and mixed-effects models (Johnson, 2009) are used to measure the contribution of individual variables, while hierarchical clustering and multidimensional scaling enable aggregated inference across multiple variables (Nerbonne, 2009). However, in all such work it is assumed that the relevant linguistic variables have already been identified—a timeconsuming process involving considerable linguistic expertise. We view our work as complementary to this tradition: we w</context>
</contexts>
<marker>Tagliamonte, 2006</marker>
<rawString>S. A. Tagliamonte. 2006. Analysing Sociolinguistic Variation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Wainwright</author>
<author>M I Jordan</author>
</authors>
<title>Graphical Models, Exponential Families, and Variational Inference.</title>
<date>2008</date>
<publisher>Now Publishers.</publisher>
<contexts>
<context position="12162" citStr="Wainwright and Jordan, 2008" startWordPosition="1909" endWordPosition="1912">d locations: for each document d, – Draw topic proportions from a symmetric Dirichlet prior, θ ∼ Dir(a1). – Draw the region r from the multinomial distribution ϑ. – Draw the location y from the bivariate Gaussian, y ∼ N(νT, AT). – For each word token, ∗ Draw the topic indicator z ∼ θ. ∗ Draw the word token w ∼ βTz. 4 Inference We apply mean-field variational inference: a fullyfactored variational distribution Q is chosen to minimize the Kullback-Leibler divergence from the true distribution. Mean-field variational inference with conjugate priors is described in detail elsewhere (Bishop, 2006; Wainwright and Jordan, 2008); we restrict our focus to the issues that are unique to the geographic topic model. 1279 0 W N, Y Z r D K o2 α 0 A µ � V � µk log of base topic k’s distribution over word types σk variance parameter for regional variants of topic k ηjk region j’s variant of base topic µk θd author d’s topic proportions rd author d’s latent region yd author d’s observed GPS location νj region j’s spatial center Λj region j’s spatial precision zn token n’s topic assignment wn token n’s observed word type α global prior over author-topic proportions ϑ global prior over region classes Figure 1: Plate diagram for </context>
</contexts>
<marker>Wainwright, Jordan, 2008</marker>
<rawString>M. J. Wainwright and M. I. Jordan. 2008. Graphical Models, Exponential Families, and Variational Inference. Now Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E P Xing</author>
</authors>
<title>On topic evolution.</title>
<date>2005</date>
<tech>Technical Report 05-115,</tech>
<institution>Center for Automated Learning and Discovery, Carnegie Mellon University.</institution>
<contexts>
<context position="36223" citStr="Xing, 2005" startWordPosition="5883" endWordPosition="5884">ical aspect. Sharing this intuition, Paul and Girju (2010) build topic-aspect models for the cross product of topics and aspects. They do not impose any regularity across multiple aspects of the same topic, so this approach may not scale when the number of aspects is large (they consider only two aspects). We address this issue using cascading distributions; when the observed data for a given regiontopic pair is low, the model falls back to the base topic. The use of cascading logistic normal distributions in topic models follows earlier work on dynamic topic models (Blei and Lafferty, 2006b; Xing, 2005). 9 Conclusion This paper presents a model that jointly identifies words with high regional affinity, geographicallycoherent linguistic regions, and the relationship between regional and topic variation. The key modeling assumption is that regions and topics interact to shape observed lexical frequencies. We validate this assumption on a prediction task in which our model outperforms strong alternatives that do not distinguish regional and topical variation. We see this work as a first step towards a unsupervised methodology for modeling linguistic variation using raw text. Indeed, in a study </context>
</contexts>
<marker>Xing, 2005</marker>
<rawString>E. P. Xing. 2005. On topic evolution. Technical Report 05-115, Center for Automated Learning and Discovery, Carnegie Mellon University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>