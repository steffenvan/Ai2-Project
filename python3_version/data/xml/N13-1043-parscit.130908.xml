<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.979108">
Learning to Relate Literal and Sentimental Descriptions of Visual Properties
</title>
<author confidence="0.995541">
Mark Yatskar
</author>
<affiliation confidence="0.996107">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.771532">
Seattle, WA
</address>
<email confidence="0.995746">
my89@cs.washington.edu
</email>
<author confidence="0.904883">
Asli Celikyilmaz
</author>
<affiliation confidence="0.831419">
Conversational Understanding Sciences
Microsoft
</affiliation>
<address confidence="0.70673">
Mountain View, CA
</address>
<email confidence="0.962004">
asli@ieee.org
</email>
<author confidence="0.933525">
Svitlana Volkova
</author>
<affiliation confidence="0.88547">
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.841063">
Baltimore, MD
</address>
<email confidence="0.99797">
svitlana@jhu.edu
</email>
<author confidence="0.996911">
Luke Zettlemoyer
</author>
<affiliation confidence="0.9958255">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.893697">
Seattle, WA
</address>
<email confidence="0.9896">
lsz@cs.washington.edu
</email>
<author confidence="0.670983">
Bill Dolan
</author>
<affiliation confidence="0.5186115">
NLP Group
Microsoft Research
</affiliation>
<address confidence="0.951312">
Redmond, WA
</address>
<email confidence="0.990037">
billdol@microsoft.edu
</email>
<sectionHeader confidence="0.998302" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998675">
Language can describe our visual world at
many levels, including not only what is lit-
erally there but also the sentiment that it in-
vokes. In this paper, we study visual language,
both literal and sentimental, that describes the
overall appearance and style of virtual char-
acters. Sentimental properties, including la-
bels such as “youthful” or “country western,”
must be inferred from descriptions of the more
literal properties, such as facial features and
clothing selection. We present a new dataset,
collected to describe Xbox avatars, as well as
models for learning the relationships between
these avatars and their literal and sentimen-
tal descriptions. In a series of experiments,
we demonstrate that such learned models can
be used for a range of tasks, including pre-
dicting sentimental words and using them to
rank and build avatars. Together, these re-
sults demonstrate that sentimental language
provides a concise (though noisy) means of
specifying low-level visual properties.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.935303272727273">
Language can describe varied aspects of our visual
world, including not only what is literally there but
also the social, cultural, and emotional sentiment it
invokes. Recently, there has been a growing effort
to study literal language that describes directly ob-
servable properties, such as object color, shape, or
This is a light tan young man State of mind: angry, upset,
with short and trim haircut. He determined. Likes: country
has straight eyebrows and large western, rodeo. Occupation:
brown eyes. He has a neat and cowboy, wrangler, horse trainer.
trim appearance. Overall: youthful, cowboy.
</bodyText>
<figureCaption confidence="0.835829333333333">
Figure 1: (A) Literal avatar descriptions and (B) sen-
timental descriptions of four avatar properties, in-
cluding possible occupations and interests.
</figureCaption>
<bodyText confidence="0.9958397">
category (Farhadi et al., 2009; Mitchell et al., 2010;
Matuszek et al., 2012). Here, we add a focus on
sentimental visual language, which compactly de-
scribes more subjective properties such as if a person
looks determined, if a resume looks professional, or
if a restaurant looks romantic. Such models enable
many new applications, such as text editors that au-
tomatically select properties including font, color, or
text alignment to best match high level descriptions
such as “professional” or “artistic.”
</bodyText>
<page confidence="0.984983">
416
</page>
<note confidence="0.4711195">
Proceedings of NAACL-HLT 2013, pages 416–425,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999950355555556">
In this paper, we study visual language, both lit-
eral and sentimental, that describes the overall ap-
pearance and style of virtual characters, like those in
Figure 1. We use literal language as feature norms, a
tool used for studying semantic information in cog-
nitive science (Mcrae et al., 2005). Literal words,
such “black” or “hat,” are annotated for objects to in-
dicate how people perceive visual properties. Such
feature norms provide our gold-standard visual de-
tectors, and allow us to focus on learning to model
sentimental language, such as “youthful” or “goth.”
We introduce a new corpus of descriptions of
Xbox avatars created by actual gamers. Each avatar
is specified by 19 attributes, including clothing and
body type, allowing for more than 1020 possibil-
ities. Using Amazon Mechanical Turk,1 we col-
lected literal and sentimental descriptions of com-
plete avatars and many of their component parts,
such as the cowboy hat in Figure 1(B). In all, there
are over 100K descriptions. To demonstrate poten-
tial for learning, we also report an A/B test which
shows that native speakers can use sentimental de-
scriptions to distinguish the labeled avatars from
random distractors. This new data will enable study
of the relationships between the co-occurring literal
and sentimental text in a rich visual setting.2
We describe models for three tasks: (i) classify-
ing when words match avatars, (ii) ranking avatars
given a description, and (iii) constructing avatars to
match a description. Each model includes literal part
descriptions as feature norms, enabling us to learn
which literal and sentinel word pairs best predict
complete avatars.
Experiments demonstrate the potential for jointly
modeling literal and sentimental visual descriptions
on our new dataset. The approach outperforms sev-
eral baselines and learns varied relationships be-
tween the sentimental and literal descriptions. For
example, in one experiment “nerdy student” is pre-
dictive of an avatar with features indicating its shirt
is “plaid” and glasses are “large” and faces that are
not “bearded.” We also show that individual sen-
timental words can be predicted but that multiple
avatars can match a single sentimental description.
Finally, we use our model to build complete avatars
</bodyText>
<footnote confidence="0.995903">
1www.mturk.com
2Data available at http://homes.cs.washington.
edu/˜my89/avatar.
</footnote>
<bodyText confidence="0.9692115">
and show that we can accurately predict the senti-
mental terms annotators ascribe to them.
</bodyText>
<sectionHeader confidence="0.999762" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999967255813953">
To the best of our knowledge, our focus on learn-
ing to understand visual sentiment descriptions is
novel. However, visual sentiment has been stud-
ied from other perspectives. Jrgensen (1998) pro-
vides examples which show that visual descriptions
communicate social status and story information in
addition to literal object and properties. Tousch et
al. (2012) draw the distinction between “of-ness”
(objective and concrete) and “about-ness” (subjec-
tive and abstract) in image retrieval, and observe
that many image queries are abstract (for example,
images about freedom). Finally, in descriptions of
people undergoing emotional distress, Fussell and
Moss (1998) show that literal descriptions co-occur
frequently with sentimental ones.
There has been significant work on more lit-
eral aspects of grounded language understand-
ing, both visual and non-visual. The Words-
Eye project (Coyne and Sproat, 2001) generates
3D scenes from literal paragraph-length descrip-
tions. Generating literal textual descriptions of vi-
sual scenes has also been studied, including both
captions (Kulkarni et al., 2011; Yang et al., 2011;
Feng and Lapata, 2010) and descriptions (Farhadi
et al., 2010). Furthermore, Chen and Dolan (2011)
collected literal descriptions of videos with the
goal of learning paraphrases while Zitnick and
Parikh (2013) describe a corpus of descriptions for
clip art that supports the discovery of semantic ele-
ments of visual scenes.
There has also been significant recent work on au-
tomatically recovering visual attributes, both abso-
lute (Farhadi et al., 2009) and relative (Kovashka et
al., 2012), a challenge that we avoid having to solve
with our use of feature norms (Mcrae et al., 2005).
Grounded language understanding has also re-
ceived significant attention, where the goal is to
learn to understand situated non-visual language
use. For example, there has been work on learning
to execute instructions (Branavan et al., 2009; Chen
and Mooney, 2011; Artzi and Zettlemoyer, 2013),
provide sports commentary (Chen et al., 2010), un-
derstand high level strategy guides to improve game
</bodyText>
<page confidence="0.998211">
417
</page>
<figureCaption confidence="0.9829675">
Figure 2: The number of assets per category and ex-
ample images from the hair, shirt and hat categories.
</figureCaption>
<bodyText confidence="0.981056375">
play (Branavan et al., 2011; Eisenstein et al., 2009),
and understand referring expression (Matuszek et
al., 2012).
Finally, our work is similar in spirit to sentiment
analysis (Pang et al., 2002), emotion detection from
images and speech (Zeng et al., 2009), and metaphor
understanding (Shutova, 2010a; Shutova, 2010b).
However, we focus on more general visual context.
</bodyText>
<sectionHeader confidence="0.991158" genericHeader="method">
3 Data Collection
</sectionHeader>
<bodyText confidence="0.922672478260869">
We gathered a large number of natural language de-
scriptions from Mechanical Turk (MTurk). They in-
clude: (1) literal descriptions of specific facial fea-
tures, clothing or accessories and (2) high level sub-
jective descriptions of human-generated avatars.3
Literal Descriptions We showed annotators a sin-
gle image of clothing, a facial feature or an acces-
sory and asked them to produce short descriptions.
Figure 2 shows the distribution over object types.
We restricted descriptions to be between 3 and 15
words. In all, we collected 33.2K descriptions and
had on average 7 words per descriptions. The ex-
ample annotations with highlighted overlapping pat-
terns are in Table 1.
Sentimental Descriptions We also collected 1913
gamer-created avatars from the web. The avatars
were filtered to contain only items from the set of
665 for which we gathered literal descriptions. The
gender distribution is 95% male.
3(2) also has phrases describing emotional reactions. We
also collected (3) multilingual literal, (4) relative literal and (5)
comprehensive full-body descriptions. We do not use this data,
but it will be included in the public release.
</bodyText>
<table confidence="0.8721855">
LITERAL DESCRIPTIONS
full-sleeved executive blue shirt
blue , long-sleeved button-up shirt
mens blue button dress shirt with dark blue stripes
multi-blue striped long-sleeve button-up dress
shirt with cuffs and breast pocket
</table>
<tableCaption confidence="0.999424">
Table 1: Literal descriptions of shirt in Figure 2.
</tableCaption>
<bodyText confidence="0.99756125">
To gather high level sentimental descriptions, an-
notators were presented with an image of an avatar
and asked to list phrases in response to the follow
different aspects:
</bodyText>
<listItem confidence="0.6228425">
- State of mind of the avatar.
- Things the avatar might care about.
- What the avatar might do for a living.
- Overall appearance of the avatar.
</listItem>
<bodyText confidence="0.9985442">
6144 unique vocabulary items occurred in these
descriptions, but only 1179 occurred more than 10
times. Figure 1 (B) shows an avatar and its corre-
sponding sentimental descriptions.
Quality Control All annotations in our dataset are
produced by non-expert annotators. We relied on
manual spot checks to limit poor annotations. Over
time, we developed a trusted crowd of annotators
who produced only high quality annotations during
the earliest stage of data collection.
</bodyText>
<sectionHeader confidence="0.998087" genericHeader="method">
4 Feasibility
</sectionHeader>
<bodyText confidence="0.999989526315789">
Our hypothesis is that sentimental language does not
uniquely identify an avatar, but instead summarizes
or otherwise describes its overall look. In general,
there is a trade off between concise and precise de-
scriptions. For example, given a single word you
might be able to generally describe the overall look
of an avatar, but a long, detailed, literal description
would be required to completely specify their ap-
pearance.
To demonstrate that the sentimental descriptions
we collected are precise enough to be predictive
of appearance, we conducted an experiment that
prompts people to judge when avatars match de-
scriptions. We created an A/B test where we show
English speakers two avatars and one sentimental
description. They were asked to select which avatar
is better matched by the description and how dif-
ficult they felt, on a scale from 1 to 4, it was to
judge. For 100 randomly selected descriptions, we
</bodyText>
<page confidence="0.986371">
418
</page>
<figure confidence="0.999598461538462">
1.2
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<figureCaption confidence="0.999201">
Figure 3: Judged task difficulty versus agreement,
</figureCaption>
<bodyText confidence="0.974692652173913">
gamer avatar preference, and percentage of data cov-
ered. The difficulty axis is cumulative.
asked 5 raters to compare the gamer avatars to ran-
domly generated ones (where each asset is selected
independently according to a uniform distribution).
Figure 3 shows a plot of Kappa and the percent of
the time a majority of the raters selected the gamer
avatar. The easiest 20% of the data pairs had the
strongest agreement, with kappa=.92, and two thirds
of the data has kappa = .70. While agreement falls
off to .52 for the full data set, the gamer avatar re-
mains the majority judgment 81% of the time.
The fact that random avatars are sometimes pre-
ferred indicates that it can be difficult to judge sen-
timental descriptions. Consider the avatars in Fig-
ure 4. Neither conforms to a clear sentimental de-
scription based on the questions we asked. The
right one is described with conflicting words and
the words describing the left one are very general
(like “dumb”). This corresponds to our intuition that
while many avatars can be succinctly summarized
with our questions, some would be more easily de-
scribed using literal language.
</bodyText>
<sectionHeader confidence="0.971421" genericHeader="method">
5 Tasks and Evaluation
</sectionHeader>
<bodyText confidence="0.810184357142857">
We formulate three tasks to study the feasibility of
learning the relationship between sentimental and
literal descriptions. In this section, we first define
the space of possible avatars, followed by the tasks.
Avatars Figure 5 summarizes the notation we will
develop to describe the data. An avatar is defined by
a 19 dimensional vector d where each position is an
State of mind: content, humble, satisfied,
State of mind: peaceful, relaxed, calm. Likes: fashion,
playful, happy; friends, money, cars, music, education.
Likes: sex Occupation: teacher, singer, actor,
Occupation: hobo performer, dancer, computer engineer.
Overall: dumb Overall: nerdy, cool, smart, comfy,
easygoing, reserved
</bodyText>
<figureCaption confidence="0.994831">
Figure 4: Avatars rated as difficult.
</figureCaption>
<bodyText confidence="0.990450523809524">
index into a list of possible items a. Each dimension
represents a position on the avatar, for example, hat
or nose. Each possible item is called an asset and
is associated with a set of positions it can fill. Most
assets take up exactly one position, while there are
a few cases where assets take multiple positions.4
An avatar d is valid if all of its mandatory positions
are filled, and no two assets conflict on a position.
Mandatory positions include hair, eyes, ears, eye-
brows, nose, mouth, chin, shirt, pants, and shoes.
All other positions are optional. We refer to this set
of valid d as A. Practically speaking, if an avatar is
not valid, it cannot be reliably rendered graphically.
Each item i is associated with the literal descrip-
tions 4 E D where D is the set of literal descrip-
tions. Furthermore, every avatar d is associated a list
of sentimental query words q, describing subjective
aspects of an avatar.5
Sentimental Word Prediction We first study in-
dividual words. The word prediction task is to de-
cide whether a given avatar can be described with a
</bodyText>
<footnote confidence="0.997622714285714">
4For example, long sleeve shirts cover up watches, so they
take up both shirt and wristwear positions. Costumes tend to
span many more positions, for example there a suit that takes
up shirt, pants, wristwear and shoes positions.
5We do not distinguish which prompt (e.g., “state of mind”
or “occupation”) a word in q came from, although the vocabu-
laries are relatively disjoint.
</footnote>
<page confidence="0.992705">
419
</page>
<figureCaption confidence="0.999452">
Figure 5: Avatars, queries, items, literal descriptions.
</figureCaption>
<bodyText confidence="0.992390105263158">
particular sentimental word q∗. We evaluate perfor-
mance with F-score.
Avatar Ranking We also consider an avatar re-
trieval task, where the goal is to rank the set of
avatars in our data, Uj=1...n aj, according to which
one best matches a sentimental description, qi. As
an automated evaluation, we report the average per-
centile position assigned to the true ai for each ex-
ample. However, in general, many different avatars
can match each qi, an interesting phenomena we will
further study with human evaluation.
Avatar Generation Finally, we consider the prob-
lem of generating novel, previously unseen avatars,
by selecting a set of items that best embody some
sentimental description. As with ranking, we aim to
construct the avatar a2 that matches each sentimen-
tal description qi. We evaluate by considering the
item overlap between ai and the output avatar a*,
discounting for empty positions:6
</bodyText>
<equation confidence="0.955036">
� |a*_
f = j=1 I (�a∗j — aij) (1)
max(numparts(a*), numparts(ai))
</equation>
<bodyText confidence="0.976180083333333">
where numparts returns the number of non-empty
avatar positions. The score is a conservative measure
because some items are significantly more visually
salient than others. For instance, shirts and pants oc-
cupy a large portion of the physical realization of the
avatar, while rings are small and virtually unnotice-
able. We additionally perform a human evaluation
in Section 8 to better understand these challenges.
6Optional items are infrequently used. Therefore not pre-
dicting them at all offers a strong baseline. Yet doing this
demonstrates nothing about an algorithm’s ability to predict
items which contribute to the sentimental qualities of an avatar.
</bodyText>
<sectionHeader confidence="0.999412" genericHeader="method">
6 Methods
</sectionHeader>
<bodyText confidence="0.99998175">
We present two different models: one that considers
words in isolation and another that jointly models
the query words. This section defines the models
and how we learn them.
</bodyText>
<subsectionHeader confidence="0.980584">
6.1 Independent Sentimental Word Model
</subsectionHeader>
<bodyText confidence="0.9999595">
The independent word model (S-Independent) as-
sumes that each word independently describes the
avatar. We construct a separate linear model for each
word in the vocabulary.
To train these model, we transform the data to
form a binary classification problem for each word,
where the positive data includes all avatars the word
was seen with, (q, d, 1) for all i and q E qi, and the
rest are negative, (q, a2, 0) for all i and q E� �qi.
We use the following features:
</bodyText>
<listItem confidence="0.846207625">
• an indicator feature for the cross product of a
sentiment query word q, a literal description
word w E D, and the avatar position index j
(for example, q = “angry” with w = “pointy”
and j = eyebrows):
I(q E�qi,w E dai,, j)
• a bias feature for keeping a position empty:
I(q E qz, aij = empty,j)
</listItem>
<bodyText confidence="0.99965175">
These features will allow the model to capture
correlations between our feature norms which pro-
vide descriptions of visual attributes, like black, and
sentimental words, like gothic.
</bodyText>
<page confidence="0.988455">
420
</page>
<bodyText confidence="0.999699625">
S-Independent is used for both word prediction
and ranking. For prediction, we train a linear model
using averaged binary perceptron. For ranking, we
try to rank all positive instances above negative in-
stances. We use an averaged structured perceptron
to train the ranker (Collins, 2002). To rank with re-
spect to an entire query ~qi, we sum the scores of each
word q E ~qi.
</bodyText>
<subsectionHeader confidence="0.998186">
6.2 Joint Sentimental Model
</subsectionHeader>
<bodyText confidence="0.999923333333333">
The second approach (S-Joint) jointly models the
query words to learn the relationships between lit-
eral and sentimental words with score s:
</bodyText>
<equation confidence="0.720869">
θT f(~ai, ~qj, ~dai)
</equation>
<bodyText confidence="0.999486">
Where every word in the query has a separate factor
and every position is treated independently subject
to the constraint that a~ is valid. The feature function
f uses the same features as the word independent
model above.
This model is used for ranking and generation.
For ranking, we try to rank the avatar ai for query
qi above all other avatars in the candidate set. For
generation, we try to score ai above all other valid
avatars given the query qi. In both cases, we train
with averaged structured perceptron (Collins, 2002)
on the original data, containing query, avatar pairs
(~qi, ~ai).
</bodyText>
<sectionHeader confidence="0.997983" genericHeader="method">
7 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9796245625">
Random Baseline For the ranking and avatar gen-
eration tasks, we report random baselines. For rank-
ing, we randomly order the avatars. In the genera-
tion case, we select an item randomly for every posi-
tion. This baseline does not generate optional assets
because they are rare in the real data.
Sentimental-Literal Overlap (SL-Overlap) We
also report a baseline that measures the overlap be-
tween words in the sentiment query ~qi and words in
the literal asset descriptions D. In generation, for
each position in the avatar, ~ai, SL-Overlap selects
the item whose literal description has the most words
in common with ~qi. If no item had overlap with the
query, we backoff to a random choice. In the case of
ranking, it orders avatars by the sum over every po-
sition of the number of words in common between
</bodyText>
<table confidence="0.999922619047619">
Word F-Score Precision Recall N
happi 0.84 0.89 0.78 149
student 0.78 0.82 0.74 129
friend 0.76 0.84 0.70 153
music 0.74 0.89 0.63 148
confid 0.74 0.82 0.76 157
sport 0.69 0.62 0.76 76
casual 0.63 0.6 0.67 84
youth 0.6 0.57 0.64 88
waitress 0.59 0.42 1 5
smart 0.57 0.54 0.6 88
fashion 0.54 0.54 0.54 70
monei 0.54 0.52 0.56 76
cool 0.54 0.52 0.56 84
relax 0.53 0.52 0.56 90
game 0.51 0.44 0.62 61
musician 0.51 0.44 0.61 66
parti 0.51 0.43 0.62 58
content 0.5 0.47 0.53 75
friendli 0.49 0.42 0.6 56
smooth 0.49 0.4 0.63 57
</table>
<tableCaption confidence="0.991658">
Table 2: Top 20 words (stemmed) for classification.
</tableCaption>
<bodyText confidence="0.9942845625">
N is the number of occurances in the test set.
the literal description and the query, ~qi. This base-
line tests the degree to which literal and sentimental
descriptions overlap lexically.
Feature Generation For all models that use lexi-
cal features, we limited the number of words. 6144
unique vocabulary items occur in the query set, and
3524 in the literal description set. There are over
400 million entries in the full set of features that in-
clude the cross product of these sets with all possible
avatar positions, as described in Section 6. Since this
would present a challenge for learning, we prune in
two ways. We stem all words with a Porter stemmer.
We also filter out all features which do not occur at
least 10 times in our training set. The final model
has approximately 700k features.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.9999755">
We present results for the tasks described in Sec-
tion 5 with the appropriate models from Section 6.
</bodyText>
<subsectionHeader confidence="0.986241">
8.1 Word Prediction Results
</subsectionHeader>
<bodyText confidence="0.99997625">
The goal of our first experiment is to study when
individual sentiment words can be accurately pre-
dicted. We computed sentimental word classifica-
tion accuracy for 1179 word classes with 10 or more
</bodyText>
<equation confidence="0.9929956">
� i=1
s(~a|~q,D) =
|~q|
L
j=1
</equation>
<page confidence="0.996917">
421
</page>
<table confidence="0.9987396">
Algorithm Percentile Rank
S-joint 77.3
S-independant 73.5
SL-overlap 60.4
Random 48.8
</table>
<tableCaption confidence="0.995271">
Table 3: Automatic evaluation of ranking. The aver-
</tableCaption>
<bodyText confidence="0.988329583333333">
age percentile that a test avatar was ranked given its
sentimental description.
mentions. Table 2 shows the top 20 words ordered
by F-score.7 Many common words can be predicted
with relatively high accuracy. Words with strong
individual cues like happy (a smiling mouth), and
confidence (wide eyes) and nerdi (particular glasses)
can be predicted well.
The average F-score among all words was .085.
33.2% of words have an F-score of zero. These zeros
include words like: unusual, bland, sarcastic, trust,
prepared, limber, healthy and poetry. Some of these
words indicate broad classes of avatars (e.g., unusual
avatars) and others indicate subtle modifications to
looks that without other words are not specific (e.g.,
a prepared surfer vs. a prepared business man). Fur-
thermore, evaluation was done assuming that when
a word is not mentioned, it is should be predicted as
negative. This fails to account for the fact that peo-
ple do not mention everything that’s true, but instead
make choices about what to mention based on the
most relevant qualities. Despite these difficulties,
the classification performance shows that we can ac-
curately capture usage patterns for many words.
</bodyText>
<subsectionHeader confidence="0.999852">
8.2 Ranking Results
</subsectionHeader>
<bodyText confidence="0.994608179487179">
Ranking allows us to test the hypothesis that multi-
ple avatars are valid for a high level description. Fur-
thermore, we consider the differences between S-
Joint and S-Independent, showing that jointly mod-
elings all words improves ranking performance.
Automatic Evaluation The results are shown in
Table 3. Both S-Independent and S-Joint outperform
the SL-overlap baseline. SL-Overlap’s poor perfor-
mance can be attributed to low direct overlap be-
tween sentimental words and literal words. S-Joint
also outperforms the S-Independent.
7Accuracy numbers are inappropriate in this case because
the number of negative instances, in most cases, is far larger
than the number of positive ones.
Inspection of the parameters shows that S-Joint
does better than S-Independent in modeling words
that only relate to a subset of body positions. For
example, in one case we found that for the word
“puzzled” nearly 50% of the weights were on fea-
tures that related to eyebrows and eyes. This type
of specialization was far more pronounced for S-
Joint. The joint nature of the learning allows the fea-
tures for individual words to specialize for specific
positions. In contrast, S-Independent must indepen-
dently predict all parts for every word.
Human Evaluation We report human relevancy
judgments for the top-5 returned results from S-
Joint. On average, 56.2% were marked to be rele-
vant. This shows that S-Joint is performing better
than automatic numbers would indicate, confirming
our intuition that there is a one-to-many relationship
between a sentimental description and avatars. Sen-
timental descriptions, while having significant sig-
nal, are not exact. These results also indicate that
relying on automatic measures of accuracy that as-
sume a single reference avatar underestimates per-
formance. Figure 6 shows the top ranked results
returned by S-Joint for a sentimental description
where the model performs well.
</bodyText>
<subsectionHeader confidence="0.999146">
8.3 Generation Results
</subsectionHeader>
<bodyText confidence="0.999452263157895">
Finally we evaluate three models for avatar genera-
tion: Random, SL-Overlap and S-Joint using auto-
matic measures and human evaluation.
Automatic Evaluation Table 4 presents results
for automatic evaluation. The Random baseline per-
forms badly, on average assigning items correctly to
less than 1 position in the generated avatar. The SL-
Overlap baseline improves, but still performs quite
poorly. The S-Joint model performs significantly
better, correctly guessing 2-3 items for each output
avatar. However, as we will see in the manual eval-
uation, many of the non-matching parts it produces
are still a good fit for the query.
Human Evaluation As before, there are many
reasonable avatars that could match as well as the
reference avatars. Therefore, we also evaluated gen-
eration with A/B tests, much like in Section 4. An-
notators were asked to judge which of two avatars
better matched a sentimental description. They
</bodyText>
<page confidence="0.996275">
422
</page>
<figureCaption confidence="0.9881505">
pensive,confrontational; music,socializing; musician,bar tending,club owner; smart,cool.
Figure 6: A sentimental description paired with the highest ranked avatars found by S-Joint.
</figureCaption>
<table confidence="0.952866">
Model Overlap
Random 0.041
SL-Overlap 0.049
S-Joint 0.126
</table>
<tableCaption confidence="0.94799">
Table 4: Automatic generation evaluation results.
The item overlap metric is defined in Section 5.
</tableCaption>
<table confidence="0.99983025">
Kappa Majority Random Sys.
SL-Overlap 0.20 0.25 0.34 0.32
S-Joint 0.52 0.90 0.07 0.81
Gamer 0.52 0.81 0.08 0.77
</table>
<tableCaption confidence="0.966692">
Table 5: Human evaluation of automatically gener-
</tableCaption>
<bodyText confidence="0.994389162790698">
ated avatars. Majority represents the percentage of
time the system output is preferred by a majority of
raters. Random and System (Sys.) indicate the per-
centage of time each was preferred.
could rate System A or System B as better, or re-
port that they were equal or that neither matched
the description. We consider two comparisons: SL-
Overlap vs. Random and S-Joint vs Random. Five
annotators performed each condition, rating 100 ex-
amples with randomly ordered avatars.
We report the results for human evaluation includ-
ing kappa, majority judgments, and a distribution
over judgments in Table 5. The SL-Overlap baseline
is indistinguishable from a random avatar. This con-
trasts with the ranking case, where this simple base-
line showed improvement, indicating that generation
is a much harder problem. Furthermore, agreement
is low; people felt the need to make a choice but
were not consistent.
We also see in Table 5 that people prefer the S-
Joint model outputs to random avatars as often as
they prefer gamer to random. While this does not
necessarily imply that S-Joint creates gamer-quality
avatars, it indicates substantial progress by learning
a mapping between literal and sentimental words.
Qualitative Results Table 6 presents the highest
and lowest weighted features for different sentimen-
tal query words. Figure 7 shows four descriptions
that were assigned high quality avatars.
In general, many of the weaker avatars had as-
pects of the descriptions but lacked such distinctive
overall looks. This was especially true when the
descriptions contained seemingly contradictory in-
formation. For example, one avatar was described
as being both nerdy and popular. We generated a
look that had aspects of both of these descriptions,
including a head that contained both conservative el-
ements (like glasses) and less conservative elements
(like crazy hair and earrings). However, the combi-
nation would not be described as nerdy or popular,
because of difficult to predict global interactions be-
tween the co-occurring words and items. This is an
important area for future work.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.9998884">
We explored how visual language, both literal and
sentimental, maps to the overall physical appearance
and style of virtual characters. While this paper fo-
cused on avatar design, our approach has implica-
tions for a broad class of natural language-driven
</bodyText>
<page confidence="0.999191">
423
</page>
<tableCaption confidence="0.466851">
Ambition; business, Capable, confident, firm; heavy metal, Stressed, bored, Happy, content, confident,
fashion, success; extreme sports, motorcycles; engineer, discontent; emo music; home, career, family,
salesman; smooth, mechanic, machinist; aggressive, works at a record store; secretary,student,
professional. strong, protective. goth, dark, drab. classy,clean,casual
</tableCaption>
<figureCaption confidence="0.987255">
Figure 7: Avatars automatically generated with the S-Joint model.
</figureCaption>
<table confidence="0.9960322">
Sentiment positive features negative features
happi mouth:thick, mouth:smilei, mouth:make, mouth:open mouth:tight, mouth:emotionless, mouth:brownish, mouth:attract
gothic shoes:brown, shirt:black, pants:hot, shirt:band shirt:half, shirt:tight, pants:sexi, hair:brownish
retro eyebrows:men, eyebrows:large, hair:round, pants:light eyebrows:beauti, pants:side; eyebrows:trim, pants:cut
beach pants:yello, pants:half, nose:narrow, pants:white shirt:brown, shirt:side; shoes:long, pants:jean
</table>
<tableCaption confidence="0.992691">
Table 6: Most positive and negative features for a word stem. A feature is [position]:[literal word].
</tableCaption>
<bodyText confidence="0.999971235294118">
dialog scenarios. In many situations, a user may
be perfectly able to formulate a high-level descrip-
tion of their intent (“Make my resume look cleaner”
“Buy me clothes for a summer wedding,” or “Play
something more danceable”) while having little or
no understanding of the complex parameter space
that the underlying software must manipulate in or-
der to achieve this result.
We demonstrated that these high-level sentimen-
tal specifications can have a strong relationship to
literal aspects of a problem space and showed that
sentimental language is a concise, yet noisy, way
of specifying high level characteristics. Sentimen-
tal language is an unexplored avenue for improving
natural language systems that operate in situated set-
tings. It has the potential to bridge the gap between
lay and expert understandings of a problem domain.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999803625">
This work is partially supported by the DARPA
CSSG (N11AP20020) and the NSF (IIS-1115966).
The authors would like to thank Chris Brockett,
Noelle Sophy, Rico Malvar for helping with collect-
ing and processing the data. We would also like
to thank Tom Kwiatkowski and Nicholas FitzGer-
ald and the anonymous reviewers for their helpful
comments.
</bodyText>
<sectionHeader confidence="0.993264" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.879753681818182">
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Association
for Computational Linguistics, 1(1):49–62.
SRK Branavan, H. Chen, L.S. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 82–90.
SRK Branavan, David Silver, and Regina Barzilay. 2011.
Learning to win by reading manuals in a monte-carlo
framework. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages 268–
277.
David L. Chen and William B. Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 190–200.
D.L. Chen and R.J. Mooney. 2011. Learning to interpret
natural language navigation instructions from observa-
</reference>
<page confidence="0.988744">
424
</page>
<reference confidence="0.999672755319148">
tions. In Proceedings of the 25th AAAI Conference on
Artificial Intelligence (AAAI-2011), pages 859–865.
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal ofArtificial
Intelligence Research, 37:397–435.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 conference on Empirical methods in natu-
ral language processing, pages 1–8.
B. Coyne and R. Sproat. 2001. Wordseye: an automatic
text-to-scene conversion system. In Proceedings of the
28th annual conference on Computer graphics and in-
teractive techniques, pages 487–496.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 958–967.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their attributes.
In Proceedings of the IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a story:
generating sentences from images. In Proceedings of
the 11th European conference on Computer Vision,
ECCV’10, pages 15–29.
Yansong Feng and Mirella Lapata. 2010. Topic models
for image annotation and text illustration. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 831–839.
Susan R Fussell and Mallie M Moss. 1998. Figura-
tive language in emotional communication. Social and
cognitive approaches to interpersonal communication,
page 113.
Corinne Jrgensen. 1998. Attributes of images in describ-
ing tasks. Information Processing &amp; Management,
34(23):161 – 174.
Adriana Kovashka, Devi Parikh, and Kristen Grauman.
2012. Whittlesearch: Image search with relative at-
tribute feedback. In Computer Vision and Pattern
Recognition (CVPR), pages 2973–2980.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C.
Berg, and T.L. Berg. 2011. Baby talk: Understanding
and generating simple image descriptions. In Com-
puter Vision and Pattern Recognition (CVPR), pages
1601–1608.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A Joint
Model of Language and Perception for Grounded At-
tribute Learning. In Proc. of the 2012 International
Conference on Machine Learning.
Ken Mcrae, George S. Cree, Mark S. Seidenberg, and
Chris Mcnorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37(4):547–559.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2010. Natural reference to objects in a visual domain.
In Proceedings of the 6th International Natural Lan-
guage Generation Conference, INLG ’10, pages 95–
104.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of the ACL-02 conference
on Empirical methods in natural language processing,
pages 79–86.
Ekaterina Shutova. 2010a. Automatic metaphor inter-
pretation as a paraphrasing task. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, HLT ’10, pages 1029–1037.
Ekaterina Shutova. 2010b. Models of metaphor in nlp.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL ’10, pages
688–697.
Anne-Marie Tousch, Stphane Herbin, and Jean-Yves Au-
dibert. 2012. Semantic hierarchies for image annota-
tion: A survey. Pattern Recognition, 45(1):333 – 345.
Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing.
Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009.
A survey of affect recognition methods: Audio, vi-
sual, and spontaneous expressions. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
31(1):39–58.
C Lawrence Zitnick and Devi Parikh. 2013. Bringing
semantics into focus using visual abstraction. In Com-
puter Vision and Pattern Recognition (To Appear).
</reference>
<page confidence="0.999166">
425
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.084919">
<title confidence="0.999724">Learning to Relate Literal and Sentimental Descriptions of Visual Properties</title>
<author confidence="0.999113">Mark</author>
<affiliation confidence="0.980089333333333">Computer Science &amp; University of Seattle,</affiliation>
<email confidence="0.999177">my89@cs.washington.edu</email>
<author confidence="0.838064">Asli Celikyilmaz</author>
<affiliation confidence="0.970991">Conversational Understanding Sciences Microsoft</affiliation>
<address confidence="0.97571">Mountain View, CA</address>
<email confidence="0.96689">asli@ieee.org</email>
<author confidence="0.449687">Svitlana</author>
<affiliation confidence="0.7932635">Center for Language and Speech Johns Hopkins</affiliation>
<address confidence="0.60671">Baltimore,</address>
<email confidence="0.999901">svitlana@jhu.edu</email>
<author confidence="0.999899">Luke Zettlemoyer</author>
<affiliation confidence="0.999797">Computer Science &amp; Engineering University of Washington</affiliation>
<address confidence="0.996627">Seattle, WA</address>
<email confidence="0.999609">lsz@cs.washington.edu</email>
<author confidence="0.987253">Bill</author>
<affiliation confidence="0.8123015">NLP Microsoft</affiliation>
<address confidence="0.982785">Redmond, WA</address>
<email confidence="0.999847">billdol@microsoft.edu</email>
<abstract confidence="0.999855739130435">Language can describe our visual world at many levels, including not only what is literally there but also the sentiment that it invokes. In this paper, we study visual language, both literal and sentimental, that describes the overall appearance and style of virtual characters. Sentimental properties, including labels such as “youthful” or “country western,” must be inferred from descriptions of the more literal properties, such as facial features and clothing selection. We present a new dataset, collected to describe Xbox avatars, as well as models for learning the relationships between these avatars and their literal and sentimental descriptions. In a series of experiments, we demonstrate that such learned models can be used for a range of tasks, including predicting sentimental words and using them to rank and build avatars. Together, these results demonstrate that sentimental language provides a concise (though noisy) means of specifying low-level visual properties.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="7404" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="1113" endWordPosition="1116">clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection </context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1(1):49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SRK Branavan</author>
<author>H Chen</author>
<author>L S Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>82--90</pages>
<contexts>
<context position="7351" citStr="Branavan et al., 2009" startWordPosition="1105" endWordPosition="1108"> (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we foc</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>SRK Branavan, H. Chen, L.S. Zettlemoyer, and R. Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 82–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SRK Branavan</author>
<author>David Silver</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning to win by reading manuals in a monte-carlo framework.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume</booktitle>
<volume>1</volume>
<pages>268--277</pages>
<contexts>
<context position="7642" citStr="Branavan et al., 2011" startWordPosition="1154" endWordPosition="1157">hallenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-gene</context>
</contexts>
<marker>Branavan, Silver, Barzilay, 2011</marker>
<rawString>SRK Branavan, David Silver, and Regina Barzilay. 2011. Learning to win by reading manuals in a monte-carlo framework. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 268– 277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>William B Dolan</author>
</authors>
<title>Collecting highly parallel data for paraphrase evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>190--200</pages>
<contexts>
<context position="6626" citStr="Chen and Dolan (2011)" startWordPosition="990" endWordPosition="993">ptions of people undergoing emotional distress, Fussell and Moss (1998) show that literal descriptions co-occur frequently with sentimental ones. There has been significant work on more literal aspects of grounded language understanding, both visual and non-visual. The WordsEye project (Coyne and Sproat, 2001) generates 3D scenes from literal paragraph-length descriptions. Generating literal textual descriptions of visual scenes has also been studied, including both captions (Kulkarni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the goal of learning paraphrases while Zitnick and Parikh (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand </context>
</contexts>
<marker>Chen, Dolan, 2011</marker>
<rawString>David L. Chen and William B. Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 190–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Chen</author>
<author>R J Mooney</author>
</authors>
<title>Learning to interpret natural language navigation instructions from observations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI-2011),</booktitle>
<pages>859--865</pages>
<contexts>
<context position="7374" citStr="Chen and Mooney, 2011" startWordPosition="1109" endWordPosition="1112">us of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visu</context>
</contexts>
<marker>Chen, Mooney, 2011</marker>
<rawString>D.L. Chen and R.J. Mooney. 2011. Learning to interpret natural language navigation instructions from observations. In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI-2011), pages 859–865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Training a multilingual sportscaster: Using perceptual context to learn language.</title>
<date>2010</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>37--397</pages>
<contexts>
<context position="7451" citStr="Chen et al., 2010" startWordPosition="1120" endWordPosition="1123"> of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language </context>
</contexts>
<marker>Chen, Kim, Mooney, 2010</marker>
<rawString>David L. Chen, Joohyun Kim, and Raymond J. Mooney. 2010. Training a multilingual sportscaster: Using perceptual context to learn language. Journal ofArtificial Intelligence Research, 37:397–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="17718" citStr="Collins, 2002" startWordPosition="2810" endWordPosition="2811"> “angry” with w = “pointy” and j = eyebrows): I(q E�qi,w E dai,, j) • a bias feature for keeping a position empty: I(q E qz, aij = empty,j) These features will allow the model to capture correlations between our feature norms which provide descriptions of visual attributes, like black, and sentimental words, like gothic. 420 S-Independent is used for both word prediction and ranking. For prediction, we train a linear model using averaged binary perceptron. For ranking, we try to rank all positive instances above negative instances. We use an averaged structured perceptron to train the ranker (Collins, 2002). To rank with respect to an entire query ~qi, we sum the scores of each word q E ~qi. 6.2 Joint Sentimental Model The second approach (S-Joint) jointly models the query words to learn the relationships between literal and sentimental words with score s: θT f(~ai, ~qj, ~dai) Where every word in the query has a separate factor and every position is treated independently subject to the constraint that a~ is valid. The feature function f uses the same features as the word independent model above. This model is used for ranking and generation. For ranking, we try to rank the avatar ai for query qi</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Coyne</author>
<author>R Sproat</author>
</authors>
<title>Wordseye: an automatic text-to-scene conversion system.</title>
<date>2001</date>
<booktitle>In Proceedings of the 28th annual conference on Computer graphics and interactive techniques,</booktitle>
<pages>487--496</pages>
<contexts>
<context position="6316" citStr="Coyne and Sproat, 2001" startWordPosition="944" endWordPosition="947">y information in addition to literal object and properties. Tousch et al. (2012) draw the distinction between “of-ness” (objective and concrete) and “about-ness” (subjective and abstract) in image retrieval, and observe that many image queries are abstract (for example, images about freedom). Finally, in descriptions of people undergoing emotional distress, Fussell and Moss (1998) show that literal descriptions co-occur frequently with sentimental ones. There has been significant work on more literal aspects of grounded language understanding, both visual and non-visual. The WordsEye project (Coyne and Sproat, 2001) generates 3D scenes from literal paragraph-length descriptions. Generating literal textual descriptions of visual scenes has also been studied, including both captions (Kulkarni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the goal of learning paraphrases while Zitnick and Parikh (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically rec</context>
</contexts>
<marker>Coyne, Sproat, 2001</marker>
<rawString>B. Coyne and R. Sproat. 2001. Wordseye: an automatic text-to-scene conversion system. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 487–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>J Clarke</author>
<author>D Goldwasser</author>
<author>D Roth</author>
</authors>
<title>Reading to learn: Constructing features from semantic abstracts.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>958--967</pages>
<contexts>
<context position="7668" citStr="Eisenstein et al., 2009" startWordPosition="1158" endWordPosition="1161">having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-generated avatars.3 Literal De</context>
</contexts>
<marker>Eisenstein, Clarke, Goldwasser, Roth, 2009</marker>
<rawString>J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth. 2009. Reading to learn: Constructing features from semantic abstracts. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 958–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Ian Endres</author>
<author>Derek Hoiem</author>
<author>David Forsyth</author>
</authors>
<title>Describing objects by their attributes.</title>
<date>2009</date>
<booktitle>In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="2362" citStr="Farhadi et al., 2009" startWordPosition="340" endWordPosition="343"> invokes. Recently, there has been a growing effort to study literal language that describes directly observable properties, such as object color, shape, or This is a light tan young man State of mind: angry, upset, with short and trim haircut. He determined. Likes: country has straight eyebrows and large western, rodeo. Occupation: brown eyes. He has a neat and cowboy, wrangler, horse trainer. trim appearance. Overall: youthful, cowboy. Figure 1: (A) Literal avatar descriptions and (B) sentimental descriptions of four avatar properties, including possible occupations and interests. category (Farhadi et al., 2009; Mitchell et al., 2010; Matuszek et al., 2012). Here, we add a focus on sentimental visual language, which compactly describes more subjective properties such as if a person looks determined, if a resume looks professional, or if a restaurant looks romantic. Such models enable many new applications, such as text editors that automatically select properties including font, color, or text alignment to best match high level descriptions such as “professional” or “artistic.” 416 Proceedings of NAACL-HLT 2013, pages 416–425, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Li</context>
<context position="6979" citStr="Farhadi et al., 2009" startWordPosition="1045" endWordPosition="1048">h-length descriptions. Generating literal textual descriptions of visual scenes has also been studied, including both captions (Kulkarni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the goal of learning paraphrases while Zitnick and Parikh (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from </context>
</contexts>
<marker>Farhadi, Endres, Hoiem, Forsyth, 2009</marker>
<rawString>Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. 2009. Describing objects by their attributes. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: generating sentences from images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th European conference on Computer Vision, ECCV’10,</booktitle>
<pages>15--29</pages>
<contexts>
<context position="6590" citStr="Farhadi et al., 2010" startWordPosition="985" endWordPosition="988">s about freedom). Finally, in descriptions of people undergoing emotional distress, Fussell and Moss (1998) show that literal descriptions co-occur frequently with sentimental ones. There has been significant work on more literal aspects of grounded language understanding, both visual and non-visual. The WordsEye project (Coyne and Sproat, 2001) generates 3D scenes from literal paragraph-length descriptions. Generating literal textual descriptions of visual scenes has also been studied, including both captions (Kulkarni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the goal of learning paraphrases while Zitnick and Parikh (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: generating sentences from images. In Proceedings of the 11th European conference on Computer Vision, ECCV’10, pages 15–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic models for image annotation and text illustration.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>831--839</pages>
<contexts>
<context position="6550" citStr="Feng and Lapata, 2010" startWordPosition="979" endWordPosition="982"> queries are abstract (for example, images about freedom). Finally, in descriptions of people undergoing emotional distress, Fussell and Moss (1998) show that literal descriptions co-occur frequently with sentimental ones. There has been significant work on more literal aspects of grounded language understanding, both visual and non-visual. The WordsEye project (Coyne and Sproat, 2001) generates 3D scenes from literal paragraph-length descriptions. Generating literal textual descriptions of visual scenes has also been studied, including both captions (Kulkarni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the goal of learning paraphrases while Zitnick and Parikh (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has al</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. Topic models for image annotation and text illustration. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 831–839.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan R Fussell</author>
<author>Mallie M Moss</author>
</authors>
<title>Figurative language in emotional communication. Social and cognitive approaches to interpersonal communication,</title>
<date>1998</date>
<pages>113</pages>
<contexts>
<context position="6076" citStr="Fussell and Moss (1998)" startWordPosition="908" endWordPosition="911">s on learning to understand visual sentiment descriptions is novel. However, visual sentiment has been studied from other perspectives. Jrgensen (1998) provides examples which show that visual descriptions communicate social status and story information in addition to literal object and properties. Tousch et al. (2012) draw the distinction between “of-ness” (objective and concrete) and “about-ness” (subjective and abstract) in image retrieval, and observe that many image queries are abstract (for example, images about freedom). Finally, in descriptions of people undergoing emotional distress, Fussell and Moss (1998) show that literal descriptions co-occur frequently with sentimental ones. There has been significant work on more literal aspects of grounded language understanding, both visual and non-visual. The WordsEye project (Coyne and Sproat, 2001) generates 3D scenes from literal paragraph-length descriptions. Generating literal textual descriptions of visual scenes has also been studied, including both captions (Kulkarni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the</context>
</contexts>
<marker>Fussell, Moss, 1998</marker>
<rawString>Susan R Fussell and Mallie M Moss. 1998. Figurative language in emotional communication. Social and cognitive approaches to interpersonal communication, page 113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinne Jrgensen</author>
</authors>
<title>Attributes of images in describing tasks.</title>
<date>1998</date>
<journal>Information Processing &amp; Management,</journal>
<volume>34</volume>
<issue>23</issue>
<pages>174</pages>
<contexts>
<context position="5604" citStr="Jrgensen (1998)" startWordPosition="843" endWordPosition="844"> glasses are “large” and faces that are not “bearded.” We also show that individual sentimental words can be predicted but that multiple avatars can match a single sentimental description. Finally, we use our model to build complete avatars 1www.mturk.com 2Data available at http://homes.cs.washington. edu/˜my89/avatar. and show that we can accurately predict the sentimental terms annotators ascribe to them. 2 Related Work To the best of our knowledge, our focus on learning to understand visual sentiment descriptions is novel. However, visual sentiment has been studied from other perspectives. Jrgensen (1998) provides examples which show that visual descriptions communicate social status and story information in addition to literal object and properties. Tousch et al. (2012) draw the distinction between “of-ness” (objective and concrete) and “about-ness” (subjective and abstract) in image retrieval, and observe that many image queries are abstract (for example, images about freedom). Finally, in descriptions of people undergoing emotional distress, Fussell and Moss (1998) show that literal descriptions co-occur frequently with sentimental ones. There has been significant work on more literal aspec</context>
</contexts>
<marker>Jrgensen, 1998</marker>
<rawString>Corinne Jrgensen. 1998. Attributes of images in describing tasks. Information Processing &amp; Management, 34(23):161 – 174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriana Kovashka</author>
<author>Devi Parikh</author>
<author>Kristen Grauman</author>
</authors>
<title>Whittlesearch: Image search with relative attribute feedback.</title>
<date>2012</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR),</booktitle>
<pages>2973--2980</pages>
<contexts>
<context position="7016" citStr="Kovashka et al., 2012" startWordPosition="1051" endWordPosition="1054">teral textual descriptions of visual scenes has also been studied, including both captions (Kulkarni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the goal of learning paraphrases while Zitnick and Parikh (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. p</context>
</contexts>
<marker>Kovashka, Parikh, Grauman, 2012</marker>
<rawString>Adriana Kovashka, Devi Parikh, and Kristen Grauman. 2012. Whittlesearch: Image search with relative attribute feedback. In Computer Vision and Pattern Recognition (CVPR), pages 2973–2980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kulkarni</author>
<author>V Premraj</author>
<author>S Dhar</author>
<author>S Li</author>
<author>Y Choi</author>
<author>A C Berg</author>
<author>T L Berg</author>
</authors>
<title>Baby talk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR),</booktitle>
<pages>1601--1608</pages>
<contexts>
<context position="6507" citStr="Kulkarni et al., 2011" startWordPosition="971" endWordPosition="974">age retrieval, and observe that many image queries are abstract (for example, images about freedom). Finally, in descriptions of people undergoing emotional distress, Fussell and Moss (1998) show that literal descriptions co-occur frequently with sentimental ones. There has been significant work on more literal aspects of grounded language understanding, both visual and non-visual. The WordsEye project (Coyne and Sproat, 2001) generates 3D scenes from literal paragraph-length descriptions. Generating literal textual descriptions of visual scenes has also been studied, including both captions (Kulkarni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the goal of learning paraphrases while Zitnick and Parikh (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 20</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C. Berg, and T.L. Berg. 2011. Baby talk: Understanding and generating simple image descriptions. In Computer Vision and Pattern Recognition (CVPR), pages 1601–1608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Nicholas FitzGerald</author>
<author>Luke Zettlemoyer</author>
<author>Liefeng Bo</author>
<author>Dieter Fox</author>
</authors>
<title>A Joint Model of Language and Perception for Grounded Attribute Learning.</title>
<date>2012</date>
<booktitle>In Proc. of the 2012 International Conference on Machine Learning.</booktitle>
<contexts>
<context position="2409" citStr="Matuszek et al., 2012" startWordPosition="348" endWordPosition="351">effort to study literal language that describes directly observable properties, such as object color, shape, or This is a light tan young man State of mind: angry, upset, with short and trim haircut. He determined. Likes: country has straight eyebrows and large western, rodeo. Occupation: brown eyes. He has a neat and cowboy, wrangler, horse trainer. trim appearance. Overall: youthful, cowboy. Figure 1: (A) Literal avatar descriptions and (B) sentimental descriptions of four avatar properties, including possible occupations and interests. category (Farhadi et al., 2009; Mitchell et al., 2010; Matuszek et al., 2012). Here, we add a focus on sentimental visual language, which compactly describes more subjective properties such as if a person looks determined, if a resume looks professional, or if a restaurant looks romantic. Such models enable many new applications, such as text editors that automatically select properties including font, color, or text alignment to best match high level descriptions such as “professional” or “artistic.” 416 Proceedings of NAACL-HLT 2013, pages 416–425, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics In this paper, we study visual langua</context>
<context position="7729" citStr="Matuszek et al., 2012" startWordPosition="1166" endWordPosition="1169">05). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-generated avatars.3 Literal Descriptions We showed annotators a single image of clothing, a</context>
</contexts>
<marker>Matuszek, FitzGerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A Joint Model of Language and Perception for Grounded Attribute Learning. In Proc. of the 2012 International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Mcrae</author>
<author>George S Cree</author>
<author>Mark S Seidenberg</author>
<author>Chris Mcnorgan</author>
</authors>
<title>Semantic feature production norms for a large set of living and nonliving things.</title>
<date>2005</date>
<journal>Behavior Research Methods,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="3267" citStr="Mcrae et al., 2005" startWordPosition="481" endWordPosition="484">w applications, such as text editors that automatically select properties including font, color, or text alignment to best match high level descriptions such as “professional” or “artistic.” 416 Proceedings of NAACL-HLT 2013, pages 416–425, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics In this paper, we study visual language, both literal and sentimental, that describes the overall appearance and style of virtual characters, like those in Figure 1. We use literal language as feature norms, a tool used for studying semantic information in cognitive science (Mcrae et al., 2005). Literal words, such “black” or “hat,” are annotated for objects to indicate how people perceive visual properties. Such feature norms provide our gold-standard visual detectors, and allow us to focus on learning to model sentimental language, such as “youthful” or “goth.” We introduce a new corpus of descriptions of Xbox avatars created by actual gamers. Each avatar is specified by 19 attributes, including clothing and body type, allowing for more than 1020 possibilities. Using Amazon Mechanical Turk,1 we collected literal and sentimental descriptions of complete avatars and many of their co</context>
<context position="7110" citStr="Mcrae et al., 2005" startWordPosition="1069" endWordPosition="1072">rni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the goal of learning paraphrases while Zitnick and Parikh (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Mat</context>
</contexts>
<marker>Mcrae, Cree, Seidenberg, Mcnorgan, 2005</marker>
<rawString>Ken Mcrae, George S. Cree, Mark S. Seidenberg, and Chris Mcnorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavior Research Methods, 37(4):547–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Kees van Deemter</author>
<author>Ehud Reiter</author>
</authors>
<title>Natural reference to objects in a visual domain.</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th International Natural Language Generation Conference, INLG ’10,</booktitle>
<pages>95--104</pages>
<marker>Mitchell, van Deemter, Reiter, 2010</marker>
<rawString>Margaret Mitchell, Kees van Deemter, and Ehud Reiter. 2010. Natural reference to objects in a visual domain. In Proceedings of the 6th International Natural Language Generation Conference, INLG ’10, pages 95– 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="7811" citStr="Pang et al., 2002" startWordPosition="1180" endWordPosition="1183">e goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-generated avatars.3 Literal Descriptions We showed annotators a single image of clothing, a facial feature or an accessory and asked them to produce short descriptions. Figu</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
</authors>
<title>Automatic metaphor interpretation as a paraphrasing task. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>1029--1037</pages>
<contexts>
<context position="7916" citStr="Shutova, 2010" startWordPosition="1197" endWordPosition="1198">g to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-generated avatars.3 Literal Descriptions We showed annotators a single image of clothing, a facial feature or an accessory and asked them to produce short descriptions. Figure 2 shows the distribution over object types. We restricted descriptions to be between 3 and 15 words. I</context>
</contexts>
<marker>Shutova, 2010</marker>
<rawString>Ekaterina Shutova. 2010a. Automatic metaphor interpretation as a paraphrasing task. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 1029–1037.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
</authors>
<title>Models of metaphor in nlp.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>688--697</pages>
<contexts>
<context position="7916" citStr="Shutova, 2010" startWordPosition="1197" endWordPosition="1198">g to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-generated avatars.3 Literal Descriptions We showed annotators a single image of clothing, a facial feature or an accessory and asked them to produce short descriptions. Figure 2 shows the distribution over object types. We restricted descriptions to be between 3 and 15 words. I</context>
</contexts>
<marker>Shutova, 2010</marker>
<rawString>Ekaterina Shutova. 2010b. Models of metaphor in nlp. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 688–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne-Marie Tousch</author>
<author>Stphane Herbin</author>
<author>Jean-Yves Audibert</author>
</authors>
<title>Semantic hierarchies for image annotation: A survey.</title>
<date>2012</date>
<journal>Pattern Recognition,</journal>
<volume>45</volume>
<issue>1</issue>
<pages>345</pages>
<contexts>
<context position="5773" citStr="Tousch et al. (2012)" startWordPosition="866" endWordPosition="869">sentimental description. Finally, we use our model to build complete avatars 1www.mturk.com 2Data available at http://homes.cs.washington. edu/˜my89/avatar. and show that we can accurately predict the sentimental terms annotators ascribe to them. 2 Related Work To the best of our knowledge, our focus on learning to understand visual sentiment descriptions is novel. However, visual sentiment has been studied from other perspectives. Jrgensen (1998) provides examples which show that visual descriptions communicate social status and story information in addition to literal object and properties. Tousch et al. (2012) draw the distinction between “of-ness” (objective and concrete) and “about-ness” (subjective and abstract) in image retrieval, and observe that many image queries are abstract (for example, images about freedom). Finally, in descriptions of people undergoing emotional distress, Fussell and Moss (1998) show that literal descriptions co-occur frequently with sentimental ones. There has been significant work on more literal aspects of grounded language understanding, both visual and non-visual. The WordsEye project (Coyne and Sproat, 2001) generates 3D scenes from literal paragraph-length descri</context>
</contexts>
<marker>Tousch, Herbin, Audibert, 2012</marker>
<rawString>Anne-Marie Tousch, Stphane Herbin, and Jean-Yves Audibert. 2012. Semantic hierarchies for image annotation: A survey. Pattern Recognition, 45(1):333 – 345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
</authors>
<title>Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<marker>Yang, 2011</marker>
<rawString>Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zeng</author>
<author>M Pantic</author>
<author>G I Roisman</author>
<author>T S Huang</author>
</authors>
<title>A survey of affect recognition methods: Audio, visual, and spontaneous expressions. Pattern Analysis and Machine Intelligence,</title>
<date>2009</date>
<journal>IEEE Transactions on,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="7873" citStr="Zeng et al., 2009" startWordPosition="1190" endWordPosition="1193">use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game 417 Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-generated avatars.3 Literal Descriptions We showed annotators a single image of clothing, a facial feature or an accessory and asked them to produce short descriptions. Figure 2 shows the distribution over object types. We restricted d</context>
</contexts>
<marker>Zeng, Pantic, Roisman, Huang, 2009</marker>
<rawString>Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009. A survey of affect recognition methods: Audio, visual, and spontaneous expressions. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(1):39–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lawrence Zitnick</author>
<author>Devi Parikh</author>
</authors>
<title>Bringing semantics into focus using visual abstraction.</title>
<date>2013</date>
<journal>In Computer Vision and Pattern Recognition (To Appear).</journal>
<contexts>
<context position="6737" citStr="Zitnick and Parikh (2013)" startWordPosition="1006" endWordPosition="1009">ccur frequently with sentimental ones. There has been significant work on more literal aspects of grounded language understanding, both visual and non-visual. The WordsEye project (Coyne and Sproat, 2001) generates 3D scenes from literal paragraph-length descriptions. Generating literal textual descriptions of visual scenes has also been studied, including both captions (Kulkarni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the goal of learning paraphrases while Zitnick and Parikh (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branava</context>
</contexts>
<marker>Zitnick, Parikh, 2013</marker>
<rawString>C Lawrence Zitnick and Devi Parikh. 2013. Bringing semantics into focus using visual abstraction. In Computer Vision and Pattern Recognition (To Appear).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>