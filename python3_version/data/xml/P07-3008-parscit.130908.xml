<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.045830">
<title confidence="0.994082">
Limitations of Current Grammar Induction Algorithms
</title>
<author confidence="0.998204">
Bart Cramer
</author>
<affiliation confidence="0.992945666666667">
School of Behavioral and Cognitive Neurosciences
University of Groningen
Groningen, the Netherlands
</affiliation>
<email confidence="0.9962">
bart.cramer@gmail.com
</email>
<sectionHeader confidence="0.995613" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999803083333334">
I review a number of grammar induction
algorithms (ABL, Emile, Adios), and test
them on the Eindhoven corpus, resulting in
disappointing results, compared to the usu-
ally tested corpora (ATIS, OVIS). Also, I
show that using neither POS-tags induced
from Biemann’s unsupervised POS-tagging
algorithm nor hand-corrected POS-tags as
input improves this situation. Last, I argue
for the development of entirely incremental
grammar induction algorithms instead of the
approaches of the systems discussed before.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959057692308">
Grammar induction is a task within the field of nat-
ural language processing that attempts to construct a
grammar of a given language solely on the basis of
positive examples of this language. If a successful
method is found, this will have both practical appli-
cations and considerable theoretical implications.
Concerning the practical side, this will make the
engineering of NLP systems easier, especially for
less widely studied languages. One can conceive
successful GI algorithms as an inspiration for sta-
tistical machine translation systems.
Theoretically, grammar induction is important as
well. One of the main assertions in the nativist’s
position is the Poverty of the Stimulus argument,
which means that the child does not perceive enough
positive examples of language throughout his early
youth to have learned the grammar from his parents,
without the help of innate knowledge (or: Universal
Grammar), that severely constrains the number of
hypotheses (i.e. grammars) that he can learn. Proved
more strictly for formal grammars, Gold’s (1967)
work showed that one cannot learn any type of su-
perfinite grammar (e.g. regular languages, context-
free languages), if one only perceives (an unlim-
ited amount of) positive examples. After, say, n ex-
amples, there is always more than 1 grammar that
would be able to explain the seen examples, thus
these grammar might give different judgments on an
n + 1th example, of which it is impossible to say in
advance which judgment is the correct one.
But, given this is true, isn’t the grammar induction
pursuit deemed to fail? Not really. First, there are
hints that children do receive negative information,
and that they use it for grammar acquisition. Also,
the strictness required by Gold is not needed, and an
approximation in the framework of PAC (Probably
Approximately Correct) or VC (Vapnis and Cher-
vonenkis) could then suffice. This, and other argu-
ments favouring the use of machine learning tech-
niques in linguistic theory testing, are very well re-
viewed in Lappin and Shieber (2007).
Several attempts have been made to create such
systems. The authors of these systems reported
promising results on the ATIS and OVIS treebanks. I
tried to replicate these findings on the more compli-
cated Eindhoven treebank, which turned out to yield
disappointing results, even inferior to very simple
baselines. As an attempt to ameliorate this, and as
an attempt to confirm Klein and Manning’s (2002)
and Bod’s (2006) thesis that good enough unsuper-
vised POS-taggers exist to justify using POS-tags
instead of words in evaluating GI systems, I pre-
</bodyText>
<page confidence="0.996825">
43
</page>
<bodyText confidence="0.888396166666667">
Proceedings of the ACL 2007 Student Research Workshop, pages 43–48,
Prague, June 2007. c�2007 Association for Computational Linguistics
sented the algorithms with both POS-tags that were
induced from Biemann’s unsupervised POS-tagging
algorithm and hand-corrected POS-tags. This did
not lead to improvement.
</bodyText>
<sectionHeader confidence="0.649206" genericHeader="method">
2 Current Grammar Induction Models
</sectionHeader>
<subsectionHeader confidence="0.970337">
2.1 Algorithms
</subsectionHeader>
<bodyText confidence="0.999967236842105">
Grammar induction models can be split up into two
types: tag-based and word-based grammar induc-
tion. The key feature that distinguishes between
these two is the type of input. Tag-based systems
receive part-of-speech tags as their input (i.e. the
words are already labelled), and only induce rules
using the given tags. This kind of work is done
by, for instance, Klein and Manning (2005). On the
other hand, word-based models accept plain text as
its input, and have to extract both the categories and
the syntactic rules from given input.
Recently, several word-based grammar induction
algorithms have been developed: Alignment-Based
Learning (van Zaanen, 2002), Adios (Solan et al.,
2005), Emile (Adriaans, 1992; Adriaans and Ver-
voort, 2002) and GraSp1 (Henrichsen, 2002). Al-
though the means of computation and underlying
aims differ, they all rely to a certain extent on Har-
ris’ principle (1951): if two word groups constitute
the same category, then they can be interchanged in
any sentence, without damaging the grammaticality
of that sentence. Hence, these GI system depend on
the inverse: if two word groups appear to occur in
the same contexts, they probably possess the same
syntactic characteristics.
The most prominent example of this principle is
Alignment-Based Learning, or ABL, (van Zaanen,
2002). This algorithm consists of two stages. First,
all sentences are aligned such that it finds a shared
and a distinct part of all pairs of sentences, sug-
gesting that the distinct parts have the same type.
For example, consider the pair ‘I saw the man’ and
‘I saw John’. Here, ’John’ and ’the man’ are cor-
rectly identified as examples of the same type (NP’s
in this case). The second step, that takes the same
corpus as input, tries to identify the constituents in
that sentence. Because the generated constituents
found in the previous step might overlap, the correct
</bodyText>
<footnote confidence="0.958731">
1As there was no current working version of this system, I
did not include it in this project.
</footnote>
<tableCaption confidence="0.968231">
Table 1: An example of some context/expression
</tableCaption>
<bodyText confidence="0.999840578947369">
pairs to show the workings of EMILE. Note that, un-
der standard settings, a rule covering this entire table
will be inferred, causing a phrase like ‘John talks’ to
be accepted, although there was no such input sen-
tence.
ones have to be selected. Simple heuristics are used
to achieve this, for example to take the constituent
that was generated first (ABL-first) or to take the
constituent with the highest score on some proba-
bilistic function (ABL-leaf). For details, I refer to
van Zaanen (2000). Because ABL compares all sen-
tences in the corpus with all other sentences, the al-
gorithm is quadratic in the number of sentences, but
has low memory demands. Interestingly, ABL does
not come up with an explicit grammar, but generates
just a bracketed version of the corpus instead.
Adios (Solan et al., 2005) uses Harris’ principle
as well, although it attempts to create a grammar
(either context-free or context-sensitive) more ex-
plicitly. The algorithm represents language as a di-
rected pseudograph2, with equivalence classes (ini-
tially single words) as nodes. Input sentences can
be regarded as ‘snakes’ over the nodes in the graph.
If enough support is found, words are merged into
equivalence classes, or frequently occurring edges
are put in a path (a rule in usual grammatical terms).
This generalisation process is done iteratively, until
convergence is reached.
Emile (Adriaans, 1992; Adriaans and Vervoort,
2002) is the system that to a greater extent tries to
pinpoint its reasons to accept a linguistic hypothe-
sis. Each rule is divided into expressions and types,
where types should be the interchangeable part of
two sentences. Instead of explicitly comparing each
sentence with all other sentences, it incrementally
builds up a table of type/expression pairs, and on the
basis of this table rules are extracted. An example is
given in table 1. This incrementality has two major
</bodyText>
<footnote confidence="0.897849">
2This is a graph that allows for loops and multiple edges.
</footnote>
<figure confidence="0.998843785714286">
John
(.)
Pat
(.)
Jim
(.)
walks
talks
smiles
x
x
x
x
x x
</figure>
<page confidence="0.992376">
44
</page>
<bodyText confidence="0.75088075">
consequences: it makes the system vastly more effi-
cient in terms of time, at the cost of rising memory
demands, and it models time linearly, in contrast to
ABL and Adios.
</bodyText>
<subsectionHeader confidence="0.993905">
2.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999243128205128">
Different methods of evaluation are used in GI. One
of them is visual inspection (Henrichsen, 2002).
This is not a reproducible and independent evalua-
tion measure, and it does certainly not suffice as an
assessment of the quality of the results. However,
Roberts and Atwell (2003) argue that this evaluation
should still be included in GI discussions.
A second evaluation method is shown by Solan
et al. (2005), in which Adios had to carry out a test
that is available on the Internet: English as a Second
Language (ESL). This test shows three sentences, of
which the examinee has to say which sentence is the
grammatical one. Adios answers around 60% cor-
rect on these questions, which is considered as inter-
mediate for a person who has had 6 years of English
lessons. Although this sounds impressive, no exam-
ples of test sentences are given, and the website is
not available anymore, so we are not able to assess
this result.
A third option is to have sentences generated by
the induced grammar judged on their naturalness,
and compare this average with the average of the
sentences of the original corpus. Solan et al. (2005)
showed that the judgments of Adios generated sen-
tences were comparable to the sentences in their cor-
pus. However, the algorithm might just generates
overly simple utterances, and will receive relatively
high scores that it doesn’t deserve.
The last option for evaluation is to compare the
parses with hand-annotated treebanks. This gives
the most quantifiable and detailed view on the per-
formance of a GI system. An interesting compara-
tive study between Emile and ABL using this eval-
uation method is available in van Zaanen and Adri-
aans (2001) where F-scores of 41.4% (Emile) and
61.7% (ABL) are reported on the OVIS (Openbaar
Vervoer Informatie Systeem3; Dutch) corpus, and
25.4% and 39.2% on the ATIS (Air Traffic Informa-
tion System; English) corpus.
</bodyText>
<footnote confidence="0.900351">
3This acronym means Public Transport Information System.
</footnote>
<sectionHeader confidence="0.978696" genericHeader="method">
3 Experiment 1
</sectionHeader>
<subsectionHeader confidence="0.99125">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999978157894737">
A major choice in evaluating GI systems is to decide
which corpus to train the algorithm on. The cre-
ators of ABL and Emile chose to test on the ATIS
and OVIS corpus, which is, I believe, an unfortu-
nate choice. These corpora contain sentences that
are spoken to a computer, and represent a very lim-
ited subset of language. Deep recursion, one of the
aspects that is hard to catch in grammar induction,
does not occur often. The average sentence lengths
are 7.5 (ATIS) and 4.4 (OVIS). If we want to know
whether a system is truly capable of bootstrapping
knowledge about language, there is only one way to
test it: by using natural language that is unlimited
in its expressive power. Therefore, I will test ABL,
Adios and Emile on the Eindhoven corpus, that con-
tains 7K sentences, with an average length of ap-
proximately 20 tokens. This is, as far as I know, the
first attempt to train and test word-based GI algo-
rithms on such a complicated corpus.
</bodyText>
<subsectionHeader confidence="0.999012">
3.2 Method
</subsectionHeader>
<bodyText confidence="0.99985525">
The Eindhoven corpus has been automatically anno-
tated by Alpino (Bouma et al., 2000; van der Beek
et al., 2002), a wide-coverage hand-written parser
for Dutch, with around 90% dependency triple ac-
curacy. Afterwards, this treebank has been manu-
ally corrected. The treebank does not literally con-
tain trees, but graphs: some nodes can be copied, so
that linguistic structure can be analyzed in more de-
tail. However, by removing all double nodes it is still
possible to retrieve a list of bracket-tuples from these
graphs. The graphs are also non-concatenative,
meaning that a constituent can span word groups that
are not contiguous. Therefore, if a sentence contains
a constituent wi...wjwk...wl, with k − j &gt; 1, three
bracket-tuples are generated: (i, j), (k, l) and (i, l).
Evaluation of the algorithm is done according to
PARSEVAL, except for a few changes that are also
proposed by Klein and Manning (2002). The set of
bracket-pairs that is found in the Alpino treebank
are called facts, and those from a grammar induc-
tion algorithm predictions. The intersection of the
facts and predictions are called hits. From these we
can compute the unlabeled precision, recall and F-
score. The subtleties adopted from Klein and Man-
</bodyText>
<page confidence="0.998431">
45
</page>
<bodyText confidence="0.9999677">
ning are the following: constituents of length 0 or 1,
constituents that span the whole sentence and con-
stituents just excluding punctuation are not taken
into account, as these are obvious predictions.
Three baselines were created: an algorithm that
always branches left4, idem for right-branching and
an algorithm that performs binary branching on ran-
dom points in the sentence. Note that left-branching
and right-branching yield the maximum number of
predictions.
</bodyText>
<subsectionHeader confidence="0.794588">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.9999729">
From the results in table 2, it can be seen that ABL
scores best: it is the only one that is able to slightly
outperform the random baseline. This is surpris-
ing, because it is the least complicated system of the
three. Adios and Emile performed poorly. It ap-
pears that, with larger sentences, the search space
become too sparse to actually induce any meaning-
ful structure. This is expressed in the low number of
predictions per sentence that Adios (1.5) and Emile
(0.7) make. Adjusting support parameters, to make
the algorithm accept more hypotheses, did not have
the intended effect. Still, notice that Emile has a rel-
atively high precision.
In sum, none of the systems is convincingly able
to outperform the very simple baselines. Neither
did visual inspection give the impression that mean-
ingful information was derived. Therefore, it can
be concluded that current word-based GI algorithms
are not equipped to derive syntactic structure from
corpora as complicated as the Eindhoven corpus.
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="method">
4 Experiment 2
</sectionHeader>
<subsectionHeader confidence="0.980768">
4.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999906181818182">
The second experiment deals with the difference
between tag-based and word-based systems. Intu-
itively, the latter task seems to be more challenging.
Still, Klein and Manning (2002) and Bod (2006)
stick to tag-based models. Their argumentation is
twofold.
First, Bod assumes that unsupervised POS-
tagging can be done successfully, without explic-
itly showing results that can confirm this. Klein
and Manning did tag their text using a simple un-
supervised POS-tagging algorithm, and this mod-
</bodyText>
<footnote confidence="0.602699">
4For example: [ [ [ I saw ] the ] large ] house.
</footnote>
<bodyText confidence="0.999841642857143">
erately harmed their performance: their Context-
Constituent Model’s F-score on Wall Street Journal
text fell from 71.1% to 63.2%.
Second, Klein and Manning created context vec-
tors for a number of non-terminals (NP, VP, PP), and
extracted the two principal components from these
vectors. They did the same with contexts of con-
stituents and distituents. The distribution of these
vectors suggest that the non-terminals were easier
to distinguish from each other than the constituents
from the distituents, suggesting that POS-tagging is
easier than finding syntactic rules. However, this
result would be more convincing if this is true for
POS-tags as well.
</bodyText>
<subsectionHeader confidence="0.986607">
4.2 Method
</subsectionHeader>
<bodyText confidence="0.999969117647059">
In order to test the argument above, and as an at-
tempt to improve the results from the previous ex-
periment, POS-tags were induced using Biemann’s
unsupervised POS-tagger (Biemann, 2006). Be-
cause that algorithm needs at least 50M words to
work reliably, it was trained on the concatenation of
the Eindhoven corpus and the CLEF corpus (70M
words, also newspaper text). The tags of the Eind-
hoven corpus are then used as input for the GI al-
gorithms, both under same settings as experiment 1.
The evaluation was done the same way as in experi-
ment 1.
The same method was carried out using hand-
corrected tags. Large and equal improvements will
imply the justification for tag-based grammar in-
duction. If the models only improve on the hand-
corrected tags, this will suggest the opposite.
</bodyText>
<subsectionHeader confidence="0.949184">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999895076923077">
The results can be found in table 3. Generally, more
predictions were made with respect to experiment 1,
due to the denser search space. Only a convergence
to the baseline was achieved, especially by Adios
and Emile, that were very low in predictions in the
first experiment. Again, none of the tested systems
was able to clearly outperform the baselines.
Because using neither induced nor hand-corrected
made the systems work more reliably, there seems to
be no strong evidence in favor or against Bod’s and
Klein and Manning’s conjecture. Therefore, there is
no sound justification for tag-based grammar induc-
tion yet.
</bodyText>
<page confidence="0.998005">
46
</page>
<table confidence="0.999667625">
Method Hits/Predictions Precision Recall F-score
Left 5.8K / 119K 4.9% 9.2% 6.4%
Right 4.4K / 119K 3.6% 6.9% 4.8%
Random 11K / 93K 11.7% 17.3% 14.0%
ABL-leaf 4.0K / 24K 16.9% 6.4% 9.3%
ABL-first 13K / 113K 11.6% 20.8% 14.9%
Adios 319 / 11K 2.8% 0.5% 0.9%
Emile 912 / 5.2K 17.3% 1.5% 2.7%
</table>
<tableCaption confidence="0.933777">
Table 2: This table shows the results of experiment 1. Left, Right and Random are baseline scores. The two
variants of ABL differ in the selection phase. 62.9K facts were found in the Alpino treebank.
</tableCaption>
<table confidence="0.999440166666667">
Induced tags Hand-corrected tags
Method Hits/Pred.’s Precision Recall F-score Hits/Pred.’s Precision Recall F-score
ABL-leaf 5K / 30K 16.8% 8.1% 10.9% 7.0K / 34K 21.0% 11.2% 14.6%
ABL-first 11K / 125K 9.2% 18.2% 12.2% 12.6K / 123K 10.3% 20.0% 13.6%
Adios 2.7K / 24K 11.2% 4.3% 6.3% 2.2K / 20K 11.0% 3.5% 5.3%
Emile 1.8K / 16K 11.2% 2.9% 4.6% 1.7K / 19K 8.9% 2.7% 4.1%
</table>
<tableCaption confidence="0.998511">
Table 3: This table shows the results of experiment 2. The baseline scores are identical to the ones in
experiment 1.
</tableCaption>
<sectionHeader confidence="0.996055" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99993724">
The results from experiment 1 and 2 clearly show
that ABL, Adios and Emile have severe shortcom-
ings, and that they cannot derive meaningful struc-
ture from language as complicated as the Eindhoven
corpus. An important reason for this is that a cor-
pus with only 7K sentences is not able to sufficiently
cover the search space. This can be seen from the
very low number of predictions made by Adios and
Emile: there was not enough support to accept hy-
potheses.
But how should we proceed? Any algorithm
based solely on Harris’ principle can be either incre-
mental (Emile) or non-incremental (ABL, Adios).
The previous experiments show that very large cor-
pora are needed to mitigate the very sparse search
space, leading me to conclude that non-incremental
systems are not suitable for the problem of gram-
mar induction. Also, incremental systems have the
advantage of an intuitive notion of time: it is al-
ways clear which working hypothesis of a grammar
is maintained.
Emile retains a Boolean table with all combina-
tions of types and expressions it has encountered up
until a given moment. This means that very infre-
quent words demand a disproportionally large part
of the memory. Therefore, all found words and rules
should be divided into three groups: pivotal, nor-
mal and infrequent. Initially, all encountered words
are infrequent. Transitions to the normal and piv-
otal stage occur when an estimator of the relative
frequency is high enough, for example by taking the
lower bound of the confidence interval (Mikheev,
1997). Ultimately, the number of words in the nor-
mal and pivotal stage will converge to a constant.
For example, if the relative frequency of a word
should be larger than 0.01 to become pivotal, there
can only be 100 of these words. Because one can
define upper limits for pivotal and normal words,
the size of the bookkeeping table is limited as well.
Also, when the system starts inducing syntactic cate-
gories of words, very infrequent words should not be
parsed as a separate category initially, but as a mem-
ber of another open-class category. This connects to
the cross-linguistic tendency that infrequent words
generally have simple complementation patterns.
One very important question remains: what in-
tuitions should this imaginary system use to induce
rules? First, all sentences should be sorted by length.
Then, for each sentence, the following steps are
taken:
</bodyText>
<page confidence="0.99645">
47
</page>
<listItem confidence="0.962317">
• Update the bookkeeping tables.
• Parse the sentence as deeply as possible.
• If the sentence cannot be parsed completely,
induce all possible rules that would make the
parse complete. Add all these rules to the book-
keeping tables.
</listItem>
<bodyText confidence="0.999926722222222">
The last step deserves some extra attention. If
the algorithm encounters the sentence ‘he is such a
(.)’, we can safely infer that the unknown word at
(.) is a noun. Inducing complementation patterns
should be possible as well. Imagine that the algo-
rithm understands NP’s and transitive verbs. Then
consider the following: ‘John gave Tim a book’.
It will parse ‘John gave Tim’ as a sentence, and ‘a
book’ as a noun phrase. Because these two should
be connected, a number of hypotheses are generated,
for example: ‘a book’ is a complement of ‘Tim’; ‘a
book’ is a complement of ‘John gave Tim’; ‘a book’
is a second complement of ‘gave’. Naturally, only
the last hypothesis is correct. All three inductions
are included, but only the last is likely to be repro-
duced in later sentences in the corpus, because sen-
tences of the form ‘(.) gave (.) (.)’ are more likely
than ‘John gave Tim (.)’ and ‘Tim (.)’.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="conclusions">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999790333333333">
I would like to thank Jennifer Spenader, Gertjan van
Noord and the anonymous reviewers for providing
me their invaluable comments.
</bodyText>
<sectionHeader confidence="0.997959" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998037984615385">
Pieter W. Adriaans and Mark R. Vervoort. 2002. The
EMILE 4.1 grammar induction toolbox. In Proceed-
ings of the 6th International Colloquium on Gram-
mar Induction (ICGI), pages 293–295, Amsterdam,
the Netherlands.
Pieter W. Adriaans. 1992. Language learningfrom a cat-
egorial perspective. Ph.D. thesis, University of Ams-
terdam, NL.
Chris Biemann. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In Proceed-
ings of ACL/COLING-2006 Students Research Work-
shop, pages 7–12, Sydney, Australia.
Rens Bod. 2006. An all-subtrees approach to unsuper-
vised parsing. In Proceedings ofACL/COLING-2006,
pages 865–872, Sydney, Australia.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2000. Alpino: wide-coverage computational analysis
of Dutch. In Proceedings of Computational Linguis-
tics in the Netherlands (CLIN), pages 45–59, Tilburg,
the Netherlands.
E. Mark Gold. 1967. Language identification in the
limit. Information and Control, 16:447–474.
Zellig S. Harris. 1951. Methods in Structural Linguis-
tics. University of Chicago Press, Chicago.
Peter J. Henrichsen. 2002. GraSp: Grammar learning
from unlabelled speech corpora. In Proceedings of
CoNLL-2002, pages 22–28, Pennsylvania, PA, USA.
Dan Klein and Christopher D. Manning. 2002. A gener-
ative Constituent-Context Model for improved gram-
mar induction. In Proceedings of ACL-2001, pages
128–135, Toulouse, France.
Dan Klein and Christopher D. Manning. 2005. Nat-
ural language grammar induction with a genera-
tive constituent-context model. Pattern Recognition,
9(38):1407–1419.
Shalom Lappin and Stuart M. Shieber. 2007. Machine
learning theory and practice as a source of insight into
universal grammar. Computational Linguistics, 43:1–
34.
Andrei Mikheev. 1997. Automatic rule induction for
unknown-word guessing. Computational Linguistics,
23(3):405–423.
Andrew Roberts and Eric Atwell. 2003. The use of cor-
pora for automatic evaluation of grammar inference
systems. In Proceedings of the Corpus Linguistics
2003 conference, pages 657–661, Lancaster, United
Kingdom.
Zach Solan, David Horn, Eytan Ruppin, and Shimon
Edelman. 2005. Unsupervised learning of natural lan-
guages. Proceedings of the National Academy of Sci-
ences, 102(33):11629–11634.
Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Proceedings of Computational Lin-
guistics in the Netherlands (CLIN) 2001, pages 8–22,
Enschede, the Netherlands.
Menno van Zaanen and Pieter W. Adriaans. 2001.
Alignment-Based Learning versus EMILE: A compar-
ison. In Proceedings of the 13th Dutch-Belgian Artifi-
cial Intelligence Conference (BNAIC), pages 315–322,
Amsterdam, the Netherlands.
Menno van Zaanen. 2002. Implementing Alignment-
Based Learning. In Proceedings of the 6th Interna-
tional Colloquium on Grammatical Inference (ICGI),
pages 312–314, Amsterdam, the Netherlands.
</reference>
<page confidence="0.999341">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.857241">
<title confidence="0.999742">Limitations of Current Grammar Induction Algorithms</title>
<author confidence="0.999982">Bart Cramer</author>
<affiliation confidence="0.999835">School of Behavioral and Cognitive Neurosciences University of Groningen</affiliation>
<address confidence="0.87261">Groningen, the Netherlands</address>
<email confidence="0.999896">bart.cramer@gmail.com</email>
<abstract confidence="0.998651076923077">I review a number of grammar induction algorithms (ABL, Emile, Adios), and test them on the Eindhoven corpus, resulting in disappointing results, compared to the usually tested corpora (ATIS, OVIS). Also, I show that using neither POS-tags induced from Biemann’s unsupervised POS-tagging algorithm nor hand-corrected POS-tags as input improves this situation. Last, I argue for the development of entirely incremental grammar induction algorithms instead of the approaches of the systems discussed before.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pieter W Adriaans</author>
<author>Mark R Vervoort</author>
</authors>
<title>The EMILE 4.1 grammar induction toolbox.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th International Colloquium on Grammar Induction (ICGI),</booktitle>
<pages>293--295</pages>
<location>Amsterdam, the Netherlands.</location>
<contexts>
<context position="4413" citStr="Adriaans and Vervoort, 2002" startWordPosition="676" endWordPosition="680"> that distinguishes between these two is the type of input. Tag-based systems receive part-of-speech tags as their input (i.e. the words are already labelled), and only induce rules using the given tags. This kind of work is done by, for instance, Klein and Manning (2005). On the other hand, word-based models accept plain text as its input, and have to extract both the categories and the syntactic rules from given input. Recently, several word-based grammar induction algorithms have been developed: Alignment-Based Learning (van Zaanen, 2002), Adios (Solan et al., 2005), Emile (Adriaans, 1992; Adriaans and Vervoort, 2002) and GraSp1 (Henrichsen, 2002). Although the means of computation and underlying aims differ, they all rely to a certain extent on Harris’ principle (1951): if two word groups constitute the same category, then they can be interchanged in any sentence, without damaging the grammaticality of that sentence. Hence, these GI system depend on the inverse: if two word groups appear to occur in the same contexts, they probably possess the same syntactic characteristics. The most prominent example of this principle is Alignment-Based Learning, or ABL, (van Zaanen, 2002). This algorithm consists of two</context>
<context position="7111" citStr="Adriaans and Vervoort, 2002" startWordPosition="1122" endWordPosition="1125">et al., 2005) uses Harris’ principle as well, although it attempts to create a grammar (either context-free or context-sensitive) more explicitly. The algorithm represents language as a directed pseudograph2, with equivalence classes (initially single words) as nodes. Input sentences can be regarded as ‘snakes’ over the nodes in the graph. If enough support is found, words are merged into equivalence classes, or frequently occurring edges are put in a path (a rule in usual grammatical terms). This generalisation process is done iteratively, until convergence is reached. Emile (Adriaans, 1992; Adriaans and Vervoort, 2002) is the system that to a greater extent tries to pinpoint its reasons to accept a linguistic hypothesis. Each rule is divided into expressions and types, where types should be the interchangeable part of two sentences. Instead of explicitly comparing each sentence with all other sentences, it incrementally builds up a table of type/expression pairs, and on the basis of this table rules are extracted. An example is given in table 1. This incrementality has two major 2This is a graph that allows for loops and multiple edges. John (.) Pat (.) Jim (.) walks talks smiles x x x x x x 44 consequences</context>
</contexts>
<marker>Adriaans, Vervoort, 2002</marker>
<rawString>Pieter W. Adriaans and Mark R. Vervoort. 2002. The EMILE 4.1 grammar induction toolbox. In Proceedings of the 6th International Colloquium on Grammar Induction (ICGI), pages 293–295, Amsterdam, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pieter W Adriaans</author>
</authors>
<title>Language learningfrom a categorial perspective.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Amsterdam, NL.</institution>
<contexts>
<context position="4383" citStr="Adriaans, 1992" startWordPosition="674" endWordPosition="675"> The key feature that distinguishes between these two is the type of input. Tag-based systems receive part-of-speech tags as their input (i.e. the words are already labelled), and only induce rules using the given tags. This kind of work is done by, for instance, Klein and Manning (2005). On the other hand, word-based models accept plain text as its input, and have to extract both the categories and the syntactic rules from given input. Recently, several word-based grammar induction algorithms have been developed: Alignment-Based Learning (van Zaanen, 2002), Adios (Solan et al., 2005), Emile (Adriaans, 1992; Adriaans and Vervoort, 2002) and GraSp1 (Henrichsen, 2002). Although the means of computation and underlying aims differ, they all rely to a certain extent on Harris’ principle (1951): if two word groups constitute the same category, then they can be interchanged in any sentence, without damaging the grammaticality of that sentence. Hence, these GI system depend on the inverse: if two word groups appear to occur in the same contexts, they probably possess the same syntactic characteristics. The most prominent example of this principle is Alignment-Based Learning, or ABL, (van Zaanen, 2002). </context>
<context position="7081" citStr="Adriaans, 1992" startWordPosition="1120" endWordPosition="1121">d. Adios (Solan et al., 2005) uses Harris’ principle as well, although it attempts to create a grammar (either context-free or context-sensitive) more explicitly. The algorithm represents language as a directed pseudograph2, with equivalence classes (initially single words) as nodes. Input sentences can be regarded as ‘snakes’ over the nodes in the graph. If enough support is found, words are merged into equivalence classes, or frequently occurring edges are put in a path (a rule in usual grammatical terms). This generalisation process is done iteratively, until convergence is reached. Emile (Adriaans, 1992; Adriaans and Vervoort, 2002) is the system that to a greater extent tries to pinpoint its reasons to accept a linguistic hypothesis. Each rule is divided into expressions and types, where types should be the interchangeable part of two sentences. Instead of explicitly comparing each sentence with all other sentences, it incrementally builds up a table of type/expression pairs, and on the basis of this table rules are extracted. An example is given in table 1. This incrementality has two major 2This is a graph that allows for loops and multiple edges. John (.) Pat (.) Jim (.) walks talks smil</context>
</contexts>
<marker>Adriaans, 1992</marker>
<rawString>Pieter W. Adriaans. 1992. Language learningfrom a categorial perspective. Ph.D. thesis, University of Amsterdam, NL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Unsupervised part-of-speech tagging employing efficient graph clustering.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL/COLING-2006 Students Research Workshop,</booktitle>
<pages>7--12</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="14928" citStr="Biemann, 2006" startWordPosition="2431" endWordPosition="2432">the two principal components from these vectors. They did the same with contexts of constituents and distituents. The distribution of these vectors suggest that the non-terminals were easier to distinguish from each other than the constituents from the distituents, suggesting that POS-tagging is easier than finding syntactic rules. However, this result would be more convincing if this is true for POS-tags as well. 4.2 Method In order to test the argument above, and as an attempt to improve the results from the previous experiment, POS-tags were induced using Biemann’s unsupervised POS-tagger (Biemann, 2006). Because that algorithm needs at least 50M words to work reliably, it was trained on the concatenation of the Eindhoven corpus and the CLEF corpus (70M words, also newspaper text). The tags of the Eindhoven corpus are then used as input for the GI algorithms, both under same settings as experiment 1. The evaluation was done the same way as in experiment 1. The same method was carried out using handcorrected tags. Large and equal improvements will imply the justification for tag-based grammar induction. If the models only improve on the handcorrected tags, this will suggest the opposite. 4.3 R</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann. 2006. Unsupervised part-of-speech tagging employing efficient graph clustering. In Proceedings of ACL/COLING-2006 Students Research Workshop, pages 7–12, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An all-subtrees approach to unsupervised parsing.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL/COLING-2006,</booktitle>
<pages>865--872</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="13736" citStr="Bod (2006)" startWordPosition="2240" endWordPosition="2241">e has a relatively high precision. In sum, none of the systems is convincingly able to outperform the very simple baselines. Neither did visual inspection give the impression that meaningful information was derived. Therefore, it can be concluded that current word-based GI algorithms are not equipped to derive syntactic structure from corpora as complicated as the Eindhoven corpus. 4 Experiment 2 4.1 Motivation The second experiment deals with the difference between tag-based and word-based systems. Intuitively, the latter task seems to be more challenging. Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. Their argumentation is twofold. First, Bod assumes that unsupervised POStagging can be done successfully, without explicitly showing results that can confirm this. Klein and Manning did tag their text using a simple unsupervised POS-tagging algorithm, and this mod4For example: [ [ [ I saw ] the ] large ] house. erately harmed their performance: their ContextConstituent Model’s F-score on Wall Street Journal text fell from 71.1% to 63.2%. Second, Klein and Manning created context vectors for a number of non-terminals (NP, VP, PP), and extracted the two principal comp</context>
</contexts>
<marker>Bod, 2006</marker>
<rawString>Rens Bod. 2006. An all-subtrees approach to unsupervised parsing. In Proceedings ofACL/COLING-2006, pages 865–872, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
<author>Gertjan van Noord</author>
<author>Robert Malouf</author>
</authors>
<title>Alpino: wide-coverage computational analysis of Dutch.</title>
<date>2000</date>
<booktitle>In Proceedings of Computational Linguistics in the Netherlands (CLIN),</booktitle>
<pages>45--59</pages>
<location>Tilburg, the Netherlands.</location>
<marker>Bouma, van Noord, Malouf, 2000</marker>
<rawString>Gosse Bouma, Gertjan van Noord, and Robert Malouf. 2000. Alpino: wide-coverage computational analysis of Dutch. In Proceedings of Computational Linguistics in the Netherlands (CLIN), pages 45–59, Tilburg, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mark Gold</author>
</authors>
<date>1967</date>
<booktitle>Language identification in the limit. Information and Control,</booktitle>
<pages>16--447</pages>
<marker>Gold, 1967</marker>
<rawString>E. Mark Gold. 1967. Language identification in the limit. Information and Control, 16:447–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>Methods in Structural Linguistics.</title>
<date>1951</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<marker>Harris, 1951</marker>
<rawString>Zellig S. Harris. 1951. Methods in Structural Linguistics. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter J Henrichsen</author>
</authors>
<title>GraSp: Grammar learning from unlabelled speech corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>22--28</pages>
<location>Pennsylvania, PA, USA.</location>
<contexts>
<context position="4443" citStr="Henrichsen, 2002" startWordPosition="683" endWordPosition="684">the type of input. Tag-based systems receive part-of-speech tags as their input (i.e. the words are already labelled), and only induce rules using the given tags. This kind of work is done by, for instance, Klein and Manning (2005). On the other hand, word-based models accept plain text as its input, and have to extract both the categories and the syntactic rules from given input. Recently, several word-based grammar induction algorithms have been developed: Alignment-Based Learning (van Zaanen, 2002), Adios (Solan et al., 2005), Emile (Adriaans, 1992; Adriaans and Vervoort, 2002) and GraSp1 (Henrichsen, 2002). Although the means of computation and underlying aims differ, they all rely to a certain extent on Harris’ principle (1951): if two word groups constitute the same category, then they can be interchanged in any sentence, without damaging the grammaticality of that sentence. Hence, these GI system depend on the inverse: if two word groups appear to occur in the same contexts, they probably possess the same syntactic characteristics. The most prominent example of this principle is Alignment-Based Learning, or ABL, (van Zaanen, 2002). This algorithm consists of two stages. First, all sentences </context>
<context position="7984" citStr="Henrichsen, 2002" startWordPosition="1278" endWordPosition="1279">sentence with all other sentences, it incrementally builds up a table of type/expression pairs, and on the basis of this table rules are extracted. An example is given in table 1. This incrementality has two major 2This is a graph that allows for loops and multiple edges. John (.) Pat (.) Jim (.) walks talks smiles x x x x x x 44 consequences: it makes the system vastly more efficient in terms of time, at the cost of rising memory demands, and it models time linearly, in contrast to ABL and Adios. 2.2 Evaluation Different methods of evaluation are used in GI. One of them is visual inspection (Henrichsen, 2002). This is not a reproducible and independent evaluation measure, and it does certainly not suffice as an assessment of the quality of the results. However, Roberts and Atwell (2003) argue that this evaluation should still be included in GI discussions. A second evaluation method is shown by Solan et al. (2005), in which Adios had to carry out a test that is available on the Internet: English as a Second Language (ESL). This test shows three sentences, of which the examinee has to say which sentence is the grammatical one. Adios answers around 60% correct on these questions, which is considered</context>
</contexts>
<marker>Henrichsen, 2002</marker>
<rawString>Peter J. Henrichsen. 2002. GraSp: Grammar learning from unlabelled speech corpora. In Proceedings of CoNLL-2002, pages 22–28, Pennsylvania, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative Constituent-Context Model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-2001,</booktitle>
<pages>128--135</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="11721" citStr="Klein and Manning (2002)" startWordPosition="1914" endWordPosition="1917">rally contain trees, but graphs: some nodes can be copied, so that linguistic structure can be analyzed in more detail. However, by removing all double nodes it is still possible to retrieve a list of bracket-tuples from these graphs. The graphs are also non-concatenative, meaning that a constituent can span word groups that are not contiguous. Therefore, if a sentence contains a constituent wi...wjwk...wl, with k − j &gt; 1, three bracket-tuples are generated: (i, j), (k, l) and (i, l). Evaluation of the algorithm is done according to PARSEVAL, except for a few changes that are also proposed by Klein and Manning (2002). The set of bracket-pairs that is found in the Alpino treebank are called facts, and those from a grammar induction algorithm predictions. The intersection of the facts and predictions are called hits. From these we can compute the unlabeled precision, recall and Fscore. The subtleties adopted from Klein and Man45 ning are the following: constituents of length 0 or 1, constituents that span the whole sentence and constituents just excluding punctuation are not taken into account, as these are obvious predictions. Three baselines were created: an algorithm that always branches left4, idem for </context>
<context position="13721" citStr="Klein and Manning (2002)" startWordPosition="2235" endWordPosition="2238">fect. Still, notice that Emile has a relatively high precision. In sum, none of the systems is convincingly able to outperform the very simple baselines. Neither did visual inspection give the impression that meaningful information was derived. Therefore, it can be concluded that current word-based GI algorithms are not equipped to derive syntactic structure from corpora as complicated as the Eindhoven corpus. 4 Experiment 2 4.1 Motivation The second experiment deals with the difference between tag-based and word-based systems. Intuitively, the latter task seems to be more challenging. Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. Their argumentation is twofold. First, Bod assumes that unsupervised POStagging can be done successfully, without explicitly showing results that can confirm this. Klein and Manning did tag their text using a simple unsupervised POS-tagging algorithm, and this mod4For example: [ [ [ I saw ] the ] large ] house. erately harmed their performance: their ContextConstituent Model’s F-score on Wall Street Journal text fell from 71.1% to 63.2%. Second, Klein and Manning created context vectors for a number of non-terminals (NP, VP, PP), and extracted the two</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. A generative Constituent-Context Model for improved grammar induction. In Proceedings of ACL-2001, pages 128–135, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Natural language grammar induction with a generative constituent-context model.</title>
<date>2005</date>
<journal>Pattern Recognition,</journal>
<volume>9</volume>
<issue>38</issue>
<contexts>
<context position="4057" citStr="Klein and Manning (2005)" startWordPosition="624" endWordPosition="627">l Linguistics sented the algorithms with both POS-tags that were induced from Biemann’s unsupervised POS-tagging algorithm and hand-corrected POS-tags. This did not lead to improvement. 2 Current Grammar Induction Models 2.1 Algorithms Grammar induction models can be split up into two types: tag-based and word-based grammar induction. The key feature that distinguishes between these two is the type of input. Tag-based systems receive part-of-speech tags as their input (i.e. the words are already labelled), and only induce rules using the given tags. This kind of work is done by, for instance, Klein and Manning (2005). On the other hand, word-based models accept plain text as its input, and have to extract both the categories and the syntactic rules from given input. Recently, several word-based grammar induction algorithms have been developed: Alignment-Based Learning (van Zaanen, 2002), Adios (Solan et al., 2005), Emile (Adriaans, 1992; Adriaans and Vervoort, 2002) and GraSp1 (Henrichsen, 2002). Although the means of computation and underlying aims differ, they all rely to a certain extent on Harris’ principle (1951): if two word groups constitute the same category, then they can be interchanged in any s</context>
</contexts>
<marker>Klein, Manning, 2005</marker>
<rawString>Dan Klein and Christopher D. Manning. 2005. Natural language grammar induction with a generative constituent-context model. Pattern Recognition, 9(38):1407–1419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
<author>Stuart M Shieber</author>
</authors>
<title>Machine learning theory and practice as a source of insight into universal grammar.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>43</volume>
<pages>34</pages>
<contexts>
<context position="2757" citStr="Lappin and Shieber (2007)" startWordPosition="422" endWordPosition="425">, of which it is impossible to say in advance which judgment is the correct one. But, given this is true, isn’t the grammar induction pursuit deemed to fail? Not really. First, there are hints that children do receive negative information, and that they use it for grammar acquisition. Also, the strictness required by Gold is not needed, and an approximation in the framework of PAC (Probably Approximately Correct) or VC (Vapnis and Chervonenkis) could then suffice. This, and other arguments favouring the use of machine learning techniques in linguistic theory testing, are very well reviewed in Lappin and Shieber (2007). Several attempts have been made to create such systems. The authors of these systems reported promising results on the ATIS and OVIS treebanks. I tried to replicate these findings on the more complicated Eindhoven treebank, which turned out to yield disappointing results, even inferior to very simple baselines. As an attempt to ameliorate this, and as an attempt to confirm Klein and Manning’s (2002) and Bod’s (2006) thesis that good enough unsupervised POS-taggers exist to justify using POS-tags instead of words in evaluating GI systems, I pre43 Proceedings of the ACL 2007 Student Research W</context>
</contexts>
<marker>Lappin, Shieber, 2007</marker>
<rawString>Shalom Lappin and Stuart M. Shieber. 2007. Machine learning theory and practice as a source of insight into universal grammar. Computational Linguistics, 43:1– 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
</authors>
<title>Automatic rule induction for unknown-word guessing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="18667" citStr="Mikheev, 1997" startWordPosition="3072" endWordPosition="3073">ear which working hypothesis of a grammar is maintained. Emile retains a Boolean table with all combinations of types and expressions it has encountered up until a given moment. This means that very infrequent words demand a disproportionally large part of the memory. Therefore, all found words and rules should be divided into three groups: pivotal, normal and infrequent. Initially, all encountered words are infrequent. Transitions to the normal and pivotal stage occur when an estimator of the relative frequency is high enough, for example by taking the lower bound of the confidence interval (Mikheev, 1997). Ultimately, the number of words in the normal and pivotal stage will converge to a constant. For example, if the relative frequency of a word should be larger than 0.01 to become pivotal, there can only be 100 of these words. Because one can define upper limits for pivotal and normal words, the size of the bookkeeping table is limited as well. Also, when the system starts inducing syntactic categories of words, very infrequent words should not be parsed as a separate category initially, but as a member of another open-class category. This connects to the cross-linguistic tendency that infreq</context>
</contexts>
<marker>Mikheev, 1997</marker>
<rawString>Andrei Mikheev. 1997. Automatic rule induction for unknown-word guessing. Computational Linguistics, 23(3):405–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Roberts</author>
<author>Eric Atwell</author>
</authors>
<title>The use of corpora for automatic evaluation of grammar inference systems.</title>
<date>2003</date>
<booktitle>In Proceedings of the Corpus Linguistics 2003 conference,</booktitle>
<pages>657--661</pages>
<location>Lancaster, United Kingdom.</location>
<contexts>
<context position="8165" citStr="Roberts and Atwell (2003)" startWordPosition="1306" endWordPosition="1309">ble 1. This incrementality has two major 2This is a graph that allows for loops and multiple edges. John (.) Pat (.) Jim (.) walks talks smiles x x x x x x 44 consequences: it makes the system vastly more efficient in terms of time, at the cost of rising memory demands, and it models time linearly, in contrast to ABL and Adios. 2.2 Evaluation Different methods of evaluation are used in GI. One of them is visual inspection (Henrichsen, 2002). This is not a reproducible and independent evaluation measure, and it does certainly not suffice as an assessment of the quality of the results. However, Roberts and Atwell (2003) argue that this evaluation should still be included in GI discussions. A second evaluation method is shown by Solan et al. (2005), in which Adios had to carry out a test that is available on the Internet: English as a Second Language (ESL). This test shows three sentences, of which the examinee has to say which sentence is the grammatical one. Adios answers around 60% correct on these questions, which is considered as intermediate for a person who has had 6 years of English lessons. Although this sounds impressive, no examples of test sentences are given, and the website is not available anym</context>
</contexts>
<marker>Roberts, Atwell, 2003</marker>
<rawString>Andrew Roberts and Eric Atwell. 2003. The use of corpora for automatic evaluation of grammar inference systems. In Proceedings of the Corpus Linguistics 2003 conference, pages 657–661, Lancaster, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zach Solan</author>
<author>David Horn</author>
<author>Eytan Ruppin</author>
<author>Shimon Edelman</author>
</authors>
<title>Unsupervised learning of natural languages.</title>
<date>2005</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>102</volume>
<issue>33</issue>
<contexts>
<context position="4360" citStr="Solan et al., 2005" startWordPosition="669" endWordPosition="672">ord-based grammar induction. The key feature that distinguishes between these two is the type of input. Tag-based systems receive part-of-speech tags as their input (i.e. the words are already labelled), and only induce rules using the given tags. This kind of work is done by, for instance, Klein and Manning (2005). On the other hand, word-based models accept plain text as its input, and have to extract both the categories and the syntactic rules from given input. Recently, several word-based grammar induction algorithms have been developed: Alignment-Based Learning (van Zaanen, 2002), Adios (Solan et al., 2005), Emile (Adriaans, 1992; Adriaans and Vervoort, 2002) and GraSp1 (Henrichsen, 2002). Although the means of computation and underlying aims differ, they all rely to a certain extent on Harris’ principle (1951): if two word groups constitute the same category, then they can be interchanged in any sentence, without damaging the grammaticality of that sentence. Hence, these GI system depend on the inverse: if two word groups appear to occur in the same contexts, they probably possess the same syntactic characteristics. The most prominent example of this principle is Alignment-Based Learning, or AB</context>
<context position="6496" citStr="Solan et al., 2005" startWordPosition="1029" endWordPosition="1032">ere was no such input sentence. ones have to be selected. Simple heuristics are used to achieve this, for example to take the constituent that was generated first (ABL-first) or to take the constituent with the highest score on some probabilistic function (ABL-leaf). For details, I refer to van Zaanen (2000). Because ABL compares all sentences in the corpus with all other sentences, the algorithm is quadratic in the number of sentences, but has low memory demands. Interestingly, ABL does not come up with an explicit grammar, but generates just a bracketed version of the corpus instead. Adios (Solan et al., 2005) uses Harris’ principle as well, although it attempts to create a grammar (either context-free or context-sensitive) more explicitly. The algorithm represents language as a directed pseudograph2, with equivalence classes (initially single words) as nodes. Input sentences can be regarded as ‘snakes’ over the nodes in the graph. If enough support is found, words are merged into equivalence classes, or frequently occurring edges are put in a path (a rule in usual grammatical terms). This generalisation process is done iteratively, until convergence is reached. Emile (Adriaans, 1992; Adriaans and </context>
<context position="8295" citStr="Solan et al. (2005)" startWordPosition="1328" endWordPosition="1331"> smiles x x x x x x 44 consequences: it makes the system vastly more efficient in terms of time, at the cost of rising memory demands, and it models time linearly, in contrast to ABL and Adios. 2.2 Evaluation Different methods of evaluation are used in GI. One of them is visual inspection (Henrichsen, 2002). This is not a reproducible and independent evaluation measure, and it does certainly not suffice as an assessment of the quality of the results. However, Roberts and Atwell (2003) argue that this evaluation should still be included in GI discussions. A second evaluation method is shown by Solan et al. (2005), in which Adios had to carry out a test that is available on the Internet: English as a Second Language (ESL). This test shows three sentences, of which the examinee has to say which sentence is the grammatical one. Adios answers around 60% correct on these questions, which is considered as intermediate for a person who has had 6 years of English lessons. Although this sounds impressive, no examples of test sentences are given, and the website is not available anymore, so we are not able to assess this result. A third option is to have sentences generated by the induced grammar judged on thei</context>
</contexts>
<marker>Solan, Horn, Ruppin, Edelman, 2005</marker>
<rawString>Zach Solan, David Horn, Eytan Ruppin, and Shimon Edelman. 2005. Unsupervised learning of natural languages. Proceedings of the National Academy of Sciences, 102(33):11629–11634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonoor van der Beek</author>
<author>Gosse Bouma</author>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>The Alpino dependency treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of Computational Linguistics in the Netherlands (CLIN)</booktitle>
<pages>8--22</pages>
<location>Enschede, the Netherlands.</location>
<marker>van der Beek, Bouma, Malouf, van Noord, 2002</marker>
<rawString>Leonoor van der Beek, Gosse Bouma, Robert Malouf, and Gertjan van Noord. 2002. The Alpino dependency treebank. In Proceedings of Computational Linguistics in the Netherlands (CLIN) 2001, pages 8–22, Enschede, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Menno van Zaanen</author>
<author>Pieter W Adriaans</author>
</authors>
<title>Alignment-Based Learning versus EMILE: A comparison.</title>
<date>2001</date>
<booktitle>In Proceedings of the 13th Dutch-Belgian Artificial Intelligence Conference (BNAIC),</booktitle>
<pages>315--322</pages>
<location>Amsterdam, the Netherlands.</location>
<marker>van Zaanen, Adriaans, 2001</marker>
<rawString>Menno van Zaanen and Pieter W. Adriaans. 2001. Alignment-Based Learning versus EMILE: A comparison. In Proceedings of the 13th Dutch-Belgian Artificial Intelligence Conference (BNAIC), pages 315–322, Amsterdam, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Menno van Zaanen</author>
</authors>
<title>Implementing AlignmentBased Learning.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th International Colloquium on Grammatical Inference (ICGI),</booktitle>
<pages>312--314</pages>
<location>Amsterdam, the Netherlands.</location>
<marker>van Zaanen, 2002</marker>
<rawString>Menno van Zaanen. 2002. Implementing AlignmentBased Learning. In Proceedings of the 6th International Colloquium on Grammatical Inference (ICGI), pages 312–314, Amsterdam, the Netherlands.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>