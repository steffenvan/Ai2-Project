<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008312">
<title confidence="0.868159">
Comparing Automatic Evaluation Measures for Image Description
</title>
<author confidence="0.992344">
Desmond Elliott and Frank Keller
</author>
<affiliation confidence="0.999543">
Institute for Language, Cognition, and Computation
School of Informatics, University of Edinburgh
</affiliation>
<email confidence="0.980501">
d.elliott@ed.ac.uk, keller@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997288" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999841588235294">
Image description is a new natural lan-
guage generation task, where the aim is to
generate a human-like description of an im-
age. The evaluation of computer-generated
text is a notoriously difficult problem, how-
ever, the quality of image descriptions has
typically been measured using unigram
BLEU and human judgements. The focus
of this paper is to determine the correlation
of automatic measures with human judge-
ments for this task. We estimate the correla-
tion of unigram and Smoothed BLEU, TER,
ROUGE-SU4, and Meteor against human
judgements on two data sets. The main
finding is that unigram BLEU has a weak
correlation, and Meteor has the strongest
correlation with human judgements.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996334">
Recent advances in computer vision and natural
language processing have led to an upsurge of re-
search on tasks involving both vision and language.
State of the art visual detectors have made it possi-
ble to hypothesise what is in an image (Guillaumin
et al., 2009; Felzenszwalb et al., 2010), paving
the way for automatic image description systems.
The aim of such systems is to extract and reason
about visual aspects of images to generate a human-
like description. An example of the type of image
and gold-standard descriptions available can be
seen in Figure 1. Recent approaches to this task
have been based on slot-filling (Yang et al., 2011;
Elliott and Keller, 2013), combining web-scale n-
grams (Li et al., 2011), syntactic tree substitution
(Mitchell et al., 2012), and description-by-retrieval
(Farhadi et al., 2010; Ordonez et al., 2011; Hodosh
et al., 2013). Image description has been compared
to translating an image into text (Li et al., 2011;
Kulkarni et al., 2011) or summarising an image
</bodyText>
<listItem confidence="0.998550428571429">
1. An older woman with a small dog in the snow.
2. A woman and a cat are outside in the snow.
3. A woman in a brown vest is walking on the
snow with an animal.
4. A woman with a red scarf covering her head
walks with her cat on snow-covered ground.
5. Heavy set woman in snow with a cat.
</listItem>
<figureCaption confidence="0.875249">
Figure 1: An image from the Flickr8K data set and
</figureCaption>
<bodyText confidence="0.996470125">
five human-written descriptions. These descrip-
tions vary in the adjectives or prepositional phrases
that describe the woman (1, 3, 4, 5), incorrect or un-
certain identification of the cat (1, 3), and include
a sentence without a verb (5).
(Yang et al., 2011), resulting in the adoption of the
evaluation measures from those communities.
In this paper we estimate the correlation of hu-
man judgements with five automatic evaluation
measures on two image description data sets. Our
work extends previous studies of evaluation mea-
sures for image description (Hodosh et al., 2013),
which focused on unigram-based measures and re-
ported agreement scores such as Cohen’s κ rather
than correlations. The main finding of our analysis
is that TER and unigram BLEU are weakly corre-
</bodyText>
<page confidence="0.980634">
452
</page>
<bodyText confidence="0.740173">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 452–457,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
lated against human judgements, ROUGE-SU4 and
Smoothed BLEU are moderately correlated, and the
strongest correlation is found with Meteor.
</bodyText>
<sectionHeader confidence="0.994159" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999879">
We estimate Spearman’s p for five different auto-
matic evaluation measures against human judge-
ments for the automatic image description task.
Spearman’s p is a non-parametric correlation co-
efficient that restricts the ability of outlier data
points to skew the co-efficient value. The automatic
measures are calculated on the sentence level and
correlated against human judgements of semantic
correctness.
</bodyText>
<subsectionHeader confidence="0.99977">
2.2 Automatic Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.9997393">
BLEU measures the effective overlap between a
reference sentence X and a candidate sentence Y.
It is defined as the geometric mean of the effective
n-gram precision scores, multiplied by the brevity
penalty factor BP to penalise short translations. pn
measures the effective overlap by calculating the
proportion of the maximum number of n-grams
co-occurring between a candidate and a reference
and the total number of n-grams in the candidate
text. More formally,
</bodyText>
<equation confidence="0.997583333333333">
�wn log pn
N
BLEU = BP · exp E
n=1
2.1 Data E E countclip(ngram)
cEcand ngramEc
E
ngramEc
count(ngram)
pn =
E
cEcand
</equation>
<bodyText confidence="0.999994852941177">
We perform the correlation analysis on the Flickr8K
data set of Hodosh et al. (2013), and the data set of
Elliott and Keller (2013).
The test data of the Flickr8K data set contains
1,000 images paired with five reference descrip-
tions. The images were retrieved from Flickr, the
reference descriptions were collected from Me-
chanical Turk, and the human judgements were
collected from expert annotators as follows: each
image in the test data was paired with the highest
scoring sentence(s) retrieved from all possible test
sentences by the TRI5SEM model in Hodosh et al.
(2013). Each image–description pairing in the test
data was judged for semantic correctness by three
expert human judges on a scale of 1–4. We calcu-
late automatic measures for each image–retrieved
sentence pair against the five reference descriptions
for the original image.
The test data of Elliott and Keller (2013) con-
tains 101 images paired with three reference de-
scriptions. The images were taken from the PAS-
CAL VOC Action Recognition Task, the reference
descriptions were collected from Mechanical Turk,
and the judgements were also collected from Me-
chanical Turk. Elliott and Keller (2013) gener-
ated two-sentence descriptions for each of the test
images using four variants of a slot-filling model,
and collected five human judgements of the se-
mantic correctness and grammatical correctness of
the description on a scale of 1–5 for each image–
description pair, resulting in a total of 2,042 human
judgement–description pairings. In this analysis,
we use only the first sentence of the description,
which describes the event depicted in the image.
</bodyText>
<equation confidence="0.8543625">
� 1 if c &gt; r
BP = e(1−r/c) if c G r
</equation>
<bodyText confidence="0.99948804">
Unigram BLEU without a brevity penalty has been
reported by Kulkarni et al. (2011), Li et al. (2011),
Ordonez et al. (2011), and Kuznetsova et al. (2012);
to the best of our knowledge, the only image de-
scription work to use higher-order n-grams with
BLEU is Elliott and Keller (2013). In this paper we
use the smoothed BLEU implementation of Clark et
al. (2011) to perform a sentence-level analysis, set-
ting n = 1 and no brevity penalty to get the unigram
BLEU measure, or n = 4 with the brevity penalty
to get the Smoothed BLEU measure. We note that a
higher BLEU score is better.
ROUGE measures the longest common subse-
quence of tokens between a candidate Y and refer-
ence X. There is also a variant that measures the co-
occurrence of pairs of tokens in both the candidate
and reference (a skip-bigram): ROUGE-SU*. The
skip-bigram calculation is parameterised with dskip,
the maximum number of tokens between the words
in the skip-bigram. Setting dskip to 0 is equivalent to
bigram overlap and setting dskip to — means tokens
can be any distance apart. If a = |SKIP2(X,Y)|
is the number of matching skip-bigrams between
the reference and the candidate, then skip-bigram
ROUGE is formally defined as:
</bodyText>
<equation confidence="0.982642">
\a /
RSKIP2 = a / 2
</equation>
<page confidence="0.982892">
453
</page>
<bodyText confidence="0.999497588235294">
ROUGE has been used by only Yang et al. (2011)
to measure the quality of generated descriptions,
using a variant they describe as ROUGE-1. We set
dskip = 4 and award partial credit for unigram only
matches, otherwise known as ROUGE-SU4. We use
ROUGE v.1.5.5 for the analysis, and configure the
evaluation script to return the result for the average
score for matching between the candidate and the
references. A higher ROUGE score is better.
TER measures the number of modifications a hu-
man would need to make to transform a candidate
Y into a reference X. The modifications available
are insertion, deletion, substitute a single word, and
shift a word an arbitrary distance. TER is expressed
as the percentage of the sentence that needs to be
changed, and can be greater than 100 if the candi-
date is longer than the reference. More formally,
</bodyText>
<equation confidence="0.903899333333333">
TER =
|edits|
|reference tokens|
</equation>
<bodyText confidence="0.999286705882353">
TER has not yet been used to evaluate image de-
scription models. We use v.0.8.0 of the TER evalu-
ation tool, and a lower TER is better.
Meteor is the harmonic mean of unigram preci-
sion and recall that allows for exact, synonym, and
paraphrase matchings between candidates and ref-
erences. It is calculated by generating an alignment
between the tokens in the candidate and reference
sentences, with the aim of a 1:1 alignment between
tokens and minimising the number of chunks ch
of contiguous and identically ordered tokens in the
sentence pair. The alignment is based on exact to-
ken matching, followed by Wordnet synonyms, and
then stemmed tokens. We can calculate precision,
recall, and F-measure, where m is the number of
aligned unigrams between candidate and reference.
Meteor is defined as:
</bodyText>
<equation confidence="0.997754222222222">
M = (1 − Pen) ·Fmean
g q
MPen =
PR
aP+(1−a)R
= |m|
P |unigrams in candidate|
|m|
R = |unigrams in reference|
</equation>
<bodyText confidence="0.90950025">
We calculated the Meteor scores using release 1.4.0
with the package-provided free parameter settings
of 0.85, 0.2, 0.6, and 0.75 for the matching compo-
nents. Meteor has not yet been reported to evaluate
</bodyText>
<table confidence="0.9986785">
Flickr 8K E&amp;K (2013)
co-efficient co-efficient
n = 17,466 n = 2,040
METEOR 0.524 0.233
ROUGE SU-4 0.435 0.188
Smoothed BLEU 0.429 0.177
Unigram BLEU 0.345 0.097
TER -0.279 -0.044
</table>
<tableCaption confidence="0.959464666666667">
Table 1: Spearman’s correlation co-efficient of au-
tomatic evaluation measures against human judge-
ments. All correlations are significant at p &lt; 0.001.
</tableCaption>
<bodyText confidence="0.576021">
the performance of different models on the image
description task; a higher Meteor score is better.
</bodyText>
<subsectionHeader confidence="0.992371">
2.3 Protocol
</subsectionHeader>
<bodyText confidence="0.999933444444444">
We performed the correlation analysis as follows.
The sentence-level evaluation measures were cal-
culated for each image–description–reference tu-
ple. We collected the BLEU, TER, and Meteor
scores using MultEval (Clark et al., 2011), and the
ROUGE-SU4 scores using the RELEASE-1.5.5.pl
script. The evaluation measure scores were then
compared with the human judgements using Spear-
man’s correlation estimated at the sentence-level.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="background">
3 Results
</sectionHeader>
<bodyText confidence="0.999676523809524">
Table 1 shows the correlation co-efficients between
automatic measures and human judgements and
Figures 2(a) and (b) show the distribution of scores
for each measure against human judgements. To
classify the strength of the correlations, we fol-
lowed the guidance of Dancey and Reidy (2011),
who posit that a co-efficient of 0.0–0.1 is uncor-
related, 0.11–0.4 is weak, 0.41–0.7 is moderate,
0.71–0.90 is strong, and 0.91–1.0 is perfect.
On the Flickr8k data set, all evaluation measures
can be classified as either weakly correlated or mod-
erately correlated with human judgements and all
results are significant. TER is only weakly cor-
related with human judgements but could prove
useful in comparing the types of differences be-
tween models. An analysis of the distribution of
TER scores in Figure 2(a) shows that differences in
candidate and reference length are prevalent in the
image description task. Unigram BLEU is also only
weakly correlated against human judgements, even
though it has been reported extensively for this task.
</bodyText>
<equation confidence="0.698401">
Fmean =
</equation>
<page confidence="0.992476">
454
</page>
<bodyText confidence="0.9225365">
Candidate: Football players gathering to con-
test something to collaborating officials.
Reference: A football player in red and white
is holding both hands up.
</bodyText>
<figure confidence="0.9995078">
(a)
Candidate: A man is attempting a stunt with a
bicycle.
Reference: Bmx biker Jumps off of ramp.
(b)
</figure>
<figureCaption confidence="0.998616">
Figure 3: Examples in the test data with low Meteor scores and the maximum expert human judgement.
</figureCaption>
<listItem confidence="0.4831555">
(a) the candidate and reference are from the same image, and show differences in what to describe, in
(b) the descriptions are retrieved from different images and show differences in how to describe an image.
</listItem>
<bodyText confidence="0.9997156">
some of which go beyond unigram matchings be-
tween references and candidates, whereas they only
report unigram BLEU and unigram ROUGE. It is
therefore difficult to directly compare the results
of our correlation analysis against Hodosh et al.’s
agreement analysis, but they also reach the conclu-
sion that unigram BLEU is not an appropriate mea-
sure of image description performance. However,
we do find stronger correlations with Smoothed
BLEU, skip-bigram ROUGE, and Meteor.
In contrast to the results presented here, Reiter
and Belz (2009) found no significant correlations
of automatic evaluation measures against human
judgements of the accuracy of machine-generated
weather forecasts. They did, however, find signif-
icant correlations of automatic measures against
fluency judgements. There are no fluency judge-
ments available for Flickr8K, but Elliott and Keller
(2013) report grammaticality judgements for their
data, which are comparable to fluency ratings. We
failed to find significant correlations between gram-
matlicality judgements and any of the automatic
measures on the Elliott and Keller (2013) data. This
discrepancy could be explained in terms of the dif-
ferences between the weather forecast generation
and image description tasks, or because the image
description data sets contain thousands of texts and
a few human judgements per text, whereas the data
sets of Reiter and Belz (2009) included hundreds
of texts with 30 human judges.
</bodyText>
<sectionHeader confidence="0.998492" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99949512">
In this paper we performed a sentence-level corre-
lation analysis of automatic evaluation measures
against expert human judgements for the automatic
image description task. We found that sentence-
level unigram BLEU is only weakly correlated with
human judgements, even though it has extensively
reported in the literature for this task. Meteor was
found to have the highest correlation with human
judgements, but it requires Wordnet and paraphrase
resources that are not available for all languages.
Our findings held when judgements were made on
human-written or computer-generated descriptions.
The variability in what and how people describe
images will cause problems for all of the measures
compared in this paper. Nevertheless, we propose
that unigram BLEU should no longer be used as
an objective function for automatic image descrip-
tion because it has a weak correlation with human
accuracy judgements. We recommend adopting
either Meteor, Smoothed BLEU, or ROUGE-SU4 be-
cause they show stronger correlations with human
judgements. We believe these suggestions are also
applicable to the ranking tasks proposed in Hodosh
et al. (2013), where automatic evaluation scores
could act as features to a ranking function.
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.82335875">
Alexandra Birch and R. Calen Walshe, and the
anonymous reviewers provided valuable feedback
on this paper. The research is funded by ERC
Starting Grant SYNPROC No. 203427.
</bodyText>
<page confidence="0.998847">
456
</page>
<sectionHeader confidence="0.996381" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999783054945055">
Jonathon H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
176–181, Portland, Oregon, USA.
Christine Dancey and John Reidy, 2011. Statistics
Without Maths for Psychology, page 175. Prentice
Hall, 5th edition.
Desmond Elliott and Frank Keller. 2013. Image De-
scription using Visual Dependency Representations.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1292–1302, Seattle, Washington, U.S.A.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every
picture tells a story: generating sentences from im-
ages. In Proceedings of the 11th European Confer-
ence on Computer Vision, pages 15–29, Heraklion,
Crete, Greece.
Pedro F. Felzenszwalb, Ross B. Girshick, David
McAllester, and Deva Ramanan. 2010. Object
Detection with Discriminatively Trained Part-Based
Models. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 32(9):1627–1645.
Matthieu Guillaumin, Thomas Mensink, Jakob J. Ver-
beek, and Cornelia Schmid. 2009. Tagprop: Dis-
criminative metric learning in nearest neighbor mod-
els for image auto-annotation. In IEEE 12th Interna-
tional Conference on Computer Vision, pages 309–
316, Kyoto, Japan.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing Image Description as a Ranking
Task: Data, Models and Evaluation Metrics. Jour-
nal of Artificial Intelligence Research, 47:853–899.
Laurent Itti, Christof Koch, and Ernst Niebur. 1998.
A model of saliency-based visual attention for rapid
scene analysis. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 20(11):1254–1259.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In The 24th IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 1601–1608, Colorado Springs, Col-
orado, U.S.A.
Polina Kuznetsova, Vicente Ordonez, Alexander C.
Berg, Tamara L. Berg, and Yejin Choi. 2012. Col-
lective Generation of Natural Image Descriptions.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 359–
368, Jeju Island, South Korea.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
In Fifteenth Conference on Computational Natural
Language Learning, pages 220–228, Portland, Ore-
gon, U.S.A.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Alyssa Mensch, Alex Berg,
Tamara Berg, and Hal Daum´e III. 2012. Midge :
Generating Image Descriptions From Computer Vi-
sion Detections. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 747–756, Avi-
gnon, France.
Aude Oliva and Antonio Torralba. 2001. Modeling the
Shape of the Scene: A Holistic Representation of
the Spatial Envelope. International Journal of Com-
puter Vision, 42(3):145–175.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing Images Using 1 Million
Captioned Photographs. In Advances in Neural In-
formation Processing Systems 24, Granada, Spain.
Ehud Reiter and A Belz. 2009. An investigation into
the validity of some metrics for automatically evalu-
ating natural language generation systems. Compu-
tational Linguistics, 35(4):529–558.
Antonio Torralba, Aude Oliva, Monica S. Castelhano,
and John M. Henderson. 2006. Contextual guid-
ance of eye movements and attention in real-world
scenes: the role of global features in object search.
Psychologial Review, 113(4):766–786.
Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yian-
nis Aloimonos. 2011. Corpus-Guided Sentence
Generation of Natural Images. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 444–454, Edinburgh,
Scotland, UK.
</reference>
<page confidence="0.998356">
457
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.972873">
<title confidence="0.999958">Comparing Automatic Evaluation Measures for Image Description</title>
<author confidence="0.99097">Elliott</author>
<affiliation confidence="0.9973">Institute for Language, Cognition, and School of Informatics, University of Edinburgh</affiliation>
<abstract confidence="0.999206555555556">Image description is a new natural language generation task, where the aim is to generate a human-like description of an image. The evaluation of computer-generated text is a notoriously difficult problem, however, the quality of image descriptions has typically been measured using unigram human judgements. The focus of this paper is to determine the correlation of automatic measures with human judgements for this task. We estimate the correlaof unigram and Smoothed and Meteor against human judgements on two data sets. The main is that unigram a weak correlation, and Meteor has the strongest correlation with human judgements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathon H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>176--181</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="6465" citStr="Clark et al. (2011)" startWordPosition="1046" endWordPosition="1049">5 for each image– description pair, resulting in a total of 2,042 human judgement–description pairings. In this analysis, we use only the first sentence of the description, which describes the event depicted in the image. � 1 if c &gt; r BP = e(1−r/c) if c G r Unigram BLEU without a brevity penalty has been reported by Kulkarni et al. (2011), Li et al. (2011), Ordonez et al. (2011), and Kuznetsova et al. (2012); to the best of our knowledge, the only image description work to use higher-order n-grams with BLEU is Elliott and Keller (2013). In this paper we use the smoothed BLEU implementation of Clark et al. (2011) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram BLEU measure, or n = 4 with the brevity penalty to get the Smoothed BLEU measure. We note that a higher BLEU score is better. ROUGE measures the longest common subsequence of tokens between a candidate Y and reference X. There is also a variant that measures the cooccurrence of pairs of tokens in both the candidate and reference (a skip-bigram): ROUGE-SU*. The skip-bigram calculation is parameterised with dskip, the maximum number of tokens between the words in the skip-bigram. Setting dskip to 0 is </context>
<context position="9985" citStr="Clark et al., 2011" startWordPosition="1642" endWordPosition="1645">466 n = 2,040 METEOR 0.524 0.233 ROUGE SU-4 0.435 0.188 Smoothed BLEU 0.429 0.177 Unigram BLEU 0.345 0.097 TER -0.279 -0.044 Table 1: Spearman’s correlation co-efficient of automatic evaluation measures against human judgements. All correlations are significant at p &lt; 0.001. the performance of different models on the image description task; a higher Meteor score is better. 2.3 Protocol We performed the correlation analysis as follows. The sentence-level evaluation measures were calculated for each image–description–reference tuple. We collected the BLEU, TER, and Meteor scores using MultEval (Clark et al., 2011), and the ROUGE-SU4 scores using the RELEASE-1.5.5.pl script. The evaluation measure scores were then compared with the human judgements using Spearman’s correlation estimated at the sentence-level. 3 Results Table 1 shows the correlation co-efficients between automatic measures and human judgements and Figures 2(a) and (b) show the distribution of scores for each measure against human judgements. To classify the strength of the correlations, we followed the guidance of Dancey and Reidy (2011), who posit that a co-efficient of 0.0–0.1 is uncorrelated, 0.11–0.4 is weak, 0.41–0.7 is moderate, 0.</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathon H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 176–181, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Dancey</author>
<author>John Reidy</author>
</authors>
<date>2011</date>
<pages>175</pages>
<publisher>Prentice Hall,</publisher>
<institution>Statistics Without Maths for Psychology,</institution>
<note>5th edition.</note>
<contexts>
<context position="10483" citStr="Dancey and Reidy (2011)" startWordPosition="1716" endWordPosition="1719">for each image–description–reference tuple. We collected the BLEU, TER, and Meteor scores using MultEval (Clark et al., 2011), and the ROUGE-SU4 scores using the RELEASE-1.5.5.pl script. The evaluation measure scores were then compared with the human judgements using Spearman’s correlation estimated at the sentence-level. 3 Results Table 1 shows the correlation co-efficients between automatic measures and human judgements and Figures 2(a) and (b) show the distribution of scores for each measure against human judgements. To classify the strength of the correlations, we followed the guidance of Dancey and Reidy (2011), who posit that a co-efficient of 0.0–0.1 is uncorrelated, 0.11–0.4 is weak, 0.41–0.7 is moderate, 0.71–0.90 is strong, and 0.91–1.0 is perfect. On the Flickr8k data set, all evaluation measures can be classified as either weakly correlated or moderately correlated with human judgements and all results are significant. TER is only weakly correlated with human judgements but could prove useful in comparing the types of differences between models. An analysis of the distribution of TER scores in Figure 2(a) shows that differences in candidate and reference length are prevalent in the image desc</context>
</contexts>
<marker>Dancey, Reidy, 2011</marker>
<rawString>Christine Dancey and John Reidy, 2011. Statistics Without Maths for Psychology, page 175. Prentice Hall, 5th edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond Elliott</author>
<author>Frank Keller</author>
</authors>
<title>Image Description using Visual Dependency Representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1292--1302</pages>
<location>Seattle, Washington, U.S.A.</location>
<contexts>
<context position="1615" citStr="Elliott and Keller, 2013" startWordPosition="250" endWordPosition="253"> language processing have led to an upsurge of research on tasks involving both vision and language. State of the art visual detectors have made it possible to hypothesise what is in an image (Guillaumin et al., 2009; Felzenszwalb et al., 2010), paving the way for automatic image description systems. The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1. Recent approaches to this task have been based on slot-filling (Yang et al., 2011; Elliott and Keller, 2013), combining web-scale ngrams (Li et al., 2011), syntactic tree substitution (Mitchell et al., 2012), and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013). Image description has been compared to translating an image into text (Li et al., 2011; Kulkarni et al., 2011) or summarising an image 1. An older woman with a small dog in the snow. 2. A woman and a cat are outside in the snow. 3. A woman in a brown vest is walking on the snow with an animal. 4. A woman with a red scarf covering her head walks with her cat on snow-covered ground. 5. Heavy set woman </context>
<context position="4573" citStr="Elliott and Keller (2013)" startWordPosition="733" endWordPosition="736">nce Y. It is defined as the geometric mean of the effective n-gram precision scores, multiplied by the brevity penalty factor BP to penalise short translations. pn measures the effective overlap by calculating the proportion of the maximum number of n-grams co-occurring between a candidate and a reference and the total number of n-grams in the candidate text. More formally, �wn log pn N BLEU = BP · exp E n=1 2.1 Data E E countclip(ngram) cEcand ngramEc E ngramEc count(ngram) pn = E cEcand We perform the correlation analysis on the Flickr8K data set of Hodosh et al. (2013), and the data set of Elliott and Keller (2013). The test data of the Flickr8K data set contains 1,000 images paired with five reference descriptions. The images were retrieved from Flickr, the reference descriptions were collected from Mechanical Turk, and the human judgements were collected from expert annotators as follows: each image in the test data was paired with the highest scoring sentence(s) retrieved from all possible test sentences by the TRI5SEM model in Hodosh et al. (2013). Each image–description pairing in the test data was judged for semantic correctness by three expert human judges on a scale of 1–4. We calculate automati</context>
<context position="6387" citStr="Elliott and Keller (2013)" startWordPosition="1032" endWordPosition="1035">semantic correctness and grammatical correctness of the description on a scale of 1–5 for each image– description pair, resulting in a total of 2,042 human judgement–description pairings. In this analysis, we use only the first sentence of the description, which describes the event depicted in the image. � 1 if c &gt; r BP = e(1−r/c) if c G r Unigram BLEU without a brevity penalty has been reported by Kulkarni et al. (2011), Li et al. (2011), Ordonez et al. (2011), and Kuznetsova et al. (2012); to the best of our knowledge, the only image description work to use higher-order n-grams with BLEU is Elliott and Keller (2013). In this paper we use the smoothed BLEU implementation of Clark et al. (2011) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram BLEU measure, or n = 4 with the brevity penalty to get the Smoothed BLEU measure. We note that a higher BLEU score is better. ROUGE measures the longest common subsequence of tokens between a candidate Y and reference X. There is also a variant that measures the cooccurrence of pairs of tokens in both the candidate and reference (a skip-bigram): ROUGE-SU*. The skip-bigram calculation is parameterised with dskip, the maximum</context>
<context position="12681" citStr="Elliott and Keller (2013)" startWordPosition="2061" endWordPosition="2064">eement analysis, but they also reach the conclusion that unigram BLEU is not an appropriate measure of image description performance. However, we do find stronger correlations with Smoothed BLEU, skip-bigram ROUGE, and Meteor. In contrast to the results presented here, Reiter and Belz (2009) found no significant correlations of automatic evaluation measures against human judgements of the accuracy of machine-generated weather forecasts. They did, however, find significant correlations of automatic measures against fluency judgements. There are no fluency judgements available for Flickr8K, but Elliott and Keller (2013) report grammaticality judgements for their data, which are comparable to fluency ratings. We failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the Elliott and Keller (2013) data. This discrepancy could be explained in terms of the differences between the weather forecast generation and image description tasks, or because the image description data sets contain thousands of texts and a few human judgements per text, whereas the data sets of Reiter and Belz (2009) included hundreds of texts with 30 human judges. 5 Conclusions In this</context>
</contexts>
<marker>Elliott, Keller, 2013</marker>
<rawString>Desmond Elliott and Frank Keller. 2013. Image Description using Visual Dependency Representations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292–1302, Seattle, Washington, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: generating sentences from images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th European Conference on Computer Vision,</booktitle>
<pages>15--29</pages>
<location>Heraklion, Crete, Greece.</location>
<contexts>
<context position="1766" citStr="Farhadi et al., 2010" startWordPosition="271" endWordPosition="274"> to hypothesise what is in an image (Guillaumin et al., 2009; Felzenszwalb et al., 2010), paving the way for automatic image description systems. The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1. Recent approaches to this task have been based on slot-filling (Yang et al., 2011; Elliott and Keller, 2013), combining web-scale ngrams (Li et al., 2011), syntactic tree substitution (Mitchell et al., 2012), and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013). Image description has been compared to translating an image into text (Li et al., 2011; Kulkarni et al., 2011) or summarising an image 1. An older woman with a small dog in the snow. 2. A woman and a cat are outside in the snow. 3. A woman in a brown vest is walking on the snow with an animal. 4. A woman with a red scarf covering her head walks with her cat on snow-covered ground. 5. Heavy set woman in snow with a cat. Figure 1: An image from the Flickr8K data set and five human-written descriptions. These descriptions vary in the adjectives or pre</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: generating sentences from images. In Proceedings of the 11th European Conference on Computer Vision, pages 15–29, Heraklion, Crete, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro F Felzenszwalb</author>
<author>Ross B Girshick</author>
<author>David McAllester</author>
<author>Deva Ramanan</author>
</authors>
<title>Object Detection with Discriminatively Trained Part-Based Models.</title>
<date>2010</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>9</issue>
<contexts>
<context position="1234" citStr="Felzenszwalb et al., 2010" startWordPosition="186" endWordPosition="189">elation of automatic measures with human judgements for this task. We estimate the correlation of unigram and Smoothed BLEU, TER, ROUGE-SU4, and Meteor against human judgements on two data sets. The main finding is that unigram BLEU has a weak correlation, and Meteor has the strongest correlation with human judgements. 1 Introduction Recent advances in computer vision and natural language processing have led to an upsurge of research on tasks involving both vision and language. State of the art visual detectors have made it possible to hypothesise what is in an image (Guillaumin et al., 2009; Felzenszwalb et al., 2010), paving the way for automatic image description systems. The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1. Recent approaches to this task have been based on slot-filling (Yang et al., 2011; Elliott and Keller, 2013), combining web-scale ngrams (Li et al., 2011), syntactic tree substitution (Mitchell et al., 2012), and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013). Image description has </context>
</contexts>
<marker>Felzenszwalb, Girshick, McAllester, Ramanan, 2010</marker>
<rawString>Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan. 2010. Object Detection with Discriminatively Trained Part-Based Models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1627–1645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthieu Guillaumin</author>
<author>Thomas Mensink</author>
<author>Jakob J Verbeek</author>
<author>Cornelia Schmid</author>
</authors>
<title>Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation.</title>
<date>2009</date>
<booktitle>In IEEE 12th International Conference on Computer Vision,</booktitle>
<pages>309--316</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="1206" citStr="Guillaumin et al., 2009" startWordPosition="182" endWordPosition="185"> is to determine the correlation of automatic measures with human judgements for this task. We estimate the correlation of unigram and Smoothed BLEU, TER, ROUGE-SU4, and Meteor against human judgements on two data sets. The main finding is that unigram BLEU has a weak correlation, and Meteor has the strongest correlation with human judgements. 1 Introduction Recent advances in computer vision and natural language processing have led to an upsurge of research on tasks involving both vision and language. State of the art visual detectors have made it possible to hypothesise what is in an image (Guillaumin et al., 2009; Felzenszwalb et al., 2010), paving the way for automatic image description systems. The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1. Recent approaches to this task have been based on slot-filling (Yang et al., 2011; Elliott and Keller, 2013), combining web-scale ngrams (Li et al., 2011), syntactic tree substitution (Mitchell et al., 2012), and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2</context>
</contexts>
<marker>Guillaumin, Mensink, Verbeek, Schmid, 2009</marker>
<rawString>Matthieu Guillaumin, Thomas Mensink, Jakob J. Verbeek, and Cornelia Schmid. 2009. Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation. In IEEE 12th International Conference on Computer Vision, pages 309– 316, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micah Hodosh</author>
<author>Peter Young</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>47--853</pages>
<contexts>
<context position="1810" citStr="Hodosh et al., 2013" startWordPosition="279" endWordPosition="282">min et al., 2009; Felzenszwalb et al., 2010), paving the way for automatic image description systems. The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1. Recent approaches to this task have been based on slot-filling (Yang et al., 2011; Elliott and Keller, 2013), combining web-scale ngrams (Li et al., 2011), syntactic tree substitution (Mitchell et al., 2012), and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013). Image description has been compared to translating an image into text (Li et al., 2011; Kulkarni et al., 2011) or summarising an image 1. An older woman with a small dog in the snow. 2. A woman and a cat are outside in the snow. 3. A woman in a brown vest is walking on the snow with an animal. 4. A woman with a red scarf covering her head walks with her cat on snow-covered ground. 5. Heavy set woman in snow with a cat. Figure 1: An image from the Flickr8K data set and five human-written descriptions. These descriptions vary in the adjectives or prepositional phrases that describe the woman (</context>
<context position="4526" citStr="Hodosh et al. (2013)" startWordPosition="724" endWordPosition="727">reference sentence X and a candidate sentence Y. It is defined as the geometric mean of the effective n-gram precision scores, multiplied by the brevity penalty factor BP to penalise short translations. pn measures the effective overlap by calculating the proportion of the maximum number of n-grams co-occurring between a candidate and a reference and the total number of n-grams in the candidate text. More formally, �wn log pn N BLEU = BP · exp E n=1 2.1 Data E E countclip(ngram) cEcand ngramEc E ngramEc count(ngram) pn = E cEcand We perform the correlation analysis on the Flickr8K data set of Hodosh et al. (2013), and the data set of Elliott and Keller (2013). The test data of the Flickr8K data set contains 1,000 images paired with five reference descriptions. The images were retrieved from Flickr, the reference descriptions were collected from Mechanical Turk, and the human judgements were collected from expert annotators as follows: each image in the test data was paired with the highest scoring sentence(s) retrieved from all possible test sentences by the TRI5SEM model in Hodosh et al. (2013). Each image–description pairing in the test data was judged for semantic correctness by three expert human </context>
</contexts>
<marker>Hodosh, Young, Hockenmaier, 2013</marker>
<rawString>Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics. Journal of Artificial Intelligence Research, 47:853–899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Itti</author>
<author>Christof Koch</author>
<author>Ernst Niebur</author>
</authors>
<title>A model of saliency-based visual attention for rapid scene analysis.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<issue>11</issue>
<marker>Itti, Koch, Niebur, 1998</marker>
<rawString>Laurent Itti, Christof Koch, and Ernst Niebur. 1998. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(11):1254–1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Baby talk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In The 24th IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1601--1608</pages>
<location>Colorado Springs, Colorado, U.S.A.</location>
<contexts>
<context position="1922" citStr="Kulkarni et al., 2011" startWordPosition="298" endWordPosition="301">of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1. Recent approaches to this task have been based on slot-filling (Yang et al., 2011; Elliott and Keller, 2013), combining web-scale ngrams (Li et al., 2011), syntactic tree substitution (Mitchell et al., 2012), and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013). Image description has been compared to translating an image into text (Li et al., 2011; Kulkarni et al., 2011) or summarising an image 1. An older woman with a small dog in the snow. 2. A woman and a cat are outside in the snow. 3. A woman in a brown vest is walking on the snow with an animal. 4. A woman with a red scarf covering her head walks with her cat on snow-covered ground. 5. Heavy set woman in snow with a cat. Figure 1: An image from the Flickr8K data set and five human-written descriptions. These descriptions vary in the adjectives or prepositional phrases that describe the woman (1, 3, 4, 5), incorrect or uncertain identification of the cat (1, 3), and include a sentence without a verb (5).</context>
<context position="6186" citStr="Kulkarni et al. (2011)" startWordPosition="996" endWordPosition="999">om Mechanical Turk. Elliott and Keller (2013) generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1–5 for each image– description pair, resulting in a total of 2,042 human judgement–description pairings. In this analysis, we use only the first sentence of the description, which describes the event depicted in the image. � 1 if c &gt; r BP = e(1−r/c) if c G r Unigram BLEU without a brevity penalty has been reported by Kulkarni et al. (2011), Li et al. (2011), Ordonez et al. (2011), and Kuznetsova et al. (2012); to the best of our knowledge, the only image description work to use higher-order n-grams with BLEU is Elliott and Keller (2013). In this paper we use the smoothed BLEU implementation of Clark et al. (2011) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram BLEU measure, or n = 4 with the brevity penalty to get the Smoothed BLEU measure. We note that a higher BLEU score is better. ROUGE measures the longest common subsequence of tokens between a candidate Y and reference X. There</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2011. Baby talk: Understanding and generating simple image descriptions. In The 24th IEEE Conference on Computer Vision and Pattern Recognition, pages 1601–1608, Colorado Springs, Colorado, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Collective Generation of Natural Image Descriptions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>359--368</pages>
<location>Jeju Island, South</location>
<contexts>
<context position="6257" citStr="Kuznetsova et al. (2012)" startWordPosition="1009" endWordPosition="1012">descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1–5 for each image– description pair, resulting in a total of 2,042 human judgement–description pairings. In this analysis, we use only the first sentence of the description, which describes the event depicted in the image. � 1 if c &gt; r BP = e(1−r/c) if c G r Unigram BLEU without a brevity penalty has been reported by Kulkarni et al. (2011), Li et al. (2011), Ordonez et al. (2011), and Kuznetsova et al. (2012); to the best of our knowledge, the only image description work to use higher-order n-grams with BLEU is Elliott and Keller (2013). In this paper we use the smoothed BLEU implementation of Clark et al. (2011) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram BLEU measure, or n = 4 with the brevity penalty to get the Smoothed BLEU measure. We note that a higher BLEU score is better. ROUGE measures the longest common subsequence of tokens between a candidate Y and reference X. There is also a variant that measures the cooccurrence of pairs of tokens in</context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2012</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg, Tamara L. Berg, and Yejin Choi. 2012. Collective Generation of Natural Image Descriptions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 359– 368, Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siming Li</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
<author>Alexander C Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Composing simple image descriptions using web-scale n-grams.</title>
<date>2011</date>
<booktitle>In Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>220--228</pages>
<location>Portland, Oregon, U.S.A.</location>
<contexts>
<context position="1661" citStr="Li et al., 2011" startWordPosition="258" endWordPosition="261"> on tasks involving both vision and language. State of the art visual detectors have made it possible to hypothesise what is in an image (Guillaumin et al., 2009; Felzenszwalb et al., 2010), paving the way for automatic image description systems. The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1. Recent approaches to this task have been based on slot-filling (Yang et al., 2011; Elliott and Keller, 2013), combining web-scale ngrams (Li et al., 2011), syntactic tree substitution (Mitchell et al., 2012), and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013). Image description has been compared to translating an image into text (Li et al., 2011; Kulkarni et al., 2011) or summarising an image 1. An older woman with a small dog in the snow. 2. A woman and a cat are outside in the snow. 3. A woman in a brown vest is walking on the snow with an animal. 4. A woman with a red scarf covering her head walks with her cat on snow-covered ground. 5. Heavy set woman in snow with a cat. Figure 1: An image from th</context>
<context position="6204" citStr="Li et al. (2011)" startWordPosition="1000" endWordPosition="1003">ott and Keller (2013) generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1–5 for each image– description pair, resulting in a total of 2,042 human judgement–description pairings. In this analysis, we use only the first sentence of the description, which describes the event depicted in the image. � 1 if c &gt; r BP = e(1−r/c) if c G r Unigram BLEU without a brevity penalty has been reported by Kulkarni et al. (2011), Li et al. (2011), Ordonez et al. (2011), and Kuznetsova et al. (2012); to the best of our knowledge, the only image description work to use higher-order n-grams with BLEU is Elliott and Keller (2013). In this paper we use the smoothed BLEU implementation of Clark et al. (2011) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram BLEU measure, or n = 4 with the brevity penalty to get the Smoothed BLEU measure. We note that a higher BLEU score is better. ROUGE measures the longest common subsequence of tokens between a candidate Y and reference X. There is also a variant</context>
</contexts>
<marker>Li, Kulkarni, Berg, Berg, Choi, 2011</marker>
<rawString>Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi. 2011. Composing simple image descriptions using web-scale n-grams. In Fifteenth Conference on Computational Natural Language Learning, pages 220–228, Portland, Oregon, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Kota Yamaguchi</author>
<author>Karl Stratos</author>
<author>Alyssa Mensch</author>
<author>Alex Berg</author>
<author>Tamara Berg</author>
<author>Hal Daum´e</author>
</authors>
<title>Midge : Generating Image Descriptions From Computer Vision Detections.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>747--756</pages>
<location>Avignon, France.</location>
<marker>Mitchell, Dodge, Goyal, Yamaguchi, Stratos, Mensch, Berg, Berg, Daum´e, 2012</marker>
<rawString>Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Yamaguchi, Karl Stratos, Alyssa Mensch, Alex Berg, Tamara Berg, and Hal Daum´e III. 2012. Midge : Generating Image Descriptions From Computer Vision Detections. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 747–756, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aude Oliva</author>
<author>Antonio Torralba</author>
</authors>
<title>Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope.</title>
<date>2001</date>
<journal>International Journal of Computer Vision,</journal>
<volume>42</volume>
<issue>3</issue>
<marker>Oliva, Torralba, 2001</marker>
<rawString>Aude Oliva and Antonio Torralba. 2001. Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope. International Journal of Computer Vision, 42(3):145–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicente Ordonez</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
</authors>
<title>Im2Text: Describing Images Using 1 Million Captioned Photographs.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems 24,</booktitle>
<location>Granada,</location>
<contexts>
<context position="1788" citStr="Ordonez et al., 2011" startWordPosition="275" endWordPosition="278">s in an image (Guillaumin et al., 2009; Felzenszwalb et al., 2010), paving the way for automatic image description systems. The aim of such systems is to extract and reason about visual aspects of images to generate a humanlike description. An example of the type of image and gold-standard descriptions available can be seen in Figure 1. Recent approaches to this task have been based on slot-filling (Yang et al., 2011; Elliott and Keller, 2013), combining web-scale ngrams (Li et al., 2011), syntactic tree substitution (Mitchell et al., 2012), and description-by-retrieval (Farhadi et al., 2010; Ordonez et al., 2011; Hodosh et al., 2013). Image description has been compared to translating an image into text (Li et al., 2011; Kulkarni et al., 2011) or summarising an image 1. An older woman with a small dog in the snow. 2. A woman and a cat are outside in the snow. 3. A woman in a brown vest is walking on the snow with an animal. 4. A woman with a red scarf covering her head walks with her cat on snow-covered ground. 5. Heavy set woman in snow with a cat. Figure 1: An image from the Flickr8K data set and five human-written descriptions. These descriptions vary in the adjectives or prepositional phrases tha</context>
<context position="6227" citStr="Ordonez et al. (2011)" startWordPosition="1004" endWordPosition="1007">13) generated two-sentence descriptions for each of the test images using four variants of a slot-filling model, and collected five human judgements of the semantic correctness and grammatical correctness of the description on a scale of 1–5 for each image– description pair, resulting in a total of 2,042 human judgement–description pairings. In this analysis, we use only the first sentence of the description, which describes the event depicted in the image. � 1 if c &gt; r BP = e(1−r/c) if c G r Unigram BLEU without a brevity penalty has been reported by Kulkarni et al. (2011), Li et al. (2011), Ordonez et al. (2011), and Kuznetsova et al. (2012); to the best of our knowledge, the only image description work to use higher-order n-grams with BLEU is Elliott and Keller (2013). In this paper we use the smoothed BLEU implementation of Clark et al. (2011) to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram BLEU measure, or n = 4 with the brevity penalty to get the Smoothed BLEU measure. We note that a higher BLEU score is better. ROUGE measures the longest common subsequence of tokens between a candidate Y and reference X. There is also a variant that measures the cooc</context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011. Im2Text: Describing Images Using 1 Million Captioned Photographs. In Advances in Neural Information Processing Systems 24, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>A Belz</author>
</authors>
<title>An investigation into the validity of some metrics for automatically evaluating natural language generation systems.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="12348" citStr="Reiter and Belz (2009)" startWordPosition="2016" endWordPosition="2019"> retrieved from different images and show differences in how to describe an image. some of which go beyond unigram matchings between references and candidates, whereas they only report unigram BLEU and unigram ROUGE. It is therefore difficult to directly compare the results of our correlation analysis against Hodosh et al.’s agreement analysis, but they also reach the conclusion that unigram BLEU is not an appropriate measure of image description performance. However, we do find stronger correlations with Smoothed BLEU, skip-bigram ROUGE, and Meteor. In contrast to the results presented here, Reiter and Belz (2009) found no significant correlations of automatic evaluation measures against human judgements of the accuracy of machine-generated weather forecasts. They did, however, find significant correlations of automatic measures against fluency judgements. There are no fluency judgements available for Flickr8K, but Elliott and Keller (2013) report grammaticality judgements for their data, which are comparable to fluency ratings. We failed to find significant correlations between grammatlicality judgements and any of the automatic measures on the Elliott and Keller (2013) data. This discrepancy could be</context>
</contexts>
<marker>Reiter, Belz, 2009</marker>
<rawString>Ehud Reiter and A Belz. 2009. An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics, 35(4):529–558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Torralba</author>
<author>Aude Oliva</author>
<author>Monica S Castelhano</author>
<author>John M Henderson</author>
</authors>
<title>Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search.</title>
<date>2006</date>
<journal>Psychologial Review,</journal>
<volume>113</volume>
<issue>4</issue>
<marker>Torralba, Oliva, Castelhano, Henderson, 2006</marker>
<rawString>Antonio Torralba, Aude Oliva, Monica S. Castelhano, and John M. Henderson. 2006. Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search. Psychologial Review, 113(4):766–786.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
</authors>
<title>Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>444--454</pages>
<location>Edinburgh, Scotland, UK.</location>
<marker>Yang, 2011</marker>
<rawString>Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos. 2011. Corpus-Guided Sentence Generation of Natural Images. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 444–454, Edinburgh, Scotland, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>