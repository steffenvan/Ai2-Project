<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.896133">
Learning with Annotation Noise
</title>
<author confidence="0.688826">
Eyal Beigman Beata Beigman Klebanov
</author>
<affiliation confidence="0.835348">
Olin Business School Kellogg School of Management
Washington University in St. Louis Northwestern University
</affiliation>
<email confidence="0.996615">
beigman@wustl.edu beata@northwestern.edu
</email>
<sectionHeader confidence="0.994737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984944444445">
It is usually assumed that the kind of noise
existing in annotated data is random clas-
sification noise. Yet there is evidence
that differences between annotators are not
always random attention slips but could
result from different biases towards the
classification categories, at least for the
harder-to-decide cases. Under an annota-
tion generation model that takes this into
account, there is a hazard that some of the
training instances are actually hard cases
with unreliable annotations. We show
that these are relatively unproblematic for
an algorithm operating under the 0-1 loss
model, whereas for the commonly used
voted perceptron algorithm, hard training
cases could result in incorrect prediction
on the uncontroversial cases at test time.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959">
It is assumed, often tacitly, that the kind of
noise existing in human-annotated datasets used in
computational linguistics is random classification
noise (Kearns, 1993; Angluin and Laird, 1988),
resulting from annotator attention slips randomly
distributed across instances. For example, Os-
borne (2002) evaluates noise tolerance of shallow
parsers, with random classification noise taken to
be “crudely approximating annotation errors.” It
has been shown, both theoretically and empiri-
cally, that this type of noise is tolerated well by
the commonly used machine learning algorithms
(Cohen, 1997; Blum et al., 1996; Osborne, 2002;
Reidsma and Carletta, 2008).
Yet this might be overly optimistic. Reidsma
and op den Akker (2008) show that apparent dif-
ferences between annotators are not random slips
of attention but rather result from different biases
annotators might have towards the classification
categories. When training data comes from one
annotator and test data from another, the first an-
notator’s biases are sometimes systematic enough
for a machine learner to pick them up, with detri-
mental results for the algorithm’s performance on
the test data. A small subset of doubly anno-
tated data (for inter-annotator agreement check)
and large chunks of singly annotated data (for
training algorithms) is not uncommon in compu-
tational linguistics datasets; such a setup is prone
to problems if annotators are differently biased.1
Annotator bias is consistent with a number of
noise models. For example, it could be that an
annotator’s bias is exercised on each and every in-
stance, making his preferred category likelier for
any instance than in another person’s annotations.
Another possibility, recently explored by Beigman
Klebanov and Beigman (2009), is that some items
are really quite clear-cut for an annotator with any
bias, belonging squarely within one particular ca-
tegory. However, some instances – termed hard
cases therein – are harder to decide upon, and this
is where various preferences and biases come into
play. In a metaphor annotation study reported by
Beigman Klebanov et al. (2008), certain markups
received overwhelming annotator support when
people were asked to validate annotations after a
certain time delay. Other instances saw opinions
split; moreover, Beigman Klebanov et al. (2008)
observed cases where people retracted their own
earlier annotations.
To start accounting for such annotator behavior,
Beigman Klebanov and Beigman (2009) proposed
a model where instances are either easy, and then
all annotators agree on them, or hard, and then
each annotator flips his or her own coin to de-
</bodyText>
<footnote confidence="0.9887094">
1The different biases might not amount to much in the
small doubly annotated subset, resulting in acceptable inter-
annotator agreement; yet when enacted throughout a large
number of instances they can be detrimental from a machine
learner’s perspective.
</footnote>
<page confidence="0.894969">
280
</page>
<note confidence="0.9996215">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 280–287,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999896071428571">
cide on a label (each annotator can have a different
“coin” reflecting his or her biases). For annota-
tions generated under such a model, there is a dan-
ger of hard instances posing as easy – an observed
agreement between annotators being a result of all
coins coming up heads by chance. They therefore
define the expected proportion of hard instances in
agreed items as annotation noise. They provide
an example from the literature where an annota-
tion noise rate of about 15% is likely.
The question addressed in this article is: How
problematic is learning from training data with an-
notation noise? Specifically, we are interested in
estimating the degree to which performance on
easy instances at test time can be hurt by the pre-
sence of hard instances in training data.
Definition 1 The hard case bias, T, is the portion
of easy instances in the test data that are misclas-
sified as a result of hard instances in the training
data.
This article proceeds as follows. First, we show
that a machine learner operating under a 0-1 loss
minimization principle could sustain a hard case
bias of B(√1N) in the worst case. Thus, while an-
notation noise is hazardous for small datasets, it is
better tolerated in larger ones. However, 0-1 loss
minimization is computationally intractable for
large datasets (Feldman et al., 2006; Guruswami
and Raghavendra, 2006); substitute loss functions
are often used in practice. While their tolerance to
random classification noise is as good as for 0-1
loss, their tolerance to annotation noise is worse.
For example, the perceptron family of algorithms
handle random classification noise well (Cohen,
1997). We show in section 3.4 that the widely
used Freund and Schapire (1999) voted percep-
tron algorithm could face a constant hard case bias
when confronted with annotation noise in training
data, irrespective of the size of the dataset. Finally,
we discuss the implications of our findings for the
practice of annotation studies and for data utiliza-
tion in machine learning.
</bodyText>
<sectionHeader confidence="0.986662" genericHeader="method">
2 0-1 Loss
</sectionHeader>
<bodyText confidence="0.980647684210527">
Leta sample be a sequence x1, ... , xN drawn uni-
formly from the d-dimensional discrete cube Id =
{−1,1}d with corresponding labels y1, ... , yN ∈
{−1,1}. Suppose further that the learning al-
gorithm operates by finding a hyperplane (w, 0),
w ∈ Rd, 0 ∈ R, that minimizes the empirical er-
ror L(w,0) = Ej=1...N[yj−sgn(Ei=1...d xijwi−
0)]2. Let there be H hard cases, such that the an-
notation noise is -y = H 2
N .
Theorem 1 In the worst case configuration of in-
stances a hard case bias of T = 0( √1N) cannot be
ruled out with constant confidence.
Idea of the proof: We prove by explicit con-
struction of an adversarial case. Suppose there is
a plane that perfectly separates the easy instances.
The B(N) hard instances will be concentrated in
a band parallel to the separating plane, that is
near enough to the plane so as to trap only about
</bodyText>
<equation confidence="0.504492">
�( √
</equation>
<bodyText confidence="0.950345">
N) easy instances between the plane and the
band (see figure 1 for an illustration). For a ran-
dom labeling of the hard instances, the central
limit theorem shows there is positive probability
that there would be an imbalance between +1 and
√
−1 labels in favor of −1s on the scale of N,
which, with appropriate constants, would lead to
the movement of the empirically minimal separa-
tion plane to the right of the hard case band, mis-
classifying the trapped easy cases.
Proof: Let v = v(x) = Ei=1...d xi denote the
sum of the coordinates of an instance in Id and
take ae = √d · F−1(√-y · 2_
</bodyText>
<equation confidence="0.9762485">
2— 2 + 12) and ah
=
d
√d · F−1(-y + √-y · 2− 2+ 12), where F(t) is the
</equation>
<bodyText confidence="0.955206857142857">
cumulative distribution function of the normal dis-
tribution. Suppose further that instances xj such
that ae &lt; vj &lt; ah are all and only hard instances;
their labels are coinflips. All other instances are
easy, and labeled y = y(x) = sgn(v). In this case,
the hyperplane √1 d(1...1) is the true separation
plane for the easy instances, with = 0. Figure 1
shows this configuration.
According to the central limit theorem, for d, N
large, the distribution of v is well approximated by
N(0√
, d). If N = c1 · 2d, for some 0 &lt; c1 &lt; 4,
the second application of the central limit the-
orem ensures that, with high probability, about
</bodyText>
<equation confidence="0.690231">
-yN = c1-y2d items would fall between ae and ah
V/
</equation>
<bodyText confidence="0.792573666666667">
(all hard), and √-y · 2− d 2 N = c1 -y2d would fall
between 0 and ae (all easy, all labeled +1).
Let Z be the sum of labels of the hard cases,
Z = Ei=1...H yi. Applying the central limit the-
orem a third time, for large N, Z will, with a
high probability, be distributed approximately as
2In Beigman Klebanov and Beigman (2009), annotation
noise is defined as percentage of hard instances in the agreed
annotations; this implies noise measurement on multiply an-
notated material. When there is just one annotator, no dis-
tinction between easy vs hard instances can be made; in this
sense, all hard instances are posing as easy.
</bodyText>
<page confidence="0.984815">
281
</page>
<figure confidence="0.97384">
0 λe λh
</figure>
<figureCaption confidence="0.983411">
Figure 1: The adversarial case for 0-1 loss.
Squares correspond to easy instances, circles – to
hard ones. Filled squares and circles are labeled
−1, empty ones are labeled +1.
</figureCaption>
<bodyText confidence="0.985091">
N(0, .\/γN). This implies that a value as low as
−2σ cannot be ruled out with high (say 95%) con-
fidence. Thus, an imbalance of up to 2.\/γN, or of
</bodyText>
<equation confidence="0.953654666666667">
V/
2 c1γ2d, in favor of −1s is possible.
γ2d
</equation>
<bodyText confidence="0.9744585">
There are between 0 and λh about 2.\/c1 V/
more −1 hard instances than +1 hard instances, as
</bodyText>
<equation confidence="0.530891">
V/
</equation>
<bodyText confidence="0.99354725">
opposed to c1 γ2d easy instances that are all +1.
As long as c1 &lt; 2.\/c1, i.e. c1 &lt; 4, the empirically
minimal threshold would move to λh, resulting in
√�.\/�12d
a hard case bias of τ = (1−�)·�12d = θ(√1 � ).
To see that this is the worst case scenario, we
note that 0-1 loss sustained on θ(N) hard cases
is the order of magnitude of the possible imba-
lance between −1 and +1 random labels, which
.\/
is θ( N). For hard case loss to outweigh the loss
on the misclassified easy instances, there cannot
.\/
be more than θ( N) of the latter ❑
Note that the proof requires that N = θ(2d)
namely, that asymptotically the sample includes
a fixed portion of the instances. If the sample is
asymptotically smaller, then λe will have to be ad-
justed such that λe = .\/d · F−1(θ( √1 � ) + 12).
According to theorem 1, for a 10K dataset with
15% hard case rate, a hard case bias of about 1%
cannot be ruled out with 95% confidence.
Theorem 1 suggests that annotation noise as
defined here is qualitatively different from more
malicious types of noise analyzed in the agnostic
learning framework (Kearns and Li, 1988; Haus-
sler, 1992; Kearns et al., 1994), where an adver-
sary can not only choose the placement of the hard
cases, but also their labels. In worst case, the 0-1
loss model would sustain a constant rate of error
due to malicious noise, whereas annotation noise
is tolerated quite well in large datasets.
</bodyText>
<sectionHeader confidence="0.977491" genericHeader="method">
3 Voted Perceptron
</sectionHeader>
<bodyText confidence="0.999895272727273">
Freund and Schapire (1999) describe the voted
perceptron. This algorithm and its many vari-
ants are widely used in the computational lin-
guistics community (Collins, 2002a; Collins and
Duffy, 2002; Collins, 2002b; Collins and Roark,
2004; Henderson and Titov, 2005; Viola and
Narasimhan, 2005; Cohen et al., 2004; Carreras
et al., 2005; Shen and Joshi, 2005; Ciaramita and
Johnson, 2003). In this section, we show that the
voted perceptron can be vulnerable to annotation
noise. The algorithm is shown below.
</bodyText>
<figure confidence="0.446636">
Algorithm 1 Voted Perceptron
Training
</figure>
<figureCaption confidence="0.3145885">
Input: a labeled training set (x1, y1), ... , (xN, yN)
Output: a list of perceptrons w1, ... , wN
</figureCaption>
<equation confidence="0.988389">
Initialize: t ← 0; w1 ← 0; ψ1 ← 0
fort = 1 ... N do
yt ← sign(hwt, xti + ψt)
wt+1 ← wt + yt�ˆyt
2 · xt
ψt+1 ← ψt + yt�ˆyt
2 · hwt, xti
</equation>
<subsectionHeader confidence="0.464477">
end for
</subsectionHeader>
<bodyText confidence="0.84861325">
Forecasting
Input: a list of perceptrons w1, ... ,wN
an unlabeled instance x
Output: A forecasted label y
</bodyText>
<equation confidence="0.815884">
y� ← ENt=1 sign(hwt, xti + ψt)
y ← sign(y)
</equation>
<bodyText confidence="0.99992025">
The voted perceptron algorithm is a refinement
of the perceptron algorithm (Rosenblatt, 1962;
Minsky and Papert, 1969). Perceptron is a dy-
namic algorithm; starting with an initial hyper-
plane w0, it passes repeatedly through the labeled
sample. Whenever an instance is misclassified
by wt, the hyperplane is modified to adapt to the
instance. The algorithm terminates once it has
passed through the sample without making any
classification mistakes. The algorithm terminates
iff the sample can be separated by a hyperplane,
and in this case the algorithm finds a separating
hyperplane. Novikoff (1962) gives a bound on the
number of iterations the algorithm goes through
before termination, when the sample is separable
by a margin.
</bodyText>
<page confidence="0.983889">
282
</page>
<bodyText confidence="0.999974052631579">
The perceptron algorithm is vulnerable to noise,
as even a little noise could make the sample in-
separable. In this case the algorithm would cycle
indefinitely never meeting termination conditions,
wt would obtain values within a certain dynamic
range but would not converge. In such setting,
imposing a stopping time would be equivalent to
drawing a random vector from the dynamic range.
Freund and Schapire (1999) extend the percep-
tron to inseparable samples with their voted per-
ceptron algorithm and give theoretical generaliza-
tion bounds for its performance. The basic idea
underlying the algorithm is that if the dynamic
range of the perceptron is not too large then wt
would classify most instances correctly most of
the time (for most values of t). Thus, for a sample
x1, ... , xN the new algorithm would keep track
of w0, ... , wN, and for an unlabeled instance x it
would forecast the classification most prominent
amongst these hyperplanes.
The bounds given by Freund and Schapire
(1999) depend on the hinge loss of the dataset. In
section 3.2 we construct a difficult setting for this
algorithm. To prove that voted perceptron would
suffer from a constant hard case bias in this set-
ting using the exact dynamics of the perceptron is
beyond the scope of this article. Instead, in sec-
tion 3.3 we provide a lower bound on the hinge
loss for a simplified model of the perceptron algo-
rithm dynamics, which we argue would be a good
approximation to the true dynamics in the setting
we constructed. For this simplified model, we
show that the hinge loss is large, and the bounds
in Freund and Schapire (1999) cannot rule out a
constant level of error regardless of the size of the
dataset. In section 3.4 we study the dynamics of
the model and prove that τ = θ(1) for the adver-
sarial setting.
</bodyText>
<subsectionHeader confidence="0.994325">
3.1 Hinge Loss
</subsectionHeader>
<bodyText confidence="0.854871">
Definition 2 The hinge loss of a labeled instance
</bodyText>
<equation confidence="0.577819666666667">
(x, y) with respect to hyperplane (w, ψ) and mar-
gin δ &gt; 0 is given by ζ = ζ(ψ, δ) = max(0, δ −
y - (* x) − ψ)).
</equation>
<bodyText confidence="0.99967625">
ζ measures the distance of an instance from
being classified correctly with a δ margin. Figure 2
shows examples of hinge loss for various data
points.
</bodyText>
<note confidence="0.308488">
Theorem 2 (Freund and Schapire (1999))
</note>
<footnote confidence="0.5947925">
After one pass on the sample, the probability
that the voted perceptron algorithm does not
</footnote>
<figureCaption confidence="0.9965575">
Figure 2: Hinge loss ζ for various data points in-
curred by the separator with margin δ.
</figureCaption>
<bodyText confidence="0.8462745">
predict correctly the label of a test instance
xN+1 is bounded by N+i1EN+1 d sD 2 where
</bodyText>
<equation confidence="0.96129">
D = D(w, ψ, δ) = /EiN 1 ζ2i .
</equation>
<bodyText confidence="0.998760833333333">
This result is used to explain the convergence of
weighted or voted perceptron algorithms (Collins,
2002a). It is useful as long as the expected value of
D is not too large. We show that in an adversarial
setting of the annotation noise D is large, hence
these bounds are trivial.
</bodyText>
<subsectionHeader confidence="0.997144">
3.2 Adversarial Annotation Noise
</subsectionHeader>
<bodyText confidence="0.9896616">
Let a sample be a sequence x1, ... , xN drawn uni-
formly from Id with y1, ... , yN E {−1,1}. Easy
cases are labeled y = y(x) = sgn(v) as before,
with v = v(x) = Ei=1...d xi. The true separation
plane for the easy instances is w* = �1 d(1...1),
ψ* = 0. Suppose hard cases are those where
-/
v(x) &gt; c1 d, where c1 is chosen so that the
hard instances account for γN of all instances.3
Figure 3 shows this setting.
</bodyText>
<subsectionHeader confidence="0.996097">
3.3 Lower Bound on Hinge Loss
</subsectionHeader>
<bodyText confidence="0.999883857142857">
In the simplified case, we assume that the algo-
rithm starts training with the hyperplane w0 =
w* = 1d(1...1), and keeps it throughout the
training, only updating ψ. In reality, each hard in-
stance can be decomposed into a component that is
parallel to w*, and a component that is orthogonal
to it. The expected contribution of the orthogonal
</bodyText>
<footnote confidence="0.946951">
3See the proof of 0-1 case for a similar construction using
the central limit theorem.
</footnote>
<equation confidence="0.99525">
s
t
t
t
t
t t
</equation>
<page confidence="0.995051">
283
</page>
<figure confidence="0.8219045">
misclassified easy instances dominate the loss:
0 c1√d
</figure>
<figureCaption confidence="0.9751885">
Figure 3: An adversarial case of annotation noise
for the voted perceptron algorithm.
</figureCaption>
<bodyText confidence="0.993308125">
component to the algorithm’s update will be posi-
tive due to the systematic positioning of the hard
cases, while the contributions of the parallel com-
ponents are expected to cancel out due to the sym-
metry of the hard cases around the main diagonal
that is orthogonal to w*. Thus, while wt will not
necessarily parallel w*, it will be close to parallel
for most t &gt; 0. The simplified case is thus a good
approximation of the real case, and the bound we
obtain is expected to hold for the real case as well.
For any initial value ψ0 &lt; 0 all misclassified in-
stances are labeled −1 and classified as +1, hence
the update will increase ψ0, and reach 0 soon
enough. We can therefore assume that ψt ≥ 0
for any t &gt; t0 where t0 « N.
Lemma 3 For any t &gt; t0, there exist α =
α(γ, T) &gt; 0 such that ]F(ζ2) ≥ α · δ.
Proof: For ψ ≥ 0 there are two main sources
of hinge loss: easy +1 instances that are clas-
sified as −1, and hard -1 instances classified as
+1. These correspond to the two components of
the following sum (the inequality is due to disre-
garding the loss incurred by a correct classification
with too wide a margin):
</bodyText>
<equation confidence="0.9723133125">
~d~
1 l ψ 2
2dl(√d−√d + δ)
√
Let 0 &lt; T &lt; c1 be a parameter. For ψ &gt; T d,
1dψl 2
2d l (√d−√d +δ
1 (d) T√d l 2
2dl( √d −√d + δ)
~d �
1 (T −√ l d +δ)2
2d l
Z T
1
≥ √2π (T + δ − t)2e−t2/2dt = HT(δ)
0
</equation>
<bodyText confidence="0.999830666666667">
The last inequality follows from a normal ap-
proximation of the binomial distribution (see, for
example, Feller (1968)).
</bodyText>
<equation confidence="0.913831">
√
For 0 ≤ ψ ≤ T d, misclassified hard cases
dominate:
1 d ~d~
]F(ζ2) ≥ 1 l ψ 2
2 2dl(√d−√d + δ)
,/
l=c1 d
≥ 1 Xd 1 (d) l T √d 2
2 ,/ 2dl ( √d −√d + δ)
l=c1 d
1 (t − T + δ)2e−t2/2dt
√2πl&amp;quot; (γ)
= Hγ(δ)
</equation>
<bodyText confidence="0.994396363636364">
where Φ−1(γ) is the inverse of the normal distri-
bution density.
Thus ]F(ζ2) ≥ min{HT(δ), Hγ(δ)}, and
there exists α = α(γ, T) &gt; 0 such that
min{HT(δ), Hγ(δ)} ≥ α · δ ❑
Corollary 4 The bound in theorem 2 does not
converge to zero for large N.
We recall that Freund and Schapire (1999) bound
is proportional to D2 = PNi=1 ζ2i . It follows from
lemma 3 that D2 = θ(N), hence the bound is in-
effective.
</bodyText>
<subsectionHeader confidence="0.9160865">
3.4 Lower Bound on τ for Voted Perceptron
Under Simplified Dynamics
</subsectionHeader>
<bodyText confidence="0.99996625">
Corollary 4 does not give an estimate on the hard
case bias. Indeed, it could be that wt = w* for
almost every t. There would still be significant
hinge in this case, but the hard case bias for the
voted forecast would be zero. To assess the hard
case bias we need a model of perceptron dyna-
mics that would account for the history of hyper-
planes w0, ... , wN the perceptron goes through on
</bodyText>
<equation confidence="0.991241421052631">
]F(ζ2) ≥ X [ψ] 1dψl 2
l=0 2d l (√d−√d + δ)
1 Xd
2
,/
l=c1 d
+
]F(ζ2) ≥ X [ψ]
l=0
≥ ,/
[T X d]
l=0
,/
T Xd
≥
l=0
1 ·
2
≥
</equation>
<page confidence="0.987718">
284
</page>
<bodyText confidence="0.9990486">
a sample x1, ... , xN. The key simplification in
our model is assuming that wt parallels w∗ for all
t, hence the next hyperplane depends only on the
offset ψt. This is a one dimensional Markov ran-
dom walk governed by the distribution
</bodyText>
<equation confidence="0.954362">
P(ψt+1−ψt = r|ψt) = P(x|yt −
2
</equation>
<bodyText confidence="0.944310608695652">
In general −d ≤ ψt ≤ d but as mentioned before
lemma 3, we may assume ψt &gt; 0.
Lemma 5 There exists c &gt; 0 such that with a high
√
probability ψt &gt; c · d for most 0 ≤ t ≤ N.
Proof: Let c0 = F−1(&apos;2+&apos; 12); c1 = F −1(1−γ).
We designate the intervals I0 = [0, c0 · √d]; I1 =
[c0 · √d, c1 · √d] and I2 = [c1 · √d, d] and define
AZ = {x : v(x) ∈ IZ} for i = 0, 1, 2. Note that the
constants c0 and c1 are chosen so that P(A0) = 2
and P(A2) = γ. It follows from the construction
in section 3.2 that A0 and A1 are easy instances
and A2 are hard. Given a sample x1,... , xN, a
misclassification of xt ∈ A0 by ψt could only hap-
pen when an easy +1 instance is classified as −1.
Thus the algorithm would shift ψt to the left by
no more than |vt − ψt |since vt =
shows that ψt ∈ I0 implies ψt+1 ∈ I0. In the
same manner, it is easy to verify that if ψt ∈ Ij
and xt ∈ Ak then ψt+1 ∈ Ik, unless j = 0 and
k = 1, in which case ψt+1 ∈ I0 because xt ∈ A1
would be classified correctly by ψt ∈ I0.
We construct a Markov chain with three states
</bodyText>
<equation confidence="0.930628714285714">
√ √
a0 = 0, a1 = c0 · d and a2 = c1 · d governed
by the following transition distribution:
1 − 2 0 2
� �
1 − γ
2 2
</equation>
<bodyText confidence="0.927748869565217">
Let Xt be the state at time t. The principal eigen-
vector of the transition matrix (13, 13, 13) gives the
stationary probability distribution of Xt. Thus
Xt ∈ {a1, a2} with probability 23. Since the tran-
sition distribution of Xt mirrors that of ψt, and
since aj are at the leftmost borders of Ij, respec-
tively, it follows that Xt ≤ ψt for all t, thus
Xt ∈ {a1, a2} implies ψt ∈ I1 ∪I2. It follows that
√
ψt &gt; c0 · d with probability 23, and the lemma
follows from the law of large numbers ❑
Corollary 6 With high probability τ = θ(1).
Proof: Lemma 5 shows that for a sample
x1, ... , xN with high probability ψt is most of
√
the time to the right of c · d. Consequently
√
for any x in the band 0 ≤ v ≤ c · d we get
sign(hw∗, xi+ψt) = −1 for most t hence by defi-
nition, the voted perceptron would classify such
an instance as −1, although it is in fact a +1 easy
instance. Since there are θ(N) misclassified easy
instances, τ = θ(1) ❑
</bodyText>
<sectionHeader confidence="0.998182" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.99997087804878">
In this article we show that training with annota-
tion noise can be detrimental for test-time results
on easy, uncontroversial instances; we termed this
phenomenon hard case bias. Although under
the 0-1 loss model annotation noise can be tole-
rated for larger datasets (theorem 1), minimizing
such loss becomes intractable for larger datasets.
Freund and Schapire (1999) voted perceptron al-
gorithm and its variants are widely used in compu-
tational linguistics practice; our results show that
it could suffer a constant rate of hard case bias ir-
respective of the size of the dataset (section 3.4).
How can hard case bias be reduced? One pos-
sibility is removing as many hard cases as one
can not only from the test data, as suggested in
Beigman Klebanov and Beigman (2009), but from
the training data as well. Adding the second an-
notator is expected to detect about half the hard
cases, as they would surface as disagreements be-
tween the annotators. Subsequently, a machine
learner can be told to ignore those cases during
training, reducing the risk of hard case bias. While
this is certainly a daunting task, it is possible that
for annotation studies that do not require expert
annotators and extensive annotator training, the
newly available access to a large pool of inexpen-
sive annotators, such as the Amazon Mechanical
Turk scheme (Snow et al., 2008),4 or embedding
the task in an online game played by volunteers
(Poesio et al., 2008; von Ahn, 2006) could provide
some solutions.
Reidsma and op den Akker (2008) suggest a
different option. When non-overlapping parts of
the dataset are annotated by different annotators,
each classifier can be trained to reflect the opinion
(albeit biased) of a specific annotator, using dif-
ferent parts of the datasets. Such “subjective ma-
chines” can be applied to a new set of data; an
item that causes disagreement between classifiers
is then extrapolated to be a case of potential dis-
agreement between the humans they replicate, i.e.
</bodyText>
<footnote confidence="0.585323">
4http://aws.amazon.com/mturk/
</footnote>
<equation confidence="0.934362818181818">
ˆyt ·hw∗, xi = r)
hw∗, xti. This
�
� � �
7
1 − 3-y
2 2 2
2 + γ
1
�
� � �
</equation>
<page confidence="0.995637">
285
</page>
<bodyText confidence="0.999974709677419">
a hard case. Our results suggest that, regardless
of the success of such an extrapolation scheme in
detecting hard cases, it could erroneously invali-
date easy cases: Each classifier would presumably
suffer from a certain hard case bias, i.e. classify
incorrectly things that are in fact uncontroversial
for any human annotator. If each such classifier
has a different hard case bias, some inter-classifier
disagreements would occur on easy cases. De-
pending on the distribution of those easy cases in
the feature space, this could invalidate valuable
cases. If the situation depicted in figure 1 corre-
sponds to the pattern learned by one of the clas-
sifiers, it would lead to marking the easy cases
closest to the real separation boundary (those be-
tween 0 and A,) as hard, and hence unsuitable for
learning, eliminating the most informative mate-
rial from the training data.
Reidsma and Carletta (2008) recently showed
by simulation that different types of annotator
behavior have different impact on the outcomes of
machine learning from the annotated data. Our re-
sults provide a theoretical analysis that points in
the same direction: While random classification
noise is tolerable, other types of noise – such as
annotation noise handled here – are more proble-
matic. It is therefore important to develop models
of annotator behavior and of the resulting imper-
fections of the annotated datasets, in order to di-
agnose the potential learning problem and suggest
mitigation strategies.
</bodyText>
<sectionHeader confidence="0.997243" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995194410958904">
Dana Angluin and Philip Laird. 1988. Learning from
Noisy Examples. Machine Learning, 2(4):343–370.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From Annotator Agreement to Noise Models. Com-
putational Linguistics, accepted for publication.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing Disagreements. In
COLING 2008 Workshop on Human Judgments in
Computational Linguistics, pages 2–7, Manchester,
UK.
Avrim Blum, Alan Frieze, Ravi Kannan, and Santosh
Vempala. 1996. A Polynomial-Time Algorithm for
Learning Noisy Linear Threshold Functions. In Pro-
ceedings of the 37th Annual IEEE Symposium on
Foundations of Computer Science, pages 330–338,
Burlington, Vermont, USA.
Xavier Carreras, Ll´uis M`arquez, and Jorge Castro.
2005. Filtering-Ranking Perceptron Learning for
Partial Parsing. Machine Learning, 60(1):41–71.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense Tagging of Unknown Nouns in WordNet.
In Proceedings of the Empirical Methods in Natural
Language Processing Conference, pages 168–175,
Sapporo, Japan.
William Cohen, Vitor Carvalho, and Tom Mitchell.
2004. Learning to Classify Email into “Speech
Acts”. In Proceedings of the Empirical Methods
in Natural Language Processing Conference, pages
309–316, Barcelona, Spain.
Edith Cohen. 1997. Learning Noisy Perceptrons by
a Perceptron in Polynomial Time. In Proceedings
of the 38th Annual Symposium on Foundations of
Computer Science, pages 514–523, Miami Beach,
Florida, USA.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In
Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 263–370,
Philadelphia, USA.
Michael Collins and Brian Roark. 2004. Incremen-
tal Parsing with the Perceptron Algorithm. In Pro-
ceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 111–118,
Barcelona, Spain.
Michael Collins. 2002a. Discriminative Training
Methods for Hidden Markov Hodels: Theory and
Experiments with Perceptron Algorithms. In Pro-
ceedings of the Empirical Methods in Natural Lan-
guage Processing Conference, pages 1–8, Philadel-
phia, USA.
Michael Collins. 2002b. Ranking Algorithms for
Named Entity Extraction: Boosting and the Voted
Perceptron. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 489–496, Philadelphia, USA.
Vitaly Feldman, Parikshit Gopalan, Subhash Khot, and
Ashok Ponnuswami. 2006. New Results for Learn-
ing Noisy Parities and Halfspaces. In Proceedings
of the 47th Annual IEEE Symposium on Foundations
of Computer Science, pages 563–574, Los Alamitos,
CA, USA.
William Feller. 1968. An Introduction to Probability
Theory and Its Application, volume 1. Wiley, New
York, 3rd edition.
Yoav Freund and Robert Schapire. 1999. Large Mar-
gin Classification Using the Perceptron Algorithm.
Machine Learning, 37(3):277–296.
Venkatesan Guruswami and Prasad Raghavendra.
2006. Hardness of Learning Halfspaces with Noise.
In Proceedings of the 47th Annual IEEE Symposium
on Foundations of Computer Science, pages 543–
552, Los Alamitos, CA, USA.
</reference>
<page confidence="0.976764">
286
</page>
<reference confidence="0.999886696969697">
David Haussler. 1992. Decision Theoretic General-
izations of the PAC Model for Neural Net and other
Learning Applications. Information and Computa-
tion, 100(1):78–150.
James Henderson and Ivan Titov. 2005. Data-Defined
Kernels for Parse Reranking Derived from Proba-
bilistic Models. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 181–188, Ann Arbor, Michigan, USA.
Michael Kearns and Ming Li. 1988. Learning in the
Presence of Malicious Errors. In Proceedings of the
20th Annual ACM symposium on Theory of Comput-
ing, pages 267–280, Chicago, USA.
Michael Kearns, Robert Schapire, and Linda Sellie.
1994. Toward Efficient Agnostic Learning. Ma-
chine Learning, 17(2):115–141.
Michael Kearns. 1993. Efficient Noise-Tolerant
Learning from Statistical Queries. In Proceedings
of the 25th Annual ACM Symposium on Theory of
Computing, pages 392–401, San Diego, CA, USA.
Marvin Minsky and Seymour Papert. 1969. Percep-
trons: An Introduction to Computational Geometry.
MIT Press, Cambridge, Mass.
A. B. Novikoff. 1962. On convergence proofs on per-
ceptrons. Symposium on the Mathematical Theory
of Automata, 12:615–622.
Miles Osborne. 2002. Shallow Parsing Using Noisy
and Non-Stationary Training Material. Journal of
Machine Learning Research, 2:695–719.
Massimo Poesio, Udo Kruschwitz, and Chamberlain
Jon. 2008. ANAWIKI: Creating Anaphorically An-
notated Resources through Web Cooperation. In
Proceedings of the 6th International Language Re-
sources and Evaluation Conference, Marrakech,
Morocco.
Dennis Reidsma and Jean Carletta. 2008. Reliability
measurement without limit. Computational Linguis-
tics, 34(3):319–326.
Dennis Reidsma and Rieks op den Akker. 2008. Ex-
ploiting Subjective Annotations. In COLING 2008
Workshop on Human Judgments in Computational
Linguistics, pages 8–16, Manchester, UK.
Frank Rosenblatt. 1962. Principles of Neurodynamics:
Perceptrons and the Theory of Brain Mechanisms.
Spartan Books, Washington, D.C.
Libin Shen and Aravind Joshi. 2005. Incremen-
tal LTAG Parsing. In Proceedings of the Human
Language Technology Conference and Empirical
Methods in Natural Language Processing Confer-
ence, pages 811–818, Vancouver, British Columbia,
Canada.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and Fast – But is it
Good? Evaluating Non-Expert Annotations for Nat-
ural Language Tasks. In Proceedings of the Empir-
ical Methods in Natural Language Processing Con-
ference, pages 254–263, Honolulu, Hawaii.
Paul Viola and Mukund Narasimhan. 2005. Learning
to Extract Information from Semi-Structured Text
Using a Discriminative Context Free Grammar. In
Proceedings of the 28th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval, pages 330–337, Salvador,
Brazil.
Luis von Ahn. 2006. Games with a purpose. Com-
puter, 39(6):92–94.
</reference>
<page confidence="0.997405">
287
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.865902">
<title confidence="0.999615">Learning with Annotation Noise</title>
<author confidence="0.929325">Eyal Beigman Beata Beigman Klebanov</author>
<affiliation confidence="0.9764725">Olin Business School Kellogg School of Management Washington University in St. Louis Northwestern University</affiliation>
<email confidence="0.999059">beigman@wustl.edubeata@northwestern.edu</email>
<abstract confidence="0.997386">It is usually assumed that the kind of noise existing in annotated data is random classification noise. Yet there is evidence that differences between annotators are not always random attention slips but could result from different biases towards the classification categories, at least for the harder-to-decide cases. Under an annotation generation model that takes this into account, there is a hazard that some of the training instances are actually hard cases with unreliable annotations. We show that these are relatively unproblematic for an algorithm operating under the 0-1 loss model, whereas for the commonly used voted perceptron algorithm, hard training cases could result in incorrect prediction the at test time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dana Angluin</author>
<author>Philip Laird</author>
</authors>
<title>Learning from Noisy Examples.</title>
<date>1988</date>
<booktitle>Machine Learning,</booktitle>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="1187" citStr="Angluin and Laird, 1988" startWordPosition="167" endWordPosition="170">n generation model that takes this into account, there is a hazard that some of the training instances are actually hard cases with unreliable annotations. We show that these are relatively unproblematic for an algorithm operating under the 0-1 loss model, whereas for the commonly used voted perceptron algorithm, hard training cases could result in incorrect prediction on the uncontroversial cases at test time. 1 Introduction It is assumed, often tacitly, that the kind of noise existing in human-annotated datasets used in computational linguistics is random classification noise (Kearns, 1993; Angluin and Laird, 1988), resulting from annotator attention slips randomly distributed across instances. For example, Osborne (2002) evaluates noise tolerance of shallow parsers, with random classification noise taken to be “crudely approximating annotation errors.” It has been shown, both theoretically and empirically, that this type of noise is tolerated well by the commonly used machine learning algorithms (Cohen, 1997; Blum et al., 1996; Osborne, 2002; Reidsma and Carletta, 2008). Yet this might be overly optimistic. Reidsma and op den Akker (2008) show that apparent differences between annotators are not random</context>
</contexts>
<marker>Angluin, Laird, 1988</marker>
<rawString>Dana Angluin and Philip Laird. 1988. Learning from Noisy Examples. Machine Learning, 2(4):343–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Eyal Beigman</author>
</authors>
<title>From Annotator Agreement to Noise Models. Computational Linguistics, accepted for publication.</title>
<date>2009</date>
<contexts>
<context position="2751" citStr="Klebanov and Beigman (2009)" startWordPosition="405" endWordPosition="408"> performance on the test data. A small subset of doubly annotated data (for inter-annotator agreement check) and large chunks of singly annotated data (for training algorithms) is not uncommon in computational linguistics datasets; such a setup is prone to problems if annotators are differently biased.1 Annotator bias is consistent with a number of noise models. For example, it could be that an annotator’s bias is exercised on each and every instance, making his preferred category likelier for any instance than in another person’s annotations. Another possibility, recently explored by Beigman Klebanov and Beigman (2009), is that some items are really quite clear-cut for an annotator with any bias, belonging squarely within one particular category. However, some instances – termed hard cases therein – are harder to decide upon, and this is where various preferences and biases come into play. In a metaphor annotation study reported by Beigman Klebanov et al. (2008), certain markups received overwhelming annotator support when people were asked to validate annotations after a certain time delay. Other instances saw opinions split; moreover, Beigman Klebanov et al. (2008) observed cases where people retracted th</context>
<context position="8553" citStr="Klebanov and Beigman (2009)" startWordPosition="1430" endWordPosition="1433">ion. According to the central limit theorem, for d, N large, the distribution of v is well approximated by N(0√ , d). If N = c1 · 2d, for some 0 &lt; c1 &lt; 4, the second application of the central limit theorem ensures that, with high probability, about -yN = c1-y2d items would fall between ae and ah V/ (all hard), and √-y · 2− d 2 N = c1 -y2d would fall between 0 and ae (all easy, all labeled +1). Let Z be the sum of labels of the hard cases, Z = Ei=1...H yi. Applying the central limit theorem a third time, for large N, Z will, with a high probability, be distributed approximately as 2In Beigman Klebanov and Beigman (2009), annotation noise is defined as percentage of hard instances in the agreed annotations; this implies noise measurement on multiply annotated material. When there is just one annotator, no distinction between easy vs hard instances can be made; in this sense, all hard instances are posing as easy. 281 0 λe λh Figure 1: The adversarial case for 0-1 loss. Squares correspond to easy instances, circles – to hard ones. Filled squares and circles are labeled −1, empty ones are labeled +1. N(0, .\/γN). This implies that a value as low as −2σ cannot be ruled out with high (say 95%) confidence. Thus, a</context>
<context position="21951" citStr="Klebanov and Beigman (2009)" startWordPosition="4044" endWordPosition="4047">ermed this phenomenon hard case bias. Although under the 0-1 loss model annotation noise can be tolerated for larger datasets (theorem 1), minimizing such loss becomes intractable for larger datasets. Freund and Schapire (1999) voted perceptron algorithm and its variants are widely used in computational linguistics practice; our results show that it could suffer a constant rate of hard case bias irrespective of the size of the dataset (section 3.4). How can hard case bias be reduced? One possibility is removing as many hard cases as one can not only from the test data, as suggested in Beigman Klebanov and Beigman (2009), but from the training data as well. Adding the second annotator is expected to detect about half the hard cases, as they would surface as disagreements between the annotators. Subsequently, a machine learner can be told to ignore those cases during training, reducing the risk of hard case bias. While this is certainly a daunting task, it is possible that for annotation studies that do not require expert annotators and extensive annotator training, the newly available access to a large pool of inexpensive annotators, such as the Amazon Mechanical Turk scheme (Snow et al., 2008),4 or embedding</context>
</contexts>
<marker>Klebanov, Beigman, 2009</marker>
<rawString>Beata Beigman Klebanov and Eyal Beigman. 2009. From Annotator Agreement to Noise Models. Computational Linguistics, accepted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Eyal Beigman</author>
<author>Daniel Diermeier</author>
</authors>
<title>Analyzing Disagreements.</title>
<date>2008</date>
<booktitle>In COLING 2008 Workshop on Human Judgments in Computational Linguistics,</booktitle>
<pages>2--7</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="3101" citStr="Klebanov et al. (2008)" startWordPosition="463" endWordPosition="466">e models. For example, it could be that an annotator’s bias is exercised on each and every instance, making his preferred category likelier for any instance than in another person’s annotations. Another possibility, recently explored by Beigman Klebanov and Beigman (2009), is that some items are really quite clear-cut for an annotator with any bias, belonging squarely within one particular category. However, some instances – termed hard cases therein – are harder to decide upon, and this is where various preferences and biases come into play. In a metaphor annotation study reported by Beigman Klebanov et al. (2008), certain markups received overwhelming annotator support when people were asked to validate annotations after a certain time delay. Other instances saw opinions split; moreover, Beigman Klebanov et al. (2008) observed cases where people retracted their own earlier annotations. To start accounting for such annotator behavior, Beigman Klebanov and Beigman (2009) proposed a model where instances are either easy, and then all annotators agree on them, or hard, and then each annotator flips his or her own coin to de1The different biases might not amount to much in the small doubly annotated subset</context>
</contexts>
<marker>Klebanov, Beigman, Diermeier, 2008</marker>
<rawString>Beata Beigman Klebanov, Eyal Beigman, and Daniel Diermeier. 2008. Analyzing Disagreements. In COLING 2008 Workshop on Human Judgments in Computational Linguistics, pages 2–7, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Alan Frieze</author>
<author>Ravi Kannan</author>
<author>Santosh Vempala</author>
</authors>
<title>A Polynomial-Time Algorithm for Learning Noisy Linear Threshold Functions.</title>
<date>1996</date>
<booktitle>In Proceedings of the 37th Annual IEEE Symposium on Foundations of Computer Science,</booktitle>
<pages>330--338</pages>
<location>Burlington, Vermont, USA.</location>
<contexts>
<context position="1608" citStr="Blum et al., 1996" startWordPosition="228" endWordPosition="231">n It is assumed, often tacitly, that the kind of noise existing in human-annotated datasets used in computational linguistics is random classification noise (Kearns, 1993; Angluin and Laird, 1988), resulting from annotator attention slips randomly distributed across instances. For example, Osborne (2002) evaluates noise tolerance of shallow parsers, with random classification noise taken to be “crudely approximating annotation errors.” It has been shown, both theoretically and empirically, that this type of noise is tolerated well by the commonly used machine learning algorithms (Cohen, 1997; Blum et al., 1996; Osborne, 2002; Reidsma and Carletta, 2008). Yet this might be overly optimistic. Reidsma and op den Akker (2008) show that apparent differences between annotators are not random slips of attention but rather result from different biases annotators might have towards the classification categories. When training data comes from one annotator and test data from another, the first annotator’s biases are sometimes systematic enough for a machine learner to pick them up, with detrimental results for the algorithm’s performance on the test data. A small subset of doubly annotated data (for inter-an</context>
</contexts>
<marker>Blum, Frieze, Kannan, Vempala, 1996</marker>
<rawString>Avrim Blum, Alan Frieze, Ravi Kannan, and Santosh Vempala. 1996. A Polynomial-Time Algorithm for Learning Noisy Linear Threshold Functions. In Proceedings of the 37th Annual IEEE Symposium on Foundations of Computer Science, pages 330–338, Burlington, Vermont, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Ll´uis M`arquez</author>
<author>Jorge Castro</author>
</authors>
<title>Filtering-Ranking Perceptron Learning for Partial Parsing.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<volume>60</volume>
<issue>1</issue>
<marker>Carreras, M`arquez, Castro, 2005</marker>
<rawString>Xavier Carreras, Ll´uis M`arquez, and Jorge Castro. 2005. Filtering-Ranking Perceptron Learning for Partial Parsing. Machine Learning, 60(1):41–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Mark Johnson</author>
</authors>
<title>Supersense Tagging of Unknown Nouns in WordNet.</title>
<date>2003</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference,</booktitle>
<pages>168--175</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="11135" citStr="Ciaramita and Johnson, 2003" startWordPosition="1898" endWordPosition="1901">se the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Algorithm 1 Voted Perceptron Training Input: a labeled training set (x1, y1), ... , (xN, yN) Output: a list of perceptrons w1, ... , wN Initialize: t ← 0; w1 ← 0; ψ1 ← 0 fort = 1 ... N do yt ← sign(hwt, xti + ψt) wt+1 ← wt + yt�ˆyt 2 · xt ψt+1 ← ψt + yt�ˆyt 2 · hwt, xti end for Forecasting Input: a list of perceptrons w1, ... ,wN an unlabeled instance x Output: A forecasted label y y� ← ENt=1 sign(hwt, xti + ψt) y ← sign(y) The voted perceptron algorithm is a refinement of </context>
</contexts>
<marker>Ciaramita, Johnson, 2003</marker>
<rawString>Massimiliano Ciaramita and Mark Johnson. 2003. Supersense Tagging of Unknown Nouns in WordNet. In Proceedings of the Empirical Methods in Natural Language Processing Conference, pages 168–175, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Cohen</author>
<author>Vitor Carvalho</author>
<author>Tom Mitchell</author>
</authors>
<title>Learning to Classify Email into “Speech Acts”.</title>
<date>2004</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference,</booktitle>
<pages>309--316</pages>
<location>Barcelona,</location>
<contexts>
<context position="11060" citStr="Cohen et al., 2004" startWordPosition="1886" endWordPosition="1889"> 1992; Kearns et al., 1994), where an adversary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Algorithm 1 Voted Perceptron Training Input: a labeled training set (x1, y1), ... , (xN, yN) Output: a list of perceptrons w1, ... , wN Initialize: t ← 0; w1 ← 0; ψ1 ← 0 fort = 1 ... N do yt ← sign(hwt, xti + ψt) wt+1 ← wt + yt�ˆyt 2 · xt ψt+1 ← ψt + yt�ˆyt 2 · hwt, xti end for Forecasting Input: a list of perceptrons w1, ... ,wN an unlabeled instance x Output: A forecasted label y y� ← ENt=1 sign(hw</context>
</contexts>
<marker>Cohen, Carvalho, Mitchell, 2004</marker>
<rawString>William Cohen, Vitor Carvalho, and Tom Mitchell. 2004. Learning to Classify Email into “Speech Acts”. In Proceedings of the Empirical Methods in Natural Language Processing Conference, pages 309–316, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edith Cohen</author>
</authors>
<title>Learning Noisy Perceptrons by a Perceptron in Polynomial Time.</title>
<date>1997</date>
<booktitle>In Proceedings of the 38th Annual Symposium on Foundations of Computer Science,</booktitle>
<pages>514--523</pages>
<location>Miami Beach, Florida, USA.</location>
<contexts>
<context position="1589" citStr="Cohen, 1997" startWordPosition="226" endWordPosition="227">1 Introduction It is assumed, often tacitly, that the kind of noise existing in human-annotated datasets used in computational linguistics is random classification noise (Kearns, 1993; Angluin and Laird, 1988), resulting from annotator attention slips randomly distributed across instances. For example, Osborne (2002) evaluates noise tolerance of shallow parsers, with random classification noise taken to be “crudely approximating annotation errors.” It has been shown, both theoretically and empirically, that this type of noise is tolerated well by the commonly used machine learning algorithms (Cohen, 1997; Blum et al., 1996; Osborne, 2002; Reidsma and Carletta, 2008). Yet this might be overly optimistic. Reidsma and op den Akker (2008) show that apparent differences between annotators are not random slips of attention but rather result from different biases annotators might have towards the classification categories. When training data comes from one annotator and test data from another, the first annotator’s biases are sometimes systematic enough for a machine learner to pick them up, with detrimental results for the algorithm’s performance on the test data. A small subset of doubly annotated</context>
<context position="5667" citStr="Cohen, 1997" startWordPosition="883" endWordPosition="884">1 loss minimization principle could sustain a hard case bias of B(√1N) in the worst case. Thus, while annotation noise is hazardous for small datasets, it is better tolerated in larger ones. However, 0-1 loss minimization is computationally intractable for large datasets (Feldman et al., 2006; Guruswami and Raghavendra, 2006); substitute loss functions are often used in practice. While their tolerance to random classification noise is as good as for 0-1 loss, their tolerance to annotation noise is worse. For example, the perceptron family of algorithms handle random classification noise well (Cohen, 1997). We show in section 3.4 that the widely used Freund and Schapire (1999) voted perceptron algorithm could face a constant hard case bias when confronted with annotation noise in training data, irrespective of the size of the dataset. Finally, we discuss the implications of our findings for the practice of annotation studies and for data utilization in machine learning. 2 0-1 Loss Leta sample be a sequence x1, ... , xN drawn uniformly from the d-dimensional discrete cube Id = {−1,1}d with corresponding labels y1, ... , yN ∈ {−1,1}. Suppose further that the learning algorithm operates by finding</context>
</contexts>
<marker>Cohen, 1997</marker>
<rawString>Edith Cohen. 1997. Learning Noisy Perceptrons by a Perceptron in Polynomial Time. In Proceedings of the 38th Annual Symposium on Foundations of Computer Science, pages 514–523, Miami Beach, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--370</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="10944" citStr="Collins and Duffy, 2002" startWordPosition="1868" endWordPosition="1871"> different from more malicious types of noise analyzed in the agnostic learning framework (Kearns and Li, 1988; Haussler, 1992; Kearns et al., 1994), where an adversary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Algorithm 1 Voted Perceptron Training Input: a labeled training set (x1, y1), ... , (xN, yN) Output: a list of perceptrons w1, ... , wN Initialize: t ← 0; w1 ← 0; ψ1 ← 0 fort = 1 ... N do yt ← sign(hwt, xti + ψt) wt+1 ← wt + yt�ˆyt 2 · xt ψt+1 ← ψt + yt�ˆyt 2 · hwt, xti end for Forecast</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 263–370, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental Parsing with the Perceptron Algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>111--118</pages>
<location>Barcelona,</location>
<contexts>
<context position="10985" citStr="Collins and Roark, 2004" startWordPosition="1874" endWordPosition="1877">oise analyzed in the agnostic learning framework (Kearns and Li, 1988; Haussler, 1992; Kearns et al., 1994), where an adversary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Algorithm 1 Voted Perceptron Training Input: a labeled training set (x1, y1), ... , (xN, yN) Output: a list of perceptrons w1, ... , wN Initialize: t ← 0; w1 ← 0; ψ1 ← 0 fort = 1 ... N do yt ← sign(hwt, xti + ψt) wt+1 ← wt + yt�ˆyt 2 · xt ψt+1 ← ψt + yt�ˆyt 2 · hwt, xti end for Forecasting Input: a list of perceptrons w1, ... </context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental Parsing with the Perceptron Algorithm. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 111–118, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Hodels: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="10918" citStr="Collins, 2002" startWordPosition="1866" endWordPosition="1867">is qualitatively different from more malicious types of noise analyzed in the agnostic learning framework (Kearns and Li, 1988; Haussler, 1992; Kearns et al., 1994), where an adversary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Algorithm 1 Voted Perceptron Training Input: a labeled training set (x1, y1), ... , (xN, yN) Output: a list of perceptrons w1, ... , wN Initialize: t ← 0; w1 ← 0; ψ1 ← 0 fort = 1 ... N do yt ← sign(hwt, xti + ψt) wt+1 ← wt + yt�ˆyt 2 · xt ψt+1 ← ψt + yt�ˆyt 2 ·</context>
<context position="14985" citStr="Collins, 2002" startWordPosition="2592" endWordPosition="2593"> δ − y - (* x) − ψ)). ζ measures the distance of an instance from being classified correctly with a δ margin. Figure 2 shows examples of hinge loss for various data points. Theorem 2 (Freund and Schapire (1999)) After one pass on the sample, the probability that the voted perceptron algorithm does not Figure 2: Hinge loss ζ for various data points incurred by the separator with margin δ. predict correctly the label of a test instance xN+1 is bounded by N+i1EN+1 d sD 2 where D = D(w, ψ, δ) = /EiN 1 ζ2i . This result is used to explain the convergence of weighted or voted perceptron algorithms (Collins, 2002a). It is useful as long as the expected value of D is not too large. We show that in an adversarial setting of the annotation noise D is large, hence these bounds are trivial. 3.2 Adversarial Annotation Noise Let a sample be a sequence x1, ... , xN drawn uniformly from Id with y1, ... , yN E {−1,1}. Easy cases are labeled y = y(x) = sgn(v) as before, with v = v(x) = Ei=1...d xi. The true separation plane for the easy instances is w* = �1 d(1...1), ψ* = 0. Suppose hard cases are those where -/ v(x) &gt; c1 d, where c1 is chosen so that the hard instances account for γN of all instances.3 Figure 3</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002a. Discriminative Training Methods for Hidden Markov Hodels: Theory and Experiments with Perceptron Algorithms. In Proceedings of the Empirical Methods in Natural Language Processing Conference, pages 1–8, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking Algorithms for Named Entity Extraction: Boosting and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>489--496</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="10918" citStr="Collins, 2002" startWordPosition="1866" endWordPosition="1867">is qualitatively different from more malicious types of noise analyzed in the agnostic learning framework (Kearns and Li, 1988; Haussler, 1992; Kearns et al., 1994), where an adversary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Algorithm 1 Voted Perceptron Training Input: a labeled training set (x1, y1), ... , (xN, yN) Output: a list of perceptrons w1, ... , wN Initialize: t ← 0; w1 ← 0; ψ1 ← 0 fort = 1 ... N do yt ← sign(hwt, xti + ψt) wt+1 ← wt + yt�ˆyt 2 · xt ψt+1 ← ψt + yt�ˆyt 2 ·</context>
<context position="14985" citStr="Collins, 2002" startWordPosition="2592" endWordPosition="2593"> δ − y - (* x) − ψ)). ζ measures the distance of an instance from being classified correctly with a δ margin. Figure 2 shows examples of hinge loss for various data points. Theorem 2 (Freund and Schapire (1999)) After one pass on the sample, the probability that the voted perceptron algorithm does not Figure 2: Hinge loss ζ for various data points incurred by the separator with margin δ. predict correctly the label of a test instance xN+1 is bounded by N+i1EN+1 d sD 2 where D = D(w, ψ, δ) = /EiN 1 ζ2i . This result is used to explain the convergence of weighted or voted perceptron algorithms (Collins, 2002a). It is useful as long as the expected value of D is not too large. We show that in an adversarial setting of the annotation noise D is large, hence these bounds are trivial. 3.2 Adversarial Annotation Noise Let a sample be a sequence x1, ... , xN drawn uniformly from Id with y1, ... , yN E {−1,1}. Easy cases are labeled y = y(x) = sgn(v) as before, with v = v(x) = Ei=1...d xi. The true separation plane for the easy instances is w* = �1 d(1...1), ψ* = 0. Suppose hard cases are those where -/ v(x) &gt; c1 d, where c1 is chosen so that the hard instances account for γN of all instances.3 Figure 3</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002b. Ranking Algorithms for Named Entity Extraction: Boosting and the Voted Perceptron. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 489–496, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vitaly Feldman</author>
<author>Parikshit Gopalan</author>
<author>Subhash Khot</author>
<author>Ashok Ponnuswami</author>
</authors>
<title>New Results for Learning Noisy Parities and Halfspaces.</title>
<date>2006</date>
<booktitle>In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science,</booktitle>
<pages>563--574</pages>
<location>Los Alamitos, CA, USA.</location>
<contexts>
<context position="5348" citStr="Feldman et al., 2006" startWordPosition="834" endWordPosition="837">test time can be hurt by the presence of hard instances in training data. Definition 1 The hard case bias, T, is the portion of easy instances in the test data that are misclassified as a result of hard instances in the training data. This article proceeds as follows. First, we show that a machine learner operating under a 0-1 loss minimization principle could sustain a hard case bias of B(√1N) in the worst case. Thus, while annotation noise is hazardous for small datasets, it is better tolerated in larger ones. However, 0-1 loss minimization is computationally intractable for large datasets (Feldman et al., 2006; Guruswami and Raghavendra, 2006); substitute loss functions are often used in practice. While their tolerance to random classification noise is as good as for 0-1 loss, their tolerance to annotation noise is worse. For example, the perceptron family of algorithms handle random classification noise well (Cohen, 1997). We show in section 3.4 that the widely used Freund and Schapire (1999) voted perceptron algorithm could face a constant hard case bias when confronted with annotation noise in training data, irrespective of the size of the dataset. Finally, we discuss the implications of our fin</context>
</contexts>
<marker>Feldman, Gopalan, Khot, Ponnuswami, 2006</marker>
<rawString>Vitaly Feldman, Parikshit Gopalan, Subhash Khot, and Ashok Ponnuswami. 2006. New Results for Learning Noisy Parities and Halfspaces. In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science, pages 563–574, Los Alamitos, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Feller</author>
</authors>
<title>An Introduction to Probability Theory and Its Application,</title>
<date>1968</date>
<volume>1</volume>
<publisher>Wiley,</publisher>
<location>New York,</location>
<note>3rd edition.</note>
<contexts>
<context position="17652" citStr="Feller (1968)" startWordPosition="3128" endWordPosition="3129"> 0 there are two main sources of hinge loss: easy +1 instances that are classified as −1, and hard -1 instances classified as +1. These correspond to the two components of the following sum (the inequality is due to disregarding the loss incurred by a correct classification with too wide a margin): ~d~ 1 l ψ 2 2dl(√d−√d + δ) √ Let 0 &lt; T &lt; c1 be a parameter. For ψ &gt; T d, 1dψl 2 2d l (√d−√d +δ 1 (d) T√d l 2 2dl( √d −√d + δ) ~d � 1 (T −√ l d +δ)2 2d l Z T 1 ≥ √2π (T + δ − t)2e−t2/2dt = HT(δ) 0 The last inequality follows from a normal approximation of the binomial distribution (see, for example, Feller (1968)). √ For 0 ≤ ψ ≤ T d, misclassified hard cases dominate: 1 d ~d~ ]F(ζ2) ≥ 1 l ψ 2 2 2dl(√d−√d + δ) ,/ l=c1 d ≥ 1 Xd 1 (d) l T √d 2 2 ,/ 2dl ( √d −√d + δ) l=c1 d 1 (t − T + δ)2e−t2/2dt √2πl&amp;quot; (γ) = Hγ(δ) where Φ−1(γ) is the inverse of the normal distribution density. Thus ]F(ζ2) ≥ min{HT(δ), Hγ(δ)}, and there exists α = α(γ, T) &gt; 0 such that min{HT(δ), Hγ(δ)} ≥ α · δ ❑ Corollary 4 The bound in theorem 2 does not converge to zero for large N. We recall that Freund and Schapire (1999) bound is proportional to D2 = PNi=1 ζ2i . It follows from lemma 3 that D2 = θ(N), hence the bound is ineffective. </context>
</contexts>
<marker>Feller, 1968</marker>
<rawString>William Feller. 1968. An Introduction to Probability Theory and Its Application, volume 1. Wiley, New York, 3rd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert Schapire</author>
</authors>
<title>Large Margin Classification Using the Perceptron Algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="5739" citStr="Freund and Schapire (1999)" startWordPosition="894" endWordPosition="897">as of B(√1N) in the worst case. Thus, while annotation noise is hazardous for small datasets, it is better tolerated in larger ones. However, 0-1 loss minimization is computationally intractable for large datasets (Feldman et al., 2006; Guruswami and Raghavendra, 2006); substitute loss functions are often used in practice. While their tolerance to random classification noise is as good as for 0-1 loss, their tolerance to annotation noise is worse. For example, the perceptron family of algorithms handle random classification noise well (Cohen, 1997). We show in section 3.4 that the widely used Freund and Schapire (1999) voted perceptron algorithm could face a constant hard case bias when confronted with annotation noise in training data, irrespective of the size of the dataset. Finally, we discuss the implications of our findings for the practice of annotation studies and for data utilization in machine learning. 2 0-1 Loss Leta sample be a sequence x1, ... , xN drawn uniformly from the d-dimensional discrete cube Id = {−1,1}d with corresponding labels y1, ... , yN ∈ {−1,1}. Suppose further that the learning algorithm operates by finding a hyperplane (w, 0), w ∈ Rd, 0 ∈ R, that minimizes the empirical error </context>
<context position="10776" citStr="Freund and Schapire (1999)" startWordPosition="1842" endWordPosition="1845">et with 15% hard case rate, a hard case bias of about 1% cannot be ruled out with 95% confidence. Theorem 1 suggests that annotation noise as defined here is qualitatively different from more malicious types of noise analyzed in the agnostic learning framework (Kearns and Li, 1988; Haussler, 1992; Kearns et al., 1994), where an adversary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Algorithm 1 Voted Perceptron Training Input: a labeled training set (x1, y1), ... , (xN, yN) Output: a list of perceptr</context>
<context position="12835" citStr="Freund and Schapire (1999)" startWordPosition="2193" endWordPosition="2196">nd in this case the algorithm finds a separating hyperplane. Novikoff (1962) gives a bound on the number of iterations the algorithm goes through before termination, when the sample is separable by a margin. 282 The perceptron algorithm is vulnerable to noise, as even a little noise could make the sample inseparable. In this case the algorithm would cycle indefinitely never meeting termination conditions, wt would obtain values within a certain dynamic range but would not converge. In such setting, imposing a stopping time would be equivalent to drawing a random vector from the dynamic range. Freund and Schapire (1999) extend the perceptron to inseparable samples with their voted perceptron algorithm and give theoretical generalization bounds for its performance. The basic idea underlying the algorithm is that if the dynamic range of the perceptron is not too large then wt would classify most instances correctly most of the time (for most values of t). Thus, for a sample x1, ... , xN the new algorithm would keep track of w0, ... , wN, and for an unlabeled instance x it would forecast the classification most prominent amongst these hyperplanes. The bounds given by Freund and Schapire (1999) depend on the hin</context>
<context position="14582" citStr="Freund and Schapire (1999)" startWordPosition="2515" endWordPosition="2518">w that the hinge loss is large, and the bounds in Freund and Schapire (1999) cannot rule out a constant level of error regardless of the size of the dataset. In section 3.4 we study the dynamics of the model and prove that τ = θ(1) for the adversarial setting. 3.1 Hinge Loss Definition 2 The hinge loss of a labeled instance (x, y) with respect to hyperplane (w, ψ) and margin δ &gt; 0 is given by ζ = ζ(ψ, δ) = max(0, δ − y - (* x) − ψ)). ζ measures the distance of an instance from being classified correctly with a δ margin. Figure 2 shows examples of hinge loss for various data points. Theorem 2 (Freund and Schapire (1999)) After one pass on the sample, the probability that the voted perceptron algorithm does not Figure 2: Hinge loss ζ for various data points incurred by the separator with margin δ. predict correctly the label of a test instance xN+1 is bounded by N+i1EN+1 d sD 2 where D = D(w, ψ, δ) = /EiN 1 ζ2i . This result is used to explain the convergence of weighted or voted perceptron algorithms (Collins, 2002a). It is useful as long as the expected value of D is not too large. We show that in an adversarial setting of the annotation noise D is large, hence these bounds are trivial. 3.2 Adversarial Anno</context>
<context position="18137" citStr="Freund and Schapire (1999)" startWordPosition="3239" endWordPosition="3242">− t)2e−t2/2dt = HT(δ) 0 The last inequality follows from a normal approximation of the binomial distribution (see, for example, Feller (1968)). √ For 0 ≤ ψ ≤ T d, misclassified hard cases dominate: 1 d ~d~ ]F(ζ2) ≥ 1 l ψ 2 2 2dl(√d−√d + δ) ,/ l=c1 d ≥ 1 Xd 1 (d) l T √d 2 2 ,/ 2dl ( √d −√d + δ) l=c1 d 1 (t − T + δ)2e−t2/2dt √2πl&amp;quot; (γ) = Hγ(δ) where Φ−1(γ) is the inverse of the normal distribution density. Thus ]F(ζ2) ≥ min{HT(δ), Hγ(δ)}, and there exists α = α(γ, T) &gt; 0 such that min{HT(δ), Hγ(δ)} ≥ α · δ ❑ Corollary 4 The bound in theorem 2 does not converge to zero for large N. We recall that Freund and Schapire (1999) bound is proportional to D2 = PNi=1 ζ2i . It follows from lemma 3 that D2 = θ(N), hence the bound is ineffective. 3.4 Lower Bound on τ for Voted Perceptron Under Simplified Dynamics Corollary 4 does not give an estimate on the hard case bias. Indeed, it could be that wt = w* for almost every t. There would still be significant hinge in this case, but the hard case bias for the voted forecast would be zero. To assess the hard case bias we need a model of perceptron dynamics that would account for the history of hyperplanes w0, ... , wN the perceptron goes through on ]F(ζ2) ≥ X [ψ] 1dψl 2 l=0 2</context>
<context position="21551" citStr="Freund and Schapire (1999)" startWordPosition="3972" endWordPosition="3975">band 0 ≤ v ≤ c · d we get sign(hw∗, xi+ψt) = −1 for most t hence by definition, the voted perceptron would classify such an instance as −1, although it is in fact a +1 easy instance. Since there are θ(N) misclassified easy instances, τ = θ(1) ❑ 4 Discussion In this article we show that training with annotation noise can be detrimental for test-time results on easy, uncontroversial instances; we termed this phenomenon hard case bias. Although under the 0-1 loss model annotation noise can be tolerated for larger datasets (theorem 1), minimizing such loss becomes intractable for larger datasets. Freund and Schapire (1999) voted perceptron algorithm and its variants are widely used in computational linguistics practice; our results show that it could suffer a constant rate of hard case bias irrespective of the size of the dataset (section 3.4). How can hard case bias be reduced? One possibility is removing as many hard cases as one can not only from the test data, as suggested in Beigman Klebanov and Beigman (2009), but from the training data as well. Adding the second annotator is expected to detect about half the hard cases, as they would surface as disagreements between the annotators. Subsequently, a machin</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert Schapire. 1999. Large Margin Classification Using the Perceptron Algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Venkatesan Guruswami</author>
<author>Prasad Raghavendra</author>
</authors>
<title>Hardness of Learning Halfspaces with Noise.</title>
<date>2006</date>
<booktitle>In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science,</booktitle>
<pages>543--552</pages>
<location>Los Alamitos, CA, USA.</location>
<contexts>
<context position="5382" citStr="Guruswami and Raghavendra, 2006" startWordPosition="838" endWordPosition="841">by the presence of hard instances in training data. Definition 1 The hard case bias, T, is the portion of easy instances in the test data that are misclassified as a result of hard instances in the training data. This article proceeds as follows. First, we show that a machine learner operating under a 0-1 loss minimization principle could sustain a hard case bias of B(√1N) in the worst case. Thus, while annotation noise is hazardous for small datasets, it is better tolerated in larger ones. However, 0-1 loss minimization is computationally intractable for large datasets (Feldman et al., 2006; Guruswami and Raghavendra, 2006); substitute loss functions are often used in practice. While their tolerance to random classification noise is as good as for 0-1 loss, their tolerance to annotation noise is worse. For example, the perceptron family of algorithms handle random classification noise well (Cohen, 1997). We show in section 3.4 that the widely used Freund and Schapire (1999) voted perceptron algorithm could face a constant hard case bias when confronted with annotation noise in training data, irrespective of the size of the dataset. Finally, we discuss the implications of our findings for the practice of annotati</context>
</contexts>
<marker>Guruswami, Raghavendra, 2006</marker>
<rawString>Venkatesan Guruswami and Prasad Raghavendra. 2006. Hardness of Learning Halfspaces with Noise. In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science, pages 543– 552, Los Alamitos, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Decision Theoretic Generalizations of the PAC Model for Neural Net and other Learning Applications.</title>
<date>1992</date>
<journal>Information and Computation,</journal>
<volume>100</volume>
<issue>1</issue>
<contexts>
<context position="10447" citStr="Haussler, 1992" startWordPosition="1786" endWordPosition="1788">be more than θ( N) of the latter ❑ Note that the proof requires that N = θ(2d) namely, that asymptotically the sample includes a fixed portion of the instances. If the sample is asymptotically smaller, then λe will have to be adjusted such that λe = .\/d · F−1(θ( √1 � ) + 12). According to theorem 1, for a 10K dataset with 15% hard case rate, a hard case bias of about 1% cannot be ruled out with 95% confidence. Theorem 1 suggests that annotation noise as defined here is qualitatively different from more malicious types of noise analyzed in the agnostic learning framework (Kearns and Li, 1988; Haussler, 1992; Kearns et al., 1994), where an adversary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen</context>
</contexts>
<marker>Haussler, 1992</marker>
<rawString>David Haussler. 1992. Decision Theoretic Generalizations of the PAC Model for Neural Net and other Learning Applications. Information and Computation, 100(1):78–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Ivan Titov</author>
</authors>
<title>Data-Defined Kernels for Parse Reranking Derived from Probabilistic Models.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>181--188</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="11012" citStr="Henderson and Titov, 2005" startWordPosition="1878" endWordPosition="1881">stic learning framework (Kearns and Li, 1988; Haussler, 1992; Kearns et al., 1994), where an adversary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Algorithm 1 Voted Perceptron Training Input: a labeled training set (x1, y1), ... , (xN, yN) Output: a list of perceptrons w1, ... , wN Initialize: t ← 0; w1 ← 0; ψ1 ← 0 fort = 1 ... N do yt ← sign(hwt, xti + ψt) wt+1 ← wt + yt�ˆyt 2 · xt ψt+1 ← ψt + yt�ˆyt 2 · hwt, xti end for Forecasting Input: a list of perceptrons w1, ... ,wN an unlabeled instance x</context>
</contexts>
<marker>Henderson, Titov, 2005</marker>
<rawString>James Henderson and Ivan Titov. 2005. Data-Defined Kernels for Parse Reranking Derived from Probabilistic Models. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 181–188, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kearns</author>
<author>Ming Li</author>
</authors>
<title>Learning in the Presence of Malicious Errors.</title>
<date>1988</date>
<booktitle>In Proceedings of the 20th Annual ACM symposium on Theory of Computing,</booktitle>
<pages>267--280</pages>
<location>Chicago, USA.</location>
<contexts>
<context position="10431" citStr="Kearns and Li, 1988" startWordPosition="1782" endWordPosition="1785">es, there cannot .\/ be more than θ( N) of the latter ❑ Note that the proof requires that N = θ(2d) namely, that asymptotically the sample includes a fixed portion of the instances. If the sample is asymptotically smaller, then λe will have to be adjusted such that λe = .\/d · F−1(θ( √1 � ) + 12). According to theorem 1, for a 10K dataset with 15% hard case rate, a hard case bias of about 1% cannot be ruled out with 95% confidence. Theorem 1 suggests that annotation noise as defined here is qualitatively different from more malicious types of noise analyzed in the agnostic learning framework (Kearns and Li, 1988; Haussler, 1992; Kearns et al., 1994), where an adversary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasim</context>
</contexts>
<marker>Kearns, Li, 1988</marker>
<rawString>Michael Kearns and Ming Li. 1988. Learning in the Presence of Malicious Errors. In Proceedings of the 20th Annual ACM symposium on Theory of Computing, pages 267–280, Chicago, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kearns</author>
<author>Robert Schapire</author>
<author>Linda Sellie</author>
</authors>
<date>1994</date>
<booktitle>Toward Efficient Agnostic Learning. Machine Learning,</booktitle>
<volume>17</volume>
<issue>2</issue>
<contexts>
<context position="10469" citStr="Kearns et al., 1994" startWordPosition="1789" endWordPosition="1792">N) of the latter ❑ Note that the proof requires that N = θ(2d) namely, that asymptotically the sample includes a fixed portion of the instances. If the sample is asymptotically smaller, then λe will have to be adjusted such that λe = .\/d · F−1(θ( √1 � ) + 12). According to theorem 1, for a 10K dataset with 15% hard case rate, a hard case bias of about 1% cannot be ruled out with 95% confidence. Theorem 1 suggests that annotation noise as defined here is qualitatively different from more malicious types of noise analyzed in the agnostic learning framework (Kearns and Li, 1988; Haussler, 1992; Kearns et al., 1994), where an adversary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carrera</context>
</contexts>
<marker>Kearns, Schapire, Sellie, 1994</marker>
<rawString>Michael Kearns, Robert Schapire, and Linda Sellie. 1994. Toward Efficient Agnostic Learning. Machine Learning, 17(2):115–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kearns</author>
</authors>
<title>Efficient Noise-Tolerant Learning from Statistical Queries.</title>
<date>1993</date>
<booktitle>In Proceedings of the 25th Annual ACM Symposium on Theory of Computing,</booktitle>
<pages>392--401</pages>
<location>San Diego, CA, USA.</location>
<contexts>
<context position="1161" citStr="Kearns, 1993" startWordPosition="165" endWordPosition="166">r an annotation generation model that takes this into account, there is a hazard that some of the training instances are actually hard cases with unreliable annotations. We show that these are relatively unproblematic for an algorithm operating under the 0-1 loss model, whereas for the commonly used voted perceptron algorithm, hard training cases could result in incorrect prediction on the uncontroversial cases at test time. 1 Introduction It is assumed, often tacitly, that the kind of noise existing in human-annotated datasets used in computational linguistics is random classification noise (Kearns, 1993; Angluin and Laird, 1988), resulting from annotator attention slips randomly distributed across instances. For example, Osborne (2002) evaluates noise tolerance of shallow parsers, with random classification noise taken to be “crudely approximating annotation errors.” It has been shown, both theoretically and empirically, that this type of noise is tolerated well by the commonly used machine learning algorithms (Cohen, 1997; Blum et al., 1996; Osborne, 2002; Reidsma and Carletta, 2008). Yet this might be overly optimistic. Reidsma and op den Akker (2008) show that apparent differences between</context>
</contexts>
<marker>Kearns, 1993</marker>
<rawString>Michael Kearns. 1993. Efficient Noise-Tolerant Learning from Statistical Queries. In Proceedings of the 25th Annual ACM Symposium on Theory of Computing, pages 392–401, San Diego, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marvin Minsky</author>
<author>Seymour Papert</author>
</authors>
<title>Perceptrons: An Introduction to Computational Geometry.</title>
<date>1969</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="11803" citStr="Minsky and Papert, 1969" startWordPosition="2029" endWordPosition="2032">rceptron can be vulnerable to annotation noise. The algorithm is shown below. Algorithm 1 Voted Perceptron Training Input: a labeled training set (x1, y1), ... , (xN, yN) Output: a list of perceptrons w1, ... , wN Initialize: t ← 0; w1 ← 0; ψ1 ← 0 fort = 1 ... N do yt ← sign(hwt, xti + ψt) wt+1 ← wt + yt�ˆyt 2 · xt ψt+1 ← ψt + yt�ˆyt 2 · hwt, xti end for Forecasting Input: a list of perceptrons w1, ... ,wN an unlabeled instance x Output: A forecasted label y y� ← ENt=1 sign(hwt, xti + ψt) y ← sign(y) The voted perceptron algorithm is a refinement of the perceptron algorithm (Rosenblatt, 1962; Minsky and Papert, 1969). Perceptron is a dynamic algorithm; starting with an initial hyperplane w0, it passes repeatedly through the labeled sample. Whenever an instance is misclassified by wt, the hyperplane is modified to adapt to the instance. The algorithm terminates once it has passed through the sample without making any classification mistakes. The algorithm terminates iff the sample can be separated by a hyperplane, and in this case the algorithm finds a separating hyperplane. Novikoff (1962) gives a bound on the number of iterations the algorithm goes through before termination, when the sample is separable</context>
</contexts>
<marker>Minsky, Papert, 1969</marker>
<rawString>Marvin Minsky and Seymour Papert. 1969. Perceptrons: An Introduction to Computational Geometry. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Novikoff</author>
</authors>
<title>On convergence proofs on perceptrons.</title>
<date>1962</date>
<booktitle>Symposium on the Mathematical Theory of Automata,</booktitle>
<pages>12--615</pages>
<contexts>
<context position="12285" citStr="Novikoff (1962)" startWordPosition="2106" endWordPosition="2107">t) y ← sign(y) The voted perceptron algorithm is a refinement of the perceptron algorithm (Rosenblatt, 1962; Minsky and Papert, 1969). Perceptron is a dynamic algorithm; starting with an initial hyperplane w0, it passes repeatedly through the labeled sample. Whenever an instance is misclassified by wt, the hyperplane is modified to adapt to the instance. The algorithm terminates once it has passed through the sample without making any classification mistakes. The algorithm terminates iff the sample can be separated by a hyperplane, and in this case the algorithm finds a separating hyperplane. Novikoff (1962) gives a bound on the number of iterations the algorithm goes through before termination, when the sample is separable by a margin. 282 The perceptron algorithm is vulnerable to noise, as even a little noise could make the sample inseparable. In this case the algorithm would cycle indefinitely never meeting termination conditions, wt would obtain values within a certain dynamic range but would not converge. In such setting, imposing a stopping time would be equivalent to drawing a random vector from the dynamic range. Freund and Schapire (1999) extend the perceptron to inseparable samples with</context>
</contexts>
<marker>Novikoff, 1962</marker>
<rawString>A. B. Novikoff. 1962. On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata, 12:615–622.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
</authors>
<title>Shallow Parsing Using Noisy and Non-Stationary Training Material.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--695</pages>
<contexts>
<context position="1296" citStr="Osborne (2002)" startWordPosition="182" endWordPosition="184">d cases with unreliable annotations. We show that these are relatively unproblematic for an algorithm operating under the 0-1 loss model, whereas for the commonly used voted perceptron algorithm, hard training cases could result in incorrect prediction on the uncontroversial cases at test time. 1 Introduction It is assumed, often tacitly, that the kind of noise existing in human-annotated datasets used in computational linguistics is random classification noise (Kearns, 1993; Angluin and Laird, 1988), resulting from annotator attention slips randomly distributed across instances. For example, Osborne (2002) evaluates noise tolerance of shallow parsers, with random classification noise taken to be “crudely approximating annotation errors.” It has been shown, both theoretically and empirically, that this type of noise is tolerated well by the commonly used machine learning algorithms (Cohen, 1997; Blum et al., 1996; Osborne, 2002; Reidsma and Carletta, 2008). Yet this might be overly optimistic. Reidsma and op den Akker (2008) show that apparent differences between annotators are not random slips of attention but rather result from different biases annotators might have towards the classification </context>
</contexts>
<marker>Osborne, 2002</marker>
<rawString>Miles Osborne. 2002. Shallow Parsing Using Noisy and Non-Stationary Training Material. Journal of Machine Learning Research, 2:695–719.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Udo Kruschwitz</author>
<author>Chamberlain Jon</author>
</authors>
<title>ANAWIKI: Creating Anaphorically Annotated Resources through Web Cooperation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Language Resources and Evaluation Conference,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="22620" citStr="Poesio et al., 2008" startWordPosition="4157" endWordPosition="4160">econd annotator is expected to detect about half the hard cases, as they would surface as disagreements between the annotators. Subsequently, a machine learner can be told to ignore those cases during training, reducing the risk of hard case bias. While this is certainly a daunting task, it is possible that for annotation studies that do not require expert annotators and extensive annotator training, the newly available access to a large pool of inexpensive annotators, such as the Amazon Mechanical Turk scheme (Snow et al., 2008),4 or embedding the task in an online game played by volunteers (Poesio et al., 2008; von Ahn, 2006) could provide some solutions. Reidsma and op den Akker (2008) suggest a different option. When non-overlapping parts of the dataset are annotated by different annotators, each classifier can be trained to reflect the opinion (albeit biased) of a specific annotator, using different parts of the datasets. Such “subjective machines” can be applied to a new set of data; an item that causes disagreement between classifiers is then extrapolated to be a case of potential disagreement between the humans they replicate, i.e. 4http://aws.amazon.com/mturk/ ˆyt ·hw∗, xi = r) hw∗, xti. Thi</context>
</contexts>
<marker>Poesio, Kruschwitz, Jon, 2008</marker>
<rawString>Massimo Poesio, Udo Kruschwitz, and Chamberlain Jon. 2008. ANAWIKI: Creating Anaphorically Annotated Resources through Web Cooperation. In Proceedings of the 6th International Language Resources and Evaluation Conference, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Reidsma</author>
<author>Jean Carletta</author>
</authors>
<title>Reliability measurement without limit.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="1652" citStr="Reidsma and Carletta, 2008" startWordPosition="234" endWordPosition="237">at the kind of noise existing in human-annotated datasets used in computational linguistics is random classification noise (Kearns, 1993; Angluin and Laird, 1988), resulting from annotator attention slips randomly distributed across instances. For example, Osborne (2002) evaluates noise tolerance of shallow parsers, with random classification noise taken to be “crudely approximating annotation errors.” It has been shown, both theoretically and empirically, that this type of noise is tolerated well by the commonly used machine learning algorithms (Cohen, 1997; Blum et al., 1996; Osborne, 2002; Reidsma and Carletta, 2008). Yet this might be overly optimistic. Reidsma and op den Akker (2008) show that apparent differences between annotators are not random slips of attention but rather result from different biases annotators might have towards the classification categories. When training data comes from one annotator and test data from another, the first annotator’s biases are sometimes systematic enough for a machine learner to pick them up, with detrimental results for the algorithm’s performance on the test data. A small subset of doubly annotated data (for inter-annotator agreement check) and large chunks of</context>
</contexts>
<marker>Reidsma, Carletta, 2008</marker>
<rawString>Dennis Reidsma and Jean Carletta. 2008. Reliability measurement without limit. Computational Linguistics, 34(3):319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Reidsma</author>
<author>Rieks op den Akker</author>
</authors>
<title>Exploiting Subjective Annotations.</title>
<date>2008</date>
<booktitle>In COLING 2008 Workshop on Human Judgments in Computational Linguistics,</booktitle>
<pages>8--16</pages>
<location>Manchester, UK.</location>
<marker>Reidsma, den Akker, 2008</marker>
<rawString>Dennis Reidsma and Rieks op den Akker. 2008. Exploiting Subjective Annotations. In COLING 2008 Workshop on Human Judgments in Computational Linguistics, pages 8–16, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Rosenblatt</author>
</authors>
<title>Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms.</title>
<date>1962</date>
<publisher>Spartan Books,</publisher>
<location>Washington, D.C.</location>
<contexts>
<context position="11777" citStr="Rosenblatt, 1962" startWordPosition="2027" endWordPosition="2028"> that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Algorithm 1 Voted Perceptron Training Input: a labeled training set (x1, y1), ... , (xN, yN) Output: a list of perceptrons w1, ... , wN Initialize: t ← 0; w1 ← 0; ψ1 ← 0 fort = 1 ... N do yt ← sign(hwt, xti + ψt) wt+1 ← wt + yt�ˆyt 2 · xt ψt+1 ← ψt + yt�ˆyt 2 · hwt, xti end for Forecasting Input: a list of perceptrons w1, ... ,wN an unlabeled instance x Output: A forecasted label y y� ← ENt=1 sign(hwt, xti + ψt) y ← sign(y) The voted perceptron algorithm is a refinement of the perceptron algorithm (Rosenblatt, 1962; Minsky and Papert, 1969). Perceptron is a dynamic algorithm; starting with an initial hyperplane w0, it passes repeatedly through the labeled sample. Whenever an instance is misclassified by wt, the hyperplane is modified to adapt to the instance. The algorithm terminates once it has passed through the sample without making any classification mistakes. The algorithm terminates iff the sample can be separated by a hyperplane, and in this case the algorithm finds a separating hyperplane. Novikoff (1962) gives a bound on the number of iterations the algorithm goes through before termination, wh</context>
</contexts>
<marker>Rosenblatt, 1962</marker>
<rawString>Frank Rosenblatt. 1962. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. Spartan Books, Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind Joshi</author>
</authors>
<title>Incremental LTAG Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Empirical Methods in Natural Language Processing Conference,</booktitle>
<pages>811--818</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="11105" citStr="Shen and Joshi, 2005" startWordPosition="1894" endWordPosition="1897">sary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Algorithm 1 Voted Perceptron Training Input: a labeled training set (x1, y1), ... , (xN, yN) Output: a list of perceptrons w1, ... , wN Initialize: t ← 0; w1 ← 0; ψ1 ← 0 fort = 1 ... N do yt ← sign(hwt, xti + ψt) wt+1 ← wt + yt�ˆyt 2 · xt ψt+1 ← ψt + yt�ˆyt 2 · hwt, xti end for Forecasting Input: a list of perceptrons w1, ... ,wN an unlabeled instance x Output: A forecasted label y y� ← ENt=1 sign(hwt, xti + ψt) y ← sign(y) The voted perceptron</context>
</contexts>
<marker>Shen, Joshi, 2005</marker>
<rawString>Libin Shen and Aravind Joshi. 2005. Incremental LTAG Parsing. In Proceedings of the Human Language Technology Conference and Empirical Methods in Natural Language Processing Conference, pages 811–818, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Cheap and Fast – But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference,</booktitle>
<pages>254--263</pages>
<location>Honolulu, Hawaii.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and Fast – But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks. In Proceedings of the Empirical Methods in Natural Language Processing Conference, pages 254–263, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Viola</author>
<author>Mukund Narasimhan</author>
</authors>
<title>Learning to Extract Information from Semi-Structured Text Using a Discriminative Context Free Grammar.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>330--337</pages>
<location>Salvador, Brazil.</location>
<contexts>
<context position="11040" citStr="Viola and Narasimhan, 2005" startWordPosition="1882" endWordPosition="1885">arns and Li, 1988; Haussler, 1992; Kearns et al., 1994), where an adversary can not only choose the placement of the hard cases, but also their labels. In worst case, the 0-1 loss model would sustain a constant rate of error due to malicious noise, whereas annotation noise is tolerated quite well in large datasets. 3 Voted Perceptron Freund and Schapire (1999) describe the voted perceptron. This algorithm and its many variants are widely used in the computational linguistics community (Collins, 2002a; Collins and Duffy, 2002; Collins, 2002b; Collins and Roark, 2004; Henderson and Titov, 2005; Viola and Narasimhan, 2005; Cohen et al., 2004; Carreras et al., 2005; Shen and Joshi, 2005; Ciaramita and Johnson, 2003). In this section, we show that the voted perceptron can be vulnerable to annotation noise. The algorithm is shown below. Algorithm 1 Voted Perceptron Training Input: a labeled training set (x1, y1), ... , (xN, yN) Output: a list of perceptrons w1, ... , wN Initialize: t ← 0; w1 ← 0; ψ1 ← 0 fort = 1 ... N do yt ← sign(hwt, xti + ψt) wt+1 ← wt + yt�ˆyt 2 · xt ψt+1 ← ψt + yt�ˆyt 2 · hwt, xti end for Forecasting Input: a list of perceptrons w1, ... ,wN an unlabeled instance x Output: A forecasted label </context>
</contexts>
<marker>Viola, Narasimhan, 2005</marker>
<rawString>Paul Viola and Mukund Narasimhan. 2005. Learning to Extract Information from Semi-Structured Text Using a Discriminative Context Free Grammar. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 330–337, Salvador, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
</authors>
<title>Games with a purpose.</title>
<date>2006</date>
<journal>Computer,</journal>
<volume>39</volume>
<issue>6</issue>
<marker>von Ahn, 2006</marker>
<rawString>Luis von Ahn. 2006. Games with a purpose. Computer, 39(6):92–94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>