<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.158672">
<title confidence="0.977096">
Nearly-Automated Metadata Hierarchy Creation
</title>
<author confidence="0.996724">
Emilia Stoica and Marti A. Hearst
</author>
<affiliation confidence="0.999116">
School of Information Management &amp; Systems
University of California, Berkeley
</affiliation>
<address confidence="0.779434">
102 South Hall, Berkeley CA 94720
</address>
<email confidence="0.999418">
estoica,hearst @sims.berkeley.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999862181818182">
Currently, information architects create meta-
data category hierarchies manually. We present
a nearly-automated approach for deriving such
hierarchies, by converting the lexical hierarchy
WordNet into a format that reflects the contents
of a target information collection. We use the
term “nearly-automated” because an informa-
tion architect should have to make only small
adjustments to produce an acceptable metadata
structure. We contrast the results with an algo-
rithm that uses lexical co-occurrence statistics.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998872129032258">
Human-readable hierarchies of category metadata are
needed for a wide range of information-centric applica-
tions, including information architectures for web sites
(Rosenfeld and Morville, 2002) and metadata for brows-
ing image and document collections (Yee et al., 2003).
In the information architecture community, methods
for creation of content-oriented metadata tend to be al-
most entirely manual (Rosenfeld and Morville, 2002).
The standard procedure is to gather lists of terms from ex-
isting resources, and organize them by selecting, merging
and augmenting the term lists to produce a set of hierar-
chical category labels. Usually the metadata categories
are used as labels which are assigned manually to the
items in the collection.
We advocate instead a nearly-automated approach to
building hierarchical subject category metadata, where
suggestions for metadata terms are automatically gen-
erated and grouped into hierarchies and then presented
to information architects for limited pruning and editing.
To be truly useful, these suggested groupings should be
close to the final product; if the results are too scattered, a
simple list of the most well-distributed terms is probably
more useful (a similar phenomenon is seen in machine-
aided translation systems (Church and Hovy, 1993)).
More specifically, we aim to develop algorithms for
generating category sets that (a) are intuitive to the tar-
get audience who will be browsing a web site or collec-
tion, (b) reflect the contents of the collection, and (c) al-
low for (nearly) automated assignment of the categories
to the items in the collection.
For a category system to be intuitive, modern informa-
tion science practice finds that it should consist of a set
of IS-A (hypernym) hierarchies1, from which multiple
labels can be selected and assigned to an item, follow-
ing the tenants of faceted classification (Rosenfeld and
Morville, 2002; Yee et al., 2003). For example, a medical
journal article will often simultaneously have terms as-
signed to it from anatomy, disease, and drug category hi-
erarchies. Furthermore, usability studies suggest that the
hierarchies should not be overly deep nor overly wide,
and preferably should have concave structure (meaning
broader at the root and leaves, narrower in the middle)
(Bernard, 2002).
Previous work on automated methods has primarily fo-
cused on using clustering techniques, which have the ad-
vantage of being automated and data-driven. However, a
major problem with clustering is that the groupings show
terms that are associated with one another, rather than
hierarchical parent-child relations. Studies indicate that
users prefer organized categories over associational clus-
ters (Chen et al., 1998; Pratt et al., 1999).
We have tested several approaches, including K-means
clustering, subsumption (Sanderson and Croft, 1999),
computing lexical co-occurrences (Schutze, 1993) and
building on the WordNet lexical hierarchy (Fellbaum,
1998). We have found that the latter produces by far the
most intuitive groupings that would be useful for creation
of a re-usable, human-readable category structure. Al-
though the idea of using a resource like WordNet for this
type of application seems rather obvious, to our knowl-
edge it has not been used to create subject-oriented meta-
data for browsing. This may be in part because it is very
</bodyText>
<footnote confidence="0.7137805">
1Part-of (meronymy) relations are also intuitive, but are not
considered here.
</footnote>
<bodyText confidence="0.999866071428572">
large and the word senses are assumed to be too fine-
grained (Mihalcea and Moldovan, 2001), or its structure
is assumed to be inappropriate.
However, we have found that, for some collections,
starting with the assumption that there will be a small
amount of hand-editing done after the automated pro-
cessing, combined with a bottom-up approach that ex-
tracts out those parts of the hypernym hierarchy that are
relevant to the collection, and a compression algorithm
that simplifies the hierarchical structure, we can produce
a structure that is close to the target goals.
Below we describe related work, the method for con-
verting WordNet into a more usable form, and the results
of using the algorithm on a test collection.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999964607142857">
There has been surprisingly little work on precisely the
problem that we tackle in this paper. The literature on au-
tomated text categorization is enormous, but assumes that
a set of categories has already been created, whereas the
problem here is to determine the categories of interest.
There has also been extensive work on finding synony-
mous terms and word associations, as well as automatic
acquisition of IS-A (or genus-head) relations from dic-
tionary definitions and glosses (Klavans and Whitman,
2001) and from free text (Hearst, 1992; Caraballo, 1999).
Sanderson and Croft (1999) propose a method called
subsumption for building a hierarchy for a set of docu-
ments retrieved for a query. For two terms x and y, x
is said to subsume y if the following conditions hold:
. The evaluation consisted
of asking people to define the relation that holds between
the pairs of words shown; only 23% of the pairs were
found to hold a parent-child relation; 49% were found to
fall into a more general related-to category. For a set of
medical texts, the top level consisted of the terms: dis-
ease, post polio, serious disease, dengue, infection con-
trol, immunology, etc. This kind of listing is not system-
atic enough to appear on a navigation page for a website.
Lawrie et al. (2001) use language models to produce
summaries of text collections. The results are also as-
sociational; for example, the top level for a query on
“Abuses of Email” are abuses, human, States Act, and
Nursing Home Abuses, and the second level under abuses
is e-mail, send, Money, Fax, account, address, Internet,
etc. These again are too scattered to be appropriate for a
human-readable index into a document collection.
Hofmann (1999) uses probabilistic document cluster-
ing to impose topic hierarchies. For a collection of ar-
ticles from the journal Machine Learning, the top level
cluster is labeled learn, paper, base, model, new, train
and the second level clusters are labeled process, experi,
knowledge, develop, inform, design and algorithm, func-
tion, present, result, problem, model. We would prefer
something more like the ACM classification hierarchy.
The Word Space algorithm (Schutze, 1993) uses lin-
ear regression on term co-occurrence statistics to create
groups of semantically related words. For every word, a
context vector is computed for every position at which it
occurs in text. A vector is defined as the sum of all four-
grams in a window of 1001 fourgrams centered around
the word. Cosine distance is used to compute the similar-
ity between word vectors.
Probably the closest work to that described here is the
SONIA system (Sahami et al., 1998) which used a com-
bination of unsupervised and supervised methods to or-
ganize a set of documents. The unsupervised method
(document clustering) imposes an initial organization on
a personal information collection which the user can then
modify. The resulting organization is then used to train a
supervised text categorization algorithm which automati-
cally classifies new documents.
</bodyText>
<sectionHeader confidence="0.990447" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999818428571429">
WordNet is a manually built lexical system where words
are organized into synonym sets (synsets) linked by dif-
ferent relations (Fellbaum, 1998). It can be viewed as a
huge graph, where the synsets are the nodes and the re-
lations are the links. Our algorithm for converting it to
create metadata categories for information organization
and browsing consists of the following steps:
</bodyText>
<listItem confidence="0.9989664">
1. Select representative words from the collection.
2. Get the WordNet hypernym paths for one sense of
each selected word.
3. Build a tree from the hypernym paths.
4. Compress the tree.
</listItem>
<subsectionHeader confidence="0.99919">
3.1 Select Representative Words
</subsectionHeader>
<bodyText confidence="0.999973">
To make the hierarchy size manageable, we select only
a subset of the words that are intended to best reflect the
topics covered in the documents (although in principle the
method can be used on all of the words in the collection).
The criteria for choosing the target words is informa-
tion gain (Mitchell, 1997). Define the set to be all the
unique words in the the document set . Let the distri-
bution of a word be the number of documents in D that
the word occurs in. Initially, the words in are ordered
according to their distribution in the entire collection .
At each iteration, the highest-scoring word is added to
an initially-empty set and removed from , and the
documents covered by are removed from . The pro-
cess repeats until no more documents are left in .
</bodyText>
<subsectionHeader confidence="0.996923">
3.2 Get Hypernym Paths
</subsectionHeader>
<bodyText confidence="0.997305333333333">
For every word in , we obtain the hypernym path of the
word from WordNet. In the current implementation, we
take the hypernym for the first sense of the word only,
</bodyText>
<figure confidence="0.998960029411765">
abstraction
abstraction
color
color
chromatic color
color
blue
red blue green red blue green
red blue
red blue green
(b)
(c)
(d)
(e) (f)
property
property
visual property
visual property
color
chromatic color
blue, blueness
chromatic color
red, redness blue, blueness green, greenness
color
red, redness blue, blueness
chromatic color
(a)
abstraction
property
visual property
color
chromatic color
red, redness
red
</figure>
<figureCaption confidence="0.942375333333333">
Figure 1: Building a hierarchy from WordNet. (a) The hypernym path for word red, and (b) blue. (c) Combining the
paths of words red and blue, (d) The uncompacted tree for words red, blue and green, (e) The path after eliminating
parents with less than two children, and (f) after eliminating children with name included in parent’s name.
</figureCaption>
<bodyText confidence="0.9971702">
which is usually the most general. (In the future, we plan
to explore how to disambiguate between senses based on
the context in which the word appears in the document;
see Discussion.) Figures 1(a) and 1(b) show the hyper-
nym paths for words red and blue.
</bodyText>
<subsectionHeader confidence="0.996739">
3.3 Build the Tree
</subsectionHeader>
<bodyText confidence="0.9999065">
Next we take the union of the hypernym paths of all
words in set S, obtaining a tree, as shown in Figure 1(c).
</bodyText>
<subsectionHeader confidence="0.980454">
3.4 Compress the Tree
</subsectionHeader>
<bodyText confidence="0.9982045">
The hypernym path length varies widely in WordNet, so
we compress the tree using three rules:
</bodyText>
<listItem confidence="0.9870126">
1. Eliminate selected top-level (very general) categories, like
abstraction, entity.
2. Starting from the leaves, eliminate a parent that has fewer
than n children, unless the parent is the root.
3. Eliminate a child whose name appears within the parent’s.
</listItem>
<bodyText confidence="0.994625190476191">
For example, consider the tree in Figure 1(d) and as-
sume that (eliminate parents that have fewer than
two children). Starting from the leaves, by applying Rule
2, nodes red, redness, blue, blueness, and green, green-
ness, are eliminated since they have only one child. Fig-
ure 1(e) shows the resulting tree. Next, by applying Rule
3, node chromatic color is eliminated, since it contains
the word color which also appears in the name of its par-
ent. The final tree presented in Figure 1(f) produces a
structure that is likely to be a good level of description
for an information architecture.
Mihalcea and Moldovan (2001) describe a sophisti-
cated method for simplifying WordNet, focusing on com-
bining synsets with very similar meanings or dropping
rarely used synsets. Their rules include what we define
above as Rule 3. However, they focus on simplifying
WordNet in general, rather than tailoring it to a specific
collection, and focus on NLP applications that are likely
to make use of every sense of a WordNet word. Never-
theless, it may be useful to explore using their simplified
version of WordNet in future.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.995796333333333">
We experimented with a collection of descriptions of
approximately art documents containing about
unique words.2 Some sample documents are:
</bodyText>
<construct confidence="0.8478954">
A French soldier clings to tree branches as a wolf
stands beneath the tree.
A Greek trellis with Ionic columns, meander cross-
ing diagonally; few vines; trees background; trellis
is in a circle.
</construct>
<bodyText confidence="0.999975466666667">
The descriptions are preprocessed by eliminating fre-
quent words from a stop list. Information gain is used to
select target words, in this case resulting in 849 words.
Figure 2 shows partial results obtained using the Word-
Net algorithm (where compression reduced the number
of nodes by ) and Word Space (Schutze, 1993).
Note that the WordNet-based organization is intuitive,
but if not exactly what the designer wants, should be easy
to adjust. For example, a designer may prefer to have
a “nature” category that combines the subcategories of
“geological formation,” “body of water,” and “vascular
plant”. Some terminology may also need renaming, but
note that WordNet also provides thesaurus terms that can
be used in an underlying search engine. Word Space, by
contrast, produces associationally related terms.
</bodyText>
<sectionHeader confidence="0.992954" genericHeader="conclusions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999890111111111">
We advocate the use of an existing rich lexical resource
for the nearly-automated creation of hierarchical subject-
oriented metadata for information browsing and naviga-
tion. We have created examples that show that a mod-
ified version of WordNet can produce a useful starting
point for information organization projects. These have
the added advantage ofproducing automated assignments
of multiple labels to documents. We plan to augment the
processing with more intelligent selection of hypernym
</bodyText>
<footnote confidence="0.775671">
2This collection is also used in (Yee et al., 2003).
</footnote>
<figure confidence="0.99561352112676">
instrumentation
container
bag
basket
vessel
bottle
bowl
tankard
urn
wheeled vehicle
cart
carriage
wagon
furniture
altar
desk
bench
structure, construction
amphitheater
auditorium
room
tower
geological formation
beach
hill
organism, being
person, individual
apostle
gentleman
boy
king
performer
comic
actor
dancer
musician
female
vascular plant
corn
lily
rose
shrub
body of water
canal
ocean
pond
river
sea
stream
act, human action
ambush
baptism
lesson
lying
market
performing
waiting
carpentry
washing
creation
construction
design
writing
diversion, recreation
dancing
playing
sports, athletics
floating
racing
riding
mountain
</figure>
<bodyText confidence="0.991097307692308">
(arm bent head hand back resting her leg crossed right)
(bank river stream boat shore barge distant fishing hill water)
(altar crowd gather overhead roman monk palm burn priest)
(beard trimmed moustache ruffles short straight hair collar)
(bowl cup shell tail empty vase skin rope seat inscription)
(canal bay harbor dome quiet steep dock rock few cathedral)
(glove hand turban head hat gather him arm her halo cloak)
(hunter riding air couple hunting rifle wild gun baby balcony)
(moon sun rising sky coming low pole second vine area)
(musician music play little girl drinking dance balcony drink)
(nude female male headed reclining figure lying raised seated)
(rider horse ride carriage cart pulling horseback tree path tall)
(tower hill distant church stone windmill road city fence crossing)
</bodyText>
<figure confidence="0.994936">
(a) (b)
</figure>
<figureCaption confidence="0.999976">
Figure 2: Comparison of partial results using (a) WordNet and (b) Word Space.
</figureCaption>
<bodyText confidence="0.9999685">
senses, as well as processing the descriptions to extract
noun compounds and differentiate nouns from verbs. The
method also worked well on a set of biomedical journal
titles; we are in the process of determining how generally
applicable the approach is. In addition, we are currently
designing usability studies in which we will present dif-
ferent categorization suggestions to information archi-
tects to organize. Their subjective reactions, the amount
of time it takes them to create the organizations, and the
resulting quality and coverage of the organizations, as
measured by users performing navigation tasks using the
hierarchies, will be compared to other techniques.
</bodyText>
<sectionHeader confidence="0.996029" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.984504">
This research was supported by NSF grants DBI-
0317510 and IIS-9984741.
</bodyText>
<sectionHeader confidence="0.999007" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999840111111111">
Michael L. Bernard. 2002. Examining the effects of hypertext
shape on user performance. Usability News, 4(2).
Sharon A. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Proceedings
ofACL ’99, College Park, MD.
Hsinchen Chen, Andrea L. Houston, Robin R. Sewell, and
Bruce R. Schatz. 1998. Internet browsing and searching:
User evaluations of category map and concept space tech-
niques. JASIS, 49(7).
Ken Church and Eduard Hovy. 1993. Good applications for
crummy machine translation. Machine Translation, 8.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING ’92,
Nantes, France.
Thomas Hofmann. 1999. The cluster-abstraction model: Un-
supervised learning of topic hierarchies from text data. In
Proceedings ofIJCAI’99, Stolckholm.
Judith Klavans and Brian Whitman. 2001. Extracting taxo-
nomic relationships from on-line definitional sources using
lexing. In Proceedings ofACM/IEEE DL ’01, Roanoke, VA.
Dawn Lawrie, Bruce Croft, and Arnold L. Rosenberg. 2001.
Finding topic words for hierarchical summarization. In Pro-
ceedings of SIGIR ’01, New Orleans, LA.
Rada Mihalcea and Dan I. Moldovan. 2001. Ez.wordnet: Prin-
ciples for automatic generation of a coarse grained wordnet.
In Proceedings of FLAIRS Conference 2001.
Tom Mitchell. 1997. Machine Learning. McGraw Hill.
Wanda Pratt, Marti Hearst, and Larry Fagan. 1999. A
knowledge-based approach to organizing retrieved docu-
ments. In Proceedings ofAAAI 99, Orlando, FL.
Louis Rosenfeld and Peter Morville. 2002. Information Archi-
tecture for the World Wide Web: Designing Large-scale Web
Sites. O’Reilly &amp; Associates, Inc.
Mehran Sahami, S. Yusufali, and M. Q. W. Baldonaldo. 1998.
SONIA: A service for organizing networked information au-
tonomously. In Proceedings ofDL’ 98, New York.
Mark Sanderson and Bruce Croft. 1999. Deriving concept hi-
erarchies from text. In Proceedings of SIGIR ’99.
Hinrich Schutze. 1993. Word space. Advances in NeuralInfor-
mation Processing Systems, 5:895–902.
Ka-Ping Yee, Kirsten Swearingen, Kevin Li, and Marti Hearst.
2003. Faceted metadata for image search and browsing. In
Proceedings of the CHI 2003, Fort Lauderdale, FL.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970334">
<title confidence="0.999097">Nearly-Automated Metadata Hierarchy Creation</title>
<author confidence="0.988972">A Stoica</author>
<affiliation confidence="0.9999435">School of Information Management &amp; University of California,</affiliation>
<address confidence="0.999713">102 South Hall, Berkeley CA</address>
<email confidence="0.99711">estoica,hearst@sims.berkeley.edu</email>
<abstract confidence="0.9987005">Currently, information architects create metadata category hierarchies manually. We present approach for deriving such hierarchies, by converting the lexical hierarchy WordNet into a format that reflects the contents of a target information collection. We use the term “nearly-automated” because an information architect should have to make only small adjustments to produce an acceptable metadata structure. We contrast the results with an algorithm that uses lexical co-occurrence statistics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael L Bernard</author>
</authors>
<title>Examining the effects of hypertext shape on user performance.</title>
<date>2002</date>
<journal>Usability News,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="3052" citStr="Bernard, 2002" startWordPosition="456" endWordPosition="457">ice finds that it should consist of a set of IS-A (hypernym) hierarchies1, from which multiple labels can be selected and assigned to an item, following the tenants of faceted classification (Rosenfeld and Morville, 2002; Yee et al., 2003). For example, a medical journal article will often simultaneously have terms assigned to it from anatomy, disease, and drug category hierarchies. Furthermore, usability studies suggest that the hierarchies should not be overly deep nor overly wide, and preferably should have concave structure (meaning broader at the root and leaves, narrower in the middle) (Bernard, 2002). Previous work on automated methods has primarily focused on using clustering techniques, which have the advantage of being automated and data-driven. However, a major problem with clustering is that the groupings show terms that are associated with one another, rather than hierarchical parent-child relations. Studies indicate that users prefer organized categories over associational clusters (Chen et al., 1998; Pratt et al., 1999). We have tested several approaches, including K-means clustering, subsumption (Sanderson and Croft, 1999), computing lexical co-occurrences (Schutze, 1993) and bui</context>
</contexts>
<marker>Bernard, 2002</marker>
<rawString>Michael L. Bernard. 2002. Examining the effects of hypertext shape on user performance. Usability News, 4(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
</authors>
<title>Automatic construction of a hypernym-labeled noun hierarchy from text.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL ’99,</booktitle>
<location>College Park, MD.</location>
<contexts>
<context position="5472" citStr="Caraballo, 1999" startWordPosition="839" endWordPosition="840">using the algorithm on a test collection. 2 Related Work There has been surprisingly little work on precisely the problem that we tackle in this paper. The literature on automated text categorization is enormous, but assumes that a set of categories has already been created, whereas the problem here is to determine the categories of interest. There has also been extensive work on finding synonymous terms and word associations, as well as automatic acquisition of IS-A (or genus-head) relations from dictionary definitions and glosses (Klavans and Whitman, 2001) and from free text (Hearst, 1992; Caraballo, 1999). Sanderson and Croft (1999) propose a method called subsumption for building a hierarchy for a set of documents retrieved for a query. For two terms x and y, x is said to subsume y if the following conditions hold: . The evaluation consisted of asking people to define the relation that holds between the pairs of words shown; only 23% of the pairs were found to hold a parent-child relation; 49% were found to fall into a more general related-to category. For a set of medical texts, the top level consisted of the terms: disease, post polio, serious disease, dengue, infection control, immunology,</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>Sharon A. Caraballo. 1999. Automatic construction of a hypernym-labeled noun hierarchy from text. In Proceedings ofACL ’99, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsinchen Chen</author>
<author>Andrea L Houston</author>
<author>Robin R Sewell</author>
<author>Bruce R Schatz</author>
</authors>
<title>Internet browsing and searching: User evaluations of category map and concept space techniques.</title>
<date>1998</date>
<journal>JASIS,</journal>
<volume>49</volume>
<issue>7</issue>
<contexts>
<context position="3467" citStr="Chen et al., 1998" startWordPosition="515" endWordPosition="518">dies suggest that the hierarchies should not be overly deep nor overly wide, and preferably should have concave structure (meaning broader at the root and leaves, narrower in the middle) (Bernard, 2002). Previous work on automated methods has primarily focused on using clustering techniques, which have the advantage of being automated and data-driven. However, a major problem with clustering is that the groupings show terms that are associated with one another, rather than hierarchical parent-child relations. Studies indicate that users prefer organized categories over associational clusters (Chen et al., 1998; Pratt et al., 1999). We have tested several approaches, including K-means clustering, subsumption (Sanderson and Croft, 1999), computing lexical co-occurrences (Schutze, 1993) and building on the WordNet lexical hierarchy (Fellbaum, 1998). We have found that the latter produces by far the most intuitive groupings that would be useful for creation of a re-usable, human-readable category structure. Although the idea of using a resource like WordNet for this type of application seems rather obvious, to our knowledge it has not been used to create subject-oriented metadata for browsing. This may</context>
</contexts>
<marker>Chen, Houston, Sewell, Schatz, 1998</marker>
<rawString>Hsinchen Chen, Andrea L. Houston, Robin R. Sewell, and Bruce R. Schatz. 1998. Internet browsing and searching: User evaluations of category map and concept space techniques. JASIS, 49(7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Church</author>
<author>Eduard Hovy</author>
</authors>
<title>Good applications for crummy machine translation.</title>
<date>1993</date>
<journal>Machine Translation,</journal>
<volume>8</volume>
<contexts>
<context position="2052" citStr="Church and Hovy, 1993" startWordPosition="292" endWordPosition="295">as labels which are assigned manually to the items in the collection. We advocate instead a nearly-automated approach to building hierarchical subject category metadata, where suggestions for metadata terms are automatically generated and grouped into hierarchies and then presented to information architects for limited pruning and editing. To be truly useful, these suggested groupings should be close to the final product; if the results are too scattered, a simple list of the most well-distributed terms is probably more useful (a similar phenomenon is seen in machineaided translation systems (Church and Hovy, 1993)). More specifically, we aim to develop algorithms for generating category sets that (a) are intuitive to the target audience who will be browsing a web site or collection, (b) reflect the contents of the collection, and (c) allow for (nearly) automated assignment of the categories to the items in the collection. For a category system to be intuitive, modern information science practice finds that it should consist of a set of IS-A (hypernym) hierarchies1, from which multiple labels can be selected and assigned to an item, following the tenants of faceted classification (Rosenfeld and Morville</context>
</contexts>
<marker>Church, Hovy, 1993</marker>
<rawString>Ken Church and Eduard Hovy. 1993. Good applications for crummy machine translation. Machine Translation, 8.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING ’92,</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="5454" citStr="Hearst, 1992" startWordPosition="837" endWordPosition="838">he results of using the algorithm on a test collection. 2 Related Work There has been surprisingly little work on precisely the problem that we tackle in this paper. The literature on automated text categorization is enormous, but assumes that a set of categories has already been created, whereas the problem here is to determine the categories of interest. There has also been extensive work on finding synonymous terms and word associations, as well as automatic acquisition of IS-A (or genus-head) relations from dictionary definitions and glosses (Klavans and Whitman, 2001) and from free text (Hearst, 1992; Caraballo, 1999). Sanderson and Croft (1999) propose a method called subsumption for building a hierarchy for a set of documents retrieved for a query. For two terms x and y, x is said to subsume y if the following conditions hold: . The evaluation consisted of asking people to define the relation that holds between the pairs of words shown; only 23% of the pairs were found to hold a parent-child relation; 49% were found to fall into a more general related-to category. For a set of medical texts, the top level consisted of the terms: disease, post polio, serious disease, dengue, infection co</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING ’92, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>The cluster-abstraction model: Unsupervised learning of topic hierarchies from text data.</title>
<date>1999</date>
<booktitle>In Proceedings ofIJCAI’99, Stolckholm.</booktitle>
<contexts>
<context position="6618" citStr="Hofmann (1999)" startWordPosition="1037" endWordPosition="1038"> post polio, serious disease, dengue, infection control, immunology, etc. This kind of listing is not systematic enough to appear on a navigation page for a website. Lawrie et al. (2001) use language models to produce summaries of text collections. The results are also associational; for example, the top level for a query on “Abuses of Email” are abuses, human, States Act, and Nursing Home Abuses, and the second level under abuses is e-mail, send, Money, Fax, account, address, Internet, etc. These again are too scattered to be appropriate for a human-readable index into a document collection. Hofmann (1999) uses probabilistic document clustering to impose topic hierarchies. For a collection of articles from the journal Machine Learning, the top level cluster is labeled learn, paper, base, model, new, train and the second level clusters are labeled process, experi, knowledge, develop, inform, design and algorithm, function, present, result, problem, model. We would prefer something more like the ACM classification hierarchy. The Word Space algorithm (Schutze, 1993) uses linear regression on term co-occurrence statistics to create groups of semantically related words. For every word, a context vec</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. The cluster-abstraction model: Unsupervised learning of topic hierarchies from text data. In Proceedings ofIJCAI’99, Stolckholm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Klavans</author>
<author>Brian Whitman</author>
</authors>
<title>Extracting taxonomic relationships from on-line definitional sources using lexing.</title>
<date>2001</date>
<booktitle>In Proceedings ofACM/IEEE DL ’01,</booktitle>
<location>Roanoke, VA.</location>
<contexts>
<context position="5421" citStr="Klavans and Whitman, 2001" startWordPosition="829" endWordPosition="832">nverting WordNet into a more usable form, and the results of using the algorithm on a test collection. 2 Related Work There has been surprisingly little work on precisely the problem that we tackle in this paper. The literature on automated text categorization is enormous, but assumes that a set of categories has already been created, whereas the problem here is to determine the categories of interest. There has also been extensive work on finding synonymous terms and word associations, as well as automatic acquisition of IS-A (or genus-head) relations from dictionary definitions and glosses (Klavans and Whitman, 2001) and from free text (Hearst, 1992; Caraballo, 1999). Sanderson and Croft (1999) propose a method called subsumption for building a hierarchy for a set of documents retrieved for a query. For two terms x and y, x is said to subsume y if the following conditions hold: . The evaluation consisted of asking people to define the relation that holds between the pairs of words shown; only 23% of the pairs were found to hold a parent-child relation; 49% were found to fall into a more general related-to category. For a set of medical texts, the top level consisted of the terms: disease, post polio, seri</context>
</contexts>
<marker>Klavans, Whitman, 2001</marker>
<rawString>Judith Klavans and Brian Whitman. 2001. Extracting taxonomic relationships from on-line definitional sources using lexing. In Proceedings ofACM/IEEE DL ’01, Roanoke, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dawn Lawrie</author>
<author>Bruce Croft</author>
<author>Arnold L Rosenberg</author>
</authors>
<title>Finding topic words for hierarchical summarization.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR ’01,</booktitle>
<location>New Orleans, LA.</location>
<contexts>
<context position="6190" citStr="Lawrie et al. (2001)" startWordPosition="966" endWordPosition="969">t of documents retrieved for a query. For two terms x and y, x is said to subsume y if the following conditions hold: . The evaluation consisted of asking people to define the relation that holds between the pairs of words shown; only 23% of the pairs were found to hold a parent-child relation; 49% were found to fall into a more general related-to category. For a set of medical texts, the top level consisted of the terms: disease, post polio, serious disease, dengue, infection control, immunology, etc. This kind of listing is not systematic enough to appear on a navigation page for a website. Lawrie et al. (2001) use language models to produce summaries of text collections. The results are also associational; for example, the top level for a query on “Abuses of Email” are abuses, human, States Act, and Nursing Home Abuses, and the second level under abuses is e-mail, send, Money, Fax, account, address, Internet, etc. These again are too scattered to be appropriate for a human-readable index into a document collection. Hofmann (1999) uses probabilistic document clustering to impose topic hierarchies. For a collection of articles from the journal Machine Learning, the top level cluster is labeled learn,</context>
</contexts>
<marker>Lawrie, Croft, Rosenberg, 2001</marker>
<rawString>Dawn Lawrie, Bruce Croft, and Arnold L. Rosenberg. 2001. Finding topic words for hierarchical summarization. In Proceedings of SIGIR ’01, New Orleans, LA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan I Moldovan</author>
</authors>
<title>Ez.wordnet: Principles for automatic generation of a coarse grained wordnet.</title>
<date>2001</date>
<booktitle>In Proceedings of FLAIRS Conference</booktitle>
<contexts>
<context position="4266" citStr="Mihalcea and Moldovan, 2001" startWordPosition="641" endWordPosition="644"> 1993) and building on the WordNet lexical hierarchy (Fellbaum, 1998). We have found that the latter produces by far the most intuitive groupings that would be useful for creation of a re-usable, human-readable category structure. Although the idea of using a resource like WordNet for this type of application seems rather obvious, to our knowledge it has not been used to create subject-oriented metadata for browsing. This may be in part because it is very 1Part-of (meronymy) relations are also intuitive, but are not considered here. large and the word senses are assumed to be too finegrained (Mihalcea and Moldovan, 2001), or its structure is assumed to be inappropriate. However, we have found that, for some collections, starting with the assumption that there will be a small amount of hand-editing done after the automated processing, combined with a bottom-up approach that extracts out those parts of the hypernym hierarchy that are relevant to the collection, and a compression algorithm that simplifies the hierarchical structure, we can produce a structure that is close to the target goals. Below we describe related work, the method for converting WordNet into a more usable form, and the results of using the </context>
<context position="11622" citStr="Mihalcea and Moldovan (2001)" startWordPosition="1875" endWordPosition="1878">parent’s. For example, consider the tree in Figure 1(d) and assume that (eliminate parents that have fewer than two children). Starting from the leaves, by applying Rule 2, nodes red, redness, blue, blueness, and green, greenness, are eliminated since they have only one child. Figure 1(e) shows the resulting tree. Next, by applying Rule 3, node chromatic color is eliminated, since it contains the word color which also appears in the name of its parent. The final tree presented in Figure 1(f) produces a structure that is likely to be a good level of description for an information architecture. Mihalcea and Moldovan (2001) describe a sophisticated method for simplifying WordNet, focusing on combining synsets with very similar meanings or dropping rarely used synsets. Their rules include what we define above as Rule 3. However, they focus on simplifying WordNet in general, rather than tailoring it to a specific collection, and focus on NLP applications that are likely to make use of every sense of a WordNet word. Nevertheless, it may be useful to explore using their simplified version of WordNet in future. 4 Results We experimented with a collection of descriptions of approximately art documents containing about</context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>Rada Mihalcea and Dan I. Moldovan. 2001. Ez.wordnet: Principles for automatic generation of a coarse grained wordnet. In Proceedings of FLAIRS Conference 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning.</booktitle>
<publisher>McGraw Hill.</publisher>
<contexts>
<context position="8848" citStr="Mitchell, 1997" startWordPosition="1398" endWordPosition="1399">categories for information organization and browsing consists of the following steps: 1. Select representative words from the collection. 2. Get the WordNet hypernym paths for one sense of each selected word. 3. Build a tree from the hypernym paths. 4. Compress the tree. 3.1 Select Representative Words To make the hierarchy size manageable, we select only a subset of the words that are intended to best reflect the topics covered in the documents (although in principle the method can be used on all of the words in the collection). The criteria for choosing the target words is information gain (Mitchell, 1997). Define the set to be all the unique words in the the document set . Let the distribution of a word be the number of documents in D that the word occurs in. Initially, the words in are ordered according to their distribution in the entire collection . At each iteration, the highest-scoring word is added to an initially-empty set and removed from , and the documents covered by are removed from . The process repeats until no more documents are left in . 3.2 Get Hypernym Paths For every word in , we obtain the hypernym path of the word from WordNet. In the current implementation, we take the hyp</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Tom Mitchell. 1997. Machine Learning. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanda Pratt</author>
<author>Marti Hearst</author>
<author>Larry Fagan</author>
</authors>
<title>A knowledge-based approach to organizing retrieved documents.</title>
<date>1999</date>
<booktitle>In Proceedings ofAAAI 99,</booktitle>
<location>Orlando, FL.</location>
<contexts>
<context position="3488" citStr="Pratt et al., 1999" startWordPosition="519" endWordPosition="522">he hierarchies should not be overly deep nor overly wide, and preferably should have concave structure (meaning broader at the root and leaves, narrower in the middle) (Bernard, 2002). Previous work on automated methods has primarily focused on using clustering techniques, which have the advantage of being automated and data-driven. However, a major problem with clustering is that the groupings show terms that are associated with one another, rather than hierarchical parent-child relations. Studies indicate that users prefer organized categories over associational clusters (Chen et al., 1998; Pratt et al., 1999). We have tested several approaches, including K-means clustering, subsumption (Sanderson and Croft, 1999), computing lexical co-occurrences (Schutze, 1993) and building on the WordNet lexical hierarchy (Fellbaum, 1998). We have found that the latter produces by far the most intuitive groupings that would be useful for creation of a re-usable, human-readable category structure. Although the idea of using a resource like WordNet for this type of application seems rather obvious, to our knowledge it has not been used to create subject-oriented metadata for browsing. This may be in part because i</context>
</contexts>
<marker>Pratt, Hearst, Fagan, 1999</marker>
<rawString>Wanda Pratt, Marti Hearst, and Larry Fagan. 1999. A knowledge-based approach to organizing retrieved documents. In Proceedings ofAAAI 99, Orlando, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louis Rosenfeld</author>
<author>Peter Morville</author>
</authors>
<title>Information Architecture for the World Wide Web: Designing Large-scale Web Sites. O’Reilly &amp;</title>
<date>2002</date>
<publisher>Associates, Inc.</publisher>
<contexts>
<context position="956" citStr="Rosenfeld and Morville, 2002" startWordPosition="124" endWordPosition="127"> nearly-automated approach for deriving such hierarchies, by converting the lexical hierarchy WordNet into a format that reflects the contents of a target information collection. We use the term “nearly-automated” because an information architect should have to make only small adjustments to produce an acceptable metadata structure. We contrast the results with an algorithm that uses lexical co-occurrence statistics. 1 Introduction Human-readable hierarchies of category metadata are needed for a wide range of information-centric applications, including information architectures for web sites (Rosenfeld and Morville, 2002) and metadata for browsing image and document collections (Yee et al., 2003). In the information architecture community, methods for creation of content-oriented metadata tend to be almost entirely manual (Rosenfeld and Morville, 2002). The standard procedure is to gather lists of terms from existing resources, and organize them by selecting, merging and augmenting the term lists to produce a set of hierarchical category labels. Usually the metadata categories are used as labels which are assigned manually to the items in the collection. We advocate instead a nearly-automated approach to build</context>
<context position="2658" citStr="Rosenfeld and Morville, 2002" startWordPosition="393" endWordPosition="396">Church and Hovy, 1993)). More specifically, we aim to develop algorithms for generating category sets that (a) are intuitive to the target audience who will be browsing a web site or collection, (b) reflect the contents of the collection, and (c) allow for (nearly) automated assignment of the categories to the items in the collection. For a category system to be intuitive, modern information science practice finds that it should consist of a set of IS-A (hypernym) hierarchies1, from which multiple labels can be selected and assigned to an item, following the tenants of faceted classification (Rosenfeld and Morville, 2002; Yee et al., 2003). For example, a medical journal article will often simultaneously have terms assigned to it from anatomy, disease, and drug category hierarchies. Furthermore, usability studies suggest that the hierarchies should not be overly deep nor overly wide, and preferably should have concave structure (meaning broader at the root and leaves, narrower in the middle) (Bernard, 2002). Previous work on automated methods has primarily focused on using clustering techniques, which have the advantage of being automated and data-driven. However, a major problem with clustering is that the g</context>
</contexts>
<marker>Rosenfeld, Morville, 2002</marker>
<rawString>Louis Rosenfeld and Peter Morville. 2002. Information Architecture for the World Wide Web: Designing Large-scale Web Sites. O’Reilly &amp; Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehran Sahami</author>
<author>S Yusufali</author>
<author>M Q W Baldonaldo</author>
</authors>
<title>SONIA: A service for organizing networked information autonomously.</title>
<date>1998</date>
<booktitle>In Proceedings ofDL’ 98,</booktitle>
<location>New York.</location>
<contexts>
<context position="7547" citStr="Sahami et al., 1998" startWordPosition="1185" endWordPosition="1188">hm, function, present, result, problem, model. We would prefer something more like the ACM classification hierarchy. The Word Space algorithm (Schutze, 1993) uses linear regression on term co-occurrence statistics to create groups of semantically related words. For every word, a context vector is computed for every position at which it occurs in text. A vector is defined as the sum of all fourgrams in a window of 1001 fourgrams centered around the word. Cosine distance is used to compute the similarity between word vectors. Probably the closest work to that described here is the SONIA system (Sahami et al., 1998) which used a combination of unsupervised and supervised methods to organize a set of documents. The unsupervised method (document clustering) imposes an initial organization on a personal information collection which the user can then modify. The resulting organization is then used to train a supervised text categorization algorithm which automatically classifies new documents. 3 Method WordNet is a manually built lexical system where words are organized into synonym sets (synsets) linked by different relations (Fellbaum, 1998). It can be viewed as a huge graph, where the synsets are the node</context>
</contexts>
<marker>Sahami, Yusufali, Baldonaldo, 1998</marker>
<rawString>Mehran Sahami, S. Yusufali, and M. Q. W. Baldonaldo. 1998. SONIA: A service for organizing networked information autonomously. In Proceedings ofDL’ 98, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Sanderson</author>
<author>Bruce Croft</author>
</authors>
<title>Deriving concept hierarchies from text.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR ’99.</booktitle>
<contexts>
<context position="3594" citStr="Sanderson and Croft, 1999" startWordPosition="532" endWordPosition="535">ure (meaning broader at the root and leaves, narrower in the middle) (Bernard, 2002). Previous work on automated methods has primarily focused on using clustering techniques, which have the advantage of being automated and data-driven. However, a major problem with clustering is that the groupings show terms that are associated with one another, rather than hierarchical parent-child relations. Studies indicate that users prefer organized categories over associational clusters (Chen et al., 1998; Pratt et al., 1999). We have tested several approaches, including K-means clustering, subsumption (Sanderson and Croft, 1999), computing lexical co-occurrences (Schutze, 1993) and building on the WordNet lexical hierarchy (Fellbaum, 1998). We have found that the latter produces by far the most intuitive groupings that would be useful for creation of a re-usable, human-readable category structure. Although the idea of using a resource like WordNet for this type of application seems rather obvious, to our knowledge it has not been used to create subject-oriented metadata for browsing. This may be in part because it is very 1Part-of (meronymy) relations are also intuitive, but are not considered here. large and the wor</context>
<context position="5500" citStr="Sanderson and Croft (1999)" startWordPosition="841" endWordPosition="844">m on a test collection. 2 Related Work There has been surprisingly little work on precisely the problem that we tackle in this paper. The literature on automated text categorization is enormous, but assumes that a set of categories has already been created, whereas the problem here is to determine the categories of interest. There has also been extensive work on finding synonymous terms and word associations, as well as automatic acquisition of IS-A (or genus-head) relations from dictionary definitions and glosses (Klavans and Whitman, 2001) and from free text (Hearst, 1992; Caraballo, 1999). Sanderson and Croft (1999) propose a method called subsumption for building a hierarchy for a set of documents retrieved for a query. For two terms x and y, x is said to subsume y if the following conditions hold: . The evaluation consisted of asking people to define the relation that holds between the pairs of words shown; only 23% of the pairs were found to hold a parent-child relation; 49% were found to fall into a more general related-to category. For a set of medical texts, the top level consisted of the terms: disease, post polio, serious disease, dengue, infection control, immunology, etc. This kind of listing i</context>
</contexts>
<marker>Sanderson, Croft, 1999</marker>
<rawString>Mark Sanderson and Bruce Croft. 1999. Deriving concept hierarchies from text. In Proceedings of SIGIR ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Word space.</title>
<date>1993</date>
<booktitle>Advances in NeuralInformation Processing Systems,</booktitle>
<pages>5--895</pages>
<contexts>
<context position="3644" citStr="Schutze, 1993" startWordPosition="539" endWordPosition="540">iddle) (Bernard, 2002). Previous work on automated methods has primarily focused on using clustering techniques, which have the advantage of being automated and data-driven. However, a major problem with clustering is that the groupings show terms that are associated with one another, rather than hierarchical parent-child relations. Studies indicate that users prefer organized categories over associational clusters (Chen et al., 1998; Pratt et al., 1999). We have tested several approaches, including K-means clustering, subsumption (Sanderson and Croft, 1999), computing lexical co-occurrences (Schutze, 1993) and building on the WordNet lexical hierarchy (Fellbaum, 1998). We have found that the latter produces by far the most intuitive groupings that would be useful for creation of a re-usable, human-readable category structure. Although the idea of using a resource like WordNet for this type of application seems rather obvious, to our knowledge it has not been used to create subject-oriented metadata for browsing. This may be in part because it is very 1Part-of (meronymy) relations are also intuitive, but are not considered here. large and the word senses are assumed to be too finegrained (Mihalc</context>
<context position="7084" citStr="Schutze, 1993" startWordPosition="1106" endWordPosition="1107">count, address, Internet, etc. These again are too scattered to be appropriate for a human-readable index into a document collection. Hofmann (1999) uses probabilistic document clustering to impose topic hierarchies. For a collection of articles from the journal Machine Learning, the top level cluster is labeled learn, paper, base, model, new, train and the second level clusters are labeled process, experi, knowledge, develop, inform, design and algorithm, function, present, result, problem, model. We would prefer something more like the ACM classification hierarchy. The Word Space algorithm (Schutze, 1993) uses linear regression on term co-occurrence statistics to create groups of semantically related words. For every word, a context vector is computed for every position at which it occurs in text. A vector is defined as the sum of all fourgrams in a window of 1001 fourgrams centered around the word. Cosine distance is used to compute the similarity between word vectors. Probably the closest work to that described here is the SONIA system (Sahami et al., 1998) which used a combination of unsupervised and supervised methods to organize a set of documents. The unsupervised method (document cluste</context>
<context position="12777" citStr="Schutze, 1993" startWordPosition="2066" endWordPosition="2067">escriptions of approximately art documents containing about unique words.2 Some sample documents are: A French soldier clings to tree branches as a wolf stands beneath the tree. A Greek trellis with Ionic columns, meander crossing diagonally; few vines; trees background; trellis is in a circle. The descriptions are preprocessed by eliminating frequent words from a stop list. Information gain is used to select target words, in this case resulting in 849 words. Figure 2 shows partial results obtained using the WordNet algorithm (where compression reduced the number of nodes by ) and Word Space (Schutze, 1993). Note that the WordNet-based organization is intuitive, but if not exactly what the designer wants, should be easy to adjust. For example, a designer may prefer to have a “nature” category that combines the subcategories of “geological formation,” “body of water,” and “vascular plant”. Some terminology may also need renaming, but note that WordNet also provides thesaurus terms that can be used in an underlying search engine. Word Space, by contrast, produces associationally related terms. 5 Discussion and Future Work We advocate the use of an existing rich lexical resource for the nearly-auto</context>
</contexts>
<marker>Schutze, 1993</marker>
<rawString>Hinrich Schutze. 1993. Word space. Advances in NeuralInformation Processing Systems, 5:895–902.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ka-Ping Yee</author>
<author>Kirsten Swearingen</author>
<author>Kevin Li</author>
<author>Marti Hearst</author>
</authors>
<title>Faceted metadata for image search and browsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the CHI 2003,</booktitle>
<location>Fort Lauderdale, FL.</location>
<contexts>
<context position="1032" citStr="Yee et al., 2003" startWordPosition="137" endWordPosition="140">archy WordNet into a format that reflects the contents of a target information collection. We use the term “nearly-automated” because an information architect should have to make only small adjustments to produce an acceptable metadata structure. We contrast the results with an algorithm that uses lexical co-occurrence statistics. 1 Introduction Human-readable hierarchies of category metadata are needed for a wide range of information-centric applications, including information architectures for web sites (Rosenfeld and Morville, 2002) and metadata for browsing image and document collections (Yee et al., 2003). In the information architecture community, methods for creation of content-oriented metadata tend to be almost entirely manual (Rosenfeld and Morville, 2002). The standard procedure is to gather lists of terms from existing resources, and organize them by selecting, merging and augmenting the term lists to produce a set of hierarchical category labels. Usually the metadata categories are used as labels which are assigned manually to the items in the collection. We advocate instead a nearly-automated approach to building hierarchical subject category metadata, where suggestions for metadata t</context>
<context position="2677" citStr="Yee et al., 2003" startWordPosition="397" endWordPosition="400">specifically, we aim to develop algorithms for generating category sets that (a) are intuitive to the target audience who will be browsing a web site or collection, (b) reflect the contents of the collection, and (c) allow for (nearly) automated assignment of the categories to the items in the collection. For a category system to be intuitive, modern information science practice finds that it should consist of a set of IS-A (hypernym) hierarchies1, from which multiple labels can be selected and assigned to an item, following the tenants of faceted classification (Rosenfeld and Morville, 2002; Yee et al., 2003). For example, a medical journal article will often simultaneously have terms assigned to it from anatomy, disease, and drug category hierarchies. Furthermore, usability studies suggest that the hierarchies should not be overly deep nor overly wide, and preferably should have concave structure (meaning broader at the root and leaves, narrower in the middle) (Bernard, 2002). Previous work on automated methods has primarily focused on using clustering techniques, which have the advantage of being automated and data-driven. However, a major problem with clustering is that the groupings show terms</context>
<context position="13846" citStr="Yee et al., 2003" startWordPosition="2227" endWordPosition="2230">t, produces associationally related terms. 5 Discussion and Future Work We advocate the use of an existing rich lexical resource for the nearly-automated creation of hierarchical subjectoriented metadata for information browsing and navigation. We have created examples that show that a modified version of WordNet can produce a useful starting point for information organization projects. These have the added advantage ofproducing automated assignments of multiple labels to documents. We plan to augment the processing with more intelligent selection of hypernym 2This collection is also used in (Yee et al., 2003). instrumentation container bag basket vessel bottle bowl tankard urn wheeled vehicle cart carriage wagon furniture altar desk bench structure, construction amphitheater auditorium room tower geological formation beach hill organism, being person, individual apostle gentleman boy king performer comic actor dancer musician female vascular plant corn lily rose shrub body of water canal ocean pond river sea stream act, human action ambush baptism lesson lying market performing waiting carpentry washing creation construction design writing diversion, recreation dancing playing sports, athletics fl</context>
</contexts>
<marker>Yee, Swearingen, Li, Hearst, 2003</marker>
<rawString>Ka-Ping Yee, Kirsten Swearingen, Kevin Li, and Marti Hearst. 2003. Faceted metadata for image search and browsing. In Proceedings of the CHI 2003, Fort Lauderdale, FL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>