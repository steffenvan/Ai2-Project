<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995141">
Model-Based Aligner Combination Using Dual Decomposition
</title>
<author confidence="0.970183">
John DeNero Klaus Macherey
</author>
<affiliation confidence="0.92733">
Google Research Google Research
</affiliation>
<email confidence="0.9828">
denero@google.com kmach@google.com
</email>
<sectionHeader confidence="0.994457" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999868842105263">
Unsupervised word alignment is most often
modeled as a Markov process that generates a
sentence f conditioned on its translation e. A
similar model generating e from f will make
different alignment predictions. Statistical
machine translation systems combine the pre-
dictions of two directional models, typically
using heuristic combination procedures like
grow-diag-final. This paper presents a graph-
ical model that embeds two directional align-
ers into a single model. Inference can be per-
formed via dual decomposition, which reuses
the efficient inference algorithms of the direc-
tional models. Our bidirectional model en-
forces a one-to-one phrase constraint while ac-
counting for the uncertainty in the underlying
directional models. The resulting alignments
improve upon baseline combination heuristics
in word-level and phrase-level evaluations.
</bodyText>
<sectionHeader confidence="0.998879" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995567333333334">
Word alignment is the task of identifying corre-
sponding words in sentence pairs. The standard
approach to word alignment employs directional
Markov models that align the words of a sentence
f to those of its translation e, such as IBM Model 4
(Brown et al., 1993) or the HMM-based alignment
model (Vogel et al., 1996).
Machine translation systems typically combine
the predictions of two directional models, one which
aligns f to e and the other e to f (Och et al.,
1999). Combination can reduce errors and relax
the one-to-many structural restriction of directional
models. Common combination methods include the
union or intersection of directional alignments, as
420
well as heuristic interpolations between the union
and intersection like grow-diag-final (Koehn et al.,
2003). This paper presents a model-based alterna-
tive to aligner combination. Inference in a prob-
abilistic model resolves the conflicting predictions
of two directional models, while taking into account
each model’s uncertainty over its output.
This result is achieved by embedding two direc-
tional HMM-based alignment models into a larger
bidirectional graphical model. The full model struc-
ture and potentials allow the two embedded direc-
tional models to disagree to some extent, but reward
agreement. Moreover, the bidirectional model en-
forces a one-to-one phrase alignment structure, sim-
ilar to the output of phrase alignment models (Marcu
and Wong, 2002; DeNero et al., 2008), unsuper-
vised inversion transduction grammar (ITG) models
(Blunsom et al., 2009), and supervised ITG models
(Haghighi et al., 2009; DeNero and Klein, 2010).
Inference in our combined model is not tractable
because of numerous edge cycles in the model
graph. However, we can employ dual decomposi-
tion as an approximate inference technique (Rush et
al., 2010). In this approach, we iteratively apply the
same efficient sequence algorithms for the underly-
ing directional models, and thereby optimize a dual
bound on the model objective. In cases where our
algorithm converges, we have a certificate of opti-
mality under the full model. Early stopping before
convergence still yields useful outputs.
Our model-based approach to aligner combina-
tion yields improvements in alignment quality and
phrase extraction quality in Chinese-English exper-
iments, relative to typical heuristic combinations
methods applied to the predictions of independent
directional models.
</bodyText>
<note confidence="0.9795335">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 420–429,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.840708" genericHeader="method">
2 Model Definition
</sectionHeader>
<bodyText confidence="0.999960923076923">
Our bidirectional model !9 = (V, D) is a globally
normalized, undirected graphical model of the word
alignment for a fixed sentence pair (e, f). Each ver-
tex in the vertex set V corresponds to a model vari-
able Vi, and each undirected edge in the edge set D
corresponds to a pair of variables (Vi, Vj). Each ver-
tex has an associated potential function ωi(vi) that
assigns a real-valued potential to each possible value
vi of Vi.1 Likewise, each edge has an associated po-
tential function µij(vi, vj) that scores pairs of val-
ues. The probability under the model of any full as-
signment v to the model variables, indexed by V,
factors over vertex and edge potentials.
</bodyText>
<equation confidence="0.991126">
P(v) a H Hωi(vi) · µij(vi, vj)
vi∈V (vi,vj)∈D
</equation>
<bodyText confidence="0.99991725">
Our model contains two directional hidden
Markov alignment models, which we review in Sec-
tion 2.1, along with additional structure that that we
introduce in Section 2.2.
</bodyText>
<subsectionHeader confidence="0.866159">
2.1 HMM-Based Alignment Model
</subsectionHeader>
<bodyText confidence="0.999867190476191">
This section describes the classic hidden Markov
model (HMM) based alignment model (Vogel et al.,
1996). The model generates a sequence of words f
conditioned on a word sequence e. We convention-
ally index the words of e by i and f by j. P(f|e)
is defined in terms of a latent alignment vector a,
where aj = i indicates that word position i of e
aligns to word position j of f.
where c(i0 − i) is a learned distribution over signed
distances, normalized over the possible transitions
from i. The parameters of the conditional multino-
mial M and the transition model c can be learned
from a sentence aligned corpus via the expectation
maximization algorithm. The null parameter po is
typically fixed.2
The highest probability word alignment vector
under the model for a given sentence pair (e, f) can
be computed exactly using the standard Viterbi al-
gorithm for HMMs in O(|e|2 · |f|) time.
An alignment vector a can be converted trivially
into a set of word alignment links A:
</bodyText>
<equation confidence="0.856254">
Aa = {(i,j) : aj = i,i =�01 .
</equation>
<bodyText confidence="0.994546833333333">
Aa is constrained to be many-to-one from f to e;
many positions j can align to the same i, but each j
appears at most once.
We have defined a directional model that gener-
ates f from e. An identically structured model can
be defined that generates e from f. Let b be a vector
of alignments where bi = j indicates that word po-
sition j of f aligns to word position i of e. Then,
P(e, b|f) is defined similarly to Equation 1, but
with e and f swapped. We can distinguish the tran-
sition and emission distributions of the two models
by subscripting them with their generative direction.
</bodyText>
<equation confidence="0.995046">
|e|
P(e,b|f) = H Df→e(bi|bi−1)Mf→e(ei|fbi) .
j=1
</equation>
<bodyText confidence="0.997443666666667">
The vector b can be interpreted as a set of align-
ment links that is one-to-many: each value i appears
at most once in the set.
</bodyText>
<equation confidence="0.974655666666667">
Ab={(i,j): bi=j,j =�01 .
P(f|e) = 1: P(f, a|e)
a
|f|
P(f, a|e) = H D(aj|aj−1)M(fj|eaj) . (1)
j=1
</equation>
<bodyText confidence="0.9998274">
In Equation 1 above, the emission model M is
a learned multinomial distribution over word types.
The transition model D is a multinomial over tran-
sition distances, which treats null alignments as a
special case.
</bodyText>
<equation confidence="0.9990645">
D(aj = 0|aj−1 = i) = po
D(aj = i0 =� 0|aj−1 = i) = (1 − po) · c(i0 − i) ,
</equation>
<footnote confidence="0.934703">
1Potentials in an undirected model play the same role as con-
ditional probabilities in a directed model, but do not need to be
locally normalized.
</footnote>
<subsectionHeader confidence="0.997617">
2.2 A Bidirectional Alignment Model
</subsectionHeader>
<bodyText confidence="0.9844616">
We can combine two HMM-based directional align-
ment models by embedding them in a larger model
2In experiments, we set po = 10−s. Transitions from a null-
aligned state aj−1 = 0 are also drawn from a fixed distribution,
where D(aj = 0jaj−1 = 0) = 10−4 and for i0 ~ 1,
</bodyText>
<equation confidence="0.919917">
D(aj = i jaj−1 = 0) / 0.8~−1&amp;quot;· �e� iri
−j .
</equation>
<bodyText confidence="0.946913">
With small po, the shape of this distribution has little effect on
the alignment outcome.
</bodyText>
<page confidence="0.997111">
421
</page>
<figure confidence="0.337313">
How are you
</figure>
<figureCaption confidence="0.992874">
Figure 1: The structure of our graphical model for a sim-
ple sentence pair. The variables a are blue, b are red, and
c are green.
</figureCaption>
<bodyText confidence="0.961893">
that includes all of the random variables of two di-
rectional models, along with additional structure that
promotes agreement and resolves discrepancies.
</bodyText>
<equation confidence="0.742671">
c11(a) c12(a)
</equation>
<bodyText confidence="0.9999655">
The original directional models include observed
word sequences e and f, along with the two latent
alignment vectors a and b defined in Section 2.1.
Because the word types and lengths of e and f are
</bodyText>
<subsectionHeader confidence="0.95023">
(a (a)
</subsectionHeader>
<bodyText confidence="0.999152666666667">
always fixed by the observed sentence pair, we can
define our model only over a and b, where the edge
potentials between any aj, fj, and e are compiled
into a vertex potential function ω(a)
j on aj, defined
in terms of f and e, and likewise for any bi.
</bodyText>
<equation confidence="0.931642285714286">
a1 a2
ωi (j) = Mf→e(ei|fj)
(b)
The edge potentials between a and b encode the
transition model in Equation 1.
µj−1,j(i, i0) = De→f(aj = i0|aj−1 = i)
µi−1,i(j, j0) = Df→e(bi = j0|bi−1 = j)
</equation>
<bodyText confidence="0.876089666666667">
In addition, we include in our model a latent
boolean matrix c that encodes the output of the com-
bined aligners:
</bodyText>
<equation confidence="0.741108">
c ∈ {0,1}|e|×|f |.
</equation>
<bodyText confidence="0.9799405">
This matrix encodes the alignment links proposed
by the bidirectional model:
</bodyText>
<equation confidence="0.7483115">
Ac = {(i,j) : cij
= 1} .
</equation>
<bodyText confidence="0.999907666666667">
Each model node for an element cijcE210, 1} is
connected to aj and bi via coherence edges. These
edges allow the model to ensure that the three sets
of variables, a, b, and c, together encode a coher-
ent alignment analysis of the(sentence pair. Figure 1
depicts the graph structure of the model.
</bodyText>
<subsectionHeader confidence="0.999734">
2.3 Coherence Potentials
</subsectionHeader>
<bodyText confidence="0.997289272727273">
The potentials on coherence edges are not learned
and do not express any patterns in the data. Instead,
they are fixed functions that promote consistency be-
tween the integer-valued directional alignment vec-
tors a and b and the boolean-valued matrix c.
Consider the assignment aj = i, where i = 0
indicates that word fj is null-aligned, and i ≥ 1 in-
dicates that fj aligns to ei. The coherence potential
ensures the following relationship between the vari-
able assignment aj = i and the variables ci0j, for
any i0 ∈ [1, |e|].
</bodyText>
<listItem confidence="0.999737">
• If i = 0 (null-aligned), then all ci0 j = 0.
• If i &gt; 0, then cij = 1.
• ci0j = 1 only if i0 ∈ {i − 1, i, i + 1}.
• Assigning ci0j = 1 for i0 =6 i incurs a cost e−α.
</listItem>
<bodyText confidence="0.976599625">
Collectively, the list of cases above enforce an intu-
itive correspondence: an alignment aj = i ensures
that cij must be 1, adjacent neighbors may be 1 but
incur a cost, and all other elements are 0.
This pattern of effects can be encoded in a poten-
tial function µ(c) for each coherence edge. These
e potential functions takes an integer value i for
some variable aj and a binary value k for some ci0j.
</bodyText>
<equation confidence="0.999233142857143">
1 i = 0 ∧ k = 0
0 i = 0 ∧ k = 1
1 i = i0 ∧ k = 1
0 i = i0 ∧ k = 0 (2)
1 i =6i0 ∧ k = 0
e−α |i − i0 |= 1 ∧ k = 1
0 |i − i0 |&gt; 1 ∧ k = 1
</equation>
<bodyText confidence="0.894394333333333">
Above, potentials of 0 effectively disallow certain
cases because a full assignment to (a, b, c) is scored
by the product of all model potentials. The poten-
tial function µ(c)
(bi,cij0)(j, k) for a coherence edge be-
tween b and c is defined similarly.
</bodyText>
<equation confidence="0.997588933333333">
c21 c22 c23
c11
a1
c12 c13
a2 a3
b2
b1
你
好
ω(a) (i) M (f I e )
j — e—&apos;f 9 i you
edg
(c))
µ(aj ci0j)(i, k=
{
</equation>
<page confidence="0.993108">
422
</page>
<subsectionHeader confidence="0.992694">
2.4 Model Properties
</subsectionHeader>
<bodyText confidence="0.999944708333333">
We interpret c as the final alignment produced by the
model, ignoring a and b. In this way, we relax the
one-to-many constraints of the directional models.
However, all of the information about how words
align is expressed by the vertex and edge potentials
on a and b. The coherence edges and the link ma-
trix c only serve to resolve conflicts between the di-
rectional models and communicate information be-
tween them.
Because directional alignments are preserved in-
tact as components of our model, extensions or
refinements to the underlying directional Markov
alignment model could be integrated cleanly into
our model as well, including lexicalized transition
models (He, 2007), extended conditioning contexts
(Brunning et al., 2009), and external information
(Shindo et al., 2010).
For any assignment to (a, b, c) with non-zero
probability, c must encode a one-to-one phrase
alignment with a maximum phrase length of 3. That
is, any word in either sentence can align to at most
three words in the opposite sentence, and those
words must be contiguous. This restriction is di-
rectly enforced by the edge potential in Equation 2.
</bodyText>
<sectionHeader confidence="0.970343" genericHeader="method">
3 Model Inference
</sectionHeader>
<bodyText confidence="0.99976">
In general, graphical models admit efficient, exact
inference algorithms if they do not contain cycles.
Unfortunately, our model contains numerous cycles.
For every pair of indices (i, j) and (i&apos;, j&apos;), the fol-
lowing cycle exists in the graph:
</bodyText>
<equation confidence="0.997082">
cij → bi → cij0 → aj0 →
ci0j0 → bi0 → ci0j → aj → cij
</equation>
<bodyText confidence="0.9997112">
Additional cycles also exist in the graph through
the edges between aj−1 and aj and between bi−1
and bi. The general phrase alignment problem under
an arbitrary model is known to be NP-hard (DeNero
and Klein, 2008).
</bodyText>
<subsectionHeader confidence="0.99584">
3.1 Dual Decomposition
</subsectionHeader>
<bodyText confidence="0.999969">
While the entire graphical model has loops, there are
two overlapping subgraphs that are cycle-free. One
subgraph Ga includes all of the vertices correspond-
ing to variables a and c. The other subgraph Gb in-
cludes vertices for variables b and c. Every edge in
the graph belongs to exactly one of these two sub-
graphs.
The dual decomposition inference approach al-
lows us to exploit this sub-graph structure (Rush et
al., 2010). In particular, we can iteratively apply
exact inference to the subgraph problems, adjusting
their potentials to reflect the constraints of the full
problem. The technique of dual decomposition has
recently been shown to yield state-of-the-art perfor-
mance in dependency parsing (Koo et al., 2010).
</bodyText>
<subsectionHeader confidence="0.994129">
3.2 Dual Problem Formulation
</subsectionHeader>
<bodyText confidence="0.999741416666667">
To describe a dual decomposition inference proce-
dure for our model, we first restate the inference
problem under our graphical model in terms of the
two overlapping subgraphs that admit tractable in-
ference. Let c(a) be a copy of c associated with Ga,
and c(b) with Gb. Also, let f(a, c(a)) be the un-
normalized log-probability of an assignment to Ga
and g(b, c(b)) be the unnormalized log-probability
of an assignment to Gb. Finally, let I be the index
set of all (i, j) for c. Then, the maximum likelihood
assignment to our original model can be found by
optimizing
</bodyText>
<equation confidence="0.9656988">
max f(a, c(a)) + g(b, c(b)) (3)
a,b,c(a),c(b)
such that: c(a)
ij = c(b)
ij ∀ (i, j) ∈ I .
The Lagrangian relaxation of this optimization
problem is L(a, b, c(a), c(b), u) =
X
f(a,c(a))+g(b,c(b))+
(i,j)EZ
</equation>
<bodyText confidence="0.893893142857143">
Hence, we can rewrite the original problem as
L(a, b, c(a), c(b), u) .
We can form a dual problem that is an up-
per bound on the original optimization problem by
swapping the order of min and max. In this case,
the dual problem decomposes into two terms that are
each local to an acyclic subgraph.
</bodyText>
<equation confidence="0.962741571428571">
⎛max ⎡ f (a, c(a)) + X u(i, j)c(a)
ij
a,c(a)
⎡ ⎤ ⎞
X
⎣g(b,c(b)) − u(i,j)c(b) ⎦ ⎠(4)
ij
</equation>
<page confidence="0.393271">
i,j
</page>
<figure confidence="0.9165923">
u(i,j)(c(a)
i,j − c(b)
i,j ) .
max min
a,b,c(a),c(b) u
min
u
i,j
+ max
b,c(b)
</figure>
<page confidence="0.585564">
423
</page>
<figure confidence="0.933245826086957">
How are you
c21(a)
c11(a)
a1 a2 a3
c21(b)
c11(b)
c12(a)
c22(a)
c12(b)
c22(b)
c23(a)
c13(a)
c23(b)
c13(b)
b1
b2
你
好
c11(a)
c21(a) c22(a) c23(a)
a1 a2 a3
c12(a) c13(a)
How are you
</figure>
<figureCaption confidence="0.9948065">
Figure 2: Our combined model decomposes into two
acyclic models that each contain a copy of c.
</figureCaption>
<bodyText confidence="0.99952925">
The decomposed model is depicted in Figure 2.
As in previous work, we solve for the dual variable
u by repeatedly performing inference in the two de-
coupled maximization problems.
</bodyText>
<subsectionHeader confidence="0.994722">
3.3 Sub-Graph Inference
</subsectionHeader>
<bodyText confidence="0.999968">
We now address the problem of evaluating Equa-
tion 4 for fixed u. Consider the first line of Equa-
tion 4, which includes variables a and c(a).
</bodyText>
<equation confidence="0.9028748">
⎡ ⎤
X
⎣f(a, c(a)) + u(i, j)c(a) ⎦ (5)
ij
i,j
</equation>
<bodyText confidence="0.999956">
Because the graph Ga is tree-structured, Equa-
tion 5 can be evaluated in polynomial time. In fact,
we can make a stronger claim: we can reuse the
Viterbi inference algorithm for linear chain graph-
ical models that applies to the embedded directional
HMM models. That is, we can cast the optimization
of Equation 5 as
</bodyText>
<equation confidence="0.931815">
⎤
De→f(aj|aj−1) · M0j(aj = i) ⎦.
</equation>
<bodyText confidence="0.999004166666667">
In the original HMM-based aligner, the vertex po-
tentials correspond to bilexical probabilities. Those
quantities appear in f(a, c(a)), and therefore will be
a part of M0j(·) above. The additional terms of Equa-
tion 5 can also be factored into the vertex poten-
tials of this linear chain model, because the optimal
</bodyText>
<figureCaption confidence="0.966145">
Figure 3: The tree-structured subgraph Ga can be mapped
to an equivalent chain-structured model by optimizing
over ciij for aj = i.
</figureCaption>
<bodyText confidence="0.968485846153846">
choice of each cij can be determined from aj and the
model parameters. If aj = i, then cij = 1 according
to our edge potential defined in Equation 2. Hence,
setting aj = i requires the inclusion of the corre-
sponding vertex potential w(a)
j (i), as well as u(i, j).
For i0 =6 i, either ci0j = 0, which contributes noth-
ing to Equation 5, or ci0j = 1, which contributes
u(i0, j)−α, according to our edge potential between
aj and ci0j.
Thus, we can capture the net effect of assigning
aj and then optimally assigning all ci0j in a single
potential M0j(aj = i) =
</bodyText>
<equation confidence="0.994009">
wja) (i) + exp⎡⎣u(i, j) + Xmax(0, u(i, j0) − α)⎦
j0:|j0−j|=1
</equation>
<bodyText confidence="0.999725384615385">
Note that Equation 5 and f are sums of terms in
log space, while Viterbi inference for linear chains
assumes a product of terms in probability space,
which introduces the exponentiation above.
Defining this potential allows us to collapse the
source-side sub-graph inference problem defined
by Equation 5, into a simple linear chain model
that only includes potential functions M0j and µ(a).
Hence, we can use a highly optimized linear chain
inference implementation rather than a solver for
general tree-structured graphical models. Figure 3
depicts this transformation.
An equivalent approach allows us to evaluate the
</bodyText>
<figure confidence="0.9933512">
max
a,c(a)
max ⎡
a Y|f|
⎣j=1
</figure>
<page confidence="0.997626">
424
</page>
<bodyText confidence="0.980229">
Algorithm 1 Dual decomposition inference algo-
rithm for the bidirectional model
</bodyText>
<equation confidence="0.903240588235294">
for t = 1 to max iterations do
r +— 1 . Learning rate
t
c(a) +— arg max f(a, c(a)) + Pi,j u(i, j)c(a)
ij
c(b) +— arg max g(b,c(b)) − P i,j u(i,j)c(b)
ij
if c(a) = c(b) then
return c(a) . Converged
u +— u + r · (c(b) − c(a)) . Dual update
return combine(c(a), c(b)) . Stop early
second line of Equation 4 for fixed u:
max ⎡ ⎤
b,c(b) X
⎣g(b, c(b)) + u(i, j)c(b) ⎦ . (6)
ij
i,j
</equation>
<subsectionHeader confidence="0.88885">
3.4 Dual Decomposition Algorithm
</subsectionHeader>
<bodyText confidence="0.999986133333334">
Now that we have the means to efficiently evalu-
ate Equation 4 for fixed u, we can define the full
dual decomposition algorithm for our model, which
searches for a u that optimizes Equation 4. We can
iteratively search for such a u via sub-gradient de-
scent. We use a learning rate 1 t that decays with the
number of iterations t. The full dual decomposition
optimization procedure appears in Algorithm 1.
If Algorithm 1 converges, then we have found a u
such that the value of c(a) that optimizes Equation 5
is identical to the value of c(b) that optimizes Equa-
tion 6. Hence, it is also a solution to our original
optimization problem: Equation 3. Since the dual
problem is an upper bound on the original problem,
this solution must be optimal for Equation 3.
</bodyText>
<subsectionHeader confidence="0.986887">
3.5 Convergence and Early Stopping
</subsectionHeader>
<bodyText confidence="0.99979425">
Our dual decomposition algorithm provides an infer-
ence method that is exact upon convergence.3 When
Algorithm 1 does not converge, the two alignments
c(a) and c(b) can still be used. While these align-
ments may differ, they will likely be more similar
than the alignments of independent aligners.
These alignments will still need to be combined
procedurally (e.g., taking their union), but because
</bodyText>
<footnote confidence="0.465639333333333">
3This certificate of optimality is not provided by other ap-
proximate inference algorithms, such as belief propagation,
sampling, or simulated annealing.
</footnote>
<bodyText confidence="0.999835333333333">
they are more similar, the importance of the combi-
nation procedure is reduced. We analyze the behav-
ior of early stopping experimentally in Section 5.
</bodyText>
<subsectionHeader confidence="0.801003">
3.6 Inference Properties
</subsectionHeader>
<bodyText confidence="0.999995157894737">
Because we set a maximum number of iterations
n in the dual decomposition algorithm, and each
iteration only involves optimization in a sequence
model, our entire inference procedure is only a con-
stant multiple n more computationally expensive
than evaluating the original directional aligners.
Moreover, the value of u is specific to a sen-
tence pair. Therefore, our approach does not require
any additional communication overhead relative to
the independent directional models in a distributed
aligner implementation. Memory requirements are
virtually identical to the baseline: only u must be
stored for each sentence pair as it is being processed,
but can then be immediately discarded once align-
ments are inferred.
Other approaches to generating one-to-one phrase
alignments are generally more expensive. In par-
ticular, an ITG model requires O(|e|3 · |f|3) time,
whereas our algorithm requires only
</bodyText>
<equation confidence="0.512298">
O(n · (|f||e|&apos; + |e||f|&apos;)) .
</equation>
<bodyText confidence="0.999919333333333">
Moreover, our approach allows Markov distortion
potentials, while standard ITG models are restricted
to only hierarchical distortion.
</bodyText>
<sectionHeader confidence="0.999865" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.937205214285714">
Alignment combination normally involves selecting
some A from the output of two directional models.
Common approaches include forming the union or
intersection of the directional sets.
Au = Aa u Ab
An=AanAb.
More complex combiners, such as the grow-diag-
final heuristic (Koehn et al., 2003), produce align-
ment link sets that include all of An and some sub-
set of Au based on the relationship of multiple links
(Och et al., 1999).
In addition, supervised word alignment models
often use the output of directional unsupervised
aligners as features or pruning signals. In the case
</bodyText>
<page confidence="0.998155">
425
</page>
<bodyText confidence="0.999662854166667">
that a supervised model is restricted to proposing
alignment links that appear in the output of a di-
rectional aligner, these models can be interpreted as
a combination technique (Deng and Zhou, 2009).
Such a model-based approach differs from ours in
that it requires a supervised dataset and treats the di-
rectional aligners’ output as fixed.
Combination is also related to agreement-based
learning (Liang et al., 2006). This approach to
jointly learning two directional alignment mod-
els yields state-of-the-art unsupervised performance.
Our method is complementary to agreement-based
learning, as it applies to Viterbi inference under the
model rather than computing expectations. In fact,
we employ agreement-based training to estimate the
parameters of the directional aligners in our experi-
ments.
A parallel idea that closely relates to our bidi-
rectional model is posterior regularization, which
has also been applied to the word alignment prob-
lem (Grac¸a et al., 2008). One form of posterior
regularization stipulates that the posterior probabil-
ity of alignments from two models must agree, and
enforces this agreement through an iterative proce-
dure similar to Algorithm 1. This approach also
yields state-of-the-art unsupervised alignment per-
formance on some datasets, along with improve-
ments in end-to-end translation quality (Ganchev et
al., 2008).
Our method differs from this posterior regulariza-
tion work in two ways. First, we iterate over Viterbi
predictions rather than posteriors. More importantly,
we have changed the output space of the model to
be a one-to-one phrase alignment via the coherence
edge potential functions.
Another similar line of work applies belief prop-
agation to factor graphs that enforce a one-to-one
word alignment (Cromi`eres and Kurohashi, 2009).
The details of our models differ: we employ
distance-based distortion, while they add structural
correspondence terms based on syntactic parse trees.
Also, our model training is identical to the HMM-
based baseline training, while they employ belief
propagation for both training and Viterbi inference.
Although differing in both model and inference, our
work and theirs both find improvements from defin-
ing graphical models for alignment that do not admit
exact polynomial-time inference algorithms.
</bodyText>
<table confidence="0.992674">
Aligner Intersection Union Agreement
Model |An ||Au ||An|/|Au|
Baseline 5,554 10,998 50.5%
Bidirectional 7,620 10,262 74.3%
</table>
<tableCaption confidence="0.94216625">
Table 1: The bidirectional model’s dual decomposition
algorithm substantially increases the overlap between the
predictions of the directional models, measured by the
number of links in their intersection.
</tableCaption>
<sectionHeader confidence="0.980244" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999814">
We evaluated our bidirectional model by comparing
its output to the annotations of a hand-aligned cor-
pus. In this way, we can show that the bidirectional
model improves alignment quality and enables the
extraction of more correct phrase pairs.
</bodyText>
<subsectionHeader confidence="0.988083">
5.1 Data Conditions
</subsectionHeader>
<bodyText confidence="0.999899090909091">
We evaluated alignment quality on a hand-aligned
portion of the NIST 2002 Chinese-English test set
(Ayan and Dorr, 2006). We trained the model on a
portion of FBIS data that has been used previously
for alignment model evaluation (Ayan and Dorr,
2006; Haghighi et al., 2009; DeNero and Klein,
2010). We conducted our evaluation on the first 150
sentences of the dataset, following previous work.
This portion of the dataset is commonly used to train
supervised models.
We trained the parameters of the directional mod-
els using the agreement training variant of the expec-
tation maximization algorithm (Liang et al., 2006).
Agreement-trained IBM Model 1 was used to ini-
tialize the parameters of the HMM-based alignment
models (Brown et al., 1993). Both IBM Model 1
and the HMM alignment models were trained for
5 iterations on a 6.2 million word parallel corpus
of FBIS newswire. This training regimen on this
data set has provided state-of-the-art unsupervised
results that outperform IBM Model 4 (Haghighi et
al., 2009).
</bodyText>
<subsectionHeader confidence="0.999567">
5.2 Convergence Analysis
</subsectionHeader>
<bodyText confidence="0.9999278">
With n = 250 maximum iterations, our dual decom-
position inference algorithm only converges 6.2%
of the time, perhaps largely due to the fact that the
two directional models have different one-to-many
structural constraints. However, the dual decompo-
</bodyText>
<page confidence="0.996584">
426
</page>
<table confidence="0.998492285714286">
Model Combiner Prec Rec AER
union 57.6 80.0 33.4
Baseline intersect 86.2 62.7 27.2
grow-diag 60.1 78.8 32.1
union 63.3 81.5 29.1
Bidirectional intersect 77.5 75.1 23.6
grow-diag 65.6 80.6 28.0
</table>
<tableCaption confidence="0.994547666666667">
Table 2: Alignment error rate results for the bidirectional
model versus the baseline directional models. “grow-
diag” denotes the grow-diag-final heuristic.
</tableCaption>
<table confidence="0.999800285714286">
Model Combiner Prec Rec F1
union 75.1 33.5 46.3
Baseline intersect 64.3 43.4 51.8
grow-diag 68.3 37.5 48.4
union 63.2 44.9 52.5
Bidirectional intersect 57.1 53.6 55.3
grow-diag 60.2 47.4 53.0
</table>
<tableCaption confidence="0.933205">
Table 3: Phrase pair extraction accuracy for phrase pairs
up to length 5. “grow-diag” denotes the grow-diag-final
heuristic.
</tableCaption>
<bodyText confidence="0.9999214">
sition algorithm does promote agreement between
the two models. We can measure the agreement
between models as the fraction of alignment links
in the union Au that also appear in the intersection
An of the two directional models. Table 1 shows
a 47% relative increase in the fraction of links that
both models agree on by running dual decomposi-
tion (bidirectional), relative to independent direc-
tional inference (baseline). Improving convergence
rates represents an important area of future work.
</bodyText>
<subsectionHeader confidence="0.999179">
5.3 Alignment Error Evaluation
</subsectionHeader>
<bodyText confidence="0.993963714285714">
To evaluate alignment error of the baseline direc-
tional aligners, we must apply a combination pro-
cedure such as union or intersection to Aa and Ab.
Likewise, in order to evaluate alignment error for
our combined model in cases where the inference
algorithm does not converge, we must apply combi-
nation to c(a) and c(b). In cases where the algorithm
does converge, c(a) = c(b) and so no further combi-
nation is necessary.
We evaluate alignments relative to hand-aligned
data using two metrics. First, we measure align-
ment error rate (AER), which compares the pro-
posed alignment set A to the sure set S and possible
set P in the annotation, where S C P.
</bodyText>
<equation confidence="0.788988166666667">
Prec(A, P) = |A n P|
|A|
Rec(A, S) = |A n S|
|S|
AER(A, S, P) = 1 − |A n S |+ |A n P|
|A |+ |S|
</equation>
<bodyText confidence="0.963417696969697">
AER results for Chinese-English are reported in
Table 2. The bidirectional model improves both pre-
cision and recall relative to all heuristic combination
techniques, including grow-diag-final (Koehn et al.,
2003). Intersected alignments, which are one-to-one
phrase alignments, achieve the best AER.
Second, we measure phrase extraction accuracy.
Extraction-based evaluations of alignment better co-
incide with the role of word aligners in machine
translation systems (Ayan and Dorr, 2006). Let
R5(S, P) be the set of phrases up to length 5 ex-
tracted from the sure link set S and possible link set
P. Possible links are both included and excluded
from phrase pairs during extraction, as in DeNero
and Klein (2010). Null aligned words are never in-
cluded in phrase pairs for evaluation. Phrase ex-
traction precision, recall, and F1 for R5(A,A) are
reported in Table 3. Correct phrase pair recall in-
creases from 43.4% to 53.6% (a 23.5% relative in-
crease) for the bidirectional model, relative to the
best baseline.
Finally, we evaluated our bidirectional model in a
large-scale end-to-end phrase-based machine trans-
lation system from Chinese to English, based on
the alignment template approach (Och and Ney,
2004). The translation model weights were tuned for
both the baseline and bidirectional alignments using
lattice-based minimum error rate training (Kumar et
al., 2009). In both cases, union alignments outper-
formed other combination heuristics. Bidirectional
alignments yielded a modest improvement of 0.2%
BLEU4 on a single-reference evaluation set of sen-
tences sampled from the web (Papineni et al., 2002).
</bodyText>
<footnote confidence="0.79772325">
4BLEU improved from 29.59% to 29.82% after training
IBM Model 1 for 3 iterations and training the IIMM-based
alignment model for 3 iterations. During training, link poste-
riors were symmetrized by pointwise linear interpolation.
</footnote>
<page confidence="0.983176">
427
</page>
<bodyText confidence="0.993068777777778">
As our model only provides small improvements in David Burkett, John Blitzer, and Dan Klein. 2010.
alignment precision and recall for the union com- Joint parsing and alignment with weakly synchronized
biner, the magnitude of the BLEU improvement is grammars. In Proceedings of the North American As-
not surprising. sociation for Computational Linguistics and IJCNLP.
6 Conclusion Fabien Cromi`eres and Sadao Kurohashi. 2009. An
We have presented a graphical model that combines alignment algorithm using belief propagation and a
two classical HMM-based alignment models. Our structure-based distortion model. In Proceedings of
bidirectional model, which requires no additional the European Chapter of the Association for Compu-
learning and no supervised data, can be applied us- tational Linguistics and IJCNLP.
ing dual decomposition with only a constant factor Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
additional computation relative to independent di- of-speech tagging with bilingual graph-based projec-
rectional inference. The resulting predictions im- tions. In Proceedings of the Association for Computa-
prove the precision and recall of both alignment tional Linguistics.
links and extraced phrase pairs in Chinese-English John DeNero and Dan Klein. 2008. The complexity of
experiments. The best results follow from combina- phrase alignment problems. In Proceedings of the As-
tion via intersection. sociation for Computational Linguistics.
Because our technique is defined declaratively in John DeNero and Dan Klein. 2010. Discriminative mod-
terms of a graphical model, it can be extended in a eling of extraction sets for machine translation. In
straightforward manner, for instance with additional Proceedings of the Association for Computational Lin-
potentials on c or improvements to the component guistics.
directional models. We also look forward to dis- John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein.
covering the best way to take advantage of these 2008. Sampling alignment structure under a Bayesian
new alignments in downstream applications like ma- translation model. In Proceedings of the Conference
chine translation, supervised word alignment, bilin- on Empirical Methods in Natural Language Process-
gual parsing (Burkett et al., 2010), part-of-speech ing.
tag induction (Naseem et al., 2009), or cross-lingual Yonggang Deng and Bowen Zhou. 2009. Optimizing
model projection (Smith and Eisner, 2009; Das and word alignment combination for phrase table training.
</bodyText>
<reference confidence="0.985394225352113">
Petrov, 2011). In Proceedings of the Association for Computational
References Linguistics.
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going be- Kuzman Ganchev, Joao Grac¸a, and Ben Taskar. 2008.
yond AER: An extensive analysis of word alignments Better alignments = better translations? In Proceed-
and their impact on MT. In Proceedings of the Asso- ings of the Association for Computational Linguistics.
ciation for Computational Linguistics. Joao Grac¸a, Kuzman Ganchev, and Ben Taskar. 2008.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os- Expectation maximization and posterior constraints.
borne. 2009. A Gibbs sampler for phrasal syn- In Proceedings of Neural Information Processing Sys-
chronous grammar induction. In Proceedings of the tems.
Association for Computational Linguistics. Aria Haghighi, John Blitzer, John DeNero, and Dan
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Klein. 2009. Better word alignments with supervised
Pietra, and Robert L. Mercer. 1993. The mathematics ITG models. In Proceedings of the Association for
of statistical machine translation: Parameter estima- Computational Linguistics.
tion. Computational Linguistics. Xiaodong He. 2007. Using word-dependent transition
Jamie Brunning, Adria de Gispert, and William Byrne. models in HMM-based word alignment for statistical
2009. Context-dependent alignment models for statis- machine. In ACL Workshop on Statistical Machine
tical machine translation. In Proceedings of the North Translation.
American Chapter of the Association for Computa- Philipp Koehn, Franz Josef Och, and Daniel Marcu.
tional Linguistics. 2003. Statistical phrase-based translation. In Proceed-
428 ings of the North American Chapter of the Association
for Computational Linguistics.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Josef Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of the
Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. Journal ofAr-
tificial Intelligence Research.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.
Franz Josef Och, Christopher Tillman, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
Association for Computational Linguistics.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2010. Word alignment with synonym regularization.
In Proceedings of the Association for Computational
Linguistics.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the Conference on Computa-
tional linguistics.
</reference>
<page confidence="0.99923">
429
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.936839">
<title confidence="0.999704">Model-Based Aligner Combination Using Dual Decomposition</title>
<author confidence="0.998542">John DeNero Klaus Macherey</author>
<affiliation confidence="0.953896">Google Research Google Research</affiliation>
<email confidence="0.994024">denero@google.comkmach@google.com</email>
<abstract confidence="0.9994235">Unsupervised word alignment is most often modeled as a Markov process that generates a on its translation A model generating make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>