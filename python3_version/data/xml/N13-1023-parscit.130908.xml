<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006472">
<title confidence="0.997738">
Segmentation Strategies for Streaming Speech Translation
</title>
<author confidence="0.7762855">
Vivek Kumar Rangarajan Sridhar, John Chen, Srinivas Bangalore
Andrej Ljolje, Rathinavelu Chengalvarayan
</author>
<affiliation confidence="0.651255">
AT&amp;T Labs - Research
</affiliation>
<address confidence="0.745982">
180 Park Avenue, Florham Park, NJ 07932
</address>
<email confidence="0.998415">
vkumar,jchen,srini,alj,rathi@research.att.com
</email>
<sectionHeader confidence="0.993888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999579884615384">
The study presented in this work is a first ef-
fort at real-time speech translation of TED
talks, a compendium of public talks with dif-
ferent speakers addressing a variety of top-
ics. We address the goal of achieving a sys-
tem that balances translation accuracy and la-
tency. In order to improve ASR performance
for our diverse data set, adaptation techniques
such as constrained model adaptation and vo-
cal tract length normalization are found to be
useful. In order to improve machine transla-
tion (MT) performance, techniques that could
be employed in real-time such as monotonic
and partial translation retention are found to
be of use. We also experiment with inserting
text segmenters of various types between ASR
and MT in a series of real-time translation ex-
periments. Among other results, our experi-
ments demonstrate that a good segmentation
is useful, and a novel conjunction-based seg-
mentation strategy improves translation qual-
ity nearly as much as other strategies such
as comma-based segmentation. It was also
found to be important to synchronize various
pipeline components in order to minimize la-
tency.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980148936171">
The quality of automatic speech-to-text and speech-
to-speech (S2S) translation has improved so signifi-
cantly over the last several decades that such systems
are now widely deployed and used by an increasing
number of consumers. Under the hood, the individ-
ual components such as automatic speech recogni-
tion (ASR), machine translation (MT) and text-to-
speech synthesis (TTS) that constitute a S2S sys-
tem are still loosely coupled and typically trained
on disparate data and domains. Nevertheless, the
models as well as the pipeline have been optimized
in several ways to achieve tasks such as high qual-
ity offline speech translation (Cohen, 2007; Kings-
bury et al., 2011; Federico et al., 2011), on-demand
web based speech and text translation, low-latency
real-time translation (Wahlster, 2000; Hamon et al.,
2009; Bangalore et al., 2012), etc. The design of a
S2S translation system is highly dependent on the
nature of the audio stimuli. For example, talks, lec-
tures and audio broadcasts are typically long and re-
quire appropriate segmentation strategies to chunk
the input signal to ensure high quality translation.
In contrast, single utterance translation in several
consumer applications (apps) are typically short and
can be processed without the need for additional
chunking. Another key parameter in designing a
S2S translation system for any task is latency. In
offline scenarios where high latencies are permit-
ted, several adaptation strategies (speaker, language
model, translation model), denser data structures (N-
best lists, word sausages, lattices) and rescoring pro-
cedures can be utilized to improve the quality of
end-to-end translation. On the other hand, real-
time speech-to-text or speech-to-speech translation
demand the best possible accuracy at low latencies
such that communication is not hindered due to po-
tential delay in processing.
In this work, we focus on the speech translation
of talks. We investigate the tradeoff between accu-
racy and latency for both offline and real-time trans-
lation of talks. In both these scenarios, appropriate
segmentation of the audio signal as well as the ASR
hypothesis that is fed into machine translation is crit-
ical for maximizing the overall translation quality of
the talk. Ideally, one would like to train the models
on entire talks. However, such corpora are not avail-
able in large amounts. Hence, it is necessary to con-
</bodyText>
<page confidence="0.951253">
230
</page>
<note confidence="0.471718">
Proceedings of NAACL-HLT 2013, pages 230–238,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999399363636364">
form to appropriately sized segments that are similar
to the sentence units used in training the language
and translation models. We propose several non-
linguistic and linguistic segmentation strategies for
the segmentation of text (reference or ASR hypothe-
ses) for machine translation. We address the prob-
lem of latency in real-time translation as a function
of the segmentation strategy; i.e., we ask the ques-
tion “what is the segmentation strategy that maxi-
mizes the number of segments while still maximiz-
ing translation accuracy?”.
</bodyText>
<sectionHeader confidence="0.999779" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999817121212121">
Speech translation of European Parliamentary
speeches has been addressed as part of the TC-
STAR project (Vilar et al., 2005; F¨ugen et al., 2006).
The project focused primarily on offline translation
of speeches. Simultaneous translation of lectures
and speeches has been addressed in (Hamon et al.,
2009; F¨ugen et al., 2007). However, the work fo-
cused on a single speaker in a limited domain. Of-
fline speech translation of TED1 talks has been ad-
dressed through the IWSLT 2011 and 2012 evalua-
tion tracks. The talks are from a variety of speakers
with varying dialects and cover a range of topics.
The study presented in this work is the first effort on
real-time speech translation of TED talks. In com-
parison with previous work, we also present a sys-
tematic study of the accuracy versus latency tradeoff
for both offline and real-time translation on the same
dataset.
Various utterance segmentation strategies for of-
fline machine translation of text and ASR output
have been presented in (Cettolo and Federico, 2006;
Rao et al., 2007; Matusov et al., 2007). The work
in (F¨ugen et al., 2007; F¨ugen and Kolss, 2007)
also examines the impact of segmentation on of-
fline speech translation of talks. However, the real-
time analysis in that work is presented only for
speech recognition. In contrast with previous work,
we tackle the latency issue in simultaneous transla-
tion of talks as a function of segmentation strategy
and present some new linguistic and non-linguistic
methodologies. We investigate the accuracy versus
latency tradeoff across translation of reference text,
utterance segmented speech recognition output and
</bodyText>
<footnote confidence="0.984533">
1http://www.ted.com
</footnote>
<bodyText confidence="0.556159">
partial speech recognition hypotheses.
</bodyText>
<sectionHeader confidence="0.964054" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.9974945">
The basic problem of text translation can be formu-
lated as follows. Given a source (French) sentence
f = fJ1 = f1, · · · , fJ, we aim to translate it into
target (English) sentence e� = ˆeI1 = ˆe1, · · · , ˆeI.
</bodyText>
<equation confidence="0.9911955">
e(f) = arg max Pr(e|f) (1)
e
</equation>
<bodyText confidence="0.98056">
If, as in talks, the source text (reference or ASR hy-
pothesis) is very long, i.e., J is large, we attempt
to break down the source string into shorter se-
quences, S = s1 · · · sk · · · sQs, where each sequence
sk = [fjkfjk+1 ··· fj(k+1)−1], j1 = 1,jQs+1 =
J + 1. Let the translation of each foreign sequence
sk be denoted by tk = [eikeik+1 · · · ei(k+1)−1], i1 =
1, iQs+1 = I&apos; + 12. The segmented sequences can
be translated using a variety of techniques such as
independent chunk-wise translation or chunk-wise
translation conditioned on history as shown in Eqs. 2
and 3, respectively. In Eq. 3, t∗i denotes the best
translation for source sequence si.
</bodyText>
<equation confidence="0.8553345">
ˆe(f) = argmax Pr(t1|s1) · · · arg max Pr(tk|sk)
t1 tk
ˆe(f) = argmax Pr(t1|s1) arg max Pr(t2|s2, s1, t∗1)
t1 t2
··· arg max Pr(tk|s1, ··· , sk, t∗1, ··· , t∗k−1)
tk
</equation>
<bodyText confidence="0.99988175">
Typically, the hypothesis eˆˆ will be more accurate
than eˆ for long texts as the models approximating
Pr(e|f) are conventionally trained on short text seg-
ments. In Eqs. 2 and 3, the number of sequences Qs
is inversely proportional to the time it takes to gen-
erate partial target hypotheses. Our main focus in
this work is to obtain a segmentation S such that the
quality of translation is maximized with minimal la-
tency. The above formulation for automatic speech
recognition is very similar except that the foreign
string�f = ˇfJ1 = ˇf1, · · · , fˇJ� is obtained by decoding
the input speech signal.
</bodyText>
<footnote confidence="0.9444425">
2The segmented and unsegmented talk may not be equal in
length, i.e., 154 I
</footnote>
<page confidence="0.991147">
231
</page>
<table confidence="0.999752375">
Model Language Vocabulary #words #sents Corpora
ASR Acoustic Model en 46899 2611144 148460 1119 TED talks
Language Model en 378915 3398460155 151923101 Europarl, WMT11 Gigaword, WMT11 News crawl
WMT11 News-commentary, WMT11 UN, IWSLT11 TED training
MT Parallel text en 503765 76886659 7464857 IWSLT11 TED training talks, Europarl, JRC-ACQUIS
Opensubtitles, Web data
es 519354 83717810 7464857
Language Model es 519354 83717810 7464857 Spanish side of parallel text
</table>
<tableCaption confidence="0.999861">
Table 1: Statistics of the data used for training the speech translation models.
</tableCaption>
<sectionHeader confidence="0.991087" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999987">
In this work, we focus on the speech translation
of TED talks, a compendium of public talks from
several speakers covering a variety of topics. Over
the past couple of years, the International Work-
shop on Spoken Language Translation (IWSLT) has
been conducting the evaluation of speech translation
on TED talks for English-French. We leverage the
IWSLT TED campaign by using identical develop-
ment (dev2010) and test data (tst2010). However,
English-Spanish is our target language pair as our
internal projects are cater mostly to this pair. As a
result, we created parallel text for English-Spanish
based on the reference English segments released as
part of the evaluation (Cettolo et al., 2012).
We also harvested the audio data from the TED
website for building an acoustic model. A total
of 1308 talks in English were downloaded, out of
which we used 1119 talks recorded prior to Decem-
ber 2011. We split the stereo audio file and dupli-
cated the data to account for any variations in the
channels. The data for the language models was also
restricted to that permitted in the IWSLT 2011 eval-
uation. The parallel text for building the English-
Spanish translation model was obtained from sev-
eral corpora: Europarl (Koehn, 2005), JRC-Acquis
corpus (Steinberger et al., 2006), Opensubtitle cor-
pus (Tiedemann and Lars Nygaard, 2004), Web
crawling (Rangarajan Sridhar et al., 2011) as well as
human translation of proprietary data. Table 1 sum-
marizes the data used in building the models. It is
important to note that the IWSLT evaluation on TED
talks is completely offline. In this work, we perform
the first investigation into the real-time translation of
these talks.
</bodyText>
<sectionHeader confidence="0.97332" genericHeader="method">
5 Speech Translation Models
</sectionHeader>
<bodyText confidence="0.9997095">
In this section, we describe the acoustic, language
and translation models used in our experiments.
</bodyText>
<subsectionHeader confidence="0.980733">
5.1 Acoustic and Language Model
</subsectionHeader>
<bodyText confidence="0.99998571875">
We use the AT&amp;T WATSONSM speech recog-
nizer (Goffin et al., 2004). The speech recogni-
tion component consisted of a three-pass decoding
approach utilizing two acoustic models. The mod-
els used three-state left-to-right HMMs representing
just over 100 phonemes. The phonemes represented
general English, spelled letters and head-body-tail
representation for the eleven digits (with ”zero” and
”oh”). The pronunciation dictionary used the appro-
priate phoneme subset, depending on the type of the
word. The models had 10.5k states and 27k HMMs,
trained on just over 300k utterances, using both of
the stereo channels. The baseline model training was
initialized with several iterations of ML training, in-
cluding two builds of context dependency trees, fol-
lowed by three iterations of Minimum Phone Error
(MPE) training.
The Vocal Tract Length Normalization (VTLN)
was applied in two different ways. One was esti-
mated on an utterance level, and the other at the talk
level. No speaker clustering was attempted in train-
ing. The performance at test time was comparable
for both approaches on the development set. Once
the warps were estimated, after five iterations, the
ML trained model was updated using MPE training.
Constrained model adaptation (CMA) was applied
to the warped features and the adapted features were
recognized in the final pass with the VTLN model.
All the passes used the same LM. For offline recog-
nition the warps, and the CMA adaptation, are per-
formed at the talk level. For the real-time speech
translation experiments, we used the VTLN model.
</bodyText>
<page confidence="0.965747">
232
</page>
<bodyText confidence="0.99996288">
The English language model was built using the
permissible data in the IWSLT 2011 evaluation. The
texts were normalized using a variety of cleanup,
number and spelling normalization techniques and
filtered by restricting the vocabulary to the top
375000 types; i.e., any sentence containing a to-
ken outside the vocabulary was discarded. First, we
removed extraneous characters beyond the ASCII
range followed by removal of punctuations. Sub-
sequently, we normalized hyphenated words and re-
moved words with more than 25 characters. The re-
sultant text was normalized using a variety of num-
ber conversion routines and each corpus was fil-
tered by restricting the vocabulary to the top 150000
types; i.e., any sentence containing a token outside
the vocabulary was discarded. The vocabulary from
all the corpora was then consolidated and another
round of filtering to the top 375000 most frequent
types was performed. The OOV rate on the TED
dev2010 set is 1.1%. We used the AT&amp;T FSM
toolkit (Mohri et al., 1997) to train a trigram lan-
guage model (LM) for each component (corpus). Fi-
nally, the component language models were interpo-
lated by minimizing the perplexity on the dev2010
set. The results are shown in Table 2.
</bodyText>
<table confidence="0.9969966">
Accuracy (%)
Model dev2010 test2010
Baseline MPE 75.5 73.8
VTLN 78.8 77.4
CMA 80.5 80.0
</table>
<tableCaption confidence="0.9775955">
Table 2: ASR word accuracies on the IWSLT data
sets.3
</tableCaption>
<subsectionHeader confidence="0.985986">
5.2 Translation Model
</subsectionHeader>
<bodyText confidence="0.99993">
We used the Moses toolkit (Koehn et al., 2007) for
performing statistical machine translation. Mini-
mum error rate training (MERT) was performed on
the development set (dev2010) to optimize the fea-
ture weights of the log-linear model used in trans-
lation. During decoding, the unknown words were
preserved in the hypotheses. The data used to train
the model is summarized in Table 1.
</bodyText>
<footnote confidence="0.93829">
3We used the standard NIST scoring package as we did not
have access to the IWSLT evaluation server that may normalize
and score differently
</footnote>
<bodyText confidence="0.99993592">
We also used a finite-state implementation of
translation without reordering. Reordering can pose
a challenge in real-time S2S translation as the text-
to-speech synthesis is monotonic and cannot retract
already synthesized speech. While we do not ad-
dress the text-to-speech synthesis of target text in
this work, we perform this analysis as a precursor
to future work. We represent the phrase transla-
tion table as a weighted finite state transducer (FST)
and the language model as a finite state acceptor
(FSA). The weight on the arcs of the FST is the
dot product of the MERT weights with the transla-
tion scores. In addition, a word insertion penalty
was also applied to each word to penalize short hy-
potheses. The decoding process consists of compos-
ing all possible segmentations of an input sentence
with the phrase table FST and language model, fol-
lowed by searching for the best path. Our FST-based
translation is the equivalent of phrase-based transla-
tion in Moses without reordering. We present re-
sults using the independent chunk-wise strategy and
chunk-wise translation conditioned on history in Ta-
ble 3. The chunk-wise translation conditioned on
history was performed using the continue-partial-
translation option in Moses.
</bodyText>
<sectionHeader confidence="0.977976" genericHeader="method">
6 Segmentation Strategies
</sectionHeader>
<bodyText confidence="0.99997355">
The output of ASR for talks is a long string of
words with no punctuation, capitalization or seg-
mentation markers. In most offline ASR systems,
the talk is first segmented into short utterance-like
audio segments before passing them to the decoder.
Prior work has shown that additional segmentation
of ASR hypotheses of these segments may be nec-
essary to improve translation quality (Rao et al.,
2007; Matusov et al., 2007). In a simultaneous
speech translation system, one can neither find the
optimal segmentation of the entire talk nor tolerate
high latencies associated with long segments. Con-
sequently, it is necessary to decode the incoming au-
dio incrementally as well as segment the ASR hy-
potheses appropriately to maximize MT quality. We
present a variety of linguistic and non-linguistic seg-
mentation strategies for segmenting the source text
input into MT. In our experiments, they are applied
to different inputs including reference text, ASR 1-
best hypothesis for manually segmented audio and
</bodyText>
<page confidence="0.992283">
233
</page>
<bodyText confidence="0.95881">
incremental ASR hypotheses from entire talks.
</bodyText>
<subsectionHeader confidence="0.929843">
6.1 Non-linguistic segmentation
</subsectionHeader>
<bodyText confidence="0.999991263157895">
The simplest method is to segment the incoming text
according to length in number of words. Such a pro-
cedure can destroy semantic context but has little to
no overhead in additional processing. We experi-
ment with segmenting the text according to word
window sizes of length 4, 8, 11, and 15 (denoted
as data sets win4, win8, win11, win15, respectively
in Table 3). We also experiment with concatenating
all of the text from one TED talk into a single chunk
(complete talk).
A novel hold-output model was also developed in
order to segment the input text. Given a pair of par-
allel sentences, the model segments the source sen-
tence into minimally sized chunks such that crossing
links and links of one target word to many source
words in an optimal GIZA++ alignment (Och and
Ney, 2003) occur only within individual chunks.
The motivation behind this model is that if a segment
s0 is input at time t0 to an incremental MT system,
it can be translated right away without waiting for a
segment si that is input at a later time ti7 ti &gt; 0. The
hold-output model detects these kinds of segments
given a sequence of English words that are input
from left to right. A kernel-based SVM was used to
develop this model. It tags a token t in the input with
either the label HOLD, meaning to chunk it with the
next token, or the label OUTPUT, meaning to output
the chunk constructed from the maximal consecutive
sequence of tokens preceding t that were all tagged
as HOLD. The model considers a five word and POS
window around the target token t. Unigram, bigram,
and trigram word and POS features based upon this
window are used for classification. Training and de-
velopment data for the model was derived from the
English-Spanish TED data (see Table 1) after run-
ning it through GIZA++. Accuracy of the model on
the development set was 66.62% F-measure for the
HOLD label and 82.75% for the OUTPUT label.
</bodyText>
<subsectionHeader confidence="0.99864">
6.2 Linguistic segmentation
</subsectionHeader>
<bodyText confidence="0.999989886792453">
Since MT models are trained on parallel text sen-
tences, we investigate segmenting the source text
into sentences. We also investigate segmenting the
text further by predicting comma separated chunks
within sentences. These tasks are performed by
training a kernel-based SVM (Haffner et al., 2003)
on a subset of English TED data. This dataset con-
tained 1029 human-transcribed talks consisting of
about 103,000 sentences containing about 1.6 mil-
lion words. Punctuation in this dataset was normal-
ized as follows. Different kinds of sentence ending
punctuations were transformed into a uniform end of
sentence marker. Double-hyphens were transformed
into commas. Commas already existing in the input
were kept while all other kinds of punctuation sym-
bols were deleted. A part of speech (POS) tagger
was applied to this input. For speed, a unigram POS
tagger was implemented which was trained on the
Penn Treebank (Marcus et al., 1993) and used or-
thographic features to predict the POS of unknown
words. The SVM-based punctuation classifier relies
on a five word and POS window in order to classify
the target word. Specifically, token t0 is classified
given as input the window t_2t_1tot1t2. Unigram,
bigram, and trigram word and POS features based on
this window were used for classification. Accuracy
of the classifier on the development set was 60.51%
F-measure for sentence end detection and 43.43%
F-measure for comma detection. Subsequently, data
sets pred-sent (sentences) and pred-punct (comma-
separated chunks) were obtained. Corresponding to
these, two other data sets ref-sent and ref-punct were
obtained based upon gold-standard punctuations in
the reference.
Besides investigating the use of comma-separated
segments, we investigated other linguistically moti-
vated segments. These included conjunction-word
based segments. These segments are separated at
either conjunction (e.g. “and,” “or”) or sentence-
ending word boundaries. Conjunctions were iden-
tified using the unigram POS tagger. F-measure
performance for detecting conjunctions by the tag-
ger on the development set was quite high, 99.35%.
As an alternative, text chunking was performed
within each sentence, with each chunk correspond-
ing to one segment. Text chunks are non-recursive
syntactic phrases in the input text. We investi-
gated segmenting the source into text chunks us-
ing TreeTagger, a decision-tree based text chun-
ker (Schmid, 1994). Initial sets of text chunks
were created by using either gold-standard sentence
boundaries or boundaries detected using the punc-
tuation classifier, yielding the data sets chunk-ref-
</bodyText>
<page confidence="0.996068">
234
</page>
<table confidence="0.999965285714286">
Segmentation Segmentation Reference text ASR 1-best
type strategy
BLEU Mean BLEU Mean
#words #words
per segment per segment
Independent chunk-wise chunk-wise Independent chunk-wise chunk-wise
with history with history
FST Moses FST Moses
win4 22.6 21.0 25.5 3.9±0.1 17.7 17.1 20.0 3.9±0.1
win8 26.6 26.2 28.2 7.9±0.3 20.6 20.9 22.3 7.9±0.2
Non-linguistic win11 27.2 27.4 29.2 10.9 ± 0.3 21.5 21.8 23.1 10.9±0.4
win15 28.5 28.5 29.4 14.9±0.6 22.3 22.8 23.3 14.9±0.7
ref-hold 13.3 14.0 17.1 1.6±1.9 12.7 13.1 17.5 1.5±1.0
pred-hold 15.9 15.7 16.3 2.2±1.9 12.6 12.9 17.4 1.5±1.0
complete talk 23.8 23.9 – 2504 18.8 19.2 – 2515
ref-sent 30.6 31.5 30.5 16.7±11.8 24.3 25.1 24.4 17.0±11.6
ref-punct 30.4 31.5 30.3 7.1±5.3 24.2 25.1 24.1 8.7±6.1
pred-punct 30.6 31.5 30.4 8.7±8.8 24.1 25.0 24.0 8.8±6.8
conj-ref-eos 30.5 31.5 30.2 11.2±7.5 24.1 24.9 24.0 11.5±7.7
conj-pred-eos 30.3 31.2 30.3 10.9±7.9 24.0 24.8 24.0 11.4±8.5
chunk-ref-punct 17.9 18.9 21.4 1.3±0.7 14.5 15.2 16.9 1.4±0.7
Linguistic lgchunk1-ref-punct 21.0 21.8 25.1 1.7±1.0 16.9 17.4 19.6 1.8±1.0
lgchunk2-ref-punct 22.4 23.1 26.0 2.1±1.1 17.9 18.4 20.4 2.1±1.1
lgchunk3-ref-punct 24.3 25.1 27.4 2.5±1.7 19.2 19.9 21.3 2.5±1.7
chunk-pred-punct 17.9 18.9 21.4 1.3±0.7 14.5 15.1 16.9 1.4±0.7
lgchunk1-pred-punct 21.2 21.9 25.2 1.8±1.0 16.7 17.2 19.7 1.8±1.0
lgchunk2-pred-punct 22.6 23.1 26.0 2.1±1.2 17.7 18.3 20.5 2.1±1.2
lgchunk3-pred-punct 24.5 25.3 27.4 2.6±1.8 19.1 20.0 21.3 2.5±1.7
</table>
<tableCaption confidence="0.998075">
Table 3: BLEU scores at the talk level for reference text and ASR 1-best for various segmentation strategies.
</tableCaption>
<bodyText confidence="0.983371548387097">
The ASR 1-best was performed on manually segmented audio chunks provided in tst2010 set.
punct and chunk -pred -punct. Chunk types included
NC (noun chunk), VC (verb chunk), PRT (particle),
and ADVC (adverbial chunk).
Because these chunks may not provide sufficient
context for translation, we also experimented with
concatenating neighboring chunks of certain types
to form larger chunks. Data sets lgchunk1 concate-
nate together neighboring chunk sequences of the
form NC, VC or NC, ADVC, VC, intended to cap-
ture as single chunks instances of subject and verb.
In addition to this, data sets lgchunk2 capture chunks
such as PC (prepositional phrase) and VC followed
by VC (control and raising verbs). Finally, data sets
lgchunk3 capture as single chunks VC followed by
NC and optionally followed by PRT (verb and its di-
rect object).
Applying the conjunction segmenter after the
aforementioned punctuation classifier in order to de-
tect the ends of sentences yields the data set conj-
pred-eos. Applying it on sentences derived from the
gold-standard punctuations yields the data set conj-
ref-eos. Finally, applying the hold-output model to
sentences derived using the punctuation classifier
produces the data set pred-hold. Obtaining English
sentences tagged with HOLD and OUTPUT directly
from the output of GIZA++ on English-Spanish sen-
tences in the reference produces the data set ref-hold.
The strategies containing the keyword ref for ASR
simply means that the ASR hypotheses are used in
place of the gold reference text.
</bodyText>
<figure confidence="0.991975666666667">
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0
0 1000 2000 3000 4000 5000 6000
ASR timeout (ms)
</figure>
<figureCaption confidence="0.9902215">
Figure 1: Latencies and BLEU scores for tst2010 set
using incremental ASR decoding and translation
</figureCaption>
<bodyText confidence="0.984998666666667">
We also performed real-time speech translation by
using incremental speech recognition, i.e., the de-
coder returns partial hypotheses that, independent of
</bodyText>
<figure confidence="0.995694769230769">
I
15.1
15.1 15.1 15.1 15.1
10.0 11.7 12.6 13.3
14.6 14.7 14.7 14.8
13.7
14.0 14.1 14.3
ASR+Punct Seg+MT (BLEU)
15.1
15.1 15.1 15.1
ASR+MT (BLEU)
15.1
15.1 15.1
</figure>
<page confidence="0.997103">
235
</page>
<bodyText confidence="0.9999692">
the pruning during search, will not change in the
future. Figure 1 shows the plot for two scenarios:
one in which the partial hypotheses are sent directly
to machine translation and another where the best
segmentation strategy pred-punct is used to segment
the partial output before sending it to MT. The plot
shows the BLEU scores as a function of ASR time-
outs used to generate the partial hypotheses. Fig-
ure 1 also shows the average latency involved in in-
cremental speech translation.
</bodyText>
<sectionHeader confidence="0.996995" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999904493975904">
The BLEU scores for the segmentation strategies
over ASR hypotheses was computed at the talk level.
Since the ASR hypotheses do not align with the
reference source text, it is not feasible to evalu-
ate the translation performance using the gold refer-
ence. While other studies have used an approximate
edit distance algorithm for resegmentation of the hy-
potheses (Matusov et al., 2005), we simply concate-
nate all the segments and perform the evaluation at
the talk level.
The hold segmentation strategy yields the poor-
est translation performance. The significant drop in
BLEU score can be attributed to relatively short seg-
ments (2-4 words) that was generated by the model.
The scheme oversegments the text and since the
translation and language models are trained on sen-
tence like chunks, the performance is poor. For ex-
ample, the input text the sea should be translated
as el mar, but instead the hold segmenter chunks it
as the·sea which MT’s chunk translation renders as
el·el mar. It will be interesting to increase the span
of the hold strategy to subsume more contiguous se-
quences and we plan to investigate this as part of
future work.
The chunk segmentation strategy yields quite poor
translation performance. In general, it does not
make the same kinds of errors that the hold strat-
egy makes; for example, the input text the sea will
be treated as one NC chunk by the chunk seg-
mentation strategy, leading MT to translate it cor-
rectly as el mar. The short chunk sizes of chunk
lead to other kinds of errors. For example, the in-
put text we use will be chunked into the NC we
and the VC use, which will be translated incor-
rectly as nosotros·usar; the infinitive usar is se-
lected rather than the properly conjugated form us-
amos. However, there is a marked improvement in
translation accuracy with increasingly larger chunk
sizes (lgchunk1, lgchunk2, and lgchunk3). Notably,
lgchunk3 yields performance that approaches that of
win8 with a chunk size that is one third of win8’s.
The conj-pred-eos and pred-punct strategies work
the best, and it can be seen that the average seg-
ment length (8-12 words) generated in both these
schemes is very similar to that used for training the
models. It is also about the average latency (4-5
seconds) that can be tolerated in cross-lingual com-
munication, also known as ear-voice span (Lederer,
1978). The non-linguistic segmentation using fixed
word length windows also performs well, especially
for the longer length windows. However, longer
windows (win15) increase the latency and any fixed
length window typically destroys the semantic con-
text. It can also be seen from Table 3 that translat-
ing the complete talk is suboptimal in comparison
with segmenting the text. This is primarily due to
bias on sentence length distributions in the training
data. Training models on complete talks is likely to
resolve this issue. Contrasting the use of reference
segments as input to MT (ref-sent, ref-punct, conj-
ref-eos) versus the use of predicted segments (pred-
sent, pred-punct, conj-pred-eos, respectively), it is
interesting to note that the MT accuracies never dif-
fered greatly between the two, despite the noise in
the set of predicted segments.
The performance of the real-time speech transla-
tion of TED talks is much lower than the offline sce-
nario. First, we use only a VTLN model as perform-
ing CMA adaptation in a real-time scenario typically
increases latency. Second, the ASR language model
is trained on sentence-like units and decoding the en-
tire talk with this LM is not optimal. A language
model trained on complete talks will be more appro-
priate for such a framework and we are investigating
this as part of current work.
Comparing the accuracies of different speech
translation strategies, Table 3 shows that pred-punct
performs the best. When embedded in an incremen-
tal MT speech recognition system, Figure 1 shows
that it is more accurate than the system that sends
partial ASR hypotheses directly to MT. This advan-
tage decreases, however, when the ASR timeout pa-
rameter is increased to more than five or six sec-
</bodyText>
<page confidence="0.992696">
236
</page>
<bodyText confidence="0.99994175">
onds. In terms of latency, Figure 1 shows that the
addition of the pred-punct segmenter into the incre-
mental system introduces a significant delay. About
one third of the increase in delay can be attributed
to merely maintaining the two word lookahead win-
dow that the segmenter’s classifier needs to make
decisions. This is significant because this kind of
window has been used quite frequently in previous
work on simultaneous translation (cf. (F¨ugen et al.,
2007)), and yet to our knowledge this penalty asso-
ciated with this configuration was never mentioned.
The remaining delay can be attributed to the long
chunk sizes that the segmenter produces. An inter-
esting aspect of the latency curve associated with the
segmenter in Figure 1 is that there are two peaks at
ASR timeouts of 2,500 and 4,500 ms, and that the
lowest latency is achieved at 3,000 ms rather than at
a smaller value. This may be attributed to the fact
that the system is a pipeline consisting of ASR, seg-
menter, and MT, and that 3,000 ms is roughly the
length of time to recite comma-separated chunks.
Consequently, the two latency peaks appear to cor-
respond with ASR producing segments that are most
divergent with segments that the segmenter pro-
duces, leading to the most pipeline “stalls.” Con-
versely, the lowest latency occurs when the timeout
is set so that ASR’s segments most resemble the seg-
menter’s output to MT.
</bodyText>
<sectionHeader confidence="0.997373" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999968903225806">
We investigated various approaches for incremen-
tal speech translation of TED talks, with the aim
of producing a system with high MT accuracy and
low latency. For acoustic modeling, we found that
VTLN and CMA adaptation were useful for increas-
ing the accuracy of ASR, leading to a word accuracy
of 80% on TED talks used in the IWSLT evalua-
tion track. In our offline MT experiments retention
of partial translations was found useful for increas-
ing MT accuracy, with the latter being slightly more
helpful. We experimented with several linguistic
and non-linguistic strategies for text segmentation
before translation. Our experiments indicate that a
novel segmentation into conjunction-separated sen-
tence chunks resulted in accuracies almost as high
and latencies almost as short as comma-separated
sentence chunks. They also indicated that signifi-
cant noise in the detection of sentences and punc-
tuation did not seriously impact the resulting MT
accuracy. Experiments on real-time simultaneous
speech translation using partial recognition hypothe-
ses demonstrate that introduction of a segmenter in-
creases MT accuracy. They also showed that in or-
der to reduce latency it is important for buffers in dif-
ferent pipeline components to be synchronized so as
to minimize pipeline stalls. As part of future work,
we plan to extend the framework presented in this
work for performing speech-to-speech translation.
We also plan to address the challenges involved in
S2S translation across languages with very different
word order.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999985">
We would like to thank Simon Byers for his help
with organizing the TED talks data.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999915666666667">
S. Bangalore, V. K. Rangarajan Sridhar, P. Kolan,
L. Golipour, and A. Jimenez. 2012. Real-time in-
cremental speech-to-speech translation of dialogs. In
Proceedings of NAACL:HLT, June.
M. Cettolo and M. Federico. 2006. Text segmentation
criteria for statistical machine translation. In Proceed-
ings of the 5th international conference on Advances
in Natural Language Processing.
M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3:
Web Inventory of Transcribed and Translated Talks. In
Proceedings of EAMT.
J. Cohen. 2007. The GALE project: A description and
an update. In Proceedings of ASRU Workshop.
M. Federico, L. Bentivogli, M. Paul, and S. St¨uker. 2011.
Overview of the IWSLT 2011 evaluation campaign. In
Proceedings of IWSLT.
C. F¨ugen and M. Kolss. 2007. The influence of utterance
chunking on machine translation performance. In Pro-
ceedings of Interspeech.
C. F¨ugen, M. Kolss, D. Bernreuther, M. Paulik, S. Stuker,
S. Vogel, and A. Waibel. 2006. Open domain speech
recognition &amp; translation: Lectures and speeches. In
Proceedings of ICASSP.
C. F¨ugen, A. Waibel, and M. Kolss. 2007. Simultaneous
translation of lectures and speeches. Machine Trans-
lation, 21:209–252.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-T¨ur,
A. Ljolje, and S. Parthasarathy. 2004. The AT&amp;T
Watson Speech Recognizer. Technical report, Septem-
ber.
</reference>
<page confidence="0.961275">
237
</page>
<reference confidence="0.999707524590164">
P. Haffner, G. T¨ur, and J. Wright. 2003. Optimizing
svms for complex call classification. In Proceedings
of ICASSP’03.
O. Hamon, C. F¨ugen, D. Mostefa, V. Arranz, M. Kolss,
A. Waibel, and K. Choukri. 2009. End-to-end evalua-
tion in simultaneous translation. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), March.
B. Kingsbury, H. Soltau, G. Saon, S. Chu, Hong-Kwang
Kuo, L. Mangu, S. Ravuri, N. Morgan, and A. Janin.
2011. The IBM 2009 GALE Arabic speech translation
system. In Proceedings of ICASSP.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, Shen W.,
C. Moran, R. Zens, C. J. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings ofACL.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit.
M. Lederer. 1978. Simultaneous interpretation: units of
meaning and other features. In D. Gerver and H. W.
Sinaiko, editors, Language interpretation and commu-
nication, pages 323–332. Plenum Press, New York.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn treebank. Computational Linguistics,
19(2):313–330.
E. Matusov, G. Leusch, O. Bender, and H. Ney. 2005.
Evaluating machine translation output with automatic
sentence segmentation. In Proceedings of IWSLT.
E. Matusov, D. Hillard, M. Magimai-Doss, D. Hakkani-
T¨ur, M. Ostendorf, and H. Ney. 2007. Improving
speech translation with automatic boundary predic-
tion. In Proceedings of Interspeech.
M. Mohri, F. Pereira, and M. Riley. 1997. At&amp;t
general-purpose finite-state machine software tools,
http://www.research.att.com/sw/tools/fsm/.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
V. K. Rangarajan Sridhar, L. Barbosa, and S. Bangalore.
2011. A scalable approach to building a parallel cor-
pus from the Web. In Proceedings of Interspeech.
S. Rao, I. Lane, and T. Schultz. 2007. Optimizing sen-
tence segmentation for spoken language translation. In
Proceedings of Interspeech.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on New Methods in Language Pro-
cessing.
R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-
javec, and D. Tufis. 2006. The JRC-Acquis: A multi-
lingual aligned parallel corpus with 20+ languages. In
Proceedings of LREC.
J. Tiedemann and L. Lars Nygaard. 2004. The OPUS
corpus - parallel &amp; free. In Proceedings of LREC.
D. Vilar, E. Matusov, S. Hasan, R. Zens, and H. Ney.
2005. Statistical machine translation of European par-
liamentary speeches. In Proceedings of MT Summit.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
</reference>
<page confidence="0.997065">
238
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.557245">
<title confidence="0.999807">Segmentation Strategies for Streaming Speech Translation</title>
<author confidence="0.849346">Vivek Kumar Rangarajan Sridhar</author>
<author confidence="0.849346">John Chen</author>
<author confidence="0.849346">Srinivas Andrej Ljolje</author>
<author confidence="0.849346">Rathinavelu</author>
<affiliation confidence="0.971932">AT&amp;T Labs -</affiliation>
<address confidence="0.998798">180 Park Avenue, Florham Park, NJ</address>
<email confidence="0.999135">vkumar,jchen,srini,alj,rathi@research.att.com</email>
<abstract confidence="0.993274814814815">The study presented in this work is a first effort at real-time speech translation of TED talks, a compendium of public talks with different speakers addressing a variety of topics. We address the goal of achieving a system that balances translation accuracy and latency. In order to improve ASR performance for our diverse data set, adaptation techniques such as constrained model adaptation and vocal tract length normalization are found to be useful. In order to improve machine translation (MT) performance, techniques that could be employed in real-time such as monotonic and partial translation retention are found to be of use. We also experiment with inserting text segmenters of various types between ASR and MT in a series of real-time translation experiments. Among other results, our experiments demonstrate that a good segmentation is useful, and a novel conjunction-based segmentation strategy improves translation quality nearly as much as other strategies such as comma-based segmentation. It was also found to be important to synchronize various pipeline components in order to minimize latency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>V K Rangarajan Sridhar</author>
<author>P Kolan</author>
<author>L Golipour</author>
<author>A Jimenez</author>
</authors>
<title>Real-time incremental speech-to-speech translation of dialogs.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL:HLT,</booktitle>
<contexts>
<context position="2240" citStr="Bangalore et al., 2012" startWordPosition="341" endWordPosition="344"> of consumers. Under the hood, the individual components such as automatic speech recognition (ASR), machine translation (MT) and text-tospeech synthesis (TTS) that constitute a S2S system are still loosely coupled and typically trained on disparate data and domains. Nevertheless, the models as well as the pipeline have been optimized in several ways to achieve tasks such as high quality offline speech translation (Cohen, 2007; Kingsbury et al., 2011; Federico et al., 2011), on-demand web based speech and text translation, low-latency real-time translation (Wahlster, 2000; Hamon et al., 2009; Bangalore et al., 2012), etc. The design of a S2S translation system is highly dependent on the nature of the audio stimuli. For example, talks, lectures and audio broadcasts are typically long and require appropriate segmentation strategies to chunk the input signal to ensure high quality translation. In contrast, single utterance translation in several consumer applications (apps) are typically short and can be processed without the need for additional chunking. Another key parameter in designing a S2S translation system for any task is latency. In offline scenarios where high latencies are permitted, several adap</context>
</contexts>
<marker>Bangalore, Sridhar, Kolan, Golipour, Jimenez, 2012</marker>
<rawString>S. Bangalore, V. K. Rangarajan Sridhar, P. Kolan, L. Golipour, and A. Jimenez. 2012. Real-time incremental speech-to-speech translation of dialogs. In Proceedings of NAACL:HLT, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cettolo</author>
<author>M Federico</author>
</authors>
<title>Text segmentation criteria for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th international conference on Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="5486" citStr="Cettolo and Federico, 2006" startWordPosition="856" endWordPosition="859">in. Offline speech translation of TED1 talks has been addressed through the IWSLT 2011 and 2012 evaluation tracks. The talks are from a variety of speakers with varying dialects and cover a range of topics. The study presented in this work is the first effort on real-time speech translation of TED talks. In comparison with previous work, we also present a systematic study of the accuracy versus latency tradeoff for both offline and real-time translation on the same dataset. Various utterance segmentation strategies for offline machine translation of text and ASR output have been presented in (Cettolo and Federico, 2006; Rao et al., 2007; Matusov et al., 2007). The work in (F¨ugen et al., 2007; F¨ugen and Kolss, 2007) also examines the impact of segmentation on offline speech translation of talks. However, the realtime analysis in that work is presented only for speech recognition. In contrast with previous work, we tackle the latency issue in simultaneous translation of talks as a function of segmentation strategy and present some new linguistic and non-linguistic methodologies. We investigate the accuracy versus latency tradeoff across translation of reference text, utterance segmented speech recognition o</context>
</contexts>
<marker>Cettolo, Federico, 2006</marker>
<rawString>M. Cettolo and M. Federico. 2006. Text segmentation criteria for statistical machine translation. In Proceedings of the 5th international conference on Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cettolo</author>
<author>C Girardi</author>
<author>M Federico</author>
</authors>
<title>WIT3: Web Inventory of Transcribed and Translated Talks.</title>
<date>2012</date>
<booktitle>In Proceedings of EAMT.</booktitle>
<contexts>
<context position="9167" citStr="Cettolo et al., 2012" startWordPosition="1484" endWordPosition="1487"> of public talks from several speakers covering a variety of topics. Over the past couple of years, the International Workshop on Spoken Language Translation (IWSLT) has been conducting the evaluation of speech translation on TED talks for English-French. We leverage the IWSLT TED campaign by using identical development (dev2010) and test data (tst2010). However, English-Spanish is our target language pair as our internal projects are cater mostly to this pair. As a result, we created parallel text for English-Spanish based on the reference English segments released as part of the evaluation (Cettolo et al., 2012). We also harvested the audio data from the TED website for building an acoustic model. A total of 1308 talks in English were downloaded, out of which we used 1119 talks recorded prior to December 2011. We split the stereo audio file and duplicated the data to account for any variations in the channels. The data for the language models was also restricted to that permitted in the IWSLT 2011 evaluation. The parallel text for building the EnglishSpanish translation model was obtained from several corpora: Europarl (Koehn, 2005), JRC-Acquis corpus (Steinberger et al., 2006), Opensubtitle corpus (</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks. In Proceedings of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>The GALE project: A description and an update.</title>
<date>2007</date>
<booktitle>In Proceedings of ASRU Workshop.</booktitle>
<contexts>
<context position="2047" citStr="Cohen, 2007" startWordPosition="314" endWordPosition="315">-to-text and speechto-speech (S2S) translation has improved so significantly over the last several decades that such systems are now widely deployed and used by an increasing number of consumers. Under the hood, the individual components such as automatic speech recognition (ASR), machine translation (MT) and text-tospeech synthesis (TTS) that constitute a S2S system are still loosely coupled and typically trained on disparate data and domains. Nevertheless, the models as well as the pipeline have been optimized in several ways to achieve tasks such as high quality offline speech translation (Cohen, 2007; Kingsbury et al., 2011; Federico et al., 2011), on-demand web based speech and text translation, low-latency real-time translation (Wahlster, 2000; Hamon et al., 2009; Bangalore et al., 2012), etc. The design of a S2S translation system is highly dependent on the nature of the audio stimuli. For example, talks, lectures and audio broadcasts are typically long and require appropriate segmentation strategies to chunk the input signal to ensure high quality translation. In contrast, single utterance translation in several consumer applications (apps) are typically short and can be processed wit</context>
</contexts>
<marker>Cohen, 2007</marker>
<rawString>J. Cohen. 2007. The GALE project: A description and an update. In Proceedings of ASRU Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Federico</author>
<author>L Bentivogli</author>
<author>M Paul</author>
<author>S St¨uker</author>
</authors>
<title>evaluation campaign.</title>
<date>2011</date>
<journal>Overview of the IWSLT</journal>
<booktitle>In Proceedings of IWSLT.</booktitle>
<marker>Federico, Bentivogli, Paul, St¨uker, 2011</marker>
<rawString>M. Federico, L. Bentivogli, M. Paul, and S. St¨uker. 2011. Overview of the IWSLT 2011 evaluation campaign. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F¨ugen</author>
<author>M Kolss</author>
</authors>
<title>The influence of utterance chunking on machine translation performance.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<marker>F¨ugen, Kolss, 2007</marker>
<rawString>C. F¨ugen and M. Kolss. 2007. The influence of utterance chunking on machine translation performance. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F¨ugen</author>
<author>M Kolss</author>
<author>D Bernreuther</author>
<author>M Paulik</author>
<author>S Stuker</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Open domain speech recognition &amp; translation: Lectures and speeches.</title>
<date>2006</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<marker>F¨ugen, Kolss, Bernreuther, Paulik, Stuker, Vogel, Waibel, 2006</marker>
<rawString>C. F¨ugen, M. Kolss, D. Bernreuther, M. Paulik, S. Stuker, S. Vogel, and A. Waibel. 2006. Open domain speech recognition &amp; translation: Lectures and speeches. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F¨ugen</author>
<author>A Waibel</author>
<author>M Kolss</author>
</authors>
<title>Simultaneous translation of lectures and speeches.</title>
<date>2007</date>
<booktitle>Machine Translation,</booktitle>
<pages>21--209</pages>
<marker>F¨ugen, Waibel, Kolss, 2007</marker>
<rawString>C. F¨ugen, A. Waibel, and M. Kolss. 2007. Simultaneous translation of lectures and speeches. Machine Translation, 21:209–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Goffin</author>
<author>C Allauzen</author>
<author>E Bocchieri</author>
<author>D Hakkani-T¨ur</author>
<author>A Ljolje</author>
<author>S Parthasarathy</author>
</authors>
<title>The AT&amp;T Watson Speech Recognizer.</title>
<date>2004</date>
<tech>Technical report,</tech>
<marker>Goffin, Allauzen, Bocchieri, Hakkani-T¨ur, Ljolje, Parthasarathy, 2004</marker>
<rawString>V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-T¨ur, A. Ljolje, and S. Parthasarathy. 2004. The AT&amp;T Watson Speech Recognizer. Technical report, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Haffner</author>
<author>G T¨ur</author>
<author>J Wright</author>
</authors>
<title>Optimizing svms for complex call classification.</title>
<date>2003</date>
<booktitle>In Proceedings of ICASSP’03.</booktitle>
<marker>Haffner, T¨ur, Wright, 2003</marker>
<rawString>P. Haffner, G. T¨ur, and J. Wright. 2003. Optimizing svms for complex call classification. In Proceedings of ICASSP’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Hamon</author>
<author>C F¨ugen</author>
<author>D Mostefa</author>
<author>V Arranz</author>
<author>M Kolss</author>
<author>A Waibel</author>
<author>K Choukri</author>
</authors>
<title>End-to-end evaluation in simultaneous translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<marker>Hamon, F¨ugen, Mostefa, Arranz, Kolss, Waibel, Choukri, 2009</marker>
<rawString>O. Hamon, C. F¨ugen, D. Mostefa, V. Arranz, M. Kolss, A. Waibel, and K. Choukri. 2009. End-to-end evaluation in simultaneous translation. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kingsbury</author>
<author>H Soltau</author>
<author>G Saon</author>
<author>S Chu</author>
<author>Hong-Kwang Kuo</author>
<author>L Mangu</author>
<author>S Ravuri</author>
<author>N Morgan</author>
<author>A Janin</author>
</authors>
<title>GALE Arabic speech translation system.</title>
<date>2011</date>
<booktitle>The IBM</booktitle>
<contexts>
<context position="2071" citStr="Kingsbury et al., 2011" startWordPosition="316" endWordPosition="320">speechto-speech (S2S) translation has improved so significantly over the last several decades that such systems are now widely deployed and used by an increasing number of consumers. Under the hood, the individual components such as automatic speech recognition (ASR), machine translation (MT) and text-tospeech synthesis (TTS) that constitute a S2S system are still loosely coupled and typically trained on disparate data and domains. Nevertheless, the models as well as the pipeline have been optimized in several ways to achieve tasks such as high quality offline speech translation (Cohen, 2007; Kingsbury et al., 2011; Federico et al., 2011), on-demand web based speech and text translation, low-latency real-time translation (Wahlster, 2000; Hamon et al., 2009; Bangalore et al., 2012), etc. The design of a S2S translation system is highly dependent on the nature of the audio stimuli. For example, talks, lectures and audio broadcasts are typically long and require appropriate segmentation strategies to chunk the input signal to ensure high quality translation. In contrast, single utterance translation in several consumer applications (apps) are typically short and can be processed without the need for additi</context>
</contexts>
<marker>Kingsbury, Soltau, Saon, Chu, Kuo, Mangu, Ravuri, Morgan, Janin, 2011</marker>
<rawString>B. Kingsbury, H. Soltau, G. Saon, S. Chu, Hong-Kwang Kuo, L. Mangu, S. Ravuri, N. Morgan, and A. Janin. 2011. The IBM 2009 GALE Arabic speech translation system. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C J Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="13287" citStr="Koehn et al., 2007" startWordPosition="2158" endWordPosition="2161">was then consolidated and another round of filtering to the top 375000 most frequent types was performed. The OOV rate on the TED dev2010 set is 1.1%. We used the AT&amp;T FSM toolkit (Mohri et al., 1997) to train a trigram language model (LM) for each component (corpus). Finally, the component language models were interpolated by minimizing the perplexity on the dev2010 set. The results are shown in Table 2. Accuracy (%) Model dev2010 test2010 Baseline MPE 75.5 73.8 VTLN 78.8 77.4 CMA 80.5 80.0 Table 2: ASR word accuracies on the IWSLT data sets.3 5.2 Translation Model We used the Moses toolkit (Koehn et al., 2007) for performing statistical machine translation. Minimum error rate training (MERT) was performed on the development set (dev2010) to optimize the feature weights of the log-linear model used in translation. During decoding, the unknown words were preserved in the hypotheses. The data used to train the model is summarized in Table 1. 3We used the standard NIST scoring package as we did not have access to the IWSLT evaluation server that may normalize and score differently We also used a finite-state implementation of translation without reordering. Reordering can pose a challenge in real-time </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, Shen W., C. Moran, R. Zens, C. J. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="9698" citStr="Koehn, 2005" startWordPosition="1579" endWordPosition="1580">erence English segments released as part of the evaluation (Cettolo et al., 2012). We also harvested the audio data from the TED website for building an acoustic model. A total of 1308 talks in English were downloaded, out of which we used 1119 talks recorded prior to December 2011. We split the stereo audio file and duplicated the data to account for any variations in the channels. The data for the language models was also restricted to that permitted in the IWSLT 2011 evaluation. The parallel text for building the EnglishSpanish translation model was obtained from several corpora: Europarl (Koehn, 2005), JRC-Acquis corpus (Steinberger et al., 2006), Opensubtitle corpus (Tiedemann and Lars Nygaard, 2004), Web crawling (Rangarajan Sridhar et al., 2011) as well as human translation of proprietary data. Table 1 summarizes the data used in building the models. It is important to note that the IWSLT evaluation on TED talks is completely offline. In this work, we perform the first investigation into the real-time translation of these talks. 5 Speech Translation Models In this section, we describe the acoustic, language and translation models used in our experiments. 5.1 Acoustic and Language Model </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lederer</author>
</authors>
<title>Simultaneous interpretation: units of meaning and other features.</title>
<date>1978</date>
<booktitle>Language interpretation and communication,</booktitle>
<pages>323--332</pages>
<editor>In D. Gerver and H. W. Sinaiko, editors,</editor>
<publisher>Plenum Press,</publisher>
<location>New York.</location>
<contexts>
<context position="27048" citStr="Lederer, 1978" startWordPosition="4386" endWordPosition="4387">samos. However, there is a marked improvement in translation accuracy with increasingly larger chunk sizes (lgchunk1, lgchunk2, and lgchunk3). Notably, lgchunk3 yields performance that approaches that of win8 with a chunk size that is one third of win8’s. The conj-pred-eos and pred-punct strategies work the best, and it can be seen that the average segment length (8-12 words) generated in both these schemes is very similar to that used for training the models. It is also about the average latency (4-5 seconds) that can be tolerated in cross-lingual communication, also known as ear-voice span (Lederer, 1978). The non-linguistic segmentation using fixed word length windows also performs well, especially for the longer length windows. However, longer windows (win15) increase the latency and any fixed length window typically destroys the semantic context. It can also be seen from Table 3 that translating the complete talk is suboptimal in comparison with segmenting the text. This is primarily due to bias on sentence length distributions in the training data. Training models on complete talks is likely to resolve this issue. Contrasting the use of reference segments as input to MT (ref-sent, ref-punc</context>
</contexts>
<marker>Lederer, 1978</marker>
<rawString>M. Lederer. 1978. Simultaneous interpretation: units of meaning and other features. In D. Gerver and H. W. Sinaiko, editors, Language interpretation and communication, pages 323–332. Plenum Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="18960" citStr="Marcus et al., 1993" startWordPosition="3094" endWordPosition="3097"> English TED data. This dataset contained 1029 human-transcribed talks consisting of about 103,000 sentences containing about 1.6 million words. Punctuation in this dataset was normalized as follows. Different kinds of sentence ending punctuations were transformed into a uniform end of sentence marker. Double-hyphens were transformed into commas. Commas already existing in the input were kept while all other kinds of punctuation symbols were deleted. A part of speech (POS) tagger was applied to this input. For speed, a unigram POS tagger was implemented which was trained on the Penn Treebank (Marcus et al., 1993) and used orthographic features to predict the POS of unknown words. The SVM-based punctuation classifier relies on a five word and POS window in order to classify the target word. Specifically, token t0 is classified given as input the window t_2t_1tot1t2. Unigram, bigram, and trigram word and POS features based on this window were used for classification. Accuracy of the classifier on the development set was 60.51% F-measure for sentence end detection and 43.43% F-measure for comma detection. Subsequently, data sets pred-sent (sentences) and pred-punct (commaseparated chunks) were obtained. </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>G Leusch</author>
<author>O Bender</author>
<author>H Ney</author>
</authors>
<title>Evaluating machine translation output with automatic sentence segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of IWSLT.</booktitle>
<contexts>
<context position="25091" citStr="Matusov et al., 2005" startWordPosition="4048" endWordPosition="4051">artial output before sending it to MT. The plot shows the BLEU scores as a function of ASR timeouts used to generate the partial hypotheses. Figure 1 also shows the average latency involved in incremental speech translation. 7 Discussion The BLEU scores for the segmentation strategies over ASR hypotheses was computed at the talk level. Since the ASR hypotheses do not align with the reference source text, it is not feasible to evaluate the translation performance using the gold reference. While other studies have used an approximate edit distance algorithm for resegmentation of the hypotheses (Matusov et al., 2005), we simply concatenate all the segments and perform the evaluation at the talk level. The hold segmentation strategy yields the poorest translation performance. The significant drop in BLEU score can be attributed to relatively short segments (2-4 words) that was generated by the model. The scheme oversegments the text and since the translation and language models are trained on sentence like chunks, the performance is poor. For example, the input text the sea should be translated as el mar, but instead the hold segmenter chunks it as the·sea which MT’s chunk translation renders as el·el mar.</context>
</contexts>
<marker>Matusov, Leusch, Bender, Ney, 2005</marker>
<rawString>E. Matusov, G. Leusch, O. Bender, and H. Ney. 2005. Evaluating machine translation output with automatic sentence segmentation. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>D Hillard</author>
<author>M Magimai-Doss</author>
<author>D HakkaniT¨ur</author>
<author>M Ostendorf</author>
<author>H Ney</author>
</authors>
<title>Improving speech translation with automatic boundary prediction.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<marker>Matusov, Hillard, Magimai-Doss, HakkaniT¨ur, Ostendorf, Ney, 2007</marker>
<rawString>E. Matusov, D. Hillard, M. Magimai-Doss, D. HakkaniT¨ur, M. Ostendorf, and H. Ney. 2007. Improving speech translation with automatic boundary prediction. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
<author>F Pereira</author>
<author>M Riley</author>
</authors>
<title>At&amp;t general-purpose finite-state machine software tools,</title>
<date>1997</date>
<location>http://www.research.att.com/sw/tools/fsm/.</location>
<contexts>
<context position="12868" citStr="Mohri et al., 1997" startWordPosition="2085" endWordPosition="2088"> ASCII range followed by removal of punctuations. Subsequently, we normalized hyphenated words and removed words with more than 25 characters. The resultant text was normalized using a variety of number conversion routines and each corpus was filtered by restricting the vocabulary to the top 150000 types; i.e., any sentence containing a token outside the vocabulary was discarded. The vocabulary from all the corpora was then consolidated and another round of filtering to the top 375000 most frequent types was performed. The OOV rate on the TED dev2010 set is 1.1%. We used the AT&amp;T FSM toolkit (Mohri et al., 1997) to train a trigram language model (LM) for each component (corpus). Finally, the component language models were interpolated by minimizing the perplexity on the dev2010 set. The results are shown in Table 2. Accuracy (%) Model dev2010 test2010 Baseline MPE 75.5 73.8 VTLN 78.8 77.4 CMA 80.5 80.0 Table 2: ASR word accuracies on the IWSLT data sets.3 5.2 Translation Model We used the Moses toolkit (Koehn et al., 2007) for performing statistical machine translation. Minimum error rate training (MERT) was performed on the development set (dev2010) to optimize the feature weights of the log-linear </context>
</contexts>
<marker>Mohri, Pereira, Riley, 1997</marker>
<rawString>M. Mohri, F. Pereira, and M. Riley. 1997. At&amp;t general-purpose finite-state machine software tools, http://www.research.att.com/sw/tools/fsm/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="16893" citStr="Och and Ney, 2003" startWordPosition="2744" endWordPosition="2747">ad in additional processing. We experiment with segmenting the text according to word window sizes of length 4, 8, 11, and 15 (denoted as data sets win4, win8, win11, win15, respectively in Table 3). We also experiment with concatenating all of the text from one TED talk into a single chunk (complete talk). A novel hold-output model was also developed in order to segment the input text. Given a pair of parallel sentences, the model segments the source sentence into minimally sized chunks such that crossing links and links of one target word to many source words in an optimal GIZA++ alignment (Och and Ney, 2003) occur only within individual chunks. The motivation behind this model is that if a segment s0 is input at time t0 to an incremental MT system, it can be translated right away without waiting for a segment si that is input at a later time ti7 ti &gt; 0. The hold-output model detects these kinds of segments given a sequence of English words that are input from left to right. A kernel-based SVM was used to develop this model. It tags a token t in the input with either the label HOLD, meaning to chunk it with the next token, or the label OUTPUT, meaning to output the chunk constructed from the maxim</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V K Rangarajan Sridhar</author>
<author>L Barbosa</author>
<author>S Bangalore</author>
</authors>
<title>A scalable approach to building a parallel corpus from the Web. In</title>
<date>2011</date>
<booktitle>Proceedings of Interspeech.</booktitle>
<contexts>
<context position="9848" citStr="Sridhar et al., 2011" startWordPosition="1598" endWordPosition="1601">building an acoustic model. A total of 1308 talks in English were downloaded, out of which we used 1119 talks recorded prior to December 2011. We split the stereo audio file and duplicated the data to account for any variations in the channels. The data for the language models was also restricted to that permitted in the IWSLT 2011 evaluation. The parallel text for building the EnglishSpanish translation model was obtained from several corpora: Europarl (Koehn, 2005), JRC-Acquis corpus (Steinberger et al., 2006), Opensubtitle corpus (Tiedemann and Lars Nygaard, 2004), Web crawling (Rangarajan Sridhar et al., 2011) as well as human translation of proprietary data. Table 1 summarizes the data used in building the models. It is important to note that the IWSLT evaluation on TED talks is completely offline. In this work, we perform the first investigation into the real-time translation of these talks. 5 Speech Translation Models In this section, we describe the acoustic, language and translation models used in our experiments. 5.1 Acoustic and Language Model We use the AT&amp;T WATSONSM speech recognizer (Goffin et al., 2004). The speech recognition component consisted of a three-pass decoding approach utilizi</context>
</contexts>
<marker>Sridhar, Barbosa, Bangalore, 2011</marker>
<rawString>V. K. Rangarajan Sridhar, L. Barbosa, and S. Bangalore. 2011. A scalable approach to building a parallel corpus from the Web. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Rao</author>
<author>I Lane</author>
<author>T Schultz</author>
</authors>
<title>Optimizing sentence segmentation for spoken language translation.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="5504" citStr="Rao et al., 2007" startWordPosition="860" endWordPosition="863">on of TED1 talks has been addressed through the IWSLT 2011 and 2012 evaluation tracks. The talks are from a variety of speakers with varying dialects and cover a range of topics. The study presented in this work is the first effort on real-time speech translation of TED talks. In comparison with previous work, we also present a systematic study of the accuracy versus latency tradeoff for both offline and real-time translation on the same dataset. Various utterance segmentation strategies for offline machine translation of text and ASR output have been presented in (Cettolo and Federico, 2006; Rao et al., 2007; Matusov et al., 2007). The work in (F¨ugen et al., 2007; F¨ugen and Kolss, 2007) also examines the impact of segmentation on offline speech translation of talks. However, the realtime analysis in that work is presented only for speech recognition. In contrast with previous work, we tackle the latency issue in simultaneous translation of talks as a function of segmentation strategy and present some new linguistic and non-linguistic methodologies. We investigate the accuracy versus latency tradeoff across translation of reference text, utterance segmented speech recognition output and 1http://</context>
<context position="15422" citStr="Rao et al., 2007" startWordPosition="2502" endWordPosition="2505">strategy and chunk-wise translation conditioned on history in Table 3. The chunk-wise translation conditioned on history was performed using the continue-partialtranslation option in Moses. 6 Segmentation Strategies The output of ASR for talks is a long string of words with no punctuation, capitalization or segmentation markers. In most offline ASR systems, the talk is first segmented into short utterance-like audio segments before passing them to the decoder. Prior work has shown that additional segmentation of ASR hypotheses of these segments may be necessary to improve translation quality (Rao et al., 2007; Matusov et al., 2007). In a simultaneous speech translation system, one can neither find the optimal segmentation of the entire talk nor tolerate high latencies associated with long segments. Consequently, it is necessary to decode the incoming audio incrementally as well as segment the ASR hypotheses appropriately to maximize MT quality. We present a variety of linguistic and non-linguistic segmentation strategies for segmenting the source text input into MT. In our experiments, they are applied to different inputs including reference text, ASR 1- best hypothesis for manually segmented audi</context>
</contexts>
<marker>Rao, Lane, Schultz, 2007</marker>
<rawString>S. Rao, I. Lane, and T. Schultz. 2007. Optimizing sentence segmentation for spoken language translation. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="20439" citStr="Schmid, 1994" startWordPosition="3314" endWordPosition="3315">d conjunction-word based segments. These segments are separated at either conjunction (e.g. “and,” “or”) or sentenceending word boundaries. Conjunctions were identified using the unigram POS tagger. F-measure performance for detecting conjunctions by the tagger on the development set was quite high, 99.35%. As an alternative, text chunking was performed within each sentence, with each chunk corresponding to one segment. Text chunks are non-recursive syntactic phrases in the input text. We investigated segmenting the source into text chunks using TreeTagger, a decision-tree based text chunker (Schmid, 1994). Initial sets of text chunks were created by using either gold-standard sentence boundaries or boundaries detected using the punctuation classifier, yielding the data sets chunk-ref234 Segmentation Segmentation Reference text ASR 1-best type strategy BLEU Mean BLEU Mean #words #words per segment per segment Independent chunk-wise chunk-wise Independent chunk-wise chunk-wise with history with history FST Moses FST Moses win4 22.6 21.0 25.5 3.9±0.1 17.7 17.1 20.0 3.9±0.1 win8 26.6 26.2 28.2 7.9±0.3 20.6 20.9 22.3 7.9±0.2 Non-linguistic win11 27.2 27.4 29.2 10.9 ± 0.3 21.5 21.8 23.1 10.9±0.4 win</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Steinberger</author>
<author>B Pouliquen</author>
<author>A Widiger</author>
<author>C Ignat</author>
<author>T Erjavec</author>
<author>D Tufis</author>
</authors>
<title>The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="9744" citStr="Steinberger et al., 2006" startWordPosition="1583" endWordPosition="1586">as part of the evaluation (Cettolo et al., 2012). We also harvested the audio data from the TED website for building an acoustic model. A total of 1308 talks in English were downloaded, out of which we used 1119 talks recorded prior to December 2011. We split the stereo audio file and duplicated the data to account for any variations in the channels. The data for the language models was also restricted to that permitted in the IWSLT 2011 evaluation. The parallel text for building the EnglishSpanish translation model was obtained from several corpora: Europarl (Koehn, 2005), JRC-Acquis corpus (Steinberger et al., 2006), Opensubtitle corpus (Tiedemann and Lars Nygaard, 2004), Web crawling (Rangarajan Sridhar et al., 2011) as well as human translation of proprietary data. Table 1 summarizes the data used in building the models. It is important to note that the IWSLT evaluation on TED talks is completely offline. In this work, we perform the first investigation into the real-time translation of these talks. 5 Speech Translation Models In this section, we describe the acoustic, language and translation models used in our experiments. 5.1 Acoustic and Language Model We use the AT&amp;T WATSONSM speech recognizer (Go</context>
</contexts>
<marker>Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufis, 2006</marker>
<rawString>R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec, and D. Tufis. 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
<author>L Lars Nygaard</author>
</authors>
<title>The OPUS corpus - parallel &amp; free.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>Tiedemann, Nygaard, 2004</marker>
<rawString>J. Tiedemann and L. Lars Nygaard. 2004. The OPUS corpus - parallel &amp; free. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vilar</author>
<author>E Matusov</author>
<author>S Hasan</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Statistical machine translation of European parliamentary speeches.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit.</booktitle>
<contexts>
<context position="4592" citStr="Vilar et al., 2005" startWordPosition="707" endWordPosition="710">to the sentence units used in training the language and translation models. We propose several nonlinguistic and linguistic segmentation strategies for the segmentation of text (reference or ASR hypotheses) for machine translation. We address the problem of latency in real-time translation as a function of the segmentation strategy; i.e., we ask the question “what is the segmentation strategy that maximizes the number of segments while still maximizing translation accuracy?”. 2 Related Work Speech translation of European Parliamentary speeches has been addressed as part of the TCSTAR project (Vilar et al., 2005; F¨ugen et al., 2006). The project focused primarily on offline translation of speeches. Simultaneous translation of lectures and speeches has been addressed in (Hamon et al., 2009; F¨ugen et al., 2007). However, the work focused on a single speaker in a limited domain. Offline speech translation of TED1 talks has been addressed through the IWSLT 2011 and 2012 evaluation tracks. The talks are from a variety of speakers with varying dialects and cover a range of topics. The study presented in this work is the first effort on real-time speech translation of TED talks. In comparison with previou</context>
</contexts>
<marker>Vilar, Matusov, Hasan, Zens, Ney, 2005</marker>
<rawString>D. Vilar, E. Matusov, S. Hasan, R. Zens, and H. Ney. 2005. Statistical machine translation of European parliamentary speeches. In Proceedings of MT Summit.</rawString>
</citation>
<citation valid="true">
<title>Verbmobil: Foundations of Speech-to-Speech Translation.</title>
<date>2000</date>
<editor>W. Wahlster, editor.</editor>
<publisher>Springer.</publisher>
<marker>2000</marker>
<rawString>W. Wahlster, editor. 2000. Verbmobil: Foundations of Speech-to-Speech Translation. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>