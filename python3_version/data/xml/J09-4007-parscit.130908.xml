<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990072">
Kernel Methods for Minimally Supervised WSD
</title>
<author confidence="0.679221">
Claudio Giuliano*
Fondazione Bruno Kessler – IRST
</author>
<affiliation confidence="0.262036">
Alfio Massimiliano Gliozzo**
</affiliation>
<author confidence="0.6409305">
Fondazione Bruno Kessler – IRST
Carlo Strapparava†
</author>
<affiliation confidence="0.417525">
Fondazione Bruno Kessler – IRST
</affiliation>
<bodyText confidence="0.999266">
We present a semi-supervised technique for word sense disambiguation that exploits external
knowledge acquired in an unsupervised manner. In particular, we use a combination of basic
kernel functions to independently estimate syntagmatic and domain similarity, building a set of
word-expert classifiers that share a common domain model acquired from a large corpus of un-
labeled data. The results show that the proposed approach achieves state-of-the-art performance
on a wide range of lexical sample tasks and on the English all-words task of Senseval-3, although
it uses a considerably smaller number of training examples than other methods.
</bodyText>
<sectionHeader confidence="0.995168" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999842578947368">
A significant challenge in many natural language processing tasks is to reduce the need
for labeled training data while maintaining an acceptable performance. This is espe-
cially true for word sense disambiguation (WSD) because when moving from the some-
what artificial lexical-sample task to the more realistic all-words task it is practically
impossible to collect a large number of training examples for each word sense. Thus,
many supervised approaches, explicitly designed for the lexical-sample task, cannot be
applied to the all-words task, even though they exhibit excellent performance. This has
led to the somewhat paradoxical situation in which completely different methods have
been developed for the two tasks, although they represent two sides of the same coin.
To address this problem, in recent work we presented a semi-supervised approach
based on kernel methods for WSD (Strapparava, Gliozzo, and Giuliano 2004; Gliozzo,
Giuliano, and Strapparava 2005; Giuliano, Gliozzo, and Strapparava 2006). In particular,
we explored the following research directions: (1) independently modeling domain and
syntagmatic aspects of sense distinction to improve feature representativeness; and
(2) exploiting external knowledge acquired from unlabeled data, with the purpose of
drastically reducing the amount of labeled training data. The first direction is based on
the linguistic assumption that syntagmatic and domain (associative) relations are crucial
for representing sense distinctions, but they are originated by different phenomena.
Regarding the second direction, one can hope to obtain a more accurate prediction
</bodyText>
<footnote confidence="0.508743333333333">
* FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: giuliano@fbk.eu.
** FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: gliozzo@fbk.eu.
† FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: strappa@fbk.eu.
</footnote>
<note confidence="0.9107815">
Submission received: 23 December 2006; revised submission received: 28 February 2008; accepted for
publication: 17 April 2008.
© 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
</note>
<bodyText confidence="0.998439277777778">
by taking into account unlabeled data relevant to the learning problem (Chapelle,
Sch¨olkopf, and Zien 2006). As a matter of fact, to test this hypothesis, most of the lexical
sample tasks of Senseval-3 (Mihalcea and Edmonds 2004) were provided with a large
amount of unlabeled training data, as well as the usual labeled training data. However,
at that time, we were the only team to use the unlabeled data (Strapparava, Gliozzo,
and Giuliano 2004).
In this article, we review our technique that combines domain and syntagmatic
information in order to define a complete kernel for WSD. The rest of the article is
organized as follows. In Section 2, we provide a general introduction to the kernel
methods, in which we give the basis for understanding our approach. Exploiting kernel
methods, we can define and combine individual kernels representing information from
different sources in a principled way. After this introductory section, in Section 3 we
present the kernels that we developed for WSD. This includes a detailed description
of the individual kernels and the way we define the composite ones. We present our
experiments in Section 4. The results obtained on a range of lexical-sample tasks and on
the English all-words task of Senseval-3 (Mihalcea and Edmonds 2004) show that our
approach achieves state-of-the-art performance. Finally, in Section 5, we offer conclu-
sions and some directions for future research.
</bodyText>
<sectionHeader confidence="0.999717" genericHeader="method">
2. Kernel Methods
</sectionHeader>
<bodyText confidence="0.999914">
Kernel methods are a popular machine learning approach within the natural lan-
guage processing community. They are theoretically well founded in statistical learn-
ing theory and have shown good empirical results in many applications (Vapnik 1999;
Cristianini and Shawe-Taylor 2000; Sch¨olkopf and Smola 2002; Shawe-Taylor and
Cristianini 2004).
The strategy adopted by kernel methods consists of splitting the learning problem
into two parts. They first embed the input data in a suitable feature space, and then
use a linear algorithm to discover nonlinear patterns in the input space. Typically, the
mapping is performed implicitly by a so-called kernel function. The kernel function
is a similarity measure between the input data that depends exclusively on the specific
data type and domain. A typical similarity function is the inner product between feature
vectors. Characterizing the similarity of the inputs plays a crucial role in determining
the success or failure of the learning algorithm, and it is one of the central questions in
the field of machine learning.
Formally, the kernel is a function K : X x X -4 R that takes as input two data objects
(e.g., vectors, texts, or parse trees) and outputs a real number characterizing their
similarity, with the property that the function is symmetric and positive semi-definite.
That is, for all xi, xj E X satisfies
</bodyText>
<equation confidence="0.975414">
K(xi,xj) φ(xi),φ(xj)) (1)
</equation>
<bodyText confidence="0.99988125">
where φ is an (implicit) mapping from X to an (inner product) feature space J7.
Kernels are used inside learning algorithms such as support vector machines (SVM)
or kernel perceptrons as the interface between the algorithm and the data. The kernel
function is then the only domain specific element of the system, while the learning
algorithm is a general purpose component.
The idea behind the SVM (one of the best known kernel-based learning algorithms)
is to map the set of training data into a high-dimensional feature space J7 via a mapping
function φ : X - 4J7, and construct a separating hyperplane with maximum margin (i.e.,
</bodyText>
<page confidence="0.991438">
514
</page>
<note confidence="0.888648">
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
</note>
<bodyText confidence="0.999297695652174">
the minimum distance between the hyperplane and data points) in that space. The use
of an appropriate non-linear transformation φ of the input yields a nonlinear decision
boundary in the input space. Kernel functions make possible the use of feature spaces
with an exponential or even infinite number of dimensions. Instead of performing the
explicit feature mapping φ, one can use a kernel function, which permits the (efficient)
computation of inner products in high-dimensional feature spaces without explicitly
carrying out the mapping φ. This is called the kernel trick in the machine learning
literature (Boser, Guyon, and Vapnik 1992).
Finally, we point out the theoretical tools required to create new kernels, and com-
bine individual kernels to form composite ones. Of course, not every similarity function
is a valid kernel because, by definition, kernels should be equivalent to some inner
product in a feature space. The function K : X x X - 4R is a valid kernel provided that
its kernel matrices1 are positive semi-definite2 for all training sets S = {x1, ..., xl}, the
so-called finitely positive semi-definite property. Note that defining similarity measures
by means of kernels may be more intuitive than performing the explicit mapping in the
feature space. Furthermore, this formulation does not require the set X to be a vector
space: for example, we shall define kernels that take strings as input.
This result is not only useful because it opens new perspectives to define kernel
functions that only implicitly correspond to a feature mapping φ. Another consequence
is that it can be used to prove a set of rules for combining basic kernels to obtain compos-
ite ones. This will allow us to integrate heterogeneous sources of information in a simple
and effective way. We shall use the following properties of kernels to define our compos-
ite kernels. Let k1 and k2 be kernels over X x X; then the following functions are kernels:
</bodyText>
<listItem confidence="0.945723">
• k(xi, xj) = k1(xi, xj) + k2(xi, xj)
• k(xi, xj) = c · k1(xi, xj), c E R+
• k(xi, xj) = k1 (xi,xj) (normalization)
-\/k1 (xi,xi) × k1 (xj,xj)
</listItem>
<bodyText confidence="0.9997918">
In summary, we can define a kernel function by following different strategies: (1)
providing an explicit feature mapping φ : X -4 Rn; (2) defining a similarity function
that is symmetric and positive semi-definite; and (3) composing different valid kernels,
using the closure properties of kernels. This forms the basis for the approach described
in the following section.
</bodyText>
<sectionHeader confidence="0.998572" genericHeader="method">
3. Kernel Methods for WSD
</sectionHeader>
<bodyText confidence="0.999762888888889">
Our approach to WSD consists of representing linguistic phenomena independently and
then defining a combination method to integrate them. As described in the previous sec-
tion, the kernel function is the only task-specific component of the learning algorithm.
Thus, to develop a WSD system, we only need to define appropriate kernel functions to
represent the domain and syntagmatic aspects of sense distinction and, second, exploit
the properties of kernel functions to define a composite kernel to combine and extend
the individual kernels.
The resulting WSD system consists of two families of kernels: the domain and the
syntagmatic kernels. The former family, described in Section 3.1, models the domain
</bodyText>
<footnote confidence="0.9447198">
1 Given a set of vectors S = {x1,..., xl}, the kernel matrix K is defined as the l x l matrix K whose entries
are Kij = k(xi, xj) = (φ(xi), φ(xj)), where k is a kernel function that evaluates the inner products in a
feature space with feature map φ.
2 A symmetric matrix is positive semi-definite if its eigenvalues are all non-negative. Actually, as we will
see in Section 3.2 using Proposition 1, it is quite easy to verify this property.
</footnote>
<page confidence="0.991351">
515
</page>
<table confidence="0.443979">
Computational Linguistics Volume 35, Number 4
</table>
<tableCaption confidence="0.996658">
Table 1
</tableCaption>
<table confidence="0.875957166666667">
An example of a domain matrix.
Medicine Computer Science
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
</table>
<bodyText confidence="0.63857">
aspects of sense distinction; it is composed of the domain kernel (KD) and the bag-of-
words kernel (KBoW). The latter, described in Section 3.2, represents the syntagmatic
aspects of sense distinction; it is composed of the collocation kernel (KColl) and the part-
of-speech kernel (KPoS). Finally, Section 3.3 describes the composite kernel for WSD.
</bodyText>
<subsectionHeader confidence="0.996979">
3.1 Domain Kernels
</subsectionHeader>
<bodyText confidence="0.99924337037037">
It has been shown that domain information is fundamental for WSD (Magnini et al.
2002). For instance, the (domain) polysemy between the computer science and the
medicine senses of the word virus can be solved by considering the domain of the
context in which it appears. Gliozzo, Strapparava, and Dagan (2004) proposed a WSD
method that exploits only domain information.
In the context of kernel methods, domain information can be exploited by defining
a kernel function that estimates the domain similarity between the contexts of the
words to be disambiguated. The simplest method to estimate the domain similarity
between two texts is to compute the cosine similarity of their vector representations
in the vector space model (VSM). The VSM is a k-dimensional space Rk, in which the
text tj is represented by a vector tj, where the ith component is the term frequency of
the term wi in tj. However, such an approach does not deal well with lexical variability
and ambiguity. For instance, despite the fact that the sentences He is affected by AIDS
and HIV is a virus express closely-related concepts, their similarity is zero in the VSM
because they have no words in common (they are represented by orthogonal vectors).
On the other hand, due to the ambiguity of the word virus, the similarity between the
sentences The laptop has been infected by a virus and HIV is a virus is greater than zero,
even though they convey very different messages.
To overcome this problem, we introduce the domain model (DM) and show how to
use it to define a domain VSM in which texts and terms are represented in a uniform
way. A DM is composed of soft clusters of terms. Each cluster represents a semantic
domain, that is, a set of terms that often co-occur in texts having similar topics. A DM
is represented by a k x k&apos; rectangular matrix D, containing the degree of association
among terms and domains, as illustrated in Table 1.
The matrix D is used to define a function D : Rk → Rk&apos;, that maps the vector tj
represented in the standard VSM into the vector
i�j in the domain VSM. D is defined
</bodyText>
<equation confidence="0.748776333333333">
as follows:3
D((j) = tj(IIDFD) = �t&apos; (2)
j
</equation>
<footnote confidence="0.951578">
3 In Wong, Ziarko, and Wong (1985), Equation (2) is used to define a generalized vector space model, of
which the domain VSM is a particular instance.
</footnote>
<page confidence="0.995135">
516
</page>
<note confidence="0.531483">
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
</note>
<bodyText confidence="0.961025428571429">
where tj is represented as a row vector, IIDF is a k x k diagonal matrix such that iIDF
i,i
=
IDF(wi), and IDF(wi) is the inverse document frequency of wi.
In the domain space, the similarity is estimated by taking into account second order
relations among terms. For example, the similarity of the two sentences He is affected
by AIDS and HIV is a virus is very high, because the terms AIDS, HIV, and virus are
strongly associated with the medicine domain.
A DM can be estimated from manually constructed lexical resources, such as Word-
Net Domains (Magnini and Cavagli`a 2000), or by performing a term-clustering process
on a (large) corpus. However, the second approach is more attractive because it allows
us to automatically acquire DMs for different languages and domains.
In Gliozzo, Giuliano, and Strapparava (2005), we use singular valued decomposi-
tion (SVD) to acquire DMs from a corpus represented by its term-by-document matrix
T, in a unsupervised way.4 SVD decomposes the term-by-document matrix T into
three matrixes T �-- VEk&apos;UT, where V and U are orthogonal matrices (i.e., VTV = I and
UTU = I) whose columns are the eigenvectors of TTT and TTT, respectively, and Eke
is the diagonal k x k matrix containing the highest k&apos; « k eigenvalues of T, and all the
remaining elements set to 0. The parameter k&apos; is the dimensionality of the domain VSM
and can be fixed in advance. Under this setting, we define the domain matrix D as
follows:
</bodyText>
<equation confidence="0.9151338">
\/
D = INV Ek~ (3)
where IN is a diagonal matrix such that iNi,i = 1i�, w&apos; is the ith row of the matrix
i
� i ,VV/Ek&apos;.5
</equation>
<bodyText confidence="0.999882083333333">
Note that in this case, with respect to Table 1, the domains are represented by the
columns of the matrix D and they do not have an explicit name. By using a small
number of domains, we can define a very compact representation of the DM and, con-
sequently, reduce the memory requirements while preserving most of the information.
There exist very efficient algorithms to perform the SVD process on sparse matrices,
allowing us to perform this operation on large corpora in a very limited time and with
reduced memory requirements.6
Therefore, we can define the domain kernel to estimate the domain similarity
between the contexts of the words to be disambiguated. It is a variant of the latent
semantic kernel (Shawe-Taylor and Cristianini 2004), in which a DM is exploited to
define an explicit mapping D : Rk --� Rk&apos; from the classical VSM into the domain VSM.
The domain kernel is explicitly defined as follows:
</bodyText>
<equation confidence="0.993606">
KD(ti,tj) = (D(ti),D(tj)) (4)
</equation>
<bodyText confidence="0.908814454545455">
where D is the domain mapping defined in Equation (2).
4 The SVD algorithm was first adopted to perform latent semantic analysis of terms and latent semantic
indexing of documents in large corpora (Deerwester et al. 1990).
5 When D is substituted in Equation (2) the domain VSM is equivalent to a latent semantic space
(Deerwester et al. 1990). The only difference in our formulation is that the vectors representing the terms
in the domain VSM are normalized by the matrix IN, and then rescaled, according to their IDF value, by
matrix IIDF. Note the analogy with the tf-idf term weighting schema (Salton and McGill 1983), widely
used in information retrieval.
6 To perform the SVD, we used LIBSVDC, an optimized package for sparse matrices that allows
us to perform this step in a few minutes even for large corpora. It can be downloaded from
http://tedlab.mit.edu/—dr/SVDLIBC/.
</bodyText>
<page confidence="0.945503">
517
</page>
<note confidence="0.272736">
Computational Linguistics Volume 35, Number 4
</note>
<bodyText confidence="0.999944181818182">
A standard approach for detecting topic (domain) similarity is to extract bag-of-
words features from a wide window of text around the words to be disambiguated.
Based on this representation, we define a linear kernel called the bag-of-words kernel
(KBoW). KBoW is a particular case of the domain kernel in which D = I in Equation (2),
where I is the identity matrix. The BoW kernel does not require a DM; therefore, it
can be applied to the strictly supervised settings, in which external knowledge is not
available.
To summarize, the domain kernel allows us to plug external knowledge into the
supervised learning process; it will be compared and combined with the standard bag-
of-words approach in Section 4. In the following section, we shall see that domain
models are also useful for defining soft-matching collocation kernels.
</bodyText>
<subsectionHeader confidence="0.998872">
3.2 Syntagmatic Kernels
</subsectionHeader>
<bodyText confidence="0.9999304">
Collocations (such as bigrams and trigrams) extracted from the local context of the word
to be disambiguated are typically used to capture syntagmatic relations (Yarowsky
1994). However, traditional approaches to WSD fail to represent non-contiguous or
shifted collocations, and fail to consider lexical variability. For example, suppose we
have to disambiguate the verb to score in the sentence Ronaldo scored the first goal, given
the labeled example The football player scored two goals in the second half as training. A
traditional approach has no clues to return the right answer because the two sentences
have no features in common.
The use of kernels on strings allows us to overcome the aforementioned problems
by representing (non-contiguous) collocations and exploiting external lexical knowl-
edge sources to define non-zero measures of similarity between words (soft-matching
criteria). In this formulation, words taken in their context are compared by kernels that
sum the number of common (non-contiguous) collocations of words, considering lexical
variability, and part-of-speech tags, avoiding an explicit feature mapping that would
lead to an exponential number of features.
String kernels (or sequence kernels) are a family of kernel functions developed
to compute the inner product among images of strings in high-dimensional feature
space using dynamic programming techniques. The gap-weighted subsequences kernel
is one of the most general types of kernel based on sequences. Roughly speaking,
it compares two strings by means of the number of contiguous and non-contiguous
substrings of a given length they have in common. Non-contiguous occurrences are
penalized according to the number of gaps they contain. Formally, let E be an al-
phabet of |E |symbols, and s = s1s2 ... slsl be a finite sequence over E (i.e., si E E,1 &lt;
i S |s|). Let i = [i1, i2, ... , in], with 1 S i1 &lt; i2 &lt; ... &lt; in S |s|, be a subset of the indices
in s; we will denote as s[i] E En the subsequence si1si2 ... sin. Note that s[i] does not
necessarily form a contiguous subsequence of s; for example, if s is the sequence
“Ronaldo scored the first goal” and i = [2,5], then s[i] is “scored goal”. The length
spanned by s[i] in s is l(i) = in − i1 + 1. The feature space associated with the gap-
weighted subsequences kernel of length n is indexed by I = En, with the embedding
given by
</bodyText>
<equation confidence="0.936048">
φn u(s) = � Al(i), u E En (5)
i:u=s[i]
</equation>
<page confidence="0.937021">
518
</page>
<bodyText confidence="0.833237333333333">
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
where 0&lt;λ&lt; 1 is the decay factor used to penalize non-contiguous subsequences.7 The
associate kernel is defined as
</bodyText>
<equation confidence="0.969539">
Kn(s,t) = �φn(s), φn(t)� = E φnu(s)φnu(t) (6)
u∈En
</equation>
<bodyText confidence="0.99802475">
An explicit computation of Equation (6) is unfeasible even for small values of n.
To evaluate Kn more efficiently, we use the recursive formulation based on a dynamic
programming implementation (Lodhi et al. 2002; Saunders, Tschach, and Shawe-Taylor
2002; Cancedda et al. 2003). It is defined in the following equations:
</bodyText>
<equation confidence="0.9980041875">
K01(s,t) = 1, ∀s, t (7)
Kl(s,t) = 0, if min(|s|, |t|) &lt; i (8)
K~~
i (s,t) = 0, if min(|s|, |t|) &lt; i (9)
�
K„ λK&amp;quot; (sx, t) if x # y;
i
=(10)
λK��
i (sx, t) + λ2K� i−1(s, t) otherwise.
(sx, ty)
Kl (sx, t) = λKl (s, t) + K~~
i (sx, t) (11)
Kn(s,t) = 0, if min(|s|, |t|) &lt; n (12)
Kn(sx, t) = Kn(s, t) + E λ2Kn−1(s,t[1 : j − 1]) (13)
j:tj=x
</equation>
<bodyText confidence="0.999976055555556">
where Kn and K~~n are auxiliary functions with a similar definition to Kn used to facilitate
the computation. Based on these definitions, Kn can be computed in O(n|s||t|). Using
this recursive definition, it turns out that computing all kernel values for subsequences
of lengths up to n is not significantly more costly than computing the kernel for n only.
The syntagmatic kernel is defined as a sum of gap-weighted subsequences kernels
that operate at word and part-of-speech tag level. In particular, following the approach
proposed by Cancedda et al. (2003), it is possible to adapt sequence kernels to operate
at word level by instancing the alphabet E with the vocabulary V = {w1, w2, ... , wk}.
Moreover, we restrict the generic definition of the gap-weighted subsequences kernel
to recognize collocations in the local context of a specified word. The resulting kernel,
called the n-gram collocation kernel (KnColl), operates on sequences of lemmata around
a specified word l0 (i.e., l−3, l−2, l−1, l0, l+1, l+2, l+3). This formulation allows us to
estimate the number of common (sparse) subsequences of lemmata (i.e., collocations)
between two examples, in order to capture syntagmatic similarity. Analogously, we
define the part-of-speech kernel (KnPoS) to operate on sequences of part-of-speech tags
p−3, p−2, p−1, p0, p+1, p+2, p+3, where p0 is the part-of-speech tag of l0.
The collocation kernel and the part-of-speech kernel are defined by Equations (14)
and (15), respectively.
</bodyText>
<equation confidence="0.975485333333333">
n
KColl(s,t) = KlColl(s, t) (14)
l=1
</equation>
<bodyText confidence="0.5933175">
7 Notice that by choosing λ = 1, sparse subsequences are not penalized. On the other hand, the kernel does
not take into account sparse subsequences with λ → 0.
</bodyText>
<page confidence="0.973636">
519
</page>
<note confidence="0.396706">
Computational Linguistics Volume 35, Number 4
</note>
<equation confidence="0.960325666666667">
n
KPoS(s, t) = KlPoS(s, t) (15)
l=1
</equation>
<bodyText confidence="0.998761666666667">
Both kernels depend on the parameter n, the length of the non-contiguous subse-
quences, and A, the decay factor. For example, K2Coll allows us to represent all (sparse)
bigrams in the local context of a word. Finally, the syntagmatic kernel is defined as
</bodyText>
<equation confidence="0.510473">
KSynt(s, t) = KColl(s, t) + KPoS(s, t) (16)
</equation>
<bodyText confidence="0.999954857142857">
In the preceding definition, only exact word-matches contribute to the similarity.
To solve this problem, external lexical knowledge is fed into the supervised learning
process, allowing us to define the soft-matching collocation kernel. In particular, we de-
fine two alternative soft-matching criteria by exploiting synonymy relations in WordNet
and DMs acquired from corpora. Both criteria are based on the assumption that every
word in a sentence can be substituted by another preserving the original meaning, if
these words are paradigmatically related (e.g., synonyms, hyponyms, or domain related
words). For example, if we consider as equivalent the terms Ronaldo and football player,
then the sentence The football player scores the first goal is equivalent to Ronaldo scores the
first goal, providing a strong evidence to disambiguate the verb to score in the second
sentence.
Following the approach proposed by Shawe-Taylor and Cristianini (2004), the soft-
matching gap-weighted subsequences kernel is now calculated recursively using Equa-
tions (7)–(9), (11), and (12), replacing Equation (10) by the equation:
</bodyText>
<equation confidence="0.987067333333333">
K&apos;&apos;
i (sx, ty) = AK&apos;&apos;
i (sx, t) + A2axyK&apos;i−1(s, t), ∀x, y (17)
</equation>
<bodyText confidence="0.867470857142857">
and modifying Equation (13) to:
Kn(sx, t) = Kn(s, t) + � |t |A2axtjK&apos;n−1(s,t[1 : j − 1]) (18)
j
where axy are entries in a similarity matrix A between terms. In order to ensure that the
resulting kernel is still valid, A must be positive semi-definite.
In the following sections, we describe the two alternative soft-matching criteria
based on WordNet Synonymy and Domain Proximity, respectively. To show that the
similarity matrices are positive semi-definite, we use the following result.
Proposition 1
A matrix A is positive semi-definite if and only if A = BTB for some real matrix B.
The proof is given in Shawe-Taylor and Cristianini (2004).
WordNet Synonymy. The first soft-matching criterion is based on WordNet8 to define
a similarity matrix between words. In particular, we substitute two words if they are
synonyms. To this end, a word is represented as vector whose dimensions are associated
</bodyText>
<footnote confidence="0.565544">
8 We used WordNet 1.7.1 and MultiWordNet for English and Italian experiments, respectively.
</footnote>
<page confidence="0.975643">
520
</page>
<note confidence="0.714799">
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
</note>
<bodyText confidence="0.999873625">
with the synsets. Formally, we define the term-by-synset matrix S as the matrix whose
rows are indexed by the terms and whose columns are indexed by the synsets. The
(i, j)th entry of S is 1 if the synset sj contains the term wi; 0 otherwise. The matrix
S gives rise to the similarity matrix A = SST between terms. Because A can be re-
written as A = (ST)TST = BTB, it follows directly from Proposition 1 that it is positive
semi-definite.
Domain Proximity. The second soft-matching criterion exploits the domain models intro-
duced in Section 3.1 to define a similarity matrix between words. Once a DM has been
defined by the matrix D, the domain space is a k&apos; dimensional space, in which both texts
and terms are represented by means of domain vectors, that is, vectors representing the
domain relevances among the linguistic object and each domain. The domain vector w&apos;i
for the term wi E V is the ith row of D, where V = {w1, w2, ... , wk} is the vocabulary of
the corpus. The term-by-domain matrix D gives rise to the similarity matrix A = DDT
between terms. It follows by Proposition 1 that A is positive semi-definite.
We shall show that the syntagmatic kernel is more effective than standard bigrams
and trigrams of lemmata and part-of-speech tags typically used as features in WSD.
</bodyText>
<subsectionHeader confidence="0.996753">
3.3 Composite Kernel
</subsectionHeader>
<bodyText confidence="0.99983875">
Having defined all the individual kernels representing syntagmatic and domain aspects
of sense distinction, we can define the composite kernel to combine and extend the
individual kernels. The closure properties of the kernel functions allows us to define
the composite kernel as
</bodyText>
<equation confidence="0.9935875">
KC(xi, xj) = n Kl(xi,xj) (19)
l=1 \/Kl(xj, xj)Kl(xi, xi)
</equation>
<bodyText confidence="0.9981826">
where Kl is a valid individual kernel. The individual kernels are normalized—this plays
an important role in allowing us to integrate information from heterogeneous feature
spaces.
Recent work (Moschitti 2004; Gliozzo, Giuliano, and Strapparava 2005; Zhao and
Grishman 2005; Giuliano, Lavelli, and Romano 2006) has empirically shown the effec-
tiveness of combining kernels in this way: The composite kernel consistently improves
the performance of the individual ones. In addition, this formulation allows us to
evaluate the individual contribution of each information source.
In order to show the effectiveness of the proposed domain model in supervised
learning, we defined two WSD kernels, Kwsd and K&apos;wsd. They are completely specified by
the n individual kernels that compose them in Equation (19).
Kwsd is composed by KColl, KPoS, and KBoW;
K&apos;wsd is composed by KColl, KPoS, KBoW, and KD.
The only difference between the two is that K&apos;wsd uses the domain kernel KD to exploit
external knowledge while Kwsd only uses the labeled training data.
</bodyText>
<page confidence="0.992941">
521
</page>
<note confidence="0.560569">
Computational Linguistics Volume 35, Number 4
</note>
<sectionHeader confidence="0.93097" genericHeader="evaluation">
4. Evaluation
</sectionHeader>
<bodyText confidence="0.999833625">
Experiments were carried out on various tasks of Senseval-3 (Mihalcea and Edmonds
2004). First of all, we conducted a preliminary set of experiments on the Catalan,
English, Italian, and Spanish lexical-sample tasks; the results are shown in Section 4.1.
Second, in order to show the general applicability of the proposed method, we evalu-
ated the system on the English all-words task; the results are presented in Section 4.2.
All the experiments were performed using the SVM package (Chang and Lin 2001)
customized to embed our own kernels. The parameters were optimized by five-fold
cross-validation on the training set.
</bodyText>
<subsectionHeader confidence="0.967129">
4.1 Lexical-Sample Tasks
</subsectionHeader>
<bodyText confidence="0.99709144">
In this section, we report the evaluation of our method on the Catalan, English, Italian,
and Spanish lexical-sample tasks of Senseval-3 (Mihalcea and Edmonds 2004). Table 2
describes the tasks we have considered. For each task, it summarizes the number of
words to be disambiguated, the mean polysemy, the size of the labeled training set,
the size of the test set, and the size of the unlabeled training set, respectively. For the
Catalan, Italian, and Spanish tasks, we acquired the DMs from the unlabeled corpora
made available by the task organizers. For the English task, we used a DM acquired
from the British National Corpus (BNC) as the task organizers have not provided
any unlabeled training data. The objectives of these experiments are to (a) estimate
the impact of different knowledge sources in WSD; (b) study the effectiveness of the
kernel combination; (c) understand the benefits of plugging external information in a
supervised framework; and (d) verify the portability of our methodology to different
languages.
4.1.1 Results. Table 3 reports the results of the individual kernels KBoW, KD, KColl, and
KPoS and their combinations Kwsd and K�wsd (the baselines for the tasks are reported in
Table 5). In our experiments, the parameters n and A (see Equation (5)) are optimized
by five-fold cross-validation. For KnColl, we obtained the best results with n = 2 and
A = 0.5. For KnPoS, n = 3 and A → 0. The domain cardinality k� was set to 50. Table 4
shows the performance of the syntagmatic kernel in different configurations: hard and
soft matching. As a baseline, we report the result of a standard approach consisting of
explicit bigrams and trigrams of words and part-of-speech tags around the words to
be disambiguated (Yarowsky 1994). We evaluated the impact of the domain kernel on
the overall performance by comparing the learning curves of Kwsd and Kwsd on the four
lexical-sample tasks. Figure 1 shows the results of our experiments. The points of the
learning curves are obtained by sampling the same percentage of training examples for
</bodyText>
<tableCaption confidence="0.98272">
Table 2
</tableCaption>
<table confidence="0.9446145">
Description of the lexical-sample tasks of Senseval-3.
Task #w mean polysemy #train #test #unlab
Catalan 27 3.11 4,469 2,253 23,935
English 57 6.47 7,860 3,944 -
Italian 45 6.30 5,145 2,439 74,788
Spanish 46 3.30 8,430 4,195 61,252
</table>
<page confidence="0.932916">
522
</page>
<note confidence="0.863689">
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
</note>
<tableCaption confidence="0.997886">
Table 3
</tableCaption>
<table confidence="0.9790689">
The performance (F1) of the basic and composite kernels on the Catalan, English, Italian, and
Spanish lexical-sample tasks of Semeval-3.
Kernel Catalan English Italian Spanish
KBoW 81.3 63.7 43.3 78.2
KD 85.2 65.5 44.5 84.4
KColl 84.2 68.5 54.0 83.6
KPoS 79.6 64.0 44.4 79.5
Kwsd 85.2 69.7 53.1 84.2
K� 89.0 73.3 61.3 88.2
wsd
</table>
<tableCaption confidence="0.693762333333333">
Table 4
Performance (F1) of the syntagmatic kernel for the Catalan, English, Italian, and Spanish
lexical-sample tasks of Semeval-3.
</tableCaption>
<table confidence="0.757783">
Method Catalan English Italian Spanish
Bigrams and trigrams 82.6 67.3 51.0 81.9
Hard matching 83.8 67.7 51.9 82.9
Soft matching (WordNet) - 67.3 51.3 -
Soft matching (Domain proximity) 84.2 68.5 54.0 83.6
</table>
<bodyText confidence="0.995874115384615">
each word. Finally, Table 5 summarizes the results we obtained, providing a comparison
with the state of the art.
4.1.2 Discussion. Table 3 shows that domain information and syntagmatic information
are crucial for WSD, and their combination significantly outperforms the individual
kernels, showing the effectiveness of the kernel combination method.
In addition, the domain kernel KD outperforms the bag-of-words kernel KBoW,
and the composite kernel Kwsd that makes use of domain information outperforms the
one Kwsd based only on the labeled training data, demonstrating our assumption (see
Section 3).
Table 4 shows that the syntagmatic kernel outperforms the baseline (bigrams and
trigrams) in any configuration (hard-/soft-matching). The soft-matching criteria further
improve the classification performance. It is interesting to note that the domain proxim-
ity obtained better results than WordNet synonymy (note that we do not have a Catalan
or a Spanish WordNet). The different results observed for Italian and English using
the domain proximity soft-matching criterion are probably due to the small size of the
unlabeled English corpus.
Figure 1 shows that Kwsd outperforms Kwsd on all lexical sample tasks, even with a
small number of examples. It is worth noting, as reported in Table 5, that Kwsd achieves
the same performance as Kwsd using about half of the labeled training data. This result
shows that the proposed semi-supervised learning approach consisting of acquiring
domain models from unlabeled corpora is effective, as it allows us to drastically reduce
the amount of labeled training data and provide a viable solution for the knowledge
acquisition bottleneck problem in WSD.
To the best of our knowledge, Kwsd turns out to be the best system for all the tested
tasks of Senseval-3, further improving the state of the art by 0.4% to 8.2% for English
and Italian, respectively. Finally, we have demonstrated the language independency
</bodyText>
<page confidence="0.995476">
523
</page>
<figure confidence="0.83449">
Computational Linguistics Volume 35, Number 4
</figure>
<figureCaption confidence="0.695496333333333">
Figure 1
From left to right, top to bottom, learning curves for the Catalan, English, Italian, and Spanish
lexical-sample tasks of Semeval-3.
</figureCaption>
<bodyText confidence="0.999579333333333">
of our approach. The DMs have been acquired for different languages from different
unlabeled corpora by adopting exactly the same methodology, without requiring any
external lexical resource or ad hoc rule.
</bodyText>
<subsectionHeader confidence="0.991249">
4.2 All-Words Task
</subsectionHeader>
<bodyText confidence="0.995153">
Encouraged by the excellent results obtained on the lexical-sample tasks, we evaluated
our approach on the all-words task, in which a very small amount of labeled training
</bodyText>
<tableCaption confidence="0.980119">
Table 5
</tableCaption>
<table confidence="0.9469435">
Comparative evaluation on the lexical sample tasks.
Task MF Agreement BEST Kwsd K�wsd DM+ % of training
Catalan 66.3 93.1 85.2 85.2 89.0 3.8 46
English 55.2 67.3 72.9 69.7 73.3 3.6 54
Italian 18.0 89.0 53.1 53.1 61.3 8.2 51
Spanish 67.7 85.3 84.2 84.2 88.2 4.0 50
</table>
<footnote confidence="0.84873375">
Columns report: the Most Frequent baseline, the inter-annotator agreement, the F1 of the best system
at Senseval-3, the F1 of Kwsd, the F1 of Kid, DM+ (the improvement due to DM, i.e., K�wsd − Kwsd),
and the percentage of sense-tagged examples required by Kid to achieve the same performance
as Kwsd with full training.
</footnote>
<page confidence="0.992579">
524
</page>
<note confidence="0.944169">
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
</note>
<tableCaption confidence="0.977004">
Table 6
</tableCaption>
<figure confidence="0.846794444444444">
The performance (F1) of the basic kernels and composite kernels on the English all-words task of
Senseval-3.
basic kernels composite kernels
KD
bnc Ksem
D Cˆse
yb, �m
KBoW KPoS KColl Kwsd Kwsd &amp;quot;wsd
F1 63.0 63.2 63.2 63.4 64.0 64.4 65.0 65.2
</figure>
<bodyText confidence="0.996591545454546">
data is typically available. We performed the evaluation on the English all-words task
of Senseval-3 (Snyder and Palmer 2004). The test set was extracted from two Wall Street
Journal articles and one text from the Brown Corpus. The test set consists of 945 words
(2,041 word occurrences) to be disambiguated with WordNet 1.7.1 senses. The inter-
annotator agreement rate in the preparation of the corpus was approximately 72.5%.
The most frequent (MF) baseline using the first WordNet sense heuristic obtained 60.9%.
We have trained and tested the system exploiting the following resources: (1) Word-
Net 1.7.1 as sense repository; (2) SemCor,9 considering only those words appearing
in the Senseval-3 all-words data set—we extracted about 61,700 tagged examples that
constitute the only labeled training set exploited by the system; and (3) the BNC, from
which we extracted the unlabeled training data.
</bodyText>
<listItem confidence="0.82106025">
4.2.1 Results. We trained 734 word-expert classifiers on the SemCor corpus. The labeled
examples for each classifier range from a minimum of one example to a maximum
of 2,275 examples. We return a random sense for those words that have no training
examples in SemCor.10 We have acquired two DMs, one from the BNC (i.e., ˆKbnc
</listItem>
<bodyText confidence="0.81099992">
D ; the
same we used in the lexical-sample task) and one from SemCor (i.e., ˆKsem
D ), obtaining a
slightly better performance with the latter.
Table 6 shows the performance of the individual kernels KBoW, KD, KColl, and KPoS,
and their composite kernels Kwsd, ˆKbnc
D and ˆKsem
D .
Since for 210 words in the test set we have no training examples, to better under-
stand the results obtained, we performed an evaluation on the subset of the test set for
which at least one training example is available in SemCor. Evaluating only on these
words the performance increases from 65.2% to 70.0%, and the most frequent baseline
becomes 65.7%. Tables 7 and 8 present a more detailed analysis that considers results
grouped according to the amount of training available and the mean polysemy of the
words in the test set, excluding from the data set the monosemous words. Table 7 shows
the results (F1) of ˆKsem
wsd at different ranges of polysemy. Table 8 presents the results (F1)
ofˆKsem
wsd on those words that have a given number of training examples. This evaluation
is limited to the best composite kernel ˆKsem
wsd.
4.2.2 Discussion. We compared our approach with the three best systems that par-
ticipated in the English all-words task of Senseval-3. The best system (Decadt et al.
2004) has comparable performance (65.2) to ours; however, it uses a larger training set
composed of 563,129 sense-tagged words. The training corpus was built by merging
</bodyText>
<footnote confidence="0.956819">
9 Texts semantically annotated with WordNet 1.6 senses (created at Princeton University), and
automatically mapped to WordNet 1.7, WordNet 1.7.1, and WordNet 2.0. Downloadable from
http://www.cs.unt.edu/∼rada/downloads.html.
10 Note that for these words the WordNet first sense is not necessarily the most frequent sense.
</footnote>
<page confidence="0.986731">
525
</page>
<note confidence="0.448251">
Computational Linguistics Volume 35, Number 4
</note>
<tableCaption confidence="0.992959">
Table 7
</tableCaption>
<table confidence="0.9938215">
The performance (F1) of ˆKsem
wsd at different ranges of polysemy. Most Frequent baseline (MF) is
also reported.
Range of polysemy
2–5 6–10 11–15 16–20 21–25 26–30 31+
ˆKsem 73.2 61.4 59.1 33.8 55.2 50.2 37.3
wsd
MF 70.0 53.4 56.4 25.7 47.2 39.0 21.7
</table>
<tableCaption confidence="0.994509">
Table 8
</tableCaption>
<table confidence="0.922031333333333">
The performance (F1) of ˆKsem
wsd on words with a given number of training examples. Most
Frequent baseline (MF) and mean polysemy for each partition are also reported.
Range of training examples
1–10 11–50 51–100 101–200 201+
ˆKsem 76.1 70.8 54.2 67.4 60.0
wsd
MF 73.5 66.4 49.4 63.2 53.0
Mean polysemy 3 5 7 9 15
</table>
<bodyText confidence="0.997218125">
SemCor, and English lexical-sample and all-words data sets taken from all the previous
editions of Senseval. The system proposed by Mihalcea and Faruque (2004) scored sec-
ond (64.6). The dimension of their training set is comparable to ours; however, they also
use additional information drawn from WordNet to derive semantic generalizations
using syntactic dependencies. Finally, the third system (Yuret 2004) obtained 64.1 using
a larger training data set (Semcor, DSO corpus of sense-tagged English, OpenMind
Word Expert, Senseval-2, and Senseval-3 lexical-sample tasks).
The small difference between the two domain models seems to indicate that a
limited amount of unlabeled data is sufficient to improve the overall performance,
and the use of unlabeled data taken from the training set helps to slightly improve
the overall performance. However, the domain model can be acquired from a different
corpus (e.g., the BNC) without significantly affecting the overall performance.
Finally, the results reported in Tables 7 and 8 show that our approach is able to dis-
ambiguate with good accuracy (F1 = 76%) words with a number of training examples
that ranges from 1 to 10, outperforming the most frequent baseline by 3%. This is an
interesting result given the extremely small number of training examples available. On
the other hand, the more training is available for a given word, the more polysemous
that word is. Nevertheless, the algorithm always outperforms the baseline and has a
more significant difference for increasing values of the mean polysemy (from 3% to
16%). These results, together with the ones obtained in the lexical sample tasks, show
that the domain kernel is able to boost the overall performance when little training data
are available, as well as with enough training data. The benefit is even more pronounced
for the latter case, even though the disambiguation task is more complex due to the high
polysemy of highly frequent words.
</bodyText>
<sectionHeader confidence="0.999101" genericHeader="conclusions">
5. Conclusions
</sectionHeader>
<bodyText confidence="0.997926">
This article summarizes the results of a word expert semi-supervised algorithm for
WSD based on a combination of kernel functions. First, we evaluated our methodology
</bodyText>
<page confidence="0.985716">
526
</page>
<note confidence="0.74991">
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
</note>
<bodyText confidence="0.99995675">
on four lexical-sample tasks of Senseval-3, significantly improving the state of the art
for all of them. In particular, we demonstrated that using external knowledge inside a
supervised framework is a viable methodology to reduce the amount of training data
required for learning. In our approach, the external knowledge is represented by means
of domain models automatically acquired from corpora in a totally unsupervised way.
Then, we applied the method so defined to the English all-words task of Senseval-
3, achieving state-of-the-art performance while requiring less labeled training data
compared to the other systems we have found in the literature.
Some slight improvement may be possible by exploiting syntactic information pro-
duced by a parser. In the framework of kernel methods, this expansion can be done by
adding a tree kernel (i.e., a kernel function that evaluates the similarity among parse
trees) to our composite kernel. However, the performance achieved is close to the upper
bound, if we consider the inter-annotator agreement as an indication of the upper-
bound performance.
Finally, we think that our semi-supervised approach is at the moment an effective
solution for developing a sense-tagging system. Indeed, we tested the system on the
English lexical-sample task of SemEval 2007, still obtaining state-of-the-art performance
(Pradhan et al. 2007). Therefore, we plan to make available an optimized version of
our system, and to exploit it for ontology learning, textual entailment, and information
retrieval.
</bodyText>
<sectionHeader confidence="0.996788" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.6548559">
Claudio Giuliano was supported by the
X-Media project (www.x-media-project.org),
sponsored by the European Commission as
part of the Information Society Technologies
(IST) programme under EC grant IST-FP6-
026978. Alfio Massimiliano Gliozzo and
Carlo Strapparava were supported by the
ONTOTEXT project, sponsored by the
Autonomous Province of Trento under the
FUP-2004 research program.
</bodyText>
<sectionHeader confidence="0.991056" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999500425925926">
Boser, Bernhard, Isabelle Guyon, and
Vladimir Vapnik. 1992. A training
algorithm for optimal margin classifier.
In Proceedings of the 5th Annual ACM
Workshop on Computational Learning
Theory, pages 144–152, Pittsburgh, PA.
Cancedda, Nicola, Eric Gaussier, Cyril
Goutte, and Jean-Michel Renders. 2003.
Word-sequence kernels. Journal of Machine
Learning Research, 32(6):1059–1082.
Chang, Chih-Chung and Chih-Jen Lin,
2001. LIBSVM: A library for support vector
machines. Software available at www.csie.
ntu.edu.tw/∼cjlin/libsvm.
Chapelle, Olivier, Bernhard Sch¨olkopf, and
Alexander Zien. 2006. Semi-Supervised
Learning. MIT Press, Cambridge, MA.
Cristianini, Nello and John Shawe-Taylor.
2000. An Introduction to Support Vector
Machines. Cambridge University Press.
Decadt, Bart, Veronique Hoste, Walter
Daelemans, and Antal van den Bosch.
2004. GAMBL, genetic algorithm
optimization of memory-based WSD. In
Proceedings of Senseval-3, pages 108–112,
Barcelona.
Deerwester, Scott, Susan Dumais, George
Furnas, Thomas Landauer, and Richard
Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American
Society of Information Science, 41:391–407.
Giuliano, Claudio, Alfio Gliozzo, and Carlo
Strapparava. 2006. Syntagmatic kernels: A
word sense disambiguation case study. In
In Proceedings of the EACL-06 Workshop on
Learning Structured Information in Natural
Language Applications, pages 57–63, Trento.
Giuliano, Claudio, Alberto Lavelli, and
Lorenza Romano. 2006. Exploiting shallow
linguistic information for relation
extraction from biomedical literature. In
Proceedings of the Eleventh Conference of the
European Chapter of the Association for
Computational Linguistics (EACL-2006),
pages 401–408, Trento.
Gliozzo, Alfio, Claudio Giuliano, and Carlo
Strapparava. 2005. Domain kernels for
word sense disambiguation. In Proceedings
of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL-05),
pages 403–410, Ann Arbor, MI.
Gliozzo, Alfio, Carlo Strapparava, and
Ido Dagan. 2004. Unsupervised and
supervised exploitation of semantic
</reference>
<page confidence="0.932068">
527
</page>
<reference confidence="0.992933860215054">
Computational Linguistics Volume 35, Number 4
domains in lexical disambiguation.
Computer Speech and Language,
18(3):275–299.
Lodhi, Huma, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. Journal
of Machine Learning Research, 2(3):419–444.
Magnini, Bernardo and Gabriela Cavagli`a.
2000. Integrating subject field codes into
WordNet. In Proceedings of LREC-2000,
pages 1413–1418, Athens.
Magnini, Bernardo, Carlo Strapparava,
Giovanni Pezzulo, and Alfio Gliozzo. 2002.
The role of domain information in word
sense disambiguation. Natural Language
Engineering, 8(4):359–373.
Mihalcea, Rada and Phil Edmonds, editors.
2004. Proceedings of Senseval-3: Third
International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text.
Barcelona.
Mihalcea, Rada and Ehsanul Faruque. 2004.
SenseLearner: Minimally supervised WSD
for all words in open text. In Senseval-3:
Third International Workshop on the
Evaluation of Systems for the Semantic
Analysis of Text, pages 155–158, Barcelona.
Moschitti, Alessandro. 2004. A study on
convolution kernels for shallow statistic
parsing. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL 2004), pages 335–342,
Barcelona.
Pradhan, Sameer, Edward Loper, Dmitriy
Dligach, and Martha Palmer. 2007.
Semeval-2007 task-17: English lexical
sample, SRL and all words. In Proceedings
of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007),
pages 87–92, Prague.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, New York.
Saunders, Craig, Hauke Tschach, and John
Shawe-Taylor. 2002. Syllables and other
string kernel extensions. In Proceedings of
19th International Conference on Machine
Learning (ICML02), pages 530–537, Sydney.
Sch¨olkopf, Bernhard and Alexander Smola.
2002. Learning with Kernels. MIT Press,
Cambridge, MA.
Shawe-Taylor, John and Nello Cristianini.
2004. Kernel Methods for Pattern Analysis.
Cambridge University Press.
Snyder, Benjamin and Martha Palmer. 2004.
The English all-words task. In Senseval-3:
Third International Workshop on the
Evaluation of Systems for the Semantic
Analysis of Text, pages 41–43, Barcelona.
Strapparava, Carlo, Alfio Gliozzo, and
Claudio Giuliano. 2004. Pattern abstraction
and term similarity for word sense
disambiguation: Irst at senseval-3. In
Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic
Analysis of Text, pages 229–234, Barcelona.
Vapnik, Vladimir N. 1999. The Nature of
Statistical Learning Theory (Information
Science and Statistics). Springer, Berlin.
Wong, S. K. M., Wojciech Ziarko, and
Patrick C. N. Wong. 1985. Generalized
vector space model in information
retrieval. In Proceedings of the 8th ACM
SIGIR Conference, pages 18–25, Montreal.
Yarowsky, David. 1994. Decision lists for
lexical ambiguity resolution: Application
to accent restoration in Spanish and
French. In Proceedings of the 32nd Annual
Meeting of the Association for Computational
Linguistics (ACL 1994), pages 88–95,
Las Cruces, NM.
Yuret, Deniz. 2004. Some experiments with a
naive Bayes WSD system. In Senseval-3:
Third International Workshop on the
Evaluation of Systems for the Semantic
Analysis of Text, pages 265–268, Barcelona.
Zhao, Shubin and Ralph Grishman. 2005.
Extracting relations with integrated
information using kernel methods. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics
(ACL 2005), pages 419–426, Ann Arbor, MI.
</reference>
<page confidence="0.996682">
528
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.297780">
<title confidence="0.968918">Kernel Methods for Minimally Supervised WSD Fondazione Bruno Kessler – IRST</title>
<author confidence="0.75663">Massimiliano</author>
<abstract confidence="0.900293111111111">Fondazione Bruno Kessler – IRST Fondazione Bruno Kessler – IRST We present a semi-supervised technique for word sense disambiguation that exploits external knowledge acquired in an unsupervised manner. In particular, we use a combination of basic kernel functions to independently estimate syntagmatic and domain similarity, building a set of word-expert classifiers that share a common domain model acquired from a large corpus of unlabeled data. The results show that the proposed approach achieves state-of-the-art performance on a wide range of lexical sample tasks and on the English all-words task of Senseval-3, although it uses a considerably smaller number of training examples than other methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernhard Boser</author>
<author>Isabelle Guyon</author>
<author>Vladimir Vapnik</author>
</authors>
<title>A training algorithm for optimal margin classifier.</title>
<date>1992</date>
<booktitle>In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory,</booktitle>
<pages>144--152</pages>
<location>Pittsburgh, PA.</location>
<marker>Boser, Guyon, Vapnik, 1992</marker>
<rawString>Boser, Bernhard, Isabelle Guyon, and Vladimir Vapnik. 1992. A training algorithm for optimal margin classifier. In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, pages 144–152, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Cancedda</author>
</authors>
<title>Eric Gaussier, Cyril Goutte, and Jean-Michel Renders.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>32</volume>
<issue>6</issue>
<marker>Cancedda, 2003</marker>
<rawString>Cancedda, Nicola, Eric Gaussier, Cyril Goutte, and Jean-Michel Renders. 2003. Word-sequence kernels. Journal of Machine Learning Research, 32(6):1059–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines. Software available at www.csie.</title>
<date>2001</date>
<tech>ntu.edu.tw/∼cjlin/libsvm.</tech>
<contexts>
<context position="28266" citStr="Chang and Lin 2001" startWordPosition="4639" endWordPosition="4642">ge while Kwsd only uses the labeled training data. 521 Computational Linguistics Volume 35, Number 4 4. Evaluation Experiments were carried out on various tasks of Senseval-3 (Mihalcea and Edmonds 2004). First of all, we conducted a preliminary set of experiments on the Catalan, English, Italian, and Spanish lexical-sample tasks; the results are shown in Section 4.1. Second, in order to show the general applicability of the proposed method, we evaluated the system on the English all-words task; the results are presented in Section 4.2. All the experiments were performed using the SVM package (Chang and Lin 2001) customized to embed our own kernels. The parameters were optimized by five-fold cross-validation on the training set. 4.1 Lexical-Sample Tasks In this section, we report the evaluation of our method on the Catalan, English, Italian, and Spanish lexical-sample tasks of Senseval-3 (Mihalcea and Edmonds 2004). Table 2 describes the tasks we have considered. For each task, it summarizes the number of words to be disambiguated, the mean polysemy, the size of the labeled training set, the size of the test set, and the size of the unlabeled training set, respectively. For the Catalan, Italian, and S</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chang, Chih-Chung and Chih-Jen Lin, 2001. LIBSVM: A library for support vector machines. Software available at www.csie. ntu.edu.tw/∼cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Chapelle</author>
<author>Bernhard Sch¨olkopf</author>
<author>Alexander Zien</author>
</authors>
<title>Semi-Supervised Learning.</title>
<date>2006</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Chapelle, Sch¨olkopf, Zien, 2006</marker>
<rawString>Chapelle, Olivier, Bernhard Sch¨olkopf, and Alexander Zien. 2006. Semi-Supervised Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4688" citStr="Cristianini and Shawe-Taylor 2000" startWordPosition="688" endWordPosition="691"> composite ones. We present our experiments in Section 4. The results obtained on a range of lexical-sample tasks and on the English all-words task of Senseval-3 (Mihalcea and Edmonds 2004) show that our approach achieves state-of-the-art performance. Finally, in Section 5, we offer conclusions and some directions for future research. 2. Kernel Methods Kernel methods are a popular machine learning approach within the natural language processing community. They are theoretically well founded in statistical learning theory and have shown good empirical results in many applications (Vapnik 1999; Cristianini and Shawe-Taylor 2000; Sch¨olkopf and Smola 2002; Shawe-Taylor and Cristianini 2004). The strategy adopted by kernel methods consists of splitting the learning problem into two parts. They first embed the input data in a suitable feature space, and then use a linear algorithm to discover nonlinear patterns in the input space. Typically, the mapping is performed implicitly by a so-called kernel function. The kernel function is a similarity measure between the input data that depends exclusively on the specific data type and domain. A typical similarity function is the inner product between feature vectors. Characte</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Cristianini, Nello and John Shawe-Taylor. 2000. An Introduction to Support Vector Machines. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Decadt</author>
<author>Veronique Hoste</author>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
</authors>
<title>GAMBL, genetic algorithm optimization of memory-based WSD.</title>
<date>2004</date>
<booktitle>In Proceedings of Senseval-3,</booktitle>
<pages>108--112</pages>
<location>Barcelona.</location>
<marker>Decadt, Hoste, Daelemans, van den Bosch, 2004</marker>
<rawString>Decadt, Bart, Veronique Hoste, Walter Daelemans, and Antal van den Bosch. 2004. GAMBL, genetic algorithm optimization of memory-based WSD. In Proceedings of Senseval-3, pages 108–112, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan Dumais</author>
<author>George Furnas</author>
<author>Thomas Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<pages>41--391</pages>
<contexts>
<context position="15783" citStr="Deerwester et al. 1990" startWordPosition="2573" endWordPosition="2576">we can define the domain kernel to estimate the domain similarity between the contexts of the words to be disambiguated. It is a variant of the latent semantic kernel (Shawe-Taylor and Cristianini 2004), in which a DM is exploited to define an explicit mapping D : Rk --� Rk&apos; from the classical VSM into the domain VSM. The domain kernel is explicitly defined as follows: KD(ti,tj) = (D(ti),D(tj)) (4) where D is the domain mapping defined in Equation (2). 4 The SVD algorithm was first adopted to perform latent semantic analysis of terms and latent semantic indexing of documents in large corpora (Deerwester et al. 1990). 5 When D is substituted in Equation (2) the domain VSM is equivalent to a latent semantic space (Deerwester et al. 1990). The only difference in our formulation is that the vectors representing the terms in the domain VSM are normalized by the matrix IN, and then rescaled, according to their IDF value, by matrix IIDF. Note the analogy with the tf-idf term weighting schema (Salton and McGill 1983), widely used in information retrieval. 6 To perform the SVD, we used LIBSVDC, an optimized package for sparse matrices that allows us to perform this step in a few minutes even for large corpora. It</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, Scott, Susan Dumais, George Furnas, Thomas Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41:391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>Syntagmatic kernels: A word sense disambiguation case study.</title>
<date>2006</date>
<booktitle>In In Proceedings of the EACL-06 Workshop on Learning Structured Information in Natural Language Applications,</booktitle>
<pages>57--63</pages>
<location>Trento.</location>
<marker>Giuliano, Gliozzo, Strapparava, 2006</marker>
<rawString>Giuliano, Claudio, Alfio Gliozzo, and Carlo Strapparava. 2006. Syntagmatic kernels: A word sense disambiguation case study. In In Proceedings of the EACL-06 Workshop on Learning Structured Information in Natural Language Applications, pages 57–63, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alberto Lavelli</author>
<author>Lorenza Romano</author>
</authors>
<title>Exploiting shallow linguistic information for relation extraction from biomedical literature.</title>
<date>2006</date>
<booktitle>In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006),</booktitle>
<pages>401--408</pages>
<location>Trento.</location>
<marker>Giuliano, Lavelli, Romano, 2006</marker>
<rawString>Giuliano, Claudio, Alberto Lavelli, and Lorenza Romano. 2006. Exploiting shallow linguistic information for relation extraction from biomedical literature. In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006), pages 401–408, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Claudio Giuliano</author>
<author>Carlo Strapparava</author>
</authors>
<title>Domain kernels for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05),</booktitle>
<pages>403--410</pages>
<location>Ann Arbor, MI.</location>
<marker>Gliozzo, Giuliano, Strapparava, 2005</marker>
<rawString>Gliozzo, Alfio, Claudio Giuliano, and Carlo Strapparava. 2005. Domain kernels for word sense disambiguation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05), pages 403–410, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
<author>Ido Dagan</author>
</authors>
<title>Unsupervised and supervised exploitation of semantic</title>
<date>2004</date>
<journal>Computational Linguistics</journal>
<volume>35</volume>
<marker>Gliozzo, Strapparava, Dagan, 2004</marker>
<rawString>Gliozzo, Alfio, Carlo Strapparava, and Ido Dagan. 2004. Unsupervised and supervised exploitation of semantic Computational Linguistics Volume 35, Number 4 domains in lexical disambiguation. Computer Speech and Language, 18(3):275–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="20227" citStr="Lodhi et al. 2002" startWordPosition="3309" endWordPosition="3312">ce associated with the gapweighted subsequences kernel of length n is indexed by I = En, with the embedding given by φn u(s) = � Al(i), u E En (5) i:u=s[i] 518 Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD where 0&lt;λ&lt; 1 is the decay factor used to penalize non-contiguous subsequences.7 The associate kernel is defined as Kn(s,t) = �φn(s), φn(t)� = E φnu(s)φnu(t) (6) u∈En An explicit computation of Equation (6) is unfeasible even for small values of n. To evaluate Kn more efficiently, we use the recursive formulation based on a dynamic programming implementation (Lodhi et al. 2002; Saunders, Tschach, and Shawe-Taylor 2002; Cancedda et al. 2003). It is defined in the following equations: K01(s,t) = 1, ∀s, t (7) Kl(s,t) = 0, if min(|s|, |t|) &lt; i (8) K~~ i (s,t) = 0, if min(|s|, |t|) &lt; i (9) � K„ λK&amp;quot; (sx, t) if x # y; i =(10) λK�� i (sx, t) + λ2K� i−1(s, t) otherwise. (sx, ty) Kl (sx, t) = λKl (s, t) + K~~ i (sx, t) (11) Kn(s,t) = 0, if min(|s|, |t|) &lt; n (12) Kn(sx, t) = Kn(s, t) + E λ2Kn−1(s,t[1 : j − 1]) (13) j:tj=x where Kn and K~~n are auxiliary functions with a similar definition to Kn used to facilitate the computation. Based on these definitions, Kn can be computed</context>
</contexts>
<marker>Lodhi, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Lodhi, Huma, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. Journal of Machine Learning Research, 2(3):419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Gabriela Cavagli`a</author>
</authors>
<title>Integrating subject field codes into WordNet.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC-2000,</booktitle>
<pages>1413--1418</pages>
<location>Athens.</location>
<marker>Magnini, Cavagli`a, 2000</marker>
<rawString>Magnini, Bernardo and Gabriela Cavagli`a. 2000. Integrating subject field codes into WordNet. In Proceedings of LREC-2000, pages 1413–1418, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Carlo Strapparava</author>
<author>Giovanni Pezzulo</author>
<author>Alfio Gliozzo</author>
</authors>
<title>The role of domain information in word sense disambiguation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="10772" citStr="Magnini et al. 2002" startWordPosition="1697" endWordPosition="1700">property. 515 Computational Linguistics Volume 35, Number 4 Table 1 An example of a domain matrix. Medicine Computer Science HIV 1 0 AIDS 1 0 virus 0.5 0.5 laptop 0 1 aspects of sense distinction; it is composed of the domain kernel (KD) and the bag-ofwords kernel (KBoW). The latter, described in Section 3.2, represents the syntagmatic aspects of sense distinction; it is composed of the collocation kernel (KColl) and the partof-speech kernel (KPoS). Finally, Section 3.3 describes the composite kernel for WSD. 3.1 Domain Kernels It has been shown that domain information is fundamental for WSD (Magnini et al. 2002). For instance, the (domain) polysemy between the computer science and the medicine senses of the word virus can be solved by considering the domain of the context in which it appears. Gliozzo, Strapparava, and Dagan (2004) proposed a WSD method that exploits only domain information. In the context of kernel methods, domain information can be exploited by defining a kernel function that estimates the domain similarity between the contexts of the words to be disambiguated. The simplest method to estimate the domain similarity between two texts is to compute the cosine similarity of their vector</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2002</marker>
<rawString>Magnini, Bernardo, Carlo Strapparava, Giovanni Pezzulo, and Alfio Gliozzo. 2002. The role of domain information in word sense disambiguation. Natural Language Engineering, 8(4):359–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Phil Edmonds</author>
<author>editors</author>
</authors>
<date>2004</date>
<booktitle>Proceedings of Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</booktitle>
<location>Barcelona.</location>
<marker>Mihalcea, Edmonds, editors, 2004</marker>
<rawString>Mihalcea, Rada and Phil Edmonds, editors. 2004. Proceedings of Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text. Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ehsanul Faruque</author>
</authors>
<title>SenseLearner: Minimally supervised WSD for all words in open text.</title>
<date>2004</date>
<booktitle>In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>155--158</pages>
<location>Barcelona.</location>
<contexts>
<context position="38733" citStr="Mihalcea and Faruque (2004)" startWordPosition="6338" endWordPosition="6341">orted. Range of polysemy 2–5 6–10 11–15 16–20 21–25 26–30 31+ ˆKsem 73.2 61.4 59.1 33.8 55.2 50.2 37.3 wsd MF 70.0 53.4 56.4 25.7 47.2 39.0 21.7 Table 8 The performance (F1) of ˆKsem wsd on words with a given number of training examples. Most Frequent baseline (MF) and mean polysemy for each partition are also reported. Range of training examples 1–10 11–50 51–100 101–200 201+ ˆKsem 76.1 70.8 54.2 67.4 60.0 wsd MF 73.5 66.4 49.4 63.2 53.0 Mean polysemy 3 5 7 9 15 SemCor, and English lexical-sample and all-words data sets taken from all the previous editions of Senseval. The system proposed by Mihalcea and Faruque (2004) scored second (64.6). The dimension of their training set is comparable to ours; however, they also use additional information drawn from WordNet to derive semantic generalizations using syntactic dependencies. Finally, the third system (Yuret 2004) obtained 64.1 using a larger training data set (Semcor, DSO corpus of sense-tagged English, OpenMind Word Expert, Senseval-2, and Senseval-3 lexical-sample tasks). The small difference between the two domain models seems to indicate that a limited amount of unlabeled data is sufficient to improve the overall performance, and the use of unlabeled d</context>
</contexts>
<marker>Mihalcea, Faruque, 2004</marker>
<rawString>Mihalcea, Rada and Ehsanul Faruque. 2004. SenseLearner: Minimally supervised WSD for all words in open text. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 155–158, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow statistic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004),</booktitle>
<pages>335--342</pages>
<location>Barcelona.</location>
<contexts>
<context position="26859" citStr="Moschitti 2004" startWordPosition="4419" endWordPosition="4420">-speech tags typically used as features in WSD. 3.3 Composite Kernel Having defined all the individual kernels representing syntagmatic and domain aspects of sense distinction, we can define the composite kernel to combine and extend the individual kernels. The closure properties of the kernel functions allows us to define the composite kernel as KC(xi, xj) = n Kl(xi,xj) (19) l=1 \/Kl(xj, xj)Kl(xi, xi) where Kl is a valid individual kernel. The individual kernels are normalized—this plays an important role in allowing us to integrate information from heterogeneous feature spaces. Recent work (Moschitti 2004; Gliozzo, Giuliano, and Strapparava 2005; Zhao and Grishman 2005; Giuliano, Lavelli, and Romano 2006) has empirically shown the effectiveness of combining kernels in this way: The composite kernel consistently improves the performance of the individual ones. In addition, this formulation allows us to evaluate the individual contribution of each information source. In order to show the effectiveness of the proposed domain model in supervised learning, we defined two WSD kernels, Kwsd and K&apos;wsd. They are completely specified by the n individual kernels that compose them in Equation (19). Kwsd i</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Moschitti, Alessandro. 2004. A study on convolution kernels for shallow statistic parsing. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004), pages 335–342, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval-2007 task-17: English lexical sample, SRL and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>87--92</pages>
<location>Prague.</location>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Pradhan, Sameer, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007. Semeval-2007 task-17: English lexical sample, SRL and all words. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 87–92, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="16184" citStr="Salton and McGill 1983" startWordPosition="2642" endWordPosition="2645"> where D is the domain mapping defined in Equation (2). 4 The SVD algorithm was first adopted to perform latent semantic analysis of terms and latent semantic indexing of documents in large corpora (Deerwester et al. 1990). 5 When D is substituted in Equation (2) the domain VSM is equivalent to a latent semantic space (Deerwester et al. 1990). The only difference in our formulation is that the vectors representing the terms in the domain VSM are normalized by the matrix IN, and then rescaled, according to their IDF value, by matrix IIDF. Note the analogy with the tf-idf term weighting schema (Salton and McGill 1983), widely used in information retrieval. 6 To perform the SVD, we used LIBSVDC, an optimized package for sparse matrices that allows us to perform this step in a few minutes even for large corpora. It can be downloaded from http://tedlab.mit.edu/—dr/SVDLIBC/. 517 Computational Linguistics Volume 35, Number 4 A standard approach for detecting topic (domain) similarity is to extract bag-ofwords features from a wide window of text around the words to be disambiguated. Based on this representation, we define a linear kernel called the bag-of-words kernel (KBoW). KBoW is a particular case of the dom</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, Gerard and Michael J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Saunders</author>
<author>Hauke Tschach</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Syllables and other string kernel extensions.</title>
<date>2002</date>
<booktitle>In Proceedings of 19th International Conference on Machine Learning (ICML02),</booktitle>
<pages>530--537</pages>
<location>Sydney.</location>
<marker>Saunders, Tschach, Shawe-Taylor, 2002</marker>
<rawString>Saunders, Craig, Hauke Tschach, and John Shawe-Taylor. 2002. Syllables and other string kernel extensions. In Proceedings of 19th International Conference on Machine Learning (ICML02), pages 530–537, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Sch¨olkopf</author>
<author>Alexander Smola</author>
</authors>
<title>Learning with Kernels.</title>
<date>2002</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Sch¨olkopf, Smola, 2002</marker>
<rawString>Sch¨olkopf, Bernhard and Alexander Smola. 2002. Learning with Kernels. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4751" citStr="Shawe-Taylor and Cristianini 2004" startWordPosition="696" endWordPosition="699">results obtained on a range of lexical-sample tasks and on the English all-words task of Senseval-3 (Mihalcea and Edmonds 2004) show that our approach achieves state-of-the-art performance. Finally, in Section 5, we offer conclusions and some directions for future research. 2. Kernel Methods Kernel methods are a popular machine learning approach within the natural language processing community. They are theoretically well founded in statistical learning theory and have shown good empirical results in many applications (Vapnik 1999; Cristianini and Shawe-Taylor 2000; Sch¨olkopf and Smola 2002; Shawe-Taylor and Cristianini 2004). The strategy adopted by kernel methods consists of splitting the learning problem into two parts. They first embed the input data in a suitable feature space, and then use a linear algorithm to discover nonlinear patterns in the input space. Typically, the mapping is performed implicitly by a so-called kernel function. The kernel function is a similarity measure between the input data that depends exclusively on the specific data type and domain. A typical similarity function is the inner product between feature vectors. Characterizing the similarity of the inputs plays a crucial role in det</context>
<context position="15362" citStr="Shawe-Taylor and Cristianini 2004" startWordPosition="2500" endWordPosition="2503">d they do not have an explicit name. By using a small number of domains, we can define a very compact representation of the DM and, consequently, reduce the memory requirements while preserving most of the information. There exist very efficient algorithms to perform the SVD process on sparse matrices, allowing us to perform this operation on large corpora in a very limited time and with reduced memory requirements.6 Therefore, we can define the domain kernel to estimate the domain similarity between the contexts of the words to be disambiguated. It is a variant of the latent semantic kernel (Shawe-Taylor and Cristianini 2004), in which a DM is exploited to define an explicit mapping D : Rk --� Rk&apos; from the classical VSM into the domain VSM. The domain kernel is explicitly defined as follows: KD(ti,tj) = (D(ti),D(tj)) (4) where D is the domain mapping defined in Equation (2). 4 The SVD algorithm was first adopted to perform latent semantic analysis of terms and latent semantic indexing of documents in large corpora (Deerwester et al. 1990). 5 When D is substituted in Equation (2) the domain VSM is equivalent to a latent semantic space (Deerwester et al. 1990). The only difference in our formulation is that the vect</context>
<context position="23700" citStr="Shawe-Taylor and Cristianini (2004)" startWordPosition="3890" endWordPosition="3893">onymy relations in WordNet and DMs acquired from corpora. Both criteria are based on the assumption that every word in a sentence can be substituted by another preserving the original meaning, if these words are paradigmatically related (e.g., synonyms, hyponyms, or domain related words). For example, if we consider as equivalent the terms Ronaldo and football player, then the sentence The football player scores the first goal is equivalent to Ronaldo scores the first goal, providing a strong evidence to disambiguate the verb to score in the second sentence. Following the approach proposed by Shawe-Taylor and Cristianini (2004), the softmatching gap-weighted subsequences kernel is now calculated recursively using Equations (7)–(9), (11), and (12), replacing Equation (10) by the equation: K&apos;&apos; i (sx, ty) = AK&apos;&apos; i (sx, t) + A2axyK&apos;i−1(s, t), ∀x, y (17) and modifying Equation (13) to: Kn(sx, t) = Kn(s, t) + � |t |A2axtjK&apos;n−1(s,t[1 : j − 1]) (18) j where axy are entries in a similarity matrix A between terms. In order to ensure that the resulting kernel is still valid, A must be positive semi-definite. In the following sections, we describe the two alternative soft-matching criteria based on WordNet Synonymy and Domain P</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>Shawe-Taylor, John and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The English all-words task. In</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>41--43</pages>
<location>Barcelona.</location>
<contexts>
<context position="35069" citStr="Snyder and Palmer 2004" startWordPosition="5733" endWordPosition="5736">e improvement due to DM, i.e., K�wsd − Kwsd), and the percentage of sense-tagged examples required by Kid to achieve the same performance as Kwsd with full training. 524 Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD Table 6 The performance (F1) of the basic kernels and composite kernels on the English all-words task of Senseval-3. basic kernels composite kernels KD bnc Ksem D Cˆse yb, �m KBoW KPoS KColl Kwsd Kwsd &amp;quot;wsd F1 63.0 63.2 63.2 63.4 64.0 64.4 65.0 65.2 data is typically available. We performed the evaluation on the English all-words task of Senseval-3 (Snyder and Palmer 2004). The test set was extracted from two Wall Street Journal articles and one text from the Brown Corpus. The test set consists of 945 words (2,041 word occurrences) to be disambiguated with WordNet 1.7.1 senses. The interannotator agreement rate in the preparation of the corpus was approximately 72.5%. The most frequent (MF) baseline using the first WordNet sense heuristic obtained 60.9%. We have trained and tested the system exploiting the following resources: (1) WordNet 1.7.1 as sense repository; (2) SemCor,9 considering only those words appearing in the Senseval-3 all-words data set—we extra</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Snyder, Benjamin and Martha Palmer. 2004. The English all-words task. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 41–43, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Alfio Gliozzo</author>
<author>Claudio Giuliano</author>
</authors>
<title>Pattern abstraction and term similarity for word sense disambiguation: Irst at senseval-3. In</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>229--234</pages>
<location>Barcelona.</location>
<marker>Strapparava, Gliozzo, Giuliano, 2004</marker>
<rawString>Strapparava, Carlo, Alfio Gliozzo, and Claudio Giuliano. 2004. Pattern abstraction and term similarity for word sense disambiguation: Irst at senseval-3. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 229–234, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory (Information Science and Statistics).</title>
<date>1999</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="4653" citStr="Vapnik 1999" startWordPosition="686" endWordPosition="687">we define the composite ones. We present our experiments in Section 4. The results obtained on a range of lexical-sample tasks and on the English all-words task of Senseval-3 (Mihalcea and Edmonds 2004) show that our approach achieves state-of-the-art performance. Finally, in Section 5, we offer conclusions and some directions for future research. 2. Kernel Methods Kernel methods are a popular machine learning approach within the natural language processing community. They are theoretically well founded in statistical learning theory and have shown good empirical results in many applications (Vapnik 1999; Cristianini and Shawe-Taylor 2000; Sch¨olkopf and Smola 2002; Shawe-Taylor and Cristianini 2004). The strategy adopted by kernel methods consists of splitting the learning problem into two parts. They first embed the input data in a suitable feature space, and then use a linear algorithm to discover nonlinear patterns in the input space. Typically, the mapping is performed implicitly by a so-called kernel function. The kernel function is a similarity measure between the input data that depends exclusively on the specific data type and domain. A typical similarity function is the inner produc</context>
</contexts>
<marker>Vapnik, 1999</marker>
<rawString>Vapnik, Vladimir N. 1999. The Nature of Statistical Learning Theory (Information Science and Statistics). Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K M Wong</author>
<author>Wojciech Ziarko</author>
<author>Patrick C N Wong</author>
</authors>
<title>Generalized vector space model in information retrieval.</title>
<date>1985</date>
<booktitle>In Proceedings of the 8th ACM SIGIR Conference,</booktitle>
<pages>18--25</pages>
<location>Montreal.</location>
<marker>Wong, Ziarko, Wong, 1985</marker>
<rawString>Wong, S. K. M., Wojciech Ziarko, and Patrick C. N. Wong. 1985. Generalized vector space model in information retrieval. In Proceedings of the 8th ACM SIGIR Conference, pages 18–25, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>88--95</pages>
<location>Las Cruces, NM.</location>
<contexts>
<context position="17524" citStr="Yarowsky 1994" startWordPosition="2857" endWordPosition="2858"> can be applied to the strictly supervised settings, in which external knowledge is not available. To summarize, the domain kernel allows us to plug external knowledge into the supervised learning process; it will be compared and combined with the standard bagof-words approach in Section 4. In the following section, we shall see that domain models are also useful for defining soft-matching collocation kernels. 3.2 Syntagmatic Kernels Collocations (such as bigrams and trigrams) extracted from the local context of the word to be disambiguated are typically used to capture syntagmatic relations (Yarowsky 1994). However, traditional approaches to WSD fail to represent non-contiguous or shifted collocations, and fail to consider lexical variability. For example, suppose we have to disambiguate the verb to score in the sentence Ronaldo scored the first goal, given the labeled example The football player scored two goals in the second half as training. A traditional approach has no clues to return the right answer because the two sentences have no features in common. The use of kernels on strings allows us to overcome the aforementioned problems by representing (non-contiguous) collocations and exploit</context>
<context position="30168" citStr="Yarowsky 1994" startWordPosition="4952" endWordPosition="4953">tions Kwsd and K�wsd (the baselines for the tasks are reported in Table 5). In our experiments, the parameters n and A (see Equation (5)) are optimized by five-fold cross-validation. For KnColl, we obtained the best results with n = 2 and A = 0.5. For KnPoS, n = 3 and A → 0. The domain cardinality k� was set to 50. Table 4 shows the performance of the syntagmatic kernel in different configurations: hard and soft matching. As a baseline, we report the result of a standard approach consisting of explicit bigrams and trigrams of words and part-of-speech tags around the words to be disambiguated (Yarowsky 1994). We evaluated the impact of the domain kernel on the overall performance by comparing the learning curves of Kwsd and Kwsd on the four lexical-sample tasks. Figure 1 shows the results of our experiments. The points of the learning curves are obtained by sampling the same percentage of training examples for Table 2 Description of the lexical-sample tasks of Senseval-3. Task #w mean polysemy #train #test #unlab Catalan 27 3.11 4,469 2,253 23,935 English 57 6.47 7,860 3,944 - Italian 45 6.30 5,145 2,439 74,788 Spanish 46 3.30 8,430 4,195 61,252 522 Giuliano, Gliozzo, and Strapparava Kernel Metho</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>Yarowsky, David. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL 1994), pages 88–95, Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>Some experiments with a naive Bayes WSD system.</title>
<date>2004</date>
<booktitle>In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>265--268</pages>
<location>Barcelona.</location>
<contexts>
<context position="38983" citStr="Yuret 2004" startWordPosition="6376" endWordPosition="6377">and mean polysemy for each partition are also reported. Range of training examples 1–10 11–50 51–100 101–200 201+ ˆKsem 76.1 70.8 54.2 67.4 60.0 wsd MF 73.5 66.4 49.4 63.2 53.0 Mean polysemy 3 5 7 9 15 SemCor, and English lexical-sample and all-words data sets taken from all the previous editions of Senseval. The system proposed by Mihalcea and Faruque (2004) scored second (64.6). The dimension of their training set is comparable to ours; however, they also use additional information drawn from WordNet to derive semantic generalizations using syntactic dependencies. Finally, the third system (Yuret 2004) obtained 64.1 using a larger training data set (Semcor, DSO corpus of sense-tagged English, OpenMind Word Expert, Senseval-2, and Senseval-3 lexical-sample tasks). The small difference between the two domain models seems to indicate that a limited amount of unlabeled data is sufficient to improve the overall performance, and the use of unlabeled data taken from the training set helps to slightly improve the overall performance. However, the domain model can be acquired from a different corpus (e.g., the BNC) without significantly affecting the overall performance. Finally, the results reporte</context>
</contexts>
<marker>Yuret, 2004</marker>
<rawString>Yuret, Deniz. 2004. Some experiments with a naive Bayes WSD system. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 265–268, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>419--426</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="26924" citStr="Zhao and Grishman 2005" startWordPosition="4426" endWordPosition="4429">site Kernel Having defined all the individual kernels representing syntagmatic and domain aspects of sense distinction, we can define the composite kernel to combine and extend the individual kernels. The closure properties of the kernel functions allows us to define the composite kernel as KC(xi, xj) = n Kl(xi,xj) (19) l=1 \/Kl(xj, xj)Kl(xi, xi) where Kl is a valid individual kernel. The individual kernels are normalized—this plays an important role in allowing us to integrate information from heterogeneous feature spaces. Recent work (Moschitti 2004; Gliozzo, Giuliano, and Strapparava 2005; Zhao and Grishman 2005; Giuliano, Lavelli, and Romano 2006) has empirically shown the effectiveness of combining kernels in this way: The composite kernel consistently improves the performance of the individual ones. In addition, this formulation allows us to evaluate the individual contribution of each information source. In order to show the effectiveness of the proposed domain model in supervised learning, we defined two WSD kernels, Kwsd and K&apos;wsd. They are completely specified by the n individual kernels that compose them in Equation (19). Kwsd is composed by KColl, KPoS, and KBoW; K&apos;wsd is composed by KColl, </context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Zhao, Shubin and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 419–426, Ann Arbor, MI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>