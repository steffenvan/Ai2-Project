<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.048471">
<title confidence="0.570513">
Wide Coverage Symbolic Surface Realization
</title>
<note confidence="0.555726666666667">
Charles B. Callaway
Istituto per la Ricerca Scientifica e Tecnologica
Istituto Trentino di Cultura, Italy (ITC-irst)
</note>
<email confidence="0.955523">
callaway@itc.it
</email>
<sectionHeader confidence="0.994614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999752083333333">
Recent evaluation techniques applied to corpus-
based systems have been introduced that can
predict quantitatively how well surface realizers
will generate unseen sentences in isolation. We
introduce a similar method for determining the
coverage on the Fuf/Surge symbolic surface re-
alizer, report that its coverage and accuracy on
the Penn TreeBank is higher than that of a sim-
ilar statistics-based generator, describe several
benefits that can be used in other areas of com-
putational linguistics, and present an updated
version of Surge for use in the NLG community.
</bodyText>
<sectionHeader confidence="0.997896" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997547368421">
Surface realization is the process of converting
the semantic and syntactic representation of a
sentence or series of sentences into the text, or
surface form, of a particular language (Elhadad,
1991; Bateman, 1995). Most surface realiz-
ers have been symbolic, grammar-based systems
using syntactic linguistic theories like HPSG.
These systems were often developed as either
proof-of-concept implementations or to support
larger end-to-end NLG systems which have pro-
duced limited amounts of domain-specific texts.
As such, determining the generic coverage of
a language has been substituted by the goal of
producing the necessary syntactic coverage for a
particular project. As described in (Langkilde-
Geary, 2002), the result has been the use of
regression testing with hand-picked examples
rather than broad evaluations of linguistic com-
petence. Instead, large syntactically annotated
corpora such as the Penn TreeBank (Marcus et
al., 1993) have allowed statistically based sys-
tems to produce large quantities of sentences
and then more objectively determine generation
coverage with automatic evaluation measures.
We conducted a similar corpus-based exper-
iment (Callaway, 2003) with the FUF/SURGE
symbolic surface realizer (Elhadad, 1991). We
describe a direct comparison with HALOGEN
(Langkilde-Geary, 2002) using Section 23 of
the TreeBank, showing that the symbolic ap-
proach improves upon the statistical system in
both coverage and accuracy. We also present
a longitudinal comparison of two versions of
FUF/SURGE showing a significant improvement
in its coverage and accuracy after new gram-
mar and morphology rules were added. This
improved version of SURGE is available for use
in the NLG community.
</bodyText>
<sectionHeader confidence="0.992015" genericHeader="related work">
2 Related Work in Wide Coverage
Generation
</sectionHeader>
<bodyText confidence="0.994067766666667">
Verifying wide coverage generation depends on
(1) a large, well structured corpus, (2) a trans-
formation algorithm that converts annotated
sentences into the surface realizer&apos;s expected in-
put form, (3) the surface realizer itself, and (4)
an automatic metric for determining the accu-
racy of the generated sentences. Large, well
structured, syntactically marked corpora such
as the Penn TreeBank (Marcus et al., 1993) can
provide a source of example sentences, while au-
tomatic metrics like simple string accuracy are
capable of giving a fast, rough estimate of qual-
ity for individual sentences.
Realization of text from corpora has been ap-
proached in several ways. In the case of Rat-
naparkhi&apos;s generator for flight information in
the air travel domain (Ratnaparkhi, 2000), the
transformation algorithm is trivial as the gen-
erator uses the corpus itself (annotated with se-
mantic information such as destination or flight
number) as input to a surface realizer with an
n-gram model of the domain, along with a max-
imum entropy probability model for selecting
when to use which phrase.
FERGUS (Bangalore and Rambow, 2000) used
the Penn TreeBank as a corpus, requiring
a more substantial transformation algorithm
since it requires a lexical predicate-argument
structure instead of the TreeBank&apos;s represen-
tation. The system uses an underlying tree-
</bodyText>
<equation confidence="0.893891857142857">
(S (NP-SBJ ((cat clause)
(NP (JJ overall) (process ((type ascriptive) (tense past)))
(NNS sales))) (participants
(VP (VBD were) ((carrier ((cat common) (lex &amp;quot;sale&amp;quot;) (number plural)
(ADJP-PRD (describer ((cat adj) (lex &amp;quot;overall&amp;quot;)))))
(RB roughly) (attribute ((cat ap) (lex &amp;quot;flat&amp;quot;)
(JJ flat)))) (modifier ((cat adv) (lex &amp;quot;roughly&amp;quot;)))))))
</equation>
<figureCaption confidence="0.999414">
Figure 1: A Penn TreeBank Sentence and Corresponding SURGE Input Representation
</figureCaption>
<bodyText confidence="0.9999926875">
based syntactic model to generate a set of pos-
sible candidate realizations, and then chooses
the best candidate with a trigram model of the
Treebank text. An evaluation of three versions
of FERGUS on randomly chosen Wall Street
Journal sentences of the TreeBank showed sim-
ple string accuracy up to 58.9%.
Finally, Langkilde&apos;s work on HALOGEN
(Langkilde-Geary, 2002) uses a rewriting algo-
rithm to convert the syntactically annotated
sentences from the TreeBank into a semantic in-
put notation via rewrite rules. The system uses
the transformed semantic input to create mil-
lions of possible realizations (most of which are
grammatical but unwieldy) in a lattice struc-
ture and then also uses n-grams to select the
most probable as its output sentence. Langk-
ilde evaluated the system using the standard
train-and-test methodology with Section 23 of
the TreeBank as the unseen set.
These systems represent a statistical ap-
proach to wide coverage realization, turning to
automatic methods to evaluate coverage and
quality based on corpus statistics. However, a
symbolic realizer can use the same evaluation
technique if a method exists to transform the
corpus annotation into the realizer&apos;s input rep-
resentation. Thus symbolic realizers can also
use the same types of evaluations employed by
the parsing and MT communities, allowing for
meaningful comparisons of their performance on
metrics such as coverage and accuracy.
</bodyText>
<sectionHeader confidence="0.97891" genericHeader="method">
3 The Penn TreeBank
</sectionHeader>
<bodyText confidence="0.999071866666667">
The Penn TreeBank (Marcus et al., 1993) is a
large set of sentences bracketed for syntactic de-
pendency and part of speech, covering almost 5
million words of text. The corpus is divided into
24 sections, with each section having on average
2000 sentences. The representation of an exam-
ple sentence is shown at the left of Figure 1.
In general, many sentences contained in the
TreeBank are not typical of those produced by
current NLG systems. For instance, newspaper
text requires extensive quoting for conveying di-
alogue, special formatting for stock reports, and
methods for dealing with contractions. These
types of constructions are not available in cur-
rent general purpose, rule-based generators:
</bodyText>
<listItem confidence="0.93329835">
• Direct and indirect quotations from re-
porters&apos; interviews (Callaway, 2001):
&amp;quot;It&apos;s turning out to be a real blockbuster,&amp;quot;
Mr. Sweig said.
• Incomplete quotations:
Then retailers &amp;quot;will probably push them
out altogether,&amp;quot; he says.
• Simple lists of facts from stock reports:
8 13/16% high, 8 1/2% low, 8 5/8% near
closing bid, 8 3/4% offered.
• Both formal and informal language:
You&apos;ve either got a chair or you don&apos;t.
• A variety of punctuation mixed with text:
$55,730,000 of school financing bonds,
1989 Series B (1987 resolution).
• Combinations of infrequent syntactic rules:
Then how should we think about service?
• Irregular and rare words:
&amp;quot;I was upset with Roger, I fumpered and
schmumpered,&amp;quot; says Mr. Peters.
</listItem>
<bodyText confidence="0.999927647058824">
By adding rules for these phenomena, NLG
realizers can significantly increase their cover-
age. For instance, approximately 15% of Penn
TreeBank sentences contain either direct, indi-
rect or incomplete written dialogue. Thus for a
newspaper domain, excluding dialogue from the
grammar greatly limits potential coverage. Fur-
thermore, using a corpus for testing a surface
realizer is akin to having a very large regression
test set, with the added benefit of being able to
robustly generate real-world sentences.
In order to compare a symbolic surface real-
izer with its statistical counterparts, we tested
an enhanced version of an off-the-shelf symbolic
generation system, the FUF/SURGE (Elhadad,
1991) surface realizer. To obtain a meaningful
comparison, we utilized the same approach as
</bodyText>
<table confidence="0.99946525">
Realizer Sentences Coverage Matches Covered Matches Total Matches Accuracy
SURGE 2.2 2416 48.1% 102 8.8% 4.2% 0.8542
SURGE+ 2416 98.9% 1474 61.7% 61.0% 0.9483
HALOGEN 2416 83.3% 1157 57.5% 47.9% 0.9450
</table>
<tableCaption confidence="0.99996">
Table 1: Comparing two SURGE versions with HALOGEN [Langkilde 2002].
</tableCaption>
<bodyText confidence="0.998134">
HALOGEN, treating Section 23 of the Treebank
as an unseen test set. We created an analo-
gous transformation algorithm (Callaway, 2003)
to convert TreeBank sentences into the SURGE
representation (Figure 1), which are then given
to the symbolic surface realizer, allowing us to
measure both coverage and accuracy.
</bodyText>
<sectionHeader confidence="0.91206" genericHeader="method">
4 Coverage and Accuracy Evaluation
</sectionHeader>
<bodyText confidence="0.99706156">
Of the three statistical systems presented above,
only (Langkilde-Geary, 2002) used a standard,
recoverable method for replicating the gener-
ation experiment. Because of the sheer num-
ber of sentences (2416), and to enable a direct
comparison with HALOGEN, we similarly used
the simple string accuracy (Doddington, 2002),
where the smallest number of Adds, Deletions,
and Insertions were used to calculate accuracy:
1 - (A + D + I) / #Characters.
Unlike typical statistical and machine learn-
ing experiments, the grammar was &amp;quot;trained&amp;quot; by
hand, though the evaluation of the resulting
sentences was performed automatically. This
resulted in numerous generalized syntactic and
morphology rules being added to the SURGE
grammar, as well as specialized rules pertain-
ing to specific domain elements from the texts.
Table 1 shows a comparative coverage and
accuracy analysis of three surface realizers on
Section 23 of the Penn TreeBank: the original
SURGE 2.2 distribution, our modified version of
SURGE, and the HALOGEN system described in
(Langkilde-Geary, 2002). The surface realizers
are measured in terms of:
</bodyText>
<listItem confidence="0.98807225">
• Coverage: The number of sentences for
which the realizer returned a recognizable
string rather than failure or an error.
• Matches: The number of identical sen-
tences (including punctuation/capitals).
• Percent of covered matches: How often the
realizer returned a sentence match given
that a sentence is produced.
• Percent of matches for all sentences: A
measure of matches from all inputs, which
penalizes systems that improve accuracy
at the expense of coverage (Matches /
2416, or Coverage * Covered Matches).
• Accuracy: The aggregate simple string ac-
curacy score for all covered sentences (as
opposed to the entire sentence set).
</listItem>
<bodyText confidence="0.999917333333333">
The first thing to note is the drastic improve-
ment between the two versions of SURGE. As
the analysis in Section 3 showed, studying the
elements of a particular domain are very impor-
tant in determining what parts of a grammar
should be improved. For instance, the TreeBank
contains many constructions which are not han-
dled by SURGE 2.2, such as quotations, which
account for 15% of the sentences. When SURGE
2.2 encounters a quotation, it fails to produce a
text string, accounting for a large chunk of the
sentences not covered (51.9% compared to 1.1%
for our enhanced version of SURGE).
Additionally, a number of morphology en-
hancements, such as contractions and punctua-
tion placement contributed to the much higher
percentage of exact matches. While some of
these are domain-specific, many are broader
generalizations which although useful, were not
included in the original grammar because they
were not encountered in previous domains or
arose only in complex sentences.
On all four measures the enhanced version of
SURGE performed much better than the statisti-
cal approach to surface realization embodied in
HALOGEN. The accuracy measure is especially
surprising given that statistical and machine
learning approaches employ maximization algo-
rithms to ensure that grammar rules are chosen
to get the highest possible accuracy. However,
given that the difference in accuracy from Surge
2.2 is relatively small while its quality is obvi-
ously poor, using such accuracy measures alone
is a bad way to compare surface realizers.
Finally, the coverage difference between the
enhanced version of SURGE and that of HALO-
GEN is especially striking. Some explanations
may be that statistical systems are not yet capa-
ble of handling certain linguistic phenomena like
long-distance dependencies (due to n-gram ap-
proaches), or given that statistical systems are
typically robust and very unlikely to produce no
output, that there were problems in the trans-
formation algorithm that converted individual
sentence representations from the corpus.
</bodyText>
<sectionHeader confidence="0.962177" genericHeader="method">
5 Additional Benefits
</sectionHeader>
<bodyText confidence="0.99997040625">
The evaluation approach presented here has
other advantages besides calculating the cover-
age and accuracy of a grammar. For instance,
in realizers where linguists must add new lexical
resources by hand, such a system allows them
to generate text by first creating sample sen-
tences in the more familiar TreeBank notation.
Sentences could also be directly generated by
feeding an example text to a parser capable of
producing TreeBank structures. This would be
especially useful in new domains to quickly see
what new specialized syntax they might need.
Additionally, the transformation program can
be used as an error-checker to assist in anno-
tating sentences in a new corpus. Rules could
be (and have been) added alongside the normal
transformation rules that detect when errors are
encountered, categorize them, and make them
available to the corpus creator for correction.
This can extend beyond the syntax level, de-
tecting even morphology errors such as incorrect
verbs, typos, or dialect differences.
Finally, such an approach can help test pars-
ing systems without the need for the time-
consuming process of annotating corpora in the
first place. If a parser creates a TreeBank repre-
sentation for a sentence, the generation system
can then attempt to regenerate that same sen-
tence automatically. Exact matches are highly
likely to have been correctly parsed, and more
time can be spent locating and resolving parses
that returned very low accuracy scores.
</bodyText>
<sectionHeader confidence="0.996953" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999938136363636">
Recent statistical systems for generation have
focused on surface realizers, offering robust-
ness, wide coverage, and domain- and language-
independence given certain resources. This pa-
per represents the analogous effort for a sym-
bolic generation system using an enhanced ver-
sion of the FUF/SURGE systemic realizer. We
presented a grammatical coverage and accu-
racy experiment showing the symbolic system
had a much higher level of coverage of English
and better accuracy as represented by the Penn
TreeBank. The improved SURGE grammar, ver-
sion 2.4, will be made freely available to the
NLG community.
While we feel that both coverage and accu-
racy could be improved even more, additional
gains would not imply a substantial improve-
ment in the quality of the grammar itself. The
reason is that most problems affecting accuracy
lie in transforming the TreeBank representation
as opposed to the grammar, which has remained
relatively stable.
</bodyText>
<sectionHeader confidence="0.998225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998748069767442">
S. Bangalore and O. Rambow. 2000. Exploiting
a probabilistic hierarchical model for genera-
tion. In COLING-2000: Proceedings of the
18th International Conference on Computa-
tional Linguistics, Saarbruecken, Germany.
John A. Bateman. 1995. KPML: The KOMET-
penman (multilingual) development environ-
ment. Technical Report Release 0.8, Insti-
tut fur Integrierte Publikations- und Informa-
tionssysteme (IPSI), GMD, Darmstadt.
Charles Callaway. 2001. A computational fea-
ture analysis for multilingual character-to-
character dialogue. In Proceedings of the Sec-
ond International Conference on Intelligent
Text Processing and Computational Linguis-
tics, pages 251{264, Mexico City, Mexico.
Charles B. Callaway. 2003. Evaluating coverage
for large symbolic NLG grammars. In Pro-
ceedings of the Eighteenth International Joint
Conference on Artificial Intelligence, pages
811{817, Acapulco, Mexico, August.
George Doddington. 2002. Automatic evalua-
tion of machine translation quality using n-
gram co-occurrence statistics. In Proceedings
of the 2002 Conference on Human Language
Technology, San Diego, CA, March.
Michael Elhadad. 1991. FUF: The universal
unifier user manual version 5.0. Technical Re-
port CUCS-038-91, Dept. of Computer Sci-
ence, Columbia University.
Irene Langkilde-Geary. 2002. An empirical ver-
ification of coverage and correctness for a
general-purpose sentence generator. In Sec-
ond International Natural Language Genera-
tion Conference, Harriman, NY, July.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of
English: The PennTreeBank. Computational
Linguistics, 26(2).
Adwait Ratnaparkhi. 2000. Trainable meth-
ods for surface natural language generation.
In Proceedings of the First North American
Conference of the ACL, Seattle, WA, May.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.322044">
<title confidence="0.999914">Wide Coverage Symbolic Surface Realization</title>
<author confidence="0.997431">Charles B Callaway</author>
<affiliation confidence="0.450694">Istituto per la Ricerca Scientifica e Tecnologica Istituto Trentino di Cultura, Italy (ITC-irst)</affiliation>
<email confidence="0.954702">callaway@itc.it</email>
<abstract confidence="0.995841">Recent evaluation techniques applied to corpusbased systems have been introduced that can predict quantitatively how well surface realizers will generate unseen sentences in isolation. We introduce a similar method for determining the coverage on the Fuf/Surge symbolic surface realizer, report that its coverage and accuracy on the Penn TreeBank is higher than that of a similar statistics-based generator, describe several benefits that can be used in other areas of computational linguistics, and present an updated version of Surge for use in the NLG community.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>O Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In COLING-2000: Proceedings of the 18th International Conference on Computational Linguistics, Saarbruecken,</booktitle>
<location>Germany.</location>
<contexts>
<context position="3630" citStr="Bangalore and Rambow, 2000" startWordPosition="545" endWordPosition="548">simple string accuracy are capable of giving a fast, rough estimate of quality for individual sentences. Realization of text from corpora has been approached in several ways. In the case of Ratnaparkhi&apos;s generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. FERGUS (Bangalore and Rambow, 2000) used the Penn TreeBank as a corpus, requiring a more substantial transformation algorithm since it requires a lexical predicate-argument structure instead of the TreeBank&apos;s representation. The system uses an underlying tree(S (NP-SBJ ((cat clause) (NP (JJ overall) (process ((type ascriptive) (tense past))) (NNS sales))) (participants (VP (VBD were) ((carrier ((cat common) (lex &amp;quot;sale&amp;quot;) (number plural) (ADJP-PRD (describer ((cat adj) (lex &amp;quot;overall&amp;quot;))))) (RB roughly) (attribute ((cat ap) (lex &amp;quot;flat&amp;quot;) (JJ flat)))) (modifier ((cat adv) (lex &amp;quot;roughly&amp;quot;))))))) Figure 1: A Penn TreeBank Sentence and C</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>S. Bangalore and O. Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In COLING-2000: Proceedings of the 18th International Conference on Computational Linguistics, Saarbruecken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bateman</author>
</authors>
<title>KPML: The KOMETpenman (multilingual) development environment.</title>
<date>1995</date>
<booktitle>Institut fur Integrierte Publikations- und Informationssysteme (IPSI), GMD,</booktitle>
<tech>Technical Report Release 0.8,</tech>
<location>Darmstadt.</location>
<contexts>
<context position="981" citStr="Bateman, 1995" startWordPosition="144" endWordPosition="145">n. We introduce a similar method for determining the coverage on the Fuf/Surge symbolic surface realizer, report that its coverage and accuracy on the Penn TreeBank is higher than that of a similar statistics-based generator, describe several benefits that can be used in other areas of computational linguistics, and present an updated version of Surge for use in the NLG community. 1 Introduction Surface realization is the process of converting the semantic and syntactic representation of a sentence or series of sentences into the text, or surface form, of a particular language (Elhadad, 1991; Bateman, 1995). Most surface realizers have been symbolic, grammar-based systems using syntactic linguistic theories like HPSG. These systems were often developed as either proof-of-concept implementations or to support larger end-to-end NLG systems which have produced limited amounts of domain-specific texts. As such, determining the generic coverage of a language has been substituted by the goal of producing the necessary syntactic coverage for a particular project. As described in (LangkildeGeary, 2002), the result has been the use of regression testing with hand-picked examples rather than broad evaluat</context>
</contexts>
<marker>Bateman, 1995</marker>
<rawString>John A. Bateman. 1995. KPML: The KOMETpenman (multilingual) development environment. Technical Report Release 0.8, Institut fur Integrierte Publikations- und Informationssysteme (IPSI), GMD, Darmstadt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Callaway</author>
</authors>
<title>A computational feature analysis for multilingual character-tocharacter dialogue.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>251--264</pages>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="6484" citStr="Callaway, 2001" startWordPosition="985" endWordPosition="986"> The corpus is divided into 24 sections, with each section having on average 2000 sentences. The representation of an example sentence is shown at the left of Figure 1. In general, many sentences contained in the TreeBank are not typical of those produced by current NLG systems. For instance, newspaper text requires extensive quoting for conveying dialogue, special formatting for stock reports, and methods for dealing with contractions. These types of constructions are not available in current general purpose, rule-based generators: • Direct and indirect quotations from reporters&apos; interviews (Callaway, 2001): &amp;quot;It&apos;s turning out to be a real blockbuster,&amp;quot; Mr. Sweig said. • Incomplete quotations: Then retailers &amp;quot;will probably push them out altogether,&amp;quot; he says. • Simple lists of facts from stock reports: 8 13/16% high, 8 1/2% low, 8 5/8% near closing bid, 8 3/4% offered. • Both formal and informal language: You&apos;ve either got a chair or you don&apos;t. • A variety of punctuation mixed with text: $55,730,000 of school financing bonds, 1989 Series B (1987 resolution). • Combinations of infrequent syntactic rules: Then how should we think about service? • Irregular and rare words: &amp;quot;I was upset with Roger, I </context>
</contexts>
<marker>Callaway, 2001</marker>
<rawString>Charles Callaway. 2001. A computational feature analysis for multilingual character-tocharacter dialogue. In Proceedings of the Second International Conference on Intelligent Text Processing and Computational Linguistics, pages 251{264, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles B Callaway</author>
</authors>
<title>Evaluating coverage for large symbolic NLG grammars.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>811--817</pages>
<location>Acapulco, Mexico,</location>
<contexts>
<context position="1944" citStr="Callaway, 2003" startWordPosition="281" endWordPosition="282">guage has been substituted by the goal of producing the necessary syntactic coverage for a particular project. As described in (LangkildeGeary, 2002), the result has been the use of regression testing with hand-picked examples rather than broad evaluations of linguistic competence. Instead, large syntactically annotated corpora such as the Penn TreeBank (Marcus et al., 1993) have allowed statistically based systems to produce large quantities of sentences and then more objectively determine generation coverage with automatic evaluation measures. We conducted a similar corpus-based experiment (Callaway, 2003) with the FUF/SURGE symbolic surface realizer (Elhadad, 1991). We describe a direct comparison with HALOGEN (Langkilde-Geary, 2002) using Section 23 of the TreeBank, showing that the symbolic approach improves upon the statistical system in both coverage and accuracy. We also present a longitudinal comparison of two versions of FUF/SURGE showing a significant improvement in its coverage and accuracy after new grammar and morphology rules were added. This improved version of SURGE is available for use in the NLG community. 2 Related Work in Wide Coverage Generation Verifying wide coverage gener</context>
<context position="8317" citStr="Callaway, 2003" startWordPosition="1273" endWordPosition="1274">al counterparts, we tested an enhanced version of an off-the-shelf symbolic generation system, the FUF/SURGE (Elhadad, 1991) surface realizer. To obtain a meaningful comparison, we utilized the same approach as Realizer Sentences Coverage Matches Covered Matches Total Matches Accuracy SURGE 2.2 2416 48.1% 102 8.8% 4.2% 0.8542 SURGE+ 2416 98.9% 1474 61.7% 61.0% 0.9483 HALOGEN 2416 83.3% 1157 57.5% 47.9% 0.9450 Table 1: Comparing two SURGE versions with HALOGEN [Langkilde 2002]. HALOGEN, treating Section 23 of the Treebank as an unseen test set. We created an analogous transformation algorithm (Callaway, 2003) to convert TreeBank sentences into the SURGE representation (Figure 1), which are then given to the symbolic surface realizer, allowing us to measure both coverage and accuracy. 4 Coverage and Accuracy Evaluation Of the three statistical systems presented above, only (Langkilde-Geary, 2002) used a standard, recoverable method for replicating the generation experiment. Because of the sheer number of sentences (2416), and to enable a direct comparison with HALOGEN, we similarly used the simple string accuracy (Doddington, 2002), where the smallest number of Adds, Deletions, and Insertions were </context>
</contexts>
<marker>Callaway, 2003</marker>
<rawString>Charles B. Callaway. 2003. Evaluating coverage for large symbolic NLG grammars. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence, pages 811{817, Acapulco, Mexico, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using ngram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Human Language Technology,</booktitle>
<location>San Diego, CA,</location>
<contexts>
<context position="8849" citStr="Doddington, 2002" startWordPosition="1352" endWordPosition="1353">an unseen test set. We created an analogous transformation algorithm (Callaway, 2003) to convert TreeBank sentences into the SURGE representation (Figure 1), which are then given to the symbolic surface realizer, allowing us to measure both coverage and accuracy. 4 Coverage and Accuracy Evaluation Of the three statistical systems presented above, only (Langkilde-Geary, 2002) used a standard, recoverable method for replicating the generation experiment. Because of the sheer number of sentences (2416), and to enable a direct comparison with HALOGEN, we similarly used the simple string accuracy (Doddington, 2002), where the smallest number of Adds, Deletions, and Insertions were used to calculate accuracy: 1 - (A + D + I) / #Characters. Unlike typical statistical and machine learning experiments, the grammar was &amp;quot;trained&amp;quot; by hand, though the evaluation of the resulting sentences was performed automatically. This resulted in numerous generalized syntactic and morphology rules being added to the SURGE grammar, as well as specialized rules pertaining to specific domain elements from the texts. Table 1 shows a comparative coverage and accuracy analysis of three surface realizers on Section 23 of the Penn </context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using ngram co-occurrence statistics. In Proceedings of the 2002 Conference on Human Language Technology, San Diego, CA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
</authors>
<title>FUF: The universal unifier user manual version 5.0.</title>
<date>1991</date>
<tech>Technical Report CUCS-038-91,</tech>
<institution>Dept. of Computer Science, Columbia University.</institution>
<contexts>
<context position="965" citStr="Elhadad, 1991" startWordPosition="142" endWordPosition="143">ces in isolation. We introduce a similar method for determining the coverage on the Fuf/Surge symbolic surface realizer, report that its coverage and accuracy on the Penn TreeBank is higher than that of a similar statistics-based generator, describe several benefits that can be used in other areas of computational linguistics, and present an updated version of Surge for use in the NLG community. 1 Introduction Surface realization is the process of converting the semantic and syntactic representation of a sentence or series of sentences into the text, or surface form, of a particular language (Elhadad, 1991; Bateman, 1995). Most surface realizers have been symbolic, grammar-based systems using syntactic linguistic theories like HPSG. These systems were often developed as either proof-of-concept implementations or to support larger end-to-end NLG systems which have produced limited amounts of domain-specific texts. As such, determining the generic coverage of a language has been substituted by the goal of producing the necessary syntactic coverage for a particular project. As described in (LangkildeGeary, 2002), the result has been the use of regression testing with hand-picked examples rather th</context>
<context position="7826" citStr="Elhadad, 1991" startWordPosition="1198" endWordPosition="1199">coverage. For instance, approximately 15% of Penn TreeBank sentences contain either direct, indirect or incomplete written dialogue. Thus for a newspaper domain, excluding dialogue from the grammar greatly limits potential coverage. Furthermore, using a corpus for testing a surface realizer is akin to having a very large regression test set, with the added benefit of being able to robustly generate real-world sentences. In order to compare a symbolic surface realizer with its statistical counterparts, we tested an enhanced version of an off-the-shelf symbolic generation system, the FUF/SURGE (Elhadad, 1991) surface realizer. To obtain a meaningful comparison, we utilized the same approach as Realizer Sentences Coverage Matches Covered Matches Total Matches Accuracy SURGE 2.2 2416 48.1% 102 8.8% 4.2% 0.8542 SURGE+ 2416 98.9% 1474 61.7% 61.0% 0.9483 HALOGEN 2416 83.3% 1157 57.5% 47.9% 0.9450 Table 1: Comparing two SURGE versions with HALOGEN [Langkilde 2002]. HALOGEN, treating Section 23 of the Treebank as an unseen test set. We created an analogous transformation algorithm (Callaway, 2003) to convert TreeBank sentences into the SURGE representation (Figure 1), which are then given to the symbolic</context>
</contexts>
<marker>Elhadad, 1991</marker>
<rawString>Michael Elhadad. 1991. FUF: The universal unifier user manual version 5.0. Technical Report CUCS-038-91, Dept. of Computer Science, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde-Geary</author>
</authors>
<title>An empirical verification of coverage and correctness for a general-purpose sentence generator.</title>
<date>2002</date>
<booktitle>In Second International Natural Language Generation Conference,</booktitle>
<location>Harriman, NY,</location>
<contexts>
<context position="2075" citStr="Langkilde-Geary, 2002" startWordPosition="298" endWordPosition="299">in (LangkildeGeary, 2002), the result has been the use of regression testing with hand-picked examples rather than broad evaluations of linguistic competence. Instead, large syntactically annotated corpora such as the Penn TreeBank (Marcus et al., 1993) have allowed statistically based systems to produce large quantities of sentences and then more objectively determine generation coverage with automatic evaluation measures. We conducted a similar corpus-based experiment (Callaway, 2003) with the FUF/SURGE symbolic surface realizer (Elhadad, 1991). We describe a direct comparison with HALOGEN (Langkilde-Geary, 2002) using Section 23 of the TreeBank, showing that the symbolic approach improves upon the statistical system in both coverage and accuracy. We also present a longitudinal comparison of two versions of FUF/SURGE showing a significant improvement in its coverage and accuracy after new grammar and morphology rules were added. This improved version of SURGE is available for use in the NLG community. 2 Related Work in Wide Coverage Generation Verifying wide coverage generation depends on (1) a large, well structured corpus, (2) a transformation algorithm that converts annotated sentences into the sur</context>
<context position="4635" citStr="Langkilde-Geary, 2002" startWordPosition="693" endWordPosition="694">ale&amp;quot;) (number plural) (ADJP-PRD (describer ((cat adj) (lex &amp;quot;overall&amp;quot;))))) (RB roughly) (attribute ((cat ap) (lex &amp;quot;flat&amp;quot;) (JJ flat)))) (modifier ((cat adv) (lex &amp;quot;roughly&amp;quot;))))))) Figure 1: A Penn TreeBank Sentence and Corresponding SURGE Input Representation based syntactic model to generate a set of possible candidate realizations, and then chooses the best candidate with a trigram model of the Treebank text. An evaluation of three versions of FERGUS on randomly chosen Wall Street Journal sentences of the TreeBank showed simple string accuracy up to 58.9%. Finally, Langkilde&apos;s work on HALOGEN (Langkilde-Geary, 2002) uses a rewriting algorithm to convert the syntactically annotated sentences from the TreeBank into a semantic input notation via rewrite rules. The system uses the transformed semantic input to create millions of possible realizations (most of which are grammatical but unwieldy) in a lattice structure and then also uses n-grams to select the most probable as its output sentence. Langkilde evaluated the system using the standard train-and-test methodology with Section 23 of the TreeBank as the unseen set. These systems represent a statistical approach to wide coverage realization, turning to a</context>
<context position="8609" citStr="Langkilde-Geary, 2002" startWordPosition="1315" endWordPosition="1316"> SURGE 2.2 2416 48.1% 102 8.8% 4.2% 0.8542 SURGE+ 2416 98.9% 1474 61.7% 61.0% 0.9483 HALOGEN 2416 83.3% 1157 57.5% 47.9% 0.9450 Table 1: Comparing two SURGE versions with HALOGEN [Langkilde 2002]. HALOGEN, treating Section 23 of the Treebank as an unseen test set. We created an analogous transformation algorithm (Callaway, 2003) to convert TreeBank sentences into the SURGE representation (Figure 1), which are then given to the symbolic surface realizer, allowing us to measure both coverage and accuracy. 4 Coverage and Accuracy Evaluation Of the three statistical systems presented above, only (Langkilde-Geary, 2002) used a standard, recoverable method for replicating the generation experiment. Because of the sheer number of sentences (2416), and to enable a direct comparison with HALOGEN, we similarly used the simple string accuracy (Doddington, 2002), where the smallest number of Adds, Deletions, and Insertions were used to calculate accuracy: 1 - (A + D + I) / #Characters. Unlike typical statistical and machine learning experiments, the grammar was &amp;quot;trained&amp;quot; by hand, though the evaluation of the resulting sentences was performed automatically. This resulted in numerous generalized syntactic and morphol</context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>Irene Langkilde-Geary. 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator. In Second International Natural Language Generation Conference, Harriman, NY, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The PennTreeBank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="1706" citStr="Marcus et al., 1993" startWordPosition="247" endWordPosition="250">ke HPSG. These systems were often developed as either proof-of-concept implementations or to support larger end-to-end NLG systems which have produced limited amounts of domain-specific texts. As such, determining the generic coverage of a language has been substituted by the goal of producing the necessary syntactic coverage for a particular project. As described in (LangkildeGeary, 2002), the result has been the use of regression testing with hand-picked examples rather than broad evaluations of linguistic competence. Instead, large syntactically annotated corpora such as the Penn TreeBank (Marcus et al., 1993) have allowed statistically based systems to produce large quantities of sentences and then more objectively determine generation coverage with automatic evaluation measures. We conducted a similar corpus-based experiment (Callaway, 2003) with the FUF/SURGE symbolic surface realizer (Elhadad, 1991). We describe a direct comparison with HALOGEN (Langkilde-Geary, 2002) using Section 23 of the TreeBank, showing that the symbolic approach improves upon the statistical system in both coverage and accuracy. We also present a longitudinal comparison of two versions of FUF/SURGE showing a significant </context>
<context position="2930" citStr="Marcus et al., 1993" startWordPosition="430" endWordPosition="433">ovement in its coverage and accuracy after new grammar and morphology rules were added. This improved version of SURGE is available for use in the NLG community. 2 Related Work in Wide Coverage Generation Verifying wide coverage generation depends on (1) a large, well structured corpus, (2) a transformation algorithm that converts annotated sentences into the surface realizer&apos;s expected input form, (3) the surface realizer itself, and (4) an automatic metric for determining the accuracy of the generated sentences. Large, well structured, syntactically marked corpora such as the Penn TreeBank (Marcus et al., 1993) can provide a source of example sentences, while automatic metrics like simple string accuracy are capable of giving a fast, rough estimate of quality for individual sentences. Realization of text from corpora has been approached in several ways. In the case of Ratnaparkhi&apos;s generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum e</context>
<context position="5745" citStr="Marcus et al., 1993" startWordPosition="866" endWordPosition="869">k as the unseen set. These systems represent a statistical approach to wide coverage realization, turning to automatic methods to evaluate coverage and quality based on corpus statistics. However, a symbolic realizer can use the same evaluation technique if a method exists to transform the corpus annotation into the realizer&apos;s input representation. Thus symbolic realizers can also use the same types of evaluations employed by the parsing and MT communities, allowing for meaningful comparisons of their performance on metrics such as coverage and accuracy. 3 The Penn TreeBank The Penn TreeBank (Marcus et al., 1993) is a large set of sentences bracketed for syntactic dependency and part of speech, covering almost 5 million words of text. The corpus is divided into 24 sections, with each section having on average 2000 sentences. The representation of an example sentence is shown at the left of Figure 1. In general, many sentences contained in the TreeBank are not typical of those produced by current NLG systems. For instance, newspaper text requires extensive quoting for conveying dialogue, special formatting for stock reports, and methods for dealing with contractions. These types of constructions are no</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The PennTreeBank. Computational Linguistics, 26(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Trainable methods for surface natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the First North American Conference of the ACL,</booktitle>
<location>Seattle, WA,</location>
<contexts>
<context position="3284" citStr="Ratnaparkhi, 2000" startWordPosition="490" endWordPosition="491">o the surface realizer&apos;s expected input form, (3) the surface realizer itself, and (4) an automatic metric for determining the accuracy of the generated sentences. Large, well structured, syntactically marked corpora such as the Penn TreeBank (Marcus et al., 1993) can provide a source of example sentences, while automatic metrics like simple string accuracy are capable of giving a fast, rough estimate of quality for individual sentences. Realization of text from corpora has been approached in several ways. In the case of Ratnaparkhi&apos;s generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. FERGUS (Bangalore and Rambow, 2000) used the Penn TreeBank as a corpus, requiring a more substantial transformation algorithm since it requires a lexical predicate-argument structure instead of the TreeBank&apos;s representation. The system uses an underlying tree(S (NP-SBJ ((cat clause) (NP (</context>
</contexts>
<marker>Ratnaparkhi, 2000</marker>
<rawString>Adwait Ratnaparkhi. 2000. Trainable methods for surface natural language generation. In Proceedings of the First North American Conference of the ACL, Seattle, WA, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>