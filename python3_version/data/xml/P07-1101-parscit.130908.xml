<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000058">
<title confidence="0.986616">
On the role of context and prosody in the interpretation of ‘okay’
</title>
<author confidence="0.999499">
Agustin Gravano, Stefan Benus, H´ector Ch´avez, Julia Hirschberg, Lauren Wilcox
</author>
<affiliation confidence="0.9741895">
Department of Computer Science
Columbia University, New York, NY, USA
</affiliation>
<email confidence="0.998167">
{agus,sbenus,hrc2009,julia,lgw23}@cs.columbia.edu
</email>
<sectionHeader confidence="0.993874" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999314">
We examine the effect of contextual and
acoustic cues in the disambiguation of three
discourse-pragmatic functions of the word
okay. Results of a perception study show
that contextual cues are stronger predictors
of discourse function than acoustic cues.
However, acoustic features capturing the
pitch excursion at the right edge of okay fea-
ture prominently in disambiguation, whether
other contextual cues are present or not.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913793103449">
CUE PHRASES (also known as DISCOURSE MARK-
ERS) are linguistic expressions that can be used to
convey explicit information about the structure of
a discourse or to convey a semantic contribution
(Grosz and Sidner, 1986; Reichman, 1985; Cohen,
1984). For example, the word okay can be used to
convey a ‘satisfactory’ evaluation of some entity in
the discourse (the movie was okay); as a backchan-
nel in a dialogue to indicate that one interlocutor
is still attending to another; to convey acknowledg-
ment or agreement; or, in its ‘cue’ use, to start or fin-
ish a discourse segment (Jefferson, 1972; Schegloff
and Sacks, 1973; Kowtko, 1997; Ward and Tsuka-
hara, 2000). A major question is how speakers indi-
cate and listeners interpret such variation in mean-
ing. From a practical perspective, understanding
how speakers and listeners disambiguate cue phrases
is important to spoken dialogue systems, so that sys-
tems can convey potentially ambiguous terms with
their intended meaning and can interpret user input
correctly.
There is considerable evidence that the different
uses of individual cue phrases can be distinguished
by variation in the prosody with which they are re-
alized. For example, (Hirschberg and Litman, 1993)
found that cue phrases in general could be disam-
biguated between their ‘semantic’ and their ‘dis-
course marker’ uses in terms of the type of pitch
accent borne by the cue phrase, the position of the
phrase in the intonational phrase, and the amount
of additional information in the phrase. Despite the
frequence of the word okay in natural dialogues,
relatively little attention has been paid to the rela-
tionship between its use and its prosodic realization.
(Hockey, 1993) did find that okay differs in terms of
the pitch contour speakers use in uttering it, suggest-
ing that a final rising pitch contour “categorically
marks a turn change,” while a downstepped falling
pitch contour usually indicates a discourse segment
boundary. However, it is not clear which, if any, of
the prosodic differences identified in this study are
actually used by listeners in interpreting these po-
tentially ambiguous items.
In this study, we address the question of how hear-
ers disambiguate the interpretation of okay. Our goal
is to identify the acoustic, prosodic and phonetic fea-
tures of okay tokens for which listeners assign differ-
ent meanings. Additionally, we want to determine
the role that discourse context plays in this classi-
fication: i.e., can subjects classify okay tokens reli-
ably from the word alone or do they require addi-
tional context?
Below we describe a perception study in which
listeners were presented with a number of spoken
productions of okay, taken from a corpus of dia-
logues between subjects playing a computer game.
The tokens were presented both in isolation and in
context. Users were asked to select the meaning
</bodyText>
<page confidence="0.947286">
800
</page>
<note confidence="0.926005">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 800–807,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.995769305555556">
of each token from three of the meanings that okay player’s laptop displayed a gameboard containing 5–
can take on: ACKNOWLEDGEMENT/AGREEMENT, 7 objects (Figure 1). In each segment of the game,
BACKCHANNEL, and CUE OF AN INITIAL DIS- both players saw the same set of objects at the same
COURSE SEGMENT. Subsequently, we examined the position on each screen, except for one object (the
acoustic, prosodic and phonetic correlates of these TARGET). For one player (the DESCRIBER), this tar-
classifications to try to infer what cues listeners used get appeared in a random location among other ob-
to interpret the tokens, and how these varied by con- jects on the screen. For the other player (the FOL-
text condition. Section 2 describes our corpus. Sec- LOWER), the target object appeared at the bottom of
tion 3 describes the perception experiment. In Sec- the screen. The describer was instructed to describe
tion 4 we analyze inter-subject agreement, introduce the position of the target object on their screen so
a novel representation of subject judgments, and ex- that the follower could move their representation of
amine the acoustic, prosodic, phonetic and contex- the target to the same location on their own screen.
tual correlates of subject classification of okays. In After the players had negotiated what they deter-
Section 5 we discuss our results and future work. mined to be the best location, they were awarded
2 Corpus up to 100 points based on the actual match of the
The materials for our perception study were selected target location on the two screens. The game pro-
from a portion of the Columbia Games Corpus, a ceeded in this way through 14 tasks, with describer
collection of 12 spontaneous task-oriented dyadic and follower alternating roles. On average, the Ob-
conversations elicited from speakers of Standard jects Games portion of each session took 21m 36s,
American English. The corpus was collected and resulting in 4h 19m of dialogue for the twelve ses-
annotated jointly by the Spoken Language Group sions in the corpus. There are 1484 unique words in
at Columbia University and the Department of Lin- this portion of the corpus, and 36,503 words in total.
guistics at Northwestern University.
Subjects were paid to play two series of com-
puter games (the CARDS GAMES and the OBJECTS
GAMES), requiring collaboration between partners
to achieve a common goal. Participants sat in front
of laptops in a soundproof booth with a curtain be-
tween them, so that all communication would be ver-
bal. Each player played with two different partners
in two different sessions. On average, each session
took 45m 39s, totalling 9h 8m of dialogue for the
whole corpus. All interactions were recorded, digi-
tized, and downsampled to 16K.
The recordings were orthographically transcribed
and words were aligned by hand by trained annota-
</bodyText>
<figureCaption confidence="0.838316583333333">
tors in a ToBI (Beckman and Hirschberg, 1994) or-
thographic tier using Praat (Boersma and Weenink,
2001) to manipulate waveforms. The corpus con-
tains 2239 unique words, with 73,831 words in total.
Nearly all of the Objects Games part of the corpus
has been intonationally transcribed, using the ToBI
conventions. Pitch, energy and duration information
has been extracted for the entire corpus automati-
cally, using Praat.
In the Objects Games portion of the corpus each
801
Figure 1: Sample screen of the Objects Games.
</figureCaption>
<bodyText confidence="0.983114898550725">
Throughout the Objects Games, we noted that
subjects made frequent use of affirmative cue words,
such as okay, yeah, alright, which appeared to vary
in meaning. To investigate the discourse functions
of such words, we first asked three labelers to inde-
pendently classify all occurrences of alright, gotcha,
huh, mmhm, okay, right, uhhuh, yeah, yep, yes, yup
in the entire Games Corpus into one of ten cate-
gories, including acknowledgment/agreement, cue
beginning or ending discourse segment, backchan-
nel, and literal modifier. Labelers were asked to
choose the most appropriate category for each to- tracted from the waveform. For the contextualized
ken, or indicate with ‘?’ if they could not make a versions, we extracted two full speaker turns for
decision. They were allowed to read the transcripts each okay including the full turns containing the tar-
and listen to the speech as they labeled. get okay plus the full turn of the previous speaker. In
For our perception experiment we chose materials the following three sample contexts, pauses are indi-
from the tokens of the most frequent of our labeled cated with ‘#’, and the target okays are underlined:
affirmative words, okay, from the Objects Games, Speaker A: yeah # um there’s like there’s some space there’s
which contained most of these tokens. Altogether, Speaker B: okay # I think I got it
there are 1151 instances of okay in this part of the Speaker A: but it’s gonna be below the onion
corpus; it is the third most frequent word, follow- Speaker B: okay
ing the, with 4565 instances, and of, with 1534. Speaker A: okay # alright # I’ll try it # okay
At least two labelers agreed on the functional cat- Speaker B: okay the owl is blinking
egory of 902 (78%) okay tokens. Of those tokens, The isolated okay tokens were single channel au-
286 (32%) were classified as BACKCxANNEL, 255 dio files; the contextualized okay tokens were for-
(28%) as ACKNOWLEDGEMENT/AGREEMENT, 141 matted so that each speaker was presented to sub-
(16%) as CUE BEGINNING, 116 (13%) as PIVOT jects on a different channel, with the speaker uttering
BEGINNING (a function that combines Acknowl- the target okay consistently on the same channel.
edgement/agreement and Cue beginning), and 104 The perception study was divided into two parts.
(11%) as one of the other functions. We sampled In the first part, each subject was presented with
from tokens the annotators had labeled as Cue be- the 54 isolated okay tokens, in a different ran-
ginning discourse segment, Backchannel, and Ac- dom ordering for each subject. They were given
knowledgement/agreement, the most frequent cate- a forced choice task to classify them as A, B, or
gories in the corpus; we will refer to these below C, with the corresponding labels (Acknowledge-
simply as ‘C’, ‘B’, and ‘A’ classes, respectively. ment/agreement, Backchannel, and Cue beginning)
3 Experiment also presented in a random order for each token. In
We next designed a perception experiment to ex- the second part, the same subject was given 54 con-
amine naive subjects’ perception of these tokens of textualized tokens, presented in a different random
okay. To obtain good coverage both of the (labeled) order, and asked to make the same choice.
A, B, and C classes, as well as the degrees of po- We recruited 20 (paid) subjects for the study, 10
tential ambiguity among these classes, we identified female, and 10 male, all between the ages of 20 and
9 categories of okay tokens to include in the experi- 60. All subjects were native speakers of Standard
ment: 3 classes (A, B, C) × 3 levels of labeler agree- American English, except for one subject who was
ment (UNANIMOUS, MAJORITY, NO-AGREEMENT). born in Jamaica but a native speaker of English. All
‘Unanimous’ refers to tokens assigned to a particu- subjects reported no hearing problems. Subjects per-
lar class label by all 3 labelers, ‘majority’ to tokens formed the study in a quiet lab using headphones to
assigned to this class by 2 of the 3 labelers, and ‘no- listen to the tokens and indicating their classification
agreement’ to tokens assigned to this class by only decisions in a GUI interface on a lab workstation.
1 labeler. To decrease variability in the stimuli, we They were given instructions on how to use the in-
selected tokens only from speakers who produced at terface before each of the two sections of the study.
least one token for each of the 9 conditions. There For the study itself, for each token in the isolated
were 6 such speakers (3 female, 3 male), which gave condition, subjects were shown a screen with the
us a total of 54 tokens. three randomly ordered classes and a link to the to-
To see whether subjects’ classifications of okay ken’s sound file. They could listen to the sound files
were dependent upon contextual information or not, as many times as they wished but were instructed
we prepared two versions of each token. The iso- not to be concerned with answering the questions
lated versions consisted of only the word okay ex-
802
&apos;We define a TURN as a maximal sequence of words spoken
by the same speaker during which the speaker holds the floor.
“correctly”, but to answer with their immediate re-
sponse if possible. However, they were allowed to
change their selection as many times as they liked
before moving to the next screen. In the contex-
tualized condition, they were also shown an ortho-
graphic transcription of part of the contextualized to-
ken, to help them identify the target okay. The mean
duration of the first part of the study was 25 minutes,
and of the second part, 27 minutes.
</bodyText>
<sectionHeader confidence="0.999975" genericHeader="introduction">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.999926">
4.1 Subject ratings
</subsectionHeader>
<bodyText confidence="0.999945888888889">
The distribution of class labels in each experimental
condition is shown in Table 1. While this distribu-
tion roughly mirrors our selection of equal numbers
of tokens from each previously-labeled class, in both
parts of the study more tokens were labeled as A
(acknowledgment/agreement) than as B (backchan-
nel) or C (cue to topic beginning). This supports
the hypothesis that acknowledgment/agreement may
function as the default interpretation of okay.
</bodyText>
<table confidence="0.7486122">
Isolated Contextualized
A 426 (39%) 452 (42%)
B 324 (30%) 306 (28%)
C 330 (31%) 322 (30%)
Total 1080 (100%) 1080 (100%)
</table>
<tableCaption confidence="0.8483355">
Table 1: Distribution of label classes in each
study condition.
</tableCaption>
<bodyText confidence="0.995177">
We examined inter-subject agreement using
Fleiss’ κ measure of inter-rater agreement for mul-
tiple raters (Fleiss, 1971).2 Table 2 shows Fleiss’ κ
calculated for each individual label vs. the other two
labels and for all three labels, in both study condi-
tions. From this table we see that, while there is very
little overall agreement among subjects about how
to classify tokens in the isolated condition, agree-
ment is higher in the contextualized condition, with
a moderate agreement for class C (κ score of .497).
This suggests that context helps distinguish the cue
beginning discourse segment function more than the
other two functions of okay.
2 This measure of agreement above chance is interpreted as
follows: 0 = None, 0 - 0.2 = Small, 0.2 - 0.4 = Fair, 0.4 - 0.6 =
Moderate, 0.6 - 0.8 = Substantial, 0.8 -1 = Almost perfect.
</bodyText>
<table confidence="0.989892">
Isolated Contextualized
A vs. rest .089 .227
B vs. rest .118 .164
C vs. rest .157 .497
all .120 .293
</table>
<tableCaption confidence="0.98198">
Table 2: Fleiss’ κ for each label class
in each study condition.
</tableCaption>
<bodyText confidence="0.998579666666667">
Recall from Section 3 that the okay tokens were
chosen in equal numbers from three classes accord-
ing to the level of agreement of our three original
labelers (unanimous, majority, and no-agreement),
who had the full dialogue context to use in making
their decisions. Table 3 shows Fleiss’ κ measure
now grouped by amount of agreement of the orig-
inal labelers, again presented for each context con-
dition. We see here that the inter-subject agreement
</bodyText>
<table confidence="0.9854494">
Isolated Context. OL
no-agreement .085 .104 -
majority .092 .299 -
unanimous .158 .452 -
all .120 .293 .312
</table>
<tableCaption confidence="0.884827">
Table 3: Fleiss’ κ in each study condition, grouped
by agreement of the three original labelers (‘OL’).
</tableCaption>
<bodyText confidence="0.919439619047619">
also mirrors the agreement of the three original la-
belers. In both study conditions, tokens which the
original labelers agreed on also had the highest κ
scores, followed by tokens in the majority and no-
agreement classes, in that order. In all cases, tokens
which subjects heard in context showed more agree-
ment than those they heard in isolation.
The overall κ is small at .120 for the isolated con-
dition, and fair at .293 for the contextualized con-
dition. The three original labelers also achieved fair
agreement at .312.3 The similarity between the lat-
ter two κ scores suggests that the full context avail-
able to the original labelers and the limited context
presented to the experiment subjets offer compara-
ble amounts of information to disambiguate between
the three functions, although lack of any context
clearly affected subjects’ decisions. We conclude
3 For the calculation of this r,, we considered four label
classes: A, B, C, and a fourth class ‘other’ that comprises the
remaining 7 word functions mentioned in Section 2. In conse-
quence, these K scores should be compared with caution.
</bodyText>
<page confidence="0.995422">
803
</page>
<bodyText confidence="0.99966925">
from these results that context is of considerable im-
portance in the interpretation of the word okay, al-
though even a very limited context appears to suf-
fice.
</bodyText>
<subsectionHeader confidence="0.996318">
4.2 Representing subject judgments
</subsectionHeader>
<bodyText confidence="0.9999931">
In this section, we present a graphical representa-
tion of subject decisions, useful for interpreting, vi-
sualizing, and comparing the way our subjects in-
terpreted the different tokens of okay. For each in-
dividual okay in the study, we define an associated
three-dimensional VOTE VECTOR, whose compo-
nents are the proportions of subjects that classified
the token as A, B or C. For example, if a particu-
lar okay was labeled as A by 5 subjects, as B by 3,
and as C by 12, then its associated vote vector is
</bodyText>
<equation confidence="0.6957555">
(520, 320, 12) = (0.25, 0.15, 0.6). Following this def-
20
inition, the vectors A = (1, 0, 0), B = (0, 1, 0) and
C = (0, 0,1) correspond to the ideal situations in
</equation>
<bodyText confidence="0.988453">
which all 20 subjects agreed on the label. We call
these vectors the UNANIMOUS-VOTE VECTORS.
Figure 2.i shows a two-dimensional representa-
tion that illustrates these definitions. The black dot
</bodyText>
<figureCaption confidence="0.767812">
Figure 2: 2D representation of a vote vector (i)
and of the cluster centroids (ii).
</figureCaption>
<bodyText confidence="0.9849926">
represents the vote vector for our example okay,
the vertices of the triangle correspond to the three
unanimous-vote vectors (A, B and C), and the cross
in the center of the triangle represents the vote vector
of a three-way tie between the labelers (1 , 1 , 1 )
</bodyText>
<equation confidence="0.603173">
3 3 3
</equation>
<bodyText confidence="0.999254833333333">
We are thus able to calculate the Euclidean dis-
tance of a vote vector to each of the unanimous-vote
vectors. The shortest of these distances corresponds
to the label assigned by the plurality4 of subjects.
Also, the smaller that distance, the higher the inter-
subject agreement for that particular token. For our
</bodyText>
<footnote confidence="0.945904">
4Plurality is also known as simple majority: the candidate
who gets more votes than any other candidate is the winner.
</footnote>
<bodyText confidence="0.99645494117647">
example okay, the distances to A, B and C are 0.972,
1.070 and 0.495, respectively; its plurality label is C.
In our experiment, each okay has two associated
vote vectors, one for each context condition. To
illustrate the relationship between decisions in the
isolated and the contextualized conditions, we first
grouped each condition’s 54 vote vectors into three
clusters, according to their plurality label. Figure
2.ii shows the cluster centroids in a two-dimensional
representation of vote vectors. The filled dots corre-
spond to the cluster centroids of the isolated condi-
tion, and the empty dots, to the centroids of the con-
textualized condition. Table 4 shows the distances
in each condition from the cluster centroids (denoted
Ac, Bc, Cc) to the respective unanimous-vote vec-
tors (A, B, C), and also the distance between each
pair of cluster centroids.
</bodyText>
<table confidence="0.994871714285714">
Isolated Contextualized
d(Ac, A) .54 .44 (–18%)
d(Bc, B) .57 .52 (–10%)
d(Cc, C) .52 .28 (–47%)
d(Ac, Bc) .41 .48 (+17%)
d(Ac, Cc) .49 .86 (+75%)
d(Bc, Cc) .54 .91 (+69%)
</table>
<tableCaption confidence="0.990861">
Table 4: Distances from the cluster centroids (Ac,
</tableCaption>
<bodyText confidence="0.996570833333333">
Bc, Cc) to the unanimous-vote vectors (A, B, C)
and between cluster centroids, in each condition.
In the isolated condition, the three cluster cen-
troids are approximately equidistant from each other
—that is, the three word functions appear to be
equally confusable. In the contextualized condi-
tion, while Cc is further apart from the other two
centroids, the distance between Ac and Bc remains
practically the same. This suggests that, with some
context available, A and B tokens are still fairly con-
fusable, while both are more easily distinguished
from C tokens. We posit two possible explanations
for this: First, C is the only function for which
the speaker uttering the okay necessarily continues
speaking; thus the role of context in disambiguat-
ing seems quite clear. Second, both A and B have a
common element of ‘acknowledgement’ that might
affect inter-subject agreement.
</bodyText>
<page confidence="0.99582">
804
</page>
<subsectionHeader confidence="0.979735">
4.3 Features of the okay tokens
</subsectionHeader>
<bodyText confidence="0.999946">
In this section, we describe a set of acoustic,
prosodic, phonetic and contextual features which
may help to explain why subjects interpret okay dif-
ferently. Acoustic features were extracted automat-
ically using Praat. Phonetic and prosodic features
were hand-labeled by expert annotators. Contextual
features were considered only in the analysis of the
contextualized condition, since they were not avail-
able to subjects in the isolated condition.
We examined a number of phonetic features to de-
termine whether these correlated with subject clas-
sifications. We first looked at the production of the
three phonemes in the target okay (/oU/, /k/, /eI/),
noting the following possible variations:
</bodyText>
<listItem confidence="0.999944333333333">
• /oU/: [], [a], [U], [O], [OU], [m], [q], [a], [OU].
• /k/: [8], [k], [kx], [q], [x].
• /eI/: [e], [eI], [E], [ea].
</listItem>
<bodyText confidence="0.9997658">
We also calculated the duration of each phone and
of the velar closure. Whether the target okay was at
least partially whispered or not, and whether there
was glottalization in the target okay were also noted.
For each target okay, we also examined its du-
ration and its maximum, mean and minimum pitch
and intensity, as well as the speaker-normalized ver-
sions of these values.5 We considered its pitch slope,
intensity slope, and stylized pitch slope, calculated
over the whole target okay, its last 50, 80 and 100
milliseconds, its second half, its second syllable, and
the second half of its second syllable, as well.
We used the ToBI labeling scheme (Pitrelli et al.,
1994) to label the prosody of the target okays and
their surrounding context.
</bodyText>
<listItem confidence="0.9769962">
• Pitch accent, if any, of the target okay (e.g., H*,
H+!H*, L*).
• Break index after the target okay (0-4).
• Phrase accent and boundary tone, if any, fol-
lowing the target okay (e.g., L-L%, !H-H%).
</listItem>
<bodyText confidence="0.99569975">
For contextualized tokens, we included several fea-
tures related to the exchange between the speaker
uttering the target okay (Speaker B) and the other
speaker (Speaker A).
5Speaker-normalized features were normalized by comput-
ing z-scores (z = (X − mean)/st.dev) for the feature, where
mean and st.dev were calculated from all okays uttered by the
speaker in the session.
</bodyText>
<listItem confidence="0.989780777777778">
• Number of words uttered by Speaker A in the
context, before and after the target okay. Same
for Speaker B.
• Latency of Speaker A before Speaker B’s turn.
• Duration of silence of Speaker B before and af-
ter the target okay.
• Duration of speech by Speaker B immediately
before and after the target okay and up to a si-
lence.
</listItem>
<subsectionHeader confidence="0.999414">
4.4 Cues to interpretation
</subsectionHeader>
<bodyText confidence="0.999080375">
We conducted a series of Pearson’s tests to look for
correlations between the proportion of subjects that
chose each label and the numeric features described
in Section 4.3, together with two-sided t-tests to find
whether such correlations differed significantly from
zero. Tables 5 and 6 show the significant results
(two-sided t-tests, p &lt; 0.05) for the isolated and
contextualized conditions, respectively.
</bodyText>
<sectionHeader confidence="0.621056" genericHeader="method">
Acknowledgement/agreement r
</sectionHeader>
<subsectionHeader confidence="0.3934105">
duration of realization of /k/
Backchannel
</subsectionHeader>
<bodyText confidence="0.862406454545455">
stylized pitch slope over 2nd half 2nd syl.
pitch slope over 2nd half of 2nd syllable
speaker-normalized maximum intensity
pitch slope over last 80 ms
speaker-normalized mean intensity
duration of realization of /eI/
word duration
Cue to discourse segment beginning r
stylized pitch slope over the whole word –0.380
pitch slope over the whole word –0.342
pitch slope over 2nd half of 2nd syllable –0.319
</bodyText>
<tableCaption confidence="0.9878695">
Table 5: Features correlated to the proportion of
votes for each label. Isolated condition.
</tableCaption>
<bodyText confidence="0.962535444444444">
Table 5 shows that in the isolated condition, sub-
jects tended to classify tokens of okay as Acknowl-
edgment/agreement (A) which had a longer realiza-
tion of the /k/ phoneme. They tended to classify
tokens as Backchannels (B) which had a lower in-
tensity, a longer duration, a longer realization of the
/eI/ phoneme, and a final rising pitch. They tended
to classify tokens as C (cue to topic beginning) that
ended with falling pitch.
</bodyText>
<figure confidence="0.890442111111111">
–0.299
r
0.752
0.409
–0.372
0.349
–0.327
0.278
0.277
</figure>
<page confidence="0.959597">
805
</page>
<sectionHeader confidence="0.690308" genericHeader="method">
Acknowledgement/agreement r
</sectionHeader>
<bodyText confidence="0.989644333333333">
latency of Spkr A before Spkr B’s turn
duration of silence by Spkr B before okay –0.404
number of words by Spkr B after okay
</bodyText>
<subsectionHeader confidence="0.70703">
Backchannel
</subsectionHeader>
<bodyText confidence="0.99887125">
pitch slope over 2nd half of 2nd syllable
pitch slope over last 80 ms
number of words by Spkr A before okay
number of words by Spkr B after okay
duration of speech by Spkr B after okay
latency of Spkr A before Spkr B’s turn
duration of silence by Spkr B before okay
intensity slope over 2nd syllable
</bodyText>
<subsectionHeader confidence="0.577088">
Cue to discourse segment beginning
</subsectionHeader>
<bodyText confidence="0.988799833333333">
latency of Spkr A before Spkr B’s turn
number of words by Spkr B after okay
number of words by Spkr A before okay –0.426
pitch slope over 2nd half of 2nd syllable –0.385
pitch slope over last 80 ms
duration of speech by Spkr B after okay
</bodyText>
<tableCaption confidence="0.9687775">
Table 6: Features correlated to the proportion of
votes for each label. Contextualized condition.
</tableCaption>
<bodyText confidence="0.962994178571429">
In the contextualized condition, we find very dif-
ferent correlations. Table 6 shows that nearly all of
the strong correlations in this condition involve con-
textual features, such as the latency before Speaker
B’s turn, or the number of words by each speaker be-
fore and after the target okay. Notably, only one of
the features that show strong correlations in the iso-
lated condition shows the same strong correlation in
the contextualized condition: the pitch slope at the
end of the word. In both conditions subjects tended
to label tokens with a final rising pitch contour as
B, and tokens with a final falling pitch contour as C.
This supports (Hockey, 1993)’s findings on the role
of pitch contour in disambiguating okay.
We next conducted a series of two-sided Fisher’s
exact tests to find correlations between subjects’ la-
belings of okay and the nominal features described
in Section 4.3. We found significant associations be-
tween the realization of the /oU/ phoneme and the
okay function in the isolated condition (p &lt; 0.005).
Table 7 shows that, in particular, [m] seems to be the
preferred realization for B okays, while [a] seems to
be the preferred one for A okays, and [OU] and [O]
for A and C okays.
? [a] [u] [OU] [O] [9] [aU] [a] [] [m]
A 0 0 5 6 4 0 0 8 0 0
B 2 0 4 1 0 1 0 1 1 5
C 1 1 2 3 4 0 1 3 0 0
</bodyText>
<tableCaption confidence="0.9232885">
Table 7: Realization of the /oU/ phoneme, grouped
by subject plurality label. Isolated condition only.
</tableCaption>
<bodyText confidence="0.997837166666667">
Notably, we did not find such significant asso-
ciations in the contextualized condition. We did
find significant correlations in both conditions, how-
ever, between okay classifications and the type of
phrase accent and boundary tone following the target
(Fisher’s Exact Test, p &lt; 0.05 for the isolated con-
dition, p &lt; 0.005 for the contextualized condition).
Table 8 shows that L-L% tends to be associated with
A and C classes, H-H% with B classes, and L-H%
with A and B classes. In this case, such correlations
are present in the isolated condition, and sustained
or enhanced in the contextualized condition.
</bodyText>
<table confidence="0.998456714285714">
H-H% H-L% L-H% L-L% other
A 0 2 4 8 9
Isolated B 3 3 1 5 3
C 1 1 0 8 5
A 0 2 3 10 10
Context. B 4 3 2 1 2
C 0 1 0 10 5
</table>
<tableCaption confidence="0.9847215">
Table 8: Phrase accent and boundary tone, grouped
by subject plurality label.
</tableCaption>
<bodyText confidence="0.994613470588235">
Summing up, when subjects listened to the okay
tokens in isolation, with only their acoustic, prosodic
and phonetic properties available, a few features
seem to strongly correlate with the perception of
word function; for example, maximum intensity,
word duration, and realizing the /oU/ phoneme as
[m] tend to be associated with backchannel, while
the duration of the realization of the /k/ phoneme,
and realizing the /oU/ phoneme as [a] tend to be as-
sociated with acknowledgment/agreement.
In the second part of the study, when subjects
listened to contextualized versions of the same to-
kens of okay, most of the strong correlations of word
function with acoustic, prosodic and phonetic fea-
tures were replaced by correlations with contextual
features, like latency and turn duration. In other
words, these results suggest that contextual features
</bodyText>
<figure confidence="0.9894633125">
–0.528
–0.277
r
0.520
0.455
0.451
–0.433
–0.413
–0.385
0.295
–0.279
r
0.645
0.481
–0.377
0.338
</figure>
<page confidence="0.995643">
806
</page>
<bodyText confidence="0.999223857142857">
might override the effect of most acoustic, prosodic
and phonetic features of okay. There is nonethe-
less one notable exception: word final intonation —
captured by the pitch slope and the ToBI labels for
phrase accent and boundary tone — seems to play a
central role in the interpretation of both isolated and
contextualized okays.
</bodyText>
<sectionHeader confidence="0.971727" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999938487804878">
In this study, we have presented evidence of differ-
ences in the interpretation of the function of isolated
and contextualized okays. We have shown that word
final intonation strongly correlates with the subjects’
classification of okays in both conditions. Addition-
ally, the higher degree of inter-subject agreement in
the contextualized condition, along with the strong
correlations found for contextualized features, sug-
gests that context, when available, plays a central
role in the disambiguation of okay. (Note, how-
ever, that further research is needed in order to assess
whether these features are indeed, in fact, perceptu-
ally important, both individually and combined.)
We have also presented results suggesting that ac-
knowledgment/agreement acts as a default function
for both isolated an contextualized okays. Further-
more, while that function remains confusable with
backchannel in both conditions, the availability of
some context helps in distinguishing those two func-
tions from cue to topic beginning.
These results are relevant to spoken dialogue sys-
tems in suggesting how systems can convey the cue
word okay with the intended meaning and can inter-
pret users’ productions of okay correctly. How these
results extend to other cue words and to other word
functions remains an open question.
As future work, we will extend this study to in-
clude the over 5800 occurrences of alright, gotcha,
huh, mmhm, okay, right, uhhuh, yeah, yep, yes, yup
in the entire Games Corpus, and all 10 discourse
functions mentioned in Section 2, as annotated by
our three original labelers. Since we have observed
considerable differences in conversation style in the
two parts of the corpus (the Objects Games elicited
more ‘dynamic’ conversations, with more overlaps
and interruptions than the Cards Games), we will
compare cue phrase usage in these two settings. Fi-
nally, we are also interested in examining speaker
entrainment in cue phrase usage, or how subjects
adapt their choice and production of cue phrases to
their conversation partner’s.
</bodyText>
<sectionHeader confidence="0.996556" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99993">
This work was funded in part by NSF IIS-0307905.
We thank Gregory Ward, Elisa Sneed, and Michael
Mulley for their valuable help in collecting and la-
beling the data, and the anonymous reviewers for
helpful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999105" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999261666666667">
Mary E. Beckman and Julia Hirschberg. 1994. The ToBI
annotation conventions. Ohio State University.
Paul Boersma and David Weenink. 2001. Praat: Doing
phonetics by computer. http://www.praat.org.
Robin Cohen. 1984. A computational theory of the func-
tion of clue words in argument understanding. 22nd
Conference of the ACL, pages 251–258.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378–382.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, Intentions, and the Structure of Discourse. Com-
putational Linguistics, 12(3):175–204.
Julia Hirschberg and Diane Litman. 1993. Empirical
Studies on the Disambiguation of Cue Phrases. Com-
putational Linguistics, 19(3):501–530.
Beth Ann Hockey. 1993. Prosody and the role of okay
and uh-huh in discourse. Proceedings of the Eastern
States Conference on Linguistics, pages 128–136.
Gail Jefferson. 1972. Side sequences. Studies in social
interaction, 294:338.
Jacqueline C. Kowtko. 1997. The function of intonation
in task-oriented dialogue. Ph.D. thesis, University of
Edinburgh.
John Pitrelli, Mary Beckman, and Julia Hirschberg.
1994. Evaluation of prosodic transcription labeling
reliability in the ToBI framework. In ICSLP94, vol-
ume 2, pages 123–126, Yokohama, Japan.
Rachel Reichman. 1985. Getting Computers to Talk Like
You and Me: Discourse Context, Focus, and Seman-
tics: (an ATNModel). MIT Press.
Emanuel A. Schegloff and Harvey Sacks. 1973. Opening
up closings. Semiotica, 8(4):289–327.
Nigel Ward and Wataru Tsukahara. 2000. Prosodic fea-
tures which cue back-channel responses in English and
Japanese. Journal ofPragmatics, 23:1177–1207.
</reference>
<page confidence="0.997874">
807
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.538837">
<title confidence="0.999553">On the role of context and prosody in the interpretation of ‘okay’</title>
<author confidence="0.999981">Agustin Gravano</author>
<author confidence="0.999981">Stefan Benus</author>
<author confidence="0.999981">H´ector Ch´avez</author>
<author confidence="0.999981">Julia Hirschberg</author>
<author confidence="0.999981">Lauren Wilcox</author>
<affiliation confidence="0.999918">Department of Computer Science</affiliation>
<address confidence="0.560783">Columbia University, New York, NY, USA</address>
<abstract confidence="0.995813545454546">We examine the effect of contextual and acoustic cues in the disambiguation of three discourse-pragmatic functions of the word Results of a perception study show that contextual cues are stronger predictors of discourse function than acoustic cues. However, acoustic features capturing the excursion at the right edge of feature prominently in disambiguation, whether other contextual cues are present or not.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mary E Beckman</author>
<author>Julia Hirschberg</author>
</authors>
<title>The ToBI annotation conventions.</title>
<date>1994</date>
<institution>Ohio State University.</institution>
<contexts>
<context position="6639" citStr="Beckman and Hirschberg, 1994" startWordPosition="1063" endWordPosition="1066">of computer games (the CARDS GAMES and the OBJECTS GAMES), requiring collaboration between partners to achieve a common goal. Participants sat in front of laptops in a soundproof booth with a curtain between them, so that all communication would be verbal. Each player played with two different partners in two different sessions. On average, each session took 45m 39s, totalling 9h 8m of dialogue for the whole corpus. All interactions were recorded, digitized, and downsampled to 16K. The recordings were orthographically transcribed and words were aligned by hand by trained annotators in a ToBI (Beckman and Hirschberg, 1994) orthographic tier using Praat (Boersma and Weenink, 2001) to manipulate waveforms. The corpus contains 2239 unique words, with 73,831 words in total. Nearly all of the Objects Games part of the corpus has been intonationally transcribed, using the ToBI conventions. Pitch, energy and duration information has been extracted for the entire corpus automatically, using Praat. In the Objects Games portion of the corpus each 801 Figure 1: Sample screen of the Objects Games. Throughout the Objects Games, we noted that subjects made frequent use of affirmative cue words, such as okay, yeah, alright, w</context>
</contexts>
<marker>Beckman, Hirschberg, 1994</marker>
<rawString>Mary E. Beckman and Julia Hirschberg. 1994. The ToBI annotation conventions. Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Boersma</author>
<author>David Weenink</author>
</authors>
<title>Praat: Doing phonetics by computer.</title>
<date>2001</date>
<note>http://www.praat.org.</note>
<contexts>
<context position="6697" citStr="Boersma and Weenink, 2001" startWordPosition="1072" endWordPosition="1075">quiring collaboration between partners to achieve a common goal. Participants sat in front of laptops in a soundproof booth with a curtain between them, so that all communication would be verbal. Each player played with two different partners in two different sessions. On average, each session took 45m 39s, totalling 9h 8m of dialogue for the whole corpus. All interactions were recorded, digitized, and downsampled to 16K. The recordings were orthographically transcribed and words were aligned by hand by trained annotators in a ToBI (Beckman and Hirschberg, 1994) orthographic tier using Praat (Boersma and Weenink, 2001) to manipulate waveforms. The corpus contains 2239 unique words, with 73,831 words in total. Nearly all of the Objects Games part of the corpus has been intonationally transcribed, using the ToBI conventions. Pitch, energy and duration information has been extracted for the entire corpus automatically, using Praat. In the Objects Games portion of the corpus each 801 Figure 1: Sample screen of the Objects Games. Throughout the Objects Games, we noted that subjects made frequent use of affirmative cue words, such as okay, yeah, alright, which appeared to vary in meaning. To investigate the disco</context>
</contexts>
<marker>Boersma, Weenink, 2001</marker>
<rawString>Paul Boersma and David Weenink. 2001. Praat: Doing phonetics by computer. http://www.praat.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cohen</author>
</authors>
<title>A computational theory of the function of clue words</title>
<date>1984</date>
<booktitle>in argument understanding. 22nd Conference of the ACL,</booktitle>
<pages>251--258</pages>
<contexts>
<context position="964" citStr="Cohen, 1984" startWordPosition="137" endWordPosition="138">of three discourse-pragmatic functions of the word okay. Results of a perception study show that contextual cues are stronger predictors of discourse function than acoustic cues. However, acoustic features capturing the pitch excursion at the right edge of okay feature prominently in disambiguation, whether other contextual cues are present or not. 1 Introduction CUE PHRASES (also known as DISCOURSE MARKERS) are linguistic expressions that can be used to convey explicit information about the structure of a discourse or to convey a semantic contribution (Grosz and Sidner, 1986; Reichman, 1985; Cohen, 1984). For example, the word okay can be used to convey a ‘satisfactory’ evaluation of some entity in the discourse (the movie was okay); as a backchannel in a dialogue to indicate that one interlocutor is still attending to another; to convey acknowledgment or agreement; or, in its ‘cue’ use, to start or finish a discourse segment (Jefferson, 1972; Schegloff and Sacks, 1973; Kowtko, 1997; Ward and Tsukahara, 2000). A major question is how speakers indicate and listeners interpret such variation in meaning. From a practical perspective, understanding how speakers and listeners disambiguate cue phra</context>
</contexts>
<marker>Cohen, 1984</marker>
<rawString>Robin Cohen. 1984. A computational theory of the function of clue words in argument understanding. 22nd Conference of the ACL, pages 251–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="13463" citStr="Fleiss, 1971" startWordPosition="2217" endWordPosition="2218">equal numbers of tokens from each previously-labeled class, in both parts of the study more tokens were labeled as A (acknowledgment/agreement) than as B (backchannel) or C (cue to topic beginning). This supports the hypothesis that acknowledgment/agreement may function as the default interpretation of okay. Isolated Contextualized A 426 (39%) 452 (42%) B 324 (30%) 306 (28%) C 330 (31%) 322 (30%) Total 1080 (100%) 1080 (100%) Table 1: Distribution of label classes in each study condition. We examined inter-subject agreement using Fleiss’ κ measure of inter-rater agreement for multiple raters (Fleiss, 1971).2 Table 2 shows Fleiss’ κ calculated for each individual label vs. the other two labels and for all three labels, in both study conditions. From this table we see that, while there is very little overall agreement among subjects about how to classify tokens in the isolated condition, agreement is higher in the contextualized condition, with a moderate agreement for class C (κ score of .497). This suggests that context helps distinguish the cue beginning discourse segment function more than the other two functions of okay. 2 This measure of agreement above chance is interpreted as follows: 0 =</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<date>1986</date>
<booktitle>Attention, Intentions, and the Structure of Discourse. Computational Linguistics,</booktitle>
<pages>12--3</pages>
<contexts>
<context position="934" citStr="Grosz and Sidner, 1986" startWordPosition="131" endWordPosition="134">and acoustic cues in the disambiguation of three discourse-pragmatic functions of the word okay. Results of a perception study show that contextual cues are stronger predictors of discourse function than acoustic cues. However, acoustic features capturing the pitch excursion at the right edge of okay feature prominently in disambiguation, whether other contextual cues are present or not. 1 Introduction CUE PHRASES (also known as DISCOURSE MARKERS) are linguistic expressions that can be used to convey explicit information about the structure of a discourse or to convey a semantic contribution (Grosz and Sidner, 1986; Reichman, 1985; Cohen, 1984). For example, the word okay can be used to convey a ‘satisfactory’ evaluation of some entity in the discourse (the movie was okay); as a backchannel in a dialogue to indicate that one interlocutor is still attending to another; to convey acknowledgment or agreement; or, in its ‘cue’ use, to start or finish a discourse segment (Jefferson, 1972; Schegloff and Sacks, 1973; Kowtko, 1997; Ward and Tsukahara, 2000). A major question is how speakers indicate and listeners interpret such variation in meaning. From a practical perspective, understanding how speakers and l</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, Intentions, and the Structure of Discourse. Computational Linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Diane Litman</author>
</authors>
<date>1993</date>
<booktitle>Empirical Studies on the Disambiguation of Cue Phrases. Computational Linguistics,</booktitle>
<contexts>
<context position="1934" citStr="Hirschberg and Litman, 1993" startWordPosition="292" endWordPosition="295">972; Schegloff and Sacks, 1973; Kowtko, 1997; Ward and Tsukahara, 2000). A major question is how speakers indicate and listeners interpret such variation in meaning. From a practical perspective, understanding how speakers and listeners disambiguate cue phrases is important to spoken dialogue systems, so that systems can convey potentially ambiguous terms with their intended meaning and can interpret user input correctly. There is considerable evidence that the different uses of individual cue phrases can be distinguished by variation in the prosody with which they are realized. For example, (Hirschberg and Litman, 1993) found that cue phrases in general could be disambiguated between their ‘semantic’ and their ‘discourse marker’ uses in terms of the type of pitch accent borne by the cue phrase, the position of the phrase in the intonational phrase, and the amount of additional information in the phrase. Despite the frequence of the word okay in natural dialogues, relatively little attention has been paid to the relationship between its use and its prosodic realization. (Hockey, 1993) did find that okay differs in terms of the pitch contour speakers use in uttering it, suggesting that a final rising pitch con</context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>Julia Hirschberg and Diane Litman. 1993. Empirical Studies on the Disambiguation of Cue Phrases. Computational Linguistics, 19(3):501–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Ann Hockey</author>
</authors>
<title>Prosody and the role of okay and uh-huh in discourse.</title>
<date>1993</date>
<booktitle>Proceedings of the Eastern States Conference on Linguistics,</booktitle>
<pages>128--136</pages>
<contexts>
<context position="2407" citStr="Hockey, 1993" startWordPosition="373" endWordPosition="374">vidual cue phrases can be distinguished by variation in the prosody with which they are realized. For example, (Hirschberg and Litman, 1993) found that cue phrases in general could be disambiguated between their ‘semantic’ and their ‘discourse marker’ uses in terms of the type of pitch accent borne by the cue phrase, the position of the phrase in the intonational phrase, and the amount of additional information in the phrase. Despite the frequence of the word okay in natural dialogues, relatively little attention has been paid to the relationship between its use and its prosodic realization. (Hockey, 1993) did find that okay differs in terms of the pitch contour speakers use in uttering it, suggesting that a final rising pitch contour “categorically marks a turn change,” while a downstepped falling pitch contour usually indicates a discourse segment boundary. However, it is not clear which, if any, of the prosodic differences identified in this study are actually used by listeners in interpreting these potentially ambiguous items. In this study, we address the question of how hearers disambiguate the interpretation of okay. Our goal is to identify the acoustic, prosodic and phonetic features of</context>
<context position="25389" citStr="Hockey, 1993" startWordPosition="4245" endWordPosition="4246">ifferent correlations. Table 6 shows that nearly all of the strong correlations in this condition involve contextual features, such as the latency before Speaker B’s turn, or the number of words by each speaker before and after the target okay. Notably, only one of the features that show strong correlations in the isolated condition shows the same strong correlation in the contextualized condition: the pitch slope at the end of the word. In both conditions subjects tended to label tokens with a final rising pitch contour as B, and tokens with a final falling pitch contour as C. This supports (Hockey, 1993)’s findings on the role of pitch contour in disambiguating okay. We next conducted a series of two-sided Fisher’s exact tests to find correlations between subjects’ labelings of okay and the nominal features described in Section 4.3. We found significant associations between the realization of the /oU/ phoneme and the okay function in the isolated condition (p &lt; 0.005). Table 7 shows that, in particular, [m] seems to be the preferred realization for B okays, while [a] seems to be the preferred one for A okays, and [OU] and [O] for A and C okays. ? [a] [u] [OU] [O] [9] [aU] [a] [] [m] A 0 0 5 6</context>
</contexts>
<marker>Hockey, 1993</marker>
<rawString>Beth Ann Hockey. 1993. Prosody and the role of okay and uh-huh in discourse. Proceedings of the Eastern States Conference on Linguistics, pages 128–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gail Jefferson</author>
</authors>
<title>Side sequences.</title>
<date>1972</date>
<booktitle>Studies in social interaction,</booktitle>
<pages>294--338</pages>
<contexts>
<context position="1309" citStr="Jefferson, 1972" startWordPosition="198" endWordPosition="199"> or not. 1 Introduction CUE PHRASES (also known as DISCOURSE MARKERS) are linguistic expressions that can be used to convey explicit information about the structure of a discourse or to convey a semantic contribution (Grosz and Sidner, 1986; Reichman, 1985; Cohen, 1984). For example, the word okay can be used to convey a ‘satisfactory’ evaluation of some entity in the discourse (the movie was okay); as a backchannel in a dialogue to indicate that one interlocutor is still attending to another; to convey acknowledgment or agreement; or, in its ‘cue’ use, to start or finish a discourse segment (Jefferson, 1972; Schegloff and Sacks, 1973; Kowtko, 1997; Ward and Tsukahara, 2000). A major question is how speakers indicate and listeners interpret such variation in meaning. From a practical perspective, understanding how speakers and listeners disambiguate cue phrases is important to spoken dialogue systems, so that systems can convey potentially ambiguous terms with their intended meaning and can interpret user input correctly. There is considerable evidence that the different uses of individual cue phrases can be distinguished by variation in the prosody with which they are realized. For example, (Hir</context>
</contexts>
<marker>Jefferson, 1972</marker>
<rawString>Gail Jefferson. 1972. Side sequences. Studies in social interaction, 294:338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacqueline C Kowtko</author>
</authors>
<title>The function of intonation in task-oriented dialogue.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1350" citStr="Kowtko, 1997" startWordPosition="204" endWordPosition="205">own as DISCOURSE MARKERS) are linguistic expressions that can be used to convey explicit information about the structure of a discourse or to convey a semantic contribution (Grosz and Sidner, 1986; Reichman, 1985; Cohen, 1984). For example, the word okay can be used to convey a ‘satisfactory’ evaluation of some entity in the discourse (the movie was okay); as a backchannel in a dialogue to indicate that one interlocutor is still attending to another; to convey acknowledgment or agreement; or, in its ‘cue’ use, to start or finish a discourse segment (Jefferson, 1972; Schegloff and Sacks, 1973; Kowtko, 1997; Ward and Tsukahara, 2000). A major question is how speakers indicate and listeners interpret such variation in meaning. From a practical perspective, understanding how speakers and listeners disambiguate cue phrases is important to spoken dialogue systems, so that systems can convey potentially ambiguous terms with their intended meaning and can interpret user input correctly. There is considerable evidence that the different uses of individual cue phrases can be distinguished by variation in the prosody with which they are realized. For example, (Hirschberg and Litman, 1993) found that cue </context>
</contexts>
<marker>Kowtko, 1997</marker>
<rawString>Jacqueline C. Kowtko. 1997. The function of intonation in task-oriented dialogue. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Pitrelli</author>
<author>Mary Beckman</author>
<author>Julia Hirschberg</author>
</authors>
<title>Evaluation of prosodic transcription labeling reliability in the ToBI framework.</title>
<date>1994</date>
<booktitle>In ICSLP94,</booktitle>
<volume>2</volume>
<pages>123--126</pages>
<location>Yokohama, Japan.</location>
<contexts>
<context position="21431" citStr="Pitrelli et al., 1994" startWordPosition="3567" endWordPosition="3570">he velar closure. Whether the target okay was at least partially whispered or not, and whether there was glottalization in the target okay were also noted. For each target okay, we also examined its duration and its maximum, mean and minimum pitch and intensity, as well as the speaker-normalized versions of these values.5 We considered its pitch slope, intensity slope, and stylized pitch slope, calculated over the whole target okay, its last 50, 80 and 100 milliseconds, its second half, its second syllable, and the second half of its second syllable, as well. We used the ToBI labeling scheme (Pitrelli et al., 1994) to label the prosody of the target okays and their surrounding context. • Pitch accent, if any, of the target okay (e.g., H*, H+!H*, L*). • Break index after the target okay (0-4). • Phrase accent and boundary tone, if any, following the target okay (e.g., L-L%, !H-H%). For contextualized tokens, we included several features related to the exchange between the speaker uttering the target okay (Speaker B) and the other speaker (Speaker A). 5Speaker-normalized features were normalized by computing z-scores (z = (X − mean)/st.dev) for the feature, where mean and st.dev were calculated from all o</context>
</contexts>
<marker>Pitrelli, Beckman, Hirschberg, 1994</marker>
<rawString>John Pitrelli, Mary Beckman, and Julia Hirschberg. 1994. Evaluation of prosodic transcription labeling reliability in the ToBI framework. In ICSLP94, volume 2, pages 123–126, Yokohama, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachel Reichman</author>
</authors>
<title>Getting Computers to Talk Like You and Me: Discourse Context, Focus, and Semantics: (an ATNModel).</title>
<date>1985</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="950" citStr="Reichman, 1985" startWordPosition="135" endWordPosition="136"> disambiguation of three discourse-pragmatic functions of the word okay. Results of a perception study show that contextual cues are stronger predictors of discourse function than acoustic cues. However, acoustic features capturing the pitch excursion at the right edge of okay feature prominently in disambiguation, whether other contextual cues are present or not. 1 Introduction CUE PHRASES (also known as DISCOURSE MARKERS) are linguistic expressions that can be used to convey explicit information about the structure of a discourse or to convey a semantic contribution (Grosz and Sidner, 1986; Reichman, 1985; Cohen, 1984). For example, the word okay can be used to convey a ‘satisfactory’ evaluation of some entity in the discourse (the movie was okay); as a backchannel in a dialogue to indicate that one interlocutor is still attending to another; to convey acknowledgment or agreement; or, in its ‘cue’ use, to start or finish a discourse segment (Jefferson, 1972; Schegloff and Sacks, 1973; Kowtko, 1997; Ward and Tsukahara, 2000). A major question is how speakers indicate and listeners interpret such variation in meaning. From a practical perspective, understanding how speakers and listeners disambi</context>
</contexts>
<marker>Reichman, 1985</marker>
<rawString>Rachel Reichman. 1985. Getting Computers to Talk Like You and Me: Discourse Context, Focus, and Semantics: (an ATNModel). MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
<author>Harvey Sacks</author>
</authors>
<title>Opening up closings.</title>
<date>1973</date>
<journal>Semiotica,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="1336" citStr="Schegloff and Sacks, 1973" startWordPosition="200" endWordPosition="203">uction CUE PHRASES (also known as DISCOURSE MARKERS) are linguistic expressions that can be used to convey explicit information about the structure of a discourse or to convey a semantic contribution (Grosz and Sidner, 1986; Reichman, 1985; Cohen, 1984). For example, the word okay can be used to convey a ‘satisfactory’ evaluation of some entity in the discourse (the movie was okay); as a backchannel in a dialogue to indicate that one interlocutor is still attending to another; to convey acknowledgment or agreement; or, in its ‘cue’ use, to start or finish a discourse segment (Jefferson, 1972; Schegloff and Sacks, 1973; Kowtko, 1997; Ward and Tsukahara, 2000). A major question is how speakers indicate and listeners interpret such variation in meaning. From a practical perspective, understanding how speakers and listeners disambiguate cue phrases is important to spoken dialogue systems, so that systems can convey potentially ambiguous terms with their intended meaning and can interpret user input correctly. There is considerable evidence that the different uses of individual cue phrases can be distinguished by variation in the prosody with which they are realized. For example, (Hirschberg and Litman, 1993) f</context>
</contexts>
<marker>Schegloff, Sacks, 1973</marker>
<rawString>Emanuel A. Schegloff and Harvey Sacks. 1973. Opening up closings. Semiotica, 8(4):289–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel Ward</author>
<author>Wataru Tsukahara</author>
</authors>
<title>Prosodic features which cue back-channel responses in English and Japanese.</title>
<date>2000</date>
<journal>Journal ofPragmatics,</journal>
<pages>23--1177</pages>
<contexts>
<context position="1377" citStr="Ward and Tsukahara, 2000" startWordPosition="206" endWordPosition="210">SE MARKERS) are linguistic expressions that can be used to convey explicit information about the structure of a discourse or to convey a semantic contribution (Grosz and Sidner, 1986; Reichman, 1985; Cohen, 1984). For example, the word okay can be used to convey a ‘satisfactory’ evaluation of some entity in the discourse (the movie was okay); as a backchannel in a dialogue to indicate that one interlocutor is still attending to another; to convey acknowledgment or agreement; or, in its ‘cue’ use, to start or finish a discourse segment (Jefferson, 1972; Schegloff and Sacks, 1973; Kowtko, 1997; Ward and Tsukahara, 2000). A major question is how speakers indicate and listeners interpret such variation in meaning. From a practical perspective, understanding how speakers and listeners disambiguate cue phrases is important to spoken dialogue systems, so that systems can convey potentially ambiguous terms with their intended meaning and can interpret user input correctly. There is considerable evidence that the different uses of individual cue phrases can be distinguished by variation in the prosody with which they are realized. For example, (Hirschberg and Litman, 1993) found that cue phrases in general could be</context>
</contexts>
<marker>Ward, Tsukahara, 2000</marker>
<rawString>Nigel Ward and Wataru Tsukahara. 2000. Prosodic features which cue back-channel responses in English and Japanese. Journal ofPragmatics, 23:1177–1207.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>