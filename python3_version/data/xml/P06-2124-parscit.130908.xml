<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000209">
<title confidence="0.991905">
BiTAM: Bilingual Topic AdMixture Models for Word Alignment
</title>
<author confidence="0.97012">
Bing Zhao† and Eric P. Xing†t
</author>
<email confidence="0.808298">
fbzhao,epxingl@cs.cmu.edu
</email>
<affiliation confidence="0.63206">
Language Technologies Institute† and Machine Learning Department$
School of Computer Science, Carnegie Mellon University
</affiliation>
<sectionHeader confidence="0.950552" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984869565217">
We propose a novel bilingual topical ad-
mixture (BiTAM) formalism for word
alignment in statistical machine transla-
tion. Under this formalism, the paral-
lel sentence-pairs within a document-pair
are assumed to constitute a mixture of
hidden topics; each word-pair follows a
topic-specific bilingual translation model.
Three BiTAM models are proposed to cap-
ture topic sharing at different levels of lin-
guistic granularity (i.e., at the sentence or
word levels). These models enable word-
alignment process to leverage topical con-
tents of document-pairs. Efficient vari-
ational approximation algorithms are de-
signed for inference and parameter esti-
mation. With the inferred latent topics,
BiTAM models facilitate coherent pairing
of bilingual linguistic entities that share
common topical aspects. Our preliminary
experiments show that the proposed mod-
els improve word alignment accuracy, and
lead to better translation quality.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968015384615">
Parallel data has been treated as sets of unre-
lated sentence-pairs in state-of-the-art statistical
machine translation (SMT) models. Most current
approaches emphasize within-sentence dependen-
cies such as the distortion in (Brown et al., 1993),
the dependency of alignment in HMM (Vogel et
al., 1996), and syntax mappings in (Yamada and
Knight, 2001). Beyond the sentence-level, corpus-
level word-correlation and contextual-level topical
information may help to disambiguate translation
candidates and word-alignment choices. For ex-
ample, the most frequent source words (e.g., func-
tional words) are likely to be translated into words
which are also frequent on the target side; words of
the same topic generally bear correlations and sim-
ilar translations. Extended contextual information
is especially useful when translation models are
vague due to their reliance solely on word-pair co-
occurrence statistics. For example, the word shot
in “It was a nice shot.” should be translated dif-
ferently depending on the context of the sentence:
a goal in the context of sports, or a photo within
the context of sightseeing. Nida (1964) stated
that sentence-pairs are tied by the logic-flow in a
document-pair; in other words, the document-pair
should be word-aligned as one entity instead of be-
ing uncorrelated instances. In this paper, we pro-
pose a probabilistic admixture model to capture
latent topics underlying the context of document-
pairs. With such topical information, the trans-
lation models are expected to be sharper and the
word-alignment process less ambiguous.
Previous works on topical translation models
concern mainly explicit logical representations of
semantics for machine translation. This include
knowledge-based (Nyberg and Mitamura, 1992)
and interlingua-based (Dorr and Habash, 2002)
approaches. These approaches can be expen-
sive, and they do not emphasize stochastic trans-
lation aspects. Recent investigations along this
line includes using word-disambiguation schemes
(Carpua and Wu, 2005) and non-overlapping bilin-
gual word-clusters (Wang et al., 1996; Och, 1999;
Zhao et al., 2005) with particular translation mod-
els, which showed various degrees of success. We
propose a new statistical formalism: Bilingual
Topic AdMixture model, or BiTAM, to facilitate
topic-based word alignment in SMT.
Variants of admixture models have appeared in
population genetics (Pritchard et al., 2000) and
text modeling (Blei et al., 2003). Statistically, an
object is said to be derived from an admixture if it
consists of a bag of elements, each sampled inde-
pendently or coupled in some way, from a mixture
model. In a typical SMT setting, each document-
pair corresponds to an object; depending on a
chosen modeling granularity, all sentence-pairs or
word-pairs in the document-pair correspond to the
elements constituting the object. Correspondingly,
a latent topic is sampled for each pair from a prior
topic distribution to induce topic-specific transla-
tions; and the resulting sentence-pairs and word-
pairs are marginally dependent. Generatively, this
admixture formalism enables word translations to
be instantiated by topic-specific bilingual models
</bodyText>
<page confidence="0.976683">
969
</page>
<note confidence="0.724227">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999780583333333">
and/or monolingual models, depending on their
contexts. In this paper we investigate three in-
stances of the BiTAM model, They are data-driven
and do not need hand-crafted knowledge engineer-
ing.
The remainder of the paper is as follows: in sec-
tion 2, we introduce notations and baselines; in
section 3, we propose the topic admixture models;
in section 4, we present the learning and inference
algorithms; and in section 5 we show experiments
of our models. We conclude with a brief discus-
sion in section 6.
</bodyText>
<sectionHeader confidence="0.820956" genericHeader="introduction">
2 Notations and Baseline
</sectionHeader>
<bodyText confidence="0.9948445">
In statistical machine translation, one typically
uses parallel data to identify entities such as
“word-pair”, “sentence-pair”, and “document-
pair”. Formally, we define the following terms1:
</bodyText>
<listItem confidence="0.9997842">
• A word-pair (fj, ez) is the basic unit for word
alignment, where fj is a French word and ez
is an English word; j and i are the position
indices in the corresponding French sentence
f and English sentence e.
• A sentence-pair (f, e) contains the source
sentence f of a sentence length of J; a target
sentence e of length I. The two sentences f
and e are translations of each other.
• A document-pair (F, E) refers to two doc-
</listItem>
<bodyText confidence="0.6267958">
uments which are translations of each other.
Assuming sentences are one-to-one corre-
spondent, a document-pair has a sequence of
N parallel sentence-pairs {(f,,,, e,,,)}, where
(f,,,, e,,,) is the n&apos;th parallel sentence-pair.
</bodyText>
<listItem confidence="0.999799">
• A parallel corpus C is a collection of M par-
allel document-pairs: {(Fd, Ed)}.
</listItem>
<subsectionHeader confidence="0.96879">
2.1 Baseline: IBM Model-1
</subsectionHeader>
<bodyText confidence="0.999427">
The translation process can be viewed as opera-
tions of word substitutions, permutations, and in-
sertions/deletions (Brown et al., 1993) in noisy-
channel modeling scheme at parallel sentence-pair
level. The translation lexicon p(f|e) is the key
component in this generative process. An efficient
way to learn p(f|e) is IBM-1:
</bodyText>
<equation confidence="0.988074">
p(fj|ei) &apos; p(ei|e). (1)
</equation>
<footnote confidence="0.9655295">
1We follow the notations in (Brown et al., 1993) for
English-French, i.e., e +-* f, although our models are tested,
in this paper, for English-Chinese. We use the end-user ter-
minology for source and target languages.
</footnote>
<bodyText confidence="0.999322571428572">
IBM-1 has global optimum; it is efficient and eas-
ily scalable to large training data; it is one of the
most informative components for re-ranking trans-
lations (Och et al., 2004). We start from IBM-1 as
our baseline model, while higher-order alignment
models can be embedded similarly within the pro-
posed framework.
</bodyText>
<sectionHeader confidence="0.971522" genericHeader="method">
3 Bilingual Topic AdMixture Model
</sectionHeader>
<bodyText confidence="0.9985334">
Now we describe the BiTAM formalism that
captures the latent topical structure and gener-
alizes word alignments and translations beyond
sentence-level via topic sharing across sentence-
pairs:
</bodyText>
<equation confidence="0.9761135">
E* =arg max p(F|E)p(E), (2)
{E}
</equation>
<bodyText confidence="0.999905620689655">
where p(F|E) is a document-level translation
model, generating the document F as one entity.
In a BiTAM model, a document-pair (F, E) is
treated as an admixture of topics, which is induced
by random draws of a topic, from a pool of topics,
for each sentence-pair. A unique normalized and
real-valued vector 0, referred to as a topic-weight
vector, which captures contributions of different
topics, are instantiated for each document-pair, so
that the sentence-pairs with their alignments are
generated from topics mixed according to these
common proportions. Marginally, a sentence-
pair is word-aligned according to a unique bilin-
gual model governed by the hidden topical assign-
ments. Therefore, the sentence-level translations
are coupled, rather than being independent as as-
sumed in the IBM models and their extensions.
Because of this coupling of sentence-pairs (via
topic sharing across sentence-pairs according to
a common topic-weight vector), BiTAM is likely
to improve the coherency of translations by treat-
ing the document as a whole entity, instead of un-
correlated segments that have to be independently
aligned and then assembled. There are at least
two levels at which the hidden topics can be sam-
pled for a document-pair, namely: the sentence-
pair and the word-pair levels. We propose three
variants of the BiTAM model to capture the latent
topics of bilingual documents at different levels.
</bodyText>
<subsectionHeader confidence="0.999224">
3.1 BiTAM-1: The Frameworks
</subsectionHeader>
<bodyText confidence="0.9995656">
In the first BiTAM model, we assume that topics
are sampled at the sentence-level. Each document-
pair is represented as a random mixture of la-
tent topics. Each topic, topic-k, is presented by a
topic-specific word-translation table: Bk, which is
</bodyText>
<equation confidence="0.992399">
i
ri
j=1
p(f|e) =
��
i=1
</equation>
<page confidence="0.992446">
970
</page>
<figure confidence="0.992701">
(a) (b) (c)
</figure>
<figureCaption confidence="0.970825666666667">
Figure 1: BiTAM models for Bilingual document- and sentence-pairs. A node in the graph represents a random variable, and
a hexagon denotes a parameter. Un-shaded nodes are hidden variables. All the plates represent replicates. The outmost plate
(M-plate) represents M bilingual document-pairs, while the inner N-plate represents the N repeated choice of topics for each
sentence-pairs in the document; the inner J-plate represents J word-pairs within each sentence-pair. (a) BiTAM-1 samples
one topic (denoted by z) per sentence-pair; (b) BiTAM-2 utilizes the sentence-level topics for both the translation model (i.e.,
p(f|e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM-3 samples one topic per word-pair.
</figureCaption>
<figure confidence="0.975367677419355">
e
I
e
I
p
e
I
a
α θ
z
a
B
θ
z
f
J
B
a
f J
B
α
θ
z
f
J
N
M
N
M
N
M
</figure>
<bodyText confidence="0.962551142857143">
a translation lexicon: BZ�j�k=p(f=fj|e=eZ, z=k),
where z is an indicator variable to denote the
choice of a topic. Given a specific topic-weight
vector Bd for a document-pair, each sentence-pair
draws its conditionally independent topics from a
mixture of topics. This generative process, for a
document-pair (Fd, Ed), is summarized as below:
</bodyText>
<listItem confidence="0.9797973">
1. Sample sentence-number N from a Poisson(-y).
2. Sample topic-weight vector Bd from a Dirichlet(a).
3. For each sentence-pair (fn, en) in the d&apos;th doc-pair ,
(a) Sample sentence-length Jn from Poisson(b);
(b) Sample a topic zdn from a Multinomial(Bd);
(c) Sample ej from a monolingual model p(ej);
(d) Sample each word alignment link aj from a uni-
form model p(aj) (or an HMM);
(e) Sample each fj according to a topic-specific
translation lexicon p(fj|e, aj, zn, B).
</listItem>
<bodyText confidence="0.9999725">
We assume that, in our model, there are K pos-
sible topics that a document-pair can bear. For
each document-pair, a K-dimensional Dirichlet
random variable Bd, referred to as the topic-weight
vector of the document, can take values in the
(K−1)-simplex following a probability density:
</bodyText>
<equation confidence="0.9948205">
p(Ola) = r(EKk=1 ak) 91 −1 ··· BK−1, (3)
Hk=1 r(ak)
</equation>
<bodyText confidence="0.992620333333333">
where the hyperparameter α is a K-dimension
vector with each component αk&gt;0, and F(�)
is the Gamma function. The alignment is
represented by a J-dimension vector a =
{a1, a2, · · · , ai}; for each French word fj at the
position j, an position variable aj maps it to an
English word eaj at the position aj in English sen-
tence. The word level translation lexicon probabil-
ities are topic-specific, and they are parameterized
by the matrix B = {Bk}.
For simplicity, in our current models we omit
the modelings of the sentence-number N and the
sentence-length Jn, and focus only on the bilin-
gual translation model. Figure 1 (a) shows the
graphical model representation for the BiTAM
generative scheme discussed so far. Note that, the
sentence-pairs are now connected by the node Bd.
Therefore, marginally, the sentence-pairs are not
independent of each other as in traditional SMT
models, instead they are conditionally indepen-
dent given the topic-weight vector Bd. Specifi-
cally, BiTAM-1 assumes that each sentence-pair
has one single topic. Thus, the word-pairs within
this sentence-pair are conditionally independent of
each other given the hidden topic index z of the
sentence-pair.
The last two sub-steps (3.d and 3.e) in the
BiTam sampling scheme define a translation
model, in which an alignment link aj is proposed
and an observation of fj is generated according
to the proposed distributions. We simplify align-
ment model of a, as in IBM-1, by assuming that
aj is sampled uniformly at random. Given the pa-
rameters α, B, and the English part E, the joint
conditional distribution of the topic-weight vector
0, the topic indicators z, the alignment vectors A,
and the document F can be written as:
p(F,A, B, z|E, a, B)=
where N is the number of the sentence-pair.
Marginalizing out 0 and z, we can obtain the
marginal conditional probability of generating F
from E for each document-pair:
</bodyText>
<equation confidence="0.9701985">
p(F, A|E, a, Bzn) =
E �
p(zn|�)p(fn, an|en, Bzn) dB, (5)
n=1
</equation>
<bodyText confidence="0.999746">
where p(fn, an|en, Bzn) is a topic-specific
sentence-level translation model. For simplicity,
we assume that the French words fj’s are condi-
tionally independent of each other; the alignment
</bodyText>
<equation confidence="0.90492825">
N
ri p(zn|O)p(fn, an|en, a, Bzn),
n=1
p(B  |a)
(4)
fp(O|a)(
ri
zn
</equation>
<page confidence="0.972189">
971
</page>
<bodyText confidence="0.999483">
variables aj’s are independent of other variables
and are uniformly distributed a priori. Therefore,
the distribution for each sentence-pair is:
</bodyText>
<equation confidence="0.9258435">
p(fn, an|en, Bzn) = p(fn|en, an, Bzn)p(an|en, Bzn)
p(fnj|eanj , Bzn). (6)
</equation>
<bodyText confidence="0.99991925">
Thus, the conditional likelihood for the entire
parallel corpus is given by taking the product
of the marginal probabilities of each individual
document-pair in Eqn. 5.
</bodyText>
<subsectionHeader confidence="0.999188">
3.2 BiTAM-2: Monolingual Admixture
</subsectionHeader>
<bodyText confidence="0.999991142857143">
In general, the monolingual model for English
can also be a rich topic-mixture. This is real-
ized by using the same topic-weight vector Bd and
the same topic indicator zdn sampled according
to Bd, as described in §3.1, to introduce not only
topic-dependent translation lexicon, but also topic-
dependent monolingual model of the source lan-
guage, English in this case, for generating each
sentence-pair (Figure 1 (b)). Now a is generated
from a topic-based language model Q, instead of a
uniform distribution in BiTAM-1. We refer to this
model as BiTAM-2.
Unlike BiTAM-1, where the information ob-
served in ez is indirectly passed to z via the node
of fj and the hidden variable aj, in BiTAM-2, the
topics of corresponding English and French sen-
tences are also strictly aligned so that the informa-
tion observed in ez can be directly passed to z, in
the hope of finding more accurate topics. The top-
ics are inferred more directly from the observed
bilingual data, and as a result, improve alignment.
</bodyText>
<subsectionHeader confidence="0.995411">
3.3 BiTAM-3: Word-level Admixture
</subsectionHeader>
<bodyText confidence="0.999860333333333">
It is straightforward to extend the sentence-level
BiTAM-1 to a word-level admixture model, by
sampling topic indicator zn,j for each word-pair
(fj, eaj) in the n&apos;th sentence-pair, rather than
once for all (words) in the sentence (Figure 1 (c)).
This gives rise to our BiTAM-3. The conditional
likelihood functions can be obtained by extending
the formulas in §3.1 to move the variable zn,j in-
side the same loop over each of the fn,j.
</bodyText>
<subsectionHeader confidence="0.989352">
3.4 Incorporation of Word “Null”
</subsectionHeader>
<bodyText confidence="0.99993">
Similar to IBM models, “Null” word is used for
the source words which have no translation coun-
terparts in the target language. For example, Chi-
nese words “de” (�) , “ba” (t) and “bei”
(R) generally do not have translations in English.
“Null” is attached to every target sentence to align
the source words which miss their translations.
Specifically, the latent Dirichlet allocation (LDA)
in (Blei et al., 2003) can be viewed as a special
case of the BiTAM-3, in which the target sentence
contains only one word: “Null”, and the alignment
link a is no longer a hidden variable.
</bodyText>
<sectionHeader confidence="0.956178" genericHeader="method">
4 Learning and Inference
</sectionHeader>
<bodyText confidence="0.999955625">
Due to the hybrid nature of the BiTAM models,
exact posterior inference of the hidden variables
A, z and 0 is intractable. A variational inference
is used to approximate the true posteriors of these
hidden variables. The inference scheme is pre-
sented for BiTAM-1; the algorithms for BiTAM-2
and BiTAM-3 are straight forward extensions and
are omitted.
</bodyText>
<subsectionHeader confidence="0.972897">
4.1 Variational Approximation
</subsectionHeader>
<bodyText confidence="0.999763">
To approximate: p(B, z, ASE, F, α, B), the joint
posterior, we use the fully factorized distribution
over the same set of hidden variables:
</bodyText>
<equation confidence="0.943473666666667">
q(θ,z, A) ∝ q(θ|γ, α)·
(7)
q(anj, fnj|ϕnj, en, B),
</equation>
<bodyText confidence="0.998568375">
where the Dirichlet parameter &apos;y, the multino-
mial parameters (O1, · · · , On), and the parameters
(�On1, · · · , �Onin) are known as variational param-
eters, and can be optimized with respect to the
Kullback-Leibler divergence from q(·) to the orig-
inal p(·) via an iterative fixed-point algorithm. It
can be shown that the fixed-point equations for the
variational parameters in BiTAM-1 are as follows:
</bodyText>
<equation confidence="0.9785635">
Nd
γk = αk + X φdnk (8)
n=1
K
3 φdnk ∝ exp Ψ(γk) − Ψ( X ´γk,) ·
k&apos;=1
</equation>
<bodyText confidence="0.999979777777778">
where IF(·) is a digamma function. Note that in
the above formulas Odnk is the variational param-
eter underlying the topic indicator zdn of the n-th
sentence-pair in document d, and it can be used to
predict the topic distribution of that sentence-pair.
Following a variational EM scheme (Beal and
Ghahramani, 2002), we estimate the model pa-
rameters α and B in an unsupervised fashion. Es-
sentially, Eqs. (8-10) above constitute the E-step,
</bodyText>
<equation confidence="0.998365947368421">
´ϕdnji log Bfj,ei,k (9)
K
3 ϕdnji ∝ exp X ´φdnk log Bfj,ei,k , (10)
k=1
XIdn
i=1
3 exp
Jdn
X
j=1
1 Jn
Y
j=1
=
IJn
n
N Jn
Y q(zn |φn) Y
n=1 j=1
</equation>
<page confidence="0.990941">
972
</page>
<bodyText confidence="0.9976835">
where the posterior estimations of the latent vari-
ables are obtained. In the M-step, we update a
and B so that they improve a lower bound of the
log-likelihood defined bellow:
</bodyText>
<equation confidence="0.7856016">
L(γ, φ, ϕ; α, B)=Eq[log p(θ|α)]+Eq[log p(z|θ)]
+Eq[log p(a)]+Eq[log p(f|z, a, B)]−Eq[log q(θ)]
−Eq[log q(z)]−Eq[log q(a)]. (11)
The close-form iterative updating formula B is:
δ(f, fj)δ(e, ei)φdnkϕdnji (12)
</equation>
<bodyText confidence="0.999427666666667">
For a, close-form update is not available, and we
resort to gradient accent as in (Sj¨olander et al.,
1996) with re-starts to ensure each updated ak&gt;0.
</bodyText>
<subsectionHeader confidence="0.998241">
4.2 Data Sparseness and Smoothing
</subsectionHeader>
<bodyText confidence="0.999987352941177">
The translation lexicons Bf,e,k have a potential
size of V 2K, assuming the vocabulary sizes for
both languages are V . The data sparsity (i.e.,
lack of large volume of document-pairs) poses a
more serious problem in estimating Bf,e,k than
the monolingual case, for instance, in (Blei et
al., 2003). To reduce the data sparsity problem,
we introduce two remedies in our models. First:
Laplace smoothing. In this approach, the matrix
set B, whose columns correspond to parameters
of conditional multinomial distributions, is treated
as a collection of random vectors all under a sym-
metric Dirichlet prior; the posterior expectation of
these multinomial parameter vectors can be esti-
mated using Bayesian theory. Second: interpola-
tion smoothing. Empirically, we can employ a lin-
ear interpolation with IBM-1 to avoid overfitting:
</bodyText>
<equation confidence="0.956384">
Bf,e,k = λBf,e,k+(1−λ)p(f|e). (13)
</equation>
<bodyText confidence="0.998759">
As in Eqn. 1, p(f|e) is learned via IBM-1; A is
estimated via EM on held out data.
</bodyText>
<subsectionHeader confidence="0.999661">
4.3 Retrieving Word Alignments
</subsectionHeader>
<bodyText confidence="0.9996655">
Two word-alignment retrieval schemes are de-
signed for BiTAMs: the uni-direction alignment
(UDA) and the bi-direction alignment (BDA). Both
use the posterior mean of the alignment indica-
tors adnji, captured by what we call the poste-
rior alignment matrix co - {codnji}. UDA uses
a French word fdnj (at the j&apos;th position of n&apos;th
sentence in the d&apos;th document) to query co to get
the best aligned English word (by taking the max-
imum point in a row of co):
</bodyText>
<equation confidence="0.9820945">
adnj = arg max ϕdnji. (14)
iE[1,Idn]
</equation>
<bodyText confidence="0.999644636363637">
BDA selects iteratively, for each f, the best
aligned e, such that the word-pair (f, e) is the
maximum of both row and column, or its neigh-
bors have more aligned pairs than the other
combpeting candidates.
A close check of {codnji} in Eqn. 10 re-
veals that it is essentially an exponential model:
weighted log probabilities from individual topic-
specific translation lexicons; or it can be viewed
as weighted geometric mean of the individual lex-
icon’s strength.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.998771571428571">
We evaluate BiTAM models on the word align-
ment accuracy and the translation quality. For
word alignment accuracy, F-measure is reported,
i.e., the harmonic mean of precision and recall
against a gold-standard reference set; for transla-
tion quality, Bleu (Papineni et al., 2002) and its
variation of NIST scores are reported.
</bodyText>
<tableCaption confidence="0.999594">
Table 1: Training and Test Data Statistics
</tableCaption>
<table confidence="0.998773285714286">
Train #Doc. #Sent. English #Tokens
Chinese
Treebank 316 4172 133K 105K
FBIS.BJ 6,111 105K 4.18M 3.54M
Sinorama 2,373 103K 3.81M 3.60M
XinHua 19,140 115K 3.85M 3.93M
Test 95 627 25,500 19,726
</table>
<bodyText confidence="0.999343055555556">
We have two training data settings with dif-
ferent sizes (see Table 1). The small one
consists of 316 document-pairs from Tree-
bank (LDC2002E17). For the large training
data setting, we collected additional document-
pairs from FBIS (LDC2003E14, Beijing part),
Sinorama (LDC2002E58), and Xinhua News
(LDC2002E18, document boundaries are kept in
our sentence-aligner (Zhao and Vogel, 2002)).
There are 27,940 document-pairs, containing
327K sentence-pairs or 12 million (12M) English
tokens and 11M Chinese tokens. To evaluate word
alignment, we hand-labeled 627 sentence-pairs
from 95 document-pairs sampled from TIDES’01
dryrun data. It contains 14,769 alignment-links.
To evaluate translation quality, TIDES’02 Eval.
test is used as development set, and TIDES’03
Eval. test is used as the unseen test data.
</bodyText>
<subsectionHeader confidence="0.99647">
5.1 Model Settings
</subsectionHeader>
<bodyText confidence="0.999444">
First, we explore the effects of Null word and
smoothing strategies. Empirically, we find that
adding “Null” word is always beneficial to all
models regardless of number of topics selected.
</bodyText>
<figure confidence="0.993688583333333">
Idn
i=1
Bf,e,k ∝
Nd
E
n=1
M
E
d
Jd n
E
j=1
</figure>
<page confidence="0.996232">
973
</page>
<table confidence="0.999762333333333">
Topics-Lexicons Topic-1 Topic-2 Topic-3 Cooc. IBM-1 HMM IBM-4
p(ChaoXian (#A&amp;_T)jKorean) 0.0612 0.2138 0.2254 38 0.2198 0.2157 0.2104
p(HanGuo (¸I)jKorean) 0.8379 0.6116 0.0243 46 0.5619 0.4723 0.4993
</table>
<tableCaption confidence="0.899794333333333">
Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM-1. The third lexicon (Topic-3) prefers to translate
the word Korean into ChaoXian (#A&amp;_T:North Korean). The co-occurrence (Cooc), IBM-1&amp;4 and HMM only prefer to translate
into HanGuo (¸I:South Korean). The two candidate translations may both fade out in the learned translation lexicons.
</tableCaption>
<table confidence="0.96039">
Unigram-rank 1 2 3 4 5 6 7 8 9 10
Topic A. foreign china u.s. development trade enterprises technology countries year economic
Topic B. chongqing companies takeovers company city billion more economic reached yuan
Topic C. sports disabled team people cause water national games handicapped members
</table>
<tableCaption confidence="0.97944">
Table 3: Three most distinctive topics are displayed. The English words for each topic are ranked according to p(ejz)
estimated from the topic-specific English sentences weighted by {0drr,kg. 33 functional words were removed to highlight the
main content of each topic. Topic A is about Us-China economic relationships; Topic B relates to Chinese companies’ merging;
Topic C shows the sports of handicapped people.
</tableCaption>
<bodyText confidence="0.99990680952381">
The interpolation smoothing in §4.2 is effec-
tive, and it gives slightly better performance than
Laplace smoothing over different number of topics
for BiTAM-1. However, the interpolation lever-
ages the competing baseline lexicon, and this can
blur the evaluations of BiTAM’s contributions.
Laplace smoothing is chosen to emphasize more
on BiTAM’s strength. Without any smoothing, F-
measure drops very quickly over two topics. In all
our following experiments, we use both Null word
and Laplace smoothing for the BiTAM models.
We train, for comparison, IBM-1&amp;4 and HMM
models with 8 iterations of IBM-1, 7 for HMM
and 3 for IBM-4 (18h743) with Null word and a
maximum fertility of 3 for Chinese-English.
Choosing the number of topics is a model se-
lection problem. We performed a ten-fold cross-
validation, and a setting of three-topic is cho-
sen for both the small and the large training data
sets. The overall computation complexity of the
BiTAM is linear to the number of hidden topics.
</bodyText>
<subsectionHeader confidence="0.998816">
5.2 Variational Inference
</subsectionHeader>
<bodyText confidence="0.90086815">
Under a non-symmetric Dirichlet prior, hyperpa-
rameter α is initialized randomly; B (K transla-
tion lexicons) are initialized uniformly as did in
IBM-1. Better initialization of B can help to avoid
local optimal as shown in § 5.5.
With the learned B and α fixed, the variational
parameters to be computed in Eqn. (8-10) are ini-
tialized randomly; the fixed-point iterative updates
stop when the change of the likelihood is smaller
than 10−5. The convergent variational parameters,
corresponding to the highest likelihood from 20
random restarts, are used for retrieving the word
alignment for unseen document-pairs. To estimate
B, Q (for BiTAM-2) and α, at most eight varia-
tional EM iterations are run on the training data.
Figure 2 shows absolute 2-3% better F-measure
over iterations of variational EM using two and
three topics of BiTAM-1 comparing with IBM-1.
BiTam with Null and Laplace Smoothing Over Var. EM Iterations
Number of EM/Variational EM Iterations for IBM−1 and BiTam−1
</bodyText>
<figureCaption confidence="0.98261425">
Figure 2: performances over eight Variational EM itera-
tions of BiTAM-1 using both the “Null” word and the laplace
smoothing; IBM-1 is shown over eight EM iterations for
comparison.
</figureCaption>
<subsectionHeader confidence="0.997585">
5.3 Topic-Specific Translation Lexicons
</subsectionHeader>
<bodyText confidence="0.999991263157895">
The topic-specific lexicons Bk are smaller in size
than IBM-1, and, typically, they contain topic
trends. For example, in our training data, North
Korean is usually related to politics and translated
into “ChaoXian” (V ); South Korean occurs
more often with economics and is translated as
“HanGuo”(¸I). BiTAMs discriminate the two
by considering the topics of the context. Table 2
shows the lexicon entries for “Korean” learned
by a 3-topic BiTAM-1. The values are relatively
sharper, and each clearly favors one of the candi-
dates. The co-occurrence count, however, only fa-
vors “HanGuo”, and this can easily dominate the
decisions of IBM and HMM models due to their
ignorance of the topical context. Monolingual
topics learned by BiTAMs are, roughly speak-
ing, fuzzy especially when the number of topics is
small. With proper filtering, we find that BiTAMs
do capture some topics as illustrated in Table 3.
</bodyText>
<subsectionHeader confidence="0.99832">
5.4 Evaluating Word Alignments
</subsectionHeader>
<bodyText confidence="0.999932">
We evaluate word alignment accuracies in vari-
ous settings. Notably, BiTAM allows to test align-
ments in two directions: English-to-Chinese (EC)
and Chinese-to-English (CE). Additional heuris-
tics are applied to further improve the accura-
cies. Inter takes the intersection of the two direc-
tions and generates high-precision alignments; the
</bodyText>
<figure confidence="0.952683642857143">
32
3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8
BiTam−1, Topic #=3
BiTam−1, Topic #=2
IBM−1
41
40
39
38
37
36
35
34
33
</figure>
<page confidence="0.992314">
974
</page>
<table confidence="0.999329888888889">
SETTING IBM-1 HMM IBM-4 BITAM-1 BITAM-2 BITAM-3
UDA BDA UDA BDA UDA BDA
CE (%) 36.27 43.00 45.00 40.13 48.26 40.26 48.63 40.47 49.02
EC (%) 32.94 44.26 45.96 36.52 46.61 37.35 46.30 37.54 46.62
REFINED (%) 41.71 44.40 48.42 45.06 49.02 47.20 47.61 47.46 48.18
UNION (%) 32.18 42.94 43.75 35.87 48.66 36.07 48.99 36.26 49.35
INTER (%) 39.86 44.87 48.65 43.65 43.85 44.91 45.18 45.13 45.48
NIST 6.458 6.822 6.926 6.937 6.954 6.904 6.976 6.967 6.962
BLEU 15.70 17.70 18.25 17.93 18.14 18.13 18.05 18.11 18.25
</table>
<tableCaption confidence="0.985337">
Table 4: Word Alignment Accuracy (F-measure) and Machine Translation Quality for BiTAM Models, comparing with IBM
Models, and HMMs with a training scheme of 18h743 on the Treebank data listed in Table 1. For each column, the highlighted
alignment (the best one under that model setting) is picked up to further evaluate the translation quality.
</tableCaption>
<bodyText confidence="0.999926263157895">
Union of two directions gives high-recall; Refined
grows the intersection with the neighboring word-
pairs seen in the union, and yields high-precision
and high-recall alignments.
As shown in Table 4, the baseline IBM-1 gives
its best performance of 36.27% in the CE direc-
tion; the UDA alignments from BiTAM-1-3 give
40.13%, 40.26%, and 40.47%, respectively, which
are significantly better than IBM-1. A close look
at the three BiTAMs does not yield significant dif-
ference. BiTAM-3 is slightly better in most set-
tings; BiTAM-1 is slightly worse than the other
two, because the topics sampled at the sentence
level are not very concentrated. The BDA align-
ments of BiTAM-1-3 yield 48.26%, 48.63% and
49.02%, which are even better than HMM and
IBM-4 — their best performances are at 44.26%
and 45.96%, respectively. This is because BDA
partially utilizes similar heuristics on the approx-
imated posterior matrix JcOdnjij instead of di-
rect operations on alignments of two directions
in the heuristics of Refined. Practically, we also
apply BDA together with heuristics for IBM-1,
HMM and IBM-4, and the best achieved perfor-
mances are at 40.56%, 46.52% and 49.18%, re-
spectively. Overall, BiTAM models achieve per-
formances close to or higher than HMM, using
only a very simple IBM-1 style alignment model.
Similar improvements over IBM models and
HMM are preserved after applying the three kinds
of heuristics in the above. As expected, since BDA
already encodes some heuristics, it is only slightly
improved with the Union heuristic; UDA, similar
to the viterbi style alignment in IBM and HMM, is
improved better by the Refined heuristic.
We also test BiTAM-3 on large training data,
and similar improvements are observed over those
of the baseline models (see Table. 5).
</bodyText>
<subsectionHeader confidence="0.967051">
5.5 Boosting BiTAM Models
</subsectionHeader>
<bodyText confidence="0.999977714285714">
The translation lexicons of Bf e k are initialized
uniformly in our previous experiments. Better ini-
tializations can potentially lead to better perfor-
mances because it can help to avoid the unde-
sirable local optima in variational EM iterations.
We use the lexicons from IBM Model-4 to initial-
ize Bf e k to boost the BiTAM models. This is
one way of applying the proposed BiTAM mod-
els into current state-of-the-art SMT systems for
further improvement. The boosted alignments are
denoted as BUDA and BBDA in Table. 5, cor-
responding to the uni-direction and bi-direction
alignments, respectively. We see an improvement
in alignment quality.
</bodyText>
<subsectionHeader confidence="0.986874">
5.6 Evaluating Translations
</subsectionHeader>
<bodyText confidence="0.999940214285714">
To further evaluate our BiTAM models, word
alignments are used in a phrase-based decoder
for evaluating translation qualities. Similar to
the Pharoah package (Koehn, 2004), we extract
phrase-pairs directly from word alignment to-
gether with coherence constraints (Fox, 2002) to
remove noisy ones. We use TIDES Eval’02 CE
test set as development data to tune the decoder
parameters; the Eval’03 data (919 sentences) is the
unseen data. A trigram language model is built
using 180 million English words. Across all the
reported comparative settings, the key difference
is the bilingual ngram-identity of the phrase-pair,
which is collected directly from the underlying
word alignment.
Shown in Table 4 are results for the small-
data track; the large-data track results are in Ta-
ble 5. For the small-data track, the baseline Bleu
scores for IBM-1, HMM and IBM-4 are 15.70,
17.70 and 18.25, respectively. The UDA align-
ment of BiTAM-1 gives an improvement over
the baseline IBM-1 from 15.70 to 17.93, and
it is close to HMM’s performance, even though
BiTAM doesn’t exploit any sequential structures
of words. The proposed BiTAM-2 and BiTAM-
3 are slightly better than BiTAM-1. Similar im-
provements are observed for the large-data track
(see Table 5). Note that, the boosted BiTAM-3 us-
</bodyText>
<page confidence="0.995017">
975
</page>
<table confidence="0.999362222222222">
SETTING IBM-1 HMM IBM-4 UDA BITAM-3 BBDA
BDA BUDA
CE (%) 46.73 49.12 54.17 50.55 56.27 55.80 57.02
EC (%) 44.33 54.56 55.08 51.59 55.18 54.76 58.76
REFINED (%) 54.64 56.39 58.47 56.45 54.57 58.26 56.23
UNION (%) 42.47 51.59 52.67 50.23 57.81 56.19 58.66
INTER (%) 52.24 54.69 57.74 52.44 52.71 54.70 55.35
NIST 7.59 7.77 7.83 7.64 7.68 8.10 8.23
BLEU 19.19 21.99 23.18 21.20 21.43 22.97 24.07
</table>
<tableCaption confidence="0.9971265">
Table 5: Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models, IBM Models,
HMMs, and boosted BiTAMs using all the training data listed in Table. 1. Other experimental conditions are similar to Table. 4.
</tableCaption>
<bodyText confidence="0.9998918">
ing IBM-4 as the seed lexicon, outperform the Re-
fined IBM-4: from 23.18 to 24.07 on Bleu score,
and from 7.83 to 8.23 on NIST. This result sug-
gests a straightforward way to leverage BiTAMs
to improve statistical machine translations.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999952583333333">
In this paper, we proposed novel formalism for
statistical word alignment based on bilingual ad-
mixture (BiTAM) models. Three BiTAM mod-
els were proposed and evaluated on word align-
ment and translation qualities against state-of-
the-art translation models. The proposed mod-
els significantly improve the alignment accuracy
and lead to better translation qualities. Incorpo-
ration of within-sentence dependencies such as
the alignment-jumps and distortions, and a better
treatment of the source monolingual model worth
further investigations.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997800166666667">
M. J. Beal and Zoubin Ghahramani. 2002. The variational
bayesian em algorithm for incomplete data: with appli-
cation to scoring graphical model structures. In Bayesian
Statistics 7.
David Blei, Andrew NG, and M.I. Jordan. 2003. Latent
dirichlet allocation. In Journal of Machine Learning
Research, volume 3, pages 1107–1135.
P.F. Brown, Stephen A. Della Pietra, Vincent. J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. In
Computational Linguistics, volume 19(2), pages 263–331.
Marine Carpua and Dekai Wu. 2005. Evaluating the word
sense disambiguation performance of statistical machine
translation. In Second International Joint Conference on
Natural Language Processing (IJCNLP-2005).
Bonnie Dorr and Nizar Habash. 2002. Interlingua approxi-
mation: A generation-heavy approach. In In Proceedings
of Workshop on Interlingua Reliability, Fifth Conference
of the Association for Machine Translation in the Ameri-
cas, AMTA-2002, Tiburon, CA.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, pages 304–
311, Philadelphia, PA, July 6-7.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based smt. In Proceedings of the Conference of
the Association for Machine Translation in the Americans
(AMTA).
Eugene A. Nida. 1964. Toward a Science of Translating:
With Special Reference to Principles Involved in Bible
Translating. Leiden, Netherlands: E.J. Brill.
Eric Nyberg and Truko Mitamura. 1992. The kant system:
Fast, accurate, high-quality translation in practical do-
mains. In Proceedings of COLING-92.
Franz J. Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin
Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin,
and Dragomir Radev. 2004. A smorgasbord of features
for statistical machine translation. In HLT/NAACL:
Human Language Technology Conference, volume 1:29,
pages 161–168.
Franz J. Och. 1999. An efficient method for determining
bilingal word classes. In Ninth Conf. of the Europ.
Chapter of the Association for Computational Linguistics
(EACL’99), pages 71–76.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proc. of the 40th Annual Conf. of
the Association for Computational Linguistics (ACL 02),
pages 311–318, Philadelphia, PA, July.
J. Pritchard, M. Stephens, and P. Donnell. 2000. Inference
of population structure using multilocus genotype data.
In Genetics, volume 155, pages 945–959.
K. Sj¨olander, K. Karplus, M. Brown, R. Hughey, A. Krogh,
I.S. Mian, and D. Haussler. 1996. Dirichlet mixtures: A
method for improving detection of weak but significant
protein sequence homology. Computer Applications in
the Biosciences, 12.
S. Vogel, Hermann Ney, and C. Tillmann. 1996. Hmm
based word alignment in statistical machine translation.
In Proc. The 16th Int. Conf. on Computational Lingustics,
(Coling’96), pages 836–841, Copenhagen, Denmark.
Yeyi Wang, John Lafferty, and Alex Waibel. 1996. Word
clustering with parallel spoken language corpora. In pro-
ceedings of the 4th International Conference on Spoken
Language Processing (ICSLP’96), pages 2364–2367.
K. Yamada and Kevin. Knight. 2001. Syntax-based statisti-
cal translation model. In Proceedings of the Conference
of the Association for Computational Linguistics (ACL-
2001).
Bing Zhao and Stephan Vogel. 2002. Adaptive parallel
sentences mining from web bilingual news collection. In
The 2002 IEEEInternational Conference on Data Mining.
Bing Zhao, Eric P. Xing, and Alex Waibel. 2005. Bilingual
word spectral clustering for statistical machine translation.
In Proceedings of the ACL Workshop on Building and
Using Parallel Texts, pages 25–32, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.99882">
976
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.324768">
<title confidence="0.985036">BiTAM: Bilingual Topic AdMixture Models for Word Alignment</title>
<author confidence="0.578459">P</author>
<email confidence="0.999282">fbzhao,epxingl@cs.cmu.edu</email>
<affiliation confidence="0.794706">Technologies Machine Learning School of Computer Science, Carnegie Mellon University</affiliation>
<abstract confidence="0.997880625">We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable wordalignment process to leverage topical contents of document-pairs. Efficient variational approximation algorithms are designed for inference and parameter estimation. With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M J Beal</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>The variational bayesian em algorithm for incomplete data: with application to scoring graphical model structures.</title>
<date>2002</date>
<journal>In Bayesian Statistics</journal>
<volume>7</volume>
<contexts>
<context position="16854" citStr="Beal and Ghahramani, 2002" startWordPosition="2706" endWordPosition="2709">d can be optimized with respect to the Kullback-Leibler divergence from q(·) to the original p(·) via an iterative fixed-point algorithm. It can be shown that the fixed-point equations for the variational parameters in BiTAM-1 are as follows: Nd γk = αk + X φdnk (8) n=1 K 3 φdnk ∝ exp Ψ(γk) − Ψ( X ´γk,) · k&apos;=1 where IF(·) is a digamma function. Note that in the above formulas Odnk is the variational parameter underlying the topic indicator zdn of the n-th sentence-pair in document d, and it can be used to predict the topic distribution of that sentence-pair. Following a variational EM scheme (Beal and Ghahramani, 2002), we estimate the model parameters α and B in an unsupervised fashion. Essentially, Eqs. (8-10) above constitute the E-step, ´ϕdnji log Bfj,ei,k (9) K 3 ϕdnji ∝ exp X ´φdnk log Bfj,ei,k , (10) k=1 XIdn i=1 3 exp Jdn X j=1 1 Jn Y j=1 = IJn n N Jn Y q(zn |φn) Y n=1 j=1 972 where the posterior estimations of the latent variables are obtained. In the M-step, we update a and B so that they improve a lower bound of the log-likelihood defined bellow: L(γ, φ, ϕ; α, B)=Eq[log p(θ|α)]+Eq[log p(z|θ)] +Eq[log p(a)]+Eq[log p(f|z, a, B)]−Eq[log q(θ)] −Eq[log q(z)]−Eq[log q(a)]. (11) The close-form iterative</context>
</contexts>
<marker>Beal, Ghahramani, 2002</marker>
<rawString>M. J. Beal and Zoubin Ghahramani. 2002. The variational bayesian em algorithm for incomplete data: with application to scoring graphical model structures. In Bayesian Statistics 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew NG</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>In Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1107--1135</pages>
<contexts>
<context position="3619" citStr="Blei et al., 2003" startWordPosition="523" endWordPosition="526">roaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. In a typical SMT setting, each documentpair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object. Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and wordpairs are marginally dependent. Ge</context>
<context position="15304" citStr="Blei et al., 2003" startWordPosition="2435" endWordPosition="2438">e to our BiTAM-3. The conditional likelihood functions can be obtained by extending the formulas in §3.1 to move the variable zn,j inside the same loop over each of the fn,j. 3.4 Incorporation of Word “Null” Similar to IBM models, “Null” word is used for the source words which have no translation counterparts in the target language. For example, Chinese words “de” (�) , “ba” (t) and “bei” (R) generally do not have translations in English. “Null” is attached to every target sentence to align the source words which miss their translations. Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM-3, in which the target sentence contains only one word: “Null”, and the alignment link a is no longer a hidden variable. 4 Learning and Inference Due to the hybrid nature of the BiTAM models, exact posterior inference of the hidden variables A, z and 0 is intractable. A variational inference is used to approximate the true posteriors of these hidden variables. The inference scheme is presented for BiTAM-1; the algorithms for BiTAM-2 and BiTAM-3 are straight forward extensions and are omitted. 4.1 Variational Approximation To approximate: p(B, z, AS</context>
<context position="17993" citStr="Blei et al., 2003" startWordPosition="2906" endWordPosition="2909">, a, B)]−Eq[log q(θ)] −Eq[log q(z)]−Eq[log q(a)]. (11) The close-form iterative updating formula B is: δ(f, fj)δ(e, ei)φdnkϕdnji (12) For a, close-form update is not available, and we resort to gradient accent as in (Sj¨olander et al., 1996) with re-starts to ensure each updated ak&gt;0. 4.2 Data Sparseness and Smoothing The translation lexicons Bf,e,k have a potential size of V 2K, assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003). To reduce the data sparsity problem, we introduce two remedies in our models. First: Laplace smoothing. In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory. Second: interpolation smoothing. Empirically, we can employ a linear interpolation with IBM-1 to avoid overfitting: Bf,e,k = λBf,e,k+(1−λ)p(f|e). (13) As in Eqn. 1, p(f|e) is learned via I</context>
</contexts>
<marker>Blei, NG, Jordan, 2003</marker>
<rawString>David Blei, Andrew NG, and M.I. Jordan. 2003. Latent dirichlet allocation. In Journal of Machine Learning Research, volume 3, pages 1107–1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>19</volume>
<issue>2</issue>
<pages>263--331</pages>
<marker>Pietra, Mercer, 1993</marker>
<rawString>P.F. Brown, Stephen A. Della Pietra, Vincent. J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. In Computational Linguistics, volume 19(2), pages 263–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpua</author>
<author>Dekai Wu</author>
</authors>
<title>Evaluating the word sense disambiguation performance of statistical machine translation.</title>
<date>2005</date>
<booktitle>In Second International Joint Conference on Natural Language Processing (IJCNLP-2005).</booktitle>
<contexts>
<context position="3187" citStr="Carpua and Wu, 2005" startWordPosition="458" endWordPosition="461">nt topics underlying the context of documentpairs. With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixtur</context>
</contexts>
<marker>Carpua, Wu, 2005</marker>
<rawString>Marine Carpua and Dekai Wu. 2005. Evaluating the word sense disambiguation performance of statistical machine translation. In Second International Joint Conference on Natural Language Processing (IJCNLP-2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
<author>Nizar Habash</author>
</authors>
<title>Interlingua approximation: A generation-heavy approach. In</title>
<date>2002</date>
<booktitle>In Proceedings of Workshop on Interlingua Reliability, Fifth Conference of the Association for Machine Translation in the Americas, AMTA-2002,</booktitle>
<location>Tiburon, CA.</location>
<contexts>
<context position="2979" citStr="Dorr and Habash, 2002" startWordPosition="429" endWordPosition="432">flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances. In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of documentpairs. With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 200</context>
</contexts>
<marker>Dorr, Habash, 2002</marker>
<rawString>Bonnie Dorr and Nizar Habash. 2002. Interlingua approximation: A generation-heavy approach. In In Proceedings of Workshop on Interlingua Reliability, Fifth Conference of the Association for Machine Translation in the Americas, AMTA-2002, Tiburon, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>304--311</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="29709" citStr="Fox, 2002" startWordPosition="4785" endWordPosition="4786"> This is one way of applying the proposed BiTAM models into current state-of-the-art SMT systems for further improvement. The boosted alignments are denoted as BUDA and BBDA in Table. 5, corresponding to the uni-direction and bi-direction alignments, respectively. We see an improvement in alignment quality. 5.6 Evaluating Translations To further evaluate our BiTAM models, word alignments are used in a phrase-based decoder for evaluating translation qualities. Similar to the Pharoah package (Koehn, 2004), we extract phrase-pairs directly from word alignment together with coherence constraints (Fox, 2002) to remove noisy ones. We use TIDES Eval’02 CE test set as development data to tune the decoder parameters; the Eval’03 data (919 sentences) is the unseen data. A trigram language model is built using 180 million English words. Across all the reported comparative settings, the key difference is the bilingual ngram-identity of the phrase-pair, which is collected directly from the underlying word alignment. Shown in Table 4 are results for the smalldata track; the large-data track results are in Table 5. For the small-data track, the baseline Bleu scores for IBM-1, HMM and IBM-4 are 15.70, 17.70</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In Proc. of the Conference on Empirical Methods in Natural Language Processing, pages 304– 311, Philadelphia, PA, July 6-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based smt.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference of the Association for Machine Translation in the Americans (AMTA).</booktitle>
<contexts>
<context position="29607" citStr="Koehn, 2004" startWordPosition="4771" endWordPosition="4772">onal EM iterations. We use the lexicons from IBM Model-4 to initialize Bf e k to boost the BiTAM models. This is one way of applying the proposed BiTAM models into current state-of-the-art SMT systems for further improvement. The boosted alignments are denoted as BUDA and BBDA in Table. 5, corresponding to the uni-direction and bi-direction alignments, respectively. We see an improvement in alignment quality. 5.6 Evaluating Translations To further evaluate our BiTAM models, word alignments are used in a phrase-based decoder for evaluating translation qualities. Similar to the Pharoah package (Koehn, 2004), we extract phrase-pairs directly from word alignment together with coherence constraints (Fox, 2002) to remove noisy ones. We use TIDES Eval’02 CE test set as development data to tune the decoder parameters; the Eval’03 data (919 sentences) is the unseen data. A trigram language model is built using 180 million English words. Across all the reported comparative settings, the key difference is the bilingual ngram-identity of the phrase-pair, which is collected directly from the underlying word alignment. Shown in Table 4 are results for the smalldata track; the large-data track results are in</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based smt. In Proceedings of the Conference of the Association for Machine Translation in the Americans (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene A Nida</author>
</authors>
<title>Toward a Science of Translating: With Special Reference to Principles Involved in Bible Translating.</title>
<date>1964</date>
<publisher>E.J. Brill.</publisher>
<location>Leiden, Netherlands:</location>
<contexts>
<context position="2307" citStr="Nida (1964)" startWordPosition="335" endWordPosition="336"> choices. For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair cooccurrence statistics. For example, the word shot in “It was a nice shot.” should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing. Nida (1964) stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances. In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of documentpairs. With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based (</context>
</contexts>
<marker>Nida, 1964</marker>
<rawString>Eugene A. Nida. 1964. Toward a Science of Translating: With Special Reference to Principles Involved in Bible Translating. Leiden, Netherlands: E.J. Brill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Nyberg</author>
<author>Truko Mitamura</author>
</authors>
<title>The kant system: Fast, accurate, high-quality translation in practical domains.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92.</booktitle>
<contexts>
<context position="2933" citStr="Nyberg and Mitamura, 1992" startWordPosition="423" endWordPosition="426"> stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances. In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of documentpairs. With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared</context>
</contexts>
<marker>Nyberg, Mitamura, 1992</marker>
<rawString>Eric Nyberg and Truko Mitamura. 1992. The kant system: Fast, accurate, high-quality translation in practical domains. In Proceedings of COLING-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In HLT/NAACL: Human Language Technology Conference,</booktitle>
<volume>1</volume>
<pages>161--168</pages>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz J. Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord of features for statistical machine translation. In HLT/NAACL: Human Language Technology Conference, volume 1:29, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>An efficient method for determining bilingal word classes.</title>
<date>1999</date>
<booktitle>In Ninth Conf. of the Europ. Chapter of the Association for Computational Linguistics (EACL’99),</booktitle>
<pages>71--76</pages>
<contexts>
<context position="3261" citStr="Och, 1999" startWordPosition="471" endWordPosition="472"> translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. In a typical SMT setting, each documentpair corresponds to an obj</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz J. Och. 1999. An efficient method for determining bilingal word classes. In Ninth Conf. of the Europ. Chapter of the Association for Computational Linguistics (EACL’99), pages 71–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="19910" citStr="Papineni et al., 2002" startWordPosition="3221" endWordPosition="3224"> its neighbors have more aligned pairs than the other combpeting candidates. A close check of {codnji} in Eqn. 10 reveals that it is essentially an exponential model: weighted log probabilities from individual topicspecific translation lexicons; or it can be viewed as weighted geometric mean of the individual lexicon’s strength. 5 Experiments We evaluate BiTAM models on the word alignment accuracy and the translation quality. For word alignment accuracy, F-measure is reported, i.e., the harmonic mean of precision and recall against a gold-standard reference set; for translation quality, Bleu (Papineni et al., 2002) and its variation of NIST scores are reported. Table 1: Training and Test Data Statistics Train #Doc. #Sent. English #Tokens Chinese Treebank 316 4172 133K 105K FBIS.BJ 6,111 105K 4.18M 3.54M Sinorama 2,373 103K 3.81M 3.60M XinHua 19,140 115K 3.85M 3.93M Test 95 627 25,500 19,726 We have two training data settings with different sizes (see Table 1). The small one consists of 316 document-pairs from Treebank (LDC2002E17). For the large training data setting, we collected additional documentpairs from FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), and Xinhua News (LDC2002E18, document </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pritchard</author>
<author>M Stephens</author>
<author>P Donnell</author>
</authors>
<title>Inference of population structure using multilocus genotype data.</title>
<date>2000</date>
<journal>In Genetics,</journal>
<volume>155</volume>
<pages>945--959</pages>
<contexts>
<context position="3581" citStr="Pritchard et al., 2000" startWordPosition="516" endWordPosition="519">orr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. In a typical SMT setting, each documentpair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object. Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and </context>
</contexts>
<marker>Pritchard, Stephens, Donnell, 2000</marker>
<rawString>J. Pritchard, M. Stephens, and P. Donnell. 2000. Inference of population structure using multilocus genotype data. In Genetics, volume 155, pages 945–959.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sj¨olander</author>
<author>K Karplus</author>
<author>M Brown</author>
<author>R Hughey</author>
<author>A Krogh</author>
<author>I S Mian</author>
<author>D Haussler</author>
</authors>
<title>Dirichlet mixtures: A method for improving detection of weak but significant protein sequence homology.</title>
<date>1996</date>
<journal>Computer Applications in the Biosciences,</journal>
<volume>12</volume>
<marker>Sj¨olander, Karplus, Brown, Hughey, Krogh, Mian, Haussler, 1996</marker>
<rawString>K. Sj¨olander, K. Karplus, M. Brown, R. Hughey, A. Krogh, I.S. Mian, and D. Haussler. 1996. Dirichlet mixtures: A method for improving detection of weak but significant protein sequence homology. Computer Applications in the Biosciences, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>Hermann Ney</author>
<author>C Tillmann</author>
</authors>
<title>Hmm based word alignment in statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proc. The 16th Int. Conf. on Computational Lingustics, (Coling’96),</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1481" citStr="Vogel et al., 1996" startWordPosition="206" endWordPosition="209">rence and parameter estimation. With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality. 1 Introduction Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001). Beyond the sentence-level, corpuslevel word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair cooccurrence sta</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, Hermann Ney, and C. Tillmann. 1996. Hmm based word alignment in statistical machine translation. In Proc. The 16th Int. Conf. on Computational Lingustics, (Coling’96), pages 836–841, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yeyi Wang</author>
<author>John Lafferty</author>
<author>Alex Waibel</author>
</authors>
<title>Word clustering with parallel spoken language corpora.</title>
<date>1996</date>
<booktitle>In proceedings of the 4th International Conference on Spoken Language Processing (ICSLP’96),</booktitle>
<pages>2364--2367</pages>
<contexts>
<context position="3250" citStr="Wang et al., 1996" startWordPosition="467" endWordPosition="470">al information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. In a typical SMT setting, each documentpair correspond</context>
</contexts>
<marker>Wang, Lafferty, Waibel, 1996</marker>
<rawString>Yeyi Wang, John Lafferty, and Alex Waibel. 1996. Word clustering with parallel spoken language corpora. In proceedings of the 4th International Conference on Spoken Language Processing (ICSLP’96), pages 2364–2367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Knight</author>
</authors>
<title>Syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL2001).</booktitle>
<contexts>
<context position="1531" citStr="Knight, 2001" startWordPosition="216" endWordPosition="217"> topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality. 1 Introduction Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001). Beyond the sentence-level, corpuslevel word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair cooccurrence statistics. For example, the word shot in “It was a n</context>
</contexts>
<marker>Knight, 2001</marker>
<rawString>K. Yamada and Kevin. Knight. 2001. Syntax-based statistical translation model. In Proceedings of the Conference of the Association for Computational Linguistics (ACL2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Stephan Vogel</author>
</authors>
<title>Adaptive parallel sentences mining from web bilingual news collection.</title>
<date>2002</date>
<booktitle>In The 2002 IEEEInternational Conference on Data Mining.</booktitle>
<contexts>
<context position="20576" citStr="Zhao and Vogel, 2002" startWordPosition="3324" endWordPosition="3327">d. Table 1: Training and Test Data Statistics Train #Doc. #Sent. English #Tokens Chinese Treebank 316 4172 133K 105K FBIS.BJ 6,111 105K 4.18M 3.54M Sinorama 2,373 103K 3.81M 3.60M XinHua 19,140 115K 3.85M 3.93M Test 95 627 25,500 19,726 We have two training data settings with different sizes (see Table 1). The small one consists of 316 document-pairs from Treebank (LDC2002E17). For the large training data setting, we collected additional documentpairs from FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), and Xinhua News (LDC2002E18, document boundaries are kept in our sentence-aligner (Zhao and Vogel, 2002)). There are 27,940 document-pairs, containing 327K sentence-pairs or 12 million (12M) English tokens and 11M Chinese tokens. To evaluate word alignment, we hand-labeled 627 sentence-pairs from 95 document-pairs sampled from TIDES’01 dryrun data. It contains 14,769 alignment-links. To evaluate translation quality, TIDES’02 Eval. test is used as development set, and TIDES’03 Eval. test is used as the unseen test data. 5.1 Model Settings First, we explore the effects of Null word and smoothing strategies. Empirically, we find that adding “Null” word is always beneficial to all models regardless </context>
</contexts>
<marker>Zhao, Vogel, 2002</marker>
<rawString>Bing Zhao and Stephan Vogel. 2002. Adaptive parallel sentences mining from web bilingual news collection. In The 2002 IEEEInternational Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Eric P Xing</author>
<author>Alex Waibel</author>
</authors>
<title>Bilingual word spectral clustering for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="3281" citStr="Zhao et al., 2005" startWordPosition="473" endWordPosition="476">n models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. In a typical SMT setting, each documentpair corresponds to an object; depending on a </context>
</contexts>
<marker>Zhao, Xing, Waibel, 2005</marker>
<rawString>Bing Zhao, Eric P. Xing, and Alex Waibel. 2005. Bilingual word spectral clustering for statistical machine translation. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 25–32, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>