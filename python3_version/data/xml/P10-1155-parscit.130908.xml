<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.995866">
All Words Domain Adapted WSD: Finding a Middle Ground between
Supervision and Unsupervision
</title>
<author confidence="0.986912">
Mitesh M. Khapra Anup Kulkarni Saurabh Sohoney Pushpak Bhattacharyya
</author>
<affiliation confidence="0.745419">
Indian Institute of Technology Bombay,
Mumbai - 400076, India.
</affiliation>
<email confidence="0.998643">
{miteshk,anup,saurabhsohoney,pb}@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.997385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960485714286">
In spite of decades of research on word
sense disambiguation (WSD), all-words
general purpose WSD has remained a dis-
tant goal. Many supervised WSD systems
have been built, but the effort of creat-
ing the training corpus - annotated sense
marked corpora - has always been a matter
of concern. Therefore, attempts have been
made to develop unsupervised and knowl-
edge based techniques for WSD which do
not need sense marked corpora. However
such approaches have not proved effective,
since they typically do not better Word-
net first sense baseline accuracy. Our re-
search reported here proposes to stick to
the supervised approach, but with far less
demand on annotation. We show that if
we have ANY sense marked corpora, be it
from mixed domain or a specific domain, a
small amount of annotation in ANY other
domain can deliver the goods almost as
if exhaustive sense marking were avail-
able in that domain. We have tested our
approach across Tourism and Health do-
main corpora, using also the well known
mixed domain SemCor corpus. Accuracy
figures close to self domain training lend
credence to the viability of our approach.
Our contribution thus lies in finding a con-
venient middle ground between pure su-
pervised and pure unsupervised WSD. Fi-
nally, our approach is not restricted to any
specific set of target words, a departure
from a commonly observed practice in do-
main specific WSD.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996437037037">
Amongst annotation tasks, sense marking surely
takes the cake, demanding as it does high level
of language competence, topic comprehension and
domain sensitivity. This makes supervised ap-
proaches to WSD a difficult proposition (Agirre
et al., 2009b; Agirre et al., 2009a; McCarthy et
al., 2007). Unsupervised and knowledge based ap-
proaches have been tried with the hope of creating
WSD systems with no need for sense marked cor-
pora (Koeling et al., 2005; McCarthy et al., 2007;
Agirre et al., 2009b). However, the accuracy fig-
ures of such systems are low.
Our work here is motivated by the desire to de-
velop annotation-lean all-words domain adapted
techniques for supervised WSD. It is a common
observation that domain specific WSD exhibits
high level of accuracy even for the all-words sce-
nario (Khapra et al., 2010) - provided training and
testing are on the same domain. Also domain
adaptation - in which training happens in one do-
main and testing in another - often is able to attain
good levels of performance, albeit on a specific set
of target words (Chan and Ng, 2007; Agirre and
de Lacalle, 2009). To the best of our knowledge
there does not exist a system that solves the com-
bined problem of all words domain adapted WSD.
We thus propose the following:
</bodyText>
<listItem confidence="0.937473">
a. For any target domain, create a small amount
of sense annotated corpus.
b. Mix it with an existing sense annotated cor-
pus – from a mixed domain or specific do-
main – to train the WSD engine.
</listItem>
<bodyText confidence="0.999895">
This procedure tested on four adaptation scenar-
ios, viz., (i) SemCor (Miller et al., 1993) to
Tourism, (ii) SemCor to Health, (iii) Tourism to
Health and (iv) Health to Tourism has consistently
yielded good performance (to be explained in sec-
tions 6 and 7).
The remainder of this paper is organized as fol-
lows. In section 2 we discuss previous work in the
area of domain adaptation for WSD. In section 3
</bodyText>
<page confidence="0.951461">
1532
</page>
<note confidence="0.942701">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1532–1541,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9995653">
we discuss three state of art supervised, unsuper-
vised and knowledge based algorithms for WSD.
Section 4 discusses the injection strategy for do-
main adaptation. In section 5 we describe the
dataset used for our experiments. We then present
the results in section 6 followed by discussions in
section 7. Section 8 examines whether there is any
need for intelligent choice of injections. Section
9 concludes the paper highlighting possible future
directions.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999913431034483">
Domain specific WSD for selected target words
has been attempted by Ng and Lee (1996), Agirre
and de Lacalle (2009), Chan and Ng (2007), Koel-
ing et al. (2005) and Agirre et al. (2009b). They
report results on three publicly available lexical
sample datasets, viz., DSO corpus (Ng and Lee,
1996), MEDLINE corpus (Weeber et al., 2001)
and the corpus made available by Koeling et al.
(2005). Each of these datasets contains a handful
of target words (41-191 words) which are sense
marked in the corpus.
Our main inspiration comes from the target-
word specific results reported by Chan and Ng
(2007) and Agirre and de Lacalle (2009). The
former showed that adding just 30% of the target
data to the source data achieved the same perfor-
mance as that obtained by taking the entire source
and target data. Agirre and de Lacalle (2009) re-
ported a 22% error reduction when source and
target data were combined for training a classi-
fier, as compared to the case when only the target
data was used for training the classifier. However,
both these works focused on target word specific
WSD and do not address all-words domain spe-
cific WSD.
In the unsupervised setting, McCarthy et al.
(2007) showed that their predominant sense acqui-
sition method gives good results on the corpus of
Koeling et al. (2005). In particular, they showed
that the performance of their method is compa-
rable to the most frequent sense obtained from a
tagged corpus, thereby making a strong case for
unsupervised methods for domain-specific WSD.
More recently, Agirre et al. (2009b) showed that
knowledge based approaches which rely only on
the semantic relations captured by the Wordnet
graph outperform supervised approaches when ap-
plied to specific domains. The good results ob-
tained by McCarthy et al. (2007) and Agirre et
al. (2009b) for unsupervised and knowledge based
approaches respectively have cast a doubt on the
viability of supervised approaches which rely on
sense tagged corpora. However, these conclusions
were drawn only from the performance on certain
target words, leaving open the question of their
utility in all words WSD.
We believe our work contributes to the WSD
research in the following way: (i) it shows that
there is promise in supervised approach to all-
word WSD, through the instrument of domain
adaptation; (ii) it places in perspective some very
recently reported unsupervised and knowledge
based techniques of WSD; (ii) it answers some
questions arising out of the debate between super-
vision and unsupervision in WSD; and finally (iv)
it explores a convenient middle ground between
unsupervised and supervised WSD – the territory
of “annotate-little and inject” paradigm.
</bodyText>
<sectionHeader confidence="0.995585" genericHeader="method">
3 WSD algorithms employed by us
</sectionHeader>
<bodyText confidence="0.998454333333333">
In this section we describe the knowledge based,
unsupervised and supervised approaches used for
our experiments.
</bodyText>
<subsectionHeader confidence="0.999478">
3.1 Knowledge Based Approach
</subsectionHeader>
<bodyText confidence="0.998834142857143">
Agirre et al. (2009b) showed that a graph based
algorithm which uses only the relations between
concepts in a Lexical Knowledge Base (LKB) can
outperform supervised approaches when tested on
specific domains (for a set of chosen target words).
We employ their method which involves the fol-
lowing steps:
</bodyText>
<listItem confidence="0.909501">
1. Represent Wordnet as a graph where the con-
cepts (i.e., synsets) act as nodes and the re-
lations between concepts define edges in the
graph.
2. Apply a context-dependent Personalized
PageRank algorithm on this graph by intro-
ducing the context words as nodes into the
graph and linking them with their respective
synsets.
3. These nodes corresponding to the context
words then inject probability mass into the
synsets they are linked to, thereby influencing
the final relevance of all nodes in the graph.
</listItem>
<bodyText confidence="0.9943885">
We used the publicly available implementation
of this algorithm1 for our experiments.
</bodyText>
<footnote confidence="0.992637">
1http://ixa2.si.ehu.es/ukb/
</footnote>
<page confidence="0.968786">
1533
</page>
<subsectionHeader confidence="0.999291">
3.2 Unsupervised Approach
</subsectionHeader>
<bodyText confidence="0.998835727272727">
McCarthy et al. (2007) used an untagged corpus to
construct a thesaurus of related words. They then
found the predominant sense (i.e., the most fre-
quent sense) of each target word using pair-wise
Wordnet based similarity measures by pairing the
target word with its top-k neighbors in the the-
saurus. Each target word is then disambiguated
by assigning it its predominant sense – the moti-
vation being that the predominant sense is a pow-
erful, hard-to-beat baseline. We implemented their
method using the following steps:
</bodyText>
<listItem confidence="0.998324">
1. Obtain a domain-specific untagged corpus (we
crawled a corpus of approximately 9M words
from the web).
2. Extract grammatical relations from this text us-
ing a dependency parser2 (Klein and Manning,
2003).
3. Use the grammatical relations thus extracted to
construct features for identifying the k nearest
neighbors for each word using the distributional
similarity score described in (Lin, 1998).
4. Rank the senses of each target word in the test
set using a weighted sum of the distributional
similarity scores of the neighbors. The weights
in the sum are based on Wordnet Similarity
scores (Patwardhan and Pedersen, 2003).
5. Each target word in the test set is then disam-
biguated by simply assigning it its predominant
sense obtained using the above method.
</listItem>
<subsectionHeader confidence="0.999594">
3.3 Supervised approach
</subsectionHeader>
<bodyText confidence="0.999972">
Khapra et al. (2010) proposed a supervised algo-
rithm for domain-specific WSD and showed that it
beats the most frequent corpus sense and performs
on par with other state of the art algorithms like
PageRank. We implemented their iterative algo-
rithm which involves the following steps:
</bodyText>
<listItem confidence="0.999805">
1. Tag all monosemous words in the sentence.
2. Iteratively disambiguate the remaining words in
the sentence in increasing order of their degree
of polysemy.
3. At each stage rank the candidate senses of
a word using the scoring function of Equa-
tion (1) which combines corpus based param-
eters (such as, sense distributions and corpus
co-occurrence) and Wordnet based parameters
</listItem>
<footnote confidence="0.9513995">
2We used the Stanford parser - http://nlp.
stanford.edu/software/lex-parser.shtml
</footnote>
<equation confidence="0.916460733333333">
(such as, semantic similarity, conceptual dis-
tance, etc.)
�
S* = arg max(OiVi +
i
jEJ
(1)
where,
i E Candidate Synsets
J = Set of disambiguated words
Oi = BelongingnessToDominantConcept(Si)
Vi = P(Si|word)
Wij = CorpusCooccurrence(Si, Sj)
* VWNConceptualDistance(Si, Sj)
* VWNSemanticGraphDistance(Si, Sj)
</equation>
<listItem confidence="0.968189">
4. Select the candidate synset with maximizes the
above score as the winner sense.
</listItem>
<sectionHeader confidence="0.999373" genericHeader="method">
4 Injections for Supervised Adaptation
</sectionHeader>
<bodyText confidence="0.999556833333333">
This section describes the main interest of our
work i.e. adaptation using injections. For su-
pervised adaptation, we use the supervised algo-
rithm described above (Khapra et al., 2010) in the
following 3 settings as proposed by Agirre et al.
(2009a):
</bodyText>
<listItem confidence="0.973421666666667">
a. Source setting: We train the algorithm on a
mixed-domain corpus (SemCor) or a domain-
specific corpus (say, Tourism) and test it on a
different domain (say, Health). A good perfor-
mance in this setting would indicate robustness
to domain-shifts.
b. Target setting: We train and test the algorithm
using data from the same domain. This gives the
skyline performance, i.e., the best performance
that can be achieved if sense marked data from
the target domain were available.
c. Adaptation setting: This setting is the main fo-
cus of interest in the paper. We augment the
training data which could be from one domain
or mixed domain with a small amount of data
from the target domain. This combined data is
then used for training. The aim here is to reach
as close to the skyline performance using as lit-
</listItem>
<bodyText confidence="0.678639">
tle data as possible. For injecting data from the
target domain we randomly select some sense
marked words from the target domain and add
</bodyText>
<equation confidence="0.652867">
Wij * Vi * Vj)
</equation>
<page confidence="0.870458">
1534
</page>
<table confidence="0.999788142857143">
Category Polysemous words Monosemous words
Tourism Health Tourism Health
Noun 53133 15437 23665 6979
Verb 15528 7348 1027 356
Adjective 19732 5877 10569 2378
Adverb 6091 1977 4323 1694
All 94484 30639 39611 11407
</table>
<tableCaption confidence="0.821482">
Table 1: Polysemous and Monosemous words per
category in each domain
</tableCaption>
<table confidence="0.999443625">
Avg. degree of Wordnet polysemy
for polysemous words
Category Health Tourism SemCor
Noun 5.24 4.95 5.60
Verb 10.60 10.10 9.89
Adjective 5.52 5.08 5.40
Adverb 3.64 4.16 3.90
All 6.49 5.77 6.43
</table>
<tableCaption confidence="0.9924075">
Table 3: Average degree of Wordnet polysemy of
polysemous words per category in the 3 domains
</tableCaption>
<table confidence="0.997323428571429">
Avg. no. of instances perpolysemous word
Category Health Tourism SemCor
Noun 7.06 12.56 10.98
Verb 7.47 9.76 11.95
Adjective 5.74 12.07 8.67
Adverb 9.11 19.78 25.44
All 6.94 12.17 11.25
</table>
<tableCaption confidence="0.986162">
Table 2: Average number of instances per polyse-
mous word per category in the 3 domains
</tableCaption>
<table confidence="0.998282875">
Avg. degree of Corpus polysemy
for polysemous words
Category Health Tourism SemCor
Noun 1.92 2.60 3.41
Verb 3.41 4.55 4.73
Adjective 2.04 2.57 2.65
Adverb 2.16 2.82 3.09
All 2.31 2.93 3.56
</table>
<tableCaption confidence="0.9738285">
Table 4: Average degree of Corpus polysemy of
polysemous words per category in the 3 domains
</tableCaption>
<bodyText confidence="0.999953111111111">
them to the training data. An obvious ques-
tion which arises at this point is “Why were the
words selected at random?” or “Can selection
of words using some active learning strategy
yield better results than a random selection?”
We discuss this question in detail in Section 7
and show that a random set of injections per-
forms no worse than a craftily selected set of
injections.
</bodyText>
<sectionHeader confidence="0.995091" genericHeader="method">
5 DataSet Preparation
</sectionHeader>
<bodyText confidence="0.999926714285714">
Due to the lack of any publicly available all-words
domain specific sense marked corpora we set upon
the task of collecting data from two domains, viz.,
Tourism and Health. The data for Tourism do-
main was downloaded from Indian Tourism web-
sites whereas the data for Health domain was ob-
tained from two doctors. This data was manu-
ally sense annotated by two lexicographers adept
in English. Princeton Wordnet 2.13 (Fellbaum,
1998) was used as the sense inventory. A total
of 1,34,095 words from the Tourism domain and
42,046 words from the Health domain were man-
ually sense marked. Some files were sense marked
by both the lexicographers and the Inter Tagger
Agreement (ITA) calculated from these files was
83% which is comparable to the 78% ITA reported
on the SemCor corpus considering the domain-
specific nature of the corpus.
We now present different statistics about the
corpora. Table 1 summarizes the number of poly-
semous and monosemous words in each category.
</bodyText>
<footnote confidence="0.522237">
3http://wordnetweb.princeton.edu/perl/webwn
</footnote>
<bodyText confidence="0.99996768">
Note that we do not use the monosemous words
while calculating precision and recall of our algo-
rithms.
Table 2 shows the average number of instances
per polysemous word in the 3 corpora. We note
that the number of instances per word in the
Tourism domain is comparable to that in the Sem-
Cor corpus whereas the number of instances per
word in the Health corpus is smaller due to the
overall smaller size of the Health corpus.
Tables 3 and 4 summarize the average degree
of Wordnet polysemy and corpus polysemy of the
polysemous words in the corpus. Wordnet poly-
semy is the number of senses of a word as listed
in the Wordnet, whereas corpus polysemy is the
number of senses of a word actually appearing in
the corpus. As expected, the average degree of
corpus polysemy (Table 4) is much less than the
average degree of Wordnet polysemy (Table 3).
Further, the average degree of corpus polysemy
(Table 4) in the two domains is less than that in the
mixed-domain SemCor corpus, which is expected
due to the domain specific nature of the corpora.
Finally, Table 5 summarizes the number of unique
polysemous words per category in each domain.
</bodyText>
<table confidence="0.997597285714286">
No. of unique polysemous words
Category Health Tourism SemCor
Noun 2188 4229 5871
Verb 984 1591 2565
Adjective 1024 1635 2640
Adverb 217 308 463
All 4413 7763 11539
</table>
<tableCaption confidence="0.971778">
Table 5: Number of unique polysemous words per category
in each domain.
</tableCaption>
<page confidence="0.983041">
1535
</page>
<bodyText confidence="0.99965475">
The data is currently being enhanced by manu-
ally sense marking more words from each domain
and will be soon freely available4 for research pur-
poses.
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999989">
We tested the 3 algorithms described in section 4
using SemCor, Tourism and Health domain cor-
pora. We did a 2-fold cross validation for su-
pervised adaptation and report the average perfor-
mance over the two folds. Since the knowledge
based and unsupervised methods do not need any
training data we simply test it on the entire corpus
from the two domains.
</bodyText>
<subsectionHeader confidence="0.989613">
6.1 Knowledge Based approach
</subsectionHeader>
<bodyText confidence="0.97910875">
The results obtained by applying the Personalized
PageRank (PPR) method to Tourism and Health
data are summarized in Table 6. We also report
the Wordnet first sense baseline (WFS).
</bodyText>
<table confidence="0.9998726">
Domain Algorithm P(%) R(%) F(%)
Tourism PPR 53.1 53.1 53.1
WFS 62.5 62.5 62.5
Health PPR 51.1 51.1 51.1
WFS 65.5 65.5 65.5
</table>
<tableCaption confidence="0.970835">
Table 6: Comparing the performance of Person-
alized PageRank (PPR) with Wordnet First Sense
Baseline (WFS)
</tableCaption>
<subsectionHeader confidence="0.998987">
6.2 Unsupervised approach
</subsectionHeader>
<bodyText confidence="0.999922705882353">
The predominant sense for each word in the two
domains was calculated using the method de-
scribed in section 4.2. McCarthy et al. (2004)
reported that the best results were obtained us-
ing k = 50 neighbors and the Wordnet Similar-
ity jcn measure (Jiang and Conrath, 1997). Fol-
lowing them, we used k = 50 and observed that
the best results for nouns and verbs were obtained
using the jcn measure and the best results for ad-
jectives and adverbs were obtained using the lesk
measure (Banerjee and Pedersen, 2002). Accord-
ingly, we used jcn for nouns and verbs and lesk
for adjectives and adverbs. Each target word in
the test set is then disambiguated by simply as-
signing it its predominant sense obtained using
the above method. We tested this approach only
on Tourism domain due to unavailability of large
</bodyText>
<footnote confidence="0.991119">
4http://www.cfilt.iitb.ac.in/wsd/annotated corpus
</footnote>
<table confidence="0.792861">
untagged Health corpus which is needed for con-
structing the thesaurus. The results are summa-
rized in Table 7.
Domain Algorithm P(%) R(%) F(%)
Tourism McCarthy 51.85 49.32 50.55
WFS 62.50 62.50 62.50
</table>
<tableCaption confidence="0.679006666666667">
Table 7: Comparing the performance of unsuper-
vised approach with Wordnet First Sense Baseline
(WFS)
</tableCaption>
<subsectionHeader confidence="0.995103">
6.3 Supervised adaptation
</subsectionHeader>
<bodyText confidence="0.99918175">
We report results in the source setting, target set-
ting and adaptation setting as described earlier
using the following four combinations for source
and target data:
</bodyText>
<listItem confidence="0.97041325">
1. SemCor to Tourism (SC-*T) where SemCor is
used as the source domain and Tourism as the
target (test) domain.
2. SemCor to Health (SC-*H) where SemCor is
used as the source domain and Health as the tar-
get (test) domain.
3. Tourism to Health (T-*H) where Tourism is
used as the source domain and Health as the tar-
get (test) domain.
4. Health to Tourism (H-*T) where Health is
used as the source domain and Tourism as the
target (test) domain.
</listItem>
<bodyText confidence="0.99957235">
In each case, the target domain data was divided
into two folds. One fold was set aside for testing
and the other for injecting data in the adaptation
setting. We increased the size of the injected target
examples from 1000 to 14000 words in increments
of 1000. We then repeated the same experiment by
reversing the role of the two folds.
Figures 1, 2, 3 and 4 show the graphs of the av-
erage F-score over the 2-folds for SC-*T, SC-*H,
T-*H and H-*T respectively. The x-axis repre-
sents the amount of training data (in words) in-
jected from the target domain and the y-axis rep-
resents the F-score. The different curves in each
graph are as follows:
a. only random : This curve plots the perfor-
mance obtained using x randomly selected
sense tagged words from the target domain and
zero sense tagged words from the source do-
main (x was varied from 1000 to 14000 words
in increments of 1000).
</bodyText>
<page confidence="0.957">
1536
</page>
<figure confidence="0.992409">
Injection Size v/s F-score
0 2000 4000 6000 8000 10000 12000 14000
Injection Size (words)
</figure>
<figureCaption confidence="0.985003">
Figure 1: Supervised adaptation from
SemCor to Tourism using injections
</figureCaption>
<figure confidence="0.984777">
Injection Size v/s F-score
Injection Size v/s F-score
0 2000 4000 6000 8000 10000 12000 14000
Injection Size (words)
</figure>
<figureCaption confidence="0.953419">
Figure 2: Supervised adaptation from
SemCor to Health using injections
</figureCaption>
<figure confidence="0.9991706">
Injection Size v/s F-score
tsky
s
srcb
only_random
random+semcor
tsky
srcb
wfs
only_random
random+semcor
F-score (%) 80
75
70
65
60
55
50
45
40
35
F-score (%) 80
75
70
65
60
55
50
45
40
35
F-score (%)
45
40
80
75
70
65
60
55
50
35
F-score (%)
45
40
80
75
70
65
60
55
50
35
tsky
wfs
srcb
only_random
random+tourism
tsky
only_random
random+health
wfs
srcb
0 2000 4000 6000 8000 10000 12000 14000
Injection Size (words)
</figure>
<figureCaption confidence="0.944274">
Figure 3: Supervised adaptation from
Tourism to Health using injections
</figureCaption>
<figure confidence="0.9961425">
0 2000 4000 6000 8000 10000 12000 14000
Injection Size (words)
</figure>
<figureCaption confidence="0.992436">
Figure 4: Supervised adaptation from
Health to Tourism using injections
</figureCaption>
<bodyText confidence="0.99205825">
b. random+source : This curve plots the perfor-
mance obtained by mixing x randomly selected
sense tagged words from the target domain with
the entire training data from the source domain
(again x was varied from 1000 to 14000 words
in increments of 1000).
c. source baseline (srcb) : This represents the F-
score obtained by training on the source data
alone without mixing any examples from the
target domain.
d. wordnet first sense (wfs) : This represents the
F-score obtained by selecting the first sense
from Wordnet, a typically reported baseline.
e. target skyline (tsky) : This represents the av-
erage 2-fold F-score obtained by training on
one entire fold of the target data itself (Health:
15320 polysemous words; Tourism: 47242 pol-
ysemous words) and testing on the other fold.
These graphs along with other results are dis-
cussed in the next section.
</bodyText>
<sectionHeader confidence="0.998237" genericHeader="evaluation">
7 Discussions
</sectionHeader>
<bodyText confidence="0.999851">
We discuss the performance of the three ap-
proaches.
</bodyText>
<subsectionHeader confidence="0.9970285">
7.1 Knowledge Based and Unsupervised
approaches
</subsectionHeader>
<bodyText confidence="0.999991384615385">
It is apparent from Tables 6 and 7 that knowl-
edge based and unsupervised approaches do not
perform well when compared to the Wordnet first
sense (which is freely available and hence can be
used for disambiguation). Further, we observe that
the performance of these approaches is even less
than the source baseline (i.e., the case when train-
ing data from a source domain is applied as it is
to a target domain - without using any injections).
These observations bring out the weaknesses of
these approaches when used in an all-words set-
ting and clearly indicate that they come nowhere
close to replacing a supervised system.
</bodyText>
<page confidence="0.985736">
1537
</page>
<subsectionHeader confidence="0.969334">
7.2 Supervised adaptation
</subsectionHeader>
<bodyText confidence="0.990416774193548">
1. The F-score obtained by training on SemCor
(mixed-domain corpus) and testing on the two
target domains without using any injections
(srcb) – F-score of 61.7% on Tourism and F-
score of 65.5% on Health – is comparable to the
best result reported on the SEMEVAL datasets
(65.02%, where both training and testing hap-
pens on a mixed-domain corpus (Snyder and
Palmer, 2004)). This is in contrast to previ-
ous studies (Escudero et al., 2000; Agirre and
Martinez, 2004) which suggest that instead of
adapting from a generic/mixed domain to a spe-
cific domain, it is better to completely ignore
the generic examples and use hand-tagged data
from the target domain itself. The main rea-
son for the contrasting results is that the ear-
lier work focused only on a handful of target
words whereas we focus on all words appearing
in the corpus. So, while the behavior of a few
target words would change drastically when the
domain changes, a majority of the words will
exhibit the same behavior (i.e., same predomi-
nant sense) even when the domain changes. We
agree that the overall performance is still lower
than that obtained by training on the domain-
specific corpora. However, it is still better than
the performance of unsupervised and knowl-
edge based approaches which tilts the scale in
favor of supervised approaches even when only
mixed domain sense marked corpora is avail-
able.
</bodyText>
<listItem confidence="0.992779944444445">
2. Adding injections from the target domain im-
proves the performance. As the amount of in-
jection increases the performance approaches
the skyline, and in the case of SC—*H and T—*H
it even crosses the skyline performance showing
that combining the source and target data can
give better performance than using the target
data alone. This is consistent with the domain
adaptation results reported by Agirre and de La-
calle (2009) on a specific set of target words.
3. The performance of random+source is always
better than only random indicating that the data
from the source domain does help to improve
performance. A detailed analysis showed that
the gain obtained by using the source data is at-
tributable to reducing recall errors by increasing
the coverage of seen words.
4. Adapting from one specific domain (Tourism or
</listItem>
<bodyText confidence="0.996035705882353">
Health) to another specific domain (Health or
Tourism) gives the same performance as that ob-
tained by adapting from a mixed-domain (Sem-
Cor) to a specific domain (Tourism, Health).
This is an interesting observation as it suggests
that as long as data from one domain is avail-
able it is easy to build a WSD engine that works
for other domains by injecting a small amount
of data from these domains.
To verify that the results are consistent, we ran-
domly selected 5 different sets of injections from
fold-1 and tested the performance on fold-2. We
then repeated the same experiment by reversing
the roles of the two folds. The results were in-
deed consistent irrespective of the set of injections
used. Due to lack of space we have not included
the results for these 5 different sets of injections.
</bodyText>
<subsectionHeader confidence="0.919763">
7.3 Quantifying the trade-off between
performance and corpus size
</subsectionHeader>
<bodyText confidence="0.999971166666667">
To correctly quantify the benefit of adding injec-
tions from the target domain, we calculated the
amount of target data (peak size) that is needed
to reach the skyline F-score (peak F) in the ab-
sence of any data from the source domain. The
peak size was found to be 35000 (Tourism) and
14000 (Health) corresponding to peak F values of
74.2% (Tourism) and 73.4% (Health). We then
plotted a graph (Figure 5) to capture the rela-
tion between the size of injections (expressed as
a percentage of the peak size) and the F-score (ex-
pressed as a percentage of the peak F).
</bodyText>
<figure confidence="0.907542666666667">
Size v/s Performance
0 20 40 60 80 100
% peak_size
</figure>
<figureCaption confidence="0.998149">
Figure 5: Trade-off between performance
and corpus size
</figureCaption>
<bodyText confidence="0.998625">
We observe that by mixing only 20-40% of the
peak size with the source domain we can obtain up
to 95% of the performance obtained by using the
</bodyText>
<figure confidence="0.977744454545455">
% peak_F
105
100
95
90
85
80
SC --&gt; H
T --&gt; H
SC --&gt; T
H --&gt; T
</figure>
<page confidence="0.983581">
1538
</page>
<bodyText confidence="0.994645523809524">
entire target data (peak size). In absolute terms,
the size of the injections is only 7000-9000 poly-
semous words which is a very small price to pay
considering the performance benefits.
Table 8 summarizes the percentage of words that
fall in each category in each of the three adapta-
tion scenarios. The fact that nearly 50-60% of the
words fall in the “conformist” category once again
makes a strong case for reusing sense tagged data
from one domain to another domain.
8 Does the choice of injections matter?
An obvious question which arises at this point is
“Why were the words selected at random?” or
“Can selection of words using some active learn-
ing strategy yield better results than a random
selection?” An answer to this question requires
a more thorough understanding of the sense-
behavior exhibited by words across domains. In
any scenario involving a shift from domain D1 to
domain D2, we will always encounter words be-
longing to the following 4 categories:
</bodyText>
<listItem confidence="0.986787193548387">
a. WD, : This class includes words which are en-
countered only in the source domain D1 and do
not appear in the target domain D2. Since we
are interested in adapting to the target domain
and since these words do not appear in the tar-
get domain, it is quite obvious that they are not
important for the problem of domain adapta-
tion.
b. WD2 : This class includes words which are en-
countered only in the target domain D2 and do
not appear in the source domain D1. Again, it
is quite obvious that these words are important
for the problem of domain adaptation. They fall
in the category of unseen words and need han-
dling from that point of view.
c. WD1D2conformists : This class includes words
which are encountered in both the domains and
exhibit the same predominant sense in both the
domains. Correct identification of these words
is important so that we can use the predomi-
nant sense learned from D1 for disambiguating
instances of these words appearing in D2.
d. WD1D2non−conformists : This class includes
words which are encountered in both the do-
mains but their predominant sense in the tar-
get domain D2 does not conform to the pre-
dominant sense learned from the source domain
D1. Correct identification of these words is im-
portant so that we can ignore the predominant
senses learned from D1 while disambiguating
instances of these words appearing in D2.
</listItem>
<table confidence="0.9992475">
Category SC—*T SC—*H T—*H
WDa 7.14% 5.45% 13.61%
Conformists 49.54% 60.43% 54.31%
Non-Conformists 43.30% 34.11% 32.06%
</table>
<tableCaption confidence="0.970074">
Table 8: Percentage of Words belonging to each
category in the three settings.
</tableCaption>
<bodyText confidence="0.999891352941177">
The above characterization suggests that an ideal
domain adaptation strategy should focus on in-
jecting WD2 and WD1D2non−conformists as these
would yield maximum benefits if injected into the
training data. While it is easy to identify the
WD2 words, “identifying non-conformists” is a
hard problem which itself requires some type of
WSD5. However, just to prove that a random in-
jection strategy does as good as an ideal strategy
we assume the presence of an oracle which iden-
tifies the WD1D2non−conformists. We then augment
the training data with 5-8 instances for WD2 and
WD1D2non−conformists words thus identified. We
observed that adding more than 5-8 instances per
word does not improve the performance. This is
due to the “one sense per domain” phenomenon –
seeing only a few instances of a word is sufficient
to identify the predominant sense of the word. Fur-
ther, to ensure a better overall performance, the
instances of the most frequent words are injected
first followed by less frequent words till we ex-
haust the total size of the injections (1000, 2000
and so on). We observed that there was a 75-
80% overlap between the words selected by ran-
dom strategy and oracle strategy. This is because
oracle selects the most frequent words which also
have a high chance of getting selected when a ran-
dom sampling is done.
Figures 6, 7, 8 and 9 compare the performance
of the two strategies. We see that the random strat-
egy does as well as the oracle strategy thereby sup-
porting our claim that if we have sense marked
corpus from one domain then simply injecting ANY
small amount of data from the target domain will
</bodyText>
<footnote confidence="0.912420333333333">
5Note that the unsupervised predominant sense acquisi-
tion method of McCarthy et al. (2007) implicitly identifies
conformists and non-conformists
</footnote>
<page confidence="0.99215">
1539
</page>
<figure confidence="0.992871333333333">
Injection Size v/s F-score
0 2000 4000 6000 8000 10000 12000 14000
Injection Size (words)
</figure>
<figureCaption confidence="0.964166">
Figure 6: Comparing random strategy
with oracle based ideal strategy for Sem-
Cor to Tourism adaptation
</figureCaption>
<figure confidence="0.994131">
Injection Size v/s F-score
0 2000 4000 6000 8000 10000 12000 14000
Injection Size (words)
</figure>
<figureCaption confidence="0.981161333333333">
Figure 8: Comparing random strat-
egy with oracle based ideal strategy for
Tourism to Health adaptation
</figureCaption>
<figure confidence="0.994066">
Injection Size v/s F-score
0 2000 4000 6000 8000 10000 12000 14000
Injection Size (words)
</figure>
<figureCaption confidence="0.943304666666667">
Figure 7: Comparing random strategy
with oracle based ideal strategy for Sem-
Cor to Health adaptation
</figureCaption>
<figure confidence="0.994103333333333">
Injection Size v/s F-score
0 2000 4000 6000 8000 10000 12000 14000
Injection Size (words)
</figure>
<figureCaption confidence="0.992672333333333">
Figure 9: Comparing random strat-
egy with oracle based ideal strategy for
Health to Tourism adaptation
</figureCaption>
<figure confidence="0.987225655737705">
tsky
random+semcor
oracle+semcor
wfs
srcb
tsky
wfs
srcb
random+tourism
oracle+tourism
tsky
srcb
wfs
random+semcor
oracle+semcor
tsky
wfs
srcb
random+health
oracle+health
F-score (%) 80
75
70
65
60
55
50
45
40
35
F-score (%) 80
75
70
65
60
55
50
45
40
35
F-score (%) 80
F-score (%) 75
70
65
60
55
50
45
40
35
80
75
70
65
60
55
50
45
40
35
do the job.
</figure>
<sectionHeader confidence="0.963253" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9996">
Based on our study of WSD in 4 domain adap-
tation scenarios, we make the following conclu-
sions:
</bodyText>
<listItem confidence="0.890345777777778">
1. Supervised adaptation by mixing small amount
of data (7000-9000 words) from the target do-
main with the source domain gives nearly the
same performance (F-score of around 70% in
all the 4 adaptation scenarios) as that obtained
by training on the entire target domain data.
2. Unsupervised and knowledge based approaches
which use distributional similarity and Word-
net based similarity measures do not compare
well with the Wordnet first sense baseline per-
formance and do not come anywhere close to
the performance of supervised adaptation.
3. Supervised adaptation from a mixed domain to
a specific domain gives the same performance
as that from one specific domain (Tourism) to
another specific domain (Health).
4. Supervised adaptation is not sensitive to the
type of data being injected. This is an interest-
</listItem>
<bodyText confidence="0.9957637">
ing finding with the following implication: as
long as one has sense marked corpus - be it from
a mixed or specific domain - simply injecting
ANY small amount of data from the target do-
main suffices to beget good accuracy.
As future work, we would like to test our work on
the Environment domain data which was released
as part of the SEMEVAL 2010 shared task on “All-
words Word Sense Disambiguation on a Specific
Domain”.
</bodyText>
<page confidence="0.989621">
1540
</page>
<sectionHeader confidence="0.998303" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993426971962617">
Eneko Agirre and Oier Lopez de Lacalle. 2009. Su-
pervised domain adaption for wsd. In EACL ’09:
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 42–50, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Eneko Agirre and David Martinez. 2004. The effect of
bias on an automatically-built word sense corpus. In
Proceedings of the 4rd International Conference on
Languages Resources and Evaluations (LREC).
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Andrea Marchetti, Antonio Toral, and Piek
Vossen. 2009a. Semeval-2010 task 17: all-words
word sense disambiguation on a specific domain. In
DEW ’09: Proceedings of the Workshop on Seman-
tic Evaluations: Recent Achievements and Future
Directions, pages 123–128, Morristown, NJ, USA.
Association for Computational Linguistics.
Eneko Agirre, Oier Lopez De Lacalle, and Aitor Soroa.
2009b. Knowledge-based wsd on specific domains:
Performing better than generic supervised wsd. In
In Proceedings ofIJCAI.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In CICLing ’02: Proceedings
of the Third International Conference on Compu-
tational Linguistics and Intelligent Text Processing,
pages 136–145, London, UK. Springer-Verlag.
Yee Seng Chan and Hwee Tou Ng. 2007. Do-
main adaptation with active learning for word sense
disambiguation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 49–56, Prague, Czech Republic,
June. Association for Computational Linguistics.
Gerard Escudero, Lluis M`arquez, and German Rigau.
2000. An empirical study of the domain depen-
dence of supervised word sense disambiguation sys-
tems. In Proceedings of the 2000 Joint SIGDAT con-
ference on Empirical methods in natural language
processing and very large corpora, pages 172–180,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database.
J.J. Jiang and D.W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proc. of the Int’l. Conf. on Research in Computa-
tional Linguistics, pages 19–33.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2010. Domain-specific word
sense disambiguation combining corpus based and
wordnet based parameters. In 5th International
Conference on Global Wordnet (GWC2010).
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In INPROCEEDINGS
OF THE 41ST ANNUAL MEETING OF THE ASSO-
CIATION FOR COMPUTATIONAL LINGUISTICS,
pages 423–430.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In HLT ’05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 419–426, Morristown, NJ, USA.
Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th
international conference on Computational linguis-
tics, pages 768–774, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In ACL ’04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, page 279, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of predom-
inant word senses. Comput. Linguist., 33(4):553–
590.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
HLT ’93: Proceedings of the workshop on Human
Language Technology, pages 303–308, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
sense: an exemplar-based approach. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 40–47, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Siddharth Patwardhan and Ted Pedersen. 2003.
The cpan wordnet::similarity package. http://search
.cpan.org/ sid/wordnet-similarity/.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Rada Mihalcea and Phil
Edmonds, editors, Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, pages 41–43, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Marc Weeber, James G. Mork, and Alan R. Aronson.
2001. Developing a test collection for biomedical
word sense disambiguation. In In Proceedings of
the AMAI Symposium, pages 746–750.
</reference>
<page confidence="0.994257">
1541
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925371">
<title confidence="0.99931">Words Domain Adapted WSD: a Middle Ground between Supervision and Unsupervision</title>
<author confidence="0.997981">Mitesh M Khapra Anup Kulkarni Saurabh Sohoney Pushpak Bhattacharyya</author>
<affiliation confidence="0.999928">Indian Institute of Technology Bombay,</affiliation>
<address confidence="0.993242">Mumbai - 400076, India.</address>
<abstract confidence="0.998180916666667">In spite of decades of research on word sense disambiguation (WSD), all-words general purpose WSD has remained a distant goal. Many supervised WSD systems have been built, but the effort of creatthe training corpus sense corpora has always been a matter of concern. Therefore, attempts have been made to develop unsupervised and knowledge based techniques for WSD which do not need sense marked corpora. However such approaches have not proved effective, since they typically do not better Wordnet first sense baseline accuracy. Our research reported here proposes to stick to the supervised approach, but with far less demand on annotation. We show that if we have ANY sense marked corpora, be it from mixed domain or a specific domain, a small amount of annotation in ANY other domain can deliver the goods almost as if exhaustive sense marking were available in that domain. We have tested our approach across Tourism and Health domain corpora, using also the well known mixed domain SemCor corpus. Accuracy figures close to self domain training lend credence to the viability of our approach. Our contribution thus lies in finding a convenient middle ground between pure supervised and pure unsupervised WSD. Finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific WSD.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
</authors>
<title>Supervised domain adaption for wsd.</title>
<date>2009</date>
<booktitle>In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>42--50</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Agirre, de Lacalle, 2009</marker>
<rawString>Eneko Agirre and Oier Lopez de Lacalle. 2009. Supervised domain adaption for wsd. In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 42–50, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
</authors>
<title>The effect of bias on an automatically-built word sense corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4rd International Conference on Languages Resources and Evaluations (LREC).</booktitle>
<contexts>
<context position="22325" citStr="Agirre and Martinez, 2004" startWordPosition="3723" endWordPosition="3726">ese approaches when used in an all-words setting and clearly indicate that they come nowhere close to replacing a supervised system. 1537 7.2 Supervised adaptation 1. The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) – F-score of 61.7% on Tourism and Fscore of 65.5% on Health – is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing happens on a mixed-domain corpus (Snyder and Palmer, 2004)). This is in contrast to previous studies (Escudero et al., 2000; Agirre and Martinez, 2004) which suggest that instead of adapting from a generic/mixed domain to a specific domain, it is better to completely ignore the generic examples and use hand-tagged data from the target domain itself. The main reason for the contrasting results is that the earlier work focused only on a handful of target words whereas we focus on all words appearing in the corpus. So, while the behavior of a few target words would change drastically when the domain changes, a majority of the words will exhibit the same behavior (i.e., same predominant sense) even when the domain changes. We agree that the over</context>
</contexts>
<marker>Agirre, Martinez, 2004</marker>
<rawString>Eneko Agirre and David Martinez. 2004. The effect of bias on an automatically-built word sense corpus. In Proceedings of the 4rd International Conference on Languages Resources and Evaluations (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
<author>Christiane Fellbaum</author>
<author>Andrea Marchetti</author>
<author>Antonio Toral</author>
<author>Piek Vossen</author>
</authors>
<title>Semeval-2010 task 17: all-words word sense disambiguation on a specific domain.</title>
<date>2009</date>
<booktitle>In DEW ’09: Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions,</booktitle>
<pages>123--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Agirre, de Lacalle, Fellbaum, Marchetti, Toral, Vossen, 2009</marker>
<rawString>Eneko Agirre, Oier Lopez de Lacalle, Christiane Fellbaum, Andrea Marchetti, Antonio Toral, and Piek Vossen. 2009a. Semeval-2010 task 17: all-words word sense disambiguation on a specific domain. In DEW ’09: Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 123–128, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez De Lacalle</author>
<author>Aitor Soroa</author>
</authors>
<title>Knowledge-based wsd on specific domains: Performing better than generic supervised wsd. In</title>
<date>2009</date>
<booktitle>In Proceedings ofIJCAI.</booktitle>
<marker>Agirre, De Lacalle, Soroa, 2009</marker>
<rawString>Eneko Agirre, Oier Lopez De Lacalle, and Aitor Soroa. 2009b. Knowledge-based wsd on specific domains: Performing better than generic supervised wsd. In In Proceedings ofIJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted lesk algorithm for word sense disambiguation using wordnet.</title>
<date>2002</date>
<booktitle>In CICLing ’02: Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>136--145</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="17092" citStr="Banerjee and Pedersen, 2002" startWordPosition="2829" endWordPosition="2832">omparing the performance of Personalized PageRank (PPR) with Wordnet First Sense Baseline (WFS) 6.2 Unsupervised approach The predominant sense for each word in the two domains was calculated using the method described in section 4.2. McCarthy et al. (2004) reported that the best results were obtained using k = 50 neighbors and the Wordnet Similarity jcn measure (Jiang and Conrath, 1997). Following them, we used k = 50 and observed that the best results for nouns and verbs were obtained using the jcn measure and the best results for adjectives and adverbs were obtained using the lesk measure (Banerjee and Pedersen, 2002). Accordingly, we used jcn for nouns and verbs and lesk for adjectives and adverbs. Each target word in the test set is then disambiguated by simply assigning it its predominant sense obtained using the above method. We tested this approach only on Tourism domain due to unavailability of large 4http://www.cfilt.iitb.ac.in/wsd/annotated corpus untagged Health corpus which is needed for constructing the thesaurus. The results are summarized in Table 7. Domain Algorithm P(%) R(%) F(%) Tourism McCarthy 51.85 49.32 50.55 WFS 62.50 62.50 62.50 Table 7: Comparing the performance of unsupervised appro</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted lesk algorithm for word sense disambiguation using wordnet. In CICLing ’02: Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, pages 136–145, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Domain adaptation with active learning for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>49--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2756" citStr="Chan and Ng, 2007" startWordPosition="448" endWordPosition="451">arthy et al., 2007; Agirre et al., 2009b). However, the accuracy figures of such systems are low. Our work here is motivated by the desire to develop annotation-lean all-words domain adapted techniques for supervised WSD. It is a common observation that domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. Also domain adaptation - in which training happens in one domain and testing in another - often is able to attain good levels of performance, albeit on a specific set of target words (Chan and Ng, 2007; Agirre and de Lacalle, 2009). To the best of our knowledge there does not exist a system that solves the combined problem of all words domain adapted WSD. We thus propose the following: a. For any target domain, create a small amount of sense annotated corpus. b. Mix it with an existing sense annotated corpus – from a mixed domain or specific domain – to train the WSD engine. This procedure tested on four adaptation scenarios, viz., (i) SemCor (Miller et al., 1993) to Tourism, (ii) SemCor to Health, (iii) Tourism to Health and (iv) Health to Tourism has consistently yielded good performance </context>
<context position="4342" citStr="Chan and Ng (2007)" startWordPosition="716" endWordPosition="719">inguistics we discuss three state of art supervised, unsupervised and knowledge based algorithms for WSD. Section 4 discusses the injection strategy for domain adaptation. In section 5 we describe the dataset used for our experiments. We then present the results in section 6 followed by discussions in section 7. Section 8 examines whether there is any need for intelligent choice of injections. Section 9 concludes the paper highlighting possible future directions. 2 Related Work Domain specific WSD for selected target words has been attempted by Ng and Lee (1996), Agirre and de Lacalle (2009), Chan and Ng (2007), Koeling et al. (2005) and Agirre et al. (2009b). They report results on three publicly available lexical sample datasets, viz., DSO corpus (Ng and Lee, 1996), MEDLINE corpus (Weeber et al., 2001) and the corpus made available by Koeling et al. (2005). Each of these datasets contains a handful of target words (41-191 words) which are sense marked in the corpus. Our main inspiration comes from the targetword specific results reported by Chan and Ng (2007) and Agirre and de Lacalle (2009). The former showed that adding just 30% of the target data to the source data achieved the same performance</context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation with active learning for word sense disambiguation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 49–56, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Escudero</author>
<author>Lluis M`arquez</author>
<author>German Rigau</author>
</authors>
<title>An empirical study of the domain dependence of supervised word sense disambiguation systems.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora,</booktitle>
<pages>172--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Escudero, M`arquez, Rigau, 2000</marker>
<rawString>Gerard Escudero, Lluis M`arquez, and German Rigau. 2000. An empirical study of the domain dependence of supervised word sense disambiguation systems. In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora, pages 172–180, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<contexts>
<context position="13646" citStr="Fellbaum, 1998" startWordPosition="2240" endWordPosition="2241"> selection?” We discuss this question in detail in Section 7 and show that a random set of injections performs no worse than a craftily selected set of injections. 5 DataSet Preparation Due to the lack of any publicly available all-words domain specific sense marked corpora we set upon the task of collecting data from two domains, viz., Tourism and Health. The data for Tourism domain was downloaded from Indian Tourism websites whereas the data for Health domain was obtained from two doctors. This data was manually sense annotated by two lexicographers adept in English. Princeton Wordnet 2.13 (Fellbaum, 1998) was used as the sense inventory. A total of 1,34,095 words from the Tourism domain and 42,046 words from the Health domain were manually sense marked. Some files were sense marked by both the lexicographers and the Inter Tagger Agreement (ITA) calculated from these files was 83% which is comparable to the 78% ITA reported on the SemCor corpus considering the domainspecific nature of the corpus. We now present different statistics about the corpora. Table 1 summarizes the number of polysemous and monosemous words in each category. 3http://wordnetweb.princeton.edu/perl/webwn Note that we do not</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. of the Int’l. Conf. on Research in Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="16854" citStr="Jiang and Conrath, 1997" startWordPosition="2786" endWordPosition="2789">sm and Health data are summarized in Table 6. We also report the Wordnet first sense baseline (WFS). Domain Algorithm P(%) R(%) F(%) Tourism PPR 53.1 53.1 53.1 WFS 62.5 62.5 62.5 Health PPR 51.1 51.1 51.1 WFS 65.5 65.5 65.5 Table 6: Comparing the performance of Personalized PageRank (PPR) with Wordnet First Sense Baseline (WFS) 6.2 Unsupervised approach The predominant sense for each word in the two domains was calculated using the method described in section 4.2. McCarthy et al. (2004) reported that the best results were obtained using k = 50 neighbors and the Wordnet Similarity jcn measure (Jiang and Conrath, 1997). Following them, we used k = 50 and observed that the best results for nouns and verbs were obtained using the jcn measure and the best results for adjectives and adverbs were obtained using the lesk measure (Banerjee and Pedersen, 2002). Accordingly, we used jcn for nouns and verbs and lesk for adjectives and adverbs. Each target word in the test set is then disambiguated by simply assigning it its predominant sense obtained using the above method. We tested this approach only on Tourism domain due to unavailability of large 4http://www.cfilt.iitb.ac.in/wsd/annotated corpus untagged Health c</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J.J. Jiang and D.W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. of the Int’l. Conf. on Research in Computational Linguistics, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh Khapra</author>
<author>Sapan Shah</author>
<author>Piyush Kedia</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Domain-specific word sense disambiguation combining corpus based and wordnet based parameters.</title>
<date>2010</date>
<booktitle>In 5th International Conference on Global Wordnet (GWC2010).</booktitle>
<contexts>
<context position="2498" citStr="Khapra et al., 2010" startWordPosition="400" endWordPosition="403">o WSD a difficult proposition (Agirre et al., 2009b; Agirre et al., 2009a; McCarthy et al., 2007). Unsupervised and knowledge based approaches have been tried with the hope of creating WSD systems with no need for sense marked corpora (Koeling et al., 2005; McCarthy et al., 2007; Agirre et al., 2009b). However, the accuracy figures of such systems are low. Our work here is motivated by the desire to develop annotation-lean all-words domain adapted techniques for supervised WSD. It is a common observation that domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. Also domain adaptation - in which training happens in one domain and testing in another - often is able to attain good levels of performance, albeit on a specific set of target words (Chan and Ng, 2007; Agirre and de Lacalle, 2009). To the best of our knowledge there does not exist a system that solves the combined problem of all words domain adapted WSD. We thus propose the following: a. For any target domain, create a small amount of sense annotated corpus. b. Mix it with an existing sense annotated corpus – from a mixed domain or spec</context>
<context position="9332" citStr="Khapra et al. (2010)" startWordPosition="1524" endWordPosition="1527">nd Manning, 2003). 3. Use the grammatical relations thus extracted to construct features for identifying the k nearest neighbors for each word using the distributional similarity score described in (Lin, 1998). 4. Rank the senses of each target word in the test set using a weighted sum of the distributional similarity scores of the neighbors. The weights in the sum are based on Wordnet Similarity scores (Patwardhan and Pedersen, 2003). 5. Each target word in the test set is then disambiguated by simply assigning it its predominant sense obtained using the above method. 3.3 Supervised approach Khapra et al. (2010) proposed a supervised algorithm for domain-specific WSD and showed that it beats the most frequent corpus sense and performs on par with other state of the art algorithms like PageRank. We implemented their iterative algorithm which involves the following steps: 1. Tag all monosemous words in the sentence. 2. Iteratively disambiguate the remaining words in the sentence in increasing order of their degree of polysemy. 3. At each stage rank the candidate senses of a word using the scoring function of Equation (1) which combines corpus based parameters (such as, sense distributions and corpus co</context>
<context position="10667" citStr="Khapra et al., 2010" startWordPosition="1727" endWordPosition="1730">er.shtml (such as, semantic similarity, conceptual distance, etc.) � S* = arg max(OiVi + i jEJ (1) where, i E Candidate Synsets J = Set of disambiguated words Oi = BelongingnessToDominantConcept(Si) Vi = P(Si|word) Wij = CorpusCooccurrence(Si, Sj) * VWNConceptualDistance(Si, Sj) * VWNSemanticGraphDistance(Si, Sj) 4. Select the candidate synset with maximizes the above score as the winner sense. 4 Injections for Supervised Adaptation This section describes the main interest of our work i.e. adaptation using injections. For supervised adaptation, we use the supervised algorithm described above (Khapra et al., 2010) in the following 3 settings as proposed by Agirre et al. (2009a): a. Source setting: We train the algorithm on a mixed-domain corpus (SemCor) or a domainspecific corpus (say, Tourism) and test it on a different domain (say, Health). A good performance in this setting would indicate robustness to domain-shifts. b. Target setting: We train and test the algorithm using data from the same domain. This gives the skyline performance, i.e., the best performance that can be achieved if sense marked data from the target domain were available. c. Adaptation setting: This setting is the main focus of in</context>
</contexts>
<marker>Khapra, Shah, Kedia, Bhattacharyya, 2010</marker>
<rawString>Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak Bhattacharyya. 2010. Domain-specific word sense disambiguation combining corpus based and wordnet based parameters. In 5th International Conference on Global Wordnet (GWC2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In INPROCEEDINGS OF THE 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="8729" citStr="Klein and Manning, 2003" startWordPosition="1426" endWordPosition="1429">und the predominant sense (i.e., the most frequent sense) of each target word using pair-wise Wordnet based similarity measures by pairing the target word with its top-k neighbors in the thesaurus. Each target word is then disambiguated by assigning it its predominant sense – the motivation being that the predominant sense is a powerful, hard-to-beat baseline. We implemented their method using the following steps: 1. Obtain a domain-specific untagged corpus (we crawled a corpus of approximately 9M words from the web). 2. Extract grammatical relations from this text using a dependency parser2 (Klein and Manning, 2003). 3. Use the grammatical relations thus extracted to construct features for identifying the k nearest neighbors for each word using the distributional similarity score described in (Lin, 1998). 4. Rank the senses of each target word in the test set using a weighted sum of the distributional similarity scores of the neighbors. The weights in the sum are based on Wordnet Similarity scores (Patwardhan and Pedersen, 2003). 5. Each target word in the test set is then disambiguated by simply assigning it its predominant sense obtained using the above method. 3.3 Supervised approach Khapra et al. (20</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In INPROCEEDINGS OF THE 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Domain-specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>419--426</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2134" citStr="Koeling et al., 2005" startWordPosition="339" endWordPosition="342">WSD. Finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific WSD. 1 Introduction Amongst annotation tasks, sense marking surely takes the cake, demanding as it does high level of language competence, topic comprehension and domain sensitivity. This makes supervised approaches to WSD a difficult proposition (Agirre et al., 2009b; Agirre et al., 2009a; McCarthy et al., 2007). Unsupervised and knowledge based approaches have been tried with the hope of creating WSD systems with no need for sense marked corpora (Koeling et al., 2005; McCarthy et al., 2007; Agirre et al., 2009b). However, the accuracy figures of such systems are low. Our work here is motivated by the desire to develop annotation-lean all-words domain adapted techniques for supervised WSD. It is a common observation that domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. Also domain adaptation - in which training happens in one domain and testing in another - often is able to attain good levels of performance, albeit on a specific set of target wo</context>
<context position="4365" citStr="Koeling et al. (2005)" startWordPosition="720" endWordPosition="724">s three state of art supervised, unsupervised and knowledge based algorithms for WSD. Section 4 discusses the injection strategy for domain adaptation. In section 5 we describe the dataset used for our experiments. We then present the results in section 6 followed by discussions in section 7. Section 8 examines whether there is any need for intelligent choice of injections. Section 9 concludes the paper highlighting possible future directions. 2 Related Work Domain specific WSD for selected target words has been attempted by Ng and Lee (1996), Agirre and de Lacalle (2009), Chan and Ng (2007), Koeling et al. (2005) and Agirre et al. (2009b). They report results on three publicly available lexical sample datasets, viz., DSO corpus (Ng and Lee, 1996), MEDLINE corpus (Weeber et al., 2001) and the corpus made available by Koeling et al. (2005). Each of these datasets contains a handful of target words (41-191 words) which are sense marked in the corpus. Our main inspiration comes from the targetword specific results reported by Chan and Ng (2007) and Agirre and de Lacalle (2009). The former showed that adding just 30% of the target data to the source data achieved the same performance as that obtained by ta</context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2005</marker>
<rawString>Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predominant sense acquisition. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 419–426, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>768--774</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8921" citStr="Lin, 1998" startWordPosition="1456" endWordPosition="1457">get word is then disambiguated by assigning it its predominant sense – the motivation being that the predominant sense is a powerful, hard-to-beat baseline. We implemented their method using the following steps: 1. Obtain a domain-specific untagged corpus (we crawled a corpus of approximately 9M words from the web). 2. Extract grammatical relations from this text using a dependency parser2 (Klein and Manning, 2003). 3. Use the grammatical relations thus extracted to construct features for identifying the k nearest neighbors for each word using the distributional similarity score described in (Lin, 1998). 4. Rank the senses of each target word in the test set using a weighted sum of the distributional similarity scores of the neighbors. The weights in the sum are based on Wordnet Similarity scores (Patwardhan and Pedersen, 2003). 5. Each target word in the test set is then disambiguated by simply assigning it its predominant sense obtained using the above method. 3.3 Supervised approach Khapra et al. (2010) proposed a supervised algorithm for domain-specific WSD and showed that it beats the most frequent corpus sense and performs on par with other state of the art algorithms like PageRank. We</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international conference on Computational linguistics, pages 768–774, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>279</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16721" citStr="McCarthy et al. (2004)" startWordPosition="2762" endWordPosition="2765">from the two domains. 6.1 Knowledge Based approach The results obtained by applying the Personalized PageRank (PPR) method to Tourism and Health data are summarized in Table 6. We also report the Wordnet first sense baseline (WFS). Domain Algorithm P(%) R(%) F(%) Tourism PPR 53.1 53.1 53.1 WFS 62.5 62.5 62.5 Health PPR 51.1 51.1 51.1 WFS 65.5 65.5 65.5 Table 6: Comparing the performance of Personalized PageRank (PPR) with Wordnet First Sense Baseline (WFS) 6.2 Unsupervised approach The predominant sense for each word in the two domains was calculated using the method described in section 4.2. McCarthy et al. (2004) reported that the best results were obtained using k = 50 neighbors and the Wordnet Similarity jcn measure (Jiang and Conrath, 1997). Following them, we used k = 50 and observed that the best results for nouns and verbs were obtained using the jcn measure and the best results for adjectives and adverbs were obtained using the lesk measure (Banerjee and Pedersen, 2002). Accordingly, we used jcn for nouns and verbs and lesk for adjectives and adverbs. Each target word in the test set is then disambiguated by simply assigning it its predominant sense obtained using the above method. We tested th</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 279, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Unsupervised acquisition of predominant word senses.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>4</issue>
<pages>590</pages>
<contexts>
<context position="1975" citStr="McCarthy et al., 2007" startWordPosition="311" endWordPosition="314">g lend credence to the viability of our approach. Our contribution thus lies in finding a convenient middle ground between pure supervised and pure unsupervised WSD. Finally, our approach is not restricted to any specific set of target words, a departure from a commonly observed practice in domain specific WSD. 1 Introduction Amongst annotation tasks, sense marking surely takes the cake, demanding as it does high level of language competence, topic comprehension and domain sensitivity. This makes supervised approaches to WSD a difficult proposition (Agirre et al., 2009b; Agirre et al., 2009a; McCarthy et al., 2007). Unsupervised and knowledge based approaches have been tried with the hope of creating WSD systems with no need for sense marked corpora (Koeling et al., 2005; McCarthy et al., 2007; Agirre et al., 2009b). However, the accuracy figures of such systems are low. Our work here is motivated by the desire to develop annotation-lean all-words domain adapted techniques for supervised WSD. It is a common observation that domain specific WSD exhibits high level of accuracy even for the all-words scenario (Khapra et al., 2010) - provided training and testing are on the same domain. Also domain adaptati</context>
<context position="5385" citStr="McCarthy et al. (2007)" startWordPosition="897" endWordPosition="900">results reported by Chan and Ng (2007) and Agirre and de Lacalle (2009). The former showed that adding just 30% of the target data to the source data achieved the same performance as that obtained by taking the entire source and target data. Agirre and de Lacalle (2009) reported a 22% error reduction when source and target data were combined for training a classifier, as compared to the case when only the target data was used for training the classifier. However, both these works focused on target word specific WSD and do not address all-words domain specific WSD. In the unsupervised setting, McCarthy et al. (2007) showed that their predominant sense acquisition method gives good results on the corpus of Koeling et al. (2005). In particular, they showed that the performance of their method is comparable to the most frequent sense obtained from a tagged corpus, thereby making a strong case for unsupervised methods for domain-specific WSD. More recently, Agirre et al. (2009b) showed that knowledge based approaches which rely only on the semantic relations captured by the Wordnet graph outperform supervised approaches when applied to specific domains. The good results obtained by McCarthy et al. (2007) and</context>
<context position="8025" citStr="McCarthy et al. (2007)" startWordPosition="1312" endWordPosition="1315"> (i.e., synsets) act as nodes and the relations between concepts define edges in the graph. 2. Apply a context-dependent Personalized PageRank algorithm on this graph by introducing the context words as nodes into the graph and linking them with their respective synsets. 3. These nodes corresponding to the context words then inject probability mass into the synsets they are linked to, thereby influencing the final relevance of all nodes in the graph. We used the publicly available implementation of this algorithm1 for our experiments. 1http://ixa2.si.ehu.es/ukb/ 1533 3.2 Unsupervised Approach McCarthy et al. (2007) used an untagged corpus to construct a thesaurus of related words. They then found the predominant sense (i.e., the most frequent sense) of each target word using pair-wise Wordnet based similarity measures by pairing the target word with its top-k neighbors in the thesaurus. Each target word is then disambiguated by assigning it its predominant sense – the motivation being that the predominant sense is a powerful, hard-to-beat baseline. We implemented their method using the following steps: 1. Obtain a domain-specific untagged corpus (we crawled a corpus of approximately 9M words from the we</context>
<context position="30031" citStr="McCarthy et al. (2007)" startWordPosition="5064" endWordPosition="5067">d that there was a 75- 80% overlap between the words selected by random strategy and oracle strategy. This is because oracle selects the most frequent words which also have a high chance of getting selected when a random sampling is done. Figures 6, 7, 8 and 9 compare the performance of the two strategies. We see that the random strategy does as well as the oracle strategy thereby supporting our claim that if we have sense marked corpus from one domain then simply injecting ANY small amount of data from the target domain will 5Note that the unsupervised predominant sense acquisition method of McCarthy et al. (2007) implicitly identifies conformists and non-conformists 1539 Injection Size v/s F-score 0 2000 4000 6000 8000 10000 12000 14000 Injection Size (words) Figure 6: Comparing random strategy with oracle based ideal strategy for SemCor to Tourism adaptation Injection Size v/s F-score 0 2000 4000 6000 8000 10000 12000 14000 Injection Size (words) Figure 8: Comparing random strategy with oracle based ideal strategy for Tourism to Health adaptation Injection Size v/s F-score 0 2000 4000 6000 8000 10000 12000 14000 Injection Size (words) Figure 7: Comparing random strategy with oracle based ideal strate</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2007</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2007. Unsupervised acquisition of predominant word senses. Comput. Linguist., 33(4):553– 590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In HLT ’93: Proceedings of the workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3227" citStr="Miller et al., 1993" startWordPosition="535" endWordPosition="538">n one domain and testing in another - often is able to attain good levels of performance, albeit on a specific set of target words (Chan and Ng, 2007; Agirre and de Lacalle, 2009). To the best of our knowledge there does not exist a system that solves the combined problem of all words domain adapted WSD. We thus propose the following: a. For any target domain, create a small amount of sense annotated corpus. b. Mix it with an existing sense annotated corpus – from a mixed domain or specific domain – to train the WSD engine. This procedure tested on four adaptation scenarios, viz., (i) SemCor (Miller et al., 1993) to Tourism, (ii) SemCor to Health, (iii) Tourism to Health and (iv) Health to Tourism has consistently yielded good performance (to be explained in sections 6 and 7). The remainder of this paper is organized as follows. In section 2 we discuss previous work in the area of domain adaptation for WSD. In section 3 1532 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1532–1541, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics we discuss three state of art supervised, unsupervised and knowledge based algorithms for WS</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In HLT ’93: Proceedings of the workshop on Human Language Technology, pages 303–308, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4292" citStr="Ng and Lee (1996)" startWordPosition="707" endWordPosition="710">July 2010. c�2010 Association for Computational Linguistics we discuss three state of art supervised, unsupervised and knowledge based algorithms for WSD. Section 4 discusses the injection strategy for domain adaptation. In section 5 we describe the dataset used for our experiments. We then present the results in section 6 followed by discussions in section 7. Section 8 examines whether there is any need for intelligent choice of injections. Section 9 concludes the paper highlighting possible future directions. 2 Related Work Domain specific WSD for selected target words has been attempted by Ng and Lee (1996), Agirre and de Lacalle (2009), Chan and Ng (2007), Koeling et al. (2005) and Agirre et al. (2009b). They report results on three publicly available lexical sample datasets, viz., DSO corpus (Ng and Lee, 1996), MEDLINE corpus (Weeber et al., 2001) and the corpus made available by Koeling et al. (2005). Each of these datasets contains a handful of target words (41-191 words) which are sense marked in the corpus. Our main inspiration comes from the targetword specific results reported by Chan and Ng (2007) and Agirre and de Lacalle (2009). The former showed that adding just 30% of the target dat</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 40–47, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ted Pedersen</author>
</authors>
<title>The cpan wordnet::similarity package. http://search .cpan.org/ sid/wordnet-similarity/.</title>
<date>2003</date>
<contexts>
<context position="9150" citStr="Patwardhan and Pedersen, 2003" startWordPosition="1493" endWordPosition="1496">s: 1. Obtain a domain-specific untagged corpus (we crawled a corpus of approximately 9M words from the web). 2. Extract grammatical relations from this text using a dependency parser2 (Klein and Manning, 2003). 3. Use the grammatical relations thus extracted to construct features for identifying the k nearest neighbors for each word using the distributional similarity score described in (Lin, 1998). 4. Rank the senses of each target word in the test set using a weighted sum of the distributional similarity scores of the neighbors. The weights in the sum are based on Wordnet Similarity scores (Patwardhan and Pedersen, 2003). 5. Each target word in the test set is then disambiguated by simply assigning it its predominant sense obtained using the above method. 3.3 Supervised approach Khapra et al. (2010) proposed a supervised algorithm for domain-specific WSD and showed that it beats the most frequent corpus sense and performs on par with other state of the art algorithms like PageRank. We implemented their iterative algorithm which involves the following steps: 1. Tag all monosemous words in the sentence. 2. Iteratively disambiguate the remaining words in the sentence in increasing order of their degree of polyse</context>
</contexts>
<marker>Patwardhan, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan and Ted Pedersen. 2003. The cpan wordnet::similarity package. http://search .cpan.org/ sid/wordnet-similarity/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The english all-words task.</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>41--43</pages>
<editor>In Rada Mihalcea and Phil Edmonds, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="22232" citStr="Snyder and Palmer, 2004" startWordPosition="3707" endWordPosition="3710">t domain - without using any injections). These observations bring out the weaknesses of these approaches when used in an all-words setting and clearly indicate that they come nowhere close to replacing a supervised system. 1537 7.2 Supervised adaptation 1. The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) – F-score of 61.7% on Tourism and Fscore of 65.5% on Health – is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing happens on a mixed-domain corpus (Snyder and Palmer, 2004)). This is in contrast to previous studies (Escudero et al., 2000; Agirre and Martinez, 2004) which suggest that instead of adapting from a generic/mixed domain to a specific domain, it is better to completely ignore the generic examples and use hand-tagged data from the target domain itself. The main reason for the contrasting results is that the earlier work focused only on a handful of target words whereas we focus on all words appearing in the corpus. So, while the behavior of a few target words would change drastically when the domain changes, a majority of the words will exhibit the same</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The english all-words task. In Rada Mihalcea and Phil Edmonds, editors, Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 41–43, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Weeber</author>
<author>James G Mork</author>
<author>Alan R Aronson</author>
</authors>
<title>Developing a test collection for biomedical word sense disambiguation. In</title>
<date>2001</date>
<booktitle>In Proceedings of the AMAI Symposium,</booktitle>
<pages>746--750</pages>
<contexts>
<context position="4539" citStr="Weeber et al., 2001" startWordPosition="749" endWordPosition="752">e the dataset used for our experiments. We then present the results in section 6 followed by discussions in section 7. Section 8 examines whether there is any need for intelligent choice of injections. Section 9 concludes the paper highlighting possible future directions. 2 Related Work Domain specific WSD for selected target words has been attempted by Ng and Lee (1996), Agirre and de Lacalle (2009), Chan and Ng (2007), Koeling et al. (2005) and Agirre et al. (2009b). They report results on three publicly available lexical sample datasets, viz., DSO corpus (Ng and Lee, 1996), MEDLINE corpus (Weeber et al., 2001) and the corpus made available by Koeling et al. (2005). Each of these datasets contains a handful of target words (41-191 words) which are sense marked in the corpus. Our main inspiration comes from the targetword specific results reported by Chan and Ng (2007) and Agirre and de Lacalle (2009). The former showed that adding just 30% of the target data to the source data achieved the same performance as that obtained by taking the entire source and target data. Agirre and de Lacalle (2009) reported a 22% error reduction when source and target data were combined for training a classifier, as co</context>
</contexts>
<marker>Weeber, Mork, Aronson, 2001</marker>
<rawString>Marc Weeber, James G. Mork, and Alan R. Aronson. 2001. Developing a test collection for biomedical word sense disambiguation. In In Proceedings of the AMAI Symposium, pages 746–750.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>