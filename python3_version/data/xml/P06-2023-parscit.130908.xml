<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.998727">
A Bio-inspired Approach for Multi-Word Expression Extraction
</title>
<author confidence="0.754458">
Jianyong Duan, Ruzhan Lu Yan Tian
Weilin Wu, Yi Hu School of Foreign Languages
</author>
<affiliation confidence="0.9975715">
Department of Computer Science Department of Computer Science
Shanghai Jiao Tong University Shanghai Jiao Tong University
</affiliation>
<address confidence="0.883211">
Shanghai, 200240, P.R. China Shanghai, 200240, P.R. China
</address>
<email confidence="0.9808165">
duanjy@hotmail.com tianyan@sjtu.edu.cn
{lu-rz,wl-wu,huyi}@cs.sjtu.edu.cn
</email>
<sectionHeader confidence="0.997319" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917333333333">
This paper proposes a new approach for
Multi-word Expression (MWE)extraction
on the motivation of gene sequence align-
ment because textual sequence is simi-
lar to gene sequence in pattern analy-
sis. Theory of Longest Common Subse-
quence (LCS) originates from computer
science and has been established as affine
gap model in Bioinformatics. We per-
form this developed LCS technique com-
bined with linguistic criteria in MWE ex-
traction. In comparison with traditional
n-gram method, which is the major tech-
nique for MWE extraction, LCS approach
is applied with great efficiency and per-
formance guarantee. Experimental results
show that LCS-based approach achieves
better results than n-gram.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952083333334">
Language is under continuous development. Peo-
ple enlarge vocabulary and let words carry more
meanings. Meanwhile the language also devel-
ops larger lexical units to carry specific meanings;
specifically MWE’s, which include compounds,
phrases, technical terms, idioms and collocations,
etc. The MWE has relatively fixed pattern because
every MWE denotes a whole concept. In compu-
tational view, the MWE repeats itself constantly in
corpus(Taneli,2003).
The extraction of MWE plays an important role
in several areas, such as machine translation (Pas-
cale,1997), information extraction (Kalliopi,2000)
etc. On the other hand, there is also a need
for MWE extraction in a much more widespread
scenario namely that of human translation and
technical writing. Many efforts have been de-
voted to the study of MWE extraction (Beat-
rice,2003; Ivan,2002; Jordi,2001). These statis-
tical methods detect MWE by frequency of can-
didate patterns. Linguistic information as a filter-
ing strategy is also performed to improve precision
by ranking their candidates (Violeta,2003; Ste-
fan,2004; Arantza,2002). Some measures based
on advance statistical methods are also used,
such as mutual expectation with single statis-
tic model (Paul,2005),C-value/NC-value method
(Katerina,2000),etc.
Frequent information is the original data for
further MWE extraction. Most approaches adopt
n-gram technique(Daniel,1977; Satanjeev,2003;
Makoto,1994). n-gram concerns about one se-
quence for each time. Every sequence can be
cut into some segments with varied lengths be-
cause any length of segment has the possibility to
become candidate MWE. The larger the context
window is, the more difficulty its parameters ac-
quire. Thus data sparseness problem deteriorates.
Another problem arises from the flexible MWE
which can be separated by an arbitrary number of
blanks, for instance, “make decision”. These
models cannot effectively distinguish all kinds of
variations in flexible MWE.
On the consideration of relations between tex-
tual sequence and gene sequence, we propose a
new bio-inspired approach for MWE identifica-
tion. Both statistical and linguistic information are
incorporated into this model.
</bodyText>
<sectionHeader confidence="0.996102" genericHeader="method">
2 Multi-word Expression
</sectionHeader>
<bodyText confidence="0.993571">
Multi-word Expression( in general, term) as the
linguistic representation of concepts, also has
some special statistical features. The component
words of terms co-occur in the same context fre-
</bodyText>
<page confidence="0.981218">
176
</page>
<bodyText confidence="0.947996857142857">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 176–182,
Sydney, July 2006. c�2006 Association for Computational Linguistics
quently. MWE extraction can be viewed as a prob-
lem of pattern extraction. It has two major phases.
The first phase is to search the candidate MWEs by
their frequent occurrence in the corpus. The sec-
ond phase is to filter true MWEs from noise candi-
dates. Filtering process involves linguistic knowl-
edge and some intelligent observations.
MWE can be classified into strict patterns and
flexible patterns by structures of their component
words(Joaquim,1999). For example, a textual se-
quence s = w1w2 · · · wi · · · wn, may contain two
kinds of patterns:
</bodyText>
<equation confidence="0.992207333333333">
Strict pattern: pi = wiwi+1wi+2
Flexible pattern: pj = witwi+2twi+4, pk =
wi t twi+3wi+4
</equation>
<bodyText confidence="0.9999775">
where t denotes the variational or active ele-
ment in pattern. The flexible pattern extraction is
always a bottleneck for MWE extraction for lack
of good knowledge of global solution.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="method">
3 Algorithms for MWE Extraction
</sectionHeader>
<subsectionHeader confidence="0.997001">
3.1 Pure Mathematical Method
</subsectionHeader>
<bodyText confidence="0.966944428571429">
Although sequence alignment algorithm has been
well-developed in bioinformatics (Michael,2003),
(Knut,2000), (Hans,1999), it was rarely reported
in MWE extraction. In fact, it also applies to
MWE extraction especially for complex struc-
tures.
Algorithm.1.
</bodyText>
<listItem confidence="0.959887857142857">
1. Input:tokenlized textual sequences Q =
{s1, s2, ··· , sn}
2. Initionalization : pool, Ω = {Ωk}, Ψ
3. Computation:
I. Pairwise sequence alignment
for all si, sj ∈ Q, si =6 sj
Similarity(si, sj)
</listItem>
<equation confidence="0.995297">
Align(si, sj) path(li,lj)
−→ {li, lj, ck}
pool ← pool ∪ {(li, ck), (lj, ck)}
Γ ← Γ ∪ ck
</equation>
<bodyText confidence="0.644825">
II. Creation of consistent set
for all ck ∈ Γ, (li, ck) ∈ pool
</bodyText>
<equation confidence="0.911613">
Ωk ← Ωk + {li}
pool ← pool − (li, ck)
</equation>
<bodyText confidence="0.3649085">
III. Multiple sequence alignment
for all Ωk
</bodyText>
<equation confidence="0.860588333333333">
star align(Ωk) → MWU Ψ ←
Ψ ∪ MWU
4. Output: Ψ
</equation>
<bodyText confidence="0.97803247826087">
Our approach is directly inspired by gene se-
quence alignment as algorithm. 1. showed. The
textual sequence should be preprocessed before in-
put. For example, plurals recognition is a rela-
tively simple task for computers which just need
to check if the word accord with the general rule
including rule (+s) and some alternative rules (-y +
ies), etc. A set of tense forms, such as past, present
and future forms, are also transformed into origi-
nal forms. These tokenlized sequences will im-
prove extraction quality.
Pairwise sequence alignment is a crucial step.
Our algorithm uses local alignment for textual se-
quences. The similarity score between s[1... i]
and t[1... i] can be computed by three arrays
G[i, j], E[i, j] ,F[i, j] and zero, where entry δ(x, y)
means word x matches with word y; V[i, j] de-
notes the best score of entry δ(x, y); G[i, j] de-
notes s[i] matched with t[j]:δ(s[i], t[j]); E[i, j]
denotes a blank of string s matched with t[j] :
δ(t, t[j]); F [i, j] denotes s[i] matched with a
blank of string t : δ(s[i], t).
Initialization:
</bodyText>
<equation confidence="0.793192823529412">
V [0,0] = 0; V [i, 0] = E[i, 0] = 0; 1 ≤ i ≤
m. V [0, j] = F[0, j] = 0; 1 ≤ j ≤ n.
A dynamic programming solution:
V [i, j] = max{G[i, j], E[i, j], G[i, j], 0};
G[i − 1,j − 1]
E[i − 1,j − 1]
G[i, j] = δ(i, j) + max − 1,j − 1]
F[i
I 0
−(h + g) + G[i, j − 1]
− g + E[i, j − 1]
−(h + g) + F[i, j − 1]
0
−(h + g) + G[i − 1,j]
−(h + g) + E[i − 1,j]
− g + F[i − 1,j]
0
</equation>
<bodyText confidence="0.9650598">
Here we explain the meaning of these arrays:
I. G[i, j] includes the entry δ(i, j), it denotes
the sum score is the last row plus the max-
imal score between prefix s[1... i − 1] and
t[1... j − 1].
</bodyText>
<equation confidence="0.994228333333333">
E[i,j] = max
F [i, j] = max
�
��� �
����
I
</equation>
<page confidence="0.959681">
177
</page>
<bodyText confidence="0.9994845">
II. Otherwise the related prefixes s[1... i] and
t[1 ... j − 1] are needed1. They are used to
check the first blank or additional blank in or-
der to give appropriate penalty.
</bodyText>
<listItem confidence="0.960886714285714">
a. For G[i, j −1] and F[i, j −1], they don’t
end with a blank in string s. The blank
s[i] is the first blank. Its score is G[i, j −
1] (or F[i, j − 1]) minus (h + g).
b. For E[i, j − 1],The blank is the addi-
tional blank which should be only sub-
tracted g.
</listItem>
<bodyText confidence="0.999902484848485">
In the maximum entry, it records the best score
of optimum local alignment. This entry can be
viewed as the started point of alignment. Then
we backtrack entries by checking arrays which are
generated from dynamic programming algorithm.
When the score decrease to zero, alignment exten-
sion terminates. Finally, the similarity and align-
ment results are easily acquired.
Lots of aligned segments are extracted from
pairwise alignment. Those segments with com-
mon component words (ck) will be collected into
the same set. It is called as consistent set for
further multiple sequence alignment. These con-
sistent sets collect similar sequences with score
greater than certain threshold.
We perform star-alignment in multiple se-
quence alignment. The center sequence in the con-
sistent set which has the highest score in com-
parison with others, is picked out from this set.
Then all the other sequences gather to the cen-
ter sequence with the technique of ”once a blank,
always a blank”. These aligned sequences form
common regions with n-column or a column. Ev-
ery column contains one or more words. Calcula-
tion of dot-matrices is a widespread tool for com-
mon region analysis. Dot-plot agreement is de-
veloped to identify common patterns and reliably
aligned regions in a set of related sequences. If
several plots calculate consistently in a sequence
set, it displays the similarity among them. It in-
creases credibility of extracted pattern in this con-
sistent set. Finally MWE with detailed pattern
emerges from this aligned sequence set.
</bodyText>
<subsectionHeader confidence="0.992621">
3.2 Linguistic Knowledge Combination
3.2.1 Heuristic Knowledge
</subsectionHeader>
<bodyText confidence="0.999965230769231">
Original candidate set is noise. Many meaning-
less patterns are extracted from corpus. Some lin-
guistic rules (Argamon,1999) are introduced into
our model. It is observed that candidate pattern
should contain content words. Some patterns are
only organized by pure function words, such as the
most frequent patterns “the to”, “of the”. These
patterns should be moved out from the candidate
set. Filter table with certain words is also per-
formed. For example, some words, like “then”,
cannot occur in the beginning position of MWE.
These filters will reduce the number of noise pat-
terns in great extent.
</bodyText>
<subsubsectionHeader confidence="0.638462">
3.2.2 Embedded Base Phrase detection
</subsubsectionHeader>
<bodyText confidence="0.98691">
Short textual sequence is apt to produce frag-
ments of MWE because local alignment ends pat-
tern extension when similarity score reduces to
zero. The matched component words increase
similarity score while unmatched words decrease
it. The similarity scores of candidates in textual
sequences are lower for lack of matched compo-
nent words. Without accumulation of higher sim-
ilarity score, pattern extension terminates quickly.
Pattern extension becomes especially sensitive to
unmatched words. Some isolated fragments are
generated in this circumstance. One solution is to
give higher scores for matched component words.
It strengthens pattern extension ability at the ex-
pense of introducing noise.
We propose Embedded base phrase(EBP) de-
tection as algorithm.2. It improves pattern ex-
traction by giving lower penalty for longer base
phrase. EBP is the base phrase in a gap (Changn-
ing,2000). It does not contain other phrase recur-
sively. Good quality of MWE should avoid irrela-
tive unit in its gap. The penalty function discerns
the true EBP and irrelative unit in a gap only by
length information. Longer gap means more irrel-
ative unit. It builds a rough penalty model for lack
of semantic information. We improve this model
by POS information. POS tagged textual sequence
is convenient to grammatical analysis. True EBP2
gives comparatively lower penalty.
Algorithm.2.
</bodyText>
<footnote confidence="0.9197084">
1. Input: LCS of sl, sk
1Analysis approaches for F[i, j] and E[i, j] are the same,
here only E[i, j] is given its detailed explanation.
2The performance of our EBP tagger is 95% accuracy for
base noun phrase and 90% accuracy for general use.
</footnote>
<page confidence="0.985458">
178
</page>
<figure confidence="0.862510066666667">
2. Check breakpoint in LCS
i. Anchor neighbored common words and
denote gaps
for all ws = wp, wt = wq
if ws E ls, wt E lt, ls =� lt
denote gst, gpq
ii. Detect EBP in gaps
EBPEBP
gst ) g0st,gpq ) g0pq
iii. Compute new similariy matrix in gaps
similarity(g0st, g0pq)
3. Link broken segment
if path(g0st, g0pq)
lst = ls + lt, lpq = lp + lq
For textual sequence: w1w2 · · · wn, and its
</figure>
<listItem confidence="0.580114">
corresponding POS tagged sequence: t1t2 · · · tn,
we suppose [wi · · · wj] is a gap from wi to wj
in sequence · · · wi−1 [wi · · · wj] wj · · · . The
corresponding tag sequence is [ti · · · tj] . We
only focus on EBP analysis in a gap instead of
global sequence. Context Free Grammar (CFG)
is employed in EBP detection. CFG rules follow
this form:
</listItem>
<equation confidence="0.9992346">
(1)EBP &lt;-- adj. + noun
(2)EBP &lt;-- noun + ”of” + noun
(3)EBP &lt;-- adv. + adj.
(4)EBP &lt;-- art. + adj. + noun
· · ·
</equation>
<bodyText confidence="0.999919777777778">
The sequences inside breakpoint of LCS are an-
alyzed by EBP detection. True base phrase will
be given lower penalty. When the gap penalty for
breakpoint is lower than threshold, the broken seg-
ment reunites. Based on experience knowledge,
when the length of a gap is less than four words,
EBP detection using CFG can gain good results.
Lower penalty for true EBP will help MWE to
emerge from noise pattern easily.
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.901363">
4.1 Resources
</subsectionHeader>
<bodyText confidence="0.999909130434782">
A large amount of free texts are collected in order
to meet the need of MWE extraction. These texts
are downloaded from internet with various aspects
including art, entertainment, military, business,
etc. Our corpus size is 200, 000 sentences. The
average sentence length is 15 words in corpus.
In addition, result evaluation is a hard job. Its
difficulty comes from two aspects. Firstly, MWE
identification for test corpus is a kind of labor-
intensive business. The judgment of MWEs re-
quires great efforts of domain expert. It is hard and
boring to make a standard test corpus for MWE
identification use. It is a bottleneck for large scales
use. Secondly it relates to human cognition in psy-
chological world. It is proved by experience that
various opinions cannot simply be judged true or
false. As a compromise way, gold standard set
can be established by some accepted resources, for
example, WordNet, as an online lexical reference
system, including many compounds and phrases.
Some terms extracted from dictionaries are also
employed in our experiments. There are nearly
70,000 MWEs in our list.
</bodyText>
<subsectionHeader confidence="0.513012">
4.2 Results and Discussion
4.2.1 Close Test
</subsectionHeader>
<bodyText confidence="0.999973129032258">
We created a closed test set of 8,000 sen-
tences. MWEs in corpus are extracted by man-
ual work. Every measure in both n-gram and LCS
approaches complies with the same threshold, for
example threshold for frequency is five times.Two
conclusions are drawn from Tab.1.
Firstly, LCS has higher recall than n-gram but
lower precision on the contrary. In close test set,
LCS recall is higher than n-gram. LCS unifies all
the cases of flexible patterns by GAM. However
n-gram only considers limited flexible patterns be-
cause of model limitation. LCS nearly includes
all the n-gram results. Higher recall decreases its
precision to a certain extent because some flexible
patterns are noisier than strict patterns. Flexible
patterns tend to be more irrelevant than strict pat-
terns. The GAM just provides a wiser choice for
all flexible patterns by its gap penalty function. N-
gram gives up analysis on many flexible patterns
without further ado. N-gram ensures its precision
by taking risk of MWE loss.
Secondly, advanced evaluation criterion can
place more MWEs in the front rank of candi-
date list. Evaluation metrics for extracted pat-
terns play an important role in MWE extraction.
Many criteria, which are reported with better per-
formances, are tested. MWE identification is sim-
ilar to IR task. These measures have their own
advantages to move interested patterns forward
in the candidate list. For example, Frequency
data contains much noise. True mutual infor-
</bodyText>
<page confidence="0.99938">
179
</page>
<tableCaption confidence="0.999867">
Table 1: Close Test for N-gram and LCS Approaches
</tableCaption>
<table confidence="0.999895142857143">
Measure Precision N-Gram F-Measure Precision LCS F-Measure
Recall Recall
(%) (%) (%) (%) (%) (%)
Frequency 35.2 38.0 36.0 32.1 48.2 38.4
TMI 44.7 56.2 49.1 43.2 62.1 51.4
ME 51.6 52.6 51.2 44.7 65.2 52.0
Rankratio 62.1 61.5 61.1 57.0 83.1 68.5
</table>
<bodyText confidence="0.935525909090909">
mation (TMI) concerns mutual information with
logarithm(Manning,1999). Mutual expectation
(ME) takes into account the relative probability of
each word compared to the phrase(Joaquim,1999).
Rankratio performs the best on both n-gram and
LCS approaches because it provides all the con-
texts which associated with each word in the cor-
pus and ranks them(Paul,2005). With the help of
advanced statistic measures, the scores of MWEs
are high enough to be detected from noisy pat-
terns.
</bodyText>
<subsubsectionHeader confidence="0.527747">
4.2.2 Open Test
</subsubsectionHeader>
<bodyText confidence="0.997854666666667">
In open test, we just show the extracted MWE
numbers in different given corpus sizes. Two phe-
nomena are observed in Fig.1.
</bodyText>
<figure confidence="0.717767">
1 2 3 4 5 6 7 8 9 10
corpus size
</figure>
<figureCaption confidence="0.980805">
Figure 1: Open Test for N-gram and LCS Ap-
proaches
</figureCaption>
<bodyText confidence="0.9993989375">
Firstly, with the enlargement of corpus
size(every step of corpus size is 10,000 sen-
tences), the detected MWE numbers increase in
both approaches. When the corpus size reaches
certain values, their increment speeds turn slower.
It is reasonable on condition that MWE follow
normal distribution. In the beginning, frequent
MWEs are detected easily, and the number
increases quickly. At a later phase, the detection
goes into comparatively infrequent area. Mining
these MWEs always need more corpus support.
Lower increment speed appears.
Secondly, although LCS always keeps ahead in
detecting MWE numbers, their gaps reduce with
the increment of corpus size. LCS is sensitive
to the MWE detection because of its alignment
mechanism in which there is no difference be-
tween flexible pattern and strict pattern. In the
beginning phase, LCS can detect MWEs which
have high frequencies with flexible patterns. N-
gram cannot effectively catch these flexible pat-
terns. LCS detects a larger number of MWE than
n-gram does. In the latter phase, many variable
patterns for flexible MWE can also be observed,
among which relatively strict patterns may appear
in the larger corpus. They will be catched by
n-gram. On the surface of observation, the dis-
crepancy of detected numbers is gradually close
to LCS. In nature, n-gram just makes up its lim-
itation at the expense of corpus size because its
detection mechanism for flexible patterns has no
radical change.
</bodyText>
<sectionHeader confidence="0.995188" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999885">
In this article, our LCS-based approach is inspired
by gene sequence alignment. In a new view, we
reconsider MWE extraction task. These two tasks
coincide with each other in pattern recognition.
Some new phenomena in natural language are also
observed. For example, we improve MWE min-
ing result by EBP detection. Comparisons with
variant n-gram approaches, which are the leading
approaches, are performed for verifying the effec-
tiveness of our approach. Although LCS approach
results in better extraction model, a lot of im-
provements for more robust model are still needed.
</bodyText>
<figure confidence="0.998549090909091">
MWU number
1800
1600
1400
1200
1000
400
800
600
N-Gram
LCS
</figure>
<page confidence="0.983144">
180
</page>
<bodyText confidence="0.99850625">
Each innovation presented here only opens the
way for more research. Some established theories
between Computational Linguistics and Bioinfor-
matics can be shared in a broader way.
</bodyText>
<sectionHeader confidence="0.99891" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999820375">
The authors would like to thank three anony-
mous reviewers for their careful reading and help-
ful suggestions. This work is supported by
National Natural Science Foundation of China
(NSFC) (No.60496326) and 863 project of China
(No.2001AA114210-11). Our thanks also go to
Yushi Xu and Hui Liu for their coding and techni-
cal support.
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999777551724138">
Arantza Casillas, Raquel Martłnez , 2002. Aligning
Multiword Terms Using a Hybrid Approach. Lecture
Notes in Computer Science 2276: The 3rd Interna-
tional Conference of Computational Linguistics and
Intelligent Text Processing.
Argamon, Shlomo, Ido Dagan and Yuval Kry-
molowski, 1999. A memory based approach to
learning shallow natural language patterns. Journal
of Experimental and Theoretical AI. 11, 369-390.
Beatrice Daille, 2003. Terminology Mining. Lecture
Notes in Computer Science 2700: Extraction in the
Web Era.
Changning Huang, Endong Xun, Zhou Ming, 2000.
A Unified Statistical Model for the Identification of
English BaseNP. The 38th Annual Meeting of the
Association for Computational Linguistics.
Daniel S. Hirschberg, 1977. Algorithms for the Longest
Common Subsequence Problem, Journal of the
ACM, 24(4), 664-675.
Diana Binnenpoorte, Catia Cucchiarini, Lou Boves
and Helmer Strik,2005. Multiword expressions
in spoken language: An exploratory study on
pronunciation variation. Computer Speech and
Language,19(4):433-449
Hans Peter Lenhof, Burkhard Morgenstern, Knut Rein-
ert, 1999. An exact solution for the segment-
to-segment multiple sequence alignment problem.
Bioinformatics. 15(3): 203-210.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
A. Copestake, Dan Flickinger, 2002. Multiword Ex-
pressions: A Pain in the Neck for NLP. Lecture
Notes in Computer Science 2276: The 3rd Interna-
tional Conference of Computational Linguistics and
Intelligent Text Processing.
Jakob. H. Havgaard, R. Lyngs, G .D. Stormo and J.
Gorodkin, 2005. Pairwise local structural alignment
of RNA sequences with sequence similarity less than
40 percernt. Bioinfomrmatics. 21(9), 1815-1824.
Joaquim Ferreira da Silva, Gael Dias, Sylvie Guil-
lore, Jose Gabriel Pereira Lopes, 1999. Using Lo-
calMaxs Algorithm for the Extraction of Contigu-
ous and Non-contiguous Multiword Lexical Units.
The 9th Portuguese Conference on Artificial Intelli-
gence.
Jordi Vivaldi, Llułs Marquez, Horacio Rodrłguez,
2001. Improving Term Extraction by System Com-
bination Using Boosting. Lecture Notes in Com-
puter Science 2167: The 12th European Conference
on Machine Learning.
Kalliopi Zervanou and John McNaught, 2000. A Term-
Based Methodology for Template Creation in Infor-
mation Extraction. Lecture Notes in Computer Sci-
ence 1835: Natural Language Processing.
Katerina Frantzi, Sophia Ananiadou, Hideki Mima,
2000. Automatic recognition of multi-word terms:
the C-value/NC-value method. Int J Digit Libr. 3(2),
115C130.
Knut Reinert, Jens Stoye, Torsten Will, 2000. An it-
erative method for faster sum-of-pairs multiple se-
quence alignment. Bioinformatics. 16(9): 808-814.
Makoto Nagao, Shinsuke Mori, 1994. A New Method
of N-gram Statistics for Large Number of n and Au-
tomatic Extraction of Words and Phrases from Large
Text Data of Japanese. The 15th International Con-
ference on Computational Linguistics.
Manning,C.D.,H.,Schutze,1999.Foundations of statis-
tical natural language processing. MIT Press.
Marcus A. Zachariah, Gavin E. Crooks, Stephen R.
Holbrook, Steven E. Brenner, 2005. A Generalized
Affine Gap Model Significantly Improves Protein
Sequence Alignment Accuracy. PROTEINS: Struc-
ture, Function, and Bioinformatics. 58(2), 329 - 338
Michael. Sammeth, B. Morgenstern, and J. Stoye,
2003. Divide-and-conquer multiple alignment with
segment-based constraints. Bioinformatics. 19(2),
189-195.
Mike Paterson, Vlado Dancik ,1994. Longest Common
Subsequences. Mathematical Foundations of Com-
puter Science.
Pascale Fung, Kathleen Mckeown, 1997. A Techni-
cal Word and Term Translation Aid Using Noisy
Parallel Corpora across Language Groups. Machine
Translation. 12, 53C87.
Paul Deane, 2005. A Nonparametric Method for Ex-
traction of Candidate Phrasal Terms. The 43rd An-
nual Meeting of the Association for Computational
Linguistics.
</reference>
<page confidence="0.979755">
181
</page>
<reference confidence="0.999674807692308">
Robertson, A.M. and Willett, P., 1998. Applications of
n-grams in textual information systems. Journal of
Documentation, 54(1), 48-69.
Satanjeev Banerjee, Ted Pedersen, 2003. The Design,
Implementation, and Use of the Ngram Statistics
Package. Lecture Notes in Computer Science 2588:
The 4th International Conference of Computational
Linguistics and Intelligent Text Processing.
Smith, T.F., Waterman, M.S., 1981. Identification of
common molecular subsequences. J. Molecular Bi-
ology. 147(1), 195-197.
Stefan Diaconescu, 2004. Multiword Expression
Translation Using Generative Dependency Gram-
mar. Lecture Notes in Computer Science 3230: Ad-
vances in Natural Language Processing.
Suleiman H. Mustafa, 2004. Character contiguity in
N-gram-based word matching: the case for Arabic
text searching. Information Processing and Manage-
ment. 41(4), 819-827.
Taneli Mielikainen, 2003. Frequency-Based Views to
Pattern Collections. IFIP/SIAM Workshop on Dis-
crete Mathematics and Data Mining.
Violeta Seretan, Luka Nerima, Eric Wehrl, 2003. Ex-
traction of Multi-Word Collocations Using Syntac-
tic Bigram Composition. International Conference
on Recent Advances in NLP.
</reference>
<page confidence="0.998007">
182
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.676069">
<title confidence="0.999902">A Bio-inspired Approach for Multi-Word Expression Extraction</title>
<author confidence="0.8510755">Jianyong Duan</author>
<author confidence="0.8510755">Ruzhan Lu Yan Tian Wu</author>
<author confidence="0.8510755">Yi Hu of Foreign Languages</author>
<affiliation confidence="0.9981825">Department of Computer Science Department of Computer Science Shanghai Jiao Tong University Shanghai Jiao Tong University</affiliation>
<address confidence="0.999801">Shanghai, 200240, P.R. China Shanghai, 200240, P.R. China</address>
<email confidence="0.972504">duanjy@hotmail.comtianyan@sjtu.edu.cn</email>
<abstract confidence="0.999547736842105">This paper proposes a new approach for Multi-word Expression (MWE)extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis. Theory of Longest Common Subsequence (LCS) originates from computer science and has been established as affine gap model in Bioinformatics. We perform this developed LCS technique combined with linguistic criteria in MWE extraction. In comparison with traditional n-gram method, which is the major technique for MWE extraction, LCS approach is applied with great efficiency and performance guarantee. Experimental results show that LCS-based approach achieves better results than n-gram.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arantza Casillas</author>
<author>Raquel Martłnez</author>
</authors>
<title>Aligning Multiword Terms Using a Hybrid Approach.</title>
<date>2002</date>
<booktitle>Lecture Notes in Computer Science 2276: The 3rd International Conference of Computational Linguistics and Intelligent Text Processing.</booktitle>
<marker>Casillas, Martłnez, 2002</marker>
<rawString>Arantza Casillas, Raquel Martłnez , 2002. Aligning Multiword Terms Using a Hybrid Approach. Lecture Notes in Computer Science 2276: The 3rd International Conference of Computational Linguistics and Intelligent Text Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
</authors>
<title>Ido Dagan and Yuval Krymolowski,</title>
<date>1999</date>
<journal>Journal of Experimental and Theoretical AI.</journal>
<volume>11</volume>
<pages>369--390</pages>
<marker>Argamon, 1999</marker>
<rawString>Argamon, Shlomo, Ido Dagan and Yuval Krymolowski, 1999. A memory based approach to learning shallow natural language patterns. Journal of Experimental and Theoretical AI. 11, 369-390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Daille</author>
</authors>
<date>2003</date>
<booktitle>Terminology Mining. Lecture Notes in Computer Science 2700: Extraction in the Web Era.</booktitle>
<marker>Daille, 2003</marker>
<rawString>Beatrice Daille, 2003. Terminology Mining. Lecture Notes in Computer Science 2700: Extraction in the Web Era.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changning Huang</author>
<author>Endong Xun</author>
<author>Zhou Ming</author>
</authors>
<title>A Unified Statistical Model for the Identification of English BaseNP.</title>
<date>2000</date>
<booktitle>The 38th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Huang, Xun, Ming, 2000</marker>
<rawString>Changning Huang, Endong Xun, Zhou Ming, 2000. A Unified Statistical Model for the Identification of English BaseNP. The 38th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel S Hirschberg</author>
</authors>
<title>Algorithms for the Longest Common Subsequence Problem,</title>
<date>1977</date>
<journal>Journal of the ACM,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>664--675</pages>
<marker>Hirschberg, 1977</marker>
<rawString>Daniel S. Hirschberg, 1977. Algorithms for the Longest Common Subsequence Problem, Journal of the ACM, 24(4), 664-675.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Diana Binnenpoorte</author>
</authors>
<title>Catia Cucchiarini, Lou Boves and Helmer Strik,2005. Multiword expressions in spoken language: An exploratory study on pronunciation variation.</title>
<journal>Computer Speech and Language,19(4):433-449</journal>
<marker>Binnenpoorte, </marker>
<rawString>Diana Binnenpoorte, Catia Cucchiarini, Lou Boves and Helmer Strik,2005. Multiword expressions in spoken language: An exploratory study on pronunciation variation. Computer Speech and Language,19(4):433-449</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Peter Lenhof</author>
<author>Burkhard Morgenstern</author>
<author>Knut Reinert</author>
</authors>
<title>An exact solution for the segmentto-segment multiple sequence alignment problem.</title>
<date>1999</date>
<journal>Bioinformatics.</journal>
<volume>15</volume>
<issue>3</issue>
<pages>203--210</pages>
<marker>Lenhof, Morgenstern, Reinert, 1999</marker>
<rawString>Hans Peter Lenhof, Burkhard Morgenstern, Knut Reinert, 1999. An exact solution for the segmentto-segment multiple sequence alignment problem. Bioinformatics. 15(3): 203-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann A Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword Expressions: A Pain in the Neck for NLP.</title>
<date>2002</date>
<booktitle>Lecture Notes in Computer Science 2276: The 3rd International Conference of Computational Linguistics and Intelligent Text Processing.</booktitle>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A. Copestake, Dan Flickinger, 2002. Multiword Expressions: A Pain in the Neck for NLP. Lecture Notes in Computer Science 2276: The 3rd International Conference of Computational Linguistics and Intelligent Text Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Havgaard</author>
<author>R Lyngs</author>
<author>G D Stormo</author>
<author>J Gorodkin</author>
</authors>
<title>Pairwise local structural alignment of RNA sequences with sequence similarity less than 40 percernt.</title>
<date>2005</date>
<journal>Bioinfomrmatics.</journal>
<volume>21</volume>
<issue>9</issue>
<pages>1815--1824</pages>
<marker>Havgaard, Lyngs, Stormo, Gorodkin, 2005</marker>
<rawString>Jakob. H. Havgaard, R. Lyngs, G .D. Stormo and J. Gorodkin, 2005. Pairwise local structural alignment of RNA sequences with sequence similarity less than 40 percernt. Bioinfomrmatics. 21(9), 1815-1824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joaquim Ferreira da Silva</author>
<author>Gael Dias</author>
</authors>
<title>Sylvie Guillore, Jose Gabriel Pereira Lopes,</title>
<date>1999</date>
<booktitle>The 9th Portuguese Conference on Artificial Intelligence.</booktitle>
<marker>Silva, Dias, 1999</marker>
<rawString>Joaquim Ferreira da Silva, Gael Dias, Sylvie Guillore, Jose Gabriel Pereira Lopes, 1999. Using LocalMaxs Algorithm for the Extraction of Contiguous and Non-contiguous Multiword Lexical Units. The 9th Portuguese Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordi Vivaldi</author>
<author>Llułs Marquez</author>
<author>Horacio Rodrłguez</author>
</authors>
<title>Improving Term Extraction by System Combination Using Boosting.</title>
<date>2001</date>
<booktitle>Lecture Notes in Computer Science 2167: The 12th European Conference on Machine Learning.</booktitle>
<marker>Vivaldi, Marquez, Rodrłguez, 2001</marker>
<rawString>Jordi Vivaldi, Llułs Marquez, Horacio Rodrłguez, 2001. Improving Term Extraction by System Combination Using Boosting. Lecture Notes in Computer Science 2167: The 12th European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalliopi Zervanou</author>
<author>John McNaught</author>
</authors>
<title>A TermBased Methodology for Template Creation in Information Extraction.</title>
<date>2000</date>
<booktitle>Lecture Notes in Computer Science 1835: Natural Language Processing.</booktitle>
<marker>Zervanou, McNaught, 2000</marker>
<rawString>Kalliopi Zervanou and John McNaught, 2000. A TermBased Methodology for Template Creation in Information Extraction. Lecture Notes in Computer Science 1835: Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katerina Frantzi</author>
</authors>
<title>Sophia Ananiadou, Hideki Mima,</title>
<date>2000</date>
<journal>Int J Digit Libr.</journal>
<volume>3</volume>
<issue>2</issue>
<pages>115--130</pages>
<marker>Frantzi, 2000</marker>
<rawString>Katerina Frantzi, Sophia Ananiadou, Hideki Mima, 2000. Automatic recognition of multi-word terms: the C-value/NC-value method. Int J Digit Libr. 3(2), 115C130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Knut Reinert</author>
<author>Jens Stoye</author>
<author>Torsten Will</author>
</authors>
<title>An iterative method for faster sum-of-pairs multiple sequence alignment.</title>
<date>2000</date>
<journal>Bioinformatics.</journal>
<volume>16</volume>
<issue>9</issue>
<pages>808--814</pages>
<marker>Reinert, Stoye, Will, 2000</marker>
<rawString>Knut Reinert, Jens Stoye, Torsten Will, 2000. An iterative method for faster sum-of-pairs multiple sequence alignment. Bioinformatics. 16(9): 808-814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Nagao</author>
<author>Shinsuke Mori</author>
</authors>
<title>A New Method of N-gram Statistics for Large Number of n and Automatic Extraction of Words and Phrases from Large Text Data of Japanese.</title>
<date>1994</date>
<booktitle>The 15th International Conference on Computational Linguistics. Manning,C.D.,H.,Schutze,1999.Foundations of</booktitle>
<publisher>MIT Press.</publisher>
<marker>Nagao, Mori, 1994</marker>
<rawString>Makoto Nagao, Shinsuke Mori, 1994. A New Method of N-gram Statistics for Large Number of n and Automatic Extraction of Words and Phrases from Large Text Data of Japanese. The 15th International Conference on Computational Linguistics. Manning,C.D.,H.,Schutze,1999.Foundations of statistical natural language processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus A Zachariah</author>
<author>Gavin E Crooks</author>
<author>Stephen R Holbrook</author>
<author>Steven E Brenner</author>
</authors>
<title>A Generalized Affine Gap Model Significantly Improves Protein Sequence Alignment Accuracy.</title>
<date>2005</date>
<journal>PROTEINS: Structure, Function, and Bioinformatics.</journal>
<volume>58</volume>
<issue>2</issue>
<pages>329--338</pages>
<marker>Zachariah, Crooks, Holbrook, Brenner, 2005</marker>
<rawString>Marcus A. Zachariah, Gavin E. Crooks, Stephen R. Holbrook, Steven E. Brenner, 2005. A Generalized Affine Gap Model Significantly Improves Protein Sequence Alignment Accuracy. PROTEINS: Structure, Function, and Bioinformatics. 58(2), 329 - 338</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Morgenstern Sammeth</author>
<author>J Stoye</author>
</authors>
<title>Divide-and-conquer multiple alignment with segment-based constraints.</title>
<date>2003</date>
<journal>Bioinformatics.</journal>
<volume>19</volume>
<issue>2</issue>
<pages>189--195</pages>
<marker>Sammeth, Stoye, 2003</marker>
<rawString>Michael. Sammeth, B. Morgenstern, and J. Stoye, 2003. Divide-and-conquer multiple alignment with segment-based constraints. Bioinformatics. 19(2), 189-195.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mike Paterson</author>
</authors>
<title>Vlado Dancik ,1994. Longest Common Subsequences.</title>
<journal>Mathematical Foundations of Computer Science.</journal>
<marker>Paterson, </marker>
<rawString>Mike Paterson, Vlado Dancik ,1994. Longest Common Subsequences. Mathematical Foundations of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Kathleen Mckeown</author>
</authors>
<title>A Technical Word and Term Translation Aid Using Noisy Parallel Corpora across Language Groups. Machine Translation.</title>
<date>1997</date>
<volume>12</volume>
<pages>53--87</pages>
<marker>Fung, Mckeown, 1997</marker>
<rawString>Pascale Fung, Kathleen Mckeown, 1997. A Technical Word and Term Translation Aid Using Noisy Parallel Corpora across Language Groups. Machine Translation. 12, 53C87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Deane</author>
</authors>
<title>A Nonparametric Method for Extraction of Candidate Phrasal Terms. The 43rd Annual Meeting of the Association for Computational Linguistics.</title>
<date>2005</date>
<marker>Deane, 2005</marker>
<rawString>Paul Deane, 2005. A Nonparametric Method for Extraction of Candidate Phrasal Terms. The 43rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Robertson</author>
<author>P Willett</author>
</authors>
<title>Applications of n-grams in textual information systems.</title>
<date>1998</date>
<journal>Journal of Documentation,</journal>
<volume>54</volume>
<issue>1</issue>
<pages>48--69</pages>
<marker>Robertson, Willett, 1998</marker>
<rawString>Robertson, A.M. and Willett, P., 1998. Applications of n-grams in textual information systems. Journal of Documentation, 54(1), 48-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>The Design, Implementation, and Use of the Ngram Statistics Package.</title>
<date>2003</date>
<booktitle>Lecture Notes in Computer Science 2588: The 4th International Conference of Computational Linguistics and Intelligent Text Processing.</booktitle>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee, Ted Pedersen, 2003. The Design, Implementation, and Use of the Ngram Statistics Package. Lecture Notes in Computer Science 2588: The 4th International Conference of Computational Linguistics and Intelligent Text Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T F Smith</author>
<author>M S Waterman</author>
</authors>
<title>Identification of common molecular subsequences.</title>
<date>1981</date>
<journal>J. Molecular Biology.</journal>
<volume>147</volume>
<issue>1</issue>
<pages>195--197</pages>
<marker>Smith, Waterman, 1981</marker>
<rawString>Smith, T.F., Waterman, M.S., 1981. Identification of common molecular subsequences. J. Molecular Biology. 147(1), 195-197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Diaconescu</author>
</authors>
<title>Multiword Expression Translation Using Generative Dependency Grammar.</title>
<date>2004</date>
<booktitle>Lecture Notes in Computer Science 3230: Advances in Natural Language Processing.</booktitle>
<marker>Diaconescu, 2004</marker>
<rawString>Stefan Diaconescu, 2004. Multiword Expression Translation Using Generative Dependency Grammar. Lecture Notes in Computer Science 3230: Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suleiman H Mustafa</author>
</authors>
<title>Character contiguity in N-gram-based word matching: the case for Arabic text searching.</title>
<date>2004</date>
<booktitle>Information Processing and Management.</booktitle>
<volume>41</volume>
<issue>4</issue>
<pages>819--827</pages>
<marker>Mustafa, 2004</marker>
<rawString>Suleiman H. Mustafa, 2004. Character contiguity in N-gram-based word matching: the case for Arabic text searching. Information Processing and Management. 41(4), 819-827.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taneli Mielikainen</author>
</authors>
<title>Frequency-Based Views to Pattern Collections.</title>
<date>2003</date>
<booktitle>IFIP/SIAM Workshop on Discrete Mathematics and Data Mining.</booktitle>
<marker>Mielikainen, 2003</marker>
<rawString>Taneli Mielikainen, 2003. Frequency-Based Views to Pattern Collections. IFIP/SIAM Workshop on Discrete Mathematics and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Violeta Seretan</author>
</authors>
<title>Luka Nerima, Eric Wehrl,</title>
<date>2003</date>
<booktitle>Composition. International Conference on Recent Advances in NLP.</booktitle>
<marker>Seretan, 2003</marker>
<rawString>Violeta Seretan, Luka Nerima, Eric Wehrl, 2003. Extraction of Multi-Word Collocations Using Syntactic Bigram Composition. International Conference on Recent Advances in NLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>