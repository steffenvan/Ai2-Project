<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000052">
<title confidence="0.818494">
Specifying Viewpoint and Information Need with Affective Metaphors
A System Demonstration of the Metaphor Magnet Web App/Service
</title>
<author confidence="0.999427">
Tony Veale Guofu Li
</author>
<affiliation confidence="0.993437">
Web Science and Technology Division, School of Computer Science &amp; Informatics,
KAIST, Daejeon, University College Dublin,
</affiliation>
<address confidence="0.943057">
South Korea. Belfield, Dublin D4, Ireland.
</address>
<email confidence="0.993628">
Tony.Veale@gmail.com Yanfen.Hao@UCD.ie
</email>
<sectionHeader confidence="0.99652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999761">
Metaphors pervade our language because
they are elastic enough to allow a speaker
to express an affective viewpoint on a topic
without committing to a specific meaning.
This balance of expressiveness and inde-
terminism means that metaphors are just as
useful for eliciting information as they are
for conveying information. We explore
here, via a demonstration of a system for
metaphor interpretation and generation
called Metaphor Magnet, the practical uses
of metaphor as a basis for formulating af-
fective information queries. We also con-
sider the kinds of deep and shallow
stereotypical knowledge that are needed for
such a system, and demonstrate how they
can be acquired from corpora and the web.
</bodyText>
<sectionHeader confidence="0.998877" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993002">
Metaphor is perhaps the most flexible and adaptive
tool in the human communication toolbox. It is
suited to any domain of discourse, to any register,
and to the description of any concept we desire.
Speakers use metaphor to communicate not just
meanings, but their feelings about those meanings.
The open-ended nature of metaphor interpretation
means that we can use metaphor to simultaneously
express and elicit opinions about a given topic.
Metaphors are flexible conceits that allow us to
express a position while seeking elaboration or
refutation of this position from others. A metaphor
is neither true or false, but a conceptual model that
allow speakers to negotiate a common viewpoint.
Computational models for the interpretation and
elaboration of metaphors should allow speakers to
exploit the same flexibility of expression with ma-
chines as they enjoy with other humans. Such a
goal clearly requires a great deal of knowledge,
since metaphor is a knowledge-hungry mechanism
par excellance (see Fass, 1997). However, much of
the knowledge required for metaphor interpretation
is already implicit in the large body of metaphors
that are active in a community (see Martin, 1990;
Mason, 2004). Existing metaphors are themselves
a valuable source of knowledge for the production
of new metaphors, so much so that a system can
mine the relevant knowledge from corpora of fig-
urative text (e.g. see Veale, 2011; Shutova, 2010).
One area of human-machine interaction that can
clearly benefit from a competence in metaphor is
that of information retrieval (IR). Speakers use
metaphors with ease when eliciting information
from each other, as e.g. when one suggests that a
certain CEO is a tyrant or a god, or that a certain
company is a dinosaur while another is a cult.
Those that agree might respond by elaborating the
metaphor and providing substantiating evidence,
while those that disagree might refute the metaphor
and switch to another of their own choosing. A
well-chosen metaphor can provide the talking
points for an informed conversation, allowing a
speaker to elicit the desired knowledge as a combi-
nation of objective and subjective elements.
In IR, such a capability should allow searchers
to express their information needs subjectively, via
affective metaphors like “X is a cult”. The goal, of
course, is not just to retrieve documents that make
explicit use of the same metaphor – a literal match-
ing of non-literal texts is of limited use – but to
</bodyText>
<page confidence="0.992418">
7
</page>
<note confidence="0.6825135">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 7–12,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999985394736843">
retrieve texts whose own metaphors are consonant
with those of the searcher, and which elaborate
upon the same talking points. This requires a com-
puter to understand the user’s metaphor, to appre-
ciate how other metaphors might convey the same
affective viewpoint, and to understand the different
guises these metaphors might assume in a text.
IR extends the reach of its retrieval efforts by
expanding the query it is given, in an attempt to
make explicit what the user has left implicit. Meta-
phors, like under-specified queries, have rich
meanings that are, for the most part, implicit: they
imply and suggest much more than they specify.
An expansionist approach to metaphor meaning, in
which an affective metaphor is interpreted by gen-
erating the space of related metaphors and talking
points that it implies, is thus very much suited to a
more creative vision of IR, as e.g. suggested by
Veale (2011). To expand a metaphorical query
(like “company-X is a cult” or “company-Y is a
dinosaur” or “Z was a tyrant”), a system must first
expand the metaphor itself, into a set of plausible
construals of the metaphor (e.g. a company that is
viewed as a dinosaur will likely be powerful, but
also bloated, lumbering and slow).
The system described in this paper, Metaphor
Magnet, demonstrates this expansionist approach
to metaphorical inference. Users express queries in
the form of affective metaphors or similes, perhaps
using explicit + or – tags to denote a positive or
negative spin on a given concept. For instance,
“Google is as –powerful as Microsoft” does not
look for documents that literally contain this simi-
le, but documents that express viewpoints that are
implied by this simile, that is, documents that dis-
cuss the negative implications of Google’s power,
where these implications are first understood in
relation to Microsoft. The system does this by first
considering the metaphors that are conventionally
used to describe Microsoft, focusing only on those
metaphors that evoke the property powerful, and
which cast a negative light on Microsoft. The im-
plications of these metaphors (e.g., dinosaur, bully,
monopoly, etc.) are then examined in the context of
Google, using the metaphors that are typically used
to describe Google as a guide to what is most apt.
Thus, since Google is often described as a giant in
web texts, the negative properties and behaviors of
a stereotypical giant – like lumbering and sprawl-
ing – will be considered apt and highlighted.
To perform this kind of analysis reliably, for a
wide range of metaphors and an even wider range
of topics, requires a robustly shallow approach.
We exploit the fact that the Google n-grams
(Brants and Franz, 2006) contains a great many
copula metaphors of the form “X is a Y” to under-
stand how X is typically viewed on the web. We
further exploit a large dictionary of affective stere-
otypes to provide an understanding of the +/- prop-
erties and behaviors of each source concept Y.
Combining these resources allows the Metaphor
Magnet system to understand the implications of a
metaphorical query “X as Z” in terms of the quali-
ties that are typically considered salient for Z and
which have been corpus-attested as apt for X.
We describe the construction of our lexicon of
affective stereotypes in section 2. Each stereotype
is associated with a set of typical properties and
behaviors (like sprawling for giant, or inspiring for
guru), where the overall affect of each stereotype
depends on which subset of qualities is activated in
a given context (e.g., giant can be construed posi-
tively or negatively, as can baby, soldier, etc.). We
describe how Metaphor Magnet exploits these ste-
reotypes in section 3, before providing a worked
example in section 4 and screenshots in section 5.
</bodyText>
<sectionHeader confidence="0.636611" genericHeader="method">
2 An Affective Lexicon of Stereotypes
</sectionHeader>
<bodyText confidence="0.999965956521739">
We construct the lexicon in two stages. In the first
stage, a large collection of stereotypical descrip-
tions is harvested from the Web. As in Liu et al.
(2003), our goal is to acquire a lightweight com-
mon-sense representation of many everyday con-
cepts. In the second stage, we link these common-
sense qualities in a support graph that captures
how they mutually support each other in their co-
description of a stereotypical idea. From this graph
we can estimate positive and negative valence
scores for each property and behavior, and default
averages for the stereotypes that exhibit them.
Similes and stereotypes share a symbiotic rela-
tionship: the former exploit the latter as reference
points for an evocative description, while the latter
are perpetuated by their constant re-use in similes.
Expanding on the approach in Veale (2011), we
use two kinds of query for harvesting stereotypes
from the web. The first, “as ADJ as a NOUN”, ac-
quires typical adjectival properties for noun con-
cepts; the second, “VERB+ing like a NOUN” and
“VERB+ed like a NOUN”, acquires typical verb
behaviors. Rather than use a wildcard * in both
</bodyText>
<page confidence="0.987924">
8
</page>
<bodyText confidence="0.998970131147542">
positions (ADJ and NOUN, or VERB and NOUN),
which yields limited results with a search engine
like Google, we generate fully instantiated similes
from hypotheses generated via the Google n-
grams. Thus, from the 3-gram “a drooling zombie”
we generate the query “drooling like a zombie”,
and from the 3-gram “a mindless zombie” we gen-
erate “as mindless as a zombie”.
Only those similes whose queries retrieve one
or more web documents via Google are considered
to contain promising associations. But this still
gives us over 250,000 web-validated simile associ-
ations for our stereotypical model. We quickly fil-
ter these candidates manually, to ensure that the
contents of the lexicon are of the highest quality.
As a result, we obtain rich descriptions for many
stereotypical ideas, such as Baby, which is de-
scribed via 163 typical properties and behaviors
like crying, drooling and guileless. After this filter-
ing phase, the stereotype lexicon maps 9,479 stere-
otypes to a set of 7,898 properties and behaviors,
to yield more than 75,000 pairings.
We construct the second level of the lexicon by
automatically linking these properties and behav-
iors to each other in a support graph. The intuition
here is that properties which reinforce each other in
a single description (e.g. “as lush and green as a
jungle” or “as hot and humid as a sauna”) are more
likely to have a similar affect than properties which
do not support each other. We first gather all
Google 3-grams in which a pair of stereotypical
properties or behaviors X and Y are linked via co-
ordination, as in “hot and humid” or “kicking and
screaming”. A bidirectional link between X and Y
is added to the support graph if one or more stereo-
types in the lexicon contain both X and Y. If this is
not so, we consider whether both descriptors ever
reinforce each other in web similes, by posing the
web query “as X and Y as”. If this query has non-
zero hits, we also add a link between X and Y.
Let N denote this support graph, and N(p) de-
note the set of neighboring terms to p, that is, the
set of properties and behaviors that can mutually
support p. Since every edge in N represents an af-
fective context, we can estimate the likelihood that
a property p is ever used in a positive or negative
context if we know the positive or negative affect
of enough members of N(p). So if we label enough
vertices of N as + or -, we can interpolate a posi-
tive/negative valence score for all vertices p in N.
To do this, we build a reference set -R of typi-
cally negative words, and a set +R of typically
positive words. Given a few seed members of -R
(such as sad, disgusting, evil, etc.) and a few seed
members of +R (such as happy, wonderful, etc.),
we find many other candidates to add to +R and -R
by considering neighbors of these seeds in N. After
three iterations in this fashion, we populate +R and
-R with approx. 2000 words each.
For a property p we can now define N+(p) and
N-(p) as follows:
</bodyText>
<listItem confidence="0.9190575">
(1) N+(p) = N(p) n +R
(2) N-(p) = N(p) n -R
</listItem>
<bodyText confidence="0.805821">
We can now assign positive and negative valence
scores to each vertex p by interpolating from ref-
erence values to their neighbors in N:
</bodyText>
<figure confidence="0.9441148">
(3) pos(p) = |N+(p)|
|N+(p) U N-(p)|
(4) neg(p) = 1 - pos(p)
If a term S denotes a stereotypical idea and is de-
scribed via a set of typical properties and behaviors
typical(S) in the lexicon, then:
(5) pos(S) = pos(p)
IpEtypical(S)
|typical(S)|
(6) neg(S) = 1 - pos(S)
</figure>
<bodyText confidence="0.650505428571429">
Thus, (5) and (6) calculate the mean affect of the
properties and behaviors of S, as represented via
typical(S). We can now use (3) and (4) to separate
typical(S) into those elements that are more nega-
tive than positive (putting a negative spin on S) and
into those that are more positive than negative
(putting a positive spin on S):
</bodyText>
<listItem confidence="0.945604">
(7) posTypical(S) = {p  |p E typical(S) n pos(p) &gt; 0.5}
(8) negTypical(S) = {p  |p E typical(S) n neg(p) &gt; 0.5}
</listItem>
<subsectionHeader confidence="0.999643">
2.1 Evaluation of Stereotypical Affect
</subsectionHeader>
<bodyText confidence="0.994427333333333">
In the process of populating +R and -R, we identi-
fy a reference set of 478 positive stereotypes (such
as saint and hero) and 677 negative stereotypes
(such as tyrant and monster). When we use these
reference points to test the effectiveness of (5) and
(6) – and thus, indirectly, of (3) and (4) and of the
</bodyText>
<page confidence="0.995112">
9
</page>
<bodyText confidence="0.999926333333333">
stereotype lexicon itself – we find that 96.7% of
the positive stereotypes in +R are correctly as-
signed a positivity score greater than 0.5 (pos(S) &gt;
neg(S)) by (5), while 96.2% of the negative stereo-
types in -R are correctly assigned a negativity
score greater than 0.5 (neg(S) &gt; pos(S)) by (6).
</bodyText>
<sectionHeader confidence="0.78071" genericHeader="method">
3 Expansion/Interpretation of Metaphors
</sectionHeader>
<bodyText confidence="0.999631181818182">
The Google n-grams are a rich source of affective
metaphors of the form Target is Source, such as
“politicians are crooks”, “Apple is a cult”, “racism
is a disease” and “Steve Jobs is a god”. Let src(T)
denote the set of stereotypes that are commonly
used to describe T, where commonality is defined
as the presence of the corresponding copula meta-
phor in the Google n-grams. To find metaphors for
proper-named entities like “Bill Gates”, we also
analyze n-grams of the form stereotype First
[Middle] Last, such as “tyrant Adolf Hitler”. Thus:
</bodyText>
<equation confidence="0.991319">
src(racism) = {problem, disease, joke, sin, poi-
son, crime, ideology, weapon}
src(Hitler) = {monster, criminal, tyrant, idiot,
madman, vegetarian, racist, ...}
</equation>
<bodyText confidence="0.999969833333333">
We do not try to discriminate literal from non-
literal assertions, nor do we even try to define liter-
ality. We simply assume each putative metaphor
offers a potentially useful perspective on a topic T.
Let srcTypical(T) denote the aggregation of all
properties ascribable to T via metaphors in src(T):
</bodyText>
<equation confidence="0.947401">
U
(9) srcTypical (T) = MEsrc(T)
</equation>
<bodyText confidence="0.999499181818182">
We can also use the posTypical and negTypical
variants in (7) and (8) to focus only on metaphors
that project positive or negative qualities onto T.
(9) is especially useful when the source S in the
metaphor T is S is not a known stereotype in the
lexicon, as happens when one describes Apple as
Scientology. When the set typical(S) is empty, src-
Typical(S) may not be, so srcTypical(S) can act as
a proxy representation for S in these cases.
The properties and behaviors that are salient to
the interpretation of T is S are given by:
</bodyText>
<listItem confidence="0.500086666666667">
(10) salient (T,S) = |srcTypical(T) U typical(T)|
n
|srcTypical(S) U typical(S)|
</listItem>
<bodyText confidence="0.736645">
In the context of T is S, the metaphorical stereotype
</bodyText>
<equation confidence="0.8953005">
M E src(S)Usrc(T)U{S} is an apt vehicle for T if:
(11) apt(M, T,S) = |salient(T,S) n typical(M) |&gt; 0
</equation>
<bodyText confidence="0.938714">
and the degree to which M is apt for T is given by:
</bodyText>
<equation confidence="0.906259">
(12) aptness(M,T,S) = |salient(T, S) n typical(M)|
|typical(M)|
</equation>
<bodyText confidence="0.9806682">
We can construct an interpretation for T is S by
considering not just {S}, but the stereotypes in
src(T) that are apt for T in the context of T is S, as
well as the stereotypes that are commonly used to
describe S – that is, src(S) – that are also apt for T:
</bodyText>
<equation confidence="0.8213645">
(13) interpretation(T, S)
= {M|M E src(T)Usrc(S)U{S} n apt(M, T, S)}
</equation>
<bodyText confidence="0.999715833333333">
In effect then, the interpretation of T is S is itself a
set of apt metaphors for T that expand upon S. The
elements {Mi} of interpretation(T, S) can now be
sorted by aptness(Mi T, S) to produce a ranked list
of interpretations (M1, M2 É Mn). For any inter-
pretation M, the salient features of M are thus:
</bodyText>
<equation confidence="0.912372">
(14) salient(M, T,S) = typical(M) n salient (T,S)
</equation>
<bodyText confidence="0.98376">
If T is S is a creative IR query – to find docu-
ments that view T as S – then interpretation(T, S)
is an expansion of T is S that includes the com-
mon metaphors that are consistent with T viewed
as S. For any viewpoint Mi, salient(Mi, T, S) is an
expansion of Mi that includes all of the qualities
that T is likely to exhibit when it behaves like Mi.
</bodyText>
<sectionHeader confidence="0.991344" genericHeader="method">
4 Metaphor Magnet: A Worked Example
</sectionHeader>
<bodyText confidence="0.999920923076923">
Consider the query “Google is Microsoft”, which
expresses a need for documents in which Google
exhibits qualities typically associated with Mi-
crosoft. Now, both Google and Microsoft are com-
plex concepts, so there are many ways in which
they can be considered similar or dissimilar, either
in a good or a bad light. However, the most salient
aspects of Microsoft will be those that underpin
our common metaphors for Microsoft, i.e., stereo-
types in src(Microsoft). These metaphors will pro-
vide the talking points for the interpretation.
The Google n-grams yield up the following
metaphors, 57 for Microsoft and 50 for Google:
</bodyText>
<equation confidence="0.822090615384615">
src(Microsoft) = {king, master, threat, bully, giant,
leader, monopoly, dinosaur ...}
typical(M)
10
src(Google) = {king, engine, threat, brand, giant,
leader, celebrity, religion ...}
So the following qualities are aggregated for each:
srcTypical(Microsoft) = {trusted, menacing, ruling,
threatening, overbearing,
admired, commanding, ...}
srcTypical(Google) = {trusted, lurking reigning,
ruling, crowned, shining,
determined, admired ...)
</equation>
<bodyText confidence="0.973575">
Now, the salient qualities highlighted by the meta-
phor, namely salient(Google, Microsoft), are:
{celebrated, menacing, trusted, challenging, estab-
lished, threatening, admired, respected, ...}
Thus, interpretation(Google, Microsoft) contains:
{king, criminal, master, leader, bully, threatening,
giant, threat, monopoly, pioneer, dinosaur, ...)
Suppose we focus on the metaphorical expansion
“Google is king”, since king is the most highly
ranked element of the interpretation. Now, sali-
ent(king, Google, Microsoft) contains:
{celebrated, revered, admired, respected, ruling,
arrogant, commanding, overbearing, reigning, ...}
These properties and behaviors are already implicit
in our perception of Google, insofar as they are
salient aspects of the stereotypes to which Google
is frequently compared. The metaphor “Google is
Microsoft” – and its expansion “Google is king” –
simply crystalizes these qualities, from perhaps
different comparisons, into a single act of ideation.
Consider the metaphor “Google is -Microsoft”.
Since -Microsoft is used to impart a negative spin
(+ would impart a positive spin), negTypical is
here used in place of typical in (9) and (10). Thus:
</bodyText>
<equation confidence="0.906612">
srcTypical(-Microsoft) =
</equation>
<construct confidence="0.496029857142857">
{menacing, threatening, twisted, raging, feared,
sinister, lurking, domineering, overbearing, ...}
salient(Google, -Microsoft) =
{menacing, bullying, roaring, dreaded...}
Now interpretation(Google, -Microsoft) becomes:
{criminal, giant, threat, bully, victim, devil, ...)
In contrast, interpretation(Google, +Microsoft) is:
</construct>
<bodyText confidence="0.9097646">
{king, master, leader, pioneer, partner, ...)
More focus is achieved with the simile query
“Google is as –powerful as Microsoft”. In explicit
similes, we need to focus on just a subset of the
salient properties, using e.g. this variant of (10):
</bodyText>
<equation confidence="0.665337">
{p  |p ∈ salient(Google, Microsoft) ∩ N(powerful)
∧ neg(p) &gt; pos(p)}
</equation>
<bodyText confidence="0.9976655">
In this -powerful case, the interpretation becomes:
{bully, giant, devil, monopoly, dinosaur, ...)
</bodyText>
<sectionHeader confidence="0.98667" genericHeader="method">
5 The Metaphor Magnet Web App
</sectionHeader>
<bodyText confidence="0.99895572972973">
Metaphor Magnet is designed to be a lightweight
web application that provides both HTML output
(for humans) and XML (for client applications).
The system allows users to enter queries such as
Google is –Microsoft, life is a +game, Steve Jobs is
Tony Stark, or even Rasputin is Karl Rove (queries
are case-sensitive). Each query is expanded into a
set of apt metaphors via mappings in the Google n-
grams, and each metaphor is expanded into a set of
contextually apt qualities. In turn, each quality is
then expanded into an IR query that is used to re-
trieve relevant hits from Google. In effect, the sys-
tem allows users to interface with a search engine
like Google using metaphor and other affective
language forms. The demonstration system can be
accessed using a standard browser at this URL:
http://boundinanutshell.com/metaphor-magnet
Metaphor Magnet can exploit the properties and
behaviors of its stock of almost 10,000 stereotypes,
and can infer salient qualities for many proper-
named entities like Karl Rove and Steve Jobs using
a combination of copula statements from the
Google n-grams (e.g., “Steve Jobs is a visionary”)
and category assignments from Wikipedia.
The interpretation of the simile/query “Google is
as -powerful as Microsoft” thus highlights a selec-
tion of affective viewpoints on the source concept,
Microsoft, and picks out an apt selection of view-
points on the target Google. Metaphor Magnet dis-
plays both selections as phrase clouds in which
each hyperlinked phrase – a combination of an apt
stereotype and a salient quality – is clickable, to
yield linguistic evidence for the selection and cor-
responding web-search results (via a Google gadg-
et). The phrase cloud representing Microsoft in this
simile is shown in the screenshot of Figure 1, while
the phrase cloud for Google is shown in Figure 2.
</bodyText>
<page confidence="0.998752">
11
</page>
<figureCaption confidence="0.801897">
Figure 1. A screenshot of a phrase cloud for the
perspective cast upon the source “Microsoft” by
the simile “Google is as –powerful as Microsoft”.
</figureCaption>
<bodyText confidence="0.999970777777778">
Metaphor Magnet demonstrates the potential utili-
ty of affective metaphors in human-computer lin-
guistic interaction, and acts as a web service from
which other NL applications can derive a measure
of metaphorical competence. When accessed as a
service, Metaphor Magnet returns either HTML or
XML data, via simple get requests. For illustrative
purposes, each HTML page also provides the URL
for the corresponding XML-structured data set.
</bodyText>
<sectionHeader confidence="0.996526" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998206142857143">
This research was partly supported by the WCU
(World Class University) program under the Na-
tional Research Foundation of Korea (Ministry of
Education, Science and Technology of Korea, Pro-
ject No: R31-30007), and partly funded by Science
Foundation Ireland via the Centre for Next Genera-
tion Localization (CNGL).
</bodyText>
<sectionHeader confidence="0.998177" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.871663">
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium.
</reference>
<figureCaption confidence="0.8718">
Figure 2. A screenshot of a phrase cloud for the
perspective cast upon the target term “Google” by
the simile “Google is as –powerful as Microsoft”.
</figureCaption>
<reference confidence="0.999480952380952">
Dan Fass. 1997. Processing Metonymy and Metaphor.
Contemporary Studies in Cognitive Science &amp; Tech-
nology. New York: Ablex.
Hugo Liu, Henry Lieberman and Ted Selker. 2003. A
Model of Textual Affect Sensing Using Real-World
Knowledge. Proc. of the 8th international conference
on Intelligent user interfaces, 125-132.
James H. Martin. 1990. A Computational Model of
Metaphor Interpretation. NY: Academic Press.
Zachary J. Mason. 2004. CorMet: A Computational,
Corpus-Based Conventional Metaphor Extraction
System, Computational Linguistics, 30(1):23-44.
Ekaterina Shutova. 2010. Metaphor Identification Using
Verb and Noun Clustering. In Proc. of the 23rd Inter-
national Conference on Computational Linguistics,
1001-1010
Tony Veale. 2011. Creative Language Retrieval. Crea-
tive Language Retrieval: A Robust Hybrid of Infor-
mation Retrieval and Linguistic Creativity. In Proc.
of ACL’2011, the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics.
</reference>
<page confidence="0.998461">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.474477">
<title confidence="0.993898">Specifying Viewpoint and Information Need with Affective Metaphors System Demonstration of the Magnet App/Service</title>
<author confidence="0.999808">Tony Veale Guofu Li</author>
<affiliation confidence="0.9808525">Web Science and Technology Division, School of Computer Science &amp; Informatics, KAIST, Daejeon, University College Dublin,</affiliation>
<address confidence="0.988393">South Korea. Belfield, Dublin D4, Ireland.</address>
<email confidence="0.511343">Tony.Veale@gmail.comYanfen.Hao@UCD.ie</email>
<abstract confidence="0.998862777777778">Metaphors pervade our language because they are elastic enough to allow a speaker to express an affective viewpoint on a topic without committing to a specific meaning. This balance of expressiveness and indeterminism means that metaphors are just as useful for eliciting information as they are for conveying information. We explore here, via a demonstration of a system for metaphor interpretation and generation the practical uses of metaphor as a basis for formulating affective information queries. We also consider the kinds of deep and shallow stereotypical knowledge that are needed for such a system, and demonstrate how they can be acquired from corpora and the web.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram Version 1. Linguistic Data Consortium.</title>
<date>2006</date>
<contexts>
<context position="6428" citStr="Brants and Franz, 2006" startWordPosition="1026" endWordPosition="1029">mplications of these metaphors (e.g., dinosaur, bully, monopoly, etc.) are then examined in the context of Google, using the metaphors that are typically used to describe Google as a guide to what is most apt. Thus, since Google is often described as a giant in web texts, the negative properties and behaviors of a stereotypical giant – like lumbering and sprawling – will be considered apt and highlighted. To perform this kind of analysis reliably, for a wide range of metaphors and an even wider range of topics, requires a robustly shallow approach. We exploit the fact that the Google n-grams (Brants and Franz, 2006) contains a great many copula metaphors of the form “X is a Y” to understand how X is typically viewed on the web. We further exploit a large dictionary of affective stereotypes to provide an understanding of the +/- properties and behaviors of each source concept Y. Combining these resources allows the Metaphor Magnet system to understand the implications of a metaphorical query “X as Z” in terms of the qualities that are typically considered salient for Z and which have been corpus-attested as apt for X. We describe the construction of our lexicon of affective stereotypes in section 2. Each </context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Fass</author>
</authors>
<date>1997</date>
<booktitle>Processing Metonymy and Metaphor. Contemporary Studies in Cognitive Science &amp; Technology.</booktitle>
<publisher>Ablex.</publisher>
<location>New York:</location>
<contexts>
<context position="2094" citStr="Fass, 1997" startWordPosition="317" endWordPosition="318"> elicit opinions about a given topic. Metaphors are flexible conceits that allow us to express a position while seeking elaboration or refutation of this position from others. A metaphor is neither true or false, but a conceptual model that allow speakers to negotiate a common viewpoint. Computational models for the interpretation and elaboration of metaphors should allow speakers to exploit the same flexibility of expression with machines as they enjoy with other humans. Such a goal clearly requires a great deal of knowledge, since metaphor is a knowledge-hungry mechanism par excellance (see Fass, 1997). However, much of the knowledge required for metaphor interpretation is already implicit in the large body of metaphors that are active in a community (see Martin, 1990; Mason, 2004). Existing metaphors are themselves a valuable source of knowledge for the production of new metaphors, so much so that a system can mine the relevant knowledge from corpora of figurative text (e.g. see Veale, 2011; Shutova, 2010). One area of human-machine interaction that can clearly benefit from a competence in metaphor is that of information retrieval (IR). Speakers use metaphors with ease when eliciting infor</context>
</contexts>
<marker>Fass, 1997</marker>
<rawString>Dan Fass. 1997. Processing Metonymy and Metaphor. Contemporary Studies in Cognitive Science &amp; Technology. New York: Ablex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Liu</author>
<author>Henry Lieberman</author>
<author>Ted Selker</author>
</authors>
<title>A Model of Textual Affect Sensing Using Real-World Knowledge.</title>
<date>2003</date>
<booktitle>Proc. of the 8th international conference on Intelligent user interfaces,</booktitle>
<pages>125--132</pages>
<contexts>
<context position="7696" citStr="Liu et al. (2003)" startWordPosition="1243" endWordPosition="1246">erties and behaviors (like sprawling for giant, or inspiring for guru), where the overall affect of each stereotype depends on which subset of qualities is activated in a given context (e.g., giant can be construed positively or negatively, as can baby, soldier, etc.). We describe how Metaphor Magnet exploits these stereotypes in section 3, before providing a worked example in section 4 and screenshots in section 5. 2 An Affective Lexicon of Stereotypes We construct the lexicon in two stages. In the first stage, a large collection of stereotypical descriptions is harvested from the Web. As in Liu et al. (2003), our goal is to acquire a lightweight common-sense representation of many everyday concepts. In the second stage, we link these commonsense qualities in a support graph that captures how they mutually support each other in their codescription of a stereotypical idea. From this graph we can estimate positive and negative valence scores for each property and behavior, and default averages for the stereotypes that exhibit them. Similes and stereotypes share a symbiotic relationship: the former exploit the latter as reference points for an evocative description, while the latter are perpetuated b</context>
</contexts>
<marker>Liu, Lieberman, Selker, 2003</marker>
<rawString>Hugo Liu, Henry Lieberman and Ted Selker. 2003. A Model of Textual Affect Sensing Using Real-World Knowledge. Proc. of the 8th international conference on Intelligent user interfaces, 125-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James H Martin</author>
</authors>
<title>A Computational Model of Metaphor Interpretation.</title>
<date>1990</date>
<publisher>NY: Academic Press.</publisher>
<contexts>
<context position="2263" citStr="Martin, 1990" startWordPosition="344" endWordPosition="345">others. A metaphor is neither true or false, but a conceptual model that allow speakers to negotiate a common viewpoint. Computational models for the interpretation and elaboration of metaphors should allow speakers to exploit the same flexibility of expression with machines as they enjoy with other humans. Such a goal clearly requires a great deal of knowledge, since metaphor is a knowledge-hungry mechanism par excellance (see Fass, 1997). However, much of the knowledge required for metaphor interpretation is already implicit in the large body of metaphors that are active in a community (see Martin, 1990; Mason, 2004). Existing metaphors are themselves a valuable source of knowledge for the production of new metaphors, so much so that a system can mine the relevant knowledge from corpora of figurative text (e.g. see Veale, 2011; Shutova, 2010). One area of human-machine interaction that can clearly benefit from a competence in metaphor is that of information retrieval (IR). Speakers use metaphors with ease when eliciting information from each other, as e.g. when one suggests that a certain CEO is a tyrant or a god, or that a certain company is a dinosaur while another is a cult. Those that ag</context>
</contexts>
<marker>Martin, 1990</marker>
<rawString>James H. Martin. 1990. A Computational Model of Metaphor Interpretation. NY: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zachary J Mason</author>
</authors>
<date>2004</date>
<journal>CorMet: A Computational, Corpus-Based Conventional Metaphor Extraction System, Computational Linguistics,</journal>
<pages>30--1</pages>
<contexts>
<context position="2277" citStr="Mason, 2004" startWordPosition="346" endWordPosition="347">phor is neither true or false, but a conceptual model that allow speakers to negotiate a common viewpoint. Computational models for the interpretation and elaboration of metaphors should allow speakers to exploit the same flexibility of expression with machines as they enjoy with other humans. Such a goal clearly requires a great deal of knowledge, since metaphor is a knowledge-hungry mechanism par excellance (see Fass, 1997). However, much of the knowledge required for metaphor interpretation is already implicit in the large body of metaphors that are active in a community (see Martin, 1990; Mason, 2004). Existing metaphors are themselves a valuable source of knowledge for the production of new metaphors, so much so that a system can mine the relevant knowledge from corpora of figurative text (e.g. see Veale, 2011; Shutova, 2010). One area of human-machine interaction that can clearly benefit from a competence in metaphor is that of information retrieval (IR). Speakers use metaphors with ease when eliciting information from each other, as e.g. when one suggests that a certain CEO is a tyrant or a god, or that a certain company is a dinosaur while another is a cult. Those that agree might resp</context>
</contexts>
<marker>Mason, 2004</marker>
<rawString>Zachary J. Mason. 2004. CorMet: A Computational, Corpus-Based Conventional Metaphor Extraction System, Computational Linguistics, 30(1):23-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
</authors>
<title>Metaphor Identification Using Verb and Noun Clustering.</title>
<date>2010</date>
<booktitle>In Proc. of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1001--1010</pages>
<contexts>
<context position="2507" citStr="Shutova, 2010" startWordPosition="384" endWordPosition="385">ility of expression with machines as they enjoy with other humans. Such a goal clearly requires a great deal of knowledge, since metaphor is a knowledge-hungry mechanism par excellance (see Fass, 1997). However, much of the knowledge required for metaphor interpretation is already implicit in the large body of metaphors that are active in a community (see Martin, 1990; Mason, 2004). Existing metaphors are themselves a valuable source of knowledge for the production of new metaphors, so much so that a system can mine the relevant knowledge from corpora of figurative text (e.g. see Veale, 2011; Shutova, 2010). One area of human-machine interaction that can clearly benefit from a competence in metaphor is that of information retrieval (IR). Speakers use metaphors with ease when eliciting information from each other, as e.g. when one suggests that a certain CEO is a tyrant or a god, or that a certain company is a dinosaur while another is a cult. Those that agree might respond by elaborating the metaphor and providing substantiating evidence, while those that disagree might refute the metaphor and switch to another of their own choosing. A well-chosen metaphor can provide the talking points for an i</context>
</contexts>
<marker>Shutova, 2010</marker>
<rawString>Ekaterina Shutova. 2010. Metaphor Identification Using Verb and Noun Clustering. In Proc. of the 23rd International Conference on Computational Linguistics, 1001-1010</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Veale</author>
</authors>
<title>Creative Language Retrieval. Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity.</title>
<date>2011</date>
<booktitle>In Proc. of ACL’2011, the 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2491" citStr="Veale, 2011" startWordPosition="382" endWordPosition="383">e same flexibility of expression with machines as they enjoy with other humans. Such a goal clearly requires a great deal of knowledge, since metaphor is a knowledge-hungry mechanism par excellance (see Fass, 1997). However, much of the knowledge required for metaphor interpretation is already implicit in the large body of metaphors that are active in a community (see Martin, 1990; Mason, 2004). Existing metaphors are themselves a valuable source of knowledge for the production of new metaphors, so much so that a system can mine the relevant knowledge from corpora of figurative text (e.g. see Veale, 2011; Shutova, 2010). One area of human-machine interaction that can clearly benefit from a competence in metaphor is that of information retrieval (IR). Speakers use metaphors with ease when eliciting information from each other, as e.g. when one suggests that a certain CEO is a tyrant or a god, or that a certain company is a dinosaur while another is a cult. Those that agree might respond by elaborating the metaphor and providing substantiating evidence, while those that disagree might refute the metaphor and switch to another of their own choosing. A well-chosen metaphor can provide the talking</context>
<context position="4641" citStr="Veale (2011)" startWordPosition="734" endWordPosition="735">ent guises these metaphors might assume in a text. IR extends the reach of its retrieval efforts by expanding the query it is given, in an attempt to make explicit what the user has left implicit. Metaphors, like under-specified queries, have rich meanings that are, for the most part, implicit: they imply and suggest much more than they specify. An expansionist approach to metaphor meaning, in which an affective metaphor is interpreted by generating the space of related metaphors and talking points that it implies, is thus very much suited to a more creative vision of IR, as e.g. suggested by Veale (2011). To expand a metaphorical query (like “company-X is a cult” or “company-Y is a dinosaur” or “Z was a tyrant”), a system must first expand the metaphor itself, into a set of plausible construals of the metaphor (e.g. a company that is viewed as a dinosaur will likely be powerful, but also bloated, lumbering and slow). The system described in this paper, Metaphor Magnet, demonstrates this expansionist approach to metaphorical inference. Users express queries in the form of affective metaphors or similes, perhaps using explicit + or – tags to denote a positive or negative spin on a given concept</context>
<context position="8373" citStr="Veale (2011)" startWordPosition="1353" endWordPosition="1354">n of many everyday concepts. In the second stage, we link these commonsense qualities in a support graph that captures how they mutually support each other in their codescription of a stereotypical idea. From this graph we can estimate positive and negative valence scores for each property and behavior, and default averages for the stereotypes that exhibit them. Similes and stereotypes share a symbiotic relationship: the former exploit the latter as reference points for an evocative description, while the latter are perpetuated by their constant re-use in similes. Expanding on the approach in Veale (2011), we use two kinds of query for harvesting stereotypes from the web. The first, “as ADJ as a NOUN”, acquires typical adjectival properties for noun concepts; the second, “VERB+ing like a NOUN” and “VERB+ed like a NOUN”, acquires typical verb behaviors. Rather than use a wildcard * in both 8 positions (ADJ and NOUN, or VERB and NOUN), which yields limited results with a search engine like Google, we generate fully instantiated similes from hypotheses generated via the Google ngrams. Thus, from the 3-gram “a drooling zombie” we generate the query “drooling like a zombie”, and from the 3-gram “a </context>
</contexts>
<marker>Veale, 2011</marker>
<rawString>Tony Veale. 2011. Creative Language Retrieval. Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity. In Proc. of ACL’2011, the 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>