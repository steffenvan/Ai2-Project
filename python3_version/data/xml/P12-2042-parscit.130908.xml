<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018416">
<title confidence="0.988815">
Towards the Unsupervised Acquisition of Discourse Relations
</title>
<author confidence="0.994856">
Christian Chiarcos
</author>
<affiliation confidence="0.972306">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
</affiliation>
<email confidence="0.683892">
chiarcos@daad-alumni.de
</email>
<sectionHeader confidence="0.992107" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998889411764706">
This paper describes a novel approach towards
the empirical approximation of discourse re-
lations between different utterances in texts.
Following the idea that every pair of events
comes with preferences regarding the range
and frequency of discourse relations connect-
ing both parts, the paper investigates whether
these preferences are manifested in the distri-
bution of relation words (that serve to signal
these relations).
Experiments on two large-scale English web
corpora show that significant correlations be-
tween pairs of adjacent events and relation
words exist, that they are reproducible on dif-
ferent data sets, and for three relation words,
that their distribution corresponds to theory-
based assumptions.
</bodyText>
<sectionHeader confidence="0.990012" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999981541666667">
Texts are not merely accumulations of isolated ut-
terances, but the arrangement of utterances conveys
meaning; human text understanding can thus be de-
scribed as a process to recover the global structure
of texts and the relations linking its different parts
(Vallduv´ı 1992; Gernsbacher et al. 2004). To capture
these aspects of meaning in NLP, it is necessary to
develop operationalizable theories, and, within a su-
pervised approach, large amounts of annotated train-
ing data. To facilitate manual annotation, weakly
supervised or unsupervised techniques can be ap-
plied as preprocessing step for semimanual anno-
tation, and this is part of the motivation of the ap-
proach described here.
Discourse relations involve different aspects of
meaning. This may include factual knowledge
about the connected discourse segments (a ‘subject-
matter’ relation, e.g., if one utterance represents
the cause for another, Mann and Thompson 1988,
p.257), argumentative purposes (a ‘presentational’
relation, e.g., one utterance motivates the reader to
accept a claim formulated in another utterance, ibid.,
p.257), or relations between entities mentioned in
the connected discourse segments (anaphoric rela-
tions, Webber et al. 2003). Discourse relations can
be indicated explicitly by optional cues, e.g., ad-
verbials (e.g., however), conjunctions (e.g., but), or
complex phrases (e.g., in contrast to what Peter said
a minute ago). Here, these cues are referred to as
relation words.
Assuming that relation words are associated with
specific discourse relations (Knott and Dale 1994;
Prasad et al. 2008), the distribution of relation words
found between two (types of) events can yield in-
sights into the range of discourse relations possi-
ble at this occasion and their respective likeliness.
For this purpose, this paper proposes a background
knowledge base (BKB) that hosts pairs of events
(here heuristically represented by verbs) along with
distributional profiles for relation words. The pri-
mary data structure of the BKB is a triple where
one event (type) is connected with a particular re-
lation word to another event (type). Triples are fur-
ther augmented with a frequency score (expressing
the likelihood of the triple to be observed), a sig-
nificance score (see below), and a correlation score
(indicating whether a pair of events has a positive or
negative correlation with a particular relation word).
</bodyText>
<page confidence="0.990152">
213
</page>
<note confidence="0.6878685">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 213–217,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.996936111111111">
Triples can be easily acquired from automatically
parsed corpora. While the relation word is usually
part of the utterance that represents the source of
the relation, determining the appropriate target (an-
tecedent) of the relation may be difficult to achieve.
As a heuristic, an adjacency preference is adopted,
i.e., the target is identified with the main event of the
preceding utterance.1 The BKB can be constructed
from a sufficiently large corpus as follows:
</bodyText>
<listItem confidence="0.999355">
• identify event types and relation words
• for every utterance
</listItem>
<bodyText confidence="0.716999857142857">
– create a candidate triple consisting of the
event type of the utterance, the relation
word, and the event type of the preceding
utterance.
– add the candidate triple to the BKB, if it
found in the BKB, increase its score by (or
initialize it with) 1,
</bodyText>
<listItem confidence="0.924376">
• perform a pruning on all candidate triples, cal-
culate significance and correlation scores
</listItem>
<bodyText confidence="0.9887151">
Pruning uses statistical significance tests to evalu-
ate whether the relative frequency of a relation word
for a pair of events is significantly higher or lower
than the relative frequency of the relation word in
the entire corpus. Assuming that incorrect candi-
date triples (i.e., where the factual target of the rela-
tion was non-adjacent) are equally distributed, they
should be filtered out by the significance tests.
The goal of this paper is to evaluate the validity of
this approach.
</bodyText>
<sectionHeader confidence="0.992371" genericHeader="introduction">
2 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9612658">
By generalizing over multiple occurrences of the
same events (or, more precisely, event types), one
can identify preferences of event pairs for one or
several relation words. These preferences capture
context-invariant characteristics of pairs of events
and are thus to considered to reflect a semantic pre-
disposition for a particular discourse relation.
Formally, an event is the semantic representa-
tion of the meaning conveyed in the utterance. We
1Relations between non-adjacent utterances are constrained
by the structure of discourse (Webber 1991), and thus less likely
than relations between adjacent utterances.
assume that the same event can reoccur in differ-
ent contexts, we are thus studying relations be-
tween types of events. For the experiment described
here, events are heuristically identified with the main
predicates of a sentence, i.e., non-auxiliar, non-
causative, non-modal verbal lexemes that serve as
heads of main clauses.
The primary data structure of the approach de-
scribed here is a triple consisting of a source event, a
relation word and a target (antecedent) event. These
triples are harvested from large syntactically anno-
tated corpora. For intersentential relations, the tar-
get is identified with the event of the immediately
preceding main clause. These extraction preferences
are heuristic approximations, and thus, an additional
pruning step is necessary.
For this purpose, statistical significance tests are
adopted (x2 for triples of frequent events and re-
lation words, t-test for rare events and/or relation
words) that compare the relative frequency of a rela-
tion word given a pair of events with the relative fre-
quency of the relation word in the entire corpus. All
results with p &gt; .05 are excluded, i.e., only triples
are preserved for which the observed positive or neg-
ative correlation between a pair of events and a re-
lation word is not due to chance with at least 95%
probability. Assuming an even distribution of incor-
rect target events, this should rule these out. Ad-
ditionally, it also serves as a means of evaluation.
Using statistical significance tests as pruning crite-
rion entails that all triples eventually confirmed are
statistically significant.2
This setup requires immense amounts of data: We
are dealing with several thousand events (theoreti-
cally, the total number of verbs of a language). The
chance probability for two events to occur in adja-
cent position is thus far below 10−6, and it decreases
further if the likelihood of a relation word is taken
into consideration. All things being equal, we thus
need millions of sentences to create the BKB.
Here, two large-scale corpora of English are em-
ployed, PukWaC and Wackypedia EN (Baroni et al.
2009). PukWaC is a 2G-token web corpus of British
English crawled from the uk domain (Ferraresi et al.
2Subsequent studies may employ less rigid pruning criteria.
For the purpose of the current paper, however, the statistical sig-
nificance of all extracted triples serves as an criterion to evaluate
methodological validity.
</bodyText>
<page confidence="0.994385">
214
</page>
<bodyText confidence="0.99965105">
2008), and parsed with MaltParser (Nivre et al.
2006). It is distributed in 5 parts; Only PukWaC-
1 to PukWaC-4 were considered here, constitut-
ing 82.2% (72.5M sentences) of the entire corpus,
PukWaC-5 is left untouched for forthcoming evalu-
ation experiments. Wackypedia EN is a 0.8G-token
dump of the English Wikipedia, annotated with the
same tools. It is distributed in 4 different files; the
last portion was left untouched for forthcoming eval-
uation experiments. The portion analyzed here com-
prises 33.2M sentences, 75.9% of the corpus.
The extraction of events in these corpora uses
simple patterns that combine dependency informa-
tion and part-of-speech tags to retrieve the main
verbs and store their lemmata as event types. The
target (antecedent) event was identified with the last
main event of the preceding sentence. As relation
words, only sentence-initial children of the source
event that were annotated as adverbial modifiers,
verb modifiers or conjunctions were considered.
</bodyText>
<sectionHeader confidence="0.999147" genericHeader="background">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999944">
To evaluate the validity of the approach, three funda-
mental questions need to be addressed: significance
(are there significant correlations between pairs of
events and relation words ?), reproducibility (can
these correlations confirmed on independent data
sets ?), and interpretability (can these correlations
be interpreted in terms of theoretically-defined dis-
course relations ?).
</bodyText>
<subsectionHeader confidence="0.995463">
3.1 Significance and Reproducibility
</subsectionHeader>
<bodyText confidence="0.999664066666667">
Significance tests are part of the pruning stage of the
algorithm. Therefore, the number of triples eventu-
ally retrieved confirms the existence of statistically
significant correlations between pairs of events and
relation words. The left column of Tab. 1 shows
the number of triples obtained from PukWaC sub-
corpora of different size.
For reproducibility, compare the triples identified
with Wackypedia EN and PukWaC subcorpora of
different size: Table 1 shows the number of triples
found in both Wackypedia EN and PukWaC, and the
agreement between both resources. For two triples
involving the same events (event types) and the same
relation word, agreement means that the relation
word shows either positive or negative correlation
</bodyText>
<table confidence="0.999255142857143">
PukWaC (sub)corpus Wackypedia EN triples
sentences triples common agreeing %
1.2M 74 20 12 60.0
4.8M 832 177 132 75.5
19.2M 7,342 938 809 86.3
38.4M 20,106 1,783 1,596 89.9
72.5M 46,680 2,643 2,393 90.5
</table>
<tableCaption confidence="0.9748945">
Table 1: Agreement with respect to positive or nega-
tive correlation of event pairs and relation words be-
tween Wackypedia EN and PukWaC subcorpora of dif-
ferent size
</tableCaption>
<table confidence="0.8907838">
PukWaC triples
total vs. H vs. T
11,042 6,805 1,525
7,251 1,413
T: then 1,791
</table>
<tableCaption confidence="0.957603">
Table 2: Agreement between but (B), however (H) and
then (T) on PukWaC
</tableCaption>
<bodyText confidence="0.9854879">
in both corpora, disagreement means positive corre-
lation in one corpus and negative correlation in the
other.
Table 1 confirms that results obtained on one re-
source can be reproduced on another. This indi-
cates that triples indeed capture context-invariant,
and hence, semantic, characteristics of the relation
between events. The data also indicates that repro-
ducibility increases with the size of corpora from
which a BKB is built.
</bodyText>
<subsectionHeader confidence="0.979073">
3.2 Interpretability
</subsectionHeader>
<bodyText confidence="0.997623571428571">
Any theory of discourse relations would predict that
relation words with similar function should have
similar distributions, whereas one would expect dif-
ferent distributions for functionally unrelated rela-
tion words. These expectations are tested here for
three of the most frequent relation words found in
the corpora, i.e., but, then and however. But and
however can be grouped together under a general-
ized notion of contrast (Knott and Dale 1994; Prasad
et al. 2008); then, on the other hand, indicates a tem-
poral and/or causal relation.
Table 2 confirms the expectation that event pairs
that are correlated with but tend to show the same
correlation with however, but not with then.
</bodyText>
<figure confidence="0.991053">
B: but
H: however
agreement (%)
vs. H vs. T
97.7 62.2
66.9
</figure>
<page confidence="0.998522">
215
</page>
<sectionHeader confidence="0.988045" genericHeader="discussions">
4 Discussion and Outlook
</sectionHeader>
<bodyText confidence="0.999993788888889">
This paper described a novel approach towards the
unsupervised acquisition of discourse relations, with
encouraging preliminary results: Large collections
of parsed text are used to assess distributional pro-
files of relation words that indicate discourse re-
lations that are possible between specific types of
events; on this basis, a background knowledge base
(BKB) was created that can be used to predict an ap-
propriate discourse marker to connect two utterances
with no overt relation word.
This information can be used, for example, to fa-
cilitate the semiautomated annotation of discourse
relations, by pointing out the ‘default’ relation word
for a given pair of events. Similarly, Zhou et al.
(2010) used a language model to predict discourse
markers for implicitly realized discourse relations.
As opposed to this shallow, n-gram-based approach,
here, the internal structure of utterances is exploited:
based on semantic considerations, syntactic patterns
have been devised that extract triples of event pairs
and relation words. The resulting BKB provides a
distributional approximation of the discourse rela-
tions that can hold between two specific event types.
Both approaches exploit complementary sources of
knowledge, and may be combined with each other
to achieve a more precise prediction of implicit dis-
course connectives.
The validity of the approach was evaluated with
respect to three evaluation criteria: The extracted as-
sociations between relation words and event pairs
could be shown to be statistically significant, and
to be reproducible on other corpora; for three
highly frequent relation words, theoretical predic-
tions about their relative distribution could be con-
firmed, indicating their interpretability in terms of
presupposed taxonomies of discourse relations.
Another prospective field of application can be
seen in NLP applications, where selection prefer-
ences for relation words may serve as a cheap re-
placement for full-fledged discourse parsing. In the
Natural Language Understanding domain, the BKB
may help to disambiguate or to identify discourse
relations between different events; in the context of
Machine Translation, it may represent a factor guid-
ing the insertion of relation words, a task that has
been found to be problematic for languages that dif-
fer in their inventory and usage of discourse mark-
ers, e.g., German and English (Stede and Schmitz
2000). The approach is language-independent (ex-
cept for the syntactic extraction patterns), and it does
not require manually annotated data. It would thus
be easy to create background knowledge bases with
relation words for other languages or specific do-
mains – given a sufficient amount of textual data.
Related research includes, for example, the un-
supervised recognition of causal and temporal rela-
tionships, as required, for example, for the recog-
nition of textual entailment. Riaz and Girju (2010)
exploit distributional information about pairs of ut-
terances. Unlike approach described here, they are
not restricted to adjacent utterances, and do not rely
on explicit and recurrent relation words. Their ap-
proach can thus be applied to comparably small
data sets. However, they are restricted to a spe-
cific type of relations whereas here the entire band-
width of discourse relations that are explicitly real-
ized in a language are covered. Prospectively, both
approaches could be combined to compensate their
respective weaknesses.
Similar observations can be made with respect to
Chambers and Jurafsky (2009) and Kasch and Oates
(2010), who also study a single discourse relation
(narration), and are thus more limited in scope than
the approach described here. However, as their ap-
proach extends beyond pairs of events to complex
event chains, it seems that both approaches provide
complementary types of information and their re-
sults could also be combined in a fruitful way to
achieve a more detailed assessment of discourse re-
lations.
The goal of this paper was to evaluate the meth-
dological validity of the approach. It thus represents
the basis for further experiments, e.g., with respect
to the enrichment the BKB with information pro-
vided by Riaz and Girju (2010), Chambers and Ju-
rafsky (2009) and Kasch and Oates (2010). Other di-
rections of subsequent research may include address
more elaborate models of events, and the investiga-
tion of the relationship between relation words and
taxonomies of discourse relations.
</bodyText>
<page confidence="0.998334">
216
</page>
<sectionHeader confidence="0.99833" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999974909090909">
This work was supported by a fellowship within
the Postdoc program of the German Academic Ex-
change Service (DAAD). Initial experiments were
conducted at the Collaborative Research Center
(SFB) 632 “Information Structure” at the Univer-
sity of Potsdam, Germany. I would also like to
thank three anonymous reviewers for valuable com-
ments and feedback, as well as Manfred Stede and
Ed Hovy whose work on discourse relations on the
one hand and proposition stores on the other hand
have been the main inspiration for this paper.
</bodyText>
<sectionHeader confidence="0.998952" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.971965857142857">
M. Baroni, S. Bernardini, A. Ferraresi, and
E. Zanchetta. The wacky wide web: a collec-
tion of very large linguistically processed web-
crawled corpora. Language Resources and Eval-
uation, 43(3):209–226, 2009.
N. Chambers and D. Jurafsky. Unsupervised learn-
ing of narrative schemas and their participants. In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language
Processing of the AFNLP: Volume 2-Volume 2,
pages 602–610. Association for Computational
Linguistics, 2009.
A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernar-
dini. Introducing and evaluating ukwac, a very
large web-derived corpus of english. In Proceed-
ings of the 4th Web as Corpus Workshop (WAC-4)
Can we beat Google, pages 47–54, 2008.
Morton Ann Gernsbacher, Rachel R. W. Robertson,
Paola Palladino, and Necia K. Werner. Manag-
ing mental representations during narrative com-
prehension. Discourse Processes, 37(2):145–164,
2004.
N. Kasch and T. Oates. Mining script-like struc-
tures from the web. In Proceedings of the NAACL
HLT 2010 First International Workshop on For-
malisms and Methodology for Learning by Read-
ing, pages 34–42. Association for Computational
Linguistics, 2010.
A. Knott and R. Dale. Using linguistic phenomena
to motivate a set of coherence relations. Discourse
processes, 18(1):35–62, 1994.
J. van Kuppevelt and R. Smith, editors. Current Di-
rections in Discourse and Dialogue. Kluwer, Dor-
drecht, 2003.
William C. Mann and Sandra A. Thompson. Rhetor-
ical Structure Theory: Toward a functional theory
of text organization. Text, 8(3):243–281, 1988.
J. Nivre, J. Hall, and J. Nilsson. Maltparser: A
data-driven parser-generator for dependency pars-
ing. In Proc. of LREC, pages 2216–2219. Cite-
seer, 2006.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. The penn
discourse treebank 2.0. In Proc. 6th International
Conference on Language Resources and Evalua-
tion (LREC 2008), Marrakech, Morocco, 2008.
M. Riaz and R. Girju. Another look at causality:
Discovering scenario-specific contingency rela-
tionships with no supervision. In Semantic Com-
puting (ICSC), 2010 IEEE Fourth International
Conference on, pages 361–368. IEEE, 2010.
M. Stede and B. Schmitz. Discourse particles and
discourse functions. Machine translation, 15(1):
125–147, 2000.
Enric Vallduvi. The Informational Component. Gar-
land, New York, 1992.
Bonnie L. Webber. Structure and ostension in the
interpretation of discourse deixis. Natural Lan-
guage and Cognitive Processes, 2(6):107–135,
1991.
Bonnie L. Webber, Matthew Stone, Aravind K.
Joshi, and Alistair Knott. Anaphora and discourse
structure. Computational Linguistics, 4(29):545–
587, 2003.
Z.-M. Zhou, Y. Xu, Z.-Y. Niu, M. Lan, J. Su, and
C.L. Tan. Predicting discourse connectives for
implicit discourse relation recognition. In COL-
ING 2010, pages 1507–1514, Beijing, China, Au-
gust 2010.
</reference>
<page confidence="0.998344">
217
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001175">
<title confidence="0.999518">Towards the Unsupervised Acquisition of Discourse Relations</title>
<author confidence="0.99923">Christian</author>
<affiliation confidence="0.998125">Information Sciences University of Southern</affiliation>
<address confidence="0.999413">4676 Admiralty Way, Marina del Rey, CA</address>
<email confidence="0.987614">chiarcos@daad-alumni.de</email>
<abstract confidence="0.984744412087914">This paper describes a novel approach towards the empirical approximation of discourse relations between different utterances in texts. Following the idea that every pair of events comes with preferences regarding the range and frequency of discourse relations connecting both parts, the paper investigates whether these preferences are manifested in the distribution of relation words (that serve to signal these relations). Experiments on two large-scale English web corpora show that significant correlations between pairs of adjacent events and relation words exist, that they are reproducible on different data sets, and for three relation words, that their distribution corresponds to theorybased assumptions. 1 Motivation Texts are not merely accumulations of isolated utterances, but the arrangement of utterances conveys human text understanding can thus be described as a process to recover the global structure of texts and the relations linking its different parts (Vallduv´ı 1992; Gernsbacher et al. 2004). To capture these aspects of meaning in NLP, it is necessary to develop operationalizable theories, and, within a supervised approach, large amounts of annotated training data. To facilitate manual annotation, weakly supervised or unsupervised techniques can be apas preprocessing step for annotation, and this is part of the motivation of the approach described here. Discourse relations involve different aspects of meaning. This may include factual knowledge about the connected discourse segments (a ‘subjectmatter’ relation, e.g., if one utterance represents the cause for another, Mann and Thompson 1988, p.257), argumentative purposes (a ‘presentational’ relation, e.g., one utterance motivates the reader to accept a claim formulated in another utterance, ibid., p.257), or relations between entities mentioned in the connected discourse segments (anaphoric relations, Webber et al. 2003). Discourse relations can be indicated explicitly by optional cues, e.g., ad- (e.g., conjunctions (e.g., or phrases (e.g., contrast to what Peter said minute Here, these cues are referred to as Assuming that relation words are associated with specific discourse relations (Knott and Dale 1994; Prasad et al. 2008), the distribution of relation words found between two (types of) events can yield insights into the range of discourse relations possible at this occasion and their respective likeliness. For this purpose, this paper proposes a background knowledge base (BKB) that hosts pairs of events (here heuristically represented by verbs) along with distributional profiles for relation words. The pridata structure of the BKB is a one event (type) is connected with a particular relation word to another event (type). Triples are furaugmented with a score likelihood of the triple to be observed), a sigscore below), and a score (indicating whether a pair of events has a positive or negative correlation with a particular relation word). 213 of the 50th Annual Meeting of the Association for Computational pages 213–217, Republic of Korea, 8-14 July 2012. Association for Computational Linguistics Triples can be easily acquired from automatically parsed corpora. While the relation word is usually part of the utterance that represents the source of the relation, determining the appropriate target (antecedent) of the relation may be difficult to achieve. As a heuristic, an adjacency preference is adopted, i.e., the target is identified with the main event of the The BKB can be constructed from a sufficiently large corpus as follows: • identify event types and relation words • for every utterance a candidate triple consisting of the event type of the utterance, the relation word, and the event type of the preceding utterance. the candidate triple to the BKB, if it found in the BKB, increase its score by (or initialize it with) 1, • perform a pruning on all candidate triples, calculate significance and correlation scores Pruning uses statistical significance tests to evaluate whether the relative frequency of a relation word for a pair of events is significantly higher or lower than the relative frequency of the relation word in the entire corpus. Assuming that incorrect candidate triples (i.e., where the factual target of the relation was non-adjacent) are equally distributed, they should be filtered out by the significance tests. The goal of this paper is to evaluate the validity of this approach. 2 Experimental Setup By generalizing over multiple occurrences of the same events (or, more precisely, event types), one can identify preferences of event pairs for one or several relation words. These preferences capture of pairs of events and are thus to considered to reflect a semantic predisposition for a particular discourse relation. Formally, an event is the semantic representation of the meaning conveyed in the utterance. We between non-adjacent utterances are constrained by the structure of discourse (Webber 1991), and thus less likely than relations between adjacent utterances. assume that the same event can reoccur in different contexts, we are thus studying relations beevents. For the experiment described here, events are heuristically identified with the main predicates of a sentence, i.e., non-auxiliar, noncausative, non-modal verbal lexemes that serve as heads of main clauses. The primary data structure of the approach described here is a triple consisting of a source event, a relation word and a target (antecedent) event. These triples are harvested from large syntactically annotated corpora. For intersentential relations, the target is identified with the event of the immediately preceding main clause. These extraction preferences are heuristic approximations, and thus, an additional pruning step is necessary. For this purpose, statistical significance tests are for triples of frequent events and rewords, for rare events and/or relation words) that compare the relative frequency of a relation word given a pair of events with the relative frequency of the relation word in the entire corpus. All with excluded, i.e., only triples are preserved for which the observed positive or negative correlation between a pair of events and a relation word is not due to chance with at least 95% probability. Assuming an even distribution of incorrect target events, this should rule these out. Additionally, it also serves as a means of evaluation. Using statistical significance tests as pruning criterion entails that all triples eventually confirmed are setup requires of data: We are dealing with several thousand events (theoretically, the total number of verbs of a language). The chance probability for two events to occur in adjaposition is thus far below and it decreases further if the likelihood of a relation word is taken into consideration. All things being equal, we thus sentences to create the BKB. Here, two large-scale corpora of English are employed, PukWaC and Wackypedia EN (Baroni et al. 2009). PukWaC is a 2G-token web corpus of British crawled from the (Ferraresi et al. studies may employ less rigid pruning criteria. For the purpose of the current paper, however, the statistical significance of all extracted triples serves as an criterion to evaluate methodological validity. 214 2008), and parsed with MaltParser (Nivre et al. 2006). It is distributed in 5 parts; Only PukWaC- 1 to PukWaC-4 were considered here, constituting 82.2% (72.5M sentences) of the entire corpus, PukWaC-5 is left untouched for forthcoming evaluation experiments. Wackypedia EN is a 0.8G-token dump of the English Wikipedia, annotated with the same tools. It is distributed in 4 different files; the last portion was left untouched for forthcoming evaluation experiments. The portion analyzed here comprises 33.2M sentences, 75.9% of the corpus. The extraction of events in these corpora uses simple patterns that combine dependency information and part-of-speech tags to retrieve the main verbs and store their lemmata as event types. The target (antecedent) event was identified with the last main event of the preceding sentence. As relation words, only sentence-initial children of the source event that were annotated as adverbial modifiers, verb modifiers or conjunctions were considered. 3 Evaluation To evaluate the validity of the approach, three fundaquestions need to be addressed: (are there significant correlations between pairs of and relation words ?), these correlations confirmed on independent data ?), and these correlations be interpreted in terms of theoretically-defined discourse relations ?). 3.1 Significance and Reproducibility Significance tests are part of the pruning stage of the algorithm. Therefore, the number of triples eventually retrieved confirms the existence of statistically significant correlations between pairs of events and relation words. The left column of Tab. 1 shows the number of triples obtained from PukWaC subcorpora of different size. For reproducibility, compare the triples identified with Wackypedia EN and PukWaC subcorpora of different size: Table 1 shows the number of triples found in both Wackypedia EN and PukWaC, and the both resources. For two triples involving the same events (event types) and the same relation word, agreement means that the relation word shows either positive or negative correlation PukWaC (sub)corpus Wackypedia EN triples sentences triples common agreeing % 1.2M 74 20 12 60.0 4.8M 832 177 132 75.5 19.2M 7,342 938 809 86.3 38.4M 20,106 1,783 1,596 89.9 72.5M 46,680 2,643 2,393 90.5 Table 1: Agreement with respect to positive or negative correlation of event pairs and relation words between Wackypedia EN and PukWaC subcorpora of different size PukWaC triples total vs. H vs. T 11,042 6,805 1,525 7,251 1,413 2: Agreement between and on PukWaC in both corpora, disagreement means positive correlation in one corpus and negative correlation in the other. Table 1 confirms that results obtained on one resource can be reproduced on another. This indicates that triples indeed capture context-invariant, and hence, semantic, characteristics of the relation between events. The data also indicates that reproducibility increases with the size of corpora from which a BKB is built. 3.2 Interpretability Any theory of discourse relations would predict that relation words with similar function should have similar distributions, whereas one would expect different distributions for functionally unrelated relation words. These expectations are tested here for three of the most frequent relation words found in corpora, i.e., then But be grouped together under a generalized notion of contrast (Knott and Dale 1994; Prasad al. 2008); the other hand, indicates a temporal and/or causal relation. Table 2 confirms the expectation that event pairs are correlated with to show the same with not with agreement vs. H vs. T 97.7 62.2 66.9 215 4 Discussion and Outlook This paper described a novel approach towards the unsupervised acquisition of discourse relations, with encouraging preliminary results: Large collections of parsed text are used to assess distributional profiles of relation words that indicate discourse relations that are possible between specific types of events; on this basis, a background knowledge base (BKB) was created that can be used to predict an appropriate discourse marker to connect two utterances with no overt relation word. This information can be used, for example, to facilitate the semiautomated annotation of discourse relations, by pointing out the ‘default’ relation word for a given pair of events. Similarly, Zhou et al. (2010) used a language model to predict discourse markers for implicitly realized discourse relations. opposed to this shallow, approach, here, the internal structure of utterances is exploited: based on semantic considerations, syntactic patterns have been devised that extract triples of event pairs and relation words. The resulting BKB provides a distributional approximation of the discourse relations that can hold between two specific event types. Both approaches exploit complementary sources of knowledge, and may be combined with each other to achieve a more precise prediction of implicit discourse connectives. The validity of the approach was evaluated with respect to three evaluation criteria: The extracted associations between relation words and event pairs could be shown to be statistically significant, and to be reproducible on other corpora; for three highly frequent relation words, theoretical predictions about their relative distribution could be confirmed, indicating their interpretability in terms of presupposed taxonomies of discourse relations. Another prospective field of application can be seen in NLP applications, where selection preferences for relation words may serve as a cheap replacement for full-fledged discourse parsing. In the Natural Language Understanding domain, the BKB may help to disambiguate or to identify discourse relations between different events; in the context of Machine Translation, it may represent a factor guiding the insertion of relation words, a task that has been found to be problematic for languages that differ in their inventory and usage of discourse markers, e.g., German and English (Stede and Schmitz 2000). The approach is language-independent (except for the syntactic extraction patterns), and it does not require manually annotated data. It would thus be easy to create background knowledge bases with relation words for other languages or specific domains – given a sufficient amount of textual data. Related research includes, for example, the unsupervised recognition of causal and temporal relationships, as required, for example, for the recognition of textual entailment. Riaz and Girju (2010) exploit distributional information about pairs of utterances. Unlike approach described here, they are not restricted to adjacent utterances, and do not rely on explicit and recurrent relation words. Their approach can thus be applied to comparably small data sets. However, they are restricted to a specific type of relations whereas here the entire bandwidth of discourse relations that are explicitly realized in a language are covered. Prospectively, both approaches could be combined to compensate their respective weaknesses. Similar observations can be made with respect to Chambers and Jurafsky (2009) and Kasch and Oates (2010), who also study a single discourse relation (narration), and are thus more limited in scope than the approach described here. However, as their approach extends beyond pairs of events to complex event chains, it seems that both approaches provide complementary types of information and their results could also be combined in a fruitful way to achieve a more detailed assessment of discourse relations. The goal of this paper was to evaluate the methdological validity of the approach. It thus represents the basis for further experiments, e.g., with respect to the enrichment the BKB with information provided by Riaz and Girju (2010), Chambers and Jurafsky (2009) and Kasch and Oates (2010). Other directions of subsequent research may include address more elaborate models of events, and the investigation of the relationship between relation words and taxonomies of discourse relations. 216 Acknowledgments This work was supported by a fellowship within the Postdoc program of the German Academic Exchange Service (DAAD). Initial experiments were conducted at the Collaborative Research Center (SFB) 632 “Information Structure” at the University of Potsdam, Germany. I would also like to thank three anonymous reviewers for valuable comments and feedback, as well as Manfred Stede and Ed Hovy whose work on discourse relations on the one hand and proposition stores on the other hand have been the main inspiration for this paper. References M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta. The wacky wide web: a collection of very large linguistically processed webcorpora. Resources and Eval- 43(3):209–226, 2009. N. Chambers and D. Jurafsky. Unsupervised learning of narrative schemas and their participants. In</abstract>
<note confidence="0.972725818181818">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language of the AFNLP: Volume 2-Volume pages 602–610. Association for Computational Linguistics, 2009. A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini. Introducing and evaluating ukwac, a very web-derived corpus of english. In Proceedings of the 4th Web as Corpus Workshop (WAC-4) we beat pages 47–54, 2008.</note>
<author confidence="0.9916935">Manag-</author>
<abstract confidence="0.835336272727273">ing mental representations during narrative com- 37(2):145–164, 2004. N. Kasch and T. Oates. Mining script-like strucfrom the web. In of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Readpages 34–42. Association for Computational Linguistics, 2010. A. Knott and R. Dale. Using linguistic phenomena motivate a set of coherence relations. 18(1):35–62, 1994. van Kuppevelt and R. Smith, editors. Diin Discourse and Kluwer, Dordrecht, 2003. William C. Mann and Sandra A. Thompson. Rhetorical Structure Theory: Toward a functional theory text organization. 8(3):243–281, 1988. J. Nivre, J. Hall, and J. Nilsson. Maltparser: A data-driven parser-generator for dependency pars- In of pages 2216–2219. Citeseer, 2006.</abstract>
<author confidence="0.3579165">The penn</author>
<note confidence="0.864048185185185">treebank 2.0. In 6th International Conference on Language Resources and Evalua- (LREC Marrakech, Morocco, 2008. M. Riaz and R. Girju. Another look at causality: Discovering scenario-specific contingency relawith no supervision. In Computing (ICSC), 2010 IEEE Fourth International pages 361–368. IEEE, 2010. M. Stede and B. Schmitz. Discourse particles and functions. 15(1): 125–147, 2000. Vallduvi. Informational Garland, New York, 1992. Bonnie L. Webber. Structure and ostension in the of discourse deixis. Lanand Cognitive 2(6):107–135, 1991. Bonnie L. Webber, Matthew Stone, Aravind K. Joshi, and Alistair Knott. Anaphora and discourse 4(29):545– 587, 2003. Z.-M. Zhou, Y. Xu, Z.-Y. Niu, M. Lan, J. Su, and C.L. Tan. Predicting discourse connectives for discourse relation recognition. In COLpages 1507–1514, Beijing, China, August 2010. 217</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>S Bernardini</author>
<author>A Ferraresi</author>
<author>E Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<volume>43</volume>
<issue>3</issue>
<contexts>
<context position="7630" citStr="Baroni et al. 2009" startWordPosition="1184" endWordPosition="1187">cance tests as pruning criterion entails that all triples eventually confirmed are statistically significant.2 This setup requires immense amounts of data: We are dealing with several thousand events (theoretically, the total number of verbs of a language). The chance probability for two events to occur in adjacent position is thus far below 10−6, and it decreases further if the likelihood of a relation word is taken into consideration. All things being equal, we thus need millions of sentences to create the BKB. Here, two large-scale corpora of English are employed, PukWaC and Wackypedia EN (Baroni et al. 2009). PukWaC is a 2G-token web corpus of British English crawled from the uk domain (Ferraresi et al. 2Subsequent studies may employ less rigid pruning criteria. For the purpose of the current paper, however, the statistical significance of all extracted triples serves as an criterion to evaluate methodological validity. 214 2008), and parsed with MaltParser (Nivre et al. 2006). It is distributed in 5 parts; Only PukWaC1 to PukWaC-4 were considered here, constituting 82.2% (72.5M sentences) of the entire corpus, PukWaC-5 is left untouched for forthcoming evaluation experiments. Wackypedia EN is a </context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta. The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chambers</author>
<author>D Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>602--610</pages>
<contexts>
<context position="15361" citStr="Chambers and Jurafsky (2009)" startWordPosition="2383" endWordPosition="2386">z and Girju (2010) exploit distributional information about pairs of utterances. Unlike approach described here, they are not restricted to adjacent utterances, and do not rely on explicit and recurrent relation words. Their approach can thus be applied to comparably small data sets. However, they are restricted to a specific type of relations whereas here the entire bandwidth of discourse relations that are explicitly realized in a language are covered. Prospectively, both approaches could be combined to compensate their respective weaknesses. Similar observations can be made with respect to Chambers and Jurafsky (2009) and Kasch and Oates (2010), who also study a single discourse relation (narration), and are thus more limited in scope than the approach described here. However, as their approach extends beyond pairs of events to complex event chains, it seems that both approaches provide complementary types of information and their results could also be combined in a fruitful way to achieve a more detailed assessment of discourse relations. The goal of this paper was to evaluate the methdological validity of the approach. It thus represents the basis for further experiments, e.g., with respect to the enrich</context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>N. Chambers and D. Jurafsky. Unsupervised learning of narrative schemas and their participants. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 602–610. Association for Computational Linguistics, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ferraresi</author>
<author>E Zanchetta</author>
<author>M Baroni</author>
<author>S Bernardini</author>
</authors>
<title>Introducing and evaluating ukwac, a very large web-derived corpus of english.</title>
<date>2008</date>
<booktitle>In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google,</booktitle>
<pages>47--54</pages>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini. Introducing and evaluating ukwac, a very large web-derived corpus of english. In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google, pages 47–54, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morton Ann Gernsbacher</author>
<author>Rachel R W Robertson</author>
<author>Paola Palladino</author>
<author>Necia K Werner</author>
</authors>
<title>Managing mental representations during narrative comprehension.</title>
<date>2004</date>
<booktitle>Discourse Processes,</booktitle>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="1249" citStr="Gernsbacher et al. 2004" startWordPosition="177" endWordPosition="180">gnal these relations). Experiments on two large-scale English web corpora show that significant correlations between pairs of adjacent events and relation words exist, that they are reproducible on different data sets, and for three relation words, that their distribution corresponds to theorybased assumptions. 1 Motivation Texts are not merely accumulations of isolated utterances, but the arrangement of utterances conveys meaning; human text understanding can thus be described as a process to recover the global structure of texts and the relations linking its different parts (Vallduv´ı 1992; Gernsbacher et al. 2004). To capture these aspects of meaning in NLP, it is necessary to develop operationalizable theories, and, within a supervised approach, large amounts of annotated training data. To facilitate manual annotation, weakly supervised or unsupervised techniques can be applied as preprocessing step for semimanual annotation, and this is part of the motivation of the approach described here. Discourse relations involve different aspects of meaning. This may include factual knowledge about the connected discourse segments (a ‘subjectmatter’ relation, e.g., if one utterance represents the cause for anot</context>
</contexts>
<marker>Gernsbacher, Robertson, Palladino, Werner, 2004</marker>
<rawString>Morton Ann Gernsbacher, Rachel R. W. Robertson, Paola Palladino, and Necia K. Werner. Managing mental representations during narrative comprehension. Discourse Processes, 37(2):145–164, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kasch</author>
<author>T Oates</author>
</authors>
<title>Mining script-like structures from the web.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading,</booktitle>
<pages>34--42</pages>
<contexts>
<context position="15388" citStr="Kasch and Oates (2010)" startWordPosition="2388" endWordPosition="2391">butional information about pairs of utterances. Unlike approach described here, they are not restricted to adjacent utterances, and do not rely on explicit and recurrent relation words. Their approach can thus be applied to comparably small data sets. However, they are restricted to a specific type of relations whereas here the entire bandwidth of discourse relations that are explicitly realized in a language are covered. Prospectively, both approaches could be combined to compensate their respective weaknesses. Similar observations can be made with respect to Chambers and Jurafsky (2009) and Kasch and Oates (2010), who also study a single discourse relation (narration), and are thus more limited in scope than the approach described here. However, as their approach extends beyond pairs of events to complex event chains, it seems that both approaches provide complementary types of information and their results could also be combined in a fruitful way to achieve a more detailed assessment of discourse relations. The goal of this paper was to evaluate the methdological validity of the approach. It thus represents the basis for further experiments, e.g., with respect to the enrichment the BKB with informati</context>
</contexts>
<marker>Kasch, Oates, 2010</marker>
<rawString>N. Kasch and T. Oates. Mining script-like structures from the web. In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 34–42. Association for Computational Linguistics, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Knott</author>
<author>R Dale</author>
</authors>
<title>Using linguistic phenomena to motivate a set of coherence relations.</title>
<date>1994</date>
<booktitle>Discourse processes,</booktitle>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="2513" citStr="Knott and Dale 1994" startWordPosition="364" endWordPosition="367">ative purposes (a ‘presentational’ relation, e.g., one utterance motivates the reader to accept a claim formulated in another utterance, ibid., p.257), or relations between entities mentioned in the connected discourse segments (anaphoric relations, Webber et al. 2003). Discourse relations can be indicated explicitly by optional cues, e.g., adverbials (e.g., however), conjunctions (e.g., but), or complex phrases (e.g., in contrast to what Peter said a minute ago). Here, these cues are referred to as relation words. Assuming that relation words are associated with specific discourse relations (Knott and Dale 1994; Prasad et al. 2008), the distribution of relation words found between two (types of) events can yield insights into the range of discourse relations possible at this occasion and their respective likeliness. For this purpose, this paper proposes a background knowledge base (BKB) that hosts pairs of events (here heuristically represented by verbs) along with distributional profiles for relation words. The primary data structure of the BKB is a triple where one event (type) is connected with a particular relation word to another event (type). Triples are further augmented with a frequency scor</context>
<context position="11529" citStr="Knott and Dale 1994" startWordPosition="1790" endWordPosition="1793">cteristics of the relation between events. The data also indicates that reproducibility increases with the size of corpora from which a BKB is built. 3.2 Interpretability Any theory of discourse relations would predict that relation words with similar function should have similar distributions, whereas one would expect different distributions for functionally unrelated relation words. These expectations are tested here for three of the most frequent relation words found in the corpora, i.e., but, then and however. But and however can be grouped together under a generalized notion of contrast (Knott and Dale 1994; Prasad et al. 2008); then, on the other hand, indicates a temporal and/or causal relation. Table 2 confirms the expectation that event pairs that are correlated with but tend to show the same correlation with however, but not with then. B: but H: however agreement (%) vs. H vs. T 97.7 62.2 66.9 215 4 Discussion and Outlook This paper described a novel approach towards the unsupervised acquisition of discourse relations, with encouraging preliminary results: Large collections of parsed text are used to assess distributional profiles of relation words that indicate discourse relations that are</context>
</contexts>
<marker>Knott, Dale, 1994</marker>
<rawString>A. Knott and R. Dale. Using linguistic phenomena to motivate a set of coherence relations. Discourse processes, 18(1):35–62, 1994.</rawString>
</citation>
<citation valid="true">
<date>2003</date>
<booktitle>Current Directions in Discourse and Dialogue.</booktitle>
<editor>J. van Kuppevelt and R. Smith, editors.</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht,</location>
<marker>2003</marker>
<rawString>J. van Kuppevelt and R. Smith, editors. Current Directions in Discourse and Dialogue. Kluwer, Dordrecht, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<journal>Text,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="1876" citStr="Mann and Thompson 1988" startWordPosition="272" endWordPosition="275">capture these aspects of meaning in NLP, it is necessary to develop operationalizable theories, and, within a supervised approach, large amounts of annotated training data. To facilitate manual annotation, weakly supervised or unsupervised techniques can be applied as preprocessing step for semimanual annotation, and this is part of the motivation of the approach described here. Discourse relations involve different aspects of meaning. This may include factual knowledge about the connected discourse segments (a ‘subjectmatter’ relation, e.g., if one utterance represents the cause for another, Mann and Thompson 1988, p.257), argumentative purposes (a ‘presentational’ relation, e.g., one utterance motivates the reader to accept a claim formulated in another utterance, ibid., p.257), or relations between entities mentioned in the connected discourse segments (anaphoric relations, Webber et al. 2003). Discourse relations can be indicated explicitly by optional cues, e.g., adverbials (e.g., however), conjunctions (e.g., but), or complex phrases (e.g., in contrast to what Peter said a minute ago). Here, these cues are referred to as relation words. Assuming that relation words are associated with specific dis</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. Rhetorical Structure Theory: Toward a functional theory of text organization. Text, 8(3):243–281, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of LREC,</booktitle>
<pages>2216--2219</pages>
<publisher>Citeseer,</publisher>
<contexts>
<context position="8006" citStr="Nivre et al. 2006" startWordPosition="1243" endWordPosition="1246"> the likelihood of a relation word is taken into consideration. All things being equal, we thus need millions of sentences to create the BKB. Here, two large-scale corpora of English are employed, PukWaC and Wackypedia EN (Baroni et al. 2009). PukWaC is a 2G-token web corpus of British English crawled from the uk domain (Ferraresi et al. 2Subsequent studies may employ less rigid pruning criteria. For the purpose of the current paper, however, the statistical significance of all extracted triples serves as an criterion to evaluate methodological validity. 214 2008), and parsed with MaltParser (Nivre et al. 2006). It is distributed in 5 parts; Only PukWaC1 to PukWaC-4 were considered here, constituting 82.2% (72.5M sentences) of the entire corpus, PukWaC-5 is left untouched for forthcoming evaluation experiments. Wackypedia EN is a 0.8G-token dump of the English Wikipedia, annotated with the same tools. It is distributed in 4 different files; the last portion was left untouched for forthcoming evaluation experiments. The portion analyzed here comprises 33.2M sentences, 75.9% of the corpus. The extraction of events in these corpora uses simple patterns that combine dependency information and part-of-sp</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>J. Nivre, J. Hall, and J. Nilsson. Maltparser: A data-driven parser-generator for dependency parsing. In Proc. of LREC, pages 2216–2219. Citeseer, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>L Robaldo</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The penn discourse treebank 2.0. In</title>
<date>2008</date>
<booktitle>Proc. 6th International Conference on Language Resources and Evaluation (LREC 2008),</booktitle>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="2534" citStr="Prasad et al. 2008" startWordPosition="368" endWordPosition="371">esentational’ relation, e.g., one utterance motivates the reader to accept a claim formulated in another utterance, ibid., p.257), or relations between entities mentioned in the connected discourse segments (anaphoric relations, Webber et al. 2003). Discourse relations can be indicated explicitly by optional cues, e.g., adverbials (e.g., however), conjunctions (e.g., but), or complex phrases (e.g., in contrast to what Peter said a minute ago). Here, these cues are referred to as relation words. Assuming that relation words are associated with specific discourse relations (Knott and Dale 1994; Prasad et al. 2008), the distribution of relation words found between two (types of) events can yield insights into the range of discourse relations possible at this occasion and their respective likeliness. For this purpose, this paper proposes a background knowledge base (BKB) that hosts pairs of events (here heuristically represented by verbs) along with distributional profiles for relation words. The primary data structure of the BKB is a triple where one event (type) is connected with a particular relation word to another event (type). Triples are further augmented with a frequency score (expressing the lik</context>
<context position="11550" citStr="Prasad et al. 2008" startWordPosition="1794" endWordPosition="1797">ation between events. The data also indicates that reproducibility increases with the size of corpora from which a BKB is built. 3.2 Interpretability Any theory of discourse relations would predict that relation words with similar function should have similar distributions, whereas one would expect different distributions for functionally unrelated relation words. These expectations are tested here for three of the most frequent relation words found in the corpora, i.e., but, then and however. But and however can be grouped together under a generalized notion of contrast (Knott and Dale 1994; Prasad et al. 2008); then, on the other hand, indicates a temporal and/or causal relation. Table 2 confirms the expectation that event pairs that are correlated with but tend to show the same correlation with however, but not with then. B: but H: however agreement (%) vs. H vs. T 97.7 62.2 66.9 215 4 Discussion and Outlook This paper described a novel approach towards the unsupervised acquisition of discourse relations, with encouraging preliminary results: Large collections of parsed text are used to assess distributional profiles of relation words that indicate discourse relations that are possible between spe</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber. The penn discourse treebank 2.0. In Proc. 6th International Conference on Language Resources and Evaluation (LREC 2008), Marrakech, Morocco, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Riaz</author>
<author>R Girju</author>
</authors>
<title>Another look at causality: Discovering scenario-specific contingency relationships with no supervision.</title>
<date>2010</date>
<booktitle>In Semantic Computing (ICSC), 2010 IEEE Fourth International Conference on,</booktitle>
<pages>361--368</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="14751" citStr="Riaz and Girju (2010)" startWordPosition="2289" endWordPosition="2292">r languages that differ in their inventory and usage of discourse markers, e.g., German and English (Stede and Schmitz 2000). The approach is language-independent (except for the syntactic extraction patterns), and it does not require manually annotated data. It would thus be easy to create background knowledge bases with relation words for other languages or specific domains – given a sufficient amount of textual data. Related research includes, for example, the unsupervised recognition of causal and temporal relationships, as required, for example, for the recognition of textual entailment. Riaz and Girju (2010) exploit distributional information about pairs of utterances. Unlike approach described here, they are not restricted to adjacent utterances, and do not rely on explicit and recurrent relation words. Their approach can thus be applied to comparably small data sets. However, they are restricted to a specific type of relations whereas here the entire bandwidth of discourse relations that are explicitly realized in a language are covered. Prospectively, both approaches could be combined to compensate their respective weaknesses. Similar observations can be made with respect to Chambers and Juraf</context>
<context position="16024" citStr="Riaz and Girju (2010)" startWordPosition="2494" endWordPosition="2497">y a single discourse relation (narration), and are thus more limited in scope than the approach described here. However, as their approach extends beyond pairs of events to complex event chains, it seems that both approaches provide complementary types of information and their results could also be combined in a fruitful way to achieve a more detailed assessment of discourse relations. The goal of this paper was to evaluate the methdological validity of the approach. It thus represents the basis for further experiments, e.g., with respect to the enrichment the BKB with information provided by Riaz and Girju (2010), Chambers and Jurafsky (2009) and Kasch and Oates (2010). Other directions of subsequent research may include address more elaborate models of events, and the investigation of the relationship between relation words and taxonomies of discourse relations. 216 Acknowledgments This work was supported by a fellowship within the Postdoc program of the German Academic Exchange Service (DAAD). Initial experiments were conducted at the Collaborative Research Center (SFB) 632 “Information Structure” at the University of Potsdam, Germany. I would also like to thank three anonymous reviewers for valuabl</context>
</contexts>
<marker>Riaz, Girju, 2010</marker>
<rawString>M. Riaz and R. Girju. Another look at causality: Discovering scenario-specific contingency relationships with no supervision. In Semantic Computing (ICSC), 2010 IEEE Fourth International Conference on, pages 361–368. IEEE, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stede</author>
<author>B Schmitz</author>
</authors>
<title>Discourse particles and discourse functions.</title>
<date>2000</date>
<journal>Machine translation,</journal>
<volume>15</volume>
<issue>1</issue>
<pages>125--147</pages>
<contexts>
<context position="14254" citStr="Stede and Schmitz 2000" startWordPosition="2212" endWordPosition="2215">urse relations. Another prospective field of application can be seen in NLP applications, where selection preferences for relation words may serve as a cheap replacement for full-fledged discourse parsing. In the Natural Language Understanding domain, the BKB may help to disambiguate or to identify discourse relations between different events; in the context of Machine Translation, it may represent a factor guiding the insertion of relation words, a task that has been found to be problematic for languages that differ in their inventory and usage of discourse markers, e.g., German and English (Stede and Schmitz 2000). The approach is language-independent (except for the syntactic extraction patterns), and it does not require manually annotated data. It would thus be easy to create background knowledge bases with relation words for other languages or specific domains – given a sufficient amount of textual data. Related research includes, for example, the unsupervised recognition of causal and temporal relationships, as required, for example, for the recognition of textual entailment. Riaz and Girju (2010) exploit distributional information about pairs of utterances. Unlike approach described here, they are</context>
</contexts>
<marker>Stede, Schmitz, 2000</marker>
<rawString>M. Stede and B. Schmitz. Discourse particles and discourse functions. Machine translation, 15(1): 125–147, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enric Vallduvi</author>
</authors>
<title>The Informational Component.</title>
<date>1992</date>
<publisher>Garland,</publisher>
<location>New York,</location>
<marker>Vallduvi, 1992</marker>
<rawString>Enric Vallduvi. The Informational Component. Garland, New York, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie L Webber</author>
</authors>
<title>Structure and ostension in the interpretation of discourse deixis.</title>
<date>1991</date>
<journal>Natural Language and Cognitive Processes,</journal>
<volume>2</volume>
<issue>6</issue>
<contexts>
<context position="5473" citStr="Webber 1991" startWordPosition="836" endWordPosition="837">valuate the validity of this approach. 2 Experimental Setup By generalizing over multiple occurrences of the same events (or, more precisely, event types), one can identify preferences of event pairs for one or several relation words. These preferences capture context-invariant characteristics of pairs of events and are thus to considered to reflect a semantic predisposition for a particular discourse relation. Formally, an event is the semantic representation of the meaning conveyed in the utterance. We 1Relations between non-adjacent utterances are constrained by the structure of discourse (Webber 1991), and thus less likely than relations between adjacent utterances. assume that the same event can reoccur in different contexts, we are thus studying relations between types of events. For the experiment described here, events are heuristically identified with the main predicates of a sentence, i.e., non-auxiliar, noncausative, non-modal verbal lexemes that serve as heads of main clauses. The primary data structure of the approach described here is a triple consisting of a source event, a relation word and a target (antecedent) event. These triples are harvested from large syntactically annota</context>
</contexts>
<marker>Webber, 1991</marker>
<rawString>Bonnie L. Webber. Structure and ostension in the interpretation of discourse deixis. Natural Language and Cognitive Processes, 2(6):107–135, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie L Webber</author>
<author>Matthew Stone</author>
<author>Aravind K Joshi</author>
<author>Alistair Knott</author>
</authors>
<title>Anaphora and discourse structure.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>4</volume>
<issue>29</issue>
<pages>587</pages>
<contexts>
<context position="2163" citStr="Webber et al. 2003" startWordPosition="311" endWordPosition="314">r semimanual annotation, and this is part of the motivation of the approach described here. Discourse relations involve different aspects of meaning. This may include factual knowledge about the connected discourse segments (a ‘subjectmatter’ relation, e.g., if one utterance represents the cause for another, Mann and Thompson 1988, p.257), argumentative purposes (a ‘presentational’ relation, e.g., one utterance motivates the reader to accept a claim formulated in another utterance, ibid., p.257), or relations between entities mentioned in the connected discourse segments (anaphoric relations, Webber et al. 2003). Discourse relations can be indicated explicitly by optional cues, e.g., adverbials (e.g., however), conjunctions (e.g., but), or complex phrases (e.g., in contrast to what Peter said a minute ago). Here, these cues are referred to as relation words. Assuming that relation words are associated with specific discourse relations (Knott and Dale 1994; Prasad et al. 2008), the distribution of relation words found between two (types of) events can yield insights into the range of discourse relations possible at this occasion and their respective likeliness. For this purpose, this paper proposes a </context>
</contexts>
<marker>Webber, Stone, Joshi, Knott, 2003</marker>
<rawString>Bonnie L. Webber, Matthew Stone, Aravind K. Joshi, and Alistair Knott. Anaphora and discourse structure. Computational Linguistics, 4(29):545– 587, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z-M Zhou</author>
<author>Y Xu</author>
<author>Z-Y Niu</author>
<author>M Lan</author>
<author>J Su</author>
<author>C L Tan</author>
</authors>
<title>Predicting discourse connectives for implicit discourse relation recognition.</title>
<date>2010</date>
<booktitle>In COLING 2010,</booktitle>
<pages>1507--1514</pages>
<location>Beijing, China,</location>
<contexts>
<context position="12560" citStr="Zhou et al. (2010)" startWordPosition="1958" endWordPosition="1961">e relations, with encouraging preliminary results: Large collections of parsed text are used to assess distributional profiles of relation words that indicate discourse relations that are possible between specific types of events; on this basis, a background knowledge base (BKB) was created that can be used to predict an appropriate discourse marker to connect two utterances with no overt relation word. This information can be used, for example, to facilitate the semiautomated annotation of discourse relations, by pointing out the ‘default’ relation word for a given pair of events. Similarly, Zhou et al. (2010) used a language model to predict discourse markers for implicitly realized discourse relations. As opposed to this shallow, n-gram-based approach, here, the internal structure of utterances is exploited: based on semantic considerations, syntactic patterns have been devised that extract triples of event pairs and relation words. The resulting BKB provides a distributional approximation of the discourse relations that can hold between two specific event types. Both approaches exploit complementary sources of knowledge, and may be combined with each other to achieve a more precise prediction of</context>
</contexts>
<marker>Zhou, Xu, Niu, Lan, Su, Tan, 2010</marker>
<rawString>Z.-M. Zhou, Y. Xu, Z.-Y. Niu, M. Lan, J. Su, and C.L. Tan. Predicting discourse connectives for implicit discourse relation recognition. In COLING 2010, pages 1507–1514, Beijing, China, August 2010.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>