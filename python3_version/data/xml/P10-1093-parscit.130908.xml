<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.987185">
Compositional Matrix-Space Models of Language
</title>
<author confidence="0.997265">
Sebastian Rudolph
</author>
<affiliation confidence="0.913874">
Karlsruhe Institute of Technology
Karlsruhe, Germany
</affiliation>
<email confidence="0.981463">
rudolph@kit.edu
</email>
<note confidence="0.783439">
Eugenie Giesbrecht
FZI Forschungszentrum Informatik
Karlsuhe, Germany
</note>
<email confidence="0.829907">
giesbrecht@fzi.de
</email>
<sectionHeader confidence="0.991911" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998107">
We propose CMSMs, a novel type of
generic compositional models for syntac-
tic and semantic aspects of natural lan-
guage, based on matrix multiplication. We
argue for the structural and cognitive plau-
sibility of this model and show that it is
able to cover and combine various com-
mon compositional NLP approaches rang-
ing from statistical word space models to
symbolic grammar formalisms.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950691176471">
In computational linguistics and information re-
trieval, Vector Space Models (Salton et al., 1975)
and its variations – such as Word Space Models
(Schütze, 1993), Hyperspace Analogue to Lan-
guage (Lund and Burgess, 1996), or Latent Se-
mantic Analysis (Deerwester et al., 1990) – have
become a mainstream paradigm for text represen-
tation. Vector Space Models (VSMs) have been
empirically justified by results from cognitive sci-
ence (Gärdenfors, 2000). They embody the distri-
butional hypothesis of meaning (Firth, 1957), ac-
cording to which the meaning of words is defined
by contexts in which they (co-)occur. Depending
on the specific model employed, these contexts
can be either local (the co-occurring words), or
global (a sentence or a paragraph or the whole doc-
ument). Indeed, VSMs proved to perform well in a
number of tasks requiring computation of seman-
tic relatedness between words, such as synonymy
identification (Landauer and Dumais, 1997), auto-
matic thesaurus construction (Grefenstette, 1994),
semantic priming, and word sense disambiguation
(Padó and Lapata, 2007).
Until recently, little attention has been paid
to the task of modeling more complex concep-
tual structures with such models, which consti-
tutes a crucial barrier for semantic vector models
on the way to model language (Widdows, 2008).
An emerging area of research receiving more and
more attention among the advocates of distribu-
tional models addresses the methods, algorithms,
and evaluation strategies for representing compo-
sitional aspects of language within a VSM frame-
work. This requires novel modeling paradigms,
as most VSMs have been predominantly used
for meaning representation of single words and
the key problem of common bag-of-words-based
VSMs is that word order information and thereby
the structure of the language is lost.
There are approaches under way to work out
a combined framework for meaning representa-
tion using both the advantages of symbolic and
distributional methods. Clark and Pulman (2007)
suggest a conceptual model which unites sym-
bolic and distributional representations by means
of traversing the parse tree of a sentence and ap-
plying a tensor product for combining vectors of
the meanings of words with the vectors of their
roles. The model is further elaborated by Clark et
al. (2008).
To overcome the aforementioned difficulties
with VSMs and work towards a tight integra-
tion of symbolic and distributional approaches,
we propose a Compositional Matrix-Space Model
(CMSM) which employs matrices instead of vec-
tors and makes use of matrix multiplication as the
one and only composition operation.
The paper is structured as follows: We start by
providing the necessary basic notions in linear al-
gebra in Section 2. In Section 3, we give a for-
mal account of the concept of compositionality,
introduce our model, and argue for the plausibil-
ity of CMSMs in the light of structural and cogni-
tive considerations. Section 4 shows how common
VSM approaches to compositionality can be cap-
tured by CMSMs while Section 5 illustrates the
capabilities of our model to likewise cover sym-
bolic approaches. In Section 6, we demonstrate
</bodyText>
<page confidence="0.945432">
907
</page>
<note confidence="0.9452715">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 907–916,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.763126333333333">
how several CMSMs can be combined into one
model. We provide an overview of related work
in Section 7 before we conclude and point out av-
enues for further research in Section 8.
908 type of functions, so-called linear mappings, hav-
tten:
</bodyText>
<equation confidence="0.7832508">
vM = v&apos;) according to
2 Preliminaries
�m
j=1
v&apos;(i) =
</equation>
<bodyText confidence="0.999458357142857">
In this section, we recap some aspects of linear
algebra to the extent needed for our considerations
about CMSMs. For a more thorough treatise we
refer the reader to a linear algebra textbook (such
as Strang (1993)).
Vectors. Given a natural number n, an n-
dimensional vector v over the reals can be seen
as a list (or tuple) containing n real numbers
r1, ... , rn E R, written v = (r1 r2 · · · rn).
Vectors will be denoted by lowercase bold font
letters and we will use the notation v(i) to refer
to the ith entry of vector v. As usual, we write
Rn to denote the set of all n-dimensional vectors
with real entries. Vectors can be added entry-
</bodyText>
<listItem confidence="0.57244125">
wise, i.e., (r1 ··· rn) + (r&apos;1 · · · r&apos;n) = (r1 +
r&apos;1 · · · rn+r&apos;n). Likewise, the entry-wise prod-
uct (also known as Hadamard product) is defined
by (r1 ··· rn) O (r&apos;1 ··· r&apos;n) = (r1 ·r&apos;1 ··· rn ·r&apos;n).
</listItem>
<bodyText confidence="0.66517275">
Matrices. Given two real numbers n, m, an nxm
matrix over the reals is an array of real numbers
with n rows and m columns. We will use capital
letters to denote matrices and, given a matrix M
we will write M(i, j) to refer to the entry in the ith
row and the jth column:
ber entries is denoted by
Obviously, m-
dimensional vectors can be seen as 1 x m matri-
ces. A matrix can be transposed by exchanging
columns and rows: given the n x m matrix M, its
transposed version MT is a m x n matr
</bodyText>
<equation confidence="0.8784064">
Rnxm.
ix defined
by MT (i, j) = M(j, i).
ain
v(j) · M(i, j)
</equation>
<bodyText confidence="0.942804714285714">
tion: we write
to denote the matrix that
corresponds to the linear mapping defined by ap-
plying first
and then M2. Formally, the matrix
product of the n x l matrix
and the l x m matrix
</bodyText>
<equation confidence="0.988104">
is an
M1M2
M1
M1
M2
n x m matrix M = M1M2 defined by
M(i, j) = �l M1(i, k) · M2(k, j)
k=1
</equation>
<bodyText confidence="0.671343333333333">
ing vectors as in- and output. More precisely, an
n x m matrix M applied to an m-dimensional vec-
tor vyields an n-dimensional vector v&apos; (wri
</bodyText>
<figure confidence="0.6465442">
mutation on
n} is a bijection (i.e., a map-
ping that is one-to-one and onto)
... n}
... n}. A permutation can be seen as a
</figure>
<bodyText confidence="0.742251">
on a list with n elements: the element
at position i will get the new position
in the
reordered list. Likewise, a permutation can be ap-
plied to a vector resulting in a rearrangement of
the entries. We write
to denote the permutation
corresponding to the n-fold application of
and
to denote the permutation that
Given a permutation
the corresponding per-
</bodyText>
<equation confidence="0.9952913125">
mutation matrix
{1 ...
Φ:{1
→{1
“reorder-
ingscheme”
Φ(i)
Φn
Φ
Φ_1
“undoes”Φ.
Φ,
MΦ is defined by
1 if
Φ(j) = i,
t0 otherwise.
</equation>
<bodyText confidence="0.99437">
can be expressed in terms of matrix multipli-
cation as well as we obtain for an
</bodyText>
<equation confidence="0.9821202">
Φ
y vector v E Rn:
=
Φ(v)
vMΦ
</equation>
<bodyText confidence="0.940938333333333">
The set of all n x m matrices with real num-
Linear Mappings. Beyond being merely array-
like data structures, matrices correspond to cert
Linear mappings can be concatenated, giving
rise to the notion of standard matrix multiplica-
Note that the matrix product is associative (i.e.,
</bodyText>
<equation confidence="0.928907333333333">
(M1M2)M3 =
always holds, thus
parentheses can
M1(M2M3)
be omitted) but not commutative
(M1M2 = M2M1 does not hold in general, i.e., the
</equation>
<bodyText confidence="0.84277">
order matters).
Permutations. Given a natural number n, a per-
Then, obviously permuting a vector according
to
Likewise, iterated application
and the in-
verses
carry over naturally to the correspond-
ing notions in matri
</bodyText>
<equation confidence="0.993719142857143">
(Φn)
Φ_n
ces.
M(1,1) M(1, 2) · · · M(1, j) · · · M(1, m)
..
...
..
. . �����������������������������������
.
j) . .....
. .
1)
·········
M(2,1) M(2, 2)
M(i, 1) M(i,
M(n,
M(1, 2)
M(n, m)
f
M =
MΦ(i, j) =
</equation>
<sectionHeader confidence="0.891507" genericHeader="introduction">
3 Compositionality and Matrices
</sectionHeader>
<bodyText confidence="0.9999366875">
The underlying principle of compositional seman-
tics is that the meaning of a sentence (or a word
phrase) can be derived from the meaning of its
constituent tokens by applying a composition op-
eration. More formally, the underlying idea can
be described as follows: given a mapping Q · ] :
E → S from a set of tokens (words) E into some
semantical space S (the elements of which we will
simply call “meanings”), we find a semantic com-
position operation ./: S∗ → S mapping sequences
of meanings to meanings such that the meaning of
a sequence of tokens σ1σ2 ... σn can be obtained
by applying ./ to the sequence Qσ1]]Qσ2]]... QQσn]].
This situation qualifies QQ · ] as a homomorphism
between (E∗, ·) and (S, ./) and can be displayed as
follows:
</bodyText>
<equation confidence="0.997678666666667">
concatenation ·
σ1 σ2 ··· σn σ1σ2 ... σn
Q·] QQ·]] QQ·] Q·]
V V V V
Qσ1]] QQσ2]] ··· Qσn] QQσ1σ2 ... σn]
composition ./
</equation>
<bodyText confidence="0.9999817">
A great variety of linguistic models are sub-
sumed by this general idea ranging from purely
symbolic approaches (like type systems and cate-
gorial grammars) to rather statistical models (like
vector space and word space models). At the first
glance, the underlying encodings of word seman-
tics as well as the composition operations differ
significantly. However, we argue that a great vari-
ety of them can be incorporated – and even freely
inter-combined – into a unified model where the
semantics of simple tokens and complex phrases
is expressed by matrices and the composition op-
eration is standard matrix multiplication.
More precisely, in Compositional Marix-Space
Models, we have S = En×n, i.e. the semantical
space consists of quadratic matrices, and the com-
position operator ./ coincides with matrix multi-
plication as introduced in Section 2. In the follow-
ing, we will provide diverse arguments illustrating
that CMSMs are intuitive and natural.
</bodyText>
<subsectionHeader confidence="0.928642">
3.1 Algebraic Plausibility –
Structural Operation Properties
</subsectionHeader>
<bodyText confidence="0.999978071428571">
Most linear-algebra-based operations that have
been proposed to model composition in language
models are associative and commutative. Thereby,
they realize a multiset (or bag-of-words) seman-
tics that makes them insensitive to structural dif-
ferences of phrases conveyed through word order.
While associativity seems somewhat acceptable
and could be defended by pointing to the stream-
like, sequential nature of language, commutativity
seems way less justifiable, arguably.
As mentioned before, matrix multiplication is
associative but non-commutative, whence we pro-
pose it as more adequate for modeling composi-
tional semantics of language.
</bodyText>
<subsectionHeader confidence="0.998309">
3.2 Neurological Plausibility –
Progression of Mental States
</subsectionHeader>
<bodyText confidence="0.999936">
From a very abstract and simplified perspective,
CMSMs can also be justified neurologically.
Suppose the mental state of a person at one spe-
cific moment in time can be encoded by a vector v
of numerical values; one might, e.g., think of the
level of excitation of neurons. Then, an external
stimulus or signal, such as a perceived word, will
result in a change of the mental state. Thus, the
external stimulus can be seen as a function being
applied to v yielding as result the vector v0 that
corresponds to the persons mental state after re-
ceiving the signal. Therefore, it seems sensible to
associate with every signal (in our case: token σ) a
respective function (a linear mapping, represented
by a matrix M = QQσ] that maps mental states to
mental states (i.e. vectors v to vectors v0 = vM).
Consequently, the subsequent reception of in-
puts σ, σ0 associated to matrices M and M0
will transform a mental vector v into the vector
(vM)M0 which by associativity equals v(MM0).
Therefore, MM0 represents the mental state tran-
sition triggered by the signal sequence σσ0. Nat-
urally, this consideration carries over to sequences
of arbitrary length. This way, abstracting from
specific initial mental state vectors, our semantic
space S can be seen as a function space of mental
transformations represented by matrices, whereby
matrix multiplication realizes subsequent execu-
tion of those transformations triggered by the in-
put token sequence.
</bodyText>
<page confidence="0.99205">
909
</page>
<subsectionHeader confidence="0.8471695">
3.3 Psychological Plausibility –
Operations on Working Memory
</subsectionHeader>
<bodyText confidence="0.999832545454545">
A structurally very similar argument can be pro-
vided on another cognitive explanatory level.
There have been extensive studies about human
language processing justifying the hypothesis of
a working memory (Baddeley, 2003). The men-
tal state vector can be seen as representation of a
person’s working memory which gets transformed
by external input. Note that matrices can per-
form standard memory operations such as storing,
deleting, copying etc. For instance, the matrix
Mcopy(k,l) defined by
</bodyText>
<equation confidence="0.927634">
( 1 if i = j # l or i = k, j = l,
Mcopy(k,l)(i, j) =
0 otherwise.
</equation>
<bodyText confidence="0.9996495">
applied to a vector v, will copy its kth entry to the
lth position. This mechanism of storage and inser-
tion can, e.g., be used to simulate simple forms of
anaphora resolution.
</bodyText>
<sectionHeader confidence="0.77151" genericHeader="method">
4 CMSMs Encode Vector Space Models
</sectionHeader>
<bodyText confidence="0.9997472">
In VSMs numerous vector operations have been
used to model composition (Widdows, 2008),
some of the more advanced ones being related to
quantum mechanics. We show how these com-
mon composition operators can be modeled by
CMSMs.1 Given a vector composition operation
./: Rn ×Rn → Rn, we provide a surjective function
ψ./ : Rn → Rn0×n0 that translates the vector rep-
resentation into a matrix representation in a way
such that for all v1, ... vk ∈ Rn holds
</bodyText>
<equation confidence="0.785418">
v1 ./ ... ./ vk = ψ−1
</equation>
<bodyText confidence="0.936225333333333">
./ (ψ./(v1) ... ψ./(vk))
where ψ./(vi)ψ./(vj) denotes matrix multiplication
of the matrices assigned to vi and vj.
</bodyText>
<subsectionHeader confidence="0.981907">
4.1 Vector Addition
</subsectionHeader>
<bodyText confidence="0.99881">
As a simple basic model for semantic composi-
tion, vector addition has been proposed. Thereby,
tokens σ get assigned (usually high-dimensional)
vectors vσ and to obtain a representation of the
meaning of a phrase or a sentence w = σ1 . . . σk,
the vector sum of the vectors associated to the con-
stituent tokens is calculated: vw = Pki=1 vσi .
1In our investigations we will focus on VSM composi-
tion operations which preserve the format (i.e. which yield a
vector of the same dimensionality), as our notion of composi-
tionality requires models that allow for iterated composition.
In particular, this rules out dot product and tensor product.
However the convolution product can be seen as a condensed
version of the tensor product.
This kind of composition operation is subsumed
by CMSMs; suppose in the original model, a token
σ gets assigned the vector vσ, then by defining
</bodyText>
<equation confidence="0.839781">
ψ+(vσ) =
vσ
</equation>
<bodyText confidence="0.998092">
(mapping n-dimensional vectors to (n+1)×(n+1)
matrices), we obtain for a phrase w = σ1 . . . σk
</bodyText>
<equation confidence="0.997655652173913">
ψ−1
+ (ψ+(vσ1) ... ψ+(vσk)) = vσ1 + ... + vσk = vw.
Proof. By induction on k. For k = 1, we have
vw = vσ = ψ−1
+ (ψ+(vσ1)). For k &gt; 1, we have
ψ−1
+ (ψ+(vσ1) ... ψ+(vσk−1)ψ+(vσk))
= ψ−1
+ (ψ+(ψ−1
+ (ψ+(vσ1) ... ψ+(vσk−1)))ψ+(vσk))
i.h. = ψ−1
+ (ψ+(Pk−1
i=1 vσi)ψ+(vσk))
1 ··· 0
... ...
0 1
Pk−1
i=1 vσi(1)· · · Pk−1
i=1 vσi(n)
1 ··· 0
... ...
0 1
Pki=1vσi(1)··· Pki=1vσi(n)
</equation>
<subsectionHeader confidence="0.787483">
4.2 Component-wise Multiplication
</subsectionHeader>
<bodyText confidence="0.999894142857143">
On the other hand, the Hadamard product (also
called entry-wise product, denoted by O) has been
proposed as an alternative way of semantically
composing token vectors.
By using a different encoding into matrices,
CMSMs can simulate this type of composition op-
eration as well. By letting
</bodyText>
<figure confidence="0.609398117647059">
we obtain an n
n matrix representation for which
...
=
0 ... o
=
4.3 Holographic Reduced Representations
Holographic reduced representations as intro-
duced by Plate (1995) can
×
ψ−1
� (ψ�(vσ1)
ψo(vσk))
vσ1
vσk
vw.
be seen as a refinement
</figure>
<footnote confidence="0.92193625">
2The proofs for the respective correspondences for o and
® as well as the permutation-based approach in the following
sections are structurally analog, hence, we will omit them for
space reasons.
</footnote>
<figure confidence="0.994271">
vσ(1) 0 ··· 0
0 vσ(2) ⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
...
···
vσ(n)
0
ψo(vσ) = ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
,
...
0
0
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1 ··· 0
... ...
0 1
0
0
1
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝ ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
=ψ−1
+
0 ...
⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠ ⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
0
1 vσk(1)··· vσk(n)
0
...⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠⎞⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
0
1
1 ··· 0
... ...
0 1
⎛⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
=ψ−1
+
0 ...k vσi q.e.d.2
0⎟⎟⎟=X
1⎠i=1
910
0
f
MΦ
vσ
... ���������������
0
1
</figure>
<bodyText confidence="0.99921925">
of convolution products with the benefit of pre-
serving dimensionality: given two vectors v1, v2 E
Rn, their circular convolution product v1 ~ v2 is
again an n-dimensional vector v3 defined by
</bodyText>
<equation confidence="0.964301666666667">
n− 1
v3(i + 1) = Z v1(k + 1) · v2((i − k mod n) + 1)
k=0
for 0 &lt;— i &lt;— n−1. Now let ψ~(v) be the nxn matrix
M with
M(i, j) = v((j − i mod n) + 1).
</equation>
<bodyText confidence="0.66732">
In the 3-dimensional case, this would result in
</bodyText>
<equation confidence="0.994323666666667">
v(1) v(2) v(3) ����������
v(3) v(1) v(2)
v(2) v(3) v(1)
Then, it can be readily checked that
ψ−1
~ (ψ~(vσ1) ... ψ~(vσk)) = vσ1 ~ ... ~ vσk = vw.
</equation>
<subsectionHeader confidence="0.897259">
4.4 Permutation-based Approaches
</subsectionHeader>
<bodyText confidence="0.999392833333333">
Sahlgren et al. (2008) use permutations on vec-
tors to account for word order. In this approach,
given a token σm occurring in a sentence w =
σ1 . . . σk with predefined “uncontextualized” vec-
tors vσ1 ... vσk, we compute the contextualized
vector vw,m for σm by
</bodyText>
<equation confidence="0.994480333333333">
vw,m = Φ1−m(vσ1) + ... + Φk−m(vσk),
which can be equivalently transformed into
Φ1−m/vσ1 + Φ(... + Φ(vσk−1 + (Φ(vσk))) ...)).
</equation>
<bodyText confidence="0.999915571428572">
Note that the approach is still token-centered, i.e.,
a vector representation of a token is endowed with
contextual representations of surrounding tokens.
Nevertheless, this setting can be transferred to a
CMSM setting by recording the position of the fo-
cused token as an additional parameter. Now, by
assigning every vσ the matrix
</bodyText>
<equation confidence="0.959969333333333">
ψΦ(vσ) =
we observe that for
Mw,m := (M−Φ)m−1ψΦ(vσ1) ... ψΦ(vσk)
we have
Mw,m = f Mk−m 0 ... ��������������� ,
Φ 0
vw,m 1
whence ψ−1�(M− Φ)m−1ψΦ(vσ1) ... ψΦ(vσk)) = vw,m.
Φ
</equation>
<sectionHeader confidence="0.951155" genericHeader="method">
5 CMSMs Encode Symbolic Approaches
</sectionHeader>
<bodyText confidence="0.999978666666667">
Now we will elaborate on symbolic approaches to
language, i.e., discrete grammar formalisms, and
show how they can conveniently be embedded into
CMSMs. This might come as a surprise, as the ap-
parent likeness of CMSMs to vector-space models
may suggest incompatibility to discrete settings.
</bodyText>
<subsectionHeader confidence="0.99056">
5.1 Group Theory
</subsectionHeader>
<bodyText confidence="0.999767733333333">
Group theory and grammar formalisms based on
groups and pre-groups play an important role
in computational linguistics (Dymetman, 1998;
Lambek, 1958). From the perspective of our com-
positionality framework, those approaches employ
a group (or pre-group) (G, ·) as semantical space S
where the group operation (often written as multi-
plication) is used as composition operation ./.
According Cayley’s Theorem (Cayley, 1854),
every group G is isomorphic to a permutation
group on some set S. Hence, assuming finite-
ness of G and consequently S, we can encode
group-based grammar formalisms into CMSMs in
a straightforward way by using permutation matri-
ces of size |S  |x |S |.
</bodyText>
<subsectionHeader confidence="0.991436">
5.2 Regular Languages
</subsectionHeader>
<bodyText confidence="0.999985384615384">
Regular languages constitute a basic type of lan-
guages characterized by a symbolic formalism.
We will show how to select the assignment Q · ]]
for a CMSM such that the matrix associated to a
token sequence exhibits whether this sequence be-
longs to a given regular language, that is if it is
accepted by a given finite state automaton. As
usual (cf. e.g., Hopcroft and Ullman (1979)) we
define a nondeterministic finite automaton A =
(Q, Σ, Δ, QI, QF) with Q = {q0, ... , qn−1} being the
set of states, Σ the input alphabet, Δ c QxΣxQ the
transition relation, and QI and QF being the sets of
initial and final states, respectively.
</bodyText>
<equation confidence="0.99698">
ψ~(v(1) v(2) v(3)) = f
</equation>
<page confidence="0.974031">
911
</page>
<bodyText confidence="0.953415785714286">
Then we assign to every token σ ∈ Σ the n × n
matrix QQσ] = M with
� 1 if (qi, σ, qj) ∈ Δ,
M(i, j) = 0 otherwise.
Hence essentially, the matrix M encodes all state
transitions which can be caused by the input σ.
Likewise, for a word w = σ1 ... σk ∈ Σ∗, the
matrix Mw := Qσ1j... QQσkjwill encode all state
transitions mediated by w. Finally, if we define
vectors vI and vF by
� 1 if qi ∈ QF,
vF(i) = 0 otherwise,
then we find that w is accepted by A exactly if
vIMwvTF ≥ 1.
</bodyText>
<subsectionHeader confidence="0.976909">
5.3 The General Case: Matrix Grammars
</subsectionHeader>
<bodyText confidence="0.9835126">
Motivated by the above findings, we now define a
general notion of matrix grammars as follows:
Definition 1 Let Σ be an alphabet. A matrix
grammar M of degree n is defined as the pair
h Q · ], ACi where Q · ] is a mapping from Σ to n×n
matrices and AC = {hv01, v1, r1i, ... , hv0m, vm, rmi}
with v01,v1,...,v0m,vm ∈ Rn and r1,...,rm ∈ R
is a finite set of acceptance conditions. The lan-
guage generated by M (denoted by L(M)) con-
tains a token sequence σ1 ... σk ∈ Σ∗ exactly if
v0iQQσ1] ... Qσk]vTi ≥ ri for all i ∈ {1, ... , m}. We
will call a language L matricible if L = L(M) for
some matrix grammar M.
Then, the following proposition is a direct con-
sequence from the preceding section.
Proposition 1 Regular languages are matricible.
However, as demonstrated by the subsequent
examples, also many non-regular and even non-
context-free languages are matricible, hinting at
the expressivity of our grammar model.
</bodyText>
<equation confidence="0.986216763157895">
Example 1 We define Mh Q · j, ACi with
Σ = {a, b, c} 0 0 3 0 0 0
3 0 0 0 ��������������� 0 1 0 0 ���������������
0 1 3 0 ��������������� 0 3 0
Qbj= 0 1 Qaj= 0 0 1
0 1 0 1 0 0 0 0
f 1 0 0 1 0 1 0 0 ���������������
AC = { h(0 3 2 3 0
h(0 0 0 0 1
Qc] = 0 0), 0i,
0 0 0), 0i}
f 2
1), (1 −1
1),(−1 1
Then L(M) contains exactly all palindromes from
{a, b, c}∗, i.e., the words d1d2 ... dn−1dn for which
d1d2 ... dn−1dn = dndn−1 ... d2d1.
Example 2 We define M = h QQ · ], ACi with
�����1 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
�������������
0 0 0 2 0 0
0 0 0 0 1 0
0 0 0 0 0 1
�����0 0 0 0 0 0
0 0 1 0 0 0
0 0 1 0 0 0
������������� ������������������
0 0 0 1 0 0
0 0 0 0 1 0
0 0 0 0 0 2
0 0), (0 0 1 0 0 0), 1i,
1 0),(0 0 0 1 −1 0), 0i,
1 1),(0 0 0 0 1 −1),0i,
1 0),(0 0 0 −1 0 1), 0i}
Then L(M) is the (non-context-free) language
{ambmcm  |m &gt; 0}.
</equation>
<bodyText confidence="0.998694625">
The following properties of matrix grammars
and matricible language are straightforward.
Proposition 2 All languages characterized by a
set of linear equations on the letter counts are ma-
tricible.
Proof. Suppose Σ = {a1,... an}. Given a word w,
let xi denote the number of occurrences of ai in w.
A linear equation on the letter counts has the form
</bodyText>
<equation confidence="0.9451065">
�k, k1, ... , kn ∈ R�
k1x1 + ... + knxn = k
</equation>
<bodyText confidence="0.999046714285714">
Now define QQaij= ψ+(ei), where ei is the ith
unit vector, i.e. it contains a 1 at he ith position and
0 in all other positions. Then, it is easy to see that
w will be mapped to M = ψ+(x1 · · · xn). Due
to the fact that en+1M = (x1 · · · xn 1) we can
enforce the above linear equation by defining the
acceptance conditions
</bodyText>
<equation confidence="0.816133">
AC = { hen+1, (k1 ... kn − k), 0i,
h−en+1, (k1 ... kn − k), 0i}.
</equation>
<bodyText confidence="0.922249375">
q.e.d.
Proposition 3 The intersection of two matricible
languages is again a matricible language.
Proof. This is a direct consequence of the con-
siderations in Section 6 together with the observa-
tion, that the new set of acceptance conditions is
trivially obtained from the old ones with adapted
dimensionalities. q.e.d.
</bodyText>
<figure confidence="0.986210833333334">
_ ( 1 if qi ∈ QI,
vI(i) t 0 otherwise,
0 1 0 0 0
0 1 0 0 0
0 0 0 0 0
QQb]= 0 0 1 0
0
f
0 0 0 0 2
0 0 0 0 0
AC = { h(1 0 0 0
h(0 0 0 1
h(0 0 0 0
h(0 0 0 1
Σ = {a, b, c} Qaj=
1
Qc]=
0
0
0
0
0
1
1
</figure>
<page confidence="0.993601">
912
</page>
<bodyText confidence="0.9999487">
Note that the fact that the language {ambmcm |
m &gt; 0} is matricible, as demonstrated in Ex-
ample 2 is a straightforward consequence of the
Propositions 1, 2, and 3, since the language in
question can be described as the intersection of
the regular language a+b+c+ with the language
characterized by the equations xa − xb = 0 and
xb − xc = 0. We proceed by giving another ac-
count of the expressivity of matrix grammars by
showing undecidability of the emptiness problem.
</bodyText>
<figureCaption confidence="0.469814666666667">
Proposition 4 The problem whether there is a
word which is accepted by a given matrix gram-
mar is undecidable.
</figureCaption>
<bodyText confidence="0.9788875">
Proof. The undecidable Post correspondence
problem (Post, 1946) is described as follows:
given two lists of words u1, ... , un and v1, ... , vn
over some alphabet E0, is there a sequence of num-
bers h1, ... , hm (1 ≤ hj ≤ n) such that uh1 ... uhm =
vh1 ... vhm?
We now reduce this problem to the emptiness
problem of a matrix grammar. W.l.o.g., let E0 =
{a1, ... , ak}. We define a bijection # from E0∗ to N
by
</bodyText>
<equation confidence="0.9505961">
#(an1an2 ... anl) =
Note that this is indeed a bijection and that for
w1, w2 ∈ E0∗, we have
#(w1w2) = #(w1) · k|w2 |+ #(w2).
Now, we define M as follows:
k|ui |0 0 ����������
0 k|vi |0
#(ui) #(vi) 1
AC = { h(0 0 1), (1 − 1 0), 0i,
h(0 0 1),(−1 1 0), 0i}
</equation>
<bodyText confidence="0.854663">
Using the above fact about # and a simple induc-
tion on m, we find that
</bodyText>
<equation confidence="0.956967666666667">
k|uh1...uhm |0 0
QQah1] ... Qahm] = 0 k|vh1...vhm |0
#(uh1. . .uhm) #(vh1. . .vhm) 1
</equation>
<bodyText confidence="0.9978706875">
Evaluating the two acceptance conditions, we
find them satisfied exactly if #(uh1 ... uhm) =
#(vh1 ... vhm). Since # is a bijection, this is the
case if and only if uh1 ... uhm = vh1 ... vhm. There-
fore M accepts bh1 ... bhm exactly if the sequence
h1, ... , hm is a solution to the given Post Corre-
spondence Problem. Consequently, the question
whether such a solution exists is equivalent to
the question whether the language L(M) is non-
empty. q.e.d.
These results demonstrate that matrix grammars
cover a wide range of formal languages. Never-
theless some important questions remain open and
need to be clarified next:
Are all context-free languages matricible? We
conjecture that this is not the case.3 Note that this
question is directly related to the question whether
Lambek calculus can be modeled by matrix gram-
mars.
Are matricible languages closed under concatena-
tion? That is: given two arbitrary matricible lan-
guages L1, L2, is the language L = {w1w2  |w1 ∈
L1, w2 ∈ L2} again matricible? Being a property
common to all language types from the Chomsky
hierarchy, answering this question is surprisingly
non-trivial for matrix grammars.
In case of a negative answer to one of the above
questions it might be worthwhile to introduce an
extended notion of context grammars to accom-
modate those desirable properties. For example,
allowing for some nondeterminism by associating
several matrices to one token would ensure closure
under concatenation.
How do the theoretical properties of matrix gram-
mars depend on the underlying algebraic struc-
ture? Remember that we considered matrices con-
taining real numbers as entries. In general, ma-
trices can be defined on top of any mathemati-
cal structure that is (at least) a semiring (Golan,
1992). Examples for semirings are the natural
numbers, boolean algebras, or polynomials with
natural number coefficients. Therefore, it would
be interesting to investigate the influence of the
choice of the underlying semiring on the prop-
erties of the matrix grammars – possibly non-
standard structures turn out to be more appropri-
ate for capturing certain compositional language
properties.
</bodyText>
<sectionHeader confidence="0.863027" genericHeader="method">
6 Combination of Different Approaches
</sectionHeader>
<bodyText confidence="0.968551666666667">
Another central advantage of the proposed matrix-
based models for word meaning is that several
matrix models can be easily combined into one.
</bodyText>
<footnote confidence="0.990320666666667">
3For instance, we have not been able to find a matrix
grammar that recognizes the language of all well-formed
parenthesis expressions.
</footnote>
<equation confidence="0.99970325">
(ni − 1) · k(l−i)
�l
i=1
E = {b1,...bn} QQbi]] = f
</equation>
<page confidence="0.995629">
913
</page>
<bodyText confidence="0.727460857142857">
Again assume a sequence w = σ1 ... σk of
tokens with associated matrices EEσ1]], . . . , Eσk]
according to one specific model and matrices
(Eσ1]), . . . ,
(Eσk]) according to another.
Then we can combine the two models into one
{E · ]} by assigning to σi the matrix
</bodyText>
<figure confidence="0.9778675">
f Eσi] 0 ··· 0
... ... 0
0 1
{Eσi]} = ··· 0 (Eσi])
0 ... 0
...
0
By doing so, we obtain the correspondence
EEσ1]... Eσk]
(Eσ1]) ... (Eσk])
</figure>
<bodyText confidence="0.999435">
In other words, the semantic compositions belong-
ing to two CMSMs can be executed “in parallel.”
Mark that by providing non-zero entries for the up-
per right and lower left matrix part, information
exchange between the two models can be easily
realized.
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999986476190477">
We are not the first to suggest an extension of
classical VSMs to matrices. Distributional mod-
els based on matrices or even higher-dimensional
arrays have been proposed in information retrieval
(Gao et al., 2004; Antonellis and Gallopoulos,
2006). However, to the best of our knowledge, the
approach of realizing compositionality via matrix
multiplication seems to be entirely original.
Among the early attempts to provide more com-
pelling combinatory functions to capture word or-
der information and the non-commutativity of lin-
guistic compositional operation in VSMs is the
work of Kintsch (2001) who is using a more so-
phisticated addition function to model predicate-
argument structures in VSMs.
Mitchell and Lapata (2008) formulate seman-
tic composition as a function m = f (w1, w2, R, K)
where R is a relation between w1 and w2 and K
is additional knowledge. They evaluate the model
with a number of addition and multiplication op-
erations for vector combination on a sentence sim-
ilarity task proposed by Kintsch (2001). Widdows
(2008) proposes a number of more advanced vec-
tor operations well-known from quantum mechan-
ics, such as tensor product and convolution, to
model composition in vector spaces. He shows
the ability of VSMs to reflect the relational and
phrasal meanings on a simplified analogy task.
Giesbrecht (2009) evaluates four vector compo-
sition operations (+, 0, tensor product, convolu-
tion) on the task of identifying multi-word units.
The evaluation results of the three studies are not
conclusive in terms of which vector operation per-
forms best; the different outcomes might be at-
tributed to the underlying word space models; e.g.,
the models of Widdows (2008) and Giesbrecht
(2009) feature dimensionality reduction while that
of Mitchell and Lapata (2008) does not. In the
light of these findings, our CMSMs provide the
benefit of just one composition operation that is
able to mimic all the others as well as combina-
tions thereof.
</bodyText>
<sectionHeader confidence="0.98248" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999453538461538">
We have introduced a generic model for compo-
sitionality in language where matrices are associ-
ated with tokens and the matrix representation of a
token sequence is obtained by iterated matrix mul-
tiplication. We have given algebraic, neurological,
and psychological plausibility indications in favor
of this choice. We have shown that the proposed
model is expressive enough to cover and combine
a variety of distributional and symbolic aspects of
natural language. This nourishes the hope that ma-
trix models can serve as a kind of lingua franca for
compositional models.
This having said, some crucial questions remain
before CMSMs can be applied in practice:
How to acquire CMSMs for large token sets and
specific purposes? We have shown the value
and expressivity of CMSMs by providing care-
fully hand-crafted encodings. In practical cases,
however, the number of token-to-matrix assign-
ments will be too large for this manual approach.
Therefore, methods to (semi-)automatically ac-
quire those assignments from available data are re-
quired. To this end, machine learning techniques
need to be investigated with respect to their ap-
plicability to this task. Presumably, hybrid ap-
proaches have to be considered, where parts of
</bodyText>
<figure confidence="0.99762175">
{Eσ1]} ...{Eσk]} = �������������������������������
0 ··· 0
... ...
0 0
0 ··· 0
... ...
0 0
1
</figure>
<page confidence="0.99383">
914
</page>
<bodyText confidence="0.998380176470588">
the matrix representation are learned whereas oth-
ers are stipulated in advance guided by external
sources (such as lexical information).
In this setting, data sparsity may be overcome
through tensor methods: given a set T of tokens
together with the matrix assignment [[]] : T -�
Enxn, this datastructure can be conceived as a 3-
dimensional array (also known as tensor) of size
nxnx|T |wherein the single token-matrices can be
found as slices. Then tensor decomposition tech-
niques can be applied in order to find a compact
representation, reduce noise, and cluster together
similar tokens (Tucker, 1966; Rendle et al., 2009).
First evaluation results employing this approach to
the task of free associations are reported by Gies-
brecht (2010).
How does linearity limit the applicability of
CMSMs? In Section 3, we justified our model by
taking the perspective of tokens being functions
which realize mental state transitions. Yet, us-
ing matrices to represent those functions restricts
them to linear mappings. Although this restric-
tion brings about benefits in terms of computabil-
ity and theoretical accessibility, the limitations in-
troduced by this assumption need to be investi-
gated. Clearly, certain linguistic effects (like a-
posteriori disambiguation) cannot be modeled via
linear mappings. Instead, we might need some
in-between application of simple nonlinear func-
tions in the spirit of quantum-collapsing of a &amp;quot;su-
perposed&amp;quot; mental state (such as the winner takes
it all, survival of the top-k vector entries, and so
forth). Thus, another avenue of further research is
to generalize from the linear approach.
</bodyText>
<sectionHeader confidence="0.996522" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998694">
This work was supported by the German Research
Foundation (DFG) under the Multipla project
(grant 38457858) as well as by the German Fed-
eral Ministry of Economics (BMWi) under the
project Theseus (number 01MQ07019).
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999790830508474">
[Antonellis and Gallopoulos2006] Ioannis Antonellis
and Efstratios Gallopoulos. 2006. Exploring
term-document matrices from matrix models in text
mining. CoRR, abs/cs/0602076.
[Baddeley2003] Alan D. Baddeley. 2003. Working
memory and language: An overview. Journal of
Communication Disorder, 36:198–208.
[Cayley1854] Arthur Cayley. 1854. On the theory of
groups as depending on the symbolic equation 0n _
1. Philos. Magazine, 7:40–47.
[Clark and Pulman2007] Stephen Clark and Stephen
Pulman. 2007. Combining symbolic and distribu-
tional models of meaning. In Proceedings of the
AAAI Spring Symposium on Quantum Interaction,
Stanford, CA, 2007, pages 52–55.
[Clark et al.2008] Stephen Clark, Bob Coecke, and
Mehrnoosh Sadrzadeh. 2008. A compositional dis-
tributional model of meaning. In Proceedings of
the Second Symposium on Quantum Interaction (QI-
2008), pages 133–140.
[Deerwester et al.1990] Scott Deerwester, Susan T. Du-
mais, George W. Furnas, Thomas K. Landauer, and
Richard Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society
for Information Science, 41:391–407.
[Dymetman1998] Marc Dymetman. 1998. Group the-
ory and computational linguistics. J. of Logic, Lang.
and Inf., 7(4):461–497.
[Firth1957] John R. Firth. 1957. A synopsis of linguis-
tic theory 1930-55. Studies in linguistic analysis,
pages 1–32.
[Gao et al.2004] Kai Gao, Yongcheng Wang, and Zhiqi
Wang. 2004. An efficient relevant evaluation model
in information retrieval and its application. In CIT
’04: Proceedings of the The Fourth International
Conference on Computer and Information Technol-
ogy, pages 845–850. IEEE Computer Society.
[Gärdenfors2000] Peter Gärdenfors. 2000. Concep-
tual Spaces: The Geometry of Thought. MIT Press,
Cambridge, MA, USA.
[Giesbrecht2009] Eugenie Giesbrecht. 2009. In search
of semantic compositionality in vector spaces. In
Sebastian Rudolph, Frithjof Dau, and Sergei O.
Kuznetsov, editors, ICCS, volume 5662 of Lec-
ture Notes in Computer Science, pages 173–184.
Springer.
[Giesbrecht2010] Eugenie Giesbrecht. 2010. Towards
a matrix-based distributional model of meaning. In
Proceedings of Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Student Research Workshop. ACL.
[Golan1992] Jonathan S. Golan. 1992. The theory of
semirings with applications in mathematics and the-
oretical computer science. Addison-Wesley Long-
man Ltd.
[Grefenstette1994] Gregory Grefenstette. 1994. Ex-
plorations in Automatic Thesaurus Discovery.
Springer.
</reference>
<page confidence="0.984196">
915
</page>
<reference confidence="0.999552096774194">
[Hopcroft and Ullman1979] John E. Hopcroft and Jef-
frey D. Ullman. 1979. Introduction to Automata
Theory, Languages and Computation. Addison-
Wesley.
[Kintsch2001] Walter Kintsch. 2001. Predication.
Cognitive Science, 25:173–202.
[Lambek1958] Joachim Lambek. 1958. The mathe-
matics of sentence structure. The American Math-
ematical Monthly, 65(3):154–170.
[Landauer and Dumais1997] Thomas K. Landauer and
Susan T. Dumais. 1997. Solution to Plato’s prob-
lem: The latent semantic analysis theory of acqui-
sition, induction and representation of knowledge.
Psychological Review, (104).
[Lund and Burgess1996] Kevin Lund and Curt Burgess.
1996. Producing high-dimensional semantic spaces
from lexical co-occurrence. Behavior Research
Methods, Instrumentation, and Computers, 28:203–
208.
[Mitchell and Lapata2008] Jeff Mitchell and Mirella
Lapata. 2008. Vector-based models of seman-
tic composition. In Proceedings of ACL-08: HLT,
pages 236–244. ACL.
[Padó and Lapata2007] Sebastian Padó and Mirella La-
pata. 2007. Dependency-based construction of se-
mantic space models. Computational Linguistics,
33(2):161–199.
[Plate1995] Tony Plate. 1995. Holographic reduced
representations. IEEE Transactions on Neural Net-
works, 6(3):623–641.
[Post1946] Emil L. Post. 1946. A variant of a recur-
sively unsolvable problem. Bulletin of the American
Mathematical Society, 52:264–268.
[Rendle et al.2009] Steffen Rendle, Leandro Balby
Marinho, Alexandros Nanopoulos, and Lars
Schmidt-Thieme. 2009. Learning optimal ranking
with tensor factorization for tag recommendation.
In John F. Elder IV, Françoise Fogelman-Soulié,
Peter A. Flach, and Mohammed Javeed Zaki,
editors, KDD, pages 727–736. ACM.
[Sahlgren et al.2008] Magnus Sahlgren, Anders Holst,
and Pentti Kanerva. 2008. Permutations as a means
to encode order in word space. In Proc. CogSci’08,
pages 1300–1305.
[Salton et al.1975] Gerard Salton, Anita Wong, and
Chung-Shu Yang. 1975. A vector space model for
automatic indexing. Commun. ACM, 18(11):613–
620.
[Schütze1993] Hinrich Schütze. 1993. Word space.
In Lee C. Giles, Stephen J. Hanson, and Jack D.
Cowan, editors, Advances in Neural Information
Processing Systems 5, pages 895–902. Morgan-
Kaufmann.
[Strang1993] Gilbert Strang. 1993. Introduction to
Linear Algebra. Wellesley-Cambridge Press.
[Tucker1966] Ledyard R. Tucker. 1966. Some math-
ematical notes on three-mode factor analysis. Psy-
chometrika, 31(3).
[Widdows2008] Dominic Widdows. 2008. Semantic
vector products: some initial investigations. In Pro-
ceedings of the Second AAAI Symposium on Quan-
tum Interaction.
</reference>
<page confidence="0.998557">
916
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.846450">
<title confidence="0.999396">Compositional Matrix-Space Models of Language</title>
<author confidence="0.999944">Sebastian Rudolph</author>
<affiliation confidence="0.999993">Karlsruhe Institute of Technology</affiliation>
<address confidence="0.994259">Karlsruhe, Germany</address>
<email confidence="0.999923">rudolph@kit.edu</email>
<author confidence="0.995254">Eugenie Giesbrecht</author>
<affiliation confidence="0.882568">FZI Forschungszentrum Informatik</affiliation>
<address confidence="0.926211">Karlsuhe, Germany</address>
<email confidence="0.999496">giesbrecht@fzi.de</email>
<abstract confidence="0.998397">We propose CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication. We argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional NLP approaches ranging from statistical word space models to symbolic grammar formalisms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ioannis Antonellis</author>
<author>Efstratios Gallopoulos</author>
</authors>
<title>Exploring term-document matrices from matrix models in text mining.</title>
<date>2006</date>
<location>CoRR, abs/cs/0602076.</location>
<marker>[Antonellis and Gallopoulos2006]</marker>
<rawString>Ioannis Antonellis and Efstratios Gallopoulos. 2006. Exploring term-document matrices from matrix models in text mining. CoRR, abs/cs/0602076.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan D Baddeley</author>
</authors>
<title>Working memory and language: An overview.</title>
<date>2003</date>
<journal>Journal of Communication Disorder,</journal>
<pages>36--198</pages>
<marker>[Baddeley2003]</marker>
<rawString>Alan D. Baddeley. 2003. Working memory and language: An overview. Journal of Communication Disorder, 36:198–208.</rawString>
</citation>
<citation valid="false">
<title>On the theory of groups as depending on the symbolic equation 0n _ 1.</title>
<tech>Philos. Magazine,</tech>
<pages>7--40</pages>
<marker>[Cayley1854]</marker>
<rawString>Arthur Cayley. 1854. On the theory of groups as depending on the symbolic equation 0n _ 1. Philos. Magazine, 7:40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Stephen Pulman</author>
</authors>
<title>Combining symbolic and distributional models of meaning.</title>
<date>2007</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Quantum Interaction,</booktitle>
<pages>52--55</pages>
<location>Stanford, CA,</location>
<marker>[Clark and Pulman2007]</marker>
<rawString>Stephen Clark and Stephen Pulman. 2007. Combining symbolic and distributional models of meaning. In Proceedings of the AAAI Spring Symposium on Quantum Interaction, Stanford, CA, 2007, pages 52–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>A compositional distributional model of meaning.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Symposium on Quantum Interaction (QI2008),</booktitle>
<pages>133--140</pages>
<marker>[Clark et al.2008]</marker>
<rawString>Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh. 2008. A compositional distributional model of meaning. In Proceedings of the Second Symposium on Quantum Interaction (QI2008), pages 133–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--391</pages>
<marker>[Deerwester et al.1990]</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41:391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Dymetman</author>
</authors>
<title>Group theory and computational linguistics.</title>
<date>1998</date>
<journal>J. of Logic, Lang. and Inf.,</journal>
<volume>7</volume>
<issue>4</issue>
<marker>[Dymetman1998]</marker>
<rawString>Marc Dymetman. 1998. Group theory and computational linguistics. J. of Logic, Lang. and Inf., 7(4):461–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-55. Studies in linguistic analysis,</title>
<date>1957</date>
<pages>1--32</pages>
<marker>[Firth1957]</marker>
<rawString>John R. Firth. 1957. A synopsis of linguistic theory 1930-55. Studies in linguistic analysis, pages 1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Gao</author>
<author>Yongcheng Wang</author>
<author>Zhiqi Wang</author>
</authors>
<title>An efficient relevant evaluation model in information retrieval and its application.</title>
<date>2004</date>
<booktitle>In CIT ’04: Proceedings of the The Fourth International Conference on Computer and Information Technology,</booktitle>
<pages>845--850</pages>
<publisher>IEEE Computer Society.</publisher>
<marker>[Gao et al.2004]</marker>
<rawString>Kai Gao, Yongcheng Wang, and Zhiqi Wang. 2004. An efficient relevant evaluation model in information retrieval and its application. In CIT ’04: Proceedings of the The Fourth International Conference on Computer and Information Technology, pages 845–850. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Gärdenfors</author>
</authors>
<title>Conceptual Spaces: The Geometry of Thought.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>[Gärdenfors2000]</marker>
<rawString>Peter Gärdenfors. 2000. Conceptual Spaces: The Geometry of Thought. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugenie Giesbrecht</author>
</authors>
<title>In search of semantic compositionality in vector spaces.</title>
<date>2009</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>5662</volume>
<pages>173--184</pages>
<editor>In Sebastian Rudolph, Frithjof Dau, and Sergei O. Kuznetsov, editors, ICCS,</editor>
<publisher>Springer.</publisher>
<marker>[Giesbrecht2009]</marker>
<rawString>Eugenie Giesbrecht. 2009. In search of semantic compositionality in vector spaces. In Sebastian Rudolph, Frithjof Dau, and Sergei O. Kuznetsov, editors, ICCS, volume 5662 of Lecture Notes in Computer Science, pages 173–184. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Towards a matrix-based distributional model of meaning.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Student Research Workshop. ACL.</booktitle>
<marker>[Giesbrecht2010]</marker>
<rawString>Eugenie Giesbrecht. 2010. Towards a matrix-based distributional model of meaning. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Student Research Workshop. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan S Golan</author>
</authors>
<title>The theory of semirings with applications in mathematics and theoretical computer science.</title>
<date>1992</date>
<publisher>Addison-Wesley Longman Ltd.</publisher>
<marker>[Golan1992]</marker>
<rawString>Jonathan S. Golan. 1992. The theory of semirings with applications in mathematics and theoretical computer science. Addison-Wesley Longman Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Springer.</publisher>
<marker>[Grefenstette1994]</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hopcroft</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages and Computation.</title>
<date>1979</date>
<publisher>AddisonWesley.</publisher>
<marker>[Hopcroft and Ullman1979]</marker>
<rawString>John E. Hopcroft and Jeffrey D. Ullman. 1979. Introduction to Automata Theory, Languages and Computation. AddisonWesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<pages>25--173</pages>
<marker>[Kintsch2001]</marker>
<rawString>Walter Kintsch. 2001. Predication. Cognitive Science, 25:173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Lambek</author>
</authors>
<title>The mathematics of sentence structure.</title>
<date>1958</date>
<journal>The American Mathematical Monthly,</journal>
<volume>65</volume>
<issue>3</issue>
<marker>[Lambek1958]</marker>
<rawString>Joachim Lambek. 1958. The mathematics of sentence structure. The American Mathematical Monthly, 65(3):154–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>Solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review,</title>
<date>1997</date>
<marker>[Landauer and Dumais1997]</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. Solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, (104).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical co-occurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instrumentation, and Computers,</journal>
<volume>28</volume>
<pages>208</pages>
<marker>[Lund and Burgess1996]</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instrumentation, and Computers, 28:203– 208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<publisher>ACL.</publisher>
<marker>[Mitchell and Lapata2008]</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Padó</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>[Padó and Lapata2007]</marker>
<rawString>Sebastian Padó and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Plate</author>
</authors>
<title>Holographic reduced representations.</title>
<date>1995</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>6</volume>
<issue>3</issue>
<marker>[Plate1995]</marker>
<rawString>Tony Plate. 1995. Holographic reduced representations. IEEE Transactions on Neural Networks, 6(3):623–641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emil L Post</author>
</authors>
<title>A variant of a recursively unsolvable problem.</title>
<date>1946</date>
<journal>Bulletin of the American Mathematical Society,</journal>
<pages>52--264</pages>
<marker>[Post1946]</marker>
<rawString>Emil L. Post. 1946. A variant of a recursively unsolvable problem. Bulletin of the American Mathematical Society, 52:264–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Rendle</author>
</authors>
<title>Leandro Balby Marinho, Alexandros Nanopoulos, and Lars Schmidt-Thieme.</title>
<date>2009</date>
<pages>727--736</pages>
<editor>John F. Elder IV, Françoise Fogelman-Soulié, Peter A. Flach, and Mohammed Javeed Zaki, editors, KDD,</editor>
<publisher>ACM.</publisher>
<marker>[Rendle et al.2009]</marker>
<rawString>Steffen Rendle, Leandro Balby Marinho, Alexandros Nanopoulos, and Lars Schmidt-Thieme. 2009. Learning optimal ranking with tensor factorization for tag recommendation. In John F. Elder IV, Françoise Fogelman-Soulié, Peter A. Flach, and Mohammed Javeed Zaki, editors, KDD, pages 727–736. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
<author>Anders Holst</author>
<author>Pentti Kanerva</author>
</authors>
<title>Permutations as a means to encode order in word space.</title>
<date>2008</date>
<booktitle>In Proc. CogSci’08,</booktitle>
<pages>1300--1305</pages>
<marker>[Sahlgren et al.2008]</marker>
<rawString>Magnus Sahlgren, Anders Holst, and Pentti Kanerva. 2008. Permutations as a means to encode order in word space. In Proc. CogSci’08, pages 1300–1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Anita Wong</author>
<author>Chung-Shu Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Commun. ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<pages>620</pages>
<marker>[Salton et al.1975]</marker>
<rawString>Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975. A vector space model for automatic indexing. Commun. ACM, 18(11):613– 620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schütze</author>
</authors>
<title>Word space. In</title>
<date>1993</date>
<booktitle>Advances in Neural Information Processing Systems 5,</booktitle>
<pages>895--902</pages>
<editor>Lee C. Giles, Stephen J. Hanson, and Jack D. Cowan, editors,</editor>
<publisher>MorganKaufmann.</publisher>
<marker>[Schütze1993]</marker>
<rawString>Hinrich Schütze. 1993. Word space. In Lee C. Giles, Stephen J. Hanson, and Jack D. Cowan, editors, Advances in Neural Information Processing Systems 5, pages 895–902. MorganKaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilbert Strang</author>
</authors>
<title>Introduction to Linear Algebra.</title>
<date>1993</date>
<publisher>Wellesley-Cambridge Press.</publisher>
<marker>[Strang1993]</marker>
<rawString>Gilbert Strang. 1993. Introduction to Linear Algebra. Wellesley-Cambridge Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ledyard R Tucker</author>
</authors>
<title>Some mathematical notes on three-mode factor analysis.</title>
<date>1966</date>
<journal>Psychometrika,</journal>
<volume>31</volume>
<issue>3</issue>
<marker>[Tucker1966]</marker>
<rawString>Ledyard R. Tucker. 1966. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Semantic vector products: some initial investigations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second AAAI Symposium on Quantum Interaction.</booktitle>
<marker>[Widdows2008]</marker>
<rawString>Dominic Widdows. 2008. Semantic vector products: some initial investigations. In Proceedings of the Second AAAI Symposium on Quantum Interaction.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>