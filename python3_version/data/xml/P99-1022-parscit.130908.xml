<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9989465">
Dynamic Nonlocal Language Modeling via
Hierarchical Topic-Based Adaptation
</title>
<author confidence="0.989852">
Radu Florian and David Yarowsky
</author>
<affiliation confidence="0.94534">
Computer Science Department and Center for Language and Speech Processing,
Johns Hopkins University
</affiliation>
<address confidence="0.717041">
Baltimore, Maryland 21218
</address>
<email confidence="0.726696">
{ rflorian,yarowsky}Ocs.jhu.edu
</email>
<sectionHeader confidence="0.978276" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996368111111111">
This paper presents a novel method of generating
and applying hierarchical, dynamic topic-based lan-
guage models. It proposes and evaluates new clus-
ter generation, hierarchical smoothing and adaptive
topic-probability estimation techniques. These com-
bined models help capture long-distance lexical de-
pendencies. Experiments on the Broadcast News
corpus show significant improvement in perplexity
(10.5% overall and 33.5% on target vocabulary).
</bodyText>
<sectionHeader confidence="0.997948" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9655842">
Statistical language models are core components of
speech recognizers, optical character recognizers and
even some machine translation systems Brown et
al. (1990). The most common language model-
ing paradigm used today is based on n-grams, local
word sequences. These models make a Markovian
assumption on word dependencies; usually that word
predictions depend on at most m previous words.
Therefore they offer the following approximation for
the computation of a word sequence probability:
</bodyText>
<equation confidence="0.615239">
P (wf) = P (wiltv::;.+1)
</equation>
<bodyText confidence="0.998401476190476">
where 94 denotes the sequence wi wi ; a common
size for m is 3 (trigram language models).
Even if n-grams were proved to be very power-
ful and robust in various tasks involving language
models, they have a certain handicap: because of
the Markov assumption, the dependency is limited
to very short local context. Cache language models
(Kuhn and de Mori (1992),Rosenfeld (1994)) try to
overcome this limitation by boosting the probabil-
ity of the words already seen in the history; trigger
models (Lau et al. (1993)), even more general, try to
capture the interrelationships between words. Mod-
els based on syntactic structure (Chelba and Jelinek
(1998), Wright et al. (1993)) effectively estimate
intra-sentence syntactic word dependencies.
The approach we present here is based on the
observation that certain words tend to have differ-
ent probability distributions in different topics. We
propose to compute the conditional language model
probability as a dynamic mixture model of K topic-
specific language models:
</bodyText>
<figure confidence="0.2704215">
Empirical Observation:
Lexical Probabilities are Sensitive to Topic and Subtopic
P( peacelsubtopie)
I Malor Topics and BS Subtopics has the Insadust News Comm
</figure>
<figureCaption confidence="0.997258">
Figure 1: Conditional probability of the word peace
given manually assigned Broadcast News topics
</figureCaption>
<equation confidence="0.9996415">
P (wiltut-1) = &gt; P (414-1) • P (wilt,wt-1)
t=1
P (t1w-1) P (111-1 -F
(1E1 t .1ws-m1)
</equation>
<bodyText confidence="0.966389071428572">
t=l )
The motivation for developing topic-sensitive lan-
guage models is twofold. First, empirically speaking,
many n-gram probabilities vary substantially when
conditioned on topic (such as in the case of content
words following several function words). A more im-
portant benefit, however, is that even when a given
bigram or trigram probability is not topic sensitive,
as in the case of sparse n-gram statistics, the topic-
sensitive unigram or bigram probabilities may con-
stitute a more informative backoff estimate than the
single global unigram or bigram estimates. Discus-
sion of these important smoothing issues is given in
Section 4.
</bodyText>
<figureCaption confidence="0.894069555555556">
Finally, we observe that lexical probability distri-
butions vary not only with topic but with subtopic
too, in a hierarchical manner. For example, con-
sider the variation of the probability of the word
peace given major news topic distinctions (e.g. BUSI-
NESS and INTERNATIONAL news) as illustrated in
Figure 1. There is substantial subtopic proba-
bility variation for peace within INTERNATIONAL
news (the word usage is 50-times more likely
</figureCaption>
<page confidence="0.994448">
167
</page>
<bodyText confidence="0.99945725">
in INTERNATIONAL:mIDDLE-EAST than INTERNA-
TIONAL:JAPAN). We propose methods of hierarchical
smoothing of P(wi Itopict) in a topic-tree to capture
this subtopic variation robustly.
</bodyText>
<sectionHeader confidence="0.667976" genericHeader="related work">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.99997412">
Recently, the speech community has begun to ad-
dress the issue of topic in language modeling. Lowe
(1995) utilized the hand-assigned topic labels for
the Switchboard speech corpus to develop topic-
specific language models for each of the 42 switch-
board topics, and used a single topic-dependent lan-
guage model to rescore the lists of N-best hypothe-
ses. Error-rate improvement over the baseline lan-
guage model of 0.44% was reported.
Iyer et al. (1994) used bottom-up clustering tech-
niques on discourse contexts, performing sentence-
level model interpolation with weights updated dy-
namically through an EM-like procedure. Evalu-
ation on the Wall Street Journal (WSJO) corpus
showed a 4% perplexity reduction and 7% word er-
ror rate reduction. In Iyer and Ostendorf (1996),
the model was improved by model probability rees-
timation and interpolation with a cache model, re-
sulting in better dynamic adaptation and an overall
22%/3% perplexity/error rate reduction due to both
components.
Seymore and Rosenfeld (1997) reported significant
improvements when using a topic detector to build
specialized language models on the Broadcast News
(BN) corpus. They used TF-IDF and Naive Bayes
classifiers to detect the most similar topics to a given
article and then built a specialized language model
to rescore the N-best lists corresponding to the arti-
cle (yielding an overall 15% perplexity reduction us-
ing document-specific parameter re-estimation, and
no significant word error rate reduction). Seymore
et al. (1998) split the vocabulary into 3 sets: gen-
eral words, on-topic words and off-topic words, and
then use a non-linear interpolation to compute the
language model. This yielded an 8% perplexity re-
duction and 1% relative word error rate reduction.
In collaborative work, Mangu (1997) investigated
the benefits of using existing an Broadcast News
topic hierarchy extracted from topic labels as a ba-
sis for language model computation. Manual tree
construction and hierarchical interpolation yielded
a 16% perplexity reduction over a baseline uni-
gram model. In a concurrent collaborative effort,
Khudanpur and Wu (1999) implemented clustering
and topic-detection techniques similar on those pre-
sented here and computed a maximum entropy topic
sensitive language model for the Switchboard cor-
pus, yielding 8% perplexity reduction and 1.8% word
error rate reduction relative to a baseline maximum
entropy trigram model.
</bodyText>
<sectionHeader confidence="0.921751" genericHeader="method">
2 The Data
</sectionHeader>
<bodyText confidence="0.999818333333333">
The data used in this research is the Broadcast News
(BN94) corpus, consisting of radio and TV news
transcripts form the year 1994. From the total of
30226 documents, 20226 were used for training and
the other 10000 were used as test and held-out data.
The vocabulary size is approximately 120k words.
</bodyText>
<sectionHeader confidence="0.968148" genericHeader="method">
3 Optimizing Document Clustering
</sectionHeader>
<subsectionHeader confidence="0.621052">
for Language Modeling
</subsectionHeader>
<bodyText confidence="0.9998551">
For the purpose of language modeling, the topic la-
bels assigned to a document or segment of a doc-
ument can be obtained either manually (by topic-
tagging the documents) or automatically, by using
an unsupervised algorithm to group similar docu-
ments in topic-like clusters. We have utilized the
latter approach, for its generality and extensibility,
and because there is no reason to believe that the
manually assigned topics are optimal for language
modeling.
</bodyText>
<subsectionHeader confidence="0.998608">
3.1 Tree Generation
</subsectionHeader>
<bodyText confidence="0.999982214285714">
In this study, we have investigated a range of hierar-
chical clustering techniques, examining extensions of
hierarchical agglomerative clustering, k-means clus-
tering and top-down EM-based clustering. The lat-
ter underperformed on evaluations in Florian (1998)
and is not reported here.
A generic hierarchical agglomerative clustering al-
gorithm proceeds as follows: initially each document
has its own cluster. Repeatedly, the two closest clus-
ters are merged and replaced by their union, until
there is only one top-level cluster. Pairwise docu-
ment similarity may be based on a range of func-
tions, but to facilitate comparative analysis we have
utilized standard cosine similarity (d (D1, D2) =
</bodyText>
<equation confidence="0.890349">
(Di ,D2)
)A11211132112
1 and IR-style term vectors (see Salton
</equation>
<bodyText confidence="0.991628428571429">
and McGill (1983)).
This procedure outputs a tree in which documents
on similar topics (indicated by similar term content)
tend to be clustered together. The difference be-
tween average-linkage and maximum-linkage algo-
rithms manifests in the way the similarity between
clusters is computed (see Duda and Hart (1973)). A
problem that appears when using hierarchical clus-
tering is that small centroids tend to cluster with
bigger centroids instead of other small centroids, of-
ten resulting in highly skewed trees such as shown
in Figure 2, a=0. To overcome the problem, we de-
vised two alternative approaches for computing the
intercluster similarity:
</bodyText>
<listItem confidence="0.9088895">
• Our first solution minimizes the attraction of
large clusters by introducing a normalizing fac-
tor a to the inter-cluster distance function:
&lt; c(Ci ), c(C2) &gt;
</listItem>
<equation confidence="0.708952">
d(Ci , C2) = N(Ci ). lic(C1)11N(C2)° jlc(C2)11 (2)
</equation>
<page confidence="0.99569">
168
</page>
<figureCaption confidence="0.97315475">
Figure 2: As a increases, the trees become more
balanced, at the expense of forced clustering
Figure 3: Tree-balance is also sensitive to the
smoothing parameter c.
</figureCaption>
<bodyText confidence="0.999740666666667">
where N (Ck) is the number of vectors (docu-
ments) in cluster Ck and c (CO is the centroid
of the ith cluster. Increasing a improves tree
balance as shown in Figure 2, but as a becomes
large the forced balancing degrades cluster qual-
ity.
</bodyText>
<listItem confidence="0.893387833333333">
• A second approach we explored is to perform
basic smoothing of term vector weights, replac-
ing all O&apos;s with a small value e. By decreasing
initial vector orthogonality, this approach facili-
tates attraction to small centroids, and leads to
more balanced clusters as shown in Figure 3.
</listItem>
<bodyText confidence="0.998228083333333">
Instead of stopping the process when the desired
number of clusters is obtained, we generate the full
tree for two reasons: (1) the full hierarchical struc-
ture is exploited in our language models and (2) once
the tree structure is generated, the objective func-
tion we used to partition the tree differs from that
used when building the tree. Since the clustering
procedure turns out to be rather expensive for large
datasets (both in terms of time and memory), only
10000 documents were used for generating the initial
hierarchical structure.
°Section 3.2 describes the choice of optimum a.
</bodyText>
<subsectionHeader confidence="0.999783">
3.2 Optimizing the Hierarchical Structure
</subsectionHeader>
<bodyText confidence="0.983952">
To be able to compute accurate language models,
one has to have sufficient data for the relative fre-
quency estimates to be reliable. Usually, even with
enough data, a smoothing scheme is employed to in-
sure that P (wilwri) &gt; o for any given word sequence
.
The trees obtained from the previous step have
documents in the leaves, therefore not enough word
mass for proper probability estimation. But, on the
path from a leaf to the root, the internal nodes grow
in mass, ending with the root where the counts from
the entire corpus are stored. Since our intention is to
use the full tree structure to interpolate between the
in-node language models, we proceeded to identify
a subset of internal nodes of the tree, which contain
sufficient data for language model estimation. The
criteria of choosing the nodes for collapsing involves
a goodness function, such that the cue is a solu-
tion to a constrained optimization problem, given
the constraint that the resulting tree has exactly k
leaves. Let this evaluation function be g(n), where
n is a node of the tree, and suppose that we want
to minimize it. Let g(n, k) be the minimum cost of
creating k leaves in the subtree of root n. When the
evaluation function g (n) satisfies the locality con-
dition that it depends solely on the values g (nj,-),
(where (ni)j=1..kare the children of node n), g (root)
can be computed efficiently using dynamic program-
ming2 :
</bodyText>
<equation confidence="0.968506">
g(n, 1) = g(n)
g(n,k) = min h (g (ni, ji) , • • • , g (nk, jk))(3)
,ik &gt; 1
Ek jk = k
</equation>
<bodyText confidence="0.991023733333333">
Let us assume for a moment that we are inter-
ested in computing a unigram topic-mixture lan-
guage model. If the topic-conditional distributions
have high entropy (e.g. the histogram of P(wl topic)
is fairly uniform), topic-sensitive language model in-
terpolation will not yield any improvement, no mat-
ter how well the topic detection procedure works.
Therefore, we are interested in clustering documents
in such a way that the topic-conditional distribution
P(witopic) is maximally skewed. With this in mind,
we selected the evaluation function to be the condi-
tional entropy of a set of words (possibly the whole
vocabulary) given the particular classification. The
conditional entropy of some set of words 14, given a
partition C is
</bodyText>
<equation confidence="0.747119266666667">
H (WIC) = E P(Ci) E P(wici) • log(P(wri))
i=1 wEINI1Ci
= +, E E c(w, co • log(P(wri)) (4)
i=1 wEwnci
lthe collection of nodes that collapse
h is an operator through which the values
g(ni,ji),... ,g(nk, jk) are combined, as E or
a = 0
a = 0.3
a = 0.5
!!1!!!!!!!!l!!1!!!!!!!!
c = 0
= 0.15
e = 0.3
c = 0.7
</equation>
<page confidence="0.798451">
169
</page>
<figure confidence="0.99669">
Conditional Entropy in the Average-Linkage Case Conditional Eatropy in the Maximum-Linkage Case
64 clusters -
77 clusters
100 clustau - -
- &amp;quot;••....
• ........ •
.............
I ......
..............
0.2
0.6
0.1
03 0.4
a
07
07
0.6
02
0.5
0.1
03
03 0.4
a
5.55
5.5
5.45
5.4
535
5.3
525
5.2
5.15
5.1
5.05
0
3.85
3.8
3.75
3.7
3.65
3.6
3.55 0
</figure>
<figureCaption confidence="0.999979">
Figure 4: Conditional entropy for different a, cluster sizes and linkage methods
</figureCaption>
<bodyText confidence="0.993826">
where c (w, Ci) is the TF-IDF factor of word w in
class Ci and T is the size of the corpus. Let us
observe that the conditional entropy does satisfy the
locality condition mentioned earlier.
Given this objective function, we identified the op-
timal tree cut using the dynamic-programming tech-
nique described above. We also optimized different
parameters (such as a and choice of linkage method).
Figure 4 illustrates that for a range of cluster sizes,
maximal linkage clustering with a=0.15-0.3 yields
optimal performance given the objective function in
equation (2).
The effect of varying a is also shown graphically in
Figure 5. Successful tree construction for language
modeling purposes will minimize the conditional en-
tropy of P (WIC). This is most clearly illustrated
for the word politics, where the tree generated with
a = 0.3 maximally focuses documents on this topic
into a single cluster. The other words shown also
exhibit this desirable highly skewed distribution of
P (WIC) in the cluster tree generated when a = 0.3.
Another investigated approach was k-means clus-
tering (see Duda and Hart (1973)) as a robust and
proven alternative to hierarchical clustering. Its ap-
plication, with both our automatically derived clus-
ters and Mangu&apos;s manually derived clusters (Mangu
(1997)) used as initial partitions, actually yielded a
small increase in conditional entropy and was not
pursued further.
</bodyText>
<sectionHeader confidence="0.997509" genericHeader="method">
4 Language Model Construction and
Evaluation
</sectionHeader>
<bodyText confidence="0.999694333333333">
Estimating the language model probabilities is a
two-phase process. First, the topic-sensitive lan-
guage model probabilities P (tui It, w,÷ 1) are com-
puted during the training phase. Then, at run-time,
or in the testing phase, topic is dynamically iden-
tified by computing the probabilities P (*Dill as
in section 4.2 and the final language model proba-
bilities are computed using Equation (1). The tree
used in the following experiments was generated us-
ing average-linkage agglomerative clustering, using
parameters that optimize the objective function in
Section 3.
</bodyText>
<subsectionHeader confidence="0.832559">
4.1 Language Model Construction
</subsectionHeader>
<bodyText confidence="0.997924">
The topic-specific language model probabilities are
computed in a four phase process:
</bodyText>
<listItem confidence="0.982247619047619">
1. Each document is assigned to one leaf in the
tree, based on the similarity to the leaves&apos; cen-
troids (using the cosine similarity). The doc-
ument counts are added to the selected leaf&apos;s
count.
2. The leaf counts are propagated up the tree such
that, in the end, the counts of every inter-
nal node are equal to the sum of its children&apos;s
counts. At this stage, each node of the tree has
an attached language model - the relative fre-
quencies.
3. In the root of the tree, a discounted Good-
Turing language model is computed (see Katz
(1987), Chen and Goodman (1998)).
4. m-gram smooth language models are computed
for each node n different than the root by
three-way interpolating between the m-gram
language model in the parent parent(n), the
(m - 1)-gram smooth language model in node
n and the m-gram relative-frequency estimate
in node n:
</listItem>
<equation confidence="0.962101">
Pn (wmitur-1) =
(win-1) Pparent n (WM I W In-1)
(tvr-&apos;) Pn (wenitv2n1
+4 (W71-1) f ,n-1
n (wm w)
</equation>
<bodyText confidence="0.988373285714286">
with Al (tur-1) + A. (wr--1) An3 (wr1) = 1
for each node n in the tree. Based on how
Ahn (wr1) depend on the particular node n and
the word history wr1, various models can be
obtained. We investigated two approaches: a
bigram model in which the A&apos;s are fixed over
the tree, and a more general trigram model in
</bodyText>
<figure confidence="0.846969896551724">
(5)
170
• Case 1: f 1
-node 001,0 0
Proot (W2IW1)
Al fnode (W2IW1) • &amp;quot;Ynode (W1) A2Pnode (W2)
Pnode (W2IW1) = + (1 — Ai — A2) Pparent(node) (W2IW1)
anode (W1) Pnode (t02)
where
if w2 E F(wi)
if w2 E (wi)
if w2 E (wl)
)
E fnode(W2IW1)
t.2EY(t.1)
E f 1—
-node0.02.W1.
7node (wi) =
(1fit) E fnod.(w2iwi), anode (W1) 7--
(1+)
w2ele(wi)
(1+0) 1— Pnode(02)
t.2E,(ts1)UR(tei)
• Case 2: fnode (WO = 0
Proot (W2 IW1)
A2Pnode (1.V2) • 7node (W1)
Pnode (W2IW1) = + (1 — )t3) &amp;quot;parent(node) (W2IW1)
anode (W1) Pnode (w2)
if w2 E F(wi)
</figure>
<figureCaption confidence="0.7695218">
if w2 E (wi)
if tv2 E (wl)
where &apos;Ynode (wl ) and anode (W1) are computed in a similar fashion such that the probabilities do sum to 1.
Figure 5: Basic Bigram Language Model Specifications
which A&apos;s adapt using an EM reestimation pro-
</figureCaption>
<figure confidence="0.951663666666667">
Model Bigram-type Example Freq.
fixed p(FWIFW) p(thelin) 45.3% least topic sensitive
fixed p(FWICW) p(of &apos;scenario) 24.8% .I.
free p(CWICW) p(oiricold) 5.3% 1
free p(CWIFW) p(oirlths) 24.5% most topic sen•itive
cedure.
</figure>
<subsectionHeader confidence="0.825029">
4.1.1 Bigram Language Model
</subsectionHeader>
<bodyText confidence="0.9938532">
Not all words are topic sensitive. Mangu (1997) ob-
served that closed-class function words (FW), such
as the, of, and with, have minimal probability vari-
ation across different topic parameterizations, while
most open-class content words (CW) exhibit sub-
stantial topic variation. This leads us to divide the
possible word pairs in two classes (topic-sensitive
and not) and compute the A&apos;s in Equation (5) in
such a way that the probabilities in the former set
are constant in all the models. To formalize this:
</bodyText>
<listItem confidence="0.999238166666667">
• .F(wi) = {w2 E VI (w1,w2) is fixed}-the
&amp;quot;fixed&amp;quot; space;
• &amp;quot;R. (wi) = {w2 E VI (wilw2) is free/variable)-
the &amp;quot;free&amp;quot; space;
• U (w1) = {w2 E VI (wi,w2) was never see*
the &amp;quot;unknown&amp;quot; space.
</listItem>
<bodyText confidence="0.974975882352941">
The imposed restriction is, then: for every word
w1 and any word w2 E (wi) Pn (w2Iwi) =
Proot (W2IW1) in any node n.
The distribution of bigrains in the training data
is as follows, with roughly 30% bigram probabilities
allowed to vary in the topic-sensitive models:
This approach raises one interesting issue: the
language model in the root assigns some probabil-
ity mass to the unseen events, equal to the single-
tons&apos; mass (see Good (1953),Katz (1987)). In our
case, based on the assumptions made in the Good-
Turing formulation, we considered that the ratio of
the probability mass that goes to the unseen events
and the one that goes to seen, free events should be
fixed over the nodes of the tree. Let # be this ratio.
Then the language model probabilities are computed
as in Figure 5.
</bodyText>
<subsectionHeader confidence="0.514067">
4.1.2 Ngram Language Model Smoothing
</subsectionHeader>
<bodyText confidence="0.999915541666667">
In general, n gram language model probabili-
ties can be computed as in formula (5), where
(win-&apos;)) k are adapted both for the p=1...3 artic-
ular node n and history w1. The proposed de-
pendency on the history is realized through the his-
tory count C (tv&apos;) and the relevance of the history
wr-1 to the topic in the nodes n and parent (n).
The intuition is that if a history is as relevant in the
current node as in the parent, then the estimates in
the parent should be given more importance, since
they are better estimated. On the other hand, if the
history is much more relevant in the current node,
then the estimates in the node should be trusted
more. The mean adapted A for a given height h
is the tree is shown in Figure 6. This is consistent
with the observation that splits in the middle of the
tree tend to be most informative, while those closer
to the leaves suffer from data fragmentation, and
hence give relatively more weight to their parent.
As before, since not all the m-grams are expected to
be topic-sensitive, we use a method to insure that
those m grams are kept &amp;quot;fixed&amp;quot; to minimize noise
and modeling effort. In this case, though, 2 lan-
guage models with different support are used: one
</bodyText>
<page confidence="0.991386">
171
</page>
<figure confidence="0.988003111111111">
It is at least on the Serb side a real setback to the
peace piece
40 50
10 20 30
0.016
. .... _
1&apos;(peace I history)
Aleace I history)
0.014
C.)
0.4 0.012
0
6&amp;quot; 0.01
&amp;quot;B
- 0.008
8 0.
0.004
0.002
0.05
P fpiece I history)
1.(piece I history)
0.3
0.25
0.2
0.15
0.1
P(topic lit is... to the)
</figure>
<figureCaption confidence="0.990632">
Figure 7: Topic sensitive probability estimation for peace and piece in context
</figureCaption>
<figure confidence="0.998635">
1
0.8
0.6
i) 0.4
0.2
2 3 4 5 6 7 8
Node Height
</figure>
<figureCaption confidence="0.9645595">
Figure 6: Mean of the estimated As at node height
h, in the unigram case
</figureCaption>
<bodyText confidence="0.824218428571429">
that supports the topic insensitive m-grams and that
is computed only once (it&apos;s a normalization of the
topic-insensitive part of the overall model), and one
that supports the rest of the mass and which is com-
puted by interpolation using formula (5). Finally,
the final language model in each node is computed
as a mixture of the two.
</bodyText>
<subsectionHeader confidence="0.99566">
4.2 Dynamic Topic Adaptation
</subsectionHeader>
<bodyText confidence="0.988833131578947">
Consider the example of predicting the word follow-
ing the Broadcast News fragment: &amp;quot;It is at least on
the Serb side a real drawback to the ? &amp;quot;. Our topic
detection model, as further detailed later in this sec-
tion, assigns a topic distribution to this left context
(including the full previous discourse), illustrated in
the upper portion of Figure 7. The model identi-
fies that this particular context has greatest affinity
with the empirically generated topic clusters #41
and #42 (which appear to have one of their foci on
international events).
The lower portion of Figure 7 illustrates the topic-
conditional bigram probabilities P(w I the, topic) for
two candidate hypotheses for w: peace (the actu-
ally observed word in this case) and piece (an in-
correct competing hypothesis). In the former case,
P(peacelthe, topic) is clearly highly elevated in the
most probable topics for this context (#41,#42),
and thus the application of our core model combi-
nation (Equation 1) yields a posterior joint product
P = Efc_, P (titut—i) • Pt (wilwitmi +0 that is
12-times more likely than the overall bigram proba-
bility, P(airlthe) = 0.001. In contrast, the obvious
accustically motivated alternative piece, has great-
est probability in a far different and much more dif-
fuse distribution of topics, yielding a joint model
probability for this particular context that is 40%
lower than its baseline bigram probability. This
context-sensitive adaptation illustrates the efficacy
of dynamic topic adaptation in increasing the model
probability of the truth.
Clearly the process of computing the topic de-
tector P (titurl) is crucial. We have investigated
several mechanisms for estimating this probability,
the most promising is a class of normalized trans-
formations of traditional cosine similarity between
the document history vector w1-1 and the topic cen-
troids:
</bodyText>
<equation confidence="0.76940825">
P (titvi) = f (Cosine-Sim (t ,w))
i-1
f (Cosine-Sim (ti, 24-1)) (6)
ti
</equation>
<bodyText confidence="0.7470825">
One obvious choice for the function f would be the
identity. However, considering a linear contribution
</bodyText>
<page confidence="0.986179">
172
</page>
<table confidence="0.999065166666667">
Language Perplexity on Perplexity on
Model the entire the target
vocabulary vocabulary
Standard Bigrarn Model 215 584
Topic LMs History size Scaled g (x) f (x) k-NN
100 yes x x&apos; - 206 460
1000 yes x x&apos; - 195 405
5000 yes* x* x.2* -* 192 (-10%) 389(-33%)
5000 yes 1 x - 202 444
5000 no x x&apos; - 193 394
5000 yes x x 15-NN 192 390
5000 yes ex xez - 196 411
</table>
<tableCaption confidence="0.999838">
Table 1: Perplexity results for topic sensitive bigram language model, different history lengths
</tableCaption>
<bodyText confidence="0.965971636363636">
of similarities poses a problem: because topic de-
tection is more accurate when the history is long,
even unrelated topics will have a non-trivial contri-
bution to the final probability3, resulting in poorer
estimates.
One class of transformations we investigated, that
directly address the previous problem, adjusts the
similarities such that closer topics weigh more and
more distant ones weigh less. Therefore, f is chosen
such that
&lt; al- for xi &lt;X2 &lt;4.
</bodyText>
<equation confidence="0.997416">
f (z2) — x;
f (I1 &lt; f Ls2) for xi &lt;_ X2
xi — Z2
</equation>
<bodyText confidence="0.997505545454545">
that is, -f-(P should be a monotonically increas-
ing function on the interval [0, 1], or, equivalently
f (x) = x • g (x), g being an increasing function on
[0,1]. Choices for g(x) include x, x7(7 &gt; 0), log (x),
ez
Another way of solving this problem is through the
scaling operator f (xi) = xi —min xi n, apply-
ing this operator, minimum values (corresponding to
low-relevancy topics) do not receive any mass at all,
and the mass is divided between the more relevant
topics. For example, a combination of scaling and
</bodyText>
<equation confidence="0.991858">
g(x) = x7 yields:
P(tiliv1-1) =
</equation>
<reference confidence="0.22720475">
sim(w1-1,t5)-mink Si-(1,t„) &apos;
mexk Sim(-1,th)—mink Sim(w4-1,tk)
Sim(4-1,t,)—mink Sirra(u4-1,th) 1 (8)
max, Sim(o—i,th)—rnink Sim(.4-1,th)
</reference>
<bodyText confidence="0.999771666666667">
A third class of transformations we investigated
considers only the closest k topics in formula (6)
and ignores the more distant topics.
</bodyText>
<subsectionHeader confidence="0.884275">
4.3 Language Model Evaluation
</subsectionHeader>
<bodyText confidence="0.6884555">
Table 1 briefly summarizes a larger table of per-
formance measured on the bigram implementation
</bodyText>
<footnote confidence="0.843304">
3Due to unimportant word co-occurrences
</footnote>
<bodyText confidence="0.999978318181818">
of this adaptive topic-based LM. For the default
parameters (indicated by *), a statistically signif-
icant overall perplexity decrease of 10.5% was ob-
served relative to a standard bigram model mea-
sured on the same 1000 test documents. System-
atically modifying these parameters, we note that
performance is decreased by using shorter discourse
contexts (as histories never cross discourse bound-
aries, 5000-word histories essentially correspond to
the full prior discourse). Keeping other parame-
ters constant, g(x) = x outperforms other candidate
transformations g(x) = 1 and g(x) = ex. Absence
of k-nn and use of scaling both yield minor perfor-
mance improvements.
It is important to note that for 66% of the vo-
cabulary the topic-based LM is identical to the core
bigram model. On the 34% of the data that falls in
the model&apos;s target vocabulary, however, perplexity
reduction is a much more substantial 33.5% improve-
ment. The ability to isolate a well-defined target
subtask and perform very well on it makes this work
especially promising for use in model combination.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999205">
In this paper we described a novel method of gen-
erating and applying hierarchical, dynamic topic-
based language models. Specifically, we have pro-
posed and evaluated hierarchical cluster genera-
tion procedures that yield specially balanced and
pruned trees directly optimized for language mod-
eling purposes. We also present a novel hierar-
chical interpolation algorithm for generating a lan-
guage model from these trees, specializing in the
hierarchical topic-conditional probability estimation
for a target topic-sensitive vocabulary (34% of the
entire vocabulary). We also propose and evalu-
ate a range of dynamic topic detection procedures
based on several transformations of content-vector
similarity measures. These dynamic estimations of
P(topicilhistory) are combined with the hierarchical
estimation of P(wordj ltopici, history) in a product
across topics, yielding a final probability estimate
</bodyText>
<equation confidence="0.601665">
(7)
</equation>
<page confidence="0.99405">
173
</page>
<bodyText confidence="0.999947625">
of P(wordi &apos;history) that effectively captures long-
distance lexical dependencies via these intermediate
topic models. Statistically significant reductions in
perplexity are obtained relative to a baseline model,
both on the entire text (10.5%) and on the target
vocabulary (33.5%). This large improvement on a
readily isolatable subset of the data bodes well for
further model combination.
</bodyText>
<sectionHeader confidence="0.996366" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999767777777778">
The research reported here was sponsored by Na-
tional Science Foundation Grant 1111-9618874. The
authors would like to thank Eric Brill, Eugene Char-
niak, Ciprian Chelba, Fred Jelinek, Sanjeev Khudan-
pur, Lidia Mangu and Jun Wu for suggestions and
feedback during the progress of this work, and An-
dreas Stolcke for use of his hierarchical clustering
tools as a basis for some of the clustering software
developed here.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9972727875">
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra,
F. Jelinek, J. Lafferty, R. Mercer, and P. Roossin`.
1990. A statistical approach to machine transla-
tion. Computational Linguistics, 16(2).
Ciprian Chelba and Fred Jelinek. 1998. Exploiting
syntactic structure for language modeling. In Pro-
ceedings COLING-ACL, volume 1, pages 225-231,
August.
Stanley F. Chen and Joshua Goodman. 1998.
An empirical study of smoothing techinques for
language modeling. Technical Report TR-10-98,
Center for Research in Computing Technology,
Harvard University, Cambridge, Massachusettes,
August.
Richard 0. Duda and Peter E. Hart. 1973. Patern
Classification and Scene Analysis. John Wiley &amp;
Sons.
Radu Florian. 1998. Exploiting nonlo-
cal word relationships in language mod-
els. Technical report, Computer Science
Department, Johns Hopkins University.
http://n1p.cs.jhu.edurrflorian/papers/topic-
lm-tech-rep.ps.
J. Good. 1953. The population of species and the
estimation of population parameters. Biometrica,
40, parts 3,4:237-264.
Rukmini Iyer and Mari Ostendorf. 1996. Modeling
long distance dependence in language: Topic mix-
tures vs. dynamic cache models. In Proceedings
of the International Conferrence on Spoken Lan-
guage Processing, volume 1, pages 236-239.
Rukmini Iyer, Mari Ostendorf, and J. Robin
Rohlicek. 1994. Language modeling with
sentence-level mixtures. In Proceedings ARPA
Workshop on Human Language Technology, pages
82-87.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component
of a speech recognizer. In IEEE Transactions on
Acoustics, Speech, and Signal Processing, 1987,
volume ASSP-35 no 3, pages 400-401, March
1987.
Sanjeev Khudanpur and Jun Wu. 1999. A maxi-
mum entropy language model integrating n-gram
and topic dependencies for conversational speech
recognition. In Proceedings on ICASSP.
R. Kuhn and R. de Mori. 1992. A cache based nat-
ural language model for speech recognition. IEEE
Transaction PAMI, 13:570-583.
R. Lau, Ronald Rosenfeld, and Salim Roukos. 1993.
Trigger based language models: a maximum en-
tropy approach. In Proceedings ICASSP, pages
45-48, April.
S. Lowe. 1995. An attempt at improving recognition
accuracy on switchboard by using topic identifi-
cation. In 1995 Johns Hopkins Speech Workshop,
Language Modeling Group, Final Report.
Lidia Mangu. 1997. Hierarchical topic-sensitive
language models for automatic speech recog-
nition. Technical report, Computer Sci-
ence Department, Johns Hopkins University.
hap: / /nlp cs .jhu.edur lidia/papers/tech-repl.ps.
Ronald Rosenfeld. 1994. A hybrid approach to
adaptive statistical language modeling. In Pro-
ceedings ARPA Workshop on Human Language
Technology, pages 76-87.
G. Salton and M. McGill. 1983. An Introduc-
tion to Modern Information Retrieval. New York,
McGram-Hill.
Kristie Seymore and Ronald Rosenfeld. 1997. Using
stopy topics for language model adaptation. In
EuroSpeech97, volume 4, pages 1987-1990.
Kristie Seymore, Stanley Chen, and Ronald Rosen-
feld. 1998. Nonlinear interpolation of topic mod-
els for language model adaptation. In Proceedings
of ICSLP98.
J. H. Wright, G. J. F. Jones, and H. Lloyd-Thomas.
1993. A consolidated language model for speech
recognition. In Proceedings EuroSpeech, volume 2,
pages 977-980.
</reference>
<page confidence="0.99842">
174
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.874792">
<title confidence="0.9998315">Dynamic Nonlocal Language Modeling via Hierarchical Topic-Based Adaptation</title>
<author confidence="0.999836">Radu Florian</author>
<author confidence="0.999836">David Yarowsky</author>
<affiliation confidence="0.9959915">Computer Science Department and Center for Language and Speech Processing, Johns Hopkins University</affiliation>
<address confidence="0.999955">Baltimore, Maryland 21218</address>
<email confidence="0.997279">rflorianOcs.jhu.edu</email>
<email confidence="0.997279">yarowskyOcs.jhu.edu</email>
<abstract confidence="0.9877342">This paper presents a novel method of generating and applying hierarchical, dynamic topic-based language models. It proposes and evaluates new cluster generation, hierarchical smoothing and adaptive topic-probability estimation techniques. These combined models help capture long-distance lexical dependencies. Experiments on the Broadcast News corpus show significant improvement in perplexity (10.5% overall and 33.5% on target vocabulary).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Si-(1,t„) &apos; mexk Sim(-1,th)—mink Sim(w4-1,tk) Sim(4-1,t,)—mink Sirra(u4-1,th)</booktitle>
<volume>1</volume>
<issue>8</issue>
<pages>1--1</pages>
<note>max, Sim(o—i,th)—rnink Sim(.4-1,th)</note>
<marker></marker>
<rawString>sim(w1-1,t5)-mink Si-(1,t„) &apos; mexk Sim(-1,th)—mink Sim(w4-1,tk) Sim(4-1,t,)—mink Sirra(u4-1,th) 1 (8) max, Sim(o—i,th)—rnink Sim(.4-1,th)</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>R Mercer</author>
<author>P Roossin`</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin`, 1990</marker>
<rawString>P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, J. Lafferty, R. Mercer, and P. Roossin`. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Fred Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings COLING-ACL,</booktitle>
<volume>1</volume>
<pages>225--231</pages>
<contexts>
<context position="1897" citStr="Chelba and Jelinek (1998)" startWordPosition="274" endWordPosition="277">uence wi wi ; a common size for m is 3 (trigram language models). Even if n-grams were proved to be very powerful and robust in various tasks involving language models, they have a certain handicap: because of the Markov assumption, the dependency is limited to very short local context. Cache language models (Kuhn and de Mori (1992),Rosenfeld (1994)) try to overcome this limitation by boosting the probability of the words already seen in the history; trigger models (Lau et al. (1993)), even more general, try to capture the interrelationships between words. Models based on syntactic structure (Chelba and Jelinek (1998), Wright et al. (1993)) effectively estimate intra-sentence syntactic word dependencies. The approach we present here is based on the observation that certain words tend to have different probability distributions in different topics. We propose to compute the conditional language model probability as a dynamic mixture model of K topicspecific language models: Empirical Observation: Lexical Probabilities are Sensitive to Topic and Subtopic P( peacelsubtopie) I Malor Topics and BS Subtopics has the Insadust News Comm Figure 1: Conditional probability of the word peace given manually assigned Br</context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>Ciprian Chelba and Fred Jelinek. 1998. Exploiting syntactic structure for language modeling. In Proceedings COLING-ACL, volume 1, pages 225-231, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techinques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Center for Research in Computing Technology, Harvard University,</institution>
<location>Cambridge, Massachusettes,</location>
<contexts>
<context position="15709" citStr="Chen and Goodman (1998)" startWordPosition="2533" endWordPosition="2536">uage model probabilities are computed in a four phase process: 1. Each document is assigned to one leaf in the tree, based on the similarity to the leaves&apos; centroids (using the cosine similarity). The document counts are added to the selected leaf&apos;s count. 2. The leaf counts are propagated up the tree such that, in the end, the counts of every internal node are equal to the sum of its children&apos;s counts. At this stage, each node of the tree has an attached language model - the relative frequencies. 3. In the root of the tree, a discounted GoodTuring language model is computed (see Katz (1987), Chen and Goodman (1998)). 4. m-gram smooth language models are computed for each node n different than the root by three-way interpolating between the m-gram language model in the parent parent(n), the (m - 1)-gram smooth language model in node n and the m-gram relative-frequency estimate in node n: Pn (wmitur-1) = (win-1) Pparent n (WM I W In-1) (tvr-&apos;) Pn (wenitv2n1 +4 (W71-1) f ,n-1 n (wm w) with Al (tur-1) + A. (wr--1) An3 (wr1) = 1 for each node n in the tree. Based on how Ahn (wr1) depend on the particular node n and the word history wr1, various models can be obtained. We investigated two approaches: a bigram</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techinques for language modeling. Technical Report TR-10-98, Center for Research in Computing Technology, Harvard University, Cambridge, Massachusettes, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duda</author>
<author>Peter E Hart</author>
</authors>
<title>Patern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="8200" citStr="Duda and Hart (1973)" startWordPosition="1248" endWordPosition="1251">merged and replaced by their union, until there is only one top-level cluster. Pairwise document similarity may be based on a range of functions, but to facilitate comparative analysis we have utilized standard cosine similarity (d (D1, D2) = (Di ,D2) )A11211132112 1 and IR-style term vectors (see Salton and McGill (1983)). This procedure outputs a tree in which documents on similar topics (indicated by similar term content) tend to be clustered together. The difference between average-linkage and maximum-linkage algorithms manifests in the way the similarity between clusters is computed (see Duda and Hart (1973)). A problem that appears when using hierarchical clustering is that small centroids tend to cluster with bigger centroids instead of other small centroids, often resulting in highly skewed trees such as shown in Figure 2, a=0. To overcome the problem, we devised two alternative approaches for computing the intercluster similarity: • Our first solution minimizes the attraction of large clusters by introducing a normalizing factor a to the inter-cluster distance function: &lt; c(Ci ), c(C2) &gt; d(Ci , C2) = N(Ci ). lic(C1)11N(C2)° jlc(C2)11 (2) 168 Figure 2: As a increases, the trees become more bal</context>
<context position="14127" citStr="Duda and Hart (1973)" startWordPosition="2276" endWordPosition="2279">-0.3 yields optimal performance given the objective function in equation (2). The effect of varying a is also shown graphically in Figure 5. Successful tree construction for language modeling purposes will minimize the conditional entropy of P (WIC). This is most clearly illustrated for the word politics, where the tree generated with a = 0.3 maximally focuses documents on this topic into a single cluster. The other words shown also exhibit this desirable highly skewed distribution of P (WIC) in the cluster tree generated when a = 0.3. Another investigated approach was k-means clustering (see Duda and Hart (1973)) as a robust and proven alternative to hierarchical clustering. Its application, with both our automatically derived clusters and Mangu&apos;s manually derived clusters (Mangu (1997)) used as initial partitions, actually yielded a small increase in conditional entropy and was not pursued further. 4 Language Model Construction and Evaluation Estimating the language model probabilities is a two-phase process. First, the topic-sensitive language model probabilities P (tui It, w,÷ 1) are computed during the training phase. Then, at run-time, or in the testing phase, topic is dynamically identified by </context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>Richard 0. Duda and Peter E. Hart. 1973. Patern Classification and Scene Analysis. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
</authors>
<title>Exploiting nonlocal word relationships in language models.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Computer Science Department, Johns Hopkins University.</institution>
<note>http://n1p.cs.jhu.edurrflorian/papers/topiclm-tech-rep.ps.</note>
<contexts>
<context position="7388" citStr="Florian (1998)" startWordPosition="1122" endWordPosition="1123">ther manually (by topictagging the documents) or automatically, by using an unsupervised algorithm to group similar documents in topic-like clusters. We have utilized the latter approach, for its generality and extensibility, and because there is no reason to believe that the manually assigned topics are optimal for language modeling. 3.1 Tree Generation In this study, we have investigated a range of hierarchical clustering techniques, examining extensions of hierarchical agglomerative clustering, k-means clustering and top-down EM-based clustering. The latter underperformed on evaluations in Florian (1998) and is not reported here. A generic hierarchical agglomerative clustering algorithm proceeds as follows: initially each document has its own cluster. Repeatedly, the two closest clusters are merged and replaced by their union, until there is only one top-level cluster. Pairwise document similarity may be based on a range of functions, but to facilitate comparative analysis we have utilized standard cosine similarity (d (D1, D2) = (Di ,D2) )A11211132112 1 and IR-style term vectors (see Salton and McGill (1983)). This procedure outputs a tree in which documents on similar topics (indicated by s</context>
</contexts>
<marker>Florian, 1998</marker>
<rawString>Radu Florian. 1998. Exploiting nonlocal word relationships in language models. Technical report, Computer Science Department, Johns Hopkins University. http://n1p.cs.jhu.edurrflorian/papers/topiclm-tech-rep.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Good</author>
</authors>
<title>The population of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrica,</journal>
<volume>40</volume>
<pages>3--4</pages>
<contexts>
<context position="18533" citStr="Good (1953)" startWordPosition="3040" endWordPosition="3041">VI (w1,w2) is fixed}-the &amp;quot;fixed&amp;quot; space; • &amp;quot;R. (wi) = {w2 E VI (wilw2) is free/variable)- the &amp;quot;free&amp;quot; space; • U (w1) = {w2 E VI (wi,w2) was never see* the &amp;quot;unknown&amp;quot; space. The imposed restriction is, then: for every word w1 and any word w2 E (wi) Pn (w2Iwi) = Proot (W2IW1) in any node n. The distribution of bigrains in the training data is as follows, with roughly 30% bigram probabilities allowed to vary in the topic-sensitive models: This approach raises one interesting issue: the language model in the root assigns some probability mass to the unseen events, equal to the singletons&apos; mass (see Good (1953),Katz (1987)). In our case, based on the assumptions made in the GoodTuring formulation, we considered that the ratio of the probability mass that goes to the unseen events and the one that goes to seen, free events should be fixed over the nodes of the tree. Let # be this ratio. Then the language model probabilities are computed as in Figure 5. 4.1.2 Ngram Language Model Smoothing In general, n gram language model probabilities can be computed as in formula (5), where (win-&apos;)) k are adapted both for the p=1...3 articular node n and history w1. The proposed dependency on the history is realize</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>J. Good. 1953. The population of species and the estimation of population parameters. Biometrica, 40, parts 3,4:237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rukmini Iyer</author>
<author>Mari Ostendorf</author>
</authors>
<title>Modeling long distance dependence in language: Topic mixtures vs. dynamic cache models.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conferrence on Spoken Language Processing,</booktitle>
<volume>1</volume>
<pages>236--239</pages>
<contexts>
<context position="4639" citStr="Iyer and Ostendorf (1996)" startWordPosition="699" endWordPosition="702">abels for the Switchboard speech corpus to develop topicspecific language models for each of the 42 switchboard topics, and used a single topic-dependent language model to rescore the lists of N-best hypotheses. Error-rate improvement over the baseline language model of 0.44% was reported. Iyer et al. (1994) used bottom-up clustering techniques on discourse contexts, performing sentencelevel model interpolation with weights updated dynamically through an EM-like procedure. Evaluation on the Wall Street Journal (WSJO) corpus showed a 4% perplexity reduction and 7% word error rate reduction. In Iyer and Ostendorf (1996), the model was improved by model probability reestimation and interpolation with a cache model, resulting in better dynamic adaptation and an overall 22%/3% perplexity/error rate reduction due to both components. Seymore and Rosenfeld (1997) reported significant improvements when using a topic detector to build specialized language models on the Broadcast News (BN) corpus. They used TF-IDF and Naive Bayes classifiers to detect the most similar topics to a given article and then built a specialized language model to rescore the N-best lists corresponding to the article (yielding an overall 15%</context>
</contexts>
<marker>Iyer, Ostendorf, 1996</marker>
<rawString>Rukmini Iyer and Mari Ostendorf. 1996. Modeling long distance dependence in language: Topic mixtures vs. dynamic cache models. In Proceedings of the International Conferrence on Spoken Language Processing, volume 1, pages 236-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rukmini Iyer</author>
<author>Mari Ostendorf</author>
<author>J Robin Rohlicek</author>
</authors>
<title>Language modeling with sentence-level mixtures.</title>
<date>1994</date>
<booktitle>In Proceedings ARPA Workshop on Human Language Technology,</booktitle>
<pages>82--87</pages>
<contexts>
<context position="4323" citStr="Iyer et al. (1994)" startWordPosition="651" endWordPosition="654">ST than INTERNATIONAL:JAPAN). We propose methods of hierarchical smoothing of P(wi Itopict) in a topic-tree to capture this subtopic variation robustly. 1.1 Related Work Recently, the speech community has begun to address the issue of topic in language modeling. Lowe (1995) utilized the hand-assigned topic labels for the Switchboard speech corpus to develop topicspecific language models for each of the 42 switchboard topics, and used a single topic-dependent language model to rescore the lists of N-best hypotheses. Error-rate improvement over the baseline language model of 0.44% was reported. Iyer et al. (1994) used bottom-up clustering techniques on discourse contexts, performing sentencelevel model interpolation with weights updated dynamically through an EM-like procedure. Evaluation on the Wall Street Journal (WSJO) corpus showed a 4% perplexity reduction and 7% word error rate reduction. In Iyer and Ostendorf (1996), the model was improved by model probability reestimation and interpolation with a cache model, resulting in better dynamic adaptation and an overall 22%/3% perplexity/error rate reduction due to both components. Seymore and Rosenfeld (1997) reported significant improvements when us</context>
</contexts>
<marker>Iyer, Ostendorf, Rohlicek, 1994</marker>
<rawString>Rukmini Iyer, Mari Ostendorf, and J. Robin Rohlicek. 1994. Language modeling with sentence-level mixtures. In Proceedings ARPA Workshop on Human Language Technology, pages 82-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<booktitle>In IEEE Transactions on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>35</volume>
<pages>400--401</pages>
<contexts>
<context position="15684" citStr="Katz (1987)" startWordPosition="2531" endWordPosition="2532">specific language model probabilities are computed in a four phase process: 1. Each document is assigned to one leaf in the tree, based on the similarity to the leaves&apos; centroids (using the cosine similarity). The document counts are added to the selected leaf&apos;s count. 2. The leaf counts are propagated up the tree such that, in the end, the counts of every internal node are equal to the sum of its children&apos;s counts. At this stage, each node of the tree has an attached language model - the relative frequencies. 3. In the root of the tree, a discounted GoodTuring language model is computed (see Katz (1987), Chen and Goodman (1998)). 4. m-gram smooth language models are computed for each node n different than the root by three-way interpolating between the m-gram language model in the parent parent(n), the (m - 1)-gram smooth language model in node n and the m-gram relative-frequency estimate in node n: Pn (wmitur-1) = (win-1) Pparent n (WM I W In-1) (tvr-&apos;) Pn (wenitv2n1 +4 (W71-1) f ,n-1 n (wm w) with Al (tur-1) + A. (wr--1) An3 (wr1) = 1 for each node n in the tree. Based on how Ahn (wr1) depend on the particular node n and the word history wr1, various models can be obtained. We investigated</context>
<context position="18545" citStr="Katz (1987)" startWordPosition="3041" endWordPosition="3042">s fixed}-the &amp;quot;fixed&amp;quot; space; • &amp;quot;R. (wi) = {w2 E VI (wilw2) is free/variable)- the &amp;quot;free&amp;quot; space; • U (w1) = {w2 E VI (wi,w2) was never see* the &amp;quot;unknown&amp;quot; space. The imposed restriction is, then: for every word w1 and any word w2 E (wi) Pn (w2Iwi) = Proot (W2IW1) in any node n. The distribution of bigrains in the training data is as follows, with roughly 30% bigram probabilities allowed to vary in the topic-sensitive models: This approach raises one interesting issue: the language model in the root assigns some probability mass to the unseen events, equal to the singletons&apos; mass (see Good (1953),Katz (1987)). In our case, based on the assumptions made in the GoodTuring formulation, we considered that the ratio of the probability mass that goes to the unseen events and the one that goes to seen, free events should be fixed over the nodes of the tree. Let # be this ratio. Then the language model probabilities are computed as in Figure 5. 4.1.2 Ngram Language Model Smoothing In general, n gram language model probabilities can be computed as in formula (5), where (win-&apos;)) k are adapted both for the p=1...3 articular node n and history w1. The proposed dependency on the history is realized through th</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. In IEEE Transactions on Acoustics, Speech, and Signal Processing, 1987, volume ASSP-35 no 3, pages 400-401, March 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjeev Khudanpur</author>
<author>Jun Wu</author>
</authors>
<title>A maximum entropy language model integrating n-gram and topic dependencies for conversational speech recognition.</title>
<date>1999</date>
<booktitle>In Proceedings on ICASSP.</booktitle>
<contexts>
<context position="5986" citStr="Khudanpur and Wu (1999)" startWordPosition="903" endWordPosition="906">e et al. (1998) split the vocabulary into 3 sets: general words, on-topic words and off-topic words, and then use a non-linear interpolation to compute the language model. This yielded an 8% perplexity reduction and 1% relative word error rate reduction. In collaborative work, Mangu (1997) investigated the benefits of using existing an Broadcast News topic hierarchy extracted from topic labels as a basis for language model computation. Manual tree construction and hierarchical interpolation yielded a 16% perplexity reduction over a baseline unigram model. In a concurrent collaborative effort, Khudanpur and Wu (1999) implemented clustering and topic-detection techniques similar on those presented here and computed a maximum entropy topic sensitive language model for the Switchboard corpus, yielding 8% perplexity reduction and 1.8% word error rate reduction relative to a baseline maximum entropy trigram model. 2 The Data The data used in this research is the Broadcast News (BN94) corpus, consisting of radio and TV news transcripts form the year 1994. From the total of 30226 documents, 20226 were used for training and the other 10000 were used as test and held-out data. The vocabulary size is approximately </context>
</contexts>
<marker>Khudanpur, Wu, 1999</marker>
<rawString>Sanjeev Khudanpur and Jun Wu. 1999. A maximum entropy language model integrating n-gram and topic dependencies for conversational speech recognition. In Proceedings on ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>R de Mori</author>
</authors>
<title>A cache based natural language model for speech recognition.</title>
<date>1992</date>
<journal>IEEE Transaction PAMI,</journal>
<pages>13--570</pages>
<marker>Kuhn, de Mori, 1992</marker>
<rawString>R. Kuhn and R. de Mori. 1992. A cache based natural language model for speech recognition. IEEE Transaction PAMI, 13:570-583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lau</author>
<author>Ronald Rosenfeld</author>
<author>Salim Roukos</author>
</authors>
<title>Trigger based language models: a maximum entropy approach.</title>
<date>1993</date>
<booktitle>In Proceedings ICASSP,</booktitle>
<pages>45--48</pages>
<contexts>
<context position="1760" citStr="Lau et al. (1993)" startWordPosition="254" endWordPosition="257">the following approximation for the computation of a word sequence probability: P (wf) = P (wiltv::;.+1) where 94 denotes the sequence wi wi ; a common size for m is 3 (trigram language models). Even if n-grams were proved to be very powerful and robust in various tasks involving language models, they have a certain handicap: because of the Markov assumption, the dependency is limited to very short local context. Cache language models (Kuhn and de Mori (1992),Rosenfeld (1994)) try to overcome this limitation by boosting the probability of the words already seen in the history; trigger models (Lau et al. (1993)), even more general, try to capture the interrelationships between words. Models based on syntactic structure (Chelba and Jelinek (1998), Wright et al. (1993)) effectively estimate intra-sentence syntactic word dependencies. The approach we present here is based on the observation that certain words tend to have different probability distributions in different topics. We propose to compute the conditional language model probability as a dynamic mixture model of K topicspecific language models: Empirical Observation: Lexical Probabilities are Sensitive to Topic and Subtopic P( peacelsubtopie) </context>
</contexts>
<marker>Lau, Rosenfeld, Roukos, 1993</marker>
<rawString>R. Lau, Ronald Rosenfeld, and Salim Roukos. 1993. Trigger based language models: a maximum entropy approach. In Proceedings ICASSP, pages 45-48, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lowe</author>
</authors>
<title>An attempt at improving recognition accuracy on switchboard by using topic identification. In</title>
<date>1995</date>
<contexts>
<context position="3979" citStr="Lowe (1995)" startWordPosition="597" endWordPosition="598">r. For example, consider the variation of the probability of the word peace given major news topic distinctions (e.g. BUSINESS and INTERNATIONAL news) as illustrated in Figure 1. There is substantial subtopic probability variation for peace within INTERNATIONAL news (the word usage is 50-times more likely 167 in INTERNATIONAL:mIDDLE-EAST than INTERNATIONAL:JAPAN). We propose methods of hierarchical smoothing of P(wi Itopict) in a topic-tree to capture this subtopic variation robustly. 1.1 Related Work Recently, the speech community has begun to address the issue of topic in language modeling. Lowe (1995) utilized the hand-assigned topic labels for the Switchboard speech corpus to develop topicspecific language models for each of the 42 switchboard topics, and used a single topic-dependent language model to rescore the lists of N-best hypotheses. Error-rate improvement over the baseline language model of 0.44% was reported. Iyer et al. (1994) used bottom-up clustering techniques on discourse contexts, performing sentencelevel model interpolation with weights updated dynamically through an EM-like procedure. Evaluation on the Wall Street Journal (WSJO) corpus showed a 4% perplexity reduction an</context>
</contexts>
<marker>Lowe, 1995</marker>
<rawString>S. Lowe. 1995. An attempt at improving recognition accuracy on switchboard by using topic identification. In 1995 Johns Hopkins Speech Workshop, Language Modeling Group, Final Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia Mangu</author>
</authors>
<title>Hierarchical topic-sensitive language models for automatic speech recognition.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>Computer Science Department, Johns Hopkins University.</institution>
<note>hap: / /nlp cs .jhu.edur lidia/papers/tech-repl.ps.</note>
<contexts>
<context position="5653" citStr="Mangu (1997)" startWordPosition="856" endWordPosition="857">e Bayes classifiers to detect the most similar topics to a given article and then built a specialized language model to rescore the N-best lists corresponding to the article (yielding an overall 15% perplexity reduction using document-specific parameter re-estimation, and no significant word error rate reduction). Seymore et al. (1998) split the vocabulary into 3 sets: general words, on-topic words and off-topic words, and then use a non-linear interpolation to compute the language model. This yielded an 8% perplexity reduction and 1% relative word error rate reduction. In collaborative work, Mangu (1997) investigated the benefits of using existing an Broadcast News topic hierarchy extracted from topic labels as a basis for language model computation. Manual tree construction and hierarchical interpolation yielded a 16% perplexity reduction over a baseline unigram model. In a concurrent collaborative effort, Khudanpur and Wu (1999) implemented clustering and topic-detection techniques similar on those presented here and computed a maximum entropy topic sensitive language model for the Switchboard corpus, yielding 8% perplexity reduction and 1.8% word error rate reduction relative to a baseline</context>
<context position="14305" citStr="Mangu (1997)" startWordPosition="2304" endWordPosition="2305">eling purposes will minimize the conditional entropy of P (WIC). This is most clearly illustrated for the word politics, where the tree generated with a = 0.3 maximally focuses documents on this topic into a single cluster. The other words shown also exhibit this desirable highly skewed distribution of P (WIC) in the cluster tree generated when a = 0.3. Another investigated approach was k-means clustering (see Duda and Hart (1973)) as a robust and proven alternative to hierarchical clustering. Its application, with both our automatically derived clusters and Mangu&apos;s manually derived clusters (Mangu (1997)) used as initial partitions, actually yielded a small increase in conditional entropy and was not pursued further. 4 Language Model Construction and Evaluation Estimating the language model probabilities is a two-phase process. First, the topic-sensitive language model probabilities P (tui It, w,÷ 1) are computed during the training phase. Then, at run-time, or in the testing phase, topic is dynamically identified by computing the probabilities P (*Dill as in section 4.2 and the final language model probabilities are computed using Equation (1). The tree used in the following experiments was </context>
<context position="17443" citStr="Mangu (1997)" startWordPosition="2850" endWordPosition="2851">node (W2IW1) = + (1 — )t3) &amp;quot;parent(node) (W2IW1) anode (W1) Pnode (w2) if w2 E F(wi) if w2 E (wi) if tv2 E (wl) where &apos;Ynode (wl ) and anode (W1) are computed in a similar fashion such that the probabilities do sum to 1. Figure 5: Basic Bigram Language Model Specifications which A&apos;s adapt using an EM reestimation proModel Bigram-type Example Freq. fixed p(FWIFW) p(thelin) 45.3% least topic sensitive fixed p(FWICW) p(of &apos;scenario) 24.8% .I. free p(CWICW) p(oiricold) 5.3% 1 free p(CWIFW) p(oirlths) 24.5% most topic sen•itive cedure. 4.1.1 Bigram Language Model Not all words are topic sensitive. Mangu (1997) observed that closed-class function words (FW), such as the, of, and with, have minimal probability variation across different topic parameterizations, while most open-class content words (CW) exhibit substantial topic variation. This leads us to divide the possible word pairs in two classes (topic-sensitive and not) and compute the A&apos;s in Equation (5) in such a way that the probabilities in the former set are constant in all the models. To formalize this: • .F(wi) = {w2 E VI (w1,w2) is fixed}-the &amp;quot;fixed&amp;quot; space; • &amp;quot;R. (wi) = {w2 E VI (wilw2) is free/variable)- the &amp;quot;free&amp;quot; space; • U (w1) = {w2</context>
</contexts>
<marker>Mangu, 1997</marker>
<rawString>Lidia Mangu. 1997. Hierarchical topic-sensitive language models for automatic speech recognition. Technical report, Computer Science Department, Johns Hopkins University. hap: / /nlp cs .jhu.edur lidia/papers/tech-repl.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A hybrid approach to adaptive statistical language modeling.</title>
<date>1994</date>
<booktitle>In Proceedings ARPA Workshop on Human Language Technology,</booktitle>
<pages>76--87</pages>
<contexts>
<context position="1623" citStr="Rosenfeld (1994)" startWordPosition="232" endWordPosition="233">ake a Markovian assumption on word dependencies; usually that word predictions depend on at most m previous words. Therefore they offer the following approximation for the computation of a word sequence probability: P (wf) = P (wiltv::;.+1) where 94 denotes the sequence wi wi ; a common size for m is 3 (trigram language models). Even if n-grams were proved to be very powerful and robust in various tasks involving language models, they have a certain handicap: because of the Markov assumption, the dependency is limited to very short local context. Cache language models (Kuhn and de Mori (1992),Rosenfeld (1994)) try to overcome this limitation by boosting the probability of the words already seen in the history; trigger models (Lau et al. (1993)), even more general, try to capture the interrelationships between words. Models based on syntactic structure (Chelba and Jelinek (1998), Wright et al. (1993)) effectively estimate intra-sentence syntactic word dependencies. The approach we present here is based on the observation that certain words tend to have different probability distributions in different topics. We propose to compute the conditional language model probability as a dynamic mixture model</context>
</contexts>
<marker>Rosenfeld, 1994</marker>
<rawString>Ronald Rosenfeld. 1994. A hybrid approach to adaptive statistical language modeling. In Proceedings ARPA Workshop on Human Language Technology, pages 76-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M McGill</author>
</authors>
<title>An Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<location>New York, McGram-Hill.</location>
<contexts>
<context position="7903" citStr="Salton and McGill (1983)" startWordPosition="1203" endWordPosition="1206">means clustering and top-down EM-based clustering. The latter underperformed on evaluations in Florian (1998) and is not reported here. A generic hierarchical agglomerative clustering algorithm proceeds as follows: initially each document has its own cluster. Repeatedly, the two closest clusters are merged and replaced by their union, until there is only one top-level cluster. Pairwise document similarity may be based on a range of functions, but to facilitate comparative analysis we have utilized standard cosine similarity (d (D1, D2) = (Di ,D2) )A11211132112 1 and IR-style term vectors (see Salton and McGill (1983)). This procedure outputs a tree in which documents on similar topics (indicated by similar term content) tend to be clustered together. The difference between average-linkage and maximum-linkage algorithms manifests in the way the similarity between clusters is computed (see Duda and Hart (1973)). A problem that appears when using hierarchical clustering is that small centroids tend to cluster with bigger centroids instead of other small centroids, often resulting in highly skewed trees such as shown in Figure 2, a=0. To overcome the problem, we devised two alternative approaches for computin</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M. McGill. 1983. An Introduction to Modern Information Retrieval. New York, McGram-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristie Seymore</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Using stopy topics for language model adaptation.</title>
<date>1997</date>
<booktitle>In EuroSpeech97,</booktitle>
<volume>4</volume>
<pages>1987--1990</pages>
<contexts>
<context position="4881" citStr="Seymore and Rosenfeld (1997)" startWordPosition="735" endWordPosition="738"> the baseline language model of 0.44% was reported. Iyer et al. (1994) used bottom-up clustering techniques on discourse contexts, performing sentencelevel model interpolation with weights updated dynamically through an EM-like procedure. Evaluation on the Wall Street Journal (WSJO) corpus showed a 4% perplexity reduction and 7% word error rate reduction. In Iyer and Ostendorf (1996), the model was improved by model probability reestimation and interpolation with a cache model, resulting in better dynamic adaptation and an overall 22%/3% perplexity/error rate reduction due to both components. Seymore and Rosenfeld (1997) reported significant improvements when using a topic detector to build specialized language models on the Broadcast News (BN) corpus. They used TF-IDF and Naive Bayes classifiers to detect the most similar topics to a given article and then built a specialized language model to rescore the N-best lists corresponding to the article (yielding an overall 15% perplexity reduction using document-specific parameter re-estimation, and no significant word error rate reduction). Seymore et al. (1998) split the vocabulary into 3 sets: general words, on-topic words and off-topic words, and then use a no</context>
</contexts>
<marker>Seymore, Rosenfeld, 1997</marker>
<rawString>Kristie Seymore and Ronald Rosenfeld. 1997. Using stopy topics for language model adaptation. In EuroSpeech97, volume 4, pages 1987-1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristie Seymore</author>
<author>Stanley Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Nonlinear interpolation of topic models for language model adaptation.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP98.</booktitle>
<contexts>
<context position="5378" citStr="Seymore et al. (1998)" startWordPosition="810" endWordPosition="813">ynamic adaptation and an overall 22%/3% perplexity/error rate reduction due to both components. Seymore and Rosenfeld (1997) reported significant improvements when using a topic detector to build specialized language models on the Broadcast News (BN) corpus. They used TF-IDF and Naive Bayes classifiers to detect the most similar topics to a given article and then built a specialized language model to rescore the N-best lists corresponding to the article (yielding an overall 15% perplexity reduction using document-specific parameter re-estimation, and no significant word error rate reduction). Seymore et al. (1998) split the vocabulary into 3 sets: general words, on-topic words and off-topic words, and then use a non-linear interpolation to compute the language model. This yielded an 8% perplexity reduction and 1% relative word error rate reduction. In collaborative work, Mangu (1997) investigated the benefits of using existing an Broadcast News topic hierarchy extracted from topic labels as a basis for language model computation. Manual tree construction and hierarchical interpolation yielded a 16% perplexity reduction over a baseline unigram model. In a concurrent collaborative effort, Khudanpur and W</context>
</contexts>
<marker>Seymore, Chen, Rosenfeld, 1998</marker>
<rawString>Kristie Seymore, Stanley Chen, and Ronald Rosenfeld. 1998. Nonlinear interpolation of topic models for language model adaptation. In Proceedings of ICSLP98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Wright</author>
<author>G J F Jones</author>
<author>H Lloyd-Thomas</author>
</authors>
<title>A consolidated language model for speech recognition.</title>
<date>1993</date>
<booktitle>In Proceedings EuroSpeech,</booktitle>
<volume>2</volume>
<pages>977--980</pages>
<contexts>
<context position="1919" citStr="Wright et al. (1993)" startWordPosition="278" endWordPosition="281"> for m is 3 (trigram language models). Even if n-grams were proved to be very powerful and robust in various tasks involving language models, they have a certain handicap: because of the Markov assumption, the dependency is limited to very short local context. Cache language models (Kuhn and de Mori (1992),Rosenfeld (1994)) try to overcome this limitation by boosting the probability of the words already seen in the history; trigger models (Lau et al. (1993)), even more general, try to capture the interrelationships between words. Models based on syntactic structure (Chelba and Jelinek (1998), Wright et al. (1993)) effectively estimate intra-sentence syntactic word dependencies. The approach we present here is based on the observation that certain words tend to have different probability distributions in different topics. We propose to compute the conditional language model probability as a dynamic mixture model of K topicspecific language models: Empirical Observation: Lexical Probabilities are Sensitive to Topic and Subtopic P( peacelsubtopie) I Malor Topics and BS Subtopics has the Insadust News Comm Figure 1: Conditional probability of the word peace given manually assigned Broadcast News topics P </context>
</contexts>
<marker>Wright, Jones, Lloyd-Thomas, 1993</marker>
<rawString>J. H. Wright, G. J. F. Jones, and H. Lloyd-Thomas. 1993. A consolidated language model for speech recognition. In Proceedings EuroSpeech, volume 2, pages 977-980.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>