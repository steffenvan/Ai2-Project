<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.971137">
Comparison of Alignment Templates and Maximum Entropy Models for
Natural Language Understanding
</title>
<author confidence="0.99569">
Oliver Bender, Klaus Macherey, Franz Josef Och, and Hermann Ney
</author>
<affiliation confidence="0.825521333333333">
Lehrstuhl fiir Informatik VI, Computer Science Department
RWTH Aachen - University of Technology
D-52056 Aachen, Germany
</affiliation>
<email confidence="0.990805">
fbender,k.macherey,och,neyl@informatik.rwth-aachen.de
</email>
<sectionHeader confidence="0.996589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9773965625">
In this paper we compare two ap-
proaches to natural language under-
standing (NLU). The first approach is
derived from the field of statistical ma-
chine translation (MT), whereas the
other uses the maximum entropy (ME)
framework. Starting with an anno-
tated corpus, we describe the problem of
NLU as a translation from a source sen-
tence to a formal language target sen-
tence. We mainly focus on the qual-
ity of the different alignment and ME
models and show that the direct ME ap-
proach outperforms the alignment tem-
plates method.
ich warde gerne von Koln nach Munchen fahren
</bodyText>
<figure confidence="0.720176">
V V
@want question @origin @destination @going
</figure>
<figureCaption confidence="0.999909">
Figure 1: Example of a word/ concept mapping.
</figureCaption>
<bodyText confidence="0.998793">
proach uses the maximum entropy (ME) frame-
work (Berger et al., 1996). For both frameworks,
the objective can be described as follows. Given
a natural source sentence fiJ = fj...f./ we
choose the formal target language sentence ef =
el •••e,...ei with the highest probability among all
possible target sentences:
</bodyText>
<figure confidence="0.539050444444444">
argmax { Pr (ej
ei
-1
ci
fi&apos;) (1)
argmax r(fi,,,)•pr(ef)
el
1 Introduction
Pr (f&apos;)
</figure>
<bodyText confidence="0.999783866666667">
The objective of natural language understanding
(NLU) is to extract all the information from a nat-
ural language based input which are relevant for
a specific task. Typical applications using NLU
components are spoken dialogue systems (Levin
and Pieraccini, 1995) or speech-to-speech transla-
tion systems (Zhou et al., 2002).
In this paper we present two approaches for an-
alyzing the semantics of natural language inputs
and discuss their advantages and drawbacks. The
first approach is derived from the field of statis-
tical machine translation (MT) and is based on
the source-channel paradigm (Brown et al., 1993).
Here, we apply a method called alignment tem-
plates (Och et al., 1999). The alternative ap-
</bodyText>
<equation confidence="0.9870475">
argmax { Pr(fief) • Pr(ef) • (2)
e
</equation>
<bodyText confidence="0.999576923076923">
Using Bayes&apos; theorem, Eq. 1 can be rewritten to
Eq. 2, where the denominator can be neglected.
The argmax operation denotes the search prob-
lem, i.e. the generation of the sequence of for-
mal semantic concepts in the target language. An
example is depicted in Figure 1. The main dif-
ference between both approaches is that the ME
framework directly models the posterior proba-
bilities whereas the statistical machine transla-
tion approach applies Bayes&apos; theorem resulting
in two distributions: the translation probability
Pr(fillef) and the language model probability
Pr(en. In the following, we compare both ap-
</bodyText>
<page confidence="0.998488">
11
</page>
<bodyText confidence="0.99982725">
proaches for two NLU tasks which are derived
from two different domains and show that the ME
approach clearly outperforms the statistical ma-
chine translation approach within these settings.
</bodyText>
<sectionHeader confidence="0.671852" genericHeader="related work">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.999942964285714">
The use of statistical machine translation for NLU
tasks was firstly proposed by (Epstein et al.,
1996). Whereas (Epstein et al., 1996) model hid-
den clumpings, we use a method called alignment
templates. Alignment templates have been proven
to be very powerful for statistical machine trans-
lation tasks since they allow for many-to-many
alignments between source and target words (Och
et al., 1999). Alignment templates for NLU tasks
were firstly proposed by (Macherey et al., 2001).
Applying ME translation models to NLU has
been firstly suggested by (Papineni et al., 1997;
Papineni et al., 1998). Here, we use a concept-
based meaning representation as formal target lan-
guage and propose different features and structural
constraints in order to improve the NLU results.
The remainder of the paper is organized as fol-
lows: in the following section, we briefly describe
the concept based meaning representation as used
for the NLU task. Section 3 describes the training
and search procedure of the alignment templates
approach. In section 4, we outline the ME frame-
work and describe the features that were used for
the experiments. Section 5 presents results for
both the alignment templates approach and the
ME framework. For both approaches, experiments
were carried out on two different German NLU
tasks.
</bodyText>
<sectionHeader confidence="0.796552" genericHeader="method">
2 Concept-based semantic representation
</sectionHeader>
<bodyText confidence="0.999940925925926">
A crucial decision, when designing an NLU sys-
tem, is the choice of a suitable semantic represen-
tation, since interpreting a user&apos;s request requires
an appropriate formalism to represent the mean-
ing of an utterance. Different semantic represen-
tations have been proposed. Among them, case
frames (Issar and Ward, 1993), semantic frames
(Bennacef et al., 1994), and variants of hierarchi-
cal concepts (Miller et al., 1994) as well as flat
concepts (Levin and Pieraccini, 1995) are the most
prominent. Since we regard NLU as a special case
of a translation problem, we have chosen a flat
concept-based target language as meaning repre-
sentation.
A semantic concept (in the following briefly
termed as concept) is defined as the smallest unit
of meaning that is relevant to a specific task (Levin
and Pieraccini, 1995). Figure 1 depicts an example
of a concept-based meaning representation for the
input utterance &apos;I would like to go from Munich
to Cologne&apos; from the domain of a German train-
timetable information system. The first line shows
the source sentence, the last line depicts the target
sentence consisting of several concepts, marked
by the preceding @ -symbol. The connections be-
tween the words describe the alignments between
source and target words.
</bodyText>
<sectionHeader confidence="0.977264" genericHeader="method">
3 Alignment Templates
</sectionHeader>
<bodyText confidence="0.999210111111111">
The statistical machine translation approach de-
composes Pr(eflg) into two probability distri-
butions, the language model probability and the
translation probability. The architecture of this
method is depicted in figure 2. For the transla-
tion approach, we use the same training proce-
dure as for the automatic translation of natural lan-
guages. When rewriting the translation probabil-
ity Pr(fiJ 4) by introducing a &apos;hidden&apos; alignment
</bodyText>
<equation confidence="0.970387166666667">
al = with aj C {1,...,1}, we
obtain:
611)
= Pr(fi&apos;
= E Pr(fi,
cr-7 =1
</equation>
<bodyText confidence="0.9999225">
The IBM models as proposed by (Brown et al.,
1993) and the HMM model as suggested by (Vo-
gel et al., 1996) result from different decompo-
sitions of Pr(fi1,4 4). For training the align-
ment model, we train a sequence of models of in-
creasing complexity. Starting from the first model
IBM1, we proceed over the HMM model, IBM3
up to IBMS. Using the model IBMS as a result of
the last training step, we use the alignment tem-
plate approach to model whole word groups.
</bodyText>
<equation confidence="0.990696333333333">
(3)
1.3-1 3-1
,a1 • ei) •
</equation>
<page confidence="0.98319">
12
</page>
<figure confidence="0.997752666666667">
Global Search
= argmax {Pr(eI) • Pr(fiT
Source Language Text
(Preprocessing)
Target Language Text
Pr(fij leD
Pr(e)
HLexicon Model
HAlignment Model
Language Model
@destination
@origin
@train determination
@want_guestion
@hello
@yes
4
&gt;4
+.1)
&gt;4 .0
El
H 0
0
U.
</figure>
<figureCaption confidence="0.9896378">
Figure 3: Example of alignment templates for rep-
resenting a natural sentence as a sequence of con-
cepts.
Figure 2: Architecture of the translation approach
based on the source-channel paradigm.
</figureCaption>
<subsectionHeader confidence="0.995304">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.957103666666667">
The alignment templates approach provides a two-
level alignment: a phrase level alignment and a
word level alignment within the phrases. As a re-
sult, source and target sentence must be segmented
into K word-groups, describing the phrases:
following way:
</bodyText>
<equation confidence="0.993307888888889">
p(fila &apos;re-) =
1) =
p(ili; a &apos;) p(f.J ei)
i=o
a 1(i, j)
/(i
—
ei/ = el ek = eik 1+1, • • • • eik, k = 1
.f = C •Jk = fik ,±1,• • • , = 1
</equation>
<bodyText confidence="0.9995315">
By decomposing the translation probability with
the above-mentioned definitions, we arrive at:
</bodyText>
<equation confidence="0.992803875">
Pr(fii
= E Pr(R 74( TT&apos;)
af(
E P(6k lak-1, K) • P(ik Fak ) •
-cif( k=1
— —
Denote z = (i&apos;, f . I a ) an alignment template,
we obtain p(f) = P(z1-) • p(fTz.&amp;quot;). The
</equation>
<bodyText confidence="0.996122">
phrase translation probability p(f z, e&amp;quot;) is decom-
posed according to the following equation:
</bodyText>
<equation confidence="0.954331">
P(f-1(-67 , ;a /),-0
6Ce-,F i) • (7. f&apos;) • H p(f., —
3=1
</equation>
<bodyText confidence="0.9986145">
where 6(...) denotes the Kronecker-function. The
probability p(f, la &apos;,&amp;quot;e-) can be decomposed in the
</bodyText>
<equation confidence="0.977188333333333">
1 if (i, j) are linked in a
&apos;(i, j) :=
0 otherwise.
</equation>
<subsectionHeader confidence="0.968015">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.9992485">
During training, we proceed over all sentence pairs
and estimate the probabilities by determining the
relative frequencies of applying an alignment tem-
plate. Figure 3 shows an example of alignment
templates computed for a sentence pair from the
German TABA corpus.
</bodyText>
<subsectionHeader confidence="0.998322">
3.3 Search
</subsectionHeader>
<bodyText confidence="0.999229">
If we insert the alignment template model and
a standard left-to-right language model in the
source-channel approach (Eq. 2), we obtain the
following search criterion in maximum approxi-
mation which is used in combination with beam
search:
</bodyText>
<equation confidence="0.950462777777778">
ei =
-/ argmax{Pr(ef ) • Pr(fil
argmax max
lc,qc=4,fik,i4cerik,zr
el
{HP(eilei-1) H
k=1
• P(ZklEdk) • P(fklZk • Eak)}} • (4)
elm
</equation>
<page confidence="0.957915">
13
</page>
<figure confidence="0.99962976">
• •
@destination
Source Language Text
(Preprocessing)
... • •
@origin —[
..
r n
..
..
Global Search
=argmax E Anehm ,
..=1
@hellOirn
i
@want_questionf
@train determination
r n
7- 1
A1 • hi (el, fiJ)
A2 h2
Ad • hm(e, fiJ)
• • • •
• • • •
Target Language Text
</figure>
<figureCaption confidence="0.979941">
Figure 4: Architecture of the maximum entropy
model approach.
</figureCaption>
<sectionHeader confidence="0.974924" genericHeader="method">
4 Maximum Entropy Models
</sectionHeader>
<bodyText confidence="0.999781555555556">
As alternative to the source-channel approach,
we can directly model the posterior probability
Pr(eflfil). A well-founded framework for doing
this is maximum entropy (Berger et al., 1996). In
this framework, we have a set of /If feature func-
tions hm(ef,m = 1, ,M. For each fea-
ture function hm, there is a model parameter A,.
The posterior probability can then be modeled as
follows:
</bodyText>
<equation confidence="0.9922826">
PAivi
exp[ Am hm(ef,
m=1
E exp[E fif)]
m=1
</equation>
<bodyText confidence="0.999918833333333">
The architecture of the ME approach is summa-
rized in Figure 4.
For our approach, we determine the correspond-
ing formal target language concept for each word
of a natural language input. Therefore, we distin-
guish whether a word is an initial or a non-initial
word of a concept. This procedure yields a one-
to-one translation from source words to formal se-
mantic concepts, i.e. the length of both sequences
must be equal (I = J). Figure 5 depicts a one-
to-one mapping applied to a sentence/concept pair
from the German TABA corpus.
</bodyText>
<figure confidence="0.6178142">
@yes
MONA 0,110)g&gt;.1A&gt;i
&apos;In CD Z 0 4) g g 0 PI 0 E-1
H 0 H It H
U CD &apos;0 000
CD &gt;1 p
It
A
i-I 14
.c)
</figure>
<figureCaption confidence="0.992870666666667">
Figure 5: Example of a sentence/concept mapping
using maximum entropy (&apos;i&apos; denotes initial con-
cepts, &apos;n&apos; non-initial concepts resp.).
</figureCaption>
<bodyText confidence="0.9926262">
Further, we assume that the decisions only de-
pend on a limited window of = f3-2.••f3+2
around the current source word fj and on the two
predecessor concepts. Thus, we obtain the follow-
ing second-order model:
</bodyText>
<equation confidence="0.992546">
Pr(ef I fil)
—1 r j-F2
—2^ Jj-2 &apos;
</equation>
<bodyText confidence="0.998589125">
Transition constraints: Due to the distinction
between initial and non-initial concepts, we have
to ensure that a non-initial concept must only fol-
low its corresponding initial one. To guarantee
this, a straightforward method is to implement a
feature function that models the transitions and to
set the feature values of all invalid transitions to
zero, so that they will be discarded during search.
</bodyText>
<subsectionHeader confidence="0.976367">
4.1 Feature functions
</subsectionHeader>
<bodyText confidence="0.725562">
We have implemented a set of binary valued fea-
ture functions for our system:
Lexical features: The words f3+2 are compared
3-2
to a vocabulary. Words which are not found in the
vocabulary are mapped onto an &apos;unknown word&apos;.
</bodyText>
<figure confidence="0.918493">
(5)
model
</figure>
<page confidence="0.942046">
14
</page>
<bodyText confidence="0.771599">
Formally, the feature
</bodyText>
<equation confidence="0.791341">
hf,a,e( ei, fii+2 2) = 6(fi+d, f) • 61(e.e)
</equation>
<bodyText confidence="0.9121844">
d {-2, ,
will fire if the word fj±d matches the vocabulary
entry f and if the prediction for the current con-
cept equals e. 6(•,.) again denotes the Kronecker-
function.
</bodyText>
<listItem confidence="0.895796125">
Word features: Word characteristics are cov-
ered by the word features, which test for:
- Capitalization: These features will fire if fj
is capitalized, has an internal capital letter, or
is fully capitalized.
- Pre- and suffixes: If the prefix (suffix) of fj
equals a given prefix (suffix), these features
will fire.
</listItem>
<bodyText confidence="0.9684525">
Transition features: Transition features model
the dependence on the two predecessor concepts:
</bodyText>
<equation confidence="0.9970635">
hei,d,e, ej, j-2
j-1fi+2 = (5(ei—d, e&apos;) • 6(e
</equation>
<bodyText confidence="0.897612">
d c {I, .
Prior features: The single concept priors are in-
corporated by prior features. They just fire for the
currently observed concept:
</bodyText>
<equation confidence="0.9735825">
he(e f7+2) = 6(e e)
i-2 J • •
</equation>
<bodyText confidence="0.999794375">
Compound features: Using the feature func-
tions defined so far, we can only specify features
that refer to a single word or concept. To en-
able also word phrases and word/concept com-
binations, we introduce the following compound
features:
have been observed on the training data at least K
times. Although this method is not minimal, i e
the reduced feature set may still contain features
that are redundant or non-informative, it turned out
to perform well in practice. Experiments were car-
ried out with different thresholds. It turned out that
for the NLU task, a threshold of 2 for all features
achieved the best results, except for the prefix and
suffix features, for which a threshold of 6 yielded
best results.
</bodyText>
<subsectionHeader confidence="0.980676">
4.2 Training
</subsectionHeader>
<bodyText confidence="0.9941598">
For the purpose of training, we consider the set of
manually annotated and segmented training sen-
tences to form a single long sentence. As train-
ing criterion, we use the maximum class posterior
probability criterion:
</bodyText>
<equation confidence="0.999244333333333">
5■11‘1 = argmaxIN
E log pA AT (ef
41 n=1
</equation>
<bodyText confidence="0.999936894736842">
This corresponds to maximizing the likelihood of
the ME model. The direct optimization of the
posterior probability in Bayes&apos; decision rule is re-
ferred to as discriminative training in automatic
speech recognition since we directly take into ac-
count the overlap in the probability distributions.
Since the optimization criterion is convex, there is
only a single optimum and no convergence prob-
lems occur. To train the model parameters we
use the Generalized Iterative Scaling (GIS) algo-
rithm (Darroch and Ratcliff, 1972).
In practice, the training procedure tends to re-
sult in an overfitted model. To avoid overfit-
ting, (Chen and Rosenfeld, 1999) have suggested
a smoothing method where a Gaussian prior on the
parameters is assumed. Instead of maximizing the
probability of the training data, we now maximize
the probability of the training data times the prior
probability of the model parameters:
</bodyText>
<figure confidence="0.933293142857143">
—1 fj+2
—2, e.7 j-2
1 pj+2 501 = argmax p(An • Ep4i(en fTh)}
hZk ,dk ( — ,e. j. ) n=1
—2
k=1
Zk e ff,e&apos;l , dk e
</figure>
<figureCaption confidence="0.4250665">
Feature selection: Feature selection plays a cru-
cial role in the ME framework. In our system we
use simple count-based feature reduction. Given
a threshold K, we only include those features that
</figureCaption>
<bodyText confidence="0.982034">
where
</bodyText>
<equation confidence="0.967347333333333">
1
p(X) =
&apos; \/27ro- exp [2;]
</equation>
<page confidence="0.938427">
15
</page>
<subsectionHeader confidence="0.919022">
4.3 Search
</subsectionHeader>
<bodyText confidence="0.99981575">
In the test phase, the search is performed using the
so called maximum approximation, i.e. the most
likely sequence of concepts ef is chosen among
all possible sequences ef :
</bodyText>
<equation confidence="0.989557">
{Pr(ei fij)}
{ E Arrth,„,(ef, f)}.
Trt=1
</equation>
<bodyText confidence="0.99890425">
Therefore, the time-consuming renormalization in
Eq. 5 is not needed during search. We run a
Viterbi search to find the highest probability se-
quence (Borthwick et al., 1998).
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999942962962963">
Experiments were performed on the German in-
house Philips TABA corpusl and the German in-
house TELDIR corpus2. The TABA corpus is a
text corpus in the domain of a train timetable infor-
mation system (Aust et al., 1995). The TELDIR
corpus is derived from the domain of a tele-
phone directory assistance. Along with the bilin-
gual annotation consisting of the source and tar-
get sentences, the corpora also provide the affil-
iated alignments between source words and con-
cepts. The corpora allocations are summarized in
table 1 and table 2. For the TABA corpus, the tar-
get language consists of 27 flat semantic concepts
(23 concepts for the TELDIR application, resp.),
including a filler concept. Table 3 summarizes an
excerpt of the most frequently observed concepts.
In order to improve the quality of both ap-
proaches, we used a set of word categories. Since
it is unlikely that every city name is observed dur-
ing training, all city names were mapped onto the
category $ CI TY{c it y name}. Table 4 shows
an excerpt of different categories which were used
for both the training and the testing corpora.
We have computed three different evaluation
criteria:
- The concept error rate (CER), which is
equally defined to the well known word error
</bodyText>
<footnote confidence="0.9882765">
&apos;The TABA corpus was kindly provided by Philips
Forschungslaboratorien Aachen.
2The data-collection was partially funded by Ericsson Eu-
rolab Deutschland GmbH.
</footnote>
<tableCaption confidence="0.980111">
Table 1: Training and testing conditions for the
TABA corpus.
</tableCaption>
<table confidence="0.9996997">
Natural Concept
Language Language
Train Sentences 25 009
Tokens 87 213 48 325
Vocabulary 1 911 27
Singletons 621 0
Test Sentences 8 015
Tokens 22 963 12 745
00V 283 0
Trigram PP 4.36
</table>
<tableCaption confidence="0.9823855">
Table 2: Training and testing conditions for the
TELDIR corpus.
</tableCaption>
<table confidence="0.9994595">
Natural Concept
Language Language
Train Sentences 1 189
Tokens 6 850 3 356
Vocabulary 752 23
Singletons 276 2
Test Sentences 510
Tokens 3 041 1 480
00V 194 0
Trigram PP 4.49
</table>
<bodyText confidence="0.999323176470588">
rate. The CER describes the ratio of the sum
of deleted, inserted, and substituted concepts
w.r.t. a Levenshtein-alignment for a given ref-
erence concept-string, and the total number
of concepts in all reference strings.
The sentence error rate (SER), which is de-
fined as ratio between the number of falsely
translated sentences and the total number of
sentences w.r.t. the concept-level.
The concept-alignment error rate (C-AER),
which is defined as the ratio of the sum of
falsely aligned words, i.e. words mapped
onto the wrong concept, and the total num-
ber of words in the reference (Macherey et
al., 2001).
The error rates obtained by using the align-
ment templates method are summarized in table 5
</bodyText>
<figure confidence="0.97328725">
argmax
C&apos;
argma,x
C&apos;
</figure>
<page confidence="0.988455">
16
</page>
<tableCaption confidence="0.996783">
Table 3: Excerpt of the most frequently observed
concept for the TABA and the TELDIR corpus.
</tableCaption>
<table confidence="0.8526342">
Concept Example
@origin von $C1TY
@destination nach $C1TY
@person mit Herrn $SURNAME
@ organization mit der $COMPANY
</table>
<tableCaption confidence="0.820044333333333">
Table 5: Effect of alignment templates on different
error rates for the TABA corpus (Model 5* uses a
given alignment in training)
</tableCaption>
<table confidence="0.9997778">
Alignment ro]
Model
SER CER C-AER
Model 5 4.2 4.3 4.3
Model 5* 3.9 3.9 3.3
</table>
<tableCaption confidence="0.999646">
Table 4: Excerpt of used word categories.
</tableCaption>
<figure confidence="0.9321116">
Category Examples
$C1TY • Berlin
$DAYT1ME • Köln
$COMPANY • Morgen
$SURNAME • Vormittag
</figure>
<listItem confidence="0.8753835">
• BASF AG
• Porsche
• Schlegel
• Wagner
</listItem>
<bodyText confidence="0.99928352">
and table 6. Table 7 and table 8 show the per-
formance of the ME approach for different types
of ME features. Starting with only lexical fea-
tures, we successively extend our model by in-
cluding additional feature functions. As can be
seen from these results, the ME models clearly
outperform the alignment models. The quality of
the translation approach is achieved within the ME
framework by just including lexical and transition
features, and is significantly improved by adding
further feature functions. Comparing the perfor-
mance on the TABA task and on the TELDIR task,
we see that the error rates are much lower for the
TABA task than for the TELDIR task; the reason
is due to the very limited training data.
One of the advantages of the ME approach re-
sults from the property that the ME framework
directly models the posterior probability and al-
lows for integrating structural information by us-
ing appropriate feature functions. Furthermore,
the ME approach is consistent with the features
observed on the training data, but otherwise makes
the fewest possible assumptions about the distri-
bution. Since the optimization criterion is con-
vex, there is only a single optimum and no con-
</bodyText>
<tableCaption confidence="0.997384666666667">
Table 6: Effect of alignment templates on different
error rates for the TELDIR corpus (Model 5* uses
a given alignment in training)
</tableCaption>
<table confidence="0.999406">
Alignment ro]
Model
SER CER C-AER
Model 5 16.1 6.9 13.6
Model 5* 14.5 5.9 6.7
</table>
<tableCaption confidence="0.976410333333333">
Table 7: Dependence on the number of included
feature types on different error rates for the TABA
corpus.
</tableCaption>
<table confidence="0.999858">
Feature Fel
Types
SER CER C-AER
lexical 8.8 6.7 4.6
+ transition 4.3 3.3 3.2
+ prior 2.1 1.6 1.5
+ capitalization 1.8 1.4 1.4
+ pre- &amp; suffixes 1.6 1.2 1.3
+ compound 1.1 0.8 0.9
</table>
<tableCaption confidence="0.982571333333333">
Table 8: Dependence on the number of in-
cluded feature types on different error rates for the
TELDIR corpus.
</tableCaption>
<table confidence="0.999440888888889">
Feature 1%1
Types
SER CER C-AER
lexical 17.3 8.4 5.9
+ transition 13.5 5.6 5.4
+ prior 12.7 5.1 4.9
+ capitalization 12.0 4.8 4.9
+ pre- &amp; suffixes 9.6 3.6 4.4
+ compound 9.0 3.6 4.1
</table>
<bodyText confidence="0.995761">
vergence problems occur. Due to the manual an-
notation using initial and non-initial concepts, we
implicitly model a one-to-one alignment from nat-
</bodyText>
<page confidence="0.996745">
17
</page>
<bodyText confidence="0.999954818181818">
ural language words to semantic concepts whereas
the translation approach tries to learn the hidden
alignment automatically. We investigated the ef-
fect of this difference by keeping the segmenta-
tion of the training data fixed for the translation
approach. This approach is referred to as Model
5*, and the results are shown in table 5 and ta-
ble 6. As can be seen from these tables, this variant
of the translation approach has a somewhat lower
error rate, but is still outperformed by the ME ap-
proach.
</bodyText>
<sectionHeader confidence="0.99659" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.999932888888889">
In this paper, we have investigated two approaches
for natural language understanding: the alignment
templates approach which is based on the source-
channel paradigm and the maximum entropy ap-
proach which directly models the posterior prob-
ability. Both approaches were tested on two dif-
ferent corpora. We have shown that within these
settings the maximum entropy method clearly out-
performs the alignment templates approach.
</bodyText>
<sectionHeader confidence="0.99742" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99982764556962">
H. Aust, M. Oerder, F. Seide, and V. Steinbiss.
1995. The Philips automatic train timetable infor-
mation system. Speech Communication, 17:249-
262, November.
S. K. Bennacef, H. Bonnea-Maynard, J. L. Gauvain,
L. F. Lamel, and W. Minker. 1994. A spoken lan-
guage system for information retrieval. In Proc.
of the Int. Conf on Spoken Language Processing
(ICSLP&apos;94), pages 1271-1274, Yokohama, Japan,
September.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39-72, March.
A. Borthwick, J. Sterling, E. Agichtein, and R. Gr-
isham. 1998. NYU: Description of the MENE
named entity system as used in MUC-7. In Pro-
ceedings of the Seventh Message Understanding
Conference (MUC-7), 6 pages, Fairfax, VA, April.
http://www.itl.nist.gov/iatn/894.02/related_projects/mue/.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263-311.
S. Chen and R. Rosenfeld. 1999. A gaussian prior
for smoothing maximum entropy models. Techni-
cal Report CMUCS-99-108, Carnegie Mellon Uni-
versity, Pittsburgh, PA.
J. N. Darroch and D. Ratcliff. 1972. Generalized iter-
ative scaling for log-linear models. Annals of Math-
ematical Statistics, 43:1470-1480.
M. Epstein, K. Papineni, S. Roukos, T. Ward, and
S. Della Pietra. 1996. Statistical natural language
understanding using hidden clumpings. In Proc. Int.
Conf on Acoustics, Speech, and Signal Processing,
volume 1, pages 176-179, Atlanta, GA, May.
S. lssar and W. Ward. 1993. CMU&apos;s robust spoken lan-
guage understanding system. In European Conf on
Speech Communication and Technology, volume 3,
pages 2147-2149, Berlin, Germany, September.
E. Levin and R. Pieraccini. 1995. Concept-based spon-
taneous speech understanding system. In European
Conf on Speech Communication and Technology,
volume 2, pages 555-558, Madrid, Spain, Septem-
ber.
K. Macherey, F. J. Och, and H. Ney. 2001. Natu-
ral language understanding using statistical machine
translation. In European Conf. on Speech Communi-
cation and Technology, pages 2205-2208, Aalborg,
Denmark, September.
S. Miller, R. Bobrow, R. Ingria, and R. Schwartz. 1994.
Hidden understanding models of natural language.
In Proceedings of the Association of Computational
Linguistics, pages 25-32, June.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint SIGDAT Conf on Empirical
Methods in Natural Language Processing and Very
Large Corpora, pages 20-28, University of Mary-
land, College Park, MD, June.
K. A. Papineni, S. Roukos, and R. T. Ward. 1997.
Feature-based language understanding. In European
Conf on Speech Communication and Technology,
pages 1435-1438, Rhodes, Greece, September.
K. A. Papineni, S. Roukos, and R. T. Ward. 1998.
Maximum likelihood and discriminative training of
direct translation models. In Proc. Int. Conf on
Acoustics, Speech, and Signal Processing, pages
189-192, Seattle, WA, May.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Inter-
national Conference on Computational Linguistics,
volume 2, pages 836-841, August.
B. Zhou, Y. Gao, J. Sorensen, Z. Diao, and M. Picheny.
2002. Statistical natural language generation for
speech-to-speech machine translation systems. In
Proc. of the Int. Conf on Spoken Language Pro-
cessing (ICSLP&apos;02), pages 1897-1900, Denver, CO,
September.
</reference>
<page confidence="0.999289">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.589502">
<title confidence="0.9997175">Comparison of Alignment Templates and Maximum Entropy Models for Natural Language Understanding</title>
<author confidence="0.999849">Oliver Bender</author>
<author confidence="0.999849">Klaus Macherey</author>
<author confidence="0.999849">Franz Josef Och</author>
<author confidence="0.999849">Hermann Ney</author>
<affiliation confidence="0.993918">Lehrstuhl fiir Informatik VI, Computer Science Department RWTH Aachen - University of Technology</affiliation>
<address confidence="0.999919">D-52056 Aachen, Germany</address>
<email confidence="0.999758">fbender,k.macherey,och,neyl@informatik.rwth-aachen.de</email>
<abstract confidence="0.9984365">In this paper we compare two approaches to natural language understanding (NLU). The first approach is derived from the field of statistical matranslation the other uses the maximum entropy (ME) framework. Starting with an annotated corpus, we describe the problem of NLU as a translation from a source sentence to a formal language target sentence. We mainly focus on the quality of the different alignment and ME models and show that the direct ME approach outperforms the alignment templates method. ich warde gerne von Koln nach Munchen fahren V V @want question @origin @destination @going Figure 1: Example of a word/ concept mapping. proach uses the maximum entropy (ME) framework (Berger et al., 1996). For both frameworks, the objective can be described as follows. Given natural source sentence we the formal target language sentence = the highest probability among all possible target sentences: { (ej -1 ci</abstract>
<intro confidence="0.620057">el</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Aust</author>
<author>M Oerder</author>
<author>F Seide</author>
<author>V Steinbiss</author>
</authors>
<title>The Philips automatic train timetable information system.</title>
<date>1995</date>
<journal>Speech Communication,</journal>
<pages>17--249</pages>
<contexts>
<context position="14795" citStr="Aust et al., 1995" startWordPosition="2505" endWordPosition="2508">h In the test phase, the search is performed using the so called maximum approximation, i.e. the most likely sequence of concepts ef is chosen among all possible sequences ef : {Pr(ei fij)} { E Arrth,„,(ef, f)}. Trt=1 Therefore, the time-consuming renormalization in Eq. 5 is not needed during search. We run a Viterbi search to find the highest probability sequence (Borthwick et al., 1998). 5 Results Experiments were performed on the German inhouse Philips TABA corpusl and the German inhouse TELDIR corpus2. The TABA corpus is a text corpus in the domain of a train timetable information system (Aust et al., 1995). The TELDIR corpus is derived from the domain of a telephone directory assistance. Along with the bilingual annotation consisting of the source and target sentences, the corpora also provide the affiliated alignments between source words and concepts. The corpora allocations are summarized in table 1 and table 2. For the TABA corpus, the target language consists of 27 flat semantic concepts (23 concepts for the TELDIR application, resp.), including a filler concept. Table 3 summarizes an excerpt of the most frequently observed concepts. In order to improve the quality of both approaches, we u</context>
</contexts>
<marker>Aust, Oerder, Seide, Steinbiss, 1995</marker>
<rawString>H. Aust, M. Oerder, F. Seide, and V. Steinbiss. 1995. The Philips automatic train timetable information system. Speech Communication, 17:249-262, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K Bennacef</author>
<author>H Bonnea-Maynard</author>
<author>J L Gauvain</author>
<author>L F Lamel</author>
<author>W Minker</author>
</authors>
<title>A spoken language system for information retrieval.</title>
<date>1994</date>
<booktitle>In Proc. of the Int. Conf on Spoken Language Processing (ICSLP&apos;94),</booktitle>
<pages>1271--1274</pages>
<location>Yokohama, Japan,</location>
<contexts>
<context position="4659" citStr="Bennacef et al., 1994" startWordPosition="735" endWordPosition="738">the features that were used for the experiments. Section 5 presents results for both the alignment templates approach and the ME framework. For both approaches, experiments were carried out on two different German NLU tasks. 2 Concept-based semantic representation A crucial decision, when designing an NLU system, is the choice of a suitable semantic representation, since interpreting a user&apos;s request requires an appropriate formalism to represent the meaning of an utterance. Different semantic representations have been proposed. Among them, case frames (Issar and Ward, 1993), semantic frames (Bennacef et al., 1994), and variants of hierarchical concepts (Miller et al., 1994) as well as flat concepts (Levin and Pieraccini, 1995) are the most prominent. Since we regard NLU as a special case of a translation problem, we have chosen a flat concept-based target language as meaning representation. A semantic concept (in the following briefly termed as concept) is defined as the smallest unit of meaning that is relevant to a specific task (Levin and Pieraccini, 1995). Figure 1 depicts an example of a concept-based meaning representation for the input utterance &apos;I would like to go from Munich to Cologne&apos; from t</context>
</contexts>
<marker>Bennacef, Bonnea-Maynard, Gauvain, Lamel, Minker, 1994</marker>
<rawString>S. K. Bennacef, H. Bonnea-Maynard, J. L. Gauvain, L. F. Lamel, and W. Minker. 1994. A spoken language system for information retrieval. In Proc. of the Int. Conf on Spoken Language Processing (ICSLP&apos;94), pages 1271-1274, Yokohama, Japan, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="1073" citStr="Berger et al., 1996" startWordPosition="163" endWordPosition="166">d from the field of statistical machine translation (MT), whereas the other uses the maximum entropy (ME) framework. Starting with an annotated corpus, we describe the problem of NLU as a translation from a source sentence to a formal language target sentence. We mainly focus on the quality of the different alignment and ME models and show that the direct ME approach outperforms the alignment templates method. ich warde gerne von Koln nach Munchen fahren V V @want question @origin @destination @going Figure 1: Example of a word/ concept mapping. proach uses the maximum entropy (ME) framework (Berger et al., 1996). For both frameworks, the objective can be described as follows. Given a natural source sentence fiJ = fj...f./ we choose the formal target language sentence ef = el •••e,...ei with the highest probability among all possible target sentences: argmax { Pr (ej ei -1 ci fi&apos;) (1) argmax r(fi,,,)•pr(ef) el 1 Introduction Pr (f&apos;) The objective of natural language understanding (NLU) is to extract all the information from a natural language based input which are relevant for a specific task. Typical applications using NLU components are spoken dialogue systems (Levin and Pieraccini, 1995) or speech-</context>
<context position="9176" citStr="Berger et al., 1996" startWordPosition="1524" endWordPosition="1527">c,qc=4,fik,i4cerik,zr el {HP(eilei-1) H k=1 • P(ZklEdk) • P(fklZk • Eak)}} • (4) elm 13 • • @destination Source Language Text (Preprocessing) ... • • @origin —[ .. r n .. .. Global Search =argmax E Anehm , ..=1 @hellOirn i @want_questionf @train determination r n 7- 1 A1 • hi (el, fiJ) A2 h2 Ad • hm(e, fiJ) • • • • • • • • Target Language Text Figure 4: Architecture of the maximum entropy model approach. 4 Maximum Entropy Models As alternative to the source-channel approach, we can directly model the posterior probability Pr(eflfil). A well-founded framework for doing this is maximum entropy (Berger et al., 1996). In this framework, we have a set of /If feature functions hm(ef,m = 1, ,M. For each feature function hm, there is a model parameter A,. The posterior probability can then be modeled as follows: PAivi exp[ Am hm(ef, m=1 E exp[E fif)] m=1 The architecture of the ME approach is summarized in Figure 4. For our approach, we determine the corresponding formal target language concept for each word of a natural language input. Therefore, we distinguish whether a word is an initial or a non-initial word of a concept. This procedure yields a oneto-one translation from source words to formal semantic c</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-72, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
<author>J Sterling</author>
<author>E Agichtein</author>
<author>R Grisham</author>
</authors>
<title>NYU: Description of the MENE named entity system as used in MUC-7.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7),</booktitle>
<volume>6</volume>
<pages>pages,</pages>
<location>Fairfax, VA,</location>
<contexts>
<context position="14568" citStr="Borthwick et al., 1998" startWordPosition="2464" endWordPosition="2467">election: Feature selection plays a crucial role in the ME framework. In our system we use simple count-based feature reduction. Given a threshold K, we only include those features that where 1 p(X) = &apos; \/27ro- exp [2;] 15 4.3 Search In the test phase, the search is performed using the so called maximum approximation, i.e. the most likely sequence of concepts ef is chosen among all possible sequences ef : {Pr(ei fij)} { E Arrth,„,(ef, f)}. Trt=1 Therefore, the time-consuming renormalization in Eq. 5 is not needed during search. We run a Viterbi search to find the highest probability sequence (Borthwick et al., 1998). 5 Results Experiments were performed on the German inhouse Philips TABA corpusl and the German inhouse TELDIR corpus2. The TABA corpus is a text corpus in the domain of a train timetable information system (Aust et al., 1995). The TELDIR corpus is derived from the domain of a telephone directory assistance. Along with the bilingual annotation consisting of the source and target sentences, the corpora also provide the affiliated alignments between source words and concepts. The corpora allocations are summarized in table 1 and table 2. For the TABA corpus, the target language consists of 27 f</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grisham, 1998</marker>
<rawString>A. Borthwick, J. Sterling, E. Agichtein, and R. Grisham. 1998. NYU: Description of the MENE named entity system as used in MUC-7. In Proceedings of the Seventh Message Understanding Conference (MUC-7), 6 pages, Fairfax, VA, April. http://www.itl.nist.gov/iatn/894.02/related_projects/mue/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="2012" citStr="Brown et al., 1993" startWordPosition="313" endWordPosition="316">r (f&apos;) The objective of natural language understanding (NLU) is to extract all the information from a natural language based input which are relevant for a specific task. Typical applications using NLU components are spoken dialogue systems (Levin and Pieraccini, 1995) or speech-to-speech translation systems (Zhou et al., 2002). In this paper we present two approaches for analyzing the semantics of natural language inputs and discuss their advantages and drawbacks. The first approach is derived from the field of statistical machine translation (MT) and is based on the source-channel paradigm (Brown et al., 1993). Here, we apply a method called alignment templates (Och et al., 1999). The alternative apargmax { Pr(fief) • Pr(ef) • (2) e Using Bayes&apos; theorem, Eq. 1 can be rewritten to Eq. 2, where the denominator can be neglected. The argmax operation denotes the search problem, i.e. the generation of the sequence of formal semantic concepts in the target language. An example is depicted in Figure 1. The main difference between both approaches is that the ME framework directly models the posterior probabilities whereas the statistical machine translation approach applies Bayes&apos; theorem resulting in two </context>
<context position="6131" citStr="Brown et al., 1993" startWordPosition="976" endWordPosition="979">e alignments between source and target words. 3 Alignment Templates The statistical machine translation approach decomposes Pr(eflg) into two probability distributions, the language model probability and the translation probability. The architecture of this method is depicted in figure 2. For the translation approach, we use the same training procedure as for the automatic translation of natural languages. When rewriting the translation probability Pr(fiJ 4) by introducing a &apos;hidden&apos; alignment al = with aj C {1,...,1}, we obtain: 611) = Pr(fi&apos; = E Pr(fi, cr-7 =1 The IBM models as proposed by (Brown et al., 1993) and the HMM model as suggested by (Vogel et al., 1996) result from different decompositions of Pr(fi1,4 4). For training the alignment model, we train a sequence of models of increasing complexity. Starting from the first model IBM1, we proceed over the HMM model, IBM3 up to IBMS. Using the model IBMS as a result of the last training step, we use the alignment template approach to model whole word groups. (3) 1.3-1 3-1 ,a1 • ei) • 12 Global Search = argmax {Pr(eI) • Pr(fiT Source Language Text (Preprocessing) Target Language Text Pr(fij leD Pr(e) HLexicon Model HAlignment Model Language Model</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMUCS-99-108,</tech>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="13570" citStr="Chen and Rosenfeld, 1999" startWordPosition="2286" endWordPosition="2289">ximizing the likelihood of the ME model. The direct optimization of the posterior probability in Bayes&apos; decision rule is referred to as discriminative training in automatic speech recognition since we directly take into account the overlap in the probability distributions. Since the optimization criterion is convex, there is only a single optimum and no convergence problems occur. To train the model parameters we use the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972). In practice, the training procedure tends to result in an overfitted model. To avoid overfitting, (Chen and Rosenfeld, 1999) have suggested a smoothing method where a Gaussian prior on the parameters is assumed. Instead of maximizing the probability of the training data, we now maximize the probability of the training data times the prior probability of the model parameters: —1 fj+2 —2, e.7 j-2 1 pj+2 501 = argmax p(An • Ep4i(en fTh)} hZk ,dk ( — ,e. j. ) n=1 —2 k=1 Zk e ff,e&apos;l , dk e Feature selection: Feature selection plays a crucial role in the ME framework. In our system we use simple count-based feature reduction. Given a threshold K, we only include those features that where 1 p(X) = &apos; \/27ro- exp [2;] 15 4.</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>S. Chen and R. Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical Report CMUCS-99-108, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>43--1470</pages>
<contexts>
<context position="13444" citStr="Darroch and Ratcliff, 1972" startWordPosition="2265" endWordPosition="2268">erion, we use the maximum class posterior probability criterion: 5■11‘1 = argmaxIN E log pA AT (ef 41 n=1 This corresponds to maximizing the likelihood of the ME model. The direct optimization of the posterior probability in Bayes&apos; decision rule is referred to as discriminative training in automatic speech recognition since we directly take into account the overlap in the probability distributions. Since the optimization criterion is convex, there is only a single optimum and no convergence problems occur. To train the model parameters we use the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972). In practice, the training procedure tends to result in an overfitted model. To avoid overfitting, (Chen and Rosenfeld, 1999) have suggested a smoothing method where a Gaussian prior on the parameters is assumed. Instead of maximizing the probability of the training data, we now maximize the probability of the training data times the prior probability of the model parameters: —1 fj+2 —2, e.7 j-2 1 pj+2 501 = argmax p(An • Ep4i(en fTh)} hZk ,dk ( — ,e. j. ) n=1 —2 k=1 Zk e ff,e&apos;l , dk e Feature selection: Feature selection plays a crucial role in the ME framework. In our system we use simple c</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. Annals of Mathematical Statistics, 43:1470-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Epstein</author>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>S Della Pietra</author>
</authors>
<title>Statistical natural language understanding using hidden clumpings.</title>
<date>1996</date>
<booktitle>In Proc. Int. Conf on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>176--179</pages>
<location>Atlanta, GA,</location>
<contexts>
<context position="3057" citStr="Epstein et al., 1996" startWordPosition="483" endWordPosition="486">th approaches is that the ME framework directly models the posterior probabilities whereas the statistical machine translation approach applies Bayes&apos; theorem resulting in two distributions: the translation probability Pr(fillef) and the language model probability Pr(en. In the following, we compare both ap11 proaches for two NLU tasks which are derived from two different domains and show that the ME approach clearly outperforms the statistical machine translation approach within these settings. 1.1 Related Work The use of statistical machine translation for NLU tasks was firstly proposed by (Epstein et al., 1996). Whereas (Epstein et al., 1996) model hidden clumpings, we use a method called alignment templates. Alignment templates have been proven to be very powerful for statistical machine translation tasks since they allow for many-to-many alignments between source and target words (Och et al., 1999). Alignment templates for NLU tasks were firstly proposed by (Macherey et al., 2001). Applying ME translation models to NLU has been firstly suggested by (Papineni et al., 1997; Papineni et al., 1998). Here, we use a conceptbased meaning representation as formal target language and propose different feat</context>
</contexts>
<marker>Epstein, Papineni, Roukos, Ward, Pietra, 1996</marker>
<rawString>M. Epstein, K. Papineni, S. Roukos, T. Ward, and S. Della Pietra. 1996. Statistical natural language understanding using hidden clumpings. In Proc. Int. Conf on Acoustics, Speech, and Signal Processing, volume 1, pages 176-179, Atlanta, GA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S lssar</author>
<author>W Ward</author>
</authors>
<title>CMU&apos;s robust spoken language understanding system.</title>
<date>1993</date>
<booktitle>In European Conf on Speech Communication and Technology,</booktitle>
<volume>3</volume>
<pages>2147--2149</pages>
<location>Berlin, Germany,</location>
<marker>lssar, Ward, 1993</marker>
<rawString>S. lssar and W. Ward. 1993. CMU&apos;s robust spoken language understanding system. In European Conf on Speech Communication and Technology, volume 3, pages 2147-2149, Berlin, Germany, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
</authors>
<title>Concept-based spontaneous speech understanding system.</title>
<date>1995</date>
<booktitle>In European Conf on Speech Communication and Technology,</booktitle>
<volume>2</volume>
<pages>555--558</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="1662" citStr="Levin and Pieraccini, 1995" startWordPosition="257" endWordPosition="260">y (ME) framework (Berger et al., 1996). For both frameworks, the objective can be described as follows. Given a natural source sentence fiJ = fj...f./ we choose the formal target language sentence ef = el •••e,...ei with the highest probability among all possible target sentences: argmax { Pr (ej ei -1 ci fi&apos;) (1) argmax r(fi,,,)•pr(ef) el 1 Introduction Pr (f&apos;) The objective of natural language understanding (NLU) is to extract all the information from a natural language based input which are relevant for a specific task. Typical applications using NLU components are spoken dialogue systems (Levin and Pieraccini, 1995) or speech-to-speech translation systems (Zhou et al., 2002). In this paper we present two approaches for analyzing the semantics of natural language inputs and discuss their advantages and drawbacks. The first approach is derived from the field of statistical machine translation (MT) and is based on the source-channel paradigm (Brown et al., 1993). Here, we apply a method called alignment templates (Och et al., 1999). The alternative apargmax { Pr(fief) • Pr(ef) • (2) e Using Bayes&apos; theorem, Eq. 1 can be rewritten to Eq. 2, where the denominator can be neglected. The argmax operation denotes </context>
<context position="4774" citStr="Levin and Pieraccini, 1995" startWordPosition="754" endWordPosition="757">pproach and the ME framework. For both approaches, experiments were carried out on two different German NLU tasks. 2 Concept-based semantic representation A crucial decision, when designing an NLU system, is the choice of a suitable semantic representation, since interpreting a user&apos;s request requires an appropriate formalism to represent the meaning of an utterance. Different semantic representations have been proposed. Among them, case frames (Issar and Ward, 1993), semantic frames (Bennacef et al., 1994), and variants of hierarchical concepts (Miller et al., 1994) as well as flat concepts (Levin and Pieraccini, 1995) are the most prominent. Since we regard NLU as a special case of a translation problem, we have chosen a flat concept-based target language as meaning representation. A semantic concept (in the following briefly termed as concept) is defined as the smallest unit of meaning that is relevant to a specific task (Levin and Pieraccini, 1995). Figure 1 depicts an example of a concept-based meaning representation for the input utterance &apos;I would like to go from Munich to Cologne&apos; from the domain of a German traintimetable information system. The first line shows the source sentence, the last line de</context>
</contexts>
<marker>Levin, Pieraccini, 1995</marker>
<rawString>E. Levin and R. Pieraccini. 1995. Concept-based spontaneous speech understanding system. In European Conf on Speech Communication and Technology, volume 2, pages 555-558, Madrid, Spain, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Macherey</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Natural language understanding using statistical machine translation.</title>
<date>2001</date>
<booktitle>In European Conf. on Speech Communication and Technology,</booktitle>
<pages>2205--2208</pages>
<location>Aalborg, Denmark,</location>
<contexts>
<context position="3436" citStr="Macherey et al., 2001" startWordPosition="542" endWordPosition="545">ains and show that the ME approach clearly outperforms the statistical machine translation approach within these settings. 1.1 Related Work The use of statistical machine translation for NLU tasks was firstly proposed by (Epstein et al., 1996). Whereas (Epstein et al., 1996) model hidden clumpings, we use a method called alignment templates. Alignment templates have been proven to be very powerful for statistical machine translation tasks since they allow for many-to-many alignments between source and target words (Och et al., 1999). Alignment templates for NLU tasks were firstly proposed by (Macherey et al., 2001). Applying ME translation models to NLU has been firstly suggested by (Papineni et al., 1997; Papineni et al., 1998). Here, we use a conceptbased meaning representation as formal target language and propose different features and structural constraints in order to improve the NLU results. The remainder of the paper is organized as follows: in the following section, we briefly describe the concept based meaning representation as used for the NLU task. Section 3 describes the training and search procedure of the alignment templates approach. In section 4, we outline the ME framework and describe</context>
<context position="17066" citStr="Macherey et al., 2001" startWordPosition="2889" endWordPosition="2892">gram PP 4.49 rate. The CER describes the ratio of the sum of deleted, inserted, and substituted concepts w.r.t. a Levenshtein-alignment for a given reference concept-string, and the total number of concepts in all reference strings. The sentence error rate (SER), which is defined as ratio between the number of falsely translated sentences and the total number of sentences w.r.t. the concept-level. The concept-alignment error rate (C-AER), which is defined as the ratio of the sum of falsely aligned words, i.e. words mapped onto the wrong concept, and the total number of words in the reference (Macherey et al., 2001). The error rates obtained by using the alignment templates method are summarized in table 5 argmax C&apos; argma,x C&apos; 16 Table 3: Excerpt of the most frequently observed concept for the TABA and the TELDIR corpus. Concept Example @origin von $C1TY @destination nach $C1TY @person mit Herrn $SURNAME @ organization mit der $COMPANY Table 5: Effect of alignment templates on different error rates for the TABA corpus (Model 5* uses a given alignment in training) Alignment ro] Model SER CER C-AER Model 5 4.2 4.3 4.3 Model 5* 3.9 3.9 3.3 Table 4: Excerpt of used word categories. Category Examples $C1TY • </context>
</contexts>
<marker>Macherey, Och, Ney, 2001</marker>
<rawString>K. Macherey, F. J. Och, and H. Ney. 2001. Natural language understanding using statistical machine translation. In European Conf. on Speech Communication and Technology, pages 2205-2208, Aalborg, Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>R Bobrow</author>
<author>R Ingria</author>
<author>R Schwartz</author>
</authors>
<title>Hidden understanding models of natural language.</title>
<date>1994</date>
<booktitle>In Proceedings of the Association of Computational Linguistics,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="4720" citStr="Miller et al., 1994" startWordPosition="745" endWordPosition="748">ents results for both the alignment templates approach and the ME framework. For both approaches, experiments were carried out on two different German NLU tasks. 2 Concept-based semantic representation A crucial decision, when designing an NLU system, is the choice of a suitable semantic representation, since interpreting a user&apos;s request requires an appropriate formalism to represent the meaning of an utterance. Different semantic representations have been proposed. Among them, case frames (Issar and Ward, 1993), semantic frames (Bennacef et al., 1994), and variants of hierarchical concepts (Miller et al., 1994) as well as flat concepts (Levin and Pieraccini, 1995) are the most prominent. Since we regard NLU as a special case of a translation problem, we have chosen a flat concept-based target language as meaning representation. A semantic concept (in the following briefly termed as concept) is defined as the smallest unit of meaning that is relevant to a specific task (Levin and Pieraccini, 1995). Figure 1 depicts an example of a concept-based meaning representation for the input utterance &apos;I would like to go from Munich to Cologne&apos; from the domain of a German traintimetable information system. The </context>
</contexts>
<marker>Miller, Bobrow, Ingria, Schwartz, 1994</marker>
<rawString>S. Miller, R. Bobrow, R. Ingria, and R. Schwartz. 1994. Hidden understanding models of natural language. In Proceedings of the Association of Computational Linguistics, pages 25-32, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint SIGDAT Conf on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<institution>University of Maryland,</institution>
<location>College Park, MD,</location>
<contexts>
<context position="2083" citStr="Och et al., 1999" startWordPosition="326" endWordPosition="329">t all the information from a natural language based input which are relevant for a specific task. Typical applications using NLU components are spoken dialogue systems (Levin and Pieraccini, 1995) or speech-to-speech translation systems (Zhou et al., 2002). In this paper we present two approaches for analyzing the semantics of natural language inputs and discuss their advantages and drawbacks. The first approach is derived from the field of statistical machine translation (MT) and is based on the source-channel paradigm (Brown et al., 1993). Here, we apply a method called alignment templates (Och et al., 1999). The alternative apargmax { Pr(fief) • Pr(ef) • (2) e Using Bayes&apos; theorem, Eq. 1 can be rewritten to Eq. 2, where the denominator can be neglected. The argmax operation denotes the search problem, i.e. the generation of the sequence of formal semantic concepts in the target language. An example is depicted in Figure 1. The main difference between both approaches is that the ME framework directly models the posterior probabilities whereas the statistical machine translation approach applies Bayes&apos; theorem resulting in two distributions: the translation probability Pr(fillef) and the language </context>
<context position="3352" citStr="Och et al., 1999" startWordPosition="529" endWordPosition="532">e both ap11 proaches for two NLU tasks which are derived from two different domains and show that the ME approach clearly outperforms the statistical machine translation approach within these settings. 1.1 Related Work The use of statistical machine translation for NLU tasks was firstly proposed by (Epstein et al., 1996). Whereas (Epstein et al., 1996) model hidden clumpings, we use a method called alignment templates. Alignment templates have been proven to be very powerful for statistical machine translation tasks since they allow for many-to-many alignments between source and target words (Och et al., 1999). Alignment templates for NLU tasks were firstly proposed by (Macherey et al., 2001). Applying ME translation models to NLU has been firstly suggested by (Papineni et al., 1997; Papineni et al., 1998). Here, we use a conceptbased meaning representation as formal target language and propose different features and structural constraints in order to improve the NLU results. The remainder of the paper is organized as follows: in the following section, we briefly describe the concept based meaning representation as used for the NLU task. Section 3 describes the training and search procedure of the </context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>F. J. Och, C. Tillmann, and H. Ney. 1999. Improved alignment models for statistical machine translation. In Proc. of the Joint SIGDAT Conf on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20-28, University of Maryland, College Park, MD, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>R T Ward</author>
</authors>
<title>Feature-based language understanding.</title>
<date>1997</date>
<booktitle>In European Conf on Speech Communication and Technology,</booktitle>
<pages>1435--1438</pages>
<location>Rhodes, Greece,</location>
<contexts>
<context position="3528" citStr="Papineni et al., 1997" startWordPosition="557" endWordPosition="560">proach within these settings. 1.1 Related Work The use of statistical machine translation for NLU tasks was firstly proposed by (Epstein et al., 1996). Whereas (Epstein et al., 1996) model hidden clumpings, we use a method called alignment templates. Alignment templates have been proven to be very powerful for statistical machine translation tasks since they allow for many-to-many alignments between source and target words (Och et al., 1999). Alignment templates for NLU tasks were firstly proposed by (Macherey et al., 2001). Applying ME translation models to NLU has been firstly suggested by (Papineni et al., 1997; Papineni et al., 1998). Here, we use a conceptbased meaning representation as formal target language and propose different features and structural constraints in order to improve the NLU results. The remainder of the paper is organized as follows: in the following section, we briefly describe the concept based meaning representation as used for the NLU task. Section 3 describes the training and search procedure of the alignment templates approach. In section 4, we outline the ME framework and describe the features that were used for the experiments. Section 5 presents results for both the al</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1997</marker>
<rawString>K. A. Papineni, S. Roukos, and R. T. Ward. 1997. Feature-based language understanding. In European Conf on Speech Communication and Technology, pages 1435-1438, Rhodes, Greece, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>R T Ward</author>
</authors>
<title>Maximum likelihood and discriminative training of direct translation models.</title>
<date>1998</date>
<booktitle>In Proc. Int. Conf on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>189--192</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="3552" citStr="Papineni et al., 1998" startWordPosition="561" endWordPosition="564">tings. 1.1 Related Work The use of statistical machine translation for NLU tasks was firstly proposed by (Epstein et al., 1996). Whereas (Epstein et al., 1996) model hidden clumpings, we use a method called alignment templates. Alignment templates have been proven to be very powerful for statistical machine translation tasks since they allow for many-to-many alignments between source and target words (Och et al., 1999). Alignment templates for NLU tasks were firstly proposed by (Macherey et al., 2001). Applying ME translation models to NLU has been firstly suggested by (Papineni et al., 1997; Papineni et al., 1998). Here, we use a conceptbased meaning representation as formal target language and propose different features and structural constraints in order to improve the NLU results. The remainder of the paper is organized as follows: in the following section, we briefly describe the concept based meaning representation as used for the NLU task. Section 3 describes the training and search procedure of the alignment templates approach. In section 4, we outline the ME framework and describe the features that were used for the experiments. Section 5 presents results for both the alignment templates approa</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1998</marker>
<rawString>K. A. Papineni, S. Roukos, and R. T. Ward. 1998. Maximum likelihood and discriminative training of direct translation models. In Proc. Int. Conf on Acoustics, Speech, and Signal Processing, pages 189-192, Seattle, WA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>836--841</pages>
<contexts>
<context position="6186" citStr="Vogel et al., 1996" startWordPosition="987" endWordPosition="991">nt Templates The statistical machine translation approach decomposes Pr(eflg) into two probability distributions, the language model probability and the translation probability. The architecture of this method is depicted in figure 2. For the translation approach, we use the same training procedure as for the automatic translation of natural languages. When rewriting the translation probability Pr(fiJ 4) by introducing a &apos;hidden&apos; alignment al = with aj C {1,...,1}, we obtain: 611) = Pr(fi&apos; = E Pr(fi, cr-7 =1 The IBM models as proposed by (Brown et al., 1993) and the HMM model as suggested by (Vogel et al., 1996) result from different decompositions of Pr(fi1,4 4). For training the alignment model, we train a sequence of models of increasing complexity. Starting from the first model IBM1, we proceed over the HMM model, IBM3 up to IBMS. Using the model IBMS as a result of the last training step, we use the alignment template approach to model whole word groups. (3) 1.3-1 3-1 ,a1 • ei) • 12 Global Search = argmax {Pr(eI) • Pr(fiT Source Language Text (Preprocessing) Target Language Text Pr(fij leD Pr(e) HLexicon Model HAlignment Model Language Model @destination @origin @train determination @want_guesti</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word alignment in statistical translation. In International Conference on Computational Linguistics, volume 2, pages 836-841, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhou</author>
<author>Y Gao</author>
<author>J Sorensen</author>
<author>Z Diao</author>
<author>M Picheny</author>
</authors>
<title>Statistical natural language generation for speech-to-speech machine translation systems.</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf on Spoken Language Processing (ICSLP&apos;02),</booktitle>
<pages>1897--1900</pages>
<location>Denver, CO,</location>
<contexts>
<context position="1722" citStr="Zhou et al., 2002" startWordPosition="266" endWordPosition="269">ctive can be described as follows. Given a natural source sentence fiJ = fj...f./ we choose the formal target language sentence ef = el •••e,...ei with the highest probability among all possible target sentences: argmax { Pr (ej ei -1 ci fi&apos;) (1) argmax r(fi,,,)•pr(ef) el 1 Introduction Pr (f&apos;) The objective of natural language understanding (NLU) is to extract all the information from a natural language based input which are relevant for a specific task. Typical applications using NLU components are spoken dialogue systems (Levin and Pieraccini, 1995) or speech-to-speech translation systems (Zhou et al., 2002). In this paper we present two approaches for analyzing the semantics of natural language inputs and discuss their advantages and drawbacks. The first approach is derived from the field of statistical machine translation (MT) and is based on the source-channel paradigm (Brown et al., 1993). Here, we apply a method called alignment templates (Och et al., 1999). The alternative apargmax { Pr(fief) • Pr(ef) • (2) e Using Bayes&apos; theorem, Eq. 1 can be rewritten to Eq. 2, where the denominator can be neglected. The argmax operation denotes the search problem, i.e. the generation of the sequence of f</context>
</contexts>
<marker>Zhou, Gao, Sorensen, Diao, Picheny, 2002</marker>
<rawString>B. Zhou, Y. Gao, J. Sorensen, Z. Diao, and M. Picheny. 2002. Statistical natural language generation for speech-to-speech machine translation systems. In Proc. of the Int. Conf on Spoken Language Processing (ICSLP&apos;02), pages 1897-1900, Denver, CO, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>