<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000063">
<title confidence="0.992728">
Automatic Sanskrit Segmentizer Using Finite State Transducers
</title>
<author confidence="0.987315">
Vipul Mittal
</author>
<affiliation confidence="0.9279305">
Language Technologies Research Center, IIIT-H,
Gachibowli, Hyderabad, India.
</affiliation>
<email confidence="0.985544">
vipulmittal@research.iiit.ac.in
</email>
<sectionHeader confidence="0.993616" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999115444444444">
In this paper, we propose a novel method
for automatic segmentation of a Sanskrit
string into different words. The input for
our segmentizer is a Sanskrit string either
encoded as a Unicode string or as a Ro-
man transliterated string and the output is
a set of possible splits with weights associ-
ated with each of them. We followed two
different approaches to segment a Sanskrit
text using sandhi1 rules extracted from a
parallel corpus of manually sandhi split
text. While the first approach augments
the finite state transducer used to analyze
Sanskrit morphology and traverse it to seg-
ment a word, the second approach gener-
ates all possible segmentations and vali-
dates each constituent using a morph an-
alyzer.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983384615385">
Sanskrit has a rich tradition of oral transmission
of texts and this process causes the text to un-
dergo euphonic changes at the word boundaries.
In oral transmission, the text is predominantly spo-
ken as a continuous speech. However, continuous
speech makes the text ambiguous. To overcome
this problem, there is also a tradition of reciting
the pada-p¯at.ha (recitation of words) in addition to
the recitation of a sam. hit¯a (a continuous sandhied
text). In the written form, because of the domi-
nance of oral transmission, the text is written as a
continuous string of letters rather than a sequence
of words. Thus, the Sanskrit texts consist of a very
</bodyText>
<footnote confidence="0.998863142857143">
1Sandhi means euphony transformation of words when
they are consecutively pronounced. Typically when a word
w1 is followed by a word w2, some terminal segment of w1
merges with some initial segment of w2 to be replaced by
a “smoothed” phonetic interpolation, corresponding to mini-
mizing the energy necessary to reconfigurate the vocal organs
at the juncture between the words.
</footnote>
<bodyText confidence="0.999680347826087">
long sequence of phonemes, with the word bound-
aries having undergone euphonic changes. This
makes it difficult to split a continuous string into
words and process the text automatically.
Sanskrit words are mostly analyzed by build-
ing a finite state transducer (Beesley, 1998). In
the first approach, this transducer was modified
by linking the final states to appropriate interme-
diate states incorporating the sandhi rules. This
approach then allows one to traverse the string
from left to right and generate all and only possible
splits that are morphologically valid. The second
approach is very closely based on the Optimality
Theory (Prince and Smolensky, 1993) where we
generate all the possible splits for a word and vali-
date each using a morphological analyzer. We use
one of the fastest morphological analyzers avail-
able viz. the one developed by Apertium group2.
The splits that are not validated are pruned out.
Based on the number of times the first answer is
correct, we achieved an accuracy of around 92%
using the second approach while the first approach
performed with around 71% accuracy.
</bodyText>
<sectionHeader confidence="0.657164" genericHeader="introduction">
2 Issues involved in Sanskrit Processing
</sectionHeader>
<bodyText confidence="0.987313642857143">
The segmentizer is an important component of
an NLP system. Especially, languages such
as Chinese (Badino, 2004), Japanese, Thai
(Haruechaiyasak, 2008) or Vietnamese (Thang et
al. , 2008) which do not mark word bound-
aries explicitly or highly agglutinative languages
like Turkish need segmentizers. In all these lan-
guages, there are no explicit delimiters to spec-
ify the word boundaries. In Thai, each syllable
is transcribed using several characters and there
is no space in the text between syllables. So the
problem of segmentation is basically twofold: (1)
syllable segmentation followed by (2) word seg-
mentation itself. A sentence in these languages
</bodyText>
<footnote confidence="0.9991135">
2http://wiki.apertium.org/wiki/lttoolbox; It processes
around 50,000 words per sec.
</footnote>
<page confidence="0.994089">
85
</page>
<note confidence="0.6052025">
Proceedings of the ACL 2010 Student Research Workshop, pages 85–90,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.987259426829268">
is segmented by predicting the word boundaries,
where euphonic changes do not occur across the
word boundaries and it is more like mere concate-
nation of words. So the task here is just to choose
between various combinations of the words in a
sentence.
However, in Sanskrit, euphonic changes occur
across word boundaries leading to addition and
deletion of some original part of the combining
words. These euphonic changes in Sanskrit intro-
duce non-determinism in the segmentation. This
makes the segmentation process in Sanskrit more
complex than in Chinese or Japanese. In case of
highly agglutinative languages like Turkish, the
components are related to each other semantically
involving dependency analysis. Whereas in
Sanskrit, only the compounds involve a certain
level of dependency analysis, while sandhi is just
gluing of words together, without the need for
words to be related semantically. For example,
consider the following part of a verse,
San: n¯aradam paripapraccha
v¯alm¯ıkirmunipu˙ngavam
gloss: to the Narada asked Valmiki-
to the wisest among sages
Eng: Valmiki asked the Narada, the wisest among
the sages.
In the above verse, the words v¯alm¯ıkih. and mu-
nipu˙ngavam (wisest among the sages - an adjec-
tive of Narada) are not related semantically, but
still undergo euphonic change and are glued to-
gether as v¯alm¯ıkirmunipu˙ngavam.
Further, the split need not be unique. Here is
an example, where a string m¯atur¯aj˜n¯amparip¯alaya
may be decomposed in two different ways after
undergoing euphonic changes across word bound-
aries.
¯atuh ¯aj˜n¯am parip¯alaya (obey the order of
mother) and,
¯a ¯atur¯aj˜n¯am parip¯alaya (do not obey the
order of the diseased).
There are special cases where the sandhied
forms are not necessarily written together. In
such cases, the white space that physically marks
the boundary of the words, logically refers to
a single sandhied form. Thus, the white space
is deceptive, and if treated as a word boundary,
the morphological analyzer fails to recognize the
word. For example, consider
´srutv¯a ca n¯arado vacah..
In this example, the space between ´srutv¯a and
ca represent a proper word boundary and the
word ´srutv¯a is recognized by the morphological
analyzer whereas the space between n¯arado and
vacah. does not mark the word boundary making
it deceptive. Because of the word vacah. , n¯aradah.
has undergone a phonetic change and is rendered
as n¯arado. In unsandhied form, it would be
written as,
San: ´srutv¯a ca n¯aradah. vacah..
gloss: after listening and Narada’s speech
Eng: And after listening to Narada’s speech
The third factor aggravating Sanskrit segmen-
tation is productive compound formation. Unlike
English, where either the components of a com-
pound are written as distinct words or are sepa-
rated by a hyphen, the components of compounds
in Sanskrit are always written together. Moreover,
before these components are joined, they undergo
the euphonic changes. The components of a com-
pound typically do not carry inflection or in other
words they are the bound morphemes used only in
compounds. This forces a need of a special mod-
ule to recognize compounds.
Assuming that a sandhi handler to handle the
sandhi involving spaces is available and a bound
morpheme recognizer is available, we discuss the
development of sandhi splitter or a segmentizer
that splits a continuous string of letters into
meaningful words. To illustrate this point, we
give an example.
Consider the text,
</bodyText>
<subsubsectionHeader confidence="0.457479">
´srutv¯a caitattrilokaj˜no v¯alm¯ıkern¯arado vacah..
</subsubsectionHeader>
<bodyText confidence="0.996474285714286">
We assume that the sandhi handler handling the
sandhi involving spaces is available and it splits
the above string as,
´srutv¯a caitattrilokaj˜nah. v¯alm¯ıkern¯aradah.
vacah..
The sandhi splitter or segmentizer is supposed
to split this into
</bodyText>
<listItem confidence="0.783996">
• m
• m
</listItem>
<page confidence="0.966194">
86
</page>
<bodyText confidence="0.997450925925926">
´srutv¯a ca etat triloka-j˜nah. v¯alm¯ıkeh. n¯aradah.
vacah..
This presupposes the availability of rules corre-
sponding to euphonic changes and a good cover-
age morphological analyzer that can also analyze
the bound morphemes in compounds.
A segmentizer for Sanskrit developed by Huet
(Huet, 2009), decorates the final states of its fi-
nite state transducer handling Sanskrit morphol-
ogy with the possible sandhi rules. However, it
is still not clear how one can prioritize various
splits with this approach. Further, this system in
current state demands some more work before the
sandhi splitter of this system can be used as a stan-
dalone system allowing plugging in of different
morphological analyzers. With a variety of mor-
phological analyzers being developed by various
researchers3, at times with complementary abili-
ties, it would be worth to experiment with vari-
ous morphological analyzers for splitting a sand-
hied text. Hence, we thought of exploring other
alternatives and present two approaches, both of
which assume the existence of a good coverage
morphological analyzer. Before we describe our
approaches, we first define the scoring matrix used
to prioritize various analyses followed by the base-
line system.
</bodyText>
<sectionHeader confidence="0.987941" genericHeader="method">
3 Scoring Matrix
</sectionHeader>
<bodyText confidence="0.999887578947368">
Just as in the case of any NLP systems, with the
sandhi splitter being no exception, it is always de-
sirable to produce the most likely output when a
machine produces multiple outputs. To ensure that
the correct output is not deeply buried down the
pile of incorrect answers, it is natural to prioritize
solutions based on some frequencies. A Parallel
corpus of Sanskrit text in sandhied and sandhi split
form is being developed as a part of the Consor-
tium project in India. The corpus contains texts
from various fields ranging from children stories,
dramas to Ayurveda texts. Around 100K words
of such a parallel corpus is available from which
around 25,000 parallel strings of unsandhied and
corresponding sandhied texts were extracted. The
same corpus was also used to extract a total of
2650 sandhi rules including the cases of mere con-
catenation, and the frequency distribution of these
sandhi rules. Each sandhi rule is a triple (x, y, z)
</bodyText>
<footnote confidence="0.832873">
3http://sanskrit.uohyd.ernet.in,
http://www.sanskritlibrary.org, http://sanskrit.jnu.ernet.in
</footnote>
<bodyText confidence="0.970097086956522">
where y is the last letter of the first primitive, z is
the first letter of the second primitive, and x is the
letter sequence created by euphonic combination.
We define the estimated probability of the occur-
rence of a sandhi rule as follows:
Let Ri denote the ith rule with fRi as the fre-
quency of occurrence in the manually split parallel
text. The probability of rule Ri is:
fRi
PRi =
where n denotes the total number of sandhi rules
found in the corpus.
Let a word be split into a candidate Sj with k
constituents as &lt; c1, c2, ..ck &gt; by applying k − 1
sandhi rules &lt; R1, R2, ..Rk−1 &gt; in between the
constituents. It should be noted here that the rules
R1, ..Rk−1 and the constituents c1, ..ck are inter-
dependent since a different rule sequence will re-
sult in a different constituents sequence. Also, ex-
cept c1 and ck, all intermediate constituents take
part in two segmentations, one as the right word
and one as the left.
The weight of the split Sj is defined as:
</bodyText>
<equation confidence="0.857012333333333">
�k−1
x=1(Pcx + Pcx+1) ∗ PRx
WSj =
</equation>
<bodyText confidence="0.999553">
where Pcx is the probability of occurrence of the
word cx in the corpus. The factor of k was intro-
duced to give more preference to the split with less
number of segments than the one with more seg-
ments.
</bodyText>
<sectionHeader confidence="0.995527" genericHeader="method">
4 Baseline System
</sectionHeader>
<bodyText confidence="0.999811333333333">
We define our own baseline system which assumes
that each Sanskrit word can be segmented only in
two constituents. A word is traversed from left to
right and is segmented by applying the first appli-
cable rule provided both the constituents are valid
morphs. Using the 2,650 rules, on a test data of
2,510 words parallel corpus, the baseline perfor-
mance of the system was around 52.7% where the
first answer was correct.
</bodyText>
<sectionHeader confidence="0.994533" genericHeader="method">
5 Two Approaches
</sectionHeader>
<bodyText confidence="0.999976">
We now present the two approaches we explored
for sandhi splitting.
</bodyText>
<subsectionHeader confidence="0.963209">
5.1 Augmenting FST with Sandhi rules
</subsectionHeader>
<bodyText confidence="0.996753">
In this approach, we build an FST, using Open-
Fst (Allauzen et al., 2007) toolkit, incorporating
</bodyText>
<equation confidence="0.7989675">
Eni=1 fRi
k
</equation>
<page confidence="0.992191">
87
</page>
<bodyText confidence="0.999144166666667">
sandhi rules in the FST itself and traverse it to find
the sandhi splittings.
We illustrate the augmentation of a sandhi rule
with an example. Let the two strings be xaXi
(dadhi)4 and awra (atra). The initial FST without
considering any sandhi rules is shown in Figure 1.
</bodyText>
<figureCaption confidence="0.9476545">
Figure 1: Initial FST accepting only two words
xaXi and awra.
</figureCaption>
<bodyText confidence="0.953794739130435">
As the figure depicts, 0 is the start state and 4 is
the final state. Each transition is a 4-tuple &lt;c, n,
i, o&gt; where c is current state, n is the next state,
i is the input symbol and o is the output. The
FST marks word boundaries by flushing out cer-
tain features about the words whenever it encoun-
ters a valid word. Multiple features are separated
by a ‘|’. E.g., the output for xaXi is lc,s|vc,s and
for awra it is vc,s where lc,s stands for locative,
singular and vc,s is vocative, singular. The FST
in Figure 1 recognize exactly two words xaXi and
awra.
One of the sandhi rule states that i+a —* ya
which will be represented as a triple (ya, i, a). Ap-
plying the sandhi rule, we get: xaXi + awra —*
xaXyawra. After adding this sandhi rule to the
FST, we get the modified FST that is represented
in Figure 2.
Figure 2: Modified FST after inserting the rule.
− − − indicates the newly added transition.
Here, a transition arc is added depicting the rule
which says that on receiving an input symbol ya
at state 3, go to state 5 with an output i+a —* ya.
</bodyText>
<footnote confidence="0.899715">
4A Roman transliteration scheme called WX translitera-
tion is used, which is one-to-one phoneme level representa-
tion of Devan¯agariscript.
</footnote>
<bodyText confidence="0.967354">
Thus the new FST accepts xaXyawra in addition
to xaXi and awra.
Thus, we see that the original transducer gets
modified with all possible transitions at the end
of a final phoneme, and hence, also explodes the
number of transitions leading to a complex trans-
ducer.
The basic outline of the algorithm to split the
given string into sub-strings is:
Algorithm 1 To split a string into sub-strings
</bodyText>
<listItem confidence="0.920602125">
1: Let the FST for morphology be f.
2: Add sandhi rules to the final states off1 link-
ing them to the intermediary states to get f′.
3: Traverse f′ to find all possible splits for a
word. If a sandhi rule is encountered, split the
word and continue with the remaining part.
4: Calculate the weights of the possible outputs
with the formula discussed in section 3.
</listItem>
<bodyText confidence="0.925939">
The pseudo-code of the algorithm used to insert
sandhi rules in the FST is illustrated here:
Algorithm 2 To insert sandhi rules in the FST
</bodyText>
<listItem confidence="0.9882598125">
1: I = Input Symbol; X = last character of the
result of the rule.
2: for each transition in the FST transition table
do
3: if next state is a final state then
4: for all rules where I is the last character
of first word do
5: S = next state from the start state on
encountering X;
6: Y = first character of the result of the
rule;
7: transition T = current state, S, Y, rule;
8: Add T into the FST;
9: end for
10: end if
11: end for
</listItem>
<bodyText confidence="0.9999099">
The main problem with this approach is that ev-
ery finite state can have as many transitions as the
number of euphonic rules resulting in phoneme
change. This increases the size of the FST con-
siderably. It should be noted that, we have not in-
cluded the cases, where there is just a concatena-
tion. In such cases, if the input string is not ex-
hausted, but the current state is a final state, we go
back to the start state with the remaining string as
the input.
</bodyText>
<page confidence="0.998691">
88
</page>
<sectionHeader confidence="0.87898" genericHeader="method">
5.1.1 Results
</sectionHeader>
<bodyText confidence="0.999353666666667">
The performance of this system measured in terms
of the number of times the highest ranked segmen-
tation is correct, with around 500 sandhi rules, and
only noun morphology tested on the same test data
used for testing baseline system gave the following
rank-wise distribution presented in Table 1.
</bodyText>
<table confidence="0.595340142857143">
Rank % of output
1 71.2509
2 5.64543
3 3.85324
4 3.35651
5 1.56123
&gt;5 14.33268
</table>
<tableCaption confidence="0.990095">
Table 1: Rank-wise Distribution for Approach-1.
</tableCaption>
<bodyText confidence="0.999717636363636">
The system was slow consuming, on an average,
around 10 seconds per string of 15 letters.5.
With the increase in the sandhi rules, though
system’s performance was better, it slowed down
the system further. Moreover, this was tested only
with the inflection morphology of nouns. The verb
inflection morphology and the derivational mor-
phology were not used at all. Since, the system is
supposed to be part of a real time application viz.
machine translation, we decided to explore other
possibilities.
</bodyText>
<subsectionHeader confidence="0.999336">
5.2 Approach based on Optimality Theory
</subsectionHeader>
<bodyText confidence="0.9999232">
Our second approach follows optimality the-
ory(OT) which proposes that the observed forms
of a language are a result of the interaction be-
tween the conflicting constraints. The three basic
components of the theory are:
</bodyText>
<listItem confidence="0.982360857142857">
1. GEN - generates all possible outputs, or can-
didates.
2. CON - provides the criteria and the con-
straints that will be used to decide between
candidates.
3. EVAL - chooses the optimal candidate based
on the conflicts on the constraints.
</listItem>
<bodyText confidence="0.971671">
OT assumes that these components are univer-
sal and the grammars differ in the way they rank
the universal constraint set, CON. The grammar of
</bodyText>
<footnote confidence="0.9966365">
5Tested on a system with 2.93GHz Core 2 Duo processor
and 2GB RAM
</footnote>
<bodyText confidence="0.999986535714286">
each language ranks the constraints in some dom-
inance order in such a way that every constraint
must have outperformed every lower ranked con-
straint. Thus a candidate A is optimal if it per-
forms better than some other candidate B on a
higher ranking constraint even if A has more vi-
olations of a lower ranked constraint than B.
The GEN function produces every possible seg-
mentation by applying the rules wherever appli-
cable. The rules tokenize the input surface form
into individual constituents. This might contain
some insignificant words that will be eventually
pruned out using the morphological analyser in
the EVAL function thus leaving the winning can-
didate. Therefore, the approach followed is very
closely based on optimality theory. The morph
analyser has no role in the generation of the can-
didates but only during their validation thus com-
posing the back-end of the segmentizer. In orig-
inal OT, the winning candidate need not satisfy
all the constraints but it must outperform all the
other candidates on some higher ranked constraint.
While in our scenario, the winning candidate must
satisfy all the constraints and therefore there could
be more than one winning candidates.
Currently we are applying only two constraints.
We are planning to introduce some more con-
straints. The constraints applied are:
</bodyText>
<listItem confidence="0.99692575">
• C1 : All the constituents of a split must be
valid morphs.
• C2 : Select the split with maximum weight,
as defined in section 3.
</listItem>
<bodyText confidence="0.7315875">
The basic outline of the algorithm is:
1: Recursively break a word at every possible po-
sition applying a sandhi rule and generate all
possible candidates for the input.
</bodyText>
<listItem confidence="0.964323111111111">
2: Pass the constituents of all the candidates
through the morph analyzer.
3: Declare the candidate as a valid candidate, if
all its constituents are recognized by the mor-
phological analyzer.
4: Assign weights to the accepted candidates and
sort them based on the weights.
5: The optimal solution will be the one with the
highest salience.
</listItem>
<sectionHeader confidence="0.897911" genericHeader="evaluation">
5.2.1 Results
</sectionHeader>
<bodyText confidence="0.981826">
The current morphological analyzer can recognize
around 140 million words. Using the 2650 rules
</bodyText>
<page confidence="0.999301">
89
</page>
<bodyText confidence="0.893143">
and the same test data used for previous approach,
we obtained the following results:
• Almost 93% of the times, the highest ranked
segmentation is correct. And in almost 98%
of the cases, the correct split was among the
top 3 possible splits.
• The system consumes around 0.04 seconds
per string of 15 letters on an average.
The complete rank wise distribution is given in Ta-
ble 2.
</bodyText>
<table confidence="0.99811475">
% of output
Rank Approach-1 Approach-2
1 71.2509 92.8771
2 5.64543 5.44693
3 3.85324 1.07076
4 3.35651 0.41899
5 1.56123 0.09311
&gt;5 14.33268 0.0931
</table>
<tableCaption confidence="0.995345">
Table 2: Complete rank-wise Distribution.
</tableCaption>
<sectionHeader confidence="0.997457" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999962285714286">
We presented two methods to automatically seg-
ment a Sanskrit word into its morphologically
valid constituents. Though both the approaches
outperformed the baseline system, the approach
that is close to optimality theory gives better re-
sults both in terms of time consumption and seg-
mentations. The results are encouraging. But the
real test of this system will be when it is inte-
grated with some real application such as a ma-
chine translation system. This sandhi splitter be-
ing modular, wherein one can plug in different
morphological analyzer and different set of sandhi
rules, the splitter can also be used for segmentiza-
tion of other languages.
Future Work The major task would be to ex-
plore ways to shift rank 2 and rank 3 segmenta-
tions more towards rank 1. We are also explor-
ing the possibility of including some semantic in-
formation about the words while defining weights.
The sandhi with white spaces also needs to be han-
dled.
</bodyText>
<sectionHeader confidence="0.998847" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994650333333333">
I would like to express my gratitude to Amba
Kulkarni and Rajeev Sangal for their guidance and
support.
</bodyText>
<sectionHeader confidence="0.995379" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999429065217391">
Akshar Bharati, Amba P. Kulkarni, and V Sheeba.
2006. Building a wide coverage Sanskrit mor-
phological analyzer: A practical approach. The
First National Symposium on Modelling and Shal-
low Parsing of Indian Languages, IIT-Bombay.
Alan Prince and Paul Smolensky. 1993. Optimality
Theory: Constraint Interaction in Generative Gram-
mar. RuCCS Technical Report 2 at Center for Cog-
nitive Science, Rutgers University, Piscataway.
Amba Kulkarni and Devanand Shukla. 2009. Sanskrit
Morphological analyzer: Some Issues. To appear in
Bh.K Festschrift volume by LSI.
Choochart Haruechaiyasak, Sarawoot Kongyoung, and
Matthew N. Dailey. 2008. A Comparative Study on
Thai Word Segmentation Approaches. ECTI-CON,
Krabi.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
General and Efficient Weighted Finite-State Trans-
ducer Library. CIAA’07, Prague, Czech Republic.
Deniz Yuret and Ergun Bic¸ici. 2009. Modeling Mor-
phologically Rich Languages Using Split Words and
Unstructured Dependencies. ACL-IJCNLP’09, Sin-
gapore.
DINH Q. Thang, LE H. Phuong, NGUYEN T. M.
Huyen, NGUYEN C. Tu, Mathias Rossignol, and
VU X. Luong. 2008. Word Segmentation of
Vietnamese Texts: a Comparison of Approaches.
LREC’08, Marrakech, Morocco.
G´erard Huet. 2009. Formal structure of Sanskrit
text: Requirements analysis for a mechanical San-
skrit processor. Sanskrit Computational Linguistics
1 &amp; 2, pages 266-277, Springer-Verlag LNAI 5402.
John C. J. Hoeks and Petra Hendriks. 2005. Optimality
Theory and Human Sentence Processing: The Case
of Coordination. Proceedings of the 27th Annual
Meeting of the Cognitive Science Society, Erlbaum,
Mahwah, NJ, pp. 959–964.
Kenneth R. Beesley. 1998. Arabic morphology using
only finite-state operations Proceedings of the ACL
Workshop on Computational Approaches to Semitic
Languages, Montr´eal, Qu´ebec.
Leonardo Badino. 2004. Chinese Text Word-
Segmentation Considering Semantic Links among
Sentences. INTERSPEECH 2004 - ICSLP , Jeju,
Korea.
</reference>
<page confidence="0.998604">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.662135">
<title confidence="0.998651">Automatic Sanskrit Segmentizer Using Finite State Transducers</title>
<author confidence="0.876362">Vipul Mittal</author>
<affiliation confidence="0.983527">Language Technologies Research Center, IIIT-H,</affiliation>
<address confidence="0.994683">Gachibowli, Hyderabad, India.</address>
<email confidence="0.98583">vipulmittal@research.iiit.ac.in</email>
<abstract confidence="0.987866842105263">In this paper, we propose a novel method for automatic segmentation of a Sanskrit string into different words. The input for our segmentizer is a Sanskrit string either encoded as a Unicode string or as a Roman transliterated string and the output is a set of possible splits with weights associated with each of them. We followed two different approaches to segment a Sanskrit using rules extracted from a parallel corpus of manually sandhi split text. While the first approach augments the finite state transducer used to analyze Sanskrit morphology and traverse it to segment a word, the second approach generates all possible segmentations and validates each constituent using a morph analyzer.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Akshar Bharati</author>
<author>Amba P Kulkarni</author>
<author>V Sheeba</author>
</authors>
<title>Building a wide coverage Sanskrit morphological analyzer: A practical approach.</title>
<date>2006</date>
<booktitle>The First National Symposium on Modelling and Shallow Parsing of Indian Languages, IIT-Bombay.</booktitle>
<marker>Bharati, Kulkarni, Sheeba, 2006</marker>
<rawString>Akshar Bharati, Amba P. Kulkarni, and V Sheeba. 2006. Building a wide coverage Sanskrit morphological analyzer: A practical approach. The First National Symposium on Modelling and Shallow Parsing of Indian Languages, IIT-Bombay.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Prince</author>
<author>Paul Smolensky</author>
</authors>
<title>Optimality Theory: Constraint Interaction in Generative Grammar.</title>
<date>1993</date>
<tech>RuCCS Technical Report 2 at</tech>
<institution>Center for Cognitive Science, Rutgers University, Piscataway.</institution>
<contexts>
<context position="2616" citStr="Prince and Smolensky, 1993" startWordPosition="411" endWordPosition="414">rd boundaries having undergone euphonic changes. This makes it difficult to split a continuous string into words and process the text automatically. Sanskrit words are mostly analyzed by building a finite state transducer (Beesley, 1998). In the first approach, this transducer was modified by linking the final states to appropriate intermediate states incorporating the sandhi rules. This approach then allows one to traverse the string from left to right and generate all and only possible splits that are morphologically valid. The second approach is very closely based on the Optimality Theory (Prince and Smolensky, 1993) where we generate all the possible splits for a word and validate each using a morphological analyzer. We use one of the fastest morphological analyzers available viz. the one developed by Apertium group2. The splits that are not validated are pruned out. Based on the number of times the first answer is correct, we achieved an accuracy of around 92% using the second approach while the first approach performed with around 71% accuracy. 2 Issues involved in Sanskrit Processing The segmentizer is an important component of an NLP system. Especially, languages such as Chinese (Badino, 2004), Japan</context>
</contexts>
<marker>Prince, Smolensky, 1993</marker>
<rawString>Alan Prince and Paul Smolensky. 1993. Optimality Theory: Constraint Interaction in Generative Grammar. RuCCS Technical Report 2 at Center for Cognitive Science, Rutgers University, Piscataway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amba Kulkarni</author>
<author>Devanand Shukla</author>
</authors>
<title>Sanskrit Morphological analyzer: Some Issues.</title>
<date>2009</date>
<note>To appear in Bh.K Festschrift volume by LSI.</note>
<marker>Kulkarni, Shukla, 2009</marker>
<rawString>Amba Kulkarni and Devanand Shukla. 2009. Sanskrit Morphological analyzer: Some Issues. To appear in Bh.K Festschrift volume by LSI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Choochart Haruechaiyasak</author>
<author>Sarawoot Kongyoung</author>
<author>Matthew N Dailey</author>
</authors>
<title>A Comparative Study on Thai Word Segmentation Approaches.</title>
<date>2008</date>
<location>ECTI-CON, Krabi.</location>
<marker>Haruechaiyasak, Kongyoung, Dailey, 2008</marker>
<rawString>Choochart Haruechaiyasak, Sarawoot Kongyoung, and Matthew N. Dailey. 2008. A Comparative Study on Thai Word Segmentation Approaches. ECTI-CON, Krabi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<date>2007</date>
<booktitle>OpenFst: A General and Efficient Weighted Finite-State Transducer Library. CIAA’07,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="11857" citStr="Allauzen et al., 2007" startWordPosition="1911" endWordPosition="1914"> System We define our own baseline system which assumes that each Sanskrit word can be segmented only in two constituents. A word is traversed from left to right and is segmented by applying the first applicable rule provided both the constituents are valid morphs. Using the 2,650 rules, on a test data of 2,510 words parallel corpus, the baseline performance of the system was around 52.7% where the first answer was correct. 5 Two Approaches We now present the two approaches we explored for sandhi splitting. 5.1 Augmenting FST with Sandhi rules In this approach, we build an FST, using OpenFst (Allauzen et al., 2007) toolkit, incorporating Eni=1 fRi k 87 sandhi rules in the FST itself and traverse it to find the sandhi splittings. We illustrate the augmentation of a sandhi rule with an example. Let the two strings be xaXi (dadhi)4 and awra (atra). The initial FST without considering any sandhi rules is shown in Figure 1. Figure 1: Initial FST accepting only two words xaXi and awra. As the figure depicts, 0 is the start state and 4 is the final state. Each transition is a 4-tuple &lt;c, n, i, o&gt; where c is current state, n is the next state, i is the input symbol and o is the output. The FST marks word bounda</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A General and Efficient Weighted Finite-State Transducer Library. CIAA’07, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Ergun Bic¸ici</author>
</authors>
<title>Modeling Morphologically Rich Languages Using Split Words and Unstructured Dependencies.</title>
<date>2009</date>
<tech>ACL-IJCNLP’09,</tech>
<marker>Yuret, Bic¸ici, 2009</marker>
<rawString>Deniz Yuret and Ergun Bic¸ici. 2009. Modeling Morphologically Rich Languages Using Split Words and Unstructured Dependencies. ACL-IJCNLP’09, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DINH Q Thang</author>
<author>LE H Phuong</author>
<author>NGUYEN T M Huyen</author>
<author>NGUYEN C Tu</author>
<author>Mathias Rossignol</author>
<author>VU X Luong</author>
</authors>
<title>Word Segmentation of Vietnamese Texts: a Comparison of Approaches. LREC’08,</title>
<date>2008</date>
<location>Marrakech, Morocco.</location>
<marker>Thang, Phuong, Huyen, Tu, Rossignol, Luong, 2008</marker>
<rawString>DINH Q. Thang, LE H. Phuong, NGUYEN T. M. Huyen, NGUYEN C. Tu, Mathias Rossignol, and VU X. Luong. 2008. Word Segmentation of Vietnamese Texts: a Comparison of Approaches. LREC’08, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G´erard Huet</author>
</authors>
<title>Formal structure of Sanskrit text: Requirements analysis for a mechanical Sanskrit processor.</title>
<date>2009</date>
<journal>Sanskrit Computational Linguistics</journal>
<volume>1</volume>
<pages>266--277</pages>
<contexts>
<context position="8039" citStr="Huet, 2009" startWordPosition="1258" endWordPosition="1259">the text, ´srutv¯a caitattrilokaj˜no v¯alm¯ıkern¯arado vacah.. We assume that the sandhi handler handling the sandhi involving spaces is available and it splits the above string as, ´srutv¯a caitattrilokaj˜nah. v¯alm¯ıkern¯aradah. vacah.. The sandhi splitter or segmentizer is supposed to split this into • m • m 86 ´srutv¯a ca etat triloka-j˜nah. v¯alm¯ıkeh. n¯aradah. vacah.. This presupposes the availability of rules corresponding to euphonic changes and a good coverage morphological analyzer that can also analyze the bound morphemes in compounds. A segmentizer for Sanskrit developed by Huet (Huet, 2009), decorates the final states of its finite state transducer handling Sanskrit morphology with the possible sandhi rules. However, it is still not clear how one can prioritize various splits with this approach. Further, this system in current state demands some more work before the sandhi splitter of this system can be used as a standalone system allowing plugging in of different morphological analyzers. With a variety of morphological analyzers being developed by various researchers3, at times with complementary abilities, it would be worth to experiment with various morphological analyzers fo</context>
</contexts>
<marker>Huet, 2009</marker>
<rawString>G´erard Huet. 2009. Formal structure of Sanskrit text: Requirements analysis for a mechanical Sanskrit processor. Sanskrit Computational Linguistics 1 &amp; 2, pages 266-277, Springer-Verlag LNAI 5402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C J Hoeks</author>
<author>Petra Hendriks</author>
</authors>
<title>Optimality Theory and Human Sentence Processing: The Case of Coordination.</title>
<date>2005</date>
<booktitle>Proceedings of the 27th Annual Meeting of the Cognitive Science Society, Erlbaum,</booktitle>
<pages>959--964</pages>
<location>Mahwah, NJ,</location>
<marker>Hoeks, Hendriks, 2005</marker>
<rawString>John C. J. Hoeks and Petra Hendriks. 2005. Optimality Theory and Human Sentence Processing: The Case of Coordination. Proceedings of the 27th Annual Meeting of the Cognitive Science Society, Erlbaum, Mahwah, NJ, pp. 959–964.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth R Beesley</author>
</authors>
<title>Arabic morphology using only finite-state operations</title>
<date>1998</date>
<booktitle>Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<location>Montr´eal, Qu´ebec.</location>
<contexts>
<context position="2226" citStr="Beesley, 1998" startWordPosition="352" endWordPosition="353">f words when they are consecutively pronounced. Typically when a word w1 is followed by a word w2, some terminal segment of w1 merges with some initial segment of w2 to be replaced by a “smoothed” phonetic interpolation, corresponding to minimizing the energy necessary to reconfigurate the vocal organs at the juncture between the words. long sequence of phonemes, with the word boundaries having undergone euphonic changes. This makes it difficult to split a continuous string into words and process the text automatically. Sanskrit words are mostly analyzed by building a finite state transducer (Beesley, 1998). In the first approach, this transducer was modified by linking the final states to appropriate intermediate states incorporating the sandhi rules. This approach then allows one to traverse the string from left to right and generate all and only possible splits that are morphologically valid. The second approach is very closely based on the Optimality Theory (Prince and Smolensky, 1993) where we generate all the possible splits for a word and validate each using a morphological analyzer. We use one of the fastest morphological analyzers available viz. the one developed by Apertium group2. The</context>
</contexts>
<marker>Beesley, 1998</marker>
<rawString>Kenneth R. Beesley. 1998. Arabic morphology using only finite-state operations Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, Montr´eal, Qu´ebec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonardo Badino</author>
</authors>
<title>Chinese Text WordSegmentation Considering Semantic Links among Sentences. INTERSPEECH</title>
<date>2004</date>
<contexts>
<context position="3209" citStr="Badino, 2004" startWordPosition="511" endWordPosition="512">and Smolensky, 1993) where we generate all the possible splits for a word and validate each using a morphological analyzer. We use one of the fastest morphological analyzers available viz. the one developed by Apertium group2. The splits that are not validated are pruned out. Based on the number of times the first answer is correct, we achieved an accuracy of around 92% using the second approach while the first approach performed with around 71% accuracy. 2 Issues involved in Sanskrit Processing The segmentizer is an important component of an NLP system. Especially, languages such as Chinese (Badino, 2004), Japanese, Thai (Haruechaiyasak, 2008) or Vietnamese (Thang et al. , 2008) which do not mark word boundaries explicitly or highly agglutinative languages like Turkish need segmentizers. In all these languages, there are no explicit delimiters to specify the word boundaries. In Thai, each syllable is transcribed using several characters and there is no space in the text between syllables. So the problem of segmentation is basically twofold: (1) syllable segmentation followed by (2) word segmentation itself. A sentence in these languages 2http://wiki.apertium.org/wiki/lttoolbox; It processes ar</context>
</contexts>
<marker>Badino, 2004</marker>
<rawString>Leonardo Badino. 2004. Chinese Text WordSegmentation Considering Semantic Links among Sentences. INTERSPEECH 2004 - ICSLP , Jeju, Korea.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>