<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.9863">
Robust VPE detection using Automatically Parsed Text
</title>
<author confidence="0.995693">
Leif Arda Nielsen
</author>
<affiliation confidence="0.963009">
Department of Computer Science
King’s College London
</affiliation>
<email confidence="0.996717">
nielsen@dcs.kcl.ac.uk
</email>
<sectionHeader confidence="0.995624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977923076923">
This paper describes a Verb Phrase El-
lipsis (VPE) detection system, built for
robustness, accuracy and domain inde-
pendence. The system is corpus-based,
and uses machine learning techniques
on free text that has been automatically
parsed. Tested on a mixed corpus com-
prising a range of genres, the system
achieves a 70% F1-score. This system is
designed as the first stage of a complete
VPE resolution system that is input free
text, detects VPEs, and proceeds to find
the antecedents and resolve them.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995729476190476">
Ellipsis is a linguistic phenomenon that has re-
ceived considerable attention, mostly focusing on
its interpretation. Most work on ellipsis (Fiengo
and May, 1994; Lappin, 1993; Dalrymple et al.,
1991; Kehler, 1993; Shieber et al., 1996) is aimed
at discerning the procedures and the level of lan-
guage processing at which ellipsis resolution takes
place, or ambiguous and difficult cases. The detec-
tion of elliptical sentences or the identification of
the antecedent and elided clauses within them are
usually not dealt with, but taken as given. Noisy or
missing input, which is unavoidable in NLP appli-
cations, is not dealt with, and neither is focusing
on specific domains or applications. It therefore
becomes clear that a robust, trainable approach is
needed.
An example of Verb Phrase Ellipsis (VPE),
which is detected by the presence of an auxiliary
verb without a verb phrase, is seen in example 1.
VPE can also occur with semi-auxiliaries, as in ex-
ample 2.
</bodyText>
<listItem confidence="0.966427928571428">
(1) John3 {loves his3 wife}2. Bill3 does1 too.
(2) But although he was terse, he didn’t {rage at
me}2 the way I expected him to1.
Several steps of work need to be done for ellip-
sis resolution:
1. Detecting ellipsis occurrences. First, elided
verbs need to be found.
2. Identifying antecedents. For most cases of
ellipsis, copying of the antecedent clause is
enough for resolution (Hardt, 1997).
3. Resolving ambiguities. For cases where am-
biguity exists, a method for generating the
full list of possible solutions, and suggesting
the most likely one is needed.
</listItem>
<bodyText confidence="0.999814411764706">
This paper describes the work done on the first
stage, the detection of elliptical verbs. First, pre-
vious work done on tagged corpora will be sum-
marised. Then, new work on parsed corpora will
be presented, showing the gains possible through
sentence-level features. Finally, experiments us-
ing unannotated data that is parsed using an auto-
matic parser are presented, as our aim is to pro-
duce a stand-alone system.
We have chosen to concentrate on VP ellipsis
due to the fact that it is far more common than
other forms of ellipsis, but pseudo-gapping, an ex-
ample of which is seen in example 3, has also been
included due to the similarity of its resolution to
VPE (Lappin, 1996). Do so/it/that and so doing
anaphora are not handled, as their resolution is dif-
ferent from that of VPE (Kehler and Ward, 1999).
</bodyText>
<listItem confidence="0.780924">
(3) John writes plays, and Bill does novels.
</listItem>
<sectionHeader confidence="0.900485" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.98838765">
Hardt’s (1997) algorithm for detecting VPE in the
Penn Treebank (see Section 3) achieves precision
levels of 44% and recall of 53%, giving an F11
of 48%, using a simple search technique, which
relies on the parse annotation having identified
empty expressions correctly.
In previous work (Nielsen, 2003a; Nielsen,
2003b) we performed experiments on the British
National Corpus using a variety of machine learn-
ing techniques. These earlier results are not di-
rectly comparable to Hardt’s, due to the differ-
ent corpora used. The expanded set of results are
summarised in Table 1, for Transformation Based
Learning (TBL) (Brill, 1995), GIS based Max-
imum Entropy Modelling (GIS-MaxEnt) (Ratna-
parkhi, 1998), L-BFGS based Maximum Entropy
Modelling (L-BFGS-MaxEnt)2 (Malouf, 2002),
Decision Tree Learning (Quinlan, 1993) and
Memory Based Learning (MBL) (Daelemans et
al., 2002).
</bodyText>
<table confidence="0.998981666666667">
Algorithm Recall Precision F1
TBL 69.63 85.14 76.61
Decision Tree 60.93 79.39 68.94
MBL 72.58 71.50 72.04
GIS-MaxEnt 71.72 63.89 67.58
L-BFGS-MaxEnt 71.93 80.58 76.01
</table>
<tableCaption confidence="0.999945">
Table 1: Comparison of algorithms
</tableCaption>
<bodyText confidence="0.350869">
1Precision, recall and F1 are defined as :
</bodyText>
<equation confidence="0.951483555555556">
No(correct ellipses found)
Recall = (1)
No(all ellipses in test)
No(correct ellipses found)
Precision = (2)
No(all ellipses found)
F1 =
2 × Precision × Recall (3)
Precision + Recall
</equation>
<footnote confidence="0.9626175">
2Downloadable from
http://www.nlplab.cn/zhangle/maxent toolkit.html
</footnote>
<bodyText confidence="0.9997552">
For all of these experiments, the training fea-
tures consisted of lexical forms and Part of Speech
(POS) tags of the words in a three word for-
ward/backward window of the auxiliary being
tested. This context size was determined empir-
ically to give optimum results, and will be used
throughout this paper. The L-BFGS-MaxEnt uses
Gaussian Prior smoothing which was optimized
for the BNC data, while the GIS-MaxEnt has a
simple smoothing option available, but this dete-
riorates results and is not used. MBL was used
with its default settings.
While TBL gave the best results, the software
we used (Lager, 1999) ran into memory problems
and proved problematic with larger datasets. Deci-
sion trees, on the other hand, tend to oversimplify
due to the very sparse nature of ellipsis, and pro-
duce a single rule that classifies everything as non-
VPE. This leaves Maximum Entropy and MBL for
further experiments.
</bodyText>
<sectionHeader confidence="0.962382" genericHeader="method">
3 Corpus description
</sectionHeader>
<bodyText confidence="0.99987675">
The British National Corpus (BNC) (Leech, 1992)
is annotated with POS tags, using the CLAWS-4
tagset. A range of V sections of the BNC, contain-
ing around 370k words3 with 645 samples of VPE
was used as training data. The separate test data
consists of around 74k words4 with 200 samples
of VPE.
The Penn Treebank (Marcus et al., 1994) has
more than a hundred phrase labels, and a number
of empty categories, but uses a coarser tagset. A
mixture of sections from the Wall Street Journal
and Brown corpus were used. The training sec-
tion5 consists of around 540k words and contains
522 samples of VPE. The test section6 consists of
around 140k words and contains 150 samples of
VPE.
</bodyText>
<sectionHeader confidence="0.917271" genericHeader="method">
4 Experiments using the Penn Treebank
</sectionHeader>
<bodyText confidence="0.9982555">
To experiment with what gains are possible
through the use of more complex data such as
</bodyText>
<footnote confidence="0.981966833333333">
3Sections CS6, A2U, J25, FU6, H7F, HA3, A19, A0P,
G1A, EWC, FNS, C8T
4Sections EDJ, FR3
5Sections WSJ 00, 01, 03, 04, 15, Brown CF, CG, CL,
CM, CN, CP
6Sections WSJ 02, 10, Brown CK, CR
</footnote>
<bodyText confidence="0.974256266666667">
parse trees, the Penn Treebank is used for the sec-
ond round of experiments. The results are pre-
sented as new features are added in a cumulative
fashion, so each experiment also contains the data
contained in those before it.
Words and POS tags
The Treebank, besides POS tags and category
headers associated with the nodes of the parse
tree, includes empty category information. For the
initial experiments, the empty category informa-
tion is ignored, and the words and POS tags are
extracted from the trees. The results in Table 2
are seen to be considerably poorer than those for
BNC, despite the comparable data sizes. This can
be accounted for by the coarser tagset employed.
</bodyText>
<table confidence="0.95767825">
Algorithm Recall Precision F1
MBL 47.71 60.33 53.28
GIS-MaxEnt 34.64 79.10 48.18
L-BFGS-MaxEnt 60.13 76.66 67.39
</table>
<tableCaption confidence="0.7347055">
Table 2: Initial results with the Treebank
Close to punctuation
</tableCaption>
<bodyText confidence="0.9999674">
A very simple feature, that checks for auxiliaries
close to punctuation marks was tested. Table 3
shows the performance of the feature itself, char-
acterised by very low precision, and results ob-
tained by using it. It gives a 2% increase in F1 for
MBL, 3% for GIS-MaxEnt, but a 1.5% decrease
for L-BFGS-MaxEnt.
This brings up the point that the individual suc-
cess rate of the features will not be in direct cor-
relation with gains in overall results. Their contri-
bution will be high if they have high precision for
the cases they are meant to address, and if they
produce a different set of results from those al-
ready handled well, complementing the existing
features. Overlap between features can be useful
to have greater confidence when they agree, but
low precision in the feature can increase false pos-
itives as well, decreasing performance. Also, the
small size of the test set can contribute to fluctua-
tions in results.
</bodyText>
<subsectionHeader confidence="0.518205">
Heuristic Baseline
</subsectionHeader>
<bodyText confidence="0.991653">
A simple heuristic approach was developed to
form a baseline. The method takes all auxiliaries
</bodyText>
<table confidence="0.9996706">
Algorithm Recall Precision F1
close-to-punctuation 30.06 2.31 4.30
MBL 50.32 61.60 55.39
GIS-MaxEnt 37.90 79.45 51.32
L-BFGS-MaxEnt 57.51 76.52 65.67
</table>
<tableCaption confidence="0.7914335">
Table 3: Effects of using the close-to-punctuation
feature
</tableCaption>
<equation confidence="0.919144666666667">
(SINV
(ADVP-PRD-TPC-2 (RB so) )
(VP (VBZ is)
(ADVP-PRD (-NONE- *T*-2) ))
(NP-SBJ (PRP$ its)
(NN balance) (NN sheet) ))
</equation>
<figureCaption confidence="0.998894">
Figure 1: Fragment of sentence from Treebank
</figureCaption>
<bodyText confidence="0.997633714285714">
as possible candidates and then eliminates them
using local syntactic information in a very simple
way. It searches forwards within a short range of
words, and if it encounters any other verbs, adjec-
tives, nouns, prepositions, pronouns or numbers,
classifies the auxiliary as not elliptical. It also does
a short backwards search for verbs. The forward
search looks 7 words ahead and the backwards
search 3. Both skip ‘asides’, which are taken to be
snippets between commas without verbs in them,
such as : “... papers do, however, show ...”. This
feature gives a 4.5% improvement for MBL (Table
4), 4% for GIS-MaxEnt and 3.5% for L-BFGS-
MaxEnt.
</bodyText>
<table confidence="0.9995002">
Algorithm Recall Precision F1
heuristic 48.36 27.61 35.15
MBL 55.55 65.38 60.07
GIS-MaxEnt 43.13 78.57 55.69
L-BFGS-MaxEnt 62.09 77.86 69.09
</table>
<tableCaption confidence="0.999625">
Table 4: Effects of using the heuristic feature
</tableCaption>
<subsectionHeader confidence="0.831829">
Surrounding categories
</subsectionHeader>
<bodyText confidence="0.995383125">
The next feature added is the categories of the pre-
vious branch of the tree, and the next branch. So in
the example in Figure 1, the previous category of
the elliptical verb is ADVP-PRD-TPC-2, and the
next category NP-SBJ. The results of using this
feature are seen in Table 5, giving a 3.5% boost to
MBL, 2% to GIS-MaxEnt, and 1.6% to L-BFGS-
MaxEnt.
</bodyText>
<table confidence="0.999401">
Algorithm Recall Precision F1
MBL 58.82 69.23 63.60
GIS-MaxEnt 45.09 81.17 57.98
L-BFGS-MaxEnt 64.70 77.95 70.71
</table>
<tableCaption confidence="0.9417615">
Table 5: Effects of using the surrounding cate-
gories
</tableCaption>
<sectionHeader confidence="0.839159" genericHeader="method">
Auxiliary-final VP
</sectionHeader>
<bodyText confidence="0.996312125">
For auxiliary verbs parsed as verb phrases (VP),
this feature checks if the final element in the VP
is an auxiliary or negation. If so, no main verb
can be present, as a main verb cannot be followed
by an auxiliary or negation. This feature was used
by Hardt (1993) and gives a 3.5% boost to perfor-
mance for MBL, 6% for GIS-MaxEnt, and 3.4%
for L-BFGS-MaxEnt (Table 6).
</bodyText>
<table confidence="0.9969054">
Algorithm Recall Precision F1
Auxiliary-final VP 72.54 35.23 47.43
MBL 63.39 71.32 67.12
GIS-MaxEnt 54.90 77.06 64.12
L-BFGS-MaxEnt 71.89 76.38 74.07
</table>
<tableCaption confidence="0.8914345">
Table 6: Effects of using the Auxiliary-final VP
feature
</tableCaption>
<subsectionHeader confidence="0.377847">
Empty VP
</subsectionHeader>
<bodyText confidence="0.999863384615384">
Hardt (1997) uses a simple pattern check to search
for empty VP’s identified by the Treebank, (VP
(-NONE- *?*)), which achieves 60% F1 on our
test set. Our findings are in line with Hardt’s, who
reports 48% F1, with the difference being due to
the different sections of the Treebank used.
It was observed that this search may be too re-
strictive to catch some examples of VPE in the cor-
pus, and pseudo-gapping. Modifying the search
pattern to be ‘(VP (-NONE- *?*)’ instead im-
proves the feature itself by 10% in F1 and gives
the results seen in Table 7, increasing MBL’s F1 by
10%, GIS-MaxEnt by 14% and L-BFGS-MaxEnt
</bodyText>
<table confidence="0.991753166666667">
by 11.7%.
Algorithm Recall Precision F1
Empty VP 54.90 97.67 70.29
MBL 77.12 77.63 77.37
GIS-MaxEnt 69.93 88.42 78.10
L-BFGS-MaxEnt 83.00 88.81 85.81
</table>
<tableCaption confidence="0.92893">
Table 7: Effects of using the improved Empty VP
feature
</tableCaption>
<bodyText confidence="0.922055571428572">
Empty categories
Finally, including empty category information
completely, such that empty categories are treated
as words and included in the context. Table 8
shows that adding this information results in a 4%
increase in F1 for MBL, 4.9% for GIS-MaxEnt,
and 2.5% for L-BFGS-MaxEnt.
</bodyText>
<table confidence="0.9968195">
Algorithm Recall Precision F1
MBL 83.00 79.87 81.41
GIS-MaxEnt 76.47 90.69 82.97
L-BFGS-MaxEnt 86.27 90.41 88.29
</table>
<tableCaption confidence="0.999415">
Table 8: Effects of using the empty categories
</tableCaption>
<sectionHeader confidence="0.963016" genericHeader="method">
5 Experiments with Automatically
</sectionHeader>
<subsectionHeader confidence="0.687483">
Parsed data
</subsectionHeader>
<bodyText confidence="0.9999762">
The next set of experiments use the BNC and
Treebank, but strip POS and parse information,
and parse them automatically using two different
parsers. This enables us to test what kind of per-
formance is possible for real-world applications.
</bodyText>
<subsectionHeader confidence="0.99778">
5.1 Parsers used
</subsectionHeader>
<bodyText confidence="0.999938642857143">
Charniak’s parser (2000) is a combination prob-
abilistic context free grammar and maximum en-
tropy parser. It is trained on the Penn Treebank,
and achieves a 90.1% recall and precision average
for sentences of 40 words or less.
Robust Accurate Statistical Parsing (RASP)
(Briscoe and Carroll, 2002) uses a combination of
statistical techniques and a hand-crafted grammar.
RASP is trained on a range of corpora, and uses
a more complex tagging system (CLAWS-2), like
that of the BNC. This parser, on our data, gener-
ated full parses for 70% of the sentences, partial
parses for 28%, while 2% were not parsed, return-
ing POS tags only.
</bodyText>
<subsectionHeader confidence="0.998559">
5.2 Reparsing the Treebank
</subsectionHeader>
<bodyText confidence="0.996990222222222">
The results of experiments using the two parsers
(Table 9) show generally similar performance.
Compared to results on the original treebank with
similar data (Table 6), the results are 4-6% lower,
or in the case of GIS-MaxEnt, 4% lower or 2%
higher, depending on parser. This drop in per-
formance is not surprising, given the errors in-
troduced by the parsing process. As the parsers
do not generate empty-category information, their
overall results are 14-20% lower, compared to
those in Table 8.
The success rate for the features used (Table
10) stay the same, except for auxiliary-final VP,
which is determined by parse structure, is only half
as successful for RASP. Conversely, the heuristic
baseline is more successful for RASP, as it relies
on POS tags, which is to be expected as RASP has
a more detailed tagset.
</bodyText>
<figure confidence="0.598898">
Feature
Charniak close-to-punct
heuristic baseline
auxiliary-final VP
RASP close-to-punct
heuristic baseline
auxiliary-final VP
</figure>
<tableCaption confidence="0.993154">
Table 10: Performance of features on re-parsed
Treebank data
</tableCaption>
<subsectionHeader confidence="0.999629">
5.3 Parsing the BNC
</subsectionHeader>
<bodyText confidence="0.932651333333333">
Experiments using parsed versions of the BNC
corpora (Table 11) show similar results to the orig-
inal results (Table 1) - except L-BFGS-MaxEnt
which scores 4-8% lower - meaning that the added
information from the features mitigates the errors
introduced in parsing. The performance of the fea-
tures (Table 12) remain similar to those for the re-
parsed treebank experiments.
Feature
</bodyText>
<tableCaption confidence="0.8018535">
Charniak close-to-punct
heuristic baseline
auxiliary-final VP
RASP close-to-punct
heuristic baseline
auxiliary-final VP
Table 12: Performance of features on parsed BNC
data
</tableCaption>
<subsectionHeader confidence="0.99964">
5.4 Combining BNC and Treebank data
</subsectionHeader>
<bodyText confidence="0.999918125">
Combining the re-parsed BNC and Treebank data
diversifies and increases the size of the test data,
making conclusions drawn empirically more reli-
able, and the wider range of training data makes
it more robust. This gives a training set of 1167
VPE’s and a test set of 350 VPE’s. The results
in Table 13 show little change from the previous
experiments.
</bodyText>
<sectionHeader confidence="0.954192" genericHeader="conclusions">
6 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.872126045454546">
This paper has presented a robust system for VPE
detection. The data is automatically tagged and
parsed, syntactic features are extracted and ma-
chine learning is used to classify instances. Three
different machine learning algorithms, Memory
Based Learning, GIS-based and L-BFGS-based
maximum entropy modeling are used. They give
similar results, with L-BFGS-MaxEnt generally
giving the highest performance. Two parsers were
used, Charniak’s and RASP, achieving similar re-
sults.
To summarise the findings:
• Using the BNC, which is tagged with a com-
plex tagging scheme but has no parse data, it
is possible to get 76% F1 using lexical forms
and POS data alone
• Using the Treebank, the coarser tagging
scheme reduces performance to 67%.
Adding extra features, including sentence-
level ones, raises this to 74%. Adding empty
category information gives 88%, compared
to previous results of 48% (Hardt, 1997)
</bodyText>
<listItem confidence="0.954748285714286">
• Re-parsing the Treebank data , top perfor-
mance is 63%, raised to 68% using extra fea-
tures
• Parsing the BNC, top performance is 71%,
raised to 72% using extra features
• Combining the parsed data, top performance
is 67%, raised to 71% using extra features
</listItem>
<bodyText confidence="0.999390428571428">
The results demonstrate that the method can be
applied to practical tasks using free text. Next,
we will experiment with an algorithm (Johnson,
2002) that can insert empty-category information
into data from Charniak’s parser, allowing replica-
tion of features that need this. Cross-validation ex-
periments will be performed to negate the effects
the small test set may cause.
As machine learning is used to combine vari-
ous features, this method can be extended to other
forms of ellipsis, and other languages. However,
a number of the features used are specific to En-
glish VPE, and would have to be adapted to such
cases. It is difficult to extrapolate how successful
</bodyText>
<table confidence="0.99989325">
Rec Prec F1
34.00 2.47 4.61
45.33 25.27 32.45
51.33 36.66 42.77
71.05 2.67 5.16
74.34 28.25 40.94
22.36 25.18 23.69
Rec Prec F1
48.00 5.52 9.90
44.00 34.50 38.68
53.00 42.91 47.42
55.32 4.06 7.57
84.77 35.15 49.70
16.24 28.57 20.71
Rec MBL F1 GIS-MaxEnt L-BFGS-MaxEnt
Prec Rec Prec F1 Rec Prec F1
Charniak Words + POS 54.00 62.30 57.85 38.66 79.45 52.01 56.66 71.42 63.19
+ features 58.00 65.41 61.48 50.66 73.78 60.07 65.33 72.05 68.53
RASP Words + POS 55.92 66.92 60.93 43.42 56.89 49.25 51.63 79.00 62.45
+ features 57.23 71.31 63.50 61.84 72.30 66.66 62.74 73.84 67.84
</table>
<tableCaption confidence="0.995752">
Table 9: Results on re-parsed data from the Treebank
</tableCaption>
<table confidence="0.999969">
Rec MBL F1 GIS-MaxEnt L-BFGS-MaxEnt
Prec Rec Prec F1 Rec Prec F1
Charniak Words + POS 66.50 63.63 65.03 55.00 75.86 63.76 71.00 70.64 70.82
+ features 67.50 67.16 67.33 65.00 75.58 69.89 71.00 73.19 72.08
RASP Words + POS 61.92 63.21 62.56 64.46 54.04 58.79 65.34 70.96 68.04
+ features 71.06 73.29 72.16 73.09 61.01 66.51 70.29 67.29 68.76
</table>
<tableCaption confidence="0.997523">
Table 11: Results on parsed data from the BNC
</tableCaption>
<table confidence="0.999963">
Rec MBL F1 GIS-MaxEnt L-BFGS-MaxEnt
Prec Rec Prec F1 Rec Prec F1
Charniak Words + POS 62.28 69.20 65.56 54.28 77.86 63.97 65.14 69.30 67.15
+ features 65.71 71.87 68.65 63.71 72.40 67.78 70.85 69.85 70.35
RASP Words + POS 63.61 67.47 65.48 59.31 55.94 57.37 57.46 71.83 63.84
+ features 68.48 69.88 69.17 67.61 71.47 69.48 70.14 72.17 71.14
</table>
<tableCaption confidence="0.999634">
Table 13: Results on parsed data using the combined dataset
</tableCaption>
<bodyText confidence="0.996572">
such approaches would be based on current work,
but it can be expected that they would be feasible,
albeit with lower performance.
</bodyText>
<sectionHeader confidence="0.999221" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999894133333333">
Eric Brill. 1995. Transformation-based error-driven learning and natural lan-
guage processing: A case study in part-of-speech tagging. Computational
Linguistics, 21(4):543–565.
E. Briscoe and J. Carroll. 2002. Robust accurate statistical annotation of gen-
eral text. In Proceedings of the 3rd International Conference on Language
Resources and Evaluation, Las Palmas, Gran Canaria.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Meeting of
the North American Chapter of the ACL, page 132.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch.
2002. Tilburg memory based learner, version 4.3, reference guide. Down-
loadable from http://ilk.kub.nl/downloads/pub/papers/ilk0210.ps.gz.
Mary Dalrymple, Stuart M. Shieber, and Fernando Pereira. 1991. Ellipsis and
higher-order unification. Linguistics and Philosophy, 14:399–452.
Robert Fiengo and Robert May. 1994. Indices and Identity. MIT Press, Cam-
bridge, MA.
Daniel Hardt. 1993. VP Ellipsis: Form, Meaning, and Processing. Ph.D.
thesis, University of Pennsylvania.
Daniel Hardt. 1997. An empirical approach to vp ellipsis. Computational
Linguistics, 23(4).
Mark Johnson. 2002. A simple pattern-matching algorithm for recovering
empty nodes and their antecedents. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguistics.
Andrew Kehler and Gregory Ward. 1999. On the semantics and pragmat-
ics of ‘identifier so’. In Ken Turner, editor, The Semantics/Pragmatics
Interface from Different Points of View (Current Research in the Seman-
tics/Pragmatics Interface Series, Volume I). Amsterdam: Elsevier.
Andrew Kehler. 1993. A discourse copying algorithm for ellipsis and
anaphora resolution. In Proceedings of the Sixth Conference of the Euro-
pean Chapter of the Associationfor Computational Linguistics (EACL-93),
Utrecht, the Netherlands.
Torbjorn Lager. 1999. The mu-tbl system: Logic programming tools for
transformation-based learning. In Third International Workshop on Com-
putational Natural Language Learning (CoNLL’99). Downloadable from
http://www.ling.gu.se/ lager/mutbl.html.
Shalom Lappin. 1993. The syntactic basis of ellipsis resolution. In S. Berman
and A. Hestvik, editors, Proceedings ofthe Stuttgart Ellipsis Workshop, Ar-
beitspapiere des Sonderforschungsbereichs 340, Bericht Nr. 29-1992. Uni-
versity of Stuttgart, Stuttgart.
Shalom Lappin. 1996. The interpretation of ellipsis. In Shalom Lappin, ed-
itor, The Handbook of Contemporary Semantic Theory, pages 145–175.
Oxford: Blackwell.
G. Leech. 1992. 100 million words of english : The British National Corpus.
Language Research, 28(1):1–13.
Robert Malouf. 2002. A comparison of algorithms for maximum entropy
parameter estimation. In Proceedings of the Sixth Conference on Natural
Language Learning (CoNLL-2002), pages 49–55.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, M. Bies, M. Fergu-
son, K. Katz, and B. Schasberger. 1994. The Penn Treebank: Annotat-
ing predicate argument structure. In Proceedings of the Human Language
Technology Workshop. Morgan Kaufmann, San Francisco.
Leif Arda Nielsen. 2003a. A corpus-based study of verb phrase ellipsis. In
Proceedings of the 6th Annual CLUKResearch Colloquium.
Leif Arda Nielsen. 2003b. Using machine learning techniques for VPE detec-
tion. In Proceedings ofRANLP.
R. Quinlan. 1993. C4.5: Programs for Machine Learning. San Mateo, CA:
Morgan Kaufmann.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language
Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania.
Stuart Shieber, Fernando Pereira, and Mary Dalrymple. 1996. Interactions of
scope and ellipsis. Linguistics and Philosophy, 19(5):527–552.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.891642">
<title confidence="0.99788">Robust VPE detection using Automatically Parsed Text</title>
<author confidence="0.999922">Leif Arda Nielsen</author>
<affiliation confidence="0.9759075">Department of Computer Science King’s College London</affiliation>
<email confidence="0.994935">nielsen@dcs.kcl.ac.uk</email>
<abstract confidence="0.9957085">This paper describes a Verb Phrase Ellipsis (VPE) detection system, built for robustness, accuracy and domain independence. The system is corpus-based, and uses machine learning techniques on free text that has been automatically parsed. Tested on a mixed corpus comprising a range of genres, the system achieves a 70% F1-score. This system is designed as the first stage of a complete VPE resolution system that is input free text, detects VPEs, and proceeds to find the antecedents and resolve them.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="3695" citStr="Brill, 1995" startWordPosition="607" endWordPosition="608">orithm for detecting VPE in the Penn Treebank (see Section 3) achieves precision levels of 44% and recall of 53%, giving an F11 of 48%, using a simple search technique, which relies on the parse annotation having identified empty expressions correctly. In previous work (Nielsen, 2003a; Nielsen, 2003b) we performed experiments on the British National Corpus using a variety of machine learning techniques. These earlier results are not directly comparable to Hardt’s, due to the different corpora used. The expanded set of results are summarised in Table 1, for Transformation Based Learning (TBL) (Brill, 1995), GIS based Maximum Entropy Modelling (GIS-MaxEnt) (Ratnaparkhi, 1998), L-BFGS based Maximum Entropy Modelling (L-BFGS-MaxEnt)2 (Malouf, 2002), Decision Tree Learning (Quinlan, 1993) and Memory Based Learning (MBL) (Daelemans et al., 2002). Algorithm Recall Precision F1 TBL 69.63 85.14 76.61 Decision Tree 60.93 79.39 68.94 MBL 72.58 71.50 72.04 GIS-MaxEnt 71.72 63.89 67.58 L-BFGS-MaxEnt 71.93 80.58 76.01 Table 1: Comparison of algorithms 1Precision, recall and F1 are defined as : No(correct ellipses found) Recall = (1) No(all ellipses in test) No(correct ellipses found) Precision = (2) No(all </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<location>Las Palmas, Gran Canaria.</location>
<contexts>
<context position="12451" citStr="Briscoe and Carroll, 2002" startWordPosition="2056" endWordPosition="2059">f using the empty categories 5 Experiments with Automatically Parsed data The next set of experiments use the BNC and Treebank, but strip POS and parse information, and parse them automatically using two different parsers. This enables us to test what kind of performance is possible for real-world applications. 5.1 Parsers used Charniak’s parser (2000) is a combination probabilistic context free grammar and maximum entropy parser. It is trained on the Penn Treebank, and achieves a 90.1% recall and precision average for sentences of 40 words or less. Robust Accurate Statistical Parsing (RASP) (Briscoe and Carroll, 2002) uses a combination of statistical techniques and a hand-crafted grammar. RASP is trained on a range of corpora, and uses a more complex tagging system (CLAWS-2), like that of the BNC. This parser, on our data, generated full parses for 70% of the sentences, partial parses for 28%, while 2% were not parsed, returning POS tags only. 5.2 Reparsing the Treebank The results of experiments using the two parsers (Table 9) show generally similar performance. Compared to results on the original treebank with similar data (Table 6), the results are 4-6% lower, or in the case of GIS-MaxEnt, 4% lower or </context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>E. Briscoe and J. Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, Las Palmas, Gran Canaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Meeting of the North American Chapter of the ACL,</booktitle>
<pages>132</pages>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Meeting of the North American Chapter of the ACL, page 132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>Tilburg memory based learner, version 4.3, reference guide. Downloadable from http://ilk.kub.nl/downloads/pub/papers/ilk0210.ps.gz.</title>
<date>2002</date>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2002</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2002. Tilburg memory based learner, version 4.3, reference guide. Downloadable from http://ilk.kub.nl/downloads/pub/papers/ilk0210.ps.gz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
<author>Stuart M Shieber</author>
<author>Fernando Pereira</author>
</authors>
<date>1991</date>
<booktitle>Ellipsis and higher-order unification. Linguistics and Philosophy,</booktitle>
<pages>14--399</pages>
<contexts>
<context position="870" citStr="Dalrymple et al., 1991" startWordPosition="130" endWordPosition="133">uracy and domain independence. The system is corpus-based, and uses machine learning techniques on free text that has been automatically parsed. Tested on a mixed corpus comprising a range of genres, the system achieves a 70% F1-score. This system is designed as the first stage of a complete VPE resolution system that is input free text, detects VPEs, and proceeds to find the antecedents and resolve them. 1 Introduction Ellipsis is a linguistic phenomenon that has received considerable attention, mostly focusing on its interpretation. Most work on ellipsis (Fiengo and May, 1994; Lappin, 1993; Dalrymple et al., 1991; Kehler, 1993; Shieber et al., 1996) is aimed at discerning the procedures and the level of language processing at which ellipsis resolution takes place, or ambiguous and difficult cases. The detection of elliptical sentences or the identification of the antecedent and elided clauses within them are usually not dealt with, but taken as given. Noisy or missing input, which is unavoidable in NLP applications, is not dealt with, and neither is focusing on specific domains or applications. It therefore becomes clear that a robust, trainable approach is needed. An example of Verb Phrase Ellipsis (</context>
</contexts>
<marker>Dalrymple, Shieber, Pereira, 1991</marker>
<rawString>Mary Dalrymple, Stuart M. Shieber, and Fernando Pereira. 1991. Ellipsis and higher-order unification. Linguistics and Philosophy, 14:399–452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Fiengo</author>
<author>Robert May</author>
</authors>
<title>Indices and Identity.</title>
<date>1994</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="832" citStr="Fiengo and May, 1994" startWordPosition="124" endWordPosition="127">on system, built for robustness, accuracy and domain independence. The system is corpus-based, and uses machine learning techniques on free text that has been automatically parsed. Tested on a mixed corpus comprising a range of genres, the system achieves a 70% F1-score. This system is designed as the first stage of a complete VPE resolution system that is input free text, detects VPEs, and proceeds to find the antecedents and resolve them. 1 Introduction Ellipsis is a linguistic phenomenon that has received considerable attention, mostly focusing on its interpretation. Most work on ellipsis (Fiengo and May, 1994; Lappin, 1993; Dalrymple et al., 1991; Kehler, 1993; Shieber et al., 1996) is aimed at discerning the procedures and the level of language processing at which ellipsis resolution takes place, or ambiguous and difficult cases. The detection of elliptical sentences or the identification of the antecedent and elided clauses within them are usually not dealt with, but taken as given. Noisy or missing input, which is unavoidable in NLP applications, is not dealt with, and neither is focusing on specific domains or applications. It therefore becomes clear that a robust, trainable approach is needed</context>
</contexts>
<marker>Fiengo, May, 1994</marker>
<rawString>Robert Fiengo and Robert May. 1994. Indices and Identity. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hardt</author>
</authors>
<title>VP Ellipsis: Form, Meaning, and Processing.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10267" citStr="Hardt (1993)" startWordPosition="1700" endWordPosition="1701">C-2, and the next category NP-SBJ. The results of using this feature are seen in Table 5, giving a 3.5% boost to MBL, 2% to GIS-MaxEnt, and 1.6% to L-BFGSMaxEnt. Algorithm Recall Precision F1 MBL 58.82 69.23 63.60 GIS-MaxEnt 45.09 81.17 57.98 L-BFGS-MaxEnt 64.70 77.95 70.71 Table 5: Effects of using the surrounding categories Auxiliary-final VP For auxiliary verbs parsed as verb phrases (VP), this feature checks if the final element in the VP is an auxiliary or negation. If so, no main verb can be present, as a main verb cannot be followed by an auxiliary or negation. This feature was used by Hardt (1993) and gives a 3.5% boost to performance for MBL, 6% for GIS-MaxEnt, and 3.4% for L-BFGS-MaxEnt (Table 6). Algorithm Recall Precision F1 Auxiliary-final VP 72.54 35.23 47.43 MBL 63.39 71.32 67.12 GIS-MaxEnt 54.90 77.06 64.12 L-BFGS-MaxEnt 71.89 76.38 74.07 Table 6: Effects of using the Auxiliary-final VP feature Empty VP Hardt (1997) uses a simple pattern check to search for empty VP’s identified by the Treebank, (VP (-NONE- *?*)), which achieves 60% F1 on our test set. Our findings are in line with Hardt’s, who reports 48% F1, with the difference being due to the different sections of the Treeb</context>
</contexts>
<marker>Hardt, 1993</marker>
<rawString>Daniel Hardt. 1993. VP Ellipsis: Form, Meaning, and Processing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hardt</author>
</authors>
<title>An empirical approach to vp ellipsis.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="2028" citStr="Hardt, 1997" startWordPosition="327" endWordPosition="328">proach is needed. An example of Verb Phrase Ellipsis (VPE), which is detected by the presence of an auxiliary verb without a verb phrase, is seen in example 1. VPE can also occur with semi-auxiliaries, as in example 2. (1) John3 {loves his3 wife}2. Bill3 does1 too. (2) But although he was terse, he didn’t {rage at me}2 the way I expected him to1. Several steps of work need to be done for ellipsis resolution: 1. Detecting ellipsis occurrences. First, elided verbs need to be found. 2. Identifying antecedents. For most cases of ellipsis, copying of the antecedent clause is enough for resolution (Hardt, 1997). 3. Resolving ambiguities. For cases where ambiguity exists, a method for generating the full list of possible solutions, and suggesting the most likely one is needed. This paper describes the work done on the first stage, the detection of elliptical verbs. First, previous work done on tagged corpora will be summarised. Then, new work on parsed corpora will be presented, showing the gains possible through sentence-level features. Finally, experiments using unannotated data that is parsed using an automatic parser are presented, as our aim is to produce a stand-alone system. We have chosen to </context>
<context position="10600" citStr="Hardt (1997)" startWordPosition="1753" endWordPosition="1754">iary-final VP For auxiliary verbs parsed as verb phrases (VP), this feature checks if the final element in the VP is an auxiliary or negation. If so, no main verb can be present, as a main verb cannot be followed by an auxiliary or negation. This feature was used by Hardt (1993) and gives a 3.5% boost to performance for MBL, 6% for GIS-MaxEnt, and 3.4% for L-BFGS-MaxEnt (Table 6). Algorithm Recall Precision F1 Auxiliary-final VP 72.54 35.23 47.43 MBL 63.39 71.32 67.12 GIS-MaxEnt 54.90 77.06 64.12 L-BFGS-MaxEnt 71.89 76.38 74.07 Table 6: Effects of using the Auxiliary-final VP feature Empty VP Hardt (1997) uses a simple pattern check to search for empty VP’s identified by the Treebank, (VP (-NONE- *?*)), which achieves 60% F1 on our test set. Our findings are in line with Hardt’s, who reports 48% F1, with the difference being due to the different sections of the Treebank used. It was observed that this search may be too restrictive to catch some examples of VPE in the corpus, and pseudo-gapping. Modifying the search pattern to be ‘(VP (-NONE- *?*)’ instead improves the feature itself by 10% in F1 and gives the results seen in Table 7, increasing MBL’s F1 by 10%, GIS-MaxEnt by 14% and L-BFGS-Max</context>
<context position="15714" citStr="Hardt, 1997" startWordPosition="2580" endWordPosition="2581"> entropy modeling are used. They give similar results, with L-BFGS-MaxEnt generally giving the highest performance. Two parsers were used, Charniak’s and RASP, achieving similar results. To summarise the findings: • Using the BNC, which is tagged with a complex tagging scheme but has no parse data, it is possible to get 76% F1 using lexical forms and POS data alone • Using the Treebank, the coarser tagging scheme reduces performance to 67%. Adding extra features, including sentencelevel ones, raises this to 74%. Adding empty category information gives 88%, compared to previous results of 48% (Hardt, 1997) • Re-parsing the Treebank data , top performance is 63%, raised to 68% using extra features • Parsing the BNC, top performance is 71%, raised to 72% using extra features • Combining the parsed data, top performance is 67%, raised to 71% using extra features The results demonstrate that the method can be applied to practical tasks using free text. Next, we will experiment with an algorithm (Johnson, 2002) that can insert empty-category information into data from Charniak’s parser, allowing replication of features that need this. Cross-validation experiments will be performed to negate the effe</context>
</contexts>
<marker>Hardt, 1997</marker>
<rawString>Daniel Hardt. 1997. An empirical approach to vp ellipsis. Computational Linguistics, 23(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>A simple pattern-matching algorithm for recovering empty nodes and their antecedents.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="16122" citStr="Johnson, 2002" startWordPosition="2651" endWordPosition="2652">ing scheme reduces performance to 67%. Adding extra features, including sentencelevel ones, raises this to 74%. Adding empty category information gives 88%, compared to previous results of 48% (Hardt, 1997) • Re-parsing the Treebank data , top performance is 63%, raised to 68% using extra features • Parsing the BNC, top performance is 71%, raised to 72% using extra features • Combining the parsed data, top performance is 67%, raised to 71% using extra features The results demonstrate that the method can be applied to practical tasks using free text. Next, we will experiment with an algorithm (Johnson, 2002) that can insert empty-category information into data from Charniak’s parser, allowing replication of features that need this. Cross-validation experiments will be performed to negate the effects the small test set may cause. As machine learning is used to combine various features, this method can be extended to other forms of ellipsis, and other languages. However, a number of the features used are specific to English VPE, and would have to be adapted to such cases. It is difficult to extrapolate how successful Rec Prec F1 34.00 2.47 4.61 45.33 25.27 32.45 51.33 36.66 42.77 71.05 2.67 5.16 74</context>
</contexts>
<marker>Johnson, 2002</marker>
<rawString>Mark Johnson. 2002. A simple pattern-matching algorithm for recovering empty nodes and their antecedents. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kehler</author>
<author>Gregory Ward</author>
</authors>
<title>On the semantics and pragmatics of ‘identifier so’.</title>
<date>1999</date>
<booktitle>The Semantics/Pragmatics Interface from Different Points of View (Current Research in the Semantics/Pragmatics Interface Series, Volume I).</booktitle>
<editor>In Ken Turner, editor,</editor>
<publisher>Elsevier.</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="3002" citStr="Kehler and Ward, 1999" startWordPosition="494" endWordPosition="497">ora will be presented, showing the gains possible through sentence-level features. Finally, experiments using unannotated data that is parsed using an automatic parser are presented, as our aim is to produce a stand-alone system. We have chosen to concentrate on VP ellipsis due to the fact that it is far more common than other forms of ellipsis, but pseudo-gapping, an example of which is seen in example 3, has also been included due to the similarity of its resolution to VPE (Lappin, 1996). Do so/it/that and so doing anaphora are not handled, as their resolution is different from that of VPE (Kehler and Ward, 1999). (3) John writes plays, and Bill does novels. 2 Previous work Hardt’s (1997) algorithm for detecting VPE in the Penn Treebank (see Section 3) achieves precision levels of 44% and recall of 53%, giving an F11 of 48%, using a simple search technique, which relies on the parse annotation having identified empty expressions correctly. In previous work (Nielsen, 2003a; Nielsen, 2003b) we performed experiments on the British National Corpus using a variety of machine learning techniques. These earlier results are not directly comparable to Hardt’s, due to the different corpora used. The expanded se</context>
</contexts>
<marker>Kehler, Ward, 1999</marker>
<rawString>Andrew Kehler and Gregory Ward. 1999. On the semantics and pragmatics of ‘identifier so’. In Ken Turner, editor, The Semantics/Pragmatics Interface from Different Points of View (Current Research in the Semantics/Pragmatics Interface Series, Volume I). Amsterdam: Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kehler</author>
</authors>
<title>A discourse copying algorithm for ellipsis and anaphora resolution.</title>
<date>1993</date>
<booktitle>In Proceedings of the Sixth Conference of the European Chapter of the Associationfor Computational Linguistics (EACL-93),</booktitle>
<location>Utrecht, the Netherlands.</location>
<contexts>
<context position="884" citStr="Kehler, 1993" startWordPosition="134" endWordPosition="135">dence. The system is corpus-based, and uses machine learning techniques on free text that has been automatically parsed. Tested on a mixed corpus comprising a range of genres, the system achieves a 70% F1-score. This system is designed as the first stage of a complete VPE resolution system that is input free text, detects VPEs, and proceeds to find the antecedents and resolve them. 1 Introduction Ellipsis is a linguistic phenomenon that has received considerable attention, mostly focusing on its interpretation. Most work on ellipsis (Fiengo and May, 1994; Lappin, 1993; Dalrymple et al., 1991; Kehler, 1993; Shieber et al., 1996) is aimed at discerning the procedures and the level of language processing at which ellipsis resolution takes place, or ambiguous and difficult cases. The detection of elliptical sentences or the identification of the antecedent and elided clauses within them are usually not dealt with, but taken as given. Noisy or missing input, which is unavoidable in NLP applications, is not dealt with, and neither is focusing on specific domains or applications. It therefore becomes clear that a robust, trainable approach is needed. An example of Verb Phrase Ellipsis (VPE), which is</context>
</contexts>
<marker>Kehler, 1993</marker>
<rawString>Andrew Kehler. 1993. A discourse copying algorithm for ellipsis and anaphora resolution. In Proceedings of the Sixth Conference of the European Chapter of the Associationfor Computational Linguistics (EACL-93), Utrecht, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torbjorn Lager</author>
</authors>
<title>The mu-tbl system: Logic programming tools for transformation-based learning.</title>
<date>1999</date>
<booktitle>In Third International Workshop on Computational Natural Language Learning (CoNLL’99). Downloadable from http://www.ling.gu.se/ lager/mutbl.html.</booktitle>
<contexts>
<context position="5035" citStr="Lager, 1999" startWordPosition="813" endWordPosition="814">t.html For all of these experiments, the training features consisted of lexical forms and Part of Speech (POS) tags of the words in a three word forward/backward window of the auxiliary being tested. This context size was determined empirically to give optimum results, and will be used throughout this paper. The L-BFGS-MaxEnt uses Gaussian Prior smoothing which was optimized for the BNC data, while the GIS-MaxEnt has a simple smoothing option available, but this deteriorates results and is not used. MBL was used with its default settings. While TBL gave the best results, the software we used (Lager, 1999) ran into memory problems and proved problematic with larger datasets. Decision trees, on the other hand, tend to oversimplify due to the very sparse nature of ellipsis, and produce a single rule that classifies everything as nonVPE. This leaves Maximum Entropy and MBL for further experiments. 3 Corpus description The British National Corpus (BNC) (Leech, 1992) is annotated with POS tags, using the CLAWS-4 tagset. A range of V sections of the BNC, containing around 370k words3 with 645 samples of VPE was used as training data. The separate test data consists of around 74k words4 with 200 sampl</context>
</contexts>
<marker>Lager, 1999</marker>
<rawString>Torbjorn Lager. 1999. The mu-tbl system: Logic programming tools for transformation-based learning. In Third International Workshop on Computational Natural Language Learning (CoNLL’99). Downloadable from http://www.ling.gu.se/ lager/mutbl.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
</authors>
<title>The syntactic basis of ellipsis resolution.</title>
<date>1993</date>
<booktitle>Proceedings ofthe Stuttgart Ellipsis Workshop, Arbeitspapiere des Sonderforschungsbereichs 340, Bericht Nr.</booktitle>
<pages>29--1992</pages>
<editor>In S. Berman and A. Hestvik, editors,</editor>
<institution>University of Stuttgart,</institution>
<location>Stuttgart.</location>
<contexts>
<context position="846" citStr="Lappin, 1993" startWordPosition="128" endWordPosition="129">obustness, accuracy and domain independence. The system is corpus-based, and uses machine learning techniques on free text that has been automatically parsed. Tested on a mixed corpus comprising a range of genres, the system achieves a 70% F1-score. This system is designed as the first stage of a complete VPE resolution system that is input free text, detects VPEs, and proceeds to find the antecedents and resolve them. 1 Introduction Ellipsis is a linguistic phenomenon that has received considerable attention, mostly focusing on its interpretation. Most work on ellipsis (Fiengo and May, 1994; Lappin, 1993; Dalrymple et al., 1991; Kehler, 1993; Shieber et al., 1996) is aimed at discerning the procedures and the level of language processing at which ellipsis resolution takes place, or ambiguous and difficult cases. The detection of elliptical sentences or the identification of the antecedent and elided clauses within them are usually not dealt with, but taken as given. Noisy or missing input, which is unavoidable in NLP applications, is not dealt with, and neither is focusing on specific domains or applications. It therefore becomes clear that a robust, trainable approach is needed. An example o</context>
</contexts>
<marker>Lappin, 1993</marker>
<rawString>Shalom Lappin. 1993. The syntactic basis of ellipsis resolution. In S. Berman and A. Hestvik, editors, Proceedings ofthe Stuttgart Ellipsis Workshop, Arbeitspapiere des Sonderforschungsbereichs 340, Bericht Nr. 29-1992. University of Stuttgart, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
</authors>
<title>The interpretation of ellipsis.</title>
<date>1996</date>
<booktitle>The Handbook of Contemporary Semantic Theory,</booktitle>
<pages>145--175</pages>
<editor>In Shalom Lappin, editor,</editor>
<publisher>Blackwell.</publisher>
<location>Oxford:</location>
<contexts>
<context position="2874" citStr="Lappin, 1996" startWordPosition="473" endWordPosition="474">tion of elliptical verbs. First, previous work done on tagged corpora will be summarised. Then, new work on parsed corpora will be presented, showing the gains possible through sentence-level features. Finally, experiments using unannotated data that is parsed using an automatic parser are presented, as our aim is to produce a stand-alone system. We have chosen to concentrate on VP ellipsis due to the fact that it is far more common than other forms of ellipsis, but pseudo-gapping, an example of which is seen in example 3, has also been included due to the similarity of its resolution to VPE (Lappin, 1996). Do so/it/that and so doing anaphora are not handled, as their resolution is different from that of VPE (Kehler and Ward, 1999). (3) John writes plays, and Bill does novels. 2 Previous work Hardt’s (1997) algorithm for detecting VPE in the Penn Treebank (see Section 3) achieves precision levels of 44% and recall of 53%, giving an F11 of 48%, using a simple search technique, which relies on the parse annotation having identified empty expressions correctly. In previous work (Nielsen, 2003a; Nielsen, 2003b) we performed experiments on the British National Corpus using a variety of machine learn</context>
</contexts>
<marker>Lappin, 1996</marker>
<rawString>Shalom Lappin. 1996. The interpretation of ellipsis. In Shalom Lappin, editor, The Handbook of Contemporary Semantic Theory, pages 145–175. Oxford: Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leech</author>
</authors>
<title>100 million words of english :</title>
<date>1992</date>
<journal>The British National Corpus. Language Research,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="5398" citStr="Leech, 1992" startWordPosition="872" endWordPosition="873"> was optimized for the BNC data, while the GIS-MaxEnt has a simple smoothing option available, but this deteriorates results and is not used. MBL was used with its default settings. While TBL gave the best results, the software we used (Lager, 1999) ran into memory problems and proved problematic with larger datasets. Decision trees, on the other hand, tend to oversimplify due to the very sparse nature of ellipsis, and produce a single rule that classifies everything as nonVPE. This leaves Maximum Entropy and MBL for further experiments. 3 Corpus description The British National Corpus (BNC) (Leech, 1992) is annotated with POS tags, using the CLAWS-4 tagset. A range of V sections of the BNC, containing around 370k words3 with 645 samples of VPE was used as training data. The separate test data consists of around 74k words4 with 200 samples of VPE. The Penn Treebank (Marcus et al., 1994) has more than a hundred phrase labels, and a number of empty categories, but uses a coarser tagset. A mixture of sections from the Wall Street Journal and Brown corpus were used. The training section5 consists of around 540k words and contains 522 samples of VPE. The test section6 consists of around 140k words </context>
</contexts>
<marker>Leech, 1992</marker>
<rawString>G. Leech. 1992. 100 million words of english : The British National Corpus. Language Research, 28(1):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Conference on Natural Language Learning (CoNLL-2002),</booktitle>
<pages>49--55</pages>
<contexts>
<context position="3837" citStr="Malouf, 2002" startWordPosition="625" endWordPosition="626"> a simple search technique, which relies on the parse annotation having identified empty expressions correctly. In previous work (Nielsen, 2003a; Nielsen, 2003b) we performed experiments on the British National Corpus using a variety of machine learning techniques. These earlier results are not directly comparable to Hardt’s, due to the different corpora used. The expanded set of results are summarised in Table 1, for Transformation Based Learning (TBL) (Brill, 1995), GIS based Maximum Entropy Modelling (GIS-MaxEnt) (Ratnaparkhi, 1998), L-BFGS based Maximum Entropy Modelling (L-BFGS-MaxEnt)2 (Malouf, 2002), Decision Tree Learning (Quinlan, 1993) and Memory Based Learning (MBL) (Daelemans et al., 2002). Algorithm Recall Precision F1 TBL 69.63 85.14 76.61 Decision Tree 60.93 79.39 68.94 MBL 72.58 71.50 72.04 GIS-MaxEnt 71.72 63.89 67.58 L-BFGS-MaxEnt 71.93 80.58 76.01 Table 1: Comparison of algorithms 1Precision, recall and F1 are defined as : No(correct ellipses found) Recall = (1) No(all ellipses in test) No(correct ellipses found) Precision = (2) No(all ellipses found) F1 = 2 × Precision × Recall (3) Precision + Recall 2Downloadable from http://www.nlplab.cn/zhangle/maxent toolkit.html For all</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the Sixth Conference on Natural Language Learning (CoNLL-2002), pages 49–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>G Kim</author>
<author>M Marcinkiewicz</author>
<author>R MacIntyre</author>
<author>M Bies</author>
<author>M Ferguson</author>
<author>K Katz</author>
<author>B Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the Human Language Technology Workshop.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="5685" citStr="Marcus et al., 1994" startWordPosition="923" endWordPosition="926">proved problematic with larger datasets. Decision trees, on the other hand, tend to oversimplify due to the very sparse nature of ellipsis, and produce a single rule that classifies everything as nonVPE. This leaves Maximum Entropy and MBL for further experiments. 3 Corpus description The British National Corpus (BNC) (Leech, 1992) is annotated with POS tags, using the CLAWS-4 tagset. A range of V sections of the BNC, containing around 370k words3 with 645 samples of VPE was used as training data. The separate test data consists of around 74k words4 with 200 samples of VPE. The Penn Treebank (Marcus et al., 1994) has more than a hundred phrase labels, and a number of empty categories, but uses a coarser tagset. A mixture of sections from the Wall Street Journal and Brown corpus were used. The training section5 consists of around 540k words and contains 522 samples of VPE. The test section6 consists of around 140k words and contains 150 samples of VPE. 4 Experiments using the Penn Treebank To experiment with what gains are possible through the use of more complex data such as 3Sections CS6, A2U, J25, FU6, H7F, HA3, A19, A0P, G1A, EWC, FNS, C8T 4Sections EDJ, FR3 5Sections WSJ 00, 01, 03, 04, 15, Brown </context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, M. Bies, M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the Human Language Technology Workshop. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leif Arda Nielsen</author>
</authors>
<title>A corpus-based study of verb phrase ellipsis.</title>
<date>2003</date>
<booktitle>In Proceedings of the 6th Annual CLUKResearch Colloquium.</booktitle>
<contexts>
<context position="3367" citStr="Nielsen, 2003" startWordPosition="555" endWordPosition="556">mple of which is seen in example 3, has also been included due to the similarity of its resolution to VPE (Lappin, 1996). Do so/it/that and so doing anaphora are not handled, as their resolution is different from that of VPE (Kehler and Ward, 1999). (3) John writes plays, and Bill does novels. 2 Previous work Hardt’s (1997) algorithm for detecting VPE in the Penn Treebank (see Section 3) achieves precision levels of 44% and recall of 53%, giving an F11 of 48%, using a simple search technique, which relies on the parse annotation having identified empty expressions correctly. In previous work (Nielsen, 2003a; Nielsen, 2003b) we performed experiments on the British National Corpus using a variety of machine learning techniques. These earlier results are not directly comparable to Hardt’s, due to the different corpora used. The expanded set of results are summarised in Table 1, for Transformation Based Learning (TBL) (Brill, 1995), GIS based Maximum Entropy Modelling (GIS-MaxEnt) (Ratnaparkhi, 1998), L-BFGS based Maximum Entropy Modelling (L-BFGS-MaxEnt)2 (Malouf, 2002), Decision Tree Learning (Quinlan, 1993) and Memory Based Learning (MBL) (Daelemans et al., 2002). Algorithm Recall Precision F1 T</context>
</contexts>
<marker>Nielsen, 2003</marker>
<rawString>Leif Arda Nielsen. 2003a. A corpus-based study of verb phrase ellipsis. In Proceedings of the 6th Annual CLUKResearch Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leif Arda Nielsen</author>
</authors>
<title>Using machine learning techniques for VPE detection.</title>
<date>2003</date>
<booktitle>In Proceedings ofRANLP.</booktitle>
<contexts>
<context position="3367" citStr="Nielsen, 2003" startWordPosition="555" endWordPosition="556">mple of which is seen in example 3, has also been included due to the similarity of its resolution to VPE (Lappin, 1996). Do so/it/that and so doing anaphora are not handled, as their resolution is different from that of VPE (Kehler and Ward, 1999). (3) John writes plays, and Bill does novels. 2 Previous work Hardt’s (1997) algorithm for detecting VPE in the Penn Treebank (see Section 3) achieves precision levels of 44% and recall of 53%, giving an F11 of 48%, using a simple search technique, which relies on the parse annotation having identified empty expressions correctly. In previous work (Nielsen, 2003a; Nielsen, 2003b) we performed experiments on the British National Corpus using a variety of machine learning techniques. These earlier results are not directly comparable to Hardt’s, due to the different corpora used. The expanded set of results are summarised in Table 1, for Transformation Based Learning (TBL) (Brill, 1995), GIS based Maximum Entropy Modelling (GIS-MaxEnt) (Ratnaparkhi, 1998), L-BFGS based Maximum Entropy Modelling (L-BFGS-MaxEnt)2 (Malouf, 2002), Decision Tree Learning (Quinlan, 1993) and Memory Based Learning (MBL) (Daelemans et al., 2002). Algorithm Recall Precision F1 T</context>
</contexts>
<marker>Nielsen, 2003</marker>
<rawString>Leif Arda Nielsen. 2003b. Using machine learning techniques for VPE detection. In Proceedings ofRANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA:</location>
<contexts>
<context position="3877" citStr="Quinlan, 1993" startWordPosition="630" endWordPosition="631">s on the parse annotation having identified empty expressions correctly. In previous work (Nielsen, 2003a; Nielsen, 2003b) we performed experiments on the British National Corpus using a variety of machine learning techniques. These earlier results are not directly comparable to Hardt’s, due to the different corpora used. The expanded set of results are summarised in Table 1, for Transformation Based Learning (TBL) (Brill, 1995), GIS based Maximum Entropy Modelling (GIS-MaxEnt) (Ratnaparkhi, 1998), L-BFGS based Maximum Entropy Modelling (L-BFGS-MaxEnt)2 (Malouf, 2002), Decision Tree Learning (Quinlan, 1993) and Memory Based Learning (MBL) (Daelemans et al., 2002). Algorithm Recall Precision F1 TBL 69.63 85.14 76.61 Decision Tree 60.93 79.39 68.94 MBL 72.58 71.50 72.04 GIS-MaxEnt 71.72 63.89 67.58 L-BFGS-MaxEnt 71.93 80.58 76.01 Table 1: Comparison of algorithms 1Precision, recall and F1 are defined as : No(correct ellipses found) Recall = (1) No(all ellipses in test) No(correct ellipses found) Precision = (2) No(all ellipses found) F1 = 2 × Precision × Recall (3) Precision + Recall 2Downloadable from http://www.nlplab.cn/zhangle/maxent toolkit.html For all of these experiments, the training feat</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>R. Quinlan. 1993. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3765" citStr="Ratnaparkhi, 1998" startWordPosition="616" endWordPosition="618">hieves precision levels of 44% and recall of 53%, giving an F11 of 48%, using a simple search technique, which relies on the parse annotation having identified empty expressions correctly. In previous work (Nielsen, 2003a; Nielsen, 2003b) we performed experiments on the British National Corpus using a variety of machine learning techniques. These earlier results are not directly comparable to Hardt’s, due to the different corpora used. The expanded set of results are summarised in Table 1, for Transformation Based Learning (TBL) (Brill, 1995), GIS based Maximum Entropy Modelling (GIS-MaxEnt) (Ratnaparkhi, 1998), L-BFGS based Maximum Entropy Modelling (L-BFGS-MaxEnt)2 (Malouf, 2002), Decision Tree Learning (Quinlan, 1993) and Memory Based Learning (MBL) (Daelemans et al., 2002). Algorithm Recall Precision F1 TBL 69.63 85.14 76.61 Decision Tree 60.93 79.39 68.94 MBL 72.58 71.50 72.04 GIS-MaxEnt 71.72 63.89 67.58 L-BFGS-MaxEnt 71.93 80.58 76.01 Table 1: Comparison of algorithms 1Precision, recall and F1 are defined as : No(correct ellipses found) Recall = (1) No(all ellipses in test) No(correct ellipses found) Precision = (2) No(all ellipses found) F1 = 2 × Precision × Recall (3) Precision + Recall 2Do</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Fernando Pereira</author>
<author>Mary Dalrymple</author>
</authors>
<title>Interactions of scope and ellipsis. Linguistics and Philosophy,</title>
<date>1996</date>
<contexts>
<context position="907" citStr="Shieber et al., 1996" startWordPosition="136" endWordPosition="139">tem is corpus-based, and uses machine learning techniques on free text that has been automatically parsed. Tested on a mixed corpus comprising a range of genres, the system achieves a 70% F1-score. This system is designed as the first stage of a complete VPE resolution system that is input free text, detects VPEs, and proceeds to find the antecedents and resolve them. 1 Introduction Ellipsis is a linguistic phenomenon that has received considerable attention, mostly focusing on its interpretation. Most work on ellipsis (Fiengo and May, 1994; Lappin, 1993; Dalrymple et al., 1991; Kehler, 1993; Shieber et al., 1996) is aimed at discerning the procedures and the level of language processing at which ellipsis resolution takes place, or ambiguous and difficult cases. The detection of elliptical sentences or the identification of the antecedent and elided clauses within them are usually not dealt with, but taken as given. Noisy or missing input, which is unavoidable in NLP applications, is not dealt with, and neither is focusing on specific domains or applications. It therefore becomes clear that a robust, trainable approach is needed. An example of Verb Phrase Ellipsis (VPE), which is detected by the presen</context>
</contexts>
<marker>Shieber, Pereira, Dalrymple, 1996</marker>
<rawString>Stuart Shieber, Fernando Pereira, and Mary Dalrymple. 1996. Interactions of scope and ellipsis. Linguistics and Philosophy, 19(5):527–552.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>