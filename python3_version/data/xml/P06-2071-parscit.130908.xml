<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000337">
<title confidence="0.991383">
Discriminating image senses by clustering with multimodal features
</title>
<author confidence="0.99797">
Nicolas Loeff
</author>
<affiliation confidence="0.9990705">
Dept. of Computer Science
University of Illinois, UC
</affiliation>
<email confidence="0.996589">
loeff@uiuc.edu
</email>
<author confidence="0.980474">
Cecilia Ovesdotter Alm
</author>
<affiliation confidence="0.996481">
Dept. of Linguistics
University of Illinois, UC
</affiliation>
<email confidence="0.997469">
ebbaalm@uiuc.edu
</email>
<author confidence="0.973965">
David A. Forsyth
</author>
<affiliation confidence="0.9988715">
Dept. of Computer Science
University of Illinois, UC
</affiliation>
<email confidence="0.998262">
daf@uiuc.edu
</email>
<sectionHeader confidence="0.993892" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998983636363636">
We discuss Image Sense Discrimination
(ISD), and apply a method based on spec-
tral clustering, using multimodal features
from the image and text of the embedding
web page. We evaluate our method on a
new data set of annotated web images, re-
trieved with ambiguous query terms. Ex-
periments investigate different levels of
sense granularity, as well as the impact of
text and image features, and global versus
local text features.
</bodyText>
<sectionHeader confidence="0.961816" genericHeader="keywords">
1 Introduction and problem clarification
</sectionHeader>
<bodyText confidence="0.99289162295082">
Semantics extends beyond words. We focus on im-
age sense discrimination (ISD)1 for web images
retrieved from ambiguous keywords, given a mul-
timodal feature set, including text from the doc-
ument which the image was embedded in. For
instance, a search for CRANE retrieves images of
crane machines, crane birds, associated other ma-
chinery or animals etc., people, as well as images
of irrelevant meanings. Current displays for im-
age queries (e.g. Google or Yahoo!) simply list
retrieved images in any order. An application is
a user display where images are presented in se-
mantically sensible clusters for improved image
browsing. Another usage of the presented model
is automatic creation of sense discriminated image
data sets, and determining available image senses
automatically.
ISD differs from word sense discrimination and
disambiguation (WSD) by increased complexity
in several respects. As an initial complication,
both word and iconographic sense distinctions
1Cf. (Sch¨utze, 1998) for a definition of sense discrimina-
tion in NLP.
matter. Whereas a search term like CRANE can
refer to, e.g. a MACHINE or a BIRD; iconographic
distinctions could additionally include birds stand-
ing, vs. in a marsh land, or flying, i.e. sense-
distinctions encoded by further descriptive modi-
fication in text. Therefore, as the number of text
senses grow with corpus size, the iconographic
senses grow even faster, and enumerating icono-
graphic senses is extremely challenging; espe-
cially since dictionary senses do not capture icono-
graphic distinctions. Thus, we focus on image-
driven word senses for ISD, but we acknowledge
the importance of iconography for visual meaning.
Also, an image often depicts a related mean-
ing. E.g. a picture retrieved for SQUASH may
depict a squash bug (i.e. an insect on a leaf of
a squash plant) instead of a squash vegetable,
whereas this does not really apply in WSD, where
each instance concerns the ambiguous term itself.
Therefore, it makes sense to consider the divi-
sion between core sense, related sense, and un-
related sense in ISD, and, as an additional com-
plication, their boundaries are often blurred. Most
importantly, whereas the one-sense-per-discourse
assumption (Yarowsky, 1995) also applies to dis-
criminating images, there is no guarantee of
a local collocational or co-occurrence context
around the target image. Design or aesthetics may
instead determine image placement. Thus, con-
sidering local text around the image may not be as
helpful as local context is for standard WSD. In
fact, the query term may even not occur in the
text body. On the other hand, one can assume that
an image spotlights the web page topic and that it
highlights important document information. Also,
images mostly depict concrete senses. Lastly, ISD
from web data is complicated by web pages being
more domain-independent than news wire, the fa-
</bodyText>
<page confidence="0.964119">
547
</page>
<bodyText confidence="0.3753555">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 547–554,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</bodyText>
<figure confidence="0.965439">
(a) squash flower (b) tennis? (c) hook (d) food (e) bow (f) speaker
</figure>
<figureCaption confidence="0.997436">
Figure 1: Example RELATED images for (a) vegetable and (b) sports senses for SQUASH, and for (c-d) fish and (e-f) musical
instrument for BASS. Related senses are associated with the semantic field of a core sense, but the core sense is visually absent
or undeterminable.
Figure 2: Which fish or instruments are BASS? Image sense annotation is more vague and subjective than in text.
</figureCaption>
<bodyText confidence="0.999852307692308">
vored corpus for WSD. As noted by (Yanai and
Barnard, 2005), whereas current image retrieval
engines include many irrelevant images, a data set
of web images gives a more real-world point of
departure for image recognition.
Outline Section 2 discusses the corpus data and
image annotation. Section 3 presents the feature
set and the clustering model. Subsequently, sec-
tion 4 introduces the evaluation used, and dis-
cusses experimental work and results. In section
5, this work is positioned with respect to previous
work. We conclude with an outline of plans for
future work in section 6.
</bodyText>
<sectionHeader confidence="0.803873" genericHeader="introduction">
2 Data and annotation
</sectionHeader>
<bodyText confidence="0.999899444444444">
Yahoo!’s image query API was used to obtain a
corpus of pairs of semantically ambiguous images,
in thumbnail and true size, and their correspond-
ing web sites for three ambiguous keywords in-
spired by (Yarowsky, 1995): BASS, CRANE, and
SQUASH. We apply query augmentation (cf. Ta-
ble 1), and exact duplicates were filtered out by
identical image URLs, but cases occurred where
both thumbnail and true-size image were included.
Also, some images shared the same webpage or
came from the same site. Generally, the lat-
ter gives important information about shared dis-
course topic, however the images do not necessar-
ily depict the same sense (e.g. a CRANE bird vs.
a meadow), and image features can separate them
into different clusters.
Annotation overview The images were anno-
tated with one of several labels by one of the au-
thors out of context (without considering the web
site and its text), after applying text-based filter-
ing (cf. section 3.1). For annotation purposes, im-
ages were numbered and displayed on a web page
in thumbnail size. In case the thumbnail was not
sufficient for disambiguation, the image linked at
its true size to the thumbnail was inspected.2 The
true-size view depended on the size of the orig-
inal picture and showed the image and its name.
However, the annotator tried to resist name influ-
ence, and make judgements based just on the im-
age. For each query, 2 to 4 core word senses (e.g.
squash vegetable and squash sport for SQUASH)
were distinguished from inspecting the data. How-
ever, because “context” was restricted to the image
content, and there was no guarantee that the image
actually depicts the query term, additional anno-
tator senses were introduced. Thus, for most core
senses, a RELATED label was included, accounting
for meanings that seemed related to core meaning
but lacked a core sense object in the image. Some
examples for RELATED senses are in Fig. 1. In ad-
dition, for each query term, a PEOPLE label was
included because such images are common due to
the nature of how people take pictures (e.g. por-
traits of persons or group pictures of crowds, when
core or related senses did not apply), as was an
</bodyText>
<footnote confidence="0.7699995">
2We noticed a few cases where Yahoo! retrieved a thumb-
nail image different from the true size image.
</footnote>
<page confidence="0.989423">
548
</page>
<table confidence="0.718259086956522">
Word (#Annot. images) QueryTerms Senses Coverage Examples of visual annotation cues
BASS 5: bass, bass guitar, 1. fish 35% any fish, people holding catch
(2881) bass instrument, 2. musical instrument 28% any bass-looking instrument, playing
bass fishing, sea 3. related: fish 10% fishing (gear, boats, farms), rel. food, rel. charts/maps
bass 4. related: musical instrument 8% speakers, accessories, works, chords, rel. music
5. unrelated 12% miscellaneous (above senses not applicable)
6. people 7% faces, crowd (above senses not applicable)
CRANE 5: crane, 1. machine 21% machine crane, incl. panoramas
(2650) construction cranes, 2. bird 26% crane bird or chick
whooping crane, 3. origami 4% origami bird
sandhill crane, 4. related: machine 11% other machinery, construction, motor, steering, seat
origami cranes 5. related: bird 11% egg, other birds, wildlife, insects, hunting, rel. maps/charts
6. related: origami 1% origami shapes (stars, pigs), paper folding
7. people 7% faces, crowd (above senses not applicable)
8. unrelated 18% miscellaneous (above senses not applicable)
9. karate 1% martial arts
SQUASH 10: squash+: rules, 1. vegetable 24% squash vegetable
(1948) butternut, vegetable, 2. sport 13% people playing, court, equipment
grow, game of, 3. related:vegetable 31% agriculture, food, plant, flower, insect, vegetables
spaghetti, winter, 4. related:sport o other sports, sports complex
types of, summer 5. people 6/0 faces, crowd (above senses not applicable)
6. unrelated 10% miscellaneous (above senses not applicable)
16%
</table>
<tableCaption confidence="0.931016">
Table 1: Web images for three ambiguous query terms were annotated manually out of context (without considering the
</tableCaption>
<bodyText confidence="0.998854325581396">
web page document). For each term, the number of annotated images, the query retrieval terms, the senses, their distribution,
and rough sample annotation guidelines are provided, with core senses marked in bold face. Because image retrieval engines
restrict hits to 1000 images, query expansion was conducted by adding narrowing query terms from askjeeves.com to
increase corpus size. We selected terms relevant to core senses, i.e. the main discrimination phenomenon.
UNRELATED label for irrelevant images which did
not fit other labels or were undeterminable.
For a human annotator, even when using more
natural word senses, assigning sense labels to im-
ages based on image alone is more challenging
and subjective than labeling word senses in tex-
tual context. First of all, the annotation is heav-
ily dependent on domain-knowledge and it is not
feasible for a layperson to recognize fine-grained
semantics. For example, it is straightforward for
the layperson to distinguish between a robin and a
crane, but determining whether a given fish should
have the common name bass applied to it, or
whether an instrument is indeed a bass instrument
or not, is extremely difficult (see Fig. 2; e.g. de-
ciding if a picture of a fish fillet is a picture of a
fish is tricky). Furthermore, most images display
objects only partially; for example just the neck
of a classical double bass instead of the whole in-
strument. In addition, scaling, proportions, and
components are key cues for object discrimina-
tion in real-life, e.g. for singling out an electric
bass from an electric guitar, but an image may
not provide these detail. Thus, senses are even
fuzzier for ISD than WSD labeling. Given that
laypeople are in the majority, it is fair to assume
their perspective and naiveness. This latter fact
also led to annotations’ level of specificity differ-
ing according to search term. Annotation criteria
depended on the keyword term and its senses and
their coverage, as shown in Table 1. Neverthe-
less, several border-line cases for label assignment
occurred. Considering that the annotation task is
quite subjective, this is to be expected. In fact,
one person’s labeling often appears as justifiable
as a contradicting label provided by another per-
son. We explore the vagueness and subjective na-
ture of image annotation further in a companion
paper (Alm, Loeff, Forsyth, 2006).
</bodyText>
<sectionHeader confidence="0.991739" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999954666666667">
Our goal is to provide a mapping between im-
ages and a set of iconographically coherent clus-
ters for a given query word, in an unsupervised
framework. Our approach involves extracting
and weighting unordered bags-of-words (BOWs;
henceforth) features from the webpage text, sim-
ple local and global features from the image, and
running spectral clustering on top. Fig. 3 shows an
overview of the implementation.
</bodyText>
<figure confidence="0.987563636363636">
Keyword
query
Filtering
Image feature
extraction
Text
feature extraction
1. Compute pair-wise document affinities
2. Compute eigenvalues
3. Embed and cluster
Evaluation of purity
</figure>
<figureCaption confidence="0.999286">
Figure 3: Overview of algorithm
</figureCaption>
<page confidence="0.98414">
549
</page>
<subsectionHeader confidence="0.995617">
3.1 Feature extraction
</subsectionHeader>
<bodyText confidence="0.999951553191489">
Document and text filtering A pruning process
was used to filter out image-document pairs based
on e.g. language specification, exclusion of “In-
dex of” pages, pages lacking an extractable target
image, or a cutoff threshold of number of tokens
in the body. For remaining documents, text was
preprocessed (e.g. lower-casing, removing punc-
tuation, tokens being very short, having numbers
or no vowels, etc.). We used a stop word list, but
avoided stemming to make the algorithm language
independent in other respects. When using image
features, grayscale images (no color histograms)
and images without salient regions (no keypoints
detected) were also removed.
Text features We used the following BOWs:
(a) tokens in the page body; (b) tokens in a ±10
window around the target image (if multiple, the
first was considered); (c) tokens in a ±10 window
around any instances of the query keyword (e.g.
squash); (d) tokens of the target image’s alt at-
tribute; (e) tokens of the title tag; (f) some meta
tokens.3 Tf-idf was applied to a weighted aver-
age of the BOWs. Webpage design is flexible, and
some inconsistencies and a certain degree of noise
remained in the text features.
Image features Given the large variability in
the retrieved image set for a given query, it is dif-
ficult to model images in an unsupervised fash-
ion. Simple features have been shown to provide
performance rivaling that of more elaborate mod-
els in object recognition (Csurka et al, 2004) and
(Chapelle, Haffner, and Vapnik, 1999), and the
following image bags of features were considered:
Bags of keypoints: In order to obtain a compact
representation of the textures of an image, patches
are extracted automatically around interesting re-
gions or keypoints in each image. The keypoint
detection algorithm (Kadir and Brady, 2001) uses
a saliency measure based on entropy to select re-
gions. After extraction, keypoints were repre-
sented by a histogram of gradient magnitude of
the pixel values in the region (SIFT) (Lowe, 2004).
These descriptors were clustered using a Gaussian
Mixture with Pz� 300 components, and the result-
ing global patch codebook (i.e. histogram of code-
book entries) was used as lookup table to assign
each keypoint to a codebook entry.
</bodyText>
<footnote confidence="0.836525">
3Adding to META content, keywords was an attribute, but
is irregular. Embedded BODY pairs are rare; thus not used.
</footnote>
<bodyText confidence="0.9983916">
Color histograms: Due to its similarity to
how humans perceive color, HSV (hue, saturation,
brightness) color space was used to bin pixel color
values for each image. Eight bins were used per
channel, obtaining an 83 dimensional vector.
</bodyText>
<subsectionHeader confidence="0.999844">
3.2 Measuring similarity between images
</subsectionHeader>
<bodyText confidence="0.998167555555556">
For the BOWs text representation, we use the com-
mon measure of cosine similarity (cs) of two tf-
idf vectors (Jurafsky and Martin, 2000). The co-
sine similarity measure is also appropriate for key-
point representation as it is also an unordered bag.
There are several measures for histogram compar-
ison (i.e. L1, x2). As in (Fowlkes et al, 2004) we
use the x2 distance measure between histograms
hi and hj.
</bodyText>
<equation confidence="0.9990015">
(hi(k) − hj(k))2 (1)
hi(k) + hj(k)
</equation>
<subsectionHeader confidence="0.991111">
3.3 Spectral Clustering
</subsectionHeader>
<bodyText confidence="0.999076333333333">
Spectral clustering is a powerful way to sepa-
rate non-convex groups of data. Spectral meth-
ods for clustering are a family of algorithms that
work by first constructing a pairwise-affinity ma-
trix from the data, computing an eigendecomposi-
tion of the data, embedding the data into this low-
dimensional manifold, and finally applying tradi-
tional clustering techniques (i.e. k-means) to it.
Consider a graph with a set of n vertices each
one representing an image document, and the
edges of the graph represent the pairwise affinities
between the vertices. Let W be an n x n symmet-
ric matrix of pairwise affinities. We define these
as the Gaussian-weighted distance
Wij = exp (−αt(1 − csti,j) − αk(1 − cski,j) − α χi,j)
(2)
where {αt, αk, αc} are scaling parameters for text,
keypoints, and color features.
It has been shown that the use of multiple eigen-
vectors of W is a valid space onto which the data
can be embedded (Ng, Jordan, Weiss, 2002). In
this space noise is reduced while the most signif-
icant affinities are preserved. After this, any tra-
ditional clustering algorithm can be applied in this
new space to get the final clusters. Note that this
is a nonlinear mapping of the original space. In
particular, we employ a variant of k-means, which
includes a selective step that is quasi-optimal in
a Vector Quantization sense (Ueda and Nakano,
1994). It has the added advantage of being more
</bodyText>
<equation confidence="0.559151714285714">
1
5 1 2
χ2
i,j =
5 1 2 2
E
k=1
</equation>
<page confidence="0.98364">
550
</page>
<bodyText confidence="0.9976255">
robust to initialization than traditional k-means.
The algorithm follows,
</bodyText>
<listItem confidence="0.935703818181818">
1. For given documents, compute the affinity
matrix W as defined in equation 2.
2. Let D be a diagonal matrix whose (i, i)-th
element is the sum of W’s i-th row, and de-
fine L = D−1/2WD−1/2.
3. Find the k largest eigenvectors V of L.
4. Define E as V , with normalized rows.
5. Perform clustering on the columns of E,
which represent the embedding of each im-
age into the new space, using a selective step
as in (Ueda and Nakano, 1994).
</listItem>
<bodyText confidence="0.981474388888889">
Why Spectral Clustering? Why apply a vari-
ant of k-means in the embedded space as opposed
to the original feature space? The k-means algo-
rithm cannot separate non-convex clusters. Fur-
thermore, it is unable to cope with noisy dimen-
sions (this is especially true in the case of the text
data) and highly non-ellipsoid clusters. (Ng, Jor-
dan, Weiss, 2002) stated that spectral clustering
outperforms k-means not only on these high di-
mensional problems, but also in low-dimensional,
multi-class data sets. Moreover, there are prob-
lems where Euclidean measures of distance re-
quired by k-means are not appropriate (for in-
stance histograms), or others where there is not
even a natural vector space representation. Also,
spectral clustering provides a simple way of com-
bining dissimilar vector spaces, like in this case
text, keypoint and color features.
</bodyText>
<sectionHeader confidence="0.991757" genericHeader="method">
4 Experiments and results
</sectionHeader>
<bodyText confidence="0.96892875">
In the first set of experiments, we used all features
for clustering. We considered three levels of sense
granularity: (1) all senses (All), (2) merging re-
lated senses with their corresponding core sense
(Meta), (3) just the core senses (Core). For ex-
periments (1) and (2), we used 40 clusters and all
labeled images. For (3), we considered only im-
ages labeled with core senses, and thus reduced the
number of clusters to 20 for a more fair compari-
son. Results were evaluated according to global
cluster purity, cf. Equation 3.4
# of most common sense in cluster
</bodyText>
<figure confidence="0.65576">
total # images
(3)
</figure>
<footnote confidence="0.972914">
4Purity did not include the small set of outlier images, de-
fined as images whose ratio of distances to the second closest
and closest clusters was below a threshold.
</footnote>
<table confidence="0.999299692307692">
Word All senses Meta senses Core senses
BASS 6 senses 4 senses 2 senses
Median 0.60 0.73 0.94
Range 0.03 0.02 0.02
Baseline 0.35 0.45 0.55
CRANE 9 senses 6 senses 4 senses
Median 0.49 0.65 0.86
Range 0.05 0.07 0.07
Baseline 0.27 0.37 0.50
SQUASH 6 senses 4 senses 2 senses
Median 0.52 0.71 0.94
Range 0.03 0.04 0.03
Baseline 0.32 0.56 0.64
</table>
<tableCaption confidence="0.9661052">
Table 2: Median and range of global clustering purity
for 5 runs with different initializations. For each keyword, the
table lists the number of senses, median, and range of global
cluster purity, followed by the baseline. All senses used the
full set of sense labels and 40 clusters. Meta senses merged
core senses with their respective related senses, considering
all images and using 40 clusters. Core senses were clustered
into 20 clusters, using only images labeled with core sense la-
bels. Purity was stable across runs, and peaked for Core. The
baseline reflected the frequency of the most common sense.
</tableCaption>
<table confidence="0.9998287">
Word Img TxtWin BodyTxt Baseline
BASS
Median 0.71 0.83 0.93 0.55
Range 0.05 0.03 0.05
CRANE
Median 0.61 0.84 0.85 0.50
Range 0.07 0.04 0.05
SQUASH
Median 0.71 0.91 0.96 0.64
Range 0.05 0.04 0.03
</table>
<tableCaption confidence="0.6116122">
Table 3: Global and local features’ performance. Core
sense images were grouped into 20 clusters, on the basis of
individual feature types, and global cluster purity was mea-
sured. The table lists the median and range from 5 runs with
different initializations. Img included just image features;
TxtWin local tokens in a ±10 window around the target im-
age anchor; BodyTxt global tokens in the page BODY; and
Baseline uses the most common sense. Text performed bet-
ter than image features, and global text appeared better than
local. All features performed above the baseline.
</tableCaption>
<bodyText confidence="0.995118230769231">
Median and range results are reported for five
runs, given each condition, comparing against the
baseline (i.e. choosing the most common sense).
Table 2 shows that purity was surprisingly good,
stable across query terms, and that it was high-
est when only core sense data was considered. In
addition, purity tended to be slightly higher for
BASS, which may be related to the annotator being
less confident about its fine-grained sense distinc-
tions, and thus less strict for assigning core sense
labels for this query term.5 In addition, we looked
at the relative performance of individual global
and local features using 20 clusters and only core
</bodyText>
<footnote confidence="0.9765475">
5A slightly modified HTML extractor yielded similar re-
sults (±0-2% median, ±0-5% range cf. to Tables 2 - 4).
</footnote>
<figure confidence="0.899440666666667">
�
Global purity =
clusters
</figure>
<page confidence="0.881398">
551
</page>
<figureCaption confidence="0.9943865">
Figure 4: First 30 images from a CRANE BIRD cluster consisting of 81 images in the median run. Individual cluster purity
for all senses was 0.67, and for meta senses 0.83. Not all clusters were as pure as this one; global purity for all 40 cluster was
0.49. This cluster appeared to show some iconography; mostly standing cranes. Interestingly, another cluster contained several
images of flying cranes. Most weighted tokens: cranes whooping birds wildlife species. Table 1 has sense labels.
Figure 5: Global purity does not tell the whole story SQUASH VEGETABLE cluster of 22 images in the median run.
Individual cluster purity for all senses was 0.5, and for meta senses 1.0. Global purity for all 40 cluster was 0.52. This cluster
both shows visually coherent images, and a sensible meta semantic field. Most weighted tokens: chayote calabaza add bitter
cup. Presumably, some tokens reflect the vegetable’s use within the cooking domain.
</figureCaption>
<bodyText confidence="0.999807071428572">
sense data based on a particular feature. Table 3
shows that global text features were most infor-
mative (although not homogenously), but also that
each feature type performed better than the base-
line in isolation. This indicates that an optimal fea-
ture combination may improve over current per-
formance, using manually selected parameters. In
addition, purity is not the whole story. Figs. 4
and 5 show examples of two selected interesting
clusters obtained for CRANE and SQUASH, respec-
tively, using combined image and text features and
all individual senses.6 Inspection of image clus-
ters indicated that image features, both in isolation
and when used in combination, appeared to con-
</bodyText>
<footnote confidence="0.9854165">
6The UIUC-ISD data set and results are currently at
http://www.visionpc.cs.uiuc.edu/isd/.
</footnote>
<bodyText confidence="0.999677">
tribute to more visually balanced clusters, espe-
cially in terms of colors and shading. This shows
that further exploring image features may be vi-
tal for attaining more subtle iconographic senses.
Moreover, as discussed in the introduction, images
are not necessarily anchored in the immediate text
which they refer to. This could explain why lo-
cal text features do not perform as well as global
ones. Lastly, in addition, Fig. 6 shows an example
of a partial cluster where the algorithm inferred a
specific related sense.
We also experimented with different number of
clusters for BASS. The results are in Table 4, lack-
ing a clear trend, with comparable variation to dif-
ferent initializations. This is surprising, since we
would expect purity to increase with number of
</bodyText>
<page confidence="0.99482">
552
</page>
<figureCaption confidence="0.712551">
Figure 6: RELATED: SQUASH VEGETABLE cluster, consisting of 27 images. The algorithm discovered a specific SQUASH
</figureCaption>
<table confidence="0.963245222222222">
BUG-PLANT sense, which appears iconographic. Individual cluster purity for all senses was 0.85, and individual meta purity:
1.0. Global purity for all 40 clusters: 0.52. Most weighted tokens: bugs bug beetle leaf-footed kentucky.
# Clusters 6 10 20 40 80
All
Median 0.61 0.55 0.58 0.60 0.61
Range 0.03 0.05 0.03 0.03 0.04
Meta
Median 0.75 0.70 0.70 0.73 0.72
Range 0.04 0.07 0.04 0.02 0.04
</table>
<tableCaption confidence="0.993685">
Table 4: Impact of cluster size? We ran BASS for different
</tableCaption>
<bodyText confidence="0.970066416666667">
number of clusters (5 runs each with distinct initializations),
and recorded median and range of global purity for all six
senses of the query term, and for the four meta senses, with-
out a clear trend.
clusters (Sch¨utze, 1998), but may be due to the
spectral clustering. Inspection showed that 6 clus-
ters were dominated by core senses, whereas with
40 clusters a few were also dominated by RE-
LATED senses or PEOPLE. No cluster was domi-
nated by an UNRELATED label, which makes sense
since semantic linkage should be absent between
unrelated items.
</bodyText>
<sectionHeader confidence="0.972761" genericHeader="method">
5 Comparison to previous work
</sectionHeader>
<bodyText confidence="0.999631744186047">
Space does not allow a complete review of the
WSD literature. (Yarowsky, 1995) demonstrated
that semi-supervised WSD could be successful.
(Sch¨utze, 1998) and (Lin and Pantel, 2002a, b)
show that clustering methods are helpful in this
area.
While ISD has received less attention, image
categorization has been approached previously
by adding text features. For example, (Frankel,
Swain, and Athitsos, 1996)’s WebSeer system
attempted to mutually distinguish photos, hand-
drawn, and computer-drawn images, using a com-
bination of HTML markup, web page text, and im-
age information. (Yanai and Barnard, 2005) found
that adding text features could benefit identifying
relevant web images. Using text-annotated images
(i.e. images annotated with relevant keywords),
(Barnard and Forsyth, 2001) clustered them ex-
ploring a semantic hierarchy; similarly (Barnard,
Duygulu, and Forsyth, 2002) conducted art clus-
tering, and (Barnard and Johnson, 2005) used text-
annotated images to improve WSD. The latter pa-
per obtained best results when combining text and
image features, but contrary to our findings, im-
age features performed better in isolation than just
text. They did use a larger set of image features
and segmentation, however, we suspect that dif-
ferences can rather be attributed to corpus type. In
fact, (Yanai, Shirahatti, and Barnard, 2005) noted
that human evaluators rated images obtained via
a keyword retrieval method higher compared to
image-based retrieval methods, which they relate
to the importance of semantics for what humans
regard as matching, and because pictorial seman-
tics is hard to detect.
(Cai et al, 2004) use similar methods to rank
visual search results. While their work does not
focus explicitly on sense and does not provide in-
depth discussion of visual sense phenomena, these
do appear in, for example, figs. 7 and 9 of their pa-
per. An interesting aspect of their work is the use
of page layout segmentation to associate text with
images in web documents. Unfortunately, the au-
</bodyText>
<page confidence="0.995593">
553
</page>
<bodyText confidence="0.999891818181818">
thors only provide an illustrative query example,
and no numerical evaluation, making any com-
parison difficult. (Wang et al, 2004) use similar
features with the goal to improve image retrieval
through similarity propagation, querying specific
web sites. (Fuji and Ishikawa, 2005) deal with
image ambiguity for establishing an online mul-
timedia encyclopedia, but their method does not
integrate image features, and appears to depend
on previous encyclopedic background knowledge,
limited to a domain set.
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999995555555556">
It is remarkable how high purity is, considering
that we are using relatively simple image and text
representation. In most corpora used to date for re-
search on illustrated text, word sense is an entirely
secondary phenomenon, whereas our data set was
collected as to emphasize possible ambiguities as-
sociated with word sense. Our results suggest that
a surprisingly degree of the meaning of an illus-
trated object is exposed on the surface.
This work is an initial attempt at addressing
the ISD problem. Future work will involve learn-
ing the algorithm’s parameters without supervi-
sion, and develop a semantically meaningful im-
age taxonomy. In particular, we intend to explore
the notion of iconographic senses; surprisingly
good results on image classification by (Chapelle,
Haffner, and Vapnik, 1999) using image features
suggest that iconography plays an important role
in the semantics of images. An important aspect
is to enhance our understanding of the interplay
between text and image features for this purpose.
Also, it remains an unsolved problem how to enu-
merate iconographic senses, and use them in man-
ual annotation and classification. Experimental
work with humans performing similar tasks may
provide increased insight into this issue, and can
also be used to validate clustering performance.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999773666666667">
We are grateful to Roxana Girju and Richard
Sproat for helpful feedback, and to Alexander
Sorokin.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999857827586207">
C. O. Alm, N. Loeff, and D. Forsyth. 2006. Challenges for
annotating images for sense disambiguation. ACL work-
shop on Frontiers in Linguistically Annotated Corpora.
K. Barnard and D. Forsyth. 2001. Learning the semantics of
words and pictures. ICCV, 408–415.
K. Barnard, P. Duygulu, and D. Forsyth. 2002. Modeling the
statistics of image features and associated text. SPIE.
K. Barnard and M. Johnson. 2005. Word sense disambigua-
tion with pictures. Artificial Intelligence, 167, 13–30.
D. Cai et al. 2004. Hierarchical clustering of WWW image
search results using visual, textual and link information.
ACMMultimedia, 952-959.
O. Chapelle and P. Haffner and V. Vapnik. 1999. Support
vector machines for histogram-based image classification.
IEEE Neural Networks, 10(5), 1055–1064.
G. Csurka et al. 2004. Visual categorization with bags
of keypoints. ECCV Int. Workshop on Stat. Learning in
Computer Vision.
C. Frankel, M. Swain, and V. Athitsos. 1996. WebSeer: an
image search engine for the World Wide Web. Univ. of
Chicago, Computer Science, Technical report #96-14.
C. Fowlkes, S. Belongie, F. Chung, and J. Malik. 2004.
Spectral grouping using the Nystr¨om method. IEEE
PAMI, 26(2),214-225.
A. Fuji and T. Ishikawa. 2005. Toward the automatic com-
pilation of multimedia encyclopedias: associating images
with term descriptions on the web. IEEE WI, 536-542.
D. Jurafsky and J. Martin 2000. Speech and Language Pro-
cessing, Prentice Hall.
T. Kadir and M. Brady. 2001. Scale, saliency and image
description. Int. Journal of Computer Vision, 45 (2):83–
105.
D. Lin and P. Pantel. 2002a. Concept discovery from text.
COLING, 577–583.
D. Lowe. 2004. Distinctive image features from scale-
invariant keypoints. Int. Journal of Computer Vision,
60(2), 91–110.
A. Ng, M. Jordan, and Y. Weiss. 2002. On spectral cluster-
ing: analysis and an algorithm. NIPS 14.
P. Pantel and D. Lin. 2002b. Discovering word senses from
text. KDD, 613–619.
H. Schuetze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97–123.
J. Shi and J. Malik. 2000. Normalized cuts and image seg-
mentation. IEEE PAMI, 22(8):888–905.
N. Ueda. and R. Nakano. 1994. A new competitive learn-
ing approach based on an equidistortion principle for
designing optimal vector quantizers. Neural Networks,
7(8):1211–1227.
X.-J. Wang et al. 2004. Multi-model similarity propagation
and its application for image retrieval. MM,944–951.
K. Yanai and K. Barnard. 2005. Probabilistic web image
gathering. SIGMM, 57–64.
K. Yanai, N. V. Shirahatti, and K. Barnard. 2005. Evaluation
strategies for image understanding and retrieval. SIGMM,
217-226.
D. Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. ACL, 189–196.
</reference>
<page confidence="0.998711">
554
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.919757">
<title confidence="0.999835">Discriminating image senses by clustering with multimodal features</title>
<author confidence="0.999891">Nicolas Loeff</author>
<affiliation confidence="0.998709">Dept. of Computer Science University of Illinois, UC</affiliation>
<email confidence="0.995476">loeff@uiuc.edu</email>
<author confidence="0.955213">Cecilia Ovesdotter Alm</author>
<affiliation confidence="0.997075">Dept. of Linguistics University of Illinois, UC</affiliation>
<email confidence="0.998459">ebbaalm@uiuc.edu</email>
<author confidence="0.999584">David A Forsyth</author>
<affiliation confidence="0.9939905">Dept. of Computer Science University of Illinois, UC</affiliation>
<email confidence="0.999393">daf@uiuc.edu</email>
<abstract confidence="0.9991425">discuss Sense Discrimination (ISD), and apply a method based on spectral clustering, using multimodal features from the image and text of the embedding web page. We evaluate our method on a new data set of annotated web images, retrieved with ambiguous query terms. Experiments investigate different levels of sense granularity, as well as the impact of text and image features, and global versus local text features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C O Alm</author>
<author>N Loeff</author>
<author>D Forsyth</author>
</authors>
<title>Challenges for annotating images for sense disambiguation. ACL workshop on Frontiers in Linguistically Annotated Corpora.</title>
<date>2006</date>
<marker>Alm, Loeff, Forsyth, 2006</marker>
<rawString>C. O. Alm, N. Loeff, and D. Forsyth. 2006. Challenges for annotating images for sense disambiguation. ACL workshop on Frontiers in Linguistically Annotated Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Barnard</author>
<author>D Forsyth</author>
</authors>
<title>Learning the semantics of words and pictures.</title>
<date>2001</date>
<journal>ICCV,</journal>
<pages>408--415</pages>
<contexts>
<context position="25363" citStr="Barnard and Forsyth, 2001" startWordPosition="4139" endWordPosition="4142">Lin and Pantel, 2002a, b) show that clustering methods are helpful in this area. While ISD has received less attention, image categorization has been approached previously by adding text features. For example, (Frankel, Swain, and Athitsos, 1996)’s WebSeer system attempted to mutually distinguish photos, handdrawn, and computer-drawn images, using a combination of HTML markup, web page text, and image information. (Yanai and Barnard, 2005) found that adding text features could benefit identifying relevant web images. Using text-annotated images (i.e. images annotated with relevant keywords), (Barnard and Forsyth, 2001) clustered them exploring a semantic hierarchy; similarly (Barnard, Duygulu, and Forsyth, 2002) conducted art clustering, and (Barnard and Johnson, 2005) used textannotated images to improve WSD. The latter paper obtained best results when combining text and image features, but contrary to our findings, image features performed better in isolation than just text. They did use a larger set of image features and segmentation, however, we suspect that differences can rather be attributed to corpus type. In fact, (Yanai, Shirahatti, and Barnard, 2005) noted that human evaluators rated images obtai</context>
</contexts>
<marker>Barnard, Forsyth, 2001</marker>
<rawString>K. Barnard and D. Forsyth. 2001. Learning the semantics of words and pictures. ICCV, 408–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Barnard</author>
<author>P Duygulu</author>
<author>D Forsyth</author>
</authors>
<title>Modeling the statistics of image features and associated text.</title>
<date>2002</date>
<publisher>SPIE.</publisher>
<contexts>
<context position="25457" citStr="Barnard, Duygulu, and Forsyth, 2002" startWordPosition="4151" endWordPosition="4155">e ISD has received less attention, image categorization has been approached previously by adding text features. For example, (Frankel, Swain, and Athitsos, 1996)’s WebSeer system attempted to mutually distinguish photos, handdrawn, and computer-drawn images, using a combination of HTML markup, web page text, and image information. (Yanai and Barnard, 2005) found that adding text features could benefit identifying relevant web images. Using text-annotated images (i.e. images annotated with relevant keywords), (Barnard and Forsyth, 2001) clustered them exploring a semantic hierarchy; similarly (Barnard, Duygulu, and Forsyth, 2002) conducted art clustering, and (Barnard and Johnson, 2005) used textannotated images to improve WSD. The latter paper obtained best results when combining text and image features, but contrary to our findings, image features performed better in isolation than just text. They did use a larger set of image features and segmentation, however, we suspect that differences can rather be attributed to corpus type. In fact, (Yanai, Shirahatti, and Barnard, 2005) noted that human evaluators rated images obtained via a keyword retrieval method higher compared to image-based retrieval methods, which the</context>
</contexts>
<marker>Barnard, Duygulu, Forsyth, 2002</marker>
<rawString>K. Barnard, P. Duygulu, and D. Forsyth. 2002. Modeling the statistics of image features and associated text. SPIE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Barnard</author>
<author>M Johnson</author>
</authors>
<title>Word sense disambiguation with pictures.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>167</volume>
<pages>13--30</pages>
<contexts>
<context position="25516" citStr="Barnard and Johnson, 2005" startWordPosition="4161" endWordPosition="4164">roached previously by adding text features. For example, (Frankel, Swain, and Athitsos, 1996)’s WebSeer system attempted to mutually distinguish photos, handdrawn, and computer-drawn images, using a combination of HTML markup, web page text, and image information. (Yanai and Barnard, 2005) found that adding text features could benefit identifying relevant web images. Using text-annotated images (i.e. images annotated with relevant keywords), (Barnard and Forsyth, 2001) clustered them exploring a semantic hierarchy; similarly (Barnard, Duygulu, and Forsyth, 2002) conducted art clustering, and (Barnard and Johnson, 2005) used textannotated images to improve WSD. The latter paper obtained best results when combining text and image features, but contrary to our findings, image features performed better in isolation than just text. They did use a larger set of image features and segmentation, however, we suspect that differences can rather be attributed to corpus type. In fact, (Yanai, Shirahatti, and Barnard, 2005) noted that human evaluators rated images obtained via a keyword retrieval method higher compared to image-based retrieval methods, which they relate to the importance of semantics for what humans reg</context>
</contexts>
<marker>Barnard, Johnson, 2005</marker>
<rawString>K. Barnard and M. Johnson. 2005. Word sense disambiguation with pictures. Artificial Intelligence, 167, 13–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cai</author>
</authors>
<title>Hierarchical clustering of WWW image search results using visual, textual and link information.</title>
<date>2004</date>
<journal>ACMMultimedia,</journal>
<pages>952--959</pages>
<marker>Cai, 2004</marker>
<rawString>D. Cai et al. 2004. Hierarchical clustering of WWW image search results using visual, textual and link information. ACMMultimedia, 952-959.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Chapelle</author>
<author>P Haffner</author>
<author>V Vapnik</author>
</authors>
<title>Support vector machines for histogram-based image classification.</title>
<date>1999</date>
<journal>IEEE Neural Networks,</journal>
<volume>10</volume>
<issue>5</issue>
<pages>1055--1064</pages>
<contexts>
<context position="13304" citStr="Chapelle, Haffner, and Vapnik, 1999" startWordPosition="2127" endWordPosition="2131">nces of the query keyword (e.g. squash); (d) tokens of the target image’s alt attribute; (e) tokens of the title tag; (f) some meta tokens.3 Tf-idf was applied to a weighted average of the BOWs. Webpage design is flexible, and some inconsistencies and a certain degree of noise remained in the text features. Image features Given the large variability in the retrieved image set for a given query, it is difficult to model images in an unsupervised fashion. Simple features have been shown to provide performance rivaling that of more elaborate models in object recognition (Csurka et al, 2004) and (Chapelle, Haffner, and Vapnik, 1999), and the following image bags of features were considered: Bags of keypoints: In order to obtain a compact representation of the textures of an image, patches are extracted automatically around interesting regions or keypoints in each image. The keypoint detection algorithm (Kadir and Brady, 2001) uses a saliency measure based on entropy to select regions. After extraction, keypoints were represented by a histogram of gradient magnitude of the pixel values in the region (SIFT) (Lowe, 2004). These descriptors were clustered using a Gaussian Mixture with Pz� 300 components, and the resulting g</context>
<context position="27900" citStr="Chapelle, Haffner, and Vapnik, 1999" startWordPosition="4541" endWordPosition="4545">strated text, word sense is an entirely secondary phenomenon, whereas our data set was collected as to emphasize possible ambiguities associated with word sense. Our results suggest that a surprisingly degree of the meaning of an illustrated object is exposed on the surface. This work is an initial attempt at addressing the ISD problem. Future work will involve learning the algorithm’s parameters without supervision, and develop a semantically meaningful image taxonomy. In particular, we intend to explore the notion of iconographic senses; surprisingly good results on image classification by (Chapelle, Haffner, and Vapnik, 1999) using image features suggest that iconography plays an important role in the semantics of images. An important aspect is to enhance our understanding of the interplay between text and image features for this purpose. Also, it remains an unsolved problem how to enumerate iconographic senses, and use them in manual annotation and classification. Experimental work with humans performing similar tasks may provide increased insight into this issue, and can also be used to validate clustering performance. 7 Acknowledgements We are grateful to Roxana Girju and Richard Sproat for helpful feedback, a</context>
</contexts>
<marker>Chapelle, Haffner, Vapnik, 1999</marker>
<rawString>O. Chapelle and P. Haffner and V. Vapnik. 1999. Support vector machines for histogram-based image classification. IEEE Neural Networks, 10(5), 1055–1064.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Csurka</author>
</authors>
<title>Visual categorization with bags of keypoints.</title>
<date>2004</date>
<booktitle>ECCV Int. Workshop on Stat. Learning in Computer Vision.</booktitle>
<marker>Csurka, 2004</marker>
<rawString>G. Csurka et al. 2004. Visual categorization with bags of keypoints. ECCV Int. Workshop on Stat. Learning in Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Frankel</author>
<author>M Swain</author>
<author>V Athitsos</author>
</authors>
<title>WebSeer: an image search engine for the World Wide Web.</title>
<date>1996</date>
<journal>Univ. of Chicago, Computer Science, Technical</journal>
<volume>report</volume>
<pages>96--14</pages>
<contexts>
<context position="24982" citStr="Frankel, Swain, and Athitsos, 1996" startWordPosition="4084" endWordPosition="4088"> 40 clusters a few were also dominated by RELATED senses or PEOPLE. No cluster was dominated by an UNRELATED label, which makes sense since semantic linkage should be absent between unrelated items. 5 Comparison to previous work Space does not allow a complete review of the WSD literature. (Yarowsky, 1995) demonstrated that semi-supervised WSD could be successful. (Sch¨utze, 1998) and (Lin and Pantel, 2002a, b) show that clustering methods are helpful in this area. While ISD has received less attention, image categorization has been approached previously by adding text features. For example, (Frankel, Swain, and Athitsos, 1996)’s WebSeer system attempted to mutually distinguish photos, handdrawn, and computer-drawn images, using a combination of HTML markup, web page text, and image information. (Yanai and Barnard, 2005) found that adding text features could benefit identifying relevant web images. Using text-annotated images (i.e. images annotated with relevant keywords), (Barnard and Forsyth, 2001) clustered them exploring a semantic hierarchy; similarly (Barnard, Duygulu, and Forsyth, 2002) conducted art clustering, and (Barnard and Johnson, 2005) used textannotated images to improve WSD. The latter paper obtain</context>
</contexts>
<marker>Frankel, Swain, Athitsos, 1996</marker>
<rawString>C. Frankel, M. Swain, and V. Athitsos. 1996. WebSeer: an image search engine for the World Wide Web. Univ. of Chicago, Computer Science, Technical report #96-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fowlkes</author>
<author>S Belongie</author>
<author>F Chung</author>
<author>J Malik</author>
</authors>
<title>Spectral grouping using the Nystr¨om method.</title>
<date>2004</date>
<journal>IEEE PAMI,</journal>
<pages>26--2</pages>
<contexts>
<context position="14764" citStr="Fowlkes et al, 2004" startWordPosition="2367" endWordPosition="2370">d. Color histograms: Due to its similarity to how humans perceive color, HSV (hue, saturation, brightness) color space was used to bin pixel color values for each image. Eight bins were used per channel, obtaining an 83 dimensional vector. 3.2 Measuring similarity between images For the BOWs text representation, we use the common measure of cosine similarity (cs) of two tfidf vectors (Jurafsky and Martin, 2000). The cosine similarity measure is also appropriate for keypoint representation as it is also an unordered bag. There are several measures for histogram comparison (i.e. L1, x2). As in (Fowlkes et al, 2004) we use the x2 distance measure between histograms hi and hj. (hi(k) − hj(k))2 (1) hi(k) + hj(k) 3.3 Spectral Clustering Spectral clustering is a powerful way to separate non-convex groups of data. Spectral methods for clustering are a family of algorithms that work by first constructing a pairwise-affinity matrix from the data, computing an eigendecomposition of the data, embedding the data into this lowdimensional manifold, and finally applying traditional clustering techniques (i.e. k-means) to it. Consider a graph with a set of n vertices each one representing an image document, and the ed</context>
</contexts>
<marker>Fowlkes, Belongie, Chung, Malik, 2004</marker>
<rawString>C. Fowlkes, S. Belongie, F. Chung, and J. Malik. 2004. Spectral grouping using the Nystr¨om method. IEEE PAMI, 26(2),214-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fuji</author>
<author>T Ishikawa</author>
</authors>
<title>Toward the automatic compilation of multimedia encyclopedias: associating images with term descriptions on the web.</title>
<date>2005</date>
<journal>IEEE WI,</journal>
<pages>536--542</pages>
<contexts>
<context position="26862" citStr="Fuji and Ishikawa, 2005" startWordPosition="4380" endWordPosition="4383">earch results. While their work does not focus explicitly on sense and does not provide indepth discussion of visual sense phenomena, these do appear in, for example, figs. 7 and 9 of their paper. An interesting aspect of their work is the use of page layout segmentation to associate text with images in web documents. Unfortunately, the au553 thors only provide an illustrative query example, and no numerical evaluation, making any comparison difficult. (Wang et al, 2004) use similar features with the goal to improve image retrieval through similarity propagation, querying specific web sites. (Fuji and Ishikawa, 2005) deal with image ambiguity for establishing an online multimedia encyclopedia, but their method does not integrate image features, and appears to depend on previous encyclopedic background knowledge, limited to a domain set. 6 Conclusion It is remarkable how high purity is, considering that we are using relatively simple image and text representation. In most corpora used to date for research on illustrated text, word sense is an entirely secondary phenomenon, whereas our data set was collected as to emphasize possible ambiguities associated with word sense. Our results suggest that a surprisi</context>
</contexts>
<marker>Fuji, Ishikawa, 2005</marker>
<rawString>A. Fuji and T. Ishikawa. 2005. Toward the automatic compilation of multimedia encyclopedias: associating images with term descriptions on the web. IEEE WI, 536-542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J Martin</author>
</authors>
<title>Speech and Language Processing,</title>
<date>2000</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="14558" citStr="Jurafsky and Martin, 2000" startWordPosition="2331" endWordPosition="2334">istogram of codebook entries) was used as lookup table to assign each keypoint to a codebook entry. 3Adding to META content, keywords was an attribute, but is irregular. Embedded BODY pairs are rare; thus not used. Color histograms: Due to its similarity to how humans perceive color, HSV (hue, saturation, brightness) color space was used to bin pixel color values for each image. Eight bins were used per channel, obtaining an 83 dimensional vector. 3.2 Measuring similarity between images For the BOWs text representation, we use the common measure of cosine similarity (cs) of two tfidf vectors (Jurafsky and Martin, 2000). The cosine similarity measure is also appropriate for keypoint representation as it is also an unordered bag. There are several measures for histogram comparison (i.e. L1, x2). As in (Fowlkes et al, 2004) we use the x2 distance measure between histograms hi and hj. (hi(k) − hj(k))2 (1) hi(k) + hj(k) 3.3 Spectral Clustering Spectral clustering is a powerful way to separate non-convex groups of data. Spectral methods for clustering are a family of algorithms that work by first constructing a pairwise-affinity matrix from the data, computing an eigendecomposition of the data, embedding the data</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>D. Jurafsky and J. Martin 2000. Speech and Language Processing, Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kadir</author>
<author>M Brady</author>
</authors>
<title>Scale, saliency and image description.</title>
<date>2001</date>
<journal>Int. Journal of Computer Vision,</journal>
<volume>45</volume>
<pages>2--83</pages>
<contexts>
<context position="13604" citStr="Kadir and Brady, 2001" startWordPosition="2174" endWordPosition="2177">e features Given the large variability in the retrieved image set for a given query, it is difficult to model images in an unsupervised fashion. Simple features have been shown to provide performance rivaling that of more elaborate models in object recognition (Csurka et al, 2004) and (Chapelle, Haffner, and Vapnik, 1999), and the following image bags of features were considered: Bags of keypoints: In order to obtain a compact representation of the textures of an image, patches are extracted automatically around interesting regions or keypoints in each image. The keypoint detection algorithm (Kadir and Brady, 2001) uses a saliency measure based on entropy to select regions. After extraction, keypoints were represented by a histogram of gradient magnitude of the pixel values in the region (SIFT) (Lowe, 2004). These descriptors were clustered using a Gaussian Mixture with Pz� 300 components, and the resulting global patch codebook (i.e. histogram of codebook entries) was used as lookup table to assign each keypoint to a codebook entry. 3Adding to META content, keywords was an attribute, but is irregular. Embedded BODY pairs are rare; thus not used. Color histograms: Due to its similarity to how humans per</context>
</contexts>
<marker>Kadir, Brady, 2001</marker>
<rawString>T. Kadir and M. Brady. 2001. Scale, saliency and image description. Int. Journal of Computer Vision, 45 (2):83– 105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Concept discovery from text.</title>
<date>2002</date>
<journal>COLING,</journal>
<pages>577--583</pages>
<contexts>
<context position="24757" citStr="Lin and Pantel, 2002" startWordPosition="4052" endWordPosition="4055">uery term, and for the four meta senses, without a clear trend. clusters (Sch¨utze, 1998), but may be due to the spectral clustering. Inspection showed that 6 clusters were dominated by core senses, whereas with 40 clusters a few were also dominated by RELATED senses or PEOPLE. No cluster was dominated by an UNRELATED label, which makes sense since semantic linkage should be absent between unrelated items. 5 Comparison to previous work Space does not allow a complete review of the WSD literature. (Yarowsky, 1995) demonstrated that semi-supervised WSD could be successful. (Sch¨utze, 1998) and (Lin and Pantel, 2002a, b) show that clustering methods are helpful in this area. While ISD has received less attention, image categorization has been approached previously by adding text features. For example, (Frankel, Swain, and Athitsos, 1996)’s WebSeer system attempted to mutually distinguish photos, handdrawn, and computer-drawn images, using a combination of HTML markup, web page text, and image information. (Yanai and Barnard, 2005) found that adding text features could benefit identifying relevant web images. Using text-annotated images (i.e. images annotated with relevant keywords), (Barnard and Forsyth,</context>
</contexts>
<marker>Lin, Pantel, 2002</marker>
<rawString>D. Lin and P. Pantel. 2002a. Concept discovery from text. COLING, 577–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lowe</author>
</authors>
<title>Distinctive image features from scaleinvariant keypoints.</title>
<date>2004</date>
<journal>Int. Journal of Computer Vision,</journal>
<volume>60</volume>
<issue>2</issue>
<pages>91--110</pages>
<contexts>
<context position="13800" citStr="Lowe, 2004" startWordPosition="2209" endWordPosition="2210">g that of more elaborate models in object recognition (Csurka et al, 2004) and (Chapelle, Haffner, and Vapnik, 1999), and the following image bags of features were considered: Bags of keypoints: In order to obtain a compact representation of the textures of an image, patches are extracted automatically around interesting regions or keypoints in each image. The keypoint detection algorithm (Kadir and Brady, 2001) uses a saliency measure based on entropy to select regions. After extraction, keypoints were represented by a histogram of gradient magnitude of the pixel values in the region (SIFT) (Lowe, 2004). These descriptors were clustered using a Gaussian Mixture with Pz� 300 components, and the resulting global patch codebook (i.e. histogram of codebook entries) was used as lookup table to assign each keypoint to a codebook entry. 3Adding to META content, keywords was an attribute, but is irregular. Embedded BODY pairs are rare; thus not used. Color histograms: Due to its similarity to how humans perceive color, HSV (hue, saturation, brightness) color space was used to bin pixel color values for each image. Eight bins were used per channel, obtaining an 83 dimensional vector. 3.2 Measuring si</context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>D. Lowe. 2004. Distinctive image features from scaleinvariant keypoints. Int. Journal of Computer Vision, 60(2), 91–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ng</author>
<author>M Jordan</author>
<author>Y Weiss</author>
</authors>
<title>On spectral clustering: analysis and an algorithm.</title>
<date>2002</date>
<journal>NIPS</journal>
<volume>14</volume>
<marker>Ng, Jordan, Weiss, 2002</marker>
<rawString>A. Ng, M. Jordan, and Y. Weiss. 2002. On spectral clustering: analysis and an algorithm. NIPS 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<pages>613--619</pages>
<publisher>KDD,</publisher>
<marker>Pantel, Lin, 2002</marker>
<rawString>P. Pantel and D. Lin. 2002b. Discovering word senses from text. KDD, 613–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schuetze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Schuetze, 1998</marker>
<rawString>H. Schuetze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shi</author>
<author>J Malik</author>
</authors>
<title>Normalized cuts and image segmentation.</title>
<date>2000</date>
<journal>IEEE PAMI,</journal>
<volume>22</volume>
<issue>8</issue>
<marker>Shi, Malik, 2000</marker>
<rawString>J. Shi and J. Malik. 2000. Normalized cuts and image segmentation. IEEE PAMI, 22(8):888–905.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nakano</author>
</authors>
<title>A new competitive learning approach based on an equidistortion principle for designing optimal vector quantizers.</title>
<date>1994</date>
<journal>Neural Networks,</journal>
<volume>7</volume>
<issue>8</issue>
<contexts>
<context position="16237" citStr="Nakano, 1994" startWordPosition="2623" endWordPosition="2624">re scaling parameters for text, keypoints, and color features. It has been shown that the use of multiple eigenvectors of W is a valid space onto which the data can be embedded (Ng, Jordan, Weiss, 2002). In this space noise is reduced while the most significant affinities are preserved. After this, any traditional clustering algorithm can be applied in this new space to get the final clusters. Note that this is a nonlinear mapping of the original space. In particular, we employ a variant of k-means, which includes a selective step that is quasi-optimal in a Vector Quantization sense (Ueda and Nakano, 1994). It has the added advantage of being more 1 5 1 2 χ2 i,j = 5 1 2 2 E k=1 550 robust to initialization than traditional k-means. The algorithm follows, 1. For given documents, compute the affinity matrix W as defined in equation 2. 2. Let D be a diagonal matrix whose (i, i)-th element is the sum of W’s i-th row, and define L = D−1/2WD−1/2. 3. Find the k largest eigenvectors V of L. 4. Define E as V , with normalized rows. 5. Perform clustering on the columns of E, which represent the embedding of each image into the new space, using a selective step as in (Ueda and Nakano, 1994). Why Spectral </context>
</contexts>
<marker>Nakano, 1994</marker>
<rawString>N. Ueda. and R. Nakano. 1994. A new competitive learning approach based on an equidistortion principle for designing optimal vector quantizers. Neural Networks, 7(8):1211–1227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X-J Wang</author>
</authors>
<title>Multi-model similarity propagation and its application for image retrieval.</title>
<date>2004</date>
<pages>944--951</pages>
<marker>Wang, 2004</marker>
<rawString>X.-J. Wang et al. 2004. Multi-model similarity propagation and its application for image retrieval. MM,944–951.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yanai</author>
<author>K Barnard</author>
</authors>
<title>Probabilistic web image gathering.</title>
<date>2005</date>
<journal>SIGMM,</journal>
<pages>57--64</pages>
<contexts>
<context position="4319" citStr="Yanai and Barnard, 2005" startWordPosition="678" endWordPosition="681">in Conference Poster Sessions, pages 547–554, Sydney, July 2006. c�2006 Association for Computational Linguistics (a) squash flower (b) tennis? (c) hook (d) food (e) bow (f) speaker Figure 1: Example RELATED images for (a) vegetable and (b) sports senses for SQUASH, and for (c-d) fish and (e-f) musical instrument for BASS. Related senses are associated with the semantic field of a core sense, but the core sense is visually absent or undeterminable. Figure 2: Which fish or instruments are BASS? Image sense annotation is more vague and subjective than in text. vored corpus for WSD. As noted by (Yanai and Barnard, 2005), whereas current image retrieval engines include many irrelevant images, a data set of web images gives a more real-world point of departure for image recognition. Outline Section 2 discusses the corpus data and image annotation. Section 3 presents the feature set and the clustering model. Subsequently, section 4 introduces the evaluation used, and discusses experimental work and results. In section 5, this work is positioned with respect to previous work. We conclude with an outline of plans for future work in section 6. 2 Data and annotation Yahoo!’s image query API was used to obtain a cor</context>
<context position="25180" citStr="Yanai and Barnard, 2005" startWordPosition="4115" endWordPosition="4118">son to previous work Space does not allow a complete review of the WSD literature. (Yarowsky, 1995) demonstrated that semi-supervised WSD could be successful. (Sch¨utze, 1998) and (Lin and Pantel, 2002a, b) show that clustering methods are helpful in this area. While ISD has received less attention, image categorization has been approached previously by adding text features. For example, (Frankel, Swain, and Athitsos, 1996)’s WebSeer system attempted to mutually distinguish photos, handdrawn, and computer-drawn images, using a combination of HTML markup, web page text, and image information. (Yanai and Barnard, 2005) found that adding text features could benefit identifying relevant web images. Using text-annotated images (i.e. images annotated with relevant keywords), (Barnard and Forsyth, 2001) clustered them exploring a semantic hierarchy; similarly (Barnard, Duygulu, and Forsyth, 2002) conducted art clustering, and (Barnard and Johnson, 2005) used textannotated images to improve WSD. The latter paper obtained best results when combining text and image features, but contrary to our findings, image features performed better in isolation than just text. They did use a larger set of image features and seg</context>
</contexts>
<marker>Yanai, Barnard, 2005</marker>
<rawString>K. Yanai and K. Barnard. 2005. Probabilistic web image gathering. SIGMM, 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yanai</author>
<author>N V Shirahatti</author>
<author>K Barnard</author>
</authors>
<title>Evaluation strategies for image understanding and retrieval.</title>
<date>2005</date>
<journal>SIGMM,</journal>
<pages>217--226</pages>
<contexts>
<context position="25915" citStr="Yanai, Shirahatti, and Barnard, 2005" startWordPosition="4226" endWordPosition="4230">d images (i.e. images annotated with relevant keywords), (Barnard and Forsyth, 2001) clustered them exploring a semantic hierarchy; similarly (Barnard, Duygulu, and Forsyth, 2002) conducted art clustering, and (Barnard and Johnson, 2005) used textannotated images to improve WSD. The latter paper obtained best results when combining text and image features, but contrary to our findings, image features performed better in isolation than just text. They did use a larger set of image features and segmentation, however, we suspect that differences can rather be attributed to corpus type. In fact, (Yanai, Shirahatti, and Barnard, 2005) noted that human evaluators rated images obtained via a keyword retrieval method higher compared to image-based retrieval methods, which they relate to the importance of semantics for what humans regard as matching, and because pictorial semantics is hard to detect. (Cai et al, 2004) use similar methods to rank visual search results. While their work does not focus explicitly on sense and does not provide indepth discussion of visual sense phenomena, these do appear in, for example, figs. 7 and 9 of their paper. An interesting aspect of their work is the use of page layout segmentation to as</context>
</contexts>
<marker>Yanai, Shirahatti, Barnard, 2005</marker>
<rawString>K. Yanai, N. V. Shirahatti, and K. Barnard. 2005. Evaluation strategies for image understanding and retrieval. SIGMM, 217-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<pages>189--196</pages>
<publisher>ACL,</publisher>
<contexts>
<context position="3007" citStr="Yarowsky, 1995" startWordPosition="464" endWordPosition="465">we acknowledge the importance of iconography for visual meaning. Also, an image often depicts a related meaning. E.g. a picture retrieved for SQUASH may depict a squash bug (i.e. an insect on a leaf of a squash plant) instead of a squash vegetable, whereas this does not really apply in WSD, where each instance concerns the ambiguous term itself. Therefore, it makes sense to consider the division between core sense, related sense, and unrelated sense in ISD, and, as an additional complication, their boundaries are often blurred. Most importantly, whereas the one-sense-per-discourse assumption (Yarowsky, 1995) also applies to discriminating images, there is no guarantee of a local collocational or co-occurrence context around the target image. Design or aesthetics may instead determine image placement. Thus, considering local text around the image may not be as helpful as local context is for standard WSD. In fact, the query term may even not occur in the text body. On the other hand, one can assume that an image spotlights the web page topic and that it highlights important document information. Also, images mostly depict concrete senses. Lastly, ISD from web data is complicated by web pages being</context>
<context position="5085" citStr="Yarowsky, 1995" startWordPosition="805" endWordPosition="806"> recognition. Outline Section 2 discusses the corpus data and image annotation. Section 3 presents the feature set and the clustering model. Subsequently, section 4 introduces the evaluation used, and discusses experimental work and results. In section 5, this work is positioned with respect to previous work. We conclude with an outline of plans for future work in section 6. 2 Data and annotation Yahoo!’s image query API was used to obtain a corpus of pairs of semantically ambiguous images, in thumbnail and true size, and their corresponding web sites for three ambiguous keywords inspired by (Yarowsky, 1995): BASS, CRANE, and SQUASH. We apply query augmentation (cf. Table 1), and exact duplicates were filtered out by identical image URLs, but cases occurred where both thumbnail and true-size image were included. Also, some images shared the same webpage or came from the same site. Generally, the latter gives important information about shared discourse topic, however the images do not necessarily depict the same sense (e.g. a CRANE bird vs. a meadow), and image features can separate them into different clusters. Annotation overview The images were annotated with one of several labels by one of th</context>
<context position="24655" citStr="Yarowsky, 1995" startWordPosition="4040" endWordPosition="4041">inct initializations), and recorded median and range of global purity for all six senses of the query term, and for the four meta senses, without a clear trend. clusters (Sch¨utze, 1998), but may be due to the spectral clustering. Inspection showed that 6 clusters were dominated by core senses, whereas with 40 clusters a few were also dominated by RELATED senses or PEOPLE. No cluster was dominated by an UNRELATED label, which makes sense since semantic linkage should be absent between unrelated items. 5 Comparison to previous work Space does not allow a complete review of the WSD literature. (Yarowsky, 1995) demonstrated that semi-supervised WSD could be successful. (Sch¨utze, 1998) and (Lin and Pantel, 2002a, b) show that clustering methods are helpful in this area. While ISD has received less attention, image categorization has been approached previously by adding text features. For example, (Frankel, Swain, and Athitsos, 1996)’s WebSeer system attempted to mutually distinguish photos, handdrawn, and computer-drawn images, using a combination of HTML markup, web page text, and image information. (Yanai and Barnard, 2005) found that adding text features could benefit identifying relevant web ima</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. ACL, 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>