<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002238">
<title confidence="0.997977">
Language-independent Probabilistic Answer Ranking
for Question Answering
</title>
<author confidence="0.997907">
Jeongwoo Ko, Teruko Mitamura, Eric Nyberg
</author>
<affiliation confidence="0.992214666666667">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.995346">
{jko, teruko, ehn}@cs.cmu.edu
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999918117647059">
This paper presents a language-independent
probabilistic answer ranking framework for
question answering. The framework esti-
mates the probability of an individual an-
swer candidate given the degree of answer
relevance and the amount of supporting evi-
dence provided in the set of answer candi-
dates for the question. Our approach was
evaluated by comparing the candidate an-
swer sets generated by Chinese and Japanese
answer extractors with the re-ranked answer
sets produced by the answer ranking frame-
work. Empirical results from testing on NT-
CIR factoid questions show a 40% perfor-
mance improvement in Chinese answer se-
lection and a 45% improvement in Japanese
answer selection.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999665625">
Question answering (QA) systems aim at find-
ing precise answers to natural language questions
from large document collections. Typical QA sys-
tems (Prager et al., 2000; Clarke et al., 2001;
Harabagiu et al., 2000) adopt a pipeline architec-
ture that incorporates four major steps: (1) question
analysis, (2) document retrieval, (3) answer extrac-
tion and (4) answer selection. Question analysis is
a process which analyzes a question and produces a
list of keywords. Document retrieval is a step that
searches for relevant documents or passages. An-
swer extraction extracts a list of answer candidates
from the retrieved documents. Answer selection is a
process which pinpoints correct answer(s) from the
extracted candidate answers.
Since the first three steps in the QA pipeline may
produce erroneous outputs, the final answer selec-
tion step often entails identifying correct answer(s)
amongst many incorrect ones. For example, given
the question “Which Chinese city has the largest
number of foreign financial companies?”, the an-
swer extraction component produces a ranked list of
five answer candidates: Beijing (AP880603-0268)1,
Hong Kong (WSJ920110-0013), Shanghai (FBIS3-
58), Taiwan (FT942-2016) and Shanghai (FBIS3-
45320). Due to imprecision in answer extraction,
an incorrect answer (“Beijing”) can be ranked in
the first position, and the correct answer (“Shang-
hai”) was extracted from two different documents
and ranked in the third and the fifth positions. In or-
der to rank “Shanghai” in the top position, we have
to address two interesting challenges:
</bodyText>
<listItem confidence="0.66064825">
• Answer Similarity. How do we exploit simi-
larity among answer candidates? For example,
when the candidates list contains redundant an-
swers (e.g., “Shanghai” as above) or several an-
</listItem>
<bodyText confidence="0.80482425">
swers which represent a single instance (e.g.
“U.S.A.” and “the United States”), how much
should we boost the rank of the redundant an-
swers?
</bodyText>
<listItem confidence="0.93792825">
• Answer Relevance. How do we identify
relevant answer(s) amongst irrelevant ones?
This task may involve searching for evi-
dence of a relationship between the answer
</listItem>
<footnote confidence="0.967312">
1Answer candidates are shown with the identifier of the
TREC document where they were found.
</footnote>
<page confidence="0.914403">
784
</page>
<note confidence="0.926905">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 784–791,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999278">
and the answer type or a question key-
word. For example, we might wish to query
a knowledge base to determine if “Shang-
hai” is a city (IS-A(Shanghai, city)),
or to determine if Shanghai is in China
(IS-IN(Shanghai, China)).
The first challenge is to exploit redundancy in the
set of answer candidates. As answer candidates are
extracted from different documents, they may con-
tain identical, similar or complementary text snip-
pets. For example, “U.S.” can appear as “United
States” or “USA” in different documents. It is im-
portant to detect redundant information and boost
answer confidence, especially for list questions that
require a set of unique answers. One approach is
to perform answer clustering (Nyberg et al., 2002;
Jijkoun et al., 2006). However, the use of cluster-
ing raises additional questions: how to calculate the
score of the clustered answers, and how to select the
cluster label.
To address the second question, several answer
selection approaches have used external knowledge
resources such as WordNet, CYC and gazetteers for
answer validation or answer reranking. Answer can-
didates are either removed or discounted if they are
not of the expected answer type (Xu et al., 2002;
Moldovan et al., 2003; Chu-Carroll et al., 2003;
Echihabi et al., 2004). The Web also has been used
for answer reranking by exploiting search engine re-
sults produced by queries containing the answer can-
didate and question keywords (Magnini et al., 2002).
This approach has been used in various languages
for answer validation. Wikipedia’s structured in-
formation was used for Spanish answer type check-
ing (Buscaldi and Rosso, 2006).
Although many QA systems have incorporated in-
dividual features and/or resources for answer selec-
tion in a single language, there has been little re-
search on a generalized probabilistic framework that
supports answer ranking in multiple languages using
any answer relevance and answer similarity features
that are appropriate for the language in question.
In this paper, we describe a probabilistic answer
ranking framework for multiple languages. The
framework uses logistic regression to estimate the
probability that an answer candidate is correct given
multiple answer relevance features and answer sim-
ilarity features. An existing framework which was
originally developed for English (Ko et al., 2007)
was extended for Chinese and Japanese answer
ranking by incorporating language-specific features.
Empirical results on NTCIR Chinese and Japanese
factoid questions show that the framework signifi-
cantly improved answer selection performance; Chi-
nese performance improved by 40% over the base-
line, and Japanese performance improved by 45%
over the baseline.
The remainder of this paper is organized as fol-
lows: Section 2 contains an overview of the answer
ranking task. Section 3 summarizes the answer rank-
ing framework. In Section 4, we explain how we
extended the framework by incorporating language-
specific features. Section 5 describes the experimen-
tal methodology and results. Finally, Section 6 con-
cludes with suggestions for future research.
</bodyText>
<sectionHeader confidence="0.936126" genericHeader="introduction">
2 Answer Ranking Task
</sectionHeader>
<bodyText confidence="0.994337956521739">
The relevance of an answer to a question can be es-
timated by the probability P(correct(AZ) JAZ, Q),
where Q is a question and AZ is an answer can-
didate. To exploit answer similarity, we estimate
the probability P(correct(AZ) JAZ,Aj), where Aj
is similar to AZ. Since both probabilities influence
overall answer ranking performance, it is important
to combine them in a unified framework and es-
timate the probability of an answer candidate as:
P(correct(AZ)1Q, Ai, ..., An).
The estimated probability is used to rank answer
candidates and select final answers from the list. For
factoid questions, the top answer is selected as a fi-
nal answer to the question. In addition, we can use
the estimated probability to classify incorrect an-
swers: if the probability of an answer candidate is
lower than 0.5, it is considered to be a wrong answer
and is filtered out of the answer list. This is useful
in deciding whether or not a valid answer to a ques-
tion exists in a given corpus (Voorhees, 2002). The
estimated probability can also be used in conjunc-
tion with a cutoff threshold when selecting multiple
answers to list questions.
</bodyText>
<sectionHeader confidence="0.979024" genericHeader="method">
3 Answer Ranking Framework
</sectionHeader>
<bodyText confidence="0.999218">
This section summarizes our answer ranking frame-
work, originally developed for English answers (Ko
</bodyText>
<page confidence="0.937416">
785
</page>
<equation confidence="0.948674555555556">
P(correct(Ai)|Q, A1,..., An)
� P(correct(Ai)|rel1(Ai), ..., relK1(Ai), sim1(Ai), ..., simK2(Ai))
exp(α0 + K1E Qkrelk(Ai) + K2E Aksimk(Ai))
k=1 k=1
1 + exp(α0 + K1E Qkrelk(Ai) + K2E Aksimk(Ai))
k=1 k=1
N
where, simk(Ai) = simk(Ai, Aj).
j=1(j�=i)
</equation>
<figureCaption confidence="0.99913">
Figure 1: Estimating correctness of an answer candidate given a question and a set of answer candidates
</figureCaption>
<bodyText confidence="0.999980260869565">
et al., 2007). The model uses logistic regression
to estimate the probability of an answer candidate
(Figure 1). Each relk(Ai) is a feature function used
to produce an answer relevance score for an an-
swer candidate Ai. Each simk(Ai, Aj) is a similar-
ity function used to calculate an answer similarity
between Ai and Aj. K1 and K2 are the number of
answer relevance and answer similarity features, re-
spectively. N is the number of answer candidates.
To incorporate multiple similarity features, each
simk(Ai) is obtained from an individual similarity
metric, simk(Ai, Aj). For example, if Levenshtein
distance is used as one similarity metric, simk(Ai)
is calculated by summing N-1 Levenshtein distances
between one answer candidate and all other candi-
dates.
The parameters α, Q, A were estimated from train-
ing data by maximizing the log likelihood. We used
the Quasi-Newton algorithm (Minka, 2003) for pa-
rameter estimation.
Multiple features were used to generate answer
relevance scores and answer similarity scores; these
are discussed below.
</bodyText>
<subsectionHeader confidence="0.999202">
3.1 Answer Relevance Features
</subsectionHeader>
<bodyText confidence="0.9978765">
Answer relevance features can be classified into
knowledge-based features or data-driven features.
</bodyText>
<sectionHeader confidence="0.966594" genericHeader="method">
1) Knowledge-based features
</sectionHeader>
<bodyText confidence="0.99987048">
Gazetteers: Gazetteers provide geographic infor-
mation, which allows us to identify strings as in-
stances of countries, their cities, continents, capitals,
etc. For answer ranking, three gazetteer resources
were used: the Tipster Gazetteer, the CIA World
Factbook and information about the US states pro-
vided by 50states.com. These resources were used
to assign an answer relevance score between -1 and
1 to each candidate. For example, given the question
“Which city in China has the largest number of for-
eign financial companies?”, the candidate “Shang-
hai” receives a score of 0.5 because it is a city in the
gazetteers. But “Taiwan” receives a score of -1.0 be-
cause it is not a city in the gazetteers. A score of 0
means the gazetteers did not contribute to the answer
selection process for that candidate.
Ontology: Ontologies such as WordNet contain
information about relationships between words and
general meaning types (synsets, semantic categories,
etc.). WordNet was used to identify answer rele-
vance in a manner analogous to the use of gazetteers.
For example, given the question “Who wrote the
book ’Song of Solomon’?”, the candidate “Mark
Twain” receives a score of 0.5 because its hyper-
nyms include “writer”.
</bodyText>
<sectionHeader confidence="0.949391" genericHeader="method">
2) Data-driven features
</sectionHeader>
<bodyText confidence="0.9992595">
Wikipedia: Wikipedia was used to generate an an-
swer relevance score. If there is a Wikipedia docu-
ment whose title matches an answer candidate, the
document is analyzed to obtain the term frequency
(tf) and the inverse term frequency (idf) of the can-
didate, from which a tf.idf score is calculated. When
there is no matched document, each question key-
word is also processed as a back-off strategy, and the
answer relevance score is calculated by summing the
tf.idf scores obtained from individual keywords.
Google: Following Magnini et al. (2002), a query
consisting of an answer candidate and question key-
</bodyText>
<page confidence="0.989608">
786
</page>
<bodyText confidence="0.999979">
words was sent to the Google search engine. Then
the top 10 text snippets returned by Google were
analyzed to generate an answer relevance score by
computing the minimum number of words between
a keyword and the answer candidate.
</bodyText>
<subsectionHeader confidence="0.99967">
3.2 Answer Similarity Features
</subsectionHeader>
<bodyText confidence="0.9999425">
Answer similarity is calculated using multiple string
distance metrics and a list of synonyms.
String Distance Metrics: String distance metrics
such as Levenshtein, Jaro-Winkler, and Cosine sim-
ilarity were used to calculate the similarity between
two English answer candidates.
Synonyms: Synonyms can be used as another
metric to calculate answer similarity. If one answer
is synonym of another answer, the score is 1. Other-
wise the score is 0. To get a list of synonyms, three
knowledge bases were used: WordNet, Wikipedia
and the CIA World Factbook. In addition, manually
generated rules were used to obtain synonyms for
different types of answer candidates. For example,
“April 12 1914” and “12th Apr. 1914” are converted
into “1914-04-12” and treated as synonyms.
</bodyText>
<sectionHeader confidence="0.99959" genericHeader="method">
4 Extensions for Multiple Languages
</sectionHeader>
<bodyText confidence="0.99992">
We extended the framework for Chinese and
Japanese QA. This section details how we incor-
porated language-specific resources into the frame-
work. As logistic regression is based on a proba-
bilistic framework, the model does not need to be
changed to support other languages. We only re-
trained the model for individual languages. To sup-
port Chinese and Japanese QA, we incorporated new
features for individual languages.
</bodyText>
<subsectionHeader confidence="0.994883">
4.1 Answer Relevance Features
</subsectionHeader>
<bodyText confidence="0.999917166666667">
We replaced the English gazetteers and WordNet
with language-specific resources for Japanese and
Chinese. As Wikipedia and the Web support mul-
tiple languages, the same algorithm was used in
searching language-specific corpora for the two lan-
guages.
</bodyText>
<sectionHeader confidence="0.983788" genericHeader="method">
1) Knowledge-based features
</sectionHeader>
<bodyText confidence="0.99537025">
The knowledge-based features involve searching for
facts in a knowledge base such as gazetteers and
WordNet. We utilized comparable resources for
Chinese and Japanese. Using language-specific re-
</bodyText>
<table confidence="0.9712924">
#Articles
Language Nov. 2005 Aug. 2006
English 1,811,554 3,583,699
Japanese 201,703 446,122
Chinese 69,936 197,447
</table>
<tableCaption confidence="0.963496">
Table 1: Articles in Wikipedia for different lan-
guages
</tableCaption>
<bodyText confidence="0.98008416">
sources, the same algorithms were applied to gener-
ate an answer relevance score between -1 and 1.
Gazetteers: There are few available gazetteers
for Chinese and Japanese. Therefore, we extracted
location data from language-specific resources. For
Japanese, we extracted Japanese location informa-
tion from Yahoo2, which contains many location
names in Japan and the relationships among them.
For Chinese, we extracted location names from the
Web. In addition, we translated country names pro-
vided by the CIA World Factbook and the Tipster
gazetteers into Chinese and Japanese names. As
there is more than one translation, top 3 translations
were used.
Ontology: For Chinese, we used HowNet (Dong,
2000) which is a Chinese version of WordNet.
It contains 65,000 Chinese concepts and 75,000
corresponding English equivalents. For Japanese,
we used semantic classes provided by Gengo
GoiTaikei3. Gengo GoiTaikei is a Japanese lexicon
containing 300,000 Japanese words with their asso-
ciated 3,000 semantic classes. The semantic infor-
mation provided by HowNet and Gengo GoiTaikei
was used to assign an answer relevance score be-
tween -1 and 1.
</bodyText>
<sectionHeader confidence="0.975698" genericHeader="method">
2) Data-driven features
</sectionHeader>
<bodyText confidence="0.9848516">
Wikipedia: As Wikipedia supports more than 200
language editions, the approach used in English can
be used for different languages without any modifi-
cation. Table 1 shows the number of text articles in
three different languages. Wikipedia’s current cov-
erage in Japanese and Chinese does not match its
coverage in English, but coverage in these languages
continues to improve.
To supplement the small corpus of Chi-
nese documents available, we used Baidu
</bodyText>
<footnote confidence="0.999295">
2http://map.yahoo.co.jp/
3http://www.kecl.ntt.co.jp/mtg/resources/GoiTaikei
</footnote>
<page confidence="0.992174">
787
</page>
<bodyText confidence="0.99967735">
(http://baike.baidu.com), which is similar to
Wikipedia but contains more articles written in
Chinese. We first search for Chinese Wikipedia.
When there is no matching document in Wikipedia,
each answer candidate is sent to Baidu and the
retrieved document is analyzed in the same way to
analyze Wikipedia documents.
The idf score was calculated using word statis-
tics from Japanese Yomiuri newspaper corpus and
the NTCIR Chinese corpus.
Google: The same algorithm was applied to ana-
lyze Japanese and Chinese snippets returned from
Google. But we restricted the language to Chi-
nese or Japanese so that Google returned only Chi-
nese or Japanese documents. To calculate the dis-
tance between an answer candidate and question
keywords, segmentation was done with linguistic
tools. For Japanese, Chasen4 was used. For Chinese
segmentation, a maximum-entropy based parser was
used (Wang et al., 2006).
</bodyText>
<sectionHeader confidence="0.884996" genericHeader="method">
3) Manual Filtering
</sectionHeader>
<bodyText confidence="0.999970833333333">
Other than the features mentioned above, we man-
ually created many rules for numeric and temporal
questions to filter out invalid answers. For example,
when the question is looking for a year as an answer,
an answer candidate which contains only the month
receives a score of -1. Otherwise, the score is 0.
</bodyText>
<subsectionHeader confidence="0.992657">
4.2 Answer Similarity Features
</subsectionHeader>
<bodyText confidence="0.999325533333333">
The same features used for English were applied
to calculate the similarity of Chinese/Japanese an-
swer candidates. To identify synonyms, Wikipedia
were used for both Chinese and Japanese. EIJIRO
dictionary was used to obtain Japanese synonyms.
EIJIRO is a English-Japanese dictionary contain-
ing 1,576,138 words and provides synonyms for
Japanese words.
As there are several different ways to represent
temporal and numeric expressions (Nyberg et al.,
2002; Greenwood, 2006), language-specific conver-
sion rules were applied to convert them into a canon-
ical format; for example, a rule to convert Japanese
Kanji characters to Arabic numbers is shown in Fig-
ure 2.
</bodyText>
<footnote confidence="0.913638">
4http://chasen.aist-nara.ac.jp/hiki/ChaSen
</footnote>
<table confidence="0.928128285714286">
Original answer string Normalized answer string
TIM Rl 3E+11 R1
3,000jWF1 3E+11 p9
-LA ® H 1993-07-04
1993 * 7 A 4 p 1993-07-04
195?0 _ 0.25
501 50%
</table>
<figureCaption confidence="0.993468">
Figure 2: Example of normalized answer strings
</figureCaption>
<sectionHeader confidence="0.998402" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999848333333333">
This section describes the experiments to evaluate
the extended answer ranking framework for Chinese
and Japanese QA.
</bodyText>
<subsectionHeader confidence="0.923433">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999993">
We used Chinese and Japanese questions provided
by the NTCIR (NII Test Collection for IR Sys-
tems), which focuses on evaluating cross-lingual
and monolingual QA tasks for Chinese, Japanese
and English. For Chinese, a total of 550 fac-
toid questions from the NTCIR5-6 QA evaluations
served as the dataset. Among them, 200 questions
were used to train the Chinese answer extractor and
350 questions were used to evaluate our answer
ranking framework. For Japanese, 700 questions
from the NTCIR5-6 QA evaluations served as the
dataset. Among them, 300 questions were used to
train the Japanese answer extractor and 400 ques-
tions were used to evaluate our framework.
Both the Chinese and Japanese answer extractors
use maximum-entropy to extract answer candidates
based on multiple features such as named entity, de-
pendency structures and some language-dependent
features.
Performance of the answer ranking framework
was measured by average answer accuracy: the
number of correct top answers divided by the num-
ber of questions where at least one correct answer
exists in the candidate list provided by an extrac-
tor. Mean Reciprocal Rank (MRR5) was also used
to calculate the average reciprocal rank of the first
correct answer in the top 5 answers.
The baseline for average answer accuracy was
calculated using the answer candidate likelihood
scores provided by each individual extractor; the
</bodyText>
<page confidence="0.9877">
788
</page>
<figure confidence="0.998712357142857">
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
T
Ba
Fr
</figure>
<figureCaption confidence="0.965974">
Figure 3: Performance of the answer ranking framework for Chinese and Japanese answer selection (TOP1:
average accuracy of top answer, TOP3: average accuracy of top 3 answers, MRR5: average of mean recip-
rocal rank of top 5 answers)
</figureCaption>
<bodyText confidence="0.969794833333333">
answer with the best extractor score was chosen,
and no validation or similarity processing was per-
formed.
3-fold cross-validation was performed, and we
used a version of Wikipedia downloaded in Aug
2006.
</bodyText>
<subsectionHeader confidence="0.985283">
5.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999419045454546">
We first analyzed the average accuracy of top 1, top3
and top 5 answers. Figure 3 compares the average
accuracy using the baseline and the answer selec-
tion framework. As can be seen, the answer rank-
ing framework significantly improved performance
on both Chinese and Japanese answer selection. As
for the average top answer accuracy, there were 40%
improvement over the baseline (Chinese) and 45%
improvement over the baseline (Japanese).
We also analyzed the degree to which the average
accuracy was affected by answer similarity and rel-
evance features. Table 2 compares the average top
answer accuracy using the baseline, the answer rel-
evance features, the answer similarity features and
all feature combinations. Both the similarity and the
relevance features significantly improved answer se-
lection performance compared to the baseline, and
combining both sets of features together produced
the best performance.
We further analyzed the utility of individual rele-
vance features (Figure 4). For both languages, filter-
ing was useful in ruling out wrong answers. The im-
</bodyText>
<table confidence="0.990977">
Baseline Rel Sim All
Chinese 0.442 0.482 0.597 0.619
Japanese 0.367 0.463 0.502 0.532
</table>
<tableCaption confidence="0.991588">
Table 2: Average top answer accuracy of individ-
</tableCaption>
<bodyText confidence="0.969826333333333">
ual features (Rel: merging relevance features, Sim:
merging similarity features, ALL: merging all fea-
tures).
pact of the ontology was more positive for Japanese;
we assume that this is because the Chinese ontol-
ogy (HowNet) contains much less information over-
all than the Japanese ontology (Gengo GoiTaikei).
The comparative impact of Wikipedia was similar.
For Chinese, there were many fewer Wikipedia doc-
uments available. Even though we used Baidu as a
supplemental resource for Chinese, this did not im-
prove answer selection performance. On the other
hand, the use of Wikipedia was very helpful for
Japanese, improving performance by 26% over the
baseline. This shows that the quality of answer
relevance estimation is significantly affected by re-
source coverage.
When comparing the data-driven features with the
knowledge-based features, the data-driven features
(such as Wikipedia and Google) tended to increase
performance more than the knowledge-based fea-
tures (such as gazetteers and WordNet).
Table 3 shows the effect of individual similar-
ity features on Chinese and Japanese answer selec-
</bodyText>
<page confidence="0.99285">
789
</page>
<figure confidence="0.998972857142857">
0.55
0.50
0.45
0.40
0.35
0.30
Baselin
</figure>
<figureCaption confidence="0.9785252">
Figure 4: Average top answer accuracy of individ-
ual answer relevance features.(FIL: filtering, ONT,
ontology, GAZ: gazetteers, GL: Google, WIKI:
Wikipedia, ALL: combination of all relevance fea-
tures)
</figureCaption>
<table confidence="0.998709857142857">
Chinese Japanese
0.3 0.5 0.3 0.5
Cosine 0.597 0.597 0.488 0.488
Jaro-Winkler 0.544 0.518 0.410 0.415
Levenshtein 0.558 0.544 0.434 0.449
Synonyms 0.527 0.527 0.493 0.493
All 0.588 0.580 0.502 0.488
</table>
<tableCaption confidence="0.681963">
Table 3: Average accuracy using individual similar-
ity features under different thresholds: 0.3 and 0.5
(“All”: combination of all similarity metrics)
</tableCaption>
<bodyText confidence="0.99967475">
tion. As some string similarity features (e.g., Lev-
enshtein distance) produce a number between 0 and
1 (where 1 means two strings are identical and 0
means they are different), similarity scores less than
a threshold can be ignored. We used two thresh-
olds: 0.3 and 0.5. In our experiments, using 0.3
as a threshold produced better results in Chinese.
In Japanese, 0,5 was a better threshold for individ-
ual features. Among three different string similar-
ity features (Levenshtein, Jaro-Winkler and Cosine
similarity), cosine similarity tended to perform bet-
ter than the others.
When comparing synonym features with string
similarity features, synonyms performed better than
string similarity in Japanese, but not in Chinese. We
had many more synonyms available for Japanese
</bodyText>
<table confidence="0.774592666666667">
Data-driven features All features
Chinese 0.606 0.619
Japanese 0.517 0.532
</table>
<tableCaption confidence="0.619596">
Table 4: Average top answer accuracy when using
data-driven features v.s. when using all features.
</tableCaption>
<bodyText confidence="0.999614666666667">
and they helped the system to better exploit answer
redundancy.
We also analyzed answer selection performance
when combining all four similarity features (“All”
in Table 3). Combining all similarity features im-
proved the performance in Japanese, but hurt the
performance in Chinese, because adding a small set
of synonyms to the string metrics worsened the per-
formance of logistic regression.
</bodyText>
<subsectionHeader confidence="0.999439">
5.3 Utility of data-driven features
</subsectionHeader>
<bodyText confidence="0.999989052631579">
In our experiments we used data-driven fea-
tures as well as knowledge-based features. As
knowledge-based features need manual effort to ac-
cess language-specific resources for individual lan-
guages, we conducted an additional experiment only
with data-driven features in order to see how much
performance gain is available without the manual
work. As Google, Wikipedia and string similarity
metrics can be used without any additional manual
effort when extended to other languages, we used
these three features and compared the performance.
Table 4 shows the performance when using data-
driven features v.s. all features. It can be seen that
data-driven features alone achieved significant im-
provement over the baseline. This indicates that the
framework can easily be extended to any language
where appropriate data resources are available, even
if knowledge-based features and resources for the
language are still under development.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999956625">
In this paper, we presented a generalized answer se-
lection framework which was applied to Chinese and
Japanese question answering. An empirical evalu-
ation using NTCIR test questions showed that the
framework significantly improves baseline answer
selection performance. For Chinese, the perfor-
mance improved by 40% over the baseline. For
Japanese, the performance improved by 45% over
</bodyText>
<page confidence="0.983064">
790
</page>
<bodyText confidence="0.999980423076923">
the baseline. This shows that our probabilistic
framework can be easily extended for multiple lan-
guages by reusing data-driven features (with new
corpora) and adding language-specific resources
(ontologies, gazetteers) for knowledge-based fea-
tures.
In our previous work, we evaluated the perfor-
mance of the framework for English QA using ques-
tions from past TREC evaluations (Ko et al., 2007).
The experimental results showed that the combina-
tion of all answer ranking features improved per-
formance by an average of 102% over the baseline.
The relevance features improved performance by an
average of 99% over the baseline, and the similar-
ity features improved performance by an average of
46% over the baseline. Our hypothesis is that answer
relevance features had a greater impact for English
QA because the quality and coverage of the data re-
sources available for English answer validation is
much higher than the quality and coverage of ex-
isting resources for Japanese and Chinese. In future
work, we will continue to evaluate the robustness of
the framework. It is also clear from our comparison
with English QA that more work can and should be
done in acquiring data resources for answer valida-
tion in Chinese and Japanese.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999551">
We would like to thank Hideki Shima, Mengqiu
Wang, Frank Lin, Justin Betteridge, Matthew
Bilotti, Andrew Schlaikjer and Luo Si for their valu-
able support. This work was supported in part
by ARDA/DTO AQUAINT program award number
NBCHC040164.
</bodyText>
<sectionHeader confidence="0.999247" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999876754385965">
D. Buscaldi and P. Rosso. 2006. Mining Knowledge
from Wikipedia for the Question Answering task. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation.
J. Chu-Carroll, J. Prager, C. Welty, K. Czuba, and D. Fer-
rucci. 2003. A Multi-Strategy and Multi-Source Ap-
proach to Question Answering. In Proceedings of Text
REtrieval Conference.
C. Clarke, G. Cormack, and T. Lynam. 2001. Exploiting
redundancy in question answering. In Proceedings of
SIGIR.
Zhendong Dong. 2000. Hownet:
http://www.keenage.com.
A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E. Melz,
and D. Ravichandran. 2004. How to select an answer
string? In T. Strzalkowski and S. Harabagiu, editors,
Advances in Textual Question Answering. Kluwer.
Mark A. Greenwood. 2006. Open-Domain Question An-
swering. Thesis.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunsecu, R. Girju, V. Rus, and
P. Morarescu. 2000. Falcon: Boosting knowledge for
answer engines. In Proceedings of TREC.
V. Jijkoun, J. van Rantwijk, D. Ahn, E. Tjong Kim Sang,
and M. de Rijke. 2006. The University of Amsterdam
at CLEF@QA 2006. In Working Notes CLEF.
J. Ko, L. Si, and E. Nyberg. 2007. A Probabilistic Frame-
work for Answer Selection in Question Answering. In
Proceedings ofNAACL/HLT.
B. Magnini, M. Negri, R. Pervete, and H. Tanev. 2002.
Comparing statistical and content-based techniques for
answer validation on the web. In Proceedings of the
VIII Convegno AI*IA.
T. Minka. 2003. A Comparison of Numerical Optimizers
for Logistic Regression. Unpublished draft.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maiorano.
2003. Cogex: A logic prover for question answering.
In Proceedings ofHLT-NAACL.
E. Nyberg, T. Mitamura, J. Carbonell, J. Callan,
K. Collins-Thompson, K. Czuba, M. Duggan,
L. Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita,
S. Murtagh, V. Pedro, and D. Svoboda. 2002. The
JAVELIN Question-Answering System at TREC 2002.
In Proceedings of Text REtrieval Conference.
J. Prager, E. Brown, A. Coden, and D. Radev. 2000.
Question answering by predictive annotation. In Pro-
ceedings of SIGIR.
E. Voorhees. 2002. Overview of the TREC 2002 ques-
tion answering track. In Proceedings of Text REtrieval
Conference.
M. Wang, K. Sagae, and T. Mitamura. 2006. A Fast, Ac-
curate Deterministic Parser for Chinese. In Proceed-
ings of COLING/ACL.
J. Xu, A. Licuanan, J. May, S. Miller, and R. Weischedel.
2002. TREC 2002 QA at BBN: Answer Selection and
Confidence Estimation. In Proceedings of Text RE-
trieval Conference.
</reference>
<page confidence="0.997958">
791
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.901529">
<title confidence="0.999302">Language-independent Probabilistic Answer Ranking for Question Answering</title>
<author confidence="0.99971">Jeongwoo Ko</author>
<author confidence="0.99971">Teruko Mitamura</author>
<author confidence="0.99971">Eric Nyberg</author>
<affiliation confidence="0.998827666666667">Language Technologies Institute School of Computer Science Carnegie Mellon University</affiliation>
<email confidence="0.930845">teruko,</email>
<abstract confidence="0.998503055555556">This paper presents a language-independent probabilistic answer ranking framework for question answering. The framework estimates the probability of an individual answer candidate given the degree of answer relevance and the amount of supporting evidence provided in the set of answer candidates for the question. Our approach was evaluated by comparing the candidate answer sets generated by Chinese and Japanese answer extractors with the re-ranked answer sets produced by the answer ranking framework. Empirical results from testing on NT- CIR factoid questions show a 40% performance improvement in Chinese answer selection and a 45% improvement in Japanese answer selection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Buscaldi</author>
<author>P Rosso</author>
</authors>
<title>Mining Knowledge from Wikipedia for the Question Answering task.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="4893" citStr="Buscaldi and Rosso, 2006" startWordPosition="751" endWordPosition="754">es such as WordNet, CYC and gazetteers for answer validation or answer reranking. Answer candidates are either removed or discounted if they are not of the expected answer type (Xu et al., 2002; Moldovan et al., 2003; Chu-Carroll et al., 2003; Echihabi et al., 2004). The Web also has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords (Magnini et al., 2002). This approach has been used in various languages for answer validation. Wikipedia’s structured information was used for Spanish answer type checking (Buscaldi and Rosso, 2006). Although many QA systems have incorporated individual features and/or resources for answer selection in a single language, there has been little research on a generalized probabilistic framework that supports answer ranking in multiple languages using any answer relevance and answer similarity features that are appropriate for the language in question. In this paper, we describe a probabilistic answer ranking framework for multiple languages. The framework uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer relevance features and ans</context>
</contexts>
<marker>Buscaldi, Rosso, 2006</marker>
<rawString>D. Buscaldi and P. Rosso. 2006. Mining Knowledge from Wikipedia for the Question Answering task. In Proceedings of the International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chu-Carroll</author>
<author>J Prager</author>
<author>C Welty</author>
<author>K Czuba</author>
<author>D Ferrucci</author>
</authors>
<title>A Multi-Strategy and Multi-Source Approach to Question Answering.</title>
<date>2003</date>
<booktitle>In Proceedings of Text REtrieval Conference.</booktitle>
<contexts>
<context position="4510" citStr="Chu-Carroll et al., 2003" startWordPosition="690" endWordPosition="693">that require a set of unique answers. One approach is to perform answer clustering (Nyberg et al., 2002; Jijkoun et al., 2006). However, the use of clustering raises additional questions: how to calculate the score of the clustered answers, and how to select the cluster label. To address the second question, several answer selection approaches have used external knowledge resources such as WordNet, CYC and gazetteers for answer validation or answer reranking. Answer candidates are either removed or discounted if they are not of the expected answer type (Xu et al., 2002; Moldovan et al., 2003; Chu-Carroll et al., 2003; Echihabi et al., 2004). The Web also has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords (Magnini et al., 2002). This approach has been used in various languages for answer validation. Wikipedia’s structured information was used for Spanish answer type checking (Buscaldi and Rosso, 2006). Although many QA systems have incorporated individual features and/or resources for answer selection in a single language, there has been little research on a generalized probabilistic framework that supports answer</context>
</contexts>
<marker>Chu-Carroll, Prager, Welty, Czuba, Ferrucci, 2003</marker>
<rawString>J. Chu-Carroll, J. Prager, C. Welty, K. Czuba, and D. Ferrucci. 2003. A Multi-Strategy and Multi-Source Approach to Question Answering. In Proceedings of Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Clarke</author>
<author>G Cormack</author>
<author>T Lynam</author>
</authors>
<title>Exploiting redundancy in question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1119" citStr="Clarke et al., 2001" startWordPosition="161" endWordPosition="164">ded in the set of answer candidates for the question. Our approach was evaluated by comparing the candidate answer sets generated by Chinese and Japanese answer extractors with the re-ranked answer sets produced by the answer ranking framework. Empirical results from testing on NTCIR factoid questions show a 40% performance improvement in Chinese answer selection and a 45% improvement in Japanese answer selection. 1 Introduction Question answering (QA) systems aim at finding precise answers to natural language questions from large document collections. Typical QA systems (Prager et al., 2000; Clarke et al., 2001; Harabagiu et al., 2000) adopt a pipeline architecture that incorporates four major steps: (1) question analysis, (2) document retrieval, (3) answer extraction and (4) answer selection. Question analysis is a process which analyzes a question and produces a list of keywords. Document retrieval is a step that searches for relevant documents or passages. Answer extraction extracts a list of answer candidates from the retrieved documents. Answer selection is a process which pinpoints correct answer(s) from the extracted candidate answers. Since the first three steps in the QA pipeline may produc</context>
</contexts>
<marker>Clarke, Cormack, Lynam, 2001</marker>
<rawString>C. Clarke, G. Cormack, and T. Lynam. 2001. Exploiting redundancy in question answering. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhendong Dong</author>
</authors>
<date>2000</date>
<note>Hownet: http://www.keenage.com.</note>
<contexts>
<context position="13885" citStr="Dong, 2000" startWordPosition="2162" endWordPosition="2163"> and 1. Gazetteers: There are few available gazetteers for Chinese and Japanese. Therefore, we extracted location data from language-specific resources. For Japanese, we extracted Japanese location information from Yahoo2, which contains many location names in Japan and the relationships among them. For Chinese, we extracted location names from the Web. In addition, we translated country names provided by the CIA World Factbook and the Tipster gazetteers into Chinese and Japanese names. As there is more than one translation, top 3 translations were used. Ontology: For Chinese, we used HowNet (Dong, 2000) which is a Chinese version of WordNet. It contains 65,000 Chinese concepts and 75,000 corresponding English equivalents. For Japanese, we used semantic classes provided by Gengo GoiTaikei3. Gengo GoiTaikei is a Japanese lexicon containing 300,000 Japanese words with their associated 3,000 semantic classes. The semantic information provided by HowNet and Gengo GoiTaikei was used to assign an answer relevance score between -1 and 1. 2) Data-driven features Wikipedia: As Wikipedia supports more than 200 language editions, the approach used in English can be used for different languages without a</context>
</contexts>
<marker>Dong, 2000</marker>
<rawString>Zhendong Dong. 2000. Hownet: http://www.keenage.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Echihabi</author>
<author>U Hermjakob</author>
<author>E Hovy</author>
<author>D Marcu</author>
<author>E Melz</author>
<author>D Ravichandran</author>
</authors>
<title>How to select an answer string? In</title>
<date>2004</date>
<booktitle>Advances in Textual Question Answering.</booktitle>
<editor>T. Strzalkowski and S. Harabagiu, editors,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="4534" citStr="Echihabi et al., 2004" startWordPosition="694" endWordPosition="697">ue answers. One approach is to perform answer clustering (Nyberg et al., 2002; Jijkoun et al., 2006). However, the use of clustering raises additional questions: how to calculate the score of the clustered answers, and how to select the cluster label. To address the second question, several answer selection approaches have used external knowledge resources such as WordNet, CYC and gazetteers for answer validation or answer reranking. Answer candidates are either removed or discounted if they are not of the expected answer type (Xu et al., 2002; Moldovan et al., 2003; Chu-Carroll et al., 2003; Echihabi et al., 2004). The Web also has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords (Magnini et al., 2002). This approach has been used in various languages for answer validation. Wikipedia’s structured information was used for Spanish answer type checking (Buscaldi and Rosso, 2006). Although many QA systems have incorporated individual features and/or resources for answer selection in a single language, there has been little research on a generalized probabilistic framework that supports answer ranking in multiple lan</context>
</contexts>
<marker>Echihabi, Hermjakob, Hovy, Marcu, Melz, Ravichandran, 2004</marker>
<rawString>A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E. Melz, and D. Ravichandran. 2004. How to select an answer string? In T. Strzalkowski and S. Harabagiu, editors, Advances in Textual Question Answering. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Greenwood</author>
</authors>
<date>2006</date>
<journal>Open-Domain Question Answering. Thesis.</journal>
<contexts>
<context position="16602" citStr="Greenwood, 2006" startWordPosition="2575" endWordPosition="2576">an answer, an answer candidate which contains only the month receives a score of -1. Otherwise, the score is 0. 4.2 Answer Similarity Features The same features used for English were applied to calculate the similarity of Chinese/Japanese answer candidates. To identify synonyms, Wikipedia were used for both Chinese and Japanese. EIJIRO dictionary was used to obtain Japanese synonyms. EIJIRO is a English-Japanese dictionary containing 1,576,138 words and provides synonyms for Japanese words. As there are several different ways to represent temporal and numeric expressions (Nyberg et al., 2002; Greenwood, 2006), language-specific conversion rules were applied to convert them into a canonical format; for example, a rule to convert Japanese Kanji characters to Arabic numbers is shown in Figure 2. 4http://chasen.aist-nara.ac.jp/hiki/ChaSen Original answer string Normalized answer string TIM Rl 3E+11 R1 3,000jWF1 3E+11 p9 -LA ® H 1993-07-04 1993 * 7 A 4 p 1993-07-04 195?0 _ 0.25 501 50% Figure 2: Example of normalized answer strings 5 Experiments This section describes the experiments to evaluate the extended answer ranking framework for Chinese and Japanese QA. 5.1 Experimental Setup We used Chinese an</context>
</contexts>
<marker>Greenwood, 2006</marker>
<rawString>Mark A. Greenwood. 2006. Open-Domain Question Answering. Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>M Pasca</author>
<author>R Mihalcea</author>
<author>M Surdeanu</author>
<author>R Bunsecu</author>
<author>R Girju</author>
<author>V Rus</author>
<author>P Morarescu</author>
</authors>
<title>Falcon: Boosting knowledge for answer engines.</title>
<date>2000</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="1144" citStr="Harabagiu et al., 2000" startWordPosition="165" endWordPosition="168">wer candidates for the question. Our approach was evaluated by comparing the candidate answer sets generated by Chinese and Japanese answer extractors with the re-ranked answer sets produced by the answer ranking framework. Empirical results from testing on NTCIR factoid questions show a 40% performance improvement in Chinese answer selection and a 45% improvement in Japanese answer selection. 1 Introduction Question answering (QA) systems aim at finding precise answers to natural language questions from large document collections. Typical QA systems (Prager et al., 2000; Clarke et al., 2001; Harabagiu et al., 2000) adopt a pipeline architecture that incorporates four major steps: (1) question analysis, (2) document retrieval, (3) answer extraction and (4) answer selection. Question analysis is a process which analyzes a question and produces a list of keywords. Document retrieval is a step that searches for relevant documents or passages. Answer extraction extracts a list of answer candidates from the retrieved documents. Answer selection is a process which pinpoints correct answer(s) from the extracted candidate answers. Since the first three steps in the QA pipeline may produce erroneous outputs, the </context>
</contexts>
<marker>Harabagiu, Moldovan, Pasca, Mihalcea, Surdeanu, Bunsecu, Girju, Rus, Morarescu, 2000</marker>
<rawString>S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. Surdeanu, R. Bunsecu, R. Girju, V. Rus, and P. Morarescu. 2000. Falcon: Boosting knowledge for answer engines. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jijkoun</author>
<author>J van Rantwijk</author>
<author>D Ahn</author>
<author>E Tjong Kim Sang</author>
<author>M de Rijke</author>
</authors>
<date>2006</date>
<journal>The University of Amsterdam at CLEF@QA</journal>
<booktitle>In Working Notes CLEF.</booktitle>
<marker>Jijkoun, van Rantwijk, Ahn, Sang, de Rijke, 2006</marker>
<rawString>V. Jijkoun, J. van Rantwijk, D. Ahn, E. Tjong Kim Sang, and M. de Rijke. 2006. The University of Amsterdam at CLEF@QA 2006. In Working Notes CLEF.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ko</author>
<author>L Si</author>
<author>E Nyberg</author>
</authors>
<title>A Probabilistic Framework for Answer Selection in Question Answering.</title>
<date>2007</date>
<booktitle>In Proceedings ofNAACL/HLT.</booktitle>
<contexts>
<context position="5600" citStr="Ko et al., 2007" startWordPosition="855" endWordPosition="858">r selection in a single language, there has been little research on a generalized probabilistic framework that supports answer ranking in multiple languages using any answer relevance and answer similarity features that are appropriate for the language in question. In this paper, we describe a probabilistic answer ranking framework for multiple languages. The framework uses logistic regression to estimate the probability that an answer candidate is correct given multiple answer relevance features and answer similarity features. An existing framework which was originally developed for English (Ko et al., 2007) was extended for Chinese and Japanese answer ranking by incorporating language-specific features. Empirical results on NTCIR Chinese and Japanese factoid questions show that the framework significantly improved answer selection performance; Chinese performance improved by 40% over the baseline, and Japanese performance improved by 45% over the baseline. The remainder of this paper is organized as follows: Section 2 contains an overview of the answer ranking task. Section 3 summarizes the answer ranking framework. In Section 4, we explain how we extended the framework by incorporating language</context>
<context position="25078" citStr="Ko et al., 2007" startWordPosition="3885" endWordPosition="3888">R test questions showed that the framework significantly improves baseline answer selection performance. For Chinese, the performance improved by 40% over the baseline. For Japanese, the performance improved by 45% over 790 the baseline. This shows that our probabilistic framework can be easily extended for multiple languages by reusing data-driven features (with new corpora) and adding language-specific resources (ontologies, gazetteers) for knowledge-based features. In our previous work, we evaluated the performance of the framework for English QA using questions from past TREC evaluations (Ko et al., 2007). The experimental results showed that the combination of all answer ranking features improved performance by an average of 102% over the baseline. The relevance features improved performance by an average of 99% over the baseline, and the similarity features improved performance by an average of 46% over the baseline. Our hypothesis is that answer relevance features had a greater impact for English QA because the quality and coverage of the data resources available for English answer validation is much higher than the quality and coverage of existing resources for Japanese and Chinese. In fut</context>
</contexts>
<marker>Ko, Si, Nyberg, 2007</marker>
<rawString>J. Ko, L. Si, and E. Nyberg. 2007. A Probabilistic Framework for Answer Selection in Question Answering. In Proceedings ofNAACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>M Negri</author>
<author>R Pervete</author>
<author>H Tanev</author>
</authors>
<title>Comparing statistical and content-based techniques for answer validation on the web.</title>
<date>2002</date>
<booktitle>In Proceedings of the VIII Convegno AI*IA.</booktitle>
<contexts>
<context position="4716" citStr="Magnini et al., 2002" startWordPosition="724" endWordPosition="727">score of the clustered answers, and how to select the cluster label. To address the second question, several answer selection approaches have used external knowledge resources such as WordNet, CYC and gazetteers for answer validation or answer reranking. Answer candidates are either removed or discounted if they are not of the expected answer type (Xu et al., 2002; Moldovan et al., 2003; Chu-Carroll et al., 2003; Echihabi et al., 2004). The Web also has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords (Magnini et al., 2002). This approach has been used in various languages for answer validation. Wikipedia’s structured information was used for Spanish answer type checking (Buscaldi and Rosso, 2006). Although many QA systems have incorporated individual features and/or resources for answer selection in a single language, there has been little research on a generalized probabilistic framework that supports answer ranking in multiple languages using any answer relevance and answer similarity features that are appropriate for the language in question. In this paper, we describe a probabilistic answer ranking framewor</context>
<context position="10967" citStr="Magnini et al. (2002)" startWordPosition="1711" endWordPosition="1714">core of 0.5 because its hypernyms include “writer”. 2) Data-driven features Wikipedia: Wikipedia was used to generate an answer relevance score. If there is a Wikipedia document whose title matches an answer candidate, the document is analyzed to obtain the term frequency (tf) and the inverse term frequency (idf) of the candidate, from which a tf.idf score is calculated. When there is no matched document, each question keyword is also processed as a back-off strategy, and the answer relevance score is calculated by summing the tf.idf scores obtained from individual keywords. Google: Following Magnini et al. (2002), a query consisting of an answer candidate and question key786 words was sent to the Google search engine. Then the top 10 text snippets returned by Google were analyzed to generate an answer relevance score by computing the minimum number of words between a keyword and the answer candidate. 3.2 Answer Similarity Features Answer similarity is calculated using multiple string distance metrics and a list of synonyms. String Distance Metrics: String distance metrics such as Levenshtein, Jaro-Winkler, and Cosine similarity were used to calculate the similarity between two English answer candidate</context>
</contexts>
<marker>Magnini, Negri, Pervete, Tanev, 2002</marker>
<rawString>B. Magnini, M. Negri, R. Pervete, and H. Tanev. 2002. Comparing statistical and content-based techniques for answer validation on the web. In Proceedings of the VIII Convegno AI*IA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Minka</author>
</authors>
<title>A Comparison of Numerical Optimizers for Logistic Regression. Unpublished draft.</title>
<date>2003</date>
<contexts>
<context position="8872" citStr="Minka, 2003" startWordPosition="1383" endWordPosition="1384">swer similarity between Ai and Aj. K1 and K2 are the number of answer relevance and answer similarity features, respectively. N is the number of answer candidates. To incorporate multiple similarity features, each simk(Ai) is obtained from an individual similarity metric, simk(Ai, Aj). For example, if Levenshtein distance is used as one similarity metric, simk(Ai) is calculated by summing N-1 Levenshtein distances between one answer candidate and all other candidates. The parameters α, Q, A were estimated from training data by maximizing the log likelihood. We used the Quasi-Newton algorithm (Minka, 2003) for parameter estimation. Multiple features were used to generate answer relevance scores and answer similarity scores; these are discussed below. 3.1 Answer Relevance Features Answer relevance features can be classified into knowledge-based features or data-driven features. 1) Knowledge-based features Gazetteers: Gazetteers provide geographic information, which allows us to identify strings as instances of countries, their cities, continents, capitals, etc. For answer ranking, three gazetteer resources were used: the Tipster Gazetteer, the CIA World Factbook and information about the US stat</context>
</contexts>
<marker>Minka, 2003</marker>
<rawString>T. Minka. 2003. A Comparison of Numerical Optimizers for Logistic Regression. Unpublished draft.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>D Clark</author>
<author>S Harabagiu</author>
<author>S Maiorano</author>
</authors>
<title>Cogex: A logic prover for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="4484" citStr="Moldovan et al., 2003" startWordPosition="686" endWordPosition="689">lly for list questions that require a set of unique answers. One approach is to perform answer clustering (Nyberg et al., 2002; Jijkoun et al., 2006). However, the use of clustering raises additional questions: how to calculate the score of the clustered answers, and how to select the cluster label. To address the second question, several answer selection approaches have used external knowledge resources such as WordNet, CYC and gazetteers for answer validation or answer reranking. Answer candidates are either removed or discounted if they are not of the expected answer type (Xu et al., 2002; Moldovan et al., 2003; Chu-Carroll et al., 2003; Echihabi et al., 2004). The Web also has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords (Magnini et al., 2002). This approach has been used in various languages for answer validation. Wikipedia’s structured information was used for Spanish answer type checking (Buscaldi and Rosso, 2006). Although many QA systems have incorporated individual features and/or resources for answer selection in a single language, there has been little research on a generalized probabilistic fram</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Maiorano, 2003</marker>
<rawString>D. Moldovan, D. Clark, S. Harabagiu, and S. Maiorano. 2003. Cogex: A logic prover for question answering. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Nyberg</author>
<author>T Mitamura</author>
<author>J Carbonell</author>
<author>J Callan</author>
<author>K Collins-Thompson</author>
<author>K Czuba</author>
<author>M Duggan</author>
<author>L Hiyakumoto</author>
<author>N Hu</author>
<author>Y Huang</author>
<author>J Ko</author>
<author>L Lita</author>
<author>S Murtagh</author>
<author>V Pedro</author>
<author>D Svoboda</author>
</authors>
<date>2002</date>
<booktitle>The JAVELIN Question-Answering System at TREC</booktitle>
<contexts>
<context position="3989" citStr="Nyberg et al., 2002" startWordPosition="606" endWordPosition="609">mine if “Shanghai” is a city (IS-A(Shanghai, city)), or to determine if Shanghai is in China (IS-IN(Shanghai, China)). The first challenge is to exploit redundancy in the set of answer candidates. As answer candidates are extracted from different documents, they may contain identical, similar or complementary text snippets. For example, “U.S.” can appear as “United States” or “USA” in different documents. It is important to detect redundant information and boost answer confidence, especially for list questions that require a set of unique answers. One approach is to perform answer clustering (Nyberg et al., 2002; Jijkoun et al., 2006). However, the use of clustering raises additional questions: how to calculate the score of the clustered answers, and how to select the cluster label. To address the second question, several answer selection approaches have used external knowledge resources such as WordNet, CYC and gazetteers for answer validation or answer reranking. Answer candidates are either removed or discounted if they are not of the expected answer type (Xu et al., 2002; Moldovan et al., 2003; Chu-Carroll et al., 2003; Echihabi et al., 2004). The Web also has been used for answer reranking by ex</context>
<context position="16584" citStr="Nyberg et al., 2002" startWordPosition="2571" endWordPosition="2574">ooking for a year as an answer, an answer candidate which contains only the month receives a score of -1. Otherwise, the score is 0. 4.2 Answer Similarity Features The same features used for English were applied to calculate the similarity of Chinese/Japanese answer candidates. To identify synonyms, Wikipedia were used for both Chinese and Japanese. EIJIRO dictionary was used to obtain Japanese synonyms. EIJIRO is a English-Japanese dictionary containing 1,576,138 words and provides synonyms for Japanese words. As there are several different ways to represent temporal and numeric expressions (Nyberg et al., 2002; Greenwood, 2006), language-specific conversion rules were applied to convert them into a canonical format; for example, a rule to convert Japanese Kanji characters to Arabic numbers is shown in Figure 2. 4http://chasen.aist-nara.ac.jp/hiki/ChaSen Original answer string Normalized answer string TIM Rl 3E+11 R1 3,000jWF1 3E+11 p9 -LA ® H 1993-07-04 1993 * 7 A 4 p 1993-07-04 195?0 _ 0.25 501 50% Figure 2: Example of normalized answer strings 5 Experiments This section describes the experiments to evaluate the extended answer ranking framework for Chinese and Japanese QA. 5.1 Experimental Setup </context>
</contexts>
<marker>Nyberg, Mitamura, Carbonell, Callan, Collins-Thompson, Czuba, Duggan, Hiyakumoto, Hu, Huang, Ko, Lita, Murtagh, Pedro, Svoboda, 2002</marker>
<rawString>E. Nyberg, T. Mitamura, J. Carbonell, J. Callan, K. Collins-Thompson, K. Czuba, M. Duggan, L. Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita, S. Murtagh, V. Pedro, and D. Svoboda. 2002. The JAVELIN Question-Answering System at TREC 2002. In Proceedings of Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>E Brown</author>
<author>A Coden</author>
<author>D Radev</author>
</authors>
<title>Question answering by predictive annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1098" citStr="Prager et al., 2000" startWordPosition="157" endWordPosition="160">orting evidence provided in the set of answer candidates for the question. Our approach was evaluated by comparing the candidate answer sets generated by Chinese and Japanese answer extractors with the re-ranked answer sets produced by the answer ranking framework. Empirical results from testing on NTCIR factoid questions show a 40% performance improvement in Chinese answer selection and a 45% improvement in Japanese answer selection. 1 Introduction Question answering (QA) systems aim at finding precise answers to natural language questions from large document collections. Typical QA systems (Prager et al., 2000; Clarke et al., 2001; Harabagiu et al., 2000) adopt a pipeline architecture that incorporates four major steps: (1) question analysis, (2) document retrieval, (3) answer extraction and (4) answer selection. Question analysis is a process which analyzes a question and produces a list of keywords. Document retrieval is a step that searches for relevant documents or passages. Answer extraction extracts a list of answer candidates from the retrieved documents. Answer selection is a process which pinpoints correct answer(s) from the extracted candidate answers. Since the first three steps in the Q</context>
</contexts>
<marker>Prager, Brown, Coden, Radev, 2000</marker>
<rawString>J. Prager, E. Brown, A. Coden, and D. Radev. 2000. Question answering by predictive annotation. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2002</date>
<booktitle>In Proceedings of Text REtrieval Conference.</booktitle>
<contexts>
<context position="7361" citStr="Voorhees, 2002" startWordPosition="1145" endWordPosition="1146"> framework and estimate the probability of an answer candidate as: P(correct(AZ)1Q, Ai, ..., An). The estimated probability is used to rank answer candidates and select final answers from the list. For factoid questions, the top answer is selected as a final answer to the question. In addition, we can use the estimated probability to classify incorrect answers: if the probability of an answer candidate is lower than 0.5, it is considered to be a wrong answer and is filtered out of the answer list. This is useful in deciding whether or not a valid answer to a question exists in a given corpus (Voorhees, 2002). The estimated probability can also be used in conjunction with a cutoff threshold when selecting multiple answers to list questions. 3 Answer Ranking Framework This section summarizes our answer ranking framework, originally developed for English answers (Ko 785 P(correct(Ai)|Q, A1,..., An) � P(correct(Ai)|rel1(Ai), ..., relK1(Ai), sim1(Ai), ..., simK2(Ai)) exp(α0 + K1E Qkrelk(Ai) + K2E Aksimk(Ai)) k=1 k=1 1 + exp(α0 + K1E Qkrelk(Ai) + K2E Aksimk(Ai)) k=1 k=1 N where, simk(Ai) = simk(Ai, Aj). j=1(j�=i) Figure 1: Estimating correctness of an answer candidate given a question and a set of answ</context>
</contexts>
<marker>Voorhees, 2002</marker>
<rawString>E. Voorhees. 2002. Overview of the TREC 2002 question answering track. In Proceedings of Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>K Sagae</author>
<author>T Mitamura</author>
</authors>
<title>A Fast, Accurate Deterministic Parser for Chinese.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL.</booktitle>
<contexts>
<context position="15770" citStr="Wang et al., 2006" startWordPosition="2444" endWordPosition="2447">e same way to analyze Wikipedia documents. The idf score was calculated using word statistics from Japanese Yomiuri newspaper corpus and the NTCIR Chinese corpus. Google: The same algorithm was applied to analyze Japanese and Chinese snippets returned from Google. But we restricted the language to Chinese or Japanese so that Google returned only Chinese or Japanese documents. To calculate the distance between an answer candidate and question keywords, segmentation was done with linguistic tools. For Japanese, Chasen4 was used. For Chinese segmentation, a maximum-entropy based parser was used (Wang et al., 2006). 3) Manual Filtering Other than the features mentioned above, we manually created many rules for numeric and temporal questions to filter out invalid answers. For example, when the question is looking for a year as an answer, an answer candidate which contains only the month receives a score of -1. Otherwise, the score is 0. 4.2 Answer Similarity Features The same features used for English were applied to calculate the similarity of Chinese/Japanese answer candidates. To identify synonyms, Wikipedia were used for both Chinese and Japanese. EIJIRO dictionary was used to obtain Japanese synonym</context>
</contexts>
<marker>Wang, Sagae, Mitamura, 2006</marker>
<rawString>M. Wang, K. Sagae, and T. Mitamura. 2006. A Fast, Accurate Deterministic Parser for Chinese. In Proceedings of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>A Licuanan</author>
<author>J May</author>
<author>S Miller</author>
<author>R Weischedel</author>
</authors>
<title>TREC</title>
<date>2002</date>
<booktitle>In Proceedings of Text REtrieval Conference.</booktitle>
<contexts>
<context position="4461" citStr="Xu et al., 2002" startWordPosition="682" endWordPosition="685">nfidence, especially for list questions that require a set of unique answers. One approach is to perform answer clustering (Nyberg et al., 2002; Jijkoun et al., 2006). However, the use of clustering raises additional questions: how to calculate the score of the clustered answers, and how to select the cluster label. To address the second question, several answer selection approaches have used external knowledge resources such as WordNet, CYC and gazetteers for answer validation or answer reranking. Answer candidates are either removed or discounted if they are not of the expected answer type (Xu et al., 2002; Moldovan et al., 2003; Chu-Carroll et al., 2003; Echihabi et al., 2004). The Web also has been used for answer reranking by exploiting search engine results produced by queries containing the answer candidate and question keywords (Magnini et al., 2002). This approach has been used in various languages for answer validation. Wikipedia’s structured information was used for Spanish answer type checking (Buscaldi and Rosso, 2006). Although many QA systems have incorporated individual features and/or resources for answer selection in a single language, there has been little research on a general</context>
</contexts>
<marker>Xu, Licuanan, May, Miller, Weischedel, 2002</marker>
<rawString>J. Xu, A. Licuanan, J. May, S. Miller, and R. Weischedel. 2002. TREC 2002 QA at BBN: Answer Selection and Confidence Estimation. In Proceedings of Text REtrieval Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>