<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001155">
<title confidence="0.9997745">
A Comparative Investigation of Morphological Language Modeling for the
Languages of the European Union
</title>
<author confidence="0.999159">
Thomas M¨uller, Hinrich Sch¨utze and Helmut Schmid
</author>
<affiliation confidence="0.9978855">
Institute for Natural Language Processing
University of Stuttgart, Germany
</affiliation>
<email confidence="0.995185">
{muellets,schmid}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.993793" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999485">
We investigate a language model that com-
bines morphological and shape features with
a Kneser-Ney model and test it in a large
crosslingual study of European languages.
Even though the model is generic and we use
the same architecture and features for all lan-
guages, the model achieves reductions in per-
plexity for all 21 languages represented in the
Europarl corpus, ranging from 3% to 11%. We
show that almost all of this perplexity reduc-
tion can be achieved by identifying suffixes by
frequency.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999816192982456">
Language models are fundamental to many natural
language processing applications. In the most com-
mon approach, language models estimate the proba-
bility of the next word based on one or more equiv-
alence classes that the history of preceding words is
a member of. The inherent productivity of natural
language poses a problem in this regard because the
history may be rare or unseen or have unusual prop-
erties that make assignment to a predictive equiva-
lence class difficult.
In many languages, morphology is a key source
of productivity that gives rise to rare and unseen
histories. For example, even if a model can learn
that words like “large”, “dangerous” and “serious”
are likely to occur after the relatively frequent his-
tory “potentially”, this knowledge cannot be trans-
ferred to the rare history “hypothetically” without
some generalization mechanism like morphological
analysis.
Our primary goal in this paper is not to de-
velop optimized language models for individual lan-
guages. Instead, we investigate whether a simple
generic language model that uses shape and mor-
phological features can be made to work well across
a large number of languages. We find that this is
the case: we achieve considerable perplexity reduc-
tions for all 21 languages in the Europarl corpus.
We see this as evidence that morphological language
modeling should be considered as a standard part of
any language model, even for languages like English
that are often not viewed as a good application of
morphological modeling due to their morphological
simplicity.
To understand which factors are important for
good performance of the morphological compo-
nent of a language model, we perform an exten-
sive crosslingual analysis of our experimental re-
sults. We look at three parameters of the morpho-
logical model we propose: the frequency threshold
0 that divides words subject to morphological clus-
tering from those that are not; the number of suffixes
used 0; and three different morphological segmen-
tation algorithms. We also investigate the differen-
tial effect of morphological language modeling on
different word shapes: alphabetical words, punctua-
tion, numbers and other shapes.
Some prior work has used morphological models
that require careful linguistic analysis and language-
dependent adaptation. In this paper we show that
simple frequency analysis performs only slightly
worse than more sophisticated morphological anal-
ysis. This potentially removes a hurdle to using
morphological models in cases where sufficient re-
sources to do the extra work required for sophisti-
cated morphological analysis are not available.
The motivation for using morphology in lan-
guage modeling is similar to distributional clustering
</bodyText>
<page confidence="0.62826">
386
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 386–395,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.989363633333333">
(Brown et al., 1992). In both cases, we form equiv-
alence classes of words with similar distributional
behavior. In a preliminary experiment, we find that
morphological equivalence classes reduce perplex-
ity as much as traditional distributional classes – a
surprising result we intend to investigate in future
work.
The main contributions of this paper are as fol-
lows. We present a language model design and a
set of morphological and shape features that achieve
reductions in perplexity for all 21 languages rep-
resented in the Europarl corpus, ranging from 3%
to 11%, compared to a Kneser-Ney model. We
show that identifying suffixes by frequency is suf-
ficient for getting almost all of this perplexity reduc-
tion. More sophisticated morphological segmenta-
tion methods do not further increase perplexity or
just slightly. Finally, we show that there is one pa-
rameter that must be tuned for good performance for
most languages: the frequency threshold 0 above
which a word is not subject to morphological gen-
eralization because it occurs frequently enough for
standard word n-gram language models to use it ef-
fectively for prediction.
The paper is organized as follows. In Section 2
we discuss related work. In Section 3 we describe
the morphological and shape features we use. Sec-
tion 4 introduces language model and experimental
setup. Section 5 discusses our results. Section 6
summarizes the contributions of this paper.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999964581818182">
Whittaker and Woodland (2000) apply language
modeling to morpheme sequences and investigate
data-driven segmentation methods. Creutz et al.
(2007) propose a similar method that improves
speech recognition for highly inflecting languages.
They use Morfessor (Creutz and Lagus, 2007) to
split words into morphemes. Both approaches are
essentially a simple form of a factored language
model (FLM) (Bilmes and Kirchhoff, 2003). In a
general FLM a number of different back-off paths
are combined by a back-off function to improve the
prediction after rare or unseen histories. Vergyri et
al. (2004) apply FLMs and morphological features
to Arabic speech recognition.
These papers and other prior work on using mor-
phology in language modeling have been language-
specific and have paid less attention to the ques-
tion as to how morphology can be useful across
languages and what generic methods are appropri-
ate for this goal. Previous work also has concen-
trated on traditional linguistic morphology whereas
we compare linguistically motivated morphologi-
cal segmentation with frequency-based segmenta-
tion and include shape features in our study.
Our initial plan for this paper was to use com-
plex language modeling frameworks that allow ex-
perimenters to include arbitrary features (including
morphological and shape features) in the model. In
particular, we looked at publicly available imple-
mentations of maximum entropy models (Rosen-
feld, 1996; Berger et al., 1996) and random forests
(Xu and Jelinek, 2004). However, we found that
these methods do not currently scale to running a
large set of experiments on a multi-gigabyte parallel
corpus of 21 languages. Similar considerations ap-
ply to other sophisticated language modeling tech-
niques like Pitman-Yor processes (Teh, 2006), re-
current neural networks (Mikolov et al., 2010) and
FLMs in their general, more powerful form. In ad-
dition, perplexity reductions of these complex mod-
els compared to simpler state-of-the-art models are
generally not large.
We therefore decided to conduct our study in the
framework of smoothed n-gram models, which cur-
rently are an order of magnitude faster and more
scalable. More specifically, we adopt a class-based
approach, where words are clustered based on mor-
phological and shape features. This approach has the
nice property that the number of features used to es-
timate the classes does not influence the time needed
to train the class language model, once the classes
have been found. This is an important consideration
in the context of the questions asked in this paper as
it allows us to use large numbers of features in our
experiments.
</bodyText>
<sectionHeader confidence="0.813814" genericHeader="method">
3 Modeling of morphology and shape
</sectionHeader>
<bodyText confidence="0.9999062">
Our basic approach is to define a number of morpho-
logical and shape features and then assign all words
with identical feature values to one class. For the
morphological features, we investigate three differ-
ent automatic suffix identification algorithms: Re-
</bodyText>
<page confidence="0.997067">
387
</page>
<bodyText confidence="0.95109075">
s, e, d, ed, n, g, ng, ing, y, t, es, r, a, l, on, er, ion,
ted, ly, tion, rs, al, o, ts, ns, le, i, ation, an, ers, m, nt,
ting, h, c, te, sed, ated, en, ty, ic, k, ent, st, ss, ons, se,
ity, ble, ne, ce, ess, ions, us, ry, re, ies, ve, p, ate, in,
tions, ia, red, able, is, ive, ness, lly, ring, ment, led,
ned, tes, as, ls, ding, ling, sing, ds, ded, ian, nce, ar,
ating, sm, ally, nts, de, nd, ism, or, ge, ist, ses, ning,
u, king, na, el
</bodyText>
<figureCaption confidence="0.947148">
Figure 1: The 100 most frequent English suffixes in Eu-
roparl, ordered by frequency
</figureCaption>
<bodyText confidence="0.999621125">
ports (Keshava and Pitler, 2006), Morfessor (Creutz
and Lagus, 2007) and Frequency, where Frequency
simply selects the most frequent word-final letter se-
quences as suffixes. The 100 most frequent suffixes
found by Frequency for English are given in Fig-
ure 1.
We use the φ most frequent suffixes for all three
algorithms, where φ is a parameter. The focus of our
work is to evaluate the utility of these algorithms for
language modeling; we do not directly evaluate the
quality of the suffixes.
A word is segmented by identifying the longest of
the φ suffixes that it ends with. Thus, each word has
one suffix feature if it ends with one of the φ suffixes
and none otherwise.
In addition to suffix features, we define features
that capture shape properties: capitalization, special
characters and word length. If a word in the test set
has a combination of feature values that does not oc-
cur in the training set, then it is assigned to the class
whose features are most similar. We described the
similarity measure and details of the shape features
in prior work (M¨uller and Sch¨utze, 2011). The shape
features are listed in Table 1.
</bodyText>
<sectionHeader confidence="0.997203" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9999246">
Experiments are performed using srilm (Stolcke,
2002), in particular the Kneser-Ney (KN) and
generic class model implementations. Estimation of
optimal interpolation parameters is based on (Bahl
et al., 1991).
</bodyText>
<subsectionHeader confidence="0.967183">
4.1 Baseline
</subsectionHeader>
<bodyText confidence="0.997797">
Our baseline is a modified KN model (Chen and
Goodman, 1999).
</bodyText>
<subsectionHeader confidence="0.995041">
4.2 Morphological class language model
</subsectionHeader>
<bodyText confidence="0.999324285714286">
We use a variation of the model proposed by Brown
et al. (1992) that we developed in prior work on En-
glish (M¨uller and Sch¨utze, 2011). This model is a
class-based language model that groups words into
classes and replaces the word transition probability
by a class transition probability and a word emission
probability:
</bodyText>
<equation confidence="0.9981245">
PC(wi|wi−1
i−N+1) =
P(g(wi)|g(wi−1
i−N+1)) &apos; P(wi|g(wi))
</equation>
<bodyText confidence="0.974286904761905">
where g(w) is the class of word w and we write
g(wi ... wj) for g(wi) ... g(wj).
Our approach targets rare and unseen histories.
We therefore exclude all frequent words from clus-
tering on the assumption that enough training data
is available for them. Thus, clustering of words is
restricted to those below a certain token frequency
threshold θ. As described above, we simply group
all words with identical feature values into one class.
Words with a training set frequency above θ are
added as singletons. The class transition probabil-
ity P(g(wi)|g(wi−1
i−N+1)) is estimated using Witten-
Bell smoothing.1
The word emission probability is defined as fol-
lows:
where c = g(w) is w’s class and N(w) is the fre-
quency of w in the training set. The class-dependent
out-of-vocabulary (OOV) rate 2(c) is estimated on
held-out data. Our final model PM interpolates PC
with a modified KN model:
</bodyText>
<equation confidence="0.998665333333333">
PM(wi|wi−N+1
i−1 ) =
λ(g(wi−1)) &apos; PC(wi|wi−N+1
i−1 )
+(1 − λ(g(wi−1))) &apos; PKN(wi|wi−N+1
i−1 ) (1)
</equation>
<bodyText confidence="0.9636524">
This model can be viewed as a generalization of
the simple interpolation αPC + (1 − α)PW used by
Brown et al. (1992) (where PW is a word n-gram
1Witten-Bell smoothing outperformed modified Kneser-Ney
(KN) and Good-Turing (GT).
</bodyText>
<equation confidence="0.998189857142857">
1 , N(w) &gt; θ
N(w)
E... N(w)
2(c) , N(w) = 0
P(w|c) = {
|c|−1, θ&gt;N(w)&gt;0
2(c)
</equation>
<page confidence="0.875018">
388
</page>
<construct confidence="0.652033714285714">
is capital(w)
is all capital(w)
capital character(w)
appears in lowercase(w)
special character(w)
digit(w)
is number(w)
first character of w is an uppercase letter
V c E w : c is an uppercase letter
I c E w : c is an uppercase letter
capital character(w) V w&apos; E ET
I c E w : c is not a letter or digit
I c E w : c is a digit
w E L([+ − E][0 − 9] (([.,][0 − 9])1[0 − 9])*)
</construct>
<tableCaption confidence="0.987547666666667">
Table 1: Shape features as defined by M¨uller and Sch¨utze (2011). ET is the vocabulary of the training corpus T, w&apos; is
obtained from w by changing all uppercase letters to lowercase and L(expr) is the language generated by the regular
expression expr.
</tableCaption>
<bodyText confidence="0.99991695">
model and PC a class n-gram model). For the set-
ting 0 = oc (clustering of all words), our model is
essentially a simple interpolation of a word n-gram
and a class n-gram model except that the interpola-
tion parameters are optimized for each class instead
of using the same interpolation parameter α for all
classes. We have found that 0 = oc is never optimal;
it is always beneficial to assign the most frequent
words to their own singleton classes.
Following Yuret and Bic¸ici (2009), we evaluate
models on the task of predicting the next word from
a vocabulary that consists of all words that occur
more than once in the training corpus and the un-
known word UNK. Performing this evaluation for
KN is straightforward: we map all words with fre-
quency one in the training set to UNK and then com-
pute PKN(UNK Ih) in testing.
In contrast, computing probability estimates for
PC is more complicated. We define the vocabulary
of the morphological model as the set of all words
found in the training corpus, including frequency-1
words, and one unknown word for each class. We
do this because – as we argued above – morpholog-
ical generalization is only expected to be useful for
rare words, so we are likely to get optimal perfor-
mance for PC if we include all words in clustering
and probability estimation, including hapax legom-
ena. Since our testing setup only evaluates on words
that occur more than once in the training set, we ide-
ally would want to compute the following estimate
when predicting the unknown word:
where we distinguish the unknown words of the
morphological classes from the unknown word used
in evaluation and by the KN model by giving the lat-
ter the subscript KN.
However, Eq. 2 cannot be computed efficiently
and we would not be able to compute it in practical
applications that require fast language models. For
this reason, we use the modified class model P0� in
Eq. 1 that is defined as follows:
</bodyText>
<equation confidence="0.9997215">
P0�(wlh) = �Pc(wlh) , N(w) &gt; 1
Pc(UNKg(w) Ih), N(w) = 0
</equation>
<bodyText confidence="0.994066333333333">
P0� and – by extension – PM are deficient. This
means that the evaluation of PM we present below
is pessimistic in the sense that the perplexity reduc-
tions would probably be higher if we were willing to
spend additional computational resources and com-
pute Eq. 2 in its full form.
</bodyText>
<subsectionHeader confidence="0.988845">
4.3 Distributional class language model
</subsectionHeader>
<bodyText confidence="0.9998573125">
The most frequently used type of class-based lan-
guage model is the distributional model introduced
by Brown et al. (1992). To understand the dif-
ferences between distributional and morphological
class language models, we compare our morpholog-
ical model PM with a distributional model PD that
has exactly the same form as PM; in particular, it
is defined by Equations (1) and (2). The only dif-
ference is that the classes are morphological for PM
and distributional for PD.
The exchange algorithm that was used by Brown
et al. (1992) has very long running times for large
corpora in standard implementations like srilm. It
is difficult to conduct the large number of cluster-
ings necessary for an extensive study like ours using
standard implementations.
</bodyText>
<equation confidence="0.980507333333333">
Pc(UNKKN Ih) =
E Pc(wlh) + � Pc(UNKc 1h) (2)
1w:N(w)=1} c
</equation>
<page confidence="0.991303">
389
</page>
<table confidence="0.999985136363636">
Language T/T 2 #Sentences
S bg Bulgarian .0183 .0094 181,415
S cs Czech .0185 .0097 369,881
S pl Polish .0189 .0096 358,747
S sk Slovak .0187 .0088 368,624
S sl Slovene .0156 .0090 365,455
G da Danish .0086 .0077 1,428,620
G de German .0091 .0073 1,391,324
G en English .0028 .0023 1,460,062
G nl Dutch .0061 .0048 1,457,629
G sv Swedish .0090 .0095 1,342,667
E el Greek .0081 .0079 851,636
R es Spanish .0040 .0031 1,429,276
R fr French .0029 .0024 1,460,062
R it Italian .0040 .0030 1,389,665
R pt Portuguese .0042 .0032 1,426,750
R ro Romanian .0142 .0079 178,284
U et Estonian .0329 .0198 375,698
U fi Finnish .0231 .0183 1,394,043
U hu Hungarian .0312 .0163 364,216
B lt Lithuanian .0265 .0147 365,437
B lv Latvian .0182 .0086 363,104
</table>
<tableCaption confidence="0.938615">
Table 2: Statistics for the 21 languages. S = Slavic, G
</tableCaption>
<bodyText confidence="0.962578666666667">
= Germanic, E = Greek, R = Romance, U = Uralic, B
= Baltic. Type/token ratio (T/T) and # sentences for the
training set and OOV rate 2 for the validation set. The
two smallest and largest values in each column are bold.
We therefore induce the distributional classes
as clusters in a whole-context distributional vector
space model (Sch¨utze and Walsh, 2011), a model
similar to the ones described by Sch¨utze (1992)
and Turney and Pantel (2010) except that dimension
words are immediate left and right neighbors (as op-
posed to neighbors within a window or specific types
of governors or dependents). Sch¨utze and Walsh
(2011) present experimental evidence that suggests
that the resulting classes are competitive with Brown
classes.
</bodyText>
<subsectionHeader confidence="0.989434">
4.4 Corpus
</subsectionHeader>
<bodyText confidence="0.999776393939394">
Our experiments are performed on the Europarl cor-
pus (Koehn, 2005), a parallel corpus of proceed-
ings of the European Parliament in 21 languages.
The languages are members of the following fam-
ilies: Baltic languages (Latvian, Lithuanian), Ger-
manic languages (Danish, Dutch, English, Ger-
man, Swedish), Romance languages (French, Ital-
ian, Portuguese, Romanian, Spanish), Slavic lan-
guages (Bulgarian, Czech, Polish, Slovak, Slovene),
Uralic languages (Estonian, Finnish, Hungarian)
and Greek. We only use the part of the corpus that
can be aligned to English sentences. All 21 corpora
are divided into training set (80%), validation set
(10%) and test set (10%). The training set is used for
morphological and distributional clustering and esti-
mation of class and KN models. The validation set
is used to estimate the OOV rates 2 and the optimal
parameters A, 0 and 0. Table 2 gives basic statistics
about the corpus. The sizes of the corpora of lan-
guages whose countries have joined the European
community more recently are smaller than for coun-
tries who have been members for several decades.
We see that English and French have the lowest
type/token ratios and OOV rates; and the Uralic lan-
guages (Estonian, Finnish, Hungarian) and Lithua-
nian the highest. The Slavic languages have higher
values than the Germanic languages, which in turn
have higher values than the Romance languages ex-
cept for Romanian. Type/token ratio and OOV
rate are one indicator of how much improvement
we would expect from a language model with
a morphological component compared to a non-
morphological language model.2
</bodyText>
<sectionHeader confidence="0.999311" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.99997425">
We performed all our experiments with an n-gram
order of 4; this was the order for which the KN
model performs best for all languages on the vali-
dation set.
</bodyText>
<subsectionHeader confidence="0.981453">
5.1 Morphological model
</subsectionHeader>
<bodyText confidence="0.952465769230769">
Using grid search, we first determined on the vali-
dation set the optimal combination of three param-
eters: (i) 0 E {100, 200, 500,1000, 2000, 5000},
(ii) 0 E {50,100, 200, 500} and (iii) segmentation
method. Recall that we only cluster words whose
frequency is below 0 and only consider the 0 most
2The tokenization of the Europarl corpus has a preference
for splitting tokens in unclear cases. OOV rates would be higher
for more conservative tokenization strategies.
4A two-tailed paired t-test on the improvements by language
shows that the morphological model significantly outperforms
the distributional model with p=0.0027. A test on the Germanic,
Romance and Greek languages yields p=0.19.
</bodyText>
<page confidence="0.99347">
390
</page>
<table confidence="0.999879181818182">
PPKN θ∗M φ∗ M∗ PPC PPM AM θ∗D PPWC PPD AD
S bg 74 200 50 f 103 69 0.07 500 141 71 0.04
S cs 141 500 100 f 217 129 0.08 1000 298 134 0.04
S pl 148 500 100 m 241 134 0.09 1000 349 141 0.05
S sk 123 500 200 f 186 111 0.10 1000 261 116 0.06
S sl 118 500 100 m 177 107 0.09 1000 232 111 0.06
G da 69 1000 100 r 89 65 0.05 2000 103 65 0.05
G de 100 2000 50 m 146 94 0.06 2000 150 94 0.06
G en 55 2000 50 f 73 53 0.03 5000 87 53 0.04
G nl 70 2000 50 r 100 67 0.04 5000 114 67 0.05
G sv 98 1000 50 m 132 92 0.06 2000 154 92 0.06
E el 80 1000 100 f 108 73 0.08 2000 134 74 0.07
R es 57 2000 100 m 77 54 0.05 5000 93 54 0.05
R fr 45 1000 50 f 56 43 0.04 5000 71 42 0.05
R it 69 2000 100 m 101 66 0.04 2000 100 66 0.05
R pt 62 2000 50 m 88 59 0.05 2000 87 59 0.05
R ro 76 500 100 m 121 70 0.07 1000 147 71 0.07
U et 256 500 100 m 422 230 0.10 1000 668 248 0.03
U fi 271 1000 500 f 410 240 0.11 2000 706 261 0.04
U hu 151 200 200 m 222 136 0.09 1000 360 145 0.03
B lt 175 500 200 m 278 161 0.08 1000 426 169 0.03
B lv 154 500 200 f 237 142 0.08 1000 322 147 0.05
</table>
<tableCaption confidence="0.836428">
Table 3: Perplexities on the test set for N = 4. S = Slavic, G = Germanic, E = Greek, R = Romance, U =
Uralic, B =Baltic. θ*.,, φ* and M* denote frequency threshold, suffix count and segmentation method optimal on the
</tableCaption>
<figureCaption confidence="0.814882">
validation set. The letters f, m and r stand for the frequency-based method, Morfessor and Reports. PPKN, PPC,
PPM, PPWC, PPD are the perplexities of KN, morphological class model, interpolated morphological class model,
distributional class model and interpolated distributional class model, respectively. A., denotes relative improvement:
(PPKN − PP.)/ PPKN. Bold numbers denote maxima and minima in the respective column.4
</figureCaption>
<bodyText confidence="0.999662107142857">
frequent suffixes. An experiment with the optimal
configuration was then run on the test set. The re-
sults are shown in Table 3. The KN perplexities vary
between 45 for French and 271 for Finnish.
The main result is that the morphological model
PM consistently achieves better performance than
KN (columns PPM and AM), in particular for
Slavic, Uralic and Baltic languages and Greek. Im-
provements range from 0.03 for English to 0.11 for
Finnish.
Column θ∗ M gives the threshold that is optimal for
the validation set. Values range from 200 to 2000.
Column φ∗ gives the optimal number of suffixes. It
ranges from 50 to 500. The morphologically com-
plex language Finnish seems to benefit from more
suffixes than morphologically simple languages like
Dutch, English and German, but there are a few lan-
guages that do not fit this generalization, e.g., Esto-
nian for which 100 suffixes are optimal.
The optimal morphological segmenter is given in
column M∗: f = Frequency, r = Reports, m = Mor-
fessor. The most sophisticated segmenter, Morfes-
sor is optimal for about half of the 21 languages, but
Frequency does surprisingly well. Reports is opti-
mal for two languages, Danish and Dutch. In gen-
eral, Morfessor seems to have an advantage for com-
plex morphologies, but is beaten by Frequency for
Finnish and Latvian.
</bodyText>
<subsectionHeader confidence="0.992612">
5.2 Distributional model
</subsectionHeader>
<bodyText confidence="0.999705666666667">
Columns PPD and AD show the performance of the
distributional class language model. As one would
perhaps expect, the morphological model is superior
to the distributional model for morphologically com-
plex languages like Estonian, Finnish and Hungar-
ian. These languages have many suffixes that have
</bodyText>
<page confidence="0.994952">
391
</page>
<table confidence="0.999945863636364">
AB+ − AB− 0+ 0− AO+ − AO− 0+ 0− AM+ − AM− M+ M−
S bg 0.03 200 5000 0.01 50 500 f m
S cs 0.03 500 5000 0.01 100 500 f r
S pl 0.03 500 5000 100 500 m r
S sk 0.02 500 5000 200 500 0.01 f r
S sl 0.03 500 5000 0.01 100 500 m r
G da 0.02 1000 100 100 50 r f
G de 0.02 2000 100 50 500 m f
G en 0.01 2000 100 50 500 f r
G nl 0.01 2000 100 50 500 r f
G sv 0.02 1000 100 50 500 m f
E el 0.02 1000 100 100 500 0.01 f r
R es 0.02 2000 100 100 500 m r
R fr 0.01 1000 100 50 500 f r
R it 0.01 2000 100 100 500 m r
R pt 0.02 2000 100 50 500 m r
R ro 0.03 500 5000 100 500 m r
U et 0.02 500 5000 0.01 100 50 0.01 m r
U fi 0.03 1000 100 0.03 500 50 0.02 f r
U hu 0.03 200 5000 0.01 200 50 m r
B lt 0.02 500 5000 200 50 m r
B lv 0.02 500 5000 200 500 f r
</table>
<tableCaption confidence="0.6534812">
Table 4: Sensitivity of perplexity values to the parameters (on the validation set). S = Slavic, G = Germanic, E =
Greek, R = Romance, U = Uralic, B = Baltic. A.+ and A, denote the relative improvement of PM over the KN
model when parameter x is set to the best (x+) and worst value (x−), respectively. The remaining parameters are set
to the optimal values of Table 3. Cells with differences of relative improvements that are smaller than 0.01 are left
empty.
</tableCaption>
<bodyText confidence="0.999914333333333">
high predictive power for the distributional contexts
in which a word can occur. A morphological model
can exploit this information even if a word with an
informative suffix did not occur in one of the lin-
guistically licensed contexts in the training set. For
a distributional model it is harder to learn this type
of generalization.
What is surprising about the comparative perfor-
mance of morphological and distributional models is
that there is no language for which the distributional
model outperforms the morphological model by a
wide margin. Perplexity reductions are lower than
or the same as those of the morphological model
in most cases, with only four exceptions – English,
French, Italian, and Dutch – where the distributional
model is better by one percentage point than the
morphological model (0.05 vs. 0.04 and 0.04 vs.
0.03).
Column 0* � gives the frequency threshold for the
distributional model. The optimal threshold ranges
from 500 to 5000. This means that the distributional
model benefits from restricting clustering to less fre-
quent words – and behaves similarly to the morpho-
logical class model in that respect. We know of no
previous work that has conducted experiments on
frequency thresholds for distributional class models
and shown that they increase perplexity reductions.
</bodyText>
<subsectionHeader confidence="0.999959">
5.3 Sensitivity analysis of parameters
</subsectionHeader>
<bodyText confidence="0.999513666666667">
Table 3 shows results for parameters that were opti-
mized on the validation set. We now want to analyze
how sensitive performance is to the three parame-
ters 0, 0 and segmentation method. To this end, we
present in Table 4 the best and worst values of each
parameter and the difference in perplexity improve-
ment between the two.
Differences of perplexity improvement between
best and worst values of BM range between 0.01
</bodyText>
<page confidence="0.995874">
392
</page>
<bodyText confidence="0.999911379310345">
and 0.03. The four languages with the smallest
difference 0.01 are morphologically simple (Dutch,
English, French, Italian). The languages with the
largest difference (0.03) are morphologically more
complex languages. In summary, the frequency
threshold 0M has a comparatively strong influence
on perplexity reduction. The strength of the effect is
correlated with the morphological complexity of the
language.
In contrast to 0, the number of suffixes 0 and
the segmentation method have negligible effect on
most languages. The perplexity reductions for dif-
ferent values of 0 are 0.03 for Finnish, 0.01 for Bul-
garian, Estonian, Hungarian, Polish and Slovenian,
and smaller than 0.01 for the other languages. This
means that, with the exception of Finnish, we can
use a value of 0 = 100 for all languages and be very
close to the optimal perplexity reduction – either be-
cause 100 is optimal or because perplexity reduction
is not sensitive to choice of 0. Finnish is the only
language that clearly benefits from a large number
of suffixes.
Surprisingly, the performance of the morphologi-
cal segmentation methods is very close for 17 of the
21 languages. For three of the four where there is
a difference in improvement of &gt; 0.01, Frequency
(f) performs best. This means that Frequency is a
good segmentation method for all languages, except
perhaps for Estonian.
</bodyText>
<subsectionHeader confidence="0.997227">
5.4 Impact of shape
</subsectionHeader>
<bodyText confidence="0.999668294117647">
The basic question we are asking in this paper is
to what extent the sequence of characters a word
is composed of can be exploited for better predic-
tion in language modeling. In the final analysis in
Table 5 we look at four different types of character
sequences and their contributions to perplexity re-
duction. The four groups are alphabetic character
sequences (W), numbers (N), single special charac-
ters (P = punctuation), and other (O). Examples for
O would be “751st” and words containing special
characters like “O’Neill”. The parameters used are
the optimal ones of Table 3. Table 5 shows that the
impact of special characters on perplexity is similar
across languages: 0.04 &lt; AP &lt; 0.06. The same is
true for numbers: 0.23 &lt; AN &lt; 0.33, with two out-
liers that show a stronger effect of this class: Finnish
AN = 0.38 and German AN = 0.40.
</bodyText>
<table confidence="0.997738636363636">
AW AP AN AO
S bg 0.07 0.04 0.28 0.16
S cs 0.09 0.04 0.26 0.33
S pl 0.10 0.05 0.23 0.22
S sk 0.10 0.05 0.25 0.28
S sl 0.10 0.04 0.28 0.28
G da 0.05 0.05 0.31 0.18
G de 0.06 0.05 0.40 0.18
G en 0.03 0.04 0.33 0.14
G nl 0.04 0.05 0.31 0.26
G sv 0.06 0.05 0.31 0.35
E el 0.08 0.05 0.33 0.14
R es 0.05 0.04 0.26 0.14
R fr 0.04 0.04 0.29 0.01
R it 0.04 0.05 0.33 0.02
R pt 0.05 0.05 0.28 0.39
R ro 0.08 0.04 0.25 0.17
U et 0.11 0.05 0.26 0.26
U fi 0.12 0.06 0.38 0.36
U hu 0.10 0.04 0.32 0.23
B lt 0.08 0.06 0.27 0.05
B lv 0.08 0.05 0.26 0.19
</table>
<tableCaption confidence="0.999219">
Table 5: Relative improvements of PM on the valida-
</tableCaption>
<bodyText confidence="0.705391">
tion set compared to KN for histories wi�1
</bodyText>
<equation confidence="0.239562">
i��+1 grouped
</equation>
<bodyText confidence="0.990486636363636">
by the type of wi_1. The possible types are alphabetic
word (W), punctuation (P), number (N) and other (O).
The fact that special characters and numbers be-
have similarly across languages is encouraging as
one would expect less crosslinguistic variation for
these two classes of words.
In contrast, “true” words (those exclusively com-
posed of alphabetic characters) show more variation
from language to language: 0.03 &lt; AW &lt; 0.12.
The range of variation is not necessarily larger than
for numbers, but since most words are alphabetical
words, class W is responsible for most of the differ-
ence in perplexity reduction between different lan-
guages. As before we observe a negative correlation
between morphological complexity and perplexity
reduction; e.g., Dutch and English have small AW
and Estonian and Finnish large values.
We provide the values of AO for completeness.
The composition of this catch-all group varies con-
siderably from language to language. For exam-
ple, many words in this class are numbers with al-
phabetic suffixes like “2012-ben” in Hungarian and
</bodyText>
<page confidence="0.997408">
393
</page>
<bodyText confidence="0.724838">
words with apostrophes in French.
</bodyText>
<sectionHeader confidence="0.996575" genericHeader="evaluation">
6 Summary
</sectionHeader>
<bodyText confidence="0.999996677419355">
We have investigated an interpolation of a KN model
with a class language model whose classes are de-
fined by morphology and shape features. We tested
this model in a large crosslingual study of European
languages.
Even though the model is generic and we use
the same architecture and features for all languages,
the model achieves reductions in perplexity for all
21 languages represented in the Europarl corpus,
ranging from 3% to 11%, when compared to a KN
model. We found perplexity reductions across all
21 languages for histories ending with four different
types of word shapes: alphabetical words, special
characters, and numbers.
We looked at the sensitivity of perplexity reduc-
tions to three parameters of the model: B, a thresh-
old that determines for which frequencies words are
given their own class; 0, the number of suffixes used
to determine class membership; and morphological
segmentation. We found that B has a considerable
influence on the performance of the model and that
optimal values vary from language to language. This
parameter should be tuned when the model is used
in practice.
In contrast, the number of suffixes and the mor-
phological segmentation method only had a small
effect on perplexity reductions. This is a surprising
result since it means that simple identification of suf-
fixes by frequency and choosing a fixed number of
suffixes 0 across languages is sufficient for getting
most of the perplexity reduction that is possible.
</bodyText>
<sectionHeader confidence="0.999728" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999981954545455">
A surprising result of our experiments was that the
perplexity reductions due to morphological classes
were generally better than those due to distributional
classes even though distributional classes are formed
directly based on the type of information that a lan-
guage model is evaluated on – the distribution of
words or which words are likely to occur in se-
quence. An intriguing question is to what extent the
effect of morphological and distributional classes is
additive. We ran an exploratory experiment with
a model that interpolates KN, morphological class
model and distributional class model. This model
only slightly outperformed the interpolation of KN
and morphological class model (column PPM in Ta-
ble 3). We would like to investigate in future work if
the information provided by the two types of classes
is indeed largely redundant or if a more sophisticated
combination would perform better than the simple
linear interpolation we have used here.
Acknowledgments. This research was funded by
DFG (grant SFB 732). We would like to thank the
anonymous reviewers for their valuable comments.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999918722222222">
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza,
Robert L. Mercer, and David Nahamoo. 1991. A fast
algorithm for deleted interpolation. In Eurospeech.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Comput. Linguist.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
NAACL-HLT.
Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist.
Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech &amp; Language.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM TSLP.
Mathias Creutz, Teemu Hirsim¨aki, Mikko Kurimo, Antti
Puurula, Janne Pylkk¨onen, Vesa Siivola, Matti Var-
jokallio, Ebru Arisoy, Murat Sarac¸lar, and Andreas
Stolcke. 2007. Morph-based speech recognition
and modeling of out-of-vocabulary words across lan-
guages. ACM TSLP.
Samarth Keshava and Emily Pitler. 2006. A simpler,
intuitive approach to morpheme induction. In PASCAL
Morpho Challenge.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit.
Tom´aˇs Mikolov, Martin Karafi´at, Luk´aˇs Burget, Jan
ˇCernock´y, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In ICSLP.
Thomas M¨uller and Hinrich Sch¨utze. 2011. Improved
modeling of out-of-vocabulary words using morpho-
logical classes. In ACL.
</reference>
<page confidence="0.987995">
394
</page>
<reference confidence="0.99809852">
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modelling. Computer
Speech &amp; Language.
Hinrich Sch¨utze and Michael Walsh. 2011. Half-context
language models. Comput. Linguist.
Hinrich Sch¨utze. 1992. Dimensions of meaning.
In ACM/IEEE Conference on Supercomputing, pages
787–796.
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Interspeech.
Yee Whye Teh. 2006. A hierarchical bayesian language
model based on Pitman-Yor processes. In ACL.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
JAIR.
Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and An-
dreas Stolcke. 2004. Morphology-based language
modeling for Arabic speech recognition. In ICSLP.
E.W.D. Whittaker and P.C. Woodland. 2000. Particle-
based language modelling. In ICSLP.
Peng Xu and Frederick Jelinek. 2004. Random forests in
language modeling. In EMNLP.
Deniz Yuret and Ergun Bic¸ici. 2009. Modeling morpho-
logically rich languages using split words and unstruc-
tured dependencies. In ACL-IJCNLP.
</reference>
<page confidence="0.999118">
395
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.649647">
<title confidence="0.958969">A Comparative Investigation of Morphological Language Modeling for the Languages of the European Union</title>
<author confidence="0.967754">Thomas M¨uller</author>
<author confidence="0.967754">Hinrich Sch¨utze</author>
<author confidence="0.967754">Helmut</author>
<affiliation confidence="0.999647">Institute for Natural Language University of Stuttgart,</affiliation>
<abstract confidence="0.976964692307692">We investigate a language model that combines morphological and shape features with a Kneser-Ney model and test it in a large crosslingual study of European languages. Even though the model is generic and we use the same architecture and features for all languages, the model achieves reductions in perplexity for all 21 languages represented in the Europarl corpus, ranging from 3% to 11%. We show that almost all of this perplexity reduction can be achieved by identifying suffixes by frequency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Peter F Brown</author>
<author>Peter V de Souza</author>
<author>Robert L Mercer</author>
<author>David Nahamoo</author>
</authors>
<title>A fast algorithm for deleted interpolation.</title>
<date>1991</date>
<booktitle>In Eurospeech.</booktitle>
<marker>Bahl, Brown, de Souza, Mercer, Nahamoo, 1991</marker>
<rawString>Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, Robert L. Mercer, and David Nahamoo. 1991. A fast algorithm for deleted interpolation. In Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Comput. Linguist.</journal>
<contexts>
<context position="6614" citStr="Berger et al., 1996" startWordPosition="1020" endWordPosition="1023">useful across languages and what generic methods are appropriate for this goal. Previous work also has concentrated on traditional linguistic morphology whereas we compare linguistically motivated morphological segmentation with frequency-based segmentation and include shape features in our study. Our initial plan for this paper was to use complex language modeling frameworks that allow experimenters to include arbitrary features (including morphological and shape features) in the model. In particular, we looked at publicly available implementations of maximum entropy models (Rosenfeld, 1996; Berger et al., 1996) and random forests (Xu and Jelinek, 2004). However, we found that these methods do not currently scale to running a large set of experiments on a multi-gigabyte parallel corpus of 21 languages. Similar considerations apply to other sophisticated language modeling techniques like Pitman-Yor processes (Teh, 2006), recurrent neural networks (Mikolov et al., 2010) and FLMs in their general, more powerful form. In addition, perplexity reductions of these complex models compared to simpler state-of-the-art models are generally not large. We therefore decided to conduct our study in the framework of</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff A Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In NAACL-HLT.</booktitle>
<contexts>
<context position="5579" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="858" endWordPosition="861">e morphological and shape features we use. Section 4 introduces language model and experimental setup. Section 5 discusses our results. Section 6 summarizes the contributions of this paper. 2 Related Work Whittaker and Woodland (2000) apply language modeling to morpheme sequences and investigate data-driven segmentation methods. Creutz et al. (2007) propose a similar method that improves speech recognition for highly inflecting languages. They use Morfessor (Creutz and Lagus, 2007) to split words into morphemes. Both approaches are essentially a simple form of a factored language model (FLM) (Bilmes and Kirchhoff, 2003). In a general FLM a number of different back-off paths are combined by a back-off function to improve the prediction after rare or unseen histories. Vergyri et al. (2004) apply FLMs and morphological features to Arabic speech recognition. These papers and other prior work on using morphology in language modeling have been languagespecific and have paid less attention to the question as to how morphology can be useful across languages and what generic methods are appropriate for this goal. Previous work also has concentrated on traditional linguistic morphology whereas we compare linguisticall</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored language models and generalized parallel backoff. In NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V de Souza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Comput. Linguist.</journal>
<marker>Brown, de Souza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased n-gram models of natural language. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech &amp; Language.</journal>
<contexts>
<context position="10040" citStr="Chen and Goodman, 1999" startWordPosition="1612" endWordPosition="1615">ord in the test set has a combination of feature values that does not occur in the training set, then it is assigned to the class whose features are most similar. We described the similarity measure and details of the shape features in prior work (M¨uller and Sch¨utze, 2011). The shape features are listed in Table 1. 4 Experimental Setup Experiments are performed using srilm (Stolcke, 2002), in particular the Kneser-Ney (KN) and generic class model implementations. Estimation of optimal interpolation parameters is based on (Bahl et al., 1991). 4.1 Baseline Our baseline is a modified KN model (Chen and Goodman, 1999). 4.2 Morphological class language model We use a variation of the model proposed by Brown et al. (1992) that we developed in prior work on English (M¨uller and Sch¨utze, 2011). This model is a class-based language model that groups words into classes and replaces the word transition probability by a class transition probability and a word emission probability: PC(wi|wi−1 i−N+1) = P(g(wi)|g(wi−1 i−N+1)) &apos; P(wi|g(wi)) where g(w) is the class of word w and we write g(wi ... wj) for g(wi) ... g(wj). Our approach targets rare and unseen histories. We therefore exclude all frequent words from clust</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech &amp; Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM TSLP.</journal>
<contexts>
<context position="5438" citStr="Creutz and Lagus, 2007" startWordPosition="836" endWordPosition="839">o use it effectively for prediction. The paper is organized as follows. In Section 2 we discuss related work. In Section 3 we describe the morphological and shape features we use. Section 4 introduces language model and experimental setup. Section 5 discusses our results. Section 6 summarizes the contributions of this paper. 2 Related Work Whittaker and Woodland (2000) apply language modeling to morpheme sequences and investigate data-driven segmentation methods. Creutz et al. (2007) propose a similar method that improves speech recognition for highly inflecting languages. They use Morfessor (Creutz and Lagus, 2007) to split words into morphemes. Both approaches are essentially a simple form of a factored language model (FLM) (Bilmes and Kirchhoff, 2003). In a general FLM a number of different back-off paths are combined by a back-off function to improve the prediction after rare or unseen histories. Vergyri et al. (2004) apply FLMs and morphological features to Arabic speech recognition. These papers and other prior work on using morphology in language modeling have been languagespecific and have paid less attention to the question as to how morphology can be useful across languages and what generic met</context>
<context position="8670" citStr="Creutz and Lagus, 2007" startWordPosition="1381" endWordPosition="1384">tification algorithms: Re387 s, e, d, ed, n, g, ng, ing, y, t, es, r, a, l, on, er, ion, ted, ly, tion, rs, al, o, ts, ns, le, i, ation, an, ers, m, nt, ting, h, c, te, sed, ated, en, ty, ic, k, ent, st, ss, ons, se, ity, ble, ne, ce, ess, ions, us, ry, re, ies, ve, p, ate, in, tions, ia, red, able, is, ive, ness, lly, ring, ment, led, ned, tes, as, ls, ding, ling, sing, ds, ded, ian, nce, ar, ating, sm, ally, nts, de, nd, ism, or, ge, ist, ses, ning, u, king, na, el Figure 1: The 100 most frequent English suffixes in Europarl, ordered by frequency ports (Keshava and Pitler, 2006), Morfessor (Creutz and Lagus, 2007) and Frequency, where Frequency simply selects the most frequent word-final letter sequences as suffixes. The 100 most frequent suffixes found by Frequency for English are given in Figure 1. We use the φ most frequent suffixes for all three algorithms, where φ is a parameter. The focus of our work is to evaluate the utility of these algorithms for language modeling; we do not directly evaluate the quality of the suffixes. A word is segmented by identifying the longest of the φ suffixes that it ends with. Thus, each word has one suffix feature if it ends with one of the φ suffixes and none othe</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM TSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
</authors>
<title>Teemu Hirsim¨aki, Mikko Kurimo, Antti Puurula, Janne Pylkk¨onen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Sarac¸lar, and Andreas Stolcke.</title>
<date>2007</date>
<publisher>ACM TSLP.</publisher>
<marker>Creutz, 2007</marker>
<rawString>Mathias Creutz, Teemu Hirsim¨aki, Mikko Kurimo, Antti Puurula, Janne Pylkk¨onen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Sarac¸lar, and Andreas Stolcke. 2007. Morph-based speech recognition and modeling of out-of-vocabulary words across languages. ACM TSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samarth Keshava</author>
<author>Emily Pitler</author>
</authors>
<title>A simpler, intuitive approach to morpheme induction.</title>
<date>2006</date>
<booktitle>In PASCAL Morpho Challenge.</booktitle>
<contexts>
<context position="8634" citStr="Keshava and Pitler, 2006" startWordPosition="1376" endWordPosition="1379"> three different automatic suffix identification algorithms: Re387 s, e, d, ed, n, g, ng, ing, y, t, es, r, a, l, on, er, ion, ted, ly, tion, rs, al, o, ts, ns, le, i, ation, an, ers, m, nt, ting, h, c, te, sed, ated, en, ty, ic, k, ent, st, ss, ons, se, ity, ble, ne, ce, ess, ions, us, ry, re, ies, ve, p, ate, in, tions, ia, red, able, is, ive, ness, lly, ring, ment, led, ned, tes, as, ls, ding, ling, sing, ds, ded, ian, nce, ar, ating, sm, ally, nts, de, nd, ism, or, ge, ist, ses, ning, u, king, na, el Figure 1: The 100 most frequent English suffixes in Europarl, ordered by frequency ports (Keshava and Pitler, 2006), Morfessor (Creutz and Lagus, 2007) and Frequency, where Frequency simply selects the most frequent word-final letter sequences as suffixes. The 100 most frequent suffixes found by Frequency for English are given in Figure 1. We use the φ most frequent suffixes for all three algorithms, where φ is a parameter. The focus of our work is to evaluate the utility of these algorithms for language modeling; we do not directly evaluate the quality of the suffixes. A word is segmented by identifying the longest of the φ suffixes that it ends with. Thus, each word has one suffix feature if it ends with</context>
</contexts>
<marker>Keshava, Pitler, 2006</marker>
<rawString>Samarth Keshava and Emily Pitler. 2006. A simpler, intuitive approach to morpheme induction. In PASCAL Morpho Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT summit.</booktitle>
<contexts>
<context position="17086" citStr="Koehn, 2005" startWordPosition="2847" endWordPosition="2848">ach column are bold. We therefore induce the distributional classes as clusters in a whole-context distributional vector space model (Sch¨utze and Walsh, 2011), a model similar to the ones described by Sch¨utze (1992) and Turney and Pantel (2010) except that dimension words are immediate left and right neighbors (as opposed to neighbors within a window or specific types of governors or dependents). Sch¨utze and Walsh (2011) present experimental evidence that suggests that the resulting classes are competitive with Brown classes. 4.4 Corpus Our experiments are performed on the Europarl corpus (Koehn, 2005), a parallel corpus of proceedings of the European Parliament in 21 languages. The languages are members of the following families: Baltic languages (Latvian, Lithuanian), Germanic languages (Danish, Dutch, English, German, Swedish), Romance languages (French, Italian, Portuguese, Romanian, Spanish), Slavic languages (Bulgarian, Czech, Polish, Slovak, Slovene), Uralic languages (Estonian, Finnish, Hungarian) and Greek. We only use the part of the corpus that can be aligned to English sentences. All 21 corpora are divided into training set (80%), validation set (10%) and test set (10%). The tra</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Mikolov</author>
<author>Martin Karafi´at</author>
<author>Luk´aˇs Burget</author>
<author>Jan ˇCernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In ICSLP.</booktitle>
<marker>Mikolov, Karafi´at, Burget, ˇCernock´y, Khudanpur, 2010</marker>
<rawString>Tom´aˇs Mikolov, Martin Karafi´at, Luk´aˇs Burget, Jan ˇCernock´y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M¨uller</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Improved modeling of out-of-vocabulary words using morphological classes.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<marker>M¨uller, Sch¨utze, 2011</marker>
<rawString>Thomas M¨uller and Hinrich Sch¨utze. 2011. Improved modeling of out-of-vocabulary words using morphological classes. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modelling.</title>
<date>1996</date>
<journal>Computer Speech &amp; Language.</journal>
<contexts>
<context position="6592" citStr="Rosenfeld, 1996" startWordPosition="1017" endWordPosition="1019">orphology can be useful across languages and what generic methods are appropriate for this goal. Previous work also has concentrated on traditional linguistic morphology whereas we compare linguistically motivated morphological segmentation with frequency-based segmentation and include shape features in our study. Our initial plan for this paper was to use complex language modeling frameworks that allow experimenters to include arbitrary features (including morphological and shape features) in the model. In particular, we looked at publicly available implementations of maximum entropy models (Rosenfeld, 1996; Berger et al., 1996) and random forests (Xu and Jelinek, 2004). However, we found that these methods do not currently scale to running a large set of experiments on a multi-gigabyte parallel corpus of 21 languages. Similar considerations apply to other sophisticated language modeling techniques like Pitman-Yor processes (Teh, 2006), recurrent neural networks (Mikolov et al., 2010) and FLMs in their general, more powerful form. In addition, perplexity reductions of these complex models compared to simpler state-of-the-art models are generally not large. We therefore decided to conduct our stu</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Ronald Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modelling. Computer Speech &amp; Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
<author>Michael Walsh</author>
</authors>
<title>Half-context language models.</title>
<date>2011</date>
<journal>Comput. Linguist.</journal>
<marker>Sch¨utze, Walsh, 2011</marker>
<rawString>Hinrich Sch¨utze and Michael Walsh. 2011. Half-context language models. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Dimensions of meaning.</title>
<date>1992</date>
<booktitle>In ACM/IEEE Conference on Supercomputing,</booktitle>
<pages>787--796</pages>
<marker>Sch¨utze, 1992</marker>
<rawString>Hinrich Sch¨utze. 1992. Dimensions of meaning. In ACM/IEEE Conference on Supercomputing, pages 787–796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Interspeech.</booktitle>
<contexts>
<context position="9810" citStr="Stolcke, 2002" startWordPosition="1579" endWordPosition="1580">has one suffix feature if it ends with one of the φ suffixes and none otherwise. In addition to suffix features, we define features that capture shape properties: capitalization, special characters and word length. If a word in the test set has a combination of feature values that does not occur in the training set, then it is assigned to the class whose features are most similar. We described the similarity measure and details of the shape features in prior work (M¨uller and Sch¨utze, 2011). The shape features are listed in Table 1. 4 Experimental Setup Experiments are performed using srilm (Stolcke, 2002), in particular the Kneser-Ney (KN) and generic class model implementations. Estimation of optimal interpolation parameters is based on (Bahl et al., 1991). 4.1 Baseline Our baseline is a modified KN model (Chen and Goodman, 1999). 4.2 Morphological class language model We use a variation of the model proposed by Brown et al. (1992) that we developed in prior work on English (M¨uller and Sch¨utze, 2011). This model is a class-based language model that groups words into classes and replaces the word transition probability by a class transition probability and a word emission probability: PC(wi|</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An extensible language modeling toolkit. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6927" citStr="Teh, 2006" startWordPosition="1070" endWordPosition="1071">s paper was to use complex language modeling frameworks that allow experimenters to include arbitrary features (including morphological and shape features) in the model. In particular, we looked at publicly available implementations of maximum entropy models (Rosenfeld, 1996; Berger et al., 1996) and random forests (Xu and Jelinek, 2004). However, we found that these methods do not currently scale to running a large set of experiments on a multi-gigabyte parallel corpus of 21 languages. Similar considerations apply to other sophisticated language modeling techniques like Pitman-Yor processes (Teh, 2006), recurrent neural networks (Mikolov et al., 2010) and FLMs in their general, more powerful form. In addition, perplexity reductions of these complex models compared to simpler state-of-the-art models are generally not large. We therefore decided to conduct our study in the framework of smoothed n-gram models, which currently are an order of magnitude faster and more scalable. More specifically, we adopt a class-based approach, where words are clustered based on morphological and shape features. This approach has the nice property that the number of features used to estimate the classes does n</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical bayesian language model based on Pitman-Yor processes. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<publisher>JAIR.</publisher>
<contexts>
<context position="16720" citStr="Turney and Pantel (2010)" startWordPosition="2789" endWordPosition="2792">31 .0183 1,394,043 U hu Hungarian .0312 .0163 364,216 B lt Lithuanian .0265 .0147 365,437 B lv Latvian .0182 .0086 363,104 Table 2: Statistics for the 21 languages. S = Slavic, G = Germanic, E = Greek, R = Romance, U = Uralic, B = Baltic. Type/token ratio (T/T) and # sentences for the training set and OOV rate 2 for the validation set. The two smallest and largest values in each column are bold. We therefore induce the distributional classes as clusters in a whole-context distributional vector space model (Sch¨utze and Walsh, 2011), a model similar to the ones described by Sch¨utze (1992) and Turney and Pantel (2010) except that dimension words are immediate left and right neighbors (as opposed to neighbors within a window or specific types of governors or dependents). Sch¨utze and Walsh (2011) present experimental evidence that suggests that the resulting classes are competitive with Brown classes. 4.4 Corpus Our experiments are performed on the Europarl corpus (Koehn, 2005), a parallel corpus of proceedings of the European Parliament in 21 languages. The languages are members of the following families: Baltic languages (Latvian, Lithuanian), Germanic languages (Danish, Dutch, English, German, Swedish), </context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. JAIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitra Vergyri</author>
<author>Katrin Kirchhoff</author>
<author>Kevin Duh</author>
<author>Andreas Stolcke</author>
</authors>
<title>Morphology-based language modeling for Arabic speech recognition.</title>
<date>2004</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="5750" citStr="Vergyri et al. (2004)" startWordPosition="887" endWordPosition="890"> this paper. 2 Related Work Whittaker and Woodland (2000) apply language modeling to morpheme sequences and investigate data-driven segmentation methods. Creutz et al. (2007) propose a similar method that improves speech recognition for highly inflecting languages. They use Morfessor (Creutz and Lagus, 2007) to split words into morphemes. Both approaches are essentially a simple form of a factored language model (FLM) (Bilmes and Kirchhoff, 2003). In a general FLM a number of different back-off paths are combined by a back-off function to improve the prediction after rare or unseen histories. Vergyri et al. (2004) apply FLMs and morphological features to Arabic speech recognition. These papers and other prior work on using morphology in language modeling have been languagespecific and have paid less attention to the question as to how morphology can be useful across languages and what generic methods are appropriate for this goal. Previous work also has concentrated on traditional linguistic morphology whereas we compare linguistically motivated morphological segmentation with frequency-based segmentation and include shape features in our study. Our initial plan for this paper was to use complex langua</context>
</contexts>
<marker>Vergyri, Kirchhoff, Duh, Stolcke, 2004</marker>
<rawString>Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and Andreas Stolcke. 2004. Morphology-based language modeling for Arabic speech recognition. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W D Whittaker</author>
<author>P C Woodland</author>
</authors>
<title>Particlebased language modelling.</title>
<date>2000</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="5186" citStr="Whittaker and Woodland (2000)" startWordPosition="802" endWordPosition="805">that there is one parameter that must be tuned for good performance for most languages: the frequency threshold 0 above which a word is not subject to morphological generalization because it occurs frequently enough for standard word n-gram language models to use it effectively for prediction. The paper is organized as follows. In Section 2 we discuss related work. In Section 3 we describe the morphological and shape features we use. Section 4 introduces language model and experimental setup. Section 5 discusses our results. Section 6 summarizes the contributions of this paper. 2 Related Work Whittaker and Woodland (2000) apply language modeling to morpheme sequences and investigate data-driven segmentation methods. Creutz et al. (2007) propose a similar method that improves speech recognition for highly inflecting languages. They use Morfessor (Creutz and Lagus, 2007) to split words into morphemes. Both approaches are essentially a simple form of a factored language model (FLM) (Bilmes and Kirchhoff, 2003). In a general FLM a number of different back-off paths are combined by a back-off function to improve the prediction after rare or unseen histories. Vergyri et al. (2004) apply FLMs and morphological featur</context>
</contexts>
<marker>Whittaker, Woodland, 2000</marker>
<rawString>E.W.D. Whittaker and P.C. Woodland. 2000. Particlebased language modelling. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Frederick Jelinek</author>
</authors>
<title>Random forests in language modeling.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6656" citStr="Xu and Jelinek, 2004" startWordPosition="1027" endWordPosition="1030">methods are appropriate for this goal. Previous work also has concentrated on traditional linguistic morphology whereas we compare linguistically motivated morphological segmentation with frequency-based segmentation and include shape features in our study. Our initial plan for this paper was to use complex language modeling frameworks that allow experimenters to include arbitrary features (including morphological and shape features) in the model. In particular, we looked at publicly available implementations of maximum entropy models (Rosenfeld, 1996; Berger et al., 1996) and random forests (Xu and Jelinek, 2004). However, we found that these methods do not currently scale to running a large set of experiments on a multi-gigabyte parallel corpus of 21 languages. Similar considerations apply to other sophisticated language modeling techniques like Pitman-Yor processes (Teh, 2006), recurrent neural networks (Mikolov et al., 2010) and FLMs in their general, more powerful form. In addition, perplexity reductions of these complex models compared to simpler state-of-the-art models are generally not large. We therefore decided to conduct our study in the framework of smoothed n-gram models, which currently a</context>
</contexts>
<marker>Xu, Jelinek, 2004</marker>
<rawString>Peng Xu and Frederick Jelinek. 2004. Random forests in language modeling. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Ergun Bic¸ici</author>
</authors>
<title>Modeling morphologically rich languages using split words and unstructured dependencies.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<marker>Yuret, Bic¸ici, 2009</marker>
<rawString>Deniz Yuret and Ergun Bic¸ici. 2009. Modeling morphologically rich languages using split words and unstructured dependencies. In ACL-IJCNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>