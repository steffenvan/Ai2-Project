<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002114">
<title confidence="0.989279">
Extending the Entity-based Coherence Model with Multiple Ranks
</title>
<author confidence="0.99779">
Vanessa Wei Feng
</author>
<affiliation confidence="0.998466">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.984851">
Toronto, ON, M5S 3G4, Canada
</address>
<email confidence="0.99943">
weifeng@cs.toronto.edu
</email>
<author confidence="0.995541">
Graeme Hirst
</author>
<affiliation confidence="0.998461">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.99214">
Toronto, ON, M5S 3G4, Canada
</address>
<email confidence="0.999768">
gh@cs.toronto.edu
</email>
<sectionHeader confidence="0.993921" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998827533333333">
We extend the original entity-based coher-
ence model (Barzilay and Lapata, 2008)
by learning from more fine-grained coher-
ence preferences in training data. We asso-
ciate multiple ranks with the set of permuta-
tions originating from the same source doc-
ument, as opposed to the original pairwise
rankings. We also study the effect of the
permutations used in training, and the effect
of the coreference component used in en-
tity extraction. With no additional manual
annotations required, our extended model
is able to outperform the original model on
two tasks: sentence ordering and summary
coherence rating.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999664696428572">
Coherence is important in a well-written docu-
ment; it helps make the text semantically mean-
ingful and interpretable. Automatic evaluation
of coherence is an essential component of vari-
ous natural language applications. Therefore, the
study of coherence models has recently become
an active research area. A particularly popular
coherence model is the entity-based local coher-
ence model of Barzilay and Lapata (B&amp;L) (2005;
2008). This model represents local coherence
by transitions, from one sentence to the next, in
the grammatical role of references to entities. It
learns a pairwise ranking preference between al-
ternative renderings of a document based on the
probability distribution of those transitions. In
particular, B&amp;L associated a lower rank with au-
tomatically created permutations of a source doc-
ument, and learned a model to discriminate an
original text from its permutations (see Section
3.1 below). However, coherence is matter of de-
gree rather than a binary distinction, so a model
based only on such pairwise rankings is insuffi-
ciently fine-grained and cannot capture the sub-
tle differences in coherence between the permuted
documents.
Since the first appearance of B&amp;L’s model,
several extensions have been proposed (see Sec-
tion 2.3 below), primarily focusing on modify-
ing or enriching the original feature set by incor-
porating other document information. By con-
trast, we wish to refine the learning procedure
in a way such that the resulting model will be
able to evaluate coherence on a more fine-grained
level. Specifically, we propose a concise exten-
sion to the standard entity-based coherence model
by learning not only from the original docu-
ment and its corresponding permutations but also
from ranking preferences among the permutations
themselves.
We show that this can be done by assigning a
suitable objective score for each permutation indi-
cating its dissimilarity from the original one. We
call this a multiple-rank model since we train our
model on a multiple-rank basis, rather than tak-
ing the original pairwise ranking approach. This
extension can also be easily combined with other
extensions by incorporating their enriched feature
sets. We show that our multiple-rank model out-
performs B&amp;L’s basic model on two tasks, sen-
tence ordering and summary coherence rating,
evaluated on the same datasets as in Barzilay and
Lapata (2008).
In sentence ordering, we experiment with
different approaches to assigning dissimilarity
scores and ranks (Section 5.1.1). We also exper-
iment with different entity extraction approaches
</bodyText>
<page confidence="0.988631">
315
</page>
<note confidence="0.8431295">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 315–324,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
Manila Miles Island Quake Baco
1 − X X −
2 S − O − −
3 X X X X X
</note>
<tableCaption confidence="0.98933">
Table 1: A fragment of an entity grid for five entities
across three sentences.
</tableCaption>
<bodyText confidence="0.99950125">
(Section 5.1.2) and different distributions of per-
mutations used in training (Section 5.1.3). We
show that these two aspects are crucial, depend-
ing on the characteristics of the dataset.
</bodyText>
<sectionHeader confidence="0.994947" genericHeader="introduction">
2 Entity-based Coherence Model
</sectionHeader>
<subsectionHeader confidence="0.895223">
2.1 Document Representation
</subsectionHeader>
<bodyText confidence="0.998099416666666">
The original entity-based coherence model is
based on the assumption that a document makes
repeated reference to elements of a set of entities
that are central to its topic. For a document d, an
entity grid is constructed, in which the columns
represent the entities referred to in d, and rows
represent the sentences. Each cell corresponds
to the grammatical role of an entity in the corre-
sponding sentence: subject (S), object (O), nei-
ther (X), or nothing (−). An example fragment
of an entity grid is shown in Table 1; it shows
the representation of three sentences from a text
on a Philippine earthquake. B&amp;L define a lo-
cal transition as a sequence {S, O, X, −}n, repre-
senting the occurrence and grammatical roles of
an entity in n adjacent sentences. Such transi-
tion sequences can be extracted from the entity
grid as continuous subsequences in each column.
For example, the entity “Manila” in Table 1 has
a bigram transition {S, X} from sentence 2 to 3.
The entity grid is then encoded as a feature vector
O(d) _ (p1(d), p2(d), ... , pm(d)), where pt(d) is
the probability of the transition t in the entity grid,
and m is the number of transitions with length no
more than a predefined optimal transition length
k. pt(d) is computed as the number of occurrences
of t in the entity grid of document d, divided by
the total number of transitions of the same length
in the entity grid.
For entity extraction, Barzilay and Lapata
(2008) had two conditions: Coreference+ and
Coreference−. In Coreference+, entity corefer-
ence relations in the document were resolved by
an automatic coreference resolution tool (Ng and
Cardie, 2002), whereas in Coreference−, nouns
are simply clustered by string matching.
</bodyText>
<subsectionHeader confidence="0.996752">
2.2 Evaluation Tasks
</subsectionHeader>
<bodyText confidence="0.999961763157895">
Two evaluation tasks for Barzilay and Lapata
(2008)’s entity-based model are sentence order-
ing and summary coherence rating.
In sentence ordering, a set of random permu-
tations is created for each source document, and
the learning procedure is conducted on this syn-
thetic mixture of coherent and incoherent docu-
ments. Barzilay and Lapata (2008) experimented
on two datasets: news articles on the topic of
earthquakes (Earthquakes) and narratives on the
topic of aviation accidents (Accidents). A train-
ing data instance is constructed as a pair con-
sisting of a source document and one of its ran-
dom permutations, and the permuted document
is always considered to be less coherent than the
source document. The entity transition features
are then used to train a support vector machine
ranker (Joachims, 2002) to rank the source docu-
ments higher than the permutations. The model is
tested on a different set of source documents and
their permutations, and the performance is evalu-
ated as the fraction of correct pairwise rankings in
the test set.
In summary coherence rating, a similar exper-
imental framework is adopted. However, in this
task, rather than training and evaluating on a set
of synthetic data, system-generated summaries
and human-composed reference summaries from
the Document Understanding Conference (DUC
2003) were used. Human annotators were asked
to give a coherence score on a seven-point scale
for each item. The pairwise ranking preferences
between summaries generated from the same in-
put document cluster (excluding the pairs consist-
ing of two human-written summaries) are used by
a support vector machine ranker to learn a dis-
criminant function to rank each pair according to
their coherence scores.
</bodyText>
<subsectionHeader confidence="0.988791">
2.3 Extended Models
</subsectionHeader>
<bodyText confidence="0.999552875">
Filippova and Strube (2007) applied Barzilay and
Lapata’s model on a German corpus of newspa-
per articles with manual syntactic, morphological,
and NP coreference annotations provided. They
further clustered entities by semantic relatedness
as computed by the WikiRelated! API (Strube and
Ponzetto, 2006). Though the improvement was
not significant, interestingly, a short subsection in
</bodyText>
<page confidence="0.998746">
316
</page>
<bodyText confidence="0.9999916">
their paper described their approach to extending
pairwise rankings to longer rankings, by supply-
ing the learner with rankings of all renderings as
computed by Kendall’s T, which is one of our
extensions considered in this paper. Although
Filippova and Strube simply discarded this idea
because it hurt accuracies when tested on their
data, we found it a promising direction for further
exploration. Cheung and Penn (2010) adapted
the standard entity-based coherence model to the
same German corpus, but replaced the original
linguistic dimension used by Barzilay and Lap-
ata (2008) — grammatical role — with topologi-
cal field information, and showed that for German
text, such a modification improves accuracy.
For English text, two extensions have been pro-
posed recently. Elsner and Charniak (2011) aug-
mented the original features used in the standard
entity-based coherence model with a large num-
ber of entity-specific features, and their extension
significantly outperformed the standard model
on two tasks: document discrimination (another
name for sentence ordering), and sentence inser-
tion. Lin et al. (2011) adapted the entity grid rep-
resentation in the standard model into a discourse
role matrix, where additional discourse informa-
tion about the document was encoded. Their ex-
tended model significantly improved ranking ac-
curacies on the same two datasets used by Barzi-
lay and Lapata (2008) as well as on the Wall Street
Journal corpus.
However, while enriching or modifying the
original features used in the standard model is cer-
tainly a direction for refinement of the model, it
usually requires more training data or a more so-
phisticated feature representation. In this paper,
we instead modify the learning approach and pro-
pose a concise and highly adaptive extension that
can be easily combined with other extended fea-
tures or applied to different languages.
</bodyText>
<sectionHeader confidence="0.998536" genericHeader="method">
3 Experimental Design
</sectionHeader>
<bodyText confidence="0.999300666666667">
Following Barzilay and Lapata (2008), we wish
to train a discriminative model to give the cor-
rect ranking preference between two documents
in terms of their degree of coherence. We experi-
ment on the same two tasks as in their work: sen-
tence ordering and summary coherence rating.
</bodyText>
<subsectionHeader confidence="0.999783">
3.1 Sentence Ordering
</subsectionHeader>
<bodyText confidence="0.999751290322581">
In the standard entity-based model, a discrimina-
tive system is trained on the pairwise rankings be-
tween source documents and their permutations
(see Section 2.2). However, a model learned from
these pairwise rankings is not sufficiently fine-
grained, since the subtle differences between the
permutations are not learned. Our major contribu-
tion is to further differentiate among the permuta-
tions generated from the same source documents,
rather than simply treating them all as being of the
same degree of coherence.
Our fundamental assumption is that there exists
a canonical ordering for the sentences of a doc-
ument; therefore we can approximate the degree
of coherence of a document by the similarity be-
tween its actual sentence ordering and that canon-
ical sentence ordering. Practically, we automati-
cally assign an objective score for each permuta-
tion to estimate its dissimilarity from the source
document (see Section 4). By learning from all
the pairs across a source document and its per-
mutations, the effective size of the training data
is increased while no further manual annotation
is required, which is favorable in real applica-
tions when available samples with manually an-
notated coherence scores are usually limited. For
r source documents each with m random permuta-
tions, the number of training instances in the stan-
dard entity-based model is therefore r x m, while
in our multiple-rank model learning process, it is
rx(m21)� 12rxm2&gt;rxm,whenm&gt;2.
</bodyText>
<subsectionHeader confidence="0.999207">
3.2 Summary Coherence Rating
</subsectionHeader>
<bodyText confidence="0.9999216875">
Compared to the standard entity-based coherence
model, our major contribution in this task is to
show that by automatically assigning an objective
score for each machine-generated summary to es-
timate its dissimilarity from the human-generated
summary from the same input document cluster,
we are able to achieve performance competitive
with, or even superior to, that of B&amp;L’s model
without knowing the true coherence score given
by human judges.
Evaluating our multiple-rank model in this task
is crucial, since in summary coherence rating,
the coherence violations that the reader might en-
counter in real machine-generated texts can be
more precisely approximated, while the sentence
ordering task is only partially capable of doing so.
</bodyText>
<page confidence="0.996756">
317
</page>
<sectionHeader confidence="0.996496" genericHeader="method">
4 Dissimilarity Metrics
</sectionHeader>
<bodyText confidence="0.999866722222222">
As mentioned previously, the subtle differences
among the permutations of the same source docu-
ment can be used to refine the model learning pro-
cess. Considering an original document d and one
of its permutations, we call o- = (1, 2, ... , N) the
reference ordering, which is the sentence order-
ing in d, and 7r = (o1, o2, ... , oN) the test order-
ing, which is the sentence ordering in that permu-
tation, where N is the number of sentences being
rendered in both documents.
In order to approximate different degrees of co-
herence among the set of permutations which bear
the same content, we need a suitable metric to
quantify the dissimilarity between the test order-
ing 7r and the reference ordering o-. Such a metric
needs to satisfy the following criteria: (1) It can be
automatically computed while being highly corre-
lated with human judgments of coherence, since
additional manual annotation is certainly undesir-
able. (2) It depends on the particular sentence
ordering in a permutation while remaining inde-
pendent of the entities within the sentences; oth-
erwise our multiple-rank model might be trained
to fit particular probability distributions of entity
transitions rather than true coherence preferences.
In our work we use three different metrics:
Kendall’s T distance, average continuity, and edit
distance.
Kendall’s T distance: This metric has been
widely used in evaluation of sentence ordering
(Lapata, 2003; Lapata, 2006; Bollegala et al.,
2006; Madnani et al., 2007)1. It measures the
disagreement between two orderings o- and 7r in
terms of the number of inversions of adjacent sen-
tences necessary to convert one ordering into an-
other. Kendall’s T distance is defined as
</bodyText>
<equation confidence="0.9806835">
2m
T = N(N − 1),
</equation>
<bodyText confidence="0.994931444444444">
where m is the number of sentence inversions nec-
essary to convert o- to 7r.
Average continuity (AC): Following Zhang
(2011), we use average continuity as the sec-
ond dissimilarity metric. It was first proposed
1Filippova and Strube (2007) found that their perfor-
mance dropped when using this metric for longer rankings;
but they were using data in a different language and with
manual annotations, so its effect on our datasets is worth try-
ing nonetheless.
by Bollegala et al. (2006). This metric esti-
mates the quality of a particular sentence order-
ing by the number of correctly arranged contin-
uous sentences, compared to the reference order-
ing. For example, if 7r = (... , 3,4,5,7, ... , oN),
then {3, 4, 5} is considered as continuous while
{3, 4, 5, 7} is not. Average continuity is calculated
as
</bodyText>
<equation confidence="0.962565">
������
log (Pi + α) �
</equation>
<bodyText confidence="0.999970421052632">
where n = min(4, N) is the maximum number
of continuous sentences to be considered, and
α = 0.01. Pi is the proportion of continuous sen-
tences of length i in 7r that are also continuous in
the reference ordering o-. To represent the dis-
similarity between the two orderings 7r and o-, we
use its complement AC0 = 1 − AC, such that the
larger AC0 is, the more dissimilar two orderings
are2.
Edit distance (ED): Edit distance is a com-
monly used metric in information theory to mea-
sure the difference between two sequences. Given
a test ordering 7r, its edit distance is defined as the
minimum number of edits (i.e., insertions, dele-
tions, and substitutions) needed to transform it
into the reference ordering o-. For permutations,
the edits are essentially movements, which can
be considered as equal numbers of insertions and
deletions.
</bodyText>
<sectionHeader confidence="0.999737" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999677">
5.1 Sentence Ordering
</subsectionHeader>
<bodyText confidence="0.999793333333333">
Our first set of experiments is on sentence order-
ing. Following Barzilay and Lapata (2008), we
use all transitions of length ≤ 3 for feature extrac-
tion. In addition, we explore three specific aspects
in our experiments: rank assignment, entity ex-
traction, and permutation generation.
</bodyText>
<subsubsectionHeader confidence="0.889488">
5.1.1 Rank Assignment
</subsubsectionHeader>
<bodyText confidence="0.999962444444444">
In our multiple-rank model, pairwise rankings
between a source document and its permutations
are extended into a longer ranking with multiple
ranks. We assign a rank to a particular permuta-
tion, based on the result of applying a chosen dis-
similarity metric from Section 4 (T, AC, or ED) to
the sentence ordering in that permutation.
We experiment with two different approaches
to assigning ranks to permutations, while each
</bodyText>
<footnote confidence="0.864634">
2We will refer to AC0 as AC from now on.
</footnote>
<figure confidence="0.947521555555556">
1
AC = exp
������
�
n − 1
n
z
i=2
,
</figure>
<page confidence="0.989498">
318
</page>
<bodyText confidence="0.998257736842106">
source document is always assigned a zero (the
highest) rank.
In the raw option, we rank the permutations di-
rectly by their dissimilarity scores to form a full
ranking for the set of permutations generated from
the same source document.
Since a full ranking might be too sensitive to
noise in training, we also experiment with the
stratified option, in which C ranks are assigned to
the permutations generated from the same source
document. The permutation with the smallest dis-
similarity score is assigned the same (zero, the
highest) rank as the source document, and the one
with the largest score is assigned the lowest (C−1)
rank; then ranks of other permutations are uni-
formly distributed in this range according to their
raw dissimilarity scores. We experiment with 3
to 6 ranks (the case where C = 2 reduces to the
standard entity-based model).
</bodyText>
<subsubsectionHeader confidence="0.855155">
5.1.2 Entity Extraction
</subsubsectionHeader>
<bodyText confidence="0.999964214285714">
Barzilay and Lapata (2008)’s best results were
achieved by employing an automatic coreference
resolution tool (Ng and Cardie, 2002) for ex-
tracting entities from a source document, and the
permutations were generated only afterwards —
entity extraction from a permuted document de-
pends on knowing the correct sentence order and
the oracular entity information from the source
document — since resolving coreference relations
in permuted documents is too unreliable for an au-
tomatic tool.
We implement our multiple-rank model with
full coreference resolution using Ng and Cardie’s
coreference resolution system, and entity extrac-
tion approach as described above — the Coref-
erence+ condition. However, as argued by El-
sner and Charniak (2011), to better simulate
the real situations that human readers might en-
counter in machine-generated documents, such
oracular information should not be taken into ac-
count. Therefore we also employ two alterna-
tive approaches for entity extraction: (1) use the
same automatic coreference resolution tool on
permuted documents — we call it the Corefer-
encet condition; (2) use no coreference reso-
lution, i.e., group head noun clusters by simple
string matching — B&amp;L’s Coreference− condi-
tion.
</bodyText>
<subsubsectionHeader confidence="0.913915">
5.1.3 Permutation Generation
</subsubsectionHeader>
<bodyText confidence="0.999982204545454">
The quality of the model learned depends on
the set of permutations used in training. We are
not aware of how B&amp;L’s permutations were gen-
erated, but we assume they are generated in a per-
fectly random fashion.
However, in reality, the probabilities of seeing
documents with different degrees of coherence are
not equal. For example, in an essay scoring task,
if the target group is (near-) native speakers with
sufficient education, we should expect their essays
to be less incoherent — most of the essays will
be coherent in most parts, with only a few minor
problems regarding discourse coherence. In such
a setting, the performance of a model trained from
permutations generated from a uniform distribu-
tion may suffer some accuracy loss.
Therefore, in addition to the set of permutations
used by Barzilay and Lapata (2008) (PSBL), we
create another set of permutations for each source
document (PSM) by assigning most of the proba-
bility mass to permutations which are mostly sim-
ilar to the original source document. Besides its
capability of better approximating real-life situ-
ations, training our model on permutations gen-
erated in this way has another benefit: in the
standard entity-based model, all permuted doc-
uments are treated as incoherent; thus there are
many more incoherent training instances than co-
herent ones (typically the proportion is 20:1). In
contrast, in our multiple-rank model, permuted
documents are assigned different ranks to fur-
ther differentiate the different degrees of coher-
ence within them. By doing so, our model will
be able to learn the characteristics of a coherent
document from those near-coherent documents as
well, and therefore the problem of lacking coher-
ent instances can be mitigated.
Our permutation generation algorithm is shown
in Algorithm 1, where α = 0.05, /3 = 5.0,
MAX NUM = 50, and K and K&apos; are two normal-
ization factors to make p(swap num) and p(i, j)
proper probability distributions. For each source
document, we create the same number of permu-
tations as PSBL.
</bodyText>
<subsectionHeader confidence="0.996686">
5.2 Summary Coherence Rating
</subsectionHeader>
<bodyText confidence="0.99976775">
In the summary coherence rating task, we are
dealing with a mixture of multi-document sum-
maries generated by systems and written by hu-
mans. Barzilay and Lapata (2008) did not assume
</bodyText>
<page confidence="0.998667">
319
</page>
<figureCaption confidence="0.310243">
Algorithm 1 Permutation Generation.
</figureCaption>
<bodyText confidence="0.955709">
Input: S 1, S2, ... , SN; o- = (1, 2, ... , N)
Choose a number of sentence swaps
swap num with probability e−αxswap num/K
for i = 1 —� swap num do
Swap a pair of sentence (Si, S j)
with probability p(i, j) = e−βx|i−j|/KI
</bodyText>
<subsectionHeader confidence="0.559225">
end for
</subsectionHeader>
<bodyText confidence="0.995780642857143">
Output: 7r = (o1, o2, ... , oN)
a simple binary distinction among the summaries
generated from the same input document clus-
ter; rather, they had human judges give scores for
each summary based on its degree of coherence
(see Section 3.2). Therefore, it seems that the
subtle differences among incoherent documents
(system-generated summaries in this case) have
already been learned by their model.
But we wish to see if we can replace hu-
man judgments by our computed dissimilarity
scores so that the original supervised learning is
converted into unsupervised learning and yet re-
tain competitive performance. However, given
a summary, computing its dissimilarity score is
a bit involved, due to the fact that we do not
know its correct sentence order. To tackle this
problem, we employ a simple sentence align-
ment between a system-generated summary and
a human-written summary originating from the
same input document cluster. Given a system-
generated summary Ds = (S s1, S s2, ... , S sn) and
its corresponding human-written summary Dh =
(Sh1, Sh2, . . . , ShN) (here it is possible that n #
N), we treat the sentence ordering (1, 2, ... , N)
in Dh as o- (the original sentence ordering), and
compute 7r = (o1, o2, ... , on) based on Ds. To
compute each oi in 7r, we find the most similar
sentence S hj, j E [1, N] in Dh by computing their
cosine similarity over all tokens in Shj and S si;
if all sentences in Dh have zero cosine similarity
with S si, we assign −1 to oi.
Once 7r is known, we can compute its “dissimi-
larity” from o- using a chosen metric. But because
now 7r is not guaranteed to be a permutation of o-
(there may be repetition or missing values, i.e.,
−1, in 7r), Kendall’s τ cannot be used, and we use
only average continuity and edit distance as dis-
similarity metrics in this experiment.
The remaining experimental configuration is
the same as that of Barzilay and Lapata (2008),
with the optimal transition length set to &lt;— 2.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="method">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.999863">
6.1 Sentence Ordering
</subsectionHeader>
<bodyText confidence="0.999993411764706">
In this task, we use the same two sets of source
documents (Earthquakes and Accidents, see Sec-
tion 3.1) as Barzilay and Lapata (2008). Each
contains 200 source documents, equally divided
between training and test sets, with up to 20 per-
mutations per document. We conduct experi-
ments on these two domains separately. For each
domain, we accompany each source document
with two different sets of permutations: the one
used by B&amp;L (PSBL), and the one generated from
our model described in Section 5.1.3 (PSM). We
train our multiple-rank model and B&amp;L’s standard
two-rank model on each set of permutations using
the SVMrank package (Joachims, 2006), and eval-
uate both systems on their test sets. Accuracy is
measured as the fraction of correct pairwise rank-
ings for the test set.
</bodyText>
<sectionHeader confidence="0.71349" genericHeader="method">
6.1.1 Full Coreference Resolution with
Oracular Information
</sectionHeader>
<bodyText confidence="0.999150142857143">
In this experiment, we implement B&amp;L’s fully-
fledged standard entity-based coherence model,
and extract entities from permuted documents us-
ing oracular information from the source docu-
ments (see Section 5.1.2).
Results are shown in Table 2. For each test sit-
uation, we list the best accuracy (in Acc columns)
for each chosen dissimilarity metric, with the cor-
responding rank assignment approach. C repre-
sents the number of ranks used in stratifying raw
scores (“N” if using raw configuration, see Sec-
tion 5.1.1 for details). Baselines are accuracies
trained using the standard entity-based coherence
model3.
Our model outperforms the standard entity-
based model on both permutation sets for both
datasets. The improvement is not significant
when trained on the permutation set PSBL, and
is achieved only with one of the three metrics;
3There are discrepancies between our reported accuracies
and those of Barzilay and Lapata (2008). The differences are
due to the fact that we use a different parser: the Stanford de-
pendency parser (de Marneffe et al., 2006), and might have
extracted entities in a slightly different way than theirs, al-
though we keep other experimental configurations as close
as possible to theirs. But when comparing our model with
theirs, we always use the exact same set of features, so the
absolute accuracies do not matter.
</bodyText>
<page confidence="0.99632">
320
</page>
<table confidence="0.972697454545455">
Condition: Coreference+
Perms Metric Earthquakes Accidents
C Acc C Acc
PSBL T 3 79.5 3 82.0
AC 4 85.2 3 83.3
ED 3 86.8 6 82.2
Baseline 85.3 83.2
PSM T 3 86.8 3 85.2*
AC 3 85.6 1 85.4*
ED N 87.9* 4 86.3*
Baseline 85.3 81.7
</table>
<tableCaption confidence="0.913962">
Table 2: Accuracies (%) of extending the stan-
dard entity-based coherence model with multiple-rank
learning in sentence ordering using Coreference+ op-
tion. Accuracies which are significantly better than the
baseline (p &lt; .05) are indicated by *.
</tableCaption>
<table confidence="0.990317181818182">
Condition: Coreferencet
Perms Metric Earthquakes Accidents
C Acc C Acc
PSBL T 3 71.0 3 73.3
AC 3 *76.8 3 74.5
ED 4 *77.4 6 74.4
Baseline 71.7 73.8
PSM T 3 55.9 3 51.5
AC 4 53.9 6 49.0
ED 4 53.9 5 52.3
Baseline 49.2 53.2
</table>
<tableCaption confidence="0.9834708">
Table 3: Accuracies (%) of extending the stan-
dard entity-based coherence model with multiple-rank
learning in sentence ordering using Coreferencet op-
tion. Accuracies which are significantly better than the
baseline (p &lt; .05) are indicated by *.
</tableCaption>
<bodyText confidence="0.999840666666667">
but when trained on PSM (the set of permutations
generated from our biased model), our model’s
performance significantly exceeds B&amp;L’s4 for all
three metrics, especially as their model’s perfor-
mance drops for dataset Accidents.
From these results, we see that in the ideal sit-
uation where we extract entities and resolve their
coreference relations based on the oracular infor-
mation from the source document, our model is
effective in terms of improving ranking accura-
cies, especially when trained on our more realistic
permutation sets PSM.
</bodyText>
<subsectionHeader confidence="0.8819565">
6.1.2 Full Coreference Resolution without
Oracular Information
</subsectionHeader>
<bodyText confidence="0.999915">
In this experiment, we apply the same auto-
matic coreference resolution tool (Ng and Cardie,
2002) on not only the source documents but also
their permutations. We want to see how removing
the oracular component in the original model af-
fects the performance of our multiple-rank model
and the standard model. Results are shown in Ta-
ble 3.
First we can see when trained on PSM, run-
ning full coreference resolution significantly hurts
performance for both models. This suggests that,
in real-life applications, where the distribution of
training instances with different degrees of co-
herence is skewed (as in the set of permutations
</bodyText>
<footnote confidence="0.9668525">
4Following Elsner and Charniak (2011), we use the
Wilcoxon Sign-rank test for significance.
</footnote>
<bodyText confidence="0.999949851851852">
generated from our model), running full corefer-
ence resolution is not a good option, since it al-
most makes the accuracies no better than random
guessing (50%).
Moreover, considering training using PSBL,
running full coreference resolution has a different
influence for the two datasets. For Earthquakes,
our model significantly outperforms B&amp;L’s while
the improvement is insignificant for Accidents.
This is most probably due to the different way that
entities are realized in these two datasets. As an-
alyzed by Barzilay and Lapata (2008), in dataset
Earthquakes, entities tend to be referred to by pro-
nouns in subsequent mentions, while in dataset
Accidents, literal string repetition is more com-
mon.
Given a balanced permutation distribution as
we assumed in PSBL, switching distant sentence
pairs in Accidents may result in very similar en-
tity distribution with the situation of switching
closer sentence pairs, as recognized by the auto-
matic tool. Therefore, compared to Earthquakes,
our multiple-rank model may be less powerful in
indicating the dissimilarity between the sentence
orderings in a permutation and its source docu-
ment, and therefore can improve on the baseline
only by a small margin.
</bodyText>
<subsectionHeader confidence="0.939782">
6.1.3 No Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.9897205">
In this experiment, we do not employ any coref-
erence resolution tool, and simply cluster head
</bodyText>
<page confidence="0.996579">
321
</page>
<table confidence="0.976798272727273">
Condition: Coreference−
Perms Metric Earthquakes Accidents
C Acc C Acc
PSBL T 4 82.8 N 82.0
AC 3 78.0 3 **84.2
ED N 78.2 3 *82.7
Baseline 83.7 80.1
PSM T 3 **86.4 N **85.7
AC 4 *84.4 N **86.6
ED 5 **86.7 N **84.6
Baseline 82.6 77.5
</table>
<tableCaption confidence="0.996235714285714">
Table 4: Accuracies (%) of extending the stan-
dard entity-based coherence model with multiple-rank
learning in sentence ordering using Coreference− op-
tion. Accuracies which are significantly better than the
baseline are indicated by * (p &lt; .05) and ** (p &lt; .01).
nouns by string matching. Results are shown in
Table 4.
</tableCaption>
<bodyText confidence="0.99996255">
Even with such a coarse approximation of
coreference resolution, our model is able to
achieve around 85% accuracy in most test cases,
except for dataset Earthquakes, training on PSBL
gives poorer performance than the standard model
by a small margin. But such inferior perfor-
mance should be expected, because as explained
above, coreference resolution is crucial to this
dataset, since entities tend to be realized through
pronouns; simple string matching introduces too
much noise into training, especially when our
model wants to train a more fine-grained discrim-
inative system than B&amp;L’s. However, we can see
from the result of training on PSM, if the per-
mutations used in training do not involve swap-
ping sentences which are too far away, the result-
ing noise is reduced, and our model outperforms
theirs. And for dataset Accidents, our model
consistently outperforms the baseline model by a
large margin (with significance test at p &lt; .01).
</bodyText>
<subsectionHeader confidence="0.582184">
6.1.4 Conclusions for Sentence Ordering
</subsectionHeader>
<bodyText confidence="0.999941285714286">
Considering the particular dissimilarity metric
used in training, we find that edit distance usually
stands out from the other two metrics. Kendall’s T
distance proves to be a fairly weak metric, which
is consistent with the findings of Filippova and
Strube (2007) (see Section 2.3). Figure 1 plots
the testing accuracies as a function of different
</bodyText>
<figure confidence="0.995788444444444">
88.0
Earthquake ED Coref+
Earthquake ED Coref±
Accidents ED Coref+
Accidents ED Coref±
Accidents τ Coref-
68.0
3 4 5 6 N
C
</figure>
<figureCaption confidence="0.9985715">
Figure 1: Effect of C on testing accuracies in selected
sentence ordering experimental configurations.
</figureCaption>
<bodyText confidence="0.9999681875">
choices of C’s with the configurations where our
model outperforms the baseline model. In each
configuration, we choose the dissimilarity metric
which achieves the best accuracy reported in Ta-
bles 2 to 4 and the PSBL permutation set. We
can see that the dependency of accuracies on the
particular choice of C is not consistent across all
experimental configurations, which suggests that
this free parameter C needs careful tuning in dif-
ferent experimental setups.
Combining our multiple-rank model with sim-
ple string matching for entity extraction is a ro-
bust option for coherence evaluation, regardless
of the particular distribution of permutations used
in training, and it significantly outperforms the
baseline in most conditions.
</bodyText>
<subsectionHeader confidence="0.997364">
6.2 Summary Coherence Rating
</subsectionHeader>
<bodyText confidence="0.9999592">
As explained in Section 3.2, we employ a simple
sentence alignment between a system-generated
summary and its corresponding human-written
summary to construct a test ordering 7r and calcu-
late its dissimilarity between the reference order-
ing o- from the human-written summary. In this
way, we convert B&amp;L’s supervised learning model
into a fully unsupervised model, since human an-
notations for coherence scores are not required.
We use the same dataset as Barzilay and Lap-
ata (2008), which includes multi-document sum-
maries from 16 input document clusters generated
by five systems, along with reference summaries
composed by humans.
In this experiment, we consider only average
continuity (AC) and edit distance (ED) as dissimi-
larity metrics, with raw configuration for rank as-
signment, and compare our multiple-rank model
with the standard entity-based model using ei-
ther full coreference resolution5 or no resolution
</bodyText>
<footnote confidence="0.856176">
5We run the coreference resolution tool on all documents.
</footnote>
<figure confidence="0.99259775">
Accuracy (%)
83.0
78.0
73.0
</figure>
<page confidence="0.987854">
322
</page>
<table confidence="0.999339857142857">
Entities Metric Same Full
AC 82.5 *72.6
Coreference+ ED 81.3 **73.0
Baseline 78.8 70.9
AC 76.3 72.0
Coreference− ED 78.8 71.7
Baseline 80.0 72.3
</table>
<tableCaption confidence="0.938740166666667">
Table 5: Accuracies (%) of extending the stan-
dard entity-based coherence model with multiple-rank
learning in summary rating. Baselines are results of
standard entity-based coherence model. Accuracies
which are significantly better than the corresponding
baseline are indicated by * (p &lt; .05) and ** (p &lt; .01).
</tableCaption>
<bodyText confidence="0.999950369565217">
for entity extraction. We train both models on
the ranking preferences (144 in all) among sum-
maries originating from the same input document
cluster using the SVMrank package (Joachims,
2006), and test on two different test sets: same-
cluster test and full test. Same-cluster test is the
one used by Barzilay and Lapata (2008), in which
only the pairwise rankings (80 in all) between
summaries originating from the same input doc-
ument cluster are tested; we also experiment with
full test, in which pairwise rankings (1520 in all)
between all summary pairs excluding two human-
written summaries are tested.
Results are shown in Table 5. Coreference+
and Coreference− denote the configuration of
using full coreference resolution or no resolu-
tion separately. First, clearly for both models,
performance on full test is inferior to that on
same-cluster test, but our model is still able to
achieve performance competitive with the stan-
dard model, even if our fundamental assumption
about the existence of canonical sentence order-
ing in documents with same content may break
down on those test pairs not originating from the
same input document cluster. Secondly, for the
baseline model, using the Coreference− configu-
ration yields better accuracy in this task (80.0%
vs. 78.8% on same-cluster test, and 72.3% vs.
70.9% on full test), which is consistent with the
findings of Barzilay and Lapata (2008). But our
multiple-rank model seems to favor the Corefer-
ence+ configuration, and our best accuracy even
exceeds B&amp;L’s best when tested on the same set:
82.5% vs. 80.0% on same-cluster test, and 73.0%
vs. 72.3% on full test.
When our model performs poorer than the
baseline (using Coreference− configuration), the
difference is not significant, which suggests that
our multiple-rank model with unsupervised score
assignment via simple cosine matching can re-
main competitive with the standard model, which
requires human annotations to obtain a more fine-
grained coherence spectrum. This observation is
consistent with Banko and Vanderwende (2004)’s
discovery that human-generated summaries look
quite extractive.
</bodyText>
<sectionHeader confidence="0.999086" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999979935483871">
In this paper, we have extended the popular co-
herence model of Barzilay and Lapata (2008) by
adopting a multiple-rank learning approach. This
is inherently different from other extensions to
this model, in which the focus is on enriching
the set of features for entity-grid construction,
whereas we simply keep their original feature set
intact, and manipulate only their learning method-
ology. We show that this concise extension is
effective and able to outperform B&amp;L’s standard
model in various experimental setups, especially
when experimental configurations are most suit-
able considering certain dataset properties (see
discussion in Section 6.1.4).
We experimented with two tasks: sentence or-
dering and summary coherence rating, following
B&amp;L’s original framework. In sentence ordering,
we also explored the influence of removing the
oracular component in their original model and
dealing with permutations generated from differ-
ent distributions, showing that our model is robust
for different experimental situations. In summary
coherence rating, we further extended their model
such that their original supervised learning is con-
verted into unsupervised learning with competi-
tive or even superior performance.
Our multiple-rank learning model can be easily
adapted into other extended entity-based coher-
ence models with their enriched feature sets, and
further improvement in ranking accuracies should
be expected.
</bodyText>
<sectionHeader confidence="0.99885" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993283333333333">
This work was financially supported by the Nat-
ural Sciences and Engineering Research Council
of Canada and by the University of Toronto.
</bodyText>
<page confidence="0.998969">
323
</page>
<sectionHeader confidence="0.995886" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999738717647059">
Michele Banko and Lucy Vanderwende. 2004. Us-
ing n-grams to understand the nature of summaries.
In Proceedings of Human Language Technologies
and North American Association for Computational
Linguistics 2004: Short Papers, pages 1–4.
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the 42rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2005),
pages 141–148.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: an entity-based approach. Compu-
tational Linguistics, 34(1):1–34.
Danushka Bollegala, Naoaki Okazaki, and Mitsuru
Ishizuka. 2006. A bottom-up approach to sen-
tence ordering for multi-document summarization.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Lin-
guistics, pages 385–392.
Jackie Chi Kit Cheung and Gerald Penn. 2010. Entity-
based local coherence modelling using topological
fields. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2010), pages 186–195.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006).
Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2011),
pages 125–129.
Katja Filippova and Michael Strube. 2007. Extend-
ing the entity-grid coherence model to semantically
related entities. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation
(ENLG 2007), pages 139–142.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the 8th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD
2002), pages 133–142.
Thorsten Joachims. 2006. Training linear SVMs
in linear time. In Proceedings of the 12th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD 2006), pages
217–226.
Mirella Lapata. 2003. Probabilistic text structuring:
Experiments with sentence ordering. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL 2003), pages
545–552.
Mirella Lapata. 2006. Automatic evaluation of in-
formation ordering: Kendall’s tau. Computational
Linguistics, 32(4):471–484.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2011), pages 997–1006.
Nitin Madnani, Rebecca Passonneau, Necip Fazil
Ayan, John M. Conroy, Bonnie J. Dorr, Ju-
dith L. Klavans, Dianne P. O’Leary, and Judith D.
Schlesinger. 2007. Measuring variability in sen-
tence ordering for news summarization. In Pro-
ceedings of the Eleventh European Workshop on
Natural Language Generation (ENLG 2007), pages
81–88.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics (ACL 2002),
pages 104–111.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! Computing semantic relatedness using
Wikipedia. In Proceedings of the 21st National
Conference on Artificial Intelligence, pages 1219–
1224.
Renxian Zhang. 2011. Sentence ordering driven by
local and global coherence for summary generation.
In Proceedings of the ACL 2011 Student Session,
pages 6–11.
</reference>
<page confidence="0.999013">
324
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.530360">
<title confidence="0.999384">Extending the Entity-based Coherence Model with Multiple Ranks</title>
<author confidence="0.999983">Vanessa Wei</author>
<affiliation confidence="0.9998505">Department of Computer University of</affiliation>
<address confidence="0.956902">Toronto, ON, M5S 3G4,</address>
<email confidence="0.999866">weifeng@cs.toronto.edu</email>
<author confidence="0.99969">Graeme Hirst</author>
<affiliation confidence="0.9999835">Department of Computer Science University of Toronto</affiliation>
<address confidence="0.999711">Toronto, ON, M5S 3G4, Canada</address>
<email confidence="0.999928">gh@cs.toronto.edu</email>
<abstract confidence="0.9996905">We extend the original entity-based coherence model (Barzilay and Lapata, 2008) by learning from more fine-grained coherence preferences in training data. We associate multiple ranks with the set of permutations originating from the same source document, as opposed to the original pairwise We also study the of the used in training, and the of the coreference component used in entity extraction. With no additional manual annotations required, our extended model is able to outperform the original model on</abstract>
<intro confidence="0.557671">tasks: ordering</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Using n-grams to understand the nature of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of Human Language Technologies and North American Association for Computational Linguistics</booktitle>
<pages>1--4</pages>
<contexts>
<context position="35364" citStr="Banko and Vanderwende (2004)" startWordPosition="5751" endWordPosition="5754">le-rank model seems to favor the Coreference+ configuration, and our best accuracy even exceeds B&amp;L’s best when tested on the same set: 82.5% vs. 80.0% on same-cluster test, and 73.0% vs. 72.3% on full test. When our model performs poorer than the baseline (using Coreference− configuration), the difference is not significant, which suggests that our multiple-rank model with unsupervised score assignment via simple cosine matching can remain competitive with the standard model, which requires human annotations to obtain a more finegrained coherence spectrum. This observation is consistent with Banko and Vanderwende (2004)’s discovery that human-generated summaries look quite extractive. 7 Conclusions In this paper, we have extended the popular coherence model of Barzilay and Lapata (2008) by adopting a multiple-rank learning approach. This is inherently different from other extensions to this model, in which the focus is on enriching the set of features for entity-grid construction, whereas we simply keep their original feature set intact, and manipulate only their learning methodology. We show that this concise extension is effective and able to outperform B&amp;L’s standard model in various experimental setups, </context>
</contexts>
<marker>Banko, Vanderwende, 2004</marker>
<rawString>Michele Banko and Lucy Vanderwende. 2004. Using n-grams to understand the nature of summaries. In Proceedings of Human Language Technologies and North American Association for Computational Linguistics 2004: Short Papers, pages 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 42rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>141--148</pages>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: An entity-based approach. In Proceedings of the 42rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: an entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="3293" citStr="Barzilay and Lapata (2008)" startWordPosition="505" endWordPosition="508">among the permutations themselves. We show that this can be done by assigning a suitable objective score for each permutation indicating its dissimilarity from the original one. We call this a multiple-rank model since we train our model on a multiple-rank basis, rather than taking the original pairwise ranking approach. This extension can also be easily combined with other extensions by incorporating their enriched feature sets. We show that our multiple-rank model outperforms B&amp;L’s basic model on two tasks, sentence ordering and summary coherence rating, evaluated on the same datasets as in Barzilay and Lapata (2008). In sentence ordering, we experiment with different approaches to assigning dissimilarity scores and ranks (Section 5.1.1). We also experiment with different entity extraction approaches 315 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 315–324, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics Manila Miles Island Quake Baco 1 − X X − 2 S − O − − 3 X X X X X Table 1: A fragment of an entity grid for five entities across three sentences. (Section 5.1.2) and different distributions of permut</context>
<context position="5524" citStr="Barzilay and Lapata (2008)" startWordPosition="887" endWordPosition="890">ty grid as continuous subsequences in each column. For example, the entity “Manila” in Table 1 has a bigram transition {S, X} from sentence 2 to 3. The entity grid is then encoded as a feature vector O(d) _ (p1(d), p2(d), ... , pm(d)), where pt(d) is the probability of the transition t in the entity grid, and m is the number of transitions with length no more than a predefined optimal transition length k. pt(d) is computed as the number of occurrences of t in the entity grid of document d, divided by the total number of transitions of the same length in the entity grid. For entity extraction, Barzilay and Lapata (2008) had two conditions: Coreference+ and Coreference−. In Coreference+, entity coreference relations in the document were resolved by an automatic coreference resolution tool (Ng and Cardie, 2002), whereas in Coreference−, nouns are simply clustered by string matching. 2.2 Evaluation Tasks Two evaluation tasks for Barzilay and Lapata (2008)’s entity-based model are sentence ordering and summary coherence rating. In sentence ordering, a set of random permutations is created for each source document, and the learning procedure is conducted on this synthetic mixture of coherent and incoherent docume</context>
<context position="8525" citStr="Barzilay and Lapata (2008)" startWordPosition="1348" endWordPosition="1352">nt, interestingly, a short subsection in 316 their paper described their approach to extending pairwise rankings to longer rankings, by supplying the learner with rankings of all renderings as computed by Kendall’s T, which is one of our extensions considered in this paper. Although Filippova and Strube simply discarded this idea because it hurt accuracies when tested on their data, we found it a promising direction for further exploration. Cheung and Penn (2010) adapted the standard entity-based coherence model to the same German corpus, but replaced the original linguistic dimension used by Barzilay and Lapata (2008) — grammatical role — with topological field information, and showed that for German text, such a modification improves accuracy. For English text, two extensions have been proposed recently. Elsner and Charniak (2011) augmented the original features used in the standard entity-based coherence model with a large number of entity-specific features, and their extension significantly outperformed the standard model on two tasks: document discrimination (another name for sentence ordering), and sentence insertion. Lin et al. (2011) adapted the entity grid representation in the standard model into </context>
<context position="9875" citStr="Barzilay and Lapata (2008)" startWordPosition="1561" endWordPosition="1564">cantly improved ranking accuracies on the same two datasets used by Barzilay and Lapata (2008) as well as on the Wall Street Journal corpus. However, while enriching or modifying the original features used in the standard model is certainly a direction for refinement of the model, it usually requires more training data or a more sophisticated feature representation. In this paper, we instead modify the learning approach and propose a concise and highly adaptive extension that can be easily combined with other extended features or applied to different languages. 3 Experimental Design Following Barzilay and Lapata (2008), we wish to train a discriminative model to give the correct ranking preference between two documents in terms of their degree of coherence. We experiment on the same two tasks as in their work: sentence ordering and summary coherence rating. 3.1 Sentence Ordering In the standard entity-based model, a discriminative system is trained on the pairwise rankings between source documents and their permutations (see Section 2.2). However, a model learned from these pairwise rankings is not sufficiently finegrained, since the subtle differences between the permutations are not learned. Our major con</context>
<context position="15883" citStr="Barzilay and Lapata (2008)" startWordPosition="2554" endWordPosition="2557">arger AC0 is, the more dissimilar two orderings are2. Edit distance (ED): Edit distance is a commonly used metric in information theory to measure the difference between two sequences. Given a test ordering 7r, its edit distance is defined as the minimum number of edits (i.e., insertions, deletions, and substitutions) needed to transform it into the reference ordering o-. For permutations, the edits are essentially movements, which can be considered as equal numbers of insertions and deletions. 5 Experiments 5.1 Sentence Ordering Our first set of experiments is on sentence ordering. Following Barzilay and Lapata (2008), we use all transitions of length ≤ 3 for feature extraction. In addition, we explore three specific aspects in our experiments: rank assignment, entity extraction, and permutation generation. 5.1.1 Rank Assignment In our multiple-rank model, pairwise rankings between a source document and its permutations are extended into a longer ranking with multiple ranks. We assign a rank to a particular permutation, based on the result of applying a chosen dissimilarity metric from Section 4 (T, AC, or ED) to the sentence ordering in that permutation. We experiment with two different approaches to assi</context>
<context position="17506" citStr="Barzilay and Lapata (2008)" startWordPosition="2831" endWordPosition="2834">tive to noise in training, we also experiment with the stratified option, in which C ranks are assigned to the permutations generated from the same source document. The permutation with the smallest dissimilarity score is assigned the same (zero, the highest) rank as the source document, and the one with the largest score is assigned the lowest (C−1) rank; then ranks of other permutations are uniformly distributed in this range according to their raw dissimilarity scores. We experiment with 3 to 6 ranks (the case where C = 2 reduces to the standard entity-based model). 5.1.2 Entity Extraction Barzilay and Lapata (2008)’s best results were achieved by employing an automatic coreference resolution tool (Ng and Cardie, 2002) for extracting entities from a source document, and the permutations were generated only afterwards — entity extraction from a permuted document depends on knowing the correct sentence order and the oracular entity information from the source document — since resolving coreference relations in permuted documents is too unreliable for an automatic tool. We implement our multiple-rank model with full coreference resolution using Ng and Cardie’s coreference resolution system, and entity extra</context>
<context position="19556" citStr="Barzilay and Lapata (2008)" startWordPosition="3155" endWordPosition="3158">m fashion. However, in reality, the probabilities of seeing documents with different degrees of coherence are not equal. For example, in an essay scoring task, if the target group is (near-) native speakers with sufficient education, we should expect their essays to be less incoherent — most of the essays will be coherent in most parts, with only a few minor problems regarding discourse coherence. In such a setting, the performance of a model trained from permutations generated from a uniform distribution may suffer some accuracy loss. Therefore, in addition to the set of permutations used by Barzilay and Lapata (2008) (PSBL), we create another set of permutations for each source document (PSM) by assigning most of the probability mass to permutations which are mostly similar to the original source document. Besides its capability of better approximating real-life situations, training our model on permutations generated in this way has another benefit: in the standard entity-based model, all permuted documents are treated as incoherent; thus there are many more incoherent training instances than coherent ones (typically the proportion is 20:1). In contrast, in our multiple-rank model, permuted documents are</context>
<context position="20944" citStr="Barzilay and Lapata (2008)" startWordPosition="3382" endWordPosition="3385">s of a coherent document from those near-coherent documents as well, and therefore the problem of lacking coherent instances can be mitigated. Our permutation generation algorithm is shown in Algorithm 1, where α = 0.05, /3 = 5.0, MAX NUM = 50, and K and K&apos; are two normalization factors to make p(swap num) and p(i, j) proper probability distributions. For each source document, we create the same number of permutations as PSBL. 5.2 Summary Coherence Rating In the summary coherence rating task, we are dealing with a mixture of multi-document summaries generated by systems and written by humans. Barzilay and Lapata (2008) did not assume 319 Algorithm 1 Permutation Generation. Input: S 1, S2, ... , SN; o- = (1, 2, ... , N) Choose a number of sentence swaps swap num with probability e−αxswap num/K for i = 1 —� swap num do Swap a pair of sentence (Si, S j) with probability p(i, j) = e−βx|i−j|/KI end for Output: 7r = (o1, o2, ... , oN) a simple binary distinction among the summaries generated from the same input document cluster; rather, they had human judges give scores for each summary based on its degree of coherence (see Section 3.2). Therefore, it seems that the subtle differences among incoherent documents (</context>
<context position="23125" citStr="Barzilay and Lapata (2008)" startWordPosition="3781" endWordPosition="3784">d the most similar sentence S hj, j E [1, N] in Dh by computing their cosine similarity over all tokens in Shj and S si; if all sentences in Dh have zero cosine similarity with S si, we assign −1 to oi. Once 7r is known, we can compute its “dissimilarity” from o- using a chosen metric. But because now 7r is not guaranteed to be a permutation of o(there may be repetition or missing values, i.e., −1, in 7r), Kendall’s τ cannot be used, and we use only average continuity and edit distance as dissimilarity metrics in this experiment. The remaining experimental configuration is the same as that of Barzilay and Lapata (2008), with the optimal transition length set to &lt;— 2. 6 Results 6.1 Sentence Ordering In this task, we use the same two sets of source documents (Earthquakes and Accidents, see Section 3.1) as Barzilay and Lapata (2008). Each contains 200 source documents, equally divided between training and test sets, with up to 20 permutations per document. We conduct experiments on these two domains separately. For each domain, we accompany each source document with two different sets of permutations: the one used by B&amp;L (PSBL), and the one generated from our model described in Section 5.1.3 (PSM). We train ou</context>
<context position="24972" citStr="Barzilay and Lapata (2008)" startWordPosition="4077" endWordPosition="4080">mns) for each chosen dissimilarity metric, with the corresponding rank assignment approach. C represents the number of ranks used in stratifying raw scores (“N” if using raw configuration, see Section 5.1.1 for details). Baselines are accuracies trained using the standard entity-based coherence model3. Our model outperforms the standard entitybased model on both permutation sets for both datasets. The improvement is not significant when trained on the permutation set PSBL, and is achieved only with one of the three metrics; 3There are discrepancies between our reported accuracies and those of Barzilay and Lapata (2008). The differences are due to the fact that we use a different parser: the Stanford dependency parser (de Marneffe et al., 2006), and might have extracted entities in a slightly different way than theirs, although we keep other experimental configurations as close as possible to theirs. But when comparing our model with theirs, we always use the exact same set of features, so the absolute accuracies do not matter. 320 Condition: Coreference+ Perms Metric Earthquakes Accidents C Acc C Acc PSBL T 3 79.5 3 82.0 AC 4 85.2 3 83.3 ED 3 86.8 6 82.2 Baseline 85.3 83.2 PSM T 3 86.8 3 85.2* AC 3 85.6 1 8</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: an entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>Naoaki Okazaki</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>A bottom-up approach to sentence ordering for multi-document summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>385--392</pages>
<contexts>
<context position="13854" citStr="Bollegala et al., 2006" startWordPosition="2199" endWordPosition="2202">judgments of coherence, since additional manual annotation is certainly undesirable. (2) It depends on the particular sentence ordering in a permutation while remaining independent of the entities within the sentences; otherwise our multiple-rank model might be trained to fit particular probability distributions of entity transitions rather than true coherence preferences. In our work we use three different metrics: Kendall’s T distance, average continuity, and edit distance. Kendall’s T distance: This metric has been widely used in evaluation of sentence ordering (Lapata, 2003; Lapata, 2006; Bollegala et al., 2006; Madnani et al., 2007)1. It measures the disagreement between two orderings o- and 7r in terms of the number of inversions of adjacent sentences necessary to convert one ordering into another. Kendall’s T distance is defined as 2m T = N(N − 1), where m is the number of sentence inversions necessary to convert o- to 7r. Average continuity (AC): Following Zhang (2011), we use average continuity as the second dissimilarity metric. It was first proposed 1Filippova and Strube (2007) found that their performance dropped when using this metric for longer rankings; but they were using data in a diffe</context>
</contexts>
<marker>Bollegala, Okazaki, Ishizuka, 2006</marker>
<rawString>Danushka Bollegala, Naoaki Okazaki, and Mitsuru Ishizuka. 2006. A bottom-up approach to sentence ordering for multi-document summarization. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 385–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Gerald Penn</author>
</authors>
<title>Entitybased local coherence modelling using topological fields.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>186--195</pages>
<contexts>
<context position="8366" citStr="Cheung and Penn (2010)" startWordPosition="1325" endWordPosition="1328">urther clustered entities by semantic relatedness as computed by the WikiRelated! API (Strube and Ponzetto, 2006). Though the improvement was not significant, interestingly, a short subsection in 316 their paper described their approach to extending pairwise rankings to longer rankings, by supplying the learner with rankings of all renderings as computed by Kendall’s T, which is one of our extensions considered in this paper. Although Filippova and Strube simply discarded this idea because it hurt accuracies when tested on their data, we found it a promising direction for further exploration. Cheung and Penn (2010) adapted the standard entity-based coherence model to the same German corpus, but replaced the original linguistic dimension used by Barzilay and Lapata (2008) — grammatical role — with topological field information, and showed that for German text, such a modification improves accuracy. For English text, two extensions have been proposed recently. Elsner and Charniak (2011) augmented the original features used in the standard entity-based coherence model with a large number of entity-specific features, and their extension significantly outperformed the standard model on two tasks: document di</context>
</contexts>
<marker>Cheung, Penn, 2010</marker>
<rawString>Jackie Chi Kit Cheung and Gerald Penn. 2010. Entitybased local coherence modelling using topological fields. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages 186–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>Extending the entity grid with entity-specific features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>125--129</pages>
<contexts>
<context position="8743" citStr="Elsner and Charniak (2011)" startWordPosition="1383" endWordPosition="1386">which is one of our extensions considered in this paper. Although Filippova and Strube simply discarded this idea because it hurt accuracies when tested on their data, we found it a promising direction for further exploration. Cheung and Penn (2010) adapted the standard entity-based coherence model to the same German corpus, but replaced the original linguistic dimension used by Barzilay and Lapata (2008) — grammatical role — with topological field information, and showed that for German text, such a modification improves accuracy. For English text, two extensions have been proposed recently. Elsner and Charniak (2011) augmented the original features used in the standard entity-based coherence model with a large number of entity-specific features, and their extension significantly outperformed the standard model on two tasks: document discrimination (another name for sentence ordering), and sentence insertion. Lin et al. (2011) adapted the entity grid representation in the standard model into a discourse role matrix, where additional discourse information about the document was encoded. Their extended model significantly improved ranking accuracies on the same two datasets used by Barzilay and Lapata (2008)</context>
<context position="18218" citStr="Elsner and Charniak (2011)" startWordPosition="2938" endWordPosition="2942">g and Cardie, 2002) for extracting entities from a source document, and the permutations were generated only afterwards — entity extraction from a permuted document depends on knowing the correct sentence order and the oracular entity information from the source document — since resolving coreference relations in permuted documents is too unreliable for an automatic tool. We implement our multiple-rank model with full coreference resolution using Ng and Cardie’s coreference resolution system, and entity extraction approach as described above — the Coreference+ condition. However, as argued by Elsner and Charniak (2011), to better simulate the real situations that human readers might encounter in machine-generated documents, such oracular information should not be taken into account. Therefore we also employ two alternative approaches for entity extraction: (1) use the same automatic coreference resolution tool on permuted documents — we call it the Coreferencet condition; (2) use no coreference resolution, i.e., group head noun clusters by simple string matching — B&amp;L’s Coreference− condition. 5.1.3 Permutation Generation The quality of the model learned depends on the set of permutations used in training. </context>
<context position="27597" citStr="Elsner and Charniak (2011)" startWordPosition="4519" endWordPosition="4522">omatic coreference resolution tool (Ng and Cardie, 2002) on not only the source documents but also their permutations. We want to see how removing the oracular component in the original model affects the performance of our multiple-rank model and the standard model. Results are shown in Table 3. First we can see when trained on PSM, running full coreference resolution significantly hurts performance for both models. This suggests that, in real-life applications, where the distribution of training instances with different degrees of coherence is skewed (as in the set of permutations 4Following Elsner and Charniak (2011), we use the Wilcoxon Sign-rank test for significance. generated from our model), running full coreference resolution is not a good option, since it almost makes the accuracies no better than random guessing (50%). Moreover, considering training using PSBL, running full coreference resolution has a different influence for the two datasets. For Earthquakes, our model significantly outperforms B&amp;L’s while the improvement is insignificant for Accidents. This is most probably due to the different way that entities are realized in these two datasets. As analyzed by Barzilay and Lapata (2008), in da</context>
</contexts>
<marker>Elsner, Charniak, 2011</marker>
<rawString>Micha Elsner and Eugene Charniak. 2011. Extending the entity grid with entity-specific features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011), pages 125–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Extending the entity-grid coherence model to semantically related entities.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh European Workshop on Natural Language Generation (ENLG</booktitle>
<pages>139--142</pages>
<contexts>
<context position="7581" citStr="Filippova and Strube (2007)" startWordPosition="1207" endWordPosition="1210">ather than training and evaluating on a set of synthetic data, system-generated summaries and human-composed reference summaries from the Document Understanding Conference (DUC 2003) were used. Human annotators were asked to give a coherence score on a seven-point scale for each item. The pairwise ranking preferences between summaries generated from the same input document cluster (excluding the pairs consisting of two human-written summaries) are used by a support vector machine ranker to learn a discriminant function to rank each pair according to their coherence scores. 2.3 Extended Models Filippova and Strube (2007) applied Barzilay and Lapata’s model on a German corpus of newspaper articles with manual syntactic, morphological, and NP coreference annotations provided. They further clustered entities by semantic relatedness as computed by the WikiRelated! API (Strube and Ponzetto, 2006). Though the improvement was not significant, interestingly, a short subsection in 316 their paper described their approach to extending pairwise rankings to longer rankings, by supplying the learner with rankings of all renderings as computed by Kendall’s T, which is one of our extensions considered in this paper. Althoug</context>
<context position="14337" citStr="Filippova and Strube (2007)" startWordPosition="2283" endWordPosition="2286">e. Kendall’s T distance: This metric has been widely used in evaluation of sentence ordering (Lapata, 2003; Lapata, 2006; Bollegala et al., 2006; Madnani et al., 2007)1. It measures the disagreement between two orderings o- and 7r in terms of the number of inversions of adjacent sentences necessary to convert one ordering into another. Kendall’s T distance is defined as 2m T = N(N − 1), where m is the number of sentence inversions necessary to convert o- to 7r. Average continuity (AC): Following Zhang (2011), we use average continuity as the second dissimilarity metric. It was first proposed 1Filippova and Strube (2007) found that their performance dropped when using this metric for longer rankings; but they were using data in a different language and with manual annotations, so its effect on our datasets is worth trying nonetheless. by Bollegala et al. (2006). This metric estimates the quality of a particular sentence ordering by the number of correctly arranged continuous sentences, compared to the reference ordering. For example, if 7r = (... , 3,4,5,7, ... , oN), then {3, 4, 5} is considered as continuous while {3, 4, 5, 7} is not. Average continuity is calculated as ������ log (Pi + α) � where n = min(4</context>
<context position="30785" citStr="Filippova and Strube (2007)" startWordPosition="5035" endWordPosition="5038">esult of training on PSM, if the permutations used in training do not involve swapping sentences which are too far away, the resulting noise is reduced, and our model outperforms theirs. And for dataset Accidents, our model consistently outperforms the baseline model by a large margin (with significance test at p &lt; .01). 6.1.4 Conclusions for Sentence Ordering Considering the particular dissimilarity metric used in training, we find that edit distance usually stands out from the other two metrics. Kendall’s T distance proves to be a fairly weak metric, which is consistent with the findings of Filippova and Strube (2007) (see Section 2.3). Figure 1 plots the testing accuracies as a function of different 88.0 Earthquake ED Coref+ Earthquake ED Coref± Accidents ED Coref+ Accidents ED Coref± Accidents τ Coref68.0 3 4 5 6 N C Figure 1: Effect of C on testing accuracies in selected sentence ordering experimental configurations. choices of C’s with the configurations where our model outperforms the baseline model. In each configuration, we choose the dissimilarity metric which achieves the best accuracy reported in Tables 2 to 4 and the PSBL permutation set. We can see that the dependency of accuracies on the parti</context>
</contexts>
<marker>Filippova, Strube, 2007</marker>
<rawString>Katja Filippova and Michael Strube. 2007. Extending the entity-grid coherence model to semantically related entities. In Proceedings of the Eleventh European Workshop on Natural Language Generation (ENLG 2007), pages 139–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD</booktitle>
<pages>133--142</pages>
<contexts>
<context position="6618" citStr="Joachims, 2002" startWordPosition="1058" endWordPosition="1059">each source document, and the learning procedure is conducted on this synthetic mixture of coherent and incoherent documents. Barzilay and Lapata (2008) experimented on two datasets: news articles on the topic of earthquakes (Earthquakes) and narratives on the topic of aviation accidents (Accidents). A training data instance is constructed as a pair consisting of a source document and one of its random permutations, and the permuted document is always considered to be less coherent than the source document. The entity transition features are then used to train a support vector machine ranker (Joachims, 2002) to rank the source documents higher than the permutations. The model is tested on a different set of source documents and their permutations, and the performance is evaluated as the fraction of correct pairwise rankings in the test set. In summary coherence rating, a similar experimental framework is adopted. However, in this task, rather than training and evaluating on a set of synthetic data, system-generated summaries and human-composed reference summaries from the Document Understanding Conference (DUC 2003) were used. Human annotators were asked to give a coherence score on a seven-point</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2002), pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear SVMs in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD</booktitle>
<pages>217--226</pages>
<contexts>
<context position="23851" citStr="Joachims, 2006" startWordPosition="3903" endWordPosition="3904"> two sets of source documents (Earthquakes and Accidents, see Section 3.1) as Barzilay and Lapata (2008). Each contains 200 source documents, equally divided between training and test sets, with up to 20 permutations per document. We conduct experiments on these two domains separately. For each domain, we accompany each source document with two different sets of permutations: the one used by B&amp;L (PSBL), and the one generated from our model described in Section 5.1.3 (PSM). We train our multiple-rank model and B&amp;L’s standard two-rank model on each set of permutations using the SVMrank package (Joachims, 2006), and evaluate both systems on their test sets. Accuracy is measured as the fraction of correct pairwise rankings for the test set. 6.1.1 Full Coreference Resolution with Oracular Information In this experiment, we implement B&amp;L’s fullyfledged standard entity-based coherence model, and extract entities from permuted documents using oracular information from the source documents (see Section 5.1.2). Results are shown in Table 2. For each test situation, we list the best accuracy (in Acc columns) for each chosen dissimilarity metric, with the corresponding rank assignment approach. C represents </context>
<context position="33514" citStr="Joachims, 2006" startWordPosition="5461" endWordPosition="5462">5 *72.6 Coreference+ ED 81.3 **73.0 Baseline 78.8 70.9 AC 76.3 72.0 Coreference− ED 78.8 71.7 Baseline 80.0 72.3 Table 5: Accuracies (%) of extending the standard entity-based coherence model with multiple-rank learning in summary rating. Baselines are results of standard entity-based coherence model. Accuracies which are significantly better than the corresponding baseline are indicated by * (p &lt; .05) and ** (p &lt; .01). for entity extraction. We train both models on the ranking preferences (144 in all) among summaries originating from the same input document cluster using the SVMrank package (Joachims, 2006), and test on two different test sets: samecluster test and full test. Same-cluster test is the one used by Barzilay and Lapata (2008), in which only the pairwise rankings (80 in all) between summaries originating from the same input document cluster are tested; we also experiment with full test, in which pairwise rankings (1520 in all) between all summary pairs excluding two humanwritten summaries are tested. Results are shown in Table 5. Coreference+ and Coreference− denote the configuration of using full coreference resolution or no resolution separately. First, clearly for both models, per</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2006), pages 217–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>545--552</pages>
<contexts>
<context position="13816" citStr="Lapata, 2003" startWordPosition="2195" endWordPosition="2196">ighly correlated with human judgments of coherence, since additional manual annotation is certainly undesirable. (2) It depends on the particular sentence ordering in a permutation while remaining independent of the entities within the sentences; otherwise our multiple-rank model might be trained to fit particular probability distributions of entity transitions rather than true coherence preferences. In our work we use three different metrics: Kendall’s T distance, average continuity, and edit distance. Kendall’s T distance: This metric has been widely used in evaluation of sentence ordering (Lapata, 2003; Lapata, 2006; Bollegala et al., 2006; Madnani et al., 2007)1. It measures the disagreement between two orderings o- and 7r in terms of the number of inversions of adjacent sentences necessary to convert one ordering into another. Kendall’s T distance is defined as 2m T = N(N − 1), where m is the number of sentence inversions necessary to convert o- to 7r. Average continuity (AC): Following Zhang (2011), we use average continuity as the second dissimilarity metric. It was first proposed 1Filippova and Strube (2007) found that their performance dropped when using this metric for longer ranking</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003), pages 545–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Automatic evaluation of information ordering: Kendall’s tau.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="13830" citStr="Lapata, 2006" startWordPosition="2197" endWordPosition="2198">ed with human judgments of coherence, since additional manual annotation is certainly undesirable. (2) It depends on the particular sentence ordering in a permutation while remaining independent of the entities within the sentences; otherwise our multiple-rank model might be trained to fit particular probability distributions of entity transitions rather than true coherence preferences. In our work we use three different metrics: Kendall’s T distance, average continuity, and edit distance. Kendall’s T distance: This metric has been widely used in evaluation of sentence ordering (Lapata, 2003; Lapata, 2006; Bollegala et al., 2006; Madnani et al., 2007)1. It measures the disagreement between two orderings o- and 7r in terms of the number of inversions of adjacent sentences necessary to convert one ordering into another. Kendall’s T distance is defined as 2m T = N(N − 1), where m is the number of sentence inversions necessary to convert o- to 7r. Average continuity (AC): Following Zhang (2011), we use average continuity as the second dissimilarity metric. It was first proposed 1Filippova and Strube (2007) found that their performance dropped when using this metric for longer rankings; but they we</context>
</contexts>
<marker>Lapata, 2006</marker>
<rawString>Mirella Lapata. 2006. Automatic evaluation of information ordering: Kendall’s tau. Computational Linguistics, 32(4):471–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>Automatically evaluating text coherence using discourse relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>997--1006</pages>
<contexts>
<context position="9058" citStr="Lin et al. (2011)" startWordPosition="1429" endWordPosition="1432">s, but replaced the original linguistic dimension used by Barzilay and Lapata (2008) — grammatical role — with topological field information, and showed that for German text, such a modification improves accuracy. For English text, two extensions have been proposed recently. Elsner and Charniak (2011) augmented the original features used in the standard entity-based coherence model with a large number of entity-specific features, and their extension significantly outperformed the standard model on two tasks: document discrimination (another name for sentence ordering), and sentence insertion. Lin et al. (2011) adapted the entity grid representation in the standard model into a discourse role matrix, where additional discourse information about the document was encoded. Their extended model significantly improved ranking accuracies on the same two datasets used by Barzilay and Lapata (2008) as well as on the Wall Street Journal corpus. However, while enriching or modifying the original features used in the standard model is certainly a direction for refinement of the model, it usually requires more training data or a more sophisticated feature representation. In this paper, we instead modify the lea</context>
</contexts>
<marker>Lin, Ng, Kan, 2011</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011), pages 997–1006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Rebecca Passonneau</author>
<author>Necip Fazil Ayan</author>
<author>John M Conroy</author>
<author>Bonnie J Dorr</author>
<author>Judith L Klavans</author>
<author>Dianne P O’Leary</author>
<author>Judith D Schlesinger</author>
</authors>
<title>Measuring variability in sentence ordering for news summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh European Workshop on Natural Language Generation (ENLG</booktitle>
<pages>81--88</pages>
<marker>Madnani, Passonneau, Ayan, Conroy, Dorr, Klavans, O’Leary, Schlesinger, 2007</marker>
<rawString>Nitin Madnani, Rebecca Passonneau, Necip Fazil Ayan, John M. Conroy, Bonnie J. Dorr, Judith L. Klavans, Dianne P. O’Leary, and Judith D. Schlesinger. 2007. Measuring variability in sentence ordering for news summarization. In Proceedings of the Eleventh European Workshop on Natural Language Generation (ENLG 2007), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL</booktitle>
<pages>104--111</pages>
<contexts>
<context position="5717" citStr="Ng and Cardie, 2002" startWordPosition="914" endWordPosition="917"> O(d) _ (p1(d), p2(d), ... , pm(d)), where pt(d) is the probability of the transition t in the entity grid, and m is the number of transitions with length no more than a predefined optimal transition length k. pt(d) is computed as the number of occurrences of t in the entity grid of document d, divided by the total number of transitions of the same length in the entity grid. For entity extraction, Barzilay and Lapata (2008) had two conditions: Coreference+ and Coreference−. In Coreference+, entity coreference relations in the document were resolved by an automatic coreference resolution tool (Ng and Cardie, 2002), whereas in Coreference−, nouns are simply clustered by string matching. 2.2 Evaluation Tasks Two evaluation tasks for Barzilay and Lapata (2008)’s entity-based model are sentence ordering and summary coherence rating. In sentence ordering, a set of random permutations is created for each source document, and the learning procedure is conducted on this synthetic mixture of coherent and incoherent documents. Barzilay and Lapata (2008) experimented on two datasets: news articles on the topic of earthquakes (Earthquakes) and narratives on the topic of aviation accidents (Accidents). A training d</context>
<context position="17611" citStr="Ng and Cardie, 2002" startWordPosition="2846" endWordPosition="2849">permutations generated from the same source document. The permutation with the smallest dissimilarity score is assigned the same (zero, the highest) rank as the source document, and the one with the largest score is assigned the lowest (C−1) rank; then ranks of other permutations are uniformly distributed in this range according to their raw dissimilarity scores. We experiment with 3 to 6 ranks (the case where C = 2 reduces to the standard entity-based model). 5.1.2 Entity Extraction Barzilay and Lapata (2008)’s best results were achieved by employing an automatic coreference resolution tool (Ng and Cardie, 2002) for extracting entities from a source document, and the permutations were generated only afterwards — entity extraction from a permuted document depends on knowing the correct sentence order and the oracular entity information from the source document — since resolving coreference relations in permuted documents is too unreliable for an automatic tool. We implement our multiple-rank model with full coreference resolution using Ng and Cardie’s coreference resolution system, and entity extraction approach as described above — the Coreference+ condition. However, as argued by Elsner and Charniak</context>
<context position="27027" citStr="Ng and Cardie, 2002" startWordPosition="4427" endWordPosition="4430"> our model’s performance significantly exceeds B&amp;L’s4 for all three metrics, especially as their model’s performance drops for dataset Accidents. From these results, we see that in the ideal situation where we extract entities and resolve their coreference relations based on the oracular information from the source document, our model is effective in terms of improving ranking accuracies, especially when trained on our more realistic permutation sets PSM. 6.1.2 Full Coreference Resolution without Oracular Information In this experiment, we apply the same automatic coreference resolution tool (Ng and Cardie, 2002) on not only the source documents but also their permutations. We want to see how removing the oracular component in the original model affects the performance of our multiple-rank model and the standard model. Results are shown in Table 3. First we can see when trained on PSM, running full coreference resolution significantly hurts performance for both models. This suggests that, in real-life applications, where the distribution of training instances with different degrees of coherence is skewed (as in the set of permutations 4Following Elsner and Charniak (2011), we use the Wilcoxon Sign-ran</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL 2002), pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Wikirelate! Computing semantic relatedness using Wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence,</booktitle>
<pages>1219--1224</pages>
<contexts>
<context position="7857" citStr="Strube and Ponzetto, 2006" startWordPosition="1246" endWordPosition="1249">ch item. The pairwise ranking preferences between summaries generated from the same input document cluster (excluding the pairs consisting of two human-written summaries) are used by a support vector machine ranker to learn a discriminant function to rank each pair according to their coherence scores. 2.3 Extended Models Filippova and Strube (2007) applied Barzilay and Lapata’s model on a German corpus of newspaper articles with manual syntactic, morphological, and NP coreference annotations provided. They further clustered entities by semantic relatedness as computed by the WikiRelated! API (Strube and Ponzetto, 2006). Though the improvement was not significant, interestingly, a short subsection in 316 their paper described their approach to extending pairwise rankings to longer rankings, by supplying the learner with rankings of all renderings as computed by Kendall’s T, which is one of our extensions considered in this paper. Although Filippova and Strube simply discarded this idea because it hurt accuracies when tested on their data, we found it a promising direction for further exploration. Cheung and Penn (2010) adapted the standard entity-based coherence model to the same German corpus, but replaced </context>
</contexts>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>Michael Strube and Simone Paolo Ponzetto. 2006. Wikirelate! Computing semantic relatedness using Wikipedia. In Proceedings of the 21st National Conference on Artificial Intelligence, pages 1219– 1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renxian Zhang</author>
</authors>
<title>Sentence ordering driven by local and global coherence for summary generation.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL 2011 Student Session,</booktitle>
<pages>6--11</pages>
<contexts>
<context position="14223" citStr="Zhang (2011)" startWordPosition="2267" endWordPosition="2268">our work we use three different metrics: Kendall’s T distance, average continuity, and edit distance. Kendall’s T distance: This metric has been widely used in evaluation of sentence ordering (Lapata, 2003; Lapata, 2006; Bollegala et al., 2006; Madnani et al., 2007)1. It measures the disagreement between two orderings o- and 7r in terms of the number of inversions of adjacent sentences necessary to convert one ordering into another. Kendall’s T distance is defined as 2m T = N(N − 1), where m is the number of sentence inversions necessary to convert o- to 7r. Average continuity (AC): Following Zhang (2011), we use average continuity as the second dissimilarity metric. It was first proposed 1Filippova and Strube (2007) found that their performance dropped when using this metric for longer rankings; but they were using data in a different language and with manual annotations, so its effect on our datasets is worth trying nonetheless. by Bollegala et al. (2006). This metric estimates the quality of a particular sentence ordering by the number of correctly arranged continuous sentences, compared to the reference ordering. For example, if 7r = (... , 3,4,5,7, ... , oN), then {3, 4, 5} is considered </context>
</contexts>
<marker>Zhang, 2011</marker>
<rawString>Renxian Zhang. 2011. Sentence ordering driven by local and global coherence for summary generation. In Proceedings of the ACL 2011 Student Session, pages 6–11.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>