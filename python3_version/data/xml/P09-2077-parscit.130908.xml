<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.080506">
<title confidence="0.731565">
Extending a Surface Realizer to Generate Coherent Discourse
</title>
<author confidence="0.966881">
Eva Banik
</author>
<affiliation confidence="0.958613">
The Open University
</affiliation>
<address confidence="0.664708">
Milton Keynes, UK
</address>
<email confidence="0.992561">
e.banik@open.ac.uk
</email>
<sectionHeader confidence="0.997312" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999620333333333">
We present a discourse-level Tree Adjoin-
ing Grammar which tightly integrates syn-
tax and discourse levels, including a repre-
sentation for discourse entities. We show
that this technique makes it possible to
extend an optimisation algorithm used in
natural language generation (polarity fil-
tering) to the discourse level. We imple-
mented the grammar in a surface realizer
and show that this technique can be used
to reduce the search space by filtering out
referentially incoherent solutions.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999991105263158">
A fundamental problem that microplanners and
surface realizers face in natural language gener-
ation is how to restrict the search space of possi-
ble solutions. A traditional solution to this compu-
tational complexity problem is to divide the gen-
eration process into tractable sub-problems, each
represented as a module in a pipeline, where every
decision made by a module restricts the number of
options available to others further down the line.
Though such pipeline architectures are computa-
tionally efficient, they severely restrict the flexibil-
ity of the system and the quality of the generated
output. Most systems with pipeline architectures
generate relatively simple, domain-specific out-
put. Systems that produce more complex linguis-
tic constructions typically achieve this by adding
more modules to the pipeline (e.g. a revision mod-
ule (Robin, 1994) or aggregation (Shaw, 2002)).
Since complex linguistic constructions often re-
quire interaction between modules, adding them to
the repertoire of pipelined NLG systems becomes
an engineering and programming task.
Integrated NLG systems have a simpler archi-
tecture because they do not need to model in-
teractions between modules. However, they still
face the problem of computational complexity
that was originally solved by the pipeline model.
Strategies that have been introduced to reduce
the search space in integrated systems include
greedy/incremental search algorithms (Stone et
al., 2003), constructing a dependency graph for a
flat semantic input and converting it into a deriva-
tion tree (Koller and Striegnitz, 2002), using plan-
ning algorithms (Appelt, 1985; Koller and Stone,
2007), polarity filtering (Kow, 2007) and using
underspecified g-derivation trees (G-TAG, Danlos
(2000)). Despite all these efforts, most systems
still don’t attempt to go above the sentence level
or generate very complex sentences. In this pa-
per we present a new technique for designing an
integrated grammar for natural language genera-
tion. Using this technique it is possible to use lin-
guistic constraints on referential coherence to au-
tomatically reduce the search space — which in
turn makes it possible to generate longer and more
coherent texts.
First we extend the grammar of a surface real-
izer to produce complex, multi-sentential output.
Then we add a representation for discourse refer-
ents to the grammar, inspired by Centering The-
ory’s notion of a backward looking center and pre-
ferred center. Having done this, we show that by
integrating discourse-level representations into a
syntactic grammar we can extend an optimization
technique — polarity filtering (Kow, 2007; Gar-
dent and Kow, 2006) — from syntactic realization
to the discourse level.
</bodyText>
<sectionHeader confidence="0.970353" genericHeader="method">
2 The Problem of Referential Coherence
</sectionHeader>
<bodyText confidence="0.99036625">
Referential coherence is the phenomenon which is
responsible for the contrast in (1), in the sense that
the example in (1b) is perceived to be more coher-
ent than (1a).
</bodyText>
<listItem confidence="0.881611">
(1) a Elixir is approved by the FDA. Viral
</listItem>
<bodyText confidence="0.835737">
skin disorders are relieved by
</bodyText>
<page confidence="0.989859">
305
</page>
<note confidence="0.9282265">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 305–308,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.993359674418605">
Aliprosan. Elixir is a white cream.
Aliprosan is an ingredient of Elixir.
b Elixir is a white cream. Elixir is
approved by the FDA. Elixir contains
Aliprosan. Aliprosan relieves viral skin
disorders.
Centering Theory (Grosz et al., 1995) is a fre-
quently used framework for modeling referential
coherence in discourse. It is based on the no-
tion that for each utterance in a discourse there
is a set of entities which are the centers of atten-
tion and which serve to link that utterance to other
utterances in the same discourse segment. Enti-
ties mentioned by an utterance (the set of forward
looking centers) form a partially ordered list called
the Cf list where roughly, subjects are ranked high-
est, followed by objects, indirect objects and other
arguments or adjuncts. The backward looking
center of Un is said to be the most highly ranked
element on the Cf list of Un-1 mentioned in the
previous utterance.
Centering Theory has been adapted to NLG by
Kibble (1999; 2001), and implemented in Kib-
ble and Power (2004). Rather than using the no-
tion of centering transitions as defined by Grosz et
al. (1995), in these papers centering theory is re-
defined as constraints on salience and cohesion.
These constraints state that there is a preference
for consecutive utterances to keep the same center
and that there is a preference for the center of Un
to be realized as the highest ranked entity on the
Cf list of Un. Kibble and Power (2004) show how
these constraints can be used to drive text plan-
ning, sentence planning and pronominalization in
an integrated fashion. Our approach is similar to
Kibble and Power (2004) in that we don’t use the
concept of centering transitions. However, our
method is more efficient in that Kibble and Power
(2004) use centering transitions to rank the set of
generated solutions (some of which are incoher-
ent), whereas we encode centering constraints in
elementary trees to reduce the search space of pos-
sible solutions before we start computing them.
</bodyText>
<sectionHeader confidence="0.995622" genericHeader="method">
3 GenI and Polarity Filtering
</sectionHeader>
<bodyText confidence="0.999931166666666">
The grammar described in the next section was
implemented in the GenI surface realizer (Kow,
2007), which uses a lexicalized feature-based Tree
Adjoining Grammar to generate all possible para-
phrases for a given flat semantic input. GenI im-
plements an optimization technique called polar-
</bodyText>
<figureCaption confidence="0.999298">
Figure 1: Elementary syntax/discourse trees
</figureCaption>
<bodyText confidence="0.999967">
ity filtering to constrain the effects of lexical am-
biguity. The basic idea of polarity filtering is to
associate elementary trees with a set of polarities.
When these polarities don’t ‘cancel each other
out’, it means that it is not possible to combine
the set of trees selected for a given input. This is
a quick way to check whether the number of ar-
gument slots is the same as the number of poten-
tial arguments. For example, if the lexical selec-
tion consists of two trees for a given input, one of
which provides an NP (-NP) and one of which ex-
pects two NPs (-2NP) then the sum of polarities
will be -NP and therefore the generator will not
attempt to combine the trees.
Values for polarities are defined as follows: ev-
ery initial tree is assigned a -cat polarity for each
substitution node of category cat and a +cat po-
larity if its root node is of category cat. Auxiliary
trees are assigned a -cat polarity for each substi-
tution node only.
Polarity filtering is a very powerful optimiza-
tion technique, because it allows the generator to
reduce the search space early on in the process,
before it attempts to combine any trees.
</bodyText>
<sectionHeader confidence="0.9544965" genericHeader="method">
4 An Integrated Syntax-Discourse
Grammar
</sectionHeader>
<bodyText confidence="0.999584666666667">
In order to generate mutisentential text, we first
define a discourse-level Tree Adjoining Gram-
mar. The trees in the grammar tightly integrate
syntax and discourse representations in the sense
that sentence-level elementary trees include one
or more discourse-level nodes. The elementary
trees in Fig. 1 illustrate what we mean by this:
every lexical item that would normally project a
sentence in a syntactic grammar (i.e., an S-rooted
</bodyText>
<figure confidence="0.987587161290322">
h1:white-cream(e)
D,
✟✟✟ ❍ ❍ ❍
S
✟✟ ❍ ❍
NP↓
[idx:e]
is cream
[idx:e]
Punct
.
VP
✟✟ ❍❍
V NP
h2:contain(e,a)
D, [c:e]
✟✟ ❍❍
D, ↓ D,
[c:e] ✟✟ ❍ ❍
S Punct
✟✟ ❍❍
contains
.
VP
✟✟ ❍ ❍
NP↓
[idx:e]
V
NP↓
[idx:a]
306
+e +a -v +e -e +a -e
.
.
.
S
✟✟✟✟ ❍ ❍ ❍ ❍
S
✟✟ ❍ ❍
S
✟✟ ❍ ❍
.
VP
✟✟ ❍ ❍
NP↓
[arg:a]
NP↓
[arg:e]
VP
✟✟ ❍ ❍
VP
✟✟❍❍
NP↓
[arg:e]
NP
V
is
a cream
V NP↓
[arg:a]
relieved by
V NP↓
[arg:e]
is ingredient
of
Dc [c:a]
❍
✟✟✟✟ ❍ ❍ ❍
Dc ↓
[c:v]
NP↓
[arg:v]
Dc [c:a]
✟✟✟✟ ❍❍ ❍ ❍
Dc ↓
[c:e]
Dc [c:e]
✟✟✟✟ ❍ ❍ ❍ ❍
Dc ↓
[c:e]
Dc [c:e]
✟✟ ❍❍
S
✟✟ ❍ ❍
VP
✟✟ ❍ ❍
V NP↓
[arg:f]
approved by
h3:approve(f,e) h6:relieve(a,v) h0:cream(e) h4:contain(e,a)
+2a -v
Elixir is approved by the FDA. Viral skin disorders are relieved by Aliprosan. Elixir is a white cream.
Aliprosan is an ingredient of Elixir.
</figure>
<figureCaption confidence="0.999965">
Figure 2: Discourse-level polarities for (1a) sum up to +2a -v
Figure 3: Discourse-level polarities for (1b) sum up to +a
</figureCaption>
<figure confidence="0.997607583333333">
+e -e +a -a +e +a -e
h3:approve(f,e) h6:relieve(a,v) h0:cream(e) h4:contain(e,a)
+a
Elixir is a white cream. Elixir is approved by the FDA. Elixir contains Aliprosan. Aliprosan relieves
viral skin disorders.
NP↓ VP
[arg:e]✟✟ ❍❍
S
✟✟✟✟ ❍❍ ❍ ❍
Dc [c:e]
✟✟ ❍❍
S
✟✟ ❍ ❍
Dc [c:e]
✟✟✟✟ ❍ ❍ ❍ ❍
contains
a cream
is
NP↓
[arg:a]
V
NP↓
[arg:v]
V
V
NP
.
VP
✟✟ ❍ ❍
NP↓
[arg:e]
VP
✟✟❍❍
NP↓
[arg:e]
VP
✟✟ ❍❍
.
.
S
✟✟ ❍ ❍
Dc ↓
[c:a]
Dc ↓
[c:e]
.
V NP↓
[arg:f]
approved by
Dc [c:a]
✟✟✟✟ ❍❍ ❍ ❍
Dc ↓
[c:e]
Dc [c:a]
✟✟✟✟ ❍ ❍ ❍ ❍
S
✟✟ ❍ ❍
NP↓
[arg:a]
relieves
</figure>
<bodyText confidence="0.999625785714286">
tree) here projects a discourse clause (i.e., a Dc
rooted tree). Every predicate that projects a dis-
course clause is assigned two kinds of elementary
trees: a discourse initial tree (Fig. 1a) and a dis-
course continuing tree (Fig. 1b), which takes the
preceding discourse clause as an argument.
We model referential coherence by associating
a discourse entity with every root- and substitution
node of category Dc. A discourse entity on a root
node is “exported” by the elementary tree to be
the center of attention in the next sentence. This
roughly corresponds to Centering Theory’s notion
of a forward looking center. A discourse entity on
a substitution node is the entity expected by the
sentence to have been the center of attention in
the previous utterance, roughly corresponding to
the notion of backward looking center in Center-
ing Theory.
For example, the tree on the left in Fig. 1. ex-
ports the discourse entity representing its subject
(‘e’) as its “forward looking center”. The tree on
the right in Fig. 1. is looking for a discourse en-
tity called ‘e’ as its “backward looking center” and
exports the same discourse entity as its “forward
looking center”. The combination of these two
trees therefore yields a coherent discourse, which
is expected to be continued with an utterance cen-
tered on ‘e’.
</bodyText>
<sectionHeader confidence="0.976403" genericHeader="method">
5 Polarity Filtering on Discourse Entities
</sectionHeader>
<bodyText confidence="0.999959375">
By treating discourse entities on Dc nodes as an
additional polarity key we can apply the polarity
filtering technique on the discourse level. This
means we can filter out lexical selections that
wouldn’t lead to a coherent discourse the same
way as those lexical selections are filtered out
which won’t lead to a syntactically well formed
sentence. To give an example, given the semantic
representation in Figure 4 potential realizations by
a generator which is not aware of discourse coher-
ence would include both of the examples in (1).
As an experiment, we generated the above ex-
ample using the same input but two different
grammars. In the first case we used a grammar
which consists of discourse-level trees but no an-
notations for discourse entities. The realizer pro-
</bodyText>
<page confidence="0.982363">
307
</page>
<figure confidence="0.942838625">
h0:white cream(e)
h1:elixir(e)
h2:fda(f)
h3:approve(f e)
h4:contain(e a)
h5:aliprosan(a)
h6:relieve(a v)
h7:viral skin disorders(v)
</figure>
<figureCaption confidence="0.999815">
Figure 4: Input for the sentences in (1)
</figureCaption>
<bodyText confidence="0.999923714285714">
duced 192 solutions, including many incoherent
ones such as (1a). In the second case, we used
a grammar with the same trees, but annotated with
discourse referents. In this case the realizer pro-
duced only 16 solutions, all of which maintained
referential coherence. In the first case, the gram-
mar provided 128 ways to associate trees with the
input (tree sets), and the 192 solutions included
all possible sentence orders. Since for most trees
in the grammar there are more than one ways to
annotate them with discourse referents, in the sec-
ond case the grammar contained more trees (dif-
fering only in their discourse referent asignments).
In this case there were 1536 tree sets selected for
the same input. Of these, 1320 were discarded by
polarity filtering on discourse entities. Of the re-
maining 216 tree sets 200 were ruled out by fea-
ture unification when the trees were combined.
Figures 2 and 3 illustrate two sets of trees that
were selected by the realizer, corresponding to the
examples in (1). Discourse-level polarity filtering
in this example (for the input in (4)) discards all
tree sets whose polarities don’t sum up to one of
the discourse entities, i.e., +e, +a, +f or +v. The
polarity of the tree set in Fig.2 is +2a -v so the
tree set is discarded. For the tree set in Fig.3 the
polarities sum up to +e and the realizer attempts
to combine the trees, which in this case leads to a
referentially coherent solution (1b).
The search space of the realizer can be further
restricted by only allowing tree sets whose polari-
ties sum up to a specific discourse entity. In this
case the realizer will produce paragraphs where
the center of attention in the last sentence is the
discourse entity used for polarity filtering.
</bodyText>
<sectionHeader confidence="0.99984" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999568">
We have described a discourse-level extension of
Tree Adjoining Grammar which tightly integrates
syntax with discourse and includes a representa-
tion of discourse entities. We have shown that in-
cluding discourse entities in the grammar of a sur-
face realizer improves the coherence of the gener-
ated text and that these variables can also be used
in a very efficient optimization technique, polarity
filtering, to filter out referentially incoherent solu-
tions.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999446">
D.E. Appelt. 1985. Planning English sentences. Cam-
bridge University Press, Cambridge.
L. Danlos. 2000. G-TAG: A lexicalized formalism for
text generation inspired by Tree Adjoining Gram-
mar. In A. Abeille and O. Rambow, editors, Tree
Adjoining Grammars: Formalisms, linguistic analy-
sis and processing, pages 343–370. CSLI, Stanford,
CA.
C. Gardent and E. Kow. 2006. Three reasons to adopt
TAG-based surface realisation. In Proceedings of
TAG+8), Sydney/Australia.
B.J. Grosz, A.K. Joshi, and S Weinstein. 1995. Cen-
tering: a framework for modelling the local co-
herence of discourse. Computational Linguistics,
21(2):203–225.
R. Kibble and R. Power. 2004. Optimizing referential
coherence in text generation. Computational Lin-
guistics, 30(4):401–416.
R. Kibble. 1999. Cb or not Cb? centering theory ap-
plied to NLG. In ACL workshop on Discourse and
Reference Structure, pages 72–81.
R. Kibble. 2001. A reformulation of rule 2 of centering
theory. Comput. Linguist., 27(4):579–587.
A. Koller and M. Stone. 2007. Sentence generation as
planning. In Proceedings of ACL.
A. Koller and K. Striegnitz. 2002. Generation as de-
pendency parsing. In Proceedings of ACL.
E. Kow. 2007. Surface realisation: ambiguity and
determinism. Ph.D. thesis, Universite de Henri
Poincare - Nancy 1.
J. Robin. 1994. Revision-based generation of Natu-
ral Language Summaries providing historical Back-
ground. Ph.D. thesis, Columbia University.
J. Shaw. 2002. Clause Aggregation: An approach
to generating concise text. Ph.D. thesis, Columbia
University.
M. Stone, C. Doran, B. Webber, T. Bleam, and
M. Palmer. 2003. Microplanning with communica-
tive intentions: The SPUD system. Computational
Intelligence, 19(4):311–381.
</reference>
<page confidence="0.998575">
308
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.982110">
<title confidence="0.999886">Extending a Surface Realizer to Generate Coherent Discourse</title>
<author confidence="0.999993">Eva Banik</author>
<affiliation confidence="0.999312">The Open University</affiliation>
<address confidence="0.986405">Milton Keynes, UK</address>
<email confidence="0.998727">e.banik@open.ac.uk</email>
<abstract confidence="0.999784307692308">We present a discourse-level Tree Adjoining Grammar which tightly integrates syntax and discourse levels, including a representation for discourse entities. We show that this technique makes it possible to extend an optimisation algorithm used in natural language generation (polarity filtering) to the discourse level. We implemented the grammar in a surface realizer and show that this technique can be used to reduce the search space by filtering out referentially incoherent solutions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D E Appelt</author>
</authors>
<title>Planning English sentences.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="2261" citStr="Appelt, 1985" startWordPosition="338" endWordPosition="339">ed NLG systems becomes an engineering and programming task. Integrated NLG systems have a simpler architecture because they do not need to model interactions between modules. However, they still face the problem of computational complexity that was originally solved by the pipeline model. Strategies that have been introduced to reduce the search space in integrated systems include greedy/incremental search algorithms (Stone et al., 2003), constructing a dependency graph for a flat semantic input and converting it into a derivation tree (Koller and Striegnitz, 2002), using planning algorithms (Appelt, 1985; Koller and Stone, 2007), polarity filtering (Kow, 2007) and using underspecified g-derivation trees (G-TAG, Danlos (2000)). Despite all these efforts, most systems still don’t attempt to go above the sentence level or generate very complex sentences. In this paper we present a new technique for designing an integrated grammar for natural language generation. Using this technique it is possible to use linguistic constraints on referential coherence to automatically reduce the search space — which in turn makes it possible to generate longer and more coherent texts. First we extend the grammar</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>D.E. Appelt. 1985. Planning English sentences. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Danlos</author>
</authors>
<title>G-TAG: A lexicalized formalism for text generation inspired by Tree Adjoining Grammar.</title>
<date>2000</date>
<pages>343--370</pages>
<editor>In A. Abeille and O. Rambow, editors,</editor>
<publisher>CSLI,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="2384" citStr="Danlos (2000)" startWordPosition="354" endWordPosition="355">y do not need to model interactions between modules. However, they still face the problem of computational complexity that was originally solved by the pipeline model. Strategies that have been introduced to reduce the search space in integrated systems include greedy/incremental search algorithms (Stone et al., 2003), constructing a dependency graph for a flat semantic input and converting it into a derivation tree (Koller and Striegnitz, 2002), using planning algorithms (Appelt, 1985; Koller and Stone, 2007), polarity filtering (Kow, 2007) and using underspecified g-derivation trees (G-TAG, Danlos (2000)). Despite all these efforts, most systems still don’t attempt to go above the sentence level or generate very complex sentences. In this paper we present a new technique for designing an integrated grammar for natural language generation. Using this technique it is possible to use linguistic constraints on referential coherence to automatically reduce the search space — which in turn makes it possible to generate longer and more coherent texts. First we extend the grammar of a surface realizer to produce complex, multi-sentential output. Then we add a representation for discourse referents to</context>
</contexts>
<marker>Danlos, 2000</marker>
<rawString>L. Danlos. 2000. G-TAG: A lexicalized formalism for text generation inspired by Tree Adjoining Grammar. In A. Abeille and O. Rambow, editors, Tree Adjoining Grammars: Formalisms, linguistic analysis and processing, pages 343–370. CSLI, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gardent</author>
<author>E Kow</author>
</authors>
<title>Three reasons to adopt TAG-based surface realisation.</title>
<date>2006</date>
<booktitle>In Proceedings of TAG+8), Sydney/Australia.</booktitle>
<contexts>
<context position="3285" citStr="Gardent and Kow, 2006" startWordPosition="497" endWordPosition="501">use linguistic constraints on referential coherence to automatically reduce the search space — which in turn makes it possible to generate longer and more coherent texts. First we extend the grammar of a surface realizer to produce complex, multi-sentential output. Then we add a representation for discourse referents to the grammar, inspired by Centering Theory’s notion of a backward looking center and preferred center. Having done this, we show that by integrating discourse-level representations into a syntactic grammar we can extend an optimization technique — polarity filtering (Kow, 2007; Gardent and Kow, 2006) — from syntactic realization to the discourse level. 2 The Problem of Referential Coherence Referential coherence is the phenomenon which is responsible for the contrast in (1), in the sense that the example in (1b) is perceived to be more coherent than (1a). (1) a Elixir is approved by the FDA. Viral skin disorders are relieved by 305 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 305–308, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP Aliprosan. Elixir is a white cream. Aliprosan is an ingredient of Elixir. b Elixir is a white cream. Elixir is approved by the FDA.</context>
</contexts>
<marker>Gardent, Kow, 2006</marker>
<rawString>C. Gardent and E. Kow. 2006. Three reasons to adopt TAG-based surface realisation. In Proceedings of TAG+8), Sydney/Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: a framework for modelling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="3991" citStr="Grosz et al., 1995" startWordPosition="614" endWordPosition="617">rence Referential coherence is the phenomenon which is responsible for the contrast in (1), in the sense that the example in (1b) is perceived to be more coherent than (1a). (1) a Elixir is approved by the FDA. Viral skin disorders are relieved by 305 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 305–308, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP Aliprosan. Elixir is a white cream. Aliprosan is an ingredient of Elixir. b Elixir is a white cream. Elixir is approved by the FDA. Elixir contains Aliprosan. Aliprosan relieves viral skin disorders. Centering Theory (Grosz et al., 1995) is a frequently used framework for modeling referential coherence in discourse. It is based on the notion that for each utterance in a discourse there is a set of entities which are the centers of attention and which serve to link that utterance to other utterances in the same discourse segment. Entities mentioned by an utterance (the set of forward looking centers) form a partially ordered list called the Cf list where roughly, subjects are ranked highest, followed by objects, indirect objects and other arguments or adjuncts. The backward looking center of Un is said to be the most highly ra</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>B.J. Grosz, A.K. Joshi, and S Weinstein. 1995. Centering: a framework for modelling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kibble</author>
<author>R Power</author>
</authors>
<title>Optimizing referential coherence in text generation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="4771" citStr="Kibble and Power (2004)" startWordPosition="750" endWordPosition="754">set of entities which are the centers of attention and which serve to link that utterance to other utterances in the same discourse segment. Entities mentioned by an utterance (the set of forward looking centers) form a partially ordered list called the Cf list where roughly, subjects are ranked highest, followed by objects, indirect objects and other arguments or adjuncts. The backward looking center of Un is said to be the most highly ranked element on the Cf list of Un-1 mentioned in the previous utterance. Centering Theory has been adapted to NLG by Kibble (1999; 2001), and implemented in Kibble and Power (2004). Rather than using the notion of centering transitions as defined by Grosz et al. (1995), in these papers centering theory is redefined as constraints on salience and cohesion. These constraints state that there is a preference for consecutive utterances to keep the same center and that there is a preference for the center of Un to be realized as the highest ranked entity on the Cf list of Un. Kibble and Power (2004) show how these constraints can be used to drive text planning, sentence planning and pronominalization in an integrated fashion. Our approach is similar to Kibble and Power (2004</context>
</contexts>
<marker>Kibble, Power, 2004</marker>
<rawString>R. Kibble and R. Power. 2004. Optimizing referential coherence in text generation. Computational Linguistics, 30(4):401–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kibble</author>
</authors>
<title>Cb or not Cb? centering theory applied to NLG.</title>
<date>1999</date>
<booktitle>In ACL workshop on Discourse and Reference Structure,</booktitle>
<pages>72--81</pages>
<contexts>
<context position="4720" citStr="Kibble (1999" startWordPosition="744" endWordPosition="745">ach utterance in a discourse there is a set of entities which are the centers of attention and which serve to link that utterance to other utterances in the same discourse segment. Entities mentioned by an utterance (the set of forward looking centers) form a partially ordered list called the Cf list where roughly, subjects are ranked highest, followed by objects, indirect objects and other arguments or adjuncts. The backward looking center of Un is said to be the most highly ranked element on the Cf list of Un-1 mentioned in the previous utterance. Centering Theory has been adapted to NLG by Kibble (1999; 2001), and implemented in Kibble and Power (2004). Rather than using the notion of centering transitions as defined by Grosz et al. (1995), in these papers centering theory is redefined as constraints on salience and cohesion. These constraints state that there is a preference for consecutive utterances to keep the same center and that there is a preference for the center of Un to be realized as the highest ranked entity on the Cf list of Un. Kibble and Power (2004) show how these constraints can be used to drive text planning, sentence planning and pronominalization in an integrated fashion</context>
</contexts>
<marker>Kibble, 1999</marker>
<rawString>R. Kibble. 1999. Cb or not Cb? centering theory applied to NLG. In ACL workshop on Discourse and Reference Structure, pages 72–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kibble</author>
</authors>
<title>A reformulation of rule 2 of centering theory.</title>
<date>2001</date>
<journal>Comput. Linguist.,</journal>
<volume>27</volume>
<issue>4</issue>
<marker>Kibble, 2001</marker>
<rawString>R. Kibble. 2001. A reformulation of rule 2 of centering theory. Comput. Linguist., 27(4):579–587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Koller</author>
<author>M Stone</author>
</authors>
<title>Sentence generation as planning.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2286" citStr="Koller and Stone, 2007" startWordPosition="340" endWordPosition="343"> becomes an engineering and programming task. Integrated NLG systems have a simpler architecture because they do not need to model interactions between modules. However, they still face the problem of computational complexity that was originally solved by the pipeline model. Strategies that have been introduced to reduce the search space in integrated systems include greedy/incremental search algorithms (Stone et al., 2003), constructing a dependency graph for a flat semantic input and converting it into a derivation tree (Koller and Striegnitz, 2002), using planning algorithms (Appelt, 1985; Koller and Stone, 2007), polarity filtering (Kow, 2007) and using underspecified g-derivation trees (G-TAG, Danlos (2000)). Despite all these efforts, most systems still don’t attempt to go above the sentence level or generate very complex sentences. In this paper we present a new technique for designing an integrated grammar for natural language generation. Using this technique it is possible to use linguistic constraints on referential coherence to automatically reduce the search space — which in turn makes it possible to generate longer and more coherent texts. First we extend the grammar of a surface realizer to</context>
</contexts>
<marker>Koller, Stone, 2007</marker>
<rawString>A. Koller and M. Stone. 2007. Sentence generation as planning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Koller</author>
<author>K Striegnitz</author>
</authors>
<title>Generation as dependency parsing.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2220" citStr="Koller and Striegnitz, 2002" startWordPosition="330" endWordPosition="333">between modules, adding them to the repertoire of pipelined NLG systems becomes an engineering and programming task. Integrated NLG systems have a simpler architecture because they do not need to model interactions between modules. However, they still face the problem of computational complexity that was originally solved by the pipeline model. Strategies that have been introduced to reduce the search space in integrated systems include greedy/incremental search algorithms (Stone et al., 2003), constructing a dependency graph for a flat semantic input and converting it into a derivation tree (Koller and Striegnitz, 2002), using planning algorithms (Appelt, 1985; Koller and Stone, 2007), polarity filtering (Kow, 2007) and using underspecified g-derivation trees (G-TAG, Danlos (2000)). Despite all these efforts, most systems still don’t attempt to go above the sentence level or generate very complex sentences. In this paper we present a new technique for designing an integrated grammar for natural language generation. Using this technique it is possible to use linguistic constraints on referential coherence to automatically reduce the search space — which in turn makes it possible to generate longer and more co</context>
</contexts>
<marker>Koller, Striegnitz, 2002</marker>
<rawString>A. Koller and K. Striegnitz. 2002. Generation as dependency parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Kow</author>
</authors>
<title>Surface realisation: ambiguity and determinism.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<volume>1</volume>
<institution>Universite de Henri Poincare - Nancy</institution>
<contexts>
<context position="2318" citStr="Kow, 2007" startWordPosition="346" endWordPosition="347"> Integrated NLG systems have a simpler architecture because they do not need to model interactions between modules. However, they still face the problem of computational complexity that was originally solved by the pipeline model. Strategies that have been introduced to reduce the search space in integrated systems include greedy/incremental search algorithms (Stone et al., 2003), constructing a dependency graph for a flat semantic input and converting it into a derivation tree (Koller and Striegnitz, 2002), using planning algorithms (Appelt, 1985; Koller and Stone, 2007), polarity filtering (Kow, 2007) and using underspecified g-derivation trees (G-TAG, Danlos (2000)). Despite all these efforts, most systems still don’t attempt to go above the sentence level or generate very complex sentences. In this paper we present a new technique for designing an integrated grammar for natural language generation. Using this technique it is possible to use linguistic constraints on referential coherence to automatically reduce the search space — which in turn makes it possible to generate longer and more coherent texts. First we extend the grammar of a surface realizer to produce complex, multi-sententi</context>
<context position="5868" citStr="Kow, 2007" startWordPosition="938" endWordPosition="939">ntence planning and pronominalization in an integrated fashion. Our approach is similar to Kibble and Power (2004) in that we don’t use the concept of centering transitions. However, our method is more efficient in that Kibble and Power (2004) use centering transitions to rank the set of generated solutions (some of which are incoherent), whereas we encode centering constraints in elementary trees to reduce the search space of possible solutions before we start computing them. 3 GenI and Polarity Filtering The grammar described in the next section was implemented in the GenI surface realizer (Kow, 2007), which uses a lexicalized feature-based Tree Adjoining Grammar to generate all possible paraphrases for a given flat semantic input. GenI implements an optimization technique called polarFigure 1: Elementary syntax/discourse trees ity filtering to constrain the effects of lexical ambiguity. The basic idea of polarity filtering is to associate elementary trees with a set of polarities. When these polarities don’t ‘cancel each other out’, it means that it is not possible to combine the set of trees selected for a given input. This is a quick way to check whether the number of argument slots is </context>
</contexts>
<marker>Kow, 2007</marker>
<rawString>E. Kow. 2007. Surface realisation: ambiguity and determinism. Ph.D. thesis, Universite de Henri Poincare - Nancy 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robin</author>
</authors>
<title>Revision-based generation of Natural Language Summaries providing historical Background.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="1496" citStr="Robin, 1994" startWordPosition="225" endWordPosition="226">ration process into tractable sub-problems, each represented as a module in a pipeline, where every decision made by a module restricts the number of options available to others further down the line. Though such pipeline architectures are computationally efficient, they severely restrict the flexibility of the system and the quality of the generated output. Most systems with pipeline architectures generate relatively simple, domain-specific output. Systems that produce more complex linguistic constructions typically achieve this by adding more modules to the pipeline (e.g. a revision module (Robin, 1994) or aggregation (Shaw, 2002)). Since complex linguistic constructions often require interaction between modules, adding them to the repertoire of pipelined NLG systems becomes an engineering and programming task. Integrated NLG systems have a simpler architecture because they do not need to model interactions between modules. However, they still face the problem of computational complexity that was originally solved by the pipeline model. Strategies that have been introduced to reduce the search space in integrated systems include greedy/incremental search algorithms (Stone et al., 2003), cons</context>
</contexts>
<marker>Robin, 1994</marker>
<rawString>J. Robin. 1994. Revision-based generation of Natural Language Summaries providing historical Background. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shaw</author>
</authors>
<title>Clause Aggregation: An approach to generating concise text.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="1524" citStr="Shaw, 2002" startWordPosition="229" endWordPosition="230"> sub-problems, each represented as a module in a pipeline, where every decision made by a module restricts the number of options available to others further down the line. Though such pipeline architectures are computationally efficient, they severely restrict the flexibility of the system and the quality of the generated output. Most systems with pipeline architectures generate relatively simple, domain-specific output. Systems that produce more complex linguistic constructions typically achieve this by adding more modules to the pipeline (e.g. a revision module (Robin, 1994) or aggregation (Shaw, 2002)). Since complex linguistic constructions often require interaction between modules, adding them to the repertoire of pipelined NLG systems becomes an engineering and programming task. Integrated NLG systems have a simpler architecture because they do not need to model interactions between modules. However, they still face the problem of computational complexity that was originally solved by the pipeline model. Strategies that have been introduced to reduce the search space in integrated systems include greedy/incremental search algorithms (Stone et al., 2003), constructing a dependency graph </context>
</contexts>
<marker>Shaw, 2002</marker>
<rawString>J. Shaw. 2002. Clause Aggregation: An approach to generating concise text. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
<author>C Doran</author>
<author>B Webber</author>
<author>T Bleam</author>
<author>M Palmer</author>
</authors>
<title>Microplanning with communicative intentions: The SPUD system.</title>
<date>2003</date>
<journal>Computational Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="2090" citStr="Stone et al., 2003" startWordPosition="309" endWordPosition="312">sion module (Robin, 1994) or aggregation (Shaw, 2002)). Since complex linguistic constructions often require interaction between modules, adding them to the repertoire of pipelined NLG systems becomes an engineering and programming task. Integrated NLG systems have a simpler architecture because they do not need to model interactions between modules. However, they still face the problem of computational complexity that was originally solved by the pipeline model. Strategies that have been introduced to reduce the search space in integrated systems include greedy/incremental search algorithms (Stone et al., 2003), constructing a dependency graph for a flat semantic input and converting it into a derivation tree (Koller and Striegnitz, 2002), using planning algorithms (Appelt, 1985; Koller and Stone, 2007), polarity filtering (Kow, 2007) and using underspecified g-derivation trees (G-TAG, Danlos (2000)). Despite all these efforts, most systems still don’t attempt to go above the sentence level or generate very complex sentences. In this paper we present a new technique for designing an integrated grammar for natural language generation. Using this technique it is possible to use linguistic constraints </context>
</contexts>
<marker>Stone, Doran, Webber, Bleam, Palmer, 2003</marker>
<rawString>M. Stone, C. Doran, B. Webber, T. Bleam, and M. Palmer. 2003. Microplanning with communicative intentions: The SPUD system. Computational Intelligence, 19(4):311–381.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>