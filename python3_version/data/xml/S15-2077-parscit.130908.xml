<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000143">
<title confidence="0.893636">
SemEval-2015 Task 9: CLIPEval Implicit Polarity of Events
</title>
<author confidence="0.880502">
Irene Russo
</author>
<affiliation confidence="0.750018">
ILC-CNR “A. Zampolli”
</affiliation>
<address confidence="0.857354">
Via G. Moruzzi, 1
56124 Pisa
</address>
<email confidence="0.99745">
irene.russo@ilc.cnr.it
</email>
<author confidence="0.793834">
Tommaso Caselli
</author>
<affiliation confidence="0.734416">
Vrije Universiteit Amsterdam
</affiliation>
<address confidence="0.900987">
De Boelelaan 1105
1081 HV Amsterdam
</address>
<email confidence="0.997895">
t.caselli@gmail.com
</email>
<author confidence="0.699441">
Carlo Strapparava
</author>
<affiliation confidence="0.512112">
Fondazione Bruno Kessler
</affiliation>
<address confidence="0.712694">
Via Sommarive, 18
38123 Povo (TN)
</address>
<email confidence="0.996385">
strappa@fbk.eu
</email>
<sectionHeader confidence="0.995607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999519142857143">
Sentiment analysis tends to focus on the po-
larity of words, combining their values to de-
tect which portion of a text is opinionated.
CLIPEval wants to promote a more holistic
approach, looking at psychological researches
that frame the connotations of words as the
emotional values activated by them. The im-
plicit polarity of events is just one aspect of
connotative meaning and we address it with a
task that is based on a dataset of sentences an-
notated as instantiations of pleasant and un-
pleasant events previously collected in psy-
chological research as the ones on which hu-
man judgments converge.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999396325">
Current research in sentiment analysis (SA, hence-
forth) is mostly focused on lexical resources
that store polarity values. For bag-of-words ap-
proaches the polarity of a text depends on the pres-
ence/absence of a set of lexical items. This method-
ology is successful to detect opinions about entities
(such as reviews) but it shows mixed results when
complex opinions about events - involving perspec-
tives and points of view - are expressed.
In terms of parts of speech involved, SA ap-
proaches tend to focus on lexical items that explic-
itly convey opinions - mainly adjectives, adverbs
and several nouns - leaving verbs on the foreground.
Improvements have been proposed by taking into ac-
count syntax (Greene and Resnik 2009) and by in-
vestigating the connotative polarity of words (Cam-
bria et al., 2009; Akkaya et al., 2009, Balhaur et
al., 2011; Russo et al. 2011; Cambria et al., 2012,
Deng et al., 2013 among others). One of the
key aspects of sentiment analysis, which has been
only marginally tackled so far, is the identification
of implicit polarity. By implicit polarity we refer
to the recognition of subjective textual units where
no polarity markers are present but still people are
able to state whether the text portion under analy-
sis expresses a positive or negative sentiment. Re-
cently, methodologies trying to address this aspect
have been developed, incorporating ideas from lin-
guistic and psychological studies on the subjective
aspects of linguistic expressions.
Aiming at promoting a more holistic approach
to sentiment analysis, combining the detection of
implicit polarity with the expression of opinions
on events, we propose CLIPEval, a task based on
a dataset of events annotated as instantiations of
pleasant and unpleasant events (PE/UPEs hence-
forth) previously collected in psychological research
as the ones that correlate with mood (both good
and bad feelings) (Lewinsohn and Amenson, 1978;
MacPhillamy and Lewinsohn, 1982).
</bodyText>
<sectionHeader confidence="0.855016" genericHeader="introduction">
2 Measuring Emotional Connotations:
</sectionHeader>
<subsectionHeader confidence="0.863423">
Psychological Studies
</subsectionHeader>
<bodyText confidence="0.999827625">
For a long time research in psychology has been
interested in a subjective cultural and/or emotional
coloration in addition to the explicit or denotative
meaning of any specific word or phrase. Starting
with the work of Charles E. Osgood, who in the 50s
developed a technique for measuring the connotative
meaning of concepts and analyzed human attitudes
(Osgood et al., 1957), psychologists have experi-
</bodyText>
<page confidence="0.99059">
443
</page>
<bodyText confidence="0.983360943661972">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 443–450,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
mented with emotional values activated by words,
often through the evaluation of their pleasantness.
Osgood and his colleagues proposed a factor anal-
ysis based on semantic differential scales measur-
ing three basic attitudes that people display cross-
culturally: evaluation (along the scale of adjectives
“good-bad”), potency (along “strong-weak”) and ac-
tivity (“active-passive”).
This line of research continued with studies eval-
uating Osgood’s findings with different population
and the pleasantness of words became also a dimen-
sion to correlate with other dimensions reported in
semantic norms studies, such as familiarity and im-
agery. We know today that pleasantness is a se-
mantic factor influencing short and long term mem-
ory (Monnier et al., 2008); similarly, (Hadley and
MacKay, 2006) showed that STM for certain un-
pleasant emotional words (i.e., taboo words) was
better than that for neutral words. Emotional words
are better recalled because they are related to long-
term representations of autobiographical and self-
reference units (Ochsner, 2000). Other factors have
a role: depressed subjects, for example, recalled
more unpleasant words than pleasant words.
Osgood’s studies were revised for the production
of the Affective Norms for English Words (ANEW)
(Bradley et al, 1999), a set of normative emotional
ratings for 1034 words in American English. This
set of verbal materials have been rated in terms of
pleasure, arousal, and dominance in order to create
a standard for use in studies of emotion and attention
(the same three basic dimensions used by Osgood).
Affective valence (or pleasure, ranging from pleas-
ant to unpleasant) and arousal (ranging from calm to
excited) were the two primary dimensions. A third,
less strongly-related dimension, was called “domi-
nance” or “control”.
Connotative meaning emerges as a complex and
stratified concept and only psychological studies can
guide in this maze, especially when they are sup-
ported by significant experimental outputs such as
list of words evaluated by human subjects.
All these studies are relevant for NLP because
connotative meanings of words can help to refine au-
tomatic sentiment analysis on social media, where
shared contents are often just short reports on pleas-
ant or unpleasant events and activities. For example,
(Fenf et al., 2013) report that connotation lexicon
guarantees better performance than other sentiment
analysis lexicons that do not encode connotations on
Twitter data.
That said, when psychological experiments ask
for judgments about single words they oversimplify:
we experience the meanings of single words as aris-
ing from compositionality, in expressions and sen-
tences. Even neutral words in specific contexts
can acquire a polarity as effect of semantic prosody
(Louw 1993).
When subjects are asked for the pleasantness of
an event they need to evaluate not just single words
but complete sentences; for this reason (Lewinsohn
and Amenson, 1978; MacPhillamy and Lewinsohn,
1982) developed two psychometric instruments, the
Pleasant Events Schedule and the Unpleasant Events
Schedule, by sampling events that were reported to
be source of pleasure or distress by highly diverse
samples of people that rated the frequency of event’s
occurrence during past month plus a complete mood
ratings.
</bodyText>
<sectionHeader confidence="0.990824" genericHeader="method">
3 CLIPEval Annotation
</sectionHeader>
<bodyText confidence="0.9999684">
The CLIPEval exercise provides the NLP com-
munity with a newly developed dataset grounded
on psychological studies about the pleasantness of
events. Dedicated annotation specifications and
guidelines for the release of the dataset have been
developed.
The starting point for the development of the an-
notation guidelines was the PE/UPEs lists, the set
of 640 pleasant and unpleasant events (320 pleas-
ant events and 320 unpleasant events, respectively)
collected by (Lewinsohn and Amenson, 1978) and
(MacPhillamy and Lewinsohn, 1982). The dataset
could not be used as it is since it is a list of generic
sentences describing either states or actions which
are labeled as pleasant or unpleasant events. To clar-
ify this, we report two examples extracted from the
original dataset. Example 1.) is a pleasant event
while example 2.) is an an unpleasant event. The
numbers in brackets at the beginning of the sentence
refer to the PE/UPEs number in the original dataset.
</bodyText>
<listItem confidence="0.994737666666667">
1.) (9) Planning trips or vacations.
2.) (10) Getting separated or divorced from my
spouse.
</listItem>
<page confidence="0.99658">
444
</page>
<bodyText confidence="0.998333875">
Furthermore, a closer examination of PE/UPEs
has shown that ambiguity occurs, with the same
events considered both as a pleasant and an unpleas-
ant one (e.g. Being alone), since this is plausible
from a psychological point of view. To overcome
these issues and to make the task relevant for sen-
tences from news articles, we have applied the fol-
lowing strategies:
</bodyText>
<listItem confidence="0.716093307692308">
• all ambiguous PE/UPEs have been removed
from the original dataset;
• PE/UPEs have been grouped into classes
whose labels describe and aggregate different
PE/UPEs, referring often to a more general
event class with respect to the one the single
instance of a PE/UPE event describes. This
choice has been necessary because the event
instances in the original psychological dataset
are conceptually similar but using the original
descriptions would result either in too generic
cases (e.g. Being with children) or too simple
(e.g. Washing my hair).
</listItem>
<bodyText confidence="0.99993282">
The grouping of PE/UPEs in classes has been
conducted in two phases by two annotators. In the
first phase, both annotators have worked indepen-
dently: for each PE/UPE the annotators had to de-
cide which of them could be clustered in a more
generic class and which were to be excluded, either
because it describes a too specific (or a too generic)
event or because it explicitly express the pleasante-
ness of the event (e.g. (25) Driving skillfully). As
a measure of agreement for this task, we preferred
not to use kappa score, because it’s not a standard
classification task, but we computed the percentage
of agreement. The first evaluation shown a relatively
low agreement, only 59.06% of the 640 events were
considered as belonging to a cluster. An analysis on
the cases of disagreement has highlighted some in-
consistencies. Thus, a second clusterization task has
been performed by asking to the same annotators to
go over the same data following new additional rules
that were developed during the analysis. The evalua-
tion of this second phase shown a clear improvement
with a percentage agreement of 68.25%. As a result
of these annotation phases, we had a set of clusters
that the annotators were allowed to discuss, finding
a joint solution in cases of disagreements and iden-
tifying the best labels for the PE/UPEs clusters. The
final output of these two phases resulted in 8 classes
of PE/UPEs (see Table 1 column “Event Class”). It
is important to point out that most of these classes
contain PE/UPEs both from the 320 pleasant events
and the 320 unpleasant events and as a consequence
the polarities of their occurrences in the training data
are mixed(see Table 1). Due to the novelty of the
task, we could not re-use available datasets for SA.
For this reason, the second step concerns the identifi-
cation and manual annotation of real sentences from
the Annotated English Gigaword corpus (Napoles
et al., 2012), an automatically-generated syntactic
and discourse structure annotated version of the En-
glish Gigaword corpus Fifth Edition, which contains
a large English corpus of newspaper articles (four
billion words ca.). To facilitate the sentence extrac-
tion phase, we manually identified the verbal and the
nominal keywords from the event mentions compos-
ing the classes. We used WN30 and the Oxford Dic-
tionary to extract all verb and noun synonyms of the
PE/UPEs in each class. We then queried the Giga-
word corpus with this extended set of keywords to
extract sentences which contain self-reported events
by means of following patterns:
</bodyText>
<listItem confidence="0.99871625">
• “I|we + [verbal keyword]”
• “I|we + [nominal keyword]”
• “I|we + [verbal keyword] + [nomi-
nal keyword]”.
</listItem>
<bodyText confidence="0.9979015">
The sentences thus extracted were manually fil-
tered and annotated with respect to the 8 classes and
to their polarity. The annotation has been conducted
at sentence level. To provide homogeneous data and
annotations, the following guidelines have been de-
veloped for the assignment of the class label:
</bodyText>
<listItem confidence="0.993466857142857">
• the class label and the polarity value must be
assigned on the basis of the event that corre-
spond syntactically to the main verb in the sen-
tence;
• in case of coordinated main clauses, only the
first main clause is taken into account to assign
the class label and the polarity value;
</listItem>
<page confidence="0.99897">
445
</page>
<tableCaption confidence="0.999944">
Table 1: CLIPEval corpus: Training data.
</tableCaption>
<table confidence="0.999869555555555">
Event Class POSITIVE NEGATIVE NEUTRAL Tot. Instances
(FEAR OF) PHYSICAL PAIN 19 131 10 160
ATTENDING EVENT 83 35 42 160
COMMUNICATION ISSUE 21 120 19 160
GOING TO PLACES 55 72 33 160
LEGAL ISSUE 24 115 21 160
MONEY ISSUE 20 109 31 160
OUTDOOR ACTIVITY 125 18 17 160
PERSONAL CARE 88 40 32 160
</table>
<tableCaption confidence="0.993539">
Table 2: CLIPEval corpus: Test data.
</tableCaption>
<table confidence="0.999875777777778">
Event Class POSITIVE NEGATIVE NEUTRAL Tot. Instances
(FEAR OF) PHYSICAL PAIN 10 30 5 45
ATTENDING EVENT 29 5 11 45
COMMUNICATION ISSUE 8 29 7 44
GOING TO PLACES 22 23 3 48
LEGAL ISSUE 5 27 13 45
MONEY ISSUE 12 27 12 51
OUTDOOR ACTIVITY 34 4 8 46
PERSONAL CARE 24 10 13 43
</table>
<listItem confidence="0.957777">
• subordinated clauses are not annotated with
class labels and polarity values.
</listItem>
<bodyText confidence="0.999267631578947">
Although all event mentions in the selected clus-
ters have either a positive (pleasant events) or neg-
ative (unpleasant events) polarity that could be re-
versed by negation, during the annotation phase a
third value, namely neutral, has been introduced to
cope with those sentences containing self-reporting
events whose occurrence is uncertain
We are referring here to the notion of event fac-
tuality (Saur´ıand Pustejovsky, 2009), i.e. the de-
grees of certainty (e.g. possible, probable, certain)
associated to an event description along the cate-
gory of epistemic modality. In the annotation we
focused on the syntactic information between tar-
get events instances and factuality markers, such as
modal auxiliaries and negation cues (including ad-
verbs, adjectives, prepositions, pronouns and deter-
miners). Events which are in the scope of factuality
markers signaling uncertainty or improbability have
been marked as neutral.
</bodyText>
<sectionHeader confidence="0.996982" genericHeader="method">
4 CLIPEval Tasks
</sectionHeader>
<bodyText confidence="0.9991285">
The CLIPEval evaluation exercise is composed of
two tasks described as follows:
</bodyText>
<listItem confidence="0.780365555555556">
• Task A: identification of the polarity value as-
sociated to the event instance. Participants are
required to associate each sentence with a po-
larity value (POSITIVE, NEGATIVE or NEU-
TRAL);
• Task B: identification of the event men-
tions with respect to one of the 8 event
class labels plus identification of the po-
larity value. The class labels used are:
</listItem>
<sectionHeader confidence="0.7695425" genericHeader="method">
ATTENDING EVENT, COMMUNICA-
TION ISSUE, GOING TO PLACES;
</sectionHeader>
<bodyText confidence="0.9949864">
LEGAL ISSUE, MONEY ISSUE, OUT-
DOOR ACTIVITIES, PERSONAL CARE,
(FEAR OF) PHYSICAL PAIN. As in Task A
the polarity values are (POSITIVE, NEGA-
TIVE or NEUTRAL).
</bodyText>
<sectionHeader confidence="0.996543" genericHeader="method">
5 Dataset Description
</sectionHeader>
<bodyText confidence="0.998930454545454">
The CLIPEval evaluation exercise is based on the
CLIPEval dataset, which consists of two parts: a
training set and a test set. The final size of the dataset
is 1,651 sentences, divided in 1,280 sentences for the
training and 371 for the test. Each event class in the
training data contains 160 sentences.
Each class in the training set is available in a sep-
arate file composed of four tab separated fields: a
sentence id, the sentence extracted from the Giga-
word corpus, the polarity value and the class label.
Each file is named with the class label. Some exam-
</bodyText>
<page confidence="0.998235">
446
</page>
<bodyText confidence="0.919977">
ples of the training data are provided in the examples
below (examples from 3.) to 5.)):
</bodyText>
<listItem confidence="0.901311">
3.) 8 I had just gone to a concert with my par-
ents and I identified with the conductor a lot
Dudamel said in Spanish during a recent in-
terview in Caracas. POSITIVE ATTEND-
ING EVENT
4.) 14 “It’s too cold and I can’t ride my
bike” he lamented. NEGATIVE OUT-
DOOR ACTIVITY
5.) 4 “I could take the boys to the sports museum’
says James. NEUTRAL GOING TO PLACES
</listItem>
<bodyText confidence="0.999743666666667">
The test data has been provided in a single file
with only two fields: the sentence id and the sen-
tence extracted from the Gigaword corpus:
</bodyText>
<listItem confidence="0.992957333333333">
6.) 12 After having given a friend a lift home I was
stopped by police.
7.) 23 And then we went to a library.
</listItem>
<bodyText confidence="0.996059888888889">
Table 1 and Table 2 report the figures for polarity
values per class in the training and in the test set, re-
spectively.
The division of the training data for the three po-
larity values is not balanced due to the event men-
tions composing the clusters. Only three clusters,
namely GOING TO PLACES, PERSONAL CARE
and ATTENDING EVENT, present a relatively bal-
anced distribution for the polarity values. This
lack of balance reflects real language data: the
prevalence of positive or negative values is due to
the classes which may have more PEs or UPEs
(e.g. OUTDOOR ACTIVITY and COMMUNICA-
TION ISSUE, respectively). Including more sen-
tences which reverse the polarity of the PEs or UPEs
to balance the occurrences per polarity value would
mean to force the data from real language toward an
artificial equilibrium.
</bodyText>
<sectionHeader confidence="0.999107" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999258307692308">
Since both Task A and Task B of CLIPEval are es-
sentially classification tasks (classification of the po-
larity value for Task A and classification of the event
instance and the polarity value for Task B), we have
used Precision, Recall and F1-measure to evaluate
the system results against the test set. Furthermore,
since this is a multi-classification task (3 possible
values for Task A and 24 possible values for Task
B), we have computed micro average Precision, Re-
call and F1-measure per class. This latter measure
has been used for the final ranking of the systems.
We have adopted standard definitions for these mea-
sures, namely:
</bodyText>
<listItem confidence="0.972740333333333">
• Precision: the number of correctly classified
positive examples, tpi per class Ci, divided by
number of examples labeled by the system as
</listItem>
<equation confidence="0.681706">
�l i=1 tpi
</equation>
<bodyText confidence="0.6398945">
positive (tpi plus false positive fpi):
tpi+fpi
</bodyText>
<listItem confidence="0.908684285714286">
• Recall: the number of correctly classified pos-
itive examples tpi per class Ci divided by the
number of positive examples in the data (tpi
plus false negatives fni) • tpi+ ni
&apos; �i+fni
• F-measure: the mean of Precision and Recall
calculated as follows: (β2+1)PrecisionRecall
</listItem>
<subsubsectionHeader confidence="0.360624">
β2Precision+Recall
</subsubsectionHeader>
<bodyText confidence="0.998223666666667">
To better evaluate systems’ performances, we
have developed three baselines, one per Task A and
two per Task B. In particular:
</bodyText>
<listItem confidence="0.998819904761905">
• Task A baseline has been obtained by assign-
ing to each sentence in the test set the most fre-
quent polarity value on the basis of the data in
the training set. This resulted in marking all
371 sentences in the test set with NEGATIVE
polarity;
• Task B baseline 1 has been obtained in two
steps: first, for each class in the training data
we have selected the most frequent nouns and
verbs lemmas. This has provided us with a list
of keywords representing each class. We have
then compared each sentence in the test set with
each group of keywords and assigned as cor-
rect the class which scored the higher number
of matches. In case of a draw, a random class
between the classes with the highest scores is
assigned. If no match is found, a random class
is assigned. As for the polarity, we have used
the absolute most frequent polarity values, like
in task A (i.e. all test set entries have been as-
signed to NEGATIVE value).
</listItem>
<page confidence="0.949138">
447
</page>
<listItem confidence="0.764937571428572">
• Task B baseline 2 has been obtained follow-
ing the approach in Task B baseline 1 for the
class assignment and we have assigned the
most frequent polarity value per class accord-
ing to training data (e.g. for items classified as
ATTENDING EVENTS the assigned polarity
value is POSITIVE).
</listItem>
<subsectionHeader confidence="0.999856">
6.1 Participant Systems
</subsectionHeader>
<bodyText confidence="0.999949090909091">
Overall 26 different teams registered for the task,
only two submitted the output of their system for
a total of 3 runs: SHELLFBK (Fondazione Bruno
Kessler) and SIGMA2320 (Peking Univeristy).
Only SHELLFBK submitted results for both tasks.
Furthermore, we can provide a short description just
for SHELLFBK since the SIGMA2320 team has
not submitted a system description paper.
SHELLFBK system implements a supervised
approach based on information retrieval techniques
for representing polarized information. During
the training phase, each sentence is analyzed by
applying the parser contained in the Stanford NLP
Library. From the results of the parsing activity,
both the list of the dependency relations and the
parsed trees are used for populating an inverted
index data structure containing the relationships
between each relation extracted from the sentences
and the corresponding information about its polar-
ization. The result of the training phase is a set of
three indexes containing, respectively, the positive,
negative, and neutral information analyzed in the
training set. When the polarity of a new sentence
has to be computed, the new sentence is given as
input to the Stanford NLP Library by obtaining
the list of its dependency relations, as well as,
the corresponding parsed tree. Such information
are built together for composing a query that is
afterwards performed on the indexes built during
the training phase. For each of the built indexes, a
retrieval score value is retrieved by the system and,
based on this, the polarity of the new sentence is
assigned.
</bodyText>
<subsectionHeader confidence="0.998126">
6.2 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.9446215">
We report in Table 3 the results of both systems for
Task A and the Task A baseline. In Table 4 we report
the results for Task B and both baseline for Task B
(baseline 1 and baseline 2, respectively).
</bodyText>
<tableCaption confidence="0.991765">
Table 3: Evaluation for Task A : polarity identification.
</tableCaption>
<table confidence="0.99993925">
System Precision Recall F1-measure
SIGMA2320 0.41 0.42 0.38
SHELLFBK 0.56 0.56 0.54
baseline 0.17 0.42 0.25
</table>
<tableCaption confidence="0.9848605">
Table 4: Evaluation for Task B : event instance and polar-
ity identification.
</tableCaption>
<table confidence="0.955635833333333">
System Precision Recall F1-measure
SHELLFBK 0.36 0.27 0.29
baseline 1 0.02 0.04 0.02
baseline 2 0.03 0.05 0.04
SHELLFBK outperforms SIGMA2320 for the
Task A; both systems improve the baseline. The
</table>
<bodyText confidence="0.996725384615385">
results are not as good as in classification tasks
concerning the polarities of tweets (Rosental et al.,
2014) or reviews (Pontiki et al., 2014) but since this
is a novel task about implicit polarity we think they
are promising.
For task B SHELLFBK has a better performance
both in terms of precision and recall if compared
with the two baselines. At the moment we do not
know if the results are due to SHELLFBK method-
ology or if data sparseness in the classes has an influ-
ence on the classification task: maybe classes more
cohesive from conceptual and lexical point of view
could be easier to detect.
</bodyText>
<sectionHeader confidence="0.997274" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999979066666667">
The implicit polarity of words concerns the aris-
ing of occasional polarized meanings in specific ex-
pressions/linguistic contexts. Labeled as semantic
prosody in corpus studies and part of what psycholo-
gists call connotative meanings, the implicit polarity
is a quite marginal concept in sentiment analysis. It
requires a dynamic representation for the polarity of
words (i.e. a verb can be neutral in the vast majority
of case but can be clearly positive in some contexts)
and a compositional approach to sentiment values
that goes beyond the oversimplifying assumptions of
bag-of-words approaches.
With the CLIPEval task we asked the NLP com-
munity to look at these complexities, considering
the detection of a set of events as relevant for SA
</bodyText>
<page confidence="0.996069">
448
</page>
<bodyText confidence="0.999856916666666">
analyses because they have been judged as pleas-
ant or unpleasant by subjects in psychological ex-
periments conducted by (Lewinsohn and Amenson,
1978; MacPhillamy and Lewinsohn, 1982). As fu-
ture work we plan to extend the dataset, including
new classes of events and annotating instances from
blogs and tweets. Also, we want to integrate the de-
tection of polarized events with the work on stance
and perspectives in news, going toward a theoretical
model for SA that takes into account the interplay of
linguistic means used by humans to express opinions
and feelings.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999936333333333">
One of the author wants to thanks the NWO Spinoza
Prize project Understanding Language by Machines
(sub-track 3) for partially supporting this work.
</bodyText>
<sectionHeader confidence="0.99892" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99940478313253">
Cem Akkaya, Janyce Wiebe and Mihalcea Rada. 2010.
Subjectivity Word Sense Disambiguation. Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 190 - 199.
Alexandra Balahur, Jes´us M. Hermida and Andr´es Mon-
toyo. 2011. Detecting implicit expressions of sen-
timent in text based on commonsense knowledge.
Proceedings of the 2nd Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis.,
pages 53 - 60.
Margaret M. Bradley and Peter J. Lang. 1999. Affective
norms for English words (ANEW): Stimuli, instruc-
tion manual and affective ratings. Tech. Rep. No. C-1
Erik Cambria, Robert Speer, Catherine Havasi and Amir
Hussain. 2010. SenticNet: A Publicly Available Se-
mantic Resource for Opinion Mining. AAAI fall sym-
posium: commonsense knowledge, pages 14 - 18.
Erik Cambria, Catherine Havasi and Amir Hussain.
2012. SenticNet 2: A Semantic and Affective Re-
source for Opinion Mining and Sentiment Analysis.
FLAIRS Conference, pages 202 - 207.
Ling Deng, Yoonjung Choi and Janyce Wiebe. 2012.
Benefactive/Malefactive Event and Writer Attitude
Annotation. Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 120 - 125.
Song Feng, Jun Seok Kang, Polina Kuznetsova, and Yejin
Choi. 2013. Connotation lexicon: A dash of senti-
ment beneath the surface meaning. Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1774 - 1784.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
Proceedings of human language technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 503 - 511.
Christopher B. Hadley and Donald G. MacKay. 2006.
Does emotion help or hinder immediate memory?
Arousal versus priority-binding mechanism. Journal
of Abnormal Psychology, 87(6), pages 644 - 654.
Peter M. Lewinsohn and Christopher S Amenson. 1978.
Some Relations between Pleasant and Unpleasant
Events and Depression. Journal of Experimental Psy-
chology: Learning, Memory, and Cognition, 32, pages
79 - 88.
Douglas J. MacPhillamy and Peter M. Lewinsohn. 1982.
The Pleasant Event Schedule: Studies on Reliability,
Validity, and Scale Intercorrelation. Journal of Coun-
seling and Clinical Psychology, 50(3), pages 363 -
380.
Catherine Monnier and Arielle SyssauMonnier. 2008.
Semantic contribution to verbal short-term memory:
Are pleasant words easier to remember than neutral
words in serial recall and serial recognition? Memory
and Cognition, 36, pages 35 - 42.
Courtney Napoles, Matthew Gormley and Benjamin Van
Durme. 2012. Annotated gigaword. Proceedings
of the Joint Workshop on Automatic Knowledge Base
Construction and Web-scale Knowledge Extraction,
pages 95 - 100.
Kevin N. Ochsner. 2000. Are affective events richly rec-
ollected or simply familiar? The experience and pro-
cess of recognizing feelings past. Journal of Experi-
mental Psychology. General, 129 (2), pages 242 261.
Charles E. Osgood, George Suci and Percy Tannenbaum.
1957. The Measurement of Meaning. University of
Illinois Press,
Roser Saur´ı and James Pustejovsky. 2009. FactBank:
A corpus annotated with event factuality. Language
resources and evaluation, 3, pages 227 - 268.
Maria Pontiki, Dimitris Galanis, Pavlopoulos Ioannis,
Harris Papageorgiou, Androutsopoulos Ion and Man-
andhar Suresh. 2014. SemEval 2014 Task 4: Aspect
Based Sentiment Analysis. Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval 2014), pages 27 - 35.
Sara Rosenthal, Alan Ritter, Preslav Nakov and Veselin
Stoyanov. 2014. SemEval-2014 Task 9: Sentiment
Analysis in Twitter. Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation (SemEval
2014)
</reference>
<page confidence="0.990415">
449
</page>
<reference confidence="0.999316142857143">
Irene Russo, Tommaso Caselli, Francesco Rubino, Ester
Boldrini and Patricio Barco Martinez. 2011. EMO-
Cause: An Easy-adaptable Approach to Extract Emo-
tion Cause Contexts. Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis (WASSA 2.011), pages 152 -
160.
</reference>
<page confidence="0.997849">
450
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.113522">
<title confidence="0.957399">SemEval-2015 Task 9: CLIPEval Implicit Polarity of Events</title>
<author confidence="0.972782">Irene</author>
<affiliation confidence="0.9131515">ILC-CNR “A. Via G. Moruzzi,</affiliation>
<address confidence="0.992059">56124</address>
<email confidence="0.992999">irene.russo@ilc.cnr.it</email>
<author confidence="0.451382">Tommaso</author>
<affiliation confidence="0.681257">Vrije Universiteit De Boelelaan</affiliation>
<address confidence="0.908349">1081 HV</address>
<email confidence="0.998931">t.caselli@gmail.com</email>
<author confidence="0.9229565">Carlo Fondazione Bruno</author>
<affiliation confidence="0.500934">Via Sommarive,</affiliation>
<address confidence="0.893639">38123 Povo</address>
<email confidence="0.989011">strappa@fbk.eu</email>
<abstract confidence="0.996911933333333">Sentiment analysis tends to focus on the polarity of words, combining their values to detect which portion of a text is opinionated. CLIPEval wants to promote a more holistic approach, looking at psychological researches that frame the connotations of words as the emotional values activated by them. The implicit polarity of events is just one aspect of connotative meaning and we address it with a task that is based on a dataset of sentences anas instantiations of unpreviously collected in psychological research as the ones on which human judgments converge.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cem Akkaya</author>
<author>Janyce Wiebe</author>
<author>Mihalcea Rada</author>
</authors>
<title>Subjectivity Word Sense Disambiguation.</title>
<date>2010</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>190--199</pages>
<marker>Akkaya, Wiebe, Rada, 2010</marker>
<rawString>Cem Akkaya, Janyce Wiebe and Mihalcea Rada. 2010. Subjectivity Word Sense Disambiguation. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190 - 199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Balahur</author>
<author>Jes´us M Hermida</author>
<author>Andr´es Montoyo</author>
</authors>
<title>Detecting implicit expressions of sentiment in text based on commonsense knowledge.</title>
<date>2011</date>
<booktitle>Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis.,</booktitle>
<pages>53--60</pages>
<marker>Balahur, Hermida, Montoyo, 2011</marker>
<rawString>Alexandra Balahur, Jes´us M. Hermida and Andr´es Montoyo. 2011. Detecting implicit expressions of sentiment in text based on commonsense knowledge. Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis., pages 53 - 60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret M Bradley</author>
<author>Peter J Lang</author>
</authors>
<title>Affective norms for English words (ANEW): Stimuli, instruction manual and affective ratings.</title>
<date>1999</date>
<tech>Tech. Rep. No. C-1</tech>
<marker>Bradley, Lang, 1999</marker>
<rawString>Margaret M. Bradley and Peter J. Lang. 1999. Affective norms for English words (ANEW): Stimuli, instruction manual and affective ratings. Tech. Rep. No. C-1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Cambria</author>
<author>Robert Speer</author>
<author>Catherine Havasi</author>
<author>Amir Hussain</author>
</authors>
<title>SenticNet: A Publicly Available Semantic Resource for Opinion Mining. AAAI fall symposium: commonsense knowledge,</title>
<date>2010</date>
<pages>14--18</pages>
<marker>Cambria, Speer, Havasi, Hussain, 2010</marker>
<rawString>Erik Cambria, Robert Speer, Catherine Havasi and Amir Hussain. 2010. SenticNet: A Publicly Available Semantic Resource for Opinion Mining. AAAI fall symposium: commonsense knowledge, pages 14 - 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Cambria</author>
<author>Catherine Havasi</author>
<author>Amir Hussain</author>
</authors>
<title>SenticNet 2: A Semantic and Affective Resource for Opinion Mining and Sentiment Analysis.</title>
<date>2012</date>
<booktitle>FLAIRS Conference,</booktitle>
<pages>202--207</pages>
<contexts>
<context position="1845" citStr="Cambria et al., 2012" startWordPosition="295" endWordPosition="298"> to detect opinions about entities (such as reviews) but it shows mixed results when complex opinions about events - involving perspectives and points of view - are expressed. In terms of parts of speech involved, SA approaches tend to focus on lexical items that explicitly convey opinions - mainly adjectives, adverbs and several nouns - leaving verbs on the foreground. Improvements have been proposed by taking into account syntax (Greene and Resnik 2009) and by investigating the connotative polarity of words (Cambria et al., 2009; Akkaya et al., 2009, Balhaur et al., 2011; Russo et al. 2011; Cambria et al., 2012, Deng et al., 2013 among others). One of the key aspects of sentiment analysis, which has been only marginally tackled so far, is the identification of implicit polarity. By implicit polarity we refer to the recognition of subjective textual units where no polarity markers are present but still people are able to state whether the text portion under analysis expresses a positive or negative sentiment. Recently, methodologies trying to address this aspect have been developed, incorporating ideas from linguistic and psychological studies on the subjective aspects of linguistic expressions. Aimi</context>
</contexts>
<marker>Cambria, Havasi, Hussain, 2012</marker>
<rawString>Erik Cambria, Catherine Havasi and Amir Hussain. 2012. SenticNet 2: A Semantic and Affective Resource for Opinion Mining and Sentiment Analysis. FLAIRS Conference, pages 202 - 207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ling Deng</author>
<author>Yoonjung Choi</author>
<author>Janyce Wiebe</author>
</authors>
<title>Benefactive/Malefactive Event and Writer Attitude Annotation.</title>
<date>2012</date>
<booktitle>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>120--125</pages>
<marker>Deng, Choi, Wiebe, 2012</marker>
<rawString>Ling Deng, Yoonjung Choi and Janyce Wiebe. 2012. Benefactive/Malefactive Event and Writer Attitude Annotation. Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 120 - 125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Jun Seok Kang</author>
<author>Polina Kuznetsova</author>
<author>Yejin Choi</author>
</authors>
<title>Connotation lexicon: A dash of sentiment beneath the surface meaning.</title>
<date>2013</date>
<booktitle>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1774--1784</pages>
<marker>Feng, Kang, Kuznetsova, Choi, 2013</marker>
<rawString>Song Feng, Jun Seok Kang, Polina Kuznetsova, and Yejin Choi. 2013. Connotation lexicon: A dash of sentiment beneath the surface meaning. Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1774 - 1784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>More than words: Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>Proceedings of human language technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>503--511</pages>
<contexts>
<context position="1684" citStr="Greene and Resnik 2009" startWordPosition="265" endWordPosition="268"> store polarity values. For bag-of-words approaches the polarity of a text depends on the presence/absence of a set of lexical items. This methodology is successful to detect opinions about entities (such as reviews) but it shows mixed results when complex opinions about events - involving perspectives and points of view - are expressed. In terms of parts of speech involved, SA approaches tend to focus on lexical items that explicitly convey opinions - mainly adjectives, adverbs and several nouns - leaving verbs on the foreground. Improvements have been proposed by taking into account syntax (Greene and Resnik 2009) and by investigating the connotative polarity of words (Cambria et al., 2009; Akkaya et al., 2009, Balhaur et al., 2011; Russo et al. 2011; Cambria et al., 2012, Deng et al., 2013 among others). One of the key aspects of sentiment analysis, which has been only marginally tackled so far, is the identification of implicit polarity. By implicit polarity we refer to the recognition of subjective textual units where no polarity markers are present but still people are able to state whether the text portion under analysis expresses a positive or negative sentiment. Recently, methodologies trying to</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. Proceedings of human language technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 503 - 511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher B Hadley</author>
<author>Donald G MacKay</author>
</authors>
<title>Does emotion help or hinder immediate memory? Arousal versus priority-binding mechanism.</title>
<date>2006</date>
<journal>Journal of Abnormal Psychology,</journal>
<volume>87</volume>
<issue>6</issue>
<pages>644--654</pages>
<contexts>
<context position="4351" citStr="Hadley and MacKay, 2006" startWordPosition="669" endWordPosition="672">mantic differential scales measuring three basic attitudes that people display crossculturally: evaluation (along the scale of adjectives “good-bad”), potency (along “strong-weak”) and activity (“active-passive”). This line of research continued with studies evaluating Osgood’s findings with different population and the pleasantness of words became also a dimension to correlate with other dimensions reported in semantic norms studies, such as familiarity and imagery. We know today that pleasantness is a semantic factor influencing short and long term memory (Monnier et al., 2008); similarly, (Hadley and MacKay, 2006) showed that STM for certain unpleasant emotional words (i.e., taboo words) was better than that for neutral words. Emotional words are better recalled because they are related to longterm representations of autobiographical and selfreference units (Ochsner, 2000). Other factors have a role: depressed subjects, for example, recalled more unpleasant words than pleasant words. Osgood’s studies were revised for the production of the Affective Norms for English Words (ANEW) (Bradley et al, 1999), a set of normative emotional ratings for 1034 words in American English. This set of verbal materials </context>
</contexts>
<marker>Hadley, MacKay, 2006</marker>
<rawString>Christopher B. Hadley and Donald G. MacKay. 2006. Does emotion help or hinder immediate memory? Arousal versus priority-binding mechanism. Journal of Abnormal Psychology, 87(6), pages 644 - 654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter M Lewinsohn</author>
<author>Christopher S Amenson</author>
</authors>
<title>Some Relations between Pleasant and Unpleasant Events and Depression.</title>
<date>1978</date>
<journal>Journal of Experimental Psychology: Learning, Memory, and Cognition,</journal>
<volume>32</volume>
<pages>79--88</pages>
<contexts>
<context position="2878" citStr="Lewinsohn and Amenson, 1978" startWordPosition="453" endWordPosition="456">Recently, methodologies trying to address this aspect have been developed, incorporating ideas from linguistic and psychological studies on the subjective aspects of linguistic expressions. Aiming at promoting a more holistic approach to sentiment analysis, combining the detection of implicit polarity with the expression of opinions on events, we propose CLIPEval, a task based on a dataset of events annotated as instantiations of pleasant and unpleasant events (PE/UPEs henceforth) previously collected in psychological research as the ones that correlate with mood (both good and bad feelings) (Lewinsohn and Amenson, 1978; MacPhillamy and Lewinsohn, 1982). 2 Measuring Emotional Connotations: Psychological Studies For a long time research in psychology has been interested in a subjective cultural and/or emotional coloration in addition to the explicit or denotative meaning of any specific word or phrase. Starting with the work of Charles E. Osgood, who in the 50s developed a technique for measuring the connotative meaning of concepts and analyzed human attitudes (Osgood et al., 1957), psychologists have experi443 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 443–450,</context>
<context position="6506" citStr="Lewinsohn and Amenson, 1978" startWordPosition="1003" endWordPosition="1006">ort that connotation lexicon guarantees better performance than other sentiment analysis lexicons that do not encode connotations on Twitter data. That said, when psychological experiments ask for judgments about single words they oversimplify: we experience the meanings of single words as arising from compositionality, in expressions and sentences. Even neutral words in specific contexts can acquire a polarity as effect of semantic prosody (Louw 1993). When subjects are asked for the pleasantness of an event they need to evaluate not just single words but complete sentences; for this reason (Lewinsohn and Amenson, 1978; MacPhillamy and Lewinsohn, 1982) developed two psychometric instruments, the Pleasant Events Schedule and the Unpleasant Events Schedule, by sampling events that were reported to be source of pleasure or distress by highly diverse samples of people that rated the frequency of event’s occurrence during past month plus a complete mood ratings. 3 CLIPEval Annotation The CLIPEval exercise provides the NLP community with a newly developed dataset grounded on psychological studies about the pleasantness of events. Dedicated annotation specifications and guidelines for the release of the dataset ha</context>
</contexts>
<marker>Lewinsohn, Amenson, 1978</marker>
<rawString>Peter M. Lewinsohn and Christopher S Amenson. 1978. Some Relations between Pleasant and Unpleasant Events and Depression. Journal of Experimental Psychology: Learning, Memory, and Cognition, 32, pages 79 - 88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas J MacPhillamy</author>
<author>Peter M Lewinsohn</author>
</authors>
<title>The Pleasant Event Schedule: Studies on Reliability, Validity, and Scale Intercorrelation.</title>
<date>1982</date>
<journal>Journal of Counseling and Clinical Psychology,</journal>
<volume>50</volume>
<issue>3</issue>
<pages>363--380</pages>
<contexts>
<context position="2912" citStr="MacPhillamy and Lewinsohn, 1982" startWordPosition="457" endWordPosition="460">g to address this aspect have been developed, incorporating ideas from linguistic and psychological studies on the subjective aspects of linguistic expressions. Aiming at promoting a more holistic approach to sentiment analysis, combining the detection of implicit polarity with the expression of opinions on events, we propose CLIPEval, a task based on a dataset of events annotated as instantiations of pleasant and unpleasant events (PE/UPEs henceforth) previously collected in psychological research as the ones that correlate with mood (both good and bad feelings) (Lewinsohn and Amenson, 1978; MacPhillamy and Lewinsohn, 1982). 2 Measuring Emotional Connotations: Psychological Studies For a long time research in psychology has been interested in a subjective cultural and/or emotional coloration in addition to the explicit or denotative meaning of any specific word or phrase. Starting with the work of Charles E. Osgood, who in the 50s developed a technique for measuring the connotative meaning of concepts and analyzed human attitudes (Osgood et al., 1957), psychologists have experi443 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 443–450, Denver, Colorado, June 4-5, 2015.</context>
<context position="6540" citStr="MacPhillamy and Lewinsohn, 1982" startWordPosition="1007" endWordPosition="1010">guarantees better performance than other sentiment analysis lexicons that do not encode connotations on Twitter data. That said, when psychological experiments ask for judgments about single words they oversimplify: we experience the meanings of single words as arising from compositionality, in expressions and sentences. Even neutral words in specific contexts can acquire a polarity as effect of semantic prosody (Louw 1993). When subjects are asked for the pleasantness of an event they need to evaluate not just single words but complete sentences; for this reason (Lewinsohn and Amenson, 1978; MacPhillamy and Lewinsohn, 1982) developed two psychometric instruments, the Pleasant Events Schedule and the Unpleasant Events Schedule, by sampling events that were reported to be source of pleasure or distress by highly diverse samples of people that rated the frequency of event’s occurrence during past month plus a complete mood ratings. 3 CLIPEval Annotation The CLIPEval exercise provides the NLP community with a newly developed dataset grounded on psychological studies about the pleasantness of events. Dedicated annotation specifications and guidelines for the release of the dataset have been developed. The starting po</context>
</contexts>
<marker>MacPhillamy, Lewinsohn, 1982</marker>
<rawString>Douglas J. MacPhillamy and Peter M. Lewinsohn. 1982. The Pleasant Event Schedule: Studies on Reliability, Validity, and Scale Intercorrelation. Journal of Counseling and Clinical Psychology, 50(3), pages 363 -380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Monnier</author>
<author>Arielle SyssauMonnier</author>
</authors>
<title>Semantic contribution to verbal short-term memory: Are pleasant words easier to remember than neutral words in serial recall and serial recognition?</title>
<date>2008</date>
<journal>Memory and Cognition,</journal>
<volume>36</volume>
<pages>35--42</pages>
<marker>Monnier, SyssauMonnier, 2008</marker>
<rawString>Catherine Monnier and Arielle SyssauMonnier. 2008. Semantic contribution to verbal short-term memory: Are pleasant words easier to remember than neutral words in serial recall and serial recognition? Memory and Cognition, 36, pages 35 - 42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Matthew Gormley</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Annotated gigaword.</title>
<date>2012</date>
<booktitle>Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,</booktitle>
<pages>95--100</pages>
<marker>Napoles, Gormley, Van Durme, 2012</marker>
<rawString>Courtney Napoles, Matthew Gormley and Benjamin Van Durme. 2012. Annotated gigaword. Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pages 95 - 100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin N Ochsner</author>
</authors>
<title>Are affective events richly recollected or simply familiar? The experience and process of recognizing feelings past.</title>
<date>2000</date>
<journal>Journal of Experimental Psychology. General,</journal>
<volume>129</volume>
<issue>2</issue>
<pages>242--261</pages>
<contexts>
<context position="4615" citStr="Ochsner, 2000" startWordPosition="711" endWordPosition="712">od’s findings with different population and the pleasantness of words became also a dimension to correlate with other dimensions reported in semantic norms studies, such as familiarity and imagery. We know today that pleasantness is a semantic factor influencing short and long term memory (Monnier et al., 2008); similarly, (Hadley and MacKay, 2006) showed that STM for certain unpleasant emotional words (i.e., taboo words) was better than that for neutral words. Emotional words are better recalled because they are related to longterm representations of autobiographical and selfreference units (Ochsner, 2000). Other factors have a role: depressed subjects, for example, recalled more unpleasant words than pleasant words. Osgood’s studies were revised for the production of the Affective Norms for English Words (ANEW) (Bradley et al, 1999), a set of normative emotional ratings for 1034 words in American English. This set of verbal materials have been rated in terms of pleasure, arousal, and dominance in order to create a standard for use in studies of emotion and attention (the same three basic dimensions used by Osgood). Affective valence (or pleasure, ranging from pleasant to unpleasant) and arousa</context>
</contexts>
<marker>Ochsner, 2000</marker>
<rawString>Kevin N. Ochsner. 2000. Are affective events richly recollected or simply familiar? The experience and process of recognizing feelings past. Journal of Experimental Psychology. General, 129 (2), pages 242 261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles E Osgood</author>
<author>George Suci</author>
<author>Percy Tannenbaum</author>
</authors>
<title>The Measurement of Meaning.</title>
<date>1957</date>
<publisher>University of Illinois Press,</publisher>
<contexts>
<context position="3348" citStr="Osgood et al., 1957" startWordPosition="524" endWordPosition="527">ceforth) previously collected in psychological research as the ones that correlate with mood (both good and bad feelings) (Lewinsohn and Amenson, 1978; MacPhillamy and Lewinsohn, 1982). 2 Measuring Emotional Connotations: Psychological Studies For a long time research in psychology has been interested in a subjective cultural and/or emotional coloration in addition to the explicit or denotative meaning of any specific word or phrase. Starting with the work of Charles E. Osgood, who in the 50s developed a technique for measuring the connotative meaning of concepts and analyzed human attitudes (Osgood et al., 1957), psychologists have experi443 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 443–450, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics mented with emotional values activated by words, often through the evaluation of their pleasantness. Osgood and his colleagues proposed a factor analysis based on semantic differential scales measuring three basic attitudes that people display crossculturally: evaluation (along the scale of adjectives “good-bad”), potency (along “strong-weak”) and activity (“active-passive”). This li</context>
</contexts>
<marker>Osgood, Suci, Tannenbaum, 1957</marker>
<rawString>Charles E. Osgood, George Suci and Percy Tannenbaum. 1957. The Measurement of Meaning. University of Illinois Press,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Saur´ı</author>
<author>James Pustejovsky</author>
</authors>
<title>FactBank: A corpus annotated with event factuality. Language resources and evaluation,</title>
<date>2009</date>
<volume>3</volume>
<pages>227--268</pages>
<marker>Saur´ı, Pustejovsky, 2009</marker>
<rawString>Roser Saur´ı and James Pustejovsky. 2009. FactBank: A corpus annotated with event factuality. Language resources and evaluation, 3, pages 227 - 268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
</authors>
<title>Dimitris Galanis, Pavlopoulos Ioannis, Harris Papageorgiou, Androutsopoulos Ion and Manandhar Suresh.</title>
<date>2014</date>
<booktitle>SemEval</booktitle>
<pages>27--35</pages>
<marker>Pontiki, 2014</marker>
<rawString>Maria Pontiki, Dimitris Galanis, Pavlopoulos Ioannis, Harris Papageorgiou, Androutsopoulos Ion and Manandhar Suresh. 2014. SemEval 2014 Task 4: Aspect Based Sentiment Analysis. Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27 - 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Alan Ritter</author>
<author>Preslav Nakov</author>
<author>Veselin Stoyanov</author>
</authors>
<date>2014</date>
<booktitle>SemEval-2014 Task 9: Sentiment Analysis in Twitter. Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>Rosenthal, Ritter, Nakov, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Alan Ritter, Preslav Nakov and Veselin Stoyanov. 2014. SemEval-2014 Task 9: Sentiment Analysis in Twitter. Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Russo</author>
<author>Tommaso Caselli</author>
<author>Francesco Rubino</author>
<author>Ester Boldrini</author>
<author>Patricio Barco Martinez</author>
</authors>
<title>EMOCause: An Easy-adaptable Approach to Extract Emotion Cause Contexts.</title>
<date>2011</date>
<booktitle>Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011),</booktitle>
<pages>152--160</pages>
<contexts>
<context position="1823" citStr="Russo et al. 2011" startWordPosition="291" endWordPosition="294">ology is successful to detect opinions about entities (such as reviews) but it shows mixed results when complex opinions about events - involving perspectives and points of view - are expressed. In terms of parts of speech involved, SA approaches tend to focus on lexical items that explicitly convey opinions - mainly adjectives, adverbs and several nouns - leaving verbs on the foreground. Improvements have been proposed by taking into account syntax (Greene and Resnik 2009) and by investigating the connotative polarity of words (Cambria et al., 2009; Akkaya et al., 2009, Balhaur et al., 2011; Russo et al. 2011; Cambria et al., 2012, Deng et al., 2013 among others). One of the key aspects of sentiment analysis, which has been only marginally tackled so far, is the identification of implicit polarity. By implicit polarity we refer to the recognition of subjective textual units where no polarity markers are present but still people are able to state whether the text portion under analysis expresses a positive or negative sentiment. Recently, methodologies trying to address this aspect have been developed, incorporating ideas from linguistic and psychological studies on the subjective aspects of lingui</context>
</contexts>
<marker>Russo, Caselli, Rubino, Boldrini, Martinez, 2011</marker>
<rawString>Irene Russo, Tommaso Caselli, Francesco Rubino, Ester Boldrini and Patricio Barco Martinez. 2011. EMOCause: An Easy-adaptable Approach to Extract Emotion Cause Contexts. Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011), pages 152 -160.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>