<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000039">
<title confidence="0.988219">
Relation Acquisition using Word Classes and Partial Patterns
</title>
<author confidence="0.6922375">
Stijn De Saeger†* Kentaro Torisawa† Masaaki Tsuchida§ Jun’ichi Kazama†
Chikara Hashimoto† Ichiro Yamada$ Jong Hoon Oh† Istv´an Varga† Yulan Yan†
</author>
<affiliation confidence="0.8985885">
† Information Analysis Laboratory, National Institute of
Information and Communications Technology, 619-0289 Kyoto, Japan
</affiliation>
<email confidence="0.978071">
{stijn,torisawa,kazama,ch,rovellia,istvan,yulan}@nict.go.jp
</email>
<note confidence="0.228751">
§ Information and Media Processing Laboratories, NEC Corporation, 630-0101 Nara, Japan
</note>
<email confidence="0.984903">
m-tsuchida@cq.jp.nec.com
</email>
<author confidence="0.987843">
$ Human &amp; Information Science Research Division,
</author>
<affiliation confidence="0.978214">
NHK Science &amp; Technology Research Laboratories, 157-8510 Tokyo, Japan
</affiliation>
<email confidence="0.994585">
yamada.i-hy@nhk.or.jp
</email>
<sectionHeader confidence="0.998579" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999105095238095">
This paper proposes a semi-supervised rela-
tion acquisition method that does not rely on
extraction patterns (e.g. “X causes Y” for
causal relations) but instead learns a combi-
nation of indirect evidence for the target re-
lation — semantic word classes and partial
patterns. This method can extract long tail
instances of semantic relations like causality
from rare and complex expressions in a large
Japanese Web corpus — in extreme cases, pat-
terns that occur only once in the entire cor-
pus. Such patterns are beyond the reach of cur-
rent pattern based methods. We show that our
method performs on par with state-of-the-art
pattern based methods, and maintains a rea-
sonable level of accuracy even for instances
acquired from infrequent patterns. This abil-
ity to acquire long tail instances is crucial for
risk management and innovation, where an ex-
haustive database of high-level semantic rela-
tions like causation is of vital importance.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.981470875">
Pattern based relation acquisition methods rely on
lexico-syntactic patterns (Hearst, 1992) for extract-
ing relation instances. These are templates of natu-
ral language expressions such as “X causes Y ” that
signal an instance of some semantic relation (i.e.,
causality). Pattern based methods (Agichtein and
Gravano, 2000; Pantel and Pennacchiotti, 2006b;
Pas¸ca et al., 2006; De Saeger et al., 2009) learn many
</bodyText>
<note confidence="0.8403235">
* This work was done when all authors were at the National
Institute of Information and Communications Technology.
</note>
<bodyText confidence="0.999573272727273">
such patterns to extract new instances (word pairs)
from the corpus.
However, since extraction patterns are learned us-
ing statistical methods that require a certain fre-
quency of observations, pattern based methods fail
to capture relations from complex expressions in
which the pattern connecting the two words is rarely
observed. Consider the following sentence:
“Curing hypertension alleviates the deteriora-
tion speed of the renal function, thereby lower-
ing the risk of causing intracranial bleeding”
Humans can infer that this sentence expresses a
causal relation between the underlined noun phrases.
But the actual pattern connecting them, i.e., “Cur-
ing X alleviates the deterioration speed of the re-
nal function, thereby lowering the risk of causing
Y ”, is rarely observed more than once even in a 108
page Web corpus. In the sense that the term pat-
tern implies a recurring event, this expression con-
tains no pattern for detecting the causal relation be-
tween hypertension and intracranial bleeding. This
is what we mean by “long tail instances” — words
that co-occur infrequently, and only in sparse extrac-
tion contexts.
Yet an important application of relation extraction
is mining the Web for so-called unknown unknowns
— in the words of D. Rumsfeld, “things we don’t
know we don’t know” (Torisawa et al., 2010). In
knowledge discovery applications like risk manage-
ment and innovation, the usefulness of relation ex-
traction lies in its ability to find many unexpected
remedies for diseases, causes of social problems,
and so on. To give an example, our relation extrac-
</bodyText>
<page confidence="0.981557">
825
</page>
<note confidence="0.957874">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 825–835,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999952744680851">
tion system found a blog post mentioning Japanese
automaker Toyota as a hidden cause of Japan’s de-
flation. Several months later the same connection
was made in an article published in an authoritative
economic magazine.
We propose a semi-supervised relation extraction
method that does not rely on direct pattern evidence
connecting the two words in a sentence. We argue
that the role of binary patterns can be replaced by a
combination of two types of indirect evidence: se-
mantic class information about the target relation
and partial patterns, which are fragments or sub-
patterns of binary patterns. The intuition is this: if
a sentence like the example sentence above contains
some word X belonging to the class of medical con-
ditions and another word Y from the class of trau-
mas, and X matches the partial pattern “... causing
X”, there is a decent chance that this sentence ex-
presses a causal relation between X and Y . We
show that just using this combination of indirect
evidence we can pick up semantic relations with
roughly 50% precision, regardless of the complexity
or frequency of the expression in which the words
co-occur. Furthermore, by combining this idea with
a straightforward machine learning approach, the
overall performance of our method is on par with
state-of-the-art pattern based methods. However,
our method manages to extract a large number of
instances from sentences that contain no pattern that
can be learned by pattern induction methods.
Our method is a two-stage system. Figure 1
presents an overview. In Stage 1 we apply a state-
of-the-art pattern based relation extractor to a Web
corpus to obtain an initial batch of relation instances.
In Stage 2 a supervised classifier is built from vari-
ous components obtained from the output of Stage
1. Given the output of Stage 1 and access to a
Web corpus, the Stage 2 extractor is completely
self-sufficient, and the whole method requires no
supervision other than a handful of seed patterns
to start the first stage extractor. The whole proce-
dure is therefore minimally supervised. Semantic
word classes and partial patterns play a crucial role
throughout all steps of the process.
We evaluate our method on three relation acqui-
sition tasks (causation, prevention and material re-
lations) using a 600 million Japanese Web page cor-
</bodyText>
<figureCaption confidence="0.999863">
Figure 1: Proposed method: data flow.
</figureCaption>
<bodyText confidence="0.999940176470588">
pus (Shinzato et al., 2008) and show that our sys-
tem can successfully acquire relations from both
frequent and infrequent patterns. Our system ex-
tracted 100,000 causal relations with 84.6% preci-
sion, 50,000 prevention relations with 58.4% preci-
sion and 25,000 material relations with 76.1% preci-
sion. In the extreme case, we acquired several thou-
sand word pairs co-occurring only in patterns that
appear once in the entire corpus. We call such pat-
terns single occurrence (SO) patterns. Word pairs
that co-occur only with SO patterns represent the
theoretical limiting case of relations that cannot be
acquired using existing pattern based methods. In
this sense our method can be seen as complemen-
tary with pattern based approaches, and merging our
method’s output with that of a pattern based method
may be beneficial.
</bodyText>
<sectionHeader confidence="0.681295" genericHeader="method">
2 Stage 1 Extractor
</sectionHeader>
<bodyText confidence="0.998654071428571">
This section introduces our Stage 1 extractor: the
pattern based method from (De Saeger et al., 2009),
which we call CDP for “class dependent patterns”.
We give a brief overview below, and refer the reader
to the original paper for a more comprehensive ex-
planation.
CDP takes a set of seed patterns as input, and au-
tomatically learns new class dependent patterns as
paraphrases of the seed patterns. Class dependent
patterns are semantic class restricted versions of or-
dinary lexico-syntactic patterns. Existing methods
use class independent patterns such as “X causes
Y ” to learn causal relations between X and Y . Class
dependent patterns however place semantic class re-
</bodyText>
<page confidence="0.997477">
826
</page>
<bodyText confidence="0.98828697826087">
strictions on the noun pairs they may extract, like
“Yaccidents causes Xincidents”. The accidents and
incidents subscripts specify the semantic class of the
X and Y slot fillers.
These class restrictions make it possible to distin-
guish between multiple senses of highly ambiguous
patterns (so-called “generic” patterns). For instance,
given the generic pattern “Y by X”, if we restrict
Y and X in to the semantic classes of injuries and
accidents (as in “death by drowning”), the class de-
pendent pattern “Yinjuries by Xaccidents” becomes a
valid paraphrase of “X causes Y ” and can safely be
used to extract causal relations, whereas other class
dependent versions of the same generic pattern (e.g.,
“Yproducts by Xcompanies”, as in “iPhone by Apple”)
may not.
CDP ranks each noun pair in the corpus accord-
ing to a score that reflects its likelihood of being
a proper instance of the target relation, by calcu-
lating the semantic similarity of a set of seed pat-
terns to the class dependent patterns this noun pair
co-occurs with. The output of CDP is a list of noun
pairs ranked by score, together with the highest scor-
ing class dependent pattern each noun pair co-occurs
with. This list becomes the input to Stage 2 of our
method, as shown in Figure 1. We adopted CDP as
Stage 1 extractor because, besides having generally
good performance, the class dependent patterns pro-
vide the two fundamental ingredients for Stage 2 of
our method — the target semantic word classes for a
given relation (in the form of the semantic class re-
strictions attached to patterns), and partial patterns.
To obtain fine-grained semantic word classes we
used the large scale word clustering algorithm from
(Kazama and Torisawa, 2008), which uses the EM
algorithm to compute the probability that a word w
belongs to class c, i.e., P(c|w). Probabilistic cluster-
ing defines no discrete boundary between members
and non-members of a semantic class, so we simply
assume w belongs to c whenever P(c|w) &gt; 0.2. For
this work we clustered 106 nouns into 500 classes.
Finally, we adopt the structural representation of
patterns introduced in (Lin and Pantel, 2001). All
sentences in our corpus are dependency parsed, and
patterns consist of words on the path of dependency
relations connecting two nouns.
</bodyText>
<sectionHeader confidence="0.971224" genericHeader="method">
3 Stage 2 Extractor
</sectionHeader>
<bodyText confidence="0.977311163265306">
We use CDP as our Stage 1 extractor, and the top
N noun pairs along with the class dependent pat-
terns that extract them are given as input to Stage 2,
which represents the main contribution of this work.
As shown in Figure 1, Stage 2 consists of three mod-
ules: a candidate generator, a training data gener-
ator and a supervised classifier. The training data
generator builds training data for the classifier from
the top N output of CDP and sentences retrieved
from the Web corpus. This classifier then scores and
ranks the candidate relations generated by the can-
didate relation generator. We introduce each module
below.
Candidate Generator This module generates
sentences containing candidate word pairs for the
target relation from the corpus. It does so using the
semantic class restrictions and partial patterns ob-
tained from the output of CDP. The set of all seman-
tic class pairs obtained from the class dependent pat-
terns that extracted the top N results become the tar-
get semantic class pairs from which new candidate
instances are generated. We extract all sentences
containing a word pair belonging to one of the target
class pairs from the corpus.
From these sentences we keep only those that con-
tain a trace of evidence for the target semantic re-
lation. For this we decompose the class dependent
patterns from the Stage 1 extractor into partial pat-
terns. As mentioned previously, patterns consist of
words on the path of dependency relations connect-
ing the two target words in a syntactic tree. To obtain
partial patterns we split this dependency path into its
two constituent branches, each one leading from the
leaf word (i.e. variable) to the syntactic head of the
pattern. For example, “X ����
�� causes ���
�� Y ” is
split into two partial patterns “X ����
�� causes” and
���
“causes �� Y ”. These partial patterns capture the
predicate structures in binary patterns.&apos; We discard
partial patterns with syntactic heads other than verbs
or adjectives.
The candidate genarator retrieves all sentences
from the corpus in which two nouns belonging to
one of the target semantic classes co-occur and
&apos; In Japanese, case information is encoded in post-positions
attached to the noun.
</bodyText>
<page confidence="0.98927">
827
</page>
<bodyText confidence="0.999987882352941">
where at least one of the nouns matches a partial pat-
tern. As shown in Figure 1, these sentences and the
candidate noun pairs they contain (called (noun pair,
sentence) triples hereafter) are submitted to the clas-
sifier for scoring. Restricting candidate noun pairs
by this combination of semantic word classes and
partial pattern matching proved to be quite powerful.
For instance, in the case of causal relations we found
that close to 60% of the (noun pair, sentence) triples
produced by the candidate generator were correct
(Figure 6).
Training Data Generator As shown in Figure 1,
the (noun pair, sentence) triples used as training data
for the SVM classifier were generated from the top
results of the Stage 1 extractor and the corpus. We
consider the noun pairs in the top N output of the
Stage 1 extractor as true instances of the target re-
lation (even though they may contain erroneous ex-
tractions), and retrieve from the corpus all sentences
in which these noun pairs co-occur and that match
one of the partial patterns mentioned above. In our
experiments we set N to 25, 000. We randomly se-
lect positive training samples from this set of (noun
pair, sentence) triples.
Negative training samples are also selected ran-
domly, as follows. If one member of the target noun
pair in the positive samples above matches a partial
pattern but the other does not, we randomly replace
the latter by another noun found in the same sen-
tence, and generate this new (noun pair, sentence)
triple as a negative training sample. In the causal
relation experiments this approach had about 5%
chance of generating false negatives — noun pairs
contained in the top N results of the Stage 1 extrac-
tor. Such samples were discarded. Our experimen-
tal results show that this scheme works quite well in
practice. We randomly sample M positive and neg-
ative samples from the autogenerated training data
to train the SVM. M was empirically set to 50,000
in our experiments.
SVM Classifier We used a straightforward fea-
ture set for training the SVM classifier. Because
our classifier will be faced with sentences contain-
ing long and infrequent patterns where the distance
between the two target nouns may be quite large,
we did not try to represent lexico-syntactic patterns
as features but deliberately restricted the feature set
to local context features of the candidate noun pair
in the target sentence. Concretely, we looked at bi-
grams and unigrams surrounding both nouns of the
candidate relation, as the local context around the
target words may contain many telling expressions
like “increase in X” or “X deficiency” which are use-
ful clues for causal relations. Also, in Japanese case
information is encoded in post-positions attached to
the noun, which is captured by the unigram features.
In addition to these base features, we include the
semantic classes to which the candidate noun pair
belongs, the partial patterns they match in this sen-
tence, and the infix words inbetween the target noun
pair. Note that this feature set is not intended to
be optimal beyond the actual claims of this paper,
and we have deliberately avoided exhaustive fea-
ture engineering so as not to obscure the contribu-
tion of semantic classes and partial pattern to our
approach. Clearly an optimal classifier will incorpo-
rate many more advanced features (see GuoDong et
al. (2005) for a comprehensive overview), but even
without sophisticated feature engineering our clas-
sifier achieved sufficient performance levels to sup-
port our claims. An overview of the feature set is
given in Table 1. The relative contribution of each
type of features is discussed in section 4. In prelim-
inary experiments we found a polynomial kernel of
degree 3 gave the best results, which suggests the ef-
fectiveness of combining different types of indirect
evidence.
The SVM classifier outputs (noun pair, sentence)
triples, ranked by SVM score. To obtain the final
output of our method we assign each unique noun
pair the maximum score from all (noun pair, sen-
tence) triples it occurs in, and discard all other sen-
tences for this noun pair. In section 4 below we eval-
uate the acquired noun pairs in the context of the
sentence that maximizes their score.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999947714285714">
We demonstrate the effectiveness of semantic word
classes and partial pattern matching for relation ex-
traction by showing that the method proposed in this
paper performs at the level of other state-of-the-art
relation acquisition methods. In addition we demon-
strate that our method can successfully extract re-
lation instances from infrequent patterns, and we
</bodyText>
<page confidence="0.994148">
828
</page>
<table confidence="0.999402833333333">
Feature type Description Number of features
Morpheme features Unigram and bigram morphemes surrounding both target words. 554,395
POS features Coarse- and fine-grained POS tags of the noun pair and morpheme features. 2,411
Semantic features Semantic word classes of the target noun pair. 1000 (500 classes x2)
Infix word features Morphemes found inbetween the target noun pair. 94,448
Partial patterns Partial patterns matching the target noun pair. 86
</table>
<tableCaption confidence="0.9999">
Table 1: Feature set used in the Stage 2 classifier, and their number for the causal relation experiments.
</tableCaption>
<bodyText confidence="0.999907666666667">
explore several criteria for what constitutes an in-
frequent pattern — including the theoretical limit-
ing case of patterns observed only once in the en-
tire corpus. These instances are impossible to ac-
quire by pattern based methods. The ability to ac-
quire relations from extremely infrequent expres-
sions with decent accuracy demonstrates the utility
of combining semantic word classes with partial pat-
tern matching.
</bodyText>
<subsectionHeader confidence="0.993665">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999940115384615">
We evaluate our method on three semantic relation
acquisition tasks: causality, prevention and mate-
rial. Two concepts stand in a causal relation when
the source concept (the “cause”) is directly or indi-
rectly responsible for the subsequent occurrence of
the target concept (its “effect”). In a prevention rela-
tion the source concept directly or indirectly acts to
avoid the occurrence of the target concept, and in a
material relation the source concept is a material or
ingredient of the target concept.
For our experiments we used the latest version
of the TSUBAKI corpus (Shinzato et al., 2008),
a collection of 600 million Japanese Web pages
dependency parsed by the Japanese dependency
parser KNP2. In our implementation of CDP, lexico-
syntactic patterns consist of words on the path con-
necting two nouns in a dependency parse tree. We
discard patterns from dependency paths longer than
8 constituent nodes. Furthermore, we estimated pat-
tern frequencies in a subset of the corpus (50 million
pages, or 1/12th of the entire corpus) and discarded
patterns that co-occur with less than 10 unique noun
pairs in this smaller corpus. These restrictions do
not apply to the proposed method, which can extract
noun pairs connected by patterns of arbitrary length,
even if found only once in the corpus. For our pur-
</bodyText>
<footnote confidence="0.68671">
2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
</footnote>
<bodyText confidence="0.9999061">
pose we treat dependency paths whose observed fre-
quency is below this threshold as insufficiently fre-
quent to be considered as “patterns”. This threshold
is of course arbitrary, but in section 4 we show that
our results are not affected by these implementation
details.
We asked three human judges to evaluate ran-
dom (noun pair, sentence) triples, i.e. candidate
noun pairs in the context of some corpus sentence
in which they co-occur. If the judges find the sen-
tence contains sufficient evidence that the target re-
lation holds between the candidate nouns, they mark
the noun pair correct. To evaluate the performance
of each method we use two evaluation criteria: strict
(all judges must agree the candidate relation is cor-
rect) and lenient (decided by the judges’ majority
vote). Over all experiments the interrater agreement
(Kappa) ranged between 0.57 and 0.82 with an aver-
age of 0.72, indicating substantial agreement (Lan-
dis and Koch, 1977).
</bodyText>
<subsubsectionHeader confidence="0.55034">
4.1.1 Methods Compared
</subsubsectionHeader>
<bodyText confidence="0.999859133333333">
We compare our results to two pattern based
methods: CDP (the Stage 1 extractor) and Espresso
(Pantel and Pennacchiotti, 2006a).
Espresso is a popular bootstrapping based method
that uses a set of seed instances to induce extraction
patterns for the target relation and then acquire new
instances in an iterative bootstrapping process. In
each iteration Espresso performs pattern induction,
pattern ranking and selection using previously ac-
quired instances, and uses the newly acquired pat-
terns to extraction new instances. Espresso com-
putes a reliability score for both instances and pat-
terns based on their pointwise mutual information
(PMI) with the top-scoring patterns and instances
from the previous iteration.3 We refer to (Pantel and
</bodyText>
<footnote confidence="0.9877795">
3 In our implementation of Espresso we found that, despite
the many parameters for controlling the bootstrapping process,
</footnote>
<page confidence="0.992497">
829
</page>
<figure confidence="0.959629052631579">
100%
90%
80%
70%
60%
50%
40%
30%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)
CDP (L)
CDP (S)
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)
CDP (L)
CDP (S)
Proposed (L)
Proposed (S)
</figure>
<figureCaption confidence="0.9615246">
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Figure 2: Precision of acquired relations (causality). L
and S denote lenient and strict evaluation.
</figureCaption>
<bodyText confidence="0.999580923076923">
Pennacchiotti, 2006a) for a more detailed descrip-
tion.
For all methods compared we rank the acquired
noun pairs by their score and evaluated 500 random
samples from the top 100,000 results. For noun pairs
acquired by CDP and Espresso we select the pattern
that extracted this noun pair (in the case of Espresso,
the pattern with the highest PMI for this noun pair),
and randomly select a sentence in which the noun
pair co-occurs with that pattern from our corpus. For
the proposed method we evaluate noun pairs in the
context of the (noun pair, sentence) triple with the
highest SVM score.
</bodyText>
<subsectionHeader confidence="0.551513">
4.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999791842105263">
The performance of each method on the causality,
prevention and material relations are shown in Fig-
ures 2, 3 and 4 respectively. In the causality exper-
iments (Figure 2) the proposed method performs on
par with CDP for the top 25,000 results, both achiev-
ing close to 90% precision. But whereas CDP’s per-
it remains very difficult to prevent semantic drift (Komachi et
al., 2008) from occurring. One small adjustment to the al-
gorithm stabilized the bootstrapping process considerably and
gave overall better results. In the pattern induction step (sec-
tion 3.2 in (Pantel and Pennacchiotti, 2006a)), Espresso com-
putes a reliability score for each candidate pattern based on the
weighted PMI of the pattern with all instances extracted so far.
As the number of extracted instances increases this dispropor-
tionally favours high frequency (i.e. generic) patterns, so in-
stead of using all instances for computing pattern reliability we
only use the m most reliable instances from the previous iter-
ation, which were used to extract the candidate patterns of the
current iteration (m = 200, like the original).
</bodyText>
<figureCaption confidence="0.993571">
Figure 3: Precision of acquired relations (prevention). L
and S denote lenient and strict evaluation.
</figureCaption>
<figure confidence="0.999777833333333">
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
</figure>
<figureCaption confidence="0.998575">
Figure 4: Precision of acquired relations (material). L
and S denote lenient and strict evaluation.
</figureCaption>
<bodyText confidence="0.9999243125">
formance drops from there our method maintains
the same high precision throughout (84.6%, lenient).
Both our method and CDP outperform Espresso by
a large margin.
For the prevention relation (Figure 3), precision
is considerably lower for all methods except the top
10,000 of CDP (82% precision, lenient). The pro-
posed method peaks at around 20,000 results (67%
precision, lenient) and performance remains more or
less constant from there on. The proposed method
overtakes CDP’s performance around the top 45,000
mark, which suggests that combining the results of
both methods may be beneficial.
In the material relations the proposed method
slightly outperforms both pattern based methods
in the top 10,000 results (92% precision, lenient).
</bodyText>
<figure confidence="0.616638857142857">
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)
CDP (L)
CDP (S)
Proposed (L)
Proposed (S)
</figure>
<figureCaption confidence="0.887155">
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
</figureCaption>
<page confidence="0.949477">
830
</page>
<note confidence="0.614443">
% of all samples
</note>
<bodyText confidence="0.999925531914894">
However for this relation our method produced only
35,409 instances. The reason is that the top 25,000
results of CDP were all extracted by just 12 patterns,
and these contained many patterns whose syntactic
head is not a verb or adjective (like “Y rich in X” or
“Y containing X”). Only 12 partial patterns were ob-
tained, which greatly reduced the output of the pro-
posed method. Taking into account the high perfor-
mance of CDP for material relations, this suggests
that for some relations our method’s N and M pa-
rameters could use some tuning. In conclusion, in
all three relations our method performs at a level
comparable to state-of-the-art pattern based meth-
ods, which is remarkable given that it only uses in-
direct evidence.
Dealing with Difficult Extractions How does our
method handle noun pairs that are difficult to ac-
quire by pattern based methods? The graphs marked
“Prop. w/o CDP” (Proposed without CDP) in Fig-
ures 2 , 3 and 4 show the number and precision of
evaluated samples from the proposed method that do
not co-occur in our corpus with any of the patterns
that extracted the top N results of the first stage ex-
tractor. These graphs show that our method is not
simply regenerating CDP’s top results but actually
extracts many noun pairs that do not co-occur in pat-
terns that are easily learned. Figure 2 shows that
roughly two thirds of the evaluated samples are in
this category, and that their performance is not sig-
nificantly worse than the overall result. The same
conclusion holds for the prevention results (Figure
3), where over 80% of the proposed method’s sam-
ples are noun pairs that do not co-occur with eas-
ily learnable patterns. Their precision is about 5%
worse than all samples from the proposed method.
For material relations (Figure 4) about half of all
evaluated samples are in this category, but their pre-
cision is markedly worse compared to all results.
For genuinely infrequent patterns, the graphs
marked “Prop. w/o pattern” (Proposed without pat-
tern) in Figures 2 , 3 and 4 show the number and
precision of noun pairs evaluated for the proposed
method that were acquired from sentences without
any discernible pattern. As explained in section 4
above, these constitute noun pairs co-occurring in a
sentence in which the path of dependency relations
connecting them is either longer than 8 nodes or can
</bodyText>
<figure confidence="0.994043">
20
15
10
5
0 1 2 32 1024 32768 1.05x106 3.36x107
# of noun pairs co-occurring with patterns
</figure>
<figureCaption confidence="0.9779966">
Pattern frequency, CDP
Pattern frequency, Proposed
Pattern frequency, Espresso
Figure 5: Frequencies of patterns in the evaluation data
(causation).
</figureCaption>
<bodyText confidence="0.9592079375">
extract fewer than 10 noun pairs in 50 million Web
pages. Note that in theory it is possible that these
noun pairs could not be acquired by pattern based
methods due to this threshold — patterns must be
able to extract more than 10 different noun pairs in
a subset of our corpus, while the proposed method
does not have this constraint. So at least in the-
ory, pattern based methods might be able to acquire
all noun pairs obtained by our method by lowering
this threshold. To see that this is unlikely to be the
case, consider Figure 5, which shows the pattern fre-
quency of the patterns induced by CDP and Espresso
for the causality experiment. The x-axis represents
pattern frequency in terms of the number of unique
noun pairs a pattern co-occurs with in our corpus (on
a log scale), and the y-axis shows the percentage of
samples that was extracted by patterns of a given fre-
quency.4 Figure 5 shows that for the pattern based
methods, the large majority of noun pairs was ex-
tracted by patterns that co-occur with several thou-
sand different noun pairs. Extrapolating the original
frequency threshold of 10 nounpairs to the size of
our entire corpus roughly corresponds to about 120
distinct noun pairs (10 times in 1/12th of the entire
corpus). In Figure 5, the histograms for the pattern
based methods CDP and Espresso start around 1000
noun pairs, which is far above this new lowerbound.
4 In the case of CDP we ignore semantic class restrictions
on the patterns when comparing frequencies. For Espresso, the
most frequent pattern (“Y by X” at the 24,889,329 data point
on the x-axis) extracted up to 53.8% of the results, but the graph
was cut at 20% for readability.
</bodyText>
<page confidence="0.992727">
831
</page>
<table confidence="0.999701944444445">
Causality ⟨ ⟩ [ ]
Because ⟨catecholamine⟩ causes a rapid increase of heart rate, the change of circulation inside the blood vessels leads to blood vessel
disorders and promotes [thrombus generation].
⟨ ⟩ [ ]
When we injected Xylocaine during a ⟨tachycardia seizure⟩, the patient suddenly lost consciousness and fell into a fit of [convulsions].
⟨ ⟩ [ ]
(...) The reason is that by taking a lot of ⟨animal proteins⟩ the causative agents of [tragomaschalia] increase.
* ⟨ ⟩ [ ]
* [Radon] heightens the (body’s) antioxidative function and is effective for eliminating activated oxygen, which is a cause of aging and
⟨lifestyle-related⟩ diseases.
Prevention ⟨ ⟩ [ ]
Because the fatty meat of tuna contains DHA and ⟨EPA⟩ in abundance, it is effective for preventing [neuralgia].
⟨ ⟩ [ ]
If you use ⟨nitrogen gas⟩ instead of air you may prevent [dust explosions].
⟨ ⟩ [ ]
In ancient Europe ⟨orthosiphon aristatus⟩ tea was called a “diet tea”, and supposedly it helps preventing triglycerides and [adult diseases].
* ⟨ ⟩ [ ]
* (It) is something that prevents [scratches] on the screen if the ⟨calash⟩ gets stuck between the screens during storage.
</table>
<tableCaption confidence="0.9869245">
Table 2: Causality and Prevention relations acquired from Single Occurrence (SO) patterns. ⟨X⟩ and [Y] indicate the
relation instance’s source and target words, and “*” indicates erroneous extractions.
</tableCaption>
<bodyText confidence="0.996667592592593">
Thus, pattern based methods naturally tend to induce
patterns that are much more frequent than the range
of patterns our method can capture, and it is unlikely
that this is a result of implementation details like pat-
tern frequency threshold.
The precision of noun pairs in the category “Prop.
w/o pattern” is clearly lower than the overall re-
sults, but the graphs demonstrate that our method
still handles these difficult cases reasonably well.
The 500 samples evaluated contained 155 such in-
stances for causality, 403 for prevention and 276 for
material. For prevention, the high ratio of these noun
pairs helps explain why the overall performance was
lower than for the other relations.
Finally, the theoretical limiting case for pattern
based algorithms consists of patterns that only co-
occur with a single noun pair in the entire corpus
(single occurrence or SO patterns). Pattern based
methods learn new patterns that share many noun
pairs with a set of reliable patterns in order to extract
new relation instances. If a noun pair that co-occurs
with a SO pattern also co-occurs with more reliable
patterns there is no need to learn the SO pattern. If
that same noun pair does not co-occur with any other
reliable pattern, the SO pattern is beyond the reach
of any pattern induction method. Thus, SO patterns
are effectively useless for pattern based methods.
For the 500 samples evaluated from the causality
and prevention relations acquired by our method we
found 7 causal noun pairs that co-occur only in SO
patterns and 29 such noun pairs for prevention. The
precision of these instances was 42.9% and 51.7%
respectively. In total we found 8,716 causal noun
pairs and 7,369 prevention noun pairs that co-occur
only with SO patterns. Table 2 shows some example
relations from our causality and prevention experi-
ments that were extracted from SO patterns. To con-
clude, our method is able to acquire correct relations
even from the most extreme infrequent expressions.
Semantic Classes, Partial Patterns or Both? In
the remainder of this section we look at how the
combination of semantic word classes and partial
patterns benefits our method. For each relation we
evaluated 1000 random (noun pair, sentence) triples
satisfying the two conditions from section 3 —
matching semantic class pairs and partial patterns.
Surprisingly, the precision of these samples was
59% for causality, 40% for prevention and 50.4%
for material, showing just how compelling these two
types of indirect evidence are in combination.
To estimate the relative contribution of each
heuristic we compared our candidate generation
method against two baselines. The first baseline
evaluates the precision of random noun pairs from
</bodyText>
<page confidence="0.992124">
832
</page>
<table confidence="0.9912036">
100 100
90 90
80
70
60
50
40
30
precision (%) 80 precision (%)
70
60
50
precision (%)
0 200 400 600 800 1000
(noun pair, sentence) triples ranked by score
Base features only
All minus semantic classes
All minus infix words
All minus partial patterns
All features
</table>
<figureCaption confidence="0.998789">
Figure 6: Contribution of feature sets (causality).
</figureCaption>
<figure confidence="0.994673266666667">
100
90
80
70
60
50
40
30
0 200 400 600 800 1000
(noun pair, sentence) triples ranked by score
Base features only
All minus semantic classes
All minus infix words
All minus partial patterns
All features
</figure>
<figureCaption confidence="0.999996">
Figure 7: Contribution of feature sets (prevention).
</figureCaption>
<bodyText confidence="0.999944055555556">
the target semantic classes that co-occur in a sen-
tence. The second baseline does the same for the
second heuristic, selecting random sentences con-
taining a noun pair that matches some partial pat-
tern. Evaluating 100 samples for causality and pre-
vention, we found the precision of the semantic class
baseline was 16% for causality and 5% for preven-
tion. The pattern fragment baseline gave 9% for
causality and 22% for prevention. This is consid-
erably lower than the precision of random samples
that satisfy both the semantic class and partial pat-
tern conditions, showing that the combination of se-
mantic classes and partial patterns is more effective
than either one individually.
Finally, we investigated the effect of the various
feature sets used in the classifier. Figures 6, 7 and
8 show the results for the respective semantic re-
lations. The “Base features” graph shows the per-
</bodyText>
<figure confidence="0.917319142857143">
0 200 400 600 800 1000
(noun pair, sentence) triples ranked by score
Base features only
All minus semantic classes
All minus infix words
All minus partial patterns
All features
</figure>
<figureCaption confidence="0.999946">
Figure 8: Contribution of feature sets (material).
</figureCaption>
<bodyText confidence="0.999577133333333">
formance the unigram, bigram and part-of-speech
features. “All features” uses all features in Table
1. The other graphs show the effect of removing
one type of features. These graphs suggest that the
contribution of the individual feature types (seman-
tic class information, partial patterns or infix words)
to the classification performance is relatively minor,
but in combination they do give a marked improve-
ment over the base features, at least for some rela-
tions like causation and material. In other words,
the main contribution of semantic word classes and
partial patterns to our method’s performance lies not
in the final classification step but seems to occur at
earlier stages of the process, in the candidate and
training data generation steps.
</bodyText>
<sectionHeader confidence="0.999984" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9999788">
Using lexico-syntactic patterns to extract semantic
relations was first explored by Hearst (Hearst, 1992),
and has inspired a large body of work on semi-
supervised relation acquisition methods (Berland
and Charniak, 1999; Agichtein and Gravano, 2000;
Etzioni et al., 2004; Pantel and Pennacchiotti,
2006b; Pas¸ca et al., 2006; De Saeger et al., 2009),
two of which were used in this work.
Some researchers have addressed the sparse-
ness problems inherent in pattern based methods.
Downey et al. (2007) starts from the output of
the unsupervised information extraction system Tex-
tRunner (Banko and Etzioni, 2008), and uses lan-
guage modeling techniques to estimate the reliabil-
ity of sparse extractions. Pas¸ca et al. (2006) alle-
</bodyText>
<page confidence="0.995996">
833
</page>
<bodyText confidence="0.999971981481482">
viates pattern sparseness by using infix patterns that
are generalized using classes of distributionally sim-
ilar words. In addition, their method employs clus-
tering based semantic similarities to filter newly ex-
tracted instances in each iteration of the bootstrap-
ping process. A comparison with our method would
have been instructive, but we were unable to imple-
ment their method because the original paper con-
tains insufficient detail to allow replication.
There is a large body of research in the super-
vised tradition that does not use explicit pattern rep-
resentations — kernel based methods (Zelenko et
al., 2003; Culotta, 2004; Bunescu and Mooney,
2005) and CRF based methods (Culotta et al., 2006).
These approaches are all fully supervised, whereas
in our work the automatic generation of candi-
dates and training data is an integral part of the
method. An interesting alternative is distant super-
vision (Mintz et al., 2009), which trains a classi-
fier using an existing database (Freebase) containing
thousands of semantic relations, with millions of in-
stances. We believe our method is more general, as
depending on external resources like a database of
semantic relations limits both the range of seman-
tic relations (i.e., Freebase contains only relations
between named entities, and none of the relations
in this work) and languages (i.e., no resource com-
parable to Freebase exists for Japanese) to which
the technology can be applied. Furthermore, it is
unclear whether distant supervision can deal with
noisy input such as automatically acquired relation
instances.
Finally, inference based methods (Carlson et al.,
2010; Schoenmackers et al., 2010; Tsuchida et al.,
2010) are another attempt at relation acquisition that
goes beyond pattern matching. Carlson et al. (2010)
proposed a method based on inductive logic pro-
gramming (Quinlan, 1990). Schoenmackers et al.
(2010) takes relation instances produced by Tex-
tRunner (Banko and Etzioni, 2008) as input and in-
duces first-order Horn clauses, and new instances are
infered using a Markov Logic Network (Richardson
and Domingo, 2006; Huynh and Mooney, 2008).
Tsuchida et al. (2010) generated new relation hy-
potheses by substituting words in seed instances
with distributionally similar words. The difference
between these works and ours lies in the treatment
of evidence. While the above methods learn infer-
ence rules to acquire new relation instances from in-
dependent information sources scattered across dif-
ferent Web pages, our method takes the other option
of working with all the clues and indirect evidence a
single sentence can provide. In the future, a combi-
nation of both approaches may prove beneficial.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999990941176471">
We have proposed a relation acquisition method that
is able to acquire semantic relations from infrequent
expressions by focusing on the evidence provided by
semantic word classes and partial pattern matching
instead of direct extraction patterns. We experimen-
tally demonstrated the effectiveness of this approach
on three relation acquisition tasks, causality, preven-
tion and material relations. In addition we showed
our method could acquire a significant number of
relation instances that are found in extremely infre-
quent expressions, the most extreme case of which
are single occurrence patterns, which are beyond
the reach of existing pattern based methods. We be-
lieve this ability is of crucial importance for acquir-
ing valuable long tail instances. In future work we
will investigate whether the current framework can
be extended to acquire inter-sentential relations.
</bodyText>
<sectionHeader confidence="0.998686" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996500181818182">
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proc. of the fifth ACM conference on Digital li-
braries, pages 85–94.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proc. of the 46th ACL-08:HLT, pages 28–36.
Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 57–64, College Park, Mary-
land, USA, June.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (HLT ’05), pages 724–731.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for neverend-
ing language learning. In Proc of the 24th AAAI, pages
1306–1313.
</reference>
<page confidence="0.986846">
834
</page>
<reference confidence="0.999876336538462">
Aron Culotta, Andrew McCallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models and
data mining to discover relations and patterns in text.
In Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics (HLT/NAACL), pages 296–303.
Aron Culotta. 2004. Dependency tree kernels for rela-
tion extraction. In In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL-04, pages 423–429.
Stijn De Saeger, Kentaro Torisawa, Jun’ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large Scale
Relation Acquisition Using Class Dependent Patterns.
In Proc. of the 9th International Conference on Data
Mining (ICDM), pages 764–769.
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007. Sparse information extraction: Unsupervised
language models to the rescue. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL2007).
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel Weld, and Alexander Yates. 2004. Web-
scale information extraction in KnowItAll. In Proc. of
the 13th international conference on World Wide Web
(WWW04), pages 100–110.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation extrac-
tion. In Proc. of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL ’05, pages
419–444.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th In-
ternational Conference on Computational Linguistics
(COLING’92), pages 539–545.
Tuyen N. Huynh and Raymond J. Mooney. 2008.
Discriminative structure and parameter learning for
markov logic networks. In Proc. of the 25th ICML,
pages 416–423.
Jun’ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL-08: HLT), pages 407–415.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in espresso-like bootstrapping algorithms.
In Proc. of EMNLP’08. Honolulu, USA, pages 1011–
1020.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In Proc. of the ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining, pages 323–328.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proc. of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 1003–1011.
Marius Pas¸ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and Similarities on
the Web: Fact Extraction in the Fast Lane. In Proc. of
the COLING-ACL06, pages 809–816.
Patrick Pantel and Marco Pennacchiotti. 2006a.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proc. of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics (COLING-ACL-06,
pages 113–120.
Patrick Pantel and Pennacchiotti Pennacchiotti, Marco.
2006b. Espresso: Leveraging generic patterns for au-
tomatically harvesting semantic relations. In Proc. of
the COLING-ACL06, pages 113–120.
J. R. Quinlan. 1990. Learning logical definitions from
relations. Machine Learning, 5(3):239–266.
Matthew Richardson and Pedro Domingo. 2006.
Markov logic networks. Machine Learning, 26:107–
136.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proc. of EMNLP2010, pages
1088–1098.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
TSUBAKI: An open search engine infrastructure for
developing new information access. In Proc. of IJC-
NLP, pages 189–196.
Kentaro Torisawa, Stijn De Saeger, Jun’ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kakizawa,
Masaaki Murata, Kow Kuroda, and Ichiro Yamada.
2010. Organizing the web’s information explosion to
discover unknown unknowns. New Generation Com-
puting, 28(3):217–236.
Masaaki Tsuchida, Stijn De Saeger, Kentaro Torisawa,
Masaki Murata, Jun’ichi Kazama, Kow Kuroda, and
Hayato Ohwada. 2010. Large scale similarity-based
relation expansion. In Proc of the 4th IUCS, pages
140–147.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
pages 1083–1106.
</reference>
<page confidence="0.998851">
835
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.206111">
<title confidence="0.7731495">Relation Acquisition using Word Classes and Partial Patterns De Kentaro</title>
<author confidence="0.502332">Hoon</author>
<affiliation confidence="0.901187">Analysis Laboratory, National Institute</affiliation>
<address confidence="0.750248">Information and Communications Technology, 619-0289 Kyoto, Japan</address>
<affiliation confidence="0.906971">and Media Processing Laboratories, NEC Corporation, 630-0101 Nara,</affiliation>
<email confidence="0.99958">m-tsuchida@cq.jp.nec.com</email>
<affiliation confidence="0.974894">amp; Information Science Research Division, NHK Science &amp; Technology Research Laboratories, 157-8510 Tokyo,</affiliation>
<email confidence="0.988565">yamada.i-hy@nhk.or.jp</email>
<abstract confidence="0.998383590909091">This paper proposes a semi-supervised relation acquisition method that does not rely on patterns (e.g. causes for causal relations) but instead learns a combination of indirect evidence for the target re- — word classes This method can extract long tail instances of semantic relations like causality from rare and complex expressions in a large Japanese Web corpus — in extreme cases, patterns that occur only once in the entire corpus. Such patterns are beyond the reach of current pattern based methods. We show that our method performs on par with state-of-the-art pattern based methods, and maintains a reasonable level of accuracy even for instances acquired from infrequent patterns. This ability to acquire long tail instances is crucial for risk management and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proc. of the fifth ACM conference on Digital libraries,</booktitle>
<pages>85--94</pages>
<contexts>
<context position="1921" citStr="Agichtein and Gravano, 2000" startWordPosition="271" endWordPosition="274"> and maintains a reasonable level of accuracy even for instances acquired from infrequent patterns. This ability to acquire long tail instances is crucial for risk management and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance. 1 Introduction Pattern based relation acquisition methods rely on lexico-syntactic patterns (Hearst, 1992) for extracting relation instances. These are templates of natural language expressions such as “X causes Y ” that signal an instance of some semantic relation (i.e., causality). Pattern based methods (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009) learn many * This work was done when all authors were at the National Institute of Information and Communications Technology. such patterns to extract new instances (word pairs) from the corpus. However, since extraction patterns are learned using statistical methods that require a certain frequency of observations, pattern based methods fail to capture relations from complex expressions in which the pattern connecting the two words is rarely observed. Consider the following sentence: “Curing hypertension alleviate</context>
<context position="35533" citStr="Agichtein and Gravano, 2000" startWordPosition="5838" endWordPosition="5841">e a marked improvement over the base features, at least for some relations like causation and material. In other words, the main contribution of semantic word classes and partial patterns to our method’s performance lies not in the final classification step but seems to occur at earlier stages of the process, in the candidate and training data generation steps. 5 Related Work Using lexico-syntactic patterns to extract semantic relations was first explored by Hearst (Hearst, 1992), and has inspired a large body of work on semisupervised relation acquisition methods (Berland and Charniak, 1999; Agichtein and Gravano, 2000; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009), two of which were used in this work. Some researchers have addressed the sparseness problems inherent in pattern based methods. Downey et al. (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. Pas¸ca et al. (2006) alle833 viates pattern sparseness by using infix patterns that are generalized using classes of distributionally similar words.</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: extracting relations from large plain-text collections. In Proc. of the fifth ACM conference on Digital libraries, pages 85–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>The tradeoffs between open and traditional relation extraction.</title>
<date>2008</date>
<booktitle>In Proc. of the 46th ACL-08:HLT,</booktitle>
<pages>28--36</pages>
<contexts>
<context position="35894" citStr="Banko and Etzioni, 2008" startWordPosition="5896" endWordPosition="5899">5 Related Work Using lexico-syntactic patterns to extract semantic relations was first explored by Hearst (Hearst, 1992), and has inspired a large body of work on semisupervised relation acquisition methods (Berland and Charniak, 1999; Agichtein and Gravano, 2000; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009), two of which were used in this work. Some researchers have addressed the sparseness problems inherent in pattern based methods. Downey et al. (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. Pas¸ca et al. (2006) alle833 viates pattern sparseness by using infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body </context>
<context position="37971" citStr="Banko and Etzioni, 2008" startWordPosition="6221" endWordPosition="6224">nguages (i.e., no resource comparable to Freebase exists for Japanese) to which the technology can be applied. Furthermore, it is unclear whether distant supervision can deal with noisy input such as automatically acquired relation instances. Finally, inference based methods (Carlson et al., 2010; Schoenmackers et al., 2010; Tsuchida et al., 2010) are another attempt at relation acquisition that goes beyond pattern matching. Carlson et al. (2010) proposed a method based on inductive logic programming (Quinlan, 1990). Schoenmackers et al. (2010) takes relation instances produced by TextRunner (Banko and Etzioni, 2008) as input and induces first-order Horn clauses, and new instances are infered using a Markov Logic Network (Richardson and Domingo, 2006; Huynh and Mooney, 2008). Tsuchida et al. (2010) generated new relation hypotheses by substituting words in seed instances with distributionally similar words. The difference between these works and ours lies in the treatment of evidence. While the above methods learn inference rules to acquire new relation instances from independent information sources scattered across different Web pages, our method takes the other option of working with all the clues and i</context>
</contexts>
<marker>Banko, Etzioni, 2008</marker>
<rawString>Michele Banko and Oren Etzioni. 2008. The tradeoffs between open and traditional relation extraction. In Proc. of the 46th ACL-08:HLT, pages 28–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Berland</author>
<author>Eugene Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>57--64</pages>
<location>College Park, Maryland, USA,</location>
<contexts>
<context position="35504" citStr="Berland and Charniak, 1999" startWordPosition="5834" endWordPosition="5837">t in combination they do give a marked improvement over the base features, at least for some relations like causation and material. In other words, the main contribution of semantic word classes and partial patterns to our method’s performance lies not in the final classification step but seems to occur at earlier stages of the process, in the candidate and training data generation steps. 5 Related Work Using lexico-syntactic patterns to extract semantic relations was first explored by Hearst (Hearst, 1992), and has inspired a large body of work on semisupervised relation acquisition methods (Berland and Charniak, 1999; Agichtein and Gravano, 2000; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009), two of which were used in this work. Some researchers have addressed the sparseness problems inherent in pattern based methods. Downey et al. (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. Pas¸ca et al. (2006) alle833 viates pattern sparseness by using infix patterns that are generalized using classes of di</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 57–64, College Park, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT ’05),</booktitle>
<pages>724--731</pages>
<contexts>
<context position="36671" citStr="Bunescu and Mooney, 2005" startWordPosition="6020" endWordPosition="6023">ng infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of research in the supervised tradition that does not use explicit pattern representations — kernel based methods (Zelenko et al., 2003; Culotta, 2004; Bunescu and Mooney, 2005) and CRF based methods (Culotta et al., 2006). These approaches are all fully supervised, whereas in our work the automatic generation of candidates and training data is an integral part of the method. An interesting alternative is distant supervision (Mintz et al., 2009), which trains a classifier using an existing database (Freebase) containing thousands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., Freebase contains only rela</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT ’05), pages 724–731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for neverending language learning.</title>
<date>2010</date>
<booktitle>In Proc of the 24th AAAI,</booktitle>
<pages>1306--1313</pages>
<contexts>
<context position="37644" citStr="Carlson et al., 2010" startWordPosition="6172" endWordPosition="6175">sands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., Freebase contains only relations between named entities, and none of the relations in this work) and languages (i.e., no resource comparable to Freebase exists for Japanese) to which the technology can be applied. Furthermore, it is unclear whether distant supervision can deal with noisy input such as automatically acquired relation instances. Finally, inference based methods (Carlson et al., 2010; Schoenmackers et al., 2010; Tsuchida et al., 2010) are another attempt at relation acquisition that goes beyond pattern matching. Carlson et al. (2010) proposed a method based on inductive logic programming (Quinlan, 1990). Schoenmackers et al. (2010) takes relation instances produced by TextRunner (Banko and Etzioni, 2008) as input and induces first-order Horn clauses, and new instances are infered using a Markov Logic Network (Richardson and Domingo, 2006; Huynh and Mooney, 2008). Tsuchida et al. (2010) generated new relation hypotheses by substituting words in seed instances with distribu</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Toward an architecture for neverending language learning. In Proc of the 24th AAAI, pages 1306–1313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
<author>Jonathan Betz</author>
</authors>
<title>Integrating probabilistic extraction models and data mining to discover relations and patterns in text.</title>
<date>2006</date>
<booktitle>In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT/NAACL),</booktitle>
<pages>296--303</pages>
<contexts>
<context position="36716" citStr="Culotta et al., 2006" startWordPosition="6028" endWordPosition="6031">ses of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of research in the supervised tradition that does not use explicit pattern representations — kernel based methods (Zelenko et al., 2003; Culotta, 2004; Bunescu and Mooney, 2005) and CRF based methods (Culotta et al., 2006). These approaches are all fully supervised, whereas in our work the automatic generation of candidates and training data is an integral part of the method. An interesting alternative is distant supervision (Mintz et al., 2009), which trains a classifier using an existing database (Freebase) containing thousands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., Freebase contains only relations between named entities, and none of the</context>
</contexts>
<marker>Culotta, McCallum, Betz, 2006</marker>
<rawString>Aron Culotta, Andrew McCallum, and Jonathan Betz. 2006. Integrating probabilistic extraction models and data mining to discover relations and patterns in text. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT/NAACL), pages 296–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
</authors>
<title>Dependency tree kernels for relation extraction. In</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04,</booktitle>
<pages>423--429</pages>
<contexts>
<context position="36644" citStr="Culotta, 2004" startWordPosition="6018" endWordPosition="6019">arseness by using infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of research in the supervised tradition that does not use explicit pattern representations — kernel based methods (Zelenko et al., 2003; Culotta, 2004; Bunescu and Mooney, 2005) and CRF based methods (Culotta et al., 2006). These approaches are all fully supervised, whereas in our work the automatic generation of candidates and training data is an integral part of the method. An interesting alternative is distant supervision (Mintz et al., 2009), which trains a classifier using an existing database (Freebase) containing thousands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., </context>
</contexts>
<marker>Culotta, 2004</marker>
<rawString>Aron Culotta. 2004. Dependency tree kernels for relation extraction. In In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04, pages 423–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stijn De Saeger</author>
<author>Kentaro Torisawa</author>
<author>Jun’ichi Kazama</author>
<author>Kow Kuroda</author>
<author>Masaki Murata</author>
</authors>
<title>Large Scale Relation Acquisition Using Class Dependent Patterns.</title>
<date>2009</date>
<booktitle>In Proc. of the 9th International Conference on Data Mining (ICDM),</booktitle>
<pages>764--769</pages>
<marker>De Saeger, Torisawa, Kazama, Kuroda, Murata, 2009</marker>
<rawString>Stijn De Saeger, Kentaro Torisawa, Jun’ichi Kazama, Kow Kuroda, and Masaki Murata. 2009. Large Scale Relation Acquisition Using Class Dependent Patterns. In Proc. of the 9th International Conference on Data Mining (ICDM), pages 764–769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
</authors>
<title>Sparse information extraction: Unsupervised language models to the rescue.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL2007).</booktitle>
<contexts>
<context position="35784" citStr="Downey et al. (2007)" startWordPosition="5880" endWordPosition="5883">but seems to occur at earlier stages of the process, in the candidate and training data generation steps. 5 Related Work Using lexico-syntactic patterns to extract semantic relations was first explored by Hearst (Hearst, 1992), and has inspired a large body of work on semisupervised relation acquisition methods (Berland and Charniak, 1999; Agichtein and Gravano, 2000; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009), two of which were used in this work. Some researchers have addressed the sparseness problems inherent in pattern based methods. Downey et al. (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. Pas¸ca et al. (2006) alle833 viates pattern sparseness by using infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement the</context>
</contexts>
<marker>Downey, Schoenmackers, Etzioni, 2007</marker>
<rawString>Doug Downey, Stefan Schoenmackers, and Oren Etzioni. 2007. Sparse information extraction: Unsupervised language models to the rescue. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>Stanley Kok</author>
<author>Ana-Maria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Webscale information extraction in KnowItAll.</title>
<date>2004</date>
<booktitle>In Proc. of the 13th international conference on World Wide Web (WWW04),</booktitle>
<pages>100--110</pages>
<contexts>
<context position="35555" citStr="Etzioni et al., 2004" startWordPosition="5842" endWordPosition="5845">he base features, at least for some relations like causation and material. In other words, the main contribution of semantic word classes and partial patterns to our method’s performance lies not in the final classification step but seems to occur at earlier stages of the process, in the candidate and training data generation steps. 5 Related Work Using lexico-syntactic patterns to extract semantic relations was first explored by Hearst (Hearst, 1992), and has inspired a large body of work on semisupervised relation acquisition methods (Berland and Charniak, 1999; Agichtein and Gravano, 2000; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009), two of which were used in this work. Some researchers have addressed the sparseness problems inherent in pattern based methods. Downey et al. (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. Pas¸ca et al. (2006) alle833 viates pattern sparseness by using infix patterns that are generalized using classes of distributionally similar words. In addition, their me</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Kok, Popescu, Shaked, Soderland, Weld, Yates, 2004</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, Stanley Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel Weld, and Alexander Yates. 2004. Webscale information extraction in KnowItAll. In Proc. of the 13th international conference on World Wide Web (WWW04), pages 100–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
<author>Su Jian</author>
<author>Zhang Jie</author>
<author>Zhang Min</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>419--444</pages>
<contexts>
<context position="15605" citStr="GuoDong et al. (2005)" startWordPosition="2538" endWordPosition="2541">he noun, which is captured by the unigram features. In addition to these base features, we include the semantic classes to which the candidate noun pair belongs, the partial patterns they match in this sentence, and the infix words inbetween the target noun pair. Note that this feature set is not intended to be optimal beyond the actual claims of this paper, and we have deliberately avoided exhaustive feature engineering so as not to obscure the contribution of semantic classes and partial pattern to our approach. Clearly an optimal classifier will incorporate many more advanced features (see GuoDong et al. (2005) for a comprehensive overview), but even without sophisticated feature engineering our classifier achieved sufficient performance levels to support our claims. An overview of the feature set is given in Table 1. The relative contribution of each type of features is discussed in section 4. In preliminary experiments we found a polynomial kernel of degree 3 gave the best results, which suggests the effectiveness of combining different types of indirect evidence. The SVM classifier outputs (noun pair, sentence) triples, ranked by SVM score. To obtain the final output of our method we assign each </context>
</contexts>
<marker>GuoDong, Jian, Jie, Min, 2005</marker>
<rawString>Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min. 2005. Exploring various knowledge in relation extraction. In Proc. of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of the 14th International Conference on Computational Linguistics (COLING’92),</booktitle>
<pages>539--545</pages>
<contexts>
<context position="1692" citStr="Hearst, 1992" startWordPosition="237" endWordPosition="238">me cases, patterns that occur only once in the entire corpus. Such patterns are beyond the reach of current pattern based methods. We show that our method performs on par with state-of-the-art pattern based methods, and maintains a reasonable level of accuracy even for instances acquired from infrequent patterns. This ability to acquire long tail instances is crucial for risk management and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance. 1 Introduction Pattern based relation acquisition methods rely on lexico-syntactic patterns (Hearst, 1992) for extracting relation instances. These are templates of natural language expressions such as “X causes Y ” that signal an instance of some semantic relation (i.e., causality). Pattern based methods (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009) learn many * This work was done when all authors were at the National Institute of Information and Communications Technology. such patterns to extract new instances (word pairs) from the corpus. However, since extraction patterns are learned using statistical methods that require a certain </context>
<context position="35390" citStr="Hearst, 1992" startWordPosition="5818" endWordPosition="5819">formation, partial patterns or infix words) to the classification performance is relatively minor, but in combination they do give a marked improvement over the base features, at least for some relations like causation and material. In other words, the main contribution of semantic word classes and partial patterns to our method’s performance lies not in the final classification step but seems to occur at earlier stages of the process, in the candidate and training data generation steps. 5 Related Work Using lexico-syntactic patterns to extract semantic relations was first explored by Hearst (Hearst, 1992), and has inspired a large body of work on semisupervised relation acquisition methods (Berland and Charniak, 1999; Agichtein and Gravano, 2000; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009), two of which were used in this work. Some researchers have addressed the sparseness problems inherent in pattern based methods. Downey et al. (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. Pas¸c</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of the 14th International Conference on Computational Linguistics (COLING’92), pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tuyen N Huynh</author>
<author>Raymond J Mooney</author>
</authors>
<title>Discriminative structure and parameter learning for markov logic networks.</title>
<date>2008</date>
<booktitle>In Proc. of the 25th ICML,</booktitle>
<pages>416--423</pages>
<contexts>
<context position="38132" citStr="Huynh and Mooney, 2008" startWordPosition="6247" endWordPosition="6250">ion can deal with noisy input such as automatically acquired relation instances. Finally, inference based methods (Carlson et al., 2010; Schoenmackers et al., 2010; Tsuchida et al., 2010) are another attempt at relation acquisition that goes beyond pattern matching. Carlson et al. (2010) proposed a method based on inductive logic programming (Quinlan, 1990). Schoenmackers et al. (2010) takes relation instances produced by TextRunner (Banko and Etzioni, 2008) as input and induces first-order Horn clauses, and new instances are infered using a Markov Logic Network (Richardson and Domingo, 2006; Huynh and Mooney, 2008). Tsuchida et al. (2010) generated new relation hypotheses by substituting words in seed instances with distributionally similar words. The difference between these works and ours lies in the treatment of evidence. While the above methods learn inference rules to acquire new relation instances from independent information sources scattered across different Web pages, our method takes the other option of working with all the clues and indirect evidence a single sentence can provide. In the future, a combination of both approaches may prove beneficial. 6 Conclusion We have proposed a relation ac</context>
</contexts>
<marker>Huynh, Mooney, 2008</marker>
<rawString>Tuyen N. Huynh and Raymond J. Mooney. 2008. Discriminative structure and parameter learning for markov logic networks. In Proc. of the 25th ICML, pages 416–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Inducing gazetteers for named entity recognition by large-scale clustering of dependency relations.</title>
<date>2008</date>
<booktitle>In Proc. of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT),</booktitle>
<pages>407--415</pages>
<contexts>
<context position="9478" citStr="Kazama and Torisawa, 2008" startWordPosition="1507" endWordPosition="1510">, together with the highest scoring class dependent pattern each noun pair co-occurs with. This list becomes the input to Stage 2 of our method, as shown in Figure 1. We adopted CDP as Stage 1 extractor because, besides having generally good performance, the class dependent patterns provide the two fundamental ingredients for Stage 2 of our method — the target semantic word classes for a given relation (in the form of the semantic class restrictions attached to patterns), and partial patterns. To obtain fine-grained semantic word classes we used the large scale word clustering algorithm from (Kazama and Torisawa, 2008), which uses the EM algorithm to compute the probability that a word w belongs to class c, i.e., P(c|w). Probabilistic clustering defines no discrete boundary between members and non-members of a semantic class, so we simply assume w belongs to c whenever P(c|w) &gt; 0.2. For this work we clustered 106 nouns into 500 classes. Finally, we adopt the structural representation of patterns introduced in (Lin and Pantel, 2001). All sentences in our corpus are dependency parsed, and patterns consist of words on the path of dependency relations connecting two nouns. 3 Stage 2 Extractor We use CDP as our </context>
</contexts>
<marker>Kazama, Torisawa, 2008</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2008. Inducing gazetteers for named entity recognition by large-scale clustering of dependency relations. In Proc. of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT), pages 407–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mamoru Komachi</author>
<author>Taku Kudo</author>
<author>Masashi Shimbo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Graph-based analysis of semantic drift in espresso-like bootstrapping algorithms.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP’08.</booktitle>
<pages>1011--1020</pages>
<location>Honolulu, USA,</location>
<contexts>
<context position="22591" citStr="Komachi et al., 2008" startWordPosition="3682" endWordPosition="3685">domly select a sentence in which the noun pair co-occurs with that pattern from our corpus. For the proposed method we evaluate noun pairs in the context of the (noun pair, sentence) triple with the highest SVM score. 4.2 Results and Discussion The performance of each method on the causality, prevention and material relations are shown in Figures 2, 3 and 4 respectively. In the causality experiments (Figure 2) the proposed method performs on par with CDP for the top 25,000 results, both achieving close to 90% precision. But whereas CDP’s perit remains very difficult to prevent semantic drift (Komachi et al., 2008) from occurring. One small adjustment to the algorithm stabilized the bootstrapping process considerably and gave overall better results. In the pattern induction step (section 3.2 in (Pantel and Pennacchiotti, 2006a)), Espresso computes a reliability score for each candidate pattern based on the weighted PMI of the pattern with all instances extracted so far. As the number of extracted instances increases this disproportionally favours high frequency (i.e. generic) patterns, so instead of using all instances for computing pattern reliability we only use the m most reliable instances from the </context>
</contexts>
<marker>Komachi, Kudo, Shimbo, Matsumoto, 2008</marker>
<rawString>Mamoru Komachi, Taku Kudo, Masashi Shimbo, and Yuji Matsumoto. 2008. Graph-based analysis of semantic drift in espresso-like bootstrapping algorithms. In Proc. of EMNLP’08. Honolulu, USA, pages 1011– 1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt - discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proc. of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="9899" citStr="Lin and Pantel, 2001" startWordPosition="1577" endWordPosition="1580">semantic class restrictions attached to patterns), and partial patterns. To obtain fine-grained semantic word classes we used the large scale word clustering algorithm from (Kazama and Torisawa, 2008), which uses the EM algorithm to compute the probability that a word w belongs to class c, i.e., P(c|w). Probabilistic clustering defines no discrete boundary between members and non-members of a semantic class, so we simply assume w belongs to c whenever P(c|w) &gt; 0.2. For this work we clustered 106 nouns into 500 classes. Finally, we adopt the structural representation of patterns introduced in (Lin and Pantel, 2001). All sentences in our corpus are dependency parsed, and patterns consist of words on the path of dependency relations connecting two nouns. 3 Stage 2 Extractor We use CDP as our Stage 1 extractor, and the top N noun pairs along with the class dependent patterns that extract them are given as input to Stage 2, which represents the main contribution of this work. As shown in Figure 1, Stage 2 consists of three modules: a candidate generator, a training data generator and a supervised classifier. The training data generator builds training data for the classifier from the top N output of CDP and</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of inference rules from text. In Proc. of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="36943" citStr="Mintz et al., 2009" startWordPosition="6065" endWordPosition="6068">d have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of research in the supervised tradition that does not use explicit pattern representations — kernel based methods (Zelenko et al., 2003; Culotta, 2004; Bunescu and Mooney, 2005) and CRF based methods (Culotta et al., 2006). These approaches are all fully supervised, whereas in our work the automatic generation of candidates and training data is an integral part of the method. An interesting alternative is distant supervision (Mintz et al., 2009), which trains a classifier using an existing database (Freebase) containing thousands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., Freebase contains only relations between named entities, and none of the relations in this work) and languages (i.e., no resource comparable to Freebase exists for Japanese) to which the technology can be applied. Furthermore, it is unclear whether distant supervision can deal with noisy input such</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
<author>Dekang Lin</author>
<author>Jeffrey Bigham</author>
<author>Andrei Lifchits</author>
<author>Alpa Jain</author>
</authors>
<title>Names and Similarities on the Web: Fact Extraction in the Fast Lane.</title>
<date>2006</date>
<booktitle>In Proc. of the COLING-ACL06,</booktitle>
<pages>809--816</pages>
<marker>Pas¸ca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>Marius Pas¸ca, Dekang Lin, Jeffrey Bigham, Andrei Lifchits, and Alpa Jain. 2006. Names and Similarities on the Web: Fact Extraction in the Fast Lane. In Proc. of the COLING-ACL06, pages 809–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL-06,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="1953" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="275" endWordPosition="278">evel of accuracy even for instances acquired from infrequent patterns. This ability to acquire long tail instances is crucial for risk management and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance. 1 Introduction Pattern based relation acquisition methods rely on lexico-syntactic patterns (Hearst, 1992) for extracting relation instances. These are templates of natural language expressions such as “X causes Y ” that signal an instance of some semantic relation (i.e., causality). Pattern based methods (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009) learn many * This work was done when all authors were at the National Institute of Information and Communications Technology. such patterns to extract new instances (word pairs) from the corpus. However, since extraction patterns are learned using statistical methods that require a certain frequency of observations, pattern based methods fail to capture relations from complex expressions in which the pattern connecting the two words is rarely observed. Consider the following sentence: “Curing hypertension alleviates the deterioration speed of the</context>
<context position="20294" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="3287" endWordPosition="3290">e contains sufficient evidence that the target relation holds between the candidate nouns, they mark the noun pair correct. To evaluate the performance of each method we use two evaluation criteria: strict (all judges must agree the candidate relation is correct) and lenient (decided by the judges’ majority vote). Over all experiments the interrater agreement (Kappa) ranged between 0.57 and 0.82 with an average of 0.72, indicating substantial agreement (Landis and Koch, 1977). 4.1.1 Methods Compared We compare our results to two pattern based methods: CDP (the Stage 1 extractor) and Espresso (Pantel and Pennacchiotti, 2006a). Espresso is a popular bootstrapping based method that uses a set of seed instances to induce extraction patterns for the target relation and then acquire new instances in an iterative bootstrapping process. In each iteration Espresso performs pattern induction, pattern ranking and selection using previously acquired instances, and uses the newly acquired patterns to extraction new instances. Espresso computes a reliability score for both instances and patterns based on their pointwise mutual information (PMI) with the top-scoring patterns and instances from the previous iteration.3 We refe</context>
<context position="22806" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="3714" endWordPosition="3717"> SVM score. 4.2 Results and Discussion The performance of each method on the causality, prevention and material relations are shown in Figures 2, 3 and 4 respectively. In the causality experiments (Figure 2) the proposed method performs on par with CDP for the top 25,000 results, both achieving close to 90% precision. But whereas CDP’s perit remains very difficult to prevent semantic drift (Komachi et al., 2008) from occurring. One small adjustment to the algorithm stabilized the bootstrapping process considerably and gave overall better results. In the pattern induction step (section 3.2 in (Pantel and Pennacchiotti, 2006a)), Espresso computes a reliability score for each candidate pattern based on the weighted PMI of the pattern with all instances extracted so far. As the number of extracted instances increases this disproportionally favours high frequency (i.e. generic) patterns, so instead of using all instances for computing pattern reliability we only use the m most reliable instances from the previous iteration, which were used to extract the candidate patterns of the current iteration (m = 200, like the original). Figure 3: Precision of acquired relations (prevention). L and S denote lenient and strict </context>
<context position="35587" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="5846" endWordPosition="5849">east for some relations like causation and material. In other words, the main contribution of semantic word classes and partial patterns to our method’s performance lies not in the final classification step but seems to occur at earlier stages of the process, in the candidate and training data generation steps. 5 Related Work Using lexico-syntactic patterns to extract semantic relations was first explored by Hearst (Hearst, 1992), and has inspired a large body of work on semisupervised relation acquisition methods (Berland and Charniak, 1999; Agichtein and Gravano, 2000; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009), two of which were used in this work. Some researchers have addressed the sparseness problems inherent in pattern based methods. Downey et al. (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. Pas¸ca et al. (2006) alle833 viates pattern sparseness by using infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based se</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006a. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proc. of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL-06, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Pennacchiotti Pennacchiotti</author>
<author>Marco</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proc. of the COLING-ACL06,</booktitle>
<pages>113--120</pages>
<marker>Pantel, Pennacchiotti, Marco, 2006</marker>
<rawString>Patrick Pantel and Pennacchiotti Pennacchiotti, Marco. 2006b. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proc. of the COLING-ACL06, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Learning logical definitions from relations.</title>
<date>1990</date>
<booktitle>Machine Learning,</booktitle>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="37868" citStr="Quinlan, 1990" startWordPosition="6208" endWordPosition="6209">ontains only relations between named entities, and none of the relations in this work) and languages (i.e., no resource comparable to Freebase exists for Japanese) to which the technology can be applied. Furthermore, it is unclear whether distant supervision can deal with noisy input such as automatically acquired relation instances. Finally, inference based methods (Carlson et al., 2010; Schoenmackers et al., 2010; Tsuchida et al., 2010) are another attempt at relation acquisition that goes beyond pattern matching. Carlson et al. (2010) proposed a method based on inductive logic programming (Quinlan, 1990). Schoenmackers et al. (2010) takes relation instances produced by TextRunner (Banko and Etzioni, 2008) as input and induces first-order Horn clauses, and new instances are infered using a Markov Logic Network (Richardson and Domingo, 2006; Huynh and Mooney, 2008). Tsuchida et al. (2010) generated new relation hypotheses by substituting words in seed instances with distributionally similar words. The difference between these works and ours lies in the treatment of evidence. While the above methods learn inference rules to acquire new relation instances from independent information sources scat</context>
</contexts>
<marker>Quinlan, 1990</marker>
<rawString>J. R. Quinlan. 1990. Learning logical definitions from relations. Machine Learning, 5(3):239–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingo</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>26--107</pages>
<contexts>
<context position="38107" citStr="Richardson and Domingo, 2006" startWordPosition="6243" endWordPosition="6246">clear whether distant supervision can deal with noisy input such as automatically acquired relation instances. Finally, inference based methods (Carlson et al., 2010; Schoenmackers et al., 2010; Tsuchida et al., 2010) are another attempt at relation acquisition that goes beyond pattern matching. Carlson et al. (2010) proposed a method based on inductive logic programming (Quinlan, 1990). Schoenmackers et al. (2010) takes relation instances produced by TextRunner (Banko and Etzioni, 2008) as input and induces first-order Horn clauses, and new instances are infered using a Markov Logic Network (Richardson and Domingo, 2006; Huynh and Mooney, 2008). Tsuchida et al. (2010) generated new relation hypotheses by substituting words in seed instances with distributionally similar words. The difference between these works and ours lies in the treatment of evidence. While the above methods learn inference rules to acquire new relation instances from independent information sources scattered across different Web pages, our method takes the other option of working with all the clues and indirect evidence a single sentence can provide. In the future, a combination of both approaches may prove beneficial. 6 Conclusion We ha</context>
</contexts>
<marker>Richardson, Domingo, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingo. 2006. Markov logic networks. Machine Learning, 26:107– 136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
<author>Jesse Davis</author>
</authors>
<title>Learning first-order horn clauses from web text.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP2010,</booktitle>
<pages>1088--1098</pages>
<contexts>
<context position="37672" citStr="Schoenmackers et al., 2010" startWordPosition="6176" endWordPosition="6179">tions, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., Freebase contains only relations between named entities, and none of the relations in this work) and languages (i.e., no resource comparable to Freebase exists for Japanese) to which the technology can be applied. Furthermore, it is unclear whether distant supervision can deal with noisy input such as automatically acquired relation instances. Finally, inference based methods (Carlson et al., 2010; Schoenmackers et al., 2010; Tsuchida et al., 2010) are another attempt at relation acquisition that goes beyond pattern matching. Carlson et al. (2010) proposed a method based on inductive logic programming (Quinlan, 1990). Schoenmackers et al. (2010) takes relation instances produced by TextRunner (Banko and Etzioni, 2008) as input and induces first-order Horn clauses, and new instances are infered using a Markov Logic Network (Richardson and Domingo, 2006; Huynh and Mooney, 2008). Tsuchida et al. (2010) generated new relation hypotheses by substituting words in seed instances with distributionally similar words. The </context>
</contexts>
<marker>Schoenmackers, Etzioni, Weld, Davis, 2010</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld, and Jesse Davis. 2010. Learning first-order horn clauses from web text. In Proc. of EMNLP2010, pages 1088–1098.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Shinzato</author>
<author>Tomohide Shibata</author>
<author>Daisuke Kawahara</author>
<author>Chikara Hashimoto</author>
<author>Sadao Kurohashi</author>
</authors>
<title>TSUBAKI: An open search engine infrastructure for developing new information access.</title>
<date>2008</date>
<booktitle>In Proc. of IJCNLP,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="6276" citStr="Shinzato et al., 2008" startWordPosition="977" endWordPosition="980">rom the output of Stage 1. Given the output of Stage 1 and access to a Web corpus, the Stage 2 extractor is completely self-sufficient, and the whole method requires no supervision other than a handful of seed patterns to start the first stage extractor. The whole procedure is therefore minimally supervised. Semantic word classes and partial patterns play a crucial role throughout all steps of the process. We evaluate our method on three relation acquisition tasks (causation, prevention and material relations) using a 600 million Japanese Web page corFigure 1: Proposed method: data flow. pus (Shinzato et al., 2008) and show that our system can successfully acquire relations from both frequent and infrequent patterns. Our system extracted 100,000 causal relations with 84.6% precision, 50,000 prevention relations with 58.4% precision and 25,000 material relations with 76.1% precision. In the extreme case, we acquired several thousand word pairs co-occurring only in patterns that appear once in the entire corpus. We call such patterns single occurrence (SO) patterns. Word pairs that co-occur only with SO patterns represent the theoretical limiting case of relations that cannot be acquired using existing pa</context>
<context position="18431" citStr="Shinzato et al., 2008" startWordPosition="2989" endWordPosition="2992">4.1 Experimental Setting We evaluate our method on three semantic relation acquisition tasks: causality, prevention and material. Two concepts stand in a causal relation when the source concept (the “cause”) is directly or indirectly responsible for the subsequent occurrence of the target concept (its “effect”). In a prevention relation the source concept directly or indirectly acts to avoid the occurrence of the target concept, and in a material relation the source concept is a material or ingredient of the target concept. For our experiments we used the latest version of the TSUBAKI corpus (Shinzato et al., 2008), a collection of 600 million Japanese Web pages dependency parsed by the Japanese dependency parser KNP2. In our implementation of CDP, lexicosyntactic patterns consist of words on the path connecting two nouns in a dependency parse tree. We discard patterns from dependency paths longer than 8 constituent nodes. Furthermore, we estimated pattern frequencies in a subset of the corpus (50 million pages, or 1/12th of the entire corpus) and discarded patterns that co-occur with less than 10 unique noun pairs in this smaller corpus. These restrictions do not apply to the proposed method, which can</context>
</contexts>
<marker>Shinzato, Shibata, Kawahara, Hashimoto, Kurohashi, 2008</marker>
<rawString>Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara, Chikara Hashimoto, and Sadao Kurohashi. 2008. TSUBAKI: An open search engine infrastructure for developing new information access. In Proc. of IJCNLP, pages 189–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Torisawa</author>
<author>Stijn De Saeger</author>
<author>Jun’ichi Kazama</author>
<author>Asuka Sumida</author>
<author>Daisuke Noguchi</author>
<author>Yasunari Kakizawa</author>
<author>Masaaki Murata</author>
<author>Kow Kuroda</author>
<author>Ichiro Yamada</author>
</authors>
<title>Organizing the web’s information explosion to discover unknown unknowns.</title>
<date>2010</date>
<journal>New Generation Computing,</journal>
<volume>28</volume>
<issue>3</issue>
<marker>Torisawa, De Saeger, Kazama, Sumida, Noguchi, Kakizawa, Murata, Kuroda, Yamada, 2010</marker>
<rawString>Kentaro Torisawa, Stijn De Saeger, Jun’ichi Kazama, Asuka Sumida, Daisuke Noguchi, Yasunari Kakizawa, Masaaki Murata, Kow Kuroda, and Ichiro Yamada. 2010. Organizing the web’s information explosion to discover unknown unknowns. New Generation Computing, 28(3):217–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Tsuchida</author>
<author>Stijn De Saeger</author>
<author>Kentaro Torisawa</author>
<author>Masaki Murata</author>
<author>Jun’ichi Kazama</author>
<author>Kow Kuroda</author>
<author>Hayato Ohwada</author>
</authors>
<title>Large scale similarity-based relation expansion.</title>
<date>2010</date>
<booktitle>In Proc of the 4th IUCS,</booktitle>
<pages>140--147</pages>
<marker>Tsuchida, De Saeger, Torisawa, Murata, Kazama, Kuroda, Ohwada, 2010</marker>
<rawString>Masaaki Tsuchida, Stijn De Saeger, Kentaro Torisawa, Masaki Murata, Jun’ichi Kazama, Kow Kuroda, and Hayato Ohwada. 2010. Large scale similarity-based relation expansion. In Proc of the 4th IUCS, pages 140–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>1083--1106</pages>
<contexts>
<context position="36629" citStr="Zelenko et al., 2003" startWordPosition="6014" endWordPosition="6017">e833 viates pattern sparseness by using infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of research in the supervised tradition that does not use explicit pattern representations — kernel based methods (Zelenko et al., 2003; Culotta, 2004; Bunescu and Mooney, 2005) and CRF based methods (Culotta et al., 2006). These approaches are all fully supervised, whereas in our work the automatic generation of candidates and training data is an integral part of the method. An interesting alternative is distant supervision (Mintz et al., 2009), which trains a classifier using an existing database (Freebase) containing thousands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic re</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. Journal of Machine Learning Research, pages 1083–1106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>