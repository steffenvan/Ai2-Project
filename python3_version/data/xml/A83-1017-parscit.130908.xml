<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001552">
<note confidence="0.7038585">
Scruffy Text Understanding;
Design and Implementation of the NOMAD System
Richard R. Granger, Chris J. Staros,
Gregory B. Taylor, Rika Yoshii
</note>
<sectionHeader confidence="0.479597" genericHeader="abstract">
Artificial Intelligence Project
</sectionHeader>
<affiliation confidence="0.74708">
Department of Information and Computer Science
University of California
Irvine, California 92717
</affiliation>
<sectionHeader confidence="0.81723" genericHeader="keywords">
ABSTRACT
II. Categories of Errors
</sectionHeader>
<bodyText confidence="0.99982675">
The task of understanding unedited naval
ship-to-shore messages is implemented in the
presence of a large database of domain specific
knowledge. The program uses internal syntactic and
semantic expectations to analyze the texts and to
correct errors that arise during understanding,
such as syntactic errors, missing punctuation, and
errors of spelling and usage. The output of the
system is a well-formed English translation of the
message. This paper describes some of the
knowledge mechanisms that have been implemented in
the NOMAD system.
</bodyText>
<sectionHeader confidence="0.960424" genericHeader="method">
I. Introduction
</sectionHeader>
<bodyText confidence="0.980039833333333">
Consider the following message,
LOCKED ON OPEN FIRED DIW.
This is an actual naval message containing sentence
boundary problems, missing subjects and objects, an
incorrect verb conjugation, and an abbreviation for
&amp;quot;dead in water.&amp;quot; The NAVY receives many thousands
of short messages like the one above in very
&apos;scruffy&apos; form, and these messages have to be put
into a more readable form before they can be passed
through many hands. Hence there is an obvious
benefit to partially automating this encoding
process.
Most Large text-understanding systems today
would not be able to automate the encoding process
mentioned above because they were designed under
the assumption that the input text consists of
well-formed and logical sentences such as newspaper
stories and other edited texts. The NOMAD system,
however, was designed to understand naval text that
contains ungrammatical or only partially complete
sentences.
This paper explains some knowledge mechanisms
that underlie the reader&apos;s ability to understand
scruffy text and how these mechanisms are
implemented within the NOMAD system.
This research was supported in part by the Naval
Ocean Systems Center grant N00123-81-C-1078.
We have encountered the following problems in
understanding Navy massages. They are listed in
the order of frequency of their occurrences. The
order was determined by examining the list of
massages provided by the Naval Ocean Systems
Center. For each error type, NOMAD&apos;s method of
recognizing and correcting the problem are
described, and the module which is responsible for
the correction is identified.
</bodyText>
<sectionHeader confidence="0.861676666666667" genericHeader="method">
A. Unknown words
Consider the message,
PEGASUS FRD 2 TALOS AT VICTOR
</sectionHeader>
<bodyText confidence="0.999971210526316">
NOMAD does not immediately recognize &apos;FRD as a
word in the dictionary. Often the message sender
will use an ad hoc abbreviation of a word or
misspell a word. Any word not found to be in the
dictionary is first put through a simple spelling
correction procedure. If none of the possible
corrections are recognizable then a morphological
analyzer is applied to recognize different possible
conjugations of a a verb.
If this fails, a mechanism called FOUL-UP
(Granger, 1977) is triggered. The FOUL-UP
mechanism handles unknown words by using the
program&apos;s own syntactic and semantic expectations
to create a temporary definition that would allow
it continue normally. FOUL-UP would later revise
the definition of the unknown word by combining the
expectations generated based on previous
information with the role the unknown word is
playing in the current context.
</bodyText>
<subsectionHeader confidence="0.921114">
B. Missinsk subject and objects
</subsectionHeader>
<bodyText confidence="0.999016">
Consider the following message of two
sentences,
</bodyText>
<sectionHeader confidence="0.905586" genericHeader="method">
CONSTELLATION SAW KASHIN. LOST CONTACT.
</sectionHeader>
<bodyText confidence="0.9997688">
A script-based (Schenk and Abelson, 1977)
inferencer generates expectations to fill the
subject and object of each sentence. Here, the
word &apos;SAW&apos; as a conjugation of &apos;SEE&apos; would give
arise to expectations related to detection and
</bodyText>
<page confidence="0.995087">
104
</page>
<bodyText confidence="0.9461215">
identification. The inferencer also uses knowledge
about typical sequences of events (identify before
fire) (Cullingford, 1977) and relationships between
their participants (friend and foe).
C.bi&amp;ALLtous word mutat
Examine the following message,
CONTACT GAINED ON KASHIN.
The example can be interpreted as either &amp;quot;Contact
was gained on Kashin&amp;quot; meaning &amp;quot;We contacted Kashin&amp;quot;
or &amp;quot;Our contact (a ship) made heading towards
Kashin.&amp;quot; NOMAD picks one of the multiple meanings
of the ambiguous word, and calls a blame assignment
module to check for goal violations, physical
impossibilities, and other semantic conflicts to
make sure that the interpretation was correct. If
the module detects any conflict, NOMAD attempts to
understand the sentence using a a different meaning
of the ambiguous word.
D. Missing sentence and clause boundaries
Consider the following message,
</bodyText>
<sectionHeader confidence="0.998923" genericHeader="method">
VISUALLY LOCKED ON AND TRACKING CHALLENGED UNIT NO
REPLY OPEN FIRED TIME 0129.1
</sectionHeader>
<bodyText confidence="0.998931071428571">
NOMAD uses semantic expectations and syntactic
expectations to detect missing boundaries.
&apos;VISUALLY LOCKED ON&apos; is understood to be a complete
sentence because there are no expecations pending
when &apos;AND&apos; is read. &apos;TRACKing&apos; is understood to be
the verb of the second sentence. With a verb
chosen and expecations for an actor pending,
&apos;CHALLENGED is used as an adverb describing
&apos;UNIT&apos;. The second phrase ends before &apos;NO REPLY
...&apos; as again there are no expecations pending at
this point. The phrase &amp;quot;NO REPLY&amp;quot; has expectations
for communication verbs to follow it, and thus when
the clause &amp;quot;OPEN FIRED&amp;quot; is encountered, the final
sentence boundry is identified.
</bodyText>
<subsectionHeader confidence="0.763419">
E. Age&amp; tense
</subsectionHeader>
<bodyText confidence="0.997425">
Consider the following fragment sentence from
our first example,
</bodyText>
<sectionHeader confidence="0.882157" genericHeader="method">
OPEN FIRED.
</sectionHeader>
<subsectionHeader confidence="0.788393">
The morphological analyzer is used also to correct
</subsectionHeader>
<bodyText confidence="0.997362">
the tense of a word. eg. OPEN FIRED --&gt; OPEN
FIRE. The script-based inferencer then determines
the tense of the given action using its knowledge
about typical sequences of events. eg. LOCKED ON.
OPEN FIRED. ---&gt; LOCKED ON. OPENED FIRE.
</bodyText>
<sectionHeader confidence="0.95638" genericHeader="method">
III. &amp;MAU Interface
</sectionHeader>
<bodyText confidence="0.99617555">
NOMAD uses a generator specifically designed
for the naval domain to produce a well formed
translation of the input message. This &apos;pretty&apos;
form of the input message is checked by a user to
assure that NOMAD has correctly understood the
message. If NOMAD is then told it has incorrectly
understood the message, alternative word
definitions and other semantic choices are made in
a second attempt at understanding.
A. ihs. Generator
The generator has been tailored to address
some of the problems that occur in describing naval
activities [Taylor, 19821. Many of the messages
are characterized by incomplete and changing
descriptions of actors. These descriptions contain
information that may be unknown but doesn&apos;t change
(name, type of ship, etc.) along with temporal
information (location and damage status). The
NOMAD generator produces unambiguous descriptions
of actors while maintaining brevity.
</bodyText>
<sectionHeader confidence="0.576776" genericHeader="method">
IV. An Example
</sectionHeader>
<bodyText confidence="0.999885666666667">
The following is an actual example shoving the
capability of NOMAD to handle multiple problems
found in a message.
</bodyText>
<sectionHeader confidence="0.9937308" genericHeader="method">
INPUT MESSAGE:
PERISCOPE SIGHTED BY CONSTELLATION ABT 2000 YDS OFF
PORT QTR, AND HS HILO VECTRED TO DATUM. GREEN
FLARES SIGHTED AFTER VISUAL ON PERISCOPE. HS
GAINED ACTIVE CTC AND CONDUCTED TWO ATTACKS.
</sectionHeader>
<bodyText confidence="0.99956664">
Two possible paraphrases are generated:
PARAPHRASE1:
The Constellation identified an enemy submarine
that was at 225 degrees 2000 miles from their
location. A helicopter-squadron pursued the enemy
submarine. The helicopter-squadron identified some
green flares. By using an active sonar, the
helicopter-squadron identified the enemy submarine,
and they fired twice at the enemy submarine.
PARAPHRASE2:
The Constellation identified an enemy submarine
that was at 225 degrees 2000 miles from their
location. A helicopter-squadron pursued the enemy
submarine. The Constellation identified some green
flares. By using an active sonar, the
helicopter-squadron identified the enemy submarine,
and they fired twice at the enemy submarine.
The main difference that is shown in the
paraphrases is the identity of the subject of the
second sentence. NOMAD gives preference in this
case to the second paraphrase because &apos;AFTER VISUAL
ON PERISCOPE&apos; implies that the subject of the
second sentence is the same as in the first
sentence. However, the user is given the final
choice.
</bodyText>
<page confidence="0.998683">
105
</page>
<sectionHeader confidence="0.861528" genericHeader="method">
V. Conclusions
</sectionHeader>
<bodyText confidence="0.97037446875">
The ability to understand text is dependent on
the ability to understand what is being described
in the text. Hence, a reader of, say, English text
must have applicable knowledge of both the
situations that may be described in texts (e.g.,
actions, states, sequences of events, goals,
methods of achieving goals, etc.) and the the
surface structures that appear in the language,
i.e., the relations between the surface order of
appearance of words and phrases, and their
corresponding meaning structures.
The process of text understanding is the
combined application of these knowledge sources as
a reader proceeds through a text. This fact
becomes clearest when we investigate the
understanding of texts that present particular
problems to a reader. Human understanding is
inherently tolerant; people are naturally able to
ignore many types of errors, omissions, poor
constructions, etc., and get straight to the
meaning of the text.
Our theories have tried to take this ability
into account by including knowledge and mechanisms
of error noticing and correcting as implicit parts
of our process models of language understanding.
The NOMAD system is the latest in a line of
&apos;tolerant&apos; language understanders, beginning with
FOUL-UP, all based on the use of knowledge of
syntax, semantics and pragmatics at all stages of
the understanding process to cope with errors.
VI. REFERENCES
Cullingford, R. 1977. Controlling Inference in
</bodyText>
<reference confidence="0.9989338">
Story Understanding. Proceedings of the Fifth
International Joint Conference on Artificial
Intelligence, Cambridge, Mass.
Granger, R. 1977. FOUL-UP: A program that
figures out meanings of words from context.
Proceedings of the Fifth IJCAIL, Cambridge, Mass.
Schank, R. and Abelson R. 1977 Scripts. Plans
Goals and Understanding. Lawrence Erlbaum
Associates, Hillsdale, N.J.
Taylor, G. 1982. English Generation Using More
Than Just CDs. Internal NOMAD Design Documenation,
UCI, 1982.
Wilensky, R. 1978. Undertanding Coal-based
Stories. Computer Science Technical Report 140,
Yale University.
</reference>
<page confidence="0.99732">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.888085">
<title confidence="0.9992645">Scruffy Text Understanding; Design and Implementation of the NOMAD System</title>
<author confidence="0.991058">Richard R Granger</author>
<author confidence="0.991058">Chris J Staros</author>
<author confidence="0.991058">Gregory B Taylor</author>
<author confidence="0.991058">Rika Yoshii</author>
<affiliation confidence="0.996915">Artificial Intelligence Project Department of Information and Computer Science University of California</affiliation>
<address confidence="0.999917">Irvine, California 92717</address>
<abstract confidence="0.993775142857143">II. Categories of Errors The task of understanding unedited naval ship-to-shore messages is implemented in the presence of a large database of domain specific knowledge. The program uses internal syntactic and semantic expectations to analyze the texts and to correct errors that arise during understanding, such as syntactic errors, missing punctuation, and errors of spelling and usage. The output of the system is a well-formed English translation of the message. This paper describes some of the knowledge mechanisms that have been implemented in the NOMAD system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Story Understanding</author>
</authors>
<booktitle>Proceedings of the Fifth International Joint Conference on Artificial Intelligence,</booktitle>
<location>Cambridge, Mass.</location>
<marker>Understanding, </marker>
<rawString>Story Understanding. Proceedings of the Fifth International Joint Conference on Artificial Intelligence, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Granger</author>
</authors>
<title>FOUL-UP: A program that figures out meanings of words from context.</title>
<date>1977</date>
<booktitle>Proceedings of the Fifth IJCAIL,</booktitle>
<location>Cambridge, Mass.</location>
<contexts>
<context position="2996" citStr="Granger, 1977" startWordPosition="457" endWordPosition="458">ed, and the module which is responsible for the correction is identified. A. Unknown words Consider the message, PEGASUS FRD 2 TALOS AT VICTOR NOMAD does not immediately recognize &apos;FRD as a word in the dictionary. Often the message sender will use an ad hoc abbreviation of a word or misspell a word. Any word not found to be in the dictionary is first put through a simple spelling correction procedure. If none of the possible corrections are recognizable then a morphological analyzer is applied to recognize different possible conjugations of a a verb. If this fails, a mechanism called FOUL-UP (Granger, 1977) is triggered. The FOUL-UP mechanism handles unknown words by using the program&apos;s own syntactic and semantic expectations to create a temporary definition that would allow it continue normally. FOUL-UP would later revise the definition of the unknown word by combining the expectations generated based on previous information with the role the unknown word is playing in the current context. B. Missinsk subject and objects Consider the following message of two sentences, CONSTELLATION SAW KASHIN. LOST CONTACT. A script-based (Schenk and Abelson, 1977) inferencer generates expectations to fill the</context>
</contexts>
<marker>Granger, 1977</marker>
<rawString>Granger, R. 1977. FOUL-UP: A program that figures out meanings of words from context. Proceedings of the Fifth IJCAIL, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schank</author>
<author>R Abelson</author>
</authors>
<title>Scripts. Plans Goals and Understanding. Lawrence Erlbaum Associates,</title>
<date>1977</date>
<location>Hillsdale, N.J.</location>
<marker>Schank, Abelson, 1977</marker>
<rawString>Schank, R. and Abelson R. 1977 Scripts. Plans Goals and Understanding. Lawrence Erlbaum Associates, Hillsdale, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Taylor</author>
</authors>
<title>English Generation Using More Than Just CDs.</title>
<date>1982</date>
<booktitle>Internal NOMAD Design Documenation, UCI,</booktitle>
<contexts>
<context position="6298" citStr="Taylor, 1982" startWordPosition="969" endWordPosition="970"> OPEN FIRED. ---&gt; LOCKED ON. OPENED FIRE. III. &amp;MAU Interface NOMAD uses a generator specifically designed for the naval domain to produce a well formed translation of the input message. This &apos;pretty&apos; form of the input message is checked by a user to assure that NOMAD has correctly understood the message. If NOMAD is then told it has incorrectly understood the message, alternative word definitions and other semantic choices are made in a second attempt at understanding. A. ihs. Generator The generator has been tailored to address some of the problems that occur in describing naval activities [Taylor, 19821. Many of the messages are characterized by incomplete and changing descriptions of actors. These descriptions contain information that may be unknown but doesn&apos;t change (name, type of ship, etc.) along with temporal information (location and damage status). The NOMAD generator produces unambiguous descriptions of actors while maintaining brevity. IV. An Example The following is an actual example shoving the capability of NOMAD to handle multiple problems found in a message. INPUT MESSAGE: PERISCOPE SIGHTED BY CONSTELLATION ABT 2000 YDS OFF PORT QTR, AND HS HILO VECTRED TO DATUM. GREEN FLARES</context>
</contexts>
<marker>Taylor, 1982</marker>
<rawString>Taylor, G. 1982. English Generation Using More Than Just CDs. Internal NOMAD Design Documenation, UCI, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wilensky</author>
</authors>
<title>Undertanding Coal-based Stories. Computer Science</title>
<date>1978</date>
<tech>Technical Report 140,</tech>
<institution>Yale University.</institution>
<marker>Wilensky, 1978</marker>
<rawString>Wilensky, R. 1978. Undertanding Coal-based Stories. Computer Science Technical Report 140, Yale University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>