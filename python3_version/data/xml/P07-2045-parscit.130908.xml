<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000116">
<title confidence="0.963273">
Moses: Open Source Toolkit for Statistical Machine Translation
</title>
<author confidence="0.906966">
Philipp Koehn
Hieu Hoang
Alexandra Birch
Chris Callison-Burch
</author>
<affiliation confidence="0.99656">
University of Edin-
</affiliation>
<email confidence="0.478223">
burgh1
</email>
<author confidence="0.714453">
Richard Zens
RWTH Aachen4
Marcello Federico
Nicola Bertoldi
</author>
<affiliation confidence="0.287349">
ITC-irst2
</affiliation>
<author confidence="0.949643">
Chris Dyer
</author>
<affiliation confidence="0.949862">
University of Maryland5
</affiliation>
<author confidence="0.739481666666667">
Brooke Cowan
Wade Shen
Christine Moran
</author>
<note confidence="0.724398">
MIT3
</note>
<title confidence="0.611291">
Ondřej Bojar
</title>
<author confidence="0.769258333333333">
Charles University6
Alexandra Constantin Evan Herbst
Williams College7 Cornell8
</author>
<equation confidence="0.2415685">
1 pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk.
2{federico, bertoldi}@itc.it. 3 brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu. 4
zens@i6.informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7
07aec_2@williams.edu. 8 evh4@cornell.edu
</equation>
<sectionHeader confidence="0.947368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999111">
We describe an open-source toolkit for sta-
tistical machine translation whose novel
contributions are (a) support for linguisti-
cally motivated factors, (b) confusion net-
work decoding, and (c) efficient data for-
mats for translation models and language
models. In addition to the SMT decoder,
the toolkit also includes a wide variety of
tools for training, tuning and applying the
system to many translation tasks.
</bodyText>
<sectionHeader confidence="0.988648" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999707659090909">
Phrase-based statistical machine translation
(Koehn et al. 2003) has emerged as the dominant
paradigm in machine translation research. How-
ever, until now, most work in this field has been
carried out on proprietary and in-house research
systems. This lack of openness has created a high
barrier to entry for researchers as many of the
components required have had to be duplicated.
This has also hindered effective comparisons of the
different elements of the systems.
By providing a free and complete toolkit, we
hope that this will stimulate the development of the
field. For this system to be adopted by the commu-
nity, it must demonstrate performance that is com-
parable to the best available systems. Moses has
shown that it achieves results comparable to the
most competitive and widely used statistical ma-
chine translation systems in translation quality and
run-time (Shen et al. 2006). It features all the ca-
pabilities of the closed sourced Pharaoh decoder
(Koehn 2004).
Apart from providing an open-source toolkit
for SMT, a further motivation for Moses is to ex-
tend phrase-based translation with factors and con-
fusion network decoding.
The current phrase-based approach to statisti-
cal machine translation is limited to the mapping of
small text chunks without any explicit use of lin-
guistic information, be it morphological, syntactic,
or semantic. These additional sources of informa-
tion have been shown to be valuable when inte-
grated into pre-processing or post-processing steps.
Moses also integrates confusion network de-
coding, which allows the translation of ambiguous
input. This enables, for instance, the tighter inte-
gration of speech recognition and machine transla-
tion. Instead of passing along the one-best output
of the recognizer, a network of different word
choices may be examined by the machine transla-
tion system.
Efficient data structures in Moses for the
memory-intensive translation model and language
model allow the exploitation of much larger data
resources with limited hardware.
</bodyText>
<page confidence="0.967283">
177
</page>
<bodyText confidence="0.5004885">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.975052" genericHeader="introduction">
2 Toolkit
</sectionHeader>
<bodyText confidence="0.998318666666667">
The toolkit is a complete out-of-the-box trans-
lation system for academic research. It consists of
all the components needed to preprocess data, train
the language models and the translation models. It
also contains tools for tuning these models using
minimum error rate training (Och 2003) and evalu-
ating the resulting translations using the BLEU
score (Papineni et al. 2002).
Moses uses standard external tools for some of
the tasks to avoid duplication, such as GIZA++
(Och and Ney 2003) for word alignments and
SRILM for language modeling. Also, since these
tasks are often CPU intensive, the toolkit has been
designed to work with Sun Grid Engine parallel
environment to increase throughput.
In order to unify the experimental stages, a
utility has been developed to run repeatable ex-
periments. This uses the tools contained in Moses
and requires minimal changes to set up and cus-
tomize.
The toolkit has been hosted and developed un-
der sourceforge.net since inception. Moses has an
active research community and has reached over
1000 downloads as of 1st March 2007.
The main online presence is at
http://www.statmt.org/moses/
where many sources of information about the
project can be found. Moses was the subject of this
year’s Johns Hopkins University Workshop on
Machine Translation (Koehn et al. 2006).
The decoder is the core component of Moses.
To minimize the learning curve for many research-
ers, the decoder was developed as a drop-in re-
placement for Pharaoh, the popular phrase-based
decoder.
In order for the toolkit to be adopted by the
community, and to make it easy for others to con-
tribute to the project, we kept to the following
principles when developing the decoder:
</bodyText>
<listItem confidence="0.9999624">
• Accessibility
• Easy to Maintain
• Flexibility
• Easy for distributed team development
• Portability
</listItem>
<bodyText confidence="0.997656">
It was developed in C++ for efficiency and fol-
lowed modular, object-oriented design.
</bodyText>
<sectionHeader confidence="0.995882" genericHeader="method">
3 Factored Translation Model
</sectionHeader>
<bodyText confidence="0.998709666666667">
Non-factored SMT typically deals only with
the surface form of words and has one phrase table,
as shown in Figure 1.
</bodyText>
<figure confidence="0.93858">
Translate:
i am buying you a green cat
je vous achète un chat vert
using phrase dictionary:
i
am buying
vous
green
chat
</figure>
<figureCaption confidence="0.999949">
Figure 1. Non-factored translation
</figureCaption>
<bodyText confidence="0.99996675">
In factored translation models, the surface
forms may be augmented with different factors,
such as POS tags or lemma. This creates a factored
representation of each word, Figure 2.
</bodyText>
<equation confidence="0.990313703703704">
⎛⎞⎛ ⎞⎛
je vous achet ⎞⎛ ⎞⎛
un chat ⎞
⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟
⎜⎟⎜ ⎟⎜
PRO PRO VB ART NN
⎟⎜ ⎟⎜ ⎟
⎜⎟⎜ ⎟⎜
je vous acheter ⎟⎜ ⎟⎜
un chat ⎟
⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟
⎝⎠⎝ ⎠⎝
1st 1st st present masc
1 / ⎠⎝ ⎠⎝ sing /masc ⎠
⎛ ⎞⎛
i buy ⎞⎛ ⎞⎛ ⎞⎛ ⎞
you a cat
⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟
⎜⎟⎜
PRO VB ⎟⎜⎟⎜ ⎟⎜ ⎟
PRO ART NN
⎜ ⎟⎜
i tobuy ⎟⎜ ⎟⎜ ⎟⎜ ⎟
you a cat
⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟
⎝⎠⎝
1st 1st / present⎠⎝1st⎠⎝sing⎠⎝sing
</equation>
<figureCaption confidence="0.9994">
Figure 2. Factored translation
</figureCaption>
<bodyText confidence="0.999945">
Mapping of source phrases to target phrases
may be decomposed into several steps. Decompo-
sition of the decoding process into various steps
means that different factors can be modeled sepa-
rately. Modeling factors in isolation allows for
flexibility in their application. It can also increase
accuracy and reduce sparsity by minimizing the
number dependencies for each step.
For example, we can decompose translating
from surface forms to surface forms and lemma, as
shown in Figure 3.
</bodyText>
<figure confidence="0.996221222222222">
je
achète
you
a
un
a
une
vert
cat
</figure>
<page confidence="0.768647">
178
</page>
<figureCaption confidence="0.999741">
Figure 3. Example of graph of decoding steps
</figureCaption>
<bodyText confidence="0.999696818181818">
By allowing the graph to be user definable, we
can experiment to find the optimum configuration
for a given language pair and available data.
The factors on the source sentence are consid-
ered fixed, therefore, there is no decoding step
which create source factors from other source fac-
tors. However, Moses can have ambiguous input in
the form of confusion networks. This input type
has been used successfully for speech to text
translation (Shen et al. 2006).
Every factor on the target language can have its
own language model. Since many factors, like
lemmas and POS tags, are less sparse than surface
forms, it is possible to create a higher order lan-
guage models for these factors. This may encour-
age more syntactically correct output. In Figure 3
we apply two language models, indicated by the
shaded arrows, one over the words and another
over the lemmas. Moses is also able to integrate
factored language models, such as those described
in (Bilmes and Kirchhoff 2003) and (Axelrod
2006).
</bodyText>
<sectionHeader confidence="0.998643" genericHeader="method">
4 Confusion Network Decoding
</sectionHeader>
<bodyText confidence="0.99340253030303">
Machine translation input currently takes the
form of simple sequences of words. However,
there are increasing demands to integrate machine
translation technology into larger information
processing systems with upstream NLP/speech
processing tools (such as named entity recognizers,
speech recognizers, morphological analyzers, etc.).
These upstream processes tend to generate multiple,
erroneous hypotheses with varying confidence.
Current MT systems are designed to process only
one input hypothesis, making them vulnerable to
errors in the input.
In experiments with confusion networks, we
have focused so far on the speech translation case,
where the input is generated by a speech recog-
nizer. Namely, our goal is to improve performance
of spoken language translation by better integrating
speech recognition and machine translation models.
Translation from speech input is considered more
difficult than translation from text for several rea-
sons. Spoken language has many styles and genres,
such as, formal read speech, unplanned speeches,
interviews, spontaneous conversations; it produces
less controlled language, presenting more relaxed
syntax and spontaneous speech phenomena. Fi-
nally, translation of spoken language is prone to
speech recognition errors, which can possibly cor-
rupt the syntax and the meaning of the input.
There is also empirical evidence that better
translations can be obtained from transcriptions of
the speech recognizer which resulted in lower
scores. This suggests that improvements can be
achieved by applying machine translation on a
large set of transcription hypotheses generated by
the speech recognizers and by combining scores of
acoustic models, language models, and translation
models.
Recently, approaches have been proposed for
improving translation quality through the process-
ing of multiple input hypotheses. We have imple-
mented in Moses confusion network decoding as
discussed in (Bertoldi and Federico 2005), and de-
veloped a simpler translation model and a more
efficient implementation of the search algorithm.
Remarkably, the confusion network decoder re-
sulted in an extension of the standard text decoder.
5 Efficient Data Structures for Transla-
tion Model and Language Models
With the availability of ever-increasing
amounts of training data, it has become a challenge
for machine translation systems to cope with the
resulting strain on computational resources. Instead
of simply buying larger machines with, say, 12 GB
of main memory, the implementation of more effi-
cient data structures in Moses makes it possible to
exploit larger data resources with limited hardware
infrastructure.
A phrase translation table easily takes up giga-
bytes of disk space, but for the translation of a sin-
gle sentence only a tiny fraction of this table is
needed. Moses implements an efficient representa-
tion of the phrase translation table. Its key proper-
ties are a prefix tree structure for source words and
on demand loading, i.e. only the fraction of the
phrase table that is needed to translate a sentence is
loaded into the working memory of the decoder.
</bodyText>
<page confidence="0.996103">
179
</page>
<bodyText confidence="0.999639714285714">
For the Chinese-English NIST task, the mem-
ory requirement of the phrase table is reduced from
1.7 gigabytes to less than 20 mega bytes, with no
loss in translation quality and speed (Zens and Ney
2007).
The other large data resource for statistical ma-
chine translation is the language model. Almost
unlimited text resources can be collected from the
Internet and used as training data for language
modeling. This results in language models that are
too large to easily fit into memory.
The Moses system implements a data structure
for language models that is more efficient than the
canonical SRILM (Stolcke 2002) implementation
used in most systems. The language model on disk
is also converted into this binary format, resulting
in a minimal loading time during start-up of the
decoder.
An even more compact representation of the
language model is the result of the quantization of
the word prediction and back-off probabilities of
the language model. Instead of representing these
probabilities with 4 byte or 8 byte floats, they are
sorted into bins, resulting in (typically) 256 bins
which can be referenced with a single 1 byte index.
This quantized language model, albeit being less
accurate, has only minimal impact on translation
performance (Federico and Bertoldi 2006).
</bodyText>
<sectionHeader confidence="0.996442" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999990090909091">
This paper has presented a suite of open-source
tools which we believe will be of value to the MT
research community.
We have also described a new SMT decoder
which can incorporate some linguistic features in a
consistent and flexible framework. This new direc-
tion in research opens up many possibilities and
issues that require further research and experimen-
tation. Initial results show the potential benefit of
factors for statistical machine translation, (Koehn
et al. 2006) and (Koehn and Hoang 2007).
</bodyText>
<sectionHeader confidence="0.998934" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998405458333333">
Axelrod, Amittai. &amp;quot;Factored Language Model for Sta-
tistical Machine Translation.&amp;quot; MRes Thesis.
Edinburgh University, 2006.
Bertoldi, Nicola, and Marcello Federico. &amp;quot;A New De-
coder for Spoken Language Translation Based
on Confusion Networks.&amp;quot; Automatic Speech
Recognition and Understanding Workshop
(ASRU), 2005.
Bilmes, Jeff A, and Katrin Kirchhoff. &amp;quot;Factored Lan-
guage Models and Generalized Parallel Back-
off.&amp;quot; HLT/NACCL, 2003.
Koehn, Philipp. &amp;quot;Pharaoh: A Beam Search Decoder for
Phrase-Based Statistical Machine Translation
Models.&amp;quot; AMTA, 2004.
Koehn, Philipp, Marcello Federico, Wade Shen, Nicola
Bertoldi, Ondrej Bojar, Chris Callison-Burch,
Brooke Cowan, Chris Dyer, Hieu Hoang,
Richard Zens, Alexandra Constantin, Christine
Corbett Moran, and Evan Herbst. &amp;quot;Open
Source Toolkit for Statistical Machine Transla-
tion&amp;quot;. Report of the 2006 Summer Workshop at
Johns Hopkins University, 2006.
Koehn, Philipp, and Hieu Hoang. &amp;quot;Factored Translation
Models.&amp;quot; EMNLP, 2007.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
&amp;quot;Statistical Phrase-Based Translation.&amp;quot;
HLT/NAACL, 2003.
Och, Franz Josef. &amp;quot;Minimum Error Rate Training for
Statistical Machine Translation.&amp;quot; ACL, 2003.
Och, Franz Josef, and Hermann Ney. &amp;quot;A Systematic
Comparison of Various Statistical Alignment
Models.&amp;quot; Computational Linguistics 29.1
(2003): 19-51.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. &amp;quot;BLEU: A Method for Automatic
Evaluation of Machine Translation.&amp;quot; ACL,
2002.
Shen, Wade, Richard Zens, Nicola Bertoldi, and
Marcello Federico. &amp;quot;The JHU Workshop 2006
Iwslt System.&amp;quot; International Workshop on Spo-
ken Language Translation, 2006.
Stolcke, Andreas. &amp;quot;SRILM an Extensible Language
Modeling Toolkit.&amp;quot; Intl. Conf. on Spoken Lan-
guage Processing, 2002.
Zens, Richard, and Hermann Ney. &amp;quot;Efficient Phrase-
Table Representation for Machine Translation
with Applications to Online MT and Speech
Recognition.&amp;quot; HLT/NAACL, 2007.
</reference>
<page confidence="0.997688">
180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999167">Moses: Open Source Toolkit for Statistical Machine Translation</title>
<author confidence="0.9776745">Philipp Koehn Hieu Hoang Alexandra Birch Chris Callison-Burch</author>
<email confidence="0.312373">of</email>
<author confidence="0.9794845">Richard Zens Marcello Federico Nicola Bertoldi Chris Dyer</author>
<email confidence="0.429736">of</email>
<author confidence="0.8753588">Brooke Cowan Wade Shen Christine Moran Bojar Alexandra Constantin Evan Herbst</author>
<note confidence="0.675195333333333">1pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk. bertoldi}@itc.it. 3brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu. 4 5redpony@umd.edu. 6bojar@ufal.ms.mff.cuni.cz. 7</note>
<email confidence="0.739492">8evh4@cornell.edu</email>
<abstract confidence="0.992759795180723">We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks. 1 Motivation Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research. However, until now, most work in this field has been carried out on proprietary and in-house research systems. This lack of openness has created a high barrier to entry for researchers as many of the components required have had to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is comparable to the best available systems. Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al. 2006). It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004). Apart from providing an open-source toolkit for SMT, a further motivation for Moses is to extend phrase-based translation with factors and confusion network decoding. The current phrase-based approach to statistical machine translation is limited to the mapping of small text chunks without any explicit use of linguistic information, be it morphological, syntactic, or semantic. These additional sources of information have been shown to be valuable when integrated into pre-processing or post-processing steps. Moses also integrates confusion network decoding, which allows the translation of ambiguous input. This enables, for instance, the tighter integration of speech recognition and machine translation. Instead of passing along the one-best output of the recognizer, a network of different word choices may be examined by the machine translation system. Efficient data structures in Moses for the memory-intensive translation model and language model allow the exploitation of much larger data resources with limited hardware. 177 of the ACL 2007 Demo and Poster pages 177–180, June 2007. Association for Computational Linguistics 2 Toolkit The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Moses has an active research community and has reached over</abstract>
<note confidence="0.4803835">downloads as of March 2007. The main online presence is at</note>
<web confidence="0.629318">http://www.statmt.org/moses/</web>
<abstract confidence="0.991864087557604">where many sources of information about the project can be found. Moses was the subject of this year’s Johns Hopkins University Workshop on Machine Translation (Koehn et al. 2006). The decoder is the core component of Moses. To minimize the learning curve for many researchers, the decoder was developed as a drop-in replacement for Pharaoh, the popular phrase-based decoder. In order for the toolkit to be adopted by the community, and to make it easy for others to contribute to the project, we kept to the following principles when developing the decoder: • Accessibility • Easy to Maintain • Flexibility • Easy for distributed team development • Portability It was developed in C++ for efficiency and followed modular, object-oriented design. 3 Factored Translation Model Non-factored SMT typically deals only with the surface form of words and has one phrase table, as shown in Figure 1. Translate: i am buying you a green cat vous achète un vert using phrase dictionary: i am buying vous green chat Figure 1. Non-factored translation In factored translation models, the surface forms may be augmented with different factors, such as POS tags or lemma. This creates a factored representation of each word, Figure 2. ⎛⎞⎛ ⎞⎛ vous achet ⎞⎛ chat ⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟ ⎜⎟⎜ ⎟⎜ PRO PRO VB ART NN ⎟⎜ ⎟⎜ ⎟ ⎜⎟⎜ ⎟⎜ je vous acheter ⎟⎜ chat ⎟⎜ ⎟⎜ ⎟⎜ ⎝⎠⎝ ⎠⎝ st present masc / ⎠⎝ ⎛ ⎞⎛ buy ⎞⎛ ⎞⎛ ⎞ you a cat ⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟ ⎜⎟⎜ VB ⎟⎜ ⎟ PRO ART NN ⎜ ⎟⎜ tobuy ⎟⎜ ⎟⎜ ⎟ you a cat ⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟ ⎝⎠⎝ Figure 2. Factored translation Mapping of source phrases to target phrases may be decomposed into several steps. Decomposition of the decoding process into various steps means that different factors can be modeled separately. Modeling factors in isolation allows for flexibility in their application. It can also increase accuracy and reduce sparsity by minimizing the number dependencies for each step. For example, we can decompose translating from surface forms to surface forms and lemma, as shown in Figure 3. je achète you a un a une vert cat 178 Figure 3. Example of graph of decoding steps By allowing the graph to be user definable, we can experiment to find the optimum configuration for a given language pair and available data. The factors on the source sentence are considered fixed, therefore, there is no decoding step which create source factors from other source factors. However, Moses can have ambiguous input in the form of confusion networks. This input type has been used successfully for speech to text translation (Shen et al. 2006). Every factor on the target language can have its own language model. Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors. This may encourage more syntactically correct output. In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas. Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006). 4 Confusion Network Decoding Machine translation input currently takes the form of simple sequences of words. However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.). These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence. Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input. In experiments with confusion networks, we have focused so far on the speech translation case, where the input is generated by a speech recognizer. Namely, our goal is to improve performance of spoken language translation by better integrating speech recognition and machine translation models. Translation from speech input is considered more difficult than translation from text for several reasons. Spoken language has many styles and genres, such as, formal read speech, unplanned speeches, interviews, spontaneous conversations; it produces less controlled language, presenting more relaxed syntax and spontaneous speech phenomena. Finally, translation of spoken language is prone to speech recognition errors, which can possibly corrupt the syntax and the meaning of the input. There is also empirical evidence that better translations can be obtained from transcriptions of the speech recognizer which resulted in lower scores. This suggests that improvements can be achieved by applying machine translation on a large set of transcription hypotheses generated by the speech recognizers and by combining scores of acoustic models, language models, and translation models. Recently, approaches have been proposed for improving translation quality through the processing of multiple input hypotheses. We have implemented in Moses confusion network decoding as discussed in (Bertoldi and Federico 2005), and developed a simpler translation model and a more efficient implementation of the search algorithm. Remarkably, the confusion network decoder resulted in an extension of the standard text decoder. 5 Efficient Data Structures for Translation Model and Language Models With the availability of ever-increasing amounts of training data, it has become a challenge for machine translation systems to cope with the resulting strain on computational resources. Instead of simply buying larger machines with, say, 12 GB of main memory, the implementation of more efficient data structures in Moses makes it possible to exploit larger data resources with limited hardware infrastructure. A phrase translation table easily takes up gigabytes of disk space, but for the translation of a single sentence only a tiny fraction of this table is needed. Moses implements an efficient representation of the phrase translation table. Its key properare a tree for source words and demand i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder. 179 For the Chinese-English NIST task, the memory requirement of the phrase table is reduced from 1.7 gigabytes to less than 20 mega bytes, with no loss in translation quality and speed (Zens and Ney 2007). The other large data resource for statistical machine translation is the language model. Almost unlimited text resources can be collected from the Internet and used as training data for language modeling. This results in language models that are too large to easily fit into memory. The Moses system implements a data structure for language models that is more efficient than the canonical SRILM (Stolcke 2002) implementation used in most systems. The language model on disk is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder. An even more compact representation of the model is the result of the the word prediction and back-off probabilities of the language model. Instead of representing these probabilities with 4 byte or 8 byte floats, they are sorted into bins, resulting in (typically) 256 bins which can be referenced with a single 1 byte index. This quantized language model, albeit being less accurate, has only minimal impact on translation performance (Federico and Bertoldi 2006). 6 Conclusion and Future Work This paper has presented a suite of open-source tools which we believe will be of value to the MT research community. We have also described a new SMT decoder which can incorporate some linguistic features in a consistent and flexible framework. This new direction in research opens up many possibilities and issues that require further research and experimentation. Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).</abstract>
<note confidence="0.61115975">References Amittai. Language Model for Sta- Machine Translation.&amp;quot; Thesis. Edinburgh University, 2006.</note>
<title confidence="0.7995085">Nicola, and Marcello Federico. New Decoder for Spoken Language Translation Based Confusion Networks.&amp;quot; Speech Recognition and Understanding Workshop</title>
<note confidence="0.43081475">(ASRU),2005. Jeff A, and Katrin Kirchhoff. Language Models and Generalized Parallel Back- HLT/NACCL,2003.</note>
<title confidence="0.882024">Philipp. A Beam Search Decoder for Phrase-Based Statistical Machine Translation</title>
<note confidence="0.735225125">AMTA,2004. Koehn, Philipp, Marcello Federico, Wade Shen, Nicola Bertoldi, Ondrej Bojar, Chris Callison-Burch, Brooke Cowan, Chris Dyer, Hieu Hoang, Richard Zens, Alexandra Constantin, Christine Moran, and Evan Herbst. Source Toolkit for Statistical Machine Transla- Report of the 2006 Summer Workshop at Johns Hopkins University, 2006. Philipp, and Hieu Hoang. Translation EMNLP,2007. Koehn, Philipp, Franz Josef Och, and Daniel Marcu. &amp;quot;Statistical Phrase-Based Translation.&amp;quot; HLT/NAACL,2003. Franz Josef. Error Rate Training for Machine Translation.&amp;quot; ACL,2003.</note>
<author confidence="0.915946">Systematic</author>
<affiliation confidence="0.920814">Comparison of Various Statistical Alignment</affiliation>
<address confidence="0.702128">Linguistics29.1</address>
<phone confidence="0.451773">(2003): 19-51.</phone>
<title confidence="0.836883">Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-</title>
<author confidence="0.71236">A Method for Automatic</author>
<affiliation confidence="0.736201">of Machine Translation.&amp;quot; ACL,</affiliation>
<address confidence="0.5762925">2002. Shen, Wade, Richard Zens, Nicola Bertoldi, and</address>
<note confidence="0.728405857142857">Federico. JHU Workshop 2006 System.&amp;quot; Workshop on Spo- Language Translation,2006. Andreas. an Extensible Language Toolkit.&amp;quot; Conf. on Spoken Lan- Processing,2002. Richard, and Hermann Ney. Phrase-</note>
<title confidence="0.9512235">Table Representation for Machine Translation with Applications to Online MT and Speech</title>
<note confidence="0.598498">HLT/NAACL,2007. 180</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
</authors>
<title>Factored Language Model for Statistical Machine Translation.&amp;quot; MRes Thesis.</title>
<date>2006</date>
<institution>Edinburgh University,</institution>
<contexts>
<context position="7608" citStr="Axelrod 2006" startWordPosition="1230" endWordPosition="1231">e has been used successfully for speech to text translation (Shen et al. 2006). Every factor on the target language can have its own language model. Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors. This may encourage more syntactically correct output. In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas. Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006). 4 Confusion Network Decoding Machine translation input currently takes the form of simple sequences of words. However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.). These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence. Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input. In experiments with</context>
</contexts>
<marker>Axelrod, 2006</marker>
<rawString>Axelrod, Amittai. &amp;quot;Factored Language Model for Statistical Machine Translation.&amp;quot; MRes Thesis. Edinburgh University, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>A New Decoder for Spoken Language Translation Based on Confusion Networks.&amp;quot; Automatic Speech Recognition and Understanding Workshop (ASRU),</title>
<date>2005</date>
<contexts>
<context position="9595" citStr="Bertoldi and Federico 2005" startWordPosition="1511" endWordPosition="1514">put. There is also empirical evidence that better translations can be obtained from transcriptions of the speech recognizer which resulted in lower scores. This suggests that improvements can be achieved by applying machine translation on a large set of transcription hypotheses generated by the speech recognizers and by combining scores of acoustic models, language models, and translation models. Recently, approaches have been proposed for improving translation quality through the processing of multiple input hypotheses. We have implemented in Moses confusion network decoding as discussed in (Bertoldi and Federico 2005), and developed a simpler translation model and a more efficient implementation of the search algorithm. Remarkably, the confusion network decoder resulted in an extension of the standard text decoder. 5 Efficient Data Structures for Translation Model and Language Models With the availability of ever-increasing amounts of training data, it has become a challenge for machine translation systems to cope with the resulting strain on computational resources. Instead of simply buying larger machines with, say, 12 GB of main memory, the implementation of more efficient data structures in Moses makes</context>
</contexts>
<marker>Bertoldi, Federico, 2005</marker>
<rawString>Bertoldi, Nicola, and Marcello Federico. &amp;quot;A New Decoder for Spoken Language Translation Based on Confusion Networks.&amp;quot; Automatic Speech Recognition and Understanding Workshop (ASRU), 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff A Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored Language Models and Generalized Parallel Backoff.&amp;quot; HLT/NACCL,</title>
<date>2003</date>
<contexts>
<context position="7589" citStr="Bilmes and Kirchhoff 2003" startWordPosition="1225" endWordPosition="1228">nfusion networks. This input type has been used successfully for speech to text translation (Shen et al. 2006). Every factor on the target language can have its own language model. Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors. This may encourage more syntactically correct output. In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas. Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006). 4 Confusion Network Decoding Machine translation input currently takes the form of simple sequences of words. However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.). These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence. Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input. </context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Bilmes, Jeff A, and Katrin Kirchhoff. &amp;quot;Factored Language Models and Generalized Parallel Backoff.&amp;quot; HLT/NACCL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models.&amp;quot;</title>
<date>2004</date>
<location>AMTA,</location>
<contexts>
<context position="2106" citStr="Koehn 2004" startWordPosition="296" endWordPosition="297">to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is comparable to the best available systems. Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al. 2006). It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004). Apart from providing an open-source toolkit for SMT, a further motivation for Moses is to extend phrase-based translation with factors and confusion network decoding. The current phrase-based approach to statistical machine translation is limited to the mapping of small text chunks without any explicit use of linguistic information, be it morphological, syntactic, or semantic. These additional sources of information have been shown to be valuable when integrated into pre-processing or post-processing steps. Moses also integrates confusion network decoding, which allows the translation of amb</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Koehn, Philipp. &amp;quot;Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models.&amp;quot; AMTA, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Marcello Federico</author>
<author>Wade Shen</author>
<author>Nicola Bertoldi</author>
<author>Ondrej Bojar</author>
<author>Chris Callison-Burch</author>
<author>Brooke Cowan</author>
<author>Chris Dyer</author>
</authors>
<title>Open Source Toolkit for Statistical Machine Translation&amp;quot;. Report of the 2006 Summer Workshop at Johns Hopkins University,</title>
<date>2006</date>
<location>Hieu Hoang, Richard Zens, Alexandra</location>
<contexts>
<context position="4603" citStr="Koehn et al. 2006" startWordPosition="682" endWordPosition="685">put. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Moses has an active research community and has reached over 1000 downloads as of 1st March 2007. The main online presence is at http://www.statmt.org/moses/ where many sources of information about the project can be found. Moses was the subject of this year’s Johns Hopkins University Workshop on Machine Translation (Koehn et al. 2006). The decoder is the core component of Moses. To minimize the learning curve for many researchers, the decoder was developed as a drop-in replacement for Pharaoh, the popular phrase-based decoder. In order for the toolkit to be adopted by the community, and to make it easy for others to contribute to the project, we kept to the following principles when developing the decoder: • Accessibility • Easy to Maintain • Flexibility • Easy for distributed team development • Portability It was developed in C++ for efficiency and followed modular, object-oriented design. 3 Factored Translation Model Non</context>
</contexts>
<marker>Koehn, Federico, Shen, Bertoldi, Bojar, Callison-Burch, Cowan, Dyer, 2006</marker>
<rawString>Koehn, Philipp, Marcello Federico, Wade Shen, Nicola Bertoldi, Ondrej Bojar, Chris Callison-Burch, Brooke Cowan, Chris Dyer, Hieu Hoang, Richard Zens, Alexandra Constantin, Christine Corbett Moran, and Evan Herbst. &amp;quot;Open Source Toolkit for Statistical Machine Translation&amp;quot;. Report of the 2006 Summer Workshop at Johns Hopkins University, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored Translation Models.&amp;quot; EMNLP,</title>
<date>2007</date>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Koehn, Philipp, and Hieu Hoang. &amp;quot;Factored Translation Models.&amp;quot; EMNLP, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.&amp;quot; HLT/NAACL,</title>
<date>2003</date>
<contexts>
<context position="1195" citStr="Koehn et al. 2003" startWordPosition="144" endWordPosition="147">6.informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 07aec_2@williams.edu. 8 evh4@cornell.edu Abstract We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks. 1 Motivation Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research. However, until now, most work in this field has been carried out on proprietary and in-house research systems. This lack of openness has created a high barrier to entry for researchers as many of the components required have had to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is co</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, Philipp, Franz Josef Och, and Daniel Marcu. &amp;quot;Statistical Phrase-Based Translation.&amp;quot; HLT/NAACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training for Statistical Machine Translation.&amp;quot; ACL,</title>
<date>2003</date>
<contexts>
<context position="3583" citStr="Och 2003" startWordPosition="519" endWordPosition="520">cient data structures in Moses for the memory-intensive translation model and language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, Prague, June 2007. c�2007 Association for Computational Linguistics 2 Toolkit The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customiz</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, Franz Josef. &amp;quot;Minimum Error Rate Training for Statistical Machine Translation.&amp;quot; ACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.&amp;quot;</title>
<date>2003</date>
<journal>Computational Linguistics</journal>
<volume>29</volume>
<contexts>
<context position="3783" citStr="Och and Ney 2003" startWordPosition="551" endWordPosition="554">ACL 2007 Demo and Poster Sessions, pages 177–180, Prague, June 2007. c�2007 Association for Computational Linguistics 2 Toolkit The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Moses has an active research community and has reached over 1000 downloads as of 1st March 2007. The main online pre</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, Franz Josef, and Hermann Ney. &amp;quot;A Systematic Comparison of Various Statistical Alignment Models.&amp;quot; Computational Linguistics 29.1 (2003): 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.&amp;quot;</title>
<date>2002</date>
<publisher>ACL,</publisher>
<contexts>
<context position="3669" citStr="Papineni et al. 2002" startWordPosition="531" endWordPosition="534">and language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, Prague, June 2007. c�2007 Association for Computational Linguistics 2 Toolkit The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Mo</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and WeiJing Zhu. &amp;quot;BLEU: A Method for Automatic Evaluation of Machine Translation.&amp;quot; ACL, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wade Shen</author>
<author>Richard Zens</author>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>The JHU Workshop</title>
<date>2006</date>
<booktitle>Iwslt System.&amp;quot; International Workshop on Spoken Language Translation,</booktitle>
<contexts>
<context position="2021" citStr="Shen et al. 2006" startWordPosition="280" endWordPosition="283">reated a high barrier to entry for researchers as many of the components required have had to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is comparable to the best available systems. Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al. 2006). It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004). Apart from providing an open-source toolkit for SMT, a further motivation for Moses is to extend phrase-based translation with factors and confusion network decoding. The current phrase-based approach to statistical machine translation is limited to the mapping of small text chunks without any explicit use of linguistic information, be it morphological, syntactic, or semantic. These additional sources of information have been shown to be valuable when integrated into pre-processing or post-processing steps. </context>
<context position="7073" citStr="Shen et al. 2006" startWordPosition="1137" endWordPosition="1140">lating from surface forms to surface forms and lemma, as shown in Figure 3. je achète you a un a une vert cat 178 Figure 3. Example of graph of decoding steps By allowing the graph to be user definable, we can experiment to find the optimum configuration for a given language pair and available data. The factors on the source sentence are considered fixed, therefore, there is no decoding step which create source factors from other source factors. However, Moses can have ambiguous input in the form of confusion networks. This input type has been used successfully for speech to text translation (Shen et al. 2006). Every factor on the target language can have its own language model. Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors. This may encourage more syntactically correct output. In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas. Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006). 4 Confusion Network Decoding Machine translation input currentl</context>
</contexts>
<marker>Shen, Zens, Bertoldi, Federico, 2006</marker>
<rawString>Shen, Wade, Richard Zens, Nicola Bertoldi, and Marcello Federico. &amp;quot;The JHU Workshop 2006 Iwslt System.&amp;quot; International Workshop on Spoken Language Translation, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM an Extensible Language Modeling Toolkit.&amp;quot;</title>
<date>2002</date>
<booktitle>Intl. Conf. on Spoken Language Processing,</booktitle>
<contexts>
<context position="11352" citStr="Stolcke 2002" startWordPosition="1801" endWordPosition="1802">For the Chinese-English NIST task, the memory requirement of the phrase table is reduced from 1.7 gigabytes to less than 20 mega bytes, with no loss in translation quality and speed (Zens and Ney 2007). The other large data resource for statistical machine translation is the language model. Almost unlimited text resources can be collected from the Internet and used as training data for language modeling. This results in language models that are too large to easily fit into memory. The Moses system implements a data structure for language models that is more efficient than the canonical SRILM (Stolcke 2002) implementation used in most systems. The language model on disk is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder. An even more compact representation of the language model is the result of the quantization of the word prediction and back-off probabilities of the language model. Instead of representing these probabilities with 4 byte or 8 byte floats, they are sorted into bins, resulting in (typically) 256 bins which can be referenced with a single 1 byte index. This quantized language model, albeit being less accurate, has only mini</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, Andreas. &amp;quot;SRILM an Extensible Language Modeling Toolkit.&amp;quot; Intl. Conf. on Spoken Language Processing, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Efficient PhraseTable Representation for Machine Translation with Applications to Online MT and Speech Recognition.&amp;quot; HLT/NAACL,</title>
<date>2007</date>
<contexts>
<context position="10940" citStr="Zens and Ney 2007" startWordPosition="1733" endWordPosition="1736">gigabytes of disk space, but for the translation of a single sentence only a tiny fraction of this table is needed. Moses implements an efficient representation of the phrase translation table. Its key properties are a prefix tree structure for source words and on demand loading, i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder. 179 For the Chinese-English NIST task, the memory requirement of the phrase table is reduced from 1.7 gigabytes to less than 20 mega bytes, with no loss in translation quality and speed (Zens and Ney 2007). The other large data resource for statistical machine translation is the language model. Almost unlimited text resources can be collected from the Internet and used as training data for language modeling. This results in language models that are too large to easily fit into memory. The Moses system implements a data structure for language models that is more efficient than the canonical SRILM (Stolcke 2002) implementation used in most systems. The language model on disk is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder. An even more</context>
</contexts>
<marker>Zens, Ney, 2007</marker>
<rawString>Zens, Richard, and Hermann Ney. &amp;quot;Efficient PhraseTable Representation for Machine Translation with Applications to Online MT and Speech Recognition.&amp;quot; HLT/NAACL, 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>