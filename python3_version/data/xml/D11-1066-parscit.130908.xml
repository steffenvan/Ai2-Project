<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000033">
<title confidence="0.996547">
Using Syntactic and Semantic Structural Kernels for
Classifying Definition Questions in Jeopardy!
</title>
<author confidence="0.923613">
Alessandro Moschitti† Jennifer Chu-Carroll‡ Siddharth Patwardhan‡
James Fan‡ Giuseppe Riccardi†
</author>
<affiliation confidence="0.989072">
†Department of Information Engineering and Computer Science
University of Trento, 38123 Povo (TN), Italy
</affiliation>
<email confidence="0.979653">
{moschitti,riccardi}@disi.unitn.it
</email>
<note confidence="0.423269">
‡IBM T.J. Watson Research Center P.O. Box 704, Yorktown Heights, NY 10598, U.S.A.
</note>
<email confidence="0.995527">
{jencc,siddharth,fanj}@us.ibm.com
</email>
<sectionHeader confidence="0.998579" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945888888889">
The last decade has seen many interesting ap-
plications of Question Answering (QA) tech-
nology. The Jeopardy! quiz show is certainly
one of the most fascinating, from the view-
points of both its broad domain and the com-
plexity of its language. In this paper, we study
kernel methods applied to syntactic/semantic
structures for accurate classification of Jeop-
ardy! definition questions. Our extensive em-
pirical analysis shows that our classification
models largely improve on classifiers based on
word-language models. Such classifiers are
also used in the state-of-the-art QA pipeline
constituting Watson, the IBM Jeopardy! sys-
tem. Our experiments measuring their impact
on Watson show enhancements in QA accu-
racy and a consequent increase in the amount
of money earned in game-based evaluation.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997520625">
Question Answering (QA) is an important research
area of Information Retrieval applications, which re-
quires the use of core NLP capabilities, such as syn-
tactic and semantic processing for a more effective
user experience. While the development of most
existing QA systems are driven by organized eval-
uation efforts such as TREC (Voorhees and Dang,
2006), CLEF (Giampiccolo et al., 2007), and NT-
CIR (Sasaki et al., 2007), there exist efforts that
leverage data from popular quiz shows, such as Who
Wants to be a Millionaire (Clarke et al., 2001; Lam
et al., 2003) and Jeopardy! (Ferrucci et al., 2010), to
demonstrate the generality of the technology.
Jeopardy! is a popular quiz show in the US which
has been on the air for 27 years. In each game, three
contestants compete for the opportunity to answer
60 questions in 12 categories of 5 questions each.
Jeopardy! questions cover an incredibly broad do-
main, from science, literature, history, to popular
culture. We are drawn to Jeopardy! as a test bed
for open-domain QA technology due to its broad do-
main, complex language, as well as the emphasis on
accuracy, confidence, and speed during game play.
While the vast majority of Jeopardy! questions
are factoid questions, we find several other types
of questions in the Jeopardy! data, which can ben-
efit from specialized processing in the QA system.
The additional processing in these questions com-
plements that of the factoid questions to achieve im-
proved overall QA performance. Among the various
types of questions handled by the system are defini-
tion questions shown in the examples below:
</bodyText>
<listItem confidence="0.9931935">
(1) GON TOMORROW: It can be the basket
below a hot-air balloon or a flat-bottomed
boat used on a canal (answer: gondola);
(2) I LOVE YOU, “MIN”: Overbearing (an-
swer: domineering);
(3) INVEST: From the Latin for “year”, it’s
an investment or retirement fund that pays
out yearly (answer: an annuity)
</listItem>
<bodyText confidence="0.98638">
where the upper case text indicates the Jeop-
ardy! category for each question1.
Several characteristics of this class of questions
warrant special processing: first, the clue (question)
</bodyText>
<footnote confidence="0.989079">
1A Jeopardy! category indicates a theme is common among
its 5 questions.
</footnote>
<page confidence="0.861618">
712
</page>
<note confidence="0.9583025">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 712–724,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999697468085106">
often aligns well with dictionary entries, making 2002; Shawe-Taylor and Cristianini, 2004; Mos-
dictionary resources potentially effective. Second, chitti, 2006). The extensive empirical analysis of
these clues often do not indicate an answer type, several advanced models shows that our best model,
which is an important feature for identifying cor- which combines different kernels, improves the F1
rect answers in factoid questions (in the examples of our baseline model by 67% relative, from 40.37
above, only (3) provided an answer type, “fund”). to 67.48. Surprisingly, with respect to previous find-
Third, definition questions are typically shorter in ings on standard QC, e.g. (Zhang and Lee, 2003;
length than the average factoid question. These dif- Moschitti, 2006), the Syntactic Tree Kernel (Collins
ferences, namely the shorter clue length and the lack and Duffy, 2002) is not effective whereas the ex-
of answer types, make the use of a specialized ma- ploitation of partial tree patterns proves to be es-
chine learning model potentially promising for im- sential. This is due to the different nature of Jeop-
proving the overall system accuracy. The first step ardy! questions, which are not expressed in the usual
for handling definitions is, of course, the automatic interrogative form.
separation of definitions from other question types, To demonstrate the benefit of our question clas-
which is not a simple task in the Jeopardy! domain. sifier, we integrated it into our Watson by coupling
For instance, consider the following example which it with search and candidate generation against spe-
is a variation of (3) above: cialized dictionary resources. We show that in end-
(4) INVEST: From the Latin for “year”, to-end evaluations, Watson with kernel-based defi-
an annuity is an investment or retirement nition classification and specialized definition ques-
fund that pays out this often (answer: tion processing achieves statistically significant im-
yearly) provement compared to our baseline systems.
Even though the clue is nearly identical to (3), the In the reminder of this paper, Section 2 describes
clue does not provide a definition for the answer Watson by focusing on the problem of definition
yearly, although at first glance we may have been question classification, Section 3 describes our mod-
misled. The source of complexity is given by the fact els for such classifiers, Section 4 presents our exper-
that Jeopardy! clues are not phrased in interrogative iments on QC, whereas Section 5 shows the final im-
form as questions typically are. This complicates the pact on Watson. Finally, Section 6 discusses related
design of definition classifiers since we cannot di- work and Section 7 derives the conclusions.
rectly use either typical structural patterns that char- 2 Watson: The IBM Jeopardy! System
acterize definition/description questions, or previous This section gives a quick overview of Watson and
approaches, e.g. (Ahn et al., 2004; Kaisser and Web- the problem of classification of definition questions,
ber, 2007; Blunsom et al., 2006). Given the com- which is the focus of this paper.
plexity and the novelty of the task, we found it use- 2.1 Overview
ful to exploit the kernel methods technology. This Watson is a massively parallel probabilistic
has shown state-of-the-art performance in Question evidence-based architecture for QA (Ferrucci et
Classification (QC), e.g. (Zhang and Lee, 2003; al., 2010). It consists of several major stages for
Suzuki et al., 2003; Moschitti et al., 2007) and it underlying sub-tasks, including analysis of the
is very well suited for engineering feature represen- question, retrieval of relevant content, scoring and
tations for novel tasks. ranking of candidate answers, as depicted in Figure
In this paper, we apply SVMs and kernel meth- 1. In the rest of this section, we provide an overview
ods to syntactic/semantic structures for modeling of Watson, focusing on the task of answering
accurate classification of Jeopardy! definition ques- definitional questions.
tions. For this purpose, we use several levels of lin- Question Analysis: The first stage of the pipeline,
guistic information: word and POS tag sequences, it applies several analytic components to identify
dependency, constituency and predicate argument key characteristics of the question (such as answer
structures and we combined them using state-of-
the-art structural kernels, e.g. (Collins and Duffy,
</bodyText>
<page confidence="0.520657">
713
</page>
<figureCaption confidence="0.99977">
Figure 1: Overview of Watson
</figureCaption>
<bodyText confidence="0.999734214285714">
type, question classes, etc.) used by later stages of
the Watson pipeline. Various general purpose NLP
components, such as a parser and named entity de-
tector, are combined with task-specific modules for
this analysis.
The task-specific analytics include several QC
components, which determine if the question be-
longs to one or more broad “question classes”.
These question classes can influence later stages of
the Watson pipeline. For instance, a question de-
tected as an abbreviation question can invoke spe-
cialized candidate generators to produce possible ex-
pansions of the abbreviated term in the clue. Simi-
larly, the question classes can impact the methods
for answer scoring and the machine learning mod-
els used for ranking candidate answers. The focus
of this paper is on the definition class, which is de-
scribed in the next section.
Hypothesis Generation: Following question anal-
ysis, the Watson pipeline searches its document col-
lection for relevant documents and passages that are
likely to contain the correct answer to the question.
This stage of the pipeline generates search queries
based on question analysis results, and obtains a
ranked list of documents and passages most relevant
to the search queries. A variety of candidate gen-
eration techniques are then applied to the retrieved
results to produce a set of candidate answers.
Information obtained from question analysis can
be used to influence the search and candidate gener-
ation processes. The question classes detected dur-
ing question analysis can focus the search towards
specific subsets of the corpus. Similarly, during can-
didate generation, strategies used to generate the set
of candidate answers are selected based on the de-
tected question classes.
Hypothesis and Evidence Scoring: A wide variety
of answer scorers are then used to gather evidence
supporting each candidate answer as the correct an-
swer to the given question. The scorers include both
context dependent as well as context independent
scorers, relying on various structured and unstruc-
tured resources for their supporting evidence.
Candidate Ranking: Finally, machine learning
models are used to weigh the gathered evidence and
rank the candidate answers. The models generate a
ranked list of answers each with an associated con-
fidence. The system can also choose to refrain from
answering a question if it has low confidence in all
candidates. This stage of the pipeline employs sev-
eral machine learning models specially trained to
handle various types of questions. These models are
trained using selected feature sets based on question
classes and candidate answers are “routed” to the
appropriate model according to the question classes
detected during question analysis.
</bodyText>
<subsectionHeader confidence="0.999684">
2.2 Answering Definition Questions
</subsectionHeader>
<bodyText confidence="0.999740272727273">
Among the many question classes that Watson iden-
tifies and leverages for special processing, of partic-
ular interest for this paper is the class we refer to
as definition questions. These are questions whose
clue texts contain one or more definitions of the cor-
rect answer. For instance, in example (3), the main
clause in the question corresponds to a dictionary
definition of the correct answer (annuity). Looking
up this definition in dictionary resources could en-
able us to answer this question correctly and with
high confidence. This suggests that special process-
</bodyText>
<page confidence="0.996481">
714
</page>
<bodyText confidence="0.99991792">
ing of such definition questions could allow us to
hone in on the correct answer through processes dif-
ferent from those used for other types of questions.
This paper explores strategies for definition ques-
tion processing to improve overall question answer-
ing performance. A key challenge we have to ad-
dress is that of accurate recognition of such ques-
tions. Given an input question the Watson question
analysis stage uses a definition question recognizer
to detect this specific class of questions. We explore
several approaches for recognition, including a rule
based approach and a variety of statistical models.
Questions that are recognized as definition ques-
tions invoke search processes targeted towards
dictionary-like sources in our system. We use a va-
riety of such sources, such as standard English dic-
tionaries, Wiktionary, WordNet, etc. After gather-
ing supporting evidence for candidate answers ex-
tracted from these sources, our system routes the
candidates to definition-specific candidate ranking
models, which have been trained with selected fea-
ture sets.
The following sections present a description and
evaluation of our approach for identifying and an-
swering definition questions.
</bodyText>
<sectionHeader confidence="0.980116" genericHeader="method">
3 Kernel Models for Question
Classification
</sectionHeader>
<bodyText confidence="0.999462833333334">
Previous work (Zhang and Lee, 2003; Suzuki et al.,
2003; Blunsom et al., 2006; Moschitti et al., 2007)
as shown that syntactic structures are essential for
QC. Given the novelty of both the domain and the
type of our classification items, we rely on kernel
methods to study and design effective representa-
tions. Indeed, these are excellent tools for auto-
matic feature engineering, especially for unknown
tasks and domains. Our approach consists of using
SVMs and kernels for structured data applied to sev-
eral types of structural lexical, syntactic and shallow
semantic information.
</bodyText>
<subsectionHeader confidence="0.998804">
3.1 Tree and Sequence Kernels
</subsectionHeader>
<bodyText confidence="0.999927">
Kernel functions are implicit scalar products be-
tween data examples (i.e. questions in our case)
in the very high dimensional space of substructures,
where each of the latter is a component of the im-
plicit vectors associated with the examples.
</bodyText>
<figure confidence="0.99898808">
JJte
in
r hit a
PIN ct
off
P nc
DT P
NN
NN
NNSmea
MP
by
this
0
form A
energy
PR
electrons
PR A
electromagnetic
A2 PR
ROOT
SBARQ
S T
K NP
VP
,
VP
DT
VBZ
PRP
NP
NP
PP
NN
,
PP
NP
phosphor
RP
NP
VBN
gives
C PA
IN
a
negative mi
WHADVP
WRB
When
</figure>
<figureCaption confidence="0.890359">
Figure 2: Constituency Tree
</figureCaption>
<figure confidence="0.998678842105263">
ROOT
VBZ
TMP
al gr
VBN
gives
PRT
RP
OBJ
NN
usP
,
SBJ
NNDT
LGS
IN
PMOD N
medNNS b
electrons
</figure>
<figureCaption confidence="0.876973">
Figure 3: Dependency Tree
</figureCaption>
<figure confidence="0.9995979">
NMOD
IN
in PMOD
TMP
WRB
DT when
hit
JJby
off NMOD energy
JJ
electromag.
NMOD phosphor
a playin
DT
aN
NN
NMOD form
DT
this
PASS
</figure>
<figureCaption confidence="0.999558">
Figure 4: A tree encoding a Predicate Argument Structure Set
</figureCaption>
<bodyText confidence="0.9486815">
Although several kernels for structured data have
been developed (see Section 6), the main distinc-
</bodyText>
<figure confidence="0.3387952">
1
tions in terms of feature spaces is given by the fol-
lowing three different kernels:
• Sequence Kernels (SK); we implemented the
1
</figure>
<bodyText confidence="0.999167">
discontinuous string kernels described in (Shawe-
Taylor and Cristianini, 2004). This allows for rep-
resenting a string of symbols in terms of its possi-
ble substrings with gaps, i.e. an arbitrary number of
symbols can be skipped during the generation of a
substring. The symbols we used in the sequential de-
scriptions of questions are words and part-of-speech
tags (in two separate sequences). Consequently, all
possible multiwords with gaps are features of the im-
plicitly generated vector space.
</bodyText>
<figure confidence="0.9899615">
P
P
P
A0
A1
A1
AM-TMP
A1
PR
A0
PR
PR
AM-MNR
phosphor hit give energy phosphor electromag. energy hit electron phosphor
</figure>
<page confidence="0.903854">
715
</page>
<listItem confidence="0.874840333333333">
• Syntactic Tree Kernel (STK) (Collins and Duffy, Figure 2 and dependency structures converted into
2002) applied to constituency parse trees. This gen- the dependency trees (DTs), e.g. shown in Figure
erates all possible tree fragments as features with 3. Note that, the POS-tags are central nodes, the
</listItem>
<bodyText confidence="0.896513357142857">
the conditions that sibling nodes from the original grammatical relation label is added as a father
trees cannot be separated. In other words, substruc- node and all the relations with the other nodes are
tures are composed by atomic building blocks cor- described by means of the connecting edges. Words
responding to nodes along with all their direct chil- are considered additional children of the POS-tag
dren. These, in case of a syntactic parse tree, are nodes (in this case the connecting edge just serves
complete production rules of the associated parser to add a lexical feature to the target POS-tag node).
grammar2. Finally, we also use predicate argument structures
• Partial Tree Kernel (PTK) (Moschitti, 2006) ap- generated by verbal and nominal relations accord-
plied to both constituency and dependency parse ing to PropBank (Palmer et al., 2005) and NomBank
trees. This generates all possible tree fragments, as (Meyers et al., 2004). Given the target sentence, the
above, but sibling nodes can be separated (so they set of its predicates are extracted and converted into
can be part of different tree fragments). In other a forest, then a fake root node, PAS, is used to con-
words, a fragment is any possible tree path, from nect these trees. For example, Figure 4 illustrates a
whose nodes other tree paths can depart. Conse- Predicate Argument Structures Set (PASS) encoding
quently, an extremely rich feature space is gener- two relations, give and hit, as well as the nominaliza-
ated. Of course, PTK subsumes STK but sometimes tion energy along with all their arguments.
the latter provides more effective solutions as the 4 Experiments on Definition Question
number of irrelevant features is smaller as well. Classification
When applied to sequences and tree structures, the In these experiments, we study the role of kernel
kernels discussed above produce many different technology for the design of accurate classification
kinds of features. Therefore, the design of appro- of definition questions. We build several classifiers
priate syntactic/semantic structures determines the based on SVMs and kernel methods. Each classi-
representational power of the kernels. Hereafter, we fier uses advanced syntactic/semantic structural fea-
show the models we used. tures and their combination. We carry out an exten-
3.2 Syntactic Semantic Structures sive comparison in terms of F1 between the different
We applied the above kernels to different structures. models on the Jeopardy! datasets.
These can be divided in sequences of words (WS) 4.1 Experimental Setup
and part of speech tags (PS) and different kinds of Corpus: the data for our QC experiments consists
trees. For example, given the non-definition Jeop- of a randomly selected set of 33 Jeopardy! games3.
ardy! question: These questions were manually annotated based on
(5) GENERAL SCIENCE: When hit by elec- whether or not they are considered definitional. This
trons, a phosphor gives off electromag- resulted in 306 definition and 4964 non-definition
netic energy in this form. (answer: light clues. Each test set is stored in a separate file con-
or photons), sisting of one line per question, which contains tab-
we use the following sequences: separated clue information and the Jeopardy! cate-
WS: [when][hit][by][electrons][,][a][phosphor][gives] gory, e.g. INVEST in example (4).
[off][electromagnetic][energy][in][this][form] Tools: for SVM learning, we used the SVMLight-
PS: [wrb][vbn][in][nns][,][dt][nn][vbz][rp][jj][nn][in] TK software4, which includes structural kernels in
[dt][nn] SVMLight (Joachims, 1999)5. For generating con-
Additionally, we use constituency trees (CTs), see
3Past Jeopardy! games can be downloaded from
http://www.j-archive.com.
</bodyText>
<footnote confidence="0.959905666666667">
4Available at http://dit.unitn.it/∼moschitt
5http://svmlight.joachims.org
2From here the name syntactic tree kernels
</footnote>
<page confidence="0.505011">
716
</page>
<bodyText confidence="0.999898375">
stituency trees, we used the Charniak parser (Char-
niak, 2000). We also used the syntactic–semantic
parser by Johansson and Nugues (2008) to gener-
ate dependency trees (Mel’ˇcuk, 1988) and predicate
argument trees according to the PropBank (Palmer
et al., 2005) and NomBank (Meyers et al., 2004)
frameworks.
Baseline Model: the first model that we used as a
baseline is a rule-based classifier (RBC). The RBC
leverages a set of rules that matches against lexical
and syntactic information in the clue to make a bi-
nary decision on whether or not the clue is consid-
ered definitional. The rule set was manually devel-
oped by a human expert, and consists of rules that
attempt to identify roughly 70 different constructs
in the clues. For instance, one of the rules matches
the parse tree structure for ”It’s X or Y”, which will
identify example (1) as a definition question.
Kernel Models: we apply the kernels described
in Section 3 to the structures extracted from Jeop-
ardy! clues. In particular, we design the following
models: BOW, i.e. linear kernel on bag-of-words
from the clues; WSK, PSK and CSK, i.e. SK applied
to the word and POS-tag sequences from the clues,
and the word sequence taken from the question cat-
egories, respectively; STK-CT, i.e. STK applied to
CTs of the clue; PTK-CT and PTK-DT, i.e. PTK
applied to CTs and DTs of the clues, respectively;
PASS, i.e. PTK applied to the Predicate Argument
Structure Set extracted from the clues; and RBC, i.e.
a linear kernel applied to the vector only constituted
by the 1/0 output of RBC.
Learning Setting: there is no particular parameteri-
zation. Since there is an imbalance between positive
and negative examples, we used a Precision/Recall
trade-off parameter in SVM-Light-TK equal to 5.6
Measures: the performance is measured with Pre-
cision, Recall and F1-measure. We estimated them
by means of Leave-One-Out7 (LOO) on the question
set.
</bodyText>
<subsectionHeader confidence="0.459767">
4.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.807528">
Table 1 shows the performance obtained using dif-
ferent kernels (feature spaces) with SVMs. We note
</bodyText>
<footnote confidence="0.9339868">
6We have selected 5 as a reasonable value, which kept bal-
anced Precision and Recall on a validation set.
7LOO applied to a corpus of N instances consists in training
on N − 1 examples and testing on the single held-out example.
This process is repeated for all instances.
</footnote>
<table confidence="0.9999255">
Kernel Space Prec. Rec. F1
RBC 28.27 70.59 40.38
BOW 47.67 46.73 47.20
WSK 47.11 50.65 48.82
STK-CT 50.51 32.35 39.44
PTK-CT 47.84 57.84 52.37
PTK-DT 44.81 57.84 50.50
PASS 33.50 21.90 26.49
PSK 39.88 45.10 42.33
CSK 39.07 77.12 51.86
</table>
<tableCaption confidence="0.9935025">
Table 1: Kernel performance using leave-one-out cross-
validation.
</tableCaption>
<bodyText confidence="0.999082424242424">
that: first, RBC has good Recall but poor Precision.
This is interesting since, on one hand, these results
validate the complexity of the task: in order to cap-
ture the large variability of the positive examples,
the rules developed by a skilled human designer are
unable to be sufficiently precise to limit the recog-
nition to those examples. On the other hand, RBC,
being a rather different approach from SVMs, can be
successfully exploited in a joint model with them.
Second, BOW yields better F1 than RBC but it
does not generalize well since its F1 is still low.
When n-grams are also added to the model by
means of WSK, the F1 improves by about 1.5 ab-
solute points. As already shown in (Zhang and Lee,
2003; Moschitti et al., 2007), syntactic structures are
needed to improve generalization.
Third, surprisingly with respect to previous work,
STK applied to CT8 provides accuracy lower than
BOW, about 8 absolute points. The reason is due to
the different nature of the Jeopardy! questions: large
syntactic variability reduces the probability of find-
ing general and well formed patterns, i.e. structures
generated by entire production rules. This suggests
that PTK, which can capture patterns derived from
partial production rules, can be more effective. In-
deed, PTK-CT achieves the highest F1, outperform-
ing WSK also when used with a different syntactic
paradigm, i.e. PTK-DT.
Next, PSK and PASS provide a lower accuracy
but they may be useful in kernel combinations as
they can complement the information captured by
the other models. Interestingly, CSK alone is rather
effective for classifying definition questions. We be-
</bodyText>
<footnote confidence="0.9421545">
8Applying it to DT does not make much sense as already
pointed out in (Moschitti, 2006).
</footnote>
<page confidence="0.980941">
717
</page>
<figureCaption confidence="0.999473">
Figure 5: Similarity according to PTK and STK
</figureCaption>
<bodyText confidence="0.999806344827586">
lieve this is because definition questions are some-
times clustered into categories such as 4-LETTER
WORDS or BEGINS WITH ”B”.
Moreover, we carried out qualitative error analy-
sis on the PTK and STK outcome, which supported
our initial hypothesis. Let us consider the bottom
tree in Figure 5 in the training set. The top tree is
a test example correctly classified by PTK but in-
correctly classified by STK. The dashed line in the
top tree contains the largest subtree matched by PTK
(against the bottom tree), whereas the dashed line in
the bottom tree indicates the largest subtree matched
by STK (against the top tree). As the figure shows,
PTK can exploit a larger number of partial patterns.
Finally, the above points suggest that different
kernels produce complementary information. It is
thus promising to experiment with their combina-
tions. The joint models can be simply built by
summing kernel functions together. The results are
shown in Table 2. We note that: (i) CSK comple-
ments the WSK information, achieving a substan-
tially better result, i.e. 62.95; (ii) PTK-CT+CSK
performs even better than WSK+CSK (as PTK out-
performs WSK); and (iii) adding RBC improves
further on the above combinations, i.e. 68.11 and
67.32, respectively. This evidently demonstrates
that RBC captures complementary information. Fi-
nally, more complex kernels, especially the overall
kernel summation, do not seem to improve the per-
</bodyText>
<table confidence="0.999928818181818">
Kernel Space Prec. Rec. F1
WSK+CSK 70.00 57.19 62.95
PTK-CT+CSK 69.43 60.13 64.45
PTK-CT+WSK+CSK 68.59 62.09 65.18
CSK+RBC 47.80 74.51 58.23
PTK-CT+CSK+RBC 59.33 74.84 65.79
BOW+CSK+RBC 60.65 73.53 66.47
PTK-CT+WSK+CSK+RBC 67.66 66.99 67.32
PTK-CT+PASS+CSK+RBC 62.46 71.24 66.56
WSK+CSK+RBC 69.26 66.99 68.11
ALL 61.42 67.65 64.38
</table>
<tableCaption confidence="0.997012">
Table 2: Performance of Kernel Combinations using
leave-one-out cross-validation.
</tableCaption>
<bodyText confidence="0.998483666666667">
formance. This is also confirmed by the PASS re-
sults derived in (Moschitti et al., 2007) on TREC
QC.
</bodyText>
<sectionHeader confidence="0.975815" genericHeader="method">
5 Experiments on the Jeopardy System
</sectionHeader>
<bodyText confidence="0.9998545">
Since the kernel-based classifiers perform substan-
tially better than RBC, we incorporate the PTK-
CT+WSK+CSK model9 into Watson for definition
classification and evaluated the QA performance
against two baseline systems. For the end-to-end ex-
periments, we used Watson’s English Slot Grammar
parser (McCord, 1980) to generate the constituency
trees. The component level evaluation shows that
we achieved comparable performance as previously
discussed with ESG.
</bodyText>
<subsectionHeader confidence="0.952196">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999991285714286">
We integrated the classifier into the question analy-
sis module, and incorporated additional components
to search against dictionary resources and extract
candidate answers from these search results when a
question is classified as definitional. In the final ma-
chine learning models, a separate model is trained
for definition questions to enable scoring tailored to
the specific characteristics of those questions.
Based on our manually annotated gold standard,
less than 10% of Jeopardy! questions are classified
as definition questions. Due to their relatively low
frequency we conduct two types of evaluations. The
first is definition-only evaluation, in which we apply
our definition question classifier to identify a large
</bodyText>
<footnote confidence="0.998881333333333">
9Since we aim to compare a purely statistical approach to
the rule-based approach, we did not experiment with the model
that uses RBC as a feature in our end-to-end experiments.
</footnote>
<page confidence="0.994186">
718
</page>
<bodyText confidence="0.9750690625">
set of definition questions and evaluate the end-to-
end system’s performance on this large set of ques-
tions. These results enable us to draw statistically
significant conclusions about our approach to ad-
dressing definition questions.
The second type of evaluation is game-based
evaluation, which assesses the impact of our defi-
nition question processing on Watson performance
while preserving the natural distribution of these
question types in Jeopardy! data. Game-based eval-
uations situate the system’s performance on defini-
tion questions relative to other types of questions,
and enable us to gauge the component’s contribu-
tions in a game-based setting.
For both evaluation settings, three configurations
of Watson are used as follows:
</bodyText>
<listItem confidence="0.9804832">
• the NoDef system, in which Watson is config-
ured without definition classification and pro-
cessing, thereby treating all definition ques-
tions as regular factoid questions;
• the StatDef system, which leverages the sta-
tistical classifier and subsequent definition spe-
cific search and candidate generation compo-
nents as described above; and
• the RuleDef system, in which Watson adopts
RBC and employs the same additional defini-
</listItem>
<bodyText confidence="0.981457315789474">
tion search and candidate generation compo-
nents as the StatDef system.
For the definition-only evaluation, we selected all
questions recognized as definitional by the statistical
classifier from roughly 1000 unseen games (60000
questions), resulting in a test set of 1606 questions.
Due to the size of the initial set, it is impractical to
manually create a gold standard for measuring Pre-
cision and Recall of the classifier. Instead, we com-
pare the StatDef system against the NoDef on these
1606 questions using two metrics: accuracy, defined
as the percentage of questions correctly answered,
and p@70, the system’s Precision when answering
only the top 70% most confident questions. P@70 is
an important metric in Jeopardy! game play as well
as in real world applications where the system may
refrain from answering a question when it is not con-
fident about any of its answers. Since RBC identifies
significantly more definition questions, we started
</bodyText>
<table confidence="0.99965525">
NoDef StatDef NoDef RuleDef
# Questions 1606 1606 1875 1875
Accuracy 63.76% 65.57% 56.64% 57.51%
P@70 82.22% 84.53% 72.73% 74.87%
</table>
<tableCaption confidence="0.998165">
Table 3: Definition-Only Evaluation Results
</tableCaption>
<bodyText confidence="0.999876384615385">
with an initial set of roughly 300 games, from which
the RBC identified 1875 questions as definitional.
We compared the RuleDef system’s performance on
these questions against the NoDef baseline using the
accuracy and p@70 metrics.
For the game-based evaluation, we randomly se-
lected 66 unseen Jeopardy! games, consisting of
3546 questions after excluding audio/visual ques-
tions.10 We contrast the StatDef system perfor-
mance against that of NoDef and RuleDef along
several dimensions: accuracy and p@70, described
above, as well as earnings, the average amount of
money earned for each game.
</bodyText>
<subsectionHeader confidence="0.984408">
5.2 Definition-Only Evaluation
</subsectionHeader>
<bodyText confidence="0.999982">
For the definition-only evaluation, we compared the
StatDef system against the NoDef system on a set of
1606 questions that the StatDef system classified as
definitional. The results are shown in the first two
columns in Table 3. To contrast the gain obtained
by the StatDef system against that achieved by the
RuleDef system, we ran the RuleDef system over
the 1875 questions identified as definitional by the
rule-based classifier. We contrast the RuleDef sys-
tem performance with that of the NoDef system, as
shown in the last two columns in Table 3.
Our results show that based on both evaluation
metrics, StatDef improved upon the NoDef baseline
more than RuleDef improved on the same baseline
system. Furthermore, for the accuracy metric where
all samples are paired and independent, the differ-
ence in performance between the StatDef and NoDef
systems is statistically significant at p&lt;0.05, while
that between the RuleDef and NoDef systems is not.
</bodyText>
<subsectionHeader confidence="0.960926">
5.3 Game-Based Evaluation
</subsectionHeader>
<bodyText confidence="0.9957424">
The game-based evaluation was carried out on 66
unseen games (roughly 3500 questions). Of these
10Audio/visual questions are those accompanied by either an
image or an audio clip. The text portions of these questions are
often insufficient for identifying the correct answers.
</bodyText>
<page confidence="0.99674">
719
</page>
<table confidence="0.999723">
# Def Q’s Accuracy P@70 Earnings
NoDef 0 69.71% 86.79% $24,818
RuleDef 480 69.23% 86.31% $24,397
StatDef 131 69.85% 87.19% $25,109
</table>
<tableCaption confidence="0.999217">
Table 4: Game-Based Evaluation Results
</tableCaption>
<bodyText confidence="0.999884909090909">
questions, the StatDef system classified 131 of them
as definitional while the RuleDef system identified
480 definition questions. Both systems were com-
pared against the NoDef system using the accuracy,
p@70, and earnings metric computed over all ques-
tions, as shown in Table 4.
Our results show that even though in the
definition-only evaluation both the RuleDef and
StatDef systems outperformed the NoDef baseline,
in our game-based evaluation, the RuleDef system
performed worse than the NoDef baseline. The low-
ered performance is due to the fact that the Preci-
sion of the RBC is much lower than that of the sta-
tistical classifier, and the special definition process-
ing applied to questions that are erroneously clas-
sified as definitional was harmful. Our evaluation
of this false positive set showed that its accuracy
dropped by 6% compared to the NoDef system. On
the other hand, the StatDef system outperformed the
two other systems, and its accuracy improvement
upon the RuleDef system is statistically significant
at p&lt;0.05.
</bodyText>
<sectionHeader confidence="0.99994" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999980052631579">
Our paper studies the use of advanced representa-
tion for QC in the Jeopardy! domain. As previously
mentioned Jeopardy! questions are stated as affir-
mative sentences, which are different from the typ-
ical QA questions. For the design of our models,
we have carefully taken into account previous work.
This shows that semantics and syntax are essential
to retrieve precise answers, e.g (Hickl et al., 2006;
Voorhees, 2004; Small et al., 2004).
We focus on definition questions, which typically
require more complex processing than factoid ques-
tions (Blair-Goldensohn et al., 2004; Chen et al.,
2006; Shen and Lapata, 2007; Bilotti et al., 2007;
Moschitti et al., 2007; Surdeanu et al., 2008; Echi-
habi and Marcu, 2003). For example, language mod-
els were applied to definitional QA in (Cui et al.,
2005) to learn soft pattern models based on bigrams.
Other related work, such as (Sasaki, 2005; Suzuki
et al., 2002), was also very tied to bag-of-words
features. Predicate argument structures have been
mainly used for reranking (Shen and Lapata, 2007;
Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu
et al., 2008).
Our work and methods are similar to (Zhang and
Lee, 2003; Moschitti et al., 2007), which achieved
the state-of-the-art in QC by applying SVMs along
with STK-CT. The results were derived by experi-
menting with a TREC dataset11(Li and Roth, 2002),
reaching an accuracy of 91.8%. However, such data
refers to typical instances from QA, whose syntactic
patterns can be easily generalized by STK. In con-
trast, we have shown that STK-CT is not effective
for our domain, as it presents very innovative ele-
ments: questions in affirmative and highly variable
format. Thus, we employed new methods such as
PTK, dependency structures, multiple sequence ker-
nels including category information and many com-
binations.
Regarding the use of Kernel Methods, there is
a considerably large body of work in Natural Lan-
guage Processing, e.g. regarding syntactic parsing
(Collins and Duffy, 2002; Kudo et al., 2005; Shen
et al., 2003; Kudo and Matsumoto, 2003; Titov and
Henderson, 2006; Toutanova et al., 2004), named
entity recognition and chunking (Cumby and Roth,
2003; Daum´e III and Marcu, 2004), relation extrac-
tion (Zelenko et al., 2002; Culotta and Sorensen,
2004; Bunescu and Mooney, 2005; Zhang et al.,
2005; Bunescu, 2007; Nguyen et al., 2009a), text
categorization (Cancedda et al., 2003), word sense
disambiguation (Gliozzo et al., 2005) and seman-
tic role labeling (SRL), e.g. (Kazama and Torisawa,
2005; Che et al., 2006a; Moschitti et al., 2008).
However, ours is the first study on the use of sev-
eral combinations of kernels applied to several struc-
tures on very complex data from the Jeopardy! do-
main.
</bodyText>
<sectionHeader confidence="0.875483" genericHeader="method">
7 Final Remarks and Conclusion
</sectionHeader>
<bodyText confidence="0.9939904">
In this paper we have experimented with advanced
structural kernels applied to several kinds of syntac-
tic/semantic linguistic structures for the classifica-
tion of questions in a new application domain, i.e.
Jeopardy!. Our findings are summarized hereafter:
</bodyText>
<footnote confidence="0.962654">
11Available at http://cogcomp.cs.illinois.
edu/Data/QA/QC/
</footnote>
<page confidence="0.994385">
720
</page>
<bodyText confidence="0.965692057471265">
First, it should be noted that basic kernels, such
as STK, PTK and SK, when applied to new repre-
sentations, i.e. syntactic/semantic structures, con-
stitute new kernels. Thus structural representations
play a major role and, from this perspective, our pa-
per makes a significant contribution.
Second, the experimental results show that the
higher variability of Jeopardy! questions prevents us
from achieving generalization with typical syntactic
patterns even if they are derived by powerful meth-
ods such as STK. In contrast, partial patterns, such
as those provided by PTK applied to constituency
(or dependency) trees, prove to be effective.
In particular, STK has been considered as the best
kernel for exploiting syntactic information in con-
stituency trees, e.g. it is state-of-the-art in: QC
(Zhang and Lee, 2003; Moschitti et al., 2007; Mos-
chitti, 2008); SRL, (Moschitti et al., 2008; Mos-
chitti et al., 2005; Che et al., 2006b); pronominal
coreference resolution (Yang et al., 2006; Versley
et al., 2008) and Relation Extraction (Zhang et al.,
2006; Nguyen et al., 2009b). We showed that, in
the complex domain of Jeopardy!, STK surprisingly
provides low accuracy whereas PTK is rather ef-
fective and greatly outperforms STK. We have also
provided an explanation of such behavior by means
of error analysis: in contrast with traditional ques-
tion classification, which focuses on basic syntactic
patterns (e.g. ”what”, ”where”, ”who” and ”how”).
Figure 5 shows that PTK captures partial patterns
that are important for more complex questions like
those in Jeopardy!
Third, we derived other interesting findings for
NLP related to this novel domain, e.g.: (i) the im-
pact of dependency trees is similar to the one of
constituency trees. (ii) A simple computational rep-
resentation of shallow semantics, i.e. PASS (Mos-
chitti, 2008), does not work in Jeopardy!. (iii) Se-
quence kernels on category cues, i.e., higher level of
lexical semantics, improve question classification.
(iv) RBC jointly used with statistical approaches is
helpful to tackle the Jeopardy! complexity.
Next, our kernel models improve up to 20 abso-
lute percent points over n-grams based approaches,
reaching a significant accuracy of about 70%. Wat-
son, exploiting such a classifier, improved previ-
ous versions using RBC and no definition classifica-
tion both in definition-only evaluations and in game-
based evaluations.
Finally, we point out that:
• Jeopardy! has a variety of different special ques-
tion types that are handled differently. We focus on
kernel methods for definition question for two rea-
sons. First, their recognition relies heavily on parse
structures and is therefore more amenable to the ap-
proach proposed in the paper than the recognition
of other question types. Second, definition is by far
the most frequent special question type in Jeopardy!;
therefore, we can obtain sufficient data for training
and testing.
• We were unable to address the whole QC prob-
lem using a statistical model due to the lack of suffi-
cient training data for most special question classes.
Furthermore, we focused only on the definition clas-
sification and its impact on system performance due
to space reasons.
• Our RBC has a rather imbalanced trade-off be-
tween Precision and Recall. This may not be the
best operating point, but the optimal point is diffi-
cult to obtain empirically for an RBC, which is a
strong motivation of the work in this paper. We ex-
perimented with tuning the trade-off between Preci-
sion and Recall with the RBC, but since RBC uses
hand-crafted rules and does not have a parameter for
that, ultimately the statistical approach proved more
effective.
In future work, we plan to extend the current re-
search by investigating models capable of exploit-
ing predicate argument structures for question clas-
sification and answer reranking. The use of syntac-
tic/semantic kernels is a promising research direc-
tion (Basili et al., 2005; Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b). In this
perspective kernel learning is a very interesting re-
search line, considering the complexity of represen-
tation and classification problems in which our ker-
nels operate.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998246">
This work has been supported by the IBM’s Open
Collaboration Research (OCR) awards program. We
are deeply in debt with Richard Johansson, who pro-
duced the earlier syntactic/semantic representations
of the Jeopardy! questions from the text format.
</bodyText>
<page confidence="0.996163">
721
</page>
<sectionHeader confidence="0.99613" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999634971698114">
Kisuh Ahn, Johan Bos, Stephen Clark, James R. Cur-
ran, Tiphaine Dalmas, Jochen L. Leidner, Matthew B.
Smillie, and Bonnie Webber. 2004. Question an-
swering with qed and wee at trec-2004. In E. M.
Voorhees and L. P. Buckland, editors, The Thirteenth
Text REtrieval Conference, TREC 2004, pages 595–
599, Gaitersburg, MD.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet semantics
via kernel-based learning. In Proceedings of CoNLL-
2005, pages 1–8, Ann Arbor, Michigan. Association
for Computational Linguistics.
M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured retrieval for question answering. In Pro-
ceedings of ACM SIGIR.
S. Blair-Goldensohn, K. R. McKeown, and A. H.
Schlaikjer. 2004. Answering definitional questions:
A hybrid approach. In M. Maybury, editor, Proceed-
ings of AAAI 2004. AAAI Press.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Proceedings of ECIR 2007, Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels. In
In Proceedings of CIKM ’07.
Phil Blunsom, Krystle Kocik, and James R. Curran.
2006. Question classification with log-linear models.
In SIGIR ’06: Proceedings of the 29th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 615–616, New
York, NY, USA. ACM.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724–731,
Vancouver, British Columbia, Canada, October.
Razvan C. Bunescu. 2007. Learning to extract relations
from the web using minimal supervision. In Proceed-
ings of ACL.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059–1082.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006a. A hybrid convolution tree kernel for seman-
tic role labeling. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 73–
80, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006b. A hybrid convolution tree kernel for semantic
role labeling. In Proceedings of the COLING/ACL on
Main conference poster sessions, COLING-ACL ’06,
pages 73–80, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Y. Chen, M. Zhou, and S. Wang. 2006. Reranking an-
swers from definitional QA using language models. In
Proceedings of ACL.
Charles Clarke, Gordon Cormack, and Thomas Lynam.
2001. Exploiting redundancy in question answering.
In Proceedings of the 24th SIGIR Conference, pages
358–365.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL’02.
H. Cui, M. Kan, and T. Chua. 2005. Generic soft pattern
models for definitional QA. In Proceedings of SIGIR,
Salvador, Brazil. ACM.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, pages 423–429, Barcelona, Spain, July.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daum´e III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP’04.
A. Echihabi and D. Marcu. 2003. A noisy-channel ap-
proach to question answering. In Proceedings of ACL.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. 2010. Building watson:
An overview of the deepqa project. AI Magazine,
31(3).
Danilo Giampiccolo, Pamela Froner, Anselmo Pe˜nas,
Christelle Ayache, Dan Cristea, Valentin Jijkoun,
Petya Osenova, Paulo Rocha, Bogdan Sacaleanu, and
Richard Suteliffe. 2007. Overview of the CLEF 2007
multilingual question anwering track. In Proceedings
of the Cross Language Evaluation Forum.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL’05, pages 403–410.
A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, and
B. Rink. 2006. Question answering with lcc chaucer
at trec 2006. In Proceedings of TREC.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in Kernel Methods – Sup-
port Vector Learning, 13.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic–semantic analysis
with PropBank and NomBank. In CoNLL 2008:
Proceedings of the Twelfth Conference on Natural
Language Learning, pages 183–187, Manchester,
United Kingdom.
</reference>
<page confidence="0.967565">
722
</page>
<reference confidence="0.999877140186916">
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In Proceedings of
the Workshop on Deep Linguistic Processing, DeepLP
’07, pages 41–48, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jun’ichi Kazama and Kentaro Torisawa. 2005. Speeding
up training with tree kernels for node relation labeling.
In Proceedings of HLT-EMNLP’05.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL’03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL’05.
Shyong Lam, David Pennock, Dan Cosley, and Steve
Lawrence. 2003. 1 billion pages = 1 milllion dollars?
mining the web to pay ”who wants to be a millionaire?
In Proceedings of the 19th Conference on Uncertainty
in AI.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of ACL.
Michael C. McCord. 1980. Slot grammars. Computa-
tional Linguistics.
Igor A. Mel’ˇcuk. 1988. Dependency Syntax: Theory and
Practice. State University Press of New York, Albany.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 24–31, Boston, United
States.
Alessandro Moschitti, Bonaventura Coppola, Ana-Maria
Giuglea, and Roberto Basili. 2005. Hierarchical se-
mantic role labeling. In CoNLL 2005 shared task.
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,
and Suresh Manandhar. 2007. Exploiting syntactic
and shallow semantic kernels for question/answer clas-
sification. In Proceedings of ACL’07.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193–224.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML’06, pages 318–329.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceeding of CIKM ’08, NY, USA.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009a. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In Proceedings of EMNLP, pages
1378–1387, Singapore, August.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009b. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In EMNLP ’09: Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1378–1387, Morristown,
NJ, USA. Association for Computational Linguistics.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71–105.
Yutaka Sasaki, Chuan-Jie Lin, Kuang-hua Chen, and
Hsin-Hsi Chen. 2007. Overview of the NTCIR-6
cross-lingual question answering (CLQA) task. In
Proceedings of the 6th NTCIR Workshop on Evalua-
tion of Information Access Technologies.
Y. Sasaki. 2005. Question answering as question-biased
term extraction: A new approach toward multilingual
qa. In Proceedings of ACL, pages 215–222.
John Shawe-Taylor and Nello Cristianini. 2004. LaTeX
User’s Guide and Document Reference Manual. Ker-
nel Methods for Pattern Analysis, Cambridge Univer-
sity Press.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL.
L. Shen, A. Sarkar, and A. Joshi. 2003. Using LTAG
Based Features in Parse Reranking. In Proceedings of
EMNLP, Sapporo, Japan.
S. Small, T. Strzalkowski, T. Liu, S. Ryan, R. Salkin,
N. Shimizu, P. Kantor, D. Kelly, and N. Wacholder.
2004. Hitiqa: Towards analytical question answering.
In Proceedings of COLING.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of ACL-HLT, Columbus, Ohio.
J. Suzuki, Y. Sasaki, and E. Maeda. 2002. Svm an-
swer selection for open-domain question answering.
In Proceedings of Coling, pages 974–980.
Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku
Maeda. 2003. Question classification using hdag ker-
nel. In Proceedings of the ACL 2003 Workshop on
Multilingual Summarization and Question Answering,
pages 61–68, Sapporo, Japan, July. Association for
Computational Linguistics.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference sys-
tems based on kernels methods. In The 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing’08), Manchester, England.
</reference>
<page confidence="0.985368">
723
</page>
<reference confidence="0.996359653846154">
Ellen M. Voorhees and Hoa Trang Dang. 2006.
Overview of the TREC 2005 question answering track.
In Proceedings of the TREC 2005 Conference.
E. M. Voorhees. 2004. Overview of the trec 2004 ques-
tion answering track. In Proceedings of TREC 2004.
Xiaofeng Yang, Jian Su, and Chewlim Tan. 2006.
Kernel-based pronoun resolution with structured syn-
tactic knowledge. In Proc. COLING-ACL 06.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181–201.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of
the 26th annual international ACM SIGIR conference
on Research and development in informaion retrieval,
pages 26–32. ACM Press.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In Proceedings of IJC-
NLP’2005, Lecture Notes in Computer Science (LNCS
3651), pages 378–389, Jeju Island, South Korea.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
</reference>
<page confidence="0.998234">
724
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.313628">
<title confidence="0.701404333333333">Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy! of Information Engineering and Computer</title>
<affiliation confidence="0.945122">University of Trento, 38123 Povo (TN),</affiliation>
<address confidence="0.835583">T.J. Watson Research Center P.O. Box 704, Yorktown Heights, NY 10598,</address>
<abstract confidence="0.999211789473684">The last decade has seen many interesting applications of Question Answering (QA) technology. The Jeopardy! quiz show is certainly one of the most fascinating, from the viewpoints of both its broad domain and the complexity of its language. In this paper, we study kernel methods applied to syntactic/semantic structures for accurate classification of Jeopardy! definition questions. Our extensive empirical analysis shows that our classification models largely improve on classifiers based on word-language models. Such classifiers are also used in the state-of-the-art QA pipeline constituting Watson, the IBM Jeopardy! system. Our experiments measuring their impact on Watson show enhancements in QA accuracy and a consequent increase in the amount of money earned in game-based evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kisuh Ahn</author>
<author>Johan Bos</author>
<author>Stephen Clark</author>
<author>James R Curran</author>
<author>Tiphaine Dalmas</author>
<author>Jochen L Leidner</author>
<author>Matthew B Smillie</author>
<author>Bonnie Webber</author>
</authors>
<title>Question answering with qed and wee at trec-2004.</title>
<date>2004</date>
<booktitle>The Thirteenth Text REtrieval Conference, TREC 2004,</booktitle>
<pages>595--599</pages>
<editor>In E. M. Voorhees and L. P. Buckland, editors,</editor>
<location>Gaitersburg, MD.</location>
<contexts>
<context position="6582" citStr="Ahn et al., 2004" startWordPosition="1018" endWordPosition="1021">y is given by the fact els for such classifiers, Section 4 presents our experthat Jeopardy! clues are not phrased in interrogative iments on QC, whereas Section 5 shows the final imform as questions typically are. This complicates the pact on Watson. Finally, Section 6 discusses related design of definition classifiers since we cannot di- work and Section 7 derives the conclusions. rectly use either typical structural patterns that char- 2 Watson: The IBM Jeopardy! System acterize definition/description questions, or previous This section gives a quick overview of Watson and approaches, e.g. (Ahn et al., 2004; Kaisser and Web- the problem of classification of definition questions, ber, 2007; Blunsom et al., 2006). Given the com- which is the focus of this paper. plexity and the novelty of the task, we found it use- 2.1 Overview ful to exploit the kernel methods technology. This Watson is a massively parallel probabilistic has shown state-of-the-art performance in Question evidence-based architecture for QA (Ferrucci et Classification (QC), e.g. (Zhang and Lee, 2003; al., 2010). It consists of several major stages for Suzuki et al., 2003; Moschitti et al., 2007) and it underlying sub-tasks, includi</context>
</contexts>
<marker>Ahn, Bos, Clark, Curran, Dalmas, Leidner, Smillie, Webber, 2004</marker>
<rawString>Kisuh Ahn, Johan Bos, Stephen Clark, James R. Curran, Tiphaine Dalmas, Jochen L. Leidner, Matthew B. Smillie, and Bonnie Webber. 2004. Question answering with qed and wee at trec-2004. In E. M. Voorhees and L. P. Buckland, editors, The Thirteenth Text REtrieval Conference, TREC 2004, pages 595– 599, Gaitersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Marco Cammisa</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Effective use of WordNet semantics via kernel-based learning.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL2005,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan.</location>
<marker>Basili, Cammisa, Moschitti, 2005</marker>
<rawString>Roberto Basili, Marco Cammisa, and Alessandro Moschitti. 2005. Effective use of WordNet semantics via kernel-based learning. In Proceedings of CoNLL2005, pages 1–8, Ann Arbor, Michigan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bilotti</author>
<author>P Ogilvie</author>
<author>J Callan</author>
<author>E Nyberg</author>
</authors>
<title>Structured retrieval for question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of ACM SIGIR.</booktitle>
<contexts>
<context position="32854" citStr="Bilotti et al., 2007" startWordPosition="5179" endWordPosition="5182">e use of advanced representation for QC in the Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the state-of-the-art in QC</context>
</contexts>
<marker>Bilotti, Ogilvie, Callan, Nyberg, 2007</marker>
<rawString>M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007. Structured retrieval for question answering. In Proceedings of ACM SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Blair-Goldensohn</author>
<author>K R McKeown</author>
<author>A H Schlaikjer</author>
</authors>
<title>Answering definitional questions: A hybrid approach.</title>
<date>2004</date>
<booktitle>Proceedings of AAAI</booktitle>
<editor>In M. Maybury, editor,</editor>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="32790" citStr="Blair-Goldensohn et al., 2004" startWordPosition="5167" endWordPosition="5170"> statistically significant at p&lt;0.05. 6 Related Work Our paper studies the use of advanced representation for QC in the Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Mo</context>
</contexts>
<marker>Blair-Goldensohn, McKeown, Schlaikjer, 2004</marker>
<rawString>S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer. 2004. Answering definitional questions: A hybrid approach. In M. Maybury, editor, Proceedings of AAAI 2004. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Combined syntactic and semantic kernels for text classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ECIR 2007,</booktitle>
<location>Rome, Italy.</location>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Stephan Bloehdorn and Alessandro Moschitti. 2007a. Combined syntactic and semantic kernels for text classification. In Proceedings of ECIR 2007, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structure and semantics for expressive text kernels. In</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM ’07.</booktitle>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Stephan Bloehdorn and Alessandro Moschitti. 2007b. Structure and semantics for expressive text kernels. In In Proceedings of CIKM ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Krystle Kocik</author>
<author>James R Curran</author>
</authors>
<title>Question classification with log-linear models.</title>
<date>2006</date>
<booktitle>In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>615--616</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6688" citStr="Blunsom et al., 2006" startWordPosition="1034" endWordPosition="1037"> not phrased in interrogative iments on QC, whereas Section 5 shows the final imform as questions typically are. This complicates the pact on Watson. Finally, Section 6 discusses related design of definition classifiers since we cannot di- work and Section 7 derives the conclusions. rectly use either typical structural patterns that char- 2 Watson: The IBM Jeopardy! System acterize definition/description questions, or previous This section gives a quick overview of Watson and approaches, e.g. (Ahn et al., 2004; Kaisser and Web- the problem of classification of definition questions, ber, 2007; Blunsom et al., 2006). Given the com- which is the focus of this paper. plexity and the novelty of the task, we found it use- 2.1 Overview ful to exploit the kernel methods technology. This Watson is a massively parallel probabilistic has shown state-of-the-art performance in Question evidence-based architecture for QA (Ferrucci et Classification (QC), e.g. (Zhang and Lee, 2003; al., 2010). It consists of several major stages for Suzuki et al., 2003; Moschitti et al., 2007) and it underlying sub-tasks, including analysis of the is very well suited for engineering feature represen- question, retrieval of relevant c</context>
<context position="12745" citStr="Blunsom et al., 2006" startWordPosition="1974" endWordPosition="1977">ed towards dictionary-like sources in our system. We use a variety of such sources, such as standard English dictionaries, Wiktionary, WordNet, etc. After gathering supporting evidence for candidate answers extracted from these sources, our system routes the candidates to definition-specific candidate ranking models, which have been trained with selected feature sets. The following sections present a description and evaluation of our approach for identifying and answering definition questions. 3 Kernel Models for Question Classification Previous work (Zhang and Lee, 2003; Suzuki et al., 2003; Blunsom et al., 2006; Moschitti et al., 2007) as shown that syntactic structures are essential for QC. Given the novelty of both the domain and the type of our classification items, we rely on kernel methods to study and design effective representations. Indeed, these are excellent tools for automatic feature engineering, especially for unknown tasks and domains. Our approach consists of using SVMs and kernels for structured data applied to several types of structural lexical, syntactic and shallow semantic information. 3.1 Tree and Sequence Kernels Kernel functions are implicit scalar products between data examp</context>
</contexts>
<marker>Blunsom, Kocik, Curran, 2006</marker>
<rawString>Phil Blunsom, Krystle Kocik, and James R. Curran. 2006. Question classification with log-linear models. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 615–616, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT and EMNLP,</booktitle>
<pages>724--731</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="34505" citStr="Bunescu and Mooney, 2005" startWordPosition="5447" endWordPosition="5450">rmat. Thus, we employed new methods such as PTK, dependency structures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds of syntactic/semantic linguistic structures for the cl</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of HLT and EMNLP, pages 724–731, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="34540" citStr="Bunescu, 2007" startWordPosition="5455" endWordPosition="5456">K, dependency structures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds of syntactic/semantic linguistic structures for the classification of questions in a new </context>
</contexts>
<marker>Bunescu, 2007</marker>
<rawString>Razvan C. Bunescu. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Cancedda</author>
<author>Eric Gaussier</author>
<author>Cyril Goutte</author>
<author>Jean Michel Renders</author>
</authors>
<title>Word sequence kernels.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1059</pages>
<contexts>
<context position="34608" citStr="Cancedda et al., 2003" startWordPosition="5463" endWordPosition="5466">ng category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds of syntactic/semantic linguistic structures for the classification of questions in a new application domain, i.e. Jeopardy!. Our findings are summarized here</context>
</contexts>
<marker>Cancedda, Gaussier, Goutte, Renders, 2003</marker>
<rawString>Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean Michel Renders. 2003. Word sequence kernels. Journal of Machine Learning Research, 3:1059–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="19161" citStr="Charniak, 2000" startWordPosition="3006" endWordPosition="3008">,][a][phosphor][gives] gory, e.g. INVEST in example (4). [off][electromagnetic][energy][in][this][form] Tools: for SVM learning, we used the SVMLightPS: [wrb][vbn][in][nns][,][dt][nn][vbz][rp][jj][nn][in] TK software4, which includes structural kernels in [dt][nn] SVMLight (Joachims, 1999)5. For generating conAdditionally, we use constituency trees (CTs), see 3Past Jeopardy! games can be downloaded from http://www.j-archive.com. 4Available at http://dit.unitn.it/∼moschitt 5http://svmlight.joachims.org 2From here the name syntactic tree kernels 716 stituency trees, we used the Charniak parser (Charniak, 2000). We also used the syntactic–semantic parser by Johansson and Nugues (2008) to generate dependency trees (Mel’ˇcuk, 1988) and predicate argument trees according to the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Baseline Model: the first model that we used as a baseline is a rule-based classifier (RBC). The RBC leverages a set of rules that matches against lexical and syntactic information in the clue to make a binary decision on whether or not the clue is considered definitional. The rule set was manually developed by a human expert, and consists of rules that</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Min Zhang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>A hybrid convolution tree kernel for semantic role labeling.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>73--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="34742" citStr="Che et al., 2006" startWordPosition="5485" endWordPosition="5488">nguage Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds of syntactic/semantic linguistic structures for the classification of questions in a new application domain, i.e. Jeopardy!. Our findings are summarized hereafter: 11Available at http://cogcomp.cs.illinois. edu/Data/QA/QC/ 720 First, it should be noted that basic kernels, such as STK, PTK a</context>
<context position="36207" citStr="Che et al., 2006" startWordPosition="5712" endWordPosition="5715">l results show that the higher variability of Jeopardy! questions prevents us from achieving generalization with typical syntactic patterns even if they are derived by powerful methods such as STK. In contrast, partial patterns, such as those provided by PTK applied to constituency (or dependency) trees, prove to be effective. In particular, STK has been considered as the best kernel for exploiting syntactic information in constituency trees, e.g. it is state-of-the-art in: QC (Zhang and Lee, 2003; Moschitti et al., 2007; Moschitti, 2008); SRL, (Moschitti et al., 2008; Moschitti et al., 2005; Che et al., 2006b); pronominal coreference resolution (Yang et al., 2006; Versley et al., 2008) and Relation Extraction (Zhang et al., 2006; Nguyen et al., 2009b). We showed that, in the complex domain of Jeopardy!, STK surprisingly provides low accuracy whereas PTK is rather effective and greatly outperforms STK. We have also provided an explanation of such behavior by means of error analysis: in contrast with traditional question classification, which focuses on basic syntactic patterns (e.g. ”what”, ”where”, ”who” and ”how”). Figure 5 shows that PTK captures partial patterns that are important for more com</context>
</contexts>
<marker>Che, Zhang, Liu, Li, 2006</marker>
<rawString>Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li. 2006a. A hybrid convolution tree kernel for semantic role labeling. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73– 80, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Min Zhang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>A hybrid convolution tree kernel for semantic role labeling.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06,</booktitle>
<pages>73--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="34742" citStr="Che et al., 2006" startWordPosition="5485" endWordPosition="5488">nguage Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds of syntactic/semantic linguistic structures for the classification of questions in a new application domain, i.e. Jeopardy!. Our findings are summarized hereafter: 11Available at http://cogcomp.cs.illinois. edu/Data/QA/QC/ 720 First, it should be noted that basic kernels, such as STK, PTK a</context>
<context position="36207" citStr="Che et al., 2006" startWordPosition="5712" endWordPosition="5715">l results show that the higher variability of Jeopardy! questions prevents us from achieving generalization with typical syntactic patterns even if they are derived by powerful methods such as STK. In contrast, partial patterns, such as those provided by PTK applied to constituency (or dependency) trees, prove to be effective. In particular, STK has been considered as the best kernel for exploiting syntactic information in constituency trees, e.g. it is state-of-the-art in: QC (Zhang and Lee, 2003; Moschitti et al., 2007; Moschitti, 2008); SRL, (Moschitti et al., 2008; Moschitti et al., 2005; Che et al., 2006b); pronominal coreference resolution (Yang et al., 2006; Versley et al., 2008) and Relation Extraction (Zhang et al., 2006; Nguyen et al., 2009b). We showed that, in the complex domain of Jeopardy!, STK surprisingly provides low accuracy whereas PTK is rather effective and greatly outperforms STK. We have also provided an explanation of such behavior by means of error analysis: in contrast with traditional question classification, which focuses on basic syntactic patterns (e.g. ”what”, ”where”, ”who” and ”how”). Figure 5 shows that PTK captures partial patterns that are important for more com</context>
</contexts>
<marker>Che, Zhang, Liu, Li, 2006</marker>
<rawString>Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li. 2006b. A hybrid convolution tree kernel for semantic role labeling. In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06, pages 73–80, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chen</author>
<author>M Zhou</author>
<author>S Wang</author>
</authors>
<title>Reranking answers from definitional QA using language models.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="32809" citStr="Chen et al., 2006" startWordPosition="5171" endWordPosition="5174">&lt;0.05. 6 Related Work Our paper studies the use of advanced representation for QC in the Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 200</context>
</contexts>
<marker>Chen, Zhou, Wang, 2006</marker>
<rawString>Y. Chen, M. Zhou, and S. Wang. 2006. Reranking answers from definitional QA using language models. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Clarke</author>
<author>Gordon Cormack</author>
<author>Thomas Lynam</author>
</authors>
<title>Exploiting redundancy in question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th SIGIR Conference,</booktitle>
<pages>358--365</pages>
<contexts>
<context position="1811" citStr="Clarke et al., 2001" startWordPosition="262" endWordPosition="265">ease in the amount of money earned in game-based evaluation. 1 Introduction Question Answering (QA) is an important research area of Information Retrieval applications, which requires the use of core NLP capabilities, such as syntactic and semantic processing for a more effective user experience. While the development of most existing QA systems are driven by organized evaluation efforts such as TREC (Voorhees and Dang, 2006), CLEF (Giampiccolo et al., 2007), and NTCIR (Sasaki et al., 2007), there exist efforts that leverage data from popular quiz shows, such as Who Wants to be a Millionaire (Clarke et al., 2001; Lam et al., 2003) and Jeopardy! (Ferrucci et al., 2010), to demonstrate the generality of the technology. Jeopardy! is a popular quiz show in the US which has been on the air for 27 years. In each game, three contestants compete for the opportunity to answer 60 questions in 12 categories of 5 questions each. Jeopardy! questions cover an incredibly broad domain, from science, literature, history, to popular culture. We are drawn to Jeopardy! as a test bed for open-domain QA technology due to its broad domain, complex language, as well as the emphasis on accuracy, confidence, and speed during </context>
</contexts>
<marker>Clarke, Cormack, Lynam, 2001</marker>
<rawString>Charles Clarke, Gordon Cormack, and Thomas Lynam. 2001. Exploiting redundancy in question answering. In Proceedings of the 24th SIGIR Conference, pages 358–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02.</booktitle>
<contexts>
<context position="34202" citStr="Collins and Duffy, 2002" startWordPosition="5398" endWordPosition="5401">hing an accuracy of 91.8%. However, such data refers to typical instances from QA, whose syntactic patterns can be easily generalized by STK. In contrast, we have shown that STK-CT is not effective for our domain, as it presents very innovative elements: questions in affirmative and highly variable format. Thus, we employed new methods such as PTK, dependency structures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. In Proceedings of ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>M Kan</author>
<author>T Chua</author>
</authors>
<title>Generic soft pattern models for definitional QA.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<publisher>ACM.</publisher>
<location>Salvador, Brazil.</location>
<contexts>
<context position="33012" citStr="Cui et al., 2005" startWordPosition="5207" endWordPosition="5210">rent from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the state-of-the-art in QC by applying SVMs along with STK-CT. The results were derived by experimenting with a TREC dataset11(Li and Roth, 2002), reaching an accuracy of 91.8%. Howeve</context>
</contexts>
<marker>Cui, Kan, Chua, 2005</marker>
<rawString>H. Cui, M. Kan, and T. Chua. 2005. Generic soft pattern models for definitional QA. In Proceedings of SIGIR, Salvador, Brazil. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>423--429</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="34479" citStr="Culotta and Sorensen, 2004" startWordPosition="5443" endWordPosition="5446">ative and highly variable format. Thus, we employed new methods such as PTK, dependency structures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds of syntactic/semantic lingui</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of ACL, pages 423–429, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chad Cumby</author>
<author>Dan Roth</author>
</authors>
<title>Kernel Methods for Relational Learning.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="34379" citStr="Cumby and Roth, 2003" startWordPosition="5427" endWordPosition="5430"> is not effective for our domain, as it presents very innovative elements: questions in affirmative and highly variable format. Thus, we employed new methods such as PTK, dependency structures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have</context>
</contexts>
<marker>Cumby, Roth, 2003</marker>
<rawString>Chad Cumby and Dan Roth. 2003. Kernel Methods for Relational Learning. In Proceedings of ICML 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Np bracketing by maximum entropy tagging and SVM reranking.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP’04.</booktitle>
<marker>Daum´e, Marcu, 2004</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2004. Np bracketing by maximum entropy tagging and SVM reranking. In Proceedings of EMNLP’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Echihabi</author>
<author>D Marcu</author>
</authors>
<title>A noisy-channel approach to question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="32928" citStr="Echihabi and Marcu, 2003" startWordPosition="5191" endWordPosition="5195">reviously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the state-of-the-art in QC by applying SVMs along with STK-CT. The results were derived by experimen</context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>A. Echihabi and D. Marcu. 2003. A noisy-channel approach to question answering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Eric Brown</author>
<author>Jennifer Chu-Carroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
</authors>
<title>Eric Nyberg,</title>
<date>2010</date>
<journal>AI Magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<location>John Prager, Nico</location>
<contexts>
<context position="1868" citStr="Ferrucci et al., 2010" startWordPosition="272" endWordPosition="275">ation. 1 Introduction Question Answering (QA) is an important research area of Information Retrieval applications, which requires the use of core NLP capabilities, such as syntactic and semantic processing for a more effective user experience. While the development of most existing QA systems are driven by organized evaluation efforts such as TREC (Voorhees and Dang, 2006), CLEF (Giampiccolo et al., 2007), and NTCIR (Sasaki et al., 2007), there exist efforts that leverage data from popular quiz shows, such as Who Wants to be a Millionaire (Clarke et al., 2001; Lam et al., 2003) and Jeopardy! (Ferrucci et al., 2010), to demonstrate the generality of the technology. Jeopardy! is a popular quiz show in the US which has been on the air for 27 years. In each game, three contestants compete for the opportunity to answer 60 questions in 12 categories of 5 questions each. Jeopardy! questions cover an incredibly broad domain, from science, literature, history, to popular culture. We are drawn to Jeopardy! as a test bed for open-domain QA technology due to its broad domain, complex language, as well as the emphasis on accuracy, confidence, and speed during game play. While the vast majority of Jeopardy! questions</context>
</contexts>
<marker>Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, 2010</marker>
<rawString>David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welty. 2010. Building watson: An overview of the deepqa project. AI Magazine, 31(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Pamela Froner</author>
<author>Anselmo Pe˜nas</author>
<author>Christelle Ayache</author>
<author>Dan Cristea</author>
<author>Valentin Jijkoun</author>
<author>Petya Osenova</author>
<author>Paulo Rocha</author>
<author>Bogdan Sacaleanu</author>
<author>Richard Suteliffe</author>
</authors>
<title>multilingual question anwering track.</title>
<date>2007</date>
<journal>Overview of the CLEF</journal>
<booktitle>In Proceedings of the Cross Language Evaluation Forum.</booktitle>
<marker>Giampiccolo, Froner, Pe˜nas, Ayache, Cristea, Jijkoun, Osenova, Rocha, Sacaleanu, Suteliffe, 2007</marker>
<rawString>Danilo Giampiccolo, Pamela Froner, Anselmo Pe˜nas, Christelle Ayache, Dan Cristea, Valentin Jijkoun, Petya Osenova, Paulo Rocha, Bogdan Sacaleanu, and Richard Suteliffe. 2007. Overview of the CLEF 2007 multilingual question anwering track. In Proceedings of the Cross Language Evaluation Forum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Claudio Giuliano</author>
<author>Carlo Strapparava</author>
</authors>
<title>Domain kernels for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05,</booktitle>
<pages>403--410</pages>
<contexts>
<context position="34658" citStr="Gliozzo et al., 2005" startWordPosition="5470" endWordPosition="5473">rding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds of syntactic/semantic linguistic structures for the classification of questions in a new application domain, i.e. Jeopardy!. Our findings are summarized hereafter: 11Available at http://cogcomp.cs.illinois. </context>
</contexts>
<marker>Gliozzo, Giuliano, Strapparava, 2005</marker>
<rawString>Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava. 2005. Domain kernels for word sense disambiguation. In Proceedings of ACL’05, pages 403–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hickl</author>
<author>J Williams</author>
<author>J Bensley</author>
<author>K Roberts</author>
<author>Y Shi</author>
<author>B Rink</author>
</authors>
<title>Question answering with lcc chaucer at trec</title>
<date>2006</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="32616" citStr="Hickl et al., 2006" startWordPosition="5142" endWordPosition="5145"> 6% compared to the NoDef system. On the other hand, the StatDef system outperformed the two other systems, and its accuracy improvement upon the RuleDef system is statistically significant at p&lt;0.05. 6 Related Work Our paper studies the use of advanced representation for QC in the Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Shi, Rink, 2006</marker>
<rawString>A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, and B. Rink. 2006. Question answering with lcc chaucer at trec 2006. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods – Support Vector Learning,</booktitle>
<volume>13</volume>
<contexts>
<context position="18836" citStr="Joachims, 1999" startWordPosition="2967" endWordPosition="2968">- resulted in 306 definition and 4964 non-definition netic energy in this form. (answer: light clues. Each test set is stored in a separate file conor photons), sisting of one line per question, which contains tabwe use the following sequences: separated clue information and the Jeopardy! cateWS: [when][hit][by][electrons][,][a][phosphor][gives] gory, e.g. INVEST in example (4). [off][electromagnetic][energy][in][this][form] Tools: for SVM learning, we used the SVMLightPS: [wrb][vbn][in][nns][,][dt][nn][vbz][rp][jj][nn][in] TK software4, which includes structural kernels in [dt][nn] SVMLight (Joachims, 1999)5. For generating conAdditionally, we use constituency trees (CTs), see 3Past Jeopardy! games can be downloaded from http://www.j-archive.com. 4Available at http://dit.unitn.it/∼moschitt 5http://svmlight.joachims.org 2From here the name syntactic tree kernels 716 stituency trees, we used the Charniak parser (Charniak, 2000). We also used the syntactic–semantic parser by Johansson and Nugues (2008) to generate dependency trees (Mel’ˇcuk, 1988) and predicate argument trees according to the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Baseline Model: the first mode</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. Advances in Kernel Methods – Support Vector Learning, 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based syntactic–semantic analysis with PropBank and NomBank.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning,</booktitle>
<pages>183--187</pages>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="19236" citStr="Johansson and Nugues (2008)" startWordPosition="3016" endWordPosition="3019">electromagnetic][energy][in][this][form] Tools: for SVM learning, we used the SVMLightPS: [wrb][vbn][in][nns][,][dt][nn][vbz][rp][jj][nn][in] TK software4, which includes structural kernels in [dt][nn] SVMLight (Joachims, 1999)5. For generating conAdditionally, we use constituency trees (CTs), see 3Past Jeopardy! games can be downloaded from http://www.j-archive.com. 4Available at http://dit.unitn.it/∼moschitt 5http://svmlight.joachims.org 2From here the name syntactic tree kernels 716 stituency trees, we used the Charniak parser (Charniak, 2000). We also used the syntactic–semantic parser by Johansson and Nugues (2008) to generate dependency trees (Mel’ˇcuk, 1988) and predicate argument trees according to the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Baseline Model: the first model that we used as a baseline is a rule-based classifier (RBC). The RBC leverages a set of rules that matches against lexical and syntactic information in the clue to make a binary decision on whether or not the clue is considered definitional. The rule set was manually developed by a human expert, and consists of rules that attempt to identify roughly 70 different constructs in the clues. For inst</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic–semantic analysis with PropBank and NomBank. In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning, pages 183–187, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kaisser</author>
<author>Bonnie Webber</author>
</authors>
<title>Question answering based on semantic roles.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Deep Linguistic Processing, DeepLP ’07,</booktitle>
<pages>41--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Kaisser, Webber, 2007</marker>
<rawString>Michael Kaisser and Bonnie Webber. 2007. Question answering based on semantic roles. In Proceedings of the Workshop on Deep Linguistic Processing, DeepLP ’07, pages 41–48, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Speeding up training with tree kernels for node relation labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP’05.</booktitle>
<contexts>
<context position="34724" citStr="Kazama and Torisawa, 2005" startWordPosition="5481" endWordPosition="5484"> body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds of syntactic/semantic linguistic structures for the classification of questions in a new application domain, i.e. Jeopardy!. Our findings are summarized hereafter: 11Available at http://cogcomp.cs.illinois. edu/Data/QA/QC/ 720 First, it should be noted that basic kernels, </context>
</contexts>
<marker>Kazama, Torisawa, 2005</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2005. Speeding up training with tree kernels for node relation labeling. In Proceedings of HLT-EMNLP’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03.</booktitle>
<contexts>
<context position="34266" citStr="Kudo and Matsumoto, 2003" startWordPosition="5410" endWordPosition="5413"> instances from QA, whose syntactic patterns can be easily generalized by STK. In contrast, we have shown that STK-CT is not effective for our domain, as it presents very innovative elements: questions in affirmative and highly variable format. Thus, we employed new methods such as PTK, dependency structures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to severa</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings of ACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Boosting-based parse reranking with subtree features.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05.</booktitle>
<contexts>
<context position="34221" citStr="Kudo et al., 2005" startWordPosition="5402" endWordPosition="5405">. However, such data refers to typical instances from QA, whose syntactic patterns can be easily generalized by STK. In contrast, we have shown that STK-CT is not effective for our domain, as it presents very innovative elements: questions in affirmative and highly variable format. Thus, we employed new methods such as PTK, dependency structures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of seve</context>
</contexts>
<marker>Kudo, Suzuki, Isozaki, 2005</marker>
<rawString>Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In Proceedings of ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shyong Lam</author>
<author>David Pennock</author>
<author>Dan Cosley</author>
<author>Steve Lawrence</author>
</authors>
<title>1 billion pages = 1 milllion dollars? mining the web to pay ”who wants to be a millionaire?</title>
<date>2003</date>
<booktitle>In Proceedings of the 19th Conference on Uncertainty in AI.</booktitle>
<contexts>
<context position="1830" citStr="Lam et al., 2003" startWordPosition="266" endWordPosition="269"> money earned in game-based evaluation. 1 Introduction Question Answering (QA) is an important research area of Information Retrieval applications, which requires the use of core NLP capabilities, such as syntactic and semantic processing for a more effective user experience. While the development of most existing QA systems are driven by organized evaluation efforts such as TREC (Voorhees and Dang, 2006), CLEF (Giampiccolo et al., 2007), and NTCIR (Sasaki et al., 2007), there exist efforts that leverage data from popular quiz shows, such as Who Wants to be a Millionaire (Clarke et al., 2001; Lam et al., 2003) and Jeopardy! (Ferrucci et al., 2010), to demonstrate the generality of the technology. Jeopardy! is a popular quiz show in the US which has been on the air for 27 years. In each game, three contestants compete for the opportunity to answer 60 questions in 12 categories of 5 questions each. Jeopardy! questions cover an incredibly broad domain, from science, literature, history, to popular culture. We are drawn to Jeopardy! as a test bed for open-domain QA technology due to its broad domain, complex language, as well as the emphasis on accuracy, confidence, and speed during game play. While th</context>
</contexts>
<marker>Lam, Pennock, Cosley, Lawrence, 2003</marker>
<rawString>Shyong Lam, David Pennock, Dan Cosley, and Steve Lawrence. 2003. 1 billion pages = 1 milllion dollars? mining the web to pay ”who wants to be a millionaire? In Proceedings of the 19th Conference on Uncertainty in AI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="33573" citStr="Li and Roth, 2002" startWordPosition="5299" endWordPosition="5302">els were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the state-of-the-art in QC by applying SVMs along with STK-CT. The results were derived by experimenting with a TREC dataset11(Li and Roth, 2002), reaching an accuracy of 91.8%. However, such data refers to typical instances from QA, whose syntactic patterns can be easily generalized by STK. In contrast, we have shown that STK-CT is not effective for our domain, as it presents very innovative elements: questions in affirmative and highly variable format. Thus, we employed new methods such as PTK, dependency structures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic par</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>X. Li and D. Roth. 2002. Learning question classifiers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C McCord</author>
</authors>
<title>Slot grammars. Computational Linguistics.</title>
<date>1980</date>
<contexts>
<context position="25739" citStr="McCord, 1980" startWordPosition="4075" endWordPosition="4076">PASS+CSK+RBC 62.46 71.24 66.56 WSK+CSK+RBC 69.26 66.99 68.11 ALL 61.42 67.65 64.38 Table 2: Performance of Kernel Combinations using leave-one-out cross-validation. formance. This is also confirmed by the PASS results derived in (Moschitti et al., 2007) on TREC QC. 5 Experiments on the Jeopardy System Since the kernel-based classifiers perform substantially better than RBC, we incorporate the PTKCT+WSK+CSK model9 into Watson for definition classification and evaluated the QA performance against two baseline systems. For the end-to-end experiments, we used Watson’s English Slot Grammar parser (McCord, 1980) to generate the constituency trees. The component level evaluation shows that we achieved comparable performance as previously discussed with ESG. 5.1 Experimental Setup We integrated the classifier into the question analysis module, and incorporated additional components to search against dictionary resources and extract candidate answers from these search results when a question is classified as definitional. In the final machine learning models, a separate model is trained for definition questions to enable scoring tailored to the specific characteristics of those questions. Based on our m</context>
</contexts>
<marker>McCord, 1980</marker>
<rawString>Michael C. McCord. 1980. Slot grammars. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University Press of</publisher>
<location>New York, Albany.</location>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>Igor A. Mel’ˇcuk. 1988. Dependency Syntax: Theory and Practice. State University Press of New York, Albany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>The NomBank project: An interim report.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation,</booktitle>
<pages>24--31</pages>
<location>Boston, United States.</location>
<contexts>
<context position="16198" citStr="Meyers et al., 2004" startWordPosition="2569" endWordPosition="2572"> with all their direct chil- are considered additional children of the POS-tag dren. These, in case of a syntactic parse tree, are nodes (in this case the connecting edge just serves complete production rules of the associated parser to add a lexical feature to the target POS-tag node). grammar2. Finally, we also use predicate argument structures • Partial Tree Kernel (PTK) (Moschitti, 2006) ap- generated by verbal and nominal relations accordplied to both constituency and dependency parse ing to PropBank (Palmer et al., 2005) and NomBank trees. This generates all possible tree fragments, as (Meyers et al., 2004). Given the target sentence, the above, but sibling nodes can be separated (so they set of its predicates are extracted and converted into can be part of different tree fragments). In other a forest, then a fake root node, PAS, is used to conwords, a fragment is any possible tree path, from nect these trees. For example, Figure 4 illustrates a whose nodes other tree paths can depart. Conse- Predicate Argument Structures Set (PASS) encoding quently, an extremely rich feature space is gener- two relations, give and hit, as well as the nominalizaated. Of course, PTK subsumes STK but sometimes tio</context>
<context position="19393" citStr="Meyers et al., 2004" startWordPosition="3041" endWordPosition="3044">cludes structural kernels in [dt][nn] SVMLight (Joachims, 1999)5. For generating conAdditionally, we use constituency trees (CTs), see 3Past Jeopardy! games can be downloaded from http://www.j-archive.com. 4Available at http://dit.unitn.it/∼moschitt 5http://svmlight.joachims.org 2From here the name syntactic tree kernels 716 stituency trees, we used the Charniak parser (Charniak, 2000). We also used the syntactic–semantic parser by Johansson and Nugues (2008) to generate dependency trees (Mel’ˇcuk, 1988) and predicate argument trees according to the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Baseline Model: the first model that we used as a baseline is a rule-based classifier (RBC). The RBC leverages a set of rules that matches against lexical and syntactic information in the clue to make a binary decision on whether or not the clue is considered definitional. The rule set was manually developed by a human expert, and consists of rules that attempt to identify roughly 70 different constructs in the clues. For instance, one of the rules matches the parse tree structure for ”It’s X or Y”, which will identify example (1) as a definition question. Kernel Models: we apply </context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The NomBank project: An interim report. In HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation, pages 24–31, Boston, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Bonaventura Coppola</author>
<author>Ana-Maria Giuglea</author>
<author>Roberto Basili</author>
</authors>
<title>Hierarchical semantic role labeling.</title>
<date>2005</date>
<booktitle>In CoNLL</booktitle>
<note>shared task.</note>
<contexts>
<context position="36189" citStr="Moschitti et al., 2005" startWordPosition="5707" endWordPosition="5711"> Second, the experimental results show that the higher variability of Jeopardy! questions prevents us from achieving generalization with typical syntactic patterns even if they are derived by powerful methods such as STK. In contrast, partial patterns, such as those provided by PTK applied to constituency (or dependency) trees, prove to be effective. In particular, STK has been considered as the best kernel for exploiting syntactic information in constituency trees, e.g. it is state-of-the-art in: QC (Zhang and Lee, 2003; Moschitti et al., 2007; Moschitti, 2008); SRL, (Moschitti et al., 2008; Moschitti et al., 2005; Che et al., 2006b); pronominal coreference resolution (Yang et al., 2006; Versley et al., 2008) and Relation Extraction (Zhang et al., 2006; Nguyen et al., 2009b). We showed that, in the complex domain of Jeopardy!, STK surprisingly provides low accuracy whereas PTK is rather effective and greatly outperforms STK. We have also provided an explanation of such behavior by means of error analysis: in contrast with traditional question classification, which focuses on basic syntactic patterns (e.g. ”what”, ”where”, ”who” and ”how”). Figure 5 shows that PTK captures partial patterns that are impo</context>
</contexts>
<marker>Moschitti, Coppola, Giuglea, Basili, 2005</marker>
<rawString>Alessandro Moschitti, Bonaventura Coppola, Ana-Maria Giuglea, and Roberto Basili. 2005. Hierarchical semantic role labeling. In CoNLL 2005 shared task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question/answer classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL’07.</booktitle>
<contexts>
<context position="7145" citStr="Moschitti et al., 2007" startWordPosition="1107" endWordPosition="1110">ck overview of Watson and approaches, e.g. (Ahn et al., 2004; Kaisser and Web- the problem of classification of definition questions, ber, 2007; Blunsom et al., 2006). Given the com- which is the focus of this paper. plexity and the novelty of the task, we found it use- 2.1 Overview ful to exploit the kernel methods technology. This Watson is a massively parallel probabilistic has shown state-of-the-art performance in Question evidence-based architecture for QA (Ferrucci et Classification (QC), e.g. (Zhang and Lee, 2003; al., 2010). It consists of several major stages for Suzuki et al., 2003; Moschitti et al., 2007) and it underlying sub-tasks, including analysis of the is very well suited for engineering feature represen- question, retrieval of relevant content, scoring and tations for novel tasks. ranking of candidate answers, as depicted in Figure In this paper, we apply SVMs and kernel meth- 1. In the rest of this section, we provide an overview ods to syntactic/semantic structures for modeling of Watson, focusing on the task of answering accurate classification of Jeopardy! definition ques- definitional questions. tions. For this purpose, we use several levels of lin- Question Analysis: The first st</context>
<context position="12770" citStr="Moschitti et al., 2007" startWordPosition="1978" endWordPosition="1981">like sources in our system. We use a variety of such sources, such as standard English dictionaries, Wiktionary, WordNet, etc. After gathering supporting evidence for candidate answers extracted from these sources, our system routes the candidates to definition-specific candidate ranking models, which have been trained with selected feature sets. The following sections present a description and evaluation of our approach for identifying and answering definition questions. 3 Kernel Models for Question Classification Previous work (Zhang and Lee, 2003; Suzuki et al., 2003; Blunsom et al., 2006; Moschitti et al., 2007) as shown that syntactic structures are essential for QC. Given the novelty of both the domain and the type of our classification items, we rely on kernel methods to study and design effective representations. Indeed, these are excellent tools for automatic feature engineering, especially for unknown tasks and domains. Our approach consists of using SVMs and kernels for structured data applied to several types of structural lexical, syntactic and shallow semantic information. 3.1 Tree and Sequence Kernels Kernel functions are implicit scalar products between data examples (i.e. questions in ou</context>
<context position="22430" citStr="Moschitti et al., 2007" startWordPosition="3560" endWordPosition="3563">ate the complexity of the task: in order to capture the large variability of the positive examples, the rules developed by a skilled human designer are unable to be sufficiently precise to limit the recognition to those examples. On the other hand, RBC, being a rather different approach from SVMs, can be successfully exploited in a joint model with them. Second, BOW yields better F1 than RBC but it does not generalize well since its F1 is still low. When n-grams are also added to the model by means of WSK, the F1 improves by about 1.5 absolute points. As already shown in (Zhang and Lee, 2003; Moschitti et al., 2007), syntactic structures are needed to improve generalization. Third, surprisingly with respect to previous work, STK applied to CT8 provides accuracy lower than BOW, about 8 absolute points. The reason is due to the different nature of the Jeopardy! questions: large syntactic variability reduces the probability of finding general and well formed patterns, i.e. structures generated by entire production rules. This suggests that PTK, which can capture patterns derived from partial production rules, can be more effective. Indeed, PTK-CT achieves the highest F1, outperforming WSK also when used wit</context>
<context position="25379" citStr="Moschitti et al., 2007" startWordPosition="4020" endWordPosition="4023">entary information. Finally, more complex kernels, especially the overall kernel summation, do not seem to improve the perKernel Space Prec. Rec. F1 WSK+CSK 70.00 57.19 62.95 PTK-CT+CSK 69.43 60.13 64.45 PTK-CT+WSK+CSK 68.59 62.09 65.18 CSK+RBC 47.80 74.51 58.23 PTK-CT+CSK+RBC 59.33 74.84 65.79 BOW+CSK+RBC 60.65 73.53 66.47 PTK-CT+WSK+CSK+RBC 67.66 66.99 67.32 PTK-CT+PASS+CSK+RBC 62.46 71.24 66.56 WSK+CSK+RBC 69.26 66.99 68.11 ALL 61.42 67.65 64.38 Table 2: Performance of Kernel Combinations using leave-one-out cross-validation. formance. This is also confirmed by the PASS results derived in (Moschitti et al., 2007) on TREC QC. 5 Experiments on the Jeopardy System Since the kernel-based classifiers perform substantially better than RBC, we incorporate the PTKCT+WSK+CSK model9 into Watson for definition classification and evaluated the QA performance against two baseline systems. For the end-to-end experiments, we used Watson’s English Slot Grammar parser (McCord, 1980) to generate the constituency trees. The component level evaluation shows that we achieved comparable performance as previously discussed with ESG. 5.1 Experimental Setup We integrated the classifier into the question analysis module, and i</context>
<context position="32878" citStr="Moschitti et al., 2007" startWordPosition="5183" endWordPosition="5186">esentation for QC in the Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the state-of-the-art in QC by applying SVMs along </context>
<context position="36117" citStr="Moschitti et al., 2007" startWordPosition="5695" endWordPosition="5698"> and, from this perspective, our paper makes a significant contribution. Second, the experimental results show that the higher variability of Jeopardy! questions prevents us from achieving generalization with typical syntactic patterns even if they are derived by powerful methods such as STK. In contrast, partial patterns, such as those provided by PTK applied to constituency (or dependency) trees, prove to be effective. In particular, STK has been considered as the best kernel for exploiting syntactic information in constituency trees, e.g. it is state-of-the-art in: QC (Zhang and Lee, 2003; Moschitti et al., 2007; Moschitti, 2008); SRL, (Moschitti et al., 2008; Moschitti et al., 2005; Che et al., 2006b); pronominal coreference resolution (Yang et al., 2006; Versley et al., 2008) and Relation Extraction (Zhang et al., 2006; Nguyen et al., 2009b). We showed that, in the complex domain of Jeopardy!, STK surprisingly provides low accuracy whereas PTK is rather effective and greatly outperforms STK. We have also provided an explanation of such behavior by means of error analysis: in contrast with traditional question classification, which focuses on basic syntactic patterns (e.g. ”what”, ”where”, ”who” and</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question/answer classification. In Proceedings of ACL’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Tree kernels for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="34768" citStr="Moschitti et al., 2008" startWordPosition="5489" endWordPosition="5492">e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds of syntactic/semantic linguistic structures for the classification of questions in a new application domain, i.e. Jeopardy!. Our findings are summarized hereafter: 11Available at http://cogcomp.cs.illinois. edu/Data/QA/QC/ 720 First, it should be noted that basic kernels, such as STK, PTK and SK, when applied to new</context>
<context position="36165" citStr="Moschitti et al., 2008" startWordPosition="5703" endWordPosition="5706">ignificant contribution. Second, the experimental results show that the higher variability of Jeopardy! questions prevents us from achieving generalization with typical syntactic patterns even if they are derived by powerful methods such as STK. In contrast, partial patterns, such as those provided by PTK applied to constituency (or dependency) trees, prove to be effective. In particular, STK has been considered as the best kernel for exploiting syntactic information in constituency trees, e.g. it is state-of-the-art in: QC (Zhang and Lee, 2003; Moschitti et al., 2007; Moschitti, 2008); SRL, (Moschitti et al., 2008; Moschitti et al., 2005; Che et al., 2006b); pronominal coreference resolution (Yang et al., 2006; Versley et al., 2008) and Relation Extraction (Zhang et al., 2006; Nguyen et al., 2009b). We showed that, in the complex domain of Jeopardy!, STK surprisingly provides low accuracy whereas PTK is rather effective and greatly outperforms STK. We have also provided an explanation of such behavior by means of error analysis: in contrast with traditional question classification, which focuses on basic syntactic patterns (e.g. ”what”, ”where”, ”who” and ”how”). Figure 5 shows that PTK captures partia</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2008</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2008. Tree kernels for semantic role labeling. Computational Linguistics, 34(2):193–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Proceedings of ECML’06,</booktitle>
<pages>318--329</pages>
<contexts>
<context position="4394" citStr="Moschitti, 2006" startWordPosition="671" endWordPosition="672">6). The extensive empirical analysis of these clues often do not indicate an answer type, several advanced models shows that our best model, which is an important feature for identifying cor- which combines different kernels, improves the F1 rect answers in factoid questions (in the examples of our baseline model by 67% relative, from 40.37 above, only (3) provided an answer type, “fund”). to 67.48. Surprisingly, with respect to previous findThird, definition questions are typically shorter in ings on standard QC, e.g. (Zhang and Lee, 2003; length than the average factoid question. These dif- Moschitti, 2006), the Syntactic Tree Kernel (Collins ferences, namely the shorter clue length and the lack and Duffy, 2002) is not effective whereas the exof answer types, make the use of a specialized ma- ploitation of partial tree patterns proves to be eschine learning model potentially promising for im- sential. This is due to the different nature of Jeopproving the overall system accuracy. The first step ardy! questions, which are not expressed in the usual for handling definitions is, of course, the automatic interrogative form. separation of definitions from other question types, To demonstrate the bene</context>
<context position="15972" citStr="Moschitti, 2006" startWordPosition="2535" endWordPosition="2536">nnot be separated. In other words, substruc- node and all the relations with the other nodes are tures are composed by atomic building blocks cor- described by means of the connecting edges. Words responding to nodes along with all their direct chil- are considered additional children of the POS-tag dren. These, in case of a syntactic parse tree, are nodes (in this case the connecting edge just serves complete production rules of the associated parser to add a lexical feature to the target POS-tag node). grammar2. Finally, we also use predicate argument structures • Partial Tree Kernel (PTK) (Moschitti, 2006) ap- generated by verbal and nominal relations accordplied to both constituency and dependency parse ing to PropBank (Palmer et al., 2005) and NomBank trees. This generates all possible tree fragments, as (Meyers et al., 2004). Given the target sentence, the above, but sibling nodes can be separated (so they set of its predicates are extracted and converted into can be part of different tree fragments). In other a forest, then a fake root node, PAS, is used to conwords, a fragment is any possible tree path, from nect these trees. For example, Figure 4 illustrates a whose nodes other tree paths</context>
<context position="23411" citStr="Moschitti, 2006" startWordPosition="3714" endWordPosition="3715">uctures generated by entire production rules. This suggests that PTK, which can capture patterns derived from partial production rules, can be more effective. Indeed, PTK-CT achieves the highest F1, outperforming WSK also when used with a different syntactic paradigm, i.e. PTK-DT. Next, PSK and PASS provide a lower accuracy but they may be useful in kernel combinations as they can complement the information captured by the other models. Interestingly, CSK alone is rather effective for classifying definition questions. We be8Applying it to DT does not make much sense as already pointed out in (Moschitti, 2006). 717 Figure 5: Similarity according to PTK and STK lieve this is because definition questions are sometimes clustered into categories such as 4-LETTER WORDS or BEGINS WITH ”B”. Moreover, we carried out qualitative error analysis on the PTK and STK outcome, which supported our initial hypothesis. Let us consider the bottom tree in Figure 5 in the training set. The top tree is a test example correctly classified by PTK but incorrectly classified by STK. The dashed line in the top tree contains the largest subtree matched by PTK (against the bottom tree), whereas the dashed line in the bottom tr</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceedings of ECML’06, pages 318–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization.</title>
<date>2008</date>
<booktitle>In Proceeding of CIKM ’08,</booktitle>
<location>NY, USA.</location>
<contexts>
<context position="36135" citStr="Moschitti, 2008" startWordPosition="5699" endWordPosition="5701">ive, our paper makes a significant contribution. Second, the experimental results show that the higher variability of Jeopardy! questions prevents us from achieving generalization with typical syntactic patterns even if they are derived by powerful methods such as STK. In contrast, partial patterns, such as those provided by PTK applied to constituency (or dependency) trees, prove to be effective. In particular, STK has been considered as the best kernel for exploiting syntactic information in constituency trees, e.g. it is state-of-the-art in: QC (Zhang and Lee, 2003; Moschitti et al., 2007; Moschitti, 2008); SRL, (Moschitti et al., 2008; Moschitti et al., 2005; Che et al., 2006b); pronominal coreference resolution (Yang et al., 2006; Versley et al., 2008) and Relation Extraction (Zhang et al., 2006; Nguyen et al., 2009b). We showed that, in the complex domain of Jeopardy!, STK surprisingly provides low accuracy whereas PTK is rather effective and greatly outperforms STK. We have also provided an explanation of such behavior by means of error analysis: in contrast with traditional question classification, which focuses on basic syntactic patterns (e.g. ”what”, ”where”, ”who” and ”how”). Figure 5 </context>
</contexts>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti. 2008. Kernel methods, syntax and semantics for relational text categorization. In Proceeding of CIKM ’08, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Convolution kernels on constituent, dependency and sequential structures for relation extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1378--1387</pages>
<location>Singapore,</location>
<contexts>
<context position="34561" citStr="Nguyen et al., 2009" startWordPosition="5457" endWordPosition="5460">tructures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds of syntactic/semantic linguistic structures for the classification of questions in a new application domain, i</context>
<context position="36351" citStr="Nguyen et al., 2009" startWordPosition="5734" endWordPosition="5737">ven if they are derived by powerful methods such as STK. In contrast, partial patterns, such as those provided by PTK applied to constituency (or dependency) trees, prove to be effective. In particular, STK has been considered as the best kernel for exploiting syntactic information in constituency trees, e.g. it is state-of-the-art in: QC (Zhang and Lee, 2003; Moschitti et al., 2007; Moschitti, 2008); SRL, (Moschitti et al., 2008; Moschitti et al., 2005; Che et al., 2006b); pronominal coreference resolution (Yang et al., 2006; Versley et al., 2008) and Relation Extraction (Zhang et al., 2006; Nguyen et al., 2009b). We showed that, in the complex domain of Jeopardy!, STK surprisingly provides low accuracy whereas PTK is rather effective and greatly outperforms STK. We have also provided an explanation of such behavior by means of error analysis: in contrast with traditional question classification, which focuses on basic syntactic patterns (e.g. ”what”, ”where”, ”who” and ”how”). Figure 5 shows that PTK captures partial patterns that are important for more complex questions like those in Jeopardy! Third, we derived other interesting findings for NLP related to this novel domain, e.g.: (i) the impact o</context>
</contexts>
<marker>Nguyen, Moschitti, Riccardi, 2009</marker>
<rawString>Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2009a. Convolution kernels on constituent, dependency and sequential structures for relation extraction. In Proceedings of EMNLP, pages 1378–1387, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Convolution kernels on constituent, dependency and sequential structures for relation extraction.</title>
<date>2009</date>
<booktitle>In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1378--1387</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="34561" citStr="Nguyen et al., 2009" startWordPosition="5457" endWordPosition="5460">tructures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds of syntactic/semantic linguistic structures for the classification of questions in a new application domain, i</context>
<context position="36351" citStr="Nguyen et al., 2009" startWordPosition="5734" endWordPosition="5737">ven if they are derived by powerful methods such as STK. In contrast, partial patterns, such as those provided by PTK applied to constituency (or dependency) trees, prove to be effective. In particular, STK has been considered as the best kernel for exploiting syntactic information in constituency trees, e.g. it is state-of-the-art in: QC (Zhang and Lee, 2003; Moschitti et al., 2007; Moschitti, 2008); SRL, (Moschitti et al., 2008; Moschitti et al., 2005; Che et al., 2006b); pronominal coreference resolution (Yang et al., 2006; Versley et al., 2008) and Relation Extraction (Zhang et al., 2006; Nguyen et al., 2009b). We showed that, in the complex domain of Jeopardy!, STK surprisingly provides low accuracy whereas PTK is rather effective and greatly outperforms STK. We have also provided an explanation of such behavior by means of error analysis: in contrast with traditional question classification, which focuses on basic syntactic patterns (e.g. ”what”, ”where”, ”who” and ”how”). Figure 5 shows that PTK captures partial patterns that are important for more complex questions like those in Jeopardy! Third, we derived other interesting findings for NLP related to this novel domain, e.g.: (i) the impact o</context>
</contexts>
<marker>Nguyen, Moschitti, Riccardi, 2009</marker>
<rawString>Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2009b. Convolution kernels on constituent, dependency and sequential structures for relation extraction. In EMNLP ’09: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378–1387, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="16110" citStr="Palmer et al., 2005" startWordPosition="2555" endWordPosition="2558"> blocks cor- described by means of the connecting edges. Words responding to nodes along with all their direct chil- are considered additional children of the POS-tag dren. These, in case of a syntactic parse tree, are nodes (in this case the connecting edge just serves complete production rules of the associated parser to add a lexical feature to the target POS-tag node). grammar2. Finally, we also use predicate argument structures • Partial Tree Kernel (PTK) (Moschitti, 2006) ap- generated by verbal and nominal relations accordplied to both constituency and dependency parse ing to PropBank (Palmer et al., 2005) and NomBank trees. This generates all possible tree fragments, as (Meyers et al., 2004). Given the target sentence, the above, but sibling nodes can be separated (so they set of its predicates are extracted and converted into can be part of different tree fragments). In other a forest, then a fake root node, PAS, is used to conwords, a fragment is any possible tree path, from nect these trees. For example, Figure 4 illustrates a whose nodes other tree paths can depart. Conse- Predicate Argument Structures Set (PASS) encoding quently, an extremely rich feature space is gener- two relations, gi</context>
<context position="19359" citStr="Palmer et al., 2005" startWordPosition="3035" endWordPosition="3038">jj][nn][in] TK software4, which includes structural kernels in [dt][nn] SVMLight (Joachims, 1999)5. For generating conAdditionally, we use constituency trees (CTs), see 3Past Jeopardy! games can be downloaded from http://www.j-archive.com. 4Available at http://dit.unitn.it/∼moschitt 5http://svmlight.joachims.org 2From here the name syntactic tree kernels 716 stituency trees, we used the Charniak parser (Charniak, 2000). We also used the syntactic–semantic parser by Johansson and Nugues (2008) to generate dependency trees (Mel’ˇcuk, 1988) and predicate argument trees according to the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks. Baseline Model: the first model that we used as a baseline is a rule-based classifier (RBC). The RBC leverages a set of rules that matches against lexical and syntactic information in the clue to make a binary decision on whether or not the clue is considered definitional. The rule set was manually developed by a human expert, and consists of rules that attempt to identify roughly 70 different constructs in the clues. For instance, one of the rules matches the parse tree structure for ”It’s X or Y”, which will identify example (1) as a definition </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yutaka Sasaki</author>
<author>Chuan-Jie Lin</author>
<author>Kuang-hua Chen</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Overview of the NTCIR-6 cross-lingual question answering (CLQA) task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th NTCIR Workshop on Evaluation of Information Access Technologies.</booktitle>
<contexts>
<context position="1687" citStr="Sasaki et al., 2007" startWordPosition="240" endWordPosition="243">IBM Jeopardy! system. Our experiments measuring their impact on Watson show enhancements in QA accuracy and a consequent increase in the amount of money earned in game-based evaluation. 1 Introduction Question Answering (QA) is an important research area of Information Retrieval applications, which requires the use of core NLP capabilities, such as syntactic and semantic processing for a more effective user experience. While the development of most existing QA systems are driven by organized evaluation efforts such as TREC (Voorhees and Dang, 2006), CLEF (Giampiccolo et al., 2007), and NTCIR (Sasaki et al., 2007), there exist efforts that leverage data from popular quiz shows, such as Who Wants to be a Millionaire (Clarke et al., 2001; Lam et al., 2003) and Jeopardy! (Ferrucci et al., 2010), to demonstrate the generality of the technology. Jeopardy! is a popular quiz show in the US which has been on the air for 27 years. In each game, three contestants compete for the opportunity to answer 60 questions in 12 categories of 5 questions each. Jeopardy! questions cover an incredibly broad domain, from science, literature, history, to popular culture. We are drawn to Jeopardy! as a test bed for open-domain</context>
</contexts>
<marker>Sasaki, Lin, Chen, Chen, 2007</marker>
<rawString>Yutaka Sasaki, Chuan-Jie Lin, Kuang-hua Chen, and Hsin-Hsi Chen. 2007. Overview of the NTCIR-6 cross-lingual question answering (CLQA) task. In Proceedings of the 6th NTCIR Workshop on Evaluation of Information Access Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Sasaki</author>
</authors>
<title>Question answering as question-biased term extraction: A new approach toward multilingual qa.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>215--222</pages>
<contexts>
<context position="33101" citStr="Sasaki, 2005" startWordPosition="5224" endWordPosition="5225"> account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the state-of-the-art in QC by applying SVMs along with STK-CT. The results were derived by experimenting with a TREC dataset11(Li and Roth, 2002), reaching an accuracy of 91.8%. However, such data refers to typical instances from QA, whose syntactic patterns can be easily </context>
</contexts>
<marker>Sasaki, 2005</marker>
<rawString>Y. Sasaki. 2005. Question answering as question-biased term extraction: A new approach toward multilingual qa. In Proceedings of ACL, pages 215–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>LaTeX User’s Guide and Document Reference Manual. Kernel Methods for Pattern Analysis,</title>
<date>2004</date>
<publisher>University Press.</publisher>
<location>Cambridge</location>
<contexts>
<context position="3710" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="565" endWordPosition="568">year”, it’s an investment or retirement fund that pays out yearly (answer: an annuity) where the upper case text indicates the Jeopardy! category for each question1. Several characteristics of this class of questions warrant special processing: first, the clue (question) 1A Jeopardy! category indicates a theme is common among its 5 questions. 712 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 712–724, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics often aligns well with dictionary entries, making 2002; Shawe-Taylor and Cristianini, 2004; Mosdictionary resources potentially effective. Second, chitti, 2006). The extensive empirical analysis of these clues often do not indicate an answer type, several advanced models shows that our best model, which is an important feature for identifying cor- which combines different kernels, improves the F1 rect answers in factoid questions (in the examples of our baseline model by 67% relative, from 40.37 above, only (3) provided an answer type, “fund”). to 67.48. Surprisingly, with respect to previous findThird, definition questions are typically shorter in ings on standard QC, e.g. (Zhang </context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. LaTeX User’s Guide and Document Reference Manual. Kernel Methods for Pattern Analysis, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>M Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="32832" citStr="Shen and Lapata, 2007" startWordPosition="5175" endWordPosition="5178">rk Our paper studies the use of advanced representation for QC in the Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the </context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>D. Shen and M. Lapata. 2007. Using semantic roles to improve question answering. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A Sarkar</author>
<author>A Joshi</author>
</authors>
<title>Using LTAG Based Features in Parse Reranking.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="34240" citStr="Shen et al., 2003" startWordPosition="5406" endWordPosition="5409">a refers to typical instances from QA, whose syntactic patterns can be easily generalized by STK. In contrast, we have shown that STK-CT is not effective for our domain, as it presents very innovative elements: questions in affirmative and highly variable format. Thus, we employed new methods such as PTK, dependency structures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of</context>
</contexts>
<marker>Shen, Sarkar, Joshi, 2003</marker>
<rawString>L. Shen, A. Sarkar, and A. Joshi. 2003. Using LTAG Based Features in Parse Reranking. In Proceedings of EMNLP, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Small</author>
<author>T Strzalkowski</author>
<author>T Liu</author>
<author>S Ryan</author>
<author>R Salkin</author>
<author>N Shimizu</author>
<author>P Kantor</author>
<author>D Kelly</author>
<author>N Wacholder</author>
</authors>
<title>Hitiqa: Towards analytical question answering.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="32653" citStr="Small et al., 2004" startWordPosition="5148" endWordPosition="5151"> the other hand, the StatDef system outperformed the two other systems, and its accuracy improvement upon the RuleDef system is statistically significant at p&lt;0.05. 6 Related Work Our paper studies the use of advanced representation for QC in the Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata,</context>
</contexts>
<marker>Small, Strzalkowski, Liu, Ryan, Salkin, Shimizu, Kantor, Kelly, Wacholder, 2004</marker>
<rawString>S. Small, T. Strzalkowski, T. Liu, S. Ryan, R. Salkin, N. Shimizu, P. Kantor, D. Kelly, and N. Wacholder. 2004. Hitiqa: Towards analytical question answering. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>M Ciaramita</author>
<author>H Zaragoza</author>
</authors>
<title>Learning to rank answers on large online QA collections.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="32901" citStr="Surdeanu et al., 2008" startWordPosition="5187" endWordPosition="5190"> Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the state-of-the-art in QC by applying SVMs along with STK-CT. The result</context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2008</marker>
<rawString>M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008. Learning to rank answers on large online QA collections. In Proceedings of ACL-HLT, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>Y Sasaki</author>
<author>E Maeda</author>
</authors>
<title>Svm answer selection for open-domain question answering.</title>
<date>2002</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>974--980</pages>
<contexts>
<context position="33123" citStr="Suzuki et al., 2002" startWordPosition="5226" endWordPosition="5229">ous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the state-of-the-art in QC by applying SVMs along with STK-CT. The results were derived by experimenting with a TREC dataset11(Li and Roth, 2002), reaching an accuracy of 91.8%. However, such data refers to typical instances from QA, whose syntactic patterns can be easily generalized by STK. In</context>
</contexts>
<marker>Suzuki, Sasaki, Maeda, 2002</marker>
<rawString>J. Suzuki, Y. Sasaki, and E. Maeda. 2002. Svm answer selection for open-domain question answering. In Proceedings of Coling, pages 974–980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hirotoshi Taira</author>
<author>Yutaka Sasaki</author>
<author>Eisaku Maeda</author>
</authors>
<title>Question classification using hdag kernel.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 Workshop on Multilingual Summarization and Question Answering,</booktitle>
<pages>61--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="7120" citStr="Suzuki et al., 2003" startWordPosition="1103" endWordPosition="1106">s section gives a quick overview of Watson and approaches, e.g. (Ahn et al., 2004; Kaisser and Web- the problem of classification of definition questions, ber, 2007; Blunsom et al., 2006). Given the com- which is the focus of this paper. plexity and the novelty of the task, we found it use- 2.1 Overview ful to exploit the kernel methods technology. This Watson is a massively parallel probabilistic has shown state-of-the-art performance in Question evidence-based architecture for QA (Ferrucci et Classification (QC), e.g. (Zhang and Lee, 2003; al., 2010). It consists of several major stages for Suzuki et al., 2003; Moschitti et al., 2007) and it underlying sub-tasks, including analysis of the is very well suited for engineering feature represen- question, retrieval of relevant content, scoring and tations for novel tasks. ranking of candidate answers, as depicted in Figure In this paper, we apply SVMs and kernel meth- 1. In the rest of this section, we provide an overview ods to syntactic/semantic structures for modeling of Watson, focusing on the task of answering accurate classification of Jeopardy! definition ques- definitional questions. tions. For this purpose, we use several levels of lin- Questi</context>
<context position="12723" citStr="Suzuki et al., 2003" startWordPosition="1970" endWordPosition="1973">arch processes targeted towards dictionary-like sources in our system. We use a variety of such sources, such as standard English dictionaries, Wiktionary, WordNet, etc. After gathering supporting evidence for candidate answers extracted from these sources, our system routes the candidates to definition-specific candidate ranking models, which have been trained with selected feature sets. The following sections present a description and evaluation of our approach for identifying and answering definition questions. 3 Kernel Models for Question Classification Previous work (Zhang and Lee, 2003; Suzuki et al., 2003; Blunsom et al., 2006; Moschitti et al., 2007) as shown that syntactic structures are essential for QC. Given the novelty of both the domain and the type of our classification items, we rely on kernel methods to study and design effective representations. Indeed, these are excellent tools for automatic feature engineering, especially for unknown tasks and domains. Our approach consists of using SVMs and kernels for structured data applied to several types of structural lexical, syntactic and shallow semantic information. 3.1 Tree and Sequence Kernels Kernel functions are implicit scalar produ</context>
</contexts>
<marker>Suzuki, Taira, Sasaki, Maeda, 2003</marker>
<rawString>Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku Maeda. 2003. Question classification using hdag kernel. In Proceedings of the ACL 2003 Workshop on Multilingual Summarization and Question Answering, pages 61–68, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Porting statistical parsers with data-defined kernels.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X.</booktitle>
<contexts>
<context position="34293" citStr="Titov and Henderson, 2006" startWordPosition="5414" endWordPosition="5417">syntactic patterns can be easily generalized by STK. In contrast, we have shown that STK-CT is not effective for our domain, as it presents very innovative elements: questions in affirmative and highly variable format. Thus, we employed new methods such as PTK, dependency structures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very comple</context>
</contexts>
<marker>Titov, Henderson, 2006</marker>
<rawString>Ivan Titov and James Henderson. 2006. Porting statistical parsers with data-defined kernels. In Proceedings of CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Penka Markova</author>
<author>Christopher Manning</author>
</authors>
<title>The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="34318" citStr="Toutanova et al., 2004" startWordPosition="5418" endWordPosition="5421">asily generalized by STK. In contrast, we have shown that STK-CT is not effective for our domain, as it presents very innovative elements: questions in affirmative and highly variable format. Thus, we employed new methods such as PTK, dependency structures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy!</context>
</contexts>
<marker>Toutanova, Markova, Manning, 2004</marker>
<rawString>Kristina Toutanova, Penka Markova, and Christopher Manning. 2004. The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
<author>Alessandro Moschitti</author>
<author>Massimo Poesio</author>
<author>Xiaofeng Yang</author>
</authors>
<title>Coreference systems based on kernels methods.</title>
<date>2008</date>
<booktitle>In The 22nd International Conference on Computational Linguistics (Coling’08),</booktitle>
<location>Manchester, England.</location>
<contexts>
<context position="36286" citStr="Versley et al., 2008" startWordPosition="5723" endWordPosition="5726"> us from achieving generalization with typical syntactic patterns even if they are derived by powerful methods such as STK. In contrast, partial patterns, such as those provided by PTK applied to constituency (or dependency) trees, prove to be effective. In particular, STK has been considered as the best kernel for exploiting syntactic information in constituency trees, e.g. it is state-of-the-art in: QC (Zhang and Lee, 2003; Moschitti et al., 2007; Moschitti, 2008); SRL, (Moschitti et al., 2008; Moschitti et al., 2005; Che et al., 2006b); pronominal coreference resolution (Yang et al., 2006; Versley et al., 2008) and Relation Extraction (Zhang et al., 2006; Nguyen et al., 2009b). We showed that, in the complex domain of Jeopardy!, STK surprisingly provides low accuracy whereas PTK is rather effective and greatly outperforms STK. We have also provided an explanation of such behavior by means of error analysis: in contrast with traditional question classification, which focuses on basic syntactic patterns (e.g. ”what”, ”where”, ”who” and ”how”). Figure 5 shows that PTK captures partial patterns that are important for more complex questions like those in Jeopardy! Third, we derived other interesting find</context>
</contexts>
<marker>Versley, Moschitti, Poesio, Yang, 2008</marker>
<rawString>Yannick Versley, Alessandro Moschitti, Massimo Poesio, and Xiaofeng Yang. 2008. Coreference systems based on kernels methods. In The 22nd International Conference on Computational Linguistics (Coling’08), Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Hoa Trang Dang</author>
</authors>
<title>question answering track.</title>
<date>2006</date>
<journal>Overview of the TREC</journal>
<booktitle>In Proceedings of the TREC 2005 Conference.</booktitle>
<contexts>
<context position="1621" citStr="Voorhees and Dang, 2006" startWordPosition="228" endWordPosition="231">lso used in the state-of-the-art QA pipeline constituting Watson, the IBM Jeopardy! system. Our experiments measuring their impact on Watson show enhancements in QA accuracy and a consequent increase in the amount of money earned in game-based evaluation. 1 Introduction Question Answering (QA) is an important research area of Information Retrieval applications, which requires the use of core NLP capabilities, such as syntactic and semantic processing for a more effective user experience. While the development of most existing QA systems are driven by organized evaluation efforts such as TREC (Voorhees and Dang, 2006), CLEF (Giampiccolo et al., 2007), and NTCIR (Sasaki et al., 2007), there exist efforts that leverage data from popular quiz shows, such as Who Wants to be a Millionaire (Clarke et al., 2001; Lam et al., 2003) and Jeopardy! (Ferrucci et al., 2010), to demonstrate the generality of the technology. Jeopardy! is a popular quiz show in the US which has been on the air for 27 years. In each game, three contestants compete for the opportunity to answer 60 questions in 12 categories of 5 questions each. Jeopardy! questions cover an incredibly broad domain, from science, literature, history, to popula</context>
</contexts>
<marker>Voorhees, Dang, 2006</marker>
<rawString>Ellen M. Voorhees and Hoa Trang Dang. 2006. Overview of the TREC 2005 question answering track. In Proceedings of the TREC 2005 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the trec 2004 question answering track.</title>
<date>2004</date>
<booktitle>In Proceedings of TREC</booktitle>
<contexts>
<context position="32632" citStr="Voorhees, 2004" startWordPosition="5146" endWordPosition="5147">NoDef system. On the other hand, the StatDef system outperformed the two other systems, and its accuracy improvement upon the RuleDef system is statistically significant at p&lt;0.05. 6 Related Work Our paper studies the use of advanced representation for QC in the Jeopardy! domain. As previously mentioned Jeopardy! questions are stated as affirmative sentences, which are different from the typical QA questions. For the design of our models, we have carefully taken into account previous work. This shows that semantics and syntax are essential to retrieve precise answers, e.g (Hickl et al., 2006; Voorhees, 2004; Small et al., 2004). We focus on definition questions, which typically require more complex processing than factoid questions (Blair-Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for rerank</context>
</contexts>
<marker>Voorhees, 2004</marker>
<rawString>E. M. Voorhees. 2004. Overview of the trec 2004 question answering track. In Proceedings of TREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Chewlim Tan</author>
</authors>
<title>Kernel-based pronoun resolution with structured syntactic knowledge.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL 06.</booktitle>
<contexts>
<context position="36263" citStr="Yang et al., 2006" startWordPosition="5719" endWordPosition="5722"> questions prevents us from achieving generalization with typical syntactic patterns even if they are derived by powerful methods such as STK. In contrast, partial patterns, such as those provided by PTK applied to constituency (or dependency) trees, prove to be effective. In particular, STK has been considered as the best kernel for exploiting syntactic information in constituency trees, e.g. it is state-of-the-art in: QC (Zhang and Lee, 2003; Moschitti et al., 2007; Moschitti, 2008); SRL, (Moschitti et al., 2008; Moschitti et al., 2005; Che et al., 2006b); pronominal coreference resolution (Yang et al., 2006; Versley et al., 2008) and Relation Extraction (Zhang et al., 2006; Nguyen et al., 2009b). We showed that, in the complex domain of Jeopardy!, STK surprisingly provides low accuracy whereas PTK is rather effective and greatly outperforms STK. We have also provided an explanation of such behavior by means of error analysis: in contrast with traditional question classification, which focuses on basic syntactic patterns (e.g. ”what”, ”where”, ”who” and ”how”). Figure 5 shows that PTK captures partial patterns that are important for more complex questions like those in Jeopardy! Third, we derived</context>
</contexts>
<marker>Yang, Su, Tan, 2006</marker>
<rawString>Xiaofeng Yang, Jian Su, and Chewlim Tan. 2006. Kernel-based pronoun resolution with structured syntactic knowledge. In Proc. COLING-ACL 06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-ACL,</booktitle>
<pages>181--201</pages>
<contexts>
<context position="34451" citStr="Zelenko et al., 2002" startWordPosition="5439" endWordPosition="5442">s: questions in affirmative and highly variable format. Thus, we employed new methods such as PTK, dependency structures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds </context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2002</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2002. Kernel methods for relation extraction. In Proceedings of EMNLP-ACL, pages 181–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>Wee Sun Lee</author>
</authors>
<title>Question classification using support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>26--32</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="4323" citStr="Zhang and Lee, 2003" startWordPosition="659" endWordPosition="662">, 2004; Mosdictionary resources potentially effective. Second, chitti, 2006). The extensive empirical analysis of these clues often do not indicate an answer type, several advanced models shows that our best model, which is an important feature for identifying cor- which combines different kernels, improves the F1 rect answers in factoid questions (in the examples of our baseline model by 67% relative, from 40.37 above, only (3) provided an answer type, “fund”). to 67.48. Surprisingly, with respect to previous findThird, definition questions are typically shorter in ings on standard QC, e.g. (Zhang and Lee, 2003; length than the average factoid question. These dif- Moschitti, 2006), the Syntactic Tree Kernel (Collins ferences, namely the shorter clue length and the lack and Duffy, 2002) is not effective whereas the exof answer types, make the use of a specialized ma- ploitation of partial tree patterns proves to be eschine learning model potentially promising for im- sential. This is due to the different nature of Jeopproving the overall system accuracy. The first step ardy! questions, which are not expressed in the usual for handling definitions is, of course, the automatic interrogative form. separ</context>
<context position="7047" citStr="Zhang and Lee, 2003" startWordPosition="1090" endWordPosition="1093">opardy! System acterize definition/description questions, or previous This section gives a quick overview of Watson and approaches, e.g. (Ahn et al., 2004; Kaisser and Web- the problem of classification of definition questions, ber, 2007; Blunsom et al., 2006). Given the com- which is the focus of this paper. plexity and the novelty of the task, we found it use- 2.1 Overview ful to exploit the kernel methods technology. This Watson is a massively parallel probabilistic has shown state-of-the-art performance in Question evidence-based architecture for QA (Ferrucci et Classification (QC), e.g. (Zhang and Lee, 2003; al., 2010). It consists of several major stages for Suzuki et al., 2003; Moschitti et al., 2007) and it underlying sub-tasks, including analysis of the is very well suited for engineering feature represen- question, retrieval of relevant content, scoring and tations for novel tasks. ranking of candidate answers, as depicted in Figure In this paper, we apply SVMs and kernel meth- 1. In the rest of this section, we provide an overview ods to syntactic/semantic structures for modeling of Watson, focusing on the task of answering accurate classification of Jeopardy! definition ques- definitional</context>
<context position="12702" citStr="Zhang and Lee, 2003" startWordPosition="1966" endWordPosition="1969">n questions invoke search processes targeted towards dictionary-like sources in our system. We use a variety of such sources, such as standard English dictionaries, Wiktionary, WordNet, etc. After gathering supporting evidence for candidate answers extracted from these sources, our system routes the candidates to definition-specific candidate ranking models, which have been trained with selected feature sets. The following sections present a description and evaluation of our approach for identifying and answering definition questions. 3 Kernel Models for Question Classification Previous work (Zhang and Lee, 2003; Suzuki et al., 2003; Blunsom et al., 2006; Moschitti et al., 2007) as shown that syntactic structures are essential for QC. Given the novelty of both the domain and the type of our classification items, we rely on kernel methods to study and design effective representations. Indeed, these are excellent tools for automatic feature engineering, especially for unknown tasks and domains. Our approach consists of using SVMs and kernels for structured data applied to several types of structural lexical, syntactic and shallow semantic information. 3.1 Tree and Sequence Kernels Kernel functions are </context>
<context position="22405" citStr="Zhang and Lee, 2003" startWordPosition="3556" endWordPosition="3559">, these results validate the complexity of the task: in order to capture the large variability of the positive examples, the rules developed by a skilled human designer are unable to be sufficiently precise to limit the recognition to those examples. On the other hand, RBC, being a rather different approach from SVMs, can be successfully exploited in a joint model with them. Second, BOW yields better F1 than RBC but it does not generalize well since its F1 is still low. When n-grams are also added to the model by means of WSK, the F1 improves by about 1.5 absolute points. As already shown in (Zhang and Lee, 2003; Moschitti et al., 2007), syntactic structures are needed to improve generalization. Third, surprisingly with respect to previous work, STK applied to CT8 provides accuracy lower than BOW, about 8 absolute points. The reason is due to the different nature of the Jeopardy! questions: large syntactic variability reduces the probability of finding general and well formed patterns, i.e. structures generated by entire production rules. This suggests that PTK, which can capture patterns derived from partial production rules, can be more effective. Indeed, PTK-CT achieves the highest F1, outperformi</context>
<context position="33386" citStr="Zhang and Lee, 2003" startWordPosition="5269" endWordPosition="5272">Goldensohn et al., 2004; Chen et al., 2006; Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Echihabi and Marcu, 2003). For example, language models were applied to definitional QA in (Cui et al., 2005) to learn soft pattern models based on bigrams. Other related work, such as (Sasaki, 2005; Suzuki et al., 2002), was also very tied to bag-of-words features. Predicate argument structures have been mainly used for reranking (Shen and Lapata, 2007; Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu et al., 2008). Our work and methods are similar to (Zhang and Lee, 2003; Moschitti et al., 2007), which achieved the state-of-the-art in QC by applying SVMs along with STK-CT. The results were derived by experimenting with a TREC dataset11(Li and Roth, 2002), reaching an accuracy of 91.8%. However, such data refers to typical instances from QA, whose syntactic patterns can be easily generalized by STK. In contrast, we have shown that STK-CT is not effective for our domain, as it presents very innovative elements: questions in affirmative and highly variable format. Thus, we employed new methods such as PTK, dependency structures, multiple sequence kernels includi</context>
<context position="36093" citStr="Zhang and Lee, 2003" startWordPosition="5691" endWordPosition="5694">ons play a major role and, from this perspective, our paper makes a significant contribution. Second, the experimental results show that the higher variability of Jeopardy! questions prevents us from achieving generalization with typical syntactic patterns even if they are derived by powerful methods such as STK. In contrast, partial patterns, such as those provided by PTK applied to constituency (or dependency) trees, prove to be effective. In particular, STK has been considered as the best kernel for exploiting syntactic information in constituency trees, e.g. it is state-of-the-art in: QC (Zhang and Lee, 2003; Moschitti et al., 2007; Moschitti, 2008); SRL, (Moschitti et al., 2008; Moschitti et al., 2005; Che et al., 2006b); pronominal coreference resolution (Yang et al., 2006; Versley et al., 2008) and Relation Extraction (Zhang et al., 2006; Nguyen et al., 2009b). We showed that, in the complex domain of Jeopardy!, STK surprisingly provides low accuracy whereas PTK is rather effective and greatly outperforms STK. We have also provided an explanation of such behavior by means of error analysis: in contrast with traditional question classification, which focuses on basic syntactic patterns (e.g. ”w</context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>Dell Zhang and Wee Sun Lee. 2003. Question classification using support vector machines. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 26–32. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jian Su</author>
<author>Danmei Wang</author>
<author>Guodong Zhou</author>
<author>Chew Lim Tan</author>
</authors>
<title>Discovering relations between named entities from a large raw corpus using tree similarity-based clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP’2005, Lecture Notes in Computer Science (LNCS 3651),</booktitle>
<pages>378--389</pages>
<location>Jeju Island, South</location>
<contexts>
<context position="34525" citStr="Zhang et al., 2005" startWordPosition="5451" endWordPosition="5454">w methods such as PTK, dependency structures, multiple sequence kernels including category information and many combinations. Regarding the use of Kernel Methods, there is a considerably large body of work in Natural Language Processing, e.g. regarding syntactic parsing (Collins and Duffy, 2002; Kudo et al., 2005; Shen et al., 2003; Kudo and Matsumoto, 2003; Titov and Henderson, 2006; Toutanova et al., 2004), named entity recognition and chunking (Cumby and Roth, 2003; Daum´e III and Marcu, 2004), relation extraction (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2005; Bunescu, 2007; Nguyen et al., 2009a), text categorization (Cancedda et al., 2003), word sense disambiguation (Gliozzo et al., 2005) and semantic role labeling (SRL), e.g. (Kazama and Torisawa, 2005; Che et al., 2006a; Moschitti et al., 2008). However, ours is the first study on the use of several combinations of kernels applied to several structures on very complex data from the Jeopardy! domain. 7 Final Remarks and Conclusion In this paper we have experimented with advanced structural kernels applied to several kinds of syntactic/semantic linguistic structures for the classification of ques</context>
</contexts>
<marker>Zhang, Su, Wang, Zhou, Tan, 2005</marker>
<rawString>Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and Chew Lim Tan. 2005. Discovering relations between named entities from a large raw corpus using tree similarity-based clustering. In Proceedings of IJCNLP’2005, Lecture Notes in Computer Science (LNCS 3651), pages 378–389, Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
</authors>
<title>Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="36330" citStr="Zhang et al., 2006" startWordPosition="5730" endWordPosition="5733">syntactic patterns even if they are derived by powerful methods such as STK. In contrast, partial patterns, such as those provided by PTK applied to constituency (or dependency) trees, prove to be effective. In particular, STK has been considered as the best kernel for exploiting syntactic information in constituency trees, e.g. it is state-of-the-art in: QC (Zhang and Lee, 2003; Moschitti et al., 2007; Moschitti, 2008); SRL, (Moschitti et al., 2008; Moschitti et al., 2005; Che et al., 2006b); pronominal coreference resolution (Yang et al., 2006; Versley et al., 2008) and Relation Extraction (Zhang et al., 2006; Nguyen et al., 2009b). We showed that, in the complex domain of Jeopardy!, STK surprisingly provides low accuracy whereas PTK is rather effective and greatly outperforms STK. We have also provided an explanation of such behavior by means of error analysis: in contrast with traditional question classification, which focuses on basic syntactic patterns (e.g. ”what”, ”where”, ”who” and ”how”). Figure 5 shows that PTK captures partial patterns that are important for more complex questions like those in Jeopardy! Third, we derived other interesting findings for NLP related to this novel domain, e</context>
</contexts>
<marker>Zhang, Zhang, Su, 2006</marker>
<rawString>Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel. In Proceedings of NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>