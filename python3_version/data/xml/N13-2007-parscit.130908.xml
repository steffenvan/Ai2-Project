<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011279">
<title confidence="0.965018">
Reversing Morphological Tokenization in English-to-Arabic SMT
</title>
<author confidence="0.987102">
Mohammad Salameh† Colin Cherry‡ Grzegorz Kondrak†
</author>
<affiliation confidence="0.9947005">
†Department of Computing Science ‡National Research Council Canada
University of Alberta 1200 Montreal Road
</affiliation>
<address confidence="0.862297">
Edmonton, AB, T6G 2E8, Canada Ottawa, ON, K1A 0R6, Canada
</address>
<email confidence="0.993028">
{msalameh,gkondrak}@ualberta.ca Colin.Cherry@nrc-cnrc.gc.ca
</email>
<sectionHeader confidence="0.995541" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999584523809524">
Morphological tokenization has been used
in machine translation for morphologically
complex languages to reduce lexical sparsity.
Unfortunately, when translating into a mor-
phologically complex language, recombining
segmented tokens to generate original word
forms is not a trivial task, due to morpho-
logical, phonological and orthographic adjust-
ments that occur during tokenization. We re-
view a number of detokenization schemes for
Arabic, such as rule-based and table-based ap-
proaches and show their limitations. We then
propose a novel detokenization scheme that
uses a character-level discriminative string
transducer to predict the original form of a
segmented word. In a comparison to a state-
of-the-art approach, we demonstrate slightly
better detokenization error rates, without the
need for any hand-crafted rules. We also
demonstrate the effectiveness of our approach
in an English-to-Arabic translation task.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995268909090909">
Statistical machine translation (SMT) relies on to-
kenization to split sentences into meaningful units
for easy processing. For morphologically complex
languages, such as Arabic or Turkish, this may in-
volve splitting words into morphemes. Through-
out this paper, we adopt the definition of tokeniza-
tion proposed by Habash (2010), which incorpo-
rates both morphological segmentation as well as
orthographic character transformations. To use an
English example, the word tries would be morpho-
logically tokenized as “try + s”, which involves
</bodyText>
<page confidence="0.989832">
47
</page>
<bodyText confidence="0.999977764705883">
orthographic changes at morpheme boundaries to
match the lexical form of each token. When trans-
lating into a tokenized language, the tokenization
must be reversed to make the generated text read-
able and evaluable. Detokenization is the process
of converting tokenized words into their original or-
thographically and morphologically correct surface
form. This includes concatenating tokens into com-
plete words and reversing any character transforma-
tions that may have taken place.
For languages like Arabic, tokenization can facil-
itate SMT by reducing lexical sparsity. Figure 1
shows how the morphological tokenization of the
Arabic word ��������.3 “and he will prevent them”
simplifies the correspondence between Arabic and
English tokens, which in turn can improve the qual-
ity of word alignment, rule extraction and decoding.
When translating from Arabic into English, the to-
kenization is a form of preprocessing, and the out-
put translation is readable, space-separated English.
However, when translating from English to Arabic,
the output will be in a tokenized form, which cannot
be compared to the original reference without detok-
enization. Simply concatenating the tokenized mor-
phemes cannot fully reverse this process, because of
character transformations that occurred during tok-
enization.
The techniques that have been proposed for the
detokenization task fall into three categories (Badr
et al., 2008). The simplest detokenization approach
concatenates morphemes based on token markers
without any adjustment. Table-based detokenization
maps tokenized words into their surface form with a
look-up table built by observing the tokenizer’s in-
</bodyText>
<note confidence="0.5518975">
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 47–53,
Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.998835">
Figure 1: Alignment between tokenized form of
“wsymnςhm” mow&apos; and its English translation.
</figureCaption>
<bodyText confidence="0.999853166666667">
put and output on large amounts of text. Rule-based
detokenization relies on hand-built rules or regular
expressions to convert the segmented form into the
original surface form. Other techniques use combi-
nations of these approaches. Each approach has its
limitations: rule-based approaches are language spe-
cific and brittle, while table-based approaches fail to
deal with sequences outside of their tables.
We present a new detokenization approach that
applies a discriminative sequence model to predict
the original form of the tokenized word. Like
table-based approaches, our sequence model re-
quires large amounts of tokenizer input-output pairs;
but instead of building a table, we use these pairs
as training data. By using features that consider
large windows of within-word input context, we are
able to intelligently transition between rule-like and
table-like behavior.
Our experimental results on Arabic text demon-
strate an improvement in terms of sentence error
rate1 of 11.9 points over a rule-based approach, and
1.1 points over a table-based approach that backs
off to rules. More importantly, we achieve a slight
improvement over the state-of-the-art approach of
El Kholy and Habash (2012), which combines rules
and tables, using a 5-gram language model to dis-
ambiguate conflicting table entries. In addition, our
detokenization method results in a small BLEU im-
provement over a rule-based approach when applied
to English-to-Arabic SMT.
</bodyText>
<footnote confidence="0.945805">
1Sentence error rate is the percentage of sentences contain-
ing at least one error after detokenization.
</footnote>
<sectionHeader confidence="0.915898" genericHeader="introduction">
2 Arabic Morphology
</sectionHeader>
<bodyText confidence="0.963260611111111">
Compared to English, Arabic has rich and complex
morphology. Arabic base words inflect to eight fea-
tures. Verbs inflect for aspect, mood, person and
voice. Nouns and adjectives inflect for case and
state. Verbs, nouns and adjectives inflect for both
gender and number. Furthermore, inflected base
words can attract various optional clitics. Clitical
prefixes include determiners, particle proclitics, con-
junctions and question particles in strict order. Clit-
ical suffixes include pronominal modifiers. As a re-
sult of clitic attachment, morpho-syntactic interac-
tions sometimes cause changes in spelling or pro-
nunciations.
Several tokenization schemes can be defined for
Arabic, depending on the clitical level that the to-
kenization is applied to. In this paper, we use
Penn Arabic Treebank (PATB) tokenization scheme,
which El Kholy and Habash (2012) report as pro-
ducing the best results for Arabic SMT. The PATB
scheme detaches all clitics except for the definite ar-
ticle Al J. Multiple prefix clitics are treated as one
token.
Some Arabic letters present further ambiguity in
text.2 For example, the different forms of Hamzated
�
Alif “h h�” are usually written without the Hamza “�”.
Likewise, when the letter Ya ’Y’ S is present at the
end of the word, it is sometimes written in the form
of “Alif Maqsura” letter ’ý’ S. Also, short vow-
els in Arabic are represented using diacritics, which
are usually absent in written text. In order to deal
with these ambiguities in SMT, normalization is of-
ten performed as a preprocessing step, which usu-
ally involves converting different forms of Alif and
Ya to a single form. This decreases Arabic’s lexical
sparsity and improves SMT performance.
</bodyText>
<sectionHeader confidence="0.999934" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999903833333333">
Sadat and Habash (2006) address the issue of lex-
ical sparsity by presenting different preprocessing
schemes for Arabic-to-English SMT. The schemes
include simple tokenization, orthographic normal-
ization, and decliticization. The combination of
these schemes results in improved translation out-
</bodyText>
<footnote confidence="0.728391">
2We use Habash-Soudi-Buckwalter transliteration scheme
(Habash, 2007) for all Arabic examples.
</footnote>
<page confidence="0.998202">
48
</page>
<bodyText confidence="0.99994644">
put. This is one of many studies on normalization
and tokenization for translation from Arabic, which
we will not attempt to review completely here.
Badr et al. (2008) show that tokenizing Arabic
also has a positive influence on English-to-Arabic
SMT. They apply two tokenization schemes on
Arabic text, and introduce detokenization schemes
through a rule-based approach, a table-based ap-
proach, and a combination of both. The combina-
tion approach detokenizes words first using the ta-
ble, falling back on rules for sequences not found in
the table.
El Kholy and Habash (2012) extend Badr’s work
by presenting a larger number of tokenization and
detokenization schemes, and comparing their effects
on SMT. They introduce an additional detokeniza-
tion schemes based on the SRILM disambig util-
ity (Stolcke, 2002), which utilizes a 5-gram untok-
enized language model to decide among different al-
ternatives found in the table. They test their schemes
on naturally occurring Arabic text and SMT output.
Their newly introduced detokenization scheme out-
performs the rule-based and table-based approaches
introduced by Badr et al. (2008), establishing the
current state-of-the-art.
</bodyText>
<subsectionHeader confidence="0.999751">
3.1 Detokenization Schemes in Detail
</subsectionHeader>
<bodyText confidence="0.99429245">
Rule-based detokenization involves manually defin-
ing a set of transformation rules to convert a se-
quence of segmented tokens into their surface form.
For example, the noun “llrˆyys”���;1L� “to the pres-
ident” is tokenized as ”l+ Alrˆyys” ( l+ “to” Alrˆyys
“the president”) in the PATB tokenization scheme.
Note that the definite article “Al” JI is kept attached
to the noun. In this case, detokenization requires
a character-level transformation after concatenation,
which we can generalize using the rule:
l+Al → ll.
Table 1 shows the rules provided by El Kholy and
Habash (2012), which we employ throughout this
paper.
There are two principal problems with the rule-
based approach. First, rules fail to account for un-
usual cases. For example, the above rule mishandles
cases where “Al” JI is a basic part of the stem and
not the definite article “the”. Thus, ’l+ AlςAb’ (l+
“to” AlςAb “games”) is erroneously detokenized to
</bodyText>
<equation confidence="0.9581795">
Rule Input Output
l+Al+l? → ll l+ Alrˆyys llrˆyys
h+(pron) → t(pron) Abnh+hA AbnthA
y+(pron) → A(pron) Alqy+h AlqAh
’+(pron) → yˆ AntmA’+hm AntmAˆyhm
y+y → y ςyny+y ςyny
n+n → n mn+nA mnA
mn+m → mm mn+mA mmA
ςn+m → ςm ςn+mA ςmA
An+lA → AlA An+lA AlA
</equation>
<tableCaption confidence="0.988879">
Table 1: Detokenization rules of El Kholy and Habash
(2012), with examples. pron stands for pronominal clitic.
</tableCaption>
<bodyText confidence="0.9978945625">
llEAb u l.L� instead of the correct form is “lAlςAb”
u l�Jy. Second, rules may fail to handle sequences
produced by tokenization errors. For example, the
word “bslTh” Z L�j“with power” can be erro-
neously tokenized as ”b+slT+h”, while the correct
tokenizations is “b+slTh”. The erroneous tokeniza-
tion will be incorrectly detokenized as ”bslTh”.
The table-based approach memorizes mappings
between words and their tokenized form. Such a
table is easily constructed by running the tokenizer
on a large amount of Arabic text, and observing the
input and output. The detokenization process con-
sults this table to retrieve surface forms of tokenized
words. In the case where a tokenized word has sev-
eral observed surface forms, the most frequent form
is selected. This approach fails when the sequence
of tokenized words is not in the table. In morpholog-
ically complex languages like Arabic, an inflected
base word can attrract many optional clitics, and ta-
bles may not include all different forms and inflec-
tions of a word.
The SRILM-disambig scheme introduced by
El Kholy and Habash (2012) extends the table-based
approach to use an untokenized Arabic language
model to disambiguate among the different alter-
natives. Hence, this scheme can make context-
dependent detokenization decisions, rather than al-
ways producing the most frequent surface form.
Both the SRILM-disambig scheme and the table-
based scheme have the option to fall back on either
rules or simple concatenation for sequences missing
from the table.
</bodyText>
<page confidence="0.998988">
49
</page>
<sectionHeader confidence="0.954454" genericHeader="method">
4 Detokenization as String Transduction
</sectionHeader>
<bodyText confidence="0.999993379310345">
We propose to approach detokenization as a string
transduction task. We train a discriminative trans-
ducer on a set of tokenized-detokenized word pairs.
The set of pairs is initially aligned on the charac-
ter level, and the alignment pairs become the opera-
tions that are applied during transduction. For deto-
kenization, most operations simply copy over char-
acters, but more complex rules such as l+ Al → ll
are learned from the training data as well.
The tool that we use to perform the transduction is
DIRECTL+, a discriminative, character-level string
transducer, which was originally designed for letter-
to-phoneme conversion (Jiampojamarn et al., 2008).
To align the characters in each training example.
DIRECTL+ uses an EM-based M2M-ALIGNER (Ji-
ampojamarn et al., 2007). After alignment is com-
plete, MIRA training repeatedly decodes the train-
ing set to tune the features that determine when each
operation should be applied. The features include
both n-gram source context and HMM-style target
transitions. DIRECTL+ employs a fully discrimina-
tive decoder to learn character transformations and
when they should be applied. The decoder resem-
bles a monotone phrase-based SMT decoder, but is
built to allow for hundreds of thousands of features.
The following example illustrates how string
transduction applies to detokenization. The seg-
mented and surface forms of bbrAsthm �����������
“with their skill” constitute a training instance:
</bodyText>
<equation confidence="0.8676608">
b+_brAsћ_+hm → bbrAsthm
The instance is aligned during the training phase as:
b+ _b r A s ћ_ + h m

b b r A s t E h m
</equation>
<bodyText confidence="0.995898555555556">
The underscore “_” indicates a space, while “E” de-
notes an empty string. The following operations are
extracted from the alignment:
b+ → b, _b → b, r → r, A → A, E → E, p_ → t,
+ → E, h → h, m → m
During training, weights are assigned to features that
associate operations with context. In our running ex-
ample, the weight assigned to the b+ → b operation
accounts for the operation itself, for the fact that the
operation appears at the beginning of a word, and for
the fact that it is followed by an underscore; in fact,
we employ a context window of 5 characters to the
left or right of the source substring “b+”, creating a
feature for each n-gram within that window.
Modeling the tokenization problem as string
transduction has several advantages. The approach
is completely language-independent. The context-
sensitive rules are learned automatically from ex-
amples, without human intervention. The rules
and features can be represented in a more com-
pact way than the full mapping table required by
table-based approaches, while still elegantly han-
dling words that were not seen during training.
Also, since the training data is generalized more
efficiently than in simple memorization of com-
plete tokenized-detokenized pairs, less training data
should be needed to achieve good accuracy.
</bodyText>
<sectionHeader confidence="0.999602" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999755333333333">
This section presents two experiments that evaluate
the effect of the detokenization schemes on both nat-
urally occurring Arabic and SMT output.
</bodyText>
<subsectionHeader confidence="0.97256">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.979018875">
To build our data-driven detokenizers, we use the
Arabic part of 4 Arabic-English parallel datasets
from the Linguistic Data Consortium as train-
ing data. The data sets are: Arabic News
(LDC2004T17), eTIRR (LDC2004E72), English
translation of Arabic Treebank (LDC2005E46), and
Ummah (LDC2004T18). The training data has
107K sentences. The Arabic part of the training data
constitutes around 2.8 million words, 3.3 million to-
kens after tokenization, and 122K word types after
filtering punctuation marks, Latin words and num-
bers (refer to Table 2 for detailed counts).
For training the SMT system’s translation and re-
ordering models, we use the same 4 datasets from
LDC. We also use 200 Million words from LDC
Arabic Gigaword corpus (LDC2011T11) to gener-
ate a 5-gram language model using SRILM toolkit
(Stolcke, 2002).
We use NIST MT 2004 evaluation set for tun-
ing (1075 sentences), and NIST MT 2005 evalua-
tions set for testing (1056 sentences). Both MT04
and MT05 have multiple English references in or-
der to evaluate Arabic-to-English translation. As we
are translating into Arabic, we take the first English
</bodyText>
<page confidence="0.987904">
50
</page>
<table confidence="0.89112975">
Data set Before After
training set 122,720 61,943
MT04 8,201 2,542
MT05 7,719 2,429
</table>
<tableCaption confidence="0.998445">
Table 2: Type counts before and after tokenization.
</tableCaption>
<bodyText confidence="0.999263">
translation to be our source in each case. We also
use the Arabic halves of MT04 and MT05 as devel-
opment and test sets for our experiments on natu-
rally occurring Arabic. The tokenized Arabic is our
input, with the original Arabic as our gold-standard
detokenization.
The Arabic text of the training, development, test-
ing set and language model are all tokenized using
MADA 3.2 (Habash et al., 2009) with the Penn Ara-
bic Treebank tokenization scheme. The English text
in the parallel corpus is lower-cased and tokenized
in the traditional sense to strip punctuation marks.
</bodyText>
<subsectionHeader confidence="0.997767">
5.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999826961538461">
To train the detokenization systems, we generate a
table of mappings from tokenized forms to surface
forms based on the Arabic part of our 4 parallel
datasets, giving us complete coverage of the out-
put vocabulary of our SMT system. In the table-
based approaches, if a tokenized form is mapped to
more than one surface form, we use the most fre-
quent surface form. For out-of-table words, we fall
back on concatenation (in T) or rules (in T+R). For
SRILM-Disambig detokenization, we maintain am-
biguous table entries along with their frequencies,
and we introduce a 5-gram language model to dis-
ambiguate detokenization choices in context. Like
the table-based approaches, the Disambig approach
can back off to either simple concatenation (T+LM)
or rules (T+R+LM) for missing entries. The latter
is a re-implementation of the state-of-the-art system
presented by El Kholy and Habash (2012).
We train our discriminative string transducer us-
ing word types from the 4 LDC catalogs. We
use M2M-ALIGNER to generate a 2-to-1 charac-
ter alignments between tokenized forms and surface
forms. For the decoder, we set Markov order to one,
joint n-gram features to 5, n-gram size to 11, and
context size to 5. This means the decoder can uti-
lize contexts up to 11 characters long, allowing it to
</bodyText>
<table confidence="0.993616625">
Detokenization WER SER BLEU
Baseline 1.710 34.3 26.30
Rules (R) 0.590 14.0 28.32
Table (T) 0.192 4.9 28.54
Table + Rules (T+R) 0.122 3.2 28.55
Disambig (T+LM) 0.164 4.1 28.53
Disambig (T+R+LM) 0.094 2.4 28.54
DIRECTL+ 0.087 2.1 28.55
</table>
<tableCaption confidence="0.988222">
Table 3: Word and sentence error rate of detokenization
schemes on the Arabic reference text of NIST MT05.
BLEU score refers to English-Arabic SMT output.
</tableCaption>
<bodyText confidence="0.925301454545454">
effectively memorize many words. We found these
settings using grid search on the development set,
NIST MT04.
For the SMT experiment, we use GIZA++ for
the alignment between English and tokenized Ara-
bic, and perform the translation using Moses phrase-
based SMT system (Hoang et al., 2007), with a max-
imum phrase length of 5. We apply each detokeniza-
tion scheme on the SMT tokenized Arabic output
test set, and evaluate using the BLEU score (Pap-
ineni et al., 2002).
</bodyText>
<subsectionHeader confidence="0.815429">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999992047619048">
Table 3 shows the performance of several detok-
enization schemes. For evaluation, we use the sen-
tence and word error rates on naturally occurring
Arabic text, and BLEU score on tokenized Arabic
output of the SMT system. The baseline scheme,
which is a simple concatenation of morphemes, in-
troduces errors in over a third of all sentences. The
table-based approach outperforms the rule-based ap-
proach, indicating that there are frequent excep-
tions to the rules in Table 1 that require memoriza-
tion. Their combination (T+R) fares better, lever-
aging the strengths of both approaches. The addi-
tion of SRILM-Disambig produces further improve-
ments as it uses a language model context to disam-
biguate the correct detokenized word form. Our sys-
tem outperforms SRILM-Disambig by a very slight
margin, indicating that the two systems are roughly
equal. This is interesting, as it is able to do so by
using only features derived from the tokenized word
itself; unlike SRILM-Disambig, it has no access to
the surrounding words to inform its decisions. In ad-
</bodyText>
<page confidence="0.994811">
51
</page>
<bodyText confidence="0.999251857142857">
dition, it is able to achieve this level of performance
without any manually constructed rules.
Improvements in detokenization do contribute to
the BLEU score of our SMT system, but only to
a point. Table 3 shows three tiers of performance,
with no detokenization being the worst, the rules be-
ing better, and the various data-driven approaches
performing best. After WER dips below 0.2, further
improvements seem to no longer affect SMT quality.
Note that BLEU scores are much lower overall than
one would expect for the translation in the reverse
direction, because of the morphological complexity
of Arabic, and the use of one (as opposed to four)
references for evaluation.
</bodyText>
<subsectionHeader confidence="0.960015">
5.4 Analysis
</subsectionHeader>
<bodyText confidence="0.995327618181818">
The sentence error rate of 2.1 represents only 21
errors that our approach makes. Among those 21,
11 errors are caused by changing p to h and vice
versa. This is due to writing p and h interchange-
ably. For example, “AjmAly+h” was detokenized
as ”AjmAlyћ” ZJ�ËAÔg.@ instead of ”AjmAlyh” éJ�ËAÔg.@.
Another 4 errors are caused by the lack of dia-
critization, which affects the choice of the Hamza
form. For example,”bnAˆwh” è9A:K., “bnAˆyh” é
KA�JK.
and ”bnA’h” èZA:K. (”its building”) are 3 different
forms of the same word where the choice of Hamza
Z is dependent on its diacritical mark or the mark
of the character that precedes it. Another 3 errors
are attributed to the case of the nominal which it in-
flects for. The case is affected by the context of the
noun which DIRECTL+ has no access to. For ex-
ample, “mfkry+hm” (”thinkers/Dual-Accusative”)
was detokenized as ”mfkrAhm” Ñë@Qº�®Ó (Dual-
Nominative) instead of ”mfkryhm” ÑîE�Qº�®Ó. The
last 3 errors are special cases of “An +y” which
can be detokenized correctly as either “Any” ú G@ or
�
”Anny” ú
æ;@.
The table-based detokenization scheme fails in
54 cases. Among these instances, 44 cases are not
in the mapping table, hence resolving back to sim-
ple concatenation ended with an error. Our trans-
duction approach succeeds in detokenizing 42 cases
out of the 54. The majority of these cases involves
changing p to h and vice versa, and changing l+Al
to ll. The only 2 instances where the tokenized
word is in the mapping table but DIRECTL+ incor-
rectly detokenizes it are due to hamza case and p
to h case described above. There are 4 instances
of the same word/case where both the table scheme
and DIRECTL+ fails due to error of tokenization
by MADA, where the proper name qwh èñs is er-
roneously tokenized as qw+p. This shows that DI-
RECTL+ handles the OOV words correctly.
The Disambig(T+R+LM) erroneously detok-
enizes 27 instances, where 21 out of them are cor-
rectly tokenized by DIRECTL+. Most of the er-
rors are due to the Hamza and p to h reasons. It
seems that even with a large size language model,
the SRILM utility needs a large mapping table to
perform well. Only 4 instances were erroneously
detokenized by both Disambig and DIRECTL+ due
to Hamza and the case of the nominal.
The analysis shows that using small size training
data, DIRECTL+ can achieve slightly better accu-
racy than SRILM scheme. The limitations of using
table and rules are handled with DIRECTL+ as it is
able to memorize more rules.
</bodyText>
<sectionHeader confidence="0.997111" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999963090909091">
In this paper, we addressed the detokenization prob-
lem for Arabic using DIRECTL+, a discriminative
training model for string transduction. Our system
performs the best among the available systems. It
manages to solve problems caused by limitations of
table-based and rule-based systems. This allows us
to match the performance of the SRILM-disambig
approach without using a language model or hand-
crafted rules. In the future, we plan to test our ap-
proach on other languages that have morphological
characteristics similar to Arabic.
</bodyText>
<sectionHeader confidence="0.999281" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999707545454546">
Ibrahim Badr, Rabih Zbib, and James Glass. 2008. Seg-
mentation for English-to-Arabic statistical machine
translation. In Proceedings of ACL, pages 153–156.
Ahmed El Kholy and Nizar Habash. 2012. Orthographic
and morphological processing for English-Arabic sta-
tistical machine translation. Machine Translation,
26(1-2):25–45, March.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+tokan: A toolkit for Arabic tokenization, dia-
critization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
</reference>
<page confidence="0.973771">
52
</page>
<reference confidence="0.999775525">
the Second International Conference on Arabic Lan-
guage Resources and Tools.
Nizar Habash. 2007. Arabic morphological represen-
tations for machine translation. In Arabic Computa-
tional Morphology: Knowledge-based and Empirical
Methods.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Synthesis Lectures on Human
Language Technologies. Morgan &amp; Claypool Publish-
ers.
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and On-
drej Bojar. 2007. Moses: Open source toolkit for
statistical machine translation. In Annual Meeting of
the Association for Computational Linguistics (ACL),
demonstration session, pages 177–180.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and HMMs to letter-to-phoneme conversion. In Pro-
ceedings of NAACL-HLT, pages 372–379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proceed-
ings of ACL-08: HLT, pages 905–913.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318.
Fatiha Sadat and Nizar Habash. 2006. Combination of
Arabic preprocessing schemes for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 1–8.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language Pro-
cessing, pages 901–904.
</reference>
<page confidence="0.999351">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.307069">
<title confidence="0.872673">Reversing Morphological Tokenization in English-to-Arabic SMT</title>
<affiliation confidence="0.699362">of Computing Science Research Council Canada University of Alberta 1200 Montreal Road</affiliation>
<address confidence="0.965919">Edmonton, AB, T6G 2E8, Canada Ottawa, ON, K1A 0R6,</address>
<email confidence="0.87335">msalameh@ualberta.caColin.Cherry@nrc-cnrc.gc.ca</email>
<email confidence="0.87335">gkondrak@ualberta.caColin.Cherry@nrc-cnrc.gc.ca</email>
<abstract confidence="0.999765727272728">Morphological tokenization has been used in machine translation for morphologically complex languages to reduce lexical sparsity. Unfortunately, when translating into a morphologically complex language, recombining segmented tokens to generate original word forms is not a trivial task, due to morphological, phonological and orthographic adjustments that occur during tokenization. We review a number of detokenization schemes for Arabic, such as rule-based and table-based approaches and show their limitations. We then propose a novel detokenization scheme that uses a character-level discriminative string transducer to predict the original form of a segmented word. In a comparison to a stateof-the-art approach, we demonstrate slightly better detokenization error rates, without the need for any hand-crafted rules. We also demonstrate the effectiveness of our approach in an English-to-Arabic translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>mentation for English-to-Arabic statistical machine translation.</title>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>153--156</pages>
<marker></marker>
<rawString>mentation for English-to-Arabic statistical machine translation. In Proceedings of ACL, pages 153–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed El Kholy</author>
<author>Nizar Habash</author>
</authors>
<title>Orthographic and morphological processing for English-Arabic statistical machine translation.</title>
<date>2012</date>
<journal>Machine Translation,</journal>
<pages>26--1</pages>
<marker>El Kholy, Habash, 2012</marker>
<rawString>Ahmed El Kholy and Nizar Habash. 2012. Orthographic and morphological processing for English-Arabic statistical machine translation. Machine Translation, 26(1-2):25–45, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>Mada+tokan: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization.</title>
<date>2009</date>
<booktitle>In Proceedings of guage Resources and Tools.</booktitle>
<contexts>
<context position="16088" citStr="Habash et al., 2009" startWordPosition="2505" endWordPosition="2508">-English translation. As we are translating into Arabic, we take the first English 50 Data set Before After training set 122,720 61,943 MT04 8,201 2,542 MT05 7,719 2,429 Table 2: Type counts before and after tokenization. translation to be our source in each case. We also use the Arabic halves of MT04 and MT05 as development and test sets for our experiments on naturally occurring Arabic. The tokenized Arabic is our input, with the original Arabic as our gold-standard detokenization. The Arabic text of the training, development, testing set and language model are all tokenized using MADA 3.2 (Habash et al., 2009) with the Penn Arabic Treebank tokenization scheme. The English text in the parallel corpus is lower-cased and tokenized in the traditional sense to strip punctuation marks. 5.2 Experimental Setup To train the detokenization systems, we generate a table of mappings from tokenized forms to surface forms based on the Arabic part of our 4 parallel datasets, giving us complete coverage of the output vocabulary of our SMT system. In the tablebased approaches, if a tokenized form is mapped to more than one surface form, we use the most frequent surface form. For out-of-table words, we fall back on c</context>
</contexts>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>Nizar Habash, Owen Rambow, and Ryan Roth. 2009. Mada+tokan: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization. In Proceedings of guage Resources and Tools.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Arabic morphological representations for machine translation.</title>
<date>2007</date>
<booktitle>In Arabic Computational Morphology: Knowledge-based and Empirical Methods.</booktitle>
<contexts>
<context position="7357" citStr="Habash, 2007" startWordPosition="1086" endWordPosition="1087">ese ambiguities in SMT, normalization is often performed as a preprocessing step, which usually involves converting different forms of Alif and Ya to a single form. This decreases Arabic’s lexical sparsity and improves SMT performance. 3 Related Work Sadat and Habash (2006) address the issue of lexical sparsity by presenting different preprocessing schemes for Arabic-to-English SMT. The schemes include simple tokenization, orthographic normalization, and decliticization. The combination of these schemes results in improved translation out2We use Habash-Soudi-Buckwalter transliteration scheme (Habash, 2007) for all Arabic examples. 48 put. This is one of many studies on normalization and tokenization for translation from Arabic, which we will not attempt to review completely here. Badr et al. (2008) show that tokenizing Arabic also has a positive influence on English-to-Arabic SMT. They apply two tokenization schemes on Arabic text, and introduce detokenization schemes through a rule-based approach, a table-based approach, and a combination of both. The combination approach detokenizes words first using the table, falling back on rules for sequences not found in the table. El Kholy and Habash (2</context>
</contexts>
<marker>Habash, 2007</marker>
<rawString>Nizar Habash. 2007. Arabic morphological representations for machine translation. In Arabic Computational Morphology: Knowledge-based and Empirical Methods.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Introduction to Arabic Natural Language Processing. Synthesis Lectures on Human Language Technologies.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="1606" citStr="Habash (2010)" startWordPosition="218" endWordPosition="219">orm of a segmented word. In a comparison to a stateof-the-art approach, we demonstrate slightly better detokenization error rates, without the need for any hand-crafted rules. We also demonstrate the effectiveness of our approach in an English-to-Arabic translation task. 1 Introduction Statistical machine translation (SMT) relies on tokenization to split sentences into meaningful units for easy processing. For morphologically complex languages, such as Arabic or Turkish, this may involve splitting words into morphemes. Throughout this paper, we adopt the definition of tokenization proposed by Habash (2010), which incorporates both morphological segmentation as well as orthographic character transformations. To use an English example, the word tries would be morphologically tokenized as “try + s”, which involves 47 orthographic changes at morpheme boundaries to match the lexical form of each token. When translating into a tokenized language, the tokenization must be reversed to make the generated text readable and evaluable. Detokenization is the process of converting tokenized words into their original orthographically and morphologically correct surface form. This includes concatenating tokens</context>
</contexts>
<marker>Habash, 2010</marker>
<rawString>Nizar Habash. 2010. Introduction to Arabic Natural Language Processing. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-burch</author>
<author>Richard Zens</author>
<author>Rwth Aachen</author>
<author>Alexandra Constantin</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Chris Dyer</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Ondrej Bojar</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="18239" citStr="Hoang et al., 2007" startWordPosition="2859" endWordPosition="2862">es (R) 0.590 14.0 28.32 Table (T) 0.192 4.9 28.54 Table + Rules (T+R) 0.122 3.2 28.55 Disambig (T+LM) 0.164 4.1 28.53 Disambig (T+R+LM) 0.094 2.4 28.54 DIRECTL+ 0.087 2.1 28.55 Table 3: Word and sentence error rate of detokenization schemes on the Arabic reference text of NIST MT05. BLEU score refers to English-Arabic SMT output. effectively memorize many words. We found these settings using grid search on the development set, NIST MT04. For the SMT experiment, we use GIZA++ for the alignment between English and tokenized Arabic, and perform the translation using Moses phrasebased SMT system (Hoang et al., 2007), with a maximum phrase length of 5. We apply each detokenization scheme on the SMT tokenized Arabic output test set, and evaluate using the BLEU score (Papineni et al., 2002). 5.3 Results Table 3 shows the performance of several detokenization schemes. For evaluation, we use the sentence and word error rates on naturally occurring Arabic text, and BLEU score on tokenized Arabic output of the SMT system. The baseline scheme, which is a simple concatenation of morphemes, introduces errors in over a third of all sentences. The table-based approach outperforms the rule-based approach, indicating </context>
</contexts>
<marker>Hoang, Birch, Callison-burch, Zens, Aachen, Constantin, Federico, Bertoldi, Dyer, Cowan, Shen, Moran, Bojar, 2007</marker>
<rawString>Hieu Hoang, Alexandra Birch, Chris Callison-burch, Richard Zens, Rwth Aachen, Alexandra Constantin, Marcello Federico, Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade Shen, Christine Moran, and Ondrej Bojar. 2007. Moses: Open source toolkit for statistical machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Grzegorz Kondrak</author>
<author>Tarek Sherif</author>
</authors>
<title>Applying many-to-many alignments and HMMs to letter-to-phoneme conversion.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>372--379</pages>
<contexts>
<context position="12201" citStr="Jiampojamarn et al., 2007" startWordPosition="1858" endWordPosition="1862"> pairs. The set of pairs is initially aligned on the character level, and the alignment pairs become the operations that are applied during transduction. For detokenization, most operations simply copy over characters, but more complex rules such as l+ Al → ll are learned from the training data as well. The tool that we use to perform the transduction is DIRECTL+, a discriminative, character-level string transducer, which was originally designed for letterto-phoneme conversion (Jiampojamarn et al., 2008). To align the characters in each training example. DIRECTL+ uses an EM-based M2M-ALIGNER (Jiampojamarn et al., 2007). After alignment is complete, MIRA training repeatedly decodes the training set to tune the features that determine when each operation should be applied. The features include both n-gram source context and HMM-style target transitions. DIRECTL+ employs a fully discriminative decoder to learn character transformations and when they should be applied. The decoder resembles a monotone phrase-based SMT decoder, but is built to allow for hundreds of thousands of features. The following example illustrates how string transduction applies to detokenization. The segmented and surface forms of bbrAst</context>
</contexts>
<marker>Jiampojamarn, Kondrak, Sherif, 2007</marker>
<rawString>Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek Sherif. 2007. Applying many-to-many alignments and HMMs to letter-to-phoneme conversion. In Proceedings of NAACL-HLT, pages 372–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Colin Cherry</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Joint processing and discriminative training for letter-to-phoneme conversion.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>905--913</pages>
<contexts>
<context position="12084" citStr="Jiampojamarn et al., 2008" startWordPosition="1841" endWordPosition="1844">kenization as a string transduction task. We train a discriminative transducer on a set of tokenized-detokenized word pairs. The set of pairs is initially aligned on the character level, and the alignment pairs become the operations that are applied during transduction. For detokenization, most operations simply copy over characters, but more complex rules such as l+ Al → ll are learned from the training data as well. The tool that we use to perform the transduction is DIRECTL+, a discriminative, character-level string transducer, which was originally designed for letterto-phoneme conversion (Jiampojamarn et al., 2008). To align the characters in each training example. DIRECTL+ uses an EM-based M2M-ALIGNER (Jiampojamarn et al., 2007). After alignment is complete, MIRA training repeatedly decodes the training set to tune the features that determine when each operation should be applied. The features include both n-gram source context and HMM-style target transitions. DIRECTL+ employs a fully discriminative decoder to learn character transformations and when they should be applied. The decoder resembles a monotone phrase-based SMT decoder, but is built to allow for hundreds of thousands of features. The follo</context>
</contexts>
<marker>Jiampojamarn, Cherry, Kondrak, 2008</marker>
<rawString>Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kondrak. 2008. Joint processing and discriminative training for letter-to-phoneme conversion. In Proceedings of ACL-08: HLT, pages 905–913.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="18414" citStr="Papineni et al., 2002" startWordPosition="2891" endWordPosition="2895">8.55 Table 3: Word and sentence error rate of detokenization schemes on the Arabic reference text of NIST MT05. BLEU score refers to English-Arabic SMT output. effectively memorize many words. We found these settings using grid search on the development set, NIST MT04. For the SMT experiment, we use GIZA++ for the alignment between English and tokenized Arabic, and perform the translation using Moses phrasebased SMT system (Hoang et al., 2007), with a maximum phrase length of 5. We apply each detokenization scheme on the SMT tokenized Arabic output test set, and evaluate using the BLEU score (Papineni et al., 2002). 5.3 Results Table 3 shows the performance of several detokenization schemes. For evaluation, we use the sentence and word error rates on naturally occurring Arabic text, and BLEU score on tokenized Arabic output of the SMT system. The baseline scheme, which is a simple concatenation of morphemes, introduces errors in over a third of all sentences. The table-based approach outperforms the rule-based approach, indicating that there are frequent exceptions to the rules in Table 1 that require memorization. Their combination (T+R) fares better, leveraging the strengths of both approaches. The ad</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fatiha Sadat</author>
<author>Nizar Habash</author>
</authors>
<title>Combination of Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="7018" citStr="Sadat and Habash (2006)" startWordPosition="1042" endWordPosition="1045">ferent forms of Hamzated � Alif “h h�” are usually written without the Hamza “�”. Likewise, when the letter Ya ’Y’ S is present at the end of the word, it is sometimes written in the form of “Alif Maqsura” letter ’ý’ S. Also, short vowels in Arabic are represented using diacritics, which are usually absent in written text. In order to deal with these ambiguities in SMT, normalization is often performed as a preprocessing step, which usually involves converting different forms of Alif and Ya to a single form. This decreases Arabic’s lexical sparsity and improves SMT performance. 3 Related Work Sadat and Habash (2006) address the issue of lexical sparsity by presenting different preprocessing schemes for Arabic-to-English SMT. The schemes include simple tokenization, orthographic normalization, and decliticization. The combination of these schemes results in improved translation out2We use Habash-Soudi-Buckwalter transliteration scheme (Habash, 2007) for all Arabic examples. 48 put. This is one of many studies on normalization and tokenization for translation from Arabic, which we will not attempt to review completely here. Badr et al. (2008) show that tokenizing Arabic also has a positive influence on Eng</context>
</contexts>
<marker>Sadat, Habash, 2006</marker>
<rawString>Fatiha Sadat and Nizar Habash. 2006. Combination of Arabic preprocessing schemes for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Intl. Conf. Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="8194" citStr="Stolcke, 2002" startWordPosition="1218" endWordPosition="1219">c also has a positive influence on English-to-Arabic SMT. They apply two tokenization schemes on Arabic text, and introduce detokenization schemes through a rule-based approach, a table-based approach, and a combination of both. The combination approach detokenizes words first using the table, falling back on rules for sequences not found in the table. El Kholy and Habash (2012) extend Badr’s work by presenting a larger number of tokenization and detokenization schemes, and comparing their effects on SMT. They introduce an additional detokenization schemes based on the SRILM disambig utility (Stolcke, 2002), which utilizes a 5-gram untokenized language model to decide among different alternatives found in the table. They test their schemes on naturally occurring Arabic text and SMT output. Their newly introduced detokenization scheme outperforms the rule-based and table-based approaches introduced by Badr et al. (2008), establishing the current state-of-the-art. 3.1 Detokenization Schemes in Detail Rule-based detokenization involves manually defining a set of transformation rules to convert a sequence of segmented tokens into their surface form. For example, the noun “llrˆyys”���;1L� “to the pre</context>
<context position="15257" citStr="Stolcke, 2002" startWordPosition="2366" endWordPosition="2367">IRR (LDC2004E72), English translation of Arabic Treebank (LDC2005E46), and Ummah (LDC2004T18). The training data has 107K sentences. The Arabic part of the training data constitutes around 2.8 million words, 3.3 million tokens after tokenization, and 122K word types after filtering punctuation marks, Latin words and numbers (refer to Table 2 for detailed counts). For training the SMT system’s translation and reordering models, we use the same 4 datasets from LDC. We also use 200 Million words from LDC Arabic Gigaword corpus (LDC2011T11) to generate a 5-gram language model using SRILM toolkit (Stolcke, 2002). We use NIST MT 2004 evaluation set for tuning (1075 sentences), and NIST MT 2005 evaluations set for testing (1056 sentences). Both MT04 and MT05 have multiple English references in order to evaluate Arabic-to-English translation. As we are translating into Arabic, we take the first English 50 Data set Before After training set 122,720 61,943 MT04 8,201 2,542 MT05 7,719 2,429 Table 2: Type counts before and after tokenization. translation to be our source in each case. We also use the Arabic halves of MT04 and MT05 as development and test sets for our experiments on naturally occurring Arabi</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Intl. Conf. Spoken Language Processing, pages 901–904.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>