<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000664">
<title confidence="0.994577">
Finding Short Definitions of Terms on Web Pages
</title>
<author confidence="0.997019">
Gerasimos Lampouras* and Ion Androutsopoulos*+
</author>
<affiliation confidence="0.9879705">
*Department of Informatics, Athens University of Economics and Business, Greece
+Digital Curation Unit, Research Centre “Athena”, Athens, Greece
</affiliation>
<sectionHeader confidence="0.989022" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999708714285714">
We present a system that finds short def-
initions of terms on Web pages. It em-
ploys a Maximum Entropy classifier, but it
is trained on automatically generated ex-
amples; hence, it is in effect unsupervised.
We use ROUGE-W to generate training ex-
amples from encyclopedias and Web snip-
pets, a method that outperforms an alter-
native centroid-based one. After training,
our system can be used to find definitions
of terms that are not covered by encyclo-
pedias. The system outperforms a compa-
rable publicly available system, as well as
a previously published form of our system.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999790314285715">
Definitions of terms are among the most com-
mon types of information users search for on the
Web. In the TREC 2001 QA track (Voorhees,
2001), where the distribution of question types re-
flected that of real user logs, 27% of the ques-
tions were requests for definitions (e.g., “What is
gasohol?”, “Who was Duke Ellington?”). Conse-
quently, some Web search engines provide special
facilities (e.g., Google’s “define:” query prefix)
that seek definitions of user-specified terms in on-
line encyclopedias or glossaries; to save space, we
call both “encyclopedias”. There are, however, of-
ten terms that are too recent, too old, or less widely
used to be included in encyclopedias. Their defi-
nitions may be present on other Web pages (e.g.,
newspaper articles), but they may be provided in-
directly (e.g., “He said that gasohol, a mixture of
gasoline and ethanol, has been great for his busi-
ness.”) and they may be difficult to locate with
generic search engines that may return dozens of
pages containing, but not defining the terms.
We present a system to find short definitions
of user-specified terms on Web pages. It can be
used as an add-on to generic search engines, when
no definitions can be found in on-line encyclope-
dias. The system first invokes a search engine us-
ing the (possibly multi-word) term whose defini-
tion is sought, the target term, as the query. It
then scans the top pages returned by the search
engine to locate 250-character snippets with the
target term at their centers; we call these snippets
windows. The windows are candidate definitions
of the target term, and they are then classified as
acceptable (positive class) or unacceptable (nega-
tive class) using supervised machine learning. The
system reports the windows for which it is most
confident that they belong in the positive class. Ta-
ble 1 shows examples of short definitions found by
our system. In our experiments, we allow the sys-
tem to return up to five windows per target term,
and the system’s response is counted as correct if
any of the returned windows contains an accept-
able short definition of the target. This is similar
to the treatment of definition questions in TREC
2000 and 2001 (Voorhees, 2000; Voorhees, 2001),
but the answer is sought on the Web, not in a given
document collection of a particular genre.
More recent TREC QA tracks required definition
questions to be answered by lists of complemen-
tary text snippets, jointly providing required or op-
tional information nuggets (Voorhees, 2003). In
contrast, we focus on locating single snippets that
include self-contained short definitions. Despite
its simpler nature, we believe the task we address
is of practical use: a list of single-snippet defini-
tions from Web pages accompanied by the source
URLs is a good starting point for users seeking
definitions of terms not covered by encyclopedias.
We also note that evaluating multi-snippet defini-
tions can be problematic, because it is often dif-
ficult to agree which information nuggets should
be treated as required, or even optional (Hilde-
brandt et al., 2004). In contrast, earlier experimen-
tal results we have reported (Androutsopoulos and
Galanis, 2005) show strong inter-assessor agree-
ment (K &gt; 0.8) for single-snippet definitions (Eu-
genio and Glass, 2004). The task we address also
differs from DUC’s query focused summarization
(Dang, 2005; Dang, 2006). Our queries are sin-
gle terms, whereas DUC queries are longer topic
</bodyText>
<page confidence="0.904481">
1270
</page>
<note confidence="0.768885666666667">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1270–1279,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
Target term: Babesiosis
</note>
<bodyText confidence="0.970701181818182">
(...) Babesiosis is a rare, severe and sometimes fatal tick-
borne disease caused by various types of Babesia, a micro-
scopic parasite that infects red blood cells. In New York
state, the causative parasite is babesia microti. Who gets
Babesiosis? Babesiosis (...)
Target term: anorexia nervosa
(...) anorexia nervosa is an illness that usually occurs in
teenage girls, but it can also occur in teenage boys, and adult
women and men. People with anorexia are obsessed with
being thin. They lose a lot of weight and are terrified of
gaining weight. The (...)
</bodyText>
<subsectionHeader confidence="0.305443">
Target term: Kinabalu
</subsectionHeader>
<bodyText confidence="0.5835192">
(...) one hundred and thirty eight kilometers from Kota Kin-
abalu, the capital of the Malaysian state of Sabah, rises the
majestic mount Kinabalu. With its peak at 4,101 meters
(and growing), mount Kinabalu is the highest mountain in
south-east Asia. This (...)
</bodyText>
<subsectionHeader confidence="0.416671">
Target term: Pythagoras
</subsectionHeader>
<bodyText confidence="0.9713816">
(...) Pythagoras of Samos about 569 BC - about 475
BC click the picture above to see eleven larger pictures
Pythagoras was a Greek philosopher who made important
developments in mathematics, astronomy, and the theory of
music. The theorem now known as (...)
</bodyText>
<subsectionHeader confidence="0.703686">
Target term: Sacajawea
</subsectionHeader>
<bodyText confidence="0.997831363636364">
(...) Sacajawea was a Shoshone Indian princess. The
Shoshone lived from the rocky mountains to the plains.
They lived primarily on buffalo meat. The shoshone trav-
eled for many days searching for buffalo. They hunted on
horseback using the buffalo for food (...)
Target term: tale of Genji
(...) the tale of Genji This site aims to promote a wider
understanding and appreciation of the tale of Genji - the
11th century Japanese classic written by a Heian court lady
known as Murasaki Shikibu. It also serves as a kind of travel
guide to the world (...)
</bodyText>
<subsectionHeader confidence="0.595699">
Target term: Jacques Lacan
</subsectionHeader>
<bodyText confidence="0.9975098">
(...) who is Jacques Lacan? John Haber in New York
city a primer for pre-post-structuralists Jacques Lacan is a
Parisian psychoanalyst who has influenced literary criticism
and feminism. He began work in the 1950s, in the Freudian
society there. It was a (...)
</bodyText>
<tableCaption confidence="0.995494">
Table 1: Definitions found by our system.
</tableCaption>
<bodyText confidence="0.999548161764706">
descriptions, often entire paragraphs; furthermore,
we do not attempt to compose coherent and cohe-
sive summaries from several snippets.
The system we present is based on our ear-
lier work (Miliaraki and Androutsopoulos, 2004),
where an SVM classifier (Cristianini and Shawe-
Taylor, 2000) was used to separate acceptable win-
dows from unacceptable ones; the SVM also re-
turned confidence scores, which were used to rank
the acceptable windows. On datasets from the
TREC 2000 and 2001 QA tracks, our earlier sys-
tem clearly outperformed the methods of Joho and
Sanderson (2000; 2001) and Prager et al. (2001;
2002), as reported in previous work (Miliaraki
and Androutsopoulos, 2004). To train the SVM,
however, thousands of training windows were re-
quired, each tagged as a positive or negative exam-
ple. Obtaining large numbers of training windows
is easy, but manually tagging them is very time-
consuming. In the TREC 2000 and 2001 datasets,
it was possible to tag the training windows auto-
matically by using training target terms and ac-
companying regular expression patterns provided
by the TREC organizers. The regular expressions
covered all the known acceptable definitions of the
corresponding terms that can be extracted from the
datasets. When the training windows, however,
are obtained from the Web, it is impossible to con-
struct manually regular expressions for all the pos-
sible phrasings of the acceptable definitions in the
training windows.
In subsequent work (Androutsopoulos and
Galanis, 2005), we developed ATTW (automatic
tagging of training windows), a technique that pro-
duces arbitrarily large collections of training win-
dows from the Web with practically no manual
effort, in effect making our overall system unsu-
pervised. ATTW uses training terms for which
several encyclopedia definitions are available, and
compares each Web training window (each win-
dow extracted from the pages the search engine
returned for a training term) to the corresponding
encyclopedia definitions. Web training windows
that are very similar (or dissimilar) to the corre-
sponding encyclopedia definitions are tagged as
positive (or negative) examples; if the similarity is
neither too high nor too low, the window is not in-
cluded in the classifier’s training data. Previously
reported experiments (Androutsopoulos and Gala-
nis, 2005) showed that ATTW leads to significantly
better results, compared to training the classifier
on all the available TREC windows, for which reg-
ular expressions are available, and then using it to
classify Web windows.
Note that in ATTW the encyclopedia definitions
are used only during training. Once the classifier
has been trained, it can be used to discover defini-
tions on arbitrary Web pages. In fact, during test-
ing we discard windows originating from on-line
encyclopedias, simulating the case where we seek
definitions of terms not covered by encyclopedias;
we also ignore windows from on-line encyclope-
dias during training. Also, note that the classifier
is trained on Web windows, not directly on ency-
clopedia definitions, which allows it to avoid rely-
ing excessively on phrasings that are common in
encyclopedia definitions, but uncommon in more
indirect definitions of arbitrary Web pages. Fur-
</bodyText>
<page confidence="0.982537">
1271
</page>
<bodyText confidence="0.999985875">
thermore, training the classifier directly on ency-
clopedia definitions would not provide negative
examples.
In our previous work with ATTW (Androut-
sopoulos and Galanis, 2005) we used a mea-
sure constructed by ourselves to assess the sim-
ilarity between Web windows and encyclopedia
definitions. Here, we use the more established
ROUGE-W measure (Lin, 2004) instead. ROUGE-
W and other versions of ROUGE have been used in
summarization to measure how close a machine-
authored summary is to multiple human sum-
maries of the same input. We use ROUGE-W in
a similar setting, to measure how close a training
window is to multiple encyclopedia definitions of
the same term. A further difference from our pre-
vious work is that we also use ROUGE-W when
computing the features of the windows to be clas-
sified. Previously, the SVM relied, among others,
on Boolean features indicating if the target term
was preceded or followed in the window to be
classified by a particular phrase indicating a def-
inition (e.g., “target, a kind of”, “such as target”).
The indicative phrases are selected automatically
during training, but now the corresponding fea-
tures are not Boolean; their values are the ROUGE-
W similarity scores between an indicative phrase
and the context of the target term in the window.
This allows the system to soft-match the phrases
to the windows (e.g., encountering “target, another
kind of”, instead of “target, a kind of”).1
In our new system we also use a Maximum En-
tropy (MAXENT) classifier (Ratnaparkhi, 1997) in-
stead of an SVM, because much faster implemen-
tations of the former are available.2 We present
experimental results showing that our new sys-
tem significantly outperforms our previously pub-
lished one. The use of the MAXENT classifier by it-
self improved slightly our results, but the improve-
ments come mostly from using ROUGE-W.
Apart from presenting an improved version of
our system, the main contribution of this paper is a
detailed experimental comparison of our new sys-
tem against Cui et al.’s (2004; 2005; 2006; 2007).
The latter is particularly interesting, because it
is well published, it includes both an alterna-
tive, centroid-based technique to automatically tag
training examples and a soft-matching classifier,
</bodyText>
<footnote confidence="0.87088475">
1We also experimented with other similarity measures
(e.g., edit distance) and ROUGE variants, but we obtained the
best results with ROUGE-W.
2We use Stanford’s classifier; see http://nlp.stanford.edu/.
</footnote>
<bodyText confidence="0.996098538461538">
and it is publicly available.3 We show that ATTW
outperforms Cui et al.’s centroid-based technique,
and that our overall system is also clearly better
than Cui et al.’s in the task we address.
Section 2 discusses ATTW with ROUGE-W, Cui
et al.’s centroid-based method to tag training ex-
amples, and experiments showing that ATTW is
better. Section 3 describes our new overall system,
the system of Cui et al., and the baselines. Sec-
tion 4 reports experimental results showing that
our system is better than Cui et al.’s, and better
than our previously published system. Section 5
discusses related work; and section 6 concludes.
</bodyText>
<sectionHeader confidence="0.758829" genericHeader="method">
2 Tagging training windows
</sectionHeader>
<bodyText confidence="0.9999121875">
During both training and testing, for each tar-
get term we keep the r most highly ranked Web
pages the search engine returns. We then extract
the first f windows of the target term from each
page, since early occurrences of the target terms
on pages are more likely to be definitions. We,
thus, obtain r · f windows per term.4 When test-
ing, we return the k windows of the target term
that the classifier is most certain they belong in the
positive class. In our experiments, r = 10, f = 5,
k = 5. During training, we train the classifier on
the q · r · f windows we obtain for q training tar-
get terms; in our experiments, q ranged from 50 to
1500. Training requires tagging first the training
windows as positive or negative, possibly discard-
ing windows that cannot be tagged automatically.
</bodyText>
<subsectionHeader confidence="0.993155">
2.1 ATTW with ROUGE-W similarity
</subsectionHeader>
<bodyText confidence="0.998773222222222">
To tag a training window w of a training term t
with ATTW and ROUGE-W, we obtain a set Ct of
definitions of t from encyclopedias.5 Stop-words,
punctuation, and non-alphanumeric characters are
removed from Ct and w, and a stemmer is ap-
plied; the testing windows undergo the same pre-
processing.6 For each definition d E Ct, we find
the longest common word subsequence of w and
d. If w is the word sequence (A, B, F, C, D, E)
</bodyText>
<footnote confidence="0.999493727272727">
3See http://www.cuihang.com/software.html. The soft-
ware and a demo of our system, and the datasets we used
are also freely available; see http://nlp.cs.aueb.gr/.
4We used Altavista in our experiments. We remove HTML
tags and retain only the plain text of the pages.
5The training terms were randomly selected from the in-
dex of http://www.encyclopedia.com/. We used Google’s
“define:” to obtain definitions from other encyclopedias.
6We use the 100 most frequent words of the BNC corpus
(http://www.natcorp.ox.ac.uk/) as the stop-list, and Porter’s
stemmer (http://tartarus.org/∼martin/PorterStemmer/).
</footnote>
<page confidence="0.991643">
1272
</page>
<bodyText confidence="0.971669615384615">
and d = (A, B, E, C, G, D), the longest com-
mon subsequence is (A, B, C, D). The longest
common subsequence is divided into consecutive
matches, producing in our example (A, B|C|D).
We then compute the following score (weighted
longest common subsequence), where m is the
number of consecutive matches, ki is the length
of the i-th consecutive match, and f is a weight-
ing function. We use f(k) = ka, where a &gt; 1 is a
parameter we tune experimentally.
WLCS(w, d) = Emi=0 f(ki)
We then compute the following quantities, where
|· |is word length, and f−1 is the inverse of f.
</bodyText>
<equation confidence="0.9968558">
P (w, d)= f−1(WLCS(w,d)
f(|w|) )
R(w, d)= f−1(WLCS(w,d)
f(|d|) )
F(w, d) = (1+β2)·R(w,d)·P(w,d) R(w,d)+β2·P(w,d)
</equation>
<bodyText confidence="0.999755409090909">
In effect, P(w, d) examines how close the
longest common substring is to w and R(w, d)
how close it is to d. Following Lin (2004), we use
β = 8, assigning greater importance to R(w, d). If
R(w, d) is high, the longest common substring is
very similar to d; then w (which also includes the
longest common substring) intuitively contains al-
most all the information of d, i.e., all the informa-
tion of a known acceptable definition (high recall).
If P(w, d) is high, the longest common substring
is very similar to w; then d (which also includes
the longest common substring) contains almost all
the information of w, i.e., w does not contain any
(redundant) information not included in a known
acceptable definition, something we care less for.
The ROUGE-W similarity sim(w, Ct) between
w and Ct is the maximum F(w, d), for all d E
Ct. Training windows with sim(w, Ct) &gt; T+ are
tagged as positive; if sim(w, Ct) &lt; T−, they are
tagged as negative; and if T− &lt; sim(w, Ct) &lt;
T+, they are discarded. We tune the thresholds T+
and T− experimentally, as discussed below.
</bodyText>
<subsectionHeader confidence="0.99348">
2.2 The centroid-based tagging approach
</subsectionHeader>
<bodyText confidence="0.9999798">
This method is used in the system of Cui et al.
(2004; 2005; 2006; 2007). For each training target
term, we construct a “centroid” pseudo-text con-
taining the words that co-occur most frequently
with the target term. We then compute the similar-
ity between each training window and the centroid
of its target term. If it exceeds a threshold, the win-
dow is tagged as positive; Cui et al. produce only
positive examples.
The centroid of a training target term t is con-
structed as follows. For each word u in t’s training
windows, we compute the centrality score defined
below, where SFt is the number of t’s training
windows, SFu is the number of u’s windows that
can be extracted from the retained Web pages the
search engine returned for t, SFtnu is the number
of windows on the same pages that contain both
t and u, and idf(u) is the inverse document fre-
quency of w.7 Centrality scores are pointwise mu-
tual information with an extra idf (u) factor.
</bodyText>
<equation confidence="0.998826">
centrality(u) = −log( SFt∩u
SFt+SFu) · idf (u)
</equation>
<bodyText confidence="0.9999555">
The words u whose centrality scores exceed the
mean by at least a standard deviation are added
to the centroid of t. Before computing the cen-
trality scores, stop-words, punctuation, and non-
alphanumeric characters are removed, and a stem-
mer is applied, as in ATTW. The similarities be-
tween training windows and centroids are then
computed using cosine similarity, after turning the
centroids and windows into binary vectors that
show which words they contain.
</bodyText>
<subsectionHeader confidence="0.997737">
2.3 Comparing the tagging approaches
</subsectionHeader>
<bodyText confidence="0.999159166666666">
To evaluate the two methods that tag training win-
dows, we selected randomly q = 200 target terms,
different from those used for training and testing.
We collected the q · r · f = 200 · 10 · 5 windows
from the corresponding Web pages, we selected
randomly 400 from the collected 10,000 windows,
and tagged them manually as positive or negative.
Figure 1 plots the positive precision of the two
methods against their positive recall, and figure 2
shows negative precision against negative recall.
For different values of T+, we obtain a different
point in figure 1; similarly for T− and figure 2.
Positive precision is TP/(TP + FP), positive re-
call is TP/(TP + FN), and likewise for nega-
tive precision and recall; TP (true positives) are
the positive training windows the method has cor-
rectly tagged as positive, FP are the negative win-
dows the method has tagged as positives etc.
For very high (strict) T+ values, the methods tag
very few (or none) training windows as positive;
hence, both TP and TP + FP approach (or be-
come) zero; we take positive precision to be zero
in that case. Positive recall also approaches (or be-
comes) zero, which is why both positive recall and
</bodyText>
<footnote confidence="0.936424">
7We obtained idf (u) from BNC. Cui et al. use sentences
instead of windows, reducing the risk of truncating defini-
tions. We used windows in all systems, to compare fairly.
</footnote>
<page confidence="0.955341">
1273
</page>
<figureCaption confidence="0.9999385">
Figure 1: Results of generating positive examples.
Figure 2: Results of generating negative examples.
</figureCaption>
<bodyText confidence="0.999641409090909">
precision reach zero in the left of figure 1. Simi-
lar comments apply to figure 2, though both meth-
ods always tagged correctly at least a few training
windows as negative, for the T_ values we tried;
hence, negative precision was never zero.
Positive precision shows how certain we can be
that training windows tagged as positive are in-
deed positive; whereas positive recall is the per-
centage of true positive examples that we manage
to tag as such. Figure 1 shows that when using
ATTW, we need to settle for a low positive recall,
i.e., miss out many positive examples, in order to
obtain a reasonably high precision. It also shows
that the centroid method is clearly worse when tag-
ging positive examples; its positive precision is al-
most always less than 0.3. Figure 2 shows that
both methods achieve high negative precision and
recall; they manage to assign trustworthy nega-
tive labels without missing many negative exam-
ples. However, ATTW is significantly better when
tagging positive examples, as shown in figure 1;
hence, it is better than the centroid method.8
</bodyText>
<footnote confidence="0.673452">
8We tried different values of ROUGE-W’s a parameter in
</footnote>
<bodyText confidence="0.99973804">
When using ATTW in practice, we need to se-
lect T+ and T_. We assign more importance to
selecting a T+ (a point of ATTW’s curve in figure
1) that yields high positive precision; the choice of
T_ (point in figure 2) is less important, because
ATTW’s negative precision is always reasonably
high. Based on figure 1, we set T+ to 0.58, which
corresponds to positive precision 0.66 and posi-
tive recall 0.16. By tuning the two thresholds we
can control the number of positively or negatively
tagged examples we produce (and their ratio), and
the number of examples we discard. Having set
T+, we set T_ to 0.30, a value that maintains the
ratio of truly positive to truly negative windows
of the 400 manually tagged windows (0.2 to 1),
since this is approximately the ratio the classifier
will confront during testing; we also experimented
with a 1 to 1 ratio, but the results were worse. This
T_ value corresponds negative precision 0.70 and
negative recall 0.02. Thus, both positive and neg-
ative precision is approximately 0.7, which means
that approximately 30% of the tags we assign to
the examples are incorrect. Our experiments, how-
ever, indicate that the classifier is able to general-
ize well over this noise.
</bodyText>
<sectionHeader confidence="0.992406" genericHeader="method">
3 Finding new definitions
</sectionHeader>
<bodyText confidence="0.9995615">
We now present our overall system, the system of
Cui et al., and the baselines.
</bodyText>
<subsectionHeader confidence="0.998934">
3.1 Our system
</subsectionHeader>
<bodyText confidence="0.986441888888889">
Given a target term, our system extracts r · f =
10 · 5 windows from the pages returned by the
search engine, and uses the MAXENT classifier to
separate them into acceptable and unacceptable
definitions.9 It then returns the k = 5 windows
the classifier is most confident they are acceptable.
The classifier is trained on windows tagged as pos-
itive or negative using ATTW. It views each win-
dow as a vector of the following features:10
SN: The ordinal number of the window on the
page it originates from (e.g., second window of the
target term from the beginning of the page). Early
mentions of a term are more likely to define it.
RK: The ranking of the Web page the window
originates from, as returned by the search engine.
the interval (1, 2]. We use a = 1.4, which was the value with
the best results on the 400 windows. We did not try a &gt; 2, as
the results were declining as a approached 2.
</bodyText>
<footnote confidence="0.995278333333333">
9We do not discuss MAXENT classifiers, since they are a
well documented in the literature.
10SN and WC originate from Joho and Sanderson (2000).
</footnote>
<page confidence="0.995909">
1274
</page>
<bodyText confidence="0.999926972972973">
WC: We create a simple centroid of the window’s
target term, much as in section 2.2. The centroid’s
words are chosen based on their frequency in the r·
f windows of the target term; the 20 most frequent
words are chosen. WC is the percentage of the 20
words that appear in the vector’s window.
Manual patterns: 13 Boolean features, each sig-
naling if the window matches a different manually
constructed lexical pattern (e.g., “target, a/an/the”,
as in “Tony Blair, the British prime minister”).
The patterns are those used by Joho and Sander-
son (2000), and four more introduced in our pre-
vious work (Androutsopoulos and Galanis, 2005)
and (Miliaraki and Androutsopoulos, 2004). They
are intended to perform well across text genres.
Automatic patterns: m numeric features, each
showing the degree to which the window matches
a different automatically acquired lexical pattern.
The patterns are word n-grams (n E 11, 2, 3}) that
must occur directly before or after the target term
(e.g., “target which is”). The patterns are acquired
as follows. First, all the n-grams directly before
or after any target term in the training windows
are collected. The n-grams that have been en-
countered at least 10 times are candidate patterns.
From those, the m patterns with the highest pre-
cision scores are retained, where precision is the
number of positive training windows the pattern
matches over the total number of training windows
it matches; we use m = 300 in our experiments,
based on the results of our previous work. The au-
tomatically acquired patterns allow the system to
detect definition contexts that are not captured by
the manual patterns, including genre-specific con-
texts. The value of each feature is the ROUGE-W
score between a pattern and the left or right con-
text of the target term in the window.
</bodyText>
<subsectionHeader confidence="0.99963">
3.2 Cui et al.’s system
</subsectionHeader>
<bodyText confidence="0.996582538461538">
Given a target term t, Cui et al. (2004; 2005; 2006;
2007) initially locate sentences containing t in rel-
evant documents. We use the r·f = 10·5 windows
from the pages returned by the search engine, in-
stead of sentences. Cui et al. then construct the
centroid of t, and compute the cosine similarity of
each one of the r · f windows to the centroid, as
in section 2.2. The 10 windows that are closer to
the centroid are considered candidate answers. All
candidate answers are then processed by a part-of-
speech (POS) tagger and a chunker. The words
of the centroid are replaced in all the candidate
answers by their POS tags; the target term, noun
phrases, forms of the verb “to be”, and articles
are replaced by special tags (e.g., TARGET, NP),
while adjectives and adverbs are removed. The
candidate answers are then cropped to L tokens
to the left and right of the target term, producing
two subsequences (left and right) per candidate an-
swer; we set L = 3, which is Cui et al.’s default.
Cui et al. experimented with two approaches to
rank the candidate answers, called Bigram Model
and Profile Hidden Markov Model (PHMM). Both
are learning components that produce soft pat-
terns, though PHMM is much more complicated. In
their earlier work, Cui et al. (2005) found the Bi-
gram Model to perform better than PHMM; in more
recent experiments with more data (Cui, 2006; Cui
et al., 2007) they found PHMM to perform better,
but the difference was not statistically significant.
Given these results and the complexity of PHMM,
we experimented only with the Bigram Model.
In the Bigram Model, the left and right subse-
quences of each candidate answer are considered
separately. Below S1, ... , SL refer to the slots
(word positions) of a (left or right) subsequence,
and t1, ... , tL to the particular words in the slots.
For each subsequence (S1 = t1, ... , SL = tL) of
a candidate answer, we first estimate:
</bodyText>
<equation confidence="0.99652825">
P(ti|Si) = |Si(ti) |+ δ
Et, |Si(ti) |+ δ · N
P(ti|ti−1) = |Si(ti) n Si−1(ti−1)|
|Si(ti)|
</equation>
<bodyText confidence="0.999864789473684">
P(ti|Si) is the probability that ti will appear in
slot Si of a left or right subsequence (depending on
the subsequence considered) of an acceptable can-
didate answer. P(ti|ti−1) is the probability that
ti will follow ti−1 in a (left or right) subsequence
of an acceptable candidate answer. Cui et al. use
only positive training examples, generated by the
centroid-based approach of section 2.2. |Si(ti) |is
the number of times ti appeared in Si in the (left
or right) subsequences of the training examples.
t0 ranges over all the words that occurred in Si in
the training examples. |Si(ti) n Si−1(ti−1) |is the
number of times ti and ti−1 co-occurred in the cor-
responding slots in the training examples. N is the
number of different words that occurred in the (left
or right) training subsequences, and δ is a constant
set to 2, as in Cui et al.’s experiments. Following
Cui et al., if ti is a POS or other special tag then
the probabilities above are estimated by counting
</bodyText>
<page confidence="0.973597">
1275
</page>
<bodyText confidence="0.999652">
only the tags of the training examples. Similarly,
if ti is an actual word, only the actual words (not
tags) of the training examples are considered.
The probability of each subsequence could then
be estimated as:
</bodyText>
<equation confidence="0.9976415">
P(t1, ... , tL) = P(t1|S1) ·
L ri2 (λ · P(ti|ti−1) + (1 − λ) · P(ti|Si))
</equation>
<bodyText confidence="0.94813675">
Instead, Cui et al. use the following scoring mea-
sure, which also accounts for the fact that some
subsequences may have length l &lt; L. They tune
λ by Expectation Maximization.
</bodyText>
<equation confidence="0.985157">
1
Pnorm(t1, . . . ,tL) = l · [log P(t1|S1) +
L
log(λ · P(ti|ti−1) + (1 − λ) · P(ti|Si))]
i=2
The overall score of a candidate answer is then:
P = (1 − α) · Pnorm(left) + α · Pnorm(right)
</equation>
<bodyText confidence="0.999577857142857">
Again, Cui et al. tune a by Expectation Maximiza-
tion. Instead, we tuned λ and α by a grid search
in [0, 1] × [0, 1], with step 0.1 for both parame-
ters. For the tuning, we trained Cui et al.’s system
on 2,000 randomly selected target terms, exclud-
ing terms used for other purposes. We used 160
manually tagged windows to evaluate the system’s
performance with the different values of λ and α;
the 160 windows were selected randomly from the
10,000 windows of section 2.3, after excluding the
400 manually tagged windows of that section. The
resulting values for λ and α were 0.7 and 0.6, re-
spectively. Apart from the modifications we men-
tioned, we use Cui et al.’s original implementation.
</bodyText>
<subsectionHeader confidence="0.96286">
3.3 Baseline methods
</subsectionHeader>
<bodyText confidence="0.99991">
The first baseline selects the first window of each
one of the five highest ranked Web pages, as re-
turned by the search engine, and returns the five
windows. The second baseline returns five win-
dows chosen randomly from the r · f = 10 · 5
available ones. The third baseline (centroid base-
line) creates a centroid of the r · f windows, as in
section 2.2, and returns the five windows with the
highest cosine similarity to the centroid.11
</bodyText>
<footnote confidence="0.956802">
11We also reimplemented the definitions component of
Chu-Carroll et al. (2004; 2005), but its performance was
worse than our centroid baseline.
</footnote>
<figureCaption confidence="0.998993">
Figure 3: Correct responses, 5 answers/question.
</figureCaption>
<sectionHeader confidence="0.88401" genericHeader="method">
4 Evaluation of systems
</sectionHeader>
<bodyText confidence="0.999820696969697">
We used q training target terms in the experi-
ments of this section, with q ranging from 50 to
1500, and 200 testing terms, with no overlap be-
tween training and testing terms, and excluding
terms that had been used for other purpose.12 We
had to use testing terms for which encyclopedia
definitions were also available, to judge the ac-
ceptability of the systems’ responses, since many
terms are highly technical. We discarded, how-
ever, windows extracted from encyclopedia pages
when testing, simulating the case where the target
terms are not covered by encyclopedias.
As already mentioned, for each target term we
extract r · f = 10 · 5 windows (or fewer, if fewer
are available) from the pages the search engine re-
turns. We then provide these windows to each of
the systems, allowing them to return up to k = 5
windows, ordered by decreasing confidence. If
any of the k windows contains an acceptable short
definition of the target term, as judged by a hu-
man evaluator, the system’s response is counted as
correct. We also calculate the Mean Reciprocal
Rank (MRR) of each system’s responses, as in the
TREC QA track: if the first acceptable definition of
a response is in the j-th position (1 ≤ j ≤ k), the
response’s score is 1/j; MRR is the mean of the re-
sponses’ scores, i.e., it rewards systems that return
acceptable definitions higher in their responses.
Figures 3 and 4 show the results of our experi-
ments as percentage of correct responses and MRR,
respectively; the error bars of figure 3 correspond
to 95% confidence intervals. Our system clearly
outperforms Cui et al.’s, despite the fact that the
</bodyText>
<footnote confidence="0.975419">
12The reader is reminded that all terms were selected ran-
domly from the index of an on-line encyclopedia.
</footnote>
<page confidence="0.984751">
1276
</page>
<figureCaption confidence="0.999662">
Figure 4: MRR scores, 5 answers per question.
</figureCaption>
<bodyText confidence="0.9999662">
latter uses more linguistic resources (a POS tag-
ger and a chunker). Both systems outperform the
baselines, of which the centroid baseline is the
best, and both systems perform better as the size
of the training set increases. The baselines con-
tain no learning components; hence, their curves
are flat. We also show the results (Base-Attrs)
of our system when the features that correspond
to automatically acquired patterns are excluded.
Clearly, these patterns help our system achieve
significantly better results; however, our system
outperforms Cui et al.’s even without them. With-
out the automatic patterns, our system also shows
signs of saturation as the training data increase.
Figures 5 and 6 show the performance of our
new system against our previously published one
(Androutsopoulos and Galanis, 2005); the new
system clearly outperforms the old one. Addi-
tional experiments we conducted with the old sys-
tem replacing the SVM by the MAXENT classifier
(without using ROUGE-W) indicate that the use of
MAXENT by itself also improved slightly the re-
sults, but the differences are too minor to show; the
improvement is mostly due to the use of ROUGE-
W instead of our previous measure.
</bodyText>
<sectionHeader confidence="0.999989" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.982102818181818">
Xu et al. (2004) use an information extraction en-
gine to extract linguistic features from documents
relevant to the target term. The features are mostly
phrases, such as appositives, and phrases express-
ing relations. The features are then ranked by their
type and similarity to a centroid, and the most
highly ranked ones are returned. Xu et al. seem
to aim at generating multi-snippet definitions, un-
like the single-snippet definitions we seek.
Blair-Goldensohn et al. (2003; 2004) extract
sentences that may provide definitional informa-
</bodyText>
<figureCaption confidence="0.991283">
Figure 5: Correct responses of our new and previ-
ous system, allowing 5 answers per question.
Figure 6: MRR of our new and previous system.
</figureCaption>
<bodyText confidence="0.999988416666667">
tion from documents retrieved for the target term;
a decision tree learner and manually tagged train-
ing data are used. The sentences are then matched
against manually constructed patterns, which op-
erate on syntax trees, to detect sentences ex-
pressing the target term’s genus, species, or both
(genus+species). The system composes its an-
swer by placing first the genus+species sentence
that is closer to the centroid of the extracted sen-
tences. The remaining sentences are ranked by
their distance from the centroid, and the most
highly ranked ones are clustered. The system then
selects iteratively the cluster that is closer to the
centroid of the extracted sentences and the most
recently used cluster. The cluster’s most repre-
sentative sentence, i.e., the sentence closest to the
centroid of the cluster’s sentences, is added to the
response. The iterations stop when a maximum re-
sponse length is reached. Multi-snippet definitions
are generated.
Han et al. (2004; 2006) parse a definition ques-
tion to locate the head word of the target term.
They also use a named entity recognizer to deter-
mine the target term’s type (person, organization,
</bodyText>
<page confidence="0.976688">
1277
</page>
<bodyText confidence="0.999947363636364">
etc.). They then extract from documents relevant
to the target term sentences containing its head
word, as well as sentences the extracted ones refer
to (e.g., via pronouns). The resulting sentences are
matched against manually constructed syntactic
patterns to detect phrases conveying definitional
information. The resulting phrases are ranked by
criteria like the degree to which the phrase con-
tains words common in definitions of the target
term’s type, and the highest ranked phrases are in-
cluded in a multi-snippet summary. Other mecha-
nisms discard phrases duplicating information.
Xu et al. (2005) aim to extract all the definitions
in a document collection. They parse the docu-
ments to detect base noun phrases (without em-
bedded noun phrases). Base noun phrases are pos-
sible target terms; the paragraphs containing them
are matched against manually constructed patterns
that look for definitions. An SVM then separates
the remaining paragraphs into good, indifferent,
and bad definitions. Redundant paragraphs, iden-
tified by edit distance similarity, are removed.
</bodyText>
<sectionHeader confidence="0.997363" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999990547619048">
We presented a freely available system that finds
short definitions of user-specified terms on Web
pages. It employs a MAXENT classifier, which
is trained on automatically generated examples;
hence, the system is in effect unsupervised. We
use ROUGE-W to generate training examples from
Web snippets and encyclopedias, a method that
outperforms an alternative centroid-based one.
Once our system has been trained, it can find short
definitions of terms that are not covered by ency-
clopedias. Experiments show our system outper-
forms a comparable well-published system and a
previously published form of our system.
Our system does not require linguistic process-
ing tools, such as named entity recognizers, POS
taggers, chunkers, parsers; hence, it can be easily
used in languages where such tools are unavail-
able. It could be improved by exploiting the HTML
markup of Web pages and the Web’s hyperlinks.
For example, the target term is sometimes written
in italics in definitions, and some definitions are
provided on pages (e.g., pop-up windows) that oc-
currences of the target term link to.
The work reported here was conducted in the
context of project INDIGO, where an autonomous
robotic guide for museum collections is being de-
veloped (Galanis et al., 2009). The guide engages
the museum’s visitors in spoken dialogues, and it
describes the exhibits the visitors select by gen-
erating spoken natural language descriptions from
an ontology. Among other requests, the visitors
can ask follow up questions, and we have found
that the most common kind of follow up questions
are requests to define terms (e.g., names of per-
sons, events, architectural terms, etc.) mentioned
in the generated exhibit descriptions. Some of
these definition requests can be handled by gener-
ating new texts from the ontology, but some times
the ontology contains no information for the target
terms. We are, thus, experimenting with the possi-
bility of obtaining short definitions from the Web,
using the system we presented.
</bodyText>
<sectionHeader confidence="0.999024" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999514">
This work was carried out in INDIGO, an FP6 IST
project funded by the European Union, with addi-
tional funding provided by the Greek General Sec-
retariat of Research and Technology.13
</bodyText>
<sectionHeader confidence="0.999444" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99971116">
Androutsopoulos, I., and Galanis, D. 2005. A Prac-
tically Unsupervised Learning Method to Identify
Single-Snippet Answers to Definition Questions on
the Web. In HLT/EMNLP, Vancouver, Canada, 323–
330.
Blair-Goldensohn, S., McKeown, K., Schlaikjer, A.H.
2003. A Hybrid Approach for QA Track Definitional
Questions. In TREC 2003, Gaithersburg, MD, USA.
Blair-Goldensohn, S., McKeown, K.R., and Schlaikjer,
A.H. 2004. Answering Definitional Questions: A
Hybrid Approach. In Maybury, M. (Ed.), New Di-
rections in Question answering, AAAI Press.
Chu-Carroll, J., Czuba, K., Prager, J., Ittycheriah, A.,
Blair-Goldensohn, S. 2004. IBM’s PIQUANT II in
TREC 2004. In TREC 2004, Gaithersburg, MD, USA.
Chu-Carroll, J., Czuba, K., Duboue, P., and Prager, J.
2005. IBM’s PIQUANT II in TREC 2005. In TREC
2005, Gaithersburg, MD, USA.
Cristianini, N. and Shawe-Taylor, J. 2000. An Intro-
duction to SVMs. Cambridge University Press.
Cui, H., Kan, M.-Y., Chua, T.-S., and Xiao, J. 2004. A
Comparative Study on Sentence Retrieval for Defi-
nitional Question Answering. In SIGIR workshop on
Information Retrieval for Question Answering, Sal-
vador, Brazil.
</reference>
<footnote confidence="0.623366">
13Consult http://www.ics.forth.gr/indigo/.
</footnote>
<page confidence="0.91587">
1278
</page>
<reference confidence="0.999856581081081">
Cui, H., Kan, M.Y., Chua, T.S. 2004. Unsupervised
Learning of Soft Patterns for Generating Definitions
from Online News. In WWW, New York, NY, USA.
Cui, H., Kan, M.Y., Chua, T.S. 2005. Generic Soft Pat-
tern Models for Definitional Question Answering. In
ACM SIGIR, Salvador, Brazil.
Cui, H. 2006. Soft Matching for Question Answering.
Ph.D. thesis, National University of Singapore.
Cui, H., Kan, M., and Chua, T. 2007. Soft Pattern
Matching Models for Definitional Question Answer-
ing. ACM Transactions on Information Systems,
25(2):1–30.
Dang, H. T. 2005. Overview of DUC 2005. In DUC at
HLT-EMNLP, Vancouver, Canada.
Dang, H. T. 2006. Overview of DUC 2006. In DUC at
HLT-NAACL, New York, NY, USA.
Eugenio, B. D., Glass, M. 2004. The Kappa Statis-
tic: a Second Look. Computational Linguistics,
301(1):95-101.
Galanis, D., Karakatsiotis, G., Lampouras, G., and An-
droutsopoulos, I. 2009. An Open-Source Natu-
ral Language Generator for OWL Ontologies and
its Use in Protege and Second Life. EACL system
demonstration, Athens, Greece.
Han, K.S., Chung, H., Kim, S.B., Song, Y.I., Lee, J.Y.,
Rim, H.C. 2004. Korea University QA System at
TREC 2004. In TREC 2004, Gaithersburg, MD, USA.
Han, K.S., Song, Y.I., Kim, S.B., and Rim, H.C. 2006.
A Definitional Question Answering System Based on
Phrase Extraction Using Syntactic Patterns. IEICE
Transactions on Information and Systems, vol. E89-
D, No. 4, 1601–1605.
Hildebrandt, W., Katz, B., and Lin, J. 2004. Answer-
ing Definition Questions Using Multiple Knowledge
Sources. In HLT-NAACL, Boston, MA, USA, 49–56.
Joho, H. and Sanderson, M. 2000. Retrieving Descrip-
tive Phrases from Large Amounts of Free Text. Inter-
national Conference on Information and Knowledge
Management, McLean, VA, USA, 180–186.
Joho, H. and Sanderson, M. 2001. Large Scale Testing
of a Descriptive Phrase Finder. In HLT-NAACL, San
Diego, CA, USA, 219–221.
Lin, C.Y. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In ACL workshop “Text
Summarization Branches Out”, Barcelona, Spain.
Miliaraki, S. and Androutsopoulos, I. 2004. Learn-
ing to Identify Single-Snippet Answers to Definition
Questions. In COLING, Geneva, Switzerland, 1360–
1366.
Prager, J., Radev, D., and Czuba, K. 2001. Answering
What-Is Questions by Virtual Annotation. In HLT-
NAACL, San Diego, CA, USA, 26–30.
Prager, J., Chu-Carroll, J., and Czuba, K. 2002. Use of
WordNet Hypernyms for Answering What-Is Ques-
tions. In TREC 2001, Gaithersburg, MD, USA.
Ratnaparkhi A. 1997. A Simple Introduction to Max-
imum Entropy Models for Natural Language Pro-
cessing. Technical Report 97-08, Institute for Re-
search in Cognitive Science, University of Pennsyl-
vania, 1997.
Voorhees, E.M. 2000. Overview of the TREC-9 Ques-
tion Answering Track. NIST, USA.
Voorhees, E.M. 2001. Overview of the TREC 2001
Question Answering Track. NIST, USA.
Voorhees, E.M. 2001. The TREC QA Track. Natural
Language Engineering, 7(4):361–378.
Voorhees, E.M. 2003. Evaluating Answers to Defini-
tion Questions. In HLT-NAACL, Edmonton, Canada.
Xu, J., Weischedel, R., Licuanan, A. 2004. Evaluation
of an Extraction-based Approach to Answering Def-
initional Questions. In ACM SIGIR, Sheffield, UK.
Xu, J., Cao, Y., Li, H., Zhao, M. 2005. Ranking Defini-
tions with Supervised Learning Methods. In WWW,
Chiba, Japan, 811–819.
</reference>
<page confidence="0.995138">
1279
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.815435">
<title confidence="0.991691">Finding Short Definitions of Terms on Web Pages</title>
<affiliation confidence="0.944268">of Informatics, Athens University of Economics and Business,</affiliation>
<address confidence="0.862064">Curation Unit, Research Centre “Athena”, Athens, Greece</address>
<abstract confidence="0.9996184">We present a system that finds short definitions of terms on Web pages. It employs a Maximum Entropy classifier, but it is trained on automatically generated examples; hence, it is in effect unsupervised. use generate training examples from encyclopedias and Web snippets, a method that outperforms an alternative centroid-based one. After training, our system can be used to find definitions of terms that are not covered by encyclopedias. The system outperforms a comparable publicly available system, as well as a previously published form of our system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Androutsopoulos</author>
<author>D Galanis</author>
</authors>
<title>A Practically Unsupervised Learning Method to Identify Single-Snippet Answers to Definition Questions on the Web. In HLT/EMNLP,</title>
<date>2005</date>
<volume>323</volume>
<pages>330</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="3992" citStr="Androutsopoulos and Galanis, 2005" startWordPosition="649" endWordPosition="652">ing single snippets that include self-contained short definitions. Despite its simpler nature, we believe the task we address is of practical use: a list of single-snippet definitions from Web pages accompanied by the source URLs is a good starting point for users seeking definitions of terms not covered by encyclopedias. We also note that evaluating multi-snippet definitions can be problematic, because it is often difficult to agree which information nuggets should be treated as required, or even optional (Hildebrandt et al., 2004). In contrast, earlier experimental results we have reported (Androutsopoulos and Galanis, 2005) show strong inter-assessor agreement (K &gt; 0.8) for single-snippet definitions (Eugenio and Glass, 2004). The task we address also differs from DUC’s query focused summarization (Dang, 2005; Dang, 2006). Our queries are single terms, whereas DUC queries are longer topic 1270 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1270–1279, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Target term: Babesiosis (...) Babesiosis is a rare, severe and sometimes fatal tickborne disease caused by various types of Babesia, a microscopic parasite that infects r</context>
<context position="7971" citStr="Androutsopoulos and Galanis, 2005" startWordPosition="1294" endWordPosition="1297">ing them is very timeconsuming. In the TREC 2000 and 2001 datasets, it was possible to tag the training windows automatically by using training target terms and accompanying regular expression patterns provided by the TREC organizers. The regular expressions covered all the known acceptable definitions of the corresponding terms that can be extracted from the datasets. When the training windows, however, are obtained from the Web, it is impossible to construct manually regular expressions for all the possible phrasings of the acceptable definitions in the training windows. In subsequent work (Androutsopoulos and Galanis, 2005), we developed ATTW (automatic tagging of training windows), a technique that produces arbitrarily large collections of training windows from the Web with practically no manual effort, in effect making our overall system unsupervised. ATTW uses training terms for which several encyclopedia definitions are available, and compares each Web training window (each window extracted from the pages the search engine returned for a training term) to the corresponding encyclopedia definitions. Web training windows that are very similar (or dissimilar) to the corresponding encyclopedia definitions are ta</context>
<context position="9873" citStr="Androutsopoulos and Galanis, 2005" startWordPosition="1585" endWordPosition="1589">ng from on-line encyclopedias, simulating the case where we seek definitions of terms not covered by encyclopedias; we also ignore windows from on-line encyclopedias during training. Also, note that the classifier is trained on Web windows, not directly on encyclopedia definitions, which allows it to avoid relying excessively on phrasings that are common in encyclopedia definitions, but uncommon in more indirect definitions of arbitrary Web pages. Fur1271 thermore, training the classifier directly on encyclopedia definitions would not provide negative examples. In our previous work with ATTW (Androutsopoulos and Galanis, 2005) we used a measure constructed by ourselves to assess the similarity between Web windows and encyclopedia definitions. Here, we use the more established ROUGE-W measure (Lin, 2004) instead. ROUGEW and other versions of ROUGE have been used in summarization to measure how close a machineauthored summary is to multiple human summaries of the same input. We use ROUGE-W in a similar setting, to measure how close a training window is to multiple encyclopedia definitions of the same term. A further difference from our previous work is that we also use ROUGE-W when computing the features of the windo</context>
<context position="23507" citStr="Androutsopoulos and Galanis, 2005" startWordPosition="3933" endWordPosition="3936"> WC: We create a simple centroid of the window’s target term, much as in section 2.2. The centroid’s words are chosen based on their frequency in the r· f windows of the target term; the 20 most frequent words are chosen. WC is the percentage of the 20 words that appear in the vector’s window. Manual patterns: 13 Boolean features, each signaling if the window matches a different manually constructed lexical pattern (e.g., “target, a/an/the”, as in “Tony Blair, the British prime minister”). The patterns are those used by Joho and Sanderson (2000), and four more introduced in our previous work (Androutsopoulos and Galanis, 2005) and (Miliaraki and Androutsopoulos, 2004). They are intended to perform well across text genres. Automatic patterns: m numeric features, each showing the degree to which the window matches a different automatically acquired lexical pattern. The patterns are word n-grams (n E 11, 2, 3}) that must occur directly before or after the target term (e.g., “target which is”). The patterns are acquired as follows. First, all the n-grams directly before or after any target term in the training windows are collected. The n-grams that have been encountered at least 10 times are candidate patterns. From t</context>
<context position="32266" citStr="Androutsopoulos and Galanis, 2005" startWordPosition="5463" endWordPosition="5466">etter as the size of the training set increases. The baselines contain no learning components; hence, their curves are flat. We also show the results (Base-Attrs) of our system when the features that correspond to automatically acquired patterns are excluded. Clearly, these patterns help our system achieve significantly better results; however, our system outperforms Cui et al.’s even without them. Without the automatic patterns, our system also shows signs of saturation as the training data increase. Figures 5 and 6 show the performance of our new system against our previously published one (Androutsopoulos and Galanis, 2005); the new system clearly outperforms the old one. Additional experiments we conducted with the old system replacing the SVM by the MAXENT classifier (without using ROUGE-W) indicate that the use of MAXENT by itself also improved slightly the results, but the differences are too minor to show; the improvement is mostly due to the use of ROUGEW instead of our previous measure. 5 Related work Xu et al. (2004) use an information extraction engine to extract linguistic features from documents relevant to the target term. The features are mostly phrases, such as appositives, and phrases expressing r</context>
</contexts>
<marker>Androutsopoulos, Galanis, 2005</marker>
<rawString>Androutsopoulos, I., and Galanis, D. 2005. A Practically Unsupervised Learning Method to Identify Single-Snippet Answers to Definition Questions on the Web. In HLT/EMNLP, Vancouver, Canada, 323– 330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Blair-Goldensohn</author>
<author>K McKeown</author>
<author>A H Schlaikjer</author>
</authors>
<title>A Hybrid Approach for QA Track Definitional Questions.</title>
<date>2003</date>
<booktitle>In TREC 2003,</booktitle>
<location>Gaithersburg, MD, USA.</location>
<contexts>
<context position="33134" citStr="Blair-Goldensohn et al. (2003" startWordPosition="5608" endWordPosition="5611">he results, but the differences are too minor to show; the improvement is mostly due to the use of ROUGEW instead of our previous measure. 5 Related work Xu et al. (2004) use an information extraction engine to extract linguistic features from documents relevant to the target term. The features are mostly phrases, such as appositives, and phrases expressing relations. The features are then ranked by their type and similarity to a centroid, and the most highly ranked ones are returned. Xu et al. seem to aim at generating multi-snippet definitions, unlike the single-snippet definitions we seek. Blair-Goldensohn et al. (2003; 2004) extract sentences that may provide definitional informaFigure 5: Correct responses of our new and previous system, allowing 5 answers per question. Figure 6: MRR of our new and previous system. tion from documents retrieved for the target term; a decision tree learner and manually tagged training data are used. The sentences are then matched against manually constructed patterns, which operate on syntax trees, to detect sentences expressing the target term’s genus, species, or both (genus+species). The system composes its answer by placing first the genus+species sentence that is close</context>
</contexts>
<marker>Blair-Goldensohn, McKeown, Schlaikjer, 2003</marker>
<rawString>Blair-Goldensohn, S., McKeown, K., Schlaikjer, A.H. 2003. A Hybrid Approach for QA Track Definitional Questions. In TREC 2003, Gaithersburg, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Blair-Goldensohn</author>
<author>K R McKeown</author>
<author>A H Schlaikjer</author>
</authors>
<title>Answering Definitional Questions: A Hybrid Approach. In</title>
<date>2004</date>
<publisher>AAAI Press.</publisher>
<marker>Blair-Goldensohn, McKeown, Schlaikjer, 2004</marker>
<rawString>Blair-Goldensohn, S., McKeown, K.R., and Schlaikjer, A.H. 2004. Answering Definitional Questions: A Hybrid Approach. In Maybury, M. (Ed.), New Directions in Question answering, AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
<author>J Prager</author>
<author>A Ittycheriah</author>
<author>S Blair-Goldensohn</author>
</authors>
<date>2004</date>
<booktitle>IBM’s PIQUANT II in TREC 2004. In TREC 2004,</booktitle>
<location>Gaithersburg, MD, USA.</location>
<contexts>
<context position="29548" citStr="Chu-Carroll et al. (2004" startWordPosition="5007" endWordPosition="5010"> Apart from the modifications we mentioned, we use Cui et al.’s original implementation. 3.3 Baseline methods The first baseline selects the first window of each one of the five highest ranked Web pages, as returned by the search engine, and returns the five windows. The second baseline returns five windows chosen randomly from the r · f = 10 · 5 available ones. The third baseline (centroid baseline) creates a centroid of the r · f windows, as in section 2.2, and returns the five windows with the highest cosine similarity to the centroid.11 11We also reimplemented the definitions component of Chu-Carroll et al. (2004; 2005), but its performance was worse than our centroid baseline. Figure 3: Correct responses, 5 answers/question. 4 Evaluation of systems We used q training target terms in the experiments of this section, with q ranging from 50 to 1500, and 200 testing terms, with no overlap between training and testing terms, and excluding terms that had been used for other purpose.12 We had to use testing terms for which encyclopedia definitions were also available, to judge the acceptability of the systems’ responses, since many terms are highly technical. We discarded, however, windows extracted from en</context>
</contexts>
<marker>Chu-Carroll, Czuba, Prager, Ittycheriah, Blair-Goldensohn, 2004</marker>
<rawString>Chu-Carroll, J., Czuba, K., Prager, J., Ittycheriah, A., Blair-Goldensohn, S. 2004. IBM’s PIQUANT II in TREC 2004. In TREC 2004, Gaithersburg, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
<author>P Duboue</author>
<author>J Prager</author>
</authors>
<title>IBM’s PIQUANT II in TREC</title>
<date>2005</date>
<booktitle>In TREC 2005,</booktitle>
<location>Gaithersburg, MD, USA.</location>
<marker>Chu-Carroll, Czuba, Duboue, Prager, 2005</marker>
<rawString>Chu-Carroll, J., Czuba, K., Duboue, P., and Prager, J. 2005. IBM’s PIQUANT II in TREC 2005. In TREC 2005, Gaithersburg, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
</authors>
<title>An Introduction to SVMs.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Cristianini, N. and Shawe-Taylor, J. 2000. An Introduction to SVMs. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>M-Y Kan</author>
<author>T-S Chua</author>
<author>J Xiao</author>
</authors>
<title>A Comparative Study on Sentence Retrieval for Definitional Question Answering.</title>
<date>2004</date>
<booktitle>In SIGIR workshop on Information Retrieval for Question Answering,</booktitle>
<location>Salvador, Brazil.</location>
<contexts>
<context position="16486" citStr="Cui et al. (2004" startWordPosition="2708" endWordPosition="2711">es the longest common substring) contains almost all the information of w, i.e., w does not contain any (redundant) information not included in a known acceptable definition, something we care less for. The ROUGE-W similarity sim(w, Ct) between w and Ct is the maximum F(w, d), for all d E Ct. Training windows with sim(w, Ct) &gt; T+ are tagged as positive; if sim(w, Ct) &lt; T−, they are tagged as negative; and if T− &lt; sim(w, Ct) &lt; T+, they are discarded. We tune the thresholds T+ and T− experimentally, as discussed below. 2.2 The centroid-based tagging approach This method is used in the system of Cui et al. (2004; 2005; 2006; 2007). For each training target term, we construct a “centroid” pseudo-text containing the words that co-occur most frequently with the target term. We then compute the similarity between each training window and the centroid of its target term. If it exceeds a threshold, the window is tagged as positive; Cui et al. produce only positive examples. The centroid of a training target term t is constructed as follows. For each word u in t’s training windows, we compute the centrality score defined below, where SFt is the number of t’s training windows, SFu is the number of u’s window</context>
<context position="24743" citStr="Cui et al. (2004" startWordPosition="4142" endWordPosition="4145"> with the highest precision scores are retained, where precision is the number of positive training windows the pattern matches over the total number of training windows it matches; we use m = 300 in our experiments, based on the results of our previous work. The automatically acquired patterns allow the system to detect definition contexts that are not captured by the manual patterns, including genre-specific contexts. The value of each feature is the ROUGE-W score between a pattern and the left or right context of the target term in the window. 3.2 Cui et al.’s system Given a target term t, Cui et al. (2004; 2005; 2006; 2007) initially locate sentences containing t in relevant documents. We use the r·f = 10·5 windows from the pages returned by the search engine, instead of sentences. Cui et al. then construct the centroid of t, and compute the cosine similarity of each one of the r · f windows to the centroid, as in section 2.2. The 10 windows that are closer to the centroid are considered candidate answers. All candidate answers are then processed by a part-ofspeech (POS) tagger and a chunker. The words of the centroid are replaced in all the candidate answers by their POS tags; the target term</context>
</contexts>
<marker>Cui, Kan, Chua, Xiao, 2004</marker>
<rawString>Cui, H., Kan, M.-Y., Chua, T.-S., and Xiao, J. 2004. A Comparative Study on Sentence Retrieval for Definitional Question Answering. In SIGIR workshop on Information Retrieval for Question Answering, Salvador, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>M Y Kan</author>
<author>T S Chua</author>
</authors>
<title>Unsupervised Learning of Soft Patterns for Generating Definitions from Online News. In WWW,</title>
<date>2004</date>
<location>New York, NY, USA.</location>
<contexts>
<context position="16486" citStr="Cui et al. (2004" startWordPosition="2708" endWordPosition="2711">es the longest common substring) contains almost all the information of w, i.e., w does not contain any (redundant) information not included in a known acceptable definition, something we care less for. The ROUGE-W similarity sim(w, Ct) between w and Ct is the maximum F(w, d), for all d E Ct. Training windows with sim(w, Ct) &gt; T+ are tagged as positive; if sim(w, Ct) &lt; T−, they are tagged as negative; and if T− &lt; sim(w, Ct) &lt; T+, they are discarded. We tune the thresholds T+ and T− experimentally, as discussed below. 2.2 The centroid-based tagging approach This method is used in the system of Cui et al. (2004; 2005; 2006; 2007). For each training target term, we construct a “centroid” pseudo-text containing the words that co-occur most frequently with the target term. We then compute the similarity between each training window and the centroid of its target term. If it exceeds a threshold, the window is tagged as positive; Cui et al. produce only positive examples. The centroid of a training target term t is constructed as follows. For each word u in t’s training windows, we compute the centrality score defined below, where SFt is the number of t’s training windows, SFu is the number of u’s window</context>
<context position="24743" citStr="Cui et al. (2004" startWordPosition="4142" endWordPosition="4145"> with the highest precision scores are retained, where precision is the number of positive training windows the pattern matches over the total number of training windows it matches; we use m = 300 in our experiments, based on the results of our previous work. The automatically acquired patterns allow the system to detect definition contexts that are not captured by the manual patterns, including genre-specific contexts. The value of each feature is the ROUGE-W score between a pattern and the left or right context of the target term in the window. 3.2 Cui et al.’s system Given a target term t, Cui et al. (2004; 2005; 2006; 2007) initially locate sentences containing t in relevant documents. We use the r·f = 10·5 windows from the pages returned by the search engine, instead of sentences. Cui et al. then construct the centroid of t, and compute the cosine similarity of each one of the r · f windows to the centroid, as in section 2.2. The 10 windows that are closer to the centroid are considered candidate answers. All candidate answers are then processed by a part-ofspeech (POS) tagger and a chunker. The words of the centroid are replaced in all the candidate answers by their POS tags; the target term</context>
</contexts>
<marker>Cui, Kan, Chua, 2004</marker>
<rawString>Cui, H., Kan, M.Y., Chua, T.S. 2004. Unsupervised Learning of Soft Patterns for Generating Definitions from Online News. In WWW, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>M Y Kan</author>
<author>T S Chua</author>
</authors>
<title>Generic Soft Pattern Models for Definitional Question Answering.</title>
<date>2005</date>
<booktitle>In ACM SIGIR,</booktitle>
<location>Salvador, Brazil.</location>
<contexts>
<context position="25964" citStr="Cui et al. (2005)" startWordPosition="4357" endWordPosition="4360">un phrases, forms of the verb “to be”, and articles are replaced by special tags (e.g., TARGET, NP), while adjectives and adverbs are removed. The candidate answers are then cropped to L tokens to the left and right of the target term, producing two subsequences (left and right) per candidate answer; we set L = 3, which is Cui et al.’s default. Cui et al. experimented with two approaches to rank the candidate answers, called Bigram Model and Profile Hidden Markov Model (PHMM). Both are learning components that produce soft patterns, though PHMM is much more complicated. In their earlier work, Cui et al. (2005) found the Bigram Model to perform better than PHMM; in more recent experiments with more data (Cui, 2006; Cui et al., 2007) they found PHMM to perform better, but the difference was not statistically significant. Given these results and the complexity of PHMM, we experimented only with the Bigram Model. In the Bigram Model, the left and right subsequences of each candidate answer are considered separately. Below S1, ... , SL refer to the slots (word positions) of a (left or right) subsequence, and t1, ... , tL to the particular words in the slots. For each subsequence (S1 = t1, ... , SL = tL)</context>
</contexts>
<marker>Cui, Kan, Chua, 2005</marker>
<rawString>Cui, H., Kan, M.Y., Chua, T.S. 2005. Generic Soft Pattern Models for Definitional Question Answering. In ACM SIGIR, Salvador, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
</authors>
<title>Soft Matching for Question Answering.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>National University of Singapore.</institution>
<contexts>
<context position="26069" citStr="Cui, 2006" startWordPosition="4378" endWordPosition="4379">ives and adverbs are removed. The candidate answers are then cropped to L tokens to the left and right of the target term, producing two subsequences (left and right) per candidate answer; we set L = 3, which is Cui et al.’s default. Cui et al. experimented with two approaches to rank the candidate answers, called Bigram Model and Profile Hidden Markov Model (PHMM). Both are learning components that produce soft patterns, though PHMM is much more complicated. In their earlier work, Cui et al. (2005) found the Bigram Model to perform better than PHMM; in more recent experiments with more data (Cui, 2006; Cui et al., 2007) they found PHMM to perform better, but the difference was not statistically significant. Given these results and the complexity of PHMM, we experimented only with the Bigram Model. In the Bigram Model, the left and right subsequences of each candidate answer are considered separately. Below S1, ... , SL refer to the slots (word positions) of a (left or right) subsequence, and t1, ... , tL to the particular words in the slots. For each subsequence (S1 = t1, ... , SL = tL) of a candidate answer, we first estimate: P(ti|Si) = |Si(ti) |+ δ Et, |Si(ti) |+ δ · N P(ti|ti−1) = |Si(</context>
</contexts>
<marker>Cui, 2006</marker>
<rawString>Cui, H. 2006. Soft Matching for Question Answering. Ph.D. thesis, National University of Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>M Kan</author>
<author>T Chua</author>
</authors>
<title>Soft Pattern Matching Models for Definitional Question Answering.</title>
<date>2007</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="26088" citStr="Cui et al., 2007" startWordPosition="4380" endWordPosition="4383">verbs are removed. The candidate answers are then cropped to L tokens to the left and right of the target term, producing two subsequences (left and right) per candidate answer; we set L = 3, which is Cui et al.’s default. Cui et al. experimented with two approaches to rank the candidate answers, called Bigram Model and Profile Hidden Markov Model (PHMM). Both are learning components that produce soft patterns, though PHMM is much more complicated. In their earlier work, Cui et al. (2005) found the Bigram Model to perform better than PHMM; in more recent experiments with more data (Cui, 2006; Cui et al., 2007) they found PHMM to perform better, but the difference was not statistically significant. Given these results and the complexity of PHMM, we experimented only with the Bigram Model. In the Bigram Model, the left and right subsequences of each candidate answer are considered separately. Below S1, ... , SL refer to the slots (word positions) of a (left or right) subsequence, and t1, ... , tL to the particular words in the slots. For each subsequence (S1 = t1, ... , SL = tL) of a candidate answer, we first estimate: P(ti|Si) = |Si(ti) |+ δ Et, |Si(ti) |+ δ · N P(ti|ti−1) = |Si(ti) n Si−1(ti−1)| |</context>
</contexts>
<marker>Cui, Kan, Chua, 2007</marker>
<rawString>Cui, H., Kan, M., and Chua, T. 2007. Soft Pattern Matching Models for Definitional Question Answering. ACM Transactions on Information Systems, 25(2):1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2005</date>
<booktitle>In DUC at HLT-EMNLP,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="4181" citStr="Dang, 2005" startWordPosition="680" endWordPosition="681"> the source URLs is a good starting point for users seeking definitions of terms not covered by encyclopedias. We also note that evaluating multi-snippet definitions can be problematic, because it is often difficult to agree which information nuggets should be treated as required, or even optional (Hildebrandt et al., 2004). In contrast, earlier experimental results we have reported (Androutsopoulos and Galanis, 2005) show strong inter-assessor agreement (K &gt; 0.8) for single-snippet definitions (Eugenio and Glass, 2004). The task we address also differs from DUC’s query focused summarization (Dang, 2005; Dang, 2006). Our queries are single terms, whereas DUC queries are longer topic 1270 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1270–1279, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Target term: Babesiosis (...) Babesiosis is a rare, severe and sometimes fatal tickborne disease caused by various types of Babesia, a microscopic parasite that infects red blood cells. In New York state, the causative parasite is babesia microti. Who gets Babesiosis? Babesiosis (...) Target term: anorexia nervosa (...) anorexia nervosa is an illness that u</context>
</contexts>
<marker>Dang, 2005</marker>
<rawString>Dang, H. T. 2005. Overview of DUC 2005. In DUC at HLT-EMNLP, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2006</date>
<booktitle>In DUC at HLT-NAACL,</booktitle>
<location>New York, NY, USA.</location>
<contexts>
<context position="4194" citStr="Dang, 2006" startWordPosition="682" endWordPosition="683">URLs is a good starting point for users seeking definitions of terms not covered by encyclopedias. We also note that evaluating multi-snippet definitions can be problematic, because it is often difficult to agree which information nuggets should be treated as required, or even optional (Hildebrandt et al., 2004). In contrast, earlier experimental results we have reported (Androutsopoulos and Galanis, 2005) show strong inter-assessor agreement (K &gt; 0.8) for single-snippet definitions (Eugenio and Glass, 2004). The task we address also differs from DUC’s query focused summarization (Dang, 2005; Dang, 2006). Our queries are single terms, whereas DUC queries are longer topic 1270 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1270–1279, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Target term: Babesiosis (...) Babesiosis is a rare, severe and sometimes fatal tickborne disease caused by various types of Babesia, a microscopic parasite that infects red blood cells. In New York state, the causative parasite is babesia microti. Who gets Babesiosis? Babesiosis (...) Target term: anorexia nervosa (...) anorexia nervosa is an illness that usually occurs</context>
</contexts>
<marker>Dang, 2006</marker>
<rawString>Dang, H. T. 2006. Overview of DUC 2006. In DUC at HLT-NAACL, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B D Eugenio</author>
<author>M Glass</author>
</authors>
<title>The Kappa Statistic: a Second Look. Computational Linguistics,</title>
<date>2004</date>
<pages>301--1</pages>
<contexts>
<context position="4096" citStr="Eugenio and Glass, 2004" startWordPosition="664" endWordPosition="668"> we address is of practical use: a list of single-snippet definitions from Web pages accompanied by the source URLs is a good starting point for users seeking definitions of terms not covered by encyclopedias. We also note that evaluating multi-snippet definitions can be problematic, because it is often difficult to agree which information nuggets should be treated as required, or even optional (Hildebrandt et al., 2004). In contrast, earlier experimental results we have reported (Androutsopoulos and Galanis, 2005) show strong inter-assessor agreement (K &gt; 0.8) for single-snippet definitions (Eugenio and Glass, 2004). The task we address also differs from DUC’s query focused summarization (Dang, 2005; Dang, 2006). Our queries are single terms, whereas DUC queries are longer topic 1270 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1270–1279, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Target term: Babesiosis (...) Babesiosis is a rare, severe and sometimes fatal tickborne disease caused by various types of Babesia, a microscopic parasite that infects red blood cells. In New York state, the causative parasite is babesia microti. Who gets Babesiosis? Babes</context>
</contexts>
<marker>Eugenio, Glass, 2004</marker>
<rawString>Eugenio, B. D., Glass, M. 2004. The Kappa Statistic: a Second Look. Computational Linguistics, 301(1):95-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Galanis</author>
<author>G Karakatsiotis</author>
<author>G Lampouras</author>
<author>I Androutsopoulos</author>
</authors>
<title>An Open-Source Natural Language Generator for OWL Ontologies and its Use in Protege and Second Life. EACL system demonstration,</title>
<date>2009</date>
<location>Athens, Greece.</location>
<contexts>
<context position="36848" citStr="Galanis et al., 2009" startWordPosition="6194" endWordPosition="6197">re linguistic processing tools, such as named entity recognizers, POS taggers, chunkers, parsers; hence, it can be easily used in languages where such tools are unavailable. It could be improved by exploiting the HTML markup of Web pages and the Web’s hyperlinks. For example, the target term is sometimes written in italics in definitions, and some definitions are provided on pages (e.g., pop-up windows) that occurrences of the target term link to. The work reported here was conducted in the context of project INDIGO, where an autonomous robotic guide for museum collections is being developed (Galanis et al., 2009). The guide engages the museum’s visitors in spoken dialogues, and it describes the exhibits the visitors select by generating spoken natural language descriptions from an ontology. Among other requests, the visitors can ask follow up questions, and we have found that the most common kind of follow up questions are requests to define terms (e.g., names of persons, events, architectural terms, etc.) mentioned in the generated exhibit descriptions. Some of these definition requests can be handled by generating new texts from the ontology, but some times the ontology contains no information for t</context>
</contexts>
<marker>Galanis, Karakatsiotis, Lampouras, Androutsopoulos, 2009</marker>
<rawString>Galanis, D., Karakatsiotis, G., Lampouras, G., and Androutsopoulos, I. 2009. An Open-Source Natural Language Generator for OWL Ontologies and its Use in Protege and Second Life. EACL system demonstration, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Han</author>
<author>H Chung</author>
<author>S B Kim</author>
<author>Y I Song</author>
<author>J Y Lee</author>
<author>H C Rim</author>
</authors>
<date>2004</date>
<journal>Korea University QA System at TREC</journal>
<booktitle>In TREC 2004,</booktitle>
<location>Gaithersburg, MD, USA.</location>
<contexts>
<context position="34302" citStr="Han et al. (2004" startWordPosition="5795" endWordPosition="5798">g first the genus+species sentence that is closer to the centroid of the extracted sentences. The remaining sentences are ranked by their distance from the centroid, and the most highly ranked ones are clustered. The system then selects iteratively the cluster that is closer to the centroid of the extracted sentences and the most recently used cluster. The cluster’s most representative sentence, i.e., the sentence closest to the centroid of the cluster’s sentences, is added to the response. The iterations stop when a maximum response length is reached. Multi-snippet definitions are generated. Han et al. (2004; 2006) parse a definition question to locate the head word of the target term. They also use a named entity recognizer to determine the target term’s type (person, organization, 1277 etc.). They then extract from documents relevant to the target term sentences containing its head word, as well as sentences the extracted ones refer to (e.g., via pronouns). The resulting sentences are matched against manually constructed syntactic patterns to detect phrases conveying definitional information. The resulting phrases are ranked by criteria like the degree to which the phrase contains words common </context>
</contexts>
<marker>Han, Chung, Kim, Song, Lee, Rim, 2004</marker>
<rawString>Han, K.S., Chung, H., Kim, S.B., Song, Y.I., Lee, J.Y., Rim, H.C. 2004. Korea University QA System at TREC 2004. In TREC 2004, Gaithersburg, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Han</author>
<author>Y I Song</author>
<author>S B Kim</author>
<author>H C Rim</author>
</authors>
<title>A Definitional Question Answering System Based on Phrase Extraction Using Syntactic Patterns.</title>
<date>2006</date>
<journal>IEICE Transactions on Information and Systems,</journal>
<volume>89</volume>
<pages>1601--1605</pages>
<marker>Han, Song, Kim, Rim, 2006</marker>
<rawString>Han, K.S., Song, Y.I., Kim, S.B., and Rim, H.C. 2006. A Definitional Question Answering System Based on Phrase Extraction Using Syntactic Patterns. IEICE Transactions on Information and Systems, vol. E89-D, No. 4, 1601–1605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hildebrandt</author>
<author>B Katz</author>
<author>J Lin</author>
</authors>
<title>Answering Definition Questions Using Multiple Knowledge Sources.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>49--56</pages>
<location>Boston, MA, USA,</location>
<contexts>
<context position="3896" citStr="Hildebrandt et al., 2004" startWordPosition="635" endWordPosition="639">quired or optional information nuggets (Voorhees, 2003). In contrast, we focus on locating single snippets that include self-contained short definitions. Despite its simpler nature, we believe the task we address is of practical use: a list of single-snippet definitions from Web pages accompanied by the source URLs is a good starting point for users seeking definitions of terms not covered by encyclopedias. We also note that evaluating multi-snippet definitions can be problematic, because it is often difficult to agree which information nuggets should be treated as required, or even optional (Hildebrandt et al., 2004). In contrast, earlier experimental results we have reported (Androutsopoulos and Galanis, 2005) show strong inter-assessor agreement (K &gt; 0.8) for single-snippet definitions (Eugenio and Glass, 2004). The task we address also differs from DUC’s query focused summarization (Dang, 2005; Dang, 2006). Our queries are single terms, whereas DUC queries are longer topic 1270 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1270–1279, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Target term: Babesiosis (...) Babesiosis is a rare, severe and sometimes f</context>
</contexts>
<marker>Hildebrandt, Katz, Lin, 2004</marker>
<rawString>Hildebrandt, W., Katz, B., and Lin, J. 2004. Answering Definition Questions Using Multiple Knowledge Sources. In HLT-NAACL, Boston, MA, USA, 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Joho</author>
<author>M Sanderson</author>
</authors>
<title>Retrieving Descriptive Phrases from Large Amounts of Free</title>
<date>2000</date>
<booktitle>Text. International Conference on Information and Knowledge Management,</booktitle>
<location>McLean, VA, USA,</location>
<contexts>
<context position="7040" citStr="Joho and Sanderson (2000" startWordPosition="1148" endWordPosition="1151"> was a (...) Table 1: Definitions found by our system. descriptions, often entire paragraphs; furthermore, we do not attempt to compose coherent and cohesive summaries from several snippets. The system we present is based on our earlier work (Miliaraki and Androutsopoulos, 2004), where an SVM classifier (Cristianini and ShaweTaylor, 2000) was used to separate acceptable windows from unacceptable ones; the SVM also returned confidence scores, which were used to rank the acceptable windows. On datasets from the TREC 2000 and 2001 QA tracks, our earlier system clearly outperformed the methods of Joho and Sanderson (2000; 2001) and Prager et al. (2001; 2002), as reported in previous work (Miliaraki and Androutsopoulos, 2004). To train the SVM, however, thousands of training windows were required, each tagged as a positive or negative example. Obtaining large numbers of training windows is easy, but manually tagging them is very timeconsuming. In the TREC 2000 and 2001 datasets, it was possible to tag the training windows automatically by using training target terms and accompanying regular expression patterns provided by the TREC organizers. The regular expressions covered all the known acceptable definitions</context>
<context position="22867" citStr="Joho and Sanderson (2000)" startWordPosition="3824" endWordPosition="3827">following features:10 SN: The ordinal number of the window on the page it originates from (e.g., second window of the target term from the beginning of the page). Early mentions of a term are more likely to define it. RK: The ranking of the Web page the window originates from, as returned by the search engine. the interval (1, 2]. We use a = 1.4, which was the value with the best results on the 400 windows. We did not try a &gt; 2, as the results were declining as a approached 2. 9We do not discuss MAXENT classifiers, since they are a well documented in the literature. 10SN and WC originate from Joho and Sanderson (2000). 1274 WC: We create a simple centroid of the window’s target term, much as in section 2.2. The centroid’s words are chosen based on their frequency in the r· f windows of the target term; the 20 most frequent words are chosen. WC is the percentage of the 20 words that appear in the vector’s window. Manual patterns: 13 Boolean features, each signaling if the window matches a different manually constructed lexical pattern (e.g., “target, a/an/the”, as in “Tony Blair, the British prime minister”). The patterns are those used by Joho and Sanderson (2000), and four more introduced in our previous </context>
</contexts>
<marker>Joho, Sanderson, 2000</marker>
<rawString>Joho, H. and Sanderson, M. 2000. Retrieving Descriptive Phrases from Large Amounts of Free Text. International Conference on Information and Knowledge Management, McLean, VA, USA, 180–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Joho</author>
<author>M Sanderson</author>
</authors>
<title>Large Scale Testing of a Descriptive Phrase Finder. In HLT-NAACL,</title>
<date>2001</date>
<pages>219--221</pages>
<location>San Diego, CA, USA,</location>
<marker>Joho, Sanderson, 2001</marker>
<rawString>Joho, H. and Sanderson, M. 2001. Large Scale Testing of a Descriptive Phrase Finder. In HLT-NAACL, San Diego, CA, USA, 219–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In ACL workshop “Text Summarization Branches Out”,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="10053" citStr="Lin, 2004" startWordPosition="1617" endWordPosition="1618">lassifier is trained on Web windows, not directly on encyclopedia definitions, which allows it to avoid relying excessively on phrasings that are common in encyclopedia definitions, but uncommon in more indirect definitions of arbitrary Web pages. Fur1271 thermore, training the classifier directly on encyclopedia definitions would not provide negative examples. In our previous work with ATTW (Androutsopoulos and Galanis, 2005) we used a measure constructed by ourselves to assess the similarity between Web windows and encyclopedia definitions. Here, we use the more established ROUGE-W measure (Lin, 2004) instead. ROUGEW and other versions of ROUGE have been used in summarization to measure how close a machineauthored summary is to multiple human summaries of the same input. We use ROUGE-W in a similar setting, to measure how close a training window is to multiple encyclopedia definitions of the same term. A further difference from our previous work is that we also use ROUGE-W when computing the features of the windows to be classified. Previously, the SVM relied, among others, on Boolean features indicating if the target term was preceded or followed in the window to be classified by a partic</context>
<context position="15460" citStr="Lin (2004)" startWordPosition="2526" endWordPosition="2527">lowing score (weighted longest common subsequence), where m is the number of consecutive matches, ki is the length of the i-th consecutive match, and f is a weighting function. We use f(k) = ka, where a &gt; 1 is a parameter we tune experimentally. WLCS(w, d) = Emi=0 f(ki) We then compute the following quantities, where |· |is word length, and f−1 is the inverse of f. P (w, d)= f−1(WLCS(w,d) f(|w|) ) R(w, d)= f−1(WLCS(w,d) f(|d|) ) F(w, d) = (1+β2)·R(w,d)·P(w,d) R(w,d)+β2·P(w,d) In effect, P(w, d) examines how close the longest common substring is to w and R(w, d) how close it is to d. Following Lin (2004), we use β = 8, assigning greater importance to R(w, d). If R(w, d) is high, the longest common substring is very similar to d; then w (which also includes the longest common substring) intuitively contains almost all the information of d, i.e., all the information of a known acceptable definition (high recall). If P(w, d) is high, the longest common substring is very similar to w; then d (which also includes the longest common substring) contains almost all the information of w, i.e., w does not contain any (redundant) information not included in a known acceptable definition, something we ca</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Lin, C.Y. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In ACL workshop “Text Summarization Branches Out”, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miliaraki</author>
<author>I Androutsopoulos</author>
</authors>
<title>Learning to Identify Single-Snippet Answers to Definition Questions.</title>
<date>2004</date>
<booktitle>In COLING,</booktitle>
<volume>1360</volume>
<pages>1366</pages>
<location>Geneva,</location>
<contexts>
<context position="6695" citStr="Miliaraki and Androutsopoulos, 2004" startWordPosition="1091" endWordPosition="1094"> as Murasaki Shikibu. It also serves as a kind of travel guide to the world (...) Target term: Jacques Lacan (...) who is Jacques Lacan? John Haber in New York city a primer for pre-post-structuralists Jacques Lacan is a Parisian psychoanalyst who has influenced literary criticism and feminism. He began work in the 1950s, in the Freudian society there. It was a (...) Table 1: Definitions found by our system. descriptions, often entire paragraphs; furthermore, we do not attempt to compose coherent and cohesive summaries from several snippets. The system we present is based on our earlier work (Miliaraki and Androutsopoulos, 2004), where an SVM classifier (Cristianini and ShaweTaylor, 2000) was used to separate acceptable windows from unacceptable ones; the SVM also returned confidence scores, which were used to rank the acceptable windows. On datasets from the TREC 2000 and 2001 QA tracks, our earlier system clearly outperformed the methods of Joho and Sanderson (2000; 2001) and Prager et al. (2001; 2002), as reported in previous work (Miliaraki and Androutsopoulos, 2004). To train the SVM, however, thousands of training windows were required, each tagged as a positive or negative example. Obtaining large numbers of t</context>
<context position="23549" citStr="Miliaraki and Androutsopoulos, 2004" startWordPosition="3938" endWordPosition="3941">window’s target term, much as in section 2.2. The centroid’s words are chosen based on their frequency in the r· f windows of the target term; the 20 most frequent words are chosen. WC is the percentage of the 20 words that appear in the vector’s window. Manual patterns: 13 Boolean features, each signaling if the window matches a different manually constructed lexical pattern (e.g., “target, a/an/the”, as in “Tony Blair, the British prime minister”). The patterns are those used by Joho and Sanderson (2000), and four more introduced in our previous work (Androutsopoulos and Galanis, 2005) and (Miliaraki and Androutsopoulos, 2004). They are intended to perform well across text genres. Automatic patterns: m numeric features, each showing the degree to which the window matches a different automatically acquired lexical pattern. The patterns are word n-grams (n E 11, 2, 3}) that must occur directly before or after the target term (e.g., “target which is”). The patterns are acquired as follows. First, all the n-grams directly before or after any target term in the training windows are collected. The n-grams that have been encountered at least 10 times are candidate patterns. From those, the m patterns with the highest prec</context>
</contexts>
<marker>Miliaraki, Androutsopoulos, 2004</marker>
<rawString>Miliaraki, S. and Androutsopoulos, I. 2004. Learning to Identify Single-Snippet Answers to Definition Questions. In COLING, Geneva, Switzerland, 1360– 1366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>D Radev</author>
<author>K Czuba</author>
</authors>
<title>Answering What-Is Questions by Virtual Annotation.</title>
<date>2001</date>
<booktitle>In HLTNAACL,</booktitle>
<pages>26--30</pages>
<location>San Diego, CA, USA,</location>
<contexts>
<context position="7071" citStr="Prager et al. (2001" startWordPosition="1154" endWordPosition="1157">und by our system. descriptions, often entire paragraphs; furthermore, we do not attempt to compose coherent and cohesive summaries from several snippets. The system we present is based on our earlier work (Miliaraki and Androutsopoulos, 2004), where an SVM classifier (Cristianini and ShaweTaylor, 2000) was used to separate acceptable windows from unacceptable ones; the SVM also returned confidence scores, which were used to rank the acceptable windows. On datasets from the TREC 2000 and 2001 QA tracks, our earlier system clearly outperformed the methods of Joho and Sanderson (2000; 2001) and Prager et al. (2001; 2002), as reported in previous work (Miliaraki and Androutsopoulos, 2004). To train the SVM, however, thousands of training windows were required, each tagged as a positive or negative example. Obtaining large numbers of training windows is easy, but manually tagging them is very timeconsuming. In the TREC 2000 and 2001 datasets, it was possible to tag the training windows automatically by using training target terms and accompanying regular expression patterns provided by the TREC organizers. The regular expressions covered all the known acceptable definitions of the corresponding terms tha</context>
</contexts>
<marker>Prager, Radev, Czuba, 2001</marker>
<rawString>Prager, J., Radev, D., and Czuba, K. 2001. Answering What-Is Questions by Virtual Annotation. In HLTNAACL, San Diego, CA, USA, 26–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
</authors>
<title>Use of WordNet Hypernyms for Answering What-Is Questions.</title>
<date>2002</date>
<booktitle>In TREC 2001,</booktitle>
<location>Gaithersburg, MD, USA.</location>
<marker>Prager, Chu-Carroll, Czuba, 2002</marker>
<rawString>Prager, J., Chu-Carroll, J., and Czuba, K. 2002. Use of WordNet Hypernyms for Answering What-Is Questions. In TREC 2001, Gaithersburg, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A Simple Introduction to Maximum Entropy Models for Natural Language Processing.</title>
<date>1997</date>
<tech>Technical Report 97-08,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania,</institution>
<contexts>
<context position="11212" citStr="Ratnaparkhi, 1997" startWordPosition="1813" endWordPosition="1814">eded or followed in the window to be classified by a particular phrase indicating a definition (e.g., “target, a kind of”, “such as target”). The indicative phrases are selected automatically during training, but now the corresponding features are not Boolean; their values are the ROUGEW similarity scores between an indicative phrase and the context of the target term in the window. This allows the system to soft-match the phrases to the windows (e.g., encountering “target, another kind of”, instead of “target, a kind of”).1 In our new system we also use a Maximum Entropy (MAXENT) classifier (Ratnaparkhi, 1997) instead of an SVM, because much faster implementations of the former are available.2 We present experimental results showing that our new system significantly outperforms our previously published one. The use of the MAXENT classifier by itself improved slightly our results, but the improvements come mostly from using ROUGE-W. Apart from presenting an improved version of our system, the main contribution of this paper is a detailed experimental comparison of our new system against Cui et al.’s (2004; 2005; 2006; 2007). The latter is particularly interesting, because it is well published, it in</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Ratnaparkhi A. 1997. A Simple Introduction to Maximum Entropy Models for Natural Language Processing. Technical Report 97-08, Institute for Research in Cognitive Science, University of Pennsylvania, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the TREC-9 Question Answering Track.</title>
<date>2000</date>
<location>NIST, USA.</location>
<contexts>
<context position="3024" citStr="Voorhees, 2000" startWordPosition="499" endWordPosition="500">erm, and they are then classified as acceptable (positive class) or unacceptable (negative class) using supervised machine learning. The system reports the windows for which it is most confident that they belong in the positive class. Table 1 shows examples of short definitions found by our system. In our experiments, we allow the system to return up to five windows per target term, and the system’s response is counted as correct if any of the returned windows contains an acceptable short definition of the target. This is similar to the treatment of definition questions in TREC 2000 and 2001 (Voorhees, 2000; Voorhees, 2001), but the answer is sought on the Web, not in a given document collection of a particular genre. More recent TREC QA tracks required definition questions to be answered by lists of complementary text snippets, jointly providing required or optional information nuggets (Voorhees, 2003). In contrast, we focus on locating single snippets that include self-contained short definitions. Despite its simpler nature, we believe the task we address is of practical use: a list of single-snippet definitions from Web pages accompanied by the source URLs is a good starting point for users s</context>
</contexts>
<marker>Voorhees, 2000</marker>
<rawString>Voorhees, E.M. 2000. Overview of the TREC-9 Question Answering Track. NIST, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2001</date>
<location>NIST, USA.</location>
<contexts>
<context position="975" citStr="Voorhees, 2001" startWordPosition="155" endWordPosition="156">, but it is trained on automatically generated examples; hence, it is in effect unsupervised. We use ROUGE-W to generate training examples from encyclopedias and Web snippets, a method that outperforms an alternative centroid-based one. After training, our system can be used to find definitions of terms that are not covered by encyclopedias. The system outperforms a comparable publicly available system, as well as a previously published form of our system. 1 Introduction Definitions of terms are among the most common types of information users search for on the Web. In the TREC 2001 QA track (Voorhees, 2001), where the distribution of question types reflected that of real user logs, 27% of the questions were requests for definitions (e.g., “What is gasohol?”, “Who was Duke Ellington?”). Consequently, some Web search engines provide special facilities (e.g., Google’s “define:” query prefix) that seek definitions of user-specified terms in online encyclopedias or glossaries; to save space, we call both “encyclopedias”. There are, however, often terms that are too recent, too old, or less widely used to be included in encyclopedias. Their definitions may be present on other Web pages (e.g., newspape</context>
<context position="3041" citStr="Voorhees, 2001" startWordPosition="501" endWordPosition="502">e then classified as acceptable (positive class) or unacceptable (negative class) using supervised machine learning. The system reports the windows for which it is most confident that they belong in the positive class. Table 1 shows examples of short definitions found by our system. In our experiments, we allow the system to return up to five windows per target term, and the system’s response is counted as correct if any of the returned windows contains an acceptable short definition of the target. This is similar to the treatment of definition questions in TREC 2000 and 2001 (Voorhees, 2000; Voorhees, 2001), but the answer is sought on the Web, not in a given document collection of a particular genre. More recent TREC QA tracks required definition questions to be answered by lists of complementary text snippets, jointly providing required or optional information nuggets (Voorhees, 2003). In contrast, we focus on locating single snippets that include self-contained short definitions. Despite its simpler nature, we believe the task we address is of practical use: a list of single-snippet definitions from Web pages accompanied by the source URLs is a good starting point for users seeking definition</context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>Voorhees, E.M. 2001. Overview of the TREC 2001 Question Answering Track. NIST, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<date>2001</date>
<journal>The TREC QA Track. Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="975" citStr="Voorhees, 2001" startWordPosition="155" endWordPosition="156">, but it is trained on automatically generated examples; hence, it is in effect unsupervised. We use ROUGE-W to generate training examples from encyclopedias and Web snippets, a method that outperforms an alternative centroid-based one. After training, our system can be used to find definitions of terms that are not covered by encyclopedias. The system outperforms a comparable publicly available system, as well as a previously published form of our system. 1 Introduction Definitions of terms are among the most common types of information users search for on the Web. In the TREC 2001 QA track (Voorhees, 2001), where the distribution of question types reflected that of real user logs, 27% of the questions were requests for definitions (e.g., “What is gasohol?”, “Who was Duke Ellington?”). Consequently, some Web search engines provide special facilities (e.g., Google’s “define:” query prefix) that seek definitions of user-specified terms in online encyclopedias or glossaries; to save space, we call both “encyclopedias”. There are, however, often terms that are too recent, too old, or less widely used to be included in encyclopedias. Their definitions may be present on other Web pages (e.g., newspape</context>
<context position="3041" citStr="Voorhees, 2001" startWordPosition="501" endWordPosition="502">e then classified as acceptable (positive class) or unacceptable (negative class) using supervised machine learning. The system reports the windows for which it is most confident that they belong in the positive class. Table 1 shows examples of short definitions found by our system. In our experiments, we allow the system to return up to five windows per target term, and the system’s response is counted as correct if any of the returned windows contains an acceptable short definition of the target. This is similar to the treatment of definition questions in TREC 2000 and 2001 (Voorhees, 2000; Voorhees, 2001), but the answer is sought on the Web, not in a given document collection of a particular genre. More recent TREC QA tracks required definition questions to be answered by lists of complementary text snippets, jointly providing required or optional information nuggets (Voorhees, 2003). In contrast, we focus on locating single snippets that include self-contained short definitions. Despite its simpler nature, we believe the task we address is of practical use: a list of single-snippet definitions from Web pages accompanied by the source URLs is a good starting point for users seeking definition</context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>Voorhees, E.M. 2001. The TREC QA Track. Natural Language Engineering, 7(4):361–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Evaluating Answers to Definition Questions. In HLT-NAACL,</title>
<date>2003</date>
<location>Edmonton, Canada.</location>
<contexts>
<context position="3326" citStr="Voorhees, 2003" startWordPosition="547" endWordPosition="548">n our experiments, we allow the system to return up to five windows per target term, and the system’s response is counted as correct if any of the returned windows contains an acceptable short definition of the target. This is similar to the treatment of definition questions in TREC 2000 and 2001 (Voorhees, 2000; Voorhees, 2001), but the answer is sought on the Web, not in a given document collection of a particular genre. More recent TREC QA tracks required definition questions to be answered by lists of complementary text snippets, jointly providing required or optional information nuggets (Voorhees, 2003). In contrast, we focus on locating single snippets that include self-contained short definitions. Despite its simpler nature, we believe the task we address is of practical use: a list of single-snippet definitions from Web pages accompanied by the source URLs is a good starting point for users seeking definitions of terms not covered by encyclopedias. We also note that evaluating multi-snippet definitions can be problematic, because it is often difficult to agree which information nuggets should be treated as required, or even optional (Hildebrandt et al., 2004). In contrast, earlier experim</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>Voorhees, E.M. 2003. Evaluating Answers to Definition Questions. In HLT-NAACL, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>R Weischedel</author>
<author>A Licuanan</author>
</authors>
<title>Evaluation of an Extraction-based Approach to Answering Definitional Questions.</title>
<date>2004</date>
<booktitle>In ACM SIGIR,</booktitle>
<location>Sheffield, UK.</location>
<contexts>
<context position="32675" citStr="Xu et al. (2004)" startWordPosition="5536" endWordPosition="5539">terns, our system also shows signs of saturation as the training data increase. Figures 5 and 6 show the performance of our new system against our previously published one (Androutsopoulos and Galanis, 2005); the new system clearly outperforms the old one. Additional experiments we conducted with the old system replacing the SVM by the MAXENT classifier (without using ROUGE-W) indicate that the use of MAXENT by itself also improved slightly the results, but the differences are too minor to show; the improvement is mostly due to the use of ROUGEW instead of our previous measure. 5 Related work Xu et al. (2004) use an information extraction engine to extract linguistic features from documents relevant to the target term. The features are mostly phrases, such as appositives, and phrases expressing relations. The features are then ranked by their type and similarity to a centroid, and the most highly ranked ones are returned. Xu et al. seem to aim at generating multi-snippet definitions, unlike the single-snippet definitions we seek. Blair-Goldensohn et al. (2003; 2004) extract sentences that may provide definitional informaFigure 5: Correct responses of our new and previous system, allowing 5 answers</context>
</contexts>
<marker>Xu, Weischedel, Licuanan, 2004</marker>
<rawString>Xu, J., Weischedel, R., Licuanan, A. 2004. Evaluation of an Extraction-based Approach to Answering Definitional Questions. In ACM SIGIR, Sheffield, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>Y Cao</author>
<author>H Li</author>
<author>M Zhao</author>
</authors>
<title>Ranking Definitions with Supervised Learning Methods.</title>
<date>2005</date>
<booktitle>In WWW,</booktitle>
<location>Chiba, Japan, 811–819.</location>
<contexts>
<context position="35090" citStr="Xu et al. (2005)" startWordPosition="5919" endWordPosition="5922">ation, 1277 etc.). They then extract from documents relevant to the target term sentences containing its head word, as well as sentences the extracted ones refer to (e.g., via pronouns). The resulting sentences are matched against manually constructed syntactic patterns to detect phrases conveying definitional information. The resulting phrases are ranked by criteria like the degree to which the phrase contains words common in definitions of the target term’s type, and the highest ranked phrases are included in a multi-snippet summary. Other mechanisms discard phrases duplicating information. Xu et al. (2005) aim to extract all the definitions in a document collection. They parse the documents to detect base noun phrases (without embedded noun phrases). Base noun phrases are possible target terms; the paragraphs containing them are matched against manually constructed patterns that look for definitions. An SVM then separates the remaining paragraphs into good, indifferent, and bad definitions. Redundant paragraphs, identified by edit distance similarity, are removed. 6 Conclusions and future work We presented a freely available system that finds short definitions of user-specified terms on Web pag</context>
</contexts>
<marker>Xu, Cao, Li, Zhao, 2005</marker>
<rawString>Xu, J., Cao, Y., Li, H., Zhao, M. 2005. Ranking Definitions with Supervised Learning Methods. In WWW, Chiba, Japan, 811–819.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>