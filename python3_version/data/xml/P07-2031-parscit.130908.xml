<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023964">
<title confidence="0.999327">
Predicting Evidence of Understanding by Monitoring User’s Task
Manipulation in Multimodal Conversations
</title>
<author confidence="0.994095">
Yukiko I. Nakano†
</author>
<affiliation confidence="0.9436575">
Yoshiko Arimoto†††Tokyo University of Agri-
culture and Technology
</affiliation>
<address confidence="0.9318">
2-24-16 Nakacho, Koganei-
shi, Tokyo 184-8588, Japan
{nakano, kmurata, meno-
</address>
<email confidence="0.999257">
moto}@cc.tuat.ac.jp
</email>
<author confidence="0.991782">
Kazuyoshi Murata†
</author>
<affiliation confidence="0.89302">
Yasuhiro Asa†††††Tokyo University of
Technology
</affiliation>
<address confidence="0.9088445">
1404-1 Katakura, Hachioji,
Tokyo 192-0981, Japan
</address>
<email confidence="0.998948">
ar@mf.teu.ac.jp
</email>
<author confidence="0.963171">
Mika Enomoto†
</author>
<affiliation confidence="0.759338">
Hirohiko Sagawa††††††Central Research Laboratory,
Hitachi, Ltd.
1-280, Higashi-koigakubo Kokub-
unji-shi, Tokyo 185-8601, Japan
</affiliation>
<address confidence="0.566642">
{yasuhiro.asa.mk, hiro-
</address>
<email confidence="0.995255">
hiko.sagawa.cu}@hitachi.com
</email>
<sectionHeader confidence="0.995596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999988352941177">
The aim of this paper is to develop ani-
mated agents that can control multimodal
instruction dialogues by monitoring user’s
behaviors. First, this paper reports on our
Wizard-of-Oz experiments, and then, using
the collected corpus, proposes a probabilis-
tic model of fine-grained timing dependen-
cies among multimodal communication
behaviors: speech, gestures, and mouse
manipulations. A preliminary evaluation
revealed that our model can predict a in-
structor’s grounding judgment and a lis-
tener’s successful mouse manipulation
quite accurately, suggesting that the model
is useful in estimating the user’s under-
standing, and can be applied to determining
the agent’s next action.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998296692307692">
In face-to-face conversation, speakers adjust their
utterances in progress according to the listener’s
feedback expressed in multimodal manners, such
as speech, facial expression, and eye-gaze. In task-
manipulation situations where the listener manipu-
lates objects by following the speaker’s instruc-
tions, correct task manipulation by the listener
serves as more direct evidence of understanding
(Brennan 2000, Clark and Krych 2004), and affects
the speaker’s dialogue control strategies.
Figure 1 shows an example of a software in-
struction dialogue in a video-mediated situation
(originally in Japanese). While the learner says
</bodyText>
<figureCaption confidence="0.999707">
Figure 1: Example of task manipulation dialogue
</figureCaption>
<bodyText confidence="0.9998018">
nothing, the instructor gives the instruction in
small pieces, simultaneously modifying her ges-
tures and utterances according to the learner’s
mouse movements.
To accomplish such interaction between human
users and animated help agents, and to assist the
user through natural conversational interaction, this
paper proposes a probabilistic model that computes
timing dependencies among different types of be-
haviors in different modalities: speech, gestures,
and mouse events. The model predicts (a) whether
the instructor’s current utterance will be success-
fully understood by the learner and grounded
(Clark and Schaefer 1989), and (b) whether the
learner will successfully manipulate the object in
the near future. These predictions can be used as
constraints in determining agent actions. For ex-
ample, if the current utterance will not be grounded,
then the help agent must add more information.
In the following sections, first, we collect hu-
man-agent conversations by employing a Wizard-
of-Oz method, and annotate verbal and nonverbal
behaviors. The annotated corpus is used to build a
Bayesian network model for the multimodal in-
struction dialogues. Finally, we will evaluate how
</bodyText>
<figure confidence="0.998894666666667">
Instructor:
Instructor:
Learner:
Instructor:
Learner:
Pointing gesture &lt;preparation&gt;
“That” (204ms pause)
“at the most” (395ms pause)
Mouse move
“left-hand side”
Mouse move
&lt;stroke&gt;
</figure>
<page confidence="0.963126">
121
</page>
<bodyText confidence="0.82537525">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 121–124,
Prague, June 2007. c�2007 Association for Computational Linguistics
accurately the model can predict the events in (a)
and (b) mentioned above.
</bodyText>
<sectionHeader confidence="0.996373" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999936027777778">
In their psychological study, Clark and Krych
(2004) showed that speakers alter their utterances
midcourse while monitoring not only the listener’s
vocal signals, but also the listener’s gestural sig-
nals as well as through other mutually visible
events. Such a bilateral process functions as a joint
activity to ground the presented information, and
task manipulation as a mutually visible event con-
tributes to the grounding process (Brennan 2000,
Whittaker 2003). Dillenbourg, Traum, et al. (1996)
also discussed cross-modality in grounding: ver-
bally presented information is grounded by an ac-
tion in the task environment.
Studies on interface agents have presented com-
putational models of multimodal interaction
(Cassell, Bickmore, et al. 2000). Paek and Horvitz
(1999) focused on uncertainty in speech-based in-
teraction, and employed a Bayesian network to
understand the user’s speech input. For user moni-
toring, Nakano, Reinstein, et al. (2003) used a head
tracker to build a conversational agent which can
monitor the user’s eye-gaze and head nods as non-
verbal signals in grounding.
These previous studies provide psychological
evidence about the speaker’s monitoring behaviors
as well as conversation modeling techniques in
computational linguistics. However, little has been
studied about how systems (agents) should monitor
the user’s task manipulation, which gives direct
evidence of understanding to estimate the user’s
understanding, and exploits the predicted evidence
as constraints in selecting the agent’s next action.
Based on these previous attempts, this study pro-
poses a multimodal interaction model by focusing
on task manipulation, and predicts conversation
states using probabilistic reasoning.
</bodyText>
<sectionHeader confidence="0.984888" genericHeader="method">
3 Data collection
</sectionHeader>
<bodyText confidence="0.999636285714286">
A data collection experiment was conducted using
a Wizard-of-Oz agent assisting a user in learning a
PCTV application, a system for watching and re-
cording TV programs on a PC.
The output of the PC operated by the user was
displayed on a 23-inch monitor in front of the user,
and also projected on a 120-inch big screen, in
</bodyText>
<figure confidence="0.986677">
(a) Instructor (b) PC output
</figure>
<figureCaption confidence="0.999493">
Figure 2: Wizard-of-Oz agent controlled by instructor
</figureCaption>
<bodyText confidence="0.999921222222222">
front of which a human instructor was standing
(Figure 2 (a)). Therefore, the participants shared
visual events output from the PC (Figure 2 (b))
while sitting in different rooms. In addition, a rab-
bit-like animated agent was controlled through the
instructor’s motion data captured by motion sen-
sors. The instructor’s voice was changed through a
voice transformation system to make it sound like
a rabbit agent.
</bodyText>
<sectionHeader confidence="0.998528" genericHeader="method">
4 Corpus
</sectionHeader>
<bodyText confidence="0.998784714285714">
We collected 20 conversations from 10 pairs, and
annotated 11 conversations of 6 pairs using the
Anvil video annotating tool (Kipp 2004).
Agent’s verbal behaviors: The agent’s (actually,
instructor’s) speech data was split by pauses longer
than 200ms. For each inter pausal unit (IPU), utter-
ance content type defined as follows was assigned.
</bodyText>
<listItem confidence="0.98611">
• Identification (id): identification of a target
object for the next operation
• Operation (op): request to execute a mouse
click or a similar primitive action on the target
• Identification + operation (idop): referring to
identification and operation in one IPU
</listItem>
<bodyText confidence="0.998788636363636">
In addition to these main categories, we also
used: State (referring to a state before/after an op-
eration), Function (explaining a function of the
system), Goal (referring to a task goal to be ac-
complished), and Acknowledgment. The inter-
coder agreement for this coding scheme is very
high K=0.89 (Cohen’s Kappa), suggesting that the
assigned tags are reliable.
Agent’s nonverbal behaviors: As the most salient
instructor’s nonverbal behaviors in the collected
data, we annotated agent pointing gestures:
</bodyText>
<listItem confidence="0.9629095">
• Agent movement: agent’s position movement
• Agent touching target (att): agent’s touching
the target object as a stroke of a pointing ges-
ture
</listItem>
<page confidence="0.990932">
122
</page>
<figureCaption confidence="0.998685">
Figure 3: Example dialogue between Wizard-of-Oz agent and user
</figureCaption>
<figure confidence="0.999211193548387">
Speech
[a4] [a5]
Beside the DVD There is a button
starts with “V”
id id id id id+op
Speech
Gesture
[a1]
[a3]
the TV
[a6]
Press it
Well,
right of
[a2]
Agent
att att att
Yes
Ah, yes Er, yes
This
ack ack ack ack
User
targ
et
On
Mouser actions
Mouse move
Mouse on
Off
click
Yes View
</figure>
<bodyText confidence="0.847803333333333">
User’s nonverbal behaviors: We annotated three
types of mouse manipulation for the user’s task
manipulation as follows:
</bodyText>
<listItem confidence="0.999954">
• Mouse movement: movement of the mouse
cursor
• Mouse-on-target: the mouse cursor is on the
target object
• Click target: click on the target object
</listItem>
<subsectionHeader confidence="0.998125">
4.1 Example of collected data
</subsectionHeader>
<bodyText confidence="0.9967564">
An example of an annotated corpus is shown in
Figure 3. The upper two tracks illustrate the
agent’s verbal and nonverbal behaviors, and the
other two illustrate the user’s behaviors. The agent
was pointing at the target (att) and giving a se-
quence of identification descriptions [a1-3]. Since
the user’s mouse did not move at all, the agent
added another identification IPU [a4] accompanied
by another pointing gesture. Immediately after that,
the user’s mouse cursor started moving towards the
target object. After finishing the next IPU, the
agent finally requested the user to click the object
in [a6]. Note that the collected Wizard-of-Oz con-
versations are very similar to the human-human
instruction dialogues shown in Figure 1. While
carefully monitoring the user’s mouse actions, the
Wizard-of-Oz agent provided information in small
pieces. If it was uncertain that the user was follow-
ing the instruction, the agent added more explana-
tion without continuing.
</bodyText>
<sectionHeader confidence="0.9275705" genericHeader="method">
5 Probabilistic model of user-agent mul-
timodal interaction
</sectionHeader>
<subsectionHeader confidence="0.999754">
5.1 Building a Bayesian network model
</subsectionHeader>
<bodyText confidence="0.999974214285714">
To consider multiple factors for verbal and non-
verbal behaviors in probabilistic reasoning, we
employed a Bayesian network technique, which
can infer the likelihood of the occurrence of a tar-
get event based on the dependencies among multi-
ple kinds of evidence. We extracted the conversa-
tional data from the beginning of an instructor&apos;s
identification utterance for a new target object to
the point that the user clicks on the object. Each
IPU was split at 500ms intervals, and 1395 inter-
vals were obtained. As shown in Figure 4, the net-
work consists of 9 properties concerning verbal
and nonverbal behaviors for past, current, and fu-
ture interval(s).
</bodyText>
<subsectionHeader confidence="0.999835">
5.2 Predicting evidence of understanding
</subsectionHeader>
<bodyText confidence="0.999791714285714">
As a preliminary evaluation, we tested how ac-
curately our Bayesian network model can predict
an instructor’s grounding judgment, and the user’s
mouse click. The following five kinds of evidence
were given to the network to predict future states.
As evidence for the previous three intervals (1.5
sec), we used (1) the percentage of time the agent
touched the target (att), (2) the number of the
user’s mouse movements. Evidence for the current
interval is (3) current IPU’s content type, (4)
whether the end of the current interval will be the
end of the IPU (i.e. whether a pause will follow
after the current interval), and (5) whether the
mouse is on the target object.
</bodyText>
<figureCaption confidence="0.990059">
Figure 4: Bayesian network model
</figureCaption>
<page confidence="0.998963">
123
</page>
<tableCaption confidence="0.999381">
Table 1: Preliminary evaluation results
</tableCaption>
<table confidence="0.995324166666667">
Precision Recall F-
measure
Content 0.53 0.99 0.68
change
Same 1.00 0.81 0.90
content
</table>
<bodyText confidence="0.996978172413793">
(a) Predicting grounding judgment: We tested
how accurately the model can predict whether the
instructor will go on to the next leg of the instruc-
tion or will give additional explanations using the
same utterance content type (the current message
will not be grounded).
The results of 5-fold cross-validation are shown
in Table 1. Since 83% of the data are “same con-
tent” cases, prediction for “same content” is very
accurate (F-measure is 0.90). However, it is not
very easy to find “content change” case because of
its less frequency (F-measure is 0.68). It would be
better to test the model using more balanced data.
(b) Predicting user’s mouse click: As a measure
of the smoothness of task manipulation, the net-
work predicted whether the user’s mouse click
would be successfully performed within the next 5
intervals (2.5sec). If a mouse click is predicted, the
agent should just wait without annoying the user
by unnecessary explanation. Since randomized
data is not appropriate to test mouse click predic-
tion, we used 299 sequences of utterances that w-
ere not used for training. Our model predicted 84%
of the user’s mouse clicks: 80% of them were pre-
dicted 3-5 intervals before the actual occurrence of
the mouse click, and 20% were predicted 1 interval
before. However, the model frequently generates
wrong predictions. Improving precision rate is
necessary.
</bodyText>
<sectionHeader confidence="0.99898" genericHeader="discussions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999993086956522">
We employed a Bayesian network technique to our
goal of developing conversational agents that can
generate fine-grained multimodal instruction dia-
logues, and we proposed a probabilistic model for
predicting grounding judgment and user’s success-
ful mouse click. The results of preliminary evalua-
tion suggest that separate models of each modality
for each conversational participant cannot properly
describe the complex process of on-going multi-
modal interaction, but modeling the interaction as
dyadic activities with multiple tracks of modalities
is a promising approach.
The advantage of employing the Bayesian net-
work technique is that, by considering the cost of
misclassification and the benefit of correct classifi-
cation, the model can be easily adjusted according
to the purpose of the system or the user’s skill level.
For example, we can make the model more cau-
tious or incautious. Thus, our next step is to im-
plement the proposed model into a conversational
agent, and evaluate our model not only in its accu-
racy, but also in its effectiveness by testing the
model with various utility values.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999910628571429">
Brennan, S. 2000. Processes that shape conversation and
their implications for computational linguistics. In
Proceedings of 38th Annual Meeting of the ACL.
Cassell, J., Bickmore, T., Campbell, L., Vilhjalmsson, H.
and Yan, H. (2000). Human Conversation as a Sys-
tem Framework: Designing Embodied Conversa-
tional Agents. Embodied Conversational Agents. J.
Cassell, J. Sullivan, S. Prevost and E. Churchill.
Cambridge, MA, MIT Press: 29-63.
Clark, H. H. and Schaefer, E. F. 1989. Contributing to
discourse. Cognitive Science 13: 259-294.
Clark, H. H. and Krych, M. A. 2004. Speaking while
monitoring addressees for understanding. Journal of
Memory and Language 50(1): 62-81.
Dillenbourg, P., Traum, D. R. and Schneider, D. 1996.
Grounding in Multi-modal Task Oriented Collabora-
tion. In Proceedings of EuroAI&amp;Education Confer-
ence: 415-425.
Kipp, M. 2004. Gesture Generation by Imitation - From
Human Behavior to Computer Character Animation,
Boca Raton, Florida: Dissertation.com.
Nakano, Y. I., Reinstein, G., Stocky, T. and Cassell, J.
2003. Towards a Model of Face-to-Face Grounding.
In Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics: 553-561.
Paek, T. and Horvitz, E. (1999). Uncertainty, Utility,
and Misunderstanding: A Decision-Theoretic Per-
spective on Grounding in Conversational Systems.
Working Papers of the AAAI Fall Symposium on
Psychological Models of Communication in Collabo-
rative Systems. S. E. Brennan, A. Giboin and D.
Traum: 85-92.
Whittaker, S. (2003). Theories and Methods in Medi-
ated Communication. The Handbook of Discourse
Processes. A. Graesser, MIT Press.
</reference>
<page confidence="0.998318">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.075888">
<title confidence="0.984688">Predicting Evidence of Understanding by Monitoring User’s Task Manipulation in Multimodal Conversations</title>
<author confidence="0.437385">I</author>
<affiliation confidence="0.933261666666667">University of culture and Technology Nakacho,</affiliation>
<address confidence="0.996901">shi, Tokyo 184-8588, Japan</address>
<email confidence="0.9041005">kmurata,moto}@cc.tuat.ac.jp</email>
<affiliation confidence="0.98929">University of Technology</affiliation>
<address confidence="0.9929095">1404-1 Katakura, Hachioji, Tokyo 192-0981, Japan</address>
<email confidence="0.940352">ar@mf.teu.ac.jp</email>
<affiliation confidence="0.624606">Research Laboratory, Hitachi, Ltd. Higashi-koigakubo</affiliation>
<address confidence="0.787995">unji-shi, Tokyo 185-8601, Japan</address>
<email confidence="0.998979">hiko.sagawa.cu}@hitachi.com</email>
<abstract confidence="0.997865833333333">The aim of this paper is to develop animated agents that can control multimodal instruction dialogues by monitoring user’s behaviors. First, this paper reports on our Wizard-of-Oz experiments, and then, using the collected corpus, proposes a probabilistic model of fine-grained timing dependencies among multimodal communication behaviors: speech, gestures, and mouse manipulations. A preliminary evaluation revealed that our model can predict a instructor’s grounding judgment and a listener’s successful mouse manipulation quite accurately, suggesting that the model is useful in estimating the user’s understanding, and can be applied to determining the agent’s next action.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Brennan</author>
</authors>
<title>Processes that shape conversation and their implications for computational linguistics.</title>
<date>2000</date>
<booktitle>In Proceedings of 38th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1711" citStr="Brennan 2000" startWordPosition="222" endWordPosition="223">ner’s successful mouse manipulation quite accurately, suggesting that the model is useful in estimating the user’s understanding, and can be applied to determining the agent’s next action. 1 Introduction In face-to-face conversation, speakers adjust their utterances in progress according to the listener’s feedback expressed in multimodal manners, such as speech, facial expression, and eye-gaze. In taskmanipulation situations where the listener manipulates objects by following the speaker’s instructions, correct task manipulation by the listener serves as more direct evidence of understanding (Brennan 2000, Clark and Krych 2004), and affects the speaker’s dialogue control strategies. Figure 1 shows an example of a software instruction dialogue in a video-mediated situation (originally in Japanese). While the learner says Figure 1: Example of task manipulation dialogue nothing, the instructor gives the instruction in small pieces, simultaneously modifying her gestures and utterances according to the learner’s mouse movements. To accomplish such interaction between human users and animated help agents, and to assist the user through natural conversational interaction, this paper proposes a probab</context>
<context position="4024" citStr="Brennan 2000" startWordPosition="566" endWordPosition="567">1–124, Prague, June 2007. c�2007 Association for Computational Linguistics accurately the model can predict the events in (a) and (b) mentioned above. 2 Related work In their psychological study, Clark and Krych (2004) showed that speakers alter their utterances midcourse while monitoring not only the listener’s vocal signals, but also the listener’s gestural signals as well as through other mutually visible events. Such a bilateral process functions as a joint activity to ground the presented information, and task manipulation as a mutually visible event contributes to the grounding process (Brennan 2000, Whittaker 2003). Dillenbourg, Traum, et al. (1996) also discussed cross-modality in grounding: verbally presented information is grounded by an action in the task environment. Studies on interface agents have presented computational models of multimodal interaction (Cassell, Bickmore, et al. 2000). Paek and Horvitz (1999) focused on uncertainty in speech-based interaction, and employed a Bayesian network to understand the user’s speech input. For user monitoring, Nakano, Reinstein, et al. (2003) used a head tracker to build a conversational agent which can monitor the user’s eye-gaze and hea</context>
</contexts>
<marker>Brennan, 2000</marker>
<rawString>Brennan, S. 2000. Processes that shape conversation and their implications for computational linguistics. In Proceedings of 38th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>T Bickmore</author>
<author>L Campbell</author>
<author>H Vilhjalmsson</author>
<author>H Yan</author>
</authors>
<title>Human Conversation as a System Framework: Designing Embodied Conversational Agents. Embodied Conversational Agents.</title>
<date>2000</date>
<pages>29--63</pages>
<publisher>MIT Press:</publisher>
<location>Cambridge, MA,</location>
<marker>Cassell, Bickmore, Campbell, Vilhjalmsson, Yan, 2000</marker>
<rawString>Cassell, J., Bickmore, T., Campbell, L., Vilhjalmsson, H. and Yan, H. (2000). Human Conversation as a System Framework: Designing Embodied Conversational Agents. Embodied Conversational Agents. J. Cassell, J. Sullivan, S. Prevost and E. Churchill. Cambridge, MA, MIT Press: 29-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>E F Schaefer</author>
</authors>
<title>Contributing to discourse.</title>
<date>1989</date>
<journal>Cognitive Science</journal>
<volume>13</volume>
<pages>259--294</pages>
<contexts>
<context position="2606" citStr="Clark and Schaefer 1989" startWordPosition="349" endWordPosition="352">hing, the instructor gives the instruction in small pieces, simultaneously modifying her gestures and utterances according to the learner’s mouse movements. To accomplish such interaction between human users and animated help agents, and to assist the user through natural conversational interaction, this paper proposes a probabilistic model that computes timing dependencies among different types of behaviors in different modalities: speech, gestures, and mouse events. The model predicts (a) whether the instructor’s current utterance will be successfully understood by the learner and grounded (Clark and Schaefer 1989), and (b) whether the learner will successfully manipulate the object in the near future. These predictions can be used as constraints in determining agent actions. For example, if the current utterance will not be grounded, then the help agent must add more information. In the following sections, first, we collect human-agent conversations by employing a Wizardof-Oz method, and annotate verbal and nonverbal behaviors. The annotated corpus is used to build a Bayesian network model for the multimodal instruction dialogues. Finally, we will evaluate how Instructor: Instructor: Learner: Instructo</context>
</contexts>
<marker>Clark, Schaefer, 1989</marker>
<rawString>Clark, H. H. and Schaefer, E. F. 1989. Contributing to discourse. Cognitive Science 13: 259-294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>M A Krych</author>
</authors>
<title>Speaking while monitoring addressees for understanding.</title>
<date>2004</date>
<journal>Journal of Memory and Language</journal>
<volume>50</volume>
<issue>1</issue>
<pages>62--81</pages>
<contexts>
<context position="1734" citStr="Clark and Krych 2004" startWordPosition="224" endWordPosition="227">ul mouse manipulation quite accurately, suggesting that the model is useful in estimating the user’s understanding, and can be applied to determining the agent’s next action. 1 Introduction In face-to-face conversation, speakers adjust their utterances in progress according to the listener’s feedback expressed in multimodal manners, such as speech, facial expression, and eye-gaze. In taskmanipulation situations where the listener manipulates objects by following the speaker’s instructions, correct task manipulation by the listener serves as more direct evidence of understanding (Brennan 2000, Clark and Krych 2004), and affects the speaker’s dialogue control strategies. Figure 1 shows an example of a software instruction dialogue in a video-mediated situation (originally in Japanese). While the learner says Figure 1: Example of task manipulation dialogue nothing, the instructor gives the instruction in small pieces, simultaneously modifying her gestures and utterances according to the learner’s mouse movements. To accomplish such interaction between human users and animated help agents, and to assist the user through natural conversational interaction, this paper proposes a probabilistic model that comp</context>
<context position="3630" citStr="Clark and Krych (2004)" startWordPosition="504" endWordPosition="507">nonverbal behaviors. The annotated corpus is used to build a Bayesian network model for the multimodal instruction dialogues. Finally, we will evaluate how Instructor: Instructor: Learner: Instructor: Learner: Pointing gesture &lt;preparation&gt; “That” (204ms pause) “at the most” (395ms pause) Mouse move “left-hand side” Mouse move &lt;stroke&gt; 121 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 121–124, Prague, June 2007. c�2007 Association for Computational Linguistics accurately the model can predict the events in (a) and (b) mentioned above. 2 Related work In their psychological study, Clark and Krych (2004) showed that speakers alter their utterances midcourse while monitoring not only the listener’s vocal signals, but also the listener’s gestural signals as well as through other mutually visible events. Such a bilateral process functions as a joint activity to ground the presented information, and task manipulation as a mutually visible event contributes to the grounding process (Brennan 2000, Whittaker 2003). Dillenbourg, Traum, et al. (1996) also discussed cross-modality in grounding: verbally presented information is grounded by an action in the task environment. Studies on interface agents </context>
</contexts>
<marker>Clark, Krych, 2004</marker>
<rawString>Clark, H. H. and Krych, M. A. 2004. Speaking while monitoring addressees for understanding. Journal of Memory and Language 50(1): 62-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dillenbourg</author>
<author>D R Traum</author>
<author>D Schneider</author>
</authors>
<title>Grounding in Multi-modal Task Oriented Collaboration.</title>
<date>1996</date>
<booktitle>In Proceedings of EuroAI&amp;Education Conference:</booktitle>
<pages>415--425</pages>
<marker>Dillenbourg, Traum, Schneider, 1996</marker>
<rawString>Dillenbourg, P., Traum, D. R. and Schneider, D. 1996. Grounding in Multi-modal Task Oriented Collaboration. In Proceedings of EuroAI&amp;Education Conference: 415-425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kipp</author>
</authors>
<title>Gesture Generation by Imitation - From Human Behavior to Computer Character Animation,</title>
<date>2004</date>
<location>Boca Raton, Florida: Dissertation.com.</location>
<contexts>
<context position="6277" citStr="Kipp 2004" startWordPosition="912" endWordPosition="913">gure 2: Wizard-of-Oz agent controlled by instructor front of which a human instructor was standing (Figure 2 (a)). Therefore, the participants shared visual events output from the PC (Figure 2 (b)) while sitting in different rooms. In addition, a rabbit-like animated agent was controlled through the instructor’s motion data captured by motion sensors. The instructor’s voice was changed through a voice transformation system to make it sound like a rabbit agent. 4 Corpus We collected 20 conversations from 10 pairs, and annotated 11 conversations of 6 pairs using the Anvil video annotating tool (Kipp 2004). Agent’s verbal behaviors: The agent’s (actually, instructor’s) speech data was split by pauses longer than 200ms. For each inter pausal unit (IPU), utterance content type defined as follows was assigned. • Identification (id): identification of a target object for the next operation • Operation (op): request to execute a mouse click or a similar primitive action on the target • Identification + operation (idop): referring to identification and operation in one IPU In addition to these main categories, we also used: State (referring to a state before/after an operation), Function (explaining </context>
</contexts>
<marker>Kipp, 2004</marker>
<rawString>Kipp, M. 2004. Gesture Generation by Imitation - From Human Behavior to Computer Character Animation, Boca Raton, Florida: Dissertation.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y I Nakano</author>
<author>G Reinstein</author>
<author>T Stocky</author>
<author>J Cassell</author>
</authors>
<title>Towards a Model of Face-to-Face Grounding.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics:</booktitle>
<pages>553--561</pages>
<marker>Nakano, Reinstein, Stocky, Cassell, 2003</marker>
<rawString>Nakano, Y. I., Reinstein, G., Stocky, T. and Cassell, J. 2003. Towards a Model of Face-to-Face Grounding. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics: 553-561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
<author>E Horvitz</author>
</authors>
<title>Uncertainty, Utility, and Misunderstanding: A Decision-Theoretic Perspective on Grounding in Conversational Systems.</title>
<date>1999</date>
<booktitle>Working Papers of the AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems.</booktitle>
<pages>85--92</pages>
<contexts>
<context position="4349" citStr="Paek and Horvitz (1999)" startWordPosition="611" endWordPosition="614">vocal signals, but also the listener’s gestural signals as well as through other mutually visible events. Such a bilateral process functions as a joint activity to ground the presented information, and task manipulation as a mutually visible event contributes to the grounding process (Brennan 2000, Whittaker 2003). Dillenbourg, Traum, et al. (1996) also discussed cross-modality in grounding: verbally presented information is grounded by an action in the task environment. Studies on interface agents have presented computational models of multimodal interaction (Cassell, Bickmore, et al. 2000). Paek and Horvitz (1999) focused on uncertainty in speech-based interaction, and employed a Bayesian network to understand the user’s speech input. For user monitoring, Nakano, Reinstein, et al. (2003) used a head tracker to build a conversational agent which can monitor the user’s eye-gaze and head nods as nonverbal signals in grounding. These previous studies provide psychological evidence about the speaker’s monitoring behaviors as well as conversation modeling techniques in computational linguistics. However, little has been studied about how systems (agents) should monitor the user’s task manipulation, which giv</context>
</contexts>
<marker>Paek, Horvitz, 1999</marker>
<rawString>Paek, T. and Horvitz, E. (1999). Uncertainty, Utility, and Misunderstanding: A Decision-Theoretic Perspective on Grounding in Conversational Systems. Working Papers of the AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems. S. E. Brennan, A. Giboin and D. Traum: 85-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Whittaker</author>
</authors>
<date>2003</date>
<booktitle>Theories and Methods in Mediated Communication. The Handbook of Discourse Processes. A. Graesser,</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4041" citStr="Whittaker 2003" startWordPosition="568" endWordPosition="569"> June 2007. c�2007 Association for Computational Linguistics accurately the model can predict the events in (a) and (b) mentioned above. 2 Related work In their psychological study, Clark and Krych (2004) showed that speakers alter their utterances midcourse while monitoring not only the listener’s vocal signals, but also the listener’s gestural signals as well as through other mutually visible events. Such a bilateral process functions as a joint activity to ground the presented information, and task manipulation as a mutually visible event contributes to the grounding process (Brennan 2000, Whittaker 2003). Dillenbourg, Traum, et al. (1996) also discussed cross-modality in grounding: verbally presented information is grounded by an action in the task environment. Studies on interface agents have presented computational models of multimodal interaction (Cassell, Bickmore, et al. 2000). Paek and Horvitz (1999) focused on uncertainty in speech-based interaction, and employed a Bayesian network to understand the user’s speech input. For user monitoring, Nakano, Reinstein, et al. (2003) used a head tracker to build a conversational agent which can monitor the user’s eye-gaze and head nods as nonverb</context>
</contexts>
<marker>Whittaker, 2003</marker>
<rawString>Whittaker, S. (2003). Theories and Methods in Mediated Communication. The Handbook of Discourse Processes. A. Graesser, MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>