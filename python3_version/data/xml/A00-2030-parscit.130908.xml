<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9944645">
A Novel Use of Statistical Parsing to Extract Information from
Text
</title>
<author confidence="0.920704">
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph Weischedel
</author>
<affiliation confidence="0.285347">
BBN Technologies
</affiliation>
<address confidence="0.857584">
70 Fawcett Street, Cambridge, MA 02138
</address>
<email confidence="0.997837">
szmiller@bbn.com
</email>
<sectionHeader confidence="0.993859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998374">
Since 1995, a few statistical parsing
algorithms have demonstrated a
breakthrough in parsing accuracy, as
measured against the UPenn TREEBANK
as a gold standard. In this paper we report
adapting a lexic al ized, probabilistic
context-free parser to information
extraction and evaluate this new technique
on MUC-7 template elements and template
relations.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995590947368421">
Since 1995, a few statistical parsing
algorithms (Magerman, 1995; Collins, 1996
and 1997; Charniak, 1997; Rathnaparki, 1997)
demonstrated a breakthrough in parsing
accuracy, as measured against the University
of Pennsylvania TREEBANK as a gold
standard. Yet, relatively few have embedded
one of these algorithms in a task. Chiba,
(1999) was able to use such a parsing
algorithm to reduce perplexity with the long
term goal of improved speech recognition.
In this paper, we report adapting a lexicalized,
probabilistic context-free parser with head
rules (LPCFG-HR) to information extraction.
The technique was benchmarked in the
Seventh Message Understanding Conference
(MUC-7) in 1998.
Several technical challenges confronted us and
were solved:
</bodyText>
<listItem confidence="0.9012718">
• How could the limited semantic
interpretation required in information
extraction be integrated into the statistical
learning algorithm? We were able to integrate
both syntactic and semantic information into
the parsing process, thus avoiding potential
errors of syntax first followed by semantics.
• Would TREEBANICing of the variety of
news sources in MUC-7 be required? Or
could the University of Pennsylvania&apos;s
</listItem>
<bodyText confidence="0.779583545454545">
TREEBANK on Wall Street Journal
adequately train the algorithm for New York
Times newswire, which includes dozens of
newspapers? Manually creating source-
specific training data for syntax was not
required. Instead, our parsing algorithm,
trained on the UPenn TREEBANK, was run
on the New York Times source to create
unsupervised syntactic training which was
constrained to be consistent with semantic
annotation.
</bodyText>
<listItem confidence="0.7588676">
• Would semantic annotation require
computational linguists? We were able to
specify relatively simple guidelines that
students with no training in computational
linguistics could annotate.
</listItem>
<sectionHeader confidence="0.982828" genericHeader="introduction">
2 Information Extraction Tasks
</sectionHeader>
<bodyText confidence="0.999957117647059">
We evaluated the new approach to information
extraction on two of the tasks of the Seventh
Message Understanding Conference (MUC-7)
and reported in (Marsh, 1998). The Template
Element (TE) task identifies organizations,
persons, locations, and some artifacts (rocket
and airplane-related artifacts). For each
organization in an article, one must identify all
of its names as used in the article, its type
(corporation, government, or other), and any
significant description of it. For each person,
one must find all of the person&apos;s names within
the document, his/her type (civilian or
military), and any significant descriptions
(e.g., titles). For each location, one must also
give its type (city, province, county, body of
water, etc.). For the following example, the
</bodyText>
<page confidence="0.996949">
226
</page>
<figure confidence="0.915198833333333">
template element in Figure 1 was to be
generated: &amp;quot;...according to the report by
Edwin Dorn, under secretary of defense for
personnel and readiness. ...Dorn&apos;s conclusion
that Washington...&amp;quot;
&lt;ENTITY-9601020516-13&gt; :=
ENT NAME: &amp;quot;Edwin Dorn&amp;quot;
&amp;quot;Dorn&amp;quot;
ENT_TYPE: PERSON
ENT_DESCRIPTOR: &amp;quot;under secretary of
defense for personnel and readiness&amp;quot;
ENT_CATEGORY: PER_CIV
</figure>
<figureCaption confidence="0.98706">
Figure 1: An example of the information to be
extracted for TE.
</figureCaption>
<bodyText confidence="0.898159333333333">
The Template Relations (TR) task involves
identifying instances of three relations in the
text:
</bodyText>
<listItem confidence="0.9867435">
• the products made by each company
• the employees of each organization,
• the (headquarters) location of each
organization.
</listItem>
<bodyText confidence="0.8178525">
TR builds on TE in that TR reports binary
relations between elements of TE. For the
following example, the template relation in
Figure 2 was to be generated: &amp;quot;Donald M.
Goldstein, a historian at the University of
Pittsburgh who helped write...&amp;quot;
</bodyText>
<equation confidence="0.6185272">
&lt;EMPLOYEE_OF-9601020516-5&gt; :=
PERSON: &lt;ENTITY-9601020516-18&gt;
ORGANIZATION: &lt;ENTITY-
9601020516-9&gt;
&lt;ENTITY-9601020516-9&gt; :=
</equation>
<footnote confidence="0.965344125">
ENT_NAIv1E: &amp;quot;University of Pittsburgh&amp;quot;
ENT_TYPE: ORGANIZATION
ENT_CATEGORY: ORG_CO
&lt;ENTITY-9601020516-18&gt; :=
ENT_NAME: &amp;quot;Donald M. Goldstein&amp;quot;
ENT_TYPE: PERSON
ENT_DESCR1PTOR: &amp;quot;a historian at the
University of Pittsburgh&amp;quot;
</footnote>
<figureCaption confidence="0.9562565">
Figure 2: An example of information to be
extracted for TR
</figureCaption>
<sectionHeader confidence="0.9949" genericHeader="method">
3 Integrated Sentential Processing
</sectionHeader>
<bodyText confidence="0.998387428571428">
Almost all approaches to information
extraction — even at the sentence level — are
based on the divide-and-conquer strategy of
reducing a complex problem to a set of simpler
ones. Currently, the prevailing architecture for
dividing sentential processing is a four-stage
pipeline consisting of:
</bodyText>
<listItem confidence="0.988072833333333">
1. part-of-speech tagging
2. name finding
3. syntactic analysis, often limited to noun
and verb group chunking
4. semantic interpretation, usually based on
pattern matching
</listItem>
<bodyText confidence="0.99990821875">
Since we were interested in exploiting recent
advances in parsing, replacing the syntactic
analysis stage of the standard pipeline with a
modern statistical parser was an obvious
possibility. However, pipelined architectures
suffer from a serious disadvantage: errors
accumulate as they propagate through the
pipeline. For example, an error made during
part-of-speech-tagging may cause a future
error in syntactic analysis, which may in turn
cause a semantic interpretation failure. There
is no opportunity for a later stage, such as
parsing, to influence or correct an earlier stage
such as part-of-speech tagging.
An integrated model can limit the propagation
of errors by making all decisions jointly. For
this reason, we focused on designing an
integrated model in which tagging, name-
finding, parsing, and semantic interpretation
decisions all have the opportunity to mutually
influence each other.
A second consideration influenced our
decision toward an integrated model. We were
already using a generative statistical model for
part-of-speech tagging (Weischedel et al.
1993), and more recently, had begun using a
generative statistical model for name finding
(Bikel et al. 1997). Finally, our newly
constructed parser, like that of (Collins 1997),
was based on a generative statistical model.
Thus, each component of what would be the
first three stages of our pipeline was based on
</bodyText>
<page confidence="0.98154">
227
</page>
<bodyText confidence="0.999941375">
the same general class of statistical model.
Although each model differed in its detailed
probability structure, we believed that the
essential elements of all three models could be
generalized in a single probability model.
If the single generalized model could then be
extended to semantic analysis, all necessary
sentence level processing would be contained
in that model. Because generative statistical
models had already proven successful for each
of the first three stages, we were optimistic
that some of their properties — especially their
ability to learn from large amounts of data, and
their robustness when presented with
unexpected inputs — would also benefit
semantic analysis.
</bodyText>
<sectionHeader confidence="0.9695855" genericHeader="method">
4 Representing Syntax and Semantics
Jointly
</sectionHeader>
<bodyText confidence="0.99851">
Our integrated model represents syntax and
semantics jointly using augmented parse trees.
In these trees, the standard TREEBANK
structures are augmented to convey semantic
information, that is, entities and relations. An
example of an augmented parse tree is shown
in Figure 3. The five key facts in this example
are:
</bodyText>
<listItem confidence="0.9852796">
• &amp;quot;Nance&amp;quot; is the name of a person.
• &amp;quot;A paid consultant to ABC News&amp;quot;
describes a person.
• &amp;quot;ABC News&amp;quot; is the name of an
organization.
• The person described as &amp;quot;a paid consultant
to ABC News&amp;quot; is employed by ABC News.
• The person named &amp;quot;Nance&amp;quot; and the person
described as &amp;quot;a paid consultant to ABC News&amp;quot;
are the same person.
</listItem>
<bodyText confidence="0.999909466666667">
Here, each &amp;quot;reportable&amp;quot; name or description is
identified by a &amp;quot;-r&amp;quot; suffix attached to its
semantic label. For example, &amp;quot;per-r&amp;quot; identifies
&amp;quot;Nance&amp;quot; as a named person, and &amp;quot;per-desc-r&amp;quot;
identifies &amp;quot;a paid consultant to ABC News&amp;quot; as
a person description. Other labels indicate
relations among entities. For example, the co-
reference relation between &amp;quot;Nance&amp;quot; and &amp;quot;a
paid consultant to ABC News&amp;quot; is indicated by
&amp;quot;per-desc-of.&amp;quot; In this case, because the
argument does not connect directly to the
relation, the intervening nodes are labeled with
semantics &amp;quot;-ptr&amp;quot; to indicate the connection.
Further details are discussed in the section
Tree Augmentation.
</bodyText>
<sectionHeader confidence="0.944929" genericHeader="method">
5 Creating the Training Data
</sectionHeader>
<bodyText confidence="0.999751944444445">
To train our integrated model, we required a
large corpus of augmented parse trees. Since it
was known that the MUC-7 evaluation data
would be drawn from a variety of newswire
sources, and that the articles would focus on
rocket launches, it was important that our
training corpus be drawn from similar sources
and that it cover similar events. Thus, we did
not consider simply adding semantic labels to
the existing Penn TREEBANK, which is
drawn from a single source — the Wall Street
Journal — and is impoverished in articles about
rocket launches.
Instead, we applied an information retrieval
system to select a large number of articles
from the desired sources, yielding a corpus
rich in the desired types of events. The
retrieved articles would then be annotated with
augmented tree structures to serve as a training
corpus.
Initially, we tried to annotate the training
corpus by hand marking, for each sentence, the
entire augmented tree. It soon became
painfully obvious that this task could not be
performed in the available time. Our
annotation staff found syntactic analysis
particularly complex and slow going. By
necessity, we adopted the strategy of hand
marking only the semantics.
Figure 4 shows an example of the semantic
annotation, which was the only type of manual
annotation we performed.
To produce a corpus of augmented parse trees,
we used the following multi-step training
procedure which exploited the Penn
TREEBANK
</bodyText>
<page confidence="0.995419">
228
</page>
<figureCaption confidence="0.996758">
Figure 3: An example of an augmented parse tree.
</figureCaption>
<figure confidence="0.99951616">
vp
per/np
per-desc-of/sbar-Ink
1
per-desc-ptr/sbar
\
per-desc-ptr/vp
per-desc-r/np
emp-of/pp-Ink
org-ptr/pp
per-r/np
I
per/nnp
I
Nance
wtmp advp per-desc/np
wp vbz rb det vbn per-desc/nn
who is also a paid consultant
to org&apos;/nnp org/nnp ,
I I I I
to ABC News ,
I
said
,
I
</figure>
<listItem confidence="0.660375">
1. The model (see Section 7) was first trained
on purely syntactic parse trees from the
TREEBANK, producing a model capable
of broad-coverage syntactic parsing.
2. Next, for each sentence in the semantically
annotated corpus:
</listItem>
<figureCaption confidence="0.4080024">
a. The model was applied to parse the
sentence, constrained to produce only
parses that were consistent with the
semantic annotation. A parse was
considered consistent if no syntactic
constituents crossed an annotated entity or
description boundary.
b. The resulting parse tree was then
augmented to reflect semantic structure in
addition to syntactic structure.
</figureCaption>
<figure confidence="0.7374932">
employee
relation
coreference
Fperson
Nance
, who is also a paid
person-descriptor
r organization -I
consultant to ABC News
said
</figure>
<figureCaption confidence="0.994532">
Figure 4: An example of semantic annotation.
</figureCaption>
<page confidence="0.990437">
229
</page>
<bodyText confidence="0.984257">
Applying this procedure yielded a new version
of the semantically annotated corpus, now
annotated with complete augmented trees like
that in Figure 3.
</bodyText>
<sectionHeader confidence="0.987261" genericHeader="method">
6 Tree Augmentation
</sectionHeader>
<bodyText confidence="0.99817725">
In this section, we describe the algorithm that
was used to automatically produce augmented
trees, starting with a) human-generated
semantic annotations and b) machine-
generated syntactic parse trees. For each
sentence, combining these two sources
involved five steps. These steps are given
below:
</bodyText>
<sectionHeader confidence="0.354174" genericHeader="method">
Tree Augmentation Algorithm
</sectionHeader>
<listItem confidence="0.88614034375">
1. Nodes are inserted into the parse tree to
distinguish names and descriptors that are
not bracketed in the parse. For example,
the parser produces a single noun phrase
with no internal structure for &amp;quot;Lt. Cmdr.
David Edwin Lewis&amp;quot;. Additional nodes
must be inserted to distinguish the
description, &amp;quot;Lt. Cmdr.,&amp;quot; and the name,
&amp;quot;David Edwin Lewis.&amp;quot;
2. Semantic labels are attached to all nodes
that correspond to names or descriptors.
These labels reflect the entity type, such as
person, organization, or location, as well
as whether the node is a proper name or a
descriptor.
3. For relations between entities, where one
entity is not a syntactic modifier of the
other, the lowermost parse node that spans
both entities is identified. A semantic tag
is then added to that node denoting the
relationship. For example, in the sentence
&amp;quot;Mary Fackler Schiavo is the inspector
general of the U.S. Department of
Transportation,&amp;quot; a co-reference semantic
label is added to the S node spanning the
name, &amp;quot;Mary Fackler Schiavo,&amp;quot; and the
descriptor, &amp;quot;the inspector general of the
U.S. Department of Transportation.&amp;quot;
4. Nodes are inserted into the parse tree to
distinguish the arguments to each relation.
In cases where there is a relation between
two entities, and one of the entities is a
</listItem>
<bodyText confidence="0.821255428571429">
syntactic modifier of the other, the inserted
node serves to indicate the relation as well
as the argument. For example, in the
phrase &amp;quot;Lt. Cmdr. David Edwin Lewis,&amp;quot; a
node is inserted to indicate that &amp;quot;Lt.
Cmdr.&amp;quot; is a descriptor for &amp;quot;David Edwin
Lewis.&amp;quot;
5. Whenever a relation involves an entity that
is not a direct descendant of that relation
in the parse tree, semantic pointer labels
are attached to all of the intermediate
nodes. These labels serve to form a
continuous chain between the relation and
its argument.
</bodyText>
<sectionHeader confidence="0.919225" genericHeader="method">
7 Model Structure
</sectionHeader>
<bodyText confidence="0.9999432">
In our statistical model, trees are generated
according to a process similar to that described
in (Collins 1996, 1997). The detailed
probability structure differs, however, in that it
was designed to jointly perform part-of-speech
tagging, name finding, syntactic parsing, and
relation finding in a single process.
For each constituent, the head is generated
first, followed by the modifiers, which are
generated from the head outward. Head
words, along with their part-of-speech tags and
features, are generated for each modifier as
soon as the modifier is created. Word features
are introduced primarily to help with unknown
words, as in (Weischedel et al. 1993).
We illustrate the generation process by
walking through a few of the steps of the parse
shown in Figure 3. At each step in the
process, a choice is made from a statistical
distribution, with the probability of each
possible selection dependent on particular
features of previously generated elements. We
pick up the derivation just after the topmost S
and its head word, said, have been produced.
The next steps are to generate in order:
</bodyText>
<listItem confidence="0.8514195">
1. A head constituent for the S, in this case a
VP.
2. Pre-modifier constituents for the S. In this
case, there is only one: a PER/NP.
3. A head part-of-speech tag for the PER/NP,
in this case PER/NNP.
</listItem>
<page confidence="0.923851">
230
</page>
<listItem confidence="0.975089285714286">
4. A head word for the PER/NP, in this case
nance.
5. Word features for the head word of the
PER/NP, in this case capitalized.
6. A head constituent for the PER/NP, in this
case a PER -R/NP.
7. Pre-modifier constituents for the PER/NP.
</listItem>
<bodyText confidence="0.910674272727273">
In this case, there are none.
8. Post-modifier constituents for the
PER/NP. First a comma, then an SBAR
structure, and then a second comma are
each generated in turn.
This generation process is continued until the
entire tree has been produced.
We now briefly summarize the probability
structure of the model. The categories for
head constituents, cl„ are predicted based
solely on the category of the parent node, cp:
</bodyText>
<equation confidence="0.751438">
P(ch ), e.g. P(vp(s)
</equation>
<bodyText confidence="0.999271714285714">
Modifier constituent categories, cm, are
predicted based on their parent node, cp, the
head constituent of their parent node, chp, the
previously generated modifier, c„,_1, and the
head word of their parent, wp. Separate
probabilities are maintained for left (pre) and
right (post) modifiers:
</bodyText>
<equation confidence="0.98743325">
PL(cmjcp,chp,cm_l,wp), e.g.
PL(per I npl s,vp,null,said)
PR(c,„Icp,chp,c„,_1,wp), e.g.
PR(null I s,vp,null,said)
</equation>
<bodyText confidence="0.999209">
Part-of-speech tags, t,,, for modifiers are
predicted based on the modifier, cm, the part-
of-speech tag of the head word, th, and the
head word itself, wh:
</bodyText>
<equation confidence="0.912491">
P(tmlc,,,,th,wh), e.g.
P(per I nnp I per I np,vbd,said)
</equation>
<bodyText confidence="0.930924166666667">
Head words, w„„ for modifiers are predicted
based on the modifier, cm, the part-of-speech
tag of the modifier word , t„„ the part-of-
speech tag of the head word , th, and the head
word itself, wh:
lAwmicm,tm,th,wh), e.g.
</bodyText>
<subsubsectionHeader confidence="0.730306">
P(nancel per I np, per! nnp,vbd, said)
</subsubsectionHeader>
<bodyText confidence="0.9999765">
Finally, word features, fm, for modifiers are
predicted based on the modifier, cm, the part-
of-speech tag of the modifier word , t„„ the
part-of-speech tag of the head word th, the
head word itself, wh, and whether or not the
modifier head word, w„„ is known or unknown.
</bodyText>
<equation confidence="0.8959955">
P( f„, ICm,tmth,wh,known(w,n)) , e.g.
P(capl per I np, per I nnp,vbd,said,true)
</equation>
<bodyText confidence="0.999965285714286">
The probability of a complete tree is the
product of the probabilities of generating each
element in the tree. If we generalize the tree
components (constituent labels, words, tags,
etc.) and treat them all as simply elements, e,
and treat all the conditioning factors as the
history, h, we can write:
</bodyText>
<equation confidence="0.904062">
P(tree)= [JP(eh)
CE tree
</equation>
<sectionHeader confidence="0.94385" genericHeader="method">
8 Training the Model
</sectionHeader>
<bodyText confidence="0.999405222222222">
Maximum likelihood estimates for the model
probabilities can be obtained by observing
frequencies in the training corpus. However,
because these estimates are too sparse to be
relied upon, we use interpolated estimates
consisting of mixtures of successively lower-
order estimates (as in Placeway et al. 1993).
For modifier constituents, the mixture
components are:
</bodyText>
<equation confidence="0.827596333333333">
P&apos;(c„,lc p) =
P(c„,lc p,chp,c,„_,,wp)
+22 P(c„,1cp,chp,c„,„)
</equation>
<bodyText confidence="0.5546025">
For part-of-speech tags, the mixture
components are:
</bodyText>
<figure confidence="0.645089">
P(t,„Ic„„th,wh)= Al P(t„,ic„„wh)
+22 P(t„,jc„„th)
+23 P(t„,tc„,)
For head words, the mixture components are:
P&apos;(w„,Ic„„t„„th,wh)= P(w„,Ic„„tm,wh)
+22 P(wmicnutnoth)
+23 P(wmic„,,t„,)
+24 Rwmitm)
</figure>
<bodyText confidence="0.9935645">
Finally, for word features, the mixture
components are:
</bodyText>
<page confidence="0.986508">
231
</page>
<figure confidence="0.9956042">
P&apos;(f„,lct„,,t,„w„,known(w„,))=
A, P(f„,)c„„t„„w„,known(w„,))
+A, P(flc„„t„„t,„known(w,„))
+A, P(f„,fc„„t„„known(w„,))
+A, P(f,„)t„,,known(w„,))
</figure>
<sectionHeader confidence="0.933114" genericHeader="method">
9 Searching the Model
</sectionHeader>
<bodyText confidence="0.999759181818182">
Given a sentence to be analyzed, the search
program must find the most likely semantic
and syntactic interpretation. More precisely, it
must find the most likely augmented parse
tree. Although mathematically the model
predicts tree elements in a top-down fashion,
we search the space bottom-up using a chart-
based search. The search is kept tractable
through a combination of CKY-style dynamic
programming and pruning of low probability
elements.
</bodyText>
<subsectionHeader confidence="0.920645">
9.1 Dynamic Programming
</subsectionHeader>
<bodyText confidence="0.94668">
Whenever two or more constituents are
equivalent relative to all possible later parsing
decisions, we apply dynamic programming,
keeping only the most likely constituent in the
chart. Two constituents are considered
equivalent if:
</bodyText>
<listItem confidence="0.99248275">
1. They have identical category labels.
2. Their head constituents have identical
labels.
3. They have the same head word.
4. Their leftmost modifiers have identical
labels.
5. Their rightmost modifiers have identical
labels.
</listItem>
<subsectionHeader confidence="0.990746">
9.2 Pruning
</subsectionHeader>
<bodyText confidence="0.969800625">
threshold of the highest scoring constituent are
maintained; all others are pruned. For
purposes of pruning, and only for purposes of
pruning, the prior probability of each
constituent category is multiplied by the
generative probability of that constituent
(Goodman, 1997). We can think of this prior
probability as an estimate of the probability of
generating a subtree with the constituent
category, starting at the topmost node. Thus,
the scores used in pruning can be considered
as the product of:
1. The probability of generating a constituent
of the specified category, starting at the
topmost node.
2. The probability of generating the structure
beneath that constituent, having already
generated a constituent of that category.
Given a new sentence, the outcome of this
search process is a tree structure that encodes
both the syntactic and semantic structure of the
sentence. The semantics — that is, the entities
and relations — can then be directly extracted
from these sentential trees.
</bodyText>
<sectionHeader confidence="0.988804" genericHeader="method">
10 Experimental Results
</sectionHeader>
<bodyText confidence="0.999837357142857">
Our system for MUC-7 consisted of the
sentential model described in this paper,
coupled with a simple probability model for
cross-sentence merging. The evaluation
results are summarized in Table 1.
In both Template Entity (TE) and Template
Relation (TR), our system finished in second
place among all entrants. Nearly all of the
work was done by the sentential model;
disabling the cross-sentence model entirely
reduced our overall F-Score by only 2 points.
Given multiple constituents that cover
identical spans in the chart, only those
constituents with probabilities within a
</bodyText>
<table confidence="0.995728">
Task Recall Precision F-Score
Entities (TE) 83% 84% 83.49%
Relations (TR) 64% 81% 71.23%
</table>
<tableCaption confidence="0.999471">
Table 1: MUC-7 scores.
</tableCaption>
<page confidence="0.815159">
232
</page>
<table confidence="0.99973825">
Task Score
Part-of-Speech Tagging 95.99 (% correct)
Parsing (sentences &lt;40 words) 85.06 (F-Score)
Name Finding 92.28 (F-Score)
</table>
<tableCaption confidence="0.999037">
Table 2: Component task performance.
</tableCaption>
<bodyText confidence="0.999600933333333">
While our focus throughout the project was on
TE and TR, we became curious about how
well the model did at part-of-speech tagging,
syntactic parsing, and at name finding. We
evaluated part-of-speech tagging and parsing
accuracy on the Wall Street Journal using a
now standard procedure (see Collins 97), and
evaluated name finding accuracy on the MUC-
7 named entity test. The results are
summarized in Table 2.
While performance did not quite match the
best previously reported results for any of
these three tasks, we were pleased to observe
that the scores were at or near state-of-the-art
levels for all cases.
</bodyText>
<sectionHeader confidence="0.993977" genericHeader="conclusions">
11 Conclusions
</sectionHeader>
<bodyText confidence="0.999994642857143">
We have demonstrated, at least for one
problem, that a lexicalized, probabilistic
context-free parser with head rules (LPCFG-
HR) can be used effectively for information
extraction. A single model proved capable of
performing all necessary sentential processing,
both syntactic and semantic. We were able to
use the Penn TREEBANK to estimate the
syntactic parameters; no additional syntactic
training was required. The semantic training
corpus was produced by students according to
a simple set of guidelines. This simple
semantic annotation was the only source of
task knowledge used to configure the model.
</bodyText>
<sectionHeader confidence="0.997513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999642">
The work reported here was supported in part
by the Defense Advanced Research Projects
Agency. Technical agents for part of this work
were Fort Huachucha and AFRL under
contract numbers DABT63-94-C-0062,
F30602-97-C-0096, and 4132-BBN-001. The
views and conclusions contained in this
document are those of the authors and should
not be interpreted as necessarily representing
the official policies, either expressed or
implied, of the Defense Advanced Research
Projects Agency or the United States
Government.
We thank Michael Collins of the University of
Pennsylvania for his valuable suggestions.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999959090909091">
Bike!, Dan; S. Miller; R. Schwartz; and R.
Weischedel. (1997) &amp;quot;NYMBLE: A High-
Performance Learning Name-finder.&amp;quot; In
Proceedings of the Fifth Conference on Applied
Natural Language Processing, Association for
Computational Linguistics, pp. 194-201.
Collins, Michael. (1996) &amp;quot;A New Statistical Parser
Based on Bigram Lexical Dependencies.&amp;quot; In
Proceedings of the 34th Annual Meeting of the
Association for Computational Linguistics, pp.
184-191.
Collins, Michael. (1997) —Three Generative,
Lexicalised Models for Statistical Parsing.&amp;quot; In
Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics, pp.
16-23.
Marcus, M.; B. Santorini; and M. Marcinkiewicz.
(1993) &amp;quot;Building a Large Annotated Corpus of
English: the Penn Treebank.&amp;quot; Computational
Linguistics, 19 (2): 313-330.
Goodman, Joshua. (1997) &amp;quot;Global Thresh°!ding
and Multiple-Pass Parsing.&amp;quot; In Proceedings of
the Second Conference on Empirical Methods in
Natural Language Processing, Association for
Computational Linguistics, pp. 11-25.
Placeway, P., R. Schwartz, et al. (1993). &amp;quot;The
Estimation of Powerful Language Models from
Small and Large Corpora.&amp;quot; IEEE ICASSP
Weischedel, Ralph; Marie Meteer; Richard
Schwartz; Lance Ramshaw; and Jeff Palmucci.
(1993) &amp;quot;Coping with Ambiguity and Unknown
Words through Probabilistic Models.&amp;quot;
Computational Linguistics, 19(2):359-382.
</reference>
<page confidence="0.998958">
233
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.877106">
<title confidence="0.963913">A Novel Use of Statistical Parsing to Extract Information from Text</title>
<author confidence="0.999698">Scott Miller</author>
<author confidence="0.999698">Heidi Fox</author>
<author confidence="0.999698">Lance Ramshaw</author>
<author confidence="0.999698">Ralph Weischedel</author>
<affiliation confidence="0.975341">BBN Technologies</affiliation>
<address confidence="0.999573">70 Fawcett Street, Cambridge, MA 02138</address>
<email confidence="0.999863">szmiller@bbn.com</email>
<abstract confidence="0.997175090909091">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dan Bike</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>NYMBLE: A HighPerformance Learning Name-finder.&amp;quot;</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing, Association for Computational Linguistics,</booktitle>
<pages>194--201</pages>
<marker>Bike, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>Bike!, Dan; S. Miller; R. Schwartz; and R. Weischedel. (1997) &amp;quot;NYMBLE: A HighPerformance Learning Name-finder.&amp;quot; In Proceedings of the Fifth Conference on Applied Natural Language Processing, Association for Computational Linguistics, pp. 194-201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A New Statistical Parser Based on Bigram Lexical Dependencies.&amp;quot;</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="660" citStr="Collins, 1996" startWordPosition="93" endWordPosition="94">nformation from Text Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph Weischedel BBN Technologies 70 Fawcett Street, Cambridge, MA 02138 szmiller@bbn.com Abstract Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations. 1 Introduction Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard. Yet, relatively few have embedded one of these algorithms in a task. Chiba, (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition. In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction. The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) i</context>
<context position="13349" citStr="Collins 1996" startWordPosition="2049" endWordPosition="2050">nserted node serves to indicate the relation as well as the argument. For example, in the phrase &amp;quot;Lt. Cmdr. David Edwin Lewis,&amp;quot; a node is inserted to indicate that &amp;quot;Lt. Cmdr.&amp;quot; is a descriptor for &amp;quot;David Edwin Lewis.&amp;quot; 5. Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes. These labels serve to form a continuous chain between the relation and its argument. 7 Model Structure In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997). The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process. For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward. Head words, along with their part-of-speech tags and features, are generated for each modifier as soon as the modifier is created. Word features are introduced primarily to help with unknown words, as in (Weischedel et al. 1993). We illustrate the generation process by walki</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Collins, Michael. (1996) &amp;quot;A New Statistical Parser Based on Bigram Lexical Dependencies.&amp;quot; In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pp. 184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three Generative, Lexicalised Models for Statistical Parsing.&amp;quot;</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="6196" citStr="Collins 1997" startWordPosition="896" endWordPosition="897">imit the propagation of errors by making all decisions jointly. For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other. A second consideration influenced our decision toward an integrated model. We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al. 1997). Finally, our newly constructed parser, like that of (Collins 1997), was based on a generative statistical model. Thus, each component of what would be the first three stages of our pipeline was based on 227 the same general class of statistical model. Although each model differed in its detailed probability structure, we believed that the essential elements of all three models could be generalized in a single probability model. If the single generalized model could then be extended to semantic analysis, all necessary sentence level processing would be contained in that model. Because generative statistical models had already proven successful for each of the</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, Michael. (1997) —Three Generative, Lexicalised Models for Statistical Parsing.&amp;quot; In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pp. 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: the Penn Treebank.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M.; B. Santorini; and M. Marcinkiewicz. (1993) &amp;quot;Building a Large Annotated Corpus of English: the Penn Treebank.&amp;quot; Computational Linguistics, 19 (2): 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Global Thresh°!ding and Multiple-Pass Parsing.&amp;quot;</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics,</booktitle>
<pages>11--25</pages>
<contexts>
<context position="18909" citStr="Goodman, 1997" startWordPosition="2913" endWordPosition="2914"> keeping only the most likely constituent in the chart. Two constituents are considered equivalent if: 1. They have identical category labels. 2. Their head constituents have identical labels. 3. They have the same head word. 4. Their leftmost modifiers have identical labels. 5. Their rightmost modifiers have identical labels. 9.2 Pruning threshold of the highest scoring constituent are maintained; all others are pruned. For purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman, 1997). We can think of this prior probability as an estimate of the probability of generating a subtree with the constituent category, starting at the topmost node. Thus, the scores used in pruning can be considered as the product of: 1. The probability of generating a constituent of the specified category, starting at the topmost node. 2. The probability of generating the structure beneath that constituent, having already generated a constituent of that category. Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of t</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Goodman, Joshua. (1997) &amp;quot;Global Thresh°!ding and Multiple-Pass Parsing.&amp;quot; In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, pp. 11-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Placeway</author>
<author>R Schwartz</author>
</authors>
<title>The Estimation of Powerful Language Models from Small and Large Corpora.&amp;quot;</title>
<date>1993</date>
<journal>IEEE ICASSP</journal>
<marker>Placeway, Schwartz, 1993</marker>
<rawString>Placeway, P., R. Schwartz, et al. (1993). &amp;quot;The Estimation of Powerful Language Models from Small and Large Corpora.&amp;quot; IEEE ICASSP</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Marie Meteer</author>
<author>Richard Schwartz</author>
<author>Lance Ramshaw</author>
<author>Jeff Palmucci</author>
</authors>
<title>Coping with Ambiguity and Unknown Words through Probabilistic Models.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="6024" citStr="Weischedel et al. 1993" startWordPosition="867" endWordPosition="870">interpretation failure. There is no opportunity for a later stage, such as parsing, to influence or correct an earlier stage such as part-of-speech tagging. An integrated model can limit the propagation of errors by making all decisions jointly. For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other. A second consideration influenced our decision toward an integrated model. We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al. 1997). Finally, our newly constructed parser, like that of (Collins 1997), was based on a generative statistical model. Thus, each component of what would be the first three stages of our pipeline was based on 227 the same general class of statistical model. Although each model differed in its detailed probability structure, we believed that the essential elements of all three models could be generalized in a single probability model. If the single generalized model could then be extended to sema</context>
<context position="13902" citStr="Weischedel et al. 1993" startWordPosition="2131" endWordPosition="2134">ated according to a process similar to that described in (Collins 1996, 1997). The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process. For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward. Head words, along with their part-of-speech tags and features, are generated for each modifier as soon as the modifier is created. Word features are introduced primarily to help with unknown words, as in (Weischedel et al. 1993). We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3. At each step in the process, a choice is made from a statistical distribution, with the probability of each possible selection dependent on particular features of previously generated elements. We pick up the derivation just after the topmost S and its head word, said, have been produced. The next steps are to generate in order: 1. A head constituent for the S, in this case a VP. 2. Pre-modifier constituents for the S. In this case, there is only one: a PER/NP. 3. A head part-of-speech </context>
</contexts>
<marker>Weischedel, Meteer, Schwartz, Ramshaw, Palmucci, 1993</marker>
<rawString>Weischedel, Ralph; Marie Meteer; Richard Schwartz; Lance Ramshaw; and Jeff Palmucci. (1993) &amp;quot;Coping with Ambiguity and Unknown Words through Probabilistic Models.&amp;quot; Computational Linguistics, 19(2):359-382.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>