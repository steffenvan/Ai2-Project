<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000566">
<title confidence="0.650588">
Beyond Left-to-Right: Multiple Decomposition Structures for SMT
</title>
<note confidence="0.862525333333333">
Hui Zhang* Kristina Toutanova Chris Quirk Jianfeng Gao
USC/ISI Microsoft Research Microsoft Research Microsoft Research
Los Angeles, CA 90089 Redmond, WA 98502 Redmond, WA 98502 Redmond, WA 98502
</note>
<email confidence="0.829786">
hzhang@isi.edu kristout@microsoft.com chrisq@microsoft.com jfgao@microsoft.com
</email>
<sectionHeader confidence="0.990751" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999408652173913">
Standard phrase-based translation models do
not explicitly model context dependence be-
tween translation units. As a result, they rely
on large phrase pairs and target language mod-
els to recover contextual effects in translation.
In this work, we explore n-gram models over
Minimal Translation Units (MTUs) to explic-
itly capture contextual dependencies across
phrase boundaries in the channel model. As
there is no single best direction in which con-
textual information should flow, we explore
multiple decomposition structures as well as
dynamic bidirectional decomposition. The
resulting models are evaluated in an intrin-
sic task of lexical selection for MT as well
as a full MT system, through n-best rerank-
ing. These experiments demonstrate that ad-
ditional contextual modeling does indeed ben-
efit a phrase-based system and that the direc-
tion of conditioning is important. Integrating
multiple conditioning orders provides consis-
tent benefit, and the most important directions
differ by language pair.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999361">
The translation procedure of a classical phrase-
based translation model (Koehn et al., 2003) first di-
vides the input sentence into a sequence of phrases,
translates each phrase, explores reorderings of these
translations, and then scores the resulting candi-
dates with a linear combination of models. Conven-
tional models include phrase-based channel models
that effectively model each phrase as a large uni-
gram, reordering models, and target language mod-
els. Of these models, only the target language model
</bodyText>
<footnote confidence="0.9611425">
This research was conducted during the author’s internship
at Microsoft Research
</footnote>
<page confidence="0.992784">
12
</page>
<bodyText confidence="0.999965972222222">
(and, to some weak extent, the lexicalized reordering
model) captures some lexical dependencies that span
phrase boundaries, though it is not able to model in-
formation from the source side. Larger phrases cap-
ture more contextual dependencies within a phrase,
but individual phrases are still translated almost in-
dependently.
To address this limitation, several researchers
have proposed bilingual n-gram Markov models
(Marino et al., 2006) to capture contextual depen-
dencies between phrase pairs. Much of their work
is limited by the requirement “that the source and
target side of a tuple of words are synchronized, i.e.
that they occur in the same order in their respective
languages” (Crego and Yvon, 2010).
For language pairs with significant typological di-
vergences, such as Chinese-English, it is quite dif-
ficult to extract a synchronized sequence of units;
in the limit, the smallest synchronized unit may be
the whole sentence. Other approaches explore incor-
poration into syntax-based MT systems or replacing
the phrasal translation system altogether.
We investigate the addition of MTUs to a phrasal
translation system to improve modeling of con-
text and to provide more robust estimation of long
phrases. However, in a phrase-based system there
is no single synchronized traversal order; instead,
we may consider the translation units in many pos-
sible orders: left-to-right or right-to-left according
to either the source or the target are natural choices.
Alternatively we consider translating a particularly
unambiguous unit in the middle of the sentence
and building outwards from there. We investigate
both consistent and dynamic decomposition orders
in several language pairs, looking at distinct orders
in isolation and combination.
</bodyText>
<note confidence="0.5324165">
Proceedings of NAACL-HLT 2013, pages 12–21,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.998152" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999889815789474">
Marino et al. (2006) proposed a translation model
using a Markov model of bilingual n-grams, demon-
strating state-of-the-art performance compared to
conventional phrase-based models. Crego and
Yvon (2010) further explored factorized n-gram ap-
proaches, though both models considered rather
large n-grams; this paper focuses on small units with
asynchronous orders in source and target. Durrani
et al. (2011) developed a joint model that captures
translation of contiguous and gapped units as well as
reordering. Two prior approaches explored similar
models in syntax based systems. MTUs have been
used in dependency translation models (Quirk and
Menezes, 2006) to augment syntax directed trans-
lation systems. Likewise in target language syntax
systems, one can consider Markov models over min-
imal rules, where the translation probability of each
rule is adjusted to include context information from
parent rules (Vaswani et al., 2011).
Most prior work tends to replace the existing
probabilities rather than augmenting them. We be-
lieve that Markov rules provide an additional sig-
nal but are not a replacement. Their distributions
should be more informative than the so-called “lex-
ical weighting” models, and less sparse than rela-
tive frequency estimates, though potentially not as
effective for truly non-compositional units. There-
fore, we explore the inclusion of all such informa-
tion. Also, unlike prior work, we explore combina-
tions of multiple decomposition orders, as well as
dynamic decompositions. The most useful context
for translation differs by language pair, an important
finding when working with many language pairs.
We build upon a standard phrase-based approach
(Koehn et al., 2003). This acts as a proposal dis-
tribution for translations; the MTU Markov models
provide additional signal as to which translations are
correct.
</bodyText>
<sectionHeader confidence="0.991917" genericHeader="method">
3 MTU n-gram Markov models
</sectionHeader>
<bodyText confidence="0.9983856">
We begin by defining Minimal Translation Units
(MTUs) and describing how to identify them in
word-aligned text. Next we define n-gram Markov
models over MTUs, which requires us to define
traversal orders over MTUs.
</bodyText>
<figureCaption confidence="0.995474">
Figure 1: Word alignment and minimum translation units.
</figureCaption>
<subsectionHeader confidence="0.99753">
3.1 Definition of an MTU
</subsectionHeader>
<bodyText confidence="0.978696">
Informally, the notion of a minimal translation unit
is simple: it is a translation rule that cannot be
</bodyText>
<equation confidence="0.656887">
? ? ? ? ?
</equation>
<bodyText confidence="0.999776413793104">
broken down any further without violating the con-
straints of the rules. We restrict ourselves to contigu-
ous MTUs. They are similar to small phrase pairs,
though unlike phrase pairs we allow MTUs to have
either an empty source or empty target side, thereby
allowing insertion and deletion phrases. Conven-
tional phrase pairs may be viewed as compositions
of these MTUs up to a given size limit.
Consider a word-aligned sentence pair consisting
of a sequence of source words s = s1 ... sm, a se-
quence of target words t = t1 ... tn, and a word align-
ment relation between the source and target words
— c {1..ml x {1..nl. A translation unit is a sequence
of source words si..sj and a sequence of target words
tk..tl (one of which may be empty) such that for all
aligned pairs i&apos; — k&apos;, we have i &lt;_ i&apos; &lt;_ j if and only
if k &lt;_ k&apos; &lt;_ l. This definition, nearly identical to
that of a phrase pair (Koehn et al., 2003), relaxes the
constraint that one aligned word must be present.
A set of translation units is a partition of the sen-
tence pair if each source and target word is covered
exactly once. Minimal translation units is the par-
tition with the smallest average unit size, or, equiv-
alently, the largest number of units. For example,
Figure 1 shows a word-aligned sentence pair and its
corresponding set of MTUs. We extract these min-
imal translation units with an algorithm similar to
that of phrase extraction.
We train n-gram Markov models only over min-
</bodyText>
<figure confidence="0.99148975">
M1 M2 M3 M4 M5
held the meeting null yesterday
null
会谈
HuiTan
于
Yu
昨天
ZuoTian
举V
JuXing
M1: Yu =&gt; null
M2: ZuoTian =&gt; yesterday
M3: JuXing =&gt; held
M4: null =&gt; the
M5: HuiTan =&gt; meeting
</figure>
<page confidence="0.998049">
13
</page>
<bodyText confidence="0.999528647058823">
imal rules for two reasons. First, the segmentation
of the sentence pair is not unique under composed
rules, which makes probability estimation compli-
cated. Second, some phrase pairs are very large,
which results in sparse data issues and compromises
the model quality. Therefore, training an n-gram
model over minimal translation units turns out to
be a simple and clean choice: the resulting segmen-
tation is unique, and the distribution is smooth. If
we want to capture more context, we can simply in-
crease the order of the Markov model.
Such Markov models address issues in large
phrase-based translation approaches. Where stan-
dard phrase-based models rely upon large unigrams
to capture contextual information, n-grams of mini-
mal translation units allow a robust contextual model
that is less constrained by segmentation.
</bodyText>
<subsectionHeader confidence="0.998489">
3.2 MTU enumeration orders
</subsectionHeader>
<bodyText confidence="0.996806108108108">
When defining a joint probability distribution over
MTUs of an aligned sentence pair, it is necessary
to define a decomposition, or generation order for
the sentence pair. For a single sequence in lan-
guage modeling or synchronized sequences in chan-
nel modeling, the default enumeration order has
been left-to-right.
Different decomposition orders have been used
in part-of-speech tagging and named entity recog-
nition (Tsuruoka and Tsujii, 2005). Intuitively, in-
formation from the left or right could be more use-
ful for particular disambiguation choices. Our re-
search on different decomposition orders was moti-
vated by this work. When applying such ideas to
machine translation, there are additional challenges
and opportunities. The task exhibits much more am-
biguity – the number of possible MTUs is in the
millions. An opportunity arises from the reordering
phenomenon in machine translation: while in POS
tagging the natural decomposition orders to study
are only left-to-right and right-to-left, in machine
translation we can further distinguish source and tar-
get sentence orders.
We first define the source left-to-right and the tar-
get left-to-right orders of the aligned sets of MTUs.
The definition is straightforward when there are no
inserted or deleted word. To place the nulls corre-
sponding to such word we use the following defi-
nition: the source position of the null for a target
inserted word is just after the position of the last
source word aligned to the closest preceding non-
null aligned target word. The target position for a
null corresponding to a source deleted MTU is de-
fined analogously. In Figure 1 we define the posi-
tion of M4 to be right after M3 (because “the” is
after “held” in left-to-right order on the target side).
The complete MTU sequence in source left-to-
right order is M1-M2-M3-M4-M5. The sequence
in target left-to-right order is M3-M4-M5-M1-M2.
This illustrates that decomposition structure may
differ significantly depending on which language is
used to define the enumeration order.
Once a sentence pair is represented as a sequence
of MTUs, we can define the probability of the
sentence pair using a conventional n-gram Markov
model (MM) over MTUs. For example, the 3-gram
MM probability of the sentence pair in Figure 1
under the source left-to-right order is as follows:
P(M1)·P(M2|M1)·P(M3|M1, M2)·P(M4|M2, M3)·
P(M5|M3, M4).
Different decomposition orders use different con-
text for disambiguation and it is not clear apriori
which would perform best. We compare all four
decomposition orders (source order left-to-right and
right-to-left, and target order left-to-right and right-
to-left). Although the independence assumptions of
left-to-right and right-to-left are the same, the result-
ing models may be different due to smoothing.
In addition to studying these four basic decompo-
sition orders, we report performance of two cyclic
orders: cyclic in source or target sentence order.
These models are inspired by the cyclic depen-
dency network model proposed for POS tagging
(Toutanova et al., 2003) and also used as a baseline
in previous work on dynamic decomposition orders
(Tsuruoka and Tsujii, 2005). 1
The probability according to the cyclic orders is
defined by conditioning each MTU on both its left
and right neighbor MTUs. For example, the prob-
ability of the sentence pair in Figure 1 under the
source cyclic order, using a 3-gram model is defined
as: P(M1|M2) · P(M2|M1, M3) · P(M3|M2, M4) ·
P(M4|M3, M5) · P(M5|M4).
All n-gram Markov models over MTUs are esti-
</bodyText>
<footnote confidence="0.741900333333333">
1The correct application of such models requires sampling
to find the highest scoring sequence, but we apply the max prod-
uct approximation as done in previous work.
</footnote>
<page confidence="0.997791">
14
</page>
<bodyText confidence="0.999895">
mated using Kneser-Ney smoothing. Each MTU is
treated as an atomic unit in the vocabulary of the
n-gram model. Counts of all n-grams are obtained
from the parallel MT training data, using different
MTU enumeration orders.
Note that if we use a target-order decomposition,
the model provides a distribution over target sen-
tences and the corresponding source sides of MTUs,
albeit unordered. Likewise source order based mod-
els provide distributions over source sentences and
unordered target sides of MTUs. We attempted to
introduce reordering models to predict an order over
the resulting MTU sequences using approaches sim-
ilar to reordering models for phrases. Although
these models produced gains in some language pairs
when used without translation MTU MMs, there
were no additional gains over a model using mul-
tiple translation MTU MMs.
</bodyText>
<sectionHeader confidence="0.985514" genericHeader="method">
4 Lexical selection
</sectionHeader>
<bodyText confidence="0.977803">
We perform an empirical evaluation of different
</bodyText>
<equation confidence="0.8516705">
M1 M2 M3 M4 M5
MTU decomposition orders on a simplified machine
translation task: lexical selection. In this task we
于 昨天 举行 会谈
</equation>
<bodyText confidence="0.964444875">
Y ZTi JXi nll HiT
assume that the source sentence segmentation into
minimal translation units is given and that the or-
der of the corresponding target sides of the minimal
translation units is also given. The problem is to
held the meeting null yesteday
predict the target sides of the MTUs, called target
MTUs for brevity (see Figure 2). The lexical selec-
</bodyText>
<equation confidence="0.5863575">
M1 Yu &gt; ull
un
tion task is thus similar to sequence tagging tasks
M3: JuXng &gt; held
</equation>
<bodyText confidence="0.7623755">
likeMpart-of-speech tagging, though much more dif-
ficult: the predicted variables are sequences of target
M5: HuiTan &gt; meeting
language words with millions of possible outcomes.
</bodyText>
<figureCaption confidence="0.996842">
Figure 2: Lexical selection.
</figureCaption>
<bodyText confidence="0.999952666666667">
We use this constrained MT setting to evaluate the
performance of models using different MTU decom-
position orders and models using combinations of
decomposition orders. The simplified setting allows
controlled experimentation while lessening the im-
pact of complicating factors in a full machine trans-
lation setting (search error, reordering limits, phrase
table pruning, interaction with other models).
To perform the tagging task, we use trigram MTU
models. The four basic decomposition orders for
MTU Markov models we use are left-to-right in tar-
get sentence order, right-to-left in target sentence or-
der, left-to-right in source sentence order, and right-
to-left in source sentence order. We also consider
cyclic orders in source and target.
Regardless of the decomposition order used, we
perform decoding using a beam search decoder, sim-
ilar to ones used in phrase-based machine transla-
tion. The decoder builds target hypotheses in left-
to-right target sentence order. At each step, it fills in
the translation of the next source MTU, in the con-
text of the already predicted MTUs to its left. The
top scoring complete hypotheses covering the first m
MTUs are maintained in a beam. When scoring with
a target left-to-right MTU Markov model (L2RT),
we can score each partial hypothesis exactly at each
step. When scoring using a R2LT model or a source
order model, we use lower-order approximations to
the trigram MTU Markov model scores as future
scores, since not all needed context is available for a
hypothesis at the time of construction. As additional
context becomes available, the exact score can be
computed. 2
</bodyText>
<subsectionHeader confidence="0.990048">
4.1 Basic decomposition order combinations
</subsectionHeader>
<bodyText confidence="0.9998176">
We first introduce two methods of combining differ-
ent decomposition orders: product and system com-
bination.
The product method arises naturally in the ma-
chine translation setting, where probabilities from
different models are multiplied together and further
weighted to form the log-linear model for machine
translation (Och and Ney, 2002). We define a similar
scoring function using a set of MTU Markov models
MM1, ..., MMk for a hypothesis h as follows:
</bodyText>
<equation confidence="0.955835">
Score(h) _ A1logPMM1(h) + ... + AklogPMMk(h)
</equation>
<bodyText confidence="0.934390166666667">
2We apply hypothesis recombination, which can merge hy-
potheses that are indistinguishable with respect to future contin-
uations. This is similar to recombination in a standard-phrase
based decoder with the difference that it is not always the last
two target MTUs that define the context needed by future ex-
tensions.
</bodyText>
<figure confidence="0.991862909090909">
于
Yu
? ? ?
昨天
ZuoTian
举行
JuXing
null
? ?
会谈
HuiTan
</figure>
<page confidence="0.924467">
15
</page>
<bodyText confidence="0.99992764">
The weights A of different models are trained on a
development set using MER training to maximize
the BLEU score of the resulting model. Note that
this method of model combination was not consid-
ered in any of the previous works comparing differ-
ent decompositions.
The system combination method is motivated
by prior work in machine translation which com-
bined left-to-right and right-to-left machine trans-
lation systems (Finch and Sumita, 2009). Simi-
larly, we perform sentence-level system combina-
tion between systems using different MTU Markov
models to come up with most likely translations.
If we have k systems guessing hypotheses based
on MM1, ... , MMk respectively, we generate 1000-
best lists from each system, resulting in a pool of
up to 1000k possible distinct translations. Each of
the candidate hypotheses from MMi is scored with
its Markov model log-probability logPMMi(h). We
compute normalized probabilities for each system’s
n-best by exponentiating and normalizing: Pi(h) a
PMMi(h). If a hypothesis h is not in system i’s n-
best list, we assume its probability is zero according
to that system. The final scoring function for each
hypothesis in the combined list of candidates is:
</bodyText>
<equation confidence="0.996771">
Score(h) = A1P1(h) + ... + AkPk(h)
</equation>
<bodyText confidence="0.9990865">
The weights A for the combination are tuned using
MERT as for the product model.
</bodyText>
<subsectionHeader confidence="0.960777">
4.2 Dynamic decomposition orders
</subsectionHeader>
<bodyText confidence="0.971089411764706">
A more complex combination method chooses the
best possible decomposition order for each transla-
tion dynamically, using a set of constraints to de-
fine the possible decomposition orders, and a set of
features to score the candidate decompositions. We
term this method dynamic combination. The score
of each translation is defined as its score according
to the highest-scoring decomposition order for that
translation.
This method is very similar to the bidirectional
tagging approach of Tsuruoka and Tsujii (2005).
For this approach we only explored combinations of
target language orders (L2RT, CycT, and R2LT). If
source language orders were included, the complex-
ity of decoding would increase substantially.
Figure 3 shows two possible decompositions for
a short MTU sequence. The structures displayed are
</bodyText>
<figureCaption confidence="0.999437">
Figure 3: Different decompositions.
</figureCaption>
<bodyText confidence="0.999865382352941">
directed graphical models. They define the set of
parents (context) used to predict each target MTU.
The decomposition structures we consider are lim-
ited to acyclic graphs where each node can have one
of the following parent configurations: no parents
(C = 0 in the Figure), one left parent (C = 1L),
one right parent (C = 1R), one left and one right
parent (C = LR), two left parents (C = 2L), and
two right parents (C = 2R). If all nodes have two
left parents, we recover the left-to-right decomposi-
tion order, and if all nodes have two right parents,
the right-to-left decomposition order. A mixture of
parent configurations defines a mixed, dynamic de-
composition order. The decomposition order chosen
varies from translation to translation.
A directed graphical model defines the probability
of an assignment of MTUs to the variable nodes as a
product of local probabilities of MTUs given their
parents. Here we extend this definition to scores
of assignments by using a linear model with con-
figuration features and log-probability features. The
configuration features are indicators of which par-
ent configuration is active at a node and the settings
of these features for the decompositions in Figure
3 are shown as assignments to the C variables. The
log-probability feature values are obtained by query-
ing the appropriate n-gram model: L2RT, CycT, or
R2LT. For a node with one or two left parents, the
log-probability is computed according to the L2RT
model. For a node with one or two right parents, the
R2LT model is queried. The CycT model is used for
nodes with one left and one right parent.
To find the best translation of a sentence the
model now searches over hidden decomposition or-
</bodyText>
<equation confidence="0.997665428571429">
1
( 1�
1
2
( 2 |1�
2
( 3 |2, 1)
3
2
4
( 4 |3, 2)
1
( 1) ( 2 |1, 3)
( 3 |4) ( 4)
</equation>
<page confidence="0.988454">
16
</page>
<bodyText confidence="0.999923111111111">
ders in addition to assignments to target MTUs. The
final score of a translation and decomposition is a
linear combination of the two types of feature values
– model log-probabilities and configuration types.
There is one feature weight for each parent con-
figuration (six configuration weights) and one fea-
ture weight for each component model (three model
weights). The final score of the second decomposi-
tion and assignment in Figure 3 is:
</bodyText>
<equation confidence="0.9948095">
Score(h)
= 2 * wC0 + wCLR + wC1R
+ wL2Rlo9PLR(m1) + wCyclo9PCyc(m2|m1, m3)
+ wR2Llo9PRL(m3|m4) + wL2Rlo9PLR(m4)
</equation>
<bodyText confidence="0.999985577777778">
There are two main differences between our ap-
proach and that of Tsuruoka and Tsujii (2005): we
perform beam search with hypothesis recombination
instead of exact decoding (due to the larger size of
the hypothesis set), and we use parameters to be
able to globally weight the probabilities from dif-
ferent models and to develop preferences for using
certain types of decompositions. For example, the
model can learn to prefer right-to-left decomposi-
tions for one language pair, and left-to-right decom-
positions for another. An additional difference from
prior work is the definition of the possible decompo-
sition orders that are searched over.
Compared to the structures allowed in (Tsuruoka
and Tsujii, 2005) for a trigram baseline model, our
allowed structures are a subset; in (Tsuruoka and
Tsujii, 2005) there are sixteen possible parent con-
figurations (up to two left and two right parents),
whereas we allow only six. We train and use only
three n-gram Markov models to assign probabilities:
L2RT, R2LT, and CycT, whereas the prior work used
sixteen models. One could potentially see additional
gains from considering a larger space of structures
but the training time and runtime memory require-
ments might become prohibitive for the machine
translation task.
Because of the maximization over decomposition
structures, the score of a translation is not a simple
linear function of the features, but rather a maximum
over linear functions. The score of a translation for
a fixed decomposition is a linear function of the fea-
tures, but the score of a translation is a maximum of
linear functions (over decompositions). Therefore,
if we define hypotheses as just containing transla-
tions, MERT training does not work directly for op-
timizing the weights of the dynamic combination
method. 3 We used a combination of approaches;
we did MERT training followed by local simplex-
method search starting from three starting points:
the MERT solution, a weight vector that strongly
prefers left-to-right decompositions, and a weight-
vector that strongly prefers right-to-left decomposi-
tions. In the Experiments section, we report results
for the weights that achieved the best development
set performance.
</bodyText>
<sectionHeader confidence="0.993935" genericHeader="method">
5 N-best reranking
</sectionHeader>
<bodyText confidence="0.9999695">
To evaluate the impact of these models in a full MT
system, we investigate n-best reranking. We use a
phrase-based MT system to output 1000-best can-
didate translations. For each candidate translation,
we have access to the phrase pairs it used as well as
the alignments inside each phrase pair. Thus, each
source sentence and its candidate translation form a
word-aligned parallel sentence pair. We can extract
MTU sequences from this sentence pair and com-
pute its probability according to MTU Markov mod-
els. These MTU MM log-probabilities are appended
to the original MT features and used to rerank the
1000-best list. The weight vectors for systems using
the original features along with one or more MTU
Markov model log-probabilities are trained on the
development set using MERT.
</bodyText>
<sectionHeader confidence="0.999619" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999675">
We report experimental results on the lexical selec-
tion task and the reranking task on three language
pairs. The datasets used for the different languages
are described in detail in Section 6.2.
</bodyText>
<subsectionHeader confidence="0.99879">
6.1 Lexical selection experiments
</subsectionHeader>
<bodyText confidence="0.9993112">
The data used for the lexical selection experiments
consists of the training portion of the datasets used
for MT. These training sets are split into three sec-
tions: - , for training MTU Markov models
and extracting possible translations for each source
</bodyText>
<footnote confidence="0.653184">
3If we include the decompositions in the hypotheses we
could use MERT but then the n-best lists used for training might
not contain much variety in terms of translation options. This is
an interesting direction for future research.
</footnote>
<page confidence="0.994825">
17
</page>
<table confidence="0.9979987">
Model Chs-En Deu-En En-Bgr
Dev Test Dev Test Dev Test
Baseline 06.45 06.30 11.60 10.98 15.09 14.40
Oracle 69.79 70.78 72.28 75.39 85.15 84.32
L2RT 24.02 25.09 28.69 28.70 49.86 46.45
R2LT 23.79 24.91 30.14 30.14* 49.22 46.58
CycT 18.59 20.33 25.91 26.83 41.30 38.85
L2RS 25.81 27.89* 25.52 25.10 45.69 43.98
R2LS 26.48 27.96* 26.03 26.30 47.36 43.91
CycS 21.62 23.38 22.68 23.58 39.11 36.44
</table>
<tableCaption confidence="0.9849945">
Table 1: Lexical selection results for individual MTU
Markov models.
</tableCaption>
<bodyText confidence="0.995888114285714">
MTU, - for tuning combination weights for
systems using several MTU MMs, and - , for
final evaluation results. The possible translations for
each source MTU are defined as the most frequent
100 translations seen in - . The - sets
contain 200 sentence pairs each and the - sets
contains 1000 sentence pairs each. These develop-
ment and test sets consist of equally spaced sen-
tences taken from the full MT training sets.
We start by reporting BLEU scores of the six in-
dividual MTU MMs on the three language pairs in
Table 1. The baseline predicts the most frequent tar-
get MTU for each source MTU (unigram MM not
using context). The oracle looks at the correct trans-
lation and always chooses the correct target MTU if
it is in the vocabulary of available MTUs.
We can see that there is a large difference between
the baseline and oracle performance, underscoring
the importance of modeling context for accurate pre-
diction. The best decomposition order varies from
language to language: right-to-left in source order is
best for Chinese-English, right-to-left in target order
is best for German-English and left-to-right or right-
to-left in target order are best in English-Bulgarian.
We computed statistical significance tests, testing
the difference between the L2RT model (the stan-
dard in prior work) and models achieving higher test
set performance. The models that are significantly
better at significance α &lt; 0.01 are marked with a
star in the table. We used a paired bootstrap test with
10,000 trials (Koehn, 2004).
Next we evaluate the methods for combining de-
composition orders introduced in Sections 4.1 and
4.2. The results are reported in Table 2. The up-
per part of the table focuses on combining different
</bodyText>
<table confidence="0.997789888888889">
Model Chs-En Deu-En En-Bgr
Dev Test Dev Test Dev Test
Baseline-1 24.04 25.09 30.14 30.14 49.86 46.45
TgtProduct 25.27 25.84* 30.47 30.49 51.04 47.27*
TgtSysComb 24.49 25.27 30.20 30.15 50.46 46.31
TgtDynamic 24.07 25.10 30.60 30.41 49.99 46.52
Baseline-2 26.48 27.96 30.14 30.14 49.86 46.45
AllProduct 28.68 29.59* 31.54 31.36* 51.50 48.10*
AllSyscomb 27.02 28.30 30.20 30.17 50.90 46.53
</table>
<tableCaption confidence="0.947226">
Table 2: Lexical selection results for combinations of
MTU Markov models.
</tableCaption>
<bodyText confidence="0.999845333333333">
target-order decompositions. The lower part of the
table looks at combining all six decomposition or-
ders. The baseline for the target order combinations,
Baseline-1, is the best single target MTU Markov
model from Table 1. Baseline-2 in the lower part
of the table is the best individual model out of all
six. We can see that the product models TgtProduct
(a product of the three target-order MTU MMs) and
AllProduct (a product of all six MTU MMs) are con-
sistently best. The dynamic decomposition models
TgtDynamic achieve slight but not significant gains
over the baseline. The combination models that are
statistically significantly better than corresponding
baselines (α &lt; 0.01) are marked with a star.
Our takeaway from these experiments is that mul-
tiple decomposition orders are good, and thus taking
a product (which encourages agreement among the
models) is a good choice for this task. The dynamic
decomposition method shows some promise, but it
does not outperform the simpler product approach.
Perhaps a lager space of decompositions would
achieve better results, especially given a larger pa-
rameter set to trade off decompositions and better
tuning for those parameters.
</bodyText>
<subsectionHeader confidence="0.99986">
6.2 Datasets and reranking settings
</subsectionHeader>
<bodyText confidence="0.999320545454545">
For Chinese-English, the training corpus consists
of 1 million sentence pairs from the FBIS and
HongKong portions of the LDC data for the NIST
MT evaluation. We used the union of the NIST
2002 and 2003 test sets as the development set and
the NIST 2005 test set as our test set. The baseline
phrasal system uses a 5-gram language model with
modified Kneser-Ney smoothing (Kenser and Ney,
1995), trained on the Xinhua portion of the English
Gigaword corpus (238M English words).
For German-English we used the dataset from
</bodyText>
<page confidence="0.995872">
18
</page>
<table confidence="0.9967975">
Language Training Dev Test
Chs-En 1 Mln NIST02+03 NIST05
Deu-En 751 K WMT06dev WMT06test
En-Bgr 4 Mln 1,497 2,498
</table>
<tableCaption confidence="0.999684">
Table 3: Data sets for different language pairs.
</tableCaption>
<bodyText confidence="0.999714545454545">
the WMT 2006 shared task on machine translation
(Koehn and Monz, 2006). The parallel training set
contains approximately 751K sentences. We also
used the English monolingual data of around 1 mil-
lion sentences for language model training. The de-
velopment set contains 2000 sentences. The final
test set (the in-domain test set for the shared task)
also contains 2000 sentences. Two Kneser-Ney lan-
guage models were used as separate features: a 4-
gram LM trained on the parallel portion of the data,
and a 5-gram LM trained on the monolingual corpus.
For English-Bulgarian we used a dataset con-
taining sentences from several data sources: JRC-
Acquis (Steinberger et al., 2006), TAUS4, and web-
scraped data. The development set consists of 1,497
sentences, the English side from WMT 2009 news
test data, and the Bulgarian side a human translation
thereof. The test set comes from the same mixture of
sources as the training set. For this system we used
a single four-gram target language model trained on
the target side of the parallel corpus.
All systems used phrase tables with a maximum
length of seven words on either side and lexicalized
reordering models. For the Chinese-English sys-
tem we used GIZA++ alignments, and for the other
two we used alignments by an HMM model aug-
mented with word-based distortion (He, 2007). The
alignments were symmetrized and then combined
with the heuristics ”grow-diag-final-and”. 5 We tune
parameters using MERT (Och, 2003) with random
restarts (Moore and Quirk, 2008) on the develop-
ment set. Case-insensitive BLEU-4 is our evaluation
metric (Papineni et al., 2002).
</bodyText>
<table confidence="0.999332333333333">
Model 3-gram models 5-gram models
Dev Test Dev Test
Baseline 32.58 31.78 32.58 31.78
L2RT 33.05 32.78* 33.16 32.88*
R2LT 33.05 32.96* 33.16 32.81*
L2RS 32.90 33.00* 32.98 32.98*
R2LS 32.94 32.98* 33.09 32.96*
4 MMs 33.22 33.07* 33.37 33.00*
4 MMs phrs 32.58 31.78 32.58 31.78
</table>
<tableCaption confidence="0.984236">
Table 4: Reranking with 3-gram and 5-gram MTU trans-
lation models on Chinese-English. Starred results on the
test set indicate significantly better performance than the
baseline.
</tableCaption>
<subsectionHeader confidence="0.987771">
6.3 MT reranking experiments
</subsectionHeader>
<bodyText confidence="0.999985">
We first report detailed experiments on Chinese-
English, and then verify our main conclusions on the
other language pairs. Table 4 looks at the impact of
individual 3-gram and 5-gram MTU Markov models
and their combination. Amongst the decomposition
orders tested (L2RT, R2LT, L2RS, and R2LS), each
of the individual MTU MMs was able to achieve
significant improvement over the baseline, around 1
BLEU point.6 The results achieved by the individ-
ual models differ, and the combination of four direc-
tions is better than the best individual direction, but
the difference is not statistically significant.
We ran an additional experiment to test whether
MTU MMs make effective use of context across
phrase boundaries, or whether they simply pro-
vide better smoothed estimates of phrasal transla-
tion probabilities. The last row of the table reports
the results achieved by a combination of MTU MMs
that do not use context across the phrasal bound-
aries. Since an MTU MM limited to look only inside
phrases can provide improved smoothing compared
to whole phrase relative frequency counts, it is con-
ceivable it could provide a large improvement. How-
ever, there is no improvement in practice for this lan-
guage pair; the additional improvements from MTU
MMs stem from modeling cross-phrase context.
</bodyText>
<footnote confidence="0.999788">
4www.tausdata.org
5The combination heuristic was further refined to disallow
crossing one-to-many alignments, which would result in the ex-
traction of larger minimum translation units. We found that this
further refinement on the combination heuristic consistently im-
proved the BLEU scores by between 0.3 and 0.7.
6Here again we call a difference significant if the paired
bootstrap p-value is less than 0.01.
</footnote>
<page confidence="0.999145">
19
</page>
<bodyText confidence="0.999968466666667">
Table 5 shows the test set results of individ-
ual 3-gram MTU Markov models and the com-
bination of 3-gram and 5-gram models on the
English-Bulgarian and German-English datasets.
For English-Bulgarian all individual 3-gram Markov
models achieve significant improvements of close to
one point; their combination is better than the best
individual model (but not significantly). The indi-
vidual 5-gram models and their combination bring
much larger improvement, for a total increase of
2.82 points over the baseline. We believe the 5-
gram models were more effective in this setting be-
cause the larger training set allowed for successful
training of models of larger capacity. Also the in-
creased context size helps to resolve ambiguity in
the forms of morphologically-rich Bulgarian words.
For German-English we see a similar pattern, with
the combination of models outperforming the in-
dividual ones, and the 5-gram models being better
than the 3-gram. Here the individual 3-gram models
are better than the baseline at significance level 0.02
and their combination is better than the baseline at
our earlier defined threshold of 0.01. The within-
phrase MTU MMs (results shown in the last two
rows) improve upon the baseline slightly, but here
again the improvements mostly stem from the use of
context across phrase boundaries. Our final results
on German-English are better than the best result of
27.30 from the shared task (Koehn and Monz, 2006).
Thanks to the reviewers for referring us to re-
cent work by (Clark et al., 2011) that pointed out
problems with significance tests for machine trans-
lation, where the randomness and local optima in the
MERT weight tuning method lead to a large vari-
ance in development and test set performance across
different runs of optimization (using a different ran-
dom seed or starting point). (Clark et al., 2011) pro-
posed a stratified approximate randomization statis-
tical significance test, which controls for optimizer
instability. Using this test, for the English-Bulgarian
system, we confirmed that the combination of four
3-gram MMs and the combination of 5-gram MMs
is better than the baseline (p = .0001 for both, using
five runs of parameter tuning). We have not run the
test for the other language pairs.
</bodyText>
<table confidence="0.9980697">
Model En-Bgr Deu-En
Baseline 45.75 27.92
L2RT 3-gram 47.07* 28.15
R2LT 3-gram 47.06* 28.19
L2RS 3-gram 46.44* 28.15
R2LS 3-gram 47.04* 28.18
4 3-gram 47.17* 28.37*
4 5-gram 48.57* 28.47*
4 3-gram phrs 46.08 27.92
4 5-gram phrs 46.17* 27.93
</table>
<tableCaption confidence="0.940884">
Table 5: English-Bulgarian and German-English test set
results: reranking with MTU translation models.
</tableCaption>
<sectionHeader confidence="0.998379" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999988304347826">
We introduced models of Minimal Translation Units
for phrasal systems, and showed that they make a
substantial and statistically significant improvement
on three distinct language-pairs. Additionally we
studied the importance of decomposition order when
defining the probability of MTU sequences. In a
simplified lexical selection task, we saw that there
were large differences in performance among the
different decompositions, with the best decomposi-
tions differing by language. We investigated multi-
ple methods to combine decompositions and found
that a simple product approach was most effective.
Results in the lexical selection task were consistent
with those obtained in a full MT system, although
the differences among decompositions were smaller.
In future work, perhaps we would see larger gains
by including additional decomposition orders (e.g.,
top-down in a dependency tree), and taking this idea
deeper into the machine translation model, down to
the word-alignment and language-modeling levels.
We were surprised to find n-best reranking so ef-
fective. We are incorporating the models into first
pass decoding, in hopes of even greater gains.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999689125">
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proc. ACL-11.
JM Crego and F Yvon. 2010. Factored bilingual n-
gram language models for statistical machine transla-
tion. Machine Translation, Special Issue: Pushing the
frontiers of SMT, 24(2):159–175.
</reference>
<page confidence="0.902927">
20
</page>
<reference confidence="0.999575362318841">
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with inte-
grated reordering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1045–
1054, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Andrew Finch and Eiichiro Sumita. 2009. Bidirectional
phrase-based machine translation. In In proceedings
of EMNLP.
Xiaodong He. 2007. Using word-dependent transition
models in hmm based word alignment for statistical
machine translation. In WMT workshop.
Reinhard Kenser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Proc.
ICASSP 1995, pages 181–184.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between eu-
ropean languages. In Proceedings on the Workshop on
Statistical Machine Translation, pages 102–121, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003, pages 127–133.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In In Proceedings of
EMNLP.
JB Marino, RE Banchs, JM Crego, A de Gispert, P Lam-
bert, JA Fonollosa, and MR Costa-Jussa. 2006. N-
gram-based machine translation. Computational Lin-
guistics, 32(4):527–549.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error training for statistical ma-
chine translation. In Proc. Coling-08.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In In Proceedings of ACL,
pages 295–302.
Franz Joseph Och. 2003. Minimum error training in sta-
tistical machine translation. In Proc. ACL-03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. B : a method for automatic eval-
uation of machine translation. In Proc. 40th Annual
Meeting of the ACL, pages 311–318.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 9–16, New York City, USA,
June. Association for Computational Linguistics.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma Erjavec, Dan Tufis, and Dniel
Varga. 2006. The JRC-Acquis: A multilingual
aligned parallel corpus with 20+ languages. In LREC,
Genoa, Italy.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In In Pro-
ceedings of HLT-NAACL.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In In proceedings of
HLT/EMNLP.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 856–864,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.999438">
21
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978658">
<title confidence="0.99874">Beyond Left-to-Right: Multiple Decomposition Structures for SMT</title>
<author confidence="0.998122">Kristina Toutanova Chris Quirk Jianfeng Gao</author>
<affiliation confidence="0.999901">Microsoft Research Microsoft Research Microsoft Research</affiliation>
<address confidence="0.999421">Los Angeles, CA 90089 Redmond, WA 98502 Redmond, WA 98502 Redmond, WA 98502</address>
<email confidence="0.997742">hzhang@isi.edukristout@microsoft.comchrisq@microsoft.comjfgao@microsoft.com</email>
<abstract confidence="0.999240666666667">Standard phrase-based translation models do not explicitly model context dependence between translation units. As a result, they rely on large phrase pairs and target language modto recover contextual in translation. In this work, we explore n-gram models over Minimal Translation Units (MTUs) to explicitly capture contextual dependencies across phrase boundaries in the channel model. As there is no single best direction in which contextual information should flow, we explore multiple decomposition structures as well as dynamic bidirectional decomposition. The resulting models are evaluated in an intrinsic task of lexical selection for MT as well as a full MT system, through n-best reranking. These experiments demonstrate that additional contextual modeling does indeed benefit a phrase-based system and that the direction of conditioning is important. Integrating multiple conditioning orders provides consistent benefit, and the most important directions by language pair.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In</title>
<date>2011</date>
<booktitle>Proc. ACL-11.</booktitle>
<contexts>
<context position="34615" citStr="Clark et al., 2011" startWordPosition="5577" endWordPosition="5580">5-gram models being better than the 3-gram. Here the individual 3-gram models are better than the baseline at significance level 0.02 and their combination is better than the baseline at our earlier defined threshold of 0.01. The withinphrase MTU MMs (results shown in the last two rows) improve upon the baseline slightly, but here again the improvements mostly stem from the use of context across phrase boundaries. Our final results on German-English are better than the best result of 27.30 from the shared task (Koehn and Monz, 2006). Thanks to the reviewers for referring us to recent work by (Clark et al., 2011) that pointed out problems with significance tests for machine translation, where the randomness and local optima in the MERT weight tuning method lead to a large variance in development and test set performance across different runs of optimization (using a different random seed or starting point). (Clark et al., 2011) proposed a stratified approximate randomization statistical significance test, which controls for optimizer instability. Using this test, for the English-Bulgarian system, we confirmed that the combination of four 3-gram MMs and the combination of 5-gram MMs is better than the </context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proc. ACL-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JM Crego</author>
<author>F Yvon</author>
</authors>
<title>Factored bilingual ngram language models for statistical machine translation. Machine Translation, Special Issue: Pushing the frontiers of SMT,</title>
<date>2010</date>
<pages>24--2</pages>
<contexts>
<context position="2664" citStr="Crego and Yvon, 2010" startWordPosition="390" endWordPosition="393">dencies that span phrase boundaries, though it is not able to model information from the source side. Larger phrases capture more contextual dependencies within a phrase, but individual phrases are still translated almost independently. To address this limitation, several researchers have proposed bilingual n-gram Markov models (Marino et al., 2006) to capture contextual dependencies between phrase pairs. Much of their work is limited by the requirement “that the source and target side of a tuple of words are synchronized, i.e. that they occur in the same order in their respective languages” (Crego and Yvon, 2010). For language pairs with significant typological divergences, such as Chinese-English, it is quite difficult to extract a synchronized sequence of units; in the limit, the smallest synchronized unit may be the whole sentence. Other approaches explore incorporation into syntax-based MT systems or replacing the phrasal translation system altogether. We investigate the addition of MTUs to a phrasal translation system to improve modeling of context and to provide more robust estimation of long phrases. However, in a phrase-based system there is no single synchronized traversal order; instead, we </context>
<context position="4048" citStr="Crego and Yvon (2010)" startWordPosition="591" endWordPosition="594">atively we consider translating a particularly unambiguous unit in the middle of the sentence and building outwards from there. We investigate both consistent and dynamic decomposition orders in several language pairs, looking at distinct orders in isolation and combination. Proceedings of NAACL-HLT 2013, pages 12–21, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics 2 Related work Marino et al. (2006) proposed a translation model using a Markov model of bilingual n-grams, demonstrating state-of-the-art performance compared to conventional phrase-based models. Crego and Yvon (2010) further explored factorized n-gram approaches, though both models considered rather large n-grams; this paper focuses on small units with asynchronous orders in source and target. Durrani et al. (2011) developed a joint model that captures translation of contiguous and gapped units as well as reordering. Two prior approaches explored similar models in syntax based systems. MTUs have been used in dependency translation models (Quirk and Menezes, 2006) to augment syntax directed translation systems. Likewise in target language syntax systems, one can consider Markov models over minimal rules, w</context>
</contexts>
<marker>Crego, Yvon, 2010</marker>
<rawString>JM Crego and F Yvon. 2010. Factored bilingual ngram language models for statistical machine translation. Machine Translation, Special Issue: Pushing the frontiers of SMT, 24(2):159–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
</authors>
<title>A joint sequence translation model with integrated reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1045--1054</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="4250" citStr="Durrani et al. (2011)" startWordPosition="621" endWordPosition="624"> language pairs, looking at distinct orders in isolation and combination. Proceedings of NAACL-HLT 2013, pages 12–21, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics 2 Related work Marino et al. (2006) proposed a translation model using a Markov model of bilingual n-grams, demonstrating state-of-the-art performance compared to conventional phrase-based models. Crego and Yvon (2010) further explored factorized n-gram approaches, though both models considered rather large n-grams; this paper focuses on small units with asynchronous orders in source and target. Durrani et al. (2011) developed a joint model that captures translation of contiguous and gapped units as well as reordering. Two prior approaches explored similar models in syntax based systems. MTUs have been used in dependency translation models (Quirk and Menezes, 2006) to augment syntax directed translation systems. Likewise in target language syntax systems, one can consider Markov models over minimal rules, where the translation probability of each rule is adjusted to include context information from parent rules (Vaswani et al., 2011). Most prior work tends to replace the existing probabilities rather than</context>
</contexts>
<marker>Durrani, Schmid, Fraser, 2011</marker>
<rawString>Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrated reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1045– 1054, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Bidirectional phrase-based machine translation. In</title>
<date>2009</date>
<booktitle>In proceedings of EMNLP.</booktitle>
<contexts>
<context position="16955" citStr="Finch and Sumita, 2009" startWordPosition="2698" endWordPosition="2701">coder with the difference that it is not always the last two target MTUs that define the context needed by future extensions. 于 Yu ? ? ? 昨天 ZuoTian 举行 JuXing null ? ? 会谈 HuiTan 15 The weights A of different models are trained on a development set using MER training to maximize the BLEU score of the resulting model. Note that this method of model combination was not considered in any of the previous works comparing different decompositions. The system combination method is motivated by prior work in machine translation which combined left-to-right and right-to-left machine translation systems (Finch and Sumita, 2009). Similarly, we perform sentence-level system combination between systems using different MTU Markov models to come up with most likely translations. If we have k systems guessing hypotheses based on MM1, ... , MMk respectively, we generate 1000- best lists from each system, resulting in a pool of up to 1000k possible distinct translations. Each of the candidate hypotheses from MMi is scored with its Markov model log-probability logPMMi(h). We compute normalized probabilities for each system’s n-best by exponentiating and normalizing: Pi(h) a PMMi(h). If a hypothesis h is not in system i’s nbe</context>
</contexts>
<marker>Finch, Sumita, 2009</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2009. Bidirectional phrase-based machine translation. In In proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
</authors>
<title>Using word-dependent transition models in hmm based word alignment for statistical machine translation.</title>
<date>2007</date>
<booktitle>In WMT workshop.</booktitle>
<contexts>
<context position="30628" citStr="He, 2007" startWordPosition="4945" endWordPosition="4946">lopment set consists of 1,497 sentences, the English side from WMT 2009 news test data, and the Bulgarian side a human translation thereof. The test set comes from the same mixture of sources as the training set. For this system we used a single four-gram target language model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). Model 3-gram models 5-gram models Dev Test Dev Test Baseline 32.58 31.78 32.58 31.78 L2RT 33.05 32.78* 33.16 32.88* R2LT 33.05 32.96* 33.16 32.81* L2RS 32.90 33.00* 32.98 32.98* R2LS 32.94 32.98* 33.09 32.96* 4 MMs 33.22 33.07* 33.37 33.00* 4 MMs phrs 32.58 31.78 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram </context>
</contexts>
<marker>He, 2007</marker>
<rawString>Xiaodong He. 2007. Using word-dependent transition models in hmm based word alignment for statistical machine translation. In WMT workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kenser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proc. ICASSP</booktitle>
<pages>181--184</pages>
<contexts>
<context position="29013" citStr="Kenser and Ney, 1995" startWordPosition="4675" endWordPosition="4678">uct approach. Perhaps a lager space of decompositions would achieve better results, especially given a larger parameter set to trade off decompositions and better tuning for those parameters. 6.2 Datasets and reranking settings For Chinese-English, the training corpus consists of 1 million sentence pairs from the FBIS and HongKong portions of the LDC data for the NIST MT evaluation. We used the union of the NIST 2002 and 2003 test sets as the development set and the NIST 2005 test set as our test set. The baseline phrasal system uses a 5-gram language model with modified Kneser-Ney smoothing (Kenser and Ney, 1995), trained on the Xinhua portion of the English Gigaword corpus (238M English words). For German-English we used the dataset from 18 Language Training Dev Test Chs-En 1 Mln NIST02+03 NIST05 Deu-En 751 K WMT06dev WMT06test En-Bgr 4 Mln 1,497 2,498 Table 3: Data sets for different language pairs. the WMT 2006 shared task on machine translation (Koehn and Monz, 2006). The parallel training set contains approximately 751K sentences. We also used the English monolingual data of around 1 million sentences for language model training. The development set contains 2000 sentences. The final test set (th</context>
</contexts>
<marker>Kenser, Ney, 1995</marker>
<rawString>Reinhard Kenser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proc. ICASSP 1995, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between european languages.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<contexts>
<context position="29378" citStr="Koehn and Monz, 2006" startWordPosition="4735" endWordPosition="4738"> NIST MT evaluation. We used the union of the NIST 2002 and 2003 test sets as the development set and the NIST 2005 test set as our test set. The baseline phrasal system uses a 5-gram language model with modified Kneser-Ney smoothing (Kenser and Ney, 1995), trained on the Xinhua portion of the English Gigaword corpus (238M English words). For German-English we used the dataset from 18 Language Training Dev Test Chs-En 1 Mln NIST02+03 NIST05 Deu-En 751 K WMT06dev WMT06test En-Bgr 4 Mln 1,497 2,498 Table 3: Data sets for different language pairs. the WMT 2006 shared task on machine translation (Koehn and Monz, 2006). The parallel training set contains approximately 751K sentences. We also used the English monolingual data of around 1 million sentences for language model training. The development set contains 2000 sentences. The final test set (the in-domain test set for the shared task) also contains 2000 sentences. Two Kneser-Ney language models were used as separate features: a 4- gram LM trained on the parallel portion of the data, and a 5-gram LM trained on the monolingual corpus. For English-Bulgarian we used a dataset containing sentences from several data sources: JRCAcquis (Steinberger et al., 20</context>
<context position="34534" citStr="Koehn and Monz, 2006" startWordPosition="5561" endWordPosition="5564">pattern, with the combination of models outperforming the individual ones, and the 5-gram models being better than the 3-gram. Here the individual 3-gram models are better than the baseline at significance level 0.02 and their combination is better than the baseline at our earlier defined threshold of 0.01. The withinphrase MTU MMs (results shown in the last two rows) improve upon the baseline slightly, but here again the improvements mostly stem from the use of context across phrase boundaries. Our final results on German-English are better than the best result of 27.30 from the shared task (Koehn and Monz, 2006). Thanks to the reviewers for referring us to recent work by (Clark et al., 2011) that pointed out problems with significance tests for machine translation, where the randomness and local optima in the MERT weight tuning method lead to a large variance in development and test set performance across different runs of optimization (using a different random seed or starting point). (Clark et al., 2011) proposed a stratified approximate randomization statistical significance test, which controls for optimizer instability. Using this test, for the English-Bulgarian system, we confirmed that the com</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proceedings on the Workshop on Statistical Machine Translation, pages 102–121, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1457" citStr="Koehn et al., 2003" startWordPosition="203" endWordPosition="206">e multiple decomposition structures as well as dynamic bidirectional decomposition. The resulting models are evaluated in an intrinsic task of lexical selection for MT as well as a full MT system, through n-best reranking. These experiments demonstrate that additional contextual modeling does indeed benefit a phrase-based system and that the direction of conditioning is important. Integrating multiple conditioning orders provides consistent benefit, and the most important directions differ by language pair. 1 Introduction The translation procedure of a classical phrasebased translation model (Koehn et al., 2003) first divides the input sentence into a sequence of phrases, translates each phrase, explores reorderings of these translations, and then scores the resulting candidates with a linear combination of models. Conventional models include phrase-based channel models that effectively model each phrase as a large unigram, reordering models, and target language models. Of these models, only the target language model This research was conducted during the author’s internship at Microsoft Research 12 (and, to some weak extent, the lexicalized reordering model) captures some lexical dependencies that s</context>
<context position="5541" citStr="Koehn et al., 2003" startWordPosition="818" endWordPosition="821">nal but are not a replacement. Their distributions should be more informative than the so-called “lexical weighting” models, and less sparse than relative frequency estimates, though potentially not as effective for truly non-compositional units. Therefore, we explore the inclusion of all such information. Also, unlike prior work, we explore combinations of multiple decomposition orders, as well as dynamic decompositions. The most useful context for translation differs by language pair, an important finding when working with many language pairs. We build upon a standard phrase-based approach (Koehn et al., 2003). This acts as a proposal distribution for translations; the MTU Markov models provide additional signal as to which translations are correct. 3 MTU n-gram Markov models We begin by defining Minimal Translation Units (MTUs) and describing how to identify them in word-aligned text. Next we define n-gram Markov models over MTUs, which requires us to define traversal orders over MTUs. Figure 1: Word alignment and minimum translation units. 3.1 Definition of an MTU Informally, the notion of a minimal translation unit is simple: it is a translation rule that cannot be ? ? ? ? ? broken down any furt</context>
<context position="7029" citStr="Koehn et al., 2003" startWordPosition="1089" endWordPosition="1092">hrases. Conventional phrase pairs may be viewed as compositions of these MTUs up to a given size limit. Consider a word-aligned sentence pair consisting of a sequence of source words s = s1 ... sm, a sequence of target words t = t1 ... tn, and a word alignment relation between the source and target words — c {1..ml x {1..nl. A translation unit is a sequence of source words si..sj and a sequence of target words tk..tl (one of which may be empty) such that for all aligned pairs i&apos; — k&apos;, we have i &lt;_ i&apos; &lt;_ j if and only if k &lt;_ k&apos; &lt;_ l. This definition, nearly identical to that of a phrase pair (Koehn et al., 2003), relaxes the constraint that one aligned word must be present. A set of translation units is a partition of the sentence pair if each source and target word is covered exactly once. Minimal translation units is the partition with the smallest average unit size, or, equivalently, the largest number of units. For example, Figure 1 shows a word-aligned sentence pair and its corresponding set of MTUs. We extract these minimal translation units with an algorithm similar to that of phrase extraction. We train n-gram Markov models only over minM1 M2 M3 M4 M5 held the meeting null yesterday null 会谈 H</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. HLT-NAACL 2003, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation. In</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="26742" citStr="Koehn, 2004" startWordPosition="4311" endWordPosition="4312">ate prediction. The best decomposition order varies from language to language: right-to-left in source order is best for Chinese-English, right-to-left in target order is best for German-English and left-to-right or rightto-left in target order are best in English-Bulgarian. We computed statistical significance tests, testing the difference between the L2RT model (the standard in prior work) and models achieving higher test set performance. The models that are significantly better at significance α &lt; 0.01 are marked with a star in the table. We used a paired bootstrap test with 10,000 trials (Koehn, 2004). Next we evaluate the methods for combining decomposition orders introduced in Sections 4.1 and 4.2. The results are reported in Table 2. The upper part of the table focuses on combining different Model Chs-En Deu-En En-Bgr Dev Test Dev Test Dev Test Baseline-1 24.04 25.09 30.14 30.14 49.86 46.45 TgtProduct 25.27 25.84* 30.47 30.49 51.04 47.27* TgtSysComb 24.49 25.27 30.20 30.15 50.46 46.31 TgtDynamic 24.07 25.10 30.60 30.41 49.99 46.52 Baseline-2 26.48 27.96 30.14 30.14 49.86 46.45 AllProduct 28.68 29.59* 31.54 31.36* 51.50 48.10* AllSyscomb 27.02 28.30 30.20 30.17 50.90 46.53 Table 2: Lexic</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JB Marino</author>
<author>RE Banchs</author>
<author>JM Crego</author>
<author>A de Gispert</author>
<author>P Lambert</author>
<author>JA Fonollosa</author>
<author>MR Costa-Jussa</author>
</authors>
<title>Ngram-based machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Marino, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-Jussa, 2006</marker>
<rawString>JB Marino, RE Banchs, JM Crego, A de Gispert, P Lambert, JA Fonollosa, and MR Costa-Jussa. 2006. Ngram-based machine translation. Computational Linguistics, 32(4):527–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>Random restarts in minimum error training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. Coling-08.</booktitle>
<contexts>
<context position="30811" citStr="Moore and Quirk, 2008" startWordPosition="4969" endWordPosition="4972">me mixture of sources as the training set. For this system we used a single four-gram target language model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). Model 3-gram models 5-gram models Dev Test Dev Test Baseline 32.58 31.78 32.58 31.78 L2RT 33.05 32.78* 33.16 32.88* R2LT 33.05 32.96* 33.16 32.81* L2RS 32.90 33.00* 32.98 32.98* R2LS 32.94 32.98* 33.09 32.96* 4 MMs 33.22 33.07* 33.37 33.00* 4 MMs phrs 32.58 31.78 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starred results on the test set indicate significantly better performance than the baseline. 6.3 MT reranking experiments We first report de</context>
</contexts>
<marker>Moore, Quirk, 2008</marker>
<rawString>Robert C. Moore and Chris Quirk. 2008. Random restarts in minimum error training for statistical machine translation. In Proc. Coling-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation. In</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="15980" citStr="Och and Ney, 2002" startWordPosition="2533" endWordPosition="2536">approximations to the trigram MTU Markov model scores as future scores, since not all needed context is available for a hypothesis at the time of construction. As additional context becomes available, the exact score can be computed. 2 4.1 Basic decomposition order combinations We first introduce two methods of combining different decomposition orders: product and system combination. The product method arises naturally in the machine translation setting, where probabilities from different models are multiplied together and further weighted to form the log-linear model for machine translation (Och and Ney, 2002). We define a similar scoring function using a set of MTU Markov models MM1, ..., MMk for a hypothesis h as follows: Score(h) _ A1logPMM1(h) + ... + AklogPMMk(h) 2We apply hypothesis recombination, which can merge hypotheses that are indistinguishable with respect to future continuations. This is similar to recombination in a standard-phrase based decoder with the difference that it is not always the last two target MTUs that define the context needed by future extensions. 于 Yu ? ? ? 昨天 ZuoTian 举行 JuXing null ? ? 会谈 HuiTan 15 The weights A of different models are trained on a development set u</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In In Proceedings of ACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Minimum error training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL-03.</booktitle>
<contexts>
<context position="30766" citStr="Och, 2003" startWordPosition="4964" endWordPosition="4965">f. The test set comes from the same mixture of sources as the training set. For this system we used a single four-gram target language model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). Model 3-gram models 5-gram models Dev Test Dev Test Baseline 32.58 31.78 32.58 31.78 L2RT 33.05 32.78* 33.16 32.88* R2LT 33.05 32.96* 33.16 32.81* L2RS 32.90 33.00* 32.98 32.98* R2LS 32.94 32.98* 33.09 32.96* 4 MMs 33.22 33.07* 33.37 33.00* 4 MMs phrs 32.58 31.78 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starred results on the test set indicate significantly better performance than the baseline. 6.</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Joseph Och. 2003. Minimum error training in statistical machine translation. In Proc. ACL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>B : a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. 40th Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="30908" citStr="Papineni et al., 2002" startWordPosition="4984" endWordPosition="4987">uage model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). Model 3-gram models 5-gram models Dev Test Dev Test Baseline 32.58 31.78 32.58 31.78 L2RT 33.05 32.78* 33.16 32.88* R2LT 33.05 32.96* 33.16 32.81* L2RS 32.90 33.00* 32.98 32.98* R2LS 32.94 32.98* 33.09 32.96* 4 MMs 33.22 33.07* 33.37 33.00* 4 MMs phrs 32.58 31.78 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starred results on the test set indicate significantly better performance than the baseline. 6.3 MT reranking experiments We first report detailed experiments on ChineseEnglish, and then verify our main conclusions on the other language </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. B : a method for automatic evaluation of machine translation. In Proc. 40th Annual Meeting of the ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
</authors>
<title>Do we need phrases? challenging the conventional wisdom in statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="4503" citStr="Quirk and Menezes, 2006" startWordPosition="659" endWordPosition="662">anslation model using a Markov model of bilingual n-grams, demonstrating state-of-the-art performance compared to conventional phrase-based models. Crego and Yvon (2010) further explored factorized n-gram approaches, though both models considered rather large n-grams; this paper focuses on small units with asynchronous orders in source and target. Durrani et al. (2011) developed a joint model that captures translation of contiguous and gapped units as well as reordering. Two prior approaches explored similar models in syntax based systems. MTUs have been used in dependency translation models (Quirk and Menezes, 2006) to augment syntax directed translation systems. Likewise in target language syntax systems, one can consider Markov models over minimal rules, where the translation probability of each rule is adjusted to include context information from parent rules (Vaswani et al., 2011). Most prior work tends to replace the existing probabilities rather than augmenting them. We believe that Markov rules provide an additional signal but are not a replacement. Their distributions should be more informative than the so-called “lexical weighting” models, and less sparse than relative frequency estimates, thoug</context>
</contexts>
<marker>Quirk, Menezes, 2006</marker>
<rawString>Chris Quirk and Arul Menezes. 2006. Do we need phrases? challenging the conventional wisdom in statistical machine translation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 9–16, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
</authors>
<title>Bruno Pouliquen, Anna Widiger, Camelia Ignat, Toma Erjavec, Dan Tufis, and Dniel Varga.</title>
<date>2006</date>
<location>Genoa, Italy.</location>
<marker>Steinberger, 2006</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Toma Erjavec, Dan Tufis, and Dniel Varga. 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In LREC, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="11657" citStr="Toutanova et al., 2003" startWordPosition="1833" endWordPosition="1836">text for disambiguation and it is not clear apriori which would perform best. We compare all four decomposition orders (source order left-to-right and right-to-left, and target order left-to-right and rightto-left). Although the independence assumptions of left-to-right and right-to-left are the same, the resulting models may be different due to smoothing. In addition to studying these four basic decomposition orders, we report performance of two cyclic orders: cyclic in source or target sentence order. These models are inspired by the cyclic dependency network model proposed for POS tagging (Toutanova et al., 2003) and also used as a baseline in previous work on dynamic decomposition orders (Tsuruoka and Tsujii, 2005). 1 The probability according to the cyclic orders is defined by conditioning each MTU on both its left and right neighbor MTUs. For example, the probability of the sentence pair in Figure 1 under the source cyclic order, using a 3-gram model is defined as: P(M1|M2) · P(M2|M1, M3) · P(M3|M2, M4) · P(M4|M3, M5) · P(M5|M4). All n-gram Markov models over MTUs are esti1The correct application of such models requires sampling to find the highest scoring sequence, but we apply the max product app</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data. In</title>
<date>2005</date>
<booktitle>In proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="9058" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="1422" endWordPosition="1425">unigrams to capture contextual information, n-grams of minimal translation units allow a robust contextual model that is less constrained by segmentation. 3.2 MTU enumeration orders When defining a joint probability distribution over MTUs of an aligned sentence pair, it is necessary to define a decomposition, or generation order for the sentence pair. For a single sequence in language modeling or synchronized sequences in channel modeling, the default enumeration order has been left-to-right. Different decomposition orders have been used in part-of-speech tagging and named entity recognition (Tsuruoka and Tsujii, 2005). Intuitively, information from the left or right could be more useful for particular disambiguation choices. Our research on different decomposition orders was motivated by this work. When applying such ideas to machine translation, there are additional challenges and opportunities. The task exhibits much more ambiguity – the number of possible MTUs is in the millions. An opportunity arises from the reordering phenomenon in machine translation: while in POS tagging the natural decomposition orders to study are only left-to-right and right-to-left, in machine translation we can further disting</context>
<context position="11762" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="1850" endWordPosition="1853">mposition orders (source order left-to-right and right-to-left, and target order left-to-right and rightto-left). Although the independence assumptions of left-to-right and right-to-left are the same, the resulting models may be different due to smoothing. In addition to studying these four basic decomposition orders, we report performance of two cyclic orders: cyclic in source or target sentence order. These models are inspired by the cyclic dependency network model proposed for POS tagging (Toutanova et al., 2003) and also used as a baseline in previous work on dynamic decomposition orders (Tsuruoka and Tsujii, 2005). 1 The probability according to the cyclic orders is defined by conditioning each MTU on both its left and right neighbor MTUs. For example, the probability of the sentence pair in Figure 1 under the source cyclic order, using a 3-gram model is defined as: P(M1|M2) · P(M2|M1, M3) · P(M3|M2, M4) · P(M4|M3, M5) · P(M5|M4). All n-gram Markov models over MTUs are esti1The correct application of such models requires sampling to find the highest scoring sequence, but we apply the max product approximation as done in previous work. 14 mated using Kneser-Ney smoothing. Each MTU is treated as an atomi</context>
<context position="18371" citStr="Tsuruoka and Tsujii (2005)" startWordPosition="2923" endWordPosition="2926">The weights A for the combination are tuned using MERT as for the product model. 4.2 Dynamic decomposition orders A more complex combination method chooses the best possible decomposition order for each translation dynamically, using a set of constraints to define the possible decomposition orders, and a set of features to score the candidate decompositions. We term this method dynamic combination. The score of each translation is defined as its score according to the highest-scoring decomposition order for that translation. This method is very similar to the bidirectional tagging approach of Tsuruoka and Tsujii (2005). For this approach we only explored combinations of target language orders (L2RT, CycT, and R2LT). If source language orders were included, the complexity of decoding would increase substantially. Figure 3 shows two possible decompositions for a short MTU sequence. The structures displayed are Figure 3: Different decompositions. directed graphical models. They define the set of parents (context) used to predict each target MTU. The decomposition structures we consider are limited to acyclic graphs where each node can have one of the following parent configurations: no parents (C = 0 in the Fi</context>
<context position="21127" citStr="Tsuruoka and Tsujii (2005)" startWordPosition="3399" endWordPosition="3402">to assignments to target MTUs. The final score of a translation and decomposition is a linear combination of the two types of feature values – model log-probabilities and configuration types. There is one feature weight for each parent configuration (six configuration weights) and one feature weight for each component model (three model weights). The final score of the second decomposition and assignment in Figure 3 is: Score(h) = 2 * wC0 + wCLR + wC1R + wL2Rlo9PLR(m1) + wCyclo9PCyc(m2|m1, m3) + wR2Llo9PRL(m3|m4) + wL2Rlo9PLR(m4) There are two main differences between our approach and that of Tsuruoka and Tsujii (2005): we perform beam search with hypothesis recombination instead of exact decoding (due to the larger size of the hypothesis set), and we use parameters to be able to globally weight the probabilities from different models and to develop preferences for using certain types of decompositions. For example, the model can learn to prefer right-to-left decompositions for one language pair, and left-to-right decompositions for another. An additional difference from prior work is the definition of the possible decomposition orders that are searched over. Compared to the structures allowed in (Tsuruoka </context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In In proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Rule markov models for fast tree-tostring translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>856--864</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="4777" citStr="Vaswani et al., 2011" startWordPosition="701" endWordPosition="704">er focuses on small units with asynchronous orders in source and target. Durrani et al. (2011) developed a joint model that captures translation of contiguous and gapped units as well as reordering. Two prior approaches explored similar models in syntax based systems. MTUs have been used in dependency translation models (Quirk and Menezes, 2006) to augment syntax directed translation systems. Likewise in target language syntax systems, one can consider Markov models over minimal rules, where the translation probability of each rule is adjusted to include context information from parent rules (Vaswani et al., 2011). Most prior work tends to replace the existing probabilities rather than augmenting them. We believe that Markov rules provide an additional signal but are not a replacement. Their distributions should be more informative than the so-called “lexical weighting” models, and less sparse than relative frequency estimates, though potentially not as effective for truly non-compositional units. Therefore, we explore the inclusion of all such information. Also, unlike prior work, we explore combinations of multiple decomposition orders, as well as dynamic decompositions. The most useful context for t</context>
</contexts>
<marker>Vaswani, Mi, Huang, Chiang, 2011</marker>
<rawString>Ashish Vaswani, Haitao Mi, Liang Huang, and David Chiang. 2011. Rule markov models for fast tree-tostring translation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 856–864, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>