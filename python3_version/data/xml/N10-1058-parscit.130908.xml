<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001624">
<title confidence="0.658326">
Improving Semantic Role Classification with Selectional Preferences
Be˜nat Zapirain, Eneko Agirre Lluis M`arquez Mihai Surdeanu
</title>
<note confidence="0.5624805">
IXA NLP Group TALP Research Center Stanford NLP Group
Basque Country Univ. Technical Univ. of Catalonia Stanford Univ.
</note>
<email confidence="0.994701">
{benat.zapirain,e.agirre}@ehu.es lluism@lsi.upc.edu mihais@stanford.edu
</email>
<sectionHeader confidence="0.993793" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999199533333333">
This work incorporates Selectional Prefer-
ences (SP) into a Semantic Role (SR) Clas-
sification system. We learn separate selec-
tional preferences for noun phrases and prepo-
sitional phrases and we integrate them in a
state-of-the-art SR classification system both
in the form of features and individual class
predictors. We show that the inclusion of the
refined SPs yields statistically significant im-
provements on both in domain and out of do-
main data (14.07% and 11.67% error reduc-
tion, respectively). The key factor for success
is the combination of several SP methods with
the original classification model using meta-
classification.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999538702702703">
Semantic Role Labeling (SRL) is the process of
extracting simple event structures, i.e., “who” did
“what” to “whom”, “when” and “where”. Current
systems usually perform SRL in two pipelined steps:
argument identification and argument classification.
While identification is mostly syntactic, classifica-
tion requires semantic knowledge to be taken into
account. Semantic information is usually captured
through lexicalized features on the predicate and the
head–word of the argument to be classified. Since
lexical features tend to be sparse, SRL systems are
prone to overfit the training data and generalize
poorly to new corpora.
Indeed, the SRL evaluation exercises at CoNLL-
2004 and 2005 (Carreras and M`arquez, 2005) ob-
served that all systems showed a significant perfor-
mance degradation (-10 F1 points) when applied to
test data from a different genre of that of the training
set. Pradhan et al. (2008) showed that this perfor-
mance degradation is essentially caused by the argu-
ment classification subtask, and suggested the lexi-
cal data sparseness as one of the main reasons. The
same authors studied the contribution of the different
feature types in SRL and concluded that the lexical
features were the most salient features in argument
classification (Pradhan et al., 2007).
In recent work, we showed (Zapirain et al., 2009)
how automatically generated selectional preferences
(SP) for verbs were able to perform better than pure
lexical features in a role classification experiment,
disconnected from a full-fledged SRL system. SPs
introduce semantic generalizations on the type of ar-
guments preferred by the predicates and, thus, they
are expected to improve results on infrequent and
unknown words. The positive effect was especially
relevant for out-of-domain data. In this paper we ad-
vance (Zapirain et al., 2009) in two directions:
</bodyText>
<listItem confidence="0.9016792">
(1) We learn separate SPs for prepositions and verbs,
showing improvement over using SPs for verbs
alone.
(2) We integrate the information of several SP mod-
els in a state-of-the-art SRL system (SwiRL1) and
</listItem>
<bodyText confidence="0.88507625">
show significant improvements in SR classifica-
tion. The key for the improvement lies in a meta-
classifier, trained to select among the predictions
provided by several role classification models.
</bodyText>
<sectionHeader confidence="0.852694" genericHeader="method">
2 SPs for SR Classification
</sectionHeader>
<bodyText confidence="0.6624284">
SPs have been widely believed to be an impor-
tant knowledge source when parsing and perform-
ing SRL, especially role classification. Still, present
parsers and SRL systems use just lexical features,
which can be seen as the most simple form of SP,
</bodyText>
<footnote confidence="0.994986">
1http://www.surdeanu.name/mihai/swirl/
</footnote>
<page confidence="0.968101">
373
</page>
<subsubsectionHeader confidence="0.568403">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373–376,
</subsubsectionHeader>
<subsectionHeader confidence="0.273469">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999967303571429">
where the headword needs to be seen in the training
data, and otherwise the SP is not satisfied. Gildea
and Jurafsky (2002) showed barely significant im-
provements in semantic role classification of NPs
for FrameNet roles using distributional clusters. In
(Erk, 2007) a number of SP models are tested in
a pseudo-task related to SRL. More recently, we
showed (Zapirain et al., 2009) that several methods
to automatically generate SPs generalize well and
outperform lexical match in a large dataset for se-
mantic role classification, but the impact on a full
system was not explored.
In this work we apply a subset of the SP meth-
ods proposed in (Zapirain et al., 2009). These meth-
ods can be split in two main families, depending on
the resource used to compute similarity: WordNet-
based methods and distributional methods. Both
families define a similarity score between a word
(the headword of the argument to be classified) and a
set of words (the headwords of arguments of a given
role).
WordNet-based similarity: One of the models
that we used is based on Resnik’s similarity mea-
sure (1993), referring to it as res. The other model is
an in-house method (Zapirain et al., 2009), referred
as wn, which only takes into account the depth of
the most common ancestor, and returns SPs that are
as specific as possible.
Distributional similarity: Following (Zapirain et
al., 2009) we considered both first order and second
order similarity. In first order similarity, the simi-
larity of two words was computed using the cosine
(or Jaccard measure) of the co-occurrence vectors of
the two words. Co-occurrence vectors where con-
structed using freely available software (Pad´o and
Lapata, 2007) run over the British National Corpus.
We used the optimal parameters (Pad´o and Lapata,
2007, p. 179). We will refer to these similarities as
simcos and simJac, respectively. In contrast, sec-
ond order similarity uses vectors of similar words,
i.e., the similarity of two words was computed us-
ing the cosine (or Jaccard measure) between the
thesaurus entries of those words in Lin’s thesaurus
(Lin, 1998). We refer to these as sim�os and sim�ac.
Given a target sentence with a verb and its argu-
ments, the task of SR classification is to assign the
correct role to each of the arguments. When using
SPs alone, we only use the headwords of the ar-
guments, and each argument is classified indepen-
dently of the rest. For each headword, we select the
role (r) of the verb (c) which fits best the head word
(w), where the goodness of fit (SPsim(v, r, w)) is
modeled using one of the similarity models above,
between the headword w and the headwords seen in
training data for role r of verb v. This selection rule
is formalized as follows:
</bodyText>
<equation confidence="0.9660115">
Rsim(v, w) = arg max
rERoles(v)
</equation>
<bodyText confidence="0.999988411764706">
In our previous work (Zapirain et al., 2009), we
modelled SPs for pairs of predicates (verbs) and ar-
guments, independently of the fact that the argu-
ment is a core argument (typically a noun) or an
adjunct argument (typically a prepositional phrase).
In contrast, (Litkowski and Hargraves, 2005) show
that prepositions have SPs of their own, especially
when functioning as adjuncts. We therefore decided
to split SPs according to whether the potential argu-
ment is a Prepositional Phrase (PP) or a Noun Phrase
(NP). For NPs, which tend to be core arguments2,
we use the SPs of the verb (as formalized above).
For PPs, which have an even distribution between
core and adjunct arguments, we use the SPs of the
prepositions alone, ignoring the verbs. Implementa-
tion wise, this means that in Eq. (1), we change v
for p, where p is the preposition heading the PP.
</bodyText>
<sectionHeader confidence="0.979052" genericHeader="method">
3 Experiments with SPs in isolation
</sectionHeader>
<bodyText confidence="0.999561588235294">
In this section we evaluate the use of SPs for classi-
fication in isolation, i.e., we use formula 1, and no
other information. In addition we contrast the use
of both verb-role and preposition-role SPs, as com-
pared to the use of verb-role SPs alone.
The dataset used in these experiments (and in Sec-
tion 4) is the same as provided by the CoNLL-2005
shared task on SRL (Carreras and M`arquez, 2005).
This dataset comprises several sections of the Prop-
Bank corpus (news from the WSJ) as well as an ex-
tract of the Brown Corpus. Sections 02-21 are used
for generating the SPs and training, Section 00 for
development, and Section 23 for testing, as custom-
ary. The Brown Corpus is used for out-of-domain
testing, but due to the limited size of the provided
section, we extended it with instances from Sem-
Link3. Since the focus of this work is on argument
</bodyText>
<footnote confidence="0.99887">
2In our training data, NPs are adjuncts only 5% of the times
3http://verbs.colorado.edu/semlink/
</footnote>
<equation confidence="0.651024">
SPsim(v, r, w) (1)
</equation>
<page confidence="0.75878">
374
</page>
<table confidence="0.999755666666667">
Verb-Role SPs Preposition-Role and Verb-Role SPs
WSJ-test Brown WSJ-test Brown
prec. rec. Fl prec. rec. Fl prec. rec. Fl prec. rec. Fl
lexical 70.75 26.66 39.43 59.39 05.51 10.08 82.98 43.77 57.31 68.47 13.60 22.69
SP,.es 45.07 37.11 40.71 36.34 27.58 31.33 63.47 53.24 57.91 55.12 44.15 49.03
SP,,,,, 55.44 45.58 50.03 41.76 31.58 35.96 65.70 63.88 64.78 60.08 48.10 53.43
SPsimJac 48.85 46.38 47.58 42.10 34.34 37.82 61.83 61.40 61.61 55.42 53.45 54.42
SPsimcos 53.13 50.44 51.75 43.24 35.27 38.85 64.67 64.22 64.44 56.56 54.54 55.53
SPim2 61.76 58.63 60.16 51.97 42.39 46.69 70.82 70.33 70.57 62.37 60.15 61.24
s
Jac
SPsim2 cos 61.12 58.12 59.63 51.92 42.35 46.65 70.28 69.80 70.04 62.36 60.14 61.23
</table>
<tableCaption confidence="0.8418728">
Table 1: Results for SPs in isolation, left for verb SPs, and right both preposition and verb SPs.
Labels proposed by the base models
Number of base models that proposed this datum’s label
List of actual base models that proposed this datum’s label
Table 2: Features of the binary meta-classifier.
</tableCaption>
<bodyText confidence="0.999953142857143">
classification, we use the gold PropBank data to
identify argument boundaries. Considering that SPs
can handle only nominal arguments, in these exper-
iments we used only arguments mapped to NPs and
PPs containing a nominal head. From the training
sections, we extracted over 140K such arguments for
the supervised generation of SPs. The development
and test sections contain over 5K and 8K examples,
respectively, and the portion of the Brown Corpus
comprises an amount of 8.1K examples.
Table 1 lists the results of the different SPs in iso-
lation. The results reported in the left part of Table
1 are comparable to those we reported in (Zapirain
et al., 2009). The differences are due to the fact that
we do not discard roles like MOD, DIS, NEG and
that our previous work used only the subset of the
data that could be mapped to VerbNet (around 50%).
All in all, the table shows that splitting SPs into verb
and preposition SPs yields better results, both in pre-
cision and recall, improving Fl up to 10 points in
some cases.
</bodyText>
<sectionHeader confidence="0.94853" genericHeader="method">
4 Integrating SPs in a SRL system
</sectionHeader>
<bodyText confidence="0.998818608695652">
For these experiments we modified SwiRL (Sur-
deanu et al., 2007): (a) we matched the gold bound-
aries against syntactic constituents predicted inter-
nally using the Charniak parser (Charniak, 2000);
and (b) we classified these constituents with their
semantic role using a modified version of SwiRL’s
feature set.
We explored two different strategies for integrat-
ing SPs in SwiRL. The first, obvious method is to
extend SwiRL’s feature set with features that model
the preferences of the SPs, i.e., for each SP model
SPi we add a feature whose value is Ri. The second
method combines SwiRL’s classification model and
our SP models using meta-classification. We opted
for a binary classification approach: first, for each
constituent we generate n datums, one for each dis-
tinct role label proposed by the pool of base models;
then we use a binary meta-classifier to label each
candidate role as correct or incorrect. Table 2 lists
the features of the meta-classifier. We trained the
meta-classifier on the usual PropBank training par-
tition, using cross-validation to generate outputs for
the base models that require the same training ma-
terial. At prediction time, for each candidate con-
stituent we selected the role label that was classified
as correct with the highest confidence.
Table 3 compares the performance of both
combination approaches against the standalone
SwiRL classifier. We show results for both core
arguments (Core), adjunct arguments (Arg) and
all arguments combined (All). In the table, the
SwiRL+SP* models stand for SwiRL classifiers
enhanced with one feature from the correspond-
ing SP. Adding more than one SP-based feature to
SwiRL did not improve results. Our conjecture
is that the SwiRL classifier enhanced with SP-
based features does not learn relevant weights for
these features because their signal is “drowned” by
SwiRL’s large initial feature set and the correlation
between the different SPs. This observation moti-
vated the development of the meta-classifier. The
meta-classifier shown in the table combines the out-
put of the SwiRL+SP* models with the predictions
of SP models used in isolation. We implemented
the meta-classifier using Support Vector Machines
(SVM)4 with a quadratic polynomial kernel, and
</bodyText>
<footnote confidence="0.995095">
4http://svmlight.joachims.org
</footnote>
<page confidence="0.992774">
375
</page>
<table confidence="0.999536545454545">
WSJ-test Brown
Core Adj All Core Adj All
SwiRL 93.25 81.31 90.83 84.42 57.76 79.52
+SPRes 93.17 81.08 90.76 84.52 59.24 79.86
+SP,,,,, 92.88 81.11 90.56 84.26 59.69 79.73
+SPsim,_ 93.37 80.30 90.86 84.43 59.54 79.83
+SPsim_ 93.33 80.92 90.87 85.14 60.16 80.50
+SPsim2 93.03 82.75 90.95 85.62 59.63 80.75
Jac
+SPsim2 cos 93.78 80.56 91.23 84.95 61.01 80.48
Meta 94.37 83.40 92.12 86.20 63.40 81.91
</table>
<tableCaption confidence="0.9672765">
Table 3: Classification accuracy for the combination ap-
proaches. +SP., stands for SwiRL plus each SP model.
</tableCaption>
<bodyText confidence="0.96855284375">
C = 0.01 (tuned in development).
Table 3 indicates that four out of the six
SwiRL+SP* models perform better than SwiRL in
domain (WSJ-test), and all of them outperform
SwiRL out of domain (Brown). However, the im-
provements are small and, generally, not statistically
significant. On the other hand, the meta-classifier
outperforms SwiRL both in domain (14.07% error
reduction) and out of domain (11.67% error reduc-
tion), and the differences are statistically signifi-
cant (measured using two-tailed paired t-test at 99%
confidence interval on 100 samples generated us-
ing bootstrap resampling). We also implemented
two unsupervised voting baselines, one unweighted
(each base model has the same weight) and one
weighted (each base model is weighted by its accu-
racy in development). However, none of these base-
lines outperformed the standalone SwiRL classifier.
This is further proof that, for SR classification, meta-
classification is crucial because it can learn the dis-
tinct specializations of the various base models.
Finally, Table 3 shows that our approach yields
consistent improvements for both core and adjunct
arguments. Out of domain, we see a bigger accuracy
improvement for adjunct arguments (5.64 absolute
points) vs. core arguments (1.78 points). This is
to be expected, as most core arguments fall under
the Arg0 and Arg1 classes, which can typically be
disambiguated based on syntactic information, i.e.,
subject vs. object. On the other hand, there are no
syntactic hints for adjunct arguments, so the system
learns to rely more on SP information in this case.
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999940533333334">
This paper is the first work to show that SPs improve
a state-of-the-art SR classification system. Sev-
eral decisions were crucial for success: (a) we de-
ployed separate SP models for verbs and preposi-
tions, which in conjunction outperform SP models
for verbs alone; (b) we incorporated SPs into SR
classification using a meta-classification approach
that combines eight base models, developed from
variants of a state-of-the-art SRL system and the
above SP models. We show that the resulting system
outperforms the original SR classification system for
arguments mapped to nominal or prepositional con-
stituents. The improvements are statistically sig-
nificant both on in-domain and out-of-domain data
sets.
</bodyText>
<sectionHeader confidence="0.998291" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.989199">
This work was partially supported by projects KNOW-
2 (TIN2009-14715-C04-01 / 04), KYOTO (ICT-2007-
211423) and OpenMT-2 (TIN2009-14675C03)
</bodyText>
<sectionHeader confidence="0.9989" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997346875">
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic role labeling. In
Proc. of CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL.
K. Erk. 2007. A simple, similarity-based model for se-
lectional preferences. In Proc. of ACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of COLING-ACL.
K. Litkowski and O. Hargraves. 2005. The preposi-
tion project. In Proceedings of the Workshop on The
Linguistic Dimensions of Prepositions and their Use
in Computational Linguistic Formalisms and Applica-
tions.
S. Pad´o and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2).
S. Pradhan, W. Ward, and J. Martin. 2007. Towards ro-
bust semantic role labeling. In Proc. of NAACL-HLT.
S. Pradhan, W. Ward, and J. Martin. 2008. Towards ro-
bust semantic role labeling. Computational Linguis-
tics, 34(2).
P. Resnik. 1993. Semantic classes and syntactic ambigu-
ity. In Proc. of HLT.
M. Surdeanu, L. M`arquez, X. Carreras, and P.R. Comas.
2007. Combination strategies for semantic role label-
ing. Journal of Artificial Intelligence Research, 29.
B. Zapirain, E. Agirre, and L. M`arquez. 2009. General-
izing over lexical features: Selectional preferences for
semantic role classification. In Proc. of ACL-IJCNLP.
</reference>
<page confidence="0.999108">
376
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.586966">
<title confidence="0.998983">Improving Semantic Role Classification with Selectional Preferences</title>
<author confidence="0.930831">Eneko Agirre Lluis M`arquez Mihai Surdeanu Zapirain</author>
<affiliation confidence="0.9799365">IXA NLP Group TALP Research Center Stanford NLP Group Basque Country Univ. Technical Univ. of Catalonia Stanford Univ.</affiliation>
<email confidence="0.999618">lluism@lsi.upc.edumihais@stanford.edu</email>
<abstract confidence="0.977989875">This work incorporates Selectional Preferences (SP) into a Semantic Role (SR) Classification system. We learn separate selectional preferences for noun phrases and prepositional phrases and we integrate them in a state-of-the-art SR classification system both in the form of features and individual class predictors. We show that the inclusion of the refined SPs yields statistically significant improvements on both in domain and out of domain data (14.07% and 11.67% error reduction, respectively). The key factor for success is the combination of several SP methods with the original classification model using metaclassification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 Shared Task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>X. Carreras and L. M`arquez. 2005. Introduction to the CoNLL-2005 Shared Task: Semantic role labeling. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="10601" citStr="Charniak, 2000" startWordPosition="1734" endWordPosition="1735">apirain et al., 2009). The differences are due to the fact that we do not discard roles like MOD, DIS, NEG and that our previous work used only the subset of the data that could be mapped to VerbNet (around 50%). All in all, the table shows that splitting SPs into verb and preposition SPs yields better results, both in precision and recall, improving Fl up to 10 points in some cases. 4 Integrating SPs in a SRL system For these experiments we modified SwiRL (Surdeanu et al., 2007): (a) we matched the gold boundaries against syntactic constituents predicted internally using the Charniak parser (Charniak, 2000); and (b) we classified these constituents with their semantic role using a modified version of SwiRL’s feature set. We explored two different strategies for integrating SPs in SwiRL. The first, obvious method is to extend SwiRL’s feature set with features that model the preferences of the SPs, i.e., for each SP model SPi we add a feature whose value is Ri. The second method combines SwiRL’s classification model and our SP models using meta-classification. We opted for a binary classification approach: first, for each constituent we generate n datums, one for each distinct role label proposed </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4003" citStr="Erk, 2007" startWordPosition="601" endWordPosition="602">nt parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, 1http://www.surdeanu.name/mihai/swirl/ 373 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373–376, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, we showed (Zapirain et al., 2009) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (Zapirain et al., 2009). These methods can be split in two main families, depending on the resource used to compute similarity: WordNetbased methods and distributional methods. Both families define a similarity score between</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>K. Erk. 2007. A simple, similarity-based model for selectional preferences. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="3860" citStr="Gildea and Jurafsky (2002)" startWordPosition="579" endWordPosition="582"> Classification SPs have been widely believed to be an important knowledge source when parsing and performing SRL, especially role classification. Still, present parsers and SRL systems use just lexical features, which can be seen as the most simple form of SP, 1http://www.surdeanu.name/mihai/swirl/ 373 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373–376, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics where the headword needs to be seen in the training data, and otherwise the SP is not satisfied. Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. More recently, we showed (Zapirain et al., 2009) that several methods to automatically generate SPs generalize well and outperform lexical match in a large dataset for semantic role classification, but the impact on a full system was not explored. In this work we apply a subset of the SP methods proposed in (Zapirain et al., 2009). These methods can be split in two main families, dependi</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="5825" citStr="Lin, 1998" startWordPosition="904" endWordPosition="905">f two words was computed using the cosine (or Jaccard measure) of the co-occurrence vectors of the two words. Co-occurrence vectors where constructed using freely available software (Pad´o and Lapata, 2007) run over the British National Corpus. We used the optimal parameters (Pad´o and Lapata, 2007, p. 179). We will refer to these similarities as simcos and simJac, respectively. In contrast, second order similarity uses vectors of similar words, i.e., the similarity of two words was computed using the cosine (or Jaccard measure) between the thesaurus entries of those words in Lin’s thesaurus (Lin, 1998). We refer to these as sim�os and sim�ac. Given a target sentence with a verb and its arguments, the task of SR classification is to assign the correct role to each of the arguments. When using SPs alone, we only use the headwords of the arguments, and each argument is classified independently of the rest. For each headword, we select the role (r) of the verb (c) which fits best the head word (w), where the goodness of fit (SPsim(v, r, w)) is modeled using one of the similarity models above, between the headword w and the headwords seen in training data for role r of verb v. This selection rul</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Litkowski</author>
<author>O Hargraves</author>
</authors>
<title>The preposition project.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on The Linguistic Dimensions of Prepositions and their Use in Computational Linguistic Formalisms and Applications.</booktitle>
<contexts>
<context position="6779" citStr="Litkowski and Hargraves, 2005" startWordPosition="1074" endWordPosition="1077">d, we select the role (r) of the verb (c) which fits best the head word (w), where the goodness of fit (SPsim(v, r, w)) is modeled using one of the similarity models above, between the headword w and the headwords seen in training data for role r of verb v. This selection rule is formalized as follows: Rsim(v, w) = arg max rERoles(v) In our previous work (Zapirain et al., 2009), we modelled SPs for pairs of predicates (verbs) and arguments, independently of the fact that the argument is a core argument (typically a noun) or an adjunct argument (typically a prepositional phrase). In contrast, (Litkowski and Hargraves, 2005) show that prepositions have SPs of their own, especially when functioning as adjuncts. We therefore decided to split SPs according to whether the potential argument is a Prepositional Phrase (PP) or a Noun Phrase (NP). For NPs, which tend to be core arguments2, we use the SPs of the verb (as formalized above). For PPs, which have an even distribution between core and adjunct arguments, we use the SPs of the prepositions alone, ignoring the verbs. Implementation wise, this means that in Eq. (1), we change v for p, where p is the preposition heading the PP. 3 Experiments with SPs in isolation I</context>
</contexts>
<marker>Litkowski, Hargraves, 2005</marker>
<rawString>K. Litkowski and O. Hargraves. 2005. The preposition project. In Proceedings of the Workshop on The Linguistic Dimensions of Prepositions and their Use in Computational Linguistic Formalisms and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pad´o</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>S. Pad´o and M. Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>W Ward</author>
<author>J Martin</author>
</authors>
<title>Towards robust semantic role labeling.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="2257" citStr="Pradhan et al., 2007" startWordPosition="332" endWordPosition="335">xercises at CoNLL2004 and 2005 (Carreras and M`arquez, 2005) observed that all systems showed a significant performance degradation (-10 F1 points) when applied to test data from a different genre of that of the training set. Pradhan et al. (2008) showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons. The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007). In recent work, we showed (Zapirain et al., 2009) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system. SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words. The positive effect was especially relevant for out-of-domain data. In this paper we advance (Zapirain et al., 2009) in two directions: (1) We learn separate SPs for prepo</context>
</contexts>
<marker>Pradhan, Ward, Martin, 2007</marker>
<rawString>S. Pradhan, W. Ward, and J. Martin. 2007. Towards robust semantic role labeling. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>W Ward</author>
<author>J Martin</author>
</authors>
<title>Towards robust semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="1883" citStr="Pradhan et al. (2008)" startWordPosition="273" endWordPosition="276">actic, classification requires semantic knowledge to be taken into account. Semantic information is usually captured through lexicalized features on the predicate and the head–word of the argument to be classified. Since lexical features tend to be sparse, SRL systems are prone to overfit the training data and generalize poorly to new corpora. Indeed, the SRL evaluation exercises at CoNLL2004 and 2005 (Carreras and M`arquez, 2005) observed that all systems showed a significant performance degradation (-10 F1 points) when applied to test data from a different genre of that of the training set. Pradhan et al. (2008) showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons. The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (Pradhan et al., 2007). In recent work, we showed (Zapirain et al., 2009) how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected fr</context>
</contexts>
<marker>Pradhan, Ward, Martin, 2008</marker>
<rawString>S. Pradhan, W. Ward, and J. Martin. 2008. Towards robust semantic role labeling. Computational Linguistics, 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Semantic classes and syntactic ambiguity.</title>
<date>1993</date>
<booktitle>In Proc. of HLT.</booktitle>
<marker>Resnik, 1993</marker>
<rawString>P. Resnik. 1993. Semantic classes and syntactic ambiguity. In Proc. of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>L M`arquez</author>
<author>X Carreras</author>
<author>P R Comas</author>
</authors>
<title>Combination strategies for semantic role labeling.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>29</volume>
<marker>Surdeanu, M`arquez, Carreras, Comas, 2007</marker>
<rawString>M. Surdeanu, L. M`arquez, X. Carreras, and P.R. Comas. 2007. Combination strategies for semantic role labeling. Journal of Artificial Intelligence Research, 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zapirain</author>
<author>E Agirre</author>
<author>L M`arquez</author>
</authors>
<title>Generalizing over lexical features: Selectional preferences for semantic role classification.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<marker>Zapirain, Agirre, M`arquez, 2009</marker>
<rawString>B. Zapirain, E. Agirre, and L. M`arquez. 2009. Generalizing over lexical features: Selectional preferences for semantic role classification. In Proc. of ACL-IJCNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>