<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.928906">
Maximum Entropy Based Phrase Reordering
Model for Statistical Machine Translation
</title>
<author confidence="0.989338">
Qun Liu and Shouxun Lin
</author>
<affiliation confidence="0.9697">
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.666994">
Beijing, China, 100080
</address>
<email confidence="0.953585">
{liuqun, sxlin}@ict.ac.cn
</email>
<note confidence="0.8157162">
Deyi Xiong
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, China, 100080
Graduate School of Chinese Academy of Sciences
</note>
<email confidence="0.905481">
dyxiong@ict.ac.cn
</email>
<sectionHeader confidence="0.993389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997181875">
We propose a novel reordering model for
phrase-based statistical machine transla-
tion (SMT) that uses a maximum entropy
(MaxEnt) model to predicate reorderings
of neighbor blocks (phrase pairs). The
model provides content-dependent, hier-
archical phrasal reordering with general-
ization based on features automatically
learned from a real-world bitext. We
present an algorithm to extract all reorder-
ing events of neighbor blocks from bilin-
gual data. In our experiments on Chinese-
to-English translation, this MaxEnt-based
reordering model obtains significant im-
provements in BLEU score on the NIST
MT-05 and IWSLT-04 tasks.
</bodyText>
<sectionHeader confidence="0.998125" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98763">
Phrase reordering is of great importance for
phrase-based SMT systems and becoming an ac-
tive area of research recently. Compared with
word-based SMT systems, phrase-based systems
can easily address reorderings of words within
phrases. However, at the phrase level, reordering
is still a computationally expensive problem just
like reordering at the word level (Knight, 1999).
Many systems use very simple models to re-
order phrases 1. One is distortion model (Och
and Ney, 2004; Koehn et al., 2003) which penal-
izes translations according to their jump distance
instead of their content. For example, if N words
are skipped, a penalty of N will be paid regard-
less of which words are reordered. This model
takes the risk of penalizing long distance jumps
1In this paper, we focus our discussions on phrases that
are not necessarily aligned to syntactic constituent boundary.
which are common between two languages with
very different orders. Another simple model is flat
reordering model (Wu, 1996; Zens et al., 2004;
Kumar et al., 2005) which is not content depen-
dent either. Flat model assigns constant probabili-
ties for monotone order and non-monotone order.
The two probabilities can be set to prefer mono-
tone or non-monotone orientations depending on
the language pairs.
In view of content-independency of the dis-
tortion and flat reordering models, several re-
searchers (Och et al., 2004; Tillmann, 2004; Ku-
mar et al., 2005; Koehn et al., 2005) proposed a
more powerful model called lexicalized reorder-
ing model that is phrase dependent. Lexicalized
reordering model learns local orientations (mono-
tone or non-monotone) with probabilities for each
bilingual phrase from training data. During de-
coding, the model attempts to finding a Viterbi lo-
cal orientation sequence. Performance gains have
been reported for systems with lexicalized reorder-
ing model. However, since reorderings are re-
lated to concrete phrases, researchers have to de-
sign their systems carefully in order not to cause
other problems, e.g. the data sparseness problem.
Another smart reordering model was proposed
by Chiang (2005). In his approach, phrases are re-
organized into hierarchical ones by reducing sub-
phrases to variables. This template-based scheme
not only captures the reorderings of phrases, but
also integrates some phrasal generalizations into
the global model.
In this paper, we propose a novel solution for
phrasal reordering. Here, under the ITG constraint
(Wu, 1997; Zens et al., 2004), we need to con-
sider just two kinds of reorderings, straight and
inverted between two consecutive blocks. There-
fore reordering can be modelled as a problem of
</bodyText>
<note confidence="0.692915333333333">
521
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 521–528,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999791064516129">
classification with only two labels, straight and
inverted. In this paper, we build a maximum en-
tropy based classification model as the reordering
model. Different from lexicalized reordering, we
do not use the whole block as reordering evidence,
but only features extracted from blocks. This is
more flexible. It makes our model reorder any
blocks, observed in training or not. The whole
maximum entropy based reordering model is em-
bedded inside a log-linear phrase-based model of
translation. Following the Bracketing Transduc-
tion Grammar (BTG) (Wu, 1996), we built a
CKY-style decoder for our system, which makes
it possible to reorder phrases hierarchically.
To create a maximum entropy based reordering
model, the first step is learning reordering exam-
ples from training data, similar to the lexicalized
reordering model. But in our way, any evidences
of reorderings will be extracted, not limited to re-
orderings of bilingual phrases of length less than a
predefined number of words. Secondly, features
will be extracted from reordering examples ac-
cording to feature templates. Finally, a maximum
entropy classifier will be trained on the features.
In this paper we describe our system and the
MaxEnt-based reordering model with the associ-
ated algorithm. We also present experiments that
indicate that the MaxEnt-based reordering model
improves translation significantly compared with
other reordering approaches and a state-of-the-art
distortion-based system (Koehn, 2004).
</bodyText>
<sectionHeader confidence="0.911003" genericHeader="method">
2 System Overview
</sectionHeader>
<subsectionHeader confidence="0.978288">
2.1 Model
</subsectionHeader>
<bodyText confidence="0.9999125">
Under the BTG scheme, translation is more
like monolingual parsing through derivations.
Throughout the translation procedure, three rules
are used to derive the translation
</bodyText>
<equation confidence="0.999875666666667">
A-] (A1, A2) (1)
A �→) (A1, A2) (2)
A → (x, y) (3)
</equation>
<bodyText confidence="0.998562285714286">
During decoding, the source sentence is seg-
mented into a sequence of phrases as in a standard
phrase-based model. Then the lexical rule (3) 2 is
2Currently, we restrict phrases x and y not to be null.
Therefore neither deletion nor insertion is carried out during
decoding. However, these operations are to be considered in
our future version of model.
used to translate source phrase y into target phrase
x and generate a block A. Later, the straight rule
(1) merges two consecutive blocks into a single
larger block in the straight order; while the in-
verted rule (2) merges them in the inverted order.
These two merging rules will be used continuously
until the whole source sentence is covered. When
the translation is finished, a tree indicating the hi-
erarchical segmentation of the source sentence is
also produced.
In the following, we will define the model in
a straight way, not in the dynamic programming
recursion way used by (Wu, 1996; Zens et al.,
2004). We focus on defining the probabilities of
different rules by separating different features (in-
cluding the language model) out from the rule
probabilities and organizing them in a log-linear
form. This straight way makes it clear how rules
are used and what they depend on.
For the two merging rules straight and inverted,
applying them on two consecutive blocks A1 and
</bodyText>
<equation confidence="0.9226535">
A2 is assigned a probability Prm(A)
17t S2 -&apos;� λLM
Pr (A) = (4)
pLM(A1,A2)
</equation>
<bodyText confidence="0.9983955">
where the Q is the reordering score of block A1
and A2, an is its weight, and 4pLM(A1,A2) is the
increment of the language model score of the two
blocks according to their final order, ALM is its
weight.
For the lexical rule, applying it is assigned a
</bodyText>
<equation confidence="0.9370264">
probability Prl(A)
Prl(A) = p(x|y)λ1 · p(y|x)λ2 · plex(x|y)λ3
·plex(y|x)λ4 · exp(1)λ5 · exp(|x|)λs
λLM
pLM (x) (5)
</equation>
<bodyText confidence="0.9996531">
where p(·) are the phrase translation probabilities
in both directions, plex(·) are the lexical transla-
tion probabilities in both directions, and exp(1)
and exp(|x|) are the phrase penalty and word
penalty, respectively. These features are very com-
mon in state-of-the-art systems (Koehn et al.,
2005; Chiang, 2005) and As are weights of fea-
tures.
For the reordering model Q, we define it on the
two consecutive blocks A1 and A2 and their order
</bodyText>
<equation confidence="0.780862">
o ∈ {straight, inverted}
Q = f(o, A1, A2) (6)
</equation>
<bodyText confidence="0.958371">
Under this framework, different reordering mod-
els can be designed. In fact, we defined four re-
ordering models in our experiments. The first one
</bodyText>
<page confidence="0.752099">
522
</page>
<bodyText confidence="0.99153525">
is NONE, meaning no explicit reordering features
at all. We set n to 1 for all different pairs of
blocks and their orders. So the phrasal reorder-
ing is totally dependent on the language model.
This model is obviously different from the mono-
tone search, which does not use the inverted rule at
all. The second one is a distortion style reordering
model, which is formulated as
</bodyText>
<equation confidence="0.9968305">
{
exp(0), o = straight
n =
exp(|A&apos;|) + (|A2|), o = inverted
</equation>
<bodyText confidence="0.999632333333333">
where |AZ |denotes the number of words on the
source side of blocks. When an &lt; 0, this de-
sign will penalize those non-monotone transla-
tions. The third one is a flat reordering model,
which assigns probabilities for the straight and in-
verted order. It is formulated as
</bodyText>
<equation confidence="0.9648345">
pm, o = straight
1 − pm, o = inverted
</equation>
<bodyText confidence="0.9994449">
In our experiments on Chinese-English tasks, the
probability for the straight order is set at pm =
0.95. This is because word order in Chinese and
English is usually similar. The last one is the maxi-
mum entropy based reordering model proposed by
us, which will be described in the next section.
We define a derivation D as a sequence of appli-
cations of rules (1) − (3), and let c(D) and e(D)
be the Chinese and English yields of D. The prob-
ability of a derivation D is
</bodyText>
<equation confidence="0.9994945">
P r(D) = � Pr(i) (7)
Z
</equation>
<bodyText confidence="0.971993">
where Pr(i) is the probability of the ith applica-
tion of rules. Given an input sentence c, the final
translation e* is derived from the best derivation
D*
</bodyText>
<equation confidence="0.999136">
D* = argmax
c(D)=c
e* = e(D*) (8)
</equation>
<subsectionHeader confidence="0.963305">
2.2 Decoder
</subsectionHeader>
<bodyText confidence="0.997138203389831">
We developed a CKY style decoder that employs a
beam search algorithm, similar to the one by Chi-
ang (2005). The decoder finds the best derivation
that generates the input sentence and its transla-
tion. From the best derivation, the best English e*
is produced.
Given a source sentence c, firstly we initiate the
chart with phrases from phrase translation table
by applying the lexical rule. Then for each cell
that spans from i to j on the source side, all pos-
sible derivations spanning from i to j are gener-
ated. Our algorithm guarantees that any sub-cells
within (i, j) have been expanded before cell (i, j)
is expanded. Therefore the way to generate deriva-
tions in cell (i, j) is to merge derivations from
any two neighbor sub-cells. This combination is
done by applying the straight and inverted rules.
Each application of these two rules will generate
a new derivation covering cell (i, j). The score of
the new generated derivation is derived from the
scores of its two sub-derivations, reordering model
score and the increment of the language model
score according to the Equation (4). When the
whole input sentence is covered, the decoding is
over.
Pruning of the search space is very important for
the decoder. We use three pruning ways. The first
one is recombination. When two derivations in
the same cell have the same w leftmost/rightmost
words on the English yields, where w depends on
the order of the language model, they will be re-
combined by discarding the derivation with lower
score. The second one is the threshold pruning
which discards derivations that have a score worse
than a times the best score in the same cell. The
last one is the histogram pruning which only keeps
the top n best derivations for each cell. In all our
experiments, we set n = 40, a = 0.5 to get a
tradeoff between speed and performance in the de-
velopment set.
Another feature of our decoder is the k-best list
generation. The k-best list is very important for
the minimum error rate training (Och, 2003a)
which is used for tuning the weights A for our
model. We use a very lazy algorithm for the k-best
list generation, which runs two phases similarly to
the one by Huang et al. (2005). In the first phase,
the decoder runs as usual except that it keeps some
information of weaker derivations which are to be
discarded during recombination. This will gener-
ate not only the first-best of final derivation but
also a shared forest. In the second phase, the
lazy algorithm runs recursively on the shared for-
est. It finds the second-best of the final deriva-
tion, which makes its children to find their second-
best, and children’s children’s second-best, until
the leaf node’s second-best. Then it finds the third-
best, forth-best, and so on. In all our experiments,
we set k = 200.
</bodyText>
<equation confidence="0.849391333333333">
{n =
Pr(D)
523
</equation>
<bodyText confidence="0.9999468">
The decoder is implemented in C++. Using the
pruning settings described above, without the k-
best list generation, it takes about 6 seconds to
translate a sentence of average length 28.3 words
on a 2GHz Linux system with 4G RAM memory.
</bodyText>
<sectionHeader confidence="0.9759285" genericHeader="method">
3 Maximum Entropy Based Reordering
Model
</sectionHeader>
<bodyText confidence="0.999481846153846">
In this section, we discuss how to create a max-
imum entropy based reordering model. As de-
scribed above, we defined the reordering model Q
on the three factors: order o, block A1 and block
A2. The central problem is, given two neighbor
blocks A1 and A2, how to predicate their order
o ∈ {straight, inverted}. This is a typical prob-
lem of two-class classification. To be consistent
with the whole model, the conditional probabil-
ity p(o|A1, A2) is calculated. A simple way to
compute this probability is to take counts from the
training data and then to use the maximum likeli-
hood estimate (MLE)
</bodyText>
<equation confidence="0.998547">
p(o|A1, A2) = Count(o, A1, A2) (9)
Count(A1, A2)
</equation>
<bodyText confidence="0.999841615384615">
The similar way is used by lexicalized reordering
model. However, in our model this way can’t work
because blocks become larger and larger due to us-
ing the merging rules, and finally unseen in the
training data. This means we can not use blocks
as direct reordering evidences.
A good way to this problem is to use features of
blocks as reordering evidences. Good features can
not only capture reorderings, avoid sparseness, but
also integrate generalizations. It is very straight
to use maximum entropy model to integrate fea-
tures to predicate reorderings of blocks. Under the
MaxEnt model, we have
</bodyText>
<equation confidence="0.979355333333333">
Q 1 2 exp(&amp; Bihi(o, A1, A2))
= pe(o|A,A) = �o exp(Ei Bihi(o, A1, A2))
(10)
</equation>
<bodyText confidence="0.999963">
where the functions hi ∈ {0, 1} are model features
and the Bi are weights of the model features which
can be trained by different algorithms (Malouf,
2002).
</bodyText>
<subsectionHeader confidence="0.938669">
3.1 Reordering Example Extraction
Algorithm
</subsectionHeader>
<bodyText confidence="0.97894525">
The input for the algorithm is a bilingual corpus
with high-precision word alignments. We obtain
the word alignments using the way of Koehn et al.
(2005). After running GIZA++ (Och and Ney,
</bodyText>
<figure confidence="0.602673">
source
</figure>
<figureCaption confidence="0.941992">
Figure 1: The bold dots are corners. The ar-
</figureCaption>
<bodyText confidence="0.99513">
rows from the corners are their links. Corner c1 is
shared by block b1 and b2, which in turn are linked
by the STRAIGHT links, bottomleft and topright
of c1. Similarly, block b3 and b4 are linked by the
INVERTED links, topleft and bottomright of c2.
2000) in both directions, we apply the “grow-
diag-final” refinement rule on the intersection
alignments for each sentence pair.
Before we introduce this algorithm, we intro-
duce some formal definitions. The first one is
block which is a pair of source and target contigu-
ous sequences of words
</bodyText>
<figure confidence="0.587783333333333">
b = (sZi, 31)
b must be consistent with the word alignment M
∀(i,j) ∈ M,i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2
</figure>
<bodyText confidence="0.999877363636364">
This definition is similar to that of bilingual phrase
except that there is no length limitation over block.
A reordering example is a triple of (o, b1, b2)
where b1 and b2 are two neighbor blocks and o
is the order between them. We define each vertex
of block as corner. Each corner has four links in
four directions: topright, topleft, bottomright, bot-
tomleft, and each link links a set of blocks which
have the corner as their vertex. The topright and
bottomleft link blocks with the straight order, so
we call them STRAIGHT links. Similarly, we call
the topleft and bottomright INVERTED links since
they link blocks with the inverted order. For con-
venience, we use b ←� Z✓ to denote that block b
is linked by the link Z. Note that the STRAIGHT
links can not coexist with the INVERTED links.
These definitions are illustrated in Figure 1.
The reordering example extraction algorithm is
shown in Figure 2. The basic idea behind this al-
gorithm is to register all neighbor blocks to the
associated links of corners which are shared by
them. To do this, we keep an array to record link
</bodyText>
<figure confidence="0.981376625">
c.
b3
b&apos;
b.
c1
b1
target
524
</figure>
<listItem confidence="0.8885675">
1: Input: sentence pair (s, t) and their alignment M
2: OR := 0
3: for each span (i1, i2) G s do
4: find block b = (si&apos;
i�, tj&apos;
j�) that is consistent with M
5: Extend block b on the target boundary with one possi-
ble non-aligned word to get blocks E(b)
6: for each block b* G b U E(b) do
7: Register b* to the links of four corners of it
8: end for
9: end for
10: for each corner C in the matrix M do
11: if STRAIGHT links exist then
</listItem>
<figure confidence="0.874685375">
OR := OR U{(straight, b1, b2)},
b1 C.bottomleft, b2 +— C.topright
13: else if INVERTED links exist then
OR := OR U{(inverted, b1, b2)},
b1 C.topleft, b2 +— C.bottomright
15: end if
16: end for
17: Output: reordering examples OR
</figure>
<figureCaption confidence="0.967461">
Figure 2: Reordering Example Extraction Algo-
rithm.
</figureCaption>
<bodyText confidence="0.999979555555556">
information of corners when extracting blocks.
Line 4 and 5 are similar to the phrase extraction
algorithm by Och (2003b). Different from Och,
we just extend one word which is aligned to null
on the boundary of target side. If we put some
length limitation over the extracted blocks and out-
put them, we get bilingual phrases used in standard
phrase-based SMT systems and also in our sys-
tem. Line 7 updates all links associated with the
current block. You can attach the current block
to each of these links. However this will increase
reordering examples greatly, especially those with
the straight order. In our Experiments, we just at-
tach the smallest blocks to the STRAIGHT links,
and the largest blocks to the INVERTED links.
This will keep the number of reordering examples
acceptable but without performance degradation.
Line 12 and 14 extract reordering examples.
</bodyText>
<subsectionHeader confidence="0.97406">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999977">
With the extracted reordering examples, we can
obtain features for our MaxEnt-based reordering
model. We design two kinds of features, lexi-
cal features and collocation features. For a block
b = (s, t), we use s1 to denote the first word of the
source s, t1 to denote the first word of the target t.
Lexical features are defined on the single word
s1 or t1. Collocation features are defined on the
combination s1 or t1 between two blocks b1 and
b2. Three kinds of combinations are used. The first
one is source collocation, b1.s1&amp;b2.s1. The sec-
ond is target collocation, b1.t1&amp;b2.t1. The last one
</bodyText>
<equation confidence="0.880191666666667">
hi(o, b1, b2) = J 1, b1.t1 = E1, o = O
0, otherwise
h o b1 b2 = J 1, b1.t1 = E1, b2.t1 = E2, o = O
</equation>
<bodyText confidence="0.784833">
(&apos;,) l 0, otherwise
</bodyText>
<figureCaption confidence="0.858676">
Figure 3: MaxEnt-based reordering feature tem-
</figureCaption>
<bodyText confidence="0.962675727272727">
plates. The first one is a lexical feature, and the
second one is a target collocation feature, where
Ei are English words, O E {straight, inverted}.
is block collocation, b1.s1&amp;b1.t1 and b2.s1&amp;b2.t1.
The templates for the lexical feature and the collo-
cation feature are shown in Figure 3.
Why do we use the first words as features?
These words are nicely at the boundary of blocks.
One of assumptions of phrase-based SMT is that
phrase cohere across two languages (Fox, 2002),
which means phrases in one language tend to be
moved together during translation. This indicates
that boundary words of blocks may keep informa-
tion for their movements/reorderings. To test this
hypothesis, we calculate the information gain ra-
tio (IGR) for boundary words as well as the whole
blocks against the order on the reordering exam-
ples extracted by the algorithm described above.
The IGR is the measure used in the decision tree
learning to select features (Quinlan, 1993). It
represents how precisely the feature predicate the
class. For feature f and class c, the IGR(f, c)
</bodyText>
<equation confidence="0.994015">
IGR(f, c) = En(c) − En(c |f)En(f)
(11)
</equation>
<bodyText confidence="0.9890617">
where En(·) is the entropy and En(·|·)
is the conditional entropy. To our sur-
prise, the IGR for the four boundary words
(IGR((b1.s1, b2.s1, b1.t1, b2.t1), order) =
0.2637) is very close to that for the two blocks
together (IGR((b1, b2), order) = 0.2655).
Although our reordering examples do not cover
all reordering events in the training data, this
result shows that boundary words do provide
some clues for predicating reorderings.
</bodyText>
<sectionHeader confidence="0.999569" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99668125">
We carried out experiments to compare against
various reordering models and systems to demon-
strate the competitiveness of MaxEnt-based re-
ordering:
</bodyText>
<listItem confidence="0.942876125">
1. Monotone search: the inverted rule is not
used.
525
2. Reordering variants: the NONE, distortion
and flat reordering models described in Sec-
tion 2.1.
3. Pharaoh: A state-of-the-art distortion-based
decoder (Koehn, 2004).
</listItem>
<subsectionHeader confidence="0.980029">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999977272727273">
Our experiments were made on two Chinese-to-
English translation tasks: NIST MT-05 (news do-
main) and IWSLT-04 (travel dialogue domain).
NIST MT-05. In this task, the bilingual train-
ing data comes from the FBIS corpus with 7.06M
Chinese words and 9.15M English words. The tri-
gram language model training data consists of En-
glish texts mostly derived from the English side
of the UN corpus (catalog number LDC2004E12),
which totally contains 81M English words. For the
efficiency of minimum error rate training, we built
our development set using sentences of length at
most 50 characters from the NIST MT-02 evalua-
tion test data.
IWSLT-04. For this task, our experiments were
carried out on the small data track. Both the
bilingual training data and the trigram language
model training data are restricted to the supplied
corpus, which contains 20k sentences, 179k Chi-
nese words and 157k English words. We used the
CSTAR 2003 test set consisting of 506 sentence
pairs as development set.
</bodyText>
<subsectionHeader confidence="0.993685">
4.2 Training
</subsectionHeader>
<bodyText confidence="0.999959941176471">
We obtained high-precision word alignments us-
ing the way described in Section 3.1. Then we
ran our reordering example extraction algorithm to
output blocks of length at most 7 words on the Chi-
nese side together with their internal alignments.
We also limited the length ratio between the target
and source language (max(|s|, |t|)/min(|s|, |t|))
to 3. After extracting phrases, we calculated the
phrase translation probabilities and lexical transla-
tion probabilities in both directions for each bilin-
gual phrase.
For the minimum-error-rate training, we re-
implemented Venugopal’s trainer 3 (Venugopal
et al., 2005) in C++. For all experiments, we ran
this trainer with the decoder iteratively to tune the
weights As to maximize the BLEU score on the
development set.
</bodyText>
<footnote confidence="0.905468">
3See http://www.cs.cmu.edu/ ashishv/mer.html. This is a
Matlab implementation.
</footnote>
<note confidence="0.394157">
Pharaoh
</note>
<bodyText confidence="0.999981727272727">
We shared the same phrase translation tables
between Pharaoh and our system since the two
systems use the same features of phrases. In fact,
we extracted more phrases than Pharaoh’s trainer
with its default settings. And we also used our re-
implemented trainer to tune lambdas of Pharaoh
to maximize its BLEU score. During decoding,
we pruned the phrase table with b = 100 (default
20), pruned the chart with n = 100, a = 10−5
(default setting), and limited distortions to 4
(default 0).
</bodyText>
<subsectionHeader confidence="0.788666">
MazEnt-based Reordering Model
</subsectionHeader>
<bodyText confidence="0.999998722222222">
We firstly ran our reordering example extraction
algorithm on the bilingual training data without
any length limitations to obtain reordering ex-
amples and then extracted features from these
examples. In the task of NIST MT-05, we
obtained about 2.7M reordering examples with
the straight order, and 367K with the inverted
order, from which 112K lexical features and
1.7M collocation features after deleting those
with one occurrence were extracted. In the task
of IWSLT-04, we obtained 79.5k reordering
examples with the straight order, 9.3k with the
inverted order, from which 16.9K lexical features
and 89.6K collocation features after deleting those
with one occurrence were extracted. Finally, we
ran the MaxEnt toolkit by Zhang 4 to tune the
feature weights. We set iteration number to 100
and Gaussian prior to 1 for avoiding overfitting.
</bodyText>
<subsectionHeader confidence="0.950133">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999890142857143">
We dropped unknown words (Koehn et al., 2005)
of translations for both tasks before evaluating
their BLEU scores. To be consistent with the
official evaluation criterions of both tasks, case-
sensitive BLEU-4 scores were computed For the
NIST MT-05 task and case-insensitive BLEU-4
scores were computed for the IWSLT-04 task 5.
Experimental results on both tasks are shown in
Table 1. Italic numbers refer to results for which
the difference to the best result (indicated in bold)
is not statistically significant. For all scores, we
also show the 95% confidence intervals computed
using Zhang’s significant tester (Zhang et al.,
2004) which was modified to conform to NIST’s
</bodyText>
<footnote confidence="0.9795505">
4See http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
5Note that the evaluation criterion of IWSLT-04 is not to-
tally matched since we didn’t remove punctuation marks.
</footnote>
<page confidence="0.85709">
526
</page>
<bodyText confidence="0.999808272727273">
definition of the BLEU brevity penalty.
We observe that if phrasal reordering is totally
dependent on the language model (NONE) we
get the worst performance, even worse than the
monotone search. This indicates that our language
models were not strong to discriminate between
straight orders and inverted orders. The flat and
distortion reordering models (Row 3 and 4) show
similar performance with Pharaoh. Although they
are not dependent on phrases, they really reorder
phrases with penalties to wrong orders supported
by the language model and therefore outperform
the monotone search. In row 6, only lexical fea-
tures are used for the MaxEnt-based reordering
model; while row 7 uses lexical features and col-
location features. On both tasks, we observe that
various reordering approaches show similar and
stable performance ranks in different domains and
the MaxEnt-based reordering models achieve the
best performance among them. Using all features
for the MaxEnt model (lex + col) is marginally
better than using only lex features (lex).
</bodyText>
<subsectionHeader confidence="0.998742">
4.4 Scaling to Large Bitezts
</subsectionHeader>
<bodyText confidence="0.999996952380953">
In the experiments described above, collocation
features do not make great contributions to the per-
formance improvement but make the total num-
ber of features increase greatly. This is a prob-
lem for MaxEnt parameter estimation if it is scaled
to large bitexts. Therefore, for the integration of
MaxEnt-based phrase reordering model in the sys-
tem trained on large bitexts, we remove colloca-
tion features and only use lexical features from
the last words of blocks (similar to those from the
first words of blocks with similar performance).
This time the bilingual training data contain 2.4M
sentence pairs (68.1M Chinese words and 73.8M
English words) and two trigram language models
are used. One is trained on the English side of
the bilingual training data. The other is trained on
the Xinhua portion of the Gigaword corpus with
181.1M words. We also use some rules to trans-
late numbers, time expressions and Chinese per-
son names. The new Bleu score on NIST MT-05
is 0.291 which is very promising.
</bodyText>
<sectionHeader confidence="0.997761" genericHeader="conclusions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.99851275">
In this paper we presented a MaxEnt-based phrase
reordering model for SMT. We used lexical fea-
tures and collocation features from boundary
words of blocks to predicate reorderings of neigh-
</bodyText>
<table confidence="0.999762375">
Systems NIST MT-05 IWSLT-04
monotone 20.1 f 0.8 37.8 f 3.2
NONE 19.6 f 0.8 36.3 f 2.9
Distortion 20.9 f 0.8 38.8 f 3.0
Flat 20.5 f 0.8 38.7 f 2.8
Pharaoh 20.8 f 0.8 38.9 f 3.3
MaxEnt (lex) 22.0 f 0.8 42.4 f 3.3
MaxEnt (lex + col) 22.2 f 0.8 42.8 f 3.3
</table>
<tableCaption confidence="0.996263">
Table 1: BLEU-4 scores (%) with the 95% confi-
</tableCaption>
<bodyText confidence="0.997657275">
dence intervals. Italic numbers refer to results for
which the difference to the best result (indicated in
bold) is not statistically significant.
bor blocks. Experiments on standard Chinese-
English translation tasks from two different do-
mains showed that our method achieves a signif-
icant improvement over the distortion/flat reorder-
ing models.
Traditional distortion/flat-based SMT transla-
tion systems are good for learning phrase transla-
tion pairs, but learn nothing for phrasal reorder-
ings from real-world data. This is our original
motivation for designing a new reordering model,
which can learn reorderings from training data just
like learning phrasal translations. Lexicalized re-
ordering model learns reorderings from training
data, but it binds reorderings to individual concrete
phrases, which restricts the model to reorderings
of phrases seen in training data. On the contrary,
the MaxEnt-based reordering model is not limited
by this constraint since it is based on features of
phrase, not phrase itself. It can be easily general-
ized to reorder unseen phrases provided that some
features are fired on these phrases.
Another advantage of the MaxEnt-based re-
ordering model is that it can take more fea-
tures into reordering, even though they are non-
independent. Tillmann et. al (2005) also use a
MaxEnt model to integrate various features. The
difference is that they use the MaxEnt model to
predict not only orders but also blocks. To do that,
it is necessary for the MaxEnt model to incorpo-
rate real-valued features such as the block trans-
lation probability and the language model proba-
bility. Due to the expensive computation, a local
model is built. However, our MaxEnt model is just
a module of the whole log-linear model of transla-
tion which uses its score as a real-valued feature.
The modularity afforded by this design does not
incur any computation problems, and make it eas-
</bodyText>
<page confidence="0.583935">
527
</page>
<bodyText confidence="0.999907636363636">
ier to update one sub-model with other modules
unchanged.
Beyond the MaxEnt-based reordering model,
another feature deserving attention in our system
is the CKY style decoder which observes the ITG.
This is different from the work of Zens et. al.
(2004). In their approach, translation is generated
linearly, word by word and phrase by phrase in a
traditional way with respect to the incorporation
of the language model. It can be said that their de-
coder did not violate the ITG constraints but not
that it observed the ITG. The ITG not only de-
creases reorderings greatly but also makes reorder-
ing hierarchical. Hierarchical reordering is more
meaningful for languages which are organized hi-
erarchically. From this point, our decoder is simi-
lar to the work by Chiang (2005).
The future work is to investigate other valuable
features, e.g. binary features that explain blocks
from the syntactical view. We think that there is
still room for improvement if more contributing
features are used.
</bodyText>
<sectionHeader confidence="0.997277" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99952075">
This work was supported in part by National High
Technology Research and Development Program
under grant #2005AA114140 and National Nat-
ural Science Foundation of China under grant
#60573188. Special thanks to Yajuan L¨u for
discussions of the manuscript of this paper and
three anonymous reviewers who provided valuable
comments.
</bodyText>
<sectionHeader confidence="0.998442" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999918410958904">
Ashish Venugopal, Stephan Vogel. 2005. Considerations in
Maximum Mutual Information and Minimum Classifica-
tion Error training for Statistical Machine Translation. In
the Proceedings ofEAMT-05, Budapest, Hungary May 30-
31.
Christoph Tillmann. 2004. A block orientation model for
statistical machine translation. In HLT-NAACL, Boston,
MA, USA.
Christoph Tillmann and Tong Zhang. 2005. A Localized
Prediction Model for statistical machine translation. In
Proceedings ofACL 2005, pages 557–564.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings ofACL
2005, pages 263–270.
Dekai Wu. 1996. A Polynomial-Time Algorithm for Statis-
tical Machine Translation. In Proceedings ofACL 1996.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23:377–404.
Franz Josef Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings ofACL 2000, pages
440–447.
Franz Josef Och. 2003a. Minimum error rate training in sta-
tistical machine translation. In Proceedings ofACL 2003,
pages 160–167.
Franz Josef Och. 2003b. Statistical Machine Translation:
From Single-Word Models to Alignment Templates The-
sis.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation. Com-
putational Linguistics, 30:417–449.
Franz Josef Och, Ignacio Thayer, Daniel Marcu, Kevin
Knight, Dragos Stefan Munteanu, Quamrul Tipu, Michel
Galley, and Mark Hopkins. 2004. Arabic and Chinese MT
at USC/ISI. Presentation given at NIST Machine Transla-
tion Evaluation Workshop.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings ofEMNLP 2002.
J. R. Quinlan. 1993. C4.5: progarms for machine learning.
Morgan Kaufmann Publishers.
Kevin Knight. 1999. Decoding complexity in wordreplace-
ment translation models. Computational Linguistics,
Squibs &amp; Discussion, 25(4).
Liang Huang and David Chiang. 2005. Better k-best parsing.
In Proceedings of the Ninth International Workshop on
Parsing Technology, Vancouver, October, pages 53–64.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings of
HLT/NAACL.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proceedings of the Sixth Conference of the Association for
Machine Translation in the Americas, pages 115–124.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne,
Chris Callison-Burch, Miles Osborne and David Talbot.
2005. Edinburgh System Description for the 2005 IWSLT
Speech Translation Evaluation. In International Work-
shop on Spoken Language Translation.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Re-
ordering Constraints for Phrase-Based Statistical Machine
Translation. In Proceedings of CoLing 2004, Geneva,
Switzerland, pp. 205-211.
Robert Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of the
Sixth Conference on Natural Language Learning (CoNLL-
2002).
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation. In
Proceedings of HLT-EMNLP.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do
we need to have a better system? In Proceedings ofLREC
2004, pages 2051– 2054.
</reference>
<page confidence="0.911738">
528
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.599894">
<title confidence="0.999703">Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation</title>
<author confidence="0.997852">Qun Liu</author>
<author confidence="0.997852">Shouxun Lin</author>
<affiliation confidence="0.996008">Institute of Computing Technology Chinese Academy of Sciences</affiliation>
<address confidence="0.999346">Beijing, China, 100080</address>
<author confidence="0.848185">Deyi Xiong</author>
<affiliation confidence="0.9960235">Institute of Computing Technology Chinese Academy of Sciences</affiliation>
<address confidence="0.995216">Beijing, China, 100080</address>
<affiliation confidence="0.920215">Graduate School of Chinese Academy of Sciences</affiliation>
<email confidence="0.854242">dyxiong@ict.ac.cn</email>
<abstract confidence="0.993088058823529">We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs). The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext. We present an algorithm to extract all reordering events of neighbor blocks from bilingual data. In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
</authors>
<title>Considerations in Maximum Mutual Information and Minimum Classification Error training for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In the Proceedings ofEAMT-05,</booktitle>
<pages>30--31</pages>
<location>Budapest, Hungary</location>
<marker>Venugopal, Vogel, 2005</marker>
<rawString>Ashish Venugopal, Stephan Vogel. 2005. Considerations in Maximum Mutual Information and Minimum Classification Error training for Statistical Machine Translation. In the Proceedings ofEAMT-05, Budapest, Hungary May 30-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A block orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<location>Boston, MA, USA.</location>
<contexts>
<context position="2426" citStr="Tillmann, 2004" startWordPosition="365" endWordPosition="366">ns on phrases that are not necessarily aligned to syntactic constituent boundary. which are common between two languages with very different orders. Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either. Flat model assigns constant probabilities for monotone order and non-monotone order. The two probabilities can be set to prefer monotone or non-monotone orientations depending on the language pairs. In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent. Lexicalized reordering model learns local orientations (monotone or non-monotone) with probabilities for each bilingual phrase from training data. During decoding, the model attempts to finding a Viterbi local orientation sequence. Performance gains have been reported for systems with lexicalized reordering model. However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, </context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A block orientation model for statistical machine translation. In HLT-NAACL, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A Localized Prediction Model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>557--564</pages>
<marker>Tillmann, Zhang, 2005</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2005. A Localized Prediction Model for statistical machine translation. In Proceedings ofACL 2005, pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="3120" citStr="Chiang (2005)" startWordPosition="471" endWordPosition="472">exicalized reordering model that is phrase dependent. Lexicalized reordering model learns local orientations (monotone or non-monotone) with probabilities for each bilingual phrase from training data. During decoding, the model attempts to finding a Viterbi local orientation sequence. Performance gains have been reported for systems with lexicalized reordering model. However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, e.g. the data sparseness problem. Another smart reordering model was proposed by Chiang (2005). In his approach, phrases are reorganized into hierarchical ones by reducing subphrases to variables. This template-based scheme not only captures the reorderings of phrases, but also integrates some phrasal generalizations into the global model. In this paper, we propose a novel solution for phrasal reordering. Here, under the ITG constraint (Wu, 1997; Zens et al., 2004), we need to consider just two kinds of reorderings, straight and inverted between two consecutive blocks. Therefore reordering can be modelled as a problem of 521 Proceedings of the 21st International Conference on Computati</context>
<context position="7680" citStr="Chiang, 2005" startWordPosition="1203" endWordPosition="1204">ight, and 4pLM(A1,A2) is the increment of the language model score of the two blocks according to their final order, ALM is its weight. For the lexical rule, applying it is assigned a probability Prl(A) Prl(A) = p(x|y)λ1 · p(y|x)λ2 · plex(x|y)λ3 ·plex(y|x)λ4 · exp(1)λ5 · exp(|x|)λs λLM pLM (x) (5) where p(·) are the phrase translation probabilities in both directions, plex(·) are the lexical translation probabilities in both directions, and exp(1) and exp(|x|) are the phrase penalty and word penalty, respectively. These features are very common in state-of-the-art systems (Koehn et al., 2005; Chiang, 2005) and As are weights of features. For the reordering model Q, we define it on the two consecutive blocks A1 and A2 and their order o ∈ {straight, inverted} Q = f(o, A1, A2) (6) Under this framework, different reordering models can be designed. In fact, we defined four reordering models in our experiments. The first one 522 is NONE, meaning no explicit reordering features at all. We set n to 1 for all different pairs of blocks and their orders. So the phrasal reordering is totally dependent on the language model. This model is obviously different from the monotone search, which does not use the </context>
<context position="9545" citStr="Chiang (2005)" startWordPosition="1555" endWordPosition="1557">e is the maximum entropy based reordering model proposed by us, which will be described in the next section. We define a derivation D as a sequence of applications of rules (1) − (3), and let c(D) and e(D) be the Chinese and English yields of D. The probability of a derivation D is P r(D) = � Pr(i) (7) Z where Pr(i) is the probability of the ith application of rules. Given an input sentence c, the final translation e* is derived from the best derivation D* D* = argmax c(D)=c e* = e(D*) (8) 2.2 Decoder We developed a CKY style decoder that employs a beam search algorithm, similar to the one by Chiang (2005). The decoder finds the best derivation that generates the input sentence and its translation. From the best derivation, the best English e* is produced. Given a source sentence c, firstly we initiate the chart with phrases from phrase translation table by applying the lexical rule. Then for each cell that spans from i to j on the source side, all possible derivations spanning from i to j are generated. Our algorithm guarantees that any sub-cells within (i, j) have been expanded before cell (i, j) is expanded. Therefore the way to generate derivations in cell (i, j) is to merge derivations fro</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings ofACL 2005, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A Polynomial-Time Algorithm for Statistical Machine Translation.</title>
<date>1996</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="2016" citStr="Wu, 1996" startWordPosition="299" endWordPosition="300">ms use very simple models to reorder phrases 1. One is distortion model (Och and Ney, 2004; Koehn et al., 2003) which penalizes translations according to their jump distance instead of their content. For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered. This model takes the risk of penalizing long distance jumps 1In this paper, we focus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary. which are common between two languages with very different orders. Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either. Flat model assigns constant probabilities for monotone order and non-monotone order. The two probabilities can be set to prefer monotone or non-monotone orientations depending on the language pairs. In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent. Lexicalized reordering model learns local orientations </context>
<context position="4413" citStr="Wu, 1996" startWordPosition="669" endWordPosition="670">6. c�2006 Association for Computational Linguistics classification with only two labels, straight and inverted. In this paper, we build a maximum entropy based classification model as the reordering model. Different from lexicalized reordering, we do not use the whole block as reordering evidence, but only features extracted from blocks. This is more flexible. It makes our model reorder any blocks, observed in training or not. The whole maximum entropy based reordering model is embedded inside a log-linear phrase-based model of translation. Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically. To create a maximum entropy based reordering model, the first step is learning reordering examples from training data, similar to the lexicalized reordering model. But in our way, any evidences of reorderings will be extracted, not limited to reorderings of bilingual phrases of length less than a predefined number of words. Secondly, features will be extracted from reordering examples according to feature templates. Finally, a maximum entropy classifier will be trained on the features. In </context>
<context position="6533" citStr="Wu, 1996" startWordPosition="1010" endWordPosition="1011">rsion of model. used to translate source phrase y into target phrase x and generate a block A. Later, the straight rule (1) merges two consecutive blocks into a single larger block in the straight order; while the inverted rule (2) merges them in the inverted order. These two merging rules will be used continuously until the whole source sentence is covered. When the translation is finished, a tree indicating the hierarchical segmentation of the source sentence is also produced. In the following, we will define the model in a straight way, not in the dynamic programming recursion way used by (Wu, 1996; Zens et al., 2004). We focus on defining the probabilities of different rules by separating different features (including the language model) out from the rule probabilities and organizing them in a log-linear form. This straight way makes it clear how rules are used and what they depend on. For the two merging rules straight and inverted, applying them on two consecutive blocks A1 and A2 is assigned a probability Prm(A) 17t S2 -&apos;� λLM Pr (A) = (4) pLM(A1,A2) where the Q is the reordering score of block A1 and A2, an is its weight, and 4pLM(A1,A2) is the increment of the language model score</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Dekai Wu. 1996. A Polynomial-Time Algorithm for Statistical Machine Translation. In Proceedings ofACL 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--377</pages>
<contexts>
<context position="3475" citStr="Wu, 1997" startWordPosition="525" endWordPosition="526">ng model. However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, e.g. the data sparseness problem. Another smart reordering model was proposed by Chiang (2005). In his approach, phrases are reorganized into hierarchical ones by reducing subphrases to variables. This template-based scheme not only captures the reorderings of phrases, but also integrates some phrasal generalizations into the global model. In this paper, we propose a novel solution for phrasal reordering. Here, under the ITG constraint (Wu, 1997; Zens et al., 2004), we need to consider just two kinds of reorderings, straight and inverted between two consecutive blocks. Therefore reordering can be modelled as a problem of 521 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 521–528, Sydney, July 2006. c�2006 Association for Computational Linguistics classification with only two labels, straight and inverted. In this paper, we build a maximum entropy based classification model as the reordering model. Different from lexicalized reordering, we do not use the whole bl</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>440--447</pages>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings ofACL 2000, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="11437" citStr="Och, 2003" startWordPosition="1887" endWordPosition="1888">epends on the order of the language model, they will be recombined by discarding the derivation with lower score. The second one is the threshold pruning which discards derivations that have a score worse than a times the best score in the same cell. The last one is the histogram pruning which only keeps the top n best derivations for each cell. In all our experiments, we set n = 40, a = 0.5 to get a tradeoff between speed and performance in the development set. Another feature of our decoder is the k-best list generation. The k-best list is very important for the minimum error rate training (Och, 2003a) which is used for tuning the weights A for our model. We use a very lazy algorithm for the k-best list generation, which runs two phases similarly to the one by Huang et al. (2005). In the first phase, the decoder runs as usual except that it keeps some information of weaker derivations which are to be discarded during recombination. This will generate not only the first-best of final derivation but also a shared forest. In the second phase, the lazy algorithm runs recursively on the shared forest. It finds the second-best of the final derivation, which makes its children to find their seco</context>
<context position="16844" citStr="Och (2003" startWordPosition="2866" endWordPosition="2867">igned word to get blocks E(b) 6: for each block b* G b U E(b) do 7: Register b* to the links of four corners of it 8: end for 9: end for 10: for each corner C in the matrix M do 11: if STRAIGHT links exist then OR := OR U{(straight, b1, b2)}, b1 C.bottomleft, b2 +— C.topright 13: else if INVERTED links exist then OR := OR U{(inverted, b1, b2)}, b1 C.topleft, b2 +— C.bottomright 15: end if 16: end for 17: Output: reordering examples OR Figure 2: Reordering Example Extraction Algorithm. information of corners when extracting blocks. Line 4 and 5 are similar to the phrase extraction algorithm by Och (2003b). Different from Och, we just extend one word which is aligned to null on the boundary of target side. If we put some length limitation over the extracted blocks and output them, we get bilingual phrases used in standard phrase-based SMT systems and also in our system. Line 7 updates all links associated with the current block. You can attach the current block to each of these links. However this will increase reordering examples greatly, especially those with the straight order. In our Experiments, we just attach the smallest blocks to the STRAIGHT links, and the largest blocks to the INVER</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003a. Minimum error rate training in statistical machine translation. In Proceedings ofACL 2003, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Statistical Machine Translation: From Single-Word Models to Alignment Templates Thesis.</title>
<date>2003</date>
<contexts>
<context position="11437" citStr="Och, 2003" startWordPosition="1887" endWordPosition="1888">epends on the order of the language model, they will be recombined by discarding the derivation with lower score. The second one is the threshold pruning which discards derivations that have a score worse than a times the best score in the same cell. The last one is the histogram pruning which only keeps the top n best derivations for each cell. In all our experiments, we set n = 40, a = 0.5 to get a tradeoff between speed and performance in the development set. Another feature of our decoder is the k-best list generation. The k-best list is very important for the minimum error rate training (Och, 2003a) which is used for tuning the weights A for our model. We use a very lazy algorithm for the k-best list generation, which runs two phases similarly to the one by Huang et al. (2005). In the first phase, the decoder runs as usual except that it keeps some information of weaker derivations which are to be discarded during recombination. This will generate not only the first-best of final derivation but also a shared forest. In the second phase, the lazy algorithm runs recursively on the shared forest. It finds the second-best of the final derivation, which makes its children to find their seco</context>
<context position="16844" citStr="Och (2003" startWordPosition="2866" endWordPosition="2867">igned word to get blocks E(b) 6: for each block b* G b U E(b) do 7: Register b* to the links of four corners of it 8: end for 9: end for 10: for each corner C in the matrix M do 11: if STRAIGHT links exist then OR := OR U{(straight, b1, b2)}, b1 C.bottomleft, b2 +— C.topright 13: else if INVERTED links exist then OR := OR U{(inverted, b1, b2)}, b1 C.topleft, b2 +— C.bottomright 15: end if 16: end for 17: Output: reordering examples OR Figure 2: Reordering Example Extraction Algorithm. information of corners when extracting blocks. Line 4 and 5 are similar to the phrase extraction algorithm by Och (2003b). Different from Och, we just extend one word which is aligned to null on the boundary of target side. If we put some length limitation over the extracted blocks and output them, we get bilingual phrases used in standard phrase-based SMT systems and also in our system. Line 7 updates all links associated with the current block. You can attach the current block to each of these links. However this will increase reordering examples greatly, especially those with the straight order. In our Experiments, we just attach the smallest blocks to the STRAIGHT links, and the largest blocks to the INVER</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003b. Statistical Machine Translation: From Single-Word Models to Alignment Templates Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="1498" citStr="Och and Ney, 2004" startWordPosition="212" endWordPosition="215">tion, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. 1 Introduction Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently. Compared with word-based SMT systems, phrase-based systems can easily address reorderings of words within phrases. However, at the phrase level, reordering is still a computationally expensive problem just like reordering at the word level (Knight, 1999). Many systems use very simple models to reorder phrases 1. One is distortion model (Och and Ney, 2004; Koehn et al., 2003) which penalizes translations according to their jump distance instead of their content. For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered. This model takes the risk of penalizing long distance jumps 1In this paper, we focus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary. which are common between two languages with very different orders. Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either. Fl</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Ignacio Thayer</author>
<author>Daniel Marcu</author>
<author>Kevin Knight</author>
<author>Dragos Stefan Munteanu</author>
<author>Quamrul Tipu</author>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
</authors>
<date>2004</date>
<booktitle>Arabic and Chinese MT at USC/ISI. Presentation given at NIST Machine Translation Evaluation Workshop.</booktitle>
<contexts>
<context position="2410" citStr="Och et al., 2004" startWordPosition="361" endWordPosition="364">ocus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary. which are common between two languages with very different orders. Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either. Flat model assigns constant probabilities for monotone order and non-monotone order. The two probabilities can be set to prefer monotone or non-monotone orientations depending on the language pairs. In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent. Lexicalized reordering model learns local orientations (monotone or non-monotone) with probabilities for each bilingual phrase from training data. During decoding, the model attempts to finding a Viterbi local orientation sequence. Performance gains have been reported for systems with lexicalized reordering model. However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause </context>
</contexts>
<marker>Och, Thayer, Marcu, Knight, Munteanu, Tipu, Galley, Hopkins, 2004</marker>
<rawString>Franz Josef Och, Ignacio Thayer, Daniel Marcu, Kevin Knight, Dragos Stefan Munteanu, Quamrul Tipu, Michel Galley, and Mark Hopkins. 2004. Arabic and Chinese MT at USC/ISI. Presentation given at NIST Machine Translation Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofEMNLP</booktitle>
<contexts>
<context position="18845" citStr="Fox, 2002" startWordPosition="3219" endWordPosition="3220">1, o = O 0, otherwise h o b1 b2 = J 1, b1.t1 = E1, b2.t1 = E2, o = O (&apos;,) l 0, otherwise Figure 3: MaxEnt-based reordering feature templates. The first one is a lexical feature, and the second one is a target collocation feature, where Ei are English words, O E {straight, inverted}. is block collocation, b1.s1&amp;b1.t1 and b2.s1&amp;b2.t1. The templates for the lexical feature and the collocation feature are shown in Figure 3. Why do we use the first words as features? These words are nicely at the boundary of blocks. One of assumptions of phrase-based SMT is that phrase cohere across two languages (Fox, 2002), which means phrases in one language tend to be moved together during translation. This indicates that boundary words of blocks may keep information for their movements/reorderings. To test this hypothesis, we calculate the information gain ratio (IGR) for boundary words as well as the whole blocks against the order on the reordering examples extracted by the algorithm described above. The IGR is the measure used in the decision tree learning to select features (Quinlan, 1993). It represents how precisely the feature predicate the class. For feature f and class c, the IGR(f, c) IGR(f, c) = En</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In Proceedings ofEMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: progarms for machine learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="19327" citStr="Quinlan, 1993" startWordPosition="3297" endWordPosition="3298"> are nicely at the boundary of blocks. One of assumptions of phrase-based SMT is that phrase cohere across two languages (Fox, 2002), which means phrases in one language tend to be moved together during translation. This indicates that boundary words of blocks may keep information for their movements/reorderings. To test this hypothesis, we calculate the information gain ratio (IGR) for boundary words as well as the whole blocks against the order on the reordering examples extracted by the algorithm described above. The IGR is the measure used in the decision tree learning to select features (Quinlan, 1993). It represents how precisely the feature predicate the class. For feature f and class c, the IGR(f, c) IGR(f, c) = En(c) − En(c |f)En(f) (11) where En(·) is the entropy and En(·|·) is the conditional entropy. To our surprise, the IGR for the four boundary words (IGR((b1.s1, b2.s1, b1.t1, b2.t1), order) = 0.2637) is very close to that for the two blocks together (IGR((b1, b2), order) = 0.2655). Although our reordering examples do not cover all reordering events in the training data, this result shows that boundary words do provide some clues for predicating reorderings. 4 Experiments We carrie</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. R. Quinlan. 1993. C4.5: progarms for machine learning. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics, Squibs &amp; Discussion,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="1396" citStr="Knight, 1999" startWordPosition="195" endWordPosition="196">ing events of neighbor blocks from bilingual data. In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. 1 Introduction Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently. Compared with word-based SMT systems, phrase-based systems can easily address reorderings of words within phrases. However, at the phrase level, reordering is still a computationally expensive problem just like reordering at the word level (Knight, 1999). Many systems use very simple models to reorder phrases 1. One is distortion model (Och and Ney, 2004; Koehn et al., 2003) which penalizes translations according to their jump distance instead of their content. For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered. This model takes the risk of penalizing long distance jumps 1In this paper, we focus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary. which are common between two languages with very different orders. Another simple model is flat reorde</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, Squibs &amp; Discussion, 25(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>53--64</pages>
<location>Vancouver,</location>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the Ninth International Workshop on Parsing Technology, Vancouver, October, pages 53–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="1519" citStr="Koehn et al., 2003" startWordPosition="216" endWordPosition="219">ased reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. 1 Introduction Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently. Compared with word-based SMT systems, phrase-based systems can easily address reorderings of words within phrases. However, at the phrase level, reordering is still a computationally expensive problem just like reordering at the word level (Knight, 1999). Many systems use very simple models to reorder phrases 1. One is distortion model (Och and Ney, 2004; Koehn et al., 2003) which penalizes translations according to their jump distance instead of their content. For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered. This model takes the risk of penalizing long distance jumps 1In this paper, we focus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary. which are common between two languages with very different orders. Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either. Flat model assigns cons</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="5334" citStr="Koehn, 2004" startWordPosition="805" endWordPosition="806">rings will be extracted, not limited to reorderings of bilingual phrases of length less than a predefined number of words. Secondly, features will be extracted from reordering examples according to feature templates. Finally, a maximum entropy classifier will be trained on the features. In this paper we describe our system and the MaxEnt-based reordering model with the associated algorithm. We also present experiments that indicate that the MaxEnt-based reordering model improves translation significantly compared with other reordering approaches and a state-of-the-art distortion-based system (Koehn, 2004). 2 System Overview 2.1 Model Under the BTG scheme, translation is more like monolingual parsing through derivations. Throughout the translation procedure, three rules are used to derive the translation A-] (A1, A2) (1) A �→) (A1, A2) (2) A → (x, y) (3) During decoding, the source sentence is segmented into a sequence of phrases as in a standard phrase-based model. Then the lexical rule (3) 2 is 2Currently, we restrict phrases x and y not to be null. Therefore neither deletion nor insertion is carried out during decoding. However, these operations are to be considered in our future version of </context>
<context position="20287" citStr="Koehn, 2004" startWordPosition="3450" endWordPosition="3451"> blocks together (IGR((b1, b2), order) = 0.2655). Although our reordering examples do not cover all reordering events in the training data, this result shows that boundary words do provide some clues for predicating reorderings. 4 Experiments We carried out experiments to compare against various reordering models and systems to demonstrate the competitiveness of MaxEnt-based reordering: 1. Monotone search: the inverted rule is not used. 525 2. Reordering variants: the NONE, distortion and flat reordering models described in Section 2.1. 3. Pharaoh: A state-of-the-art distortion-based decoder (Koehn, 2004). 4.1 Corpus Our experiments were made on two Chinese-toEnglish translation tasks: NIST MT-05 (news domain) and IWSLT-04 (travel dialogue domain). NIST MT-05. In this task, the bilingual training data comes from the FBIS corpus with 7.06M Chinese words and 9.15M English words. The trigram language model training data consists of English texts mostly derived from the English side of the UN corpus (catalog number LDC2004E12), which totally contains 81M English words. For the efficiency of minimum error rate training, we built our development set using sentences of length at most 50 characters fr</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>Edinburgh System Description for the</booktitle>
<contexts>
<context position="2467" citStr="Koehn et al., 2005" startWordPosition="372" endWordPosition="375">ly aligned to syntactic constituent boundary. which are common between two languages with very different orders. Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either. Flat model assigns constant probabilities for monotone order and non-monotone order. The two probabilities can be set to prefer monotone or non-monotone orientations depending on the language pairs. In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent. Lexicalized reordering model learns local orientations (monotone or non-monotone) with probabilities for each bilingual phrase from training data. During decoding, the model attempts to finding a Viterbi local orientation sequence. Performance gains have been reported for systems with lexicalized reordering model. However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, e.g. the data sparseness problem. Another</context>
<context position="7665" citStr="Koehn et al., 2005" startWordPosition="1199" endWordPosition="1202">and A2, an is its weight, and 4pLM(A1,A2) is the increment of the language model score of the two blocks according to their final order, ALM is its weight. For the lexical rule, applying it is assigned a probability Prl(A) Prl(A) = p(x|y)λ1 · p(y|x)λ2 · plex(x|y)λ3 ·plex(y|x)λ4 · exp(1)λ5 · exp(|x|)λs λLM pLM (x) (5) where p(·) are the phrase translation probabilities in both directions, plex(·) are the lexical translation probabilities in both directions, and exp(1) and exp(|x|) are the phrase penalty and word penalty, respectively. These features are very common in state-of-the-art systems (Koehn et al., 2005; Chiang, 2005) and As are weights of features. For the reordering model Q, we define it on the two consecutive blocks A1 and A2 and their order o ∈ {straight, inverted} Q = f(o, A1, A2) (6) Under this framework, different reordering models can be designed. In fact, we defined four reordering models in our experiments. The first one 522 is NONE, meaning no explicit reordering features at all. We set n to 1 for all different pairs of blocks and their orders. So the phrasal reordering is totally dependent on the language model. This model is obviously different from the monotone search, which do</context>
<context position="14172" citStr="Koehn et al. (2005)" startWordPosition="2361" endWordPosition="2364">id sparseness, but also integrate generalizations. It is very straight to use maximum entropy model to integrate features to predicate reorderings of blocks. Under the MaxEnt model, we have Q 1 2 exp(&amp; Bihi(o, A1, A2)) = pe(o|A,A) = �o exp(Ei Bihi(o, A1, A2)) (10) where the functions hi ∈ {0, 1} are model features and the Bi are weights of the model features which can be trained by different algorithms (Malouf, 2002). 3.1 Reordering Example Extraction Algorithm The input for the algorithm is a bilingual corpus with high-precision word alignments. We obtain the word alignments using the way of Koehn et al. (2005). After running GIZA++ (Och and Ney, source Figure 1: The bold dots are corners. The arrows from the corners are their links. Corner c1 is shared by block b1 and b2, which in turn are linked by the STRAIGHT links, bottomleft and topright of c1. Similarly, block b3 and b4 are linked by the INVERTED links, topleft and bottomright of c2. 2000) in both directions, we apply the “growdiag-final” refinement rule on the intersection alignments for each sentence pair. Before we introduce this algorithm, we introduce some formal definitions. The first one is block which is a pair of source and target co</context>
<context position="23569" citStr="Koehn et al., 2005" startWordPosition="3973" endWordPosition="3976">ight order, and 367K with the inverted order, from which 112K lexical features and 1.7M collocation features after deleting those with one occurrence were extracted. In the task of IWSLT-04, we obtained 79.5k reordering examples with the straight order, 9.3k with the inverted order, from which 16.9K lexical features and 89.6K collocation features after deleting those with one occurrence were extracted. Finally, we ran the MaxEnt toolkit by Zhang 4 to tune the feature weights. We set iteration number to 100 and Gaussian prior to 1 for avoiding overfitting. 4.3 Results We dropped unknown words (Koehn et al., 2005) of translations for both tasks before evaluating their BLEU scores. To be consistent with the official evaluation criterions of both tasks, casesensitive BLEU-4 scores were computed For the NIST MT-05 task and case-insensitive BLEU-4 scores were computed for the IWSLT-04 task 5. Experimental results on both tasks are shown in Table 1. Italic numbers refer to results for which the difference to the best result (indicated in bold) is not statistically significant. For all scores, we also show the 95% confidence intervals computed using Zhang’s significant tester (Zhang et al., 2004) which was m</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
<author>T Watanabe</author>
<author>E Sumita</author>
</authors>
<title>Reordering Constraints for Phrase-Based Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of CoLing 2004,</booktitle>
<pages>205--211</pages>
<location>Geneva,</location>
<contexts>
<context position="2035" citStr="Zens et al., 2004" startWordPosition="301" endWordPosition="304">y simple models to reorder phrases 1. One is distortion model (Och and Ney, 2004; Koehn et al., 2003) which penalizes translations according to their jump distance instead of their content. For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered. This model takes the risk of penalizing long distance jumps 1In this paper, we focus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary. which are common between two languages with very different orders. Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either. Flat model assigns constant probabilities for monotone order and non-monotone order. The two probabilities can be set to prefer monotone or non-monotone orientations depending on the language pairs. In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent. Lexicalized reordering model learns local orientations (monotone or non-mo</context>
<context position="3495" citStr="Zens et al., 2004" startWordPosition="527" endWordPosition="530">However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, e.g. the data sparseness problem. Another smart reordering model was proposed by Chiang (2005). In his approach, phrases are reorganized into hierarchical ones by reducing subphrases to variables. This template-based scheme not only captures the reorderings of phrases, but also integrates some phrasal generalizations into the global model. In this paper, we propose a novel solution for phrasal reordering. Here, under the ITG constraint (Wu, 1997; Zens et al., 2004), we need to consider just two kinds of reorderings, straight and inverted between two consecutive blocks. Therefore reordering can be modelled as a problem of 521 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 521–528, Sydney, July 2006. c�2006 Association for Computational Linguistics classification with only two labels, straight and inverted. In this paper, we build a maximum entropy based classification model as the reordering model. Different from lexicalized reordering, we do not use the whole block as reordering ev</context>
<context position="6553" citStr="Zens et al., 2004" startWordPosition="1012" endWordPosition="1015">odel. used to translate source phrase y into target phrase x and generate a block A. Later, the straight rule (1) merges two consecutive blocks into a single larger block in the straight order; while the inverted rule (2) merges them in the inverted order. These two merging rules will be used continuously until the whole source sentence is covered. When the translation is finished, a tree indicating the hierarchical segmentation of the source sentence is also produced. In the following, we will define the model in a straight way, not in the dynamic programming recursion way used by (Wu, 1996; Zens et al., 2004). We focus on defining the probabilities of different rules by separating different features (including the language model) out from the rule probabilities and organizing them in a log-linear form. This straight way makes it clear how rules are used and what they depend on. For the two merging rules straight and inverted, applying them on two consecutive blocks A1 and A2 is assigned a probability Prm(A) 17t S2 -&apos;� λLM Pr (A) = (4) pLM(A1,A2) where the Q is the reordering score of block A1 and A2, an is its weight, and 4pLM(A1,A2) is the increment of the language model score of the two blocks a</context>
</contexts>
<marker>Zens, Ney, Watanabe, Sumita, 2004</marker>
<rawString>R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Reordering Constraints for Phrase-Based Statistical Machine Translation. In Proceedings of CoLing 2004, Geneva, Switzerland, pp. 205-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Conference on Natural Language Learning (CoNLL2002).</booktitle>
<contexts>
<context position="13973" citStr="Malouf, 2002" startWordPosition="2332" endWordPosition="2333"> means we can not use blocks as direct reordering evidences. A good way to this problem is to use features of blocks as reordering evidences. Good features can not only capture reorderings, avoid sparseness, but also integrate generalizations. It is very straight to use maximum entropy model to integrate features to predicate reorderings of blocks. Under the MaxEnt model, we have Q 1 2 exp(&amp; Bihi(o, A1, A2)) = pe(o|A,A) = �o exp(Ei Bihi(o, A1, A2)) (10) where the functions hi ∈ {0, 1} are model features and the Bi are weights of the model features which can be trained by different algorithms (Malouf, 2002). 3.1 Reordering Example Extraction Algorithm The input for the algorithm is a bilingual corpus with high-precision word alignments. We obtain the word alignments using the way of Koehn et al. (2005). After running GIZA++ (Och and Ney, source Figure 1: The bold dots are corners. The arrows from the corners are their links. Corner c1 is shared by block b1 and b2, which in turn are linked by the STRAIGHT links, bottomleft and topright of c1. Similarly, block b3 and b4 are linked by the INVERTED links, topleft and bottomright of c2. 2000) in both directions, we apply the “growdiag-final” refineme</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the Sixth Conference on Natural Language Learning (CoNLL2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local phrase reordering models for statistical machine translation. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<booktitle>In Proceedings ofLREC 2004,</booktitle>
<pages>pages</pages>
<contexts>
<context position="24157" citStr="Zhang et al., 2004" startWordPosition="4065" endWordPosition="4068">nown words (Koehn et al., 2005) of translations for both tasks before evaluating their BLEU scores. To be consistent with the official evaluation criterions of both tasks, casesensitive BLEU-4 scores were computed For the NIST MT-05 task and case-insensitive BLEU-4 scores were computed for the IWSLT-04 task 5. Experimental results on both tasks are shown in Table 1. Italic numbers refer to results for which the difference to the best result (indicated in bold) is not statistically significant. For all scores, we also show the 95% confidence intervals computed using Zhang’s significant tester (Zhang et al., 2004) which was modified to conform to NIST’s 4See http://homepages.inf.ed.ac.uk/s0450736 /maxent toolkit.html. 5Note that the evaluation criterion of IWSLT-04 is not totally matched since we didn’t remove punctuation marks. 526 definition of the BLEU brevity penalty. We observe that if phrasal reordering is totally dependent on the language model (NONE) we get the worst performance, even worse than the monotone search. This indicates that our language models were not strong to discriminate between straight orders and inverted orders. The flat and distortion reordering models (Row 3 and 4) show sim</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In Proceedings ofLREC 2004, pages 2051– 2054.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>