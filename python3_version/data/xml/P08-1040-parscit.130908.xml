<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000160">
<title confidence="0.799498">
Sentence Simplification for Semantic Role Labeling
</title>
<author confidence="0.92384">
David Vickrey and Daphne Koller
</author>
<affiliation confidence="0.936311">
Stanford University
</affiliation>
<address confidence="0.859405">
Stanford, CA 94305-9010
</address>
<email confidence="0.997256">
{dvickrey,koller}@cs.stanford.edu
</email>
<sectionHeader confidence="0.994733" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99929428125">
Parse-tree paths are commonly used to incor-
porate information from syntactic parses into
NLP systems. These systems typically treat
the paths as atomic (or nearly atomic) features;
these features are quite sparse due to the im-
mense variety of syntactic expression. In this
paper, we propose a general method for learn-
ing how to iteratively simplify a sentence, thus
decomposing complicated syntax into small,
easy-to-process pieces. Our method applies
a series of hand-written transformation rules
corresponding to basic syntactic patterns —
for example, one rule “depassivizes” a sen-
tence. The model is parameterized by learned
weights specifying preferences for some rules
over others. After applying all possible trans-
formations to a sentence, we are left with a
set of candidate simplified sentences. We ap-
ply our simplification system to semantic role
labeling (SRL). As we do not have labeled ex-
amples of correct simplifications, we use la-
beled training data for the SRL task to jointly
learn both the weights of the simplification
model and of an SRL model, treating the sim-
plification as a hidden variable. By extracting
and labeling simplified sentences, this com-
bined simplification/SRL system better gener-
alizes across syntactic variation. It achieves
a statistically significant 1.2% F1 measure in-
crease over a strong baseline on the Conll-
2005 SRL task, attaining near-state-of-the-art
performance.
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999848142857143">
In semantic role labeling (SRL), given a sentence
containing a target verb, we want to label the se-
mantic arguments, or roles, of that verb. For the
verb “eat”, a correct labeling of “Tom ate a salad”
is {ARG0(Eater)=“Tom”, ARG1(Food)=“salad”}.
Current semantic role labeling systems rely pri-
marily on syntactic features in order to identify and
</bodyText>
<figure confidence="0.319744">
croutons
</figure>
<figureCaption confidence="0.998546">
Figure 1: Parse with path features for verb “eat”.
</figureCaption>
<bodyText confidence="0.999806137931034">
classify roles. Features derived from a syntactic
parse of the sentence have proven particularly useful
(Gildea &amp; Jurafsky, 2002). For example, the syntac-
tic subject of “give” is nearly always the Giver. Path
features allow systems to capture both general pat-
terns, e.g., that the ARG0 of a sentence tends to be
the subject of the sentence, and specific usage, e.g.,
that the ARG2 of “give” is often a post-verbal prepo-
sitional phrase headed by “to”. An example sentence
with extracted path features is shown in Figure 1.
A major problem with this approach is that the
path from an argument to the verb can be quite
complicated. In the sentence “He expected to re-
ceive a prize for winning,” the path from “win” to its
ARG0, “he”, involves the verbs “expect” and “re-
ceive” and the preposition “for.” The corresponding
path through the parse tree likely occurs a relatively
small number of times (or not at all) in the training
corpus. If the test set contained exactly the same
sentence but with “expected” replaced by “did not
expect” we would extract a different parse path fea-
ture; therefore, as far as the classifier is concerned,
the syntax of the two sentences is totally unrelated.
In this paper we learn a mapping from full, com-
plicated sentences to simplified sentences. For ex-
ample, given a correct parse, our system simplifies
the above sentence with target verb “win” to “He
won.” Our method combines hand-written syntac-
tic simplification rules with machine learning, which
</bodyText>
<figure confidence="0.998130294117647">
VP
to VP
eat NP
PP
NP
a salad with NP
PP(with)
croutons:
NP1 VP T
Tom wants S
S
NP VP
Tom: NP S(NP) VP S VP VP T
salad:
T
NP1
VP
</figure>
<page confidence="0.981506">
344
</page>
<note confidence="0.708924">
Proceedings of ACL-08: HLT, pages 344–352,
</note>
<page confidence="0.538029">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999965682926829">
determines which rules to prefer. We then use the
output of the simplification system as input to a SRL
system that is trained to label simplified sentences.
Compared to previous SRL models, our model
has several qualitative advantages. First, we be-
lieve that the simplification process, which repre-
sents the syntax as a set of local syntactic transfor-
mations, is more linguistically satisfying than using
the entire parse path as an atomic feature. Improving
the simplification process mainly involves adding
more linguistic knowledge in the form of simplifi-
cation rules. Second, labeling simple sentences is
much easier than labeling raw sentences and allows
us to generalize more effectively across sentences
with differing syntax. This is particularly important
for verbs with few labeled training instances; using
training examples as efficiently as possible can lead
to considerable gains in performance. Third, our
model is very effective at sharing information across
verbs, since most of our simplification rules apply
equally well regardless of the target verb.
A major difficulty in learning to simplify sen-
tences is that we do not have labeled data for this
task. To address this problem, we simultaneously
train our simplification system and the SRL system.
We treat the correct simplification as a hidden vari-
able, using labeled SRL data to guide us towards
“more useful” simplifications. Specifically, we train
our model discriminatively to predict the correct role
labeling assignment given an input sentence, treat-
ing the simplification as a hidden variable.
Applying our combined simplification/SRL
model to the Conll 2005 task, we show a significant
improvement over a strong baseline model. Our
model does best on verbs with little training data and
on instances with paths that are rare or have never
been seen before, matching our intuitions about the
strengths of the model. Our model outperforms all
but the best few Conll 2005 systems, each of which
uses multiple different automatically-generated
parses (which would likely improve our model).
</bodyText>
<sectionHeader confidence="0.937698" genericHeader="introduction">
2 Sentence Simplification
</sectionHeader>
<bodyText confidence="0.9990486">
We will begin with an example before describing our
model in detail. Figure 2 shows a series of transfor-
mations applied to the sentence “I was not given a
chance to eat,” along with the interpretation of each
transformation. Here, the target verb is “eat.”
</bodyText>
<figure confidence="0.834722777777778">
I was not given a chance to eat.
remove not
I was given a chance to eat.
Sam’s chance to eat has passed.
Someone gave me a chance to eat.
give -&gt; have
Sam has a chance to eat.
chance to X
Sam ate.
</figure>
<figureCaption confidence="0.99151">
Figure 3: Shared simplifica-
tion structure
</figureCaption>
<bodyText confidence="0.999944758620689">
There are several important things to note. First,
many of the steps do lose some semantic informa-
tion; clearly, having a chance to eat is not the same
as eating. However, since we are interested only in
labeling the core arguments of the verb (which in
this case is simply the Eater, “I”), it is not important
to maintain this information. Second, there is more
than one way to choose a set of rules which lead
to the desired final sentence “I ate.” For example,
we could have chosen to include a rule which went
directly from the second step to the fourth. In gen-
eral, the rules were designed to allow as much reuse
of rules as possible. Figure 3 shows the simplifica-
tion of “Sam’s chance to eat has passed” (again with
target verb “eat”); by simplifying both of these sen-
tences as “X had a chance to Y”, we are able to use
the same final rule in both cases.
Of course, there may be more than one way to
simplify a sentence for a given rule set; this ambigu-
ity is handled by learning which rules to prefer.
In this paper, we use simplification to mean some-
thing which is closer to canonicalization that sum-
marization. Thus, given an input sentence, our goal
is not to produce a single shortened sentence which
contains as much information from the original sen-
tence as possible. Rather, the goal is, for each
verb in the sentence, to produce a “simple” sentence
which is in a particular canonical form (described
below) relative to that verb.
</bodyText>
<sectionHeader confidence="0.983422" genericHeader="method">
3 Transformation Rules
</sectionHeader>
<bodyText confidence="0.990338166666667">
A transformation rule takes as input a parse tree and
produces as output a different, changed parse tree.
Since our goal is to produce a simplified version of
the sentence, the rules are designed to bring all sen-
tences toward the same common format.
A rule (see left side of Figure 4) consists of two
</bodyText>
<figure confidence="0.9917036">
depassivize
possessive
I had a chance to eat.
chance to X
I ate.
</figure>
<figureCaption confidence="0.952676">
Figure 2: Example
simplification
</figureCaption>
<page confidence="0.989951">
345
</page>
<figureCaption confidence="0.999298">
Figure 4: Rule for depassivizing a sentence
</figureCaption>
<bodyText confidence="0.999931794871795">
parts. The first is a “tree regular expression” which
is most simply viewed as a tree fragment with op-
tional constraints at each node. The rule assigns
numbers to each node which are referred to in the
second part of the rule. Formally, a rule node X
matches a parse-tree node A if: (1) All constraints of
node X (e.g., constituent category, head word, etc.)
are satisfied by node A. (2) For each child node Y
of X, there is a child B of A that matches Y; two
children of X cannot be matched to the same child
B. There are no other requirements. A can have
other children besides those matched, and leaves of
the rule pattern can match to internal nodes of the
parse (corresponding to entire phrases in the origi-
nal sentence). For example, the same rule is used to
simplify both “I had a chance to eat,” and “I had a
chance to eat a sandwich,” (into “I ate,” and “I ate
a sandwich,”). The insertion of the phrase “a sand-
wich” does not prevent the rule from matching.
The second part of the rule is a series of simple
steps that are applied to the matched nodes. For ex-
ample, one type of simple step applied to the pair of
nodes (X,Y) removes X from its current parent and
adds it as the final child of Y. Figure 4 shows the
depassivizing rule and the result of applying it to the
sentence “I was given a chance.” The transformation
steps are applied sequentially from top to bottom.
Note that any nodes not matched are unaffected by
the transformation; they remain where they are rel-
ative to their parents. For example, “chance” is not
matched by the rule, and thus remains as a child of
the VP headed by “give.”
There are two significant pieces of “machinery” in
our current rule set. The first is the idea of a floating
node, used for locating an argument within a subor-
dinate clause. For example, in the phrases “The cat
that ate the mouse”, “The seed that the mouse ate”,
and “The person we gave the gift to”, the modified
nouns (“cat”, “seed”, and “person”, respectively) all
</bodyText>
<table confidence="0.9938502">
Rule Category # Original Simplified
Sentence normalization 24 Thursday, I slept. I slept Thursday.
Sentence extraction 4 I said he slept. He slept.
Passive 5 I was hit by a car. A car hit me.
Misc Collapsing/Rewriting 20 John, a lawyer, ... John is a lawyer.
Conjunctions 8 I ate and slept. I ate.
Verb Collapsing/Rewriting 14 I must eat. I eat.
Verb Raising/Control (basic) 17 I want to eat. I eat.
Verb RC (ADJP/ADVP) 6 I am likely to eat. I eat.
Verb RC (Noun) 7 I have a chance to I eat.
</table>
<figureCaption confidence="0.867681714285714">
eat.
Modified nouns 8 The food I ate ... Float(The food) I
ate.
Floating nodes 5 Float(The food) I I ate the food.
ate.
Inverted sentences 7 Nor will I eat. I will eat.
Questions 7 Will I eat? I will eat.
Possessive 7 John’s chance to John has a
eat... chance to eat.
Verb acting as PP/NP 7 Including tax, the The total
includes tax.
total...
“Make” rewrites 8 Salt makes food Food is tasty.
tasty.
</figureCaption>
<tableCaption confidence="0.998609">
Table 1: Rule categories with sample simplifications.
Target verbs are underlined.
</tableCaption>
<bodyText confidence="0.999908076923077">
should be placed in different positions in the subor-
dinate clauses (subject, direct object, and object of
“to”) to produce the phrases “The cat ate the mouse,”
“The mouse ate the seed”, and “We gave the gift to
the person.” We handle these phrases by placing a
floating node in the subordinate clause which points
to the argument; other rules try to place the floating
node into each possible position in the sentence.
The second construct is a system for keeping track
of whether a sentence has a subject, and if so, what
it is. A subset of our rule set normalizes the input
sentence by moving modifiers after the verb, leaving
either a single phrase (the subject) or nothing before
the verb. For example, the sentence “Before leaving,
I ate a sandwich,” is rewritten as “I ate a sandwich
before leaving.” In many cases, keeping track of the
presence or absence of a subject greatly reduces the
set of possible simplifications.
Altogether, we currently have 154 (mostly unlex-
icalized) rules. Our general approach was to write
very conservative rules, i.e., avoid making rules
with low precision, as these can quickly lead to a
large blowup in the number of generated simple sen-
tences. Table 1 shows a summary of our rule-set,
grouped by type. Note that each row lists only one
possible sentence and simplification rule from that
</bodyText>
<figure confidence="0.995376208333333">
Rule
S-1
Original Transformed
S S-1
NP VP
NP-7
VP-4
NP-2 VP-3
VB*-6
be
VP-4
VBN-5
I VBD
was
VP
VBN NP
[Someone] VB-5 NP-2 NP
give I chance
Replace 3 with 4
Create new node 7 – [Someone]
Substitute 7 for 2
Add 2 after 5
Set category of 5 to VB
given chance
</figure>
<page confidence="0.848802">
346
</page>
<figureCaption confidence="0.999579">
Figure 5: Simple sentence constraints for “eat”
</figureCaption>
<bodyText confidence="0.99712075">
category; many of the categories handle a variety of
syntax patterns. The two examples without target
verbs are helper transformations; in more complex
sentences, they can enable further simplifications.
Another thing to note is that we use the terms Rais-
ing/Control (RC) very loosely to mean situations
where the subject of the target verb is displaced, ap-
pearing as the subject of another verb (see table).
Our rule set was developed by analyzing perfor-
mance and coverage on the PropBank WSJ training
set; neither the development set nor (of course) the
test set were used during rule creation.
</bodyText>
<sectionHeader confidence="0.961051" genericHeader="method">
4 Simple Sentence Production
</sectionHeader>
<bodyText confidence="0.999941114285714">
We now describe how to take a set of rules and pro-
duce a set of candidate simple sentences. At a high
level, the algorithm is very simple. We maintain a
set of derived parses S which is initialized to con-
tain only the original, untransformed parse. One it-
eration of the algorithm consists of applying every
possible matching transformation rule to every parse
in S, and adding all resulting parses to S. With care-
fully designed rules, repeated iterations are guaran-
teed to converge; that is, we eventually arrive at a set
S� such that if we apply an iteration of rule applica-
tion to S, no new parses will be added. Note that
we simplify the whole sentence without respect to a
particular verb. Thus, this process only needs to be
done once per sentence (not once per verb).
To label arguments of a particular target verb, we
remove any parse from our set which does not match
one of the two templates in Figure 5 (for verb “eat”).
These select simple sentences that have all non-
subject modifiers moved to the predicate and “eat”
as the main verb. Note that the constraint VB* indi-
cates any terminal verb category (e.g., VBN, VBD,
etc.) A parse that matches one of these templates
is called a valid simple sentence; this is exactly
the canonicalized version of the sentence which our
simplification rules are designed to produce.
This procedure is quite expensive; we have to
copy the entire parse tree at each step, and in gen-
eral, this procedure could generate an exponential
number of transformed parses. The first issue can be
solved, and the second alleviated, using a dynamic-
programming data structure similar to the one used
to store parse forests (as in a chart parser). This data
structure is not essential for exposition; we delay
discussion until Section 7.
</bodyText>
<sectionHeader confidence="0.923936" genericHeader="method">
5 Labeling Simple Sentences
</sectionHeader>
<bodyText confidence="0.994882933333333">
For a particular sentence/target verb pair s, v, the
output from the previous section is a set Ssv =
{tsv
i }i of valid simple sentences. Although labeling
a simple sentence is easier than labeling the original
sentence, there are still many choices to be made.
There is one key assumption that greatly reduces the
search space: in a simple sentence, only the subject
(if present) and direct modifiers of the target verb
can be arguments of that verb.
On the training set, we now extract a set of role
patterns Gv = {gv j }j for each verb v. For exam-
ple, a common role pattern for “give” is that of “I
gave him a sandwich”. We represent this pattern
as ggive
</bodyText>
<equation confidence="0.9875715">
1 = {ARG0 = Subject NP, ARG1 =
Postverb NP2, ARG2 = Postverb NP1}. Note
</equation>
<bodyText confidence="0.996271428571429">
that this is one atomic pattern; thus, we are keep-
ing track not just of occurrences of particular roles
in particular places in the simple sentence, but also
how those roles co-occur with other roles.
For a particular simple sentence tsv
i , we apply
all extracted role patterns gv j to tsv
i , obtaining a set
of possible role labelings. We call a simple sen-
tence/role labeling pair a simple labeling and denote
the set of candidate simple labelings Csv = {csvk }k.
Note that a given pair tsv
i , gvj may generate more
than one simple labeling, if there is more than one
way to assign the elements of gv j to constituents in
tsv
i . Also, for a sentence s there may be several
simple labelings that lead to the same role labeling.
In particular, there may be several simple labelings
which assign the correct labels to all constituents;
we denote this set Ksv C Csv.
</bodyText>
<sectionHeader confidence="0.999111" genericHeader="method">
6 Probabilistic Model
</sectionHeader>
<bodyText confidence="0.9986025">
We now define our probabilistic model. Given a
(possibly large) set of candidate simple labelings
Csv, we need to select a correct one. We assign
a score to each candidate based on its features:
</bodyText>
<figure confidence="0.562143625">
S-1
NP or S VP
VB*
eat eat
#children(S-1) = 2 #children(S-1) = 1
S-1
VP
VB*
</figure>
<page confidence="0.660697">
347
</page>
<equation confidence="0.884006909090909">
Rule = Depassivize
Pattern = {ARG0 = Subj NP, ARG1 = PV NP2, ARG2 = PV NP1}
Role = ARG0, Head Word = John
Role = ARG1, Head Word = sandwich
Role = ARG2, Head Word = I
Role = ARG0, Category = NP
Role = ARG1, Category = NP
Role = ARG2, Category = NP
Role = ARG0, Position = Subject NP
Role = ARG1, Position = Postverb NP2
Role = ARG2, Position = Postverb NP1
</equation>
<figureCaption confidence="0.971232">
Figure 6: Features for “John gave me a sandwich.”
</figureCaption>
<bodyText confidence="0.999979176470588">
which rules were used to obtain the simple sentence,
which role pattern was used, and features about the
assignment of constituents to roles. A log-linear
model then assigns probability to each simple label-
ing equal to the normalized exponential of the score.
The first type of feature is which rules were used
to obtain the simple sentence. These features are in-
dicator functions for each possible rule. Thus, we do
not currently learn anything about interactions be-
tween different rules. The second type of feature is
an indicator function of the role pattern used to gen-
erate the labeling. This allows us to learn that “give”
has a preference for the labeling {ARG0 = Subject
NP, ARG1 = Postverb NP2, ARG2 = Postverb NP1}.
Our final features are analogous to those used in se-
mantic role labeling, but greatly simplified due to
our use of simple sentences: head word of the con-
stituent; category (i.e., constituent label); and posi-
tion in the simple sentence. Each of these features
is combined with the role assignment, so that each
feature indicates a preference for a particular role
assignment (i.e., for “give”, head word “sandwich”
tends to be ARG1). For each feature, we have a
verb-specific and a verb-independent version, allow-
ing sharing across verbs while still permitting dif-
ferent verbs to learn different preferences. The set
of extracted features for the sentence “I was given
a sandwich by John” with simplification “John gave
me a sandwich” is shown in Figure 6. We omit verb-
specific features to save space . Note that we “stem”
all pronouns (including possessive pronouns).
For each candidate simple labeling csvk we extract
a vector of features fsvk as described above. We now
define the probability of a simple labeling csvk with
</bodyText>
<equation confidence="0.979392">
E k0ewT fsv
k0 .
</equation>
<bodyText confidence="0.980322">
Our goal is to maximize the total probability as-
signed to any correct simple labeling; therefore, for
each sentence/verb pair (s, v), we want to increase
</bodyText>
<equation confidence="0.98946525">
P
k ). This expression treats the simple
csv
k ∈Ksv P(csv
</equation>
<bodyText confidence="0.999938">
labeling (consisting of a simple sentence and a role
assignment) as a hidden variable that is summed out.
Taking the log, summing across all sentence/verb
pairs, and adding L2 regularization on the weights,
we have our final objective F(w):
</bodyText>
<equation confidence="0.995492285714286">
⎛ P ⎞
k
csv
k ∈Ksv ewT fsv wT w
⎝log wT fsv 22
P⎠�
csvk0∈Csv e k0σ
</equation>
<bodyText confidence="0.999857">
We train our model by optimizing the objective
using standard methods, specifically BFGS. Due to
the summation over the hidden variable representing
the choice of simple sentence (not observed in the
training data), our objective is not convex. Thus,
we are not guaranteed to find a global optimum; in
practice we have gotten good results using the de-
fault initialization of setting all weights to 0.
Consider the derivative of the likelihood compo-
nent with respect to a single weight wl:
</bodyText>
<table confidence="0.724261375">
X k ) X� fsv
csv fsv csv k (l)P(csv k )
k ∈Ksv k (l) P (csv k ∈Csv
P
P(
csv ∈Ksv
k0
csvk0 )
</table>
<bodyText confidence="0.932639555555556">
where fsv
k (l) denotes the lth component of fsv
k .
This formula is positive when the expected value of
the lth feature is higher on the set of correct simple
labelings Ksv than on the set of all simple labelings
Csv. Thus, the optimization procedure will tend to
be self-reinforcing, increasing the score of correct
simple labelings which already have a high score.
</bodyText>
<sectionHeader confidence="0.967971" genericHeader="method">
7 Simplification Data Structure
</sectionHeader>
<bodyText confidence="0.9999679375">
Our representation of the set of possible simplifi-
cations of a sentence addresses two computational
bottlenecks. The first is the need to repeatedly copy
large chunks of the sentence. For example, if we are
depassivizing a sentence, we can avoid copying the
subject and object of the original sentence by simply
referring back to them in the depassivized version.
At worst, we only need to add one node for each
numbered node in the transformation rule. The sec-
ond issue is the possible exponential blowup of the
number of generated sentences. Consider “I want
to eat and I want to drink and I want to play and
... ” Each subsentence can be simplified, yielding
two possibilities for each subsentence. The number
of simplifications of the entire sentence is then ex-
ponential in the length of the sentence. However,
</bodyText>
<figure confidence="0.7550395">
k
respect to a weight vector w P(csv k ) = ewT fsv
X
s,v
</figure>
<page confidence="0.808271">
348
</page>
<figureCaption confidence="0.998536666666667">
Figure 7: Data structure after applying the depassivize
rule to “I was given (a) chance.” Circular nodes are OR-
nodes, rectangular nodes are AND-nodes.
</figureCaption>
<bodyText confidence="0.98037355319149">
we can store these simplifications compactly as a set
of independent decisions, “I {want to eat OR eat}
and I {want to drink OR drink} and ... ”
Both issues can be addressed by representing the
set of simplifications using an AND-OR tree, a gen-
eral data structure also used to store parse forests
such as those produced by a chart parser. In our case,
the AND nodes are similar to constituent nodes in a
parse tree – each has a category (e.g. NP) and (if it
is a leaf) a word (e.g. “chance”), but instead of hav-
ing a list of child constituents, it instead has a list of
child OR nodes. Each OR node has one or more con-
stituent children that correspond to the different op-
tions at this point in the tree. Figure 7 shows the re-
sulting AND-OR tree after applying the depassivize
rule to the original parse of “I was given a chance.”
Because this AND-OR tree represents only two dif-
ferent parses, the original parse and the depassivized
version, only one OR node in the tree has more than
one child – the root node, which has two choices,
one for each parse. However, the AND nodes imme-
diately above “I” and “chance” each have more than
one OR-node parent, since they are shared by the
original and depassivized parses1. To extract a parse
from this data structure, we apply the following re-
cursive algorithm: starting at the root OR node, each
time we reach an OR node, we choose and recurse
on exactly one of its children; each time we reach
an AND node, we recurse on all of its children. In
Figure 7, we have only one choice: if we go left at
the root, we generate the original parse; otherwise,
we generate the depassivized version.
Unfortunately, it is difficult to find the optimal
AND-OR tree. We use a greedy but smart proce-
1In this particular example, both of these nodes are leaves,
but in general shared nodes can be entire tree fragments
dure to try to produce a small tree. We omit details
for lack of space. Using our rule set, the compact
representation is usually (but not always) small.
For our compact representation to be useful, we
need to be able to optimize our objective without ex-
panding all possible simple sentences. A relatively
straight-forward extension of the inside-outside al-
gorithm for chart-parses allows us to learn and per-
form inference in our compact representation (a sim-
ilar algorithm is presented in (Geman &amp; Johnson,
2002)). We omit details for lack of space.
</bodyText>
<sectionHeader confidence="0.996042" genericHeader="evaluation">
8 Experiments
</sectionHeader>
<bodyText confidence="0.999983382352941">
We evaluated our system using the setup of the Conll
2005 semantic role labeling task.2 Thus, we trained
on Sections 2-21 of PropBank and used Section 24
as development data. Our test data includes both the
selected portion of Section 23 of PropBank, plus the
extra data on the Brown corpus. We used the Char-
niak parses provided by the Conll distribution.
We compared to a strong Baseline SRL system
that learns a logistic regression model using the fea-
tures of Pradhan et al. (2005). It has two stages.
The first filters out nodes that are unlikely to be ar-
guments. The second stage labels each remaining
node either as a particular role (e.g. “ARGO”) or as a
non-argument. Note that the baseline feature set in-
cludes a feature corresponding to the subcategoriza-
tion of the verb (specifically, the sequence of nonter-
minals which are children of the predicate’s parent
node). Thus, Baseline does have access to some-
thing similar to our model’s role pattern feature, al-
though the Baseline subcategorization feature only
includes post-verbal modifiers and is generally much
noisier because it operates on the original sentence.
Our Transforms model takes as input the Char-
niak parses supplied by the Conll release, and labels
every node with Core arguments (ARG0-ARG5).
Our rule set does not currently handle either ref-
erent arguments (such as “who” in “The man who
ate ... ”) or non-core arguments (such as ARGM-
TMP). For these arguments, we simply filled in us-
ing our baseline system (specifically, any non-core
argument which did not overlap an argument pre-
dicted by our model was added to the labeling).
Also, on some sentences, our system did not gen-
erate any predictions because no valid simple sen-
</bodyText>
<footnote confidence="0.905191">
2http://www.lsi.upc.es/ srlconll/home.html
</footnote>
<figure confidence="0.995811666666667">
VBN(given)
NP(chance)
VBD(was)
NP(I)
S
VP
VP
ROOT
NP([Someone])
VBD(gave)
S
VP
</figure>
<page confidence="0.993299">
349
</page>
<table confidence="0.9966851">
Model Dev Test Test Test
WSJ Brown WSJ+Br
Baseline 74.7 76.9 64.7 75.3
Transforms 75.6 77.4 66.8 76.0
Combined 76.0 78.0 66.4 76.5
Punyakanok 77.35 79.44 67.75 77.92
Model Test WSJ
Baseline 87.6
Transforms 88.2
Combined 88.5
</table>
<tableCaption confidence="0.970889">
Table 3: F1 Measure using gold parses
Table 2: F1 Measure using Charniak parses
</tableCaption>
<bodyText confidence="0.999804023809524">
tences were produced by the simplification system.
Again, we used the baseline to fill in predictions (for
all arguments) for these sentences.
Baseline and Transforms were regularized using
a Gaussian prior; for both models, Q2 = 1.0 gave
the best results on the development set.
For generating role predictions from our model,
we have two reasonable options: use the labeling
given by the single highest scoring simple labeling;
or compute the distribution over predictions for each
node by summing over all simple labelings. The lat-
ter method worked slightly better, particularly when
combined with the baseline model as described be-
low, so all reported results use this method.
We also evaluated a hybrid model that combines
the Baseline with our simplification model. For a
given sentence/verb pair (s, v), we find the set of
constituents Nsv that made it past the first (filtering)
stage of Baseline. For each candidate simple sen-
tence/labeling pair csvk = (tsv
i , gvj ) proposed by our
model, we check to see which of the constituents
in Nsv are already present in our simple sentence
tsv
i . Any constituents that are not present are then as-
signed a probability distribution over possible roles
according to Baseline. Thus, we fall back Base-
line whenever the current simple sentence does not
have an “opinion” about the role of a particular con-
stituent. The Combined model is thus able to cor-
rectly label sentences when the simplification pro-
cess drops some of the arguments (generally due to
unusual syntax). Each of the two components was
trained separately and combined only at testing time.
Table 2 shows results of these three systems on
the Conll-2005 task, plus the top-performing system
(Punyakanok et al., 2005) for reference. Baseline al-
ready achieves good performance on this task, plac-
ing at about 75th percentile among evaluated sys-
tems. Our Transforms model outperforms Baseline
on all sets. Finally, our Combined model improves
over Transforms on all but the test Brown corpus,
achieving a statistically significant increase over the
Baseline system (according to confidence intervals
calculated for the Conll-2005 results).
The Combined model still does not achieve the
performance levels of the top several systems. How-
ever, these systems all use information from multi-
ple parses, allowing them to fix many errors caused
by incorrect parses. We return to this issue in Sec-
tion 10. Indeed, as shown in Table 3, performance
on gold standard parses is (as expected) much bet-
ter than on automatically generated parses, for all
systems. Importantly, the Combined model again
achieves a significant improvement over Baseline.
We expect that by labeling simple sentences, our
model will generalize well even on verbs with a
small number of training examples. Figure 8 shows
F1 measure on the WSJ test set as a function of train-
ing set size. Indeed, both the Transforms model and
the Combined model significantly outperform the
Baseline model when there are fewer than 20 train-
ing examples for the verb. While the Baseline model
has higher accuracy than the Transforms model for
verbs with a very large number of training examples,
the Combined model is at or above both of the other
models in all but the rightmost bucket, suggesting
that it gets the best of both worlds.
We also found, as expected, that our model im-
proved on sentences with very long parse paths. For
example, in the sentence “Big investment banks re-
fused to step up to the plate to support the beleagured
floor traders by buying blocks of stock, traders say,” the
parse path from “buy” to its ARG0, “Big investment
banks,” is quite long. The Transforms model cor-
rectly labels the arguments of “buy”, while the Base-
line system misses the ARG0.
To understand the importance of different types of
rules, we performed an ablation analysis. For each
major rule category in Figure 1, we deleted those
rules from the rule set, retrained, and evaluated us-
ing the Combined model. To avoid parse-related
issues, we trained and evaluated on gold-standard
parses. Most important were rules relating to (ba-
</bodyText>
<page confidence="0.994163">
350
</page>
<figureCaption confidence="0.9896274">
Figure 8: F1 Measure on the WSJ test set as a function of
training set size. Each bucket on the X-axis corresponds
to a group of verbs for which the number of training ex-
amples fell into the appropriate range; the value is the
average performance for verbs in that bucket.
</figureCaption>
<bodyText confidence="0.9999167">
sic) verb raising/control, “make” rewrites, modified
nouns, and passive constructions. Each of these rule
categories when removed lowered the F1 score by
approximately .4%. In constrast, removing rules
for non-basic control, possessives, and inverted sen-
tences caused a negligible reduction in performance.
This may be because the relevant syntactic structures
occur rarely; because Baseline does well on those
constructs; or because the simplification model has
trouble learning when to apply these rules.
</bodyText>
<sectionHeader confidence="0.99983" genericHeader="related work">
9 Related Work
</sectionHeader>
<bodyText confidence="0.999906176470588">
One area of current research which has similarities
with this work is on Lexical Functional Grammars
(LFGs). Both approaches attempt to abstract away
from the surface level syntax of the sentence (e.g.,
the XLE system3). The most obvious difference be-
tween the approaches is that we use SRL data to train
our system, avoiding the need to have labeled data
specific to our simplification scheme.
There have been a number of works which model
verb subcategorization. Approaches include incor-
porating a subcategorization feature (Gildea &amp; Ju-
rafsky, 2002; Xue &amp; Palmer, 2004), such as the one
used in our baseline; and building a model which
jointly classifies all arguments of a verb (Toutanova
et al., 2005). Our method differs from past work in
that it extracts its role pattern feature from the sim-
plified sentence. As a result, the feature is less noisy
</bodyText>
<footnote confidence="0.789323">
3http://www2.parc.com/isl/groups/nltt/xle/
</footnote>
<bodyText confidence="0.99979675862069">
and generalizes better across syntactic variation than
a feature extracted from the original sentence.
Another group of related work focuses on summa-
rizing sentences through a series of deletions (Jing,
2000; Dorr et al., 2003; Galley &amp; McKeown, 2007).
In particular, the latter two works iteratively simplify
the sentence by deleting a phrase at a time. We differ
from these works in several important ways. First,
our transformation language is not context-free; it
can reorder constituents and then apply transforma-
tion rules to the reordered sentence. Second, we are
focusing on a somewhat different task; these works
are interested in obtaining a single summary of each
sentence which maintains all “essential” informa-
tion, while in our work we produce a simplification
that may lose semantic content, but aims to contain
all arguments of a verb. Finally, training our model
on SRL data allows us to avoid the relative scarcity
of parallel simplification corpora and the issue of de-
termining what is “essential” in a sentence.
Another area of related work in the semantic role
labeling literature is that on tree kernels (Moschitti,
2004; Zhang et al., 2007). Like our method, tree ker-
nels decompose the parse path into smaller pieces
for classification. Our model can generalize better
across verbs because it first simplifies, then classifies
based on the simplified sentence. Also, through it-
erative simplifications we can discover structure that
is not immediately apparent in the original parse.
</bodyText>
<sectionHeader confidence="0.998861" genericHeader="conclusions">
10 Future Work
</sectionHeader>
<bodyText confidence="0.999983">
There are a number of improvements that could be
made to the current simplification system, includ-
ing augmenting the rule set to handle more con-
structions and doing further sentence normaliza-
tions, e.g., identifying whether a direct object exists.
Another interesting extension involves incorporating
parser uncertainty into the model; in particular, our
simplification system is capable of seamlessly ac-
cepting a parse forest as input.
There are a variety of other tasks for which sen-
tence simplification might be useful, including sum-
marization, information retrieval, information ex-
traction, machine translation and semantic entail-
ment. In each area, we could either use the sim-
plification system as learned on SRL data, or retrain
the simplification model to maximize performance
on the particular task.
</bodyText>
<figure confidence="0.998123625">
F1 Measure
0.85
0.75
0.65
0.9
0.8
0.7
0.6
0-4
5-9
10-19
F1 vs. Verb Training Examples
20-49
Training Examples
50-99
100-199
200-499
500-999
1000-1999
2000-4999
Combined
Transforms
Baseline
5000+
</figure>
<page confidence="0.992668">
351
</page>
<sectionHeader confidence="0.993045" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99881425">
Dorr, B., Zajic, D., &amp; Schwartz, R. (2003). Hedge:
A parse-and-trim approach to headline genera-
tion. Proceedings of the HLT-NAACL Text Sum-
marization Workshop and Document Understand-
ing Conference.
Galley, M., &amp; McKeown, K. (2007). Lexicalized
markov grammars for sentence compression. Pro-
ceedings of NAACL-HLT.
Geman, S., &amp; Johnson, M. (2002). Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. Proceedings ofACL.
Gildea, D., &amp; Jurafsky, D. (2002). Automatic label-
ing of semantic roles. Computational Linguistics.
Jing, H. (2000). Sentence reduction for automatic
text summarization. Proceedings ofApplied NLP.
Moschitti, A. (2004). A study on convolution ker-
nels for shallow semantic parsing. Proceedings of
ACL.
Pradhan, S., Hacioglu, K., Krugler, V., Ward, W.,
Martin, J. H., &amp; Jurafsky, D. (2005). Support vec-
tor learning for semantic argument classification.
Machine Learning, 60, 11–39.
Punyakanok, V., Koomen, P., Roth, D., &amp; Yih, W.
(2005). Generalized inference with multiple se-
mantic role labeling systems. Proceedings of
CoNLL.
Toutanova, K., Haghighi, A., &amp; Manning, C. (2005).
Joint learning improves semantic role labeling.
Proceedings of ACL, 589–596.
Xue, N., &amp; Palmer, M. (2004). Calibrating fea-
tures for semantic role labeling. Proceedings of
EMNLP.
Zhang, M., Che, W., Aw, A., Tan, C. L., Zhou, G.,
Liu, T., &amp; Li, S. (2007). A grammar-driven convo-
lution tree kernel for semantic role classification.
Proceedings of ACL.
</reference>
<page confidence="0.99826">
352
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.846494">
<title confidence="0.999838">Sentence Simplification for Semantic Role Labeling</title>
<author confidence="0.985657">Vickrey Koller</author>
<affiliation confidence="0.999994">Stanford University</affiliation>
<address confidence="0.999851">Stanford, CA 94305-9010</address>
<abstract confidence="0.992984121212121">Parse-tree paths are commonly used to incorporate information from syntactic parses into NLP systems. These systems typically treat the paths as atomic (or nearly atomic) features; these features are quite sparse due to the immense variety of syntactic expression. In this paper, we propose a general method for learning how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces. Our method applies a series of hand-written transformation rules corresponding to basic syntactic patterns — for example, one rule “depassivizes” a sen- The model is parameterized by weights specifying preferences for some rules over others. After applying all possible transformations to a sentence, we are left with a set of candidate simplified sentences. We apply our simplification system to semantic role labeling (SRL). As we do not have labeled examples of correct simplifications, we use labeled training data for the SRL task to jointly learn both the weights of the simplification model and of an SRL model, treating the simplification as a hidden variable. By extracting and labeling simplified sentences, this combined simplification/SRL system better generalizes across syntactic variation. It achieves a statistically significant 1.2% F1 measure increase over a strong baseline on the Conll- 2005 SRL task, attaining near-state-of-the-art performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Dorr</author>
<author>D Zajic</author>
<author>R Schwartz</author>
</authors>
<title>Hedge: A parse-and-trim approach to headline generation.</title>
<date>2003</date>
<booktitle>Proceedings of the HLT-NAACL Text Summarization Workshop and Document Understanding Conference.</booktitle>
<contexts>
<context position="32413" citStr="Dorr et al., 2003" startWordPosition="5595" endWordPosition="5598">gorization feature (Gildea &amp; Jurafsky, 2002; Xue &amp; Palmer, 2004), such as the one used in our baseline; and building a model which jointly classifies all arguments of a verb (Toutanova et al., 2005). Our method differs from past work in that it extracts its role pattern feature from the simplified sentence. As a result, the feature is less noisy 3http://www2.parc.com/isl/groups/nltt/xle/ and generalizes better across syntactic variation than a feature extracted from the original sentence. Another group of related work focuses on summarizing sentences through a series of deletions (Jing, 2000; Dorr et al., 2003; Galley &amp; McKeown, 2007). In particular, the latter two works iteratively simplify the sentence by deleting a phrase at a time. We differ from these works in several important ways. First, our transformation language is not context-free; it can reorder constituents and then apply transformation rules to the reordered sentence. Second, we are focusing on a somewhat different task; these works are interested in obtaining a single summary of each sentence which maintains all “essential” information, while in our work we produce a simplification that may lose semantic content, but aims to contain</context>
</contexts>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>Dorr, B., Zajic, D., &amp; Schwartz, R. (2003). Hedge: A parse-and-trim approach to headline generation. Proceedings of the HLT-NAACL Text Summarization Workshop and Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
</authors>
<title>Lexicalized markov grammars for sentence compression.</title>
<date>2007</date>
<booktitle>Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="32438" citStr="Galley &amp; McKeown, 2007" startWordPosition="5599" endWordPosition="5602">(Gildea &amp; Jurafsky, 2002; Xue &amp; Palmer, 2004), such as the one used in our baseline; and building a model which jointly classifies all arguments of a verb (Toutanova et al., 2005). Our method differs from past work in that it extracts its role pattern feature from the simplified sentence. As a result, the feature is less noisy 3http://www2.parc.com/isl/groups/nltt/xle/ and generalizes better across syntactic variation than a feature extracted from the original sentence. Another group of related work focuses on summarizing sentences through a series of deletions (Jing, 2000; Dorr et al., 2003; Galley &amp; McKeown, 2007). In particular, the latter two works iteratively simplify the sentence by deleting a phrase at a time. We differ from these works in several important ways. First, our transformation language is not context-free; it can reorder constituents and then apply transformation rules to the reordered sentence. Second, we are focusing on a somewhat different task; these works are interested in obtaining a single summary of each sentence which maintains all “essential” information, while in our work we produce a simplification that may lose semantic content, but aims to contain all arguments of a verb.</context>
</contexts>
<marker>Galley, McKeown, 2007</marker>
<rawString>Galley, M., &amp; McKeown, K. (2007). Lexicalized markov grammars for sentence compression. Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>M Johnson</author>
</authors>
<title>Dynamic programming for parsing and estimation of stochastic unification-based grammars.</title>
<date>2002</date>
<booktitle>Proceedings ofACL.</booktitle>
<contexts>
<context position="24226" citStr="Geman &amp; Johnson, 2002" startWordPosition="4249" endWordPosition="4252">particular example, both of these nodes are leaves, but in general shared nodes can be entire tree fragments dure to try to produce a small tree. We omit details for lack of space. Using our rule set, the compact representation is usually (but not always) small. For our compact representation to be useful, we need to be able to optimize our objective without expanding all possible simple sentences. A relatively straight-forward extension of the inside-outside algorithm for chart-parses allows us to learn and perform inference in our compact representation (a similar algorithm is presented in (Geman &amp; Johnson, 2002)). We omit details for lack of space. 8 Experiments We evaluated our system using the setup of the Conll 2005 semantic role labeling task.2 Thus, we trained on Sections 2-21 of PropBank and used Section 24 as development data. Our test data includes both the selected portion of Section 23 of PropBank, plus the extra data on the Brown corpus. We used the Charniak parses provided by the Conll distribution. We compared to a strong Baseline SRL system that learns a logistic regression model using the features of Pradhan et al. (2005). It has two stages. The first filters out nodes that are unlikel</context>
</contexts>
<marker>Geman, Johnson, 2002</marker>
<rawString>Geman, S., &amp; Johnson, M. (2002). Dynamic programming for parsing and estimation of stochastic unification-based grammars. Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="2131" citStr="Gildea &amp; Jurafsky, 2002" startWordPosition="318" endWordPosition="321">e on the Conll2005 SRL task, attaining near-state-of-the-art performance. 1 Introduction In semantic role labeling (SRL), given a sentence containing a target verb, we want to label the semantic arguments, or roles, of that verb. For the verb “eat”, a correct labeling of “Tom ate a salad” is {ARG0(Eater)=“Tom”, ARG1(Food)=“salad”}. Current semantic role labeling systems rely primarily on syntactic features in order to identify and croutons Figure 1: Parse with path features for verb “eat”. classify roles. Features derived from a syntactic parse of the sentence have proven particularly useful (Gildea &amp; Jurafsky, 2002). For example, the syntactic subject of “give” is nearly always the Giver. Path features allow systems to capture both general patterns, e.g., that the ARG0 of a sentence tends to be the subject of the sentence, and specific usage, e.g., that the ARG2 of “give” is often a post-verbal prepositional phrase headed by “to”. An example sentence with extracted path features is shown in Figure 1. A major problem with this approach is that the path from an argument to the verb can be quite complicated. In the sentence “He expected to receive a prize for winning,” the path from “win” to its ARG0, “he”,</context>
<context position="31839" citStr="Gildea &amp; Jurafsky, 2002" startWordPosition="5503" endWordPosition="5507">ion model has trouble learning when to apply these rules. 9 Related Work One area of current research which has similarities with this work is on Lexical Functional Grammars (LFGs). Both approaches attempt to abstract away from the surface level syntax of the sentence (e.g., the XLE system3). The most obvious difference between the approaches is that we use SRL data to train our system, avoiding the need to have labeled data specific to our simplification scheme. There have been a number of works which model verb subcategorization. Approaches include incorporating a subcategorization feature (Gildea &amp; Jurafsky, 2002; Xue &amp; Palmer, 2004), such as the one used in our baseline; and building a model which jointly classifies all arguments of a verb (Toutanova et al., 2005). Our method differs from past work in that it extracts its role pattern feature from the simplified sentence. As a result, the feature is less noisy 3http://www2.parc.com/isl/groups/nltt/xle/ and generalizes better across syntactic variation than a feature extracted from the original sentence. Another group of related work focuses on summarizing sentences through a series of deletions (Jing, 2000; Dorr et al., 2003; Galley &amp; McKeown, 2007).</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, D., &amp; Jurafsky, D. (2002). Automatic labeling of semantic roles. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>Proceedings ofApplied NLP.</booktitle>
<contexts>
<context position="32394" citStr="Jing, 2000" startWordPosition="5593" endWordPosition="5594">ng a subcategorization feature (Gildea &amp; Jurafsky, 2002; Xue &amp; Palmer, 2004), such as the one used in our baseline; and building a model which jointly classifies all arguments of a verb (Toutanova et al., 2005). Our method differs from past work in that it extracts its role pattern feature from the simplified sentence. As a result, the feature is less noisy 3http://www2.parc.com/isl/groups/nltt/xle/ and generalizes better across syntactic variation than a feature extracted from the original sentence. Another group of related work focuses on summarizing sentences through a series of deletions (Jing, 2000; Dorr et al., 2003; Galley &amp; McKeown, 2007). In particular, the latter two works iteratively simplify the sentence by deleting a phrase at a time. We differ from these works in several important ways. First, our transformation language is not context-free; it can reorder constituents and then apply transformation rules to the reordered sentence. Second, we are focusing on a somewhat different task; these works are interested in obtaining a single summary of each sentence which maintains all “essential” information, while in our work we produce a simplification that may lose semantic content, </context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>Jing, H. (2000). Sentence reduction for automatic text summarization. Proceedings ofApplied NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="33329" citStr="Moschitti, 2004" startWordPosition="5745" endWordPosition="5746">s to the reordered sentence. Second, we are focusing on a somewhat different task; these works are interested in obtaining a single summary of each sentence which maintains all “essential” information, while in our work we produce a simplification that may lose semantic content, but aims to contain all arguments of a verb. Finally, training our model on SRL data allows us to avoid the relative scarcity of parallel simplification corpora and the issue of determining what is “essential” in a sentence. Another area of related work in the semantic role labeling literature is that on tree kernels (Moschitti, 2004; Zhang et al., 2007). Like our method, tree kernels decompose the parse path into smaller pieces for classification. Our model can generalize better across verbs because it first simplifies, then classifies based on the simplified sentence. Also, through iterative simplifications we can discover structure that is not immediately apparent in the original parse. 10 Future Work There are a number of improvements that could be made to the current simplification system, including augmenting the rule set to handle more constructions and doing further sentence normalizations, e.g., identifying wheth</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Moschitti, A. (2004). A study on convolution kernels for shallow semantic parsing. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>K Hacioglu</author>
<author>V Krugler</author>
<author>W Ward</author>
<author>J H Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<volume>60</volume>
<pages>11--39</pages>
<contexts>
<context position="24761" citStr="Pradhan et al. (2005)" startWordPosition="4343" endWordPosition="4346">ur compact representation (a similar algorithm is presented in (Geman &amp; Johnson, 2002)). We omit details for lack of space. 8 Experiments We evaluated our system using the setup of the Conll 2005 semantic role labeling task.2 Thus, we trained on Sections 2-21 of PropBank and used Section 24 as development data. Our test data includes both the selected portion of Section 23 of PropBank, plus the extra data on the Brown corpus. We used the Charniak parses provided by the Conll distribution. We compared to a strong Baseline SRL system that learns a logistic regression model using the features of Pradhan et al. (2005). It has two stages. The first filters out nodes that are unlikely to be arguments. The second stage labels each remaining node either as a particular role (e.g. “ARGO”) or as a non-argument. Note that the baseline feature set includes a feature corresponding to the subcategorization of the verb (specifically, the sequence of nonterminals which are children of the predicate’s parent node). Thus, Baseline does have access to something similar to our model’s role pattern feature, although the Baseline subcategorization feature only includes post-verbal modifiers and is generally much noisier bec</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Pradhan, S., Hacioglu, K., Krugler, V., Ward, W., Martin, J. H., &amp; Jurafsky, D. (2005). Support vector learning for semantic argument classification. Machine Learning, 60, 11–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>P Koomen</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Generalized inference with multiple semantic role labeling systems.</title>
<date>2005</date>
<booktitle>Proceedings of CoNLL.</booktitle>
<contexts>
<context position="28138" citStr="Punyakanok et al., 2005" startWordPosition="4899" endWordPosition="4902">constituents that are not present are then assigned a probability distribution over possible roles according to Baseline. Thus, we fall back Baseline whenever the current simple sentence does not have an “opinion” about the role of a particular constituent. The Combined model is thus able to correctly label sentences when the simplification process drops some of the arguments (generally due to unusual syntax). Each of the two components was trained separately and combined only at testing time. Table 2 shows results of these three systems on the Conll-2005 task, plus the top-performing system (Punyakanok et al., 2005) for reference. Baseline already achieves good performance on this task, placing at about 75th percentile among evaluated systems. Our Transforms model outperforms Baseline on all sets. Finally, our Combined model improves over Transforms on all but the test Brown corpus, achieving a statistically significant increase over the Baseline system (according to confidence intervals calculated for the Conll-2005 results). The Combined model still does not achieve the performance levels of the top several systems. However, these systems all use information from multiple parses, allowing them to fix m</context>
</contexts>
<marker>Punyakanok, Koomen, Roth, Yih, 2005</marker>
<rawString>Punyakanok, V., Koomen, P., Roth, D., &amp; Yih, W. (2005). Generalized inference with multiple semantic role labeling systems. Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>A Haghighi</author>
<author>C Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>589--596</pages>
<contexts>
<context position="31994" citStr="Toutanova et al., 2005" startWordPosition="5532" endWordPosition="5535">unctional Grammars (LFGs). Both approaches attempt to abstract away from the surface level syntax of the sentence (e.g., the XLE system3). The most obvious difference between the approaches is that we use SRL data to train our system, avoiding the need to have labeled data specific to our simplification scheme. There have been a number of works which model verb subcategorization. Approaches include incorporating a subcategorization feature (Gildea &amp; Jurafsky, 2002; Xue &amp; Palmer, 2004), such as the one used in our baseline; and building a model which jointly classifies all arguments of a verb (Toutanova et al., 2005). Our method differs from past work in that it extracts its role pattern feature from the simplified sentence. As a result, the feature is less noisy 3http://www2.parc.com/isl/groups/nltt/xle/ and generalizes better across syntactic variation than a feature extracted from the original sentence. Another group of related work focuses on summarizing sentences through a series of deletions (Jing, 2000; Dorr et al., 2003; Galley &amp; McKeown, 2007). In particular, the latter two works iteratively simplify the sentence by deleting a phrase at a time. We differ from these works in several important ways</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Toutanova, K., Haghighi, A., &amp; Manning, C. (2005). Joint learning improves semantic role labeling. Proceedings of ACL, 589–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>M Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<contexts>
<context position="31860" citStr="Xue &amp; Palmer, 2004" startWordPosition="5508" endWordPosition="5511">rning when to apply these rules. 9 Related Work One area of current research which has similarities with this work is on Lexical Functional Grammars (LFGs). Both approaches attempt to abstract away from the surface level syntax of the sentence (e.g., the XLE system3). The most obvious difference between the approaches is that we use SRL data to train our system, avoiding the need to have labeled data specific to our simplification scheme. There have been a number of works which model verb subcategorization. Approaches include incorporating a subcategorization feature (Gildea &amp; Jurafsky, 2002; Xue &amp; Palmer, 2004), such as the one used in our baseline; and building a model which jointly classifies all arguments of a verb (Toutanova et al., 2005). Our method differs from past work in that it extracts its role pattern feature from the simplified sentence. As a result, the feature is less noisy 3http://www2.parc.com/isl/groups/nltt/xle/ and generalizes better across syntactic variation than a feature extracted from the original sentence. Another group of related work focuses on summarizing sentences through a series of deletions (Jing, 2000; Dorr et al., 2003; Galley &amp; McKeown, 2007). In particular, the l</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Xue, N., &amp; Palmer, M. (2004). Calibrating features for semantic role labeling. Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhang</author>
<author>W Che</author>
<author>A Aw</author>
<author>C L Tan</author>
<author>G Zhou</author>
<author>T Liu</author>
<author>S Li</author>
</authors>
<title>A grammar-driven convolution tree kernel for semantic role classification.</title>
<date>2007</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="33350" citStr="Zhang et al., 2007" startWordPosition="5747" endWordPosition="5750">d sentence. Second, we are focusing on a somewhat different task; these works are interested in obtaining a single summary of each sentence which maintains all “essential” information, while in our work we produce a simplification that may lose semantic content, but aims to contain all arguments of a verb. Finally, training our model on SRL data allows us to avoid the relative scarcity of parallel simplification corpora and the issue of determining what is “essential” in a sentence. Another area of related work in the semantic role labeling literature is that on tree kernels (Moschitti, 2004; Zhang et al., 2007). Like our method, tree kernels decompose the parse path into smaller pieces for classification. Our model can generalize better across verbs because it first simplifies, then classifies based on the simplified sentence. Also, through iterative simplifications we can discover structure that is not immediately apparent in the original parse. 10 Future Work There are a number of improvements that could be made to the current simplification system, including augmenting the rule set to handle more constructions and doing further sentence normalizations, e.g., identifying whether a direct object ex</context>
</contexts>
<marker>Zhang, Che, Aw, Tan, Zhou, Liu, Li, 2007</marker>
<rawString>Zhang, M., Che, W., Aw, A., Tan, C. L., Zhou, G., Liu, T., &amp; Li, S. (2007). A grammar-driven convolution tree kernel for semantic role classification. Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>