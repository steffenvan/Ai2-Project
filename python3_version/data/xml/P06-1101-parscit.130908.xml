<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.989706">
Semantic Taxonomy Induction from Heterogenous Evidence
</title>
<author confidence="0.998169">
Rion Snow
</author>
<affiliation confidence="0.9926085">
Computer Science Department
Stanford University
</affiliation>
<address confidence="0.87918">
Stanford, CA 94305
</address>
<email confidence="0.998878">
rion@cs.stanford.edu
</email>
<author confidence="0.977083">
Daniel Jurafsky
</author>
<affiliation confidence="0.9658325">
Linguistics Department
Stanford University
</affiliation>
<address confidence="0.87676">
Stanford, CA 94305
</address>
<email confidence="0.998864">
jurafsky@stanford.edu
</email>
<author confidence="0.993749">
Andrew Y. Ng
</author>
<affiliation confidence="0.9919705">
Computer Science Department
Stanford University
</affiliation>
<address confidence="0.878841">
Stanford, CA 94305
</address>
<email confidence="0.998862">
ang@cs.stanford.edu
</email>
<sectionHeader confidence="0.993902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999748043478261">
We propose a novel algorithm for inducing seman-
tic taxonomies. Previous algorithms for taxonomy
induction have typically focused on independent
classifiers for discovering new single relationships
based on hand-constructed or automatically discov-
ered textual patterns. By contrast, our algorithm
flexibly incorporates evidence from multiple clas-
sifiers over heterogenous relationships to optimize
the entire structure of the taxonomy, using knowl-
edge of a word’s coordinate terms to help in deter-
mining its hypernyms, and vice versa. We apply our
algorithm on the problem of sense-disambiguated
noun hyponym acquisition, where we combine the
predictions of hypernym and coordinate term clas-
sifiers with the knowledge in a preexisting seman-
tic taxonomy (WordNet 2.1). We add 10, 000 novel
synsets to WordNet 2.1 at 84% precision, a rela-
tive error reduction of 70% over a non-joint algo-
rithm using the same component classifiers. Fi-
nally, we show that a taxonomy built using our al-
gorithm shows a 23% relative F-score improvement
over WordNet 2.1 on an independent testset of hy-
pernym pairs.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977672131148">
The goal of capturing structured relational knowl-
edge about lexical terms has been the motivating
force underlying many projects in lexical acquisi-
tion, information extraction, and the construction
of semantic taxonomies. Broad-coverage seman-
tic taxonomies such as WordNet (Fellbaum, 1998)
and CYC (Lenat, 1995) have been constructed by
hand at great cost; while a crucial source of knowl-
edge about the relations between words, these tax-
onomies still suffer from sparse coverage.
Many algorithms with the potential for auto-
matically extending lexical resources have been
proposed, including work in lexical acquisition
(Riloff and Shepherd, 1997; Roark and Charniak,
1998) and in discovering instances, named enti-
ties, and alternate glosses (Etzioni et al., 2005;
Pasc¸a, 2005). Additionally, a wide variety of
relationship-specific classifiers have been pro-
posed, including pattern-based classifiers for hy-
ponyms (Hearst, 1992), meronyms (Girju, 2003),
synonyms (Lin et al., 2003), a variety of verb re-
lations (Chklovski and Pantel, 2004), and general
purpose analogy relations (Turney et al., 2003).
Such classifiers use hand-written or automatically-
induced patterns like Such NPy as NP,, or NPy
like NP,, to determine, for example that NPy is a
hyponym of NP,, (i.e., NPy IS-A NP,,). While
such classifiers have achieved some degree of suc-
cess, they frequently lack the global knowledge
necessary to integrate their predictions into a com-
plex taxonomy with multiple relations.
Past work on semantic taxonomy induction in-
cludes the noun hypernym hierarchy created in
(Caraballo, 2001), the part-whole taxonomies in
(Girju, 2003), and a great deal of recent work de-
scribed in (Buitelaar et al., 2005). Such work has
typically either focused on only inferring small
taxonomies over a single relation, or as in (Cara-
ballo, 2001), has used evidence for multiple rela-
tions independently from one another, by for ex-
ample first focusing strictly on inferring clusters
of coordinate terms, and then by inferring hyper-
nyms over those clusters.
Another major shortfall in previous techniques
for taxonomy induction has been the inability to
handle lexical ambiguity. Previous approaches
have typically sidestepped the issue of polysemy
altogether by making the assumption of only a sin-
gle sense per word, and inferring taxonomies ex-
plicitly over words and not senses. Enforcing a
false monosemy has the downside of making po-
tentially erroneous inferences; for example, col-
lapsing the polysemous term Bush into a single
sense might lead one to infer by transitivity that
a rose bush is a kind of U.S. president.
Our approach simultaneously provides a solu-
tion to the problems of jointly considering evi-
dence about multiple relationships as well as lexi-
cal ambiguity within a single probabilistic frame-
work. The key contribution of this work is to offer
a solution to two crucial problems in taxonomy in-
</bodyText>
<page confidence="0.96812">
801
</page>
<bodyText confidence="0.993512857142857">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 801–808,
Sydney, July 2006. c�2006 Association for Computational Linguistics
duction and hyponym acquisition: the problem of
combining heterogenous sources of evidence in a
flexible way, and the problem of correctly identi-
fying the appropriate word sense of each new word
added to the taxonomy.1
</bodyText>
<sectionHeader confidence="0.899551" genericHeader="method">
2 A Probabilistic Framework for
Taxonomy Induction
</sectionHeader>
<bodyText confidence="0.9998669">
In section 2.1 we introduce our definitions for tax-
onomies, relations, and the taxonomic constraints
that enforce dependencies between relations; in
section 2.2 we give a probabilistic model for defin-
ing the conditional probability of a set of relational
evidence given a taxonomy; in section 2.3 we for-
mulate a local search algorithm to find the taxon-
omy maximizing this conditional probability; and
in section 2.4 we extend our framework to deal
with lexical ambiguity.
</bodyText>
<subsectionHeader confidence="0.961289">
2.1 Taxonomies, Relations, and Taxonomic
Constraints
</subsectionHeader>
<bodyText confidence="0.999385">
We define a taxonomy T as a set of pairwise re-
lations R over some domain of objects DT. For
example, the relations in WordNet include hyper-
nymy, holonymy, verb entailment, and many oth-
ers; the objects of WordNet between which these
relations hold are its word senses or synsets. We
define that each relation R E R is a set of ordered
or unordered pairs of objects (i, j) E DT; we de-
fine Rij E T if relationship R holds over objects
</bodyText>
<equation confidence="0.610519">
(i, j) in T.
</equation>
<subsectionHeader confidence="0.845233">
Relations for Hyponym Acquisition
</subsectionHeader>
<bodyText confidence="0.999866266666666">
For the case of hyponym acquisition, the ob-
jects in our taxonomy are WordNet synsets. In
this paper we focus on two of the many possible
relationships between senses: the hypernym rela-
tion and the coordinate term relation. We treat the
hypernym or ISA relation as atomic; we use the
notation Hinj if a sense j is the n-th ancestor of a
sense i in the hypernym hierarchy. We will sim-
ply use Hij to indicate that j is an ancestor of i
at some unspecified level. Two senses are typi-
cally considered to be “coordinate terms” or “tax-
onomic sisters” if they share an immediate parent
in the hypernym hierarchy. We generalize this no-
tion of siblinghood to state that two senses i and
j are (m, n)-cousins if their closest least common
</bodyText>
<footnote confidence="0.87633275">
1The taxonomies discussed in this paper are available for
download at http://ai.stanford.edu/∼rion/swn.
subsumer (LCS)2 is within exactly m and n links,
respectively.3 We use the notation Cmn
</footnote>
<bodyText confidence="0.996700571428571">
ij to denote
that i and j are (m, n)-cousins. Thus coordinate
terms are (1,1)-cousins; technically the hypernym
relation may also be seen as a specific case of this
representation; an immediate parent in the hyper-
nym hierarchy is a (1, 0)-cousin, and the k-th an-
cestor is a (k, 0)-cousin.
</bodyText>
<subsectionHeader confidence="0.966449">
Taxonomic Constraints
</subsectionHeader>
<bodyText confidence="0.998223909090909">
A semantic taxonomy such as WordNet en-
forces certain taxonomic constraints which disal-
low particular taxonomies T. For example, the
ISA transitivity constraint in WordNet requires
that each synset inherits the hypernyms of its hy-
pernym, and the part-inheritance constraint re-
quires that each synset inherits the meronyms of
its hypernyms.
For the case of hyponym acquisition we enforce
the following two taxonomic constraints on the
hypernym and (m, n)-cousin relations:
</bodyText>
<listItem confidence="0.670544">
1. ISA Transitivity:
</listItem>
<equation confidence="0.8000405">
Hmij ∧ Hnjk � Hm+n
ik .
</equation>
<listItem confidence="0.706654">
2. Definition of (m, n)-cousinhood:
</listItem>
<equation confidence="0.514375">
Cmn
ij 1k.k = LCS(i, j) ∧ Hmik ∧ Hnjk.
</equation>
<bodyText confidence="0.97835875">
Constraint (1) requires that the each synset inherits
the hypernyms of its direct hypernym; constraint
(2) simply defines the (m, n)-cousin relation in
terms of the atomic hypernym relation.
The addition of any new hypernym relation to a
preexisting taxonomy will usually necessitate the
addition of a set of other novel relations as implied
by the taxonomic constraints. We refer to the full
set of novel relations implied by a new link Rij as
I(Rij); we discuss the efficient computation of the
set of implied links for the purpose of hyponym
acquisition in Section 3.4.
</bodyText>
<subsectionHeader confidence="0.997096">
2.2 A Probabilistic Formulation
</subsectionHeader>
<bodyText confidence="0.982273">
We propose that the event Rij E T has some
prior probability P(Rij E T), and P(Rij E
</bodyText>
<footnote confidence="0.9956065">
2A least common subsumer LCS(i, j) is defined as a
synset that is an ancestor in the hypernym hierarchy of both
i and j which has no child that is also an ancestor of both i
and j. When there is more than one LCS (due to multiple
inheritance), we refer to the closest LCS, i.e.,the LCS that
minimizes the maximum distance to i and j.
3An (m, n)-cousin for m &gt; 2 corresponds to the English
kinship relation “(m −1)-th cousin |m − n|-times removed.”
</footnote>
<page confidence="0.996596">
802
</page>
<bodyText confidence="0.9011595">
T) + P(Rij E� T) = 1. We define the probability
of the taxonomy as a whole as the joint probability
of its component relations; given a partition of all
possible relations R = {A, B} where A E T and
</bodyText>
<equation confidence="0.9256885">
B E� T, we define:
P(T) = P(A E T, B E� T).
</equation>
<bodyText confidence="0.999945476190476">
We assume that we have some set of observed evi-
dence E consisting of observed features over pairs
of objects in some domain DE; we’ll begin with
the assumption that our features are over pairs of
words, and that the objects in the taxonomy also
correspond directly to words.4 Given a set of fea-
tures ERij E E, we assume we have some model
for inferring P(Rij E T|ERij), i.e., the posterior
probability of the event Rij E T given the corre-
sponding evidence ERij for that relation. For exam-
ple, evidence for the hypernym relation EHij might
be the set of all observed lexico-syntactic patterns
containing i and j in all sentences in some corpus.
For simplicity we make the following indepen-
dence assumptions: first, we assume that each
item of observed evidence ERij is independent of
all other observed evidence given the taxonomy T,
i.e., P(E|T) = 11ERijEE P(ERij|T).
Further, we assume that each item of observed
evidence ERij depends on the taxonomy T only by
way of the corresponding relation Rij, i.e.,
</bodyText>
<equation confidence="0.9793595">
R _ P(ER  |Rij E T) if Rij E T
P(Eij|T) —{ P(A|Rij E� T) if Rij E� T
</equation>
<bodyText confidence="0.9999331">
For example, if our evidence EHij is a set of ob-
served lexico-syntactic patterns indicative of hy-
pernymy between two words i and j, we assume
that whatever dependence the relations in T have
on our observations may be explained entirely by
dependence on the existence or non-existence of
the single hypernym relation H(i, j).
Applying these two independence assumptions
we may express the conditional probability of our
evidence given the taxonomy:
</bodyText>
<equation confidence="0.999551571428571">
P(E|T) = fl P(ERij|Rij E T)
RijET
P(Rij|ERij) using Bayes Rule, we obtain:
P(Rij E T|ERij)P(ERij)
P(Rij E T)
P(Rij E� T|ERij)P(ERij)
P(Rij E� T) .
</equation>
<bodyText confidence="0.9991482">
Within our model we define the goal of taxon-
omy induction to be to find the taxonomy T� that
maximizes the conditional probability of our ob-
servations E given the relationships of T, i.e., to
find
</bodyText>
<equation confidence="0.991011">
T� = arg max
T
</equation>
<subsectionHeader confidence="0.988841">
2.3 Local Search Over Taxonomies
</subsectionHeader>
<bodyText confidence="0.9999862">
We propose a search algorithm for finding T� for
the case of hyponym acquisition. We assume we
begin with some initial (possibly empty) taxon-
omy T. We restrict our consideration of possible
new taxonomies to those created by the single op-
eration ADD-RELATION(Rij, T), which adds the
single relation Rij to T.
We define the multiplicative change OT(Rij)
to the conditional probability P(E|T) given the
addition of a single relation Rij:
</bodyText>
<equation confidence="0.998271142857143">
OT(Rij) = P(E|T&apos;)/P(E|T)
P(Rij E T|ERij)P(ERij) P(Rij E� T)
P(Rij E� T|ERij)P(ERij) �P(Rij E T)
( I
P Rij E T|ER ij
( I
1 − P Rij E T|ER ij
</equation>
<bodyText confidence="0.997391333333333">
Here k is the inverse odds of the prior on the event
Rij E T; we consider this to be a constant inde-
pendent of i, j, and the taxonomy T.
To enforce the taxonomic constraints in T, for
each application of the ADD-RELATION operator
we must add all new relations in the implied set
I(Rij) not already in T.5 Thus we define the mul-
tiplicative change of the full set of implied rela-
tions as the product over all new relations:
</bodyText>
<equation confidence="0.987791272727273">
P(E|T) = fl
RijET
fl�
RijoT
P(E|T).
�
= k �
�
� .
fl� P(ERij|Rij E� T). OT(I(Rij)) = fl OT(R).
RijoT REI(Rij)
</equation>
<bodyText confidence="0.999801">
Rewriting the conditional probability in terms
of our estimates of the posterior probabilities
</bodyText>
<footnote confidence="0.947428571428571">
4In section 2.4 we drop this assumption, extending our
model to manage lexical ambiguity.
5For example, in order to add the new synset
microsoft under the noun synset company#n#1
in WordNet 2.1, we must necessarily add the
new relations H2(microsoft, institution#n#1)
C&amp;quot;(microsoft, dotcom#n#1), and so on.
</footnote>
<page confidence="0.998229">
803
</page>
<bodyText confidence="0.99986125">
This definition leads to the following best-first
search algorithm for hyponym acquisition, which
at each iteration defines the new taxonomy as the
union of the previous taxonomy T and the set of
novel relations implied by the relation Rij that
maximizes AT(I(Rij)) and thus maximizes the
conditional probability of the evidence over all
possible single relations:
</bodyText>
<subsectionHeader confidence="0.5708665">
2.4 Extending the Model to Manage Lexical
Ambiguity
</subsectionHeader>
<bodyText confidence="0.999900571428571">
Since word senses are not directly observable, if
the objects in the taxonomy are word senses (as in
WordNet), we must extend our model to allow for
a many-to-many mapping (e.g., a word-to-sense
mapping) between DE and DT. For this setting
we assume we know the function senses(i), map-
ping from the word i to all of is possible corre-
sponding senses.
We assume that each set of word-pair evidence
ERij we possess is in fact sense-pair evidence ERkl
for a specific pair of senses k0 E senses(i), l0 E
senses(j). Further, we assume that a new relation
between two words is probable only between the
correct sense pair, i.e.:
</bodyText>
<equation confidence="0.999136">
P(Rkl|ERij) = 1{k = k0, l = l0} · P(Rij|ERij).
</equation>
<bodyText confidence="0.9852226">
When computing the conditional probability of a
specific new relation Rkl E I(Rab), we assume
that the relevant sense pair k0, l0 is the one which
maximizes the probability of the new relation, i.e.
for k E senses(i), l E senses(j),
</bodyText>
<equation confidence="0.557002">
P(Rkl E T|ERij).
</equation>
<bodyText confidence="0.999742307692308">
Our independence assumptions for this exten-
sion need only to be changed slightly; we now as-
sume that the evidence ERij depends on the taxon-
omy T via only a single relation between sense-
pairs Rkl. Using this revised independence as-
sumption the derivation for best-first search over
taxonomies for hyponym acquisition remains un-
changed. One side effect of this revised indepen-
dence assumption is that the addition of the single
“sense-collapsed” relation Rkl in the taxonomy T
will explain the evidence ERij for the relation over
words i and j now that such evidence has been re-
vealed to concern only the specific senses k and l.
</bodyText>
<sectionHeader confidence="0.988182" genericHeader="method">
3 Extending WordNet
</sectionHeader>
<bodyText confidence="0.999892058823529">
We demonstrate the ability of our model to use
evidence from multiple relations to extend Word-
Net with novel noun hyponyms. While in prin-
ciple we could use any number of relations, for
simplicity we consider two primary sources of ev-
idence: the probability of two words in WordNet
being in a hypernym relation, and the probability
of two words in WordNet being in a coordinate re-
lation.
In sections 3.1 and 3.2 we describe the construc-
tion of our hypernym and coordinate classifiers,
respectively; in section 3.3 we outline the efficient
algorithm we use to perform local search over
hyponym-extended WordNets; and in section 3.4
we give an example of the implicit structure-based
word sense disambiguation performed within our
framework.
</bodyText>
<subsectionHeader confidence="0.991235">
3.1 Hyponym Classification
</subsectionHeader>
<bodyText confidence="0.981162833333333">
Our classifier for the hypernym relation is derived
from the “hypernym-only” classifier described in
(Snow et al., 2005). The features used for pre-
dicting the hypernym relationship are obtained by
parsing a large corpus of newswire and encyclo-
pedia text with MINIPAR (Lin, 1998). From the
resulting dependency trees the evidence EHij for
each word pair (i, j) is constructed; the evidence
takes the form of a vector of counts of occurrences
that each labeled syntactic dependency path was
found as the shortest path connecting i and j in
some dependency tree. The labeled training set is
constructed by labeling the collected feature vec-
tors as positive “known hypernym” or negative
“known non-hypernym” examples using WordNet
2.0; 49,922 feature vectors were labeled as pos-
itive training examples, and 800,828 noun pairs
were labeled as negative training examples. The
model for predicting P(Hij|EHij ) is then trained
using logistic regression, predicting the noun-pair
hypernymy label from WordNet from the feature
vector of lexico-syntactic patterns.
The hypernym classifier described above pre-
dicts the probability of the generalized hypernym-
ancestor relation over words P(Hij|EHij ). For
the purposes of taxonomy induction, we would
prefer an ancestor-distance specific set of clas-
sifiers over senses, i.e., for k E senses(i), l E
senses(j), the set of classifiers estimating
{P(H� kl|EH ij ), P(Hkl|EHij ), ... }.
</bodyText>
<figure confidence="0.8625755">
WHILE max AT(I(Rij)) &gt; 1
RajoT
T +— T U I(arg max AT(I(Rij))).
RajoT
(k0, l0) = arg max
k,l
</figure>
<page confidence="0.991708">
804
</page>
<bodyText confidence="0.9998454">
One problem that arises from directly assign-
ing the probability P(Hn ij|EH ij ) a P(Hij|EHij ) for
all n is the possibility of adding a novel hyponym
to an overly-specific hypernym, which might still
satisfy P(Hn ij|EH ij ) for a very large n. In or-
der to discourage unnecessary overspecification,
we penalize each probability P(Hk ij|EH ij ) by a
factor Ak−1 for some A &lt; 1, and renormalize:
P(Hkij|EHij ) a Ak−1P(Hij|EHij ). In our experi-
ments we set A = 0.95.
</bodyText>
<subsectionHeader confidence="0.988481">
3.2 (m, n)-cousin Classification
</subsectionHeader>
<bodyText confidence="0.987795743589743">
The classifier for learning coordinate terms relies
on the notion of distributional similarity, i.e., the
idea that two words with similar meanings will be
used in similar contexts (Hindle, 1990). We ex-
tend this notion to suggest that words with similar
meanings should be near each other in a seman-
tic taxonomy, and in particular will likely share a
hypernym as a near parent.
Our classifier for (m, n)-cousins is derived
from the algorithm and corpus given in (Ravichan-
dran et al., 2005). In that work an efficient ran-
domized algorithm is derived for computing clus-
ters of similar nouns. We use a set of more than
1000 distinct clusters of English nouns collected
by their algorithm over 70 million webpages6,
with each noun i having a score representing its
cosine similarity to the centroid c of the cluster to
which it belongs, cos(B(i, c)).
We use the cluster scores of noun pairs as input
to our own algorithm for predicting the (m, n)-
cousin relationship between the senses of two
words i and j. If two words i and j appear in
a cluster together, with cluster centroid c, we set
our single coordinate input feature to be the mini-
mum cluster score min(cos(B(i, c)), cos(B(j, c))),
and zero otherwise. For each such noun pair fea-
ture, we construct a labeled training set of (m, n)-
cousin relation labels from WordNet 2.1. We de-
fine a noun pair (i, j) to be a “known (m, n)-
cousin” if for some senses k E senses(i), l E
senses(j),
Cmn
ij E WordNet; if more than one
such relation exists, we assume the relation with
smallest sum m + n, breaking ties by smallest
absolute difference |m − n|. We consider all
such labeled relationships from WordNet with 0 &lt;
m, n &lt; 7; pairs of words that have no correspond-
ing pair of synsets connected in the hypernym hi-
</bodyText>
<footnote confidence="0.947974333333333">
6As a preprocessing step we hand-edit the clusters to re-
move those containing non-English words, terms related to
adult content, and other webpage-specific clusters.
</footnote>
<bodyText confidence="0.995853833333333">
erarchy, or with min(m, n) &gt; 7, are assigned to
a single class C&apos;. Further, due to the symme-
try of the similarity score, we merge each class
Cmn = Cmn U Cnm; this implies that the result-
ing classifier will predict, as expected given a sym-
metric input, P(Cmn
</bodyText>
<equation confidence="0.8835325">
kl |ECij) = P(Cnm
kl |EC ij).
</equation>
<bodyText confidence="0.998308142857143">
We find 333,473 noun synset pairs in our train-
ing set with similarity score greater than 0.15. We
next apply softmax regression to learn a classifier
that predicts P(Cmn
ij |EC ij), predicting the Word-
Net class labels from the single similarity score
derived from the noun pair’s cluster similarity.
</bodyText>
<subsectionHeader confidence="0.999779">
3.3 Details of our Implementation
</subsectionHeader>
<bodyText confidence="0.999861405405405">
Hyponym acquisition is among the simplest and
most straightforward of the possible applications
of our model; here we show how we efficiently
implement our algorithm for this problem. First,
we identify the set of all the word pairs (i, j) over
which we have hypernym and/or coordinate ev-
idence, and which might represent additions of
a novel hyponym to the WordNet 2.1 taxonomy
(i.e., that has a known noun hypernym and an un-
known hyponym, or has a known noun coordi-
nate term and an unknown coordinate term). This
yields a list of 95,000 single links over threshold
P(Rij) &gt; 0.12.
For each unknown hyponym i we may have
several pieces of evidence; for example, for the
unknown term continental we have 21 relevant
pieces of hypernym evidence, with links to possi-
ble hypernyms {carrier, airline, unit,... }; and we
have 5 pieces of coordinate evidence, with links to
possible coordinate terms {airline, american ea-
gle, airbus, ... }.
For each proposed hypernym or coordinate link
involved with the novel hyponym i, we compute
the set of candidate hypernyms for i; in practice
we consider all senses of the immediate hypernym
j for each potential novel hypernym, and all senses
of the coordinate term k and its first two hypernym
ancestors for each potential coordinate.
In the continental example, from the 26 individ-
ual pieces of evidence over words we construct the
set of 99 unique synsets that we will consider as
possible hypernyms; these include the two senses
of the word airline, the ten senses of the word car-
rier, and so forth.
Next, we iterate through each of the possi-
ble hypernym synsets l under which we might
add the new word i; for each synset l we com-
</bodyText>
<page confidence="0.996399">
805
</page>
<bodyText confidence="0.999764538461538">
pute the change in taxonomy score resulting from
adding the implied relations I(H1il) required by
the taxonomic constraints of T. Since typically
our set of all evidence involving i will be much
smaller than the set of possible relations in I(H1il),
we may efficiently check whether, for each sense
s E senses(w), for all words where we have
some evidence ERiw, whether s participates in
some relation with i in the set of implied rela-
tions I(H1il).7 If there is more than one sense
s E senses(w), we add to I(H1il) the single re-
lationship Ris that maximizes the taxonomy like-
lihood, i.e. arg maxsEsenses(w) AT(Ris).
</bodyText>
<subsectionHeader confidence="0.992887">
3.4 Hypernym Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.999458878787879">
A major strength of our model is its ability to cor-
rectly choose the sense of a hypernym to which
to add a novel hyponym, despite collecting ev-
idence over untagged word pairs. In our algo-
rithm word sense disambiguation is an implicit
side-effect of our algorithm; since our algorithm
chooses to add the single link which, with its im-
plied links, yields the most likely taxonomy, and
since each distinct synset in WordNet has a differ-
ent immediate neighborhood of relations, our al-
gorithm simply disambiguates each node based on
its surrounding structural information.
As an example of sense disambiguation in prac-
tice, consider our example of continental. Sup-
pose we are iterating through each of the 99 pos-
sible synsets under which we might add conti-
nental as a hyponym, and we come to the synset
airline#n#2 in WordNet 2.1, i.e. “a commer-
cial organization serving as a common carrier.”
In this case we will iterate through each piece
of hypernym and coordinate evidence; we find
that the relation H(continental, carrier) is satis-
fied with high probability for the specific synset
carrier#n#S, the grandparent of airline#n#2; thus
the factor AT(H3(continental, carrier#n#S)) is
included in the factor of the set of implied rela-
tions AT (I(H1(continental, airline#n#2))).
Suppose we instead evaluate the first synset
of airline, i.e., airline#n#1, with the gloss “a
hose that carries air under pressure.” For this
synset none of the other 20 relationships di-
rectly implied by hypernym evidence or the
5 relationships implied by the coordinate ev-
</bodyText>
<footnote confidence="0.937532">
7Checking whether or not Ris E I(H il) may be effi-
ciently computed by checking whether s is in the hypernym
ancestors of l or if it shares a least common subsumer with l
within 7 steps.
</footnote>
<bodyText confidence="0.999733">
idence are implied by adding the single link
H1(continental,airline#n#1); thus the resulting
change in the set of implied links given by the cor-
rect “carrier” sense of airline is much higher than
that of the “hose” sense. In fact it is the largest of
all the 99 considered hypernym links for continen-
tal; H1(continental, airline#n#2) is link #18,736
added to the taxonomy by our algorithm.
</bodyText>
<sectionHeader confidence="0.996526" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999989583333333">
In order to evaluate our framework for taxonomy
induction, we have applied hyponym acquisition
to construct several distinct taxonomies, starting
with the base of WordNet 2.1 and only adding
novel noun hyponyms. Further, we have con-
structed taxonomies using a baseline algorithm,
which uses the identical hypernym and coordinate
classifiers used in our joint algorithm, but which
does not combine the evidence of the classifiers.
In section 4.1 we describe our evaluation
methodology; in sections 4.2 and 4.3 we analyze
the fine-grained precision and disambiguation pre-
cision of our algorithm compared to the baseline;
in section 4.4 we compare the coarse-grained pre-
cision of our links (motivated by categories de-
fined by the WordNet supersenses) against the
baseline algorithm and against an “oracle” for
named entity recognition.
Finally, in section 4.5 we evaluate the tax-
onomies inferred by our algorithm directly against
the WordNet 2.1 taxonomy; we perform this eval-
uation by testing each taxonomy on a set of human
judgments of hypernym and non-hypernym noun
pairs sampled from newswire text.
</bodyText>
<subsectionHeader confidence="0.985654">
4.1 Methodology
</subsectionHeader>
<bodyText confidence="0.9999375">
We evaluate the quality of our acquired hy-
ponyms by direct judgment. In four sep-
arate annotation sessions, two judges labeled
{50,100,100,100} samples uniformly generated
from the first {100,1000,10000,20000} single
links added by our algorithm.
For the direct measure of fine-grained precision,
we simply ask for each link H(X, Y ) added by the
system, is X a Y ? In addition to the fine-grained
precision, we give a coarse-grained evaluation, in-
spired by the idea of supersense-tagging in (Cia-
ramita and Johnson, 2003). The 26 supersenses
used in WordNet 2.1 are listed in Table 1; we label
a hyponym link as correct in the coarse-grained
evaluation if the novel hyponym is placed under
the appropriate supersense. This evaluation task
</bodyText>
<page confidence="0.994855">
806
</page>
<table confidence="0.996613">
1 Tops 8 communication 15 object 22 relation
2 act 9 event 16 person 23 shape
3 animal 10 feeling 17 phenomenon 24 state
4 artifact 11 food 18 plant 25 substance
5 attribute 12 group 19 possession 26 time
6 body 13 location 20 process
7 cognition 14 motive 21 quantity
</table>
<tableCaption confidence="0.999851">
Table 1: The 26 WordNet supersenses
</tableCaption>
<bodyText confidence="0.9998965">
is similar to a fine-grained Named Entity Recog-
nition (Fleischman and Hovy, 2002) task with 26
categories; for example, if our algorithm mistak-
enly inserts a novel non-capital city under the hy-
ponym state capital, it will inherit the correct su-
persense location. Finally, we evaluate the abil-
ity of our algorithm to correctly choose the ap-
propriate sense of the hypernym under which a
novel hyponym is being added. Our labelers cate-
gorize each candidate sense-disambiguated hyper-
nym synset suggested by our algorithm into the
following categories:
</bodyText>
<listItem confidence="0.7976524">
c1: Correct sense-disambiguated hypernym.
c2: Correct hypernym word, but incorrect sense of
that word.
c3: Incorrect hypernym, but correct supersense.
c4: Any other relation is considered incorrect.
</listItem>
<bodyText confidence="0.9991395">
A single hyponym/hypernym pair is allowed to be
simultaneously labeled 2 and 3.
</bodyText>
<subsectionHeader confidence="0.86134">
4.2 Fine-grained evaluation
</subsectionHeader>
<bodyText confidence="0.999971">
Table 2 displays the results of our evaluation of
fine-grained precision for the baseline non-joint
algorithm (Base) and our joint algorithm (Joint),
as well as the relative error reduction (ER) of our
algorithm over the baseline. We use the mini-
mum of the two judges’ scores. Here we define
fine-grained precision as c1/total. We see that
our joint algorithm strongly outperforms the base-
line, and has high precision for predicting novel
hyponyms up to 10,000 links.
</bodyText>
<subsectionHeader confidence="0.998685">
4.3 Hypernym sense disambiguation
</subsectionHeader>
<bodyText confidence="0.998552">
Also in Table 2 we compare the sense dis-
ambiguation precision of our algorithm and the
baseline. Here we measure the precision of
sense-disambiguation among all examples where
each algorithm found a correct hyponym word;
our calculation for disambiguation precision is
c1/ (c1 + c2). Again our joint algorithm outper-
forms the baseline algorithm at all levels of re-
call. Interestingly the baseline disambiguation
precision improves with higher recall; this may
</bodyText>
<table confidence="0.996969166666667">
Fine-grained Pre. Disambiguation Pre.
#Links Base Joint ER Base Joint ER
100 0.60 1.00 100% 0.86 1.00 100%
1000 0.52 0.93 85% 0.84 1.00 100%
10000 0.46 0.84 70% 0.90 1.00 100%
20000 0.46 0.68 41% 0.94 0.98 68%
</table>
<tableCaption confidence="0.5729055">
Table 2: Fine-grained and disambiguation preci-
sion and error reduction for hyponym acquisition
</tableCaption>
<table confidence="0.999888166666667">
# Links NER Base Joint ER vs. ER vs.
Oracle NER Base
100 1.00 0.72 1.00 0% 100%
1000 0.69 0.68 0.99 97% 85%
10000 0.45 0.69 0.96 93% 70%
20000 0.54 0.69 0.92 83% 41%
</table>
<tableCaption confidence="0.9020005">
Table 3: Coarse-grained precision and error reduc-
tion vs. Non-joint baseline and NER Oracle
</tableCaption>
<bodyText confidence="0.99963">
be attributed to the observation that the highest-
confidence hypernyms predicted by individual
classifiers are likely to be polysemous, whereas
hypernyms of lower confidence are more fre-
quently monosemous (and thus trivially easy to
disambiguate).
</bodyText>
<subsectionHeader confidence="0.976536">
4.4 Coarse-grained evaluation
</subsectionHeader>
<bodyText confidence="0.9995532">
We compute coarse-grained precision as (c1 +
c3)/total. Inferring the correct coarse-grained su-
persense of a novel hyponym can be viewed as a
fine-grained (26-category) Named Entity Recog-
nition task; our algorithm for taxonomy induction
can thus be viewed as performing high-accuracy
fine-grained NER. Here we compare against both
the baseline non-joint algorithm as well as an
“oracle” algorithm for Named Entity Recogni-
tion, which perfectly classifies the supersense of
all nouns that fall under the four supersenses
{person, group, location, quantity}, but works
only for those supersenses. Table 3 shows the
results of this coarse-grained evaluation. We see
that the baseline non-joint algorithm has higher
precision than the NER oracle as 10,000 and
20,000 links; however, both are significantly out-
performed by our joint algorithm, which main-
tains high coarse-grained precision (92%) even at
20,000 links.
</bodyText>
<subsectionHeader confidence="0.9566495">
4.5 Comparison of inferred taxonomies and
WordNet
</subsectionHeader>
<bodyText confidence="0.99995175">
For our final evaluation we compare our learned
taxonomies directly against the currently exist-
ing hypernym links in WordNet 2.1. In order to
compare taxonomies we use a hand-labeled test
</bodyText>
<page confidence="0.990943">
807
</page>
<table confidence="0.99962">
WN +10K +20K +30K +40K
PRE 0.524 0.524 0.574 0.583 0.571
REC 0.165 0.165 0.203 0.211 0.211
F 0.251 0.251 0.300 0.309 0.307
</table>
<tableCaption confidence="0.9708585">
Table 4: Taxonomy hypernym classification vs.
WordNet 2.1 on hand-labeled testset
</tableCaption>
<bodyText confidence="0.9997693">
set of over 5,000 noun pairs, randomly-sampled
from newswire corpora (described in (Snow et al.,
2005)). We measured the performance of both our
inferred taxonomies and WordNet against this test
set.8 The performance and comparison of the best
WordNet classifier vs. our taxonomies is given in
Table 4. Our best-performing inferred taxonomy
on this test set is achieved after adding 30,000
novel hyponyms, achieving an 23% relative im-
provement in F-score over the WN2.1 classifier.
</bodyText>
<sectionHeader confidence="0.999826" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999817">
We have presented an algorithm for inducing se-
mantic taxonomies which attempts to globally
optimize the entire structure of the taxonomy.
Our probabilistic architecture also includes a new
model for learning coordinate terms based on
(m, n)-cousin classification. The model’s ability
to integrate heterogeneous evidence from different
classifiers offers a solution to the key problem of
choosing the correct word sense to which to attach
a new hypernym.
</bodyText>
<sectionHeader confidence="0.994408" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99992175">
Thanks to Christiane Fellbaum, Rajat Raina, Bill
MacCartney, and Allison Buckley for useful dis-
cussions and assistance annotating data. Rion
Snow is supported by an NDSEG Fellowship
sponsored by the DOD and AFOSR. This work
was supported in part by the Disruptive Technol-
ogy Office (DTO)’s Advanced Question Answer-
ing for Intelligence (AQUAINT) Program.
</bodyText>
<sectionHeader confidence="0.999278" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.968404333333333">
P. Buitelaar, P. Cimiano and B. Magnini. 2005. Ontol-
ogy Learning from Text: Methods, Evaluation and
Applications. Volume 123 Frontiers in Artificial In-
telligence and Applications.
S. Caraballo. 2001. Automatic Acquisition of
a Hypernym-Labeled Noun Hierarchy from Text.
Brown University Ph.D. Thesis.
8We found that the WordNet 2.1 model achieving the
highest F-score used only the first sense of each hyponym,
and allowed a maximum distance of 4 edges between each
hyponym and its hypernym.
S. Cederberg and D. Widdows. 2003. Using LSA and
Noun Coordination Information to Improve the Pre-
cision and Recall of Automatic Hyponymy Extrac-
tion. Proc. CoNLL-2003, pp. 111–118.
T. Chklovski and P. Pantel. 2004. VerbOcean: Mining
the Web for Fine-Grained Semantic Verb Relations.
Proc. EMNLP-2004.
M. Ciaramita and M. Johnson. 2003. Supersense
Tagging of Unknown Nouns in WordNet. Proc.
EMNLP-2003.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised Named-Entity Extraction from
the Web: An Experimental Study. Artificial Intelli-
gence, 165(1):91–134.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Cambridge, MA: MIT Press.
R. Girju, A. Badulescu, and D. Moldovan. 2003.
Learning Semantic Constraints for the Automatic
Discovery of Part-Whole Relations. Proc. HLT-03.
M. Fleischman and E. Hovy. 2002. Fine grained clas-
sification of named entities. Proc. COLING-02.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. Proc. COLING-92.
D. Hindle. 1990. Noun classification from predicate-
argument structures. Proc. ACL-90.
D. Lenat. 1995. CYC: A Large-Scale Investment in
Knowledge Infrastructure, Communications of the
ACM, 38:11, 33–35.
D. Lin. 1998. Dependency-based Evaluation of MINI-
PAR. Workshop on the Evaluation of Parsing Sys-
tems, Granada, Spain.
D. Lin, S. Zhao, L. Qin and M. Zhou. 2003. Iden-
tifying Synonyms among Distributionally Similar
Words. Proc. IJCAI-03.
M. Pasc¸a. 2005. Finding Instance Names and Alter-
native Glosses on the Web: WordNet Reloaded. CI-
CLing 2005, pp. 280-292.
D. Ravichandran, P. Pantel, and E. Hovy. 2002. Ran-
domized Algorithms and NLP: Using Locality Sen-
sitive Hash Function for High Speed Noun Cluster-
ing. Proc. ACL-2002.
E. Riloff and J. Shepherd. 1997. A Corpus-Based
Approach for Building Semantic Lexicons. Proc
EMNLP-1997.
B. Roark and E. Charniak. 1998. Noun-phrase co-
occurerence statistics for semi-automatic-semantic
lexicon construction. Proc. ACL-1998.
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learn-
ing syntactic patterns for automatic hypernym dis-
covery. NIPS 2005.
P. Turney, M. Littman, J. Bigham, and V. Shnay-
der. 2003. Combining independent modules to
solve multiple-choice synonym and analogy prob-
lems. Proc. RANLP-2003, pp. 482–489.
</reference>
<page confidence="0.997491">
808
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.910639">
<title confidence="0.99569">Semantic Taxonomy Induction from Heterogenous Evidence</title>
<author confidence="0.999267">Rion Snow</author>
<affiliation confidence="0.999894">Computer Science Department Stanford University</affiliation>
<address confidence="0.999765">Stanford, CA 94305</address>
<email confidence="0.999792">rion@cs.stanford.edu</email>
<author confidence="0.998365">Daniel Jurafsky</author>
<affiliation confidence="0.9958495">Linguistics Department Stanford University</affiliation>
<address confidence="0.999826">Stanford, CA 94305</address>
<email confidence="0.999736">jurafsky@stanford.edu</email>
<author confidence="0.999963">Andrew Y Ng</author>
<affiliation confidence="0.999881">Computer Science Department Stanford University</affiliation>
<address confidence="0.999699">Stanford, CA 94305</address>
<email confidence="0.999451">ang@cs.stanford.edu</email>
<abstract confidence="0.996842916666667">We propose a novel algorithm for inducing semantic taxonomies. Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns. By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word’s coordinate terms to help in determining its hypernyms, and vice versa. We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantaxonomy (WordNet 2.1). We add to WordNet 2.1 at a relaerror reduction of a non-joint algorithm using the same component classifiers. Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
<author>P Cimiano</author>
<author>B Magnini</author>
</authors>
<date>2005</date>
<booktitle>Ontology Learning from Text: Methods, Evaluation and Applications. Volume 123 Frontiers in Artificial Intelligence and Applications.</booktitle>
<contexts>
<context position="3168" citStr="Buitelaar et al., 2005" startWordPosition="464" endWordPosition="467"> (Turney et al., 2003). Such classifiers use hand-written or automaticallyinduced patterns like Such NPy as NP,, or NPy like NP,, to determine, for example that NPy is a hyponym of NP,, (i.e., NPy IS-A NP,,). While such classifiers have achieved some degree of success, they frequently lack the global knowledge necessary to integrate their predictions into a complex taxonomy with multiple relations. Past work on semantic taxonomy induction includes the noun hypernym hierarchy created in (Caraballo, 2001), the part-whole taxonomies in (Girju, 2003), and a great deal of recent work described in (Buitelaar et al., 2005). Such work has typically either focused on only inferring small taxonomies over a single relation, or as in (Caraballo, 2001), has used evidence for multiple relations independently from one another, by for example first focusing strictly on inferring clusters of coordinate terms, and then by inferring hypernyms over those clusters. Another major shortfall in previous techniques for taxonomy induction has been the inability to handle lexical ambiguity. Previous approaches have typically sidestepped the issue of polysemy altogether by making the assumption of only a single sense per word, and </context>
</contexts>
<marker>Buitelaar, Cimiano, Magnini, 2005</marker>
<rawString>P. Buitelaar, P. Cimiano and B. Magnini. 2005. Ontology Learning from Text: Methods, Evaluation and Applications. Volume 123 Frontiers in Artificial Intelligence and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Caraballo</author>
</authors>
<date>2001</date>
<journal>Automatic Acquisition of</journal>
<contexts>
<context position="3053" citStr="Caraballo, 2001" startWordPosition="446" endWordPosition="447"> al., 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al., 2003). Such classifiers use hand-written or automaticallyinduced patterns like Such NPy as NP,, or NPy like NP,, to determine, for example that NPy is a hyponym of NP,, (i.e., NPy IS-A NP,,). While such classifiers have achieved some degree of success, they frequently lack the global knowledge necessary to integrate their predictions into a complex taxonomy with multiple relations. Past work on semantic taxonomy induction includes the noun hypernym hierarchy created in (Caraballo, 2001), the part-whole taxonomies in (Girju, 2003), and a great deal of recent work described in (Buitelaar et al., 2005). Such work has typically either focused on only inferring small taxonomies over a single relation, or as in (Caraballo, 2001), has used evidence for multiple relations independently from one another, by for example first focusing strictly on inferring clusters of coordinate terms, and then by inferring hypernyms over those clusters. Another major shortfall in previous techniques for taxonomy induction has been the inability to handle lexical ambiguity. Previous approaches have ty</context>
</contexts>
<marker>Caraballo, 2001</marker>
<rawString>S. Caraballo. 2001. Automatic Acquisition of</rawString>
</citation>
<citation valid="false">
<title>a Hypernym-Labeled Noun Hierarchy from Text. Brown University Ph.D. Thesis. 8We found that the WordNet 2.1 model achieving the highest F-score used only the first sense of each hyponym, and allowed a maximum distance of 4 edges between each hyponym and its hypernym.</title>
<marker></marker>
<rawString>a Hypernym-Labeled Noun Hierarchy from Text. Brown University Ph.D. Thesis. 8We found that the WordNet 2.1 model achieving the highest F-score used only the first sense of each hyponym, and allowed a maximum distance of 4 edges between each hyponym and its hypernym.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cederberg</author>
<author>D Widdows</author>
</authors>
<date>2003</date>
<booktitle>Using LSA and Noun Coordination Information to Improve the Precision and Recall of Automatic Hyponymy Extraction. Proc. CoNLL-2003,</booktitle>
<pages>111--118</pages>
<marker>Cederberg, Widdows, 2003</marker>
<rawString>S. Cederberg and D. Widdows. 2003. Using LSA and Noun Coordination Information to Improve the Precision and Recall of Automatic Hyponymy Extraction. Proc. CoNLL-2003, pp. 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Chklovski</author>
<author>P Pantel</author>
</authors>
<title>VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations.</title>
<date>2004</date>
<booktitle>Proc. EMNLP-2004.</booktitle>
<contexts>
<context position="2506" citStr="Chklovski and Pantel, 2004" startWordPosition="358" endWordPosition="361"> between words, these taxonomies still suffer from sparse coverage. Many algorithms with the potential for automatically extending lexical resources have been proposed, including work in lexical acquisition (Riloff and Shepherd, 1997; Roark and Charniak, 1998) and in discovering instances, named entities, and alternate glosses (Etzioni et al., 2005; Pasc¸a, 2005). Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hyponyms (Hearst, 1992), meronyms (Girju, 2003), synonyms (Lin et al., 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al., 2003). Such classifiers use hand-written or automaticallyinduced patterns like Such NPy as NP,, or NPy like NP,, to determine, for example that NPy is a hyponym of NP,, (i.e., NPy IS-A NP,,). While such classifiers have achieved some degree of success, they frequently lack the global knowledge necessary to integrate their predictions into a complex taxonomy with multiple relations. Past work on semantic taxonomy induction includes the noun hypernym hierarchy created in (Caraballo, 2001), the part-whole taxonomies in (Girju, 2003), and a g</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>T. Chklovski and P. Pantel. 2004. VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations. Proc. EMNLP-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>M Johnson</author>
</authors>
<title>Supersense Tagging of Unknown Nouns in WordNet.</title>
<date>2003</date>
<booktitle>Proc. EMNLP-2003.</booktitle>
<contexts>
<context position="25876" citStr="Ciaramita and Johnson, 2003" startWordPosition="4337" endWordPosition="4341"> set of human judgments of hypernym and non-hypernym noun pairs sampled from newswire text. 4.1 Methodology We evaluate the quality of our acquired hyponyms by direct judgment. In four separate annotation sessions, two judges labeled {50,100,100,100} samples uniformly generated from the first {100,1000,10000,20000} single links added by our algorithm. For the direct measure of fine-grained precision, we simply ask for each link H(X, Y ) added by the system, is X a Y ? In addition to the fine-grained precision, we give a coarse-grained evaluation, inspired by the idea of supersense-tagging in (Ciaramita and Johnson, 2003). The 26 supersenses used in WordNet 2.1 are listed in Table 1; we label a hyponym link as correct in the coarse-grained evaluation if the novel hyponym is placed under the appropriate supersense. This evaluation task 806 1 Tops 8 communication 15 object 22 relation 2 act 9 event 16 person 23 shape 3 animal 10 feeling 17 phenomenon 24 state 4 artifact 11 food 18 plant 25 substance 5 attribute 12 group 19 possession 26 time 6 body 13 location 20 process 7 cognition 14 motive 21 quantity Table 1: The 26 WordNet supersenses is similar to a fine-grained Named Entity Recognition (Fleischman and Hov</context>
</contexts>
<marker>Ciaramita, Johnson, 2003</marker>
<rawString>M. Ciaramita and M. Johnson. 2003. Supersense Tagging of Unknown Nouns in WordNet. Proc. EMNLP-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised Named-Entity Extraction from the Web: An Experimental Study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="2229" citStr="Etzioni et al., 2005" startWordPosition="319" endWordPosition="322">quisition, information extraction, and the construction of semantic taxonomies. Broad-coverage semantic taxonomies such as WordNet (Fellbaum, 1998) and CYC (Lenat, 1995) have been constructed by hand at great cost; while a crucial source of knowledge about the relations between words, these taxonomies still suffer from sparse coverage. Many algorithms with the potential for automatically extending lexical resources have been proposed, including work in lexical acquisition (Riloff and Shepherd, 1997; Roark and Charniak, 1998) and in discovering instances, named entities, and alternate glosses (Etzioni et al., 2005; Pasc¸a, 2005). Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hyponyms (Hearst, 1992), meronyms (Girju, 2003), synonyms (Lin et al., 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al., 2003). Such classifiers use hand-written or automaticallyinduced patterns like Such NPy as NP,, or NPy like NP,, to determine, for example that NPy is a hyponym of NP,, (i.e., NPy IS-A NP,,). While such classifiers have achieved some degree of success, they frequentl</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised Named-Entity Extraction from the Web: An Experimental Study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="1756" citStr="Fellbaum, 1998" startWordPosition="248" endWordPosition="249">00 novel synsets to WordNet 2.1 at 84% precision, a relative error reduction of 70% over a non-joint algorithm using the same component classifiers. Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs. 1 Introduction The goal of capturing structured relational knowledge about lexical terms has been the motivating force underlying many projects in lexical acquisition, information extraction, and the construction of semantic taxonomies. Broad-coverage semantic taxonomies such as WordNet (Fellbaum, 1998) and CYC (Lenat, 1995) have been constructed by hand at great cost; while a crucial source of knowledge about the relations between words, these taxonomies still suffer from sparse coverage. Many algorithms with the potential for automatically extending lexical resources have been proposed, including work in lexical acquisition (Riloff and Shepherd, 1997; Roark and Charniak, 1998) and in discovering instances, named entities, and alternate glosses (Etzioni et al., 2005; Pasc¸a, 2005). Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A Badulescu</author>
<author>D Moldovan</author>
</authors>
<title>Learning Semantic Constraints for the Automatic Discovery of Part-Whole Relations.</title>
<date>2003</date>
<booktitle>Proc. HLT-03.</booktitle>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>R. Girju, A. Badulescu, and D. Moldovan. 2003. Learning Semantic Constraints for the Automatic Discovery of Part-Whole Relations. Proc. HLT-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>E Hovy</author>
</authors>
<title>Fine grained classification of named entities.</title>
<date>2002</date>
<booktitle>Proc. COLING-02. M.</booktitle>
<location>Hearst.</location>
<contexts>
<context position="26484" citStr="Fleischman and Hovy, 2002" startWordPosition="4446" endWordPosition="4449">and Johnson, 2003). The 26 supersenses used in WordNet 2.1 are listed in Table 1; we label a hyponym link as correct in the coarse-grained evaluation if the novel hyponym is placed under the appropriate supersense. This evaluation task 806 1 Tops 8 communication 15 object 22 relation 2 act 9 event 16 person 23 shape 3 animal 10 feeling 17 phenomenon 24 state 4 artifact 11 food 18 plant 25 substance 5 attribute 12 group 19 possession 26 time 6 body 13 location 20 process 7 cognition 14 motive 21 quantity Table 1: The 26 WordNet supersenses is similar to a fine-grained Named Entity Recognition (Fleischman and Hovy, 2002) task with 26 categories; for example, if our algorithm mistakenly inserts a novel non-capital city under the hyponym state capital, it will inherit the correct supersense location. Finally, we evaluate the ability of our algorithm to correctly choose the appropriate sense of the hypernym under which a novel hyponym is being added. Our labelers categorize each candidate sense-disambiguated hypernym synset suggested by our algorithm into the following categories: c1: Correct sense-disambiguated hypernym. c2: Correct hypernym word, but incorrect sense of that word. c3: Incorrect hypernym, but co</context>
</contexts>
<marker>Fleischman, Hovy, 2002</marker>
<rawString>M. Fleischman and E. Hovy. 2002. Fine grained classification of named entities. Proc. COLING-02. M. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. Proc. COLING-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicateargument structures.</title>
<date>1990</date>
<booktitle>Proc. ACL-90.</booktitle>
<contexts>
<context position="17429" citStr="Hindle, 1990" startWordPosition="2899" endWordPosition="2900">) a P(Hij|EHij ) for all n is the possibility of adding a novel hyponym to an overly-specific hypernym, which might still satisfy P(Hn ij|EH ij ) for a very large n. In order to discourage unnecessary overspecification, we penalize each probability P(Hk ij|EH ij ) by a factor Ak−1 for some A &lt; 1, and renormalize: P(Hkij|EHij ) a Ak−1P(Hij|EHij ). In our experiments we set A = 0.95. 3.2 (m, n)-cousin Classification The classifier for learning coordinate terms relies on the notion of distributional similarity, i.e., the idea that two words with similar meanings will be used in similar contexts (Hindle, 1990). We extend this notion to suggest that words with similar meanings should be near each other in a semantic taxonomy, and in particular will likely share a hypernym as a near parent. Our classifier for (m, n)-cousins is derived from the algorithm and corpus given in (Ravichandran et al., 2005). In that work an efficient randomized algorithm is derived for computing clusters of similar nouns. We use a set of more than 1000 distinct clusters of English nouns collected by their algorithm over 70 million webpages6, with each noun i having a score representing its cosine similarity to the centroid </context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>D. Hindle. 1990. Noun classification from predicateargument structures. Proc. ACL-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lenat</author>
</authors>
<title>CYC: A Large-Scale Investment in Knowledge Infrastructure,</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<pages>33--35</pages>
<contexts>
<context position="1778" citStr="Lenat, 1995" startWordPosition="252" endWordPosition="253">et 2.1 at 84% precision, a relative error reduction of 70% over a non-joint algorithm using the same component classifiers. Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs. 1 Introduction The goal of capturing structured relational knowledge about lexical terms has been the motivating force underlying many projects in lexical acquisition, information extraction, and the construction of semantic taxonomies. Broad-coverage semantic taxonomies such as WordNet (Fellbaum, 1998) and CYC (Lenat, 1995) have been constructed by hand at great cost; while a crucial source of knowledge about the relations between words, these taxonomies still suffer from sparse coverage. Many algorithms with the potential for automatically extending lexical resources have been proposed, including work in lexical acquisition (Riloff and Shepherd, 1997; Roark and Charniak, 1998) and in discovering instances, named entities, and alternate glosses (Etzioni et al., 2005; Pasc¸a, 2005). Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hypony</context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>D. Lenat. 1995. CYC: A Large-Scale Investment in Knowledge Infrastructure, Communications of the ACM, 38:11, 33–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based Evaluation of MINIPAR. Workshop on the Evaluation of Parsing Systems,</title>
<date>1998</date>
<location>Granada,</location>
<contexts>
<context position="15499" citStr="Lin, 1998" startWordPosition="2582" endWordPosition="2583">ction of our hypernym and coordinate classifiers, respectively; in section 3.3 we outline the efficient algorithm we use to perform local search over hyponym-extended WordNets; and in section 3.4 we give an example of the implicit structure-based word sense disambiguation performed within our framework. 3.1 Hyponym Classification Our classifier for the hypernym relation is derived from the “hypernym-only” classifier described in (Snow et al., 2005). The features used for predicting the hypernym relationship are obtained by parsing a large corpus of newswire and encyclopedia text with MINIPAR (Lin, 1998). From the resulting dependency trees the evidence EHij for each word pair (i, j) is constructed; the evidence takes the form of a vector of counts of occurrences that each labeled syntactic dependency path was found as the shortest path connecting i and j in some dependency tree. The labeled training set is constructed by labeling the collected feature vectors as positive “known hypernym” or negative “known non-hypernym” examples using WordNet 2.0; 49,922 feature vectors were labeled as positive training examples, and 800,828 noun pairs were labeled as negative training examples. The model fo</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Dependency-based Evaluation of MINIPAR. Workshop on the Evaluation of Parsing Systems, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>S Zhao</author>
<author>L Qin</author>
<author>M Zhou</author>
</authors>
<title>Identifying Synonyms among Distributionally Similar Words.</title>
<date>2003</date>
<booktitle>Proc. IJCAI-03.</booktitle>
<contexts>
<context position="2448" citStr="Lin et al., 2003" startWordPosition="348" endWordPosition="351"> crucial source of knowledge about the relations between words, these taxonomies still suffer from sparse coverage. Many algorithms with the potential for automatically extending lexical resources have been proposed, including work in lexical acquisition (Riloff and Shepherd, 1997; Roark and Charniak, 1998) and in discovering instances, named entities, and alternate glosses (Etzioni et al., 2005; Pasc¸a, 2005). Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hyponyms (Hearst, 1992), meronyms (Girju, 2003), synonyms (Lin et al., 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al., 2003). Such classifiers use hand-written or automaticallyinduced patterns like Such NPy as NP,, or NPy like NP,, to determine, for example that NPy is a hyponym of NP,, (i.e., NPy IS-A NP,,). While such classifiers have achieved some degree of success, they frequently lack the global knowledge necessary to integrate their predictions into a complex taxonomy with multiple relations. Past work on semantic taxonomy induction includes the noun hypernym hierarchy created in (Caraballo, </context>
</contexts>
<marker>Lin, Zhao, Qin, Zhou, 2003</marker>
<rawString>D. Lin, S. Zhao, L. Qin and M. Zhou. 2003. Identifying Synonyms among Distributionally Similar Words. Proc. IJCAI-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasc¸a</author>
</authors>
<title>Finding Instance Names and Alternative Glosses on the Web: WordNet Reloaded. CICLing</title>
<date>2005</date>
<pages>280--292</pages>
<marker>Pasc¸a, 2005</marker>
<rawString>M. Pasc¸a. 2005. Finding Instance Names and Alternative Glosses on the Web: WordNet Reloaded. CICLing 2005, pp. 280-292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>P Pantel</author>
<author>E Hovy</author>
</authors>
<title>Randomized Algorithms and NLP: Using Locality Sensitive Hash Function for High Speed Noun Clustering.</title>
<date>2002</date>
<booktitle>Proc. ACL-2002.</booktitle>
<marker>Ravichandran, Pantel, Hovy, 2002</marker>
<rawString>D. Ravichandran, P. Pantel, and E. Hovy. 2002. Randomized Algorithms and NLP: Using Locality Sensitive Hash Function for High Speed Noun Clustering. Proc. ACL-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Shepherd</author>
</authors>
<title>A Corpus-Based Approach for Building Semantic Lexicons.</title>
<date>1997</date>
<booktitle>Proc EMNLP-1997.</booktitle>
<contexts>
<context position="2112" citStr="Riloff and Shepherd, 1997" startWordPosition="301" endWordPosition="304">g structured relational knowledge about lexical terms has been the motivating force underlying many projects in lexical acquisition, information extraction, and the construction of semantic taxonomies. Broad-coverage semantic taxonomies such as WordNet (Fellbaum, 1998) and CYC (Lenat, 1995) have been constructed by hand at great cost; while a crucial source of knowledge about the relations between words, these taxonomies still suffer from sparse coverage. Many algorithms with the potential for automatically extending lexical resources have been proposed, including work in lexical acquisition (Riloff and Shepherd, 1997; Roark and Charniak, 1998) and in discovering instances, named entities, and alternate glosses (Etzioni et al., 2005; Pasc¸a, 2005). Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hyponyms (Hearst, 1992), meronyms (Girju, 2003), synonyms (Lin et al., 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al., 2003). Such classifiers use hand-written or automaticallyinduced patterns like Such NPy as NP,, or NPy like NP,, to determine, for example that NPy is</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>E. Riloff and J. Shepherd. 1997. A Corpus-Based Approach for Building Semantic Lexicons. Proc EMNLP-1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>E Charniak</author>
</authors>
<title>Noun-phrase cooccurerence statistics for semi-automatic-semantic lexicon construction.</title>
<date>1998</date>
<booktitle>Proc. ACL-1998.</booktitle>
<contexts>
<context position="2139" citStr="Roark and Charniak, 1998" startWordPosition="305" endWordPosition="308">wledge about lexical terms has been the motivating force underlying many projects in lexical acquisition, information extraction, and the construction of semantic taxonomies. Broad-coverage semantic taxonomies such as WordNet (Fellbaum, 1998) and CYC (Lenat, 1995) have been constructed by hand at great cost; while a crucial source of knowledge about the relations between words, these taxonomies still suffer from sparse coverage. Many algorithms with the potential for automatically extending lexical resources have been proposed, including work in lexical acquisition (Riloff and Shepherd, 1997; Roark and Charniak, 1998) and in discovering instances, named entities, and alternate glosses (Etzioni et al., 2005; Pasc¸a, 2005). Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hyponyms (Hearst, 1992), meronyms (Girju, 2003), synonyms (Lin et al., 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al., 2003). Such classifiers use hand-written or automaticallyinduced patterns like Such NPy as NP,, or NPy like NP,, to determine, for example that NPy is a hyponym of NP,, (i.e., N</context>
</contexts>
<marker>Roark, Charniak, 1998</marker>
<rawString>B. Roark and E. Charniak. 1998. Noun-phrase cooccurerence statistics for semi-automatic-semantic lexicon construction. Proc. ACL-1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery. NIPS</title>
<date>2005</date>
<contexts>
<context position="15341" citStr="Snow et al., 2005" startWordPosition="2554" endWordPosition="2557">s in WordNet being in a hypernym relation, and the probability of two words in WordNet being in a coordinate relation. In sections 3.1 and 3.2 we describe the construction of our hypernym and coordinate classifiers, respectively; in section 3.3 we outline the efficient algorithm we use to perform local search over hyponym-extended WordNets; and in section 3.4 we give an example of the implicit structure-based word sense disambiguation performed within our framework. 3.1 Hyponym Classification Our classifier for the hypernym relation is derived from the “hypernym-only” classifier described in (Snow et al., 2005). The features used for predicting the hypernym relationship are obtained by parsing a large corpus of newswire and encyclopedia text with MINIPAR (Lin, 1998). From the resulting dependency trees the evidence EHij for each word pair (i, j) is constructed; the evidence takes the form of a vector of counts of occurrences that each labeled syntactic dependency path was found as the shortest path connecting i and j in some dependency tree. The labeled training set is constructed by labeling the collected feature vectors as positive “known hypernym” or negative “known non-hypernym” examples using W</context>
<context position="30521" citStr="Snow et al., 2005" startWordPosition="5080" endWordPosition="5083"> maintains high coarse-grained precision (92%) even at 20,000 links. 4.5 Comparison of inferred taxonomies and WordNet For our final evaluation we compare our learned taxonomies directly against the currently existing hypernym links in WordNet 2.1. In order to compare taxonomies we use a hand-labeled test 807 WN +10K +20K +30K +40K PRE 0.524 0.524 0.574 0.583 0.571 REC 0.165 0.165 0.203 0.211 0.211 F 0.251 0.251 0.300 0.309 0.307 Table 4: Taxonomy hypernym classification vs. WordNet 2.1 on hand-labeled testset set of over 5,000 noun pairs, randomly-sampled from newswire corpora (described in (Snow et al., 2005)). We measured the performance of both our inferred taxonomies and WordNet against this test set.8 The performance and comparison of the best WordNet classifier vs. our taxonomies is given in Table 4. Our best-performing inferred taxonomy on this test set is achieved after adding 30,000 novel hyponyms, achieving an 23% relative improvement in F-score over the WN2.1 classifier. 5 Conclusions We have presented an algorithm for inducing semantic taxonomies which attempts to globally optimize the entire structure of the taxonomy. Our probabilistic architecture also includes a new model for learnin</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. NIPS 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
<author>M Littman</author>
<author>J Bigham</author>
<author>V Shnayder</author>
</authors>
<title>Combining independent modules to solve multiple-choice synonym and analogy problems.</title>
<date>2003</date>
<booktitle>Proc. RANLP-2003,</booktitle>
<pages>482--489</pages>
<contexts>
<context position="2567" citStr="Turney et al., 2003" startWordPosition="367" endWordPosition="370">Many algorithms with the potential for automatically extending lexical resources have been proposed, including work in lexical acquisition (Riloff and Shepherd, 1997; Roark and Charniak, 1998) and in discovering instances, named entities, and alternate glosses (Etzioni et al., 2005; Pasc¸a, 2005). Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hyponyms (Hearst, 1992), meronyms (Girju, 2003), synonyms (Lin et al., 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al., 2003). Such classifiers use hand-written or automaticallyinduced patterns like Such NPy as NP,, or NPy like NP,, to determine, for example that NPy is a hyponym of NP,, (i.e., NPy IS-A NP,,). While such classifiers have achieved some degree of success, they frequently lack the global knowledge necessary to integrate their predictions into a complex taxonomy with multiple relations. Past work on semantic taxonomy induction includes the noun hypernym hierarchy created in (Caraballo, 2001), the part-whole taxonomies in (Girju, 2003), and a great deal of recent work described in (Buitelaar et al., 2005</context>
</contexts>
<marker>Turney, Littman, Bigham, Shnayder, 2003</marker>
<rawString>P. Turney, M. Littman, J. Bigham, and V. Shnayder. 2003. Combining independent modules to solve multiple-choice synonym and analogy problems. Proc. RANLP-2003, pp. 482–489.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>