<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.069271">
<title confidence="0.9969445">
CISUC-KIS: Tackling Message Polarity Classification with a Large and
Diverse set of Features
</title>
<author confidence="0.997868">
Jo˜ao Leal, Sara Pinto, Ana Bento, Hugo Gonc¸alo Oliveira, Paulo Gomes
</author>
<affiliation confidence="0.900660666666667">
CISUC, Department of Informatics Engineering
University of Coimbra
Portugal
</affiliation>
<email confidence="0.992613">
{jleal,sarap,arbc}@student.dei.uc.pt, {hroliv,pgomes}@dei.uc.pt
</email>
<sectionHeader confidence="0.993799" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999661076923077">
This paper presents the approach of the
CISUC-KIS team to the SemEval 2014
task on Sentiment Analysis in Twitter,
more precisely subtask B - Message Polar-
ity Classification. We followed a machine
learning approach where a SVM classifier
was trained from a large and diverse set
of features that included lexical, syntac-
tic, sentiment and semantic-based aspects.
This led to very interesting results which,
in different datasets, put us always in the
top-7 scores, including second position in
the LiveJournal2014 dataset.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998616">
Everyday people transmit their opinion in social
networks and microblogging services. Identifying
the sentiment transmitted in all those shared mes-
sages is of great utility for recognizing trends and
supporting decision making, key in areas such as
social marketing. Sentiment Analysis deals with
the computational treatment of sentiments in nat-
ural language text, often normalized to positive or
negative polarities. It is a very challenging task,
not only for machines, but also for humans.
SemEval 2014 is a semantic evaluation of Nat-
ural Language Processing (NLP) that comprises
several tasks. This paper describes our approach
to the Sentiment Analysis in Twitter task, which
comprises two subtasks: (A) Contextual Polarity
Disambiguation; and (B) Message Polarity Clas-
sification. We ended up addressing only task B,
which is more sentence oriented, as it targets the
polarity of the full messages and not individual
words in those messages.
</bodyText>
<footnote confidence="0.90759475">
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.99997925">
We tackled this task with a machine learning-
based approach, in which we first collect several
features from the analysis of the given text at sev-
eral levels. The collected features are then used to
learn a sentiment classification model, which can
be done with different algorithms. Features were
collected from several different resources, includ-
ing: sentiment lexicons, dictionaries and avail-
able APIs for this task. Moreover, since micro-
blogging text has particular characteristics that in-
crease the difficulty of NLP, we gave special fo-
cus on text pre-processing. Regarding the tested
features, they went from low-level ones, such as
punctuation and emoticons, to more high-level,
including topics extracted using topic modelling
techniques, as well features from sentiment lexi-
cons, some structured on plain words and others
based on WordNet, and thus structured on word
senses. Using the latter, we even explored word
sense disambiguation. We tested several learn-
ing algorithms with all these features, but Support
Vector Machines (SVM) led to the best results, so
it was used for the final evaluation.
In all our runs, a model was learned from
tweets, and no SMS were used for training. The
model’s performance was assessed with the F-
Score of positive and negative classes, with 10-
fold cross validation. In the official evaluation, we
achieved very interesting scores, namely: 74.46%
for the LiveJournal2014 (2nd place), 65.9% for the
SMS2013 (7th), 67.56% for the Twitter2013 (7th),
67.95% for the Twitter2014 (4th) and 55.49%
for the Twitter2014Sarcasm (4th) datasets, which
ranked us always among the top-7 participations.
The next section describes the external re-
sources exploited. Section 3 presents our approach
with more detail, and is followed by section 4,
where the experimental results are described. Sec-
tion 5 concludes with a brief balance and the main
lessons learned from our participation.
</bodyText>
<page confidence="0.983244">
166
</page>
<note confidence="0.8267385">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 166–170,
Dublin, Ireland, August 23-24, 2014.
</note>
<sectionHeader confidence="0.970096" genericHeader="method">
2 External resources
</sectionHeader>
<bodyText confidence="0.9999662">
We have used several external resources, includ-
ing not only several sentiment lexicons, but also
dictionaries that helped normalizing the text of the
tweets, as well as available APIs that already clas-
sify the sentiment transmitted by a piece of text.
</bodyText>
<subsectionHeader confidence="0.998838">
2.1 Sentiment Lexicons
</subsectionHeader>
<bodyText confidence="0.99995975">
We used several public handcrafted or semi-
automatically created sentiment lexicons, where
English words have assigned polarity values.
Those included lexicons structured in plain words,
namely Bing Liu’s Opinion Lexicon (Hu and Liu,
2004) (≈2,000 positive and 4,800 negative words),
the AFINN list (Nielsen, 2011) (≈2,500 words
with polarities between 5 and -5, 900 positive and
1,600 negative), the NRCEmoticon Lexicon (Mo-
hammad and Turney, 2010) (≈14,000 words,
their polarity, ≈2,300 positive, ≈3,300 negative,
and eight basic emotions), MPQA Subjectivity
Lexicon (Wilson et al., 2005) (≈2,700 positive
and ≈4,900 negative words), Sentiment140 Lexi-
con (Mohammad et al., 2013) (≈62,000 unigrams,
≈677,000 bigrams; ≈480,000 pairs), NRC Hash-
tag Lexicon (Mohammad et al., 2013) (≈54,000
unigrams; ≈316,000 bigrams; ≈308,000 pairs)
and labMT 1.0 (Dodds et al., 2011) (≈10,000
words).
We also used two resources with polar-
ities assigned automatically to a subset of
Princeton WordNet (Fellbaum, 1998) synsets,
namely SentiWordNet 3.0 (Baccianella et al.,
2010) (≈117,000 synsets with graded positive
and negatives strengths between 0 and 1), and
Q-WordNet (Agerri and Garcia-Serrano, 2010)
(≈7,400 positive and ≈8,100 negative senses).
</bodyText>
<subsectionHeader confidence="0.994368">
2.2 Dictionaries
</subsectionHeader>
<bodyText confidence="0.999967714285714">
These included handcrafted dictionaries with the
most common abbreviations, acronyms, emoti-
cons and web slang used on the Internet and their
meaning. Also, a list of regular expressions with
elongated words like ’loool’ and ’loloolll’, which
can be normalized to ’lol’, and a set of idiomatic
expressions and their corresponding polarity.
</bodyText>
<subsectionHeader confidence="0.997743">
2.3 APIs
</subsectionHeader>
<bodyText confidence="0.850851666666667">
Three public APIs were used, namely
Sentiment140 (Go et al., 2009),
SentimentAnalyzer1 and SentiStrength (Thel-
</bodyText>
<footnote confidence="0.93398">
1http://sentimentanalyzer.appspot.com/
</footnote>
<bodyText confidence="0.960539428571429">
wall et al., 2012). All of a them classify
a given text snippet as positive or negative.
Sentiment140 returns a value which can be 0
(negative polarity), 2 (neutral), and 4 (positive).
SentimentAnalyzer returns -1 (negative) or 1 (pos-
itive), and SentiStrength a strength value between
1 and 5 (positive) or -1 and -5 (negative).
</bodyText>
<sectionHeader confidence="0.993778" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999990461538462">
Our approach consisted of extracting lexical, syn-
tactic, semantic and sentiment information from
the tweets and using it in the form of features, for
learning a sentiment classifier that would detect
polarity in messages. This is a popular approach
for these types of tasks, followed by other sys-
tems, including the winner of SemEval 2013 (Mo-
hammad et al., 2013), where a variety of surface-
form, semantic, and sentiment features was used.
Our set of features is similar for the base classifier
are similar, except that we included additional fea-
tures that take advantage of word disambiguation
to get the polarity of target word senses.
</bodyText>
<subsectionHeader confidence="0.979766">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.999938666666667">
Among the collected features, some were related
to the content of the tweets and others were ob-
tained from the sentiment lexicons.
</bodyText>
<subsectionHeader confidence="0.893592">
3.1.1 Content Features
</subsectionHeader>
<bodyText confidence="0.9999735">
The tweets were tokenized and part-of-
speech (POS) tagged with the CMU ARK
Twitter NLP Tool (Gimpel et al., 2011) and
Stanford CoreNLP (Toutanova and Manning,
2000). Each tweet was represented as a feature
vector containing the following group of features:
</bodyText>
<listItem confidence="0.980550117647059">
(i) emoticons (presence or absence, sum of all
positive and negative polarities associated with
each, polarity of the last emoticon of each tweet);
(ii) length (total length of the tweet, average
length per word, number of words per tweet);
(iii) elongated words (number of all the words
containing a repeated character more than two
times); (iv) hashtags (total number of hashtags);
(v) topic modelling (id of the corresponding
topic); (vi) capital letters (number of words in
which all letters are capitalized); (vii) nega-
tion (number of words that reverse polarity to
a negative context, such as ’no’ or ’never’);
(viii) punctuation (number of punctuation se-
quences with only exclamation points, question
marks or both, ASCII code of the most common
punctuation and of the last punctuation in every
</listItem>
<page confidence="0.981322">
167
</page>
<bodyText confidence="0.9235275">
tweet); (ix) dashes and asterisks (number of words
surrounded by dashes or asterisks, such as ’*yay*’
or ’-me-’); (x) POS (number of nouns, adjectives,
adverbs, verbs and interjections).
</bodyText>
<subsectionHeader confidence="0.552744">
3.1.2 Lexicon Features
</subsectionHeader>
<bodyText confidence="0.999985714285714">
A wide range of features were created using the
lexicons. For each tweet and for each lexicon the
following set of features were generated: (i) to-
tal number of positive and negative opinion words;
(ii) sum of all positive/negative polarity values in
the tweet; (iii) the highest positive/negative po-
larity value in the tweet; and (iv) the polarity
value of the last polarity word. Those features
were collected for: unigrams, bigrams and pairs
(only on the NRC Hashtag Lexicon and Senti-
ment140), nouns, adjectives, verbs, interjections,
hashtags, all caps tokens (e.g ’GO AWAY’), elon-
gated words, asterisks and dashes tokens.
Different approaches were followed to get the
polarity of each word from the wordnets. From
SentiWordNet, we computed combined scores of
all senses, with decreasing weights for lower
ranked senses, as well as the scores of the first
sense only, both considering: (i) positive and neg-
ative; (ii) just positive; (iii) just negative scores.
Moreover, we performed word sense disambigua-
tion using the full WordNet 3.0 to get the previ-
ous scores for the selected sense. For this pur-
pose, we applied the Lesk Algorithm adapted to
wordnets (Banerjee and Pedersen, 2002), using all
the tweet’s content words as the word context, and
the synset words, gloss words and words in related
synsets as the synset’s context. Given that Senti-
WordNet is aligned to WordNet 3.0, after select-
ing the most adequate sense of the word, we could
get its polarity scores. From Q-WordNet, similar
scores were computed but, since it doesn’t use a
graded strength and only classifies word senses as
positive or negative, there were just positive or just
negative scores.
</bodyText>
<subsectionHeader confidence="0.987176">
3.2 Classifier
</subsectionHeader>
<bodyText confidence="0.998812777777778">
In our final approach we used a SVM (Fan et al.,
2008) which is an effective solution in high dimen-
sional spaces and proved to be the best learning
algorithm for this task. We tested various kernels
(e.g. PolyKernel, RBF) and their parameters with
cross validation on the training data. Given the re-
sults, we confirmed that the RBF kernel, computed
according to equation 1, is most effective with a
C = 4 and a γ = 0.0003.
</bodyText>
<equation confidence="0.976886">
K(xz, x3) = Φ(xz)T Φ(x3) = exp(−γ||xz−x3||2)
(1)
</equation>
<bodyText confidence="0.999907666666667">
Considering we are working on a multi-class
classification problem, we implemented the “one-
against-one” approach (Knerr et al., 1990) where
#classes * (#classes − 1)/2 classifiers are con-
structed and each one trains data from classes.
Due to the non-scale invariant nature of SVM al-
gorithms, we’ve scaled our data on each attribute
to have µ = 0 and Q = 1 and took caution against
class unbalance.
</bodyText>
<sectionHeader confidence="0.999583" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999927625">
For training the SVM classifier, we used a set of
9,634 tweets with a known polarity and also 1,281
tweets as development test to grid search the best
parameters. No SMS messages were used as train-
ing or as development test. For the scorer function,
we used a macro-averaged F-Score of positive and
negative classes – the one made available and used
by the task organizers.
</bodyText>
<subsectionHeader confidence="0.998266">
4.1 Some Results
</subsectionHeader>
<bodyText confidence="0.999983272727273">
The results obtained by the system were 70.41%
on the training set (using 10-Folds) and 71.03%
on the development set, after train on the train-
ing set. When tested against the training set,
after train in the same set, we get a score of
84.32%, which could indicate a case of under-
fitting. Though, our classifier generalized well,
given that we got a 74.46% official score on Live-
Journal2014, second in that category. On the other
hand, our experiments with decision trees showed
that they couldn’t generalize so well, although
they achieved scores of &gt;99 on the training set. In
the SMS category, our system would benefit from
a specific data set in the training phase. Yet, it still
managed to reach 7th place in that category. In the
sarcasm category our submission ranked 4th, with
a score of 58.16%, 2.69% below the best rank. On
the Twitter2014 dataset, we scored 67.95% (4th),
which is slightly below our prediction based on
development tests. A possible explanation is that
we might have over-fitted the classifier parameters
when grid searching.
</bodyText>
<subsectionHeader confidence="0.984434">
4.2 Features Relevance
</subsectionHeader>
<bodyText confidence="0.999869333333333">
In order to get some insights on the most relevant
group of features, we did a series of experiments
where each group of features were removed for
</bodyText>
<page confidence="0.996406">
168
</page>
<bodyText confidence="0.999971666666667">
the classification, then tested against the original
score. We concluded that the lexicon related fea-
tures contribute highly to the performance of our
system, including the set of features with n-grams
and POS. Clusters, sport score, asterisks and elon-
gated words provide little gains but, on the other
hand, emoticons and hashtags showed some im-
portance and provided enough new information
for the system to learn. The API information is
largely captured by some of our features and that
makes it much less discriminating than what they
would be on their own, but still worth using for
the small gain. We also observed that it is best to
create a diversified set of lexicon features with ex-
tra very specific targeted features, such as punc-
tuation, instead of focusing on using a specific
lexicon alone. Even though they usually over-
lap in information and may perform worse indi-
vidually than a hand-refined single dictionary ap-
proach, they complement each other and that re-
sults in larger gains.
</bodyText>
<subsectionHeader confidence="0.999595">
4.3 Selected Parameters
</subsectionHeader>
<bodyText confidence="0.9999966">
For the parameter values, we did a grid search
using the development set as a test. We also
found that large values of C (25) and small -y val-
ues (0.0001) performed worse than smaller values
of C (4) with a slightly higher -y (0.0003) when
using the development set but not when using the
training set under K-Folds. For the official eval-
uation, we opted for the best-performing results
on the development set. Using intermediate val-
ues accomplished worse results in either case.
</bodyText>
<sectionHeader confidence="0.926699" genericHeader="method">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999963684210527">
We have described the work developed for the sub-
task B of SemEval 2014 Sentiment Analysis in
Twitter task. We followed a machine learning ap-
proach, with a diversified set of features, which
tend to complemented each other. Some of the
main takeaways are that the most important fea-
tures are the lexicon related ones, including the
n-grams and POS tags. Due to time constraints,
we could not take strong conclusions on the impact
of the word sense disambiguation related features
alone. As those are probably the most differentiat-
ing features of our classifier, this is something we
wish to target in the future.
To conclude, we have achieved very interesting
results in terms of overall classification. Consider-
ing that this was our first participation in such an
evaluation, we make a very positive balance. And
of course, we are looking forward for upcoming
editions of this task.
</bodyText>
<sectionHeader confidence="0.945988" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9893035">
This work was supported by the iCIS project
(CENTRO-07-ST24-FEDER-002003), co-
financed by QREN, in the scope of the Mais
Centro Program and European Union’s FEDER.
</bodyText>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997925209302326">
Rodrigo Agerri and Ana Garc´ıa-Serrano. 2010. Q-
wordnet: Extracting polarity from WordNet senses.
In Proceedings of the 7th International Confer-
ence on Language Resources and Evaluation, LREC
2010, pages 2300–2305, La Valletta, Malta. ELRA.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the 7th International Confer-
ence on Language Resources and Evaluation, LREC
2010, pages 2200–2204, Valletta, Malta. ELRA.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted Lesk algorithm for word sense disambigua-
tion using WordNet. In Proceedings of the 3rd Inter-
national Conference on Computational Linguistics
and Intelligent Text Processing (CICLing 2002), vol-
ume 2276 of LNCS, pages 136–145, London, UK.
Springer.
Peter Sheridan Dodds, Kameron Decker Harris, Is-
abel M. Kloumann, Catherine A. Bliss, and Christo-
pher M. Danforth. 2011. Temporal patterns of hap-
piness and information in a global social network:
Hedonometrics and Twitter. PLoS ONE, 6(12), De-
cember.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874, June.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flani-
gan, and Noah A Smith. 2011. Part-of-speech tag-
ging for Twitter: Annotation, features, and exper-
iments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, ACL 2011, pages 42–47. ACL Press.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Technical report, Stanford University.
</reference>
<page confidence="0.986269">
169
</page>
<reference confidence="0.99949005882353">
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD 2004, pages
168–177.
Stefan Knerr, L´eon Personnaz, and G´erard Dreyfus.
1990. Single-layer learning revisited: a stepwise
procedure for building and training neural network.
In Proceedings of the NATO Advanced Research
Workshop on Neurocomputing, Algorithms, Archi-
tectures and Applications, Nato ASI, Computer and
Systems Sciences, pages 41–50. Springer.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Using
Mechanical Turk to create an emotion lexicon. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, CAAGET ’10, pages 26–34,
Los Angeles, CA. ACL Press.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of tweets. In 2nd Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion, SemEval 2013, pages 321–327, Atlanta, Geor-
gia, USA, June. ACL Press.
Finn ˚Arup Nielsen. 2011. A new anew: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ’Mak-
ing Sense of Microposts’: Big things come in small
packages, pages 93–98, May.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2012. Sentiment strength detection for the
social web. Journal of the American Society for In-
formation Science and Technology, 63(1):163–173,
January.
Kristina Toutanova and Christopher D Manning. 2000.
Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger. In Proceedings
of the 2000 Joint SIGDAT conference on Empirical
methods in natural language processing and very
large corpora, EMNLP 2000, pages 63–70. ACL
Press.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the
Conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, HLT ’05, pages 347–354, Vancouver, British
Columbia, Canada. ACL Press.
</reference>
<page confidence="0.997427">
170
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.297438">
<title confidence="0.9679445">CISUC-KIS: Tackling Message Polarity Classification with a Large Diverse set of Features</title>
<author confidence="0.918769">Sara Pinto Leal</author>
<author confidence="0.918769">Ana Bento</author>
<author confidence="0.918769">Hugo Oliveira</author>
<author confidence="0.918769">Paulo</author>
<affiliation confidence="0.833696">CISUC, Department of Informatics University of Portugal</affiliation>
<abstract confidence="0.956649357142857">This paper presents the approach of the CISUC-KIS team to the SemEval 2014 task on Sentiment Analysis in Twitter, more precisely subtask B - Message Polarity Classification. We followed a machine learning approach where a SVM classifier was trained from a large and diverse set of features that included lexical, syntactic, sentiment and semantic-based aspects. This led to very interesting results which, in different datasets, put us always in the top-7 scores, including second position in the LiveJournal2014 dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rodrigo Agerri</author>
<author>Ana Garc´ıa-Serrano</author>
</authors>
<title>Qwordnet: Extracting polarity from WordNet senses.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation, LREC 2010,</booktitle>
<pages>2300--2305</pages>
<location>La Valletta, Malta. ELRA.</location>
<marker>Agerri, Garc´ıa-Serrano, 2010</marker>
<rawString>Rodrigo Agerri and Ana Garc´ıa-Serrano. 2010. Qwordnet: Extracting polarity from WordNet senses. In Proceedings of the 7th International Conference on Language Resources and Evaluation, LREC 2010, pages 2300–2305, La Valletta, Malta. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation, LREC 2010,</booktitle>
<pages>2200--2204</pages>
<location>Valletta, Malta. ELRA.</location>
<contexts>
<context position="5420" citStr="Baccianella et al., 2010" startWordPosition="804" endWordPosition="807">d Turney, 2010) (≈14,000 words, their polarity, ≈2,300 positive, ≈3,300 negative, and eight basic emotions), MPQA Subjectivity Lexicon (Wilson et al., 2005) (≈2,700 positive and ≈4,900 negative words), Sentiment140 Lexicon (Mohammad et al., 2013) (≈62,000 unigrams, ≈677,000 bigrams; ≈480,000 pairs), NRC Hashtag Lexicon (Mohammad et al., 2013) (≈54,000 unigrams; ≈316,000 bigrams; ≈308,000 pairs) and labMT 1.0 (Dodds et al., 2011) (≈10,000 words). We also used two resources with polarities assigned automatically to a subset of Princeton WordNet (Fellbaum, 1998) synsets, namely SentiWordNet 3.0 (Baccianella et al., 2010) (≈117,000 synsets with graded positive and negatives strengths between 0 and 1), and Q-WordNet (Agerri and Garcia-Serrano, 2010) (≈7,400 positive and ≈8,100 negative senses). 2.2 Dictionaries These included handcrafted dictionaries with the most common abbreviations, acronyms, emoticons and web slang used on the Internet and their meaning. Also, a list of regular expressions with elongated words like ’loool’ and ’loloolll’, which can be normalized to ’lol’, and a set of idiomatic expressions and their corresponding polarity. 2.3 APIs Three public APIs were used, namely Sentiment140 (Go et al.</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the 7th International Conference on Language Resources and Evaluation, LREC 2010, pages 2200–2204, Valletta, Malta. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted Lesk algorithm for word sense disambiguation using WordNet.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Computational Linguistics and Intelligent Text Processing (CICLing</booktitle>
<volume>2276</volume>
<pages>136--145</pages>
<publisher>UK. Springer.</publisher>
<location>London,</location>
<contexts>
<context position="9714" citStr="Banerjee and Pedersen, 2002" startWordPosition="1481" endWordPosition="1484">ns (e.g ’GO AWAY’), elongated words, asterisks and dashes tokens. Different approaches were followed to get the polarity of each word from the wordnets. From SentiWordNet, we computed combined scores of all senses, with decreasing weights for lower ranked senses, as well as the scores of the first sense only, both considering: (i) positive and negative; (ii) just positive; (iii) just negative scores. Moreover, we performed word sense disambiguation using the full WordNet 3.0 to get the previous scores for the selected sense. For this purpose, we applied the Lesk Algorithm adapted to wordnets (Banerjee and Pedersen, 2002), using all the tweet’s content words as the word context, and the synset words, gloss words and words in related synsets as the synset’s context. Given that SentiWordNet is aligned to WordNet 3.0, after selecting the most adequate sense of the word, we could get its polarity scores. From Q-WordNet, similar scores were computed but, since it doesn’t use a graded strength and only classifies word senses as positive or negative, there were just positive or just negative scores. 3.2 Classifier In our final approach we used a SVM (Fan et al., 2008) which is an effective solution in high dimensiona</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted Lesk algorithm for word sense disambiguation using WordNet. In Proceedings of the 3rd International Conference on Computational Linguistics and Intelligent Text Processing (CICLing 2002), volume 2276 of LNCS, pages 136–145, London, UK. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Sheridan Dodds</author>
<author>Kameron Decker Harris</author>
<author>Isabel M Kloumann</author>
<author>Catherine A Bliss</author>
<author>Christopher M Danforth</author>
</authors>
<title>Temporal patterns of happiness and information in a global social network: Hedonometrics and Twitter.</title>
<date>2011</date>
<journal>PLoS ONE,</journal>
<volume>6</volume>
<issue>12</issue>
<contexts>
<context position="5227" citStr="Dodds et al., 2011" startWordPosition="776" endWordPosition="779">0 positive and 4,800 negative words), the AFINN list (Nielsen, 2011) (≈2,500 words with polarities between 5 and -5, 900 positive and 1,600 negative), the NRCEmoticon Lexicon (Mohammad and Turney, 2010) (≈14,000 words, their polarity, ≈2,300 positive, ≈3,300 negative, and eight basic emotions), MPQA Subjectivity Lexicon (Wilson et al., 2005) (≈2,700 positive and ≈4,900 negative words), Sentiment140 Lexicon (Mohammad et al., 2013) (≈62,000 unigrams, ≈677,000 bigrams; ≈480,000 pairs), NRC Hashtag Lexicon (Mohammad et al., 2013) (≈54,000 unigrams; ≈316,000 bigrams; ≈308,000 pairs) and labMT 1.0 (Dodds et al., 2011) (≈10,000 words). We also used two resources with polarities assigned automatically to a subset of Princeton WordNet (Fellbaum, 1998) synsets, namely SentiWordNet 3.0 (Baccianella et al., 2010) (≈117,000 synsets with graded positive and negatives strengths between 0 and 1), and Q-WordNet (Agerri and Garcia-Serrano, 2010) (≈7,400 positive and ≈8,100 negative senses). 2.2 Dictionaries These included handcrafted dictionaries with the most common abbreviations, acronyms, emoticons and web slang used on the Internet and their meaning. Also, a list of regular expressions with elongated words like ’l</context>
</contexts>
<marker>Dodds, Harris, Kloumann, Bliss, Danforth, 2011</marker>
<rawString>Peter Sheridan Dodds, Kameron Decker Harris, Isabel M. Kloumann, Catherine A. Bliss, and Christopher M. Danforth. 2011. Temporal patterns of happiness and information in a global social network: Hedonometrics and Twitter. PLoS ONE, 6(12), December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="10264" citStr="Fan et al., 2008" startWordPosition="1576" endWordPosition="1579">he Lesk Algorithm adapted to wordnets (Banerjee and Pedersen, 2002), using all the tweet’s content words as the word context, and the synset words, gloss words and words in related synsets as the synset’s context. Given that SentiWordNet is aligned to WordNet 3.0, after selecting the most adequate sense of the word, we could get its polarity scores. From Q-WordNet, similar scores were computed but, since it doesn’t use a graded strength and only classifies word senses as positive or negative, there were just positive or just negative scores. 3.2 Classifier In our final approach we used a SVM (Fan et al., 2008) which is an effective solution in high dimensional spaces and proved to be the best learning algorithm for this task. We tested various kernels (e.g. PolyKernel, RBF) and their parameters with cross validation on the training data. Given the results, we confirmed that the RBF kernel, computed according to equation 1, is most effective with a C = 4 and a γ = 0.0003. K(xz, x3) = Φ(xz)T Φ(x3) = exp(−γ||xz−x3||2) (1) Considering we are working on a multi-class classification problem, we implemented the “oneagainst-one” approach (Knerr et al., 1990) where #classes * (#classes − 1)/2 classifiers ar</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874, June.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication).</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<pages>42--47</pages>
<publisher>ACL Press.</publisher>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A Smith. 2011. Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papersVolume 2, ACL 2011, pages 42–47. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="6027" citStr="Go et al., 2009" startWordPosition="894" endWordPosition="897">l., 2010) (≈117,000 synsets with graded positive and negatives strengths between 0 and 1), and Q-WordNet (Agerri and Garcia-Serrano, 2010) (≈7,400 positive and ≈8,100 negative senses). 2.2 Dictionaries These included handcrafted dictionaries with the most common abbreviations, acronyms, emoticons and web slang used on the Internet and their meaning. Also, a list of regular expressions with elongated words like ’loool’ and ’loloolll’, which can be normalized to ’lol’, and a set of idiomatic expressions and their corresponding polarity. 2.3 APIs Three public APIs were used, namely Sentiment140 (Go et al., 2009), SentimentAnalyzer1 and SentiStrength (Thel1http://sentimentanalyzer.appspot.com/ wall et al., 2012). All of a them classify a given text snippet as positive or negative. Sentiment140 returns a value which can be 0 (negative polarity), 2 (neutral), and 4 (positive). SentimentAnalyzer returns -1 (negative) or 1 (positive), and SentiStrength a strength value between 1 and 5 (positive) or -1 and -5 (negative). 3 Approach Our approach consisted of extracting lexical, syntactic, semantic and sentiment information from the tweets and using it in the form of features, for learning a sentiment classi</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD</booktitle>
<pages>168--177</pages>
<contexts>
<context position="4601" citStr="Hu and Liu, 2004" startWordPosition="685" endWordPosition="688">tic Evaluation (SemEval 2014), pages 166–170, Dublin, Ireland, August 23-24, 2014. 2 External resources We have used several external resources, including not only several sentiment lexicons, but also dictionaries that helped normalizing the text of the tweets, as well as available APIs that already classify the sentiment transmitted by a piece of text. 2.1 Sentiment Lexicons We used several public handcrafted or semiautomatically created sentiment lexicons, where English words have assigned polarity values. Those included lexicons structured in plain words, namely Bing Liu’s Opinion Lexicon (Hu and Liu, 2004) (≈2,000 positive and 4,800 negative words), the AFINN list (Nielsen, 2011) (≈2,500 words with polarities between 5 and -5, 900 positive and 1,600 negative), the NRCEmoticon Lexicon (Mohammad and Turney, 2010) (≈14,000 words, their polarity, ≈2,300 positive, ≈3,300 negative, and eight basic emotions), MPQA Subjectivity Lexicon (Wilson et al., 2005) (≈2,700 positive and ≈4,900 negative words), Sentiment140 Lexicon (Mohammad et al., 2013) (≈62,000 unigrams, ≈677,000 bigrams; ≈480,000 pairs), NRC Hashtag Lexicon (Mohammad et al., 2013) (≈54,000 unigrams; ≈316,000 bigrams; ≈308,000 pairs) and labM</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2004, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Knerr</author>
<author>L´eon Personnaz</author>
<author>G´erard Dreyfus</author>
</authors>
<title>Single-layer learning revisited: a stepwise procedure for building and training neural network.</title>
<date>1990</date>
<booktitle>In Proceedings of the NATO Advanced Research Workshop on Neurocomputing, Algorithms, Architectures and Applications, Nato ASI, Computer and Systems Sciences,</booktitle>
<pages>41--50</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10815" citStr="Knerr et al., 1990" startWordPosition="1669" endWordPosition="1672">.2 Classifier In our final approach we used a SVM (Fan et al., 2008) which is an effective solution in high dimensional spaces and proved to be the best learning algorithm for this task. We tested various kernels (e.g. PolyKernel, RBF) and their parameters with cross validation on the training data. Given the results, we confirmed that the RBF kernel, computed according to equation 1, is most effective with a C = 4 and a γ = 0.0003. K(xz, x3) = Φ(xz)T Φ(x3) = exp(−γ||xz−x3||2) (1) Considering we are working on a multi-class classification problem, we implemented the “oneagainst-one” approach (Knerr et al., 1990) where #classes * (#classes − 1)/2 classifiers are constructed and each one trains data from classes. Due to the non-scale invariant nature of SVM algorithms, we’ve scaled our data on each attribute to have µ = 0 and Q = 1 and took caution against class unbalance. 4 Experiments For training the SVM classifier, we used a set of 9,634 tweets with a known polarity and also 1,281 tweets as development test to grid search the best parameters. No SMS messages were used as training or as development test. For the scorer function, we used a macro-averaged F-Score of positive and negative classes – the</context>
</contexts>
<marker>Knerr, Personnaz, Dreyfus, 1990</marker>
<rawString>Stefan Knerr, L´eon Personnaz, and G´erard Dreyfus. 1990. Single-layer learning revisited: a stepwise procedure for building and training neural network. In Proceedings of the NATO Advanced Research Workshop on Neurocomputing, Algorithms, Architectures and Applications, Nato ASI, Computer and Systems Sciences, pages 41–50. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET ’10,</booktitle>
<pages>26--34</pages>
<publisher>ACL Press.</publisher>
<location>Los Angeles, CA.</location>
<contexts>
<context position="4810" citStr="Mohammad and Turney, 2010" startWordPosition="716" endWordPosition="720"> dictionaries that helped normalizing the text of the tweets, as well as available APIs that already classify the sentiment transmitted by a piece of text. 2.1 Sentiment Lexicons We used several public handcrafted or semiautomatically created sentiment lexicons, where English words have assigned polarity values. Those included lexicons structured in plain words, namely Bing Liu’s Opinion Lexicon (Hu and Liu, 2004) (≈2,000 positive and 4,800 negative words), the AFINN list (Nielsen, 2011) (≈2,500 words with polarities between 5 and -5, 900 positive and 1,600 negative), the NRCEmoticon Lexicon (Mohammad and Turney, 2010) (≈14,000 words, their polarity, ≈2,300 positive, ≈3,300 negative, and eight basic emotions), MPQA Subjectivity Lexicon (Wilson et al., 2005) (≈2,700 positive and ≈4,900 negative words), Sentiment140 Lexicon (Mohammad et al., 2013) (≈62,000 unigrams, ≈677,000 bigrams; ≈480,000 pairs), NRC Hashtag Lexicon (Mohammad et al., 2013) (≈54,000 unigrams; ≈316,000 bigrams; ≈308,000 pairs) and labMT 1.0 (Dodds et al., 2011) (≈10,000 words). We also used two resources with polarities assigned automatically to a subset of Princeton WordNet (Fellbaum, 1998) synsets, namely SentiWordNet 3.0 (Baccianella et </context>
</contexts>
<marker>Mohammad, Turney, 2010</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2010. Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET ’10, pages 26–34, Los Angeles, CA. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the state-ofthe-art in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In 2nd Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation, SemEval</booktitle>
<pages>321--327</pages>
<publisher>ACL Press.</publisher>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="5041" citStr="Mohammad et al., 2013" startWordPosition="749" endWordPosition="752">created sentiment lexicons, where English words have assigned polarity values. Those included lexicons structured in plain words, namely Bing Liu’s Opinion Lexicon (Hu and Liu, 2004) (≈2,000 positive and 4,800 negative words), the AFINN list (Nielsen, 2011) (≈2,500 words with polarities between 5 and -5, 900 positive and 1,600 negative), the NRCEmoticon Lexicon (Mohammad and Turney, 2010) (≈14,000 words, their polarity, ≈2,300 positive, ≈3,300 negative, and eight basic emotions), MPQA Subjectivity Lexicon (Wilson et al., 2005) (≈2,700 positive and ≈4,900 negative words), Sentiment140 Lexicon (Mohammad et al., 2013) (≈62,000 unigrams, ≈677,000 bigrams; ≈480,000 pairs), NRC Hashtag Lexicon (Mohammad et al., 2013) (≈54,000 unigrams; ≈316,000 bigrams; ≈308,000 pairs) and labMT 1.0 (Dodds et al., 2011) (≈10,000 words). We also used two resources with polarities assigned automatically to a subset of Princeton WordNet (Fellbaum, 1998) synsets, namely SentiWordNet 3.0 (Baccianella et al., 2010) (≈117,000 synsets with graded positive and negatives strengths between 0 and 1), and Q-WordNet (Agerri and Garcia-Serrano, 2010) (≈7,400 positive and ≈8,100 negative senses). 2.2 Dictionaries These included handcrafted d</context>
<context position="6812" citStr="Mohammad et al., 2013" startWordPosition="1015" endWordPosition="1019">egative. Sentiment140 returns a value which can be 0 (negative polarity), 2 (neutral), and 4 (positive). SentimentAnalyzer returns -1 (negative) or 1 (positive), and SentiStrength a strength value between 1 and 5 (positive) or -1 and -5 (negative). 3 Approach Our approach consisted of extracting lexical, syntactic, semantic and sentiment information from the tweets and using it in the form of features, for learning a sentiment classifier that would detect polarity in messages. This is a popular approach for these types of tasks, followed by other systems, including the winner of SemEval 2013 (Mohammad et al., 2013), where a variety of surfaceform, semantic, and sentiment features was used. Our set of features is similar for the base classifier are similar, except that we included additional features that take advantage of word disambiguation to get the polarity of target word senses. 3.1 Features Among the collected features, some were related to the content of the tweets and others were obtained from the sentiment lexicons. 3.1.1 Content Features The tweets were tokenized and part-ofspeech (POS) tagged with the CMU ARK Twitter NLP Tool (Gimpel et al., 2011) and Stanford CoreNLP (Toutanova and Manning, </context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-ofthe-art in sentiment analysis of tweets. In 2nd Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation, SemEval 2013, pages 321–327, Atlanta, Georgia, USA, June. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn ˚Arup Nielsen</author>
</authors>
<title>A new anew: Evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the ESWC2011 Workshop on ’Making Sense of Microposts’: Big things come in small packages,</booktitle>
<pages>93--98</pages>
<contexts>
<context position="4676" citStr="Nielsen, 2011" startWordPosition="698" endWordPosition="699">014. 2 External resources We have used several external resources, including not only several sentiment lexicons, but also dictionaries that helped normalizing the text of the tweets, as well as available APIs that already classify the sentiment transmitted by a piece of text. 2.1 Sentiment Lexicons We used several public handcrafted or semiautomatically created sentiment lexicons, where English words have assigned polarity values. Those included lexicons structured in plain words, namely Bing Liu’s Opinion Lexicon (Hu and Liu, 2004) (≈2,000 positive and 4,800 negative words), the AFINN list (Nielsen, 2011) (≈2,500 words with polarities between 5 and -5, 900 positive and 1,600 negative), the NRCEmoticon Lexicon (Mohammad and Turney, 2010) (≈14,000 words, their polarity, ≈2,300 positive, ≈3,300 negative, and eight basic emotions), MPQA Subjectivity Lexicon (Wilson et al., 2005) (≈2,700 positive and ≈4,900 negative words), Sentiment140 Lexicon (Mohammad et al., 2013) (≈62,000 unigrams, ≈677,000 bigrams; ≈480,000 pairs), NRC Hashtag Lexicon (Mohammad et al., 2013) (≈54,000 unigrams; ≈316,000 bigrams; ≈308,000 pairs) and labMT 1.0 (Dodds et al., 2011) (≈10,000 words). We also used two resources with</context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn ˚Arup Nielsen. 2011. A new anew: Evaluation of a word list for sentiment analysis in microblogs. In Proceedings of the ESWC2011 Workshop on ’Making Sense of Microposts’: Big things come in small packages, pages 93–98, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
<author>Georgios Paltoglou</author>
</authors>
<title>Sentiment strength detection for the social web.</title>
<date>2012</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>63</volume>
<issue>1</issue>
<marker>Thelwall, Buckley, Paltoglou, 2012</marker>
<rawString>Mike Thelwall, Kevan Buckley, and Georgios Paltoglou. 2012. Sentiment strength detection for the social web. Journal of the American Society for Information Science and Technology, 63(1):163–173, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language</booktitle>
<pages>63--70</pages>
<publisher>ACL Press.</publisher>
<contexts>
<context position="7417" citStr="Toutanova and Manning, 2000" startWordPosition="1115" endWordPosition="1118">(Mohammad et al., 2013), where a variety of surfaceform, semantic, and sentiment features was used. Our set of features is similar for the base classifier are similar, except that we included additional features that take advantage of word disambiguation to get the polarity of target word senses. 3.1 Features Among the collected features, some were related to the content of the tweets and others were obtained from the sentiment lexicons. 3.1.1 Content Features The tweets were tokenized and part-ofspeech (POS) tagged with the CMU ARK Twitter NLP Tool (Gimpel et al., 2011) and Stanford CoreNLP (Toutanova and Manning, 2000). Each tweet was represented as a feature vector containing the following group of features: (i) emoticons (presence or absence, sum of all positive and negative polarities associated with each, polarity of the last emoticon of each tweet); (ii) length (total length of the tweet, average length per word, number of words per tweet); (iii) elongated words (number of all the words containing a repeated character more than two times); (iv) hashtags (total number of hashtags); (v) topic modelling (id of the corresponding topic); (vi) capital letters (number of words in which all letters are capital</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora, EMNLP 2000, pages 63–70. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>347--354</pages>
<publisher>ACL Press.</publisher>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="4951" citStr="Wilson et al., 2005" startWordPosition="736" endWordPosition="739">of text. 2.1 Sentiment Lexicons We used several public handcrafted or semiautomatically created sentiment lexicons, where English words have assigned polarity values. Those included lexicons structured in plain words, namely Bing Liu’s Opinion Lexicon (Hu and Liu, 2004) (≈2,000 positive and 4,800 negative words), the AFINN list (Nielsen, 2011) (≈2,500 words with polarities between 5 and -5, 900 positive and 1,600 negative), the NRCEmoticon Lexicon (Mohammad and Turney, 2010) (≈14,000 words, their polarity, ≈2,300 positive, ≈3,300 negative, and eight basic emotions), MPQA Subjectivity Lexicon (Wilson et al., 2005) (≈2,700 positive and ≈4,900 negative words), Sentiment140 Lexicon (Mohammad et al., 2013) (≈62,000 unigrams, ≈677,000 bigrams; ≈480,000 pairs), NRC Hashtag Lexicon (Mohammad et al., 2013) (≈54,000 unigrams; ≈316,000 bigrams; ≈308,000 pairs) and labMT 1.0 (Dodds et al., 2011) (≈10,000 words). We also used two resources with polarities assigned automatically to a subset of Princeton WordNet (Fellbaum, 1998) synsets, namely SentiWordNet 3.0 (Baccianella et al., 2010) (≈117,000 synsets with graded positive and negatives strengths between 0 and 1), and Q-WordNet (Agerri and Garcia-Serrano, 2010) (</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 347–354, Vancouver, British Columbia, Canada. ACL Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>