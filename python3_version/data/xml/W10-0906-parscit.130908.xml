<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.050321">
<title confidence="0.998906">
Open-domain Commonsense Reasoning Using Discourse Relations from a
Corpus of Weblog Stories
</title>
<author confidence="0.997653">
Matt Gerber Andrew S. Gordon and Kenji Sagae
</author>
<affiliation confidence="0.9997435">
Department of Computer Science Institute for Creative Technologies
Michigan State University University of Southern California
</affiliation>
<email confidence="0.999107">
gerberm2@msu.edu {gordon,sagae}@ict.usc.edu
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999847916666667">
We present a method of extracting open-
domain commonsense knowledge by apply-
ing discourse parsing to a large corpus of per-
sonal stories written by Internet authors. We
demonstrate the use of a linear-time, joint syn-
tax/discourse dependency parser for this pur-
pose, and we show how the extracted dis-
course relations can be used to generate open-
domain textual inferences. Our evaluations
of the discourse parser and inference models
show some success, but also identify a num-
ber of interesting directions for future work.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999470315789474">
The acquisition of open-domain knowledge in sup-
port of commonsense reasoning has long been a
bottleneck within artificial intelligence. Such rea-
soning supports fundamental tasks such as textual
entailment (Giampiccolo et al., 2008), automated
question answering (Clark et al., 2008), and narra-
tive comprehension (Graesser et al., 1994). These
tasks, when conducted in open domains, require vast
amounts of commonsense knowledge pertaining to
states, events, and their causal and temporal relation-
ships. Manually created resources such as FrameNet
(Baker et al., 1998), WordNet (Fellbaum, 1998), and
Cyc (Lenat, 1995) encode many aspects of com-
monsense knowledge; however, coverage of causal
and temporal relationships remains low for many do-
mains.
Gordon and Swanson (2008) argued that the
commonsense tasks of prediction, explanation, and
imagination (collectively called envisionment) can
</bodyText>
<page confidence="0.768023">
43
</page>
<bodyText confidence="0.999948655172414">
be supported by knowledge mined from a large cor-
pus of personal stories written by Internet weblog
authors.1 Gordon and Swanson (2008) identified
three primary obstacles to such an approach. First,
stories must be distinguished from other weblog
content (e.g., lists, recipes, and reviews). Second,
stories must be analyzed in order to extract the im-
plicit commonsense knowledge that they contain.
Third, inference mechanisms must be developed that
use the extracted knowledge to perform the core en-
visionment tasks listed above.
In the current paper, we present an approach to
open-domain commonsense inference that addresses
each of the three obstacles identified by Gordon and
Swanson (2008). We built on the work of Gordon
and Swanson (2009), who describe a classification-
based approach to the task of story identification.
The authors’ system produced a corpus of approx-
imately one million personal stories, which we used
as a starting point. We applied efficient discourse
parsing techniques to this corpus as a means of ex-
tracting causal and temporal relationships. Further-
more, we developed methods that use the extracted
knowledge to generate textual inferences for de-
scriptions of states and events. This work resulted
in an end-to-end prototype system capable of gen-
erating open-domain, commonsense inferences us-
ing a repository of knowledge extracted from un-
structured weblog text. We focused on identifying
</bodyText>
<footnote confidence="0.998485">
1We follow Gordon and Swanson (2009) in defining a story
to be a “textual discourse that describes a specific series of
causally related events in the past, spanning a period of time
of minutes, hours, or days, where the author or a close associate
is among the participants.”
</footnote>
<note confidence="0.9848035">
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 43–51,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9983404">
strengths and weaknesses of the system in an effort properties expressed by many factoids extracted by
to guide future work. Gordon et al. (2009).
We structure our presentation as follows: in Sec- Clark and Harrison (2009) pursued large-scale
tion 2, we present previous research that has inves- extraction of knowledge from text using a syntax-
tigated the use of large web corpora for natural lan- based approach that was also inspired by the work
guage processing (NLP) tasks. In Section 3, we de- of Schubert and Tong (2003). The authors showed
scribe an efficient method of automatically parsing how the extracted knowledge tuples can be used
weblog stories for discourse structure. In Section 4, to improve syntactic parsing and textual entailment
we present a set of inference mechanisms that use recognition. Bar-Haim et al. (2009) present an ef-
the extracted discourse relations to generate open- ficient method of performing inference with such
domain textual inferences. We conclude, in Section knowledge.
5, with insights into story-based envisionment that Our work is also related to the work of Persing
we hope will guide future work in this area. and Ng (2009), in which the authors developed a
2 Related work semi-supervised method of identifying the causes of
Researchers have made many attempts to use the events described in aviation safety reports. Simi-
massive amount of linguistic content created by larly, our system extracts causal (as well as tem-
users of the World Wide Web. Progress and chal- poral) knowledge; however, it does this in an open
lenges in this area have spawned multiple workshops domain and does not place limitations on the types
(e.g., those described by Gurevych and Zesch (2009) of causes to be identified. This greatly increases
and Evert et al. (2008)) that specifically target the the complexity of the inference task, and our results
use of content that is collaboratively created by In- exhibit a corresponding degradation; however, our
ternet users. Of particular relevance to the present evaluations provide important insights into the task.
work is the weblog corpus developed by Burton et 3 Discourse parsing a corpus of stories
al. (2009), which was used for the data challenge Gordon and Swanson (2009) developed a super-
portion of the International Conference on Weblogs vised classification-based approach for identifying
and Social Media (ICWSM). The ICWSM weblog personal stories within the Spinn3r corpus. Their
corpus (referred to here as Spinn3r) is freely avail- method achieved 75% precision on the binary task
able and comprises tens of millions of weblog en- of predicting story versus non-story on a held-out
tries posted between August 1st, 2008 and October subset of the Spinn3r corpus. The extracted “story
1st, 2008. corpus” comprises 960,098 personal stories written
Gordon et al. (2009) describe an approach to by weblog users. Due to its large size and broad
knowledge extraction over the Spinn3r corpus using domain coverage, the story corpus offers unique op-
techniques described by Schubert and Tong (2003). portunities to NLP researchers. For example, Swan-
In this approach, logical propositions (known as fac- son and Gordon (2008) showed how the corpus can
toids) are constructed via approximate interpreta- be used to support open-domain collaborative story
tion of syntactic analyses. As an example, the sys- writing.3
tem identified a factoid glossed as “doors to a room As described by Gordon and Swanson (2008),
may be opened”. Gordon et al. (2009) found that story identification is just the first step towards com-
the extracted factoids cover roughly half of the fac- monsense reasoning using personal stories. We ad-
toids present in the corresponding Wikipedia2 arti- dressed the second step - knowledge extraction -
cles. We used a subset of the Spinn3r corpus in by parsing the corpus using a Rhetorical Structure
our work, but focused on discourse analyses of en- Theory (Carlson and Marcu, 2001) parser based on
tire texts instead of syntactic analyses of single sen- the one described by Sagae (2009). The parser
tences. Our goal was to extract general causal and performs joint syntactic and discourse dependency
temporal propositions instead of the fine-grained
</bodyText>
<footnote confidence="0.710253">
3The system (called SayAnything) is available at
http://sayanything.ict.usc.edu
2http://en.wikipedia.org
</footnote>
<page confidence="0.662152">
44
</page>
<bodyText confidence="0.999750777777778">
parsing using a stack-based, shift-reduce algorithm
with runtime that is linear in the input length. This
lightweight approach is very efficient; however, it
may not be quite as accurate as more complex, chart-
based approaches (e.g., the approach of Charniak
and Johnson (2005) for syntactic parsing).
We trained the discourse parser over the causal
and temporal relations contained in the RST corpus.
Examples of these relations are shown below:
</bodyText>
<listItem confidence="0.87203">
(1) [cause Packages often get buried in the load]
[result and are delivered late.]
(2) [before Three months after she arrived in L.A.]
[after she spent $120 she didn’t have.]
</listItem>
<bodyText confidence="0.999975077922078">
The RST corpus defines many fine-grained rela-
tions that capture causal and temporal properties.
For example, the corpus differentiates between re-
sult and reason for causation and temporal-after and
temporal-before for temporal order. In order to in-
crease the amount of available training data, we col-
lapsed all causal and temporal relations into two
general relations causes and precedes. This step re-
quired normalization of asymmetric relations such
as temporal-before and temporal-after.
To evaluate the discourse parser described above,
we manually annotated 100 randomly selected we-
blog stories from the story corpus produced by Gor-
don and Swanson (2009). For increased efficiency,
we limited our annotation to the generalized causes
and precedes relations described above. We at-
tempted to keep our definitions of these relations
in line with those used by RST. Following previous
discourse annotation efforts, we annotated relations
over clause-level discourse units, permitting rela-
tions between adjacent sentences. In total, we an-
notated 770 instances of causes and 1,009 instances
ofprecedes.
We experimented with two versions of the RST
parser, one trained on the fine-grained RST rela-
tions and the other trained on the collapsed relations.
At testing time, we automatically mapped the fine-
grained relations to their corresponding causes or
precedes relation. We computed the following ac-
curacy statistics:
Discourse segmentation accuracy For each pre-
dicted discourse unit, we located the reference
discourse unit with the highest overlap. Accu-
racy for the predicted discourse unit is equal to
the percentage word overlap between the refer-
ence and predicted discourse units.
Argument identification accuracy For each dis-
course unit of a predicted discourse relation,
we located the reference discourse unit with the
highest overlap. Accuracy is equal to the per-
centage of times that a reference discourse rela-
tion (of any type) holds between the reference
discourse units that overlap most with the pre-
dicted discourse units.
Argument classification accuracy For the subset
of instances in which a reference discourse re-
lation holds between the units that overlap most
with the predicted discourse units, accuracy is
equal to the percentage of times that the pre-
dicted discourse relation matches the reference
discourse relation.
Complete accuracy For each predicted discourse
relation, accuracy is equal to the percentage
word overlap with a reference discourse rela-
tion of the same type.
Table 1 shows the accuracy results for the fine-
grained and collapsed versions of the RST discourse
parser. As shown in Table 1, the collapsed version
of the discourse parser exhibits higher overall ac-
curacy. Both parsers predicted the causes relation
much more often than the precedes relation, so the
overall scores are biased toward the scores for the
causes relation. For comparison, Sagae (2009) eval-
uated a similar RST parser over the test section of
the RST corpus, obtaining precision of 42.9% and
recall of 46.2% (F1 = 44.5%).
In addition to the automatic evaluation described
above, we also manually assessed the output of the
discourse parsers. One of the authors judged the
correctness of each extracted discourse relation, and
we found that the fine-grained and collapsed ver-
sions of the parser performed equally well with a
precision near 33%; however, throughout our exper-
iments, we observed more desirable discourse seg-
mentation when working with the collapsed version
of the discourse parser. This fact, combined with the
results of the automatic evaluation presented above,
</bodyText>
<page confidence="0.996779">
45
</page>
<table confidence="0.998035666666667">
Fine-grained RST parser Collapsed RST parser
Accuracy metric causes precedes overall causes precedes overall
Segmentation 36.08 44.20 36.67 44.36 30.13 43.10
Argument identification 25.00 33.33 25.86 26.15 23.08 25.87
Argument classification 66.15 50.00 64.00 79.41 83.33 79.23
Complete 22.20 28.88 22.68 31.26 21.21 30.37
</table>
<tableCaption confidence="0.999944">
Table 1: RST parser evaluation. All values are percentages.
</tableCaption>
<bodyText confidence="0.996059733333333">
led us to use the collapsed version of the parser in
all subsequent experiments.
Having developed and evaluated the discourse
parser, we conducted a full discourse parse of the
story corpus, which comprises more than 25 million
sentences split into nearly 1 million weblog entries.
The discourse parser extracted 2.2 million instances
of the causes relation and 220,000 instances of the
precedes relation. As a final step, we indexed the
extracted discourse relations with the Lucene infor-
mation retrieval engine.4 Each discourse unit (two
per discourse relation) is treated as a single docu-
ment, allowing us to query the extracted relations
using information retrieval techniques implemented
in the Lucene toolkit.
</bodyText>
<sectionHeader confidence="0.942609" genericHeader="introduction">
4 Generating textual inferences
</sectionHeader>
<bodyText confidence="0.9999833">
As mentioned previously, Gordon and Swan-
son (2008) cite three obstacles to performing com-
monsense reasoning using weblog stories. Gordon
and Swanson (2009) addressed the first (story col-
lection). We addressed the second (story analysis)
by developing a discourse parser capable of extract-
ing causal and temporal relations from weblog text
(Section 3). In this section, we present a prelimi-
nary solution to the third problem - reasoning with
the extracted knowledge.
</bodyText>
<subsectionHeader confidence="0.801008">
4.1 Inference method
</subsectionHeader>
<bodyText confidence="0.999621">
In general, we require an inference method that takes
as input the following things:
</bodyText>
<listItem confidence="0.99107825">
1. A description of the state or event of interest.
This is a free-text description of any length.
2. The type of inference to perform, either causal
or temporal.
</listItem>
<footnote confidence="0.951455">
4Available at http://lucene.apache.org
</footnote>
<bodyText confidence="0.914151761904762">
3. The inference direction, either forward or back-
ward. Forward causal inference produces the
effects of the given state or event. Backward
causal inference produces causes of the given
state or event. Similarly, forward and back-
ward temporal inferences produce subsequent
and preceding states and events, respectively.
As a simple baseline approach, we implemented the
following procedure. First, given a textual input de-
scription d, we query the extracted discourse units
using Lucene’s modified version of the vector space
model over TF-IDF term weights. This produces a
ranked list Rd of discourse units matching the input
description d. We then filter Rd, removing discourse
units that are not linked to other discourse units by
the given relation and in the given direction. Each el-
ement of the filtered Rd is thus linked to a discourse
unit that could potentially satisfy the inference re-
quest.
To demonstrate, we perform forward causal infer-
ence using the following input description d:
</bodyText>
<listItem confidence="0.965423">
(3) John traveled the world.
</listItem>
<bodyText confidence="0.995414">
Below, we list the three top-ranked discourse units
that matched d (left-hand side) and their associated
consequents (right-hand side):
</bodyText>
<listItem confidence="0.99711475">
1. traveling the world -* to murder
2. traveling from around the world to be there �
even though this crowd was international
3. traveled across the world -* to experience it
</listItem>
<bodyText confidence="0.9990428">
In a naive way, one might simply choose the top-
ranked clause in Rd and select its associated clause
as the answer to the inference request; however, in
the example above, this would incorrectly generate
“to murder” as the effect of John’s traveling (this is
</bodyText>
<page confidence="0.996887">
46
</page>
<bodyText confidence="0.999983921568627">
more appropriately viewed as the purpose of trav-
eling). The other effect clauses also appear to be
incorrect. This should not come as much of a sur-
prise because the ranking was generated soley from
the match score between the input description and
the causes in Rd, which are quite relevant.
One potential problem with the naive selection
method is that it ignores information contained in
the ranked list R′d of clauses that are associated with
the clauses in Rd. In our experiments, we often
observed redundancies in R′d that captured general
properties of the desired inference. Intuitively, con-
tent that is shared across elements of R′d could repre-
sent the core meaning of the desired inference result.
In what follows, we describe various re-rankings
of R′d using this shared content. For each model
described, the final inference prediction is the top-
ranked element of R′d.
Centroid similarity To approximate the shared
content of discourse units in R′d, we treat each
discourse unit as a vector of TF scores. We then
compute the average vector and re-rank all dis-
course units in R′d based on their cosine simi-
larity with the average vector. This favors infer-
ence results that “agree” with many alternative
hypotheses.
Description score scaling In this approach, we in-
corporate the score from Rd into the centroid
similarity score, multiplying the two and giving
equal weight to each. This captures the intu-
ition that the top-ranked element of R′d should
represent the general content of the list but
should also be linked to an element of Rd that
bears high similarity to the given state or event
description d.
Log-length scaling When working with the cen-
troid similarity score, we often observed top-
ranked elements of R′d that were only a few
words in length. This was typically the case
when components from sparse TF vectors in
R′d matched well with components from the
centroid vector. Ideally, we would like more
lengthy (but not too long) descriptions. To
achieve this, we multiplied the centroid simi-
larity score by the logarithm of the word length
of the discourse unit in R′d.
Description score/log-length scaling In this ap-
proach, we combine the description score scal-
ing and log-length scaling, multiplying the cen-
troid similarity by both and giving equal weight
to all three factors.
</bodyText>
<subsectionHeader confidence="0.992823">
4.2 Evaluating the generated textual inferences
</subsectionHeader>
<bodyText confidence="0.999924">
To evaluate the inference re-ranking models de-
scribed above, we automatically generated for-
ward/backward causal and temporal inferences for
five documents (265 sentences) drawn randomly
from the story corpus. For simplicity, we gener-
ated an inference for each sentence in each docu-
ment. Each inference re-ranking model is able to
generate four textual inferences (forward/backward
causal/temporal) for each sentence. In our experi-
ments, we only kept the highest-scoring of the four
inferences generated by a model. One of the authors
then manually evaluated the final predictions for cor-
rectness. This was a subjective process, but it was
guided by the following requirements:
</bodyText>
<listItem confidence="0.97858525">
1. The generated inference must increase the lo-
cal coherence of the document. As described
by Graesser et al. (1994), readers are typically
required to make inferences about the text that
lead to a coherent understanding thereof. We
required the generated inferences to aid in this
task.
2. The generated inferences must be globally
valid. To demonstrate global validity, consider
the following actual output:
(4) I didn’t even need a jacket (until I got
there).
</listItem>
<bodyText confidence="0.999923083333333">
In Example 4, the system-generated forward
temporal inference is shown in parentheses.
The inference makes sense given its local con-
text; however, it is clear from the surround-
ing discourse (not shown) that a jacket was not
needed at any point in time (it happened to be
a warm day). As a result, this prediction was
tagged as incorrect.
Table 2 presents the results of the evaluation. As
shown in the table, the top-performing models are
those that combine centroid similarity with one or
both of the other re-ranking heuristics.
</bodyText>
<page confidence="0.998223">
47
</page>
<table confidence="0.999539666666667">
Re-ranking model Inference accuracy (%)
None 10.19
Centroid similarity 12.83
Description score scaling 17.36
Log-length scaling 12.83
Description score/log-length scaling 16.60
</table>
<tableCaption confidence="0.997173">
Table 2: Inference generation evaluation results.
</tableCaption>
<figureCaption confidence="0.998891">
Figure 1: Inference rate versus accuracy. Values along the x-axis indicate that the top-scoring x% of all inferences
were evaluated. Values along the y-axis indicate the prediction accuracy.
</figureCaption>
<figure confidence="0.9978902">
Inference accuracy
0.25
0.15
0.05
0.3
0.2
0.1
0
Confidence-ordered percentage of all
inferences
None
Centroid similarity
Description score scaling
Log-length scaling
Combined scaling
</figure>
<bodyText confidence="0.999803619047619">
The analysis above demonstrates the relative per-
formance of the models when making inferences for
all sentences; however it is probably the case that
many generated inferences should be rejected due to
their low score. Because the output scores of a single
model can be meaningfully compared across predic-
tions, it is possible to impose a threshold on the in-
ference generation process such that any prediction
scoring at or below the threshold is withheld. We
varied the prediction threshold from zero to a value
sufficiently large that it excluded all predictions for
a model. Doing so demonstrates the trade-off be-
tween making a large number of textual inferences
and making accurate textual inferences. Figure 1
shows the effects of this variable on the re-ranking
models. As shown in Figure 1, the highest infer-
ence accuracy is reached by the re-ranker that com-
bines description score and log-length scaling with
the centroid similarity measure. This accuracy is at-
tained by keeping the top 25% most confident infer-
ences.
</bodyText>
<sectionHeader confidence="0.996487" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999731">
We have presented an approach to commonsense
reasoning that relies on (1) the availability of a large
corpus of personal weblog stories and (2) the abil-
ity to analyze and perform inference with these sto-
ries. Our current results, although preliminary, sug-
gest novel and important areas of future exploration.
We group our observations according to the last two
problems identified by Gordon and Swanson (2008):
story analysis and envisioning with the analysis re-
sults.
</bodyText>
<subsectionHeader confidence="0.999527">
5.1 Story analysis
</subsectionHeader>
<bodyText confidence="0.999893333333333">
As in other NLP tasks, we observed significant per-
formance degradation when moving from the train-
ing genre (newswire) to the testing genre (Internet
</bodyText>
<page confidence="0.998129">
48
</page>
<bodyText confidence="0.99998725">
weblog stories). Because our discourse parser relies
heavily on lexical and syntactic features for classi-
fication, and because the distribution of the feature
values varies widely between the two genres, the
performance degradation is to be expected. Recent
techniques in parser adaptation for the Brown corpus
(McClosky et al., 2006) might be usefully applied to
the weblog genre as well.
Our supervised classification-based approach to
discourse parsing could also be improved with ad-
ditional training data. Causal and temporal relations
are instantiated a combined 2,840 times in the RST
corpus, with a large majority of these being causal.
In contrast, the Penn Discourse TreeBank (Prasad et
al., 2008) contains 7,448 training instances of causal
relations and 2,763 training instances of temporal
relations. This represents a significant increase in
the amount of training data over the RST corpus. It
would be informative to compare our current results
with those obtained using a discourse parser trained
on the Penn Discourse TreeBank.
One might also extract causal and temporal rela-
tions using traditional semantic role analysis based
on FrameNet (Baker et al., 1998) or PropBank
(Kingsbury and Palmer, 2003). The former defines a
number of frames related to causation and temporal
order, and roles within the latter could be mapped to
standard thematic roles (e.g., cause) via SemLink.5
</bodyText>
<subsectionHeader confidence="0.99894">
5.2 Envisioning with the analysis results
</subsectionHeader>
<bodyText confidence="0.9999405625">
We believe commonsense reasoning based on we-
blog stories can also be improved through more so-
phisticated uses of the extracted discourse relations.
As a first step, it would be beneficial to explore alter-
nate input descriptions. As presented in Section 4.2,
we make textual inferences at the sentence level for
simplicity; however, it might be more reasonable to
make inferences at the clause level, since clauses are
the basis for RST and Penn Discourse TreeBank an-
notation. This could result in the generation of sig-
nificantly more inferences due to multi-clause sen-
tences; thus, more intelligent inference filtering will
be required.
Our models use prediction scores for the tasks
of rejecting inferences and selecting between mul-
tiple candidate inferences (i.e., forward/backward
</bodyText>
<footnote confidence="0.993611">
5Available at http://verbs.colorado.edu/semlink
</footnote>
<bodyText confidence="0.999399695652174">
causal/temporal). Instead of relying on prediction
scores for these tasks, it might be advantageous to
first identify whether or not envisionment should be
performed for a clause, and, if it should, what type
and direction of envisionment would be best. For
example, consider the following sentence:
(5) [clauses John went to the store] [clause2
because he was hungry].
It would be better - from a local coherence perspec-
tive - to infer the cause of the second clause instead
of the cause of the first. This is due to the fact that a
cause for the first clause is explicitly stated, whereas
a cause for the second clause is not. Inferences made
about the first clause (e.g., that John went to the store
because his dog was hungry), are likely to be unin-
formative or in conflict with explicitly stated infor-
mation.
Example 5 raises the important issue of context,
which we believe needs to be investigated further.
Here, context refers to the discourse that surrounds
the clause or sentence for which the system is at-
tempting to generate a textual inference. The con-
text places a number of constraints on allowable in-
ferences. For example, in addition to content-based
constraints demonstrated in Example 5, the context
limits pronoun usage, entity references, and tense.
Violations of these constraints will reduce local co-
herence.
Finally, the story corpus, with its vast size, is
likely to contain a significant amount of redundancy
for common events and states. Our centroid-based
re-ranking heuristics are inspired by this redun-
dancy, and we expect that aggregation techniques
such as clustering might be of some use when ap-
plied to the corpus as a whole. Having identified
coherent clusters of causes, it might be easier to find
a consequence for a previously unseen cause.
In summary, we have presented preliminary re-
search into the task of using a large, collaboratively
constructed corpus as a commonsense knowledge
repository. Rather than relying on hand-coded on-
tologies and event schemas, our approach relies on
the implicit knowledge contained in written natu-
ral language. We have demonstrated the feasibility
of obtaining the discourse structure of such a cor-
pus via linear-time parsing models. Furthermore,
</bodyText>
<page confidence="0.998098">
49
</page>
<bodyText confidence="0.9995538">
we have introduced inference procedures that are ca-
pable of generating open-domain textual inferences
from the extracted knowledge. Our evaluation re-
sults suggest many opportunities for future work in
this area.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999978333333333">
The authors would like to thank the anonymous
reviewers for their helpful comments and sugges-
tions. The project or effort described here has
been sponsored by the U.S. Army Research, Devel-
opment, and Engineering Command (RDECOM).
Statements and opinions expressed do not necessar-
ily reflect the position or the policy of the United
States Government, and no official endorsement
should be inferred.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999795305882353">
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In Christian Boitet
and Pete Whitelock, editors, Proceedings ofthe Thirty-
Sixth Annual Meeting of the Association for Computa-
tional Linguistics and Seventeenth International Con-
ference on Computational Linguistics, pages 86–90,
San Francisco, California. Morgan Kaufmann Publish-
ers.
Roy Bar-Haim, Jonathan Berant, and Ido Dagan. 2009.
A compact forest for scalable inference over entail-
ment and paraphrase rules. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1056–1065, Singapore, Au-
gust. Association for Computational Linguistics.
K. Burton, A. Java, and I. Soboroff. 2009. The icwsm
2009 spinn3r dataset. In Proceedings of the Third An-
nual Conference on Weblogs and Social Media.
Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging manual. Technical Report ISI-TR-545, ISI, July.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
Peter Clark and Phil Harrison. 2009. Large-scale extrac-
tion and use of knowledge from text. In K-CAP ’09:
Proceedings of the fifth international conference on
Knowledge capture, pages 153–160, New York, NY,
USA. ACM.
Peter Clark, Christiane Fellbaum, Jerry R. Hobbs, Phil
Harrison, William R. Murray, and John Thompson.
2008. Augmenting WordNet for Deep Understanding
of Text. In Johan Bos and Rodolfo Delmonte, editors,
Semantics in Text Processing. STEP 2008 Conference
Proceedings, volume 1 of Research in Computational
Semantics, pages 45–57. College Publications.
Stefan Evert, Adam Kilgarriff, and Serge Sharoff, edi-
tors. 2008. 4th Web as Corpus Workshop Can we beat
Google?
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Communi-
cation). The MIT Press, May.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, and Bill Dolan. 2008. The
fourth fascal recognizing textual entailment challenge.
In Proceedings of the First Text Analysis Conference.
Andrew Gordon and Reid Swanson. 2008. Envision-
ing with weblogs. In International Conference on New
Media Technology.
Andrew Gordon and Reid Swanson. 2009. Identifying
personal stories in millions of weblog entries. In Third
International Conference on Weblogs and Social Me-
dia.
Jonathan Gordon, Benjamin Van Durme, and Lenhart
Schubert. 2009. Weblogs as a source for extracting
general world knowledge. In K-CAP ’09: Proceed-
ings ofthefifth international conference on Knowledge
capture, pages 185–186, New York, NY, USA. ACM.
A. C. Graesser, M. Singer, and T. Trabasso. 1994. Con-
structing inferences during narrative text comprehen-
sion. Psychological Review, 101:371–395.
Iryna Gurevych and Torsten Zesch, editors. 2009. The
Peoples Web Meets NLP: Collaboratively Constructed
Semantic Resources.
Paul Kingsbury and Martha Palmer. 2003. Propbank: the
next level of treebank. In Proceedings of Treebanks
and Lexical Theories.
Douglas B. Lenat. 1995. Cyc: a large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33–38.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In ACL-44: Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics, pages 337–344, Morristown, NJ,
USA. Association for Computational Linguistics.
Isaac Persing and Vincent Ng. 2009. Semi-supervised
cause identification from aviation safety reports. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 843–851, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Rashmi Prasad, Alan Lee, Nikhil Dinesh, Eleni Milt-
sakaki, Geraud Campion, Aravind Joshi, and Bonnie
</reference>
<page confidence="0.948529">
50
</page>
<reference confidence="0.999690111111111">
Webber. 2008. Penn discourse treebank version 2.0.
Linguistic Data Consortium, February.
Kenji Sagae. 2009. Analysis of discourse structure with
syntactic dependencies and data-driven shift-reduce
parsing. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT’09), pages
81–84, Paris, France, October. Association for Com-
putational Linguistics.
Lenhart Schubert and Matthew Tong. 2003. Extract-
ing and evaluating general world knowledge from the
brown corpus. In Proceedings of the HLT-NAACL
2003 workshop on Text meaning, pages 7–13, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Reid Swanson and Andrew Gordon. 2008. Say anything:
A massively collaborative open domain story writing
companion. In First International Conference on In-
teractive Digital Storytelling.
</reference>
<page confidence="0.999121">
51
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875345">
<title confidence="0.999147">Open-domain Commonsense Reasoning Using Discourse Relations from Corpus of Weblog Stories</title>
<author confidence="0.998679">Gerber Andrew S Gordon Sagae</author>
<affiliation confidence="0.9472865">Department of Computer Science Institute for Creative Technologies Michigan State University University of Southern California</affiliation>
<abstract confidence="0.998482615384615">We present a method of extracting opendomain commonsense knowledge by applying discourse parsing to a large corpus of personal stories written by Internet authors. We demonstrate the use of a linear-time, joint syntax/discourse dependency parser for this purpose, and we show how the extracted discourse relations can be used to generate opendomain textual inferences. Our evaluations of the discourse parser and inference models show some success, but also identify a number of interesting directions for future work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Charles Fillmore</author>
<author>John Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>Proceedings ofthe ThirtySixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics,</booktitle>
<pages>86--90</pages>
<editor>In Christian Boitet and Pete Whitelock, editors,</editor>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>San Francisco, California.</location>
<contexts>
<context position="1418" citStr="Baker et al., 1998" startWordPosition="203" endWordPosition="206">eresting directions for future work. 1 Introduction The acquisition of open-domain knowledge in support of commonsense reasoning has long been a bottleneck within artificial intelligence. Such reasoning supports fundamental tasks such as textual entailment (Giampiccolo et al., 2008), automated question answering (Clark et al., 2008), and narrative comprehension (Graesser et al., 1994). These tasks, when conducted in open domains, require vast amounts of commonsense knowledge pertaining to states, events, and their causal and temporal relationships. Manually created resources such as FrameNet (Baker et al., 1998), WordNet (Fellbaum, 1998), and Cyc (Lenat, 1995) encode many aspects of commonsense knowledge; however, coverage of causal and temporal relationships remains low for many domains. Gordon and Swanson (2008) argued that the commonsense tasks of prediction, explanation, and imagination (collectively called envisionment) can 43 be supported by knowledge mined from a large corpus of personal stories written by Internet weblog authors.1 Gordon and Swanson (2008) identified three primary obstacles to such an approach. First, stories must be distinguished from other weblog content (e.g., lists, recip</context>
<context position="23214" citStr="Baker et al., 1998" startWordPosition="3626" endWordPosition="3629">a combined 2,840 times in the RST corpus, with a large majority of these being causal. In contrast, the Penn Discourse TreeBank (Prasad et al., 2008) contains 7,448 training instances of causal relations and 2,763 training instances of temporal relations. This represents a significant increase in the amount of training data over the RST corpus. It would be informative to compare our current results with those obtained using a discourse parser trained on the Penn Discourse TreeBank. One might also extract causal and temporal relations using traditional semantic role analysis based on FrameNet (Baker et al., 1998) or PropBank (Kingsbury and Palmer, 2003). The former defines a number of frames related to causation and temporal order, and roles within the latter could be mapped to standard thematic roles (e.g., cause) via SemLink.5 5.2 Envisioning with the analysis results We believe commonsense reasoning based on weblog stories can also be improved through more sophisticated uses of the extracted discourse relations. As a first step, it would be beneficial to explore alternate input descriptions. As presented in Section 4.2, we make textual inferences at the sentence level for simplicity; however, it mi</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin Baker, Charles Fillmore, and John Lowe. 1998. The Berkeley FrameNet project. In Christian Boitet and Pete Whitelock, editors, Proceedings ofthe ThirtySixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics, pages 86–90, San Francisco, California. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
</authors>
<title>A compact forest for scalable inference over entailment and paraphrase rules.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1056--1065</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4495" citStr="Bar-Haim et al. (2009)" startWordPosition="680" endWordPosition="683"> (2009) pursued large-scale tion 2, we present previous research that has inves- extraction of knowledge from text using a syntaxtigated the use of large web corpora for natural lan- based approach that was also inspired by the work guage processing (NLP) tasks. In Section 3, we de- of Schubert and Tong (2003). The authors showed scribe an efficient method of automatically parsing how the extracted knowledge tuples can be used weblog stories for discourse structure. In Section 4, to improve syntactic parsing and textual entailment we present a set of inference mechanisms that use recognition. Bar-Haim et al. (2009) present an efthe extracted discourse relations to generate open- ficient method of performing inference with such domain textual inferences. We conclude, in Section knowledge. 5, with insights into story-based envisionment that Our work is also related to the work of Persing we hope will guide future work in this area. and Ng (2009), in which the authors developed a 2 Related work semi-supervised method of identifying the causes of Researchers have made many attempts to use the events described in aviation safety reports. Simimassive amount of linguistic content created by larly, our system e</context>
</contexts>
<marker>Bar-Haim, Berant, Dagan, 2009</marker>
<rawString>Roy Bar-Haim, Jonathan Berant, and Ido Dagan. 2009. A compact forest for scalable inference over entailment and paraphrase rules. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1056–1065, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Burton</author>
<author>A Java</author>
<author>I Soboroff</author>
</authors>
<title>The icwsm 2009 spinn3r dataset.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Annual Conference on Weblogs and Social Media.</booktitle>
<marker>Burton, Java, Soboroff, 2009</marker>
<rawString>K. Burton, A. Java, and I. Soboroff. 2009. The icwsm 2009 spinn3r dataset. In Proceedings of the Third Annual Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
</authors>
<title>Discourse tagging manual.</title>
<date>2001</date>
<tech>Technical Report ISI-TR-545, ISI,</tech>
<contexts>
<context position="7638" citStr="Carlson and Marcu, 2001" startWordPosition="1181" endWordPosition="1184">yses. As an example, the sys- writing.3 tem identified a factoid glossed as “doors to a room As described by Gordon and Swanson (2008), may be opened”. Gordon et al. (2009) found that story identification is just the first step towards comthe extracted factoids cover roughly half of the fac- monsense reasoning using personal stories. We adtoids present in the corresponding Wikipedia2 arti- dressed the second step - knowledge extraction - cles. We used a subset of the Spinn3r corpus in by parsing the corpus using a Rhetorical Structure our work, but focused on discourse analyses of en- Theory (Carlson and Marcu, 2001) parser based on tire texts instead of syntactic analyses of single sen- the one described by Sagae (2009). The parser tences. Our goal was to extract general causal and performs joint syntactic and discourse dependency temporal propositions instead of the fine-grained 3The system (called SayAnything) is available at http://sayanything.ict.usc.edu 2http://en.wikipedia.org 44 parsing using a stack-based, shift-reduce algorithm with runtime that is linear in the input length. This lightweight approach is very efficient; however, it may not be quite as accurate as more complex, chartbased approac</context>
</contexts>
<marker>Carlson, Marcu, 2001</marker>
<rawString>Lynn Carlson and Daniel Marcu. 2001. Discourse tagging manual. Technical Report ISI-TR-545, ISI, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8292" citStr="Charniak and Johnson (2005)" startWordPosition="1275" endWordPosition="1278"> instead of syntactic analyses of single sen- the one described by Sagae (2009). The parser tences. Our goal was to extract general causal and performs joint syntactic and discourse dependency temporal propositions instead of the fine-grained 3The system (called SayAnything) is available at http://sayanything.ict.usc.edu 2http://en.wikipedia.org 44 parsing using a stack-based, shift-reduce algorithm with runtime that is linear in the input length. This lightweight approach is very efficient; however, it may not be quite as accurate as more complex, chartbased approaches (e.g., the approach of Charniak and Johnson (2005) for syntactic parsing). We trained the discourse parser over the causal and temporal relations contained in the RST corpus. Examples of these relations are shown below: (1) [cause Packages often get buried in the load] [result and are delivered late.] (2) [before Three months after she arrived in L.A.] [after she spent $120 she didn’t have.] The RST corpus defines many fine-grained relations that capture causal and temporal properties. For example, the corpus differentiates between result and reason for causation and temporal-after and temporal-before for temporal order. In order to increase </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
<author>Phil Harrison</author>
</authors>
<title>Large-scale extraction and use of knowledge from text.</title>
<date>2009</date>
<booktitle>In K-CAP ’09: Proceedings of the fifth international conference on Knowledge capture,</booktitle>
<pages>153--160</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3880" citStr="Clark and Harrison (2009)" startWordPosition="581" endWordPosition="584">escribes a specific series of causally related events in the past, spanning a period of time of minutes, hours, or days, where the author or a close associate is among the participants.” Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 43–51, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics strengths and weaknesses of the system in an effort properties expressed by many factoids extracted by to guide future work. Gordon et al. (2009). We structure our presentation as follows: in Sec- Clark and Harrison (2009) pursued large-scale tion 2, we present previous research that has inves- extraction of knowledge from text using a syntaxtigated the use of large web corpora for natural lan- based approach that was also inspired by the work guage processing (NLP) tasks. In Section 3, we de- of Schubert and Tong (2003). The authors showed scribe an efficient method of automatically parsing how the extracted knowledge tuples can be used weblog stories for discourse structure. In Section 4, to improve syntactic parsing and textual entailment we present a set of inference mechanisms that use recognition. Bar-Hai</context>
</contexts>
<marker>Clark, Harrison, 2009</marker>
<rawString>Peter Clark and Phil Harrison. 2009. Large-scale extraction and use of knowledge from text. In K-CAP ’09: Proceedings of the fifth international conference on Knowledge capture, pages 153–160, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
<author>Christiane Fellbaum</author>
<author>Jerry R Hobbs</author>
<author>Phil Harrison</author>
<author>William R Murray</author>
<author>John Thompson</author>
</authors>
<title>Augmenting WordNet for Deep Understanding of Text.</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings,</booktitle>
<volume>1</volume>
<pages>45--57</pages>
<editor>In Johan Bos and Rodolfo Delmonte, editors,</editor>
<publisher>College Publications.</publisher>
<contexts>
<context position="1133" citStr="Clark et al., 2008" startWordPosition="161" endWordPosition="164">r-time, joint syntax/discourse dependency parser for this purpose, and we show how the extracted discourse relations can be used to generate opendomain textual inferences. Our evaluations of the discourse parser and inference models show some success, but also identify a number of interesting directions for future work. 1 Introduction The acquisition of open-domain knowledge in support of commonsense reasoning has long been a bottleneck within artificial intelligence. Such reasoning supports fundamental tasks such as textual entailment (Giampiccolo et al., 2008), automated question answering (Clark et al., 2008), and narrative comprehension (Graesser et al., 1994). These tasks, when conducted in open domains, require vast amounts of commonsense knowledge pertaining to states, events, and their causal and temporal relationships. Manually created resources such as FrameNet (Baker et al., 1998), WordNet (Fellbaum, 1998), and Cyc (Lenat, 1995) encode many aspects of commonsense knowledge; however, coverage of causal and temporal relationships remains low for many domains. Gordon and Swanson (2008) argued that the commonsense tasks of prediction, explanation, and imagination (collectively called envisionm</context>
</contexts>
<marker>Clark, Fellbaum, Hobbs, Harrison, Murray, Thompson, 2008</marker>
<rawString>Peter Clark, Christiane Fellbaum, Jerry R. Hobbs, Phil Harrison, William R. Murray, and John Thompson. 2008. Augmenting WordNet for Deep Understanding of Text. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, volume 1 of Research in Computational Semantics, pages 45–57. College Publications.</rawString>
</citation>
<citation valid="true">
<date>2008</date>
<booktitle>4th Web as Corpus Workshop Can we beat Google?</booktitle>
<editor>Stefan Evert, Adam Kilgarriff, and Serge Sharoff, editors.</editor>
<contexts>
<context position="1624" citStr="(2008)" startWordPosition="237" endWordPosition="237">ntal tasks such as textual entailment (Giampiccolo et al., 2008), automated question answering (Clark et al., 2008), and narrative comprehension (Graesser et al., 1994). These tasks, when conducted in open domains, require vast amounts of commonsense knowledge pertaining to states, events, and their causal and temporal relationships. Manually created resources such as FrameNet (Baker et al., 1998), WordNet (Fellbaum, 1998), and Cyc (Lenat, 1995) encode many aspects of commonsense knowledge; however, coverage of causal and temporal relationships remains low for many domains. Gordon and Swanson (2008) argued that the commonsense tasks of prediction, explanation, and imagination (collectively called envisionment) can 43 be supported by knowledge mined from a large corpus of personal stories written by Internet weblog authors.1 Gordon and Swanson (2008) identified three primary obstacles to such an approach. First, stories must be distinguished from other weblog content (e.g., lists, recipes, and reviews). Second, stories must be analyzed in order to extract the implicit commonsense knowledge that they contain. Third, inference mechanisms must be developed that use the extracted knowledge to</context>
<context position="5453" citStr="(2008)" startWordPosition="840" endWordPosition="840">thors developed a 2 Related work semi-supervised method of identifying the causes of Researchers have made many attempts to use the events described in aviation safety reports. Simimassive amount of linguistic content created by larly, our system extracts causal (as well as temusers of the World Wide Web. Progress and chal- poral) knowledge; however, it does this in an open lenges in this area have spawned multiple workshops domain and does not place limitations on the types (e.g., those described by Gurevych and Zesch (2009) of causes to be identified. This greatly increases and Evert et al. (2008)) that specifically target the the complexity of the inference task, and our results use of content that is collaboratively created by In- exhibit a corresponding degradation; however, our ternet users. Of particular relevance to the present evaluations provide important insights into the task. work is the weblog corpus developed by Burton et 3 Discourse parsing a corpus of stories al. (2009), which was used for the data challenge Gordon and Swanson (2009) developed a superportion of the International Conference on Weblogs vised classification-based approach for identifying and Social Media (I</context>
<context position="6863" citStr="(2008)" startWordPosition="1058" endWordPosition="1058">millions of weblog en- of predicting story versus non-story on a held-out tries posted between August 1st, 2008 and October subset of the Spinn3r corpus. The extracted “story 1st, 2008. corpus” comprises 960,098 personal stories written Gordon et al. (2009) describe an approach to by weblog users. Due to its large size and broad knowledge extraction over the Spinn3r corpus using domain coverage, the story corpus offers unique optechniques described by Schubert and Tong (2003). portunities to NLP researchers. For example, SwanIn this approach, logical propositions (known as fac- son and Gordon (2008) showed how the corpus can toids) are constructed via approximate interpreta- be used to support open-domain collaborative story tion of syntactic analyses. As an example, the sys- writing.3 tem identified a factoid glossed as “doors to a room As described by Gordon and Swanson (2008), may be opened”. Gordon et al. (2009) found that story identification is just the first step towards comthe extracted factoids cover roughly half of the fac- monsense reasoning using personal stories. We adtoids present in the corresponding Wikipedia2 arti- dressed the second step - knowledge extraction - cles. W</context>
<context position="13411" citStr="(2008)" startWordPosition="2065" endWordPosition="2065">hich comprises more than 25 million sentences split into nearly 1 million weblog entries. The discourse parser extracted 2.2 million instances of the causes relation and 220,000 instances of the precedes relation. As a final step, we indexed the extracted discourse relations with the Lucene information retrieval engine.4 Each discourse unit (two per discourse relation) is treated as a single document, allowing us to query the extracted relations using information retrieval techniques implemented in the Lucene toolkit. 4 Generating textual inferences As mentioned previously, Gordon and Swanson (2008) cite three obstacles to performing commonsense reasoning using weblog stories. Gordon and Swanson (2009) addressed the first (story collection). We addressed the second (story analysis) by developing a discourse parser capable of extracting causal and temporal relations from weblog text (Section 3). In this section, we present a preliminary solution to the third problem - reasoning with the extracted knowledge. 4.1 Inference method In general, we require an inference method that takes as input the following things: 1. A description of the state or event of interest. This is a free-text descri</context>
<context position="21807" citStr="(2008)" startWordPosition="3412" endWordPosition="3412">the re-ranker that combines description score and log-length scaling with the centroid similarity measure. This accuracy is attained by keeping the top 25% most confident inferences. 5 Conclusions We have presented an approach to commonsense reasoning that relies on (1) the availability of a large corpus of personal weblog stories and (2) the ability to analyze and perform inference with these stories. Our current results, although preliminary, suggest novel and important areas of future exploration. We group our observations according to the last two problems identified by Gordon and Swanson (2008): story analysis and envisioning with the analysis results. 5.1 Story analysis As in other NLP tasks, we observed significant performance degradation when moving from the training genre (newswire) to the testing genre (Internet 48 weblog stories). Because our discourse parser relies heavily on lexical and syntactic features for classification, and because the distribution of the feature values varies widely between the two genres, the performance degradation is to be expected. Recent techniques in parser adaptation for the Brown corpus (McClosky et al., 2006) might be usefully applied to the w</context>
</contexts>
<marker>2008</marker>
<rawString>Stefan Evert, Adam Kilgarriff, and Serge Sharoff, editors. 2008. 4th Web as Corpus Workshop Can we beat Google?</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication).</title>
<date>1998</date>
<publisher>The MIT Press,</publisher>
<contexts>
<context position="1444" citStr="Fellbaum, 1998" startWordPosition="208" endWordPosition="209"> work. 1 Introduction The acquisition of open-domain knowledge in support of commonsense reasoning has long been a bottleneck within artificial intelligence. Such reasoning supports fundamental tasks such as textual entailment (Giampiccolo et al., 2008), automated question answering (Clark et al., 2008), and narrative comprehension (Graesser et al., 1994). These tasks, when conducted in open domains, require vast amounts of commonsense knowledge pertaining to states, events, and their causal and temporal relationships. Manually created resources such as FrameNet (Baker et al., 1998), WordNet (Fellbaum, 1998), and Cyc (Lenat, 1995) encode many aspects of commonsense knowledge; however, coverage of causal and temporal relationships remains low for many domains. Gordon and Swanson (2008) argued that the commonsense tasks of prediction, explanation, and imagination (collectively called envisionment) can 43 be supported by knowledge mined from a large corpus of personal stories written by Internet weblog authors.1 Gordon and Swanson (2008) identified three primary obstacles to such an approach. First, stories must be distinguished from other weblog content (e.g., lists, recipes, and reviews). Second, </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The MIT Press, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Hoa Trang Dang</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The fourth fascal recognizing textual entailment challenge.</title>
<date>2008</date>
<booktitle>In Proceedings of the First Text Analysis Conference.</booktitle>
<contexts>
<context position="1082" citStr="Giampiccolo et al., 2008" startWordPosition="154" endWordPosition="157">en by Internet authors. We demonstrate the use of a linear-time, joint syntax/discourse dependency parser for this purpose, and we show how the extracted discourse relations can be used to generate opendomain textual inferences. Our evaluations of the discourse parser and inference models show some success, but also identify a number of interesting directions for future work. 1 Introduction The acquisition of open-domain knowledge in support of commonsense reasoning has long been a bottleneck within artificial intelligence. Such reasoning supports fundamental tasks such as textual entailment (Giampiccolo et al., 2008), automated question answering (Clark et al., 2008), and narrative comprehension (Graesser et al., 1994). These tasks, when conducted in open domains, require vast amounts of commonsense knowledge pertaining to states, events, and their causal and temporal relationships. Manually created resources such as FrameNet (Baker et al., 1998), WordNet (Fellbaum, 1998), and Cyc (Lenat, 1995) encode many aspects of commonsense knowledge; however, coverage of causal and temporal relationships remains low for many domains. Gordon and Swanson (2008) argued that the commonsense tasks of prediction, explanat</context>
</contexts>
<marker>Giampiccolo, Dang, Magnini, Dagan, Dolan, 2008</marker>
<rawString>Danilo Giampiccolo, Hoa Trang Dang, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2008. The fourth fascal recognizing textual entailment challenge. In Proceedings of the First Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gordon</author>
<author>Reid Swanson</author>
</authors>
<title>Envisioning with weblogs.</title>
<date>2008</date>
<booktitle>In International Conference on New Media Technology.</booktitle>
<contexts>
<context position="1624" citStr="Gordon and Swanson (2008)" startWordPosition="234" endWordPosition="237">ng supports fundamental tasks such as textual entailment (Giampiccolo et al., 2008), automated question answering (Clark et al., 2008), and narrative comprehension (Graesser et al., 1994). These tasks, when conducted in open domains, require vast amounts of commonsense knowledge pertaining to states, events, and their causal and temporal relationships. Manually created resources such as FrameNet (Baker et al., 1998), WordNet (Fellbaum, 1998), and Cyc (Lenat, 1995) encode many aspects of commonsense knowledge; however, coverage of causal and temporal relationships remains low for many domains. Gordon and Swanson (2008) argued that the commonsense tasks of prediction, explanation, and imagination (collectively called envisionment) can 43 be supported by knowledge mined from a large corpus of personal stories written by Internet weblog authors.1 Gordon and Swanson (2008) identified three primary obstacles to such an approach. First, stories must be distinguished from other weblog content (e.g., lists, recipes, and reviews). Second, stories must be analyzed in order to extract the implicit commonsense knowledge that they contain. Third, inference mechanisms must be developed that use the extracted knowledge to</context>
<context position="7148" citStr="Gordon and Swanson (2008)" startWordPosition="1100" endWordPosition="1103"> an approach to by weblog users. Due to its large size and broad knowledge extraction over the Spinn3r corpus using domain coverage, the story corpus offers unique optechniques described by Schubert and Tong (2003). portunities to NLP researchers. For example, SwanIn this approach, logical propositions (known as fac- son and Gordon (2008) showed how the corpus can toids) are constructed via approximate interpreta- be used to support open-domain collaborative story tion of syntactic analyses. As an example, the sys- writing.3 tem identified a factoid glossed as “doors to a room As described by Gordon and Swanson (2008), may be opened”. Gordon et al. (2009) found that story identification is just the first step towards comthe extracted factoids cover roughly half of the fac- monsense reasoning using personal stories. We adtoids present in the corresponding Wikipedia2 arti- dressed the second step - knowledge extraction - cles. We used a subset of the Spinn3r corpus in by parsing the corpus using a Rhetorical Structure our work, but focused on discourse analyses of en- Theory (Carlson and Marcu, 2001) parser based on tire texts instead of syntactic analyses of single sen- the one described by Sagae (2009). Th</context>
<context position="13411" citStr="Gordon and Swanson (2008)" startWordPosition="2061" endWordPosition="2065">the story corpus, which comprises more than 25 million sentences split into nearly 1 million weblog entries. The discourse parser extracted 2.2 million instances of the causes relation and 220,000 instances of the precedes relation. As a final step, we indexed the extracted discourse relations with the Lucene information retrieval engine.4 Each discourse unit (two per discourse relation) is treated as a single document, allowing us to query the extracted relations using information retrieval techniques implemented in the Lucene toolkit. 4 Generating textual inferences As mentioned previously, Gordon and Swanson (2008) cite three obstacles to performing commonsense reasoning using weblog stories. Gordon and Swanson (2009) addressed the first (story collection). We addressed the second (story analysis) by developing a discourse parser capable of extracting causal and temporal relations from weblog text (Section 3). In this section, we present a preliminary solution to the third problem - reasoning with the extracted knowledge. 4.1 Inference method In general, we require an inference method that takes as input the following things: 1. A description of the state or event of interest. This is a free-text descri</context>
<context position="21807" citStr="Gordon and Swanson (2008)" startWordPosition="3409" endWordPosition="3412">racy is reached by the re-ranker that combines description score and log-length scaling with the centroid similarity measure. This accuracy is attained by keeping the top 25% most confident inferences. 5 Conclusions We have presented an approach to commonsense reasoning that relies on (1) the availability of a large corpus of personal weblog stories and (2) the ability to analyze and perform inference with these stories. Our current results, although preliminary, suggest novel and important areas of future exploration. We group our observations according to the last two problems identified by Gordon and Swanson (2008): story analysis and envisioning with the analysis results. 5.1 Story analysis As in other NLP tasks, we observed significant performance degradation when moving from the training genre (newswire) to the testing genre (Internet 48 weblog stories). Because our discourse parser relies heavily on lexical and syntactic features for classification, and because the distribution of the feature values varies widely between the two genres, the performance degradation is to be expected. Recent techniques in parser adaptation for the Brown corpus (McClosky et al., 2006) might be usefully applied to the w</context>
</contexts>
<marker>Gordon, Swanson, 2008</marker>
<rawString>Andrew Gordon and Reid Swanson. 2008. Envisioning with weblogs. In International Conference on New Media Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gordon</author>
<author>Reid Swanson</author>
</authors>
<title>Identifying personal stories in millions of weblog entries.</title>
<date>2009</date>
<booktitle>In Third International Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="2490" citStr="Gordon and Swanson (2009)" startWordPosition="365" endWordPosition="368">Swanson (2008) identified three primary obstacles to such an approach. First, stories must be distinguished from other weblog content (e.g., lists, recipes, and reviews). Second, stories must be analyzed in order to extract the implicit commonsense knowledge that they contain. Third, inference mechanisms must be developed that use the extracted knowledge to perform the core envisionment tasks listed above. In the current paper, we present an approach to open-domain commonsense inference that addresses each of the three obstacles identified by Gordon and Swanson (2008). We built on the work of Gordon and Swanson (2009), who describe a classificationbased approach to the task of story identification. The authors’ system produced a corpus of approximately one million personal stories, which we used as a starting point. We applied efficient discourse parsing techniques to this corpus as a means of extracting causal and temporal relationships. Furthermore, we developed methods that use the extracted knowledge to generate textual inferences for descriptions of states and events. This work resulted in an end-to-end prototype system capable of generating open-domain, commonsense inferences using a repository of kn</context>
<context position="5913" citStr="Gordon and Swanson (2009)" startWordPosition="909" endWordPosition="912">oes not place limitations on the types (e.g., those described by Gurevych and Zesch (2009) of causes to be identified. This greatly increases and Evert et al. (2008)) that specifically target the the complexity of the inference task, and our results use of content that is collaboratively created by In- exhibit a corresponding degradation; however, our ternet users. Of particular relevance to the present evaluations provide important insights into the task. work is the weblog corpus developed by Burton et 3 Discourse parsing a corpus of stories al. (2009), which was used for the data challenge Gordon and Swanson (2009) developed a superportion of the International Conference on Weblogs vised classification-based approach for identifying and Social Media (ICWSM). The ICWSM weblog personal stories within the Spinn3r corpus. Their corpus (referred to here as Spinn3r) is freely avail- method achieved 75% precision on the binary task able and comprises tens of millions of weblog en- of predicting story versus non-story on a held-out tries posted between August 1st, 2008 and October subset of the Spinn3r corpus. The extracted “story 1st, 2008. corpus” comprises 960,098 personal stories written Gordon et al. (2009</context>
<context position="9295" citStr="Gordon and Swanson (2009)" startWordPosition="1429" endWordPosition="1433">grained relations that capture causal and temporal properties. For example, the corpus differentiates between result and reason for causation and temporal-after and temporal-before for temporal order. In order to increase the amount of available training data, we collapsed all causal and temporal relations into two general relations causes and precedes. This step required normalization of asymmetric relations such as temporal-before and temporal-after. To evaluate the discourse parser described above, we manually annotated 100 randomly selected weblog stories from the story corpus produced by Gordon and Swanson (2009). For increased efficiency, we limited our annotation to the generalized causes and precedes relations described above. We attempted to keep our definitions of these relations in line with those used by RST. Following previous discourse annotation efforts, we annotated relations over clause-level discourse units, permitting relations between adjacent sentences. In total, we annotated 770 instances of causes and 1,009 instances ofprecedes. We experimented with two versions of the RST parser, one trained on the fine-grained RST relations and the other trained on the collapsed relations. At testi</context>
<context position="13516" citStr="Gordon and Swanson (2009)" startWordPosition="2077" endWordPosition="2080">es. The discourse parser extracted 2.2 million instances of the causes relation and 220,000 instances of the precedes relation. As a final step, we indexed the extracted discourse relations with the Lucene information retrieval engine.4 Each discourse unit (two per discourse relation) is treated as a single document, allowing us to query the extracted relations using information retrieval techniques implemented in the Lucene toolkit. 4 Generating textual inferences As mentioned previously, Gordon and Swanson (2008) cite three obstacles to performing commonsense reasoning using weblog stories. Gordon and Swanson (2009) addressed the first (story collection). We addressed the second (story analysis) by developing a discourse parser capable of extracting causal and temporal relations from weblog text (Section 3). In this section, we present a preliminary solution to the third problem - reasoning with the extracted knowledge. 4.1 Inference method In general, we require an inference method that takes as input the following things: 1. A description of the state or event of interest. This is a free-text description of any length. 2. The type of inference to perform, either causal or temporal. 4Available at http:/</context>
</contexts>
<marker>Gordon, Swanson, 2009</marker>
<rawString>Andrew Gordon and Reid Swanson. 2009. Identifying personal stories in millions of weblog entries. In Third International Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Gordon</author>
<author>Benjamin Van Durme</author>
<author>Lenhart Schubert</author>
</authors>
<title>Weblogs as a source for extracting general world knowledge.</title>
<date>2009</date>
<booktitle>In K-CAP ’09: Proceedings ofthefifth international conference on Knowledge capture,</booktitle>
<pages>185--186</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Gordon, Van Durme, Schubert, 2009</marker>
<rawString>Jonathan Gordon, Benjamin Van Durme, and Lenhart Schubert. 2009. Weblogs as a source for extracting general world knowledge. In K-CAP ’09: Proceedings ofthefifth international conference on Knowledge capture, pages 185–186, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Graesser</author>
<author>M Singer</author>
<author>T Trabasso</author>
</authors>
<title>Constructing inferences during narrative text comprehension. Psychological Review,</title>
<date>1994</date>
<pages>101--371</pages>
<contexts>
<context position="1186" citStr="Graesser et al., 1994" startWordPosition="169" endWordPosition="172">or this purpose, and we show how the extracted discourse relations can be used to generate opendomain textual inferences. Our evaluations of the discourse parser and inference models show some success, but also identify a number of interesting directions for future work. 1 Introduction The acquisition of open-domain knowledge in support of commonsense reasoning has long been a bottleneck within artificial intelligence. Such reasoning supports fundamental tasks such as textual entailment (Giampiccolo et al., 2008), automated question answering (Clark et al., 2008), and narrative comprehension (Graesser et al., 1994). These tasks, when conducted in open domains, require vast amounts of commonsense knowledge pertaining to states, events, and their causal and temporal relationships. Manually created resources such as FrameNet (Baker et al., 1998), WordNet (Fellbaum, 1998), and Cyc (Lenat, 1995) encode many aspects of commonsense knowledge; however, coverage of causal and temporal relationships remains low for many domains. Gordon and Swanson (2008) argued that the commonsense tasks of prediction, explanation, and imagination (collectively called envisionment) can 43 be supported by knowledge mined from a la</context>
<context position="18876" citStr="Graesser et al. (1994)" startWordPosition="2948" endWordPosition="2951">drawn randomly from the story corpus. For simplicity, we generated an inference for each sentence in each document. Each inference re-ranking model is able to generate four textual inferences (forward/backward causal/temporal) for each sentence. In our experiments, we only kept the highest-scoring of the four inferences generated by a model. One of the authors then manually evaluated the final predictions for correctness. This was a subjective process, but it was guided by the following requirements: 1. The generated inference must increase the local coherence of the document. As described by Graesser et al. (1994), readers are typically required to make inferences about the text that lead to a coherent understanding thereof. We required the generated inferences to aid in this task. 2. The generated inferences must be globally valid. To demonstrate global validity, consider the following actual output: (4) I didn’t even need a jacket (until I got there). In Example 4, the system-generated forward temporal inference is shown in parentheses. The inference makes sense given its local context; however, it is clear from the surrounding discourse (not shown) that a jacket was not needed at any point in time (</context>
</contexts>
<marker>Graesser, Singer, Trabasso, 1994</marker>
<rawString>A. C. Graesser, M. Singer, and T. Trabasso. 1994. Constructing inferences during narrative text comprehension. Psychological Review, 101:371–395.</rawString>
</citation>
<citation valid="true">
<title>The Peoples Web Meets NLP: Collaboratively Constructed Semantic Resources.</title>
<date>2009</date>
<editor>Iryna Gurevych and Torsten Zesch, editors.</editor>
<contexts>
<context position="2490" citStr="(2009)" startWordPosition="368" endWordPosition="368">tified three primary obstacles to such an approach. First, stories must be distinguished from other weblog content (e.g., lists, recipes, and reviews). Second, stories must be analyzed in order to extract the implicit commonsense knowledge that they contain. Third, inference mechanisms must be developed that use the extracted knowledge to perform the core envisionment tasks listed above. In the current paper, we present an approach to open-domain commonsense inference that addresses each of the three obstacles identified by Gordon and Swanson (2008). We built on the work of Gordon and Swanson (2009), who describe a classificationbased approach to the task of story identification. The authors’ system produced a corpus of approximately one million personal stories, which we used as a starting point. We applied efficient discourse parsing techniques to this corpus as a means of extracting causal and temporal relationships. Furthermore, we developed methods that use the extracted knowledge to generate textual inferences for descriptions of states and events. This work resulted in an end-to-end prototype system capable of generating open-domain, commonsense inferences using a repository of kn</context>
<context position="3803" citStr="(2009)" startWordPosition="572" endWordPosition="572">009) in defining a story to be a “textual discourse that describes a specific series of causally related events in the past, spanning a period of time of minutes, hours, or days, where the author or a close associate is among the participants.” Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 43–51, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics strengths and weaknesses of the system in an effort properties expressed by many factoids extracted by to guide future work. Gordon et al. (2009). We structure our presentation as follows: in Sec- Clark and Harrison (2009) pursued large-scale tion 2, we present previous research that has inves- extraction of knowledge from text using a syntaxtigated the use of large web corpora for natural lan- based approach that was also inspired by the work guage processing (NLP) tasks. In Section 3, we de- of Schubert and Tong (2003). The authors showed scribe an efficient method of automatically parsing how the extracted knowledge tuples can be used weblog stories for discourse structure. In Section 4, to improve syntactic parsing and textual enta</context>
<context position="5378" citStr="(2009)" startWordPosition="827" endWordPosition="827">we hope will guide future work in this area. and Ng (2009), in which the authors developed a 2 Related work semi-supervised method of identifying the causes of Researchers have made many attempts to use the events described in aviation safety reports. Simimassive amount of linguistic content created by larly, our system extracts causal (as well as temusers of the World Wide Web. Progress and chal- poral) knowledge; however, it does this in an open lenges in this area have spawned multiple workshops domain and does not place limitations on the types (e.g., those described by Gurevych and Zesch (2009) of causes to be identified. This greatly increases and Evert et al. (2008)) that specifically target the the complexity of the inference task, and our results use of content that is collaboratively created by In- exhibit a corresponding degradation; however, our ternet users. Of particular relevance to the present evaluations provide important insights into the task. work is the weblog corpus developed by Burton et 3 Discourse parsing a corpus of stories al. (2009), which was used for the data challenge Gordon and Swanson (2009) developed a superportion of the International Conference on Webl</context>
<context position="7186" citStr="(2009)" startWordPosition="1110" endWordPosition="1110">d broad knowledge extraction over the Spinn3r corpus using domain coverage, the story corpus offers unique optechniques described by Schubert and Tong (2003). portunities to NLP researchers. For example, SwanIn this approach, logical propositions (known as fac- son and Gordon (2008) showed how the corpus can toids) are constructed via approximate interpreta- be used to support open-domain collaborative story tion of syntactic analyses. As an example, the sys- writing.3 tem identified a factoid glossed as “doors to a room As described by Gordon and Swanson (2008), may be opened”. Gordon et al. (2009) found that story identification is just the first step towards comthe extracted factoids cover roughly half of the fac- monsense reasoning using personal stories. We adtoids present in the corresponding Wikipedia2 arti- dressed the second step - knowledge extraction - cles. We used a subset of the Spinn3r corpus in by parsing the corpus using a Rhetorical Structure our work, but focused on discourse analyses of en- Theory (Carlson and Marcu, 2001) parser based on tire texts instead of syntactic analyses of single sen- the one described by Sagae (2009). The parser tences. Our goal was to extra</context>
<context position="9295" citStr="(2009)" startWordPosition="1433" endWordPosition="1433">hat capture causal and temporal properties. For example, the corpus differentiates between result and reason for causation and temporal-after and temporal-before for temporal order. In order to increase the amount of available training data, we collapsed all causal and temporal relations into two general relations causes and precedes. This step required normalization of asymmetric relations such as temporal-before and temporal-after. To evaluate the discourse parser described above, we manually annotated 100 randomly selected weblog stories from the story corpus produced by Gordon and Swanson (2009). For increased efficiency, we limited our annotation to the generalized causes and precedes relations described above. We attempted to keep our definitions of these relations in line with those used by RST. Following previous discourse annotation efforts, we annotated relations over clause-level discourse units, permitting relations between adjacent sentences. In total, we annotated 770 instances of causes and 1,009 instances ofprecedes. We experimented with two versions of the RST parser, one trained on the fine-grained RST relations and the other trained on the collapsed relations. At testi</context>
<context position="11535" citStr="(2009)" startWordPosition="1779" endWordPosition="1779">tion matches the reference discourse relation. Complete accuracy For each predicted discourse relation, accuracy is equal to the percentage word overlap with a reference discourse relation of the same type. Table 1 shows the accuracy results for the finegrained and collapsed versions of the RST discourse parser. As shown in Table 1, the collapsed version of the discourse parser exhibits higher overall accuracy. Both parsers predicted the causes relation much more often than the precedes relation, so the overall scores are biased toward the scores for the causes relation. For comparison, Sagae (2009) evaluated a similar RST parser over the test section of the RST corpus, obtaining precision of 42.9% and recall of 46.2% (F1 = 44.5%). In addition to the automatic evaluation described above, we also manually assessed the output of the discourse parsers. One of the authors judged the correctness of each extracted discourse relation, and we found that the fine-grained and collapsed versions of the parser performed equally well with a precision near 33%; however, throughout our experiments, we observed more desirable discourse segmentation when working with the collapsed version of the discours</context>
<context position="13516" citStr="(2009)" startWordPosition="2080" endWordPosition="2080">arser extracted 2.2 million instances of the causes relation and 220,000 instances of the precedes relation. As a final step, we indexed the extracted discourse relations with the Lucene information retrieval engine.4 Each discourse unit (two per discourse relation) is treated as a single document, allowing us to query the extracted relations using information retrieval techniques implemented in the Lucene toolkit. 4 Generating textual inferences As mentioned previously, Gordon and Swanson (2008) cite three obstacles to performing commonsense reasoning using weblog stories. Gordon and Swanson (2009) addressed the first (story collection). We addressed the second (story analysis) by developing a discourse parser capable of extracting causal and temporal relations from weblog text (Section 3). In this section, we present a preliminary solution to the third problem - reasoning with the extracted knowledge. 4.1 Inference method In general, we require an inference method that takes as input the following things: 1. A description of the state or event of interest. This is a free-text description of any length. 2. The type of inference to perform, either causal or temporal. 4Available at http:/</context>
</contexts>
<marker>2009</marker>
<rawString>Iryna Gurevych and Torsten Zesch, editors. 2009. The Peoples Web Meets NLP: Collaboratively Constructed Semantic Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>Propbank: the next level of treebank.</title>
<date>2003</date>
<booktitle>In Proceedings of Treebanks and Lexical Theories.</booktitle>
<contexts>
<context position="23255" citStr="Kingsbury and Palmer, 2003" startWordPosition="3632" endWordPosition="3635"> corpus, with a large majority of these being causal. In contrast, the Penn Discourse TreeBank (Prasad et al., 2008) contains 7,448 training instances of causal relations and 2,763 training instances of temporal relations. This represents a significant increase in the amount of training data over the RST corpus. It would be informative to compare our current results with those obtained using a discourse parser trained on the Penn Discourse TreeBank. One might also extract causal and temporal relations using traditional semantic role analysis based on FrameNet (Baker et al., 1998) or PropBank (Kingsbury and Palmer, 2003). The former defines a number of frames related to causation and temporal order, and roles within the latter could be mapped to standard thematic roles (e.g., cause) via SemLink.5 5.2 Envisioning with the analysis results We believe commonsense reasoning based on weblog stories can also be improved through more sophisticated uses of the extracted discourse relations. As a first step, it would be beneficial to explore alternate input descriptions. As presented in Section 4.2, we make textual inferences at the sentence level for simplicity; however, it might be more reasonable to make inferences</context>
</contexts>
<marker>Kingsbury, Palmer, 2003</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2003. Propbank: the next level of treebank. In Proceedings of Treebanks and Lexical Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Lenat</author>
</authors>
<title>Cyc: a large-scale investment in knowledge infrastructure.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="1467" citStr="Lenat, 1995" startWordPosition="212" endWordPosition="213">acquisition of open-domain knowledge in support of commonsense reasoning has long been a bottleneck within artificial intelligence. Such reasoning supports fundamental tasks such as textual entailment (Giampiccolo et al., 2008), automated question answering (Clark et al., 2008), and narrative comprehension (Graesser et al., 1994). These tasks, when conducted in open domains, require vast amounts of commonsense knowledge pertaining to states, events, and their causal and temporal relationships. Manually created resources such as FrameNet (Baker et al., 1998), WordNet (Fellbaum, 1998), and Cyc (Lenat, 1995) encode many aspects of commonsense knowledge; however, coverage of causal and temporal relationships remains low for many domains. Gordon and Swanson (2008) argued that the commonsense tasks of prediction, explanation, and imagination (collectively called envisionment) can 43 be supported by knowledge mined from a large corpus of personal stories written by Internet weblog authors.1 Gordon and Swanson (2008) identified three primary obstacles to such an approach. First, stories must be distinguished from other weblog content (e.g., lists, recipes, and reviews). Second, stories must be analyze</context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>Douglas B. Lenat. 1995. Cyc: a large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11):33–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation. In</title>
<date>2006</date>
<booktitle>ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>337--344</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="22372" citStr="McClosky et al., 2006" startWordPosition="3496" endWordPosition="3499">last two problems identified by Gordon and Swanson (2008): story analysis and envisioning with the analysis results. 5.1 Story analysis As in other NLP tasks, we observed significant performance degradation when moving from the training genre (newswire) to the testing genre (Internet 48 weblog stories). Because our discourse parser relies heavily on lexical and syntactic features for classification, and because the distribution of the feature values varies widely between the two genres, the performance degradation is to be expected. Recent techniques in parser adaptation for the Brown corpus (McClosky et al., 2006) might be usefully applied to the weblog genre as well. Our supervised classification-based approach to discourse parsing could also be improved with additional training data. Causal and temporal relations are instantiated a combined 2,840 times in the RST corpus, with a large majority of these being causal. In contrast, the Penn Discourse TreeBank (Prasad et al., 2008) contains 7,448 training instances of causal relations and 2,763 training instances of temporal relations. This represents a significant increase in the amount of training data over the RST corpus. It would be informative to com</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 337–344, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac Persing</author>
<author>Vincent Ng</author>
</authors>
<title>Semi-supervised cause identification from aviation safety reports.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>843--851</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<marker>Persing, Ng, 2009</marker>
<rawString>Isaac Persing and Vincent Ng. 2009. Semi-supervised cause identification from aviation safety reports. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 843–851, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Alan Lee</author>
<author>Nikhil Dinesh</author>
<author>Eleni Miltsakaki</author>
<author>Geraud Campion</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>Penn discourse treebank version 2.0. Linguistic Data Consortium,</title>
<date>2008</date>
<contexts>
<context position="22744" citStr="Prasad et al., 2008" startWordPosition="3554" endWordPosition="3557">features for classification, and because the distribution of the feature values varies widely between the two genres, the performance degradation is to be expected. Recent techniques in parser adaptation for the Brown corpus (McClosky et al., 2006) might be usefully applied to the weblog genre as well. Our supervised classification-based approach to discourse parsing could also be improved with additional training data. Causal and temporal relations are instantiated a combined 2,840 times in the RST corpus, with a large majority of these being causal. In contrast, the Penn Discourse TreeBank (Prasad et al., 2008) contains 7,448 training instances of causal relations and 2,763 training instances of temporal relations. This represents a significant increase in the amount of training data over the RST corpus. It would be informative to compare our current results with those obtained using a discourse parser trained on the Penn Discourse TreeBank. One might also extract causal and temporal relations using traditional semantic role analysis based on FrameNet (Baker et al., 1998) or PropBank (Kingsbury and Palmer, 2003). The former defines a number of frames related to causation and temporal order, and role</context>
</contexts>
<marker>Prasad, Lee, Dinesh, Miltsakaki, Campion, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Alan Lee, Nikhil Dinesh, Eleni Miltsakaki, Geraud Campion, Aravind Joshi, and Bonnie Webber. 2008. Penn discourse treebank version 2.0. Linguistic Data Consortium, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
</authors>
<title>Analysis of discourse structure with syntactic dependencies and data-driven shift-reduce parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>81--84</pages>
<location>Paris, France,</location>
<contexts>
<context position="7744" citStr="Sagae (2009)" startWordPosition="1201" endWordPosition="1202">d Swanson (2008), may be opened”. Gordon et al. (2009) found that story identification is just the first step towards comthe extracted factoids cover roughly half of the fac- monsense reasoning using personal stories. We adtoids present in the corresponding Wikipedia2 arti- dressed the second step - knowledge extraction - cles. We used a subset of the Spinn3r corpus in by parsing the corpus using a Rhetorical Structure our work, but focused on discourse analyses of en- Theory (Carlson and Marcu, 2001) parser based on tire texts instead of syntactic analyses of single sen- the one described by Sagae (2009). The parser tences. Our goal was to extract general causal and performs joint syntactic and discourse dependency temporal propositions instead of the fine-grained 3The system (called SayAnything) is available at http://sayanything.ict.usc.edu 2http://en.wikipedia.org 44 parsing using a stack-based, shift-reduce algorithm with runtime that is linear in the input length. This lightweight approach is very efficient; however, it may not be quite as accurate as more complex, chartbased approaches (e.g., the approach of Charniak and Johnson (2005) for syntactic parsing). We trained the discourse pa</context>
<context position="11535" citStr="Sagae (2009)" startWordPosition="1778" endWordPosition="1779">e relation matches the reference discourse relation. Complete accuracy For each predicted discourse relation, accuracy is equal to the percentage word overlap with a reference discourse relation of the same type. Table 1 shows the accuracy results for the finegrained and collapsed versions of the RST discourse parser. As shown in Table 1, the collapsed version of the discourse parser exhibits higher overall accuracy. Both parsers predicted the causes relation much more often than the precedes relation, so the overall scores are biased toward the scores for the causes relation. For comparison, Sagae (2009) evaluated a similar RST parser over the test section of the RST corpus, obtaining precision of 42.9% and recall of 46.2% (F1 = 44.5%). In addition to the automatic evaluation described above, we also manually assessed the output of the discourse parsers. One of the authors judged the correctness of each extracted discourse relation, and we found that the fine-grained and collapsed versions of the parser performed equally well with a precision near 33%; however, throughout our experiments, we observed more desirable discourse segmentation when working with the collapsed version of the discours</context>
</contexts>
<marker>Sagae, 2009</marker>
<rawString>Kenji Sagae. 2009. Analysis of discourse structure with syntactic dependencies and data-driven shift-reduce parsing. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09), pages 81–84, Paris, France, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lenhart Schubert</author>
<author>Matthew Tong</author>
</authors>
<title>Extracting and evaluating general world knowledge from the brown corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 workshop on Text meaning,</booktitle>
<pages>7--13</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4184" citStr="Schubert and Tong (2003)" startWordPosition="633" endWordPosition="636">pages 43–51, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics strengths and weaknesses of the system in an effort properties expressed by many factoids extracted by to guide future work. Gordon et al. (2009). We structure our presentation as follows: in Sec- Clark and Harrison (2009) pursued large-scale tion 2, we present previous research that has inves- extraction of knowledge from text using a syntaxtigated the use of large web corpora for natural lan- based approach that was also inspired by the work guage processing (NLP) tasks. In Section 3, we de- of Schubert and Tong (2003). The authors showed scribe an efficient method of automatically parsing how the extracted knowledge tuples can be used weblog stories for discourse structure. In Section 4, to improve syntactic parsing and textual entailment we present a set of inference mechanisms that use recognition. Bar-Haim et al. (2009) present an efthe extracted discourse relations to generate open- ficient method of performing inference with such domain textual inferences. We conclude, in Section knowledge. 5, with insights into story-based envisionment that Our work is also related to the work of Persing we hope will</context>
<context position="6737" citStr="Schubert and Tong (2003)" startWordPosition="1036" endWordPosition="1039">corpus. Their corpus (referred to here as Spinn3r) is freely avail- method achieved 75% precision on the binary task able and comprises tens of millions of weblog en- of predicting story versus non-story on a held-out tries posted between August 1st, 2008 and October subset of the Spinn3r corpus. The extracted “story 1st, 2008. corpus” comprises 960,098 personal stories written Gordon et al. (2009) describe an approach to by weblog users. Due to its large size and broad knowledge extraction over the Spinn3r corpus using domain coverage, the story corpus offers unique optechniques described by Schubert and Tong (2003). portunities to NLP researchers. For example, SwanIn this approach, logical propositions (known as fac- son and Gordon (2008) showed how the corpus can toids) are constructed via approximate interpreta- be used to support open-domain collaborative story tion of syntactic analyses. As an example, the sys- writing.3 tem identified a factoid glossed as “doors to a room As described by Gordon and Swanson (2008), may be opened”. Gordon et al. (2009) found that story identification is just the first step towards comthe extracted factoids cover roughly half of the fac- monsense reasoning using perso</context>
</contexts>
<marker>Schubert, Tong, 2003</marker>
<rawString>Lenhart Schubert and Matthew Tong. 2003. Extracting and evaluating general world knowledge from the brown corpus. In Proceedings of the HLT-NAACL 2003 workshop on Text meaning, pages 7–13, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reid Swanson</author>
<author>Andrew Gordon</author>
</authors>
<title>Say anything: A massively collaborative open domain story writing companion.</title>
<date>2008</date>
<booktitle>In First International Conference on Interactive Digital Storytelling.</booktitle>
<marker>Swanson, Gordon, 2008</marker>
<rawString>Reid Swanson and Andrew Gordon. 2008. Say anything: A massively collaborative open domain story writing companion. In First International Conference on Interactive Digital Storytelling.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>