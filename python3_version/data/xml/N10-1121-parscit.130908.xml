<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000991">
<title confidence="0.996164">
Convolution Kernels for Opinion Holder Extraction
</title>
<author confidence="0.991553">
Michael Wiegand and Dietrich Klakow
</author>
<affiliation confidence="0.9874305">
Spoken Language Systems
Saarland University
</affiliation>
<address confidence="0.844301">
D-66123 Saarbr¨ucken, Germany
</address>
<email confidence="0.998706">
{Michael.Wiegand|Dietrich.Klakow}@lsv.uni-saarland.de
</email>
<sectionHeader confidence="0.998594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999786095238095">
Opinion holder extraction is one of the impor-
tant subtasks in sentiment analysis. The ef-
fective detection of an opinion holder depends
on the consideration of various cues on vari-
ous levels of representation, though they are
hard to formulate explicitly as features. In this
work, we propose to use convolution kernels
for that task which identify meaningful frag-
ments of sequences or trees by themselves.
We not only investigate how different levels
of information can be effectively combined
in different kernels but also examine how the
scope of these kernels should be chosen. In
general relation extraction, the two candidate
entities thought to be involved in a relation are
commonly chosen to be the boundaries of se-
quences and trees. The definition of bound-
aries in opinion holder extraction, however, is
less straightforward since there might be sev-
eral expressions beside the candidate opinion
holder to be eligible for being a boundary.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906972972973">
In recent years, there has been a growing interest
in the automatic detection of opinionated content
in natural language text. One of the more impor-
tant tasks in sentiment analysis is the extraction of
opinion holders. Opinion holder extraction is one
of the critical components of an opinion question-
answering system (i.e. systems which automatically
answer opinion questions, such as “What does [X]
like about [Y]?”). Such systems need to be able to
distinguish which entities in a candidate answer sen-
tence are the sources of opinions (= opinion holder)
and which are the targets.
On other NLP tasks, in particular, on relation extrac-
tion, there has been much work on convolution ker-
nels, i.e. kernel functions exploiting huge amounts
of features without an explicit feature representa-
tion. Previous research on that task has shown that
convolution kernels, such as sequence and tree ker-
nels, are quite effective when compared to manual
feature engineering (Moschitti, 2008; Bunescu and
Mooney, 2005; Nguyen et al., 2009). In order to
effectively use convolution kernels, it is often nec-
essary to choose appropriate substructures of a sen-
tence rather than represent the sentence as a whole
structure (Bunescu and Mooney, 2005; Zhang et al.,
2006; Moschitti, 2008). As for tree kernels, for ex-
ample, one typically chooses the syntactic subtree
immediately enclosing two entities potentially ex-
pressing a specific relation in a given sentence. The
opinion holder detection task is different from this
scenario. There can be several cues within a sen-
tence to indicate the presence of a genuine opinion
holder and these cues need not be member of a par-
ticular word group, e.g. they can be opinion words
(see Sentences 1-3), communication words, such as
maintained in Sentence 2, or other lexical cues, such
as according in Sentence 3.
</bodyText>
<listItem confidence="0.985907">
1. The U.S. commanders consideropinion the prisoners to be un-
lawful combatantsopinion as opposed to prisoners of war.
2. During the summit, Koizumi maintainedcommunication a
clear-cut collaborative stanceopinion towards the U.S. and em-
phasized that the President was objectiveopinion and circum-
spect.
3. Accordingcue to Fernandez, it was the worst mistakeopinion in
the history of the Argentine economy.
</listItem>
<page confidence="0.972653">
795
</page>
<note confidence="0.7505255">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 795–803,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999914416666667">
Thus, the definition of boundaries of the structures
for the convolution kernels is less straightforward in
opinion holder extraction.
The aim of this paper is to explore in how far convo-
lution kernels can be beneficial for effective opinion
holder detection. We are not only interested in how
far different kernel types contribute to this extraction
task but we also contrast the performance of these
kernels with a manually designed feature set used
as a standard vector kernel. Finally, we also exam-
ine the effectiveness of expanding word sequences
or syntactic trees by additional prior knowledge.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99974618">
Choi et al. (2005) examine opinion holder extraction
using CRFs with various manually defined linguis-
tic features and patterns automatically learnt by the
AutoSlog system (Riloff, 1996). The linguistic fea-
tures focus on named-entity information and syntac-
tic relations to opinion words. In this paper, we use
very similar settings. The features presented in Kim
and Hovy (2005) and Bloom et al. (2007) resemble
very much Choi et al. (2005). Bloom et al. (2007)
also consider communication words to be predictive
cues for opinion holders.
Kim and Hovy (2006) and Bethard et al. (2005) ex-
plore the usefulness of semantic roles provided by
FrameNet (Fillmore et al., 2003) for both opinion
holder and opinion target extraction. Due to data
sparseness, Kim and Hovy (2006) expand FrameNet
data by using an unsupervised clustering algorithm.
Choi et al. (2006) is an extension of Choi et al.
(2005) in that opinion holder extraction is learnt
jointly with opinion detection. This requires that
opinion expressions and their relations to opinion
holders are annotated in the training data. Seman-
tic roles are also taken as a potential source of in-
formation. In our work, we deliberately work with
minimal annotation and, thus, do not consider any
labeled opinion expressions and relations to opinion
holders in the training data. We exclusively rely on
entities marked as opinion holders. In many practi-
cal situations, the annotation beyond opinion holder
labeling is too expensive.
Complex convolution kernels have been success-
fully applied to various NLP tasks, such as rela-
tion extraction (Bunescu and Mooney, 2005; Zhang
et al., 2006; Nguyen et al., 2009), question an-
swering (Zhang and Lee, 2003; Moschitti, 2008),
and semantic role labeling (Moschitti et al., 2008).
In all these tasks, they offer competitive perfor-
mance to manually designed feature sets. Bunescu
and Mooney (2005) combine different sequence ker-
nels encoding different contexts of candidate en-
tities in a sentence. They argue that several ker-
nels encoding different contexts are more effective
than just using one kernel with one specific context.
We build on that idea and compare various scopes
eligible for opinion holder extraction. Moschitti
(2008) and Nguyen et al. (2009) suggest that differ-
ent kinds of information, such as word sequences,
part-of-speech tags, syntactic and semantic informa-
tion should be contained in separate convolution ker-
nels. We also adhere to this notion.
</bodyText>
<sectionHeader confidence="0.998519" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.99989725">
As labeled data, we use the sentiment annotation of
the MPQA 2.0 corpus1. Opinion holders are not ex-
plicitly labeled as such. However sources of pri-
vate states and subjective speech events (Wiebe et
al., 2003) are a fairly good approximation of the
task. Previous work (Choi et al., 2005; Kim and
Hovy, 2005; Choi et al., 2006) uses similar approxi-
mations.
</bodyText>
<sectionHeader confidence="0.988747" genericHeader="method">
4 Method
</sectionHeader>
<bodyText confidence="0.999971142857143">
In this work, we consider all noun phrases (NPs)
as possible candidate opinion holders. Therefore,
the set of all data instances is the set of the NPs
within the MPQA 2.0 corpus. Each NP is labeled
as to whether it is a genuine opinion holder or not.
Throughout this section, we will use Sentence 2
from Section 1 as an example.
</bodyText>
<subsectionHeader confidence="0.998377">
4.1 The Different Levels of Representation
</subsectionHeader>
<bodyText confidence="0.999652166666667">
Several levels of representation are important for
opinion holder extraction. Table 1 lists all the dif-
ferent levels that are used in this work. Generalized
sequences employ named-entity tags, an OPINION
tag for opinion words and a COMM tag for com-
munication words2. Thus, in a generalized word se-
</bodyText>
<footnote confidence="0.999387333333333">
1www.cs.pitt.edu/mpqa/databaserelease
2Note that all candidate tokens are reduced to one generic
CAND token. Thus, we hope to account for data sparseness in
</footnote>
<page confidence="0.998254">
796
</page>
<bodyText confidence="0.998524652173913">
quence (WRDGN) a word is replaced by a general-
ized token whereas in a generalized part-of-speech
sequence (POSGN) a part-of-speech tag is replaced.
For augmented constituent trees (CONSTAUG), the
same sources of information are used. The differ-
ence to generalizing sequences is that instead of re-
placing words by generalized tokens, we add a node
in the syntax tree with a generalized token so that it
dominates the pertaining leaf node (see also nodes
marked with AUG in Figure 2). All sources used for
this type of generalization are known to be predictive
for opinion holder classification (Choi et al., 2005;
Kim and Hovy, 2005; Choi et al., 2006; Kim and
Hovy, 2006; Bloom et al., 2007).
Note that the grammatical relation paths, i.e.
GRAMWRD and GRAMPOS, can only be applied
in case there is another expression in the focus in
addition to the candidate of the data instance itself,
e.g. the nearest opinion expression to the candidate.
Section 4.4 explains in detail how this is done.
Predicate-argument structures (PAS) are repre-
sented by PropBank trees (Kingsbury and Palmer,
2002).
</bodyText>
<subsectionHeader confidence="0.659213">
4.2 Support Vector Machines and Kernel
Methods
</subsectionHeader>
<bodyText confidence="0.9998971">
Support Vector Machines (SVMs) are one of the
most robust supervised machine learning techniques
in which training data instances x� are separated by a
hyperplane H(x) = w� · x� + b = 0 where w E R&apos;
and b E R. One advantage of SVMs is that ker-
nel methods can be applied which map the data to
other feature spaces in which they can be separated
more easily. Given a feature function 0 : ® —* R,
where ® is the set of the objects, the kernel trick
allows the decision hyperplane to be rewritten as:
</bodyText>
<equation confidence="0.963631333333333">
�xi · x�+ b =
yiαi0 (oi) · 0 (o) + b
i=1...l
</equation>
<bodyText confidence="0.9799215">
where yi is equal to 1 for positive and −1 for
negative examples, αi E R with αi &gt; 0, oibi E
11, ... , l} are the training instances and the product
K(oi, o) = (0(oi) · 0(o)) is the kernel function as-
sociated with the mapping 0.
case there are several tokens making up the candidate.
</bodyText>
<subsectionHeader confidence="0.996306">
4.3 Sequence and Tree Kernels
</subsectionHeader>
<bodyText confidence="0.974619152173913">
A sequence kernel (SK) measures the similarity
of two sequences by counting the number of com-
mon subsequences. We use the kernel by Taylor
and Christianini (2004) which has the advantage that
it also considers subsequences of the original se-
quence with some elements missing. The extent of
these gaps in a sequence is suitably reflected by a
weighting function incorporated into the kernel.
Tree kernels (TKs) represent trees by their sub-
structures. The feature space of these substructures,
or fragments, is mapped onto a vector space. The
kernel function computes the similarity of pairs of
trees by counting the number of common fragments.
In this work, we evaluate two tree kernels: Subset
Tree Kernel (STK) (Collins and Duffy, 2002) and
Partial Tree Kernel (PTKbasic) (Moschitti, 2006).
In STK, a tree fragment can be any set of nodes
and edges of the original tree provided that every
node has either all or none of its children. This con-
straint makes that kind of kernel well-suited for con-
stituency trees which have been generated by con-
text free grammars since the constraint corresponds
to the restriction that no grammatical rule must be
broken. For example, STK enforces that a subtree,
such as [VP [VBZ, NP]], cannot be matched with
[VP [VBZ]] since the latter VP node only possesses
one of the children of the former.
PTKbasic is more flexible since the constraint
of STK on nodes is relaxed. This makes this
type of tree kernel less suitable for constituency
trees. We, therefore, apply it only to trees
representing predicate-argument structures (PAS)
(see Figure 1). Note that a data instance is
represented by a set of those structures3 rather
than a single structure. Thus, the actual partial
tree kernel function we use for this task, PTK,
sums over all possible pairs PASl and PAS,,, of
two data instances xi and xj: PTK(xi, xj) =
� � PTKbasic(PASl, PAS,.).
PASl∈xi PAS,∈xp
To summarize, Table 2 lists the different kernel
types we use coupled with the suitable levels of rep-
resentation. This choice of pairing has already been
motivated and empirically proven suitable on other
3i.e. all predicate-argument structures of a sentence in which
the head of the candidate opinion holder occurs
</bodyText>
<equation confidence="0.997647625">
H(x) = �
i
= 1...l
yiαi
�x�+ b =
i=1...l
Yi ·
yiαi
</equation>
<page confidence="0.992178">
797
</page>
<table confidence="0.99955225">
Type Description Example
WRD sequence of words During the summit, KoizumiCAND maintained a clear-cut
collaborative stance ...
WRDGN sequence of generalized words During the summit, CAND COMM OPINION ...
POS part-of-speech sequence IN DET NN PUNC CAND VBD DET JJ JJ NN ...
POSGN generalized part-of-speech sequence IN DET NN PUNC CAND COMM OPINION ...
CONST constituency tree see Figure 2 without nodes marked AUG
CONSTAUG augmented constituency tree see Figure 2
GRAMWRD grammatical relation path labels with words KoizumiCAND NSUBJT maintained DOBJJ stance
GRAMPOS grammatical relation path labels with part-of-speech tags CAND NSUBJT VBD DOBJJ NN
PAS predicate argument structures see Figure 1(a)
PASAUG augmented predicate argument structures see Figure 1(b)
</table>
<tableCaption confidence="0.999965">
Table 1: The different levels of representation.
</tableCaption>
<figure confidence="0.989873">
(b) augmented
</figure>
<figureCaption confidence="0.997453">
Figure 1: Predicate-argument structures (PAS).
</figureCaption>
<table confidence="0.965701285714286">
tasks (Moschitti, 2008; Nguyen et al., 2009).
Type Description Levels of Representation
SK Sequential Kernel WRD(GN), POS(GN),
STK Subset Tree Kernel GRAMWRD, GRAMPOS
PTK Partial Tree Kernel CONST(AUG)
V K Vector Kernel PAS
not restricted
</table>
<tableCaption confidence="0.999704">
Table 2: The different types of kernels.
</tableCaption>
<subsectionHeader confidence="0.991747">
4.4 The Different Scopes
</subsectionHeader>
<bodyText confidence="0.94548815">
We argue that using the entire word sequence or syn-
tax tree of the sentence in which a candidate opinion
holder is situated to represent a data instance pro-
duces too large structures for a convolution kernel.
Since a classifier based on convolution kernels has
to derive meaningful features by itself, the larger
these structures are, the more likely noise is included
in the model. Previous work in relation extraction
has also shown that the usage of more focused sub-
structures, e.g. the smallest subtree containing the
two candidate entities of a relation, is more effec-
tive (Zhang et al., 2006). Unfortunately, in our task
there is only one explicit entity we know of for each
data instance which is the candidate opinion holder.
However, there are several indicative cues within the
context of the candidate which might be considered
important. We identify three different cues being the
nearest predicate, i.e. full verb or nominalization,
opinion word and communication word4. For each
of these expressions, we define a scope where the
boundaries are the candidate opinion holder and the
pertaining cue. Given these scopes, we can define
resulting subsequences/subtrees and combine them.
We further add two background scopes, one being
the semantic scope of the candidate opinion holder
and the entire sentence. As semantic scope we con-
sider the subclause in which a candidate opinion
holder is situated5.
Figure 2 illustrates the different scopes. Abbre-
viations are explained in Table 3. As already men-
tioned in Section 4.1 for grammatical relation paths,
a second expression in addition to the candidate
opinion holder is required. These expressions can be
derived from the different scopes, i.e. for PRED it
4These three expressions may coincide but do not have to.
5Typically, the subtree representing a subclause has the clos-
est S node dominating the candidate opinion holder as the root
node and it contains only those nodes from the original sentence
parse which are also dominated by that S node and whose path
to that node does not contain another S node.
</bodyText>
<figure confidence="0.978438">
(a) plain
</figure>
<page confidence="0.980836">
798
</page>
<bodyText confidence="0.9974615">
is the nearest predicate to the candidate, for OP it is
the nearest opinion word and for COMM it is the
nearest communication word. For the background
scopes SEM and SENT, however, there is no sec-
ond expression in focus. Therefore, grammatical re-
lation paths cannot be defined for these scopes.
</bodyText>
<table confidence="0.9989664">
Type Description
PRED scope with the boundaries being the candidate opinion
holder and the nearest predicate
OP scope with the boundaries being the candidate opinion
holder and nearest opinion word
COMM scope with the boundaries being the candidate opinion
holder and the nearest communication word
SEM semantic scope of the candidate opinion holder, i.e.
subclause containing the candidate
SENT entire sentence in which in the opinion holder occurs
</table>
<tableCaption confidence="0.999927">
Table 3: The different types of scope.
</tableCaption>
<subsectionHeader confidence="0.861717">
4.5 Manually Designed Feature Set for a
Standard Vector Kernel
</subsectionHeader>
<bodyText confidence="0.999942875">
In addition to the different types of convolution ker-
nels, we also define an explicit feature set for a vec-
tor kernel (V K). Many of these features mainly de-
scribe properties of the relation between the candi-
date and the nearest predicate6 since in our initial
experiments the nearest predicate has always been
the strongest cue. Adding these types of features
for other cues, e.g. the nearest opinion or commu-
nication word, only resulted in a decrease in perfor-
mance. Table 4 lists all the features we use. Note
that this manual feature set employs all those sources
of information which are also exploited by the con-
volution kernels. Some of the information contained
in the convolution kernels can, however, only be rep-
resented in a more simplified fashion when using
a manual feature set. For example, the first PAS
in Figure 1(a) is converted to just the pair of pred-
icate and argument representing the candidate (i.e.
REL:maintain A0:Koizumi). The entire PAS is not
used since it would create too sparse features. Con-
volution kernels can cope with fairly complex struc-
tures as input since they internally match substruc-
tures. Manual features are less flexible since they do
not account for partial matches.
</bodyText>
<footnote confidence="0.985723">
6We select the nearest predicate by using the syntactic parse
tree. Thus, we hope to select the predicate which syntactically
</footnote>
<table confidence="0.878222933333333">
headword/governing category of CAND
is CAND capitalized/a person?
is CAND subj|dobj|iobj|pobj of OPINION/COMM?
is CAND preceded by according to? (Choi et al., 2005)
does CAND contain possessive and is followed by OPIN-
ION/COMM? (Choi et al., 2005)
is CAND preceded by by which is attached to OPINION/COMM?
(Choi et al., 2005)
predicate-argument pairs in which CAND occurs
lemma/part-of-speech tag/subcategorization frame/voice of nearest
predicate
is nearest predicate OPINION/COMM?
does CAND precede/follow nearest predicate?
words between nearest predicate and CAND (bag of words)
part-of-speech sequence between nearest predicate and CAND
</table>
<tableCaption confidence="0.751601">
constituency path/grammatical relation path from predicate to
CAND
Table 4: Manually designed feature set.
</tableCaption>
<sectionHeader confidence="0.998944" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999904833333334">
We used 400 documents of the MPQA corpus for
five-fold crossvalidation and 133 documents as a de-
velopment set. We report statistical significance on
the basis of a paired t-test using 0.05 as the signif-
icance level. All experiments were done with the
SVM-Light-TK toolkit7. We evaluated on the basis
of exact phrase matching. We set the trade-off pa-
rameter j = 5 for all feature sets. For the manual
feature set we used a polynomial kernel of third de-
gree. These two critical parameters were tuned on
the development set. As far as the sequence and
tree kernels are concerned, we used the parameter
settings from Moschitti (2008), i.e. λ = 0.4 and
p = 0.4. Kernels were combined using plain sum-
mation. The documents were parsed using the Stan-
ford Parser (Klein and Manning, 2003). Named-
entity information was obtained by the Stanford tag-
ger (Finkel et al., 2005). Semantic roles were ob-
tained by using the parser by Zhang et al. (2008).
Opinion expressions were identified using the Sub-
jectivity Lexicon from the MPQA project (Wil-
son et al., 2005). Communication words were ob-
tained by using the Appraisal Lexicon (Bloom et al.,
2007). Nominalizations were recognized by looking
</bodyText>
<footnote confidence="0.731744">
relates to the candidate opinion holder.
7available at disi.unitn.it/moschitti
</footnote>
<page confidence="0.996662">
799
</page>
<figureCaption confidence="0.996181">
Figure 2: Illustration of the different scopes on a CONSTAUG; nodes belonging to the candidate opinion holder are
marked with LAND.
</figureCaption>
<bodyText confidence="0.816972">
up nouns in NOMLEX (Macleod et al., 1998).
</bodyText>
<subsectionHeader confidence="0.882272">
5.1 Notation
</subsectionHeader>
<bodyText confidence="0.997667909090909">
Each kernel is represented as a triple
(levelOfRepresentation (Table 1), Scope (Table 3), typeOfXernel
(Table 2)), e.g. (CONST, SENT, STK) is a Subset
Tree Kernel of a constituency parse having the
scope of the entire sentence. Note that not all com-
binations of these three parameters are meaningful.
In the following, we will just focus on important
and effective combinations. The kernel composed
of manually designed features is denoted by just
V K. The kernel composed of predicate-argument
structures is denoted by (PAS, SENT, PTK).
</bodyText>
<subsectionHeader confidence="0.980795">
5.2 Vector Kernel (VK)
</subsectionHeader>
<bodyText confidence="0.999981571428571">
The first line in Table 7 displays the result of the
vector kernel using a manually designed feature set.
It should be interpreted as a baseline. Due to the
high class imbalance we will focus on the compari-
son of F(1)-Score throughout this paper rather than
accuracy which is fairly biased on this data set. The
F-Score of this classifier is at 56.16%.
</bodyText>
<subsectionHeader confidence="0.995589">
5.3 Sequence Kernels (SKs)
</subsectionHeader>
<bodyText confidence="0.999312416666667">
For both sequence and tree kernels we need to find
out what the best scope is, whether it is worthwhile
to combine different scopes and what different lay-
ers of representation can be usefully combined.
The upper part of Table 5 lists the results of simple
word kernels using the different scopes. The perfor-
mance of the kernels using individual scopes varies
greatly. The best scope is PRED (1), the second
best is SEM (2). The good performance of PRED
does not come as a surprise since the sequence is the
smallest among the different scopes, so this scope is
least affected by data sparseness. Moreover, this re-
sult is consistent with our initial experiments on the
manual feature set (see Section 4.5).
Using different combinations of the word se-
quence kernels shows that PRED and SEM (6)
are a good combination, whereas OP, COMM,
and SENT (7;8;9) do not positively contribute to
the overall performance which is consistent which
the individual scope evaluation. Apparently, these
scopes capture less linguistically relevant structure.
The next part of Table 5 shows the contribution of
POS kernels when added to WRD kernels. Adding
the corresponding POS kernel to the WRD kernel
with PRED scope (10) results in an improvement
by more than 5% in F-Score. We get another im-
provement by approx. 3% when the corresponding
SEM kernels (11) are added. This suggests that
POS is an effective generalization and that the two
scopes PRED and SEM are complementary.
For the GRAMWRD kernel, the PRED scope
(12) is again most effective. We assume that this ker-
nel most likely expresses meaningful syntactic rela-
tionships for our task. Adding the GRAMppS ker-
nel (14) gives another boost by almost 4%.
Generalized sequence kernels are important.
</bodyText>
<page confidence="0.986475">
800
</page>
<bodyText confidence="0.99986275">
Adding the corresponding WRDGN kernels to the
WRD kernel with PRED and SEM scope results
in an improvement from 47.77% (1) to 53.00% (15)
which is a bit less than the combination of WRD
and POS(GN) kernels (16). However, these types of
kernels seem to be complementary since their com-
bination provides an F-Score of 56.06% (17). This
kernel combination already performs on a par with
the manually designed vector kernel though less in-
formation is taken into consideration.
Finally, the best combination of sequence ker-
nels (18) comprises WRD, WRDGN, POS, and
POSGN kernels with PRED and SEM scope
combined with a GRAMWpD and a GRAMpOS
kernel with PRED scope. The performance of
58.70% significantly outperforms the vector kernel.
</bodyText>
<subsectionHeader confidence="0.976593">
5.4 Tree Kernels (TKs)
</subsectionHeader>
<bodyText confidence="0.9998297">
Table 6 shows the results of the different tree ker-
nels. The table is divided into two halves. The
left half (A) are plain tree kernels, whereas the right
half (B) are the augmented tree kernels. As far as
CONST kernels are concerned, there is a system-
atic improvement by approximately 2% using tree
augmentation. This proves that further non-syntactic
knowledge added to the tree itself results in an im-
proved F-Score. However, tree augmentation does
not have any impact on the PAS kernels.
The overall performance of the tree kernels shows
that they are much more expressive than sequence
kernels. For instance, in order to obtain the same
performance as of (CONSTAUG, PRED, STK)
(19B), i.e. a single kernel with an F-Score 56.52, it
requires several sequence kernels, hence much more
effort. The performance of the different CONST
kernels relative to each other resembles the results
of the WRD kernels. The best scope is PRED
(19). By far the worst performance is obtained by
the SENT scope (23). The combination of PRED
and SEM scope achieves an F-Score of 59.67%
(25B) which is already slightly better than the best
configuration of sequence kernels (18).
The performance of the PAS kernel (28A) with
an F-Score of 53.51% is slightly worse than the best
single plain CONST kernel (19A). The PAS ker-
nel and the CONST kernels are complementary,
since their best combination (29B) achieves an F-
Score of 61.67% which is significantly better than
</bodyText>
<table confidence="0.999805125">
Combination Acc. Prec. Rec. F1
VK 93.63 53.28 59.37 56.16
best SKs 94.21 57.64 59.81 58.70
best TKs 94.16 56.18 68.36 61.67*
VK+best SKs 94.34 58.44 61.27 59.82*
VK+best TKs 94.33 57.41 68.03 62.27*
best SKs +best TKs 94.49 59.22 63.96 61.49*
VK + best SKs + best TKs 94.53 59.10 66.57 62.61*†
</table>
<tableCaption confidence="0.998064333333333">
Table 7: Results of kernel combinations (*: significantly
better than best SKs; †: significantly better than best TKs;
all convolution kernels are significantly better than VK).
</tableCaption>
<bodyText confidence="0.8185335">
the best combination of CONST kernels (25B) or
sequence kernels (18).
</bodyText>
<subsectionHeader confidence="0.970688">
5.5 Combinations
</subsectionHeader>
<bodyText confidence="0.9974932">
Table 7 lists the results of the different kernel type
combinations. If VK is added to the best TKs, the
best SKs, or both, a slight increase in F-Score is
achieved. The best performance with an F-Score of
62.61% is obtained by combining all kernels.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999810636363636">
In this paper, we compared convolution kernels for
opinion holder extraction. We showed that, in gen-
eral, a combination of two scopes, namely the scope
immediately encompassing the candidate opinion
holder and its nearest predicate and the subclause
containing the candidate opinion holder provide best
performance. Tree kernels containing constituency
parse information and semantic roles achieve better
performance than sequence kernels or vector kernels
using a manually designed feature set. Best perfor-
mance is achieved if all kernels are combined.
</bodyText>
<sectionHeader confidence="0.996497" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9992433">
Michael Wiegand was funded by the German research
council DFG through the International Research Training
Group “IRTG” between Saarland University and Univer-
sity of Edinburgh.
The authors would like to thank Yi Zhang for pro-
cessing the MPQA corpus with his semantic-role label-
ing system, the researchers from the MPQA project for
helping to create an opinion holder corpus, and, in partic-
ular, Alessandro Moschitti for insightful comments and
suggestions.
</bodyText>
<page confidence="0.992537">
801
</page>
<table confidence="0.9998416">
ID Kernel Acc. Prec. Rec. F1
1 (WRD, PRED, SK) 93.25 51.08 42.29 46.26
2 (WRD,OP,SK) 92.77 46.38 32.52 38.21
3 (WRD, COMM, SK) 92.42 43.70 35.99 39.46
4 (WRD, SEM, SK) 93.16 50.32 34.65 41.04
5 (WRD, SENT, SK) 90.60 29.90 27.29 28.53
6 (WRD, PRED, SK) + (WRD, SEM, SK) 93.78 56.55 41.36 47.77
7 PjE{PRED,OP,COMM}(WRD, j,SK) 93.55 54.26 39.50 45.71
8 PjEScopes\SENT(WRD, j, SK) 93.82 57.21 40.28 47.26
9 PjEScopes(WRD, j, SK) 93.63 55.15 39.52 46.03
10 (WRD, PRED, SK) + (POS, PRED, SK) 93.03 49.39 53.53 51.37
11 PiE{PRED,SEM} ((WRD, i, SK) + (POS, i, SK)) 93.86 55.60 53.22 54.38
12 14 94.01 58.19 45.88 51.29
13 PiE{PRED,SEM}(WRD, i, SK) + (GRAMWRD, PRED, SK) 93.83 56.28 45.64 50.40
P iE{P RED,SEM}(W RD, i, SK) + P jE{P RED,OP,COMM}(GRAMW RD, j, SK) 93.98 56.59 53.92 55.21
X (WRD, i, SK)+(GRAMWRD, PRED, SK)+(GRAMPOS, PRED, SK)
iE{PRED,SEM}
15 17 93.97 57.08 49.46 53.00
16 PiE{PRED,SEM} ((WRD, i, SK) + (WRDGN, i, SK)) 93.97 56.60 52.42 54.42
PiE{PRED,SEM} ((WRD, i, SK) + (POSGN, i, SK)) 93.85 55.16 57.00 56.06
X ((WRD, i, SK) + (WRDGN, i, SK) + (POS, i, SK) + (POSGN, i, SK))
iE{PRED,SEM}
18 X ((WRD, i, SK) + (WRDGN, i, SK) + (POS, i, SK) + (POSGN, i, SK)) 94.21 57.64 59.81 58.70
iE{PRED,SEM}
+(GRAMWRD, PRED, SK) + (GRAMPOS, PRED, SK)
</table>
<tableCaption confidence="0.994576">
Table 5: Results of the different sequence kernels.
</tableCaption>
<table confidence="0.999464368421052">
A B
i = CONST, j = PAS i = CONSTAUG, j = PASAUG
ID Kernel Acc. Prec. Rec. F1 Acc. Prec. Rec. F1
19 (i, PRED, STK) 92.89 48.68 62.34 54.67 93.12 49.99 65.04 56.52
20 (i, OP, STK) 93.04 49.49 54.71 51.96 93.27 50.93 59.06 54.68
21 (i, COMM, STK) 92.76 47.79 55.89 51.50 92.96 49.03 58.85 53.47
22 (i, SEM, STK) 93.70 54.40 52.13 53.23 93.90 55.47 56.59 56.03
23 (i, SENT, STK) 92.42 44.34 39.92 41.99 92.50 45.20 42.40 43.74
24 PkE{PRED,OP,COMM}(i, k, STK) 93.62 53.26 60.05 56.44 93.77 54.06 63.21 58.26
25 PkE{PRED,SEM}(i, k, STK) 93.90 55.26 59.50 57.30 94.13 56.57 63.12 59.67
26 PkEScopes\SENT (i, k, STK) 94.09 56.65 59.68 58.11 94.21 57.21 62.61 59.80
27 PkEScopes(i, k, STK) 94.14 57.41 57.88 57.63 94.29 58.11 61.10 59.56
28 (j, SENT, PTK) 92.11 45.02 69.96 53.51 91.92 44.27 67.39 53.43
29 94.05 55.68 66.01 60.40 94.16 56.18 68.36 61.67
30 94.30 57.95 62.62 60.19 94.36 58.07 64.94 61.31
X (i, k, STK)+(PAS, SENT, PTK)
kE{PRED,SEM}
X (i, k, STK) + (PAS, SENT, PTK)
kEScopes\SENT
</table>
<tableCaption confidence="0.999117">
Table 6: Results of the different tree kernels.
</tableCaption>
<page confidence="0.995988">
802
</page>
<sectionHeader confidence="0.996151" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999755915094339">
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2005. Extracting
Opinion Propositions and Opinion Holders using Syn-
tactic and Lexical Cues. In Computing Attitude and
Affect in Text: Theory and Applications. Springer.
Kenneth Bloom, Sterling Stein, and Shlomo Argamon.
2007. Appraisal Extraction for News Opinion Analy-
sis at NTCIR-6. In Proceedings ofNTCIR-6 Workshop
Meeting, Tokyo, Japan.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence Kernels for Relation Extraction. In Pro-
ceedings of the Conference on Neural Information
Processing Systems (NIPS), Vancouver, Canada.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying Sources of Opinions
with Conditional Random Fields and Extraction Pat-
terns. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP), Vancouver,
Canada.
Yejin Choi, Eric Breck, and Claire Cardie. 2006.
Joint Extraction of Entities and Relations for Opin-
ion Recognition. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Sydney, Australia.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Philadelphia, USA.
Charles. J. Fillmore, Christopher R. Johnson, and
Miriam R. Petruck. 2003. Background to FrameNet.
International Journal ofLexicography, 16:235 – 250.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. In Proceedings of the Annual Meeting of the
Associationfor Computational Linguistics (ACL), Ann
Arbor, USA.
Soo-Min Kim and Eduard Hovy. 2005. Identifying
Opinion Holders for Question Answering in Opin-
ion Texts. In Proceedings of AAAI-05 Workshop
on Question Answering in Restricted Domains, Pitts-
burgh, USA.
Soo-Min Kim and Eduard Hovy. 2006. Extracting Opin-
ions, Opinion Holders, and Topics Expressed in On-
line News Media Text. In Proceedings of the ACL
Workshop on Sentiment and Subjectivity in Text, Syd-
ney, Australia.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the 3rd Confer-
ence on Language Resources and Evaluation (LREC),
Las Palmas, Spain.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of the Annual
Meeting ofthe Association for Computational Linguis-
tics (ACL), Sapporo, Japan.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A Lexicon of Nominalizations. In Proceedings ofEU-
RALEX, Li`ege, Belgium.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree Kernels for Semantic Role Label-
ing. Computational Linguistics, 34(2):193 – 224.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML), Berlin, Germany.
Alessandro Moschitti. 2008. Kernel Methods, Syn-
tax and Semantics for Relational Text Categorization.
In Proceedings of the Conference on Information and
Knowledge Management (CIKM), Napa Valley, USA.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution Kernels on
Constituent, Dependency and Sequential Structures
for Relation Extraction. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Singapore.
Ellen Riloff. 1996. An Empirical Study of Automated
Dictionary Construction for Information Extraction.
Artificial Intelligence, 85.
John Taylor and Nello Christianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2003.
Annotating Expressions of Opinions and Emotions in
Language. Language Resources and Evaluation, 1:2.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP), Vancouver, Canada.
Dell Zhang and Wee Sun Lee. 2003. Question Classifi-
cation using Support Vector Machines. In Proceedings
of the ACM Special Interest Group on Information Re-
trieval (SIGIR), Toronto, Canada.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution Tree Kernel. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL (HLT/NAACL), New
York City, USA.
Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hy-
brid Learning of Dependency Structures from Het-
erogeneous Linguistic Resources. In Proceedings of
the Conference on Computational Natural Language
Learning (CoNLL), Manchester, United Kingdom.
</reference>
<page confidence="0.99915">
803
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.303103">
<title confidence="0.999461">Convolution Kernels for Opinion Holder Extraction</title>
<author confidence="0.650583">Wiegand</author>
<affiliation confidence="0.6251895">Spoken Language Saarland</affiliation>
<address confidence="0.675038">D-66123 Saarbr¨ucken,</address>
<abstract confidence="0.995839045454546">Opinion holder extraction is one of the important subtasks in sentiment analysis. The effective detection of an opinion holder depends on the consideration of various cues on various levels of representation, though they are hard to formulate explicitly as features. In this work, we propose to use convolution kernels for that task which identify meaningful fragments of sequences or trees by themselves. We not only investigate how different levels of information can be effectively combined in different kernels but also examine how the scope of these kernels should be chosen. In general relation extraction, the two candidate entities thought to be involved in a relation are commonly chosen to be the boundaries of sequences and trees. The definition of boundaries in opinion holder extraction, however, is less straightforward since there might be several expressions beside the candidate opinion holder to be eligible for being a boundary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>Hong Yu</author>
<author>Ashley Thornton</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Dan Jurafsky</author>
</authors>
<title>Extracting Opinion Propositions and Opinion Holders using Syntactic and Lexical Cues.</title>
<date>2005</date>
<booktitle>In Computing Attitude and Affect in Text: Theory and Applications.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="4826" citStr="Bethard et al. (2005)" startWordPosition="749" endWordPosition="752">ional prior knowledge. 2 Related Work Choi et al. (2005) examine opinion holder extraction using CRFs with various manually defined linguistic features and patterns automatically learnt by the AutoSlog system (Riloff, 1996). The linguistic features focus on named-entity information and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. Due to data sparseness, Kim and Hovy (2006) expand FrameNet data by using an unsupervised clustering algorithm. Choi et al. (2006) is an extension of Choi et al. (2005) in that opinion holder extraction is learnt jointly with opinion detection. This requires that opinion expressions and their relations to opinion holders are annotated in the training data. Semantic roles are also taken as a potential source of information. In our work, we deliberately wo</context>
</contexts>
<marker>Bethard, Yu, Thornton, Hatzivassiloglou, Jurafsky, 2005</marker>
<rawString>Steven Bethard, Hong Yu, Ashley Thornton, Vasileios Hatzivassiloglou, and Dan Jurafsky. 2005. Extracting Opinion Propositions and Opinion Holders using Syntactic and Lexical Cues. In Computing Attitude and Affect in Text: Theory and Applications. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Bloom</author>
<author>Sterling Stein</author>
<author>Shlomo Argamon</author>
</authors>
<title>Appraisal Extraction for News Opinion Analysis at NTCIR-6.</title>
<date>2007</date>
<booktitle>In Proceedings ofNTCIR-6 Workshop Meeting,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="4644" citStr="Bloom et al. (2007)" startWordPosition="718" endWordPosition="721">se kernels with a manually designed feature set used as a standard vector kernel. Finally, we also examine the effectiveness of expanding word sequences or syntactic trees by additional prior knowledge. 2 Related Work Choi et al. (2005) examine opinion holder extraction using CRFs with various manually defined linguistic features and patterns automatically learnt by the AutoSlog system (Riloff, 1996). The linguistic features focus on named-entity information and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. Due to data sparseness, Kim and Hovy (2006) expand FrameNet data by using an unsupervised clustering algorithm. Choi et al. (2006) is an extension of Choi et al. (2005) in that opinion holder extraction is learnt jointly with opinion detection. This requires that opinion expr</context>
<context position="8593" citStr="Bloom et al., 2007" startWordPosition="1366" endWordPosition="1369">neralized part-of-speech sequence (POSGN) a part-of-speech tag is replaced. For augmented constituent trees (CONSTAUG), the same sources of information are used. The difference to generalizing sequences is that instead of replacing words by generalized tokens, we add a node in the syntax tree with a generalized token so that it dominates the pertaining leaf node (see also nodes marked with AUG in Figure 2). All sources used for this type of generalization are known to be predictive for opinion holder classification (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006; Kim and Hovy, 2006; Bloom et al., 2007). Note that the grammatical relation paths, i.e. GRAMWRD and GRAMPOS, can only be applied in case there is another expression in the focus in addition to the candidate of the data instance itself, e.g. the nearest opinion expression to the candidate. Section 4.4 explains in detail how this is done. Predicate-argument structures (PAS) are represented by PropBank trees (Kingsbury and Palmer, 2002). 4.2 Support Vector Machines and Kernel Methods Support Vector Machines (SVMs) are one of the most robust supervised machine learning techniques in which training data instances x� are separated by a h</context>
<context position="19507" citStr="Bloom et al., 2007" startWordPosition="3169" endWordPosition="3172">t set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and p = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7available at disi.unitn.it/moschitti 799 Figure 2: Illustration of the different scopes on a CONSTAUG; nodes belonging to the candidate opinion holder are marked with LAND. up nouns in NOMLEX (Macleod et al., 1998). 5.1 Notation Each kernel is represented as a triple (levelOfRepresentation (Table 1), Scope (Table 3), typeOfXernel (Table 2)), e.g. (CONST, SENT, STK) is a Subset Tree Kernel of a constituency parse having the scope of the entire sentence. Note that not all combinations of these three parameters</context>
</contexts>
<marker>Bloom, Stein, Argamon, 2007</marker>
<rawString>Kenneth Bloom, Sterling Stein, and Shlomo Argamon. 2007. Appraisal Extraction for News Opinion Analysis at NTCIR-6. In Proceedings ofNTCIR-6 Workshop Meeting, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Subsequence Kernels for Relation Extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Neural Information Processing Systems (NIPS),</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="2188" citStr="Bunescu and Mooney, 2005" startWordPosition="332" endWordPosition="335">such as “What does [X] like about [Y]?”). Such systems need to be able to distinguish which entities in a candidate answer sentence are the sources of opinions (= opinion holder) and which are the targets. On other NLP tasks, in particular, on relation extraction, there has been much work on convolution kernels, i.e. kernel functions exploiting huge amounts of features without an explicit feature representation. Previous research on that task has shown that convolution kernels, such as sequence and tree kernels, are quite effective when compared to manual feature engineering (Moschitti, 2008; Bunescu and Mooney, 2005; Nguyen et al., 2009). In order to effectively use convolution kernels, it is often necessary to choose appropriate substructures of a sentence rather than represent the sentence as a whole structure (Bunescu and Mooney, 2005; Zhang et al., 2006; Moschitti, 2008). As for tree kernels, for example, one typically chooses the syntactic subtree immediately enclosing two entities potentially expressing a specific relation in a given sentence. The opinion holder detection task is different from this scenario. There can be several cues within a sentence to indicate the presence of a genuine opinion </context>
<context position="5854" citStr="Bunescu and Mooney, 2005" startWordPosition="912" endWordPosition="915">t opinion expressions and their relations to opinion holders are annotated in the training data. Semantic roles are also taken as a potential source of information. In our work, we deliberately work with minimal annotation and, thus, do not consider any labeled opinion expressions and relations to opinion holders in the training data. We exclusively rely on entities marked as opinion holders. In many practical situations, the annotation beyond opinion holder labeling is too expensive. Complex convolution kernels have been successfully applied to various NLP tasks, such as relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), question answering (Zhang and Lee, 2003; Moschitti, 2008), and semantic role labeling (Moschitti et al., 2008). In all these tasks, they offer competitive performance to manually designed feature sets. Bunescu and Mooney (2005) combine different sequence kernels encoding different contexts of candidate entities in a sentence. They argue that several kernels encoding different contexts are more effective than just using one kernel with one specific context. We build on that idea and compare various scopes eligible for opinion holder extraction. Moschi</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. Subsequence Kernels for Relation Extraction. In Proceedings of the Conference on Neural Information Processing Systems (NIPS), Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT/EMNLP),</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="4261" citStr="Choi et al. (2005)" startWordPosition="659" endWordPosition="662">ion of boundaries of the structures for the convolution kernels is less straightforward in opinion holder extraction. The aim of this paper is to explore in how far convolution kernels can be beneficial for effective opinion holder detection. We are not only interested in how far different kernel types contribute to this extraction task but we also contrast the performance of these kernels with a manually designed feature set used as a standard vector kernel. Finally, we also examine the effectiveness of expanding word sequences or syntactic trees by additional prior knowledge. 2 Related Work Choi et al. (2005) examine opinion holder extraction using CRFs with various manually defined linguistic features and patterns automatically learnt by the AutoSlog system (Riloff, 1996). The linguistic features focus on named-entity information and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic</context>
<context position="6993" citStr="Choi et al., 2005" startWordPosition="1096" endWordPosition="1099">idea and compare various scopes eligible for opinion holder extraction. Moschitti (2008) and Nguyen et al. (2009) suggest that different kinds of information, such as word sequences, part-of-speech tags, syntactic and semantic information should be contained in separate convolution kernels. We also adhere to this notion. 3 Data As labeled data, we use the sentiment annotation of the MPQA 2.0 corpus1. Opinion holders are not explicitly labeled as such. However sources of private states and subjective speech events (Wiebe et al., 2003) are a fairly good approximation of the task. Previous work (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006) uses similar approximations. 4 Method In this work, we consider all noun phrases (NPs) as possible candidate opinion holders. Therefore, the set of all data instances is the set of the NPs within the MPQA 2.0 corpus. Each NP is labeled as to whether it is a genuine opinion holder or not. Throughout this section, we will use Sentence 2 from Section 1 as an example. 4.1 The Different Levels of Representation Several levels of representation are important for opinion holder extraction. Table 1 lists all the different levels that are used in this work. Gene</context>
<context position="8513" citStr="Choi et al., 2005" startWordPosition="1350" endWordPosition="1353">n 796 quence (WRDGN) a word is replaced by a generalized token whereas in a generalized part-of-speech sequence (POSGN) a part-of-speech tag is replaced. For augmented constituent trees (CONSTAUG), the same sources of information are used. The difference to generalizing sequences is that instead of replacing words by generalized tokens, we add a node in the syntax tree with a generalized token so that it dominates the pertaining leaf node (see also nodes marked with AUG in Figure 2). All sources used for this type of generalization are known to be predictive for opinion holder classification (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006; Kim and Hovy, 2006; Bloom et al., 2007). Note that the grammatical relation paths, i.e. GRAMWRD and GRAMPOS, can only be applied in case there is another expression in the focus in addition to the candidate of the data instance itself, e.g. the nearest opinion expression to the candidate. Section 4.4 explains in detail how this is done. Predicate-argument structures (PAS) are represented by PropBank trees (Kingsbury and Palmer, 2002). 4.2 Support Vector Machines and Kernel Methods Support Vector Machines (SVMs) are one of the most robust supervised mach</context>
<context position="17775" citStr="Choi et al., 2005" startWordPosition="2891" endWordPosition="2894">ument representing the candidate (i.e. REL:maintain A0:Koizumi). The entire PAS is not used since it would create too sparse features. Convolution kernels can cope with fairly complex structures as input since they internally match substructures. Manual features are less flexible since they do not account for partial matches. 6We select the nearest predicate by using the syntactic parse tree. Thus, we hope to select the predicate which syntactically headword/governing category of CAND is CAND capitalized/a person? is CAND subj|dobj|iobj|pobj of OPINION/COMM? is CAND preceded by according to? (Choi et al., 2005) does CAND contain possessive and is followed by OPINION/COMM? (Choi et al., 2005) is CAND preceded by by which is attached to OPINION/COMM? (Choi et al., 2005) predicate-argument pairs in which CAND occurs lemma/part-of-speech tag/subcategorization frame/voice of nearest predicate is nearest predicate OPINION/COMM? does CAND precede/follow nearest predicate? words between nearest predicate and CAND (bag of words) part-of-speech sequence between nearest predicate and CAND constituency path/grammatical relation path from predicate to CAND Table 4: Manually designed feature set. 5 Experiments We</context>
</contexts>
<marker>Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT/EMNLP), Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Eric Breck</author>
<author>Claire Cardie</author>
</authors>
<title>Joint Extraction of Entities and Relations for Opinion Recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="5098" citStr="Choi et al. (2006)" startWordPosition="792" endWordPosition="795">rmation and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. Due to data sparseness, Kim and Hovy (2006) expand FrameNet data by using an unsupervised clustering algorithm. Choi et al. (2006) is an extension of Choi et al. (2005) in that opinion holder extraction is learnt jointly with opinion detection. This requires that opinion expressions and their relations to opinion holders are annotated in the training data. Semantic roles are also taken as a potential source of information. In our work, we deliberately work with minimal annotation and, thus, do not consider any labeled opinion expressions and relations to opinion holders in the training data. We exclusively rely on entities marked as opinion holders. In many practical situations, the annotation beyond opinion holder label</context>
<context position="7033" citStr="Choi et al., 2006" startWordPosition="1104" endWordPosition="1107">e for opinion holder extraction. Moschitti (2008) and Nguyen et al. (2009) suggest that different kinds of information, such as word sequences, part-of-speech tags, syntactic and semantic information should be contained in separate convolution kernels. We also adhere to this notion. 3 Data As labeled data, we use the sentiment annotation of the MPQA 2.0 corpus1. Opinion holders are not explicitly labeled as such. However sources of private states and subjective speech events (Wiebe et al., 2003) are a fairly good approximation of the task. Previous work (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006) uses similar approximations. 4 Method In this work, we consider all noun phrases (NPs) as possible candidate opinion holders. Therefore, the set of all data instances is the set of the NPs within the MPQA 2.0 corpus. Each NP is labeled as to whether it is a genuine opinion holder or not. Throughout this section, we will use Sentence 2 from Section 1 as an example. 4.1 The Different Levels of Representation Several levels of representation are important for opinion holder extraction. Table 1 lists all the different levels that are used in this work. Generalized sequences employ named-entity ta</context>
<context position="8552" citStr="Choi et al., 2006" startWordPosition="1358" endWordPosition="1361"> by a generalized token whereas in a generalized part-of-speech sequence (POSGN) a part-of-speech tag is replaced. For augmented constituent trees (CONSTAUG), the same sources of information are used. The difference to generalizing sequences is that instead of replacing words by generalized tokens, we add a node in the syntax tree with a generalized token so that it dominates the pertaining leaf node (see also nodes marked with AUG in Figure 2). All sources used for this type of generalization are known to be predictive for opinion holder classification (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006; Kim and Hovy, 2006; Bloom et al., 2007). Note that the grammatical relation paths, i.e. GRAMWRD and GRAMPOS, can only be applied in case there is another expression in the focus in addition to the candidate of the data instance itself, e.g. the nearest opinion expression to the candidate. Section 4.4 explains in detail how this is done. Predicate-argument structures (PAS) are represented by PropBank trees (Kingsbury and Palmer, 2002). 4.2 Support Vector Machines and Kernel Methods Support Vector Machines (SVMs) are one of the most robust supervised machine learning techniques in which traini</context>
</contexts>
<marker>Choi, Breck, Cardie, 2006</marker>
<rawString>Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint Extraction of Entities and Relations for Opinion Recognition. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="10633" citStr="Collins and Duffy, 2002" startWordPosition="1730" endWordPosition="1733"> use the kernel by Taylor and Christianini (2004) which has the advantage that it also considers subsequences of the original sequence with some elements missing. The extent of these gaps in a sequence is suitably reflected by a weighting function incorporated into the kernel. Tree kernels (TKs) represent trees by their substructures. The feature space of these substructures, or fragments, is mapped onto a vector space. The kernel function computes the similarity of pairs of trees by counting the number of common fragments. In this work, we evaluate two tree kernels: Subset Tree Kernel (STK) (Collins and Duffy, 2002) and Partial Tree Kernel (PTKbasic) (Moschitti, 2006). In STK, a tree fragment can be any set of nodes and edges of the original tree provided that every node has either all or none of its children. This constraint makes that kind of kernel well-suited for constituency trees which have been generated by context free grammars since the constraint corresponds to the restriction that no grammatical rule must be broken. For example, STK enforces that a subtree, such as [VP [VBZ, NP]], cannot be matched with [VP [VBZ]] since the latter VP node only possesses one of the children of the former. PTKba</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New Ranking Algorithms for Parsing and Tagging. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fillmore</author>
<author>Christopher R Johnson</author>
<author>Miriam R Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal ofLexicography, 16:235 –</journal>
<pages>250</pages>
<contexts>
<context position="4912" citStr="Fillmore et al., 2003" startWordPosition="763" endWordPosition="766">ction using CRFs with various manually defined linguistic features and patterns automatically learnt by the AutoSlog system (Riloff, 1996). The linguistic features focus on named-entity information and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. Due to data sparseness, Kim and Hovy (2006) expand FrameNet data by using an unsupervised clustering algorithm. Choi et al. (2006) is an extension of Choi et al. (2005) in that opinion holder extraction is learnt jointly with opinion detection. This requires that opinion expressions and their relations to opinion holders are annotated in the training data. Semantic roles are also taken as a potential source of information. In our work, we deliberately work with minimal annotation and, thus, do not consider any labeled opinion expressions </context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>Charles. J. Fillmore, Christopher R. Johnson, and Miriam R. Petruck. 2003. Background to FrameNet. International Journal ofLexicography, 16:235 – 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL),</booktitle>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="19235" citStr="Finkel et al., 2005" startWordPosition="3123" endWordPosition="3126">with the SVM-Light-TK toolkit7. We evaluated on the basis of exact phrase matching. We set the trade-off parameter j = 5 for all feature sets. For the manual feature set we used a polynomial kernel of third degree. These two critical parameters were tuned on the development set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and p = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7available at disi.unitn.it/moschitti 799 Figure 2: Illustration of the different scopes on a CONSTAUG; nodes belonging to the candidate opinion holder are marked with LAND. up nouns in NOMLEX (Macleod et al., 1998). 5.1 Notation Each kernel </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL), Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Identifying Opinion Holders for Question Answering in Opinion Texts.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI-05 Workshop on Question Answering in Restricted Domains,</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="4620" citStr="Kim and Hovy (2005)" startWordPosition="713" endWordPosition="716">t the performance of these kernels with a manually designed feature set used as a standard vector kernel. Finally, we also examine the effectiveness of expanding word sequences or syntactic trees by additional prior knowledge. 2 Related Work Choi et al. (2005) examine opinion holder extraction using CRFs with various manually defined linguistic features and patterns automatically learnt by the AutoSlog system (Riloff, 1996). The linguistic features focus on named-entity information and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. Due to data sparseness, Kim and Hovy (2006) expand FrameNet data by using an unsupervised clustering algorithm. Choi et al. (2006) is an extension of Choi et al. (2005) in that opinion holder extraction is learnt jointly with opinion detection. This re</context>
<context position="7013" citStr="Kim and Hovy, 2005" startWordPosition="1100" endWordPosition="1103">rious scopes eligible for opinion holder extraction. Moschitti (2008) and Nguyen et al. (2009) suggest that different kinds of information, such as word sequences, part-of-speech tags, syntactic and semantic information should be contained in separate convolution kernels. We also adhere to this notion. 3 Data As labeled data, we use the sentiment annotation of the MPQA 2.0 corpus1. Opinion holders are not explicitly labeled as such. However sources of private states and subjective speech events (Wiebe et al., 2003) are a fairly good approximation of the task. Previous work (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006) uses similar approximations. 4 Method In this work, we consider all noun phrases (NPs) as possible candidate opinion holders. Therefore, the set of all data instances is the set of the NPs within the MPQA 2.0 corpus. Each NP is labeled as to whether it is a genuine opinion holder or not. Throughout this section, we will use Sentence 2 from Section 1 as an example. 4.1 The Different Levels of Representation Several levels of representation are important for opinion holder extraction. Table 1 lists all the different levels that are used in this work. Generalized sequences em</context>
<context position="8533" citStr="Kim and Hovy, 2005" startWordPosition="1354" endWordPosition="1357">) a word is replaced by a generalized token whereas in a generalized part-of-speech sequence (POSGN) a part-of-speech tag is replaced. For augmented constituent trees (CONSTAUG), the same sources of information are used. The difference to generalizing sequences is that instead of replacing words by generalized tokens, we add a node in the syntax tree with a generalized token so that it dominates the pertaining leaf node (see also nodes marked with AUG in Figure 2). All sources used for this type of generalization are known to be predictive for opinion holder classification (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006; Kim and Hovy, 2006; Bloom et al., 2007). Note that the grammatical relation paths, i.e. GRAMWRD and GRAMPOS, can only be applied in case there is another expression in the focus in addition to the candidate of the data instance itself, e.g. the nearest opinion expression to the candidate. Section 4.4 explains in detail how this is done. Predicate-argument structures (PAS) are represented by PropBank trees (Kingsbury and Palmer, 2002). 4.2 Support Vector Machines and Kernel Methods Support Vector Machines (SVMs) are one of the most robust supervised machine learning techniq</context>
</contexts>
<marker>Kim, Hovy, 2005</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2005. Identifying Opinion Holders for Question Answering in Opinion Texts. In Proceedings of AAAI-05 Workshop on Question Answering in Restricted Domains, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL Workshop on Sentiment and Subjectivity in Text,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="4800" citStr="Kim and Hovy (2006)" startWordPosition="744" endWordPosition="747">syntactic trees by additional prior knowledge. 2 Related Work Choi et al. (2005) examine opinion holder extraction using CRFs with various manually defined linguistic features and patterns automatically learnt by the AutoSlog system (Riloff, 1996). The linguistic features focus on named-entity information and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. Due to data sparseness, Kim and Hovy (2006) expand FrameNet data by using an unsupervised clustering algorithm. Choi et al. (2006) is an extension of Choi et al. (2005) in that opinion holder extraction is learnt jointly with opinion detection. This requires that opinion expressions and their relations to opinion holders are annotated in the training data. Semantic roles are also taken as a potential source of information. In ou</context>
<context position="8572" citStr="Kim and Hovy, 2006" startWordPosition="1362" endWordPosition="1365">oken whereas in a generalized part-of-speech sequence (POSGN) a part-of-speech tag is replaced. For augmented constituent trees (CONSTAUG), the same sources of information are used. The difference to generalizing sequences is that instead of replacing words by generalized tokens, we add a node in the syntax tree with a generalized token so that it dominates the pertaining leaf node (see also nodes marked with AUG in Figure 2). All sources used for this type of generalization are known to be predictive for opinion holder classification (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006; Kim and Hovy, 2006; Bloom et al., 2007). Note that the grammatical relation paths, i.e. GRAMWRD and GRAMPOS, can only be applied in case there is another expression in the focus in addition to the candidate of the data instance itself, e.g. the nearest opinion expression to the candidate. Section 4.4 explains in detail how this is done. Predicate-argument structures (PAS) are represented by PropBank trees (Kingsbury and Palmer, 2002). 4.2 Support Vector Machines and Kernel Methods Support Vector Machines (SVMs) are one of the most robust supervised machine learning techniques in which training data instances x�</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text. In Proceedings of the ACL Workshop on Sentiment and Subjectivity in Text, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From TreeBank to PropBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="8991" citStr="Kingsbury and Palmer, 2002" startWordPosition="1429" endWordPosition="1432">th AUG in Figure 2). All sources used for this type of generalization are known to be predictive for opinion holder classification (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006; Kim and Hovy, 2006; Bloom et al., 2007). Note that the grammatical relation paths, i.e. GRAMWRD and GRAMPOS, can only be applied in case there is another expression in the focus in addition to the candidate of the data instance itself, e.g. the nearest opinion expression to the candidate. Section 4.4 explains in detail how this is done. Predicate-argument structures (PAS) are represented by PropBank trees (Kingsbury and Palmer, 2002). 4.2 Support Vector Machines and Kernel Methods Support Vector Machines (SVMs) are one of the most robust supervised machine learning techniques in which training data instances x� are separated by a hyperplane H(x) = w� · x� + b = 0 where w E R&apos; and b E R. One advantage of SVMs is that kernel methods can be applied which map the data to other feature spaces in which they can be separated more easily. Given a feature function 0 : ® —* R, where ® is the set of the objects, the kernel trick allows the decision hyperplane to be rewritten as: �xi · x�+ b = yiαi0 (oi) · 0 (o) + b i=1...l where yi </context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2002. From TreeBank to PropBank. In Proceedings of the 3rd Conference on Language Resources and Evaluation (LREC), Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics (ACL),</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="19152" citStr="Klein and Manning, 2003" startWordPosition="3109" endWordPosition="3112">sis of a paired t-test using 0.05 as the significance level. All experiments were done with the SVM-Light-TK toolkit7. We evaluated on the basis of exact phrase matching. We set the trade-off parameter j = 5 for all feature sets. For the manual feature set we used a polynomial kernel of third degree. These two critical parameters were tuned on the development set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and p = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7available at disi.unitn.it/moschitti 799 Figure 2: Illustration of the different scopes on a CONSTAUG; nodes belonging to the candidate opinion holder are mar</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics (ACL), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Macleod</author>
<author>Ralph Grishman</author>
<author>Adam Meyers</author>
<author>Leslie Barrett</author>
<author>Ruth Reeves</author>
</authors>
<title>NOMLEX: A Lexicon of Nominalizations.</title>
<date>1998</date>
<booktitle>In Proceedings ofEURALEX, Li`ege,</booktitle>
<location>Belgium.</location>
<contexts>
<context position="19808" citStr="Macleod et al., 1998" startWordPosition="3213" endWordPosition="3216">ned by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7available at disi.unitn.it/moschitti 799 Figure 2: Illustration of the different scopes on a CONSTAUG; nodes belonging to the candidate opinion holder are marked with LAND. up nouns in NOMLEX (Macleod et al., 1998). 5.1 Notation Each kernel is represented as a triple (levelOfRepresentation (Table 1), Scope (Table 3), typeOfXernel (Table 2)), e.g. (CONST, SENT, STK) is a Subset Tree Kernel of a constituency parse having the scope of the entire sentence. Note that not all combinations of these three parameters are meaningful. In the following, we will just focus on important and effective combinations. The kernel composed of manually designed features is denoted by just V K. The kernel composed of predicate-argument structures is denoted by (PAS, SENT, PTK). 5.2 Vector Kernel (VK) The first line in Table </context>
</contexts>
<marker>Macleod, Grishman, Meyers, Barrett, Reeves, 1998</marker>
<rawString>Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie Barrett, and Ruth Reeves. 1998. NOMLEX: A Lexicon of Nominalizations. In Proceedings ofEURALEX, Li`ege, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Tree Kernels for Semantic Role Labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<pages>224</pages>
<contexts>
<context position="6008" citStr="Moschitti et al., 2008" startWordPosition="937" endWordPosition="940">ormation. In our work, we deliberately work with minimal annotation and, thus, do not consider any labeled opinion expressions and relations to opinion holders in the training data. We exclusively rely on entities marked as opinion holders. In many practical situations, the annotation beyond opinion holder labeling is too expensive. Complex convolution kernels have been successfully applied to various NLP tasks, such as relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), question answering (Zhang and Lee, 2003; Moschitti, 2008), and semantic role labeling (Moschitti et al., 2008). In all these tasks, they offer competitive performance to manually designed feature sets. Bunescu and Mooney (2005) combine different sequence kernels encoding different contexts of candidate entities in a sentence. They argue that several kernels encoding different contexts are more effective than just using one kernel with one specific context. We build on that idea and compare various scopes eligible for opinion holder extraction. Moschitti (2008) and Nguyen et al. (2009) suggest that different kinds of information, such as word sequences, part-of-speech tags, syntactic and semantic infor</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2008</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2008. Tree Kernels for Semantic Role Labeling. Computational Linguistics, 34(2):193 – 224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees.</title>
<date>2006</date>
<booktitle>In Proceedings of the 17th European Conference on Machine Learning (ECML),</booktitle>
<location>Berlin, Germany.</location>
<contexts>
<context position="10686" citStr="Moschitti, 2006" startWordPosition="1739" endWordPosition="1740">he advantage that it also considers subsequences of the original sequence with some elements missing. The extent of these gaps in a sequence is suitably reflected by a weighting function incorporated into the kernel. Tree kernels (TKs) represent trees by their substructures. The feature space of these substructures, or fragments, is mapped onto a vector space. The kernel function computes the similarity of pairs of trees by counting the number of common fragments. In this work, we evaluate two tree kernels: Subset Tree Kernel (STK) (Collins and Duffy, 2002) and Partial Tree Kernel (PTKbasic) (Moschitti, 2006). In STK, a tree fragment can be any set of nodes and edges of the original tree provided that every node has either all or none of its children. This constraint makes that kind of kernel well-suited for constituency trees which have been generated by context free grammars since the constraint corresponds to the restriction that no grammatical rule must be broken. For example, STK enforces that a subtree, such as [VP [VBZ, NP]], cannot be matched with [VP [VBZ]] since the latter VP node only possesses one of the children of the former. PTKbasic is more flexible since the constraint of STK on n</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Proceedings of the 17th European Conference on Machine Learning (ECML), Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel Methods, Syntax and Semantics for Relational Text Categorization.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Information and Knowledge Management (CIKM),</booktitle>
<location>Napa Valley, USA.</location>
<contexts>
<context position="2162" citStr="Moschitti, 2008" startWordPosition="330" endWordPosition="331">inion questions, such as “What does [X] like about [Y]?”). Such systems need to be able to distinguish which entities in a candidate answer sentence are the sources of opinions (= opinion holder) and which are the targets. On other NLP tasks, in particular, on relation extraction, there has been much work on convolution kernels, i.e. kernel functions exploiting huge amounts of features without an explicit feature representation. Previous research on that task has shown that convolution kernels, such as sequence and tree kernels, are quite effective when compared to manual feature engineering (Moschitti, 2008; Bunescu and Mooney, 2005; Nguyen et al., 2009). In order to effectively use convolution kernels, it is often necessary to choose appropriate substructures of a sentence rather than represent the sentence as a whole structure (Bunescu and Mooney, 2005; Zhang et al., 2006; Moschitti, 2008). As for tree kernels, for example, one typically chooses the syntactic subtree immediately enclosing two entities potentially expressing a specific relation in a given sentence. The opinion holder detection task is different from this scenario. There can be several cues within a sentence to indicate the pres</context>
<context position="5955" citStr="Moschitti, 2008" startWordPosition="931" endWordPosition="932">es are also taken as a potential source of information. In our work, we deliberately work with minimal annotation and, thus, do not consider any labeled opinion expressions and relations to opinion holders in the training data. We exclusively rely on entities marked as opinion holders. In many practical situations, the annotation beyond opinion holder labeling is too expensive. Complex convolution kernels have been successfully applied to various NLP tasks, such as relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), question answering (Zhang and Lee, 2003; Moschitti, 2008), and semantic role labeling (Moschitti et al., 2008). In all these tasks, they offer competitive performance to manually designed feature sets. Bunescu and Mooney (2005) combine different sequence kernels encoding different contexts of candidate entities in a sentence. They argue that several kernels encoding different contexts are more effective than just using one kernel with one specific context. We build on that idea and compare various scopes eligible for opinion holder extraction. Moschitti (2008) and Nguyen et al. (2009) suggest that different kinds of information, such as word sequenc</context>
<context position="13056" citStr="Moschitti, 2008" startWordPosition="2124" endWordPosition="2125">eneralized part-of-speech sequence IN DET NN PUNC CAND COMM OPINION ... CONST constituency tree see Figure 2 without nodes marked AUG CONSTAUG augmented constituency tree see Figure 2 GRAMWRD grammatical relation path labels with words KoizumiCAND NSUBJT maintained DOBJJ stance GRAMPOS grammatical relation path labels with part-of-speech tags CAND NSUBJT VBD DOBJJ NN PAS predicate argument structures see Figure 1(a) PASAUG augmented predicate argument structures see Figure 1(b) Table 1: The different levels of representation. (b) augmented Figure 1: Predicate-argument structures (PAS). tasks (Moschitti, 2008; Nguyen et al., 2009). Type Description Levels of Representation SK Sequential Kernel WRD(GN), POS(GN), STK Subset Tree Kernel GRAMWRD, GRAMPOS PTK Partial Tree Kernel CONST(AUG) V K Vector Kernel PAS not restricted Table 2: The different types of kernels. 4.4 The Different Scopes We argue that using the entire word sequence or syntax tree of the sentence in which a candidate opinion holder is situated to represent a data instance produces too large structures for a convolution kernel. Since a classifier based on convolution kernels has to derive meaningful features by itself, the larger thes</context>
<context position="19002" citStr="Moschitti (2008)" startWordPosition="3083" endWordPosition="3084">ocuments of the MPQA corpus for five-fold crossvalidation and 133 documents as a development set. We report statistical significance on the basis of a paired t-test using 0.05 as the significance level. All experiments were done with the SVM-Light-TK toolkit7. We evaluated on the basis of exact phrase matching. We set the trade-off parameter j = 5 for all feature sets. For the manual feature set we used a polynomial kernel of third degree. These two critical parameters were tuned on the development set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and p = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7availabl</context>
</contexts>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti. 2008. Kernel Methods, Syntax and Semantics for Relational Text Categorization. In Proceedings of the Conference on Information and Knowledge Management (CIKM), Napa Valley, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="2210" citStr="Nguyen et al., 2009" startWordPosition="336" endWordPosition="339">e about [Y]?”). Such systems need to be able to distinguish which entities in a candidate answer sentence are the sources of opinions (= opinion holder) and which are the targets. On other NLP tasks, in particular, on relation extraction, there has been much work on convolution kernels, i.e. kernel functions exploiting huge amounts of features without an explicit feature representation. Previous research on that task has shown that convolution kernels, such as sequence and tree kernels, are quite effective when compared to manual feature engineering (Moschitti, 2008; Bunescu and Mooney, 2005; Nguyen et al., 2009). In order to effectively use convolution kernels, it is often necessary to choose appropriate substructures of a sentence rather than represent the sentence as a whole structure (Bunescu and Mooney, 2005; Zhang et al., 2006; Moschitti, 2008). As for tree kernels, for example, one typically chooses the syntactic subtree immediately enclosing two entities potentially expressing a specific relation in a given sentence. The opinion holder detection task is different from this scenario. There can be several cues within a sentence to indicate the presence of a genuine opinion holder and these cues </context>
<context position="5896" citStr="Nguyen et al., 2009" startWordPosition="920" endWordPosition="923">pinion holders are annotated in the training data. Semantic roles are also taken as a potential source of information. In our work, we deliberately work with minimal annotation and, thus, do not consider any labeled opinion expressions and relations to opinion holders in the training data. We exclusively rely on entities marked as opinion holders. In many practical situations, the annotation beyond opinion holder labeling is too expensive. Complex convolution kernels have been successfully applied to various NLP tasks, such as relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), question answering (Zhang and Lee, 2003; Moschitti, 2008), and semantic role labeling (Moschitti et al., 2008). In all these tasks, they offer competitive performance to manually designed feature sets. Bunescu and Mooney (2005) combine different sequence kernels encoding different contexts of candidate entities in a sentence. They argue that several kernels encoding different contexts are more effective than just using one kernel with one specific context. We build on that idea and compare various scopes eligible for opinion holder extraction. Moschitti (2008) and Nguyen et al. (2009) sugges</context>
<context position="13078" citStr="Nguyen et al., 2009" startWordPosition="2126" endWordPosition="2129">f-speech sequence IN DET NN PUNC CAND COMM OPINION ... CONST constituency tree see Figure 2 without nodes marked AUG CONSTAUG augmented constituency tree see Figure 2 GRAMWRD grammatical relation path labels with words KoizumiCAND NSUBJT maintained DOBJJ stance GRAMPOS grammatical relation path labels with part-of-speech tags CAND NSUBJT VBD DOBJJ NN PAS predicate argument structures see Figure 1(a) PASAUG augmented predicate argument structures see Figure 1(b) Table 1: The different levels of representation. (b) augmented Figure 1: Predicate-argument structures (PAS). tasks (Moschitti, 2008; Nguyen et al., 2009). Type Description Levels of Representation SK Sequential Kernel WRD(GN), POS(GN), STK Subset Tree Kernel GRAMWRD, GRAMPOS PTK Partial Tree Kernel CONST(AUG) V K Vector Kernel PAS not restricted Table 2: The different types of kernels. 4.4 The Different Scopes We argue that using the entire word sequence or syntax tree of the sentence in which a candidate opinion holder is situated to represent a data instance produces too large structures for a convolution kernel. Since a classifier based on convolution kernels has to derive meaningful features by itself, the larger these structures are, the </context>
</contexts>
<marker>Nguyen, Moschitti, Riccardi, 2009</marker>
<rawString>Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2009. Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>An Empirical Study of Automated Dictionary Construction for Information Extraction.</title>
<date>1996</date>
<journal>Artificial Intelligence,</journal>
<volume>85</volume>
<contexts>
<context position="4428" citStr="Riloff, 1996" startWordPosition="684" endWordPosition="685">tion kernels can be beneficial for effective opinion holder detection. We are not only interested in how far different kernel types contribute to this extraction task but we also contrast the performance of these kernels with a manually designed feature set used as a standard vector kernel. Finally, we also examine the effectiveness of expanding word sequences or syntactic trees by additional prior knowledge. 2 Related Work Choi et al. (2005) examine opinion holder extraction using CRFs with various manually defined linguistic features and patterns automatically learnt by the AutoSlog system (Riloff, 1996). The linguistic features focus on named-entity information and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. Due to data sparseness, Kim and Hovy (2006) expand FrameNet </context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>Ellen Riloff. 1996. An Empirical Study of Automated Dictionary Construction for Information Extraction. Artificial Intelligence, 85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Taylor</author>
<author>Nello Christianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10058" citStr="Taylor and Christianini (2004)" startWordPosition="1637" endWordPosition="1640">—* R, where ® is the set of the objects, the kernel trick allows the decision hyperplane to be rewritten as: �xi · x�+ b = yiαi0 (oi) · 0 (o) + b i=1...l where yi is equal to 1 for positive and −1 for negative examples, αi E R with αi &gt; 0, oibi E 11, ... , l} are the training instances and the product K(oi, o) = (0(oi) · 0(o)) is the kernel function associated with the mapping 0. case there are several tokens making up the candidate. 4.3 Sequence and Tree Kernels A sequence kernel (SK) measures the similarity of two sequences by counting the number of common subsequences. We use the kernel by Taylor and Christianini (2004) which has the advantage that it also considers subsequences of the original sequence with some elements missing. The extent of these gaps in a sequence is suitably reflected by a weighting function incorporated into the kernel. Tree kernels (TKs) represent trees by their substructures. The feature space of these substructures, or fragments, is mapped onto a vector space. The kernel function computes the similarity of pairs of trees by counting the number of common fragments. In this work, we evaluate two tree kernels: Subset Tree Kernel (STK) (Collins and Duffy, 2002) and Partial Tree Kernel </context>
</contexts>
<marker>Taylor, Christianini, 2004</marker>
<rawString>John Taylor and Nello Christianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<date>2003</date>
<booktitle>Annotating Expressions of Opinions and Emotions in Language. Language Resources and Evaluation,</booktitle>
<volume>1</volume>
<contexts>
<context position="6915" citStr="Wiebe et al., 2003" startWordPosition="1082" endWordPosition="1085">ffective than just using one kernel with one specific context. We build on that idea and compare various scopes eligible for opinion holder extraction. Moschitti (2008) and Nguyen et al. (2009) suggest that different kinds of information, such as word sequences, part-of-speech tags, syntactic and semantic information should be contained in separate convolution kernels. We also adhere to this notion. 3 Data As labeled data, we use the sentiment annotation of the MPQA 2.0 corpus1. Opinion holders are not explicitly labeled as such. However sources of private states and subjective speech events (Wiebe et al., 2003) are a fairly good approximation of the task. Previous work (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006) uses similar approximations. 4 Method In this work, we consider all noun phrases (NPs) as possible candidate opinion holders. Therefore, the set of all data instances is the set of the NPs within the MPQA 2.0 corpus. Each NP is labeled as to whether it is a genuine opinion holder or not. Throughout this section, we will use Sentence 2 from Section 1 as an example. 4.1 The Different Levels of Representation Several levels of representation are important for opinion holder extra</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2003</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2003. Annotating Expressions of Opinions and Emotions in Language. Language Resources and Evaluation, 1:2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in Phraselevel Sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP),</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="19420" citStr="Wilson et al., 2005" startWordPosition="3154" endWordPosition="3158">omial kernel of third degree. These two critical parameters were tuned on the development set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and p = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7available at disi.unitn.it/moschitti 799 Figure 2: Illustration of the different scopes on a CONSTAUG; nodes belonging to the candidate opinion holder are marked with LAND. up nouns in NOMLEX (Macleod et al., 1998). 5.1 Notation Each kernel is represented as a triple (levelOfRepresentation (Table 1), Scope (Table 3), typeOfXernel (Table 2)), e.g. (CONST, SENT, STK) is a Subset Tree Kernel of a constituency parse having the</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing Contextual Polarity in Phraselevel Sentiment Analysis. In Proceedings of Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>Wee Sun Lee</author>
</authors>
<title>Question Classification using Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACM Special Interest Group on Information Retrieval (SIGIR),</booktitle>
<location>Toronto, Canada.</location>
<contexts>
<context position="5937" citStr="Zhang and Lee, 2003" startWordPosition="927" endWordPosition="930">ng data. Semantic roles are also taken as a potential source of information. In our work, we deliberately work with minimal annotation and, thus, do not consider any labeled opinion expressions and relations to opinion holders in the training data. We exclusively rely on entities marked as opinion holders. In many practical situations, the annotation beyond opinion holder labeling is too expensive. Complex convolution kernels have been successfully applied to various NLP tasks, such as relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), question answering (Zhang and Lee, 2003; Moschitti, 2008), and semantic role labeling (Moschitti et al., 2008). In all these tasks, they offer competitive performance to manually designed feature sets. Bunescu and Mooney (2005) combine different sequence kernels encoding different contexts of candidate entities in a sentence. They argue that several kernels encoding different contexts are more effective than just using one kernel with one specific context. We build on that idea and compare various scopes eligible for opinion holder extraction. Moschitti (2008) and Nguyen et al. (2009) suggest that different kinds of information, su</context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>Dell Zhang and Wee Sun Lee. 2003. Question Classification using Support Vector Machines. In Proceedings of the ACM Special Interest Group on Information Retrieval (SIGIR), Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
</authors>
<title>Exploring Syntactic Features for Relation Extraction using a Convolution Tree Kernel.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL (HLT/NAACL),</booktitle>
<location>New York City, USA.</location>
<contexts>
<context position="2434" citStr="Zhang et al., 2006" startWordPosition="373" endWordPosition="376"> extraction, there has been much work on convolution kernels, i.e. kernel functions exploiting huge amounts of features without an explicit feature representation. Previous research on that task has shown that convolution kernels, such as sequence and tree kernels, are quite effective when compared to manual feature engineering (Moschitti, 2008; Bunescu and Mooney, 2005; Nguyen et al., 2009). In order to effectively use convolution kernels, it is often necessary to choose appropriate substructures of a sentence rather than represent the sentence as a whole structure (Bunescu and Mooney, 2005; Zhang et al., 2006; Moschitti, 2008). As for tree kernels, for example, one typically chooses the syntactic subtree immediately enclosing two entities potentially expressing a specific relation in a given sentence. The opinion holder detection task is different from this scenario. There can be several cues within a sentence to indicate the presence of a genuine opinion holder and these cues need not be member of a particular word group, e.g. they can be opinion words (see Sentences 1-3), communication words, such as maintained in Sentence 2, or other lexical cues, such as according in Sentence 3. 1. The U.S. co</context>
<context position="5874" citStr="Zhang et al., 2006" startWordPosition="916" endWordPosition="919">their relations to opinion holders are annotated in the training data. Semantic roles are also taken as a potential source of information. In our work, we deliberately work with minimal annotation and, thus, do not consider any labeled opinion expressions and relations to opinion holders in the training data. We exclusively rely on entities marked as opinion holders. In many practical situations, the annotation beyond opinion holder labeling is too expensive. Complex convolution kernels have been successfully applied to various NLP tasks, such as relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), question answering (Zhang and Lee, 2003; Moschitti, 2008), and semantic role labeling (Moschitti et al., 2008). In all these tasks, they offer competitive performance to manually designed feature sets. Bunescu and Mooney (2005) combine different sequence kernels encoding different contexts of candidate entities in a sentence. They argue that several kernels encoding different contexts are more effective than just using one kernel with one specific context. We build on that idea and compare various scopes eligible for opinion holder extraction. Moschitti (2008) and Nguye</context>
<context position="13937" citStr="Zhang et al., 2006" startWordPosition="2266" endWordPosition="2269">The Different Scopes We argue that using the entire word sequence or syntax tree of the sentence in which a candidate opinion holder is situated to represent a data instance produces too large structures for a convolution kernel. Since a classifier based on convolution kernels has to derive meaningful features by itself, the larger these structures are, the more likely noise is included in the model. Previous work in relation extraction has also shown that the usage of more focused substructures, e.g. the smallest subtree containing the two candidate entities of a relation, is more effective (Zhang et al., 2006). Unfortunately, in our task there is only one explicit entity we know of for each data instance which is the candidate opinion holder. However, there are several indicative cues within the context of the candidate which might be considered important. We identify three different cues being the nearest predicate, i.e. full verb or nominalization, opinion word and communication word4. For each of these expressions, we define a scope where the boundaries are the candidate opinion holder and the pertaining cue. Given these scopes, we can define resulting subsequences/subtrees and combine them. We </context>
</contexts>
<marker>Zhang, Zhang, Su, 2006</marker>
<rawString>Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring Syntactic Features for Relation Extraction using a Convolution Tree Kernel. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL (HLT/NAACL), New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Rui Wang</author>
<author>Hans Uszkoreit</author>
</authors>
<title>Hybrid Learning of Dependency Structures from Heterogeneous Linguistic Resources.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="19308" citStr="Zhang et al. (2008)" startWordPosition="3137" endWordPosition="3140">matching. We set the trade-off parameter j = 5 for all feature sets. For the manual feature set we used a polynomial kernel of third degree. These two critical parameters were tuned on the development set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and p = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7available at disi.unitn.it/moschitti 799 Figure 2: Illustration of the different scopes on a CONSTAUG; nodes belonging to the candidate opinion holder are marked with LAND. up nouns in NOMLEX (Macleod et al., 1998). 5.1 Notation Each kernel is represented as a triple (levelOfRepresentation (Table 1), Scope (Table</context>
</contexts>
<marker>Zhang, Wang, Uszkoreit, 2008</marker>
<rawString>Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hybrid Learning of Dependency Structures from Heterogeneous Linguistic Resources. In Proceedings of the Conference on Computational Natural Language Learning (CoNLL), Manchester, United Kingdom.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>