<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006985">
<title confidence="0.997548">
The Best Lexical Metric for
Phrase-Based Statistical MT System Optimization
</title>
<author confidence="0.998297">
Daniel Cer, Christopher D. Manning and Daniel Jurafsky
</author>
<affiliation confidence="0.8124775">
Stanford University
Stanford, CA 94305, USA
</affiliation>
<sectionHeader confidence="0.982113" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999927115384615">
Translation systems are generally trained to
optimize BLEU, but many alternative metrics
are available. We explore how optimizing
toward various automatic evaluation metrics
(BLEU, METEOR, NIST, TER) affects the re-
sulting model. We train a state-of-the-art MT
system using MERT on many parameteriza-
tions of each metric and evaluate the result-
ing models on the other metrics and also us-
ing human judges. In accordance with popular
wisdom, we find that it’s important to train on
the same metric used in testing. However, we
also find that training to a newer metric is only
useful to the extent that the MT model’s struc-
ture and features allow it to take advantage of
the metric. Contrasting with TER’s good cor-
relation with human judgments, we show that
people tend to prefer BLEU and NIST trained
models to those trained on edit distance based
metrics like TER or WER. Human prefer-
ences for METEOR trained models varies de-
pending on the source language. Since using
BLEU or NIST produces models that are more
robust to evaluation by other metrics and per-
form well in human judgments, we conclude
they are still the best choice for training.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999909358974359">
Since their introduction, automated measures of ma-
chine translation quality have played a critical role
in the development and evolution of SMT systems.
While such metrics were initially intended for eval-
uation, popular training methods such as minimum
error rate training (MERT) (Och, 2003) and mar-
gin infused relaxed algorithm (MIRA) (Crammer
and Singer, 2003; Watanabe et al., 2007; Chiang et
al., 2008) train translation models toward a specific
evaluation metric. This makes the quality of the re-
sulting model dependent on how accurately the au-
tomatic metric actually reflects human preferences.
The most popular metric for both comparing sys-
tems and tuning MT models has been BLEU. While
BLEU (Papineni et al., 2002) is relatively simple,
scoring translations according to their n-gram over-
lap with reference translations, it still achieves a rea-
sonable correlation with human judgments of trans-
lation quality. It is also robust enough to use for au-
tomatic optimization. However, BLEU does have a
number of shortcomings. It doesn’t penalize n-gram
scrambling (Callison-Burch et al., 2006), and since
it isn’t aware of synonymous words or phrases, it can
inappropriately penalize translations that use them.
Recently, there have been efforts to develop bet-
ter evaluation metrics. Metrics such as Translation
Edit Rate (TER) (Snover et al., 2006; Snover et al.,
2009) and METEOR&apos; (Lavie and Denkowski, 2009)
perform a more sophisticated analysis of the trans-
lations being evaluated and the scores they produce
tend to achieve a better correlation with human judg-
ments than those produced by BLEU (Snover et al.,
2009; Lavie and Denkowski, 2009; Przybocki et al.,
2008; Snover et al., 2006).
Their better correlations suggest that we might
obtain higher quality translations by making use of
these new metrics when training our models. We ex-
pect that training on a specific metric will produce
the best performing model according to that met-
</bodyText>
<footnote confidence="0.936672">
&apos;METEOR: Metric for Evaluation of Translation with Ex-
plicit ORdering.
</footnote>
<page confidence="0.935125">
555
</page>
<note confidence="0.7619425">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 555–563,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998094">
ric. Doing better on metrics that better reflect human
judgments seems to imply the translations produced
by the model would be preferred by human judges.
However, there are four potential problems. First,
some metrics could be susceptible to systematic ex-
ploitation by the training algorithm and result in
model translations that have a high score according
to the evaluation metric but that are of low qual-
ity.2 Second, other metrics may result in objective
functions that are harder to optimize. Third, some
may result in better generalization performance at
test time by not encouraging overfitting of the train-
ing data. Finally, as a practical concern, metrics used
for training cannot be too slow.
In this paper, we systematically explore these four
issues for the most popular metrics available to the
MT community. We examine how well models per-
form both on the metrics on which they were trained
and on the other alternative metrics. Multiple mod-
els are trained using each metric in order to deter-
mine the stability of the resulting models. Select
models are scored by human judges in order to deter-
mine how performance differences obtained by tun-
ing to different automated metrics relates to actual
human preferences.
The next sections introduce the metrics and our
training procedure. We follow with two sets of core
results, machine evaluation in section 5, and human
evaluation in section 6.
</bodyText>
<sectionHeader confidence="0.988498" genericHeader="introduction">
2 Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.999878875">
Designing good automated metrics for evaluating
machine translations is challenging due to the vari-
ety of acceptable translations for each foreign sen-
tence. Popular metrics produce scores primarily
based on matching sequences of words in the system
translation to those in one or more reference trans-
lations. The metrics primarily differ in how they ac-
count for reorderings and synonyms.
</bodyText>
<subsectionHeader confidence="0.91937">
2.1 BLEU
</subsectionHeader>
<bodyText confidence="0.99937175">
BLEU (Papineni et al., 2002) uses the percentage
of n-grams found in machine translations that also
occur in the reference translations. These n-gram
precisions are calculated separately for different n-
</bodyText>
<footnote confidence="0.986140333333333">
2For example, BLEU computed without the brevity penalty
would likely result in models that have a strong preference for
generating pathologically short translations.
</footnote>
<bodyText confidence="0.996338444444444">
gram lengths and then combined using a geometric
mean. The score is then scaled by a brevity penalty
if the candidate translations are shorter than the ref-
erences, BP = min(1.0, e1−len(R)/len(T)). Equa-
tion 1 gives BLEU using n-grams up to length N for
a corpus of candidate translations T and reference
translations R. A variant of BLEU called the NIST
metric (Doddington, 2002) weights n-gram matches
by how informative they are.
</bodyText>
<equation confidence="0.692159">
n-grams(T n R)
n-grams(T)
</equation>
<bodyText confidence="0.999956384615385">
While easy to compute, BLEU has a number of
shortcomings. Since the order of matching n-grams
is ignored, n-grams in a translation can be randomly
rearranged around non-matching material or other
n-gram breaks without harming the score. BLEU
also does not explicitly check whether information
is missing from the candidate translations, as it only
examines what fraction of candidate translation n-
grams are in the references and not what fraction
of references n-grams are in the candidates (i.e.,
BLEU ignores n-gram recall). Finally, the metric
does not account for words and phrases that have
similar meanings.
</bodyText>
<subsectionHeader confidence="0.992287">
2.2 METEOR
</subsectionHeader>
<bodyText confidence="0.9868235">
METEOR (Lavie and Denkowski, 2009) computes
a one-to-one alignment between matching words in
a candidate translation and a reference. If a word
matches multiple other words, preference is given to
the alignment that reorders the words the least, with
the amount of reordering measured by the number of
crossing alignments. Alignments are first generated
for exact matches between words. Additional align-
ments are created by repeatedly running the align-
ment procedure over unaligned words, first allowing
for matches between word stems, and then allow-
ing matches between words listed as synonyms in
WordNet. From the final alignment, the candidate
translation’s unigram precision and recall is calcu-
lated, P = matches and R = matches These two
length trans length ref
are then combined into a weighted harmonic mean
(2). To penalize reorderings, this value is then scaled
by a fragmentation penalty based on the number of
chunks the two sentences would need to be broken
</bodyText>
<equation confidence="0.9981502">
N
BLEU:N = H
n=1
N
BP (1)
</equation>
<page confidence="0.869855">
556
</page>
<bodyText confidence="0.7813">
into to allow them to be rearranged with no crossing
</bodyText>
<equation confidence="0.945329">
alignments, Pβ,γ = 1 − -y ( chunks )β.
matches
Fα = PR (2)
αP + (1 − α)R
METEORα,β,γ = Fα · Pβ,γ (3)
</equation>
<bodyText confidence="0.9997595">
Equation 3 gives the final METEOR score as the
product of the unigram harmonic mean, Fα, and the
fragmentation penalty, Pβ,γ. The free parameters α,
Q, and -y can be used to tune the metric to human
judgments on a specific language and variation of
the evaluation task (e.g., ranking candidate transla-
tions vs. reproducing judgments of translations ade-
quacy and fluency).
</bodyText>
<subsectionHeader confidence="0.998263">
2.3 Translation Edit Rate
</subsectionHeader>
<bodyText confidence="0.999935266666667">
TER (Snover et al., 2006) searches for the shortest
sequence of edit operations needed to turn a can-
didate translation into one of the reference transla-
tions. The allowable edits are the insertion, dele-
tion, and substitution of individual words and swaps
of adjacent sequences of words. The swap opera-
tion differentiates TER from the simpler word error
rate (WER) metric (Nießen et al., 2000), which only
makes use of insertions, deletions, and substitutions.
Swaps prevent phrase reorderings from being exces-
sively penalized. Once the shortest sequence of op-
erations is found,3 TER is calculated simply as the
number of required edits divided by the reference
translation length, or average reference translation
length when multiple are available (4).
</bodyText>
<equation confidence="0.868205333333333">
min edits
TER = (4)
avg ref length
</equation>
<bodyText confidence="0.9098874">
TER-Plus (TERp) (Snover et al., 2009) extends
TER by allowing the cost of edit operations to be
tuned in order to maximize the metric’s agreement
with human judgments. TERp also introduces three
new edit opertions: word stem matches, WordNet
synonym matches, and multiword matches using a
table of scored paraphrases.
3Since swaps prevent TER from being calculated exactly us-
ing dynamic programming, a beam search is used and this can
overestimate the number of required edits.
</bodyText>
<sectionHeader confidence="0.997245" genericHeader="method">
3 MERT
</sectionHeader>
<bodyText confidence="0.999983">
MERT is the standard technique for obtaining a ma-
chine translation model fit to a specific evaluation
metric (Och, 2003). Learning such a model cannot
be done using gradient methods since the value of
the objective function only depends on the transla-
tion model’s argmax for each sentence in the tun-
ing set. Typically, this optimization is performed as
a series of line searches that examines the value of
the evaluation metric at critical points where a new
translation argmax becomes preferred by the model.
Since the model score assigned to each candidate
translation varies linearly with changes to the model
parameters, it is possible to efficiently find the global
minimum along any given search direction with only
O(n2) operations when n-best lists are used.
Using our implementation of MERT that allows
for pluggable optimization metrics, we tune mod-
els to BLEU:N for N = 1...5, TER, two con-
figurations of TERp, WER, several configurations
of METEOR, as well as additive combinations of
these metrics. The TERp configurations include
the default configuration of TERp and TERpA:
the configuration of TERp that was trained to
match human judgments for NIST Metrics MATR
(Matthew Snover and Schwartz, 2008; Przybocki et
al., 2008). For METEOR, we used the standard ME-
TEOR English parameters (α = 0.8, Q = 2.5, -y =
0.4), and the English parameters for the ranking ME-
TEOR (α = 0.95, Q = 0.5,-y = 0.5),4 which
was tuned to maximize the metric’s correlation with
WMT-07 human ranking judgements (Agarwal and
Lavie, 2008). The default METEOR parameters fa-
vor longer translations than the other metrics, since
high α values place much more weight on unigram
recall than precision. Since this may put models
tuned to METEOR at a disadvantage when being
evaluated by the other metrics, we also use a variant
of the standard English model and of ranking ME-
TEOR with α set to 0.5, as this weights both recall
and precision equally.
For each iteration of MERT, 20 random restarts
were used in addition to the best performing point
discovered during earlier iterations of training.5
</bodyText>
<footnote confidence="0.9981435">
4Agarwal and Lavie (2008) report -y = 0.45, however the
0.8.2 release of METEOR uses -y = 0.5 for ranking English.
5This is not necessarily identical with the point returned by
the most recent MERT iteration, but rather can be any point
</footnote>
<page confidence="0.995146">
557
</page>
<bodyText confidence="0.999901666666667">
Since MERT is known to be sensitive to what restart
points are provided, we use the same series of ran-
dom restart points for each model. During each it-
eration of MERT, the random seed is based on the
MERT iteration number. Thus, while a different set
of random points is selected during each MERT iter-
ation, on any given iteration all models use the same
set of points. This prevents models from doing better
or worse just because they received different starting
points. However, it is still possible that certain ran-
dom starting points are better for some evaluation
metrics than others.
</bodyText>
<sectionHeader confidence="0.999547" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999974266666667">
Experiments were run using Phrasal (Cer et al.,
2010), a left-to-right beam search decoder that
achieves a matching BLEU score to Moses (Koehn
et al., 2007) on a variety of data sets. During de-
coding we made use of a stack size of 100, set the
distortion limit to 6, and retrieved 20 translation op-
tions for each unique source phrase.
Using the selected metrics, we train both Chi-
nese to English and Arabic to English models.6 The
Chinese to English models are trained using NIST
MT02 and evaluated on NIST MT03. The Arabic
to English experiments use NIST MT06 for train-
ing and GALE dev07 for evaluation. The resulting
models are scored using all of the standalone metrics
used during training.
</bodyText>
<subsectionHeader confidence="0.998891">
4.1 Arabic to English
</subsectionHeader>
<bodyText confidence="0.987209933333333">
Our Arabic to English system was based on a well
ranking 2009 NIST submission (Galley et al., 2009).
The phrase table was extracted using all of the al-
lowed resources for the constrained Arabic to En-
glish track. Word alignment was performed using
the Berkeley cross-EM aligner (Liang et al., 2006).
Phrases were extracted using the grow heuristic
(Koehn et al., 2003). However, we threw away all
phrases that have a P(e|f) &lt; 0.0001 in order to re-
duce the size of the phrase table. From the aligned
data, we also extracted a hierarchical reordering
model that is similar to popular lexical reordering
models (Koehn et al., 2007) but that models swaps
containing more than just one phrase (Galley and
returned during an earlier iteration of MERT.
</bodyText>
<footnote confidence="0.682511">
6Given the amount of time required to train a TERpA model,
we only present TERpA results for Chinese to English.
</footnote>
<bodyText confidence="0.971343857142857">
Manning, 2008). A 5-gram language model was cre-
ated with the SRI language modeling toolkit (Stol-
cke, 2002) using all of the English material from
the parallel data employed to train the phrase table
as well as Xinhua Chinese English Parallel News
(LDC2002E18).7 The resulting decoding model has
16 features that are optimized during MERT.
</bodyText>
<subsectionHeader confidence="0.995944">
4.2 Chinese to English
</subsectionHeader>
<bodyText confidence="0.999995529411765">
For our Chinese to English system, our phrase ta-
ble was built using 1,140,693 sentence pairs sam-
pled from the GALE Y2 training data. The Chinese
sentences were word segmented using the 2008 ver-
sion of Stanford Chinese Word Segmenter (Chang et
al., 2008; Tseng et al., 2005). Phrases were extracted
by running GIZA++ (Och and Ney, 2003) in both
directions and then merging the alignments using
the grow-diag-final heuristic (Koehn et al., 2003).
From the merged alignments we also extracted a
bidirectional lexical reordering model conditioned
on the source and the target phrases (Koehn et al.,
2007). A 5-gram language model was created with
the SRI language modeling toolkit (Stolcke, 2002)
and trained using the Gigaword corpus and English
sentences from the parallel data. The resulting de-
coding model has 14 features to be trained.
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.9999436">
As seen in tables 1 and 2, the evaluation metric we
use during training has a substantial impact on per-
formance as measured by the various other metrics.
There is a clear block structure where the best class
of metrics to train on is the same class that is used
during evaluation. Within this block structure, we
make three primary observations. First, the best
performing model according to any specific metric
configuration is usually not the model we trained to
that configuration. In the Chinese results, the model
trained on BLEU:3 scores 0.74 points higher on
BLEU:4 than the model actually trained to BLEU:4.
In fact, the BLEU:3 trained model outperforms all
other models on BLEU:N metrics. For the Arabic
results, training on NIST scores 0.27 points higher
</bodyText>
<footnote confidence="0.64162">
7In order to run multiple experiments in parallel on the com-
puters available to us, the system we use for this work differs
from our NIST submission in that we remove the Google n-
gram language model. This results in a performance drop of
less than 1.0 BLEU point on our dev data.
</footnote>
<page confidence="0.96189">
558
</page>
<table confidence="0.99996915">
Train\Eval BLEU:1 BLEU:2 BLEU:3 BLEU:4 BLEU:5 NIST TER TERp WER TERpA METR METR-r METR METR-r
α = 0.5 α = 0.5
BLEU:1 75.98 55.39 40.41 29.64 21.60 11.94 78.07 78.71 68.28 73.63 41.98 59.63 42.46 60.02
BLEU:2 76.58 57.24 42.84 32.21 24.09 12.20 77.09 77.63 67.16 72.54 43.20 60.91 43.59 61.56
BLEU:3 76.74 57.46 43.13 32.52 24.44 12.22 76.53 77.07 66.81 72.01 42.94 60.57 43.40 60.88
BLEU:4 76.24 56.86 42.43 31.80 23.77 12.14 76.75 77.25 66.78 72.01 43.29 60.94 43.10 61.27
BLEU:5 76.39 57.14 42.93 32.38 24.33 12.40 75.42 75.77 65.86 70.29 43.02 61.22 43.57 61.43
NIST 76.41 56.86 42.34 31.67 23.57 12.38 75.20 75.72 65.78 70.11 43.11 61.04 43.78 61.84
TER 73.23 53.39 39.09 28.81 21.18 12.73 71.33 71.70 63.92 66.58 38.65 55.49 41.76 59.07
TERp 72.78 52.90 38.57 28.32 20.76 12.68 71.76 72.16 64.26 66.96 38.51 56.13 41.48 58.73
TERpA 71.79 51.58 37.36 27.23 19.80 12.54 72.26 72.56 64.58 67.30 37.86 55.10 41.16 58.04
WER 74.49 54.59 40.30 29.88 22.14 12.64 71.85 72.34 63.82 67.11 39.76 57.29 42.37 59.97
METR 73.33 54.35 40.28 30.04 22.39 11.53 84.74 85.30 71.49 79.47 44.68 62.14 42.99 60.73
METR-r 74.20 54.99 40.91 30.66 22.98 11.74 82.69 83.23 70.49 77.77 44.64 62.25 43.44 61.32
METR:0.5 76.36 56.75 42.48 31.98 24.00 12.44 74.94 75.32 66.09 70.14 42.75 60.98 43.86 61.38
METR-r:0.5 76.49 56.93 42.36 31.70 23.68 12.21 77.04 77.58 67.12 72.23 43.26 61.03 43.63 61.67
Combined Models
BLEU:4-TER 75.32 55.98 41.87 31.42 23.50 12.62 72.97 73.38 64.46 67.95 41.50 59.11 43.50 60.82
BLEU:4-2TERp 75.22 55.76 41.57 31.11 23.25 12.64 72.48 72.89 64.17 67.43 41.12 58.82 42.73 60.86
BLEU:4+2MTR 75.77 56.45 42.04 31.47 23.48 11.98 79.96 80.65 68.85 74.84 44.06 61.78 43.70 61.48
</table>
<tableCaption confidence="0.981468">
Table 1: Chinese to English test set performance on MT03 using models trained using MERT on MT02. In each column,
cells shaded blue are better than average and those shaded red are below average. The intensity of the shading indicates
the degree of deviation from average. For BLEU, NIST, and METEOR, higher is better. For edit distance metrics like
TER and WER, lower is better.
</tableCaption>
<table confidence="0.999971578947368">
Train\Eval BLEU:1 BLEU:2 BLEU:3 BLEU:4 BLEU:5 NIST TER TERp WER METR METR-r METR METR-r
α = 0.5 α = 0.5
BLEU:1 79.90 65.35 54.08 45.14 37.81 10.68 46.19 61.04 49.98 49.74 67.79 49.19 68.12
BLEU:2 80.03 65.84 54.70 45.80 38.47 10.75 45.74 60.63 49.24 50.02 68.00 49.71 68.27
BLEU:3 79.87 65.71 54.59 45.67 38.34 10.72 45.86 60.80 49.18 49.87 68.32 49.61 67.67
BLEU:4 80.39 66.14 54.99 46.05 38.70 10.82 45.25 59.83 48.69 49.65 68.13 49.66 67.92
BLEU:5 79.97 65.77 54.64 45.76 38.44 10.75 45.66 60.55 49.11 49.89 68.33 49.64 68.19
NIST 80.41 66.27 55.22 46.32 38.98 10.96 44.11 57.92 47.74 48.88 67.85 49.88 68.52
TER 79.69 65.52 54.44 45.55 38.23 10.75 43.36 56.12 47.11 47.90 66.49 49.55 68.12
TERp 79.27 65.11 54.13 45.35 38.12 10.75 43.36 55.92 47.14 47.83 66.34 49.43 67.94
WER 79.42 65.28 54.30 45.51 38.27 10.78 43.44 56.13 47.13 47.82 66.33 49.38 67.88
METR 75.52 60.94 49.84 41.17 34.12 9.93 52.81 70.08 55.72 50.92 68.55 48.47 66.89
METR-r 77.42 62.91 51.67 42.81 35.61 10.24 49.87 66.26 53.17 50.95 69.29 49.29 67.89
METR:0.5 79.69 65.14 53.94 45.03 37.72 10.72 45.80 60.44 49.34 49.78 68.31 49.23 67.72
METR-r:0.5 79.76 65.12 53.82 44.88 37.57 10.67 46.53 61.55 50.17 49.66 68.57 49.58 68.25
Combined Models
BLEU:4-TER 80.37 66.31 55.27 46.36 39.00 10.96 43.94 57.46 47.46 49.00 67.10 49.85 68.41
BLEU:4-2TERp 79.65 65.53 54.54 45.75 38.48 10.80 43.42 56.16 47.15 47.90 65.93 49.09 67.90
BLEU:4+2METR 79.43 64.97 53.75 44.87 37.58 10.63 46.74 62.03 50.35 50.42 68.92 49.70 68.37
</table>
<tableCaption confidence="0.970837333333333">
Table 2: Arabic to English test set performance on dev07 using models trained using MERT on MT06. As above, in each
column, cells shaded blue are better than average and those shaded red are below average. The intensity of the shading
indicates the degree of deviation from average.
</tableCaption>
<bodyText confidence="0.999383942857143">
on BLEU:4 than training on BLEU:4, and outper-
forms all other models on BLEU:N metrics.
Second, the edit distance based metrics (WER,
TER, TERp, TERpA)8 seem to be nearly inter-
changeable. While the introduction of swaps al-
lows the scores produced by the TER metrics to
achieve better correlation with human judgments,
our models are apparently unable to exploit this dur-
ing training. This maybe due to the monotone na-
8In our implementation of multi-reference WER, we use the
length of the references that result in the lowest sentence level
WER to divide the edit costs. In contrast, TER divides by the
average reference length. This difference can sometimes result
in WER being lower than the corresponding TER. Also, as can
be seen in the Arabic to English results, TERp scores sometimes
differ dramatically from TER scores due to normalization and
tokenization differences (e.g., TERp removes punctuation prior
to scoring, while TER does not).
ture of the reference translations and the fact that
having multiple references reduces the need for re-
orderings. However, it is possible that differences
between training to WER and TER would become
more apparent using models that allow for longer
distance reorderings or that do a better job of cap-
turing what reorderings are acceptable.
Third, with the exception of BLEU:1, the perfor-
mance of the BLEU, NIST, and the METEOR α=.5
models appears to be more robust across the other
evaluation metrics than the standard METEOR, ME-
TEOR ranking, and edit distance based models
(WER, TER, TERp, an TERpA). The latter mod-
els tend to do quite well on metrics similar to what
they were trained on, while performing particularly
poorly on the other metrics. For example, on Chi-
nese, the TER and WER models perform very well
</bodyText>
<page confidence="0.996631">
559
</page>
<bodyText confidence="0.999683454545454">
on other edit distance based metrics, while perform-
ing poorly on all the other metrics except NIST.
While less pronounced, the same trend is also seen
in the Arabic data. Interestingly enough, while the
TER, TERp and standard METEOR metrics achieve
good correlations with human judgments, models
trained to them are particularly mismatched in our
results. The edit distance models do terribly on ME-
TEOR and METEOR ranking, while METEOR and
METEOR ranking models do poorly on TER, TERp,
and TERpA.
</bodyText>
<table confidence="0.9997652">
Training Itr MERT Training Itr MERT
Metric Time Metric Time
BLEU:1 13 21:57 NIST 15 78:15
BLEU:2 15 32:40 TER 7 21:00
BLEU:3 19 45:08 TERp 9 19:19
BLEU:4 10 24:13 TERpA 8 393:16
BLEU:5 16 46:12 WER 13 33:53
BL:4-TR 9 21:07 BL:4-2TRp 8 22:03
METR 12 39:16 METR 0.5 18 42:04
METR R 12 47:19 METR R:0.5 13 25:44
</table>
<tableCaption confidence="0.996274">
Table 3: Chinese to English MERT iterations and training
times, given in hours:mins and excluding decoder time.
</tableCaption>
<subsectionHeader confidence="0.987643">
5.1 Other Results
</subsectionHeader>
<bodyText confidence="0.999954433333333">
On the training data, we see a similar block struc-
ture within the results, but there is a different pattern
among the top performers. The tables are omitted,
but we observe that, for Chinese, the BLEU:5 model
performs best on the training data according to all
higher order BLEU metrics (4-7). On Arabic, the
BLEU:6 model does best on the same higher order
BLEU metrics (4-7). By rewarding higher order n-
gram matches, these objectives actually find minima
that result in more 4-gram matches than the mod-
els optimized directly to BLEU:4. However, the fact
that this performance advantage disappears on the
evaluation data suggests these higher order models
also promote overfitting.
Models trained on additive metric blends tend
to smooth out performance differences between
the classes of metrics they contain. As expected,
weighting the metrics used in the additive blends re-
sults in models that perform slightly better on the
type of metric with the highest weight.
Table 3 reports training times for select Chinese
to English models. Training to TERpA is very com-
putationally expensive due to the implementation of
the paraphrase table. The TER family of metrics
tends to converge in fewer MERT iterations than
those trained on other metrics such as BLEU, ME-
TEOR or even WER. This suggests that the learning
objective provided by these metrics is either easier to
optimize or they more easily trap the search in local
minima.
</bodyText>
<subsectionHeader confidence="0.999517">
5.2 Model Variance
</subsectionHeader>
<bodyText confidence="0.999978541666667">
One potential problem with interpreting the results
above is that learning with MERT is generally as-
sumed to be noisy, with different runs of the al-
gorithm possibly producing very different models.
We explore to what extent the results just presented
were affected by noise in the training procedure. We
perform multiple training runs using select evalua-
tion metrics and examining how consistent the re-
sulting models are. This also allows us to deter-
mine whether the metric used as a learning criteria
influences the stability of learning. For these experi-
ments, Chinese to English models are trained 5 times
using a different series of random starting points. As
before, 20 random restarts were used during each
MERT iteration.
In table 4, models trained to BLEU and METEOR
are relatively stable, with the METEOR:0.5 trained
models being the most stable. The edit distance
models, WER and TERp, vary more across train-
ing runs, but still do not exceed the interesting cross
metric differences seen in table 1. The instability of
WER and TERp, with TERp models having a stan-
dard deviation of 1.3 in TERp and 2.5 in BLEU:4,
make them risky metrics to use for training.
</bodyText>
<sectionHeader confidence="0.956409" genericHeader="method">
6 Human Evaluation
</sectionHeader>
<bodyText confidence="0.999926307692308">
The best evaluation metric to use during training is
the one that ultimately leads to the best translations
according to human judges. We perform a human
evaluation of select models using Amazon Mechan-
ical Turk, an online service for cheaply performing
simple tasks that require human intelligence. To use
the service, tasks are broken down into individual
units of work known as human intelligence tasks
(HITs). HITs are assigned a small amount of money
that is paid out to the workers that complete them.
For many natural language annotation tasks, includ-
ing machine translation evaluation, it is possible to
obtain annotations that are as good as those pro-
</bodyText>
<page confidence="0.988704">
560
</page>
<table confidence="0.999965222222222">
Train\Eval Q BLEU:1 BLEU:3 BLEU:4 BLEU:5 TERp WER METEOR METEOR:0.5
BLEU:1 0.17 0.56 0.59 0.59 0.36 0.58 0.42 0.24
BLEU:3 0.38 0.41 0.38 0.32 0.70 0.49 0.44 0.33
BLEU:4 0.27 0.29 0.29 0.27 0.67 0.50 0.41 0.29
BLEU:5 0.17 0.14 0.19 0.21 0.67 0.75 0.34 0.17
TERp 1.38 2.66 2.53 2.20 1.31 1.39 1.95 1.82
WER 0.62 1.37 1.37 1.25 1.31 1.21 1.10 1.01
METEOR 0.80 0.56 0.48 0.44 3.71 2.69 0.69 1.10
METEOR:0.5 0.32 0.11 0.09 0.11 0.23 0.12 0.07 0.11
</table>
<tableCaption confidence="0.997867666666667">
Table 4: MERT model variation for Chinese to English. We train five models to each metric listed above. The
collection of models trained to a given metric is then evaluated using the other metrics. We report the resulting
standard devation for the collection on each of the metrics. The collection with the lowest varience is bolded.
</tableCaption>
<table confidence="0.999962142857143">
Model Pair % Preferred P-value
Chinese
METR R vs. TERp 60.0 0.0028
BLEU:4 vs. TERp 57.5 0.02
NIST vs. TERp 55.0 0.089
NIST vs. TERpA 55.0 0.089
BLEU:4 vs. TERpA 54.5 0.11
BLEU:4 vs. METR R 54.5 0.11
METR:0.5 vs. METR 54.5 0.11
METR:0.5 vs. METR R 53.0 0.22
METR vs. BLEU:4 52.5 0.26
BLEU:4 vs. METR:0.5 52.5 0.26
METR vs. TERp 52.0 0.31
NIST vs. BLEU:4 52.0 0.31
BLEU:4 vs. METR R:0.5 51.5 0.36
WER vs. TERp 51.5 0.36
TERpA vs. TERp 50.5 0.47
Arabic
BLEU:4 vs. METR R 62.0 &lt; 0.001
NIST vs. TERp 56.0 0.052
BLEU:4 vs. METR:0.5 55.5 0.069
BLEU:4 vs. METR 54.5 0.11
METR R:0.5 vs METR R 54.0 0.14
NIST vs. BLEU:4 51.5 0.36
WER vs. TERp 51.5 0.36
METR:0.5 vs METR 51.5 0.36
TERp vs. BLEU:4 51.0 0.42
BLEU:4 vs. METR R:0.5 50.5 0.47
</table>
<tableCaption confidence="0.9957735">
Table 5: Select pairwise preference for models trained to
different evaluation metrics. For A vs. B, preferred indi-
cates how often A was preferred to B. We bold the better
training metric for statistically significant differences.
</tableCaption>
<bodyText confidence="0.995819">
duced by experts by having multiple workers com-
plete each HIT and then combining their answers
(Snow et al., 2008; Callison-Burch, 2009).
We perform a pairwise comparison of the trans-
lations produced for the first 200 sentences of our
Chinese to English test data (MT03) and our Arabic
to English test data (dev07). The HITs consist of a
pair of machine translated sentences and a single hu-
man generated reference translation. The reference
is chosen at random from those available for each
sentence. Capitalization of the translated sentences
is restored using an HMM based truecaser (Lita et
al., 2003). Turkers are instructed to “... select the
machine translation generated sentence that is eas-
iest to read and best conveys what is stated in the
reference”. Differences between the two machine
translations are emphasized by being underlined and
bold faced.9 The resulting HITs are made available
only to workers in the United States, as pilot experi-
ments indicated this results in more consistent pref-
erence judgments. Three preference judgments are
obtained for each pair of translations and are com-
bined using weighted majority vote.
As shown in table 5, in many cases the quality of
the translations produced by models trained to dif-
ferent metrics is remarkably similar. Training to the
simpler edit distance metric WER produces transla-
tions that are as good as those from models tuned to
the similar but more advanced TERp metric that al-
lows for swaps. Similarly, training to TERpA, which
makes use of both a paraphrase table and edit costs
</bodyText>
<footnote confidence="0.9981">
9We emphasize relative differences between the two trans-
lations rather than the difference between each translation and
the reference in order to avoid biasing evaluations toward edit
distance metrics.
</footnote>
<page confidence="0.995781">
561
</page>
<bodyText confidence="0.999816416666667">
tuned to human judgments, is no better than TERp.
For the Chinese to English results, there is a sta-
tistically significant human preference for transla-
tions that are produced by training to BLEU:4 and
a marginally significant preferences for training to
NIST over the default configuration of TERp. This
contrasts sharply with earlier work showing that
TER and TERp correlate better with human judge-
ments than BLEU (Snover et al., 2009; Przybocki
et al., 2008; Snover et al., 2006). While it is as-
sumed that, by using MERT, “improved evaluation
measures lead directly to improved machine trans-
lation quality” (Och, 2003), these results show im-
proved correlations with human judgments are not
always sufficient to establish that tuning to a metric
will result in higher quality translations. In the Ara-
bic results, we see a similar pattern where NIST is
preferred to TERp, again with marginal signficance.
Strangely, however, there is no real difference be-
tween TERp vs. BLEU:4.
For Arabic, training to ranking METEOR is worse
than BLEU:4, with the differences being very sig-
nificant. The Arabic results also trend toward sug-
gesting that BLEU:4 is better than either standard
METEOR and METEOR α 0.5. However, for the
Chinese models, training to standard METEOR and
METEOR α 0.5 is about as good as training to
BLEU:4. In both the Chinese and Arabic results, the
METEOR α 0.5 models are at least as good as those
trained to standard METEOR and METEOR rank-
ing. In contrast to the cross evaluation metric results,
where the differences between the α 0.5 models and
the standard METEOR models were always fairly
dramatic, the human preferences suggest there is of-
ten not much of a difference in the true quality of the
translations produced by these models.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999980591836735">
Training to different evaluation metrics follows the
expected pattern whereby models perform best on
the same type of metric used to train them. How-
ever, models trained using the n-gram based metrics,
BLEU and NIST, are more robust to being evaluated
using the other metrics.
Edit distance models tend to do poorly when eval-
uated on other metrics, as do models trained using
METEOR. However, training models to METEOR
can be made more robust by setting α to 0.5, which
balances the importance the metric assigns to preci-
sion and recall.
The fact that the WER, TER and TERp models
perform very similarly suggests that current phrase-
based translation systems lack either the features or
the model structure to take advantage of swap edit
operations. The situation might be improved by us-
ing a model that does a better job of both captur-
ing the structure of the source and target sentences
and their allowable reorderings, such as a syntac-
tic tree-to-string system that uses contextually rich
rewrite rules (Galley et al., 2006), or by making use
of larger more fine grained feature sets (Chiang et
al., 2009) that allow for better discrimination be-
tween hypotheses.
Human results indicate that edit distance trained
models such as WER and TERp tend to pro-
duce lower quality translations than BLEU or NIST
trained models. Tuning to METEOR works reason-
ably well for Chinese, but is not a good choice for
Arabic. We suspect that the newer RYPT metric
(Zaidan and Callison-Burch, 2009), which directly
makes use of human adequacy judgements of sub-
strings, would obtain better human results than the
automated metrics presented here. However, like
other metrics, we expect performance gains still will
be sensitive to how the mechanics of the metric inter-
act with the structure and feature set of the decoding
model being used.
BLEU and NIST’s strong showing in both the ma-
chine and human evaluation results indicates that
they are still the best general choice for training
model parameters. We emphasize that improved
metric correlations with human judgments do not
imply that models trained to a metric will result in
higher quality translations. We hope future work
on developing new evaluation metrics will explicitly
explore the translation quality of models trained to
them.
</bodyText>
<sectionHeader confidence="0.996512" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999819285714286">
The authors thank Alon Lavie for suggesting set-
ting α to 0.5 when training to METEOR. This work
was supported by the Defense Advanced Research
Projects Agency through IBM. The content does
not necessarily reflect the views of the U.S. Gov-
ernment, and no official endorsement should be in-
ferred.
</bodyText>
<page confidence="0.994866">
562
</page>
<sectionHeader confidence="0.995848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999922878787879">
Abhaya Agarwal and Alon Lavie. 2008. METEOR,
M-BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In StatMT workshop at ACL.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in ma-
chine translation research. In EACL.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon’s Me-
chanical Turk. In EMNLP.
Daniel Cer, Michel Galley, Christopher D. Manning, and
Dan Jurafsky. 2010. Phrasal: A statistical machine
translation toolkit for exploring new model features.
In NAACL.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In StatMT
workshop at ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. JMLR,
3:951–991.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In HLT.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL.
Michel Galley, Spence Green, Daniel Cer, Pi-Chuan
Chang, and Christopher D. Manning. 2009. Stanford
university’s arabic-to-english statistical machine trans-
lation system for the 2009 NIST evaluation. In NIST
Open Machine Translation Evaluation Meeting.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In NAACL.
Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and
Nanda Kambhatla. 2003. tRuEcasIng. In ACL.
Bonnie Dorr Matthew Snover, Nitin Madnani and
Richard Schwartz. 2008. TERp system description.
In MetricsMATR workshop at AMTA.
Sonja Nießen, Franz Josef Och, and Hermann Ney. 2000.
An evaluation tool for machine translation: Fast eval-
uation for MT research. In LREC.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL.
M. Przybocki, K. Peterson, and S. Bronsart. 2008.
Official results of the “Metrics for MAchine TRans-
lation” Challenge (MetricsMATR08). Techni-
cal report, NIST, http://nist.gov/speech/
tests/metricsmatr/2008/results/.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER?: exploring different human judgments with a
tunable MT metric. In StatMT workshop at EACL).
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast – but is it good? Eval-
uating non-expert annotations for natural language
tasks. In EMNLP.
Andreas Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In ICSLP.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher D. Manning. 2005. A con-
ditional random field word segmenter. In SIGHAN.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In EMNLP-CoNLL.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
bility of human-in-the-loop minimum error rate train-
ing. In EMNLP, pages 52–61, August.
</reference>
<page confidence="0.99894">
563
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.955711">
<title confidence="0.9987665">The Best Lexical Metric Phrase-Based Statistical MT System Optimization</title>
<author confidence="0.969166">Daniel Cer</author>
<author confidence="0.969166">Christopher D Manning</author>
<author confidence="0.969166">Daniel</author>
<affiliation confidence="0.999995">Stanford University</affiliation>
<address confidence="0.999743">Stanford, CA 94305, USA</address>
<abstract confidence="0.999525666666667">Translation systems are generally trained to optimize BLEU, but many alternative metrics are available. We explore how optimizing toward various automatic evaluation metrics (BLEU, METEOR, NIST, TER) affects the resulting model. We train a state-of-the-art MT system using MERT on many parameterizations of each metric and evaluate the resulting models on the other metrics and also using human judges. In accordance with popular wisdom, we find that it’s important to train on the same metric used in testing. However, we also find that training to a newer metric is only useful to the extent that the MT model’s structure and features allow it to take advantage of the metric. Contrasting with TER’s good correlation with human judgments, we show that people tend to prefer BLEU and NIST trained models to those trained on edit distance based metrics like TER or WER. Human preferences for METEOR trained models varies depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abhaya Agarwal</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR, M-BLEU and M-TER: Evaluation metrics for highcorrelation with human rankings of machine translation output.</title>
<date>2008</date>
<booktitle>In StatMT workshop at ACL.</booktitle>
<contexts>
<context position="11180" citStr="Agarwal and Lavie, 2008" startWordPosition="1807" endWordPosition="1810">figurations of TERp, WER, several configurations of METEOR, as well as additive combinations of these metrics. The TERp configurations include the default configuration of TERp and TERpA: the configuration of TERp that was trained to match human judgments for NIST Metrics MATR (Matthew Snover and Schwartz, 2008; Przybocki et al., 2008). For METEOR, we used the standard METEOR English parameters (α = 0.8, Q = 2.5, -y = 0.4), and the English parameters for the ranking METEOR (α = 0.95, Q = 0.5,-y = 0.5),4 which was tuned to maximize the metric’s correlation with WMT-07 human ranking judgements (Agarwal and Lavie, 2008). The default METEOR parameters favor longer translations than the other metrics, since high α values place much more weight on unigram recall than precision. Since this may put models tuned to METEOR at a disadvantage when being evaluated by the other metrics, we also use a variant of the standard English model and of ranking METEOR with α set to 0.5, as this weights both recall and precision equally. For each iteration of MERT, 20 random restarts were used in addition to the best performing point discovered during earlier iterations of training.5 4Agarwal and Lavie (2008) report -y = 0.45, h</context>
</contexts>
<marker>Agarwal, Lavie, 2008</marker>
<rawString>Abhaya Agarwal and Alon Lavie. 2008. METEOR, M-BLEU and M-TER: Evaluation metrics for highcorrelation with human rankings of machine translation output. In StatMT workshop at ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="2435" citStr="Callison-Burch et al., 2006" startWordPosition="387" endWordPosition="390">c. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate (TER) (Snover et al., 2006; Snover et al., 2009) and METEOR&apos; (Lavie and Denkowski, 2009) perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU (Snover et al., 2009; Lavie and Denkowski, 2009; Przybocki et al., 2008; Snover et al., 2006).</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the role of BLEU in machine translation research. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="28117" citStr="Callison-Burch, 2009" startWordPosition="4681" endWordPosition="4682"> 0.001 NIST vs. TERp 56.0 0.052 BLEU:4 vs. METR:0.5 55.5 0.069 BLEU:4 vs. METR 54.5 0.11 METR R:0.5 vs METR R 54.0 0.14 NIST vs. BLEU:4 51.5 0.36 WER vs. TERp 51.5 0.36 METR:0.5 vs METR 51.5 0.36 TERp vs. BLEU:4 51.0 0.42 BLEU:4 vs. METR R:0.5 50.5 0.47 Table 5: Select pairwise preference for models trained to different evaluation metrics. For A vs. B, preferred indicates how often A was preferred to B. We bold the better training metric for statistically significant differences. duced by experts by having multiple workers complete each HIT and then combining their answers (Snow et al., 2008; Callison-Burch, 2009). We perform a pairwise comparison of the translations produced for the first 200 sentences of our Chinese to English test data (MT03) and our Arabic to English test data (dev07). The HITs consist of a pair of machine translated sentences and a single human generated reference translation. The reference is chosen at random from those available for each sentence. Capitalization of the translated sentences is restored using an HMM based truecaser (Lita et al., 2003). Turkers are instructed to “... select the machine translation generated sentence that is easiest to read and best conveys what is </context>
<context position="32983" citStr="Callison-Burch, 2009" startWordPosition="5489" endWordPosition="5490"> source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al., 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. Human results indicate that edit distance trained models such as WER and TERp tend to produce lower quality translations than BLEU or NIST trained models. Tuning to METEOR works reasonably well for Chinese, but is not a good choice for Arabic. We suspect that the newer RYPT metric (Zaidan and Callison-Burch, 2009), which directly makes use of human adequacy judgements of substrings, would obtain better human results than the automated metrics presented here. However, like other metrics, we expect performance gains still will be sensitive to how the mechanics of the metric interact with the structure and feature set of the decoding model being used. BLEU and NIST’s strong showing in both the machine and human evaluation results indicates that they are still the best general choice for training model parameters. We emphasize that improved metric correlations with human judgments do not imply that models </context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: Evaluating translation quality using Amazon’s Mechanical Turk. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
<author>Dan Jurafsky</author>
</authors>
<title>Phrasal: A statistical machine translation toolkit for exploring new model features.</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="12633" citStr="Cer et al., 2010" startWordPosition="2058" endWordPosition="2061">hat restart points are provided, we use the same series of random restart points for each model. During each iteration of MERT, the random seed is based on the MERT iteration number. Thus, while a different set of random points is selected during each MERT iteration, on any given iteration all models use the same set of points. This prevents models from doing better or worse just because they received different starting points. However, it is still possible that certain random starting points are better for some evaluation metrics than others. 4 Experiments Experiments were run using Phrasal (Cer et al., 2010), a left-to-right beam search decoder that achieves a matching BLEU score to Moses (Koehn et al., 2007) on a variety of data sets. During decoding we made use of a stack size of 100, set the distortion limit to 6, and retrieved 20 translation options for each unique source phrase. Using the selected metrics, we train both Chinese to English and Arabic to English models.6 The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the</context>
</contexts>
<marker>Cer, Galley, Manning, Jurafsky, 2010</marker>
<rawString>Daniel Cer, Michel Galley, Christopher D. Manning, and Dan Jurafsky. 2010. Phrasal: A statistical machine translation toolkit for exploring new model features. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In StatMT workshop at ACL.</booktitle>
<contexts>
<context position="14769" citStr="Chang et al., 2008" startWordPosition="2425" endWordPosition="2428">sh. Manning, 2008). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) using all of the English material from the parallel data employed to train the phrase table as well as Xinhua Chinese English Parallel News (LDC2002E18).7 The resulting decoding model has 16 features that are optimized during MERT. 4.2 Chinese to English For our Chinese to English system, our phrase table was built using 1,140,693 sentence pairs sampled from the GALE Y2 training data. The Chinese sentences were word segmented using the 2008 version of Stanford Chinese Word Segmenter (Chang et al., 2008; Tseng et al., 2005). Phrases were extracted by running GIZA++ (Och and Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic (Koehn et al., 2003). From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Koehn et al., 2007). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. The resulting decoding model has 14 features to be trained. 5 Results As se</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing chinese word segmentation for machine translation performance. In StatMT workshop at ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1747" citStr="Chiang et al., 2008" startWordPosition="280" endWordPosition="283">e language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 1 Introduction Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a num</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="32610" citStr="Chiang et al., 2009" startWordPosition="5425" endWordPosition="5428">rtance the metric assigns to precision and recall. The fact that the WER, TER and TERp models perform very similarly suggests that current phrasebased translation systems lack either the features or the model structure to take advantage of swap edit operations. The situation might be improved by using a model that does a better job of both capturing the structure of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al., 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. Human results indicate that edit distance trained models such as WER and TERp tend to produce lower quality translations than BLEU or NIST trained models. Tuning to METEOR works reasonably well for Chinese, but is not a good choice for Arabic. We suspect that the newer RYPT metric (Zaidan and Callison-Burch, 2009), which directly makes use of human adequacy judgements of substrings, would obtain better human results than the automated metrics presented here. However, like other metrics, we expect performance gains still will be sensitiv</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>JMLR,</journal>
<pages>3--951</pages>
<contexts>
<context position="1702" citStr="Crammer and Singer, 2003" startWordPosition="272" endWordPosition="275">TEOR trained models varies depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 1 Introduction Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automati</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. JMLR, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In HLT.</booktitle>
<contexts>
<context position="6126" citStr="Doddington, 2002" startWordPosition="977" endWordPosition="978">ations. These n-gram precisions are calculated separately for different n2For example, BLEU computed without the brevity penalty would likely result in models that have a strong preference for generating pathologically short translations. gram lengths and then combined using a geometric mean. The score is then scaled by a brevity penalty if the candidate translations are shorter than the references, BP = min(1.0, e1−len(R)/len(T)). Equation 1 gives BLEU using n-grams up to length N for a corpus of candidate translations T and reference translations R. A variant of BLEU called the NIST metric (Doddington, 2002) weights n-gram matches by how informative they are. n-grams(T n R) n-grams(T) While easy to compute, BLEU has a number of shortcomings. Since the order of matching n-grams is ignored, n-grams in a translation can be randomly rearranged around non-matching material or other n-gram breaks without harming the score. BLEU also does not explicitly check whether information is missing from the candidate translations, as it only examines what fraction of candidate translation ngrams are in the references and not what fraction of references n-grams are in the candidates (i.e., BLEU ignores n-gram rec</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="32529" citStr="Galley et al., 2006" startWordPosition="5410" endWordPosition="5413">ls to METEOR can be made more robust by setting α to 0.5, which balances the importance the metric assigns to precision and recall. The fact that the WER, TER and TERp models perform very similarly suggests that current phrasebased translation systems lack either the features or the model structure to take advantage of swap edit operations. The situation might be improved by using a model that does a better job of both capturing the structure of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al., 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. Human results indicate that edit distance trained models such as WER and TERp tend to produce lower quality translations than BLEU or NIST trained models. Tuning to METEOR works reasonably well for Chinese, but is not a good choice for Arabic. We suspect that the newer RYPT metric (Zaidan and Callison-Burch, 2009), which directly makes use of human adequacy judgements of substrings, would obtain better human results than the automated metrics presented here</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Spence Green</author>
<author>Daniel Cer</author>
<author>Pi-Chuan Chang</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford university’s arabic-to-english statistical machine translation system for the 2009 NIST evaluation.</title>
<date>2009</date>
<booktitle>In NIST Open Machine Translation Evaluation Meeting.</booktitle>
<contexts>
<context position="13396" citStr="Galley et al., 2009" startWordPosition="2193" endWordPosition="2196">ing we made use of a stack size of 100, set the distortion limit to 6, and retrieved 20 translation options for each unique source phrase. Using the selected metrics, we train both Chinese to English and Arabic to English models.6 The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the standalone metrics used during training. 4.1 Arabic to English Our Arabic to English system was based on a well ranking 2009 NIST submission (Galley et al., 2009). The phrase table was extracted using all of the allowed resources for the constrained Arabic to English track. Word alignment was performed using the Berkeley cross-EM aligner (Liang et al., 2006). Phrases were extracted using the grow heuristic (Koehn et al., 2003). However, we threw away all phrases that have a P(e|f) &lt; 0.0001 in order to reduce the size of the phrase table. From the aligned data, we also extracted a hierarchical reordering model that is similar to popular lexical reordering models (Koehn et al., 2007) but that models swaps containing more than just one phrase (Galley and </context>
</contexts>
<marker>Galley, Green, Cer, Chang, Manning, 2009</marker>
<rawString>Michel Galley, Spence Green, Daniel Cer, Pi-Chuan Chang, and Christopher D. Manning. 2009. Stanford university’s arabic-to-english statistical machine translation system for the 2009 NIST evaluation. In NIST Open Machine Translation Evaluation Meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="13664" citStr="Koehn et al., 2003" startWordPosition="2237" endWordPosition="2240">ned using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the standalone metrics used during training. 4.1 Arabic to English Our Arabic to English system was based on a well ranking 2009 NIST submission (Galley et al., 2009). The phrase table was extracted using all of the allowed resources for the constrained Arabic to English track. Word alignment was performed using the Berkeley cross-EM aligner (Liang et al., 2006). Phrases were extracted using the grow heuristic (Koehn et al., 2003). However, we threw away all phrases that have a P(e|f) &lt; 0.0001 in order to reduce the size of the phrase table. From the aligned data, we also extracted a hierarchical reordering model that is similar to popular lexical reordering models (Koehn et al., 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT. 6Given the amount of time required to train a TERpA model, we only present TERpA results for Chinese to English. Manning, 2008). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) us</context>
<context position="14960" citStr="Koehn et al., 2003" startWordPosition="2455" endWordPosition="2458">phrase table as well as Xinhua Chinese English Parallel News (LDC2002E18).7 The resulting decoding model has 16 features that are optimized during MERT. 4.2 Chinese to English For our Chinese to English system, our phrase table was built using 1,140,693 sentence pairs sampled from the GALE Y2 training data. The Chinese sentences were word segmented using the 2008 version of Stanford Chinese Word Segmenter (Chang et al., 2008; Tseng et al., 2005). Phrases were extracted by running GIZA++ (Och and Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic (Koehn et al., 2003). From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Koehn et al., 2007). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. The resulting decoding model has 14 features to be trained. 5 Results As seen in tables 1 and 2, the evaluation metric we use during training has a substantial impact on performance as measured by the various other metrics. There is a clear block structure where the</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="12736" citStr="Koehn et al., 2007" startWordPosition="2075" endWordPosition="2078">ng each iteration of MERT, the random seed is based on the MERT iteration number. Thus, while a different set of random points is selected during each MERT iteration, on any given iteration all models use the same set of points. This prevents models from doing better or worse just because they received different starting points. However, it is still possible that certain random starting points are better for some evaluation metrics than others. 4 Experiments Experiments were run using Phrasal (Cer et al., 2010), a left-to-right beam search decoder that achieves a matching BLEU score to Moses (Koehn et al., 2007) on a variety of data sets. During decoding we made use of a stack size of 100, set the distortion limit to 6, and retrieved 20 translation options for each unique source phrase. Using the selected metrics, we train both Chinese to English and Arabic to English models.6 The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the standalone metrics used during training. 4.1 Arabic to English Our Arabic to English system was based </context>
<context position="15117" citStr="Koehn et al., 2007" startWordPosition="2479" endWordPosition="2482">Chinese to English For our Chinese to English system, our phrase table was built using 1,140,693 sentence pairs sampled from the GALE Y2 training data. The Chinese sentences were word segmented using the 2008 version of Stanford Chinese Word Segmenter (Chang et al., 2008; Tseng et al., 2005). Phrases were extracted by running GIZA++ (Och and Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic (Koehn et al., 2003). From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Koehn et al., 2007). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. The resulting decoding model has 14 features to be trained. 5 Results As seen in tables 1 and 2, the evaluation metric we use during training has a substantial impact on performance as measured by the various other metrics. There is a clear block structure where the best class of metrics to train on is the same class that is used during evaluation. Within this block structure, we make three primary observations. First, </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael J Denkowski</author>
</authors>
<title>The METEOR metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<volume>23</volume>
<contexts>
<context position="2752" citStr="Lavie and Denkowski, 2009" startWordPosition="437" endWordPosition="440">n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate (TER) (Snover et al., 2006; Snover et al., 2009) and METEOR&apos; (Lavie and Denkowski, 2009) perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU (Snover et al., 2009; Lavie and Denkowski, 2009; Przybocki et al., 2008; Snover et al., 2006). Their better correlations suggest that we might obtain higher quality translations by making use of these new metrics when training our models. We expect that training on a specific metric will produce the best performing model according to that met&apos;METEOR: Metric for Evaluation of Translation with Explicit ORderin</context>
<context position="6864" citStr="Lavie and Denkowski, 2009" startWordPosition="1090" endWordPosition="1093"> number of shortcomings. Since the order of matching n-grams is ignored, n-grams in a translation can be randomly rearranged around non-matching material or other n-gram breaks without harming the score. BLEU also does not explicitly check whether information is missing from the candidate translations, as it only examines what fraction of candidate translation ngrams are in the references and not what fraction of references n-grams are in the candidates (i.e., BLEU ignores n-gram recall). Finally, the metric does not account for words and phrases that have similar meanings. 2.2 METEOR METEOR (Lavie and Denkowski, 2009) computes a one-to-one alignment between matching words in a candidate translation and a reference. If a word matches multiple other words, preference is given to the alignment that reorders the words the least, with the amount of reordering measured by the number of crossing alignments. Alignments are first generated for exact matches between words. Additional alignments are created by repeatedly running the alignment procedure over unaligned words, first allowing for matches between word stems, and then allowing matches between words listed as synonyms in WordNet. From the final alignment, t</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael J. Denkowski. 2009. The METEOR metric for automatic evaluation of machine translation. Machine Translation, 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="13594" citStr="Liang et al., 2006" startWordPosition="2226" endWordPosition="2229"> and Arabic to English models.6 The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the standalone metrics used during training. 4.1 Arabic to English Our Arabic to English system was based on a well ranking 2009 NIST submission (Galley et al., 2009). The phrase table was extracted using all of the allowed resources for the constrained Arabic to English track. Word alignment was performed using the Berkeley cross-EM aligner (Liang et al., 2006). Phrases were extracted using the grow heuristic (Koehn et al., 2003). However, we threw away all phrases that have a P(e|f) &lt; 0.0001 in order to reduce the size of the phrase table. From the aligned data, we also extracted a hierarchical reordering model that is similar to popular lexical reordering models (Koehn et al., 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT. 6Given the amount of time required to train a TERpA model, we only present TERpA results for Chinese to English. Manning, 2008). A 5-gram language model</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucian Vlad Lita</author>
<author>Abe Ittycheriah</author>
<author>Salim Roukos</author>
<author>Nanda Kambhatla</author>
</authors>
<title>tRuEcasIng.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28585" citStr="Lita et al., 2003" startWordPosition="4756" endWordPosition="4759"> differences. duced by experts by having multiple workers complete each HIT and then combining their answers (Snow et al., 2008; Callison-Burch, 2009). We perform a pairwise comparison of the translations produced for the first 200 sentences of our Chinese to English test data (MT03) and our Arabic to English test data (dev07). The HITs consist of a pair of machine translated sentences and a single human generated reference translation. The reference is chosen at random from those available for each sentence. Capitalization of the translated sentences is restored using an HMM based truecaser (Lita et al., 2003). Turkers are instructed to “... select the machine translation generated sentence that is easiest to read and best conveys what is stated in the reference”. Differences between the two machine translations are emphasized by being underlined and bold faced.9 The resulting HITs are made available only to workers in the United States, as pilot experiments indicated this results in more consistent preference judgments. Three preference judgments are obtained for each pair of translations and are combined using weighted majority vote. As shown in table 5, in many cases the quality of the translati</context>
</contexts>
<marker>Lita, Ittycheriah, Roukos, Kambhatla, 2003</marker>
<rawString>Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and Nanda Kambhatla. 2003. tRuEcasIng. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Richard Schwartz</author>
</authors>
<title>TERp system description.</title>
<date>2008</date>
<booktitle>In MetricsMATR workshop at AMTA.</booktitle>
<marker>Snover, Madnani, Schwartz, 2008</marker>
<rawString>Bonnie Dorr Matthew Snover, Nitin Madnani and Richard Schwartz. 2008. TERp system description. In MetricsMATR workshop at AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>An evaluation tool for machine translation: Fast evaluation for MT research.</title>
<date>2000</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="8774" citStr="Nießen et al., 2000" startWordPosition="1415" endWordPosition="1418"> to tune the metric to human judgments on a specific language and variation of the evaluation task (e.g., ranking candidate translations vs. reproducing judgments of translations adequacy and fluency). 2.3 Translation Edit Rate TER (Snover et al., 2006) searches for the shortest sequence of edit operations needed to turn a candidate translation into one of the reference translations. The allowable edits are the insertion, deletion, and substitution of individual words and swaps of adjacent sequences of words. The swap operation differentiates TER from the simpler word error rate (WER) metric (Nießen et al., 2000), which only makes use of insertions, deletions, and substitutions. Swaps prevent phrase reorderings from being excessively penalized. Once the shortest sequence of operations is found,3 TER is calculated simply as the number of required edits divided by the reference translation length, or average reference translation length when multiple are available (4). min edits TER = (4) avg ref length TER-Plus (TERp) (Snover et al., 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric’s agreement with human judgments. TERp also introduces three new edit</context>
</contexts>
<marker>Nießen, Och, Ney, 2000</marker>
<rawString>Sonja Nießen, Franz Josef Och, and Hermann Ney. 2000. An evaluation tool for machine translation: Fast evaluation for MT research. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="14852" citStr="Och and Ney, 2003" startWordPosition="2439" endWordPosition="2442">ing toolkit (Stolcke, 2002) using all of the English material from the parallel data employed to train the phrase table as well as Xinhua Chinese English Parallel News (LDC2002E18).7 The resulting decoding model has 16 features that are optimized during MERT. 4.2 Chinese to English For our Chinese to English system, our phrase table was built using 1,140,693 sentence pairs sampled from the GALE Y2 training data. The Chinese sentences were word segmented using the 2008 version of Stanford Chinese Word Segmenter (Chang et al., 2008; Tseng et al., 2005). Phrases were extracted by running GIZA++ (Och and Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic (Koehn et al., 2003). From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Koehn et al., 2007). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. The resulting decoding model has 14 features to be trained. 5 Results As seen in tables 1 and 2, the evaluation metric we use during training has a substantia</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1632" citStr="Och, 2003" startWordPosition="263" endWordPosition="264"> based metrics like TER or WER. Human preferences for METEOR trained models varies depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 1 Introduction Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgment</context>
<context position="9776" citStr="Och, 2003" startWordPosition="1576" endWordPosition="1577">ngth TER-Plus (TERp) (Snover et al., 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric’s agreement with human judgments. TERp also introduces three new edit opertions: word stem matches, WordNet synonym matches, and multiword matches using a table of scored paraphrases. 3Since swaps prevent TER from being calculated exactly using dynamic programming, a beam search is used and this can overestimate the number of required edits. 3 MERT MERT is the standard technique for obtaining a machine translation model fit to a specific evaluation metric (Och, 2003). Learning such a model cannot be done using gradient methods since the value of the objective function only depends on the translation model’s argmax for each sentence in the tuning set. Typically, this optimization is performed as a series of line searches that examines the value of the evaluation metric at critical points where a new translation argmax becomes preferred by the model. Since the model score assigned to each candidate translation varies linearly with changes to the model parameters, it is possible to efficiently find the global minimum along any given search direction with onl</context>
<context position="30357" citStr="Och, 2003" startWordPosition="5045" endWordPosition="5046">uman judgments, is no better than TERp. For the Chinese to English results, there is a statistically significant human preference for translations that are produced by training to BLEU:4 and a marginally significant preferences for training to NIST over the default configuration of TERp. This contrasts sharply with earlier work showing that TER and TERp correlate better with human judgements than BLEU (Snover et al., 2009; Przybocki et al., 2008; Snover et al., 2006). While it is assumed that, by using MERT, “improved evaluation measures lead directly to improved machine translation quality” (Och, 2003), these results show improved correlations with human judgments are not always sufficient to establish that tuning to a metric will result in higher quality translations. In the Arabic results, we see a similar pattern where NIST is preferred to TERp, again with marginal signficance. Strangely, however, there is no real difference between TERp vs. BLEU:4. For Arabic, training to ranking METEOR is worse than BLEU:4, with the differences being very significant. The Arabic results also trend toward suggesting that BLEU:4 is better than either standard METEOR and METEOR α 0.5. However, for the Chi</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation. In</title>
<date>2002</date>
<tech>Technical report, NIST, http://nist.gov/speech/ tests/metricsmatr/2008/results/.</tech>
<contexts>
<context position="2063" citStr="Papineni et al., 2002" startWordPosition="331" endWordPosition="334">ole in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate</context>
<context position="5408" citStr="Papineni et al., 2002" startWordPosition="863" endWordPosition="866">he next sections introduce the metrics and our training procedure. We follow with two sets of core results, machine evaluation in section 5, and human evaluation in section 6. 2 Evaluation Metrics Designing good automated metrics for evaluating machine translations is challenging due to the variety of acceptable translations for each foreign sentence. Popular metrics produce scores primarily based on matching sequences of words in the system translation to those in one or more reference translations. The metrics primarily differ in how they account for reorderings and synonyms. 2.1 BLEU BLEU (Papineni et al., 2002) uses the percentage of n-grams found in machine translations that also occur in the reference translations. These n-gram precisions are calculated separately for different n2For example, BLEU computed without the brevity penalty would likely result in models that have a strong preference for generating pathologically short translations. gram lengths and then combined using a geometric mean. The score is then scaled by a brevity penalty if the candidate translations are shorter than the references, BP = min(1.0, e1−len(R)/len(T)). Equation 1 gives BLEU using n-grams up to length N for a corpus</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL. M. Przybocki, K. Peterson, and S. Bronsart. 2008. Official results of the “Metrics for MAchine TRanslation” Challenge (MetricsMATR08). Technical report, NIST, http://nist.gov/speech/ tests/metricsmatr/2008/results/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="2690" citStr="Snover et al., 2006" startWordPosition="427" endWordPosition="430">tively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate (TER) (Snover et al., 2006; Snover et al., 2009) and METEOR&apos; (Lavie and Denkowski, 2009) perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU (Snover et al., 2009; Lavie and Denkowski, 2009; Przybocki et al., 2008; Snover et al., 2006). Their better correlations suggest that we might obtain higher quality translations by making use of these new metrics when training our models. We expect that training on a specific metric will produce the best performing model according to that met&apos;METE</context>
<context position="8407" citStr="Snover et al., 2006" startWordPosition="1355" endWordPosition="1358">to be broken N BLEU:N = H n=1 N BP (1) 556 into to allow them to be rearranged with no crossing alignments, Pβ,γ = 1 − -y ( chunks )β. matches Fα = PR (2) αP + (1 − α)R METEORα,β,γ = Fα · Pβ,γ (3) Equation 3 gives the final METEOR score as the product of the unigram harmonic mean, Fα, and the fragmentation penalty, Pβ,γ. The free parameters α, Q, and -y can be used to tune the metric to human judgments on a specific language and variation of the evaluation task (e.g., ranking candidate translations vs. reproducing judgments of translations adequacy and fluency). 2.3 Translation Edit Rate TER (Snover et al., 2006) searches for the shortest sequence of edit operations needed to turn a candidate translation into one of the reference translations. The allowable edits are the insertion, deletion, and substitution of individual words and swaps of adjacent sequences of words. The swap operation differentiates TER from the simpler word error rate (WER) metric (Nießen et al., 2000), which only makes use of insertions, deletions, and substitutions. Swaps prevent phrase reorderings from being excessively penalized. Once the shortest sequence of operations is found,3 TER is calculated simply as the number of requ</context>
<context position="30218" citStr="Snover et al., 2006" startWordPosition="5021" endWordPosition="5024">her than the difference between each translation and the reference in order to avoid biasing evaluations toward edit distance metrics. 561 tuned to human judgments, is no better than TERp. For the Chinese to English results, there is a statistically significant human preference for translations that are produced by training to BLEU:4 and a marginally significant preferences for training to NIST over the default configuration of TERp. This contrasts sharply with earlier work showing that TER and TERp correlate better with human judgements than BLEU (Snover et al., 2009; Przybocki et al., 2008; Snover et al., 2006). While it is assumed that, by using MERT, “improved evaluation measures lead directly to improved machine translation quality” (Och, 2003), these results show improved correlations with human judgments are not always sufficient to establish that tuning to a metric will result in higher quality translations. In the Arabic results, we see a similar pattern where NIST is preferred to TERp, again with marginal signficance. Strangely, however, there is no real difference between TERp vs. BLEU:4. For Arabic, training to ranking METEOR is worse than BLEU:4, with the differences being very significan</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Fluency, adequacy, or HTER?: exploring different human judgments with a tunable MT metric.</title>
<date>2009</date>
<booktitle>In StatMT workshop at EACL).</booktitle>
<contexts>
<context position="2712" citStr="Snover et al., 2009" startWordPosition="431" endWordPosition="434">g translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate (TER) (Snover et al., 2006; Snover et al., 2009) and METEOR&apos; (Lavie and Denkowski, 2009) perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU (Snover et al., 2009; Lavie and Denkowski, 2009; Przybocki et al., 2008; Snover et al., 2006). Their better correlations suggest that we might obtain higher quality translations by making use of these new metrics when training our models. We expect that training on a specific metric will produce the best performing model according to that met&apos;METEOR: Metric for Evaluat</context>
<context position="9208" citStr="Snover et al., 2009" startWordPosition="1482" endWordPosition="1485">, and substitution of individual words and swaps of adjacent sequences of words. The swap operation differentiates TER from the simpler word error rate (WER) metric (Nießen et al., 2000), which only makes use of insertions, deletions, and substitutions. Swaps prevent phrase reorderings from being excessively penalized. Once the shortest sequence of operations is found,3 TER is calculated simply as the number of required edits divided by the reference translation length, or average reference translation length when multiple are available (4). min edits TER = (4) avg ref length TER-Plus (TERp) (Snover et al., 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric’s agreement with human judgments. TERp also introduces three new edit opertions: word stem matches, WordNet synonym matches, and multiword matches using a table of scored paraphrases. 3Since swaps prevent TER from being calculated exactly using dynamic programming, a beam search is used and this can overestimate the number of required edits. 3 MERT MERT is the standard technique for obtaining a machine translation model fit to a specific evaluation metric (Och, 2003). Learning such a model cannot b</context>
<context position="30172" citStr="Snover et al., 2009" startWordPosition="5013" endWordPosition="5016"> differences between the two translations rather than the difference between each translation and the reference in order to avoid biasing evaluations toward edit distance metrics. 561 tuned to human judgments, is no better than TERp. For the Chinese to English results, there is a statistically significant human preference for translations that are produced by training to BLEU:4 and a marginally significant preferences for training to NIST over the default configuration of TERp. This contrasts sharply with earlier work showing that TER and TERp correlate better with human judgements than BLEU (Snover et al., 2009; Przybocki et al., 2008; Snover et al., 2006). While it is assumed that, by using MERT, “improved evaluation measures lead directly to improved machine translation quality” (Och, 2003), these results show improved correlations with human judgments are not always sufficient to establish that tuning to a metric will result in higher quality translations. In the Arabic results, we see a similar pattern where NIST is preferred to TERp, again with marginal signficance. Strangely, however, there is no real difference between TERp vs. BLEU:4. For Arabic, training to ranking METEOR is worse than BLEU</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or HTER?: exploring different human judgments with a tunable MT metric. In StatMT workshop at EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Cheap and fast – but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast – but is it good? Evaluating non-expert annotations for natural language tasks. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="14261" citStr="Stolcke, 2002" startWordPosition="2341" endWordPosition="2343">ehn et al., 2003). However, we threw away all phrases that have a P(e|f) &lt; 0.0001 in order to reduce the size of the phrase table. From the aligned data, we also extracted a hierarchical reordering model that is similar to popular lexical reordering models (Koehn et al., 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT. 6Given the amount of time required to train a TERpA model, we only present TERpA results for Chinese to English. Manning, 2008). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) using all of the English material from the parallel data employed to train the phrase table as well as Xinhua Chinese English Parallel News (LDC2002E18).7 The resulting decoding model has 16 features that are optimized during MERT. 4.2 Chinese to English For our Chinese to English system, our phrase table was built using 1,140,693 sentence pairs sampled from the GALE Y2 training data. The Chinese sentences were word segmented using the 2008 version of Stanford Chinese Word Segmenter (Chang et al., 2008; Tseng et al., 2005). Phrases were extracted by running GIZA++ (Och and Ney, 2003) in both </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In SIGHAN.</booktitle>
<contexts>
<context position="14790" citStr="Tseng et al., 2005" startWordPosition="2429" endWordPosition="2432">A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) using all of the English material from the parallel data employed to train the phrase table as well as Xinhua Chinese English Parallel News (LDC2002E18).7 The resulting decoding model has 16 features that are optimized during MERT. 4.2 Chinese to English For our Chinese to English system, our phrase table was built using 1,140,693 sentence pairs sampled from the GALE Y2 training data. The Chinese sentences were word segmented using the 2008 version of Stanford Chinese Word Segmenter (Chang et al., 2008; Tseng et al., 2005). Phrases were extracted by running GIZA++ (Och and Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic (Koehn et al., 2003). From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Koehn et al., 2007). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. The resulting decoding model has 14 features to be trained. 5 Results As seen in tables 1 and 2,</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher D. Manning. 2005. A conditional random field word segmenter. In SIGHAN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1725" citStr="Watanabe et al., 2007" startWordPosition="276" endWordPosition="279"> depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 1 Introduction Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Feasibility of human-in-the-loop minimum error rate training.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>52--61</pages>
<contexts>
<context position="32983" citStr="Zaidan and Callison-Burch, 2009" startWordPosition="5487" endWordPosition="5490">ture of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al., 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. Human results indicate that edit distance trained models such as WER and TERp tend to produce lower quality translations than BLEU or NIST trained models. Tuning to METEOR works reasonably well for Chinese, but is not a good choice for Arabic. We suspect that the newer RYPT metric (Zaidan and Callison-Burch, 2009), which directly makes use of human adequacy judgements of substrings, would obtain better human results than the automated metrics presented here. However, like other metrics, we expect performance gains still will be sensitive to how the mechanics of the metric interact with the structure and feature set of the decoding model being used. BLEU and NIST’s strong showing in both the machine and human evaluation results indicates that they are still the best general choice for training model parameters. We emphasize that improved metric correlations with human judgments do not imply that models </context>
</contexts>
<marker>Zaidan, Callison-Burch, 2009</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2009. Feasibility of human-in-the-loop minimum error rate training. In EMNLP, pages 52–61, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>