<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.997558">
Distributed Word Clustering for Large Scale Class-Based
Language Modeling in Machine Translation
</title>
<author confidence="0.995719">
Jakob Uszkoreit* Thorsten Brants
</author>
<affiliation confidence="0.947412">
Google, Inc.
</affiliation>
<address confidence="0.788901">
1600 Amphitheatre Parkway
Mountain View, CA 94303, USA
</address>
<email confidence="0.999657">
{uszkoreit,brants}@google.com
</email>
<sectionHeader confidence="0.995673" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993935">
In statistical language modeling, one technique
to reduce the problematic effects of data spar-
sity is to partition the vocabulary into equiva-
lence classes. In this paper we investigate the
effects of applying such a technique to higher-
order n-gram models trained on large corpora.
We introduce a modification of the exchange
clustering algorithm with improved efficiency
for certain partially class-based models and a
distributed version of this algorithm to effi-
ciently obtain automatic word classifications
for large vocabularies (&gt;1 million words) us-
ing such large training corpora (&gt;30 billion to-
kens). The resulting clusterings are then used
in training partially class-based language mod-
els. We show that combining them with word-
based n-gram models in the log-linear model
of a state-of-the-art statistical machine trans-
lation system leads to improvements in trans-
lation quality as indicated by the BLEU score.
</bodyText>
<sectionHeader confidence="0.998523" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996875">
A statistical language model assigns a probability
P(w) to any given string of words wm1 = w1, ..., wm.
In the case of n-gram language models this is done
by factoring the probability:
</bodyText>
<footnote confidence="0.7652663">
P(wi|wi−1
1 )
and making a Markov assumption by approximating
this by:
p(wi|wi−1
i−n+1)
Even after making the Markov assumption and thus
treating all strings of preceding words as equal which
* Parts of this research were conducted while the author
studied at the Berlin Institute of Technology
</footnote>
<bodyText confidence="0.83579625">
do not differ in the last n − 1 words, one problem n-
gram language models suffer from is that the training
data is too sparse to reliably estimate all conditional
probabilities P(wi|wi−1
</bodyText>
<equation confidence="0.757425">
1 ).
</equation>
<bodyText confidence="0.999560861111111">
Class-based n-gram models are intended to help
overcome this data sparsity problem by grouping
words into equivalence classes rather than treating
them as distinct words and thus reducing the num-
ber of parameters of the model (Brown et al., 1990).
They have often been shown to improve the per-
formance of speech recognition systems when com-
bined with word-based language models (Martin et
al., 1998; Whittaker and Woodland, 2001). However,
in the area of statistical machine translation, espe-
cially in the context of large training corpora, fewer
experiments with class-based n-gram models have
been performed with mixed success (Raab, 2006).
Class-based n-gram models have also been shown
to benefit from their reduced number of parameters
when scaling to higher-order n-grams (Goodman and
Gao, 2000), and even despite the increasing size and
decreasing sparsity of language model training cor-
pora (Brants et al., 2007), class-based n-gram mod-
els might lead to improvements when increasing the
n-gram order.
When training class-based n-gram models on large
corpora and large vocabularies, one of the prob-
lems arising is the scalability of the typical cluster-
ing algorithms used for obtaining the word classifi-
cation. Most often, variants of the exchange algo-
rithm (Kneser and Ney, 1993; Martinet al., 1998)
or the agglomerative clustering algorithm presented
in (Brown et al., 1990) are used, both of which have
prohibitive runtimes when clustering large vocabu-
laries on the basis of large training corpora with a
sufficiently high number of classes.
In this paper we introduce a modification of the ex-
change algorithm with improved efficiency and then
present a distributed version of the modified algo-
rithm, which makes it feasible to obtain word clas-
</bodyText>
<equation confidence="0.983604375">
P(wm 1 ) = �m
i=1
P(wi|wi−1
1 ) P
�m
i=1
�m
i=1
</equation>
<page confidence="0.979995">
755
</page>
<note confidence="0.709951">
Proceedings of ACL-08: HLT, pages 755–762,
</note>
<page confidence="0.537627">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999818428571429">
sifications using billions of tokens of training data.
We then show that using partially class-based lan-
guage models trained using the resulting classifica-
tions together with word-based language models in
a state-of-the-art statistical machine translation sys-
tem yields improvements despite the very large size
of the word-based models used.
</bodyText>
<sectionHeader confidence="0.966072" genericHeader="method">
2 Class-Based Language Modeling
</sectionHeader>
<bodyText confidence="0.989962833333334">
By partitioning all N„ words of the vocabulary into
Nc sets, with c(w) mapping a word onto its equiva-
lence class and c(wi) mapping a sequence of words
onto the sequence of their respective equivalence
classes, a typical class-based n-gram model approxi-
mates P(wi|wi−1
</bodyText>
<equation confidence="0.951388">
1 ) with the two following component
probabilities:
P(wi|wi 1) ≈ p0(wi|c(wi)) · p1(c(wi)|c(wi−&apos;+1))
(1)
</equation>
<bodyText confidence="0.996673076923077">
thus greatly reducing the number of parameters in
the model, since usually Nc is much smaller than
N„.
In the following, we will call this type of model a
two-sided class-based model, as both the history of
each n-gram, the sequence of words conditioned on,
as well as the predicted word are replaced by their
class.
Once a partition of the words in the vocabulary is
obtained, two-sided class-based models can be built
just like word-based n-gram models using existing
infrastructure. In addition, the size of the model is
usually greatly reduced.
</bodyText>
<subsectionHeader confidence="0.94247">
2.1 One-Sided Class-Based Models
</subsectionHeader>
<bodyText confidence="0.999957705882353">
Two-sided class-based models received most atten-
tion in the literature. However, several different
types of mixed word and class models have been
proposed for the purpose of improving the perfor-
mance of the model (Goodman, 2000), reducing its
size (Goodman and Gao, 2000) as well as lower-
ing the complexity of related clustering algorithms
(Whittaker and Woodland, 2001).
In (Emami and Jelinek, 2005) a clustering algo-
rithm is introduced which outputs a separate clus-
tering for each word position in a trigram model. In
the experimental evaluation, the authors observe the
largest improvements using a specific clustering for
the last word of each trigram but no clustering at
all for the first two word positions. Generalizing this
leads to arbitrary order class-based n-gram models
of the form:
</bodyText>
<equation confidence="0.996345">
P(wi|wi−1
1 ) ≈ p0(wi|c(wi)) · p1(c(wi)|wi−1
i−�+1) (2)
</equation>
<bodyText confidence="0.99983">
which we will call predictive class-based models in the
following sections.
</bodyText>
<sectionHeader confidence="0.991294" genericHeader="method">
3 Exchange Clustering
</sectionHeader>
<bodyText confidence="0.999901566666667">
One of the frequently used algorithms for automat-
ically obtaining partitions of the vocabulary is the
exchange algorithm (Kneser and Ney, 1993; Martin
et al., 1998). Beginning with an initial clustering,
the algorithm greedily maximizes the log likelihood
of a two-sided class bigram or trigram model as de-
scribed in Eq. (1) on the training data. Let V be
the set of words in the vocabulary and C the set of
classes. This then leads to the following optimization
criterion, where N(w) and N(c) denote the number
of occurrences of a word w or a class c in the training
data and N(c, d) denotes the number of occurrences
of some word in class c followed by a word in class d
in the training data:
The algorithm iterates over all words in the vo-
cabulary and tentatively moves each word to each
cluster. The change in the optimization criterion is
computed for each of these tentative moves and the
exchange leading to the highest increase in the opti-
mization criterion (3) is performed. This procedure
is then repeated until the algorithm reaches a local
optimum.
To be able to efficiently calculate the changes in
the optimization criterion when exchanging a word,
the counts in Eq. (3) are computed once for the ini-
tial clustering, stored, and afterwards updated when
a word is exchanged.
Often only a limited number of iterations are per-
formed, as letting the algorithm terminate in a local
optimum can be computationally impractical.
</bodyText>
<subsectionHeader confidence="0.995273">
3.1 Complexity
</subsectionHeader>
<bodyText confidence="0.999332625">
The implementation described in (Martin et al.,
1998) uses a memory saving technique introducing
a binary search into the complexity estimation. For
the sake of simplicity, we omit this detail in the fol-
lowing complexity analysis. We also do not employ
this optimization in our implementation.
The worst case complexity of the exchange algo-
rithm is quadratic in the number of classes. However,
</bodyText>
<equation confidence="0.9981505">
�C� = argmax N(w) · log N(w) +
C w∈V
+ � N(c, d) · log N(c, d) −
c∈C,d∈C
−2 · � N(c) · log N(c) (3)
c∈C
</equation>
<page confidence="0.991242">
756
</page>
<bodyText confidence="0.784370454545455">
Input: The fixed number of clusters Nc
Compute initial clustering
while clustering changed in last iteration do
forall w ∈ V do
forall c ∈ C do
move word w tentatively to cluster
c
compute updated optimization
criterion
move word w to cluster maximizing
optimization criterion
</bodyText>
<listItem confidence="0.189717">
Algorithm 1: Exchange Algorithm Outline
</listItem>
<bodyText confidence="0.9991682">
the number of occurrences of the word v followed by
some word in class c. Then the following optimiza-
tion criterion can be derived, with F(C) being the
log likelihood function of the predictive class bigram
model given a clustering C:
</bodyText>
<equation confidence="0.998456588235294">
F(C) _ X N(w) · log p(w|c(w))
w∈V
+ X N(v,c) · log p(c|v) (6)
v∈V,c∈C
X_ N(w)
w∈V N(w) · log N(c(w))
X N(v, N(v, c) · log N(v
+ )) (7)
v∈V,c∈C
X_ N(w) · log N(w)
w∈V
X− N(w) · logN(c(w))
w∈V
+ X N(v, c) · log N(v, c)
v∈V,c∈C
X− N(v,c) · log N(v) (8)
v∈V,c∈C
</equation>
<bodyText confidence="0.996364533333333">
the average case complexity can be reduced by up-
dating only the counts which are actually affected by
moving a word from one cluster to another. This can
be done by considering only those sets of clusters for
which N(w, c) &gt; 0 or N(c, w) &gt; 0 for a word w about
to be exchanged, both of which can be calculated ef-
ficiently when exchanging a word. The algorithm
scales linearly in the size of the vocabulary.
With Npre
c and Nsuc cdenoting the average number
of clusters preceding and succeeding another cluster,
B denoting the number of distinct bigrams in the
training corpus, and I denoting the number of itera-
tions, the worst case complexity of the algorithm is
in:
</bodyText>
<equation confidence="0.996358333333333">
O(I · (2 · B+Nv · Nc · (Npre
c + Nsuc
c )))
</equation>
<bodyText confidence="0.985736375">
When using large corpora with large numbers of
bigrams the number of required updates can increase
towards the quadratic upper bound as Npre
c and
Nsuc
c approach Nc. For a more detailed description
and further analysis of the complexity, the reader is
referred to (Martin et al., 1998).
</bodyText>
<sectionHeader confidence="0.985209" genericHeader="method">
4 Predictive Exchange Clustering
</sectionHeader>
<bodyText confidence="0.999689">
Modifying the exchange algorithm in order to opti-
mize the log likelihood of a predictive class bigram
model, leads to substantial performance improve-
ments, similar to those previously reported for an-
other type of one-sided class model in (Whittaker
and Woodland, 2001).
We use a predictive class bigram model as given
in Eq. (2), for which the maximum-likelihood prob-
ability estimates for the n-grams are given by their
relative frequencies:
</bodyText>
<equation confidence="0.9971285">
P(wi|wi−1
1 ) ≈ p0(wi|c(wi)) · p1(c(wi)|wi−1)(4)
N(wi−1,c(wi)) (5)
N(wi−1)
</equation>
<bodyText confidence="0.999536166666667">
where N(w) again denotes the number of occurrences
of the word w in the training corpus and N(v, c)
The very last summation of Eq. (8) now effectively
sums over all occurrences of all words and thus can-
cels out with the first summation of (8) which leads
to:
</bodyText>
<equation confidence="0.993072">
F(C) _ X N(v, c) · log N(v, c)
v∈V,c∈C
X− N(w) · logN(c(w)) (9)
w∈V
</equation>
<bodyText confidence="0.98607925">
In the first summation of Eq. (9), for a given word v
only the set of classes which contain at least one word
w for which N(v, w) &gt; 0 must be considered, denoted
by suc(v). The second summation is equivalent to
</bodyText>
<equation confidence="0.991307">
P
c∈C N(c) · log N(c). Thus the further simplified
criterion is:
F(C) _ X N(v, c) · log N(v, c)
v∈V,c∈suc(v)
X− N(c) · log N(c) (10)
c∈C
</equation>
<bodyText confidence="0.999939714285714">
When exchanging a word w between two classes c
and c0, only two summands of the second summation
of Eq. (10) are affected. The first summation can be
updated by iterating over all bigrams ending in the
exchanged word. Throughout one iteration of the
algorithm, in which for each word in the vocabulary
each possible move to another class is evaluated, this
</bodyText>
<equation confidence="0.892235">
N(wi)
_ N(c(wi)) ·
</equation>
<page confidence="0.955402">
757
</page>
<bodyText confidence="0.998833">
amounts to the number of distinct bigrams in the
training corpus B, times the number of clusters N,.
Thus the worst case complexity using the modified
optimization criterion is in:
</bodyText>
<equation confidence="0.61842">
O(I · N, · (B+N,,))
</equation>
<bodyText confidence="0.975177909090909">
Using this optimization criterion has two effects
on the complexity of the algorithm. The first dif-
ference is that in contrast to the exchange algorithm
using a two sided class-based bigram model in its op-
timization criterion, only two clusters are affected by
moving a word. Thus the algorithm scales linearly
in the number of classes. The second difference is
that B dominates the term B + N„ for most corpora
and scales far less than linearly with the vocabulary
size, providing a significant performance advantage
over the other optimization criterion, especially when
large vocabularies are used (Whittaker and Wood-
land, 2001).
For efficiency reasons, an exchange of a word be-
tween two clusters is separated into a remove and a
move procedure. In each iteration the remove proce-
dure only has to be called once for each word, while
for a given word move is called once for every clus-
ter to compute the consequences of the tentative ex-
changes. An outline of the move procedure is given
below. The remove procedure is similar.
Procedure MoveWord
</bodyText>
<sectionHeader confidence="0.991063" genericHeader="method">
5 Distributed Clustering
</sectionHeader>
<bodyText confidence="0.9999624">
When training on large corpora, even the modified
exchange algorithm would still require several days
if not weeks of CPU time for a sufficient number of
iterations.
To overcome this we introduce a novel distributed
exchange algorithm, based on the modified exchange
algorithm described in the previous section. The vo-
cabulary is randomly partitioned into sets of roughly
equal size. With each word w in one of these sets, all
words v preceding w in the corpus are stored with
the respective bigram count N(v, w).
The clusterings generated in each iteration as well
as the initial clustering are stored as the set of words
in each cluster, the total number of occurrences of
each cluster in the training corpus, and the list of
words preceeding each cluster. For each word w in
the predecessor list of a given cluster c, the number
of times w occurs in the training corpus before any
word in c, N(w, c), is also stored.
Together with the counts stored with the vocab-
ulary partitions, this allows for efficient updating of
the terms in Eq. (10).
The initial clustering together with all the required
counts is created in an initial iteration by assigning
the n-th most frequent word to cluster n mod N,.
While (Martinet al., 1998) and (Emami and Je-
linek, 2005) observe that the initial clustering does
not seem to have a noticeable effect on the quality
of the resulting clustering or the convergence rate,
the intuition behind this method of initialization is
that it is unlikely for the most frequent words to be
clustered together due to their high numbers of oc-
currences.
In each subsequent iteration each one of a num-
ber of workers is assigned one of the partitions of
the words in the vocabulary. After loading the cur-
rent clustering, it then randomly chooses a subset
of these words of a fixed size. For each of the se-
lected words the worker then determines to which
cluster the word is to be moved in order to maxi-
mize the increase in log likelihood, using the count
updating procedures described in the previous sec-
tion. All changes a worker makes to the clustering
are accumulated locally in delta data structures. At
the end of the iteration all deltas are merged and
applied to the previous clustering, resulting in the
complete clustering loaded in the next iteration.
This algorithm fits well into the MapReduce pro-
gramming model (Dean and Ghemawat, 2004) that
we used for our implementation.
</bodyText>
<subsectionHeader confidence="0.980042">
5.1 Convergence
</subsectionHeader>
<bodyText confidence="0.999102375">
While the greedy non-distributed exchange algo-
rithm is guaranteed to converge as each exchange
increases the log likelihood of the assumed bigram
model, this is not necessarily true for the distributed
exchange algorithm. This stems from the fact that
the change in log likelihood is calculated by each
worker under the assumption that no other changes
to the clustering are performed by other workers in
</bodyText>
<table confidence="0.719388866666667">
Input: A word w, and a destination cluster c
Result: The change in the optimization
criterion when moving w to cluster c
delta ← N(c) · log N(c)
N&apos;(c) ← N(c) − N(w)
delta ← delta − N&apos;(c) · log N&apos;(c)
if not a tentative move then
N(c) ← N&apos;(c)
forall v ∈ suc(w) do
delta ← delta − N(v, c) · log N(v, c)
N&apos;(v, c) ← N(v, c) − N(v, w)
delta ← delta + N&apos;(v, c) · log N&apos;(v, c)
if not a tentative move then
N(v, c) ← N&apos;(v, c)
return delta
</table>
<page confidence="0.991774">
758
</page>
<bodyText confidence="0.999904870370371">
this iteration. However, if in each iteration only a
rather small and randomly chosen subset of all words
are considered for exchange, the intuition is that the
remaining words still define the parameters of each
cluster well enough for the algorithm to converge.
In (Emami and Jelinek, 2005) the authors observe
that only considering a subset of the vocabulary of
half the size of the complete vocabulary in each it-
eration does not affect the time required by the ex-
change algorithm to converge. Yet each iteration is
sped up by approximately a factor of two. The qual-
ity of class-based models trained using the result-
ing clusterings did not differ noticeably from those
trained using clusterings for which the full vocabu-
lary was considered in each iteration. Our experi-
ments showed that this also seems to be the case for
the distributed exchange algorithm. While consider-
ing very large subsets of the vocabulary in each iter-
ation can cause the algorithm to not converge at all,
considering only a very small fraction of the words
for exchange will increase the number of iterations
required to converge. In experiments we empirically
determined that choosing a subset of roughly a third
of the size of the full vocabulary is a good balance in
this trade-off. We did not observe the algorithm to
not converge unless we used fractions above half of
the vocabulary size.
We typically ran the clustering for 20 to 30 itera-
tions after which the number of words exchanged in
each iteration starts to stabilize at less than 5 per-
cent of the vocabulary size. Figure 1 shows the num-
ber of words exchanged in each of 34 iterations when
clustering the approximately 300,000 word vocabu-
lary of the Arabic side of the English-Arabic parallel
training data into 512 and 2,048 clusters.
Despite a steady reduction in the number of words
exchanged per iteration, we observed the conver-
gence in regards to log-likelihood to be far from
monotone. In our experiments we were able to
achieve significantly more monotone and faster con-
vergence by employing the following heuristic. As
described in Section 5, we start out the first itera-
tion with a random partition of the vocabulary into
subsets each assigned to a specific worker. However,
instead of keeping this assignment constant through-
out all iterations, after each iteration the vocabu-
lary is partitioned anew so that all words from any
given cluster are considered by the same worker in
the next iteration. The intuition behind this heuris-
tic is that as the clustering becomes more coherent,
the information each worker has about groups of sim-
ilar words is becoming increasingly accurate. In our
experiments this heuristic lead to almost monotone
convergence in log-likelihood. It also reduced the
</bodyText>
<figure confidence="0.9650775">
0 5 10 15 20 25 30 35
iteration
</figure>
<figureCaption confidence="0.99710475">
Figure 1: Number of words exchanged per iteration
when clustering the vocabulary of the Arabic side of
the English-Arabic parallel training data (347 million to-
kens).
</figureCaption>
<bodyText confidence="0.9985655">
number of iterations required to converge by up to a
factor of three.
</bodyText>
<subsectionHeader confidence="0.996109">
5.2 Resource Requirements
</subsectionHeader>
<bodyText confidence="0.99999045">
The runtime of the distributed exchange algorithm
depends highly on the number of distinct bigrams in
the training corpus. When clustering the approxi-
mately 1.5 million word vocabulary of a 405 million
token English corpus into 1,000 clusters, one itera-
tion takes approximately 5 minutes using 50 workers
based on standard hardware running the Linux oper-
ating system. When clustering the 0.5 million most
frequent words in the vocabulary of an English cor-
pus with 31 billion tokens into 1,000 clusters, one it-
eration takes approximately 30 minutes on 200 work-
ers.
When scaling up the vocabulary and corpus sizes,
the current bottleneck of our implementation is load-
ing the current clustering into memory. While the
memory requirements decrease with each iteration,
during the first few iterations a worker typically still
needs approximately 2 GB of memory to load the
clustering generated in the previous iteration when
training 1,000 clusters on the 31 billion token corpus.
</bodyText>
<sectionHeader confidence="0.999405" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999988375">
We trained a number of predictive class-based lan-
guage models on different Arabic and English cor-
pora using clusterings trained on the complete data
of the same corpus. We use the distributed training
and application infrastructure described in (Brants
et al., 2007) with modifications to allow the training
of predictive class-based models and their application
in the decoder of the machine translation system.
</bodyText>
<figure confidence="0.999325153846154">
512 clusters
2048 clusters
words exchanged 100000
90000
80000
70000
60000
50000
40000
30000
20000
10000
0
</figure>
<page confidence="0.995753">
759
</page>
<bodyText confidence="0.999859">
For all models used in our experiments, both word-
and class-based, the smoothing method used was
Stupid Backoff (Brants et al., 2007). Models with
Stupid Backoff return scores rather than normalized
probabilities, thus perplexities cannot be calculated
for these models. Instead we report BLEU scores
(Papineni et al., 2002) of the machine translation sys-
tem using different combinations of word- and class-
based models for translation tasks from English to
Arabic and Arabic to English.
</bodyText>
<subsectionHeader confidence="0.999541">
6.1 Training Data
</subsectionHeader>
<bodyText confidence="0.999778681818182">
For English we used three different training data sets:
en target: The English side of Arabic-English and
Chinese-English parallel data provided by LDC (405
million tokens).
en ldcnews: Consists of several English news data
sets provided by LDC (5 billion tokens).
en webnews: Consists of data collected up to De-
cember 2005 from web pages containing primarily
English news articles (31 billion tokens).
A fourth data set, en web, was used together with
the other three data sets to train the large word-
based model used in the second machine translation
experiment. This set consists of general web data
collected in January 2006 (2 trillion tokens).
For Arabic we used the following two different
training data sets:
ar gigaword: Consists of several Arabic news data
sets provided by LDC (629 million tokens).
ar webnews: Consists of data collected up to
December 2005 from web pages containing primarily
Arabic news articles (approximately 600 million
tokens).
</bodyText>
<subsectionHeader confidence="0.997893">
6.2 Machine Translation Results
</subsectionHeader>
<bodyText confidence="0.999759428571428">
Given a sentence f in the source language, the ma-
chine translation problem is to automatically pro-
duce a translation e� in the target language. In the
subsequent experiments, we use a phrase-based sta-
tistical machine translation system based on the log-
linear formulation of the problem described in (Och
and Ney, 2002):
</bodyText>
<equation confidence="0.997606666666667">
e� = argmax p(e|f)
e
amhm(e, f) (11)
</equation>
<bodyText confidence="0.999351777777778">
where {hm(e, f)} is a set of M feature functions and
{am} a set of weights. We use each predictive class-
based language model as well as a word-based model
as separate feature functions in the log-linear com-
bination in Eq. (11). The weights are trained using
minimum error rate training (Och, 2003) with BLEU
score as the objective function.
The dev and test data sets contain parts of the
2003, 2004 and 2005 Arabic NIST MT evaluation
sets among other parallel data. The blind test data
used is the “NIST” part of the 2006 Arabic-English
NIST MT evaluation set, and is not included in the
training data.
For the first experiment we trained predictive
class-based 5-gram models using clusterings with 64,
128, 256 and 512 clusters1 on the en target data. We
then added these models as additional features to
the log linear model of the Arabic-English machine
translation system. The word-based language model
used by the system in these experiments is a 5-gram
model also trained on the en target data set.
Table 1 shows the BLEU scores reached by the
translation system when combining the different
class-based models with the word-based model in
comparison to the BLEU scores by a system using
only the word-based model on the Arabic-English
translation task.
</bodyText>
<table confidence="0.997495833333333">
dev test nist06
word-based only 0.4085 0.3498 0.5088
64 clusters 0.4122 0.3514 0.5114
128 clusters 0.4142 0.3530 0.5109
256 clusters 0.4141 0.3536 0.5076
512 clusters 0.4120 0.3504 0.5140
</table>
<tableCaption confidence="0.984405">
Table 1: BLEU scores of the Arabic English system using
models trained on the English en target data set
</tableCaption>
<bodyText confidence="0.999836125">
Adding the class-based models leads to small im-
provements in BLEU score, with the highest im-
provements for both dev and nist06 being statisti-
cally significant 2.
In the next experiment we used two predictive
class-based models, a 5-gram model with 512 clusters
trained on the en target data set and a 6-gram model
also using 512 clusters trained on the en ldcnews
data set. We used these models in addition to
a word-based 6-gram model created by combining
models trained on all four English data sets.
Table 2 shows the BLEU scores of the machine
translation system using only this word-based model,
the scores after adding the class-based model trained
on the en target data set and when using all three
models.
</bodyText>
<footnote confidence="0.9848056">
1The beginning of sentence, end of sentence and unkown
word tokens were each treated as separate clusters
2Differences of more than 0.0051 are statistically significant
at the 0.05 level using bootstrap resampling (Noreen, 1989;
Koehn, 2004)
</footnote>
<equation confidence="0.7242194">
= argmax
e
M
E
m=1
</equation>
<page confidence="0.97982">
760
</page>
<table confidence="0.99729125">
dev test nist06
word-based only 0.4677 0.4007 0.5672
with en target 0.4682 0.4022 0.5707
all three models 0.4690 0.4059 0.5748
</table>
<tableCaption confidence="0.9969695">
Table 2: BLEU scores of the Arabic English system using
models trained on various data sets
</tableCaption>
<bodyText confidence="0.998645777777778">
For our experiment with the English Arabic trans-
lation task we trained two 5-gram predictive class-
based models with 512 clusters on the Arabic
ar gigaword and ar webnews data sets. The word-
based Arabic 5-gram model we used was created
by combining models trained on the Arabic side of
the parallel training data (347 million tokens), the
ar gigaword and ar webnews data sets, and addi-
tional Arabic web data.
</bodyText>
<table confidence="0.608941">
dev test nist06
word-based only 0.2207 0.2174 0.3033
with ar webnews 0.2237 0.2136 0.3045
all three models 0.2257 0.2260 0.3318
</table>
<tableCaption confidence="0.9955495">
Table 3: BLEU scores of the English Arabic system using
models trained on various data sets
</tableCaption>
<bodyText confidence="0.999848181818182">
As shown in Table 3, adding the predictive class-
based model trained on the ar webnews data set
leads to small improvements in dev and nist06
scores but causes the test score to decrease. How-
ever, adding the class-based model trained on the
ar gigaword data set to the other class-based and the
word-based model results in further improvement of
the dev score, but also in large improvements of the
test and nist06 scores.
We performed experiments to eliminate the pos-
sibility of data overlap between the training data
and the machine translation test data as cause for
the large improvements. In addition, our experi-
ments showed that when there is overlap between
the training and test data, the class-based models
lead to lower scores as long as they are trained only
on data also used for training the word-based model.
One explanation could be that the domain of the
ar gigaword corpus is much closer to the domain of
the test data than that of other training data sets
used. However, further investigation is required to
explain the improvements.
</bodyText>
<subsectionHeader confidence="0.980775">
6.3 Clusters
</subsectionHeader>
<bodyText confidence="0.9998166">
The clusters produced by the distributed algorithm
vary in their size and number of occurrences. In
a clustering of the en target data set with 1,024
clusters, the cluster sizes follow a typical long-
tailed distribution with the smallest cluster contain-
</bodyText>
<table confidence="0.882946870967742">
Bai Bi Bu Cai Cao Chang Chen Cheng Chou Chuang Cui Dai
Deng Ding Du Duan Fan Fu Gao Ge Geng Gong Gu Guan
Han Hou Hsiao Hsieh Hsu Hu Huang Huo Jiang Jiao Juan
Kang Kuang Kuo Li Liang Liao Lin Liu Lu Luo Mao Meets
Meng Mi Miao Mu Niu Pang Pi Pu Qian Qiao Qiu Qu Ren
Run Shan Shang Shen Si Song Su Sui Sun Tan Tang Tian Tu
Wang Wu Xie Xiong Xu Yang Yao Ye Yin Zeng Zhang Zhao
Zheng Zhou Zhu Zhuang Zou
% PERCENT cents percent
approvals bonus cash concessions cooperatives credit disburse-
ments dividends donations earnings emoluments entitlements
expenditure expenditures fund funding funds grants income
incomes inflation lending liquidity loan loans mortgage mort-
gages overhead payroll pension pensions portfolio profits pro-
tectionism quotas receipts receivables remittances remunera-
tion rent rents returns revenue revenues salaries salary savings
spending subscription subsidies subsidy surplus surpluses tax
taxation taxes tonnage tuition turnover wage wages
Abby Abigail Agnes Alexandra Alice Amanda Amy Andrea
Angela Ann Anna Anne Annette Becky Beth Betsy Bonnie
Brenda Carla Carol Carole Caroline Carolyn Carrie Catherine
Cathy Cheryl Christina Christine Cindy Claire Clare Claudia
Colleen Cristina Cynthia Danielle Daphne Dawn Debbie Deb-
orah Denise Diane Dina Dolores Donna Doris Edna Eileen
Elaine Eleanor Elena Elisabeth Ellen Emily Erica Erin Esther
Evelyn Felicia Felicity Flora Frances Gail Gertrude Gillian
Gina Ginger Gladys Gloria Gwen Harriet Heather Helen Hi-
lary Irene Isabel Jane Janice Jeanne Jennifer Jenny Jessica
Jo Joan Joanna Joanne Jodie Josie Judith Judy Julia Julie
Karen Kate Katherine Kathleen Kathryn Kathy Katie Kim-
berly Kirsten Kristen Kristin Laura Laurie Leah Lena Lil-
lian Linda Lisa Liz Liza Lois Loretta Lori Lorraine Louise
Lynne Marcia Margaret Maria Marian Marianne Marilyn Mar-
jorie Marsha Mary Maureen Meg Melanie Melinda Melissa
Merle Michele Michelle Miriam Molly Nan Nancy Naomi Na-
talie Nina Nora Norma Olivia Pam Pamela Patricia Patti
Paula Pauline Peggy Phyllis Rachel Rebecca Regina Renee
Rita Roberta Rosemary Sabrina Sally Samantha Sarah Selena
Sheila Shelley Sherry Shirley Sonia Stacy Stephanie Sue Su-
sanne Suzanne Suzy Sylvia Tammy Teresa Teri Terri Theresa
Tina Toni Tracey Ursula Valerie Vanessa Veronica Vicki Vi-
vian Wendy Yolanda Yvonne
almonds apple apples asparagus avocado bacon bananas bar-
ley basil bean beans beets berries berry boneless broccoli
cabbage carrot carrots celery cherries cherry chile chiles chili
chilies chives cilantro citrus cranberries cranberry cucumber
cucumbers dill doughnuts egg eggplant eggs elk evergreen fen-
nel figs flowers fruit fruits garlic ginger grapefruit grasses herb
herbs jalapeno Jell-O lemon lemons lettuce lime lions mac-
aroni mango maple melon mint mozzarella mushrooms oak
oaks olives onion onions orange oranges orchids oregano oys-
ter parsley pasta pastries pea peach peaches peanuts pear
pears peas pecan pecans perennials pickles pine pineapple
pines plum pumpkin pumpkins raspberries raspberry rice rose-
mary roses sage salsa scallions scallops seasonings seaweed
shallots shrimp shrubs spaghetti spices spinach strawberries
strawberry thyme tomato tomatoes truffles tulips turtles wal-
nut walnuts watermelon wildflowers zucchini
mid-April mid-August mid-December mid-February mid-
January mid-July mid-June mid-March mid-May mid-
November mid-October mid-September mid-afternoon
midafternoon midmorning midsummer
</table>
<tableCaption confidence="0.998471">
Table 4: Examples of clusters
</tableCaption>
<page confidence="0.995648">
761
</page>
<bodyText confidence="0.99989925">
ing 13 words and the largest cluster containing 20,396
words. Table 4 shows some examples of the gener-
ated clusters. For each cluster we list all words oc-
curring more than 1,000 times in the corpus.
</bodyText>
<sectionHeader confidence="0.992497" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999992413793103">
In this paper, we have introduced an efficient, dis-
tributed clustering algorithm for obtaining word clas-
sifications for predictive class-based language models
with which we were able to use billions of tokens of
training data to obtain classifications for millions of
words in relatively short amounts of time.
The experiments presented show that predictive
class-based models trained using the obtained word
classifications can improve the quality of a state-of-
the-art machine translation system as indicated by
the BLEU score in both translation tasks. When
using predictive class-based models in combination
with a word-based language model trained on very
large amounts of data, the improvements continue to
be statistically significant on the test and nist06 sets.
We conclude that even despite the large amounts of
data used to train the large word-based model in
our second experiment, class-based language models
are still an effective tool to ease the effects of data
sparsity.
We furthermore expect to be able to increase the
gains resulting from using class-based models by
using more sophisticated techniques for combining
them with word-based models such as linear inter-
polations of word- and class-based models with coef-
ficients depending on the frequency of the history.
Another interesting direction of further research is
to evaluate the use of the presented clustering tech-
nique for language model size reduction.
</bodyText>
<sectionHeader confidence="0.998394" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999757545454545">
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language models
in machine translation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing and on Computational Natural Language
Learning (EMNLP-CoNLL), pages 858–867, Prague,
Czech Republic.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de
Souza, Jennifer C. Lai, and Robert L. Mercer. 1990.
Class-based n-gram models of natural language. Com-
putational Linguistics, 18(4):467–479.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce:
Simplified data processing on large clusters. In Pro-
ceedings of the Sixth Symposium on Operating System
Design and Implementation (OSDI-04), San Francisco,
CA, USA.
Ahmad Emami and Frederick Jelinek. 2005. Ran-
dom clusterings for language modeling. In Proceedings
of the IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), Philadelphia,
PA, USA.
Joshua Goodman and Jianfeng Gao. 2000. Language
model size reduction by pruning and clustering. In
Proceedings of the IEEE International Conference on
Spoken Language Processing (ICSLP), Beijing, China.
Joshua Goodman. 2000. A bit of progress in language
modeling. Technical report, Microsoft Research.
Reinherd Kneser and Hermann Ney. 1993. Improved
clustering techniques for class-based statistical lan-
guage modelling. In Proceedings of the 3rd European
Conference on Speech Communication and Technology,
pages 973–976, Berlin, Germany.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing (EMNLP), Barcelona, Spain.
Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
Speech Communication, 24:19–37.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley &amp; Sons, New York.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 295–302, Philadelphia, PA,
USA.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160–167, Sapporo,
Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 311–318, Philadelphia,
PA, USA.
Martin Raab. 2006. Language model techniques in ma-
chine translation. Master’s thesis, Universitat Karl-
sruhe / Carnegie Mellon University.
E. W. D. Whittaker and P. C. Woodland. 2001. Effi-
cient class-based language modelling for very large vo-
cabularies. In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 545–548, Salt Lake City, UT, USA.
</reference>
<page confidence="0.996624">
762
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931949">
<title confidence="0.9964865">Distributed Word Clustering for Large Scale Class-Based Language Modeling in Machine Translation</title>
<author confidence="0.999237">Thorsten Brants</author>
<affiliation confidence="0.976059">Google, Inc.</affiliation>
<address confidence="0.9984925">1600 Amphitheatre Parkway Mountain View, CA 94303, USA</address>
<abstract confidence="0.998328952380952">In statistical language modeling, one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes. In this paper we investigate the effects of applying such a technique to highermodels trained on large corpora. We introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially class-based models and a distributed version of this algorithm to efficiently obtain automatic word classifications large vocabularies million words) ussuch large training corpora billion tokens). The resulting clusterings are then used in training partially class-based language models. We show that combining them with wordmodels in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and on Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>858--867</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2760" citStr="Brants et al., 2007" startWordPosition="425" endWordPosition="428">e of speech recognition systems when combined with word-based language models (Martin et al., 1998; Whittaker and Woodland, 2001). However, in the area of statistical machine translation, especially in the context of large training corpora, fewer experiments with class-based n-gram models have been performed with mixed success (Raab, 2006). Class-based n-gram models have also been shown to benefit from their reduced number of parameters when scaling to higher-order n-grams (Goodman and Gao, 2000), and even despite the increasing size and decreasing sparsity of language model training corpora (Brants et al., 2007), class-based n-gram models might lead to improvements when increasing the n-gram order. When training class-based n-gram models on large corpora and large vocabularies, one of the problems arising is the scalability of the typical clustering algorithms used for obtaining the word classification. Most often, variants of the exchange algorithm (Kneser and Ney, 1993; Martinet al., 1998) or the agglomerative clustering algorithm presented in (Brown et al., 1990) are used, both of which have prohibitive runtimes when clustering large vocabularies on the basis of large training corpora with a suffi</context>
<context position="20244" citStr="Brants et al., 2007" startWordPosition="3421" endWordPosition="3424">f our implementation is loading the current clustering into memory. While the memory requirements decrease with each iteration, during the first few iterations a worker typically still needs approximately 2 GB of memory to load the clustering generated in the previous iteration when training 1,000 clusters on the 31 billion token corpus. 6 Experiments We trained a number of predictive class-based language models on different Arabic and English corpora using clusterings trained on the complete data of the same corpus. We use the distributed training and application infrastructure described in (Brants et al., 2007) with modifications to allow the training of predictive class-based models and their application in the decoder of the machine translation system. 512 clusters 2048 clusters words exchanged 100000 90000 80000 70000 60000 50000 40000 30000 20000 10000 0 759 For all models used in our experiments, both wordand class-based, the smoothing method used was Stupid Backoff (Brants et al., 2007). Models with Stupid Backoff return scores rather than normalized probabilities, thus perplexities cannot be calculated for these models. Instead we report BLEU scores (Papineni et al., 2002) of the machine tran</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and on Computational Natural Language Learning (EMNLP-CoNLL), pages 858–867, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V de Souza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<marker>Brown, Pietra, de Souza, Lai, Mercer, 1990</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza, Jennifer C. Lai, and Robert L. Mercer. 1990. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>MapReduce: Simplified data processing on large clusters.</title>
<date>2004</date>
<booktitle>In Proceedings of the Sixth Symposium on Operating System Design and Implementation (OSDI-04),</booktitle>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="15058" citStr="Dean and Ghemawat, 2004" startWordPosition="2540" endWordPosition="2543">hen randomly chooses a subset of these words of a fixed size. For each of the selected words the worker then determines to which cluster the word is to be moved in order to maximize the increase in log likelihood, using the count updating procedures described in the previous section. All changes a worker makes to the clustering are accumulated locally in delta data structures. At the end of the iteration all deltas are merged and applied to the previous clustering, resulting in the complete clustering loaded in the next iteration. This algorithm fits well into the MapReduce programming model (Dean and Ghemawat, 2004) that we used for our implementation. 5.1 Convergence While the greedy non-distributed exchange algorithm is guaranteed to converge as each exchange increases the log likelihood of the assumed bigram model, this is not necessarily true for the distributed exchange algorithm. This stems from the fact that the change in log likelihood is calculated by each worker under the assumption that no other changes to the clustering are performed by other workers in Input: A word w, and a destination cluster c Result: The change in the optimization criterion when moving w to cluster c delta ← N(c) · log N</context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce: Simplified data processing on large clusters. In Proceedings of the Sixth Symposium on Operating System Design and Implementation (OSDI-04), San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmad Emami</author>
<author>Frederick Jelinek</author>
</authors>
<title>Random clusterings for language modeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="5498" citStr="Emami and Jelinek, 2005" startWordPosition="860" endWordPosition="863">bulary is obtained, two-sided class-based models can be built just like word-based n-gram models using existing infrastructure. In addition, the size of the model is usually greatly reduced. 2.1 One-Sided Class-Based Models Two-sided class-based models received most attention in the literature. However, several different types of mixed word and class models have been proposed for the purpose of improving the performance of the model (Goodman, 2000), reducing its size (Goodman and Gao, 2000) as well as lowering the complexity of related clustering algorithms (Whittaker and Woodland, 2001). In (Emami and Jelinek, 2005) a clustering algorithm is introduced which outputs a separate clustering for each word position in a trigram model. In the experimental evaluation, the authors observe the largest improvements using a specific clustering for the last word of each trigram but no clustering at all for the first two word positions. Generalizing this leads to arbitrary order class-based n-gram models of the form: P(wi|wi−1 1 ) ≈ p0(wi|c(wi)) · p1(c(wi)|wi−1 i−�+1) (2) which we will call predictive class-based models in the following sections. 3 Exchange Clustering One of the frequently used algorithms for automat</context>
<context position="13949" citStr="Emami and Jelinek, 2005" startWordPosition="2347" endWordPosition="2351">ter, the total number of occurrences of each cluster in the training corpus, and the list of words preceeding each cluster. For each word w in the predecessor list of a given cluster c, the number of times w occurs in the training corpus before any word in c, N(w, c), is also stored. Together with the counts stored with the vocabulary partitions, this allows for efficient updating of the terms in Eq. (10). The initial clustering together with all the required counts is created in an initial iteration by assigning the n-th most frequent word to cluster n mod N,. While (Martinet al., 1998) and (Emami and Jelinek, 2005) observe that the initial clustering does not seem to have a noticeable effect on the quality of the resulting clustering or the convergence rate, the intuition behind this method of initialization is that it is unlikely for the most frequent words to be clustered together due to their high numbers of occurrences. In each subsequent iteration each one of a number of workers is assigned one of the partitions of the words in the vocabulary. After loading the current clustering, it then randomly chooses a subset of these words of a fixed size. For each of the selected words the worker then determ</context>
<context position="16243" citStr="Emami and Jelinek, 2005" startWordPosition="2758" endWordPosition="2761">ing w to cluster c delta ← N(c) · log N(c) N&apos;(c) ← N(c) − N(w) delta ← delta − N&apos;(c) · log N&apos;(c) if not a tentative move then N(c) ← N&apos;(c) forall v ∈ suc(w) do delta ← delta − N(v, c) · log N(v, c) N&apos;(v, c) ← N(v, c) − N(v, w) delta ← delta + N&apos;(v, c) · log N&apos;(v, c) if not a tentative move then N(v, c) ← N&apos;(v, c) return delta 758 this iteration. However, if in each iteration only a rather small and randomly chosen subset of all words are considered for exchange, the intuition is that the remaining words still define the parameters of each cluster well enough for the algorithm to converge. In (Emami and Jelinek, 2005) the authors observe that only considering a subset of the vocabulary of half the size of the complete vocabulary in each iteration does not affect the time required by the exchange algorithm to converge. Yet each iteration is sped up by approximately a factor of two. The quality of class-based models trained using the resulting clusterings did not differ noticeably from those trained using clusterings for which the full vocabulary was considered in each iteration. Our experiments showed that this also seems to be the case for the distributed exchange algorithm. While considering very large su</context>
</contexts>
<marker>Emami, Jelinek, 2005</marker>
<rawString>Ahmad Emami and Frederick Jelinek. 2005. Random clusterings for language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
<author>Jianfeng Gao</author>
</authors>
<title>Language model size reduction by pruning and clustering.</title>
<date>2000</date>
<booktitle>In Proceedings of the IEEE International Conference on Spoken Language Processing (ICSLP),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="2641" citStr="Goodman and Gao, 2000" startWordPosition="406" endWordPosition="409">reducing the number of parameters of the model (Brown et al., 1990). They have often been shown to improve the performance of speech recognition systems when combined with word-based language models (Martin et al., 1998; Whittaker and Woodland, 2001). However, in the area of statistical machine translation, especially in the context of large training corpora, fewer experiments with class-based n-gram models have been performed with mixed success (Raab, 2006). Class-based n-gram models have also been shown to benefit from their reduced number of parameters when scaling to higher-order n-grams (Goodman and Gao, 2000), and even despite the increasing size and decreasing sparsity of language model training corpora (Brants et al., 2007), class-based n-gram models might lead to improvements when increasing the n-gram order. When training class-based n-gram models on large corpora and large vocabularies, one of the problems arising is the scalability of the typical clustering algorithms used for obtaining the word classification. Most often, variants of the exchange algorithm (Kneser and Ney, 1993; Martinet al., 1998) or the agglomerative clustering algorithm presented in (Brown et al., 1990) are used, both of</context>
<context position="5369" citStr="Goodman and Gao, 2000" startWordPosition="840" endWordPosition="843">e of words conditioned on, as well as the predicted word are replaced by their class. Once a partition of the words in the vocabulary is obtained, two-sided class-based models can be built just like word-based n-gram models using existing infrastructure. In addition, the size of the model is usually greatly reduced. 2.1 One-Sided Class-Based Models Two-sided class-based models received most attention in the literature. However, several different types of mixed word and class models have been proposed for the purpose of improving the performance of the model (Goodman, 2000), reducing its size (Goodman and Gao, 2000) as well as lowering the complexity of related clustering algorithms (Whittaker and Woodland, 2001). In (Emami and Jelinek, 2005) a clustering algorithm is introduced which outputs a separate clustering for each word position in a trigram model. In the experimental evaluation, the authors observe the largest improvements using a specific clustering for the last word of each trigram but no clustering at all for the first two word positions. Generalizing this leads to arbitrary order class-based n-gram models of the form: P(wi|wi−1 1 ) ≈ p0(wi|c(wi)) · p1(c(wi)|wi−1 i−�+1) (2) which we will call</context>
</contexts>
<marker>Goodman, Gao, 2000</marker>
<rawString>Joshua Goodman and Jianfeng Gao. 2000. Language model size reduction by pruning and clustering. In Proceedings of the IEEE International Conference on Spoken Language Processing (ICSLP), Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2000</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="5326" citStr="Goodman, 2000" startWordPosition="835" endWordPosition="836">history of each n-gram, the sequence of words conditioned on, as well as the predicted word are replaced by their class. Once a partition of the words in the vocabulary is obtained, two-sided class-based models can be built just like word-based n-gram models using existing infrastructure. In addition, the size of the model is usually greatly reduced. 2.1 One-Sided Class-Based Models Two-sided class-based models received most attention in the literature. However, several different types of mixed word and class models have been proposed for the purpose of improving the performance of the model (Goodman, 2000), reducing its size (Goodman and Gao, 2000) as well as lowering the complexity of related clustering algorithms (Whittaker and Woodland, 2001). In (Emami and Jelinek, 2005) a clustering algorithm is introduced which outputs a separate clustering for each word position in a trigram model. In the experimental evaluation, the authors observe the largest improvements using a specific clustering for the last word of each trigram but no clustering at all for the first two word positions. Generalizing this leads to arbitrary order class-based n-gram models of the form: P(wi|wi−1 1 ) ≈ p0(wi|c(wi)) · </context>
</contexts>
<marker>Goodman, 2000</marker>
<rawString>Joshua Goodman. 2000. A bit of progress in language modeling. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinherd Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved clustering techniques for class-based statistical language modelling.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd European Conference on Speech Communication and Technology,</booktitle>
<pages>973--976</pages>
<location>Berlin, Germany.</location>
<contexts>
<context position="3126" citStr="Kneser and Ney, 1993" startWordPosition="483" endWordPosition="486">ls have also been shown to benefit from their reduced number of parameters when scaling to higher-order n-grams (Goodman and Gao, 2000), and even despite the increasing size and decreasing sparsity of language model training corpora (Brants et al., 2007), class-based n-gram models might lead to improvements when increasing the n-gram order. When training class-based n-gram models on large corpora and large vocabularies, one of the problems arising is the scalability of the typical clustering algorithms used for obtaining the word classification. Most often, variants of the exchange algorithm (Kneser and Ney, 1993; Martinet al., 1998) or the agglomerative clustering algorithm presented in (Brown et al., 1990) are used, both of which have prohibitive runtimes when clustering large vocabularies on the basis of large training corpora with a sufficiently high number of classes. In this paper we introduce a modification of the exchange algorithm with improved efficiency and then present a distributed version of the modified algorithm, which makes it feasible to obtain word clasP(wm 1 ) = �m i=1 P(wi|wi−1 1 ) P �m i=1 �m i=1 755 Proceedings of ACL-08: HLT, pages 755–762, Columbus, Ohio, USA, June 2008. c�200</context>
<context position="6191" citStr="Kneser and Ney, 1993" startWordPosition="969" endWordPosition="972"> for each word position in a trigram model. In the experimental evaluation, the authors observe the largest improvements using a specific clustering for the last word of each trigram but no clustering at all for the first two word positions. Generalizing this leads to arbitrary order class-based n-gram models of the form: P(wi|wi−1 1 ) ≈ p0(wi|c(wi)) · p1(c(wi)|wi−1 i−�+1) (2) which we will call predictive class-based models in the following sections. 3 Exchange Clustering One of the frequently used algorithms for automatically obtaining partitions of the vocabulary is the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998). Beginning with an initial clustering, the algorithm greedily maximizes the log likelihood of a two-sided class bigram or trigram model as described in Eq. (1) on the training data. Let V be the set of words in the vocabulary and C the set of classes. This then leads to the following optimization criterion, where N(w) and N(c) denote the number of occurrences of a word w or a class c in the training data and N(c, d) denotes the number of occurrences of some word in class c followed by a word in class d in the training data: The algorithm iterates over all words in the vo</context>
</contexts>
<marker>Kneser, Ney, 1993</marker>
<rawString>Reinherd Kneser and Hermann Ney. 1993. Improved clustering techniques for class-based statistical language modelling. In Proceedings of the 3rd European Conference on Speech Communication and Technology, pages 973–976, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="24865" citStr="Koehn, 2004" startWordPosition="4179" endWordPosition="4180">ed on the en ldcnews data set. We used these models in addition to a word-based 6-gram model created by combining models trained on all four English data sets. Table 2 shows the BLEU scores of the machine translation system using only this word-based model, the scores after adding the class-based model trained on the en target data set and when using all three models. 1The beginning of sentence, end of sentence and unkown word tokens were each treated as separate clusters 2Differences of more than 0.0051 are statistically significant at the 0.05 level using bootstrap resampling (Noreen, 1989; Koehn, 2004) = argmax e M E m=1 760 dev test nist06 word-based only 0.4677 0.4007 0.5672 with en target 0.4682 0.4022 0.5707 all three models 0.4690 0.4059 0.5748 Table 2: BLEU scores of the Arabic English system using models trained on various data sets For our experiment with the English Arabic translation task we trained two 5-gram predictive classbased models with 512 clusters on the Arabic ar gigaword and ar webnews data sets. The wordbased Arabic 5-gram model we used was created by combining models trained on the Arabic side of the parallel training data (347 million tokens), the ar gigaword and ar </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Martin</author>
<author>Jorg Liermann</author>
<author>Hermann Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering. Speech Communication,</title>
<date>1998</date>
<pages>24--19</pages>
<contexts>
<context position="2238" citStr="Martin et al., 1998" startWordPosition="347" endWordPosition="350">erlin Institute of Technology do not differ in the last n − 1 words, one problem ngram language models suffer from is that the training data is too sparse to reliably estimate all conditional probabilities P(wi|wi−1 1 ). Class-based n-gram models are intended to help overcome this data sparsity problem by grouping words into equivalence classes rather than treating them as distinct words and thus reducing the number of parameters of the model (Brown et al., 1990). They have often been shown to improve the performance of speech recognition systems when combined with word-based language models (Martin et al., 1998; Whittaker and Woodland, 2001). However, in the area of statistical machine translation, especially in the context of large training corpora, fewer experiments with class-based n-gram models have been performed with mixed success (Raab, 2006). Class-based n-gram models have also been shown to benefit from their reduced number of parameters when scaling to higher-order n-grams (Goodman and Gao, 2000), and even despite the increasing size and decreasing sparsity of language model training corpora (Brants et al., 2007), class-based n-gram models might lead to improvements when increasing the n-g</context>
<context position="6213" citStr="Martin et al., 1998" startWordPosition="973" endWordPosition="976">n in a trigram model. In the experimental evaluation, the authors observe the largest improvements using a specific clustering for the last word of each trigram but no clustering at all for the first two word positions. Generalizing this leads to arbitrary order class-based n-gram models of the form: P(wi|wi−1 1 ) ≈ p0(wi|c(wi)) · p1(c(wi)|wi−1 i−�+1) (2) which we will call predictive class-based models in the following sections. 3 Exchange Clustering One of the frequently used algorithms for automatically obtaining partitions of the vocabulary is the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998). Beginning with an initial clustering, the algorithm greedily maximizes the log likelihood of a two-sided class bigram or trigram model as described in Eq. (1) on the training data. Let V be the set of words in the vocabulary and C the set of classes. This then leads to the following optimization criterion, where N(w) and N(c) denote the number of occurrences of a word w or a class c in the training data and N(c, d) denotes the number of occurrences of some word in class c followed by a word in class d in the training data: The algorithm iterates over all words in the vocabulary and tentative</context>
<context position="7552" citStr="Martin et al., 1998" startWordPosition="1204" endWordPosition="1207">ves and the exchange leading to the highest increase in the optimization criterion (3) is performed. This procedure is then repeated until the algorithm reaches a local optimum. To be able to efficiently calculate the changes in the optimization criterion when exchanging a word, the counts in Eq. (3) are computed once for the initial clustering, stored, and afterwards updated when a word is exchanged. Often only a limited number of iterations are performed, as letting the algorithm terminate in a local optimum can be computationally impractical. 3.1 Complexity The implementation described in (Martin et al., 1998) uses a memory saving technique introducing a binary search into the complexity estimation. For the sake of simplicity, we omit this detail in the following complexity analysis. We also do not employ this optimization in our implementation. The worst case complexity of the exchange algorithm is quadratic in the number of classes. However, �C� = argmax N(w) · log N(w) + C w∈V + � N(c, d) · log N(c, d) − c∈C,d∈C −2 · � N(c) · log N(c) (3) c∈C 756 Input: The fixed number of clusters Nc Compute initial clustering while clustering changed in last iteration do forall w ∈ V do forall c ∈ C do move wo</context>
<context position="9809" citStr="Martin et al., 1998" startWordPosition="1627" endWordPosition="1630"> the size of the vocabulary. With Npre c and Nsuc cdenoting the average number of clusters preceding and succeeding another cluster, B denoting the number of distinct bigrams in the training corpus, and I denoting the number of iterations, the worst case complexity of the algorithm is in: O(I · (2 · B+Nv · Nc · (Npre c + Nsuc c ))) When using large corpora with large numbers of bigrams the number of required updates can increase towards the quadratic upper bound as Npre c and Nsuc c approach Nc. For a more detailed description and further analysis of the complexity, the reader is referred to (Martin et al., 1998). 4 Predictive Exchange Clustering Modifying the exchange algorithm in order to optimize the log likelihood of a predictive class bigram model, leads to substantial performance improvements, similar to those previously reported for another type of one-sided class model in (Whittaker and Woodland, 2001). We use a predictive class bigram model as given in Eq. (2), for which the maximum-likelihood probability estimates for the n-grams are given by their relative frequencies: P(wi|wi−1 1 ) ≈ p0(wi|c(wi)) · p1(c(wi)|wi−1)(4) N(wi−1,c(wi)) (5) N(wi−1) where N(w) again denotes the number of occurrenc</context>
</contexts>
<marker>Martin, Liermann, Ney, 1998</marker>
<rawString>Sven Martin, Jorg Liermann, and Hermann Ney. 1998. Algorithms for bigram and trigram word clustering. Speech Communication, 24:19–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-Intensive Methods for Testing Hypotheses.</title>
<date>1989</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="24851" citStr="Noreen, 1989" startWordPosition="4177" endWordPosition="4178">clusters trained on the en ldcnews data set. We used these models in addition to a word-based 6-gram model created by combining models trained on all four English data sets. Table 2 shows the BLEU scores of the machine translation system using only this word-based model, the scores after adding the class-based model trained on the en target data set and when using all three models. 1The beginning of sentence, end of sentence and unkown word tokens were each treated as separate clusters 2Differences of more than 0.0051 are statistically significant at the 0.05 level using bootstrap resampling (Noreen, 1989; Koehn, 2004) = argmax e M E m=1 760 dev test nist06 word-based only 0.4677 0.4007 0.5672 with en target 0.4682 0.4022 0.5707 all three models 0.4690 0.4059 0.5748 Table 2: BLEU scores of the Arabic English system using models trained on various data sets For our experiment with the English Arabic translation task we trained two 5-gram predictive classbased models with 512 clusters on the Arabic ar gigaword and ar webnews data sets. The wordbased Arabic 5-gram model we used was created by combining models trained on the Arabic side of the parallel training data (347 million tokens), the ar gi</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-Intensive Methods for Testing Hypotheses. John Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="22317" citStr="Och and Ney, 2002" startWordPosition="3749" endWordPosition="3752">nt training data sets: ar gigaword: Consists of several Arabic news data sets provided by LDC (629 million tokens). ar webnews: Consists of data collected up to December 2005 from web pages containing primarily Arabic news articles (approximately 600 million tokens). 6.2 Machine Translation Results Given a sentence f in the source language, the machine translation problem is to automatically produce a translation e� in the target language. In the subsequent experiments, we use a phrase-based statistical machine translation system based on the loglinear formulation of the problem described in (Och and Ney, 2002): e� = argmax p(e|f) e amhm(e, f) (11) where {hm(e, f)} is a set of M feature functions and {am} a set of weights. We use each predictive classbased language model as well as a word-based model as separate feature functions in the log-linear combination in Eq. (11). The weights are trained using minimum error rate training (Och, 2003) with BLEU score as the objective function. The dev and test data sets contain parts of the 2003, 2004 and 2005 Arabic NIST MT evaluation sets among other parallel data. The blind test data used is the “NIST” part of the 2006 Arabic-English NIST MT evaluation set,</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 295–302, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="22653" citStr="Och, 2003" startWordPosition="3812" endWordPosition="3813">he machine translation problem is to automatically produce a translation e� in the target language. In the subsequent experiments, we use a phrase-based statistical machine translation system based on the loglinear formulation of the problem described in (Och and Ney, 2002): e� = argmax p(e|f) e amhm(e, f) (11) where {hm(e, f)} is a set of M feature functions and {am} a set of weights. We use each predictive classbased language model as well as a word-based model as separate feature functions in the log-linear combination in Eq. (11). The weights are trained using minimum error rate training (Och, 2003) with BLEU score as the objective function. The dev and test data sets contain parts of the 2003, 2004 and 2005 Arabic NIST MT evaluation sets among other parallel data. The blind test data used is the “NIST” part of the 2006 Arabic-English NIST MT evaluation set, and is not included in the training data. For the first experiment we trained predictive class-based 5-gram models using clusterings with 64, 128, 256 and 512 clusters1 on the en target data. We then added these models as additional features to the log linear model of the Arabic-English machine translation system. The word-based lang</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="20824" citStr="Papineni et al., 2002" startWordPosition="3509" endWordPosition="3512">ructure described in (Brants et al., 2007) with modifications to allow the training of predictive class-based models and their application in the decoder of the machine translation system. 512 clusters 2048 clusters words exchanged 100000 90000 80000 70000 60000 50000 40000 30000 20000 10000 0 759 For all models used in our experiments, both wordand class-based, the smoothing method used was Stupid Backoff (Brants et al., 2007). Models with Stupid Backoff return scores rather than normalized probabilities, thus perplexities cannot be calculated for these models. Instead we report BLEU scores (Papineni et al., 2002) of the machine translation system using different combinations of word- and classbased models for translation tasks from English to Arabic and Arabic to English. 6.1 Training Data For English we used three different training data sets: en target: The English side of Arabic-English and Chinese-English parallel data provided by LDC (405 million tokens). en ldcnews: Consists of several English news data sets provided by LDC (5 billion tokens). en webnews: Consists of data collected up to December 2005 from web pages containing primarily English news articles (31 billion tokens). A fourth data se</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Raab</author>
</authors>
<title>Language model techniques in machine translation.</title>
<date>2006</date>
<tech>Master’s thesis,</tech>
<institution>Universitat Karlsruhe / Carnegie Mellon University.</institution>
<contexts>
<context position="2481" citStr="Raab, 2006" startWordPosition="384" endWordPosition="385">intended to help overcome this data sparsity problem by grouping words into equivalence classes rather than treating them as distinct words and thus reducing the number of parameters of the model (Brown et al., 1990). They have often been shown to improve the performance of speech recognition systems when combined with word-based language models (Martin et al., 1998; Whittaker and Woodland, 2001). However, in the area of statistical machine translation, especially in the context of large training corpora, fewer experiments with class-based n-gram models have been performed with mixed success (Raab, 2006). Class-based n-gram models have also been shown to benefit from their reduced number of parameters when scaling to higher-order n-grams (Goodman and Gao, 2000), and even despite the increasing size and decreasing sparsity of language model training corpora (Brants et al., 2007), class-based n-gram models might lead to improvements when increasing the n-gram order. When training class-based n-gram models on large corpora and large vocabularies, one of the problems arising is the scalability of the typical clustering algorithms used for obtaining the word classification. Most often, variants of</context>
</contexts>
<marker>Raab, 2006</marker>
<rawString>Martin Raab. 2006. Language model techniques in machine translation. Master’s thesis, Universitat Karlsruhe / Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W D Whittaker</author>
<author>P C Woodland</author>
</authors>
<title>Efficient class-based language modelling for very large vocabularies.</title>
<date>2001</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>545--548</pages>
<location>Salt Lake City, UT, USA.</location>
<contexts>
<context position="2269" citStr="Whittaker and Woodland, 2001" startWordPosition="351" endWordPosition="354">chnology do not differ in the last n − 1 words, one problem ngram language models suffer from is that the training data is too sparse to reliably estimate all conditional probabilities P(wi|wi−1 1 ). Class-based n-gram models are intended to help overcome this data sparsity problem by grouping words into equivalence classes rather than treating them as distinct words and thus reducing the number of parameters of the model (Brown et al., 1990). They have often been shown to improve the performance of speech recognition systems when combined with word-based language models (Martin et al., 1998; Whittaker and Woodland, 2001). However, in the area of statistical machine translation, especially in the context of large training corpora, fewer experiments with class-based n-gram models have been performed with mixed success (Raab, 2006). Class-based n-gram models have also been shown to benefit from their reduced number of parameters when scaling to higher-order n-grams (Goodman and Gao, 2000), and even despite the increasing size and decreasing sparsity of language model training corpora (Brants et al., 2007), class-based n-gram models might lead to improvements when increasing the n-gram order. When training class-</context>
<context position="5468" citStr="Whittaker and Woodland, 2001" startWordPosition="855" endWordPosition="858"> partition of the words in the vocabulary is obtained, two-sided class-based models can be built just like word-based n-gram models using existing infrastructure. In addition, the size of the model is usually greatly reduced. 2.1 One-Sided Class-Based Models Two-sided class-based models received most attention in the literature. However, several different types of mixed word and class models have been proposed for the purpose of improving the performance of the model (Goodman, 2000), reducing its size (Goodman and Gao, 2000) as well as lowering the complexity of related clustering algorithms (Whittaker and Woodland, 2001). In (Emami and Jelinek, 2005) a clustering algorithm is introduced which outputs a separate clustering for each word position in a trigram model. In the experimental evaluation, the authors observe the largest improvements using a specific clustering for the last word of each trigram but no clustering at all for the first two word positions. Generalizing this leads to arbitrary order class-based n-gram models of the form: P(wi|wi−1 1 ) ≈ p0(wi|c(wi)) · p1(c(wi)|wi−1 i−�+1) (2) which we will call predictive class-based models in the following sections. 3 Exchange Clustering One of the frequent</context>
<context position="10112" citStr="Whittaker and Woodland, 2001" startWordPosition="1673" endWordPosition="1676">· (2 · B+Nv · Nc · (Npre c + Nsuc c ))) When using large corpora with large numbers of bigrams the number of required updates can increase towards the quadratic upper bound as Npre c and Nsuc c approach Nc. For a more detailed description and further analysis of the complexity, the reader is referred to (Martin et al., 1998). 4 Predictive Exchange Clustering Modifying the exchange algorithm in order to optimize the log likelihood of a predictive class bigram model, leads to substantial performance improvements, similar to those previously reported for another type of one-sided class model in (Whittaker and Woodland, 2001). We use a predictive class bigram model as given in Eq. (2), for which the maximum-likelihood probability estimates for the n-grams are given by their relative frequencies: P(wi|wi−1 1 ) ≈ p0(wi|c(wi)) · p1(c(wi)|wi−1)(4) N(wi−1,c(wi)) (5) N(wi−1) where N(w) again denotes the number of occurrences of the word w in the training corpus and N(v, c) The very last summation of Eq. (8) now effectively sums over all occurrences of all words and thus cancels out with the first summation of (8) which leads to: F(C) _ X N(v, c) · log N(v, c) v∈V,c∈C X− N(w) · logN(c(w)) (9) w∈V In the first summation o</context>
<context position="12248" citStr="Whittaker and Woodland, 2001" startWordPosition="2050" endWordPosition="2054">ing this optimization criterion has two effects on the complexity of the algorithm. The first difference is that in contrast to the exchange algorithm using a two sided class-based bigram model in its optimization criterion, only two clusters are affected by moving a word. Thus the algorithm scales linearly in the number of classes. The second difference is that B dominates the term B + N„ for most corpora and scales far less than linearly with the vocabulary size, providing a significant performance advantage over the other optimization criterion, especially when large vocabularies are used (Whittaker and Woodland, 2001). For efficiency reasons, an exchange of a word between two clusters is separated into a remove and a move procedure. In each iteration the remove procedure only has to be called once for each word, while for a given word move is called once for every cluster to compute the consequences of the tentative exchanges. An outline of the move procedure is given below. The remove procedure is similar. Procedure MoveWord 5 Distributed Clustering When training on large corpora, even the modified exchange algorithm would still require several days if not weeks of CPU time for a sufficient number of iter</context>
</contexts>
<marker>Whittaker, Woodland, 2001</marker>
<rawString>E. W. D. Whittaker and P. C. Woodland. 2001. Efficient class-based language modelling for very large vocabularies. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 545–548, Salt Lake City, UT, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>