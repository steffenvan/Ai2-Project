<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009873">
<title confidence="0.996917">
ECNU: Leveraging on Ensemble of Heterogeneous Features and
Information Enrichment for Cross Level Semantic Similarity Estimation
</title>
<author confidence="0.996888">
Tian Tian Zhu
</author>
<affiliation confidence="0.997623">
Department of Computer Science and
</affiliation>
<note confidence="0.632298">
Technology
East China Normal University
</note>
<email confidence="0.98428">
51111201046@ecnu.cn
</email>
<sectionHeader confidence="0.993443" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999352777777778">
This paper reports our submissions to the
Cross Level Semantic Similarity (CLSS)
task in SemEval 2014. We submitted
one Random Forest regression system on
each cross level text pair, i.e., Paragraph
to Sentence (P-S), Sentence to Phrase (S-
Ph), Phrase to Word (Ph-W) and Word
to Sense (W-Se). For text pairs on P-S
level and S-Ph level, we consider them as
sentences and extract heterogeneous types
of similarity features, i.e., string features,
knowledge based features, corpus based
features, syntactic features, machine trans-
lation based features, multi-level text fea-
tures, etc. For text pairs on Ph-W level
and W-Se level, due to lack of informa-
tion, most of these features are not ap-
plicable or available. To overcome this
problem, we propose several information
enrichment methods using WordNet syn-
onym and definition. Our systems rank the
2nd out of 18 teams both on Pearson cor-
relation (official rank) and Spearman rank
correlation. Specifically, our systems take
the second place on P-S level, S-Ph level
and Ph-W level and the 4th place on W-Se
level in terms of Pearson correlation.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997105">
Semantic similarity is an essential component of
many applications in Natural Language Process-
ing (NLP). Previous works often focus on text se-
mantic similarity on the same level, i.e., paragraph
to paragraph or sentence to sentence, and many ef-
fective text semantic measurements have been pro-
posed (Islam and Inkpen, 2008), (B¨ar et al., 2012),
</bodyText>
<footnote confidence="0.7818865">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<note confidence="0.74322425">
Man Lan*
Department of Computer Science and
Technology
East China Normal University
</note>
<email confidence="0.96048">
mlan@cs.ecnu.edu.cn*
</email>
<bodyText confidence="0.999819">
(Heilman and Madnani, 2012). However, in many
real world cases, the two texts may not always
be on the same level. The Cross Level Semantic
Similarity (CLSS) task in SemEval 2014 provides
a universal platform to measure the degree of se-
mantic equivalence between two texts across dif-
ferent levels. For each text pair on four cross lev-
els, i.e., Paragraph to Sentence (P-S), Sentence to
Phrase (S-Ph), Phrase to Word (Ph-W) and Word
to Sense (W-Se), participants are required to re-
turn a similarity score which ranges from 0 (no
relation) to 4 (semantic equivalence). We partici-
pate in all the four cross levels and take the second
place out of all 18 teams both on Pearson correla-
tion (official) and Spearman correlation ranks.
In this work, we present a supervised regres-
sion system for each cross level separately. For
P-S level and S-Ph level, we regard the paragraph
of P-S as a long sentence, and the phrase of S-
Ph as a short sentence. Then we use various types
of text similarity features including string features,
knowledge based features, corpus based features,
syntactic features, machine translation based fea-
tures, multi-level text features and so on, to cap-
ture the semantic similarity between two texts.
Some of these features are borrowed from our pre-
vious system in the Semantic Textual Similarity
(STS) task in *SEM Shared Task 2013 (Zhu and
Lan, 2013). Others followed the previous work
in (ˇSaric et al., 2012) and (Pilehvar et al., 2013).
For Ph-W level and W-Se level, since the text pairs
lack contextual information, for example, word or
sense alone no longer shares the property of sen-
tence, most features used in P-S level and S-Ph
level are not applicable or available. To overcome
the problem of insufficient information in word
and sense level, we propose several information
enrichment methods to extend information with
the aid of WordNet (Miller, 1995), which signif-
icantly improved the system performance.
The rest of this paper is organized as follows.
</bodyText>
<page confidence="0.976842">
265
</page>
<note confidence="0.729958">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 265–270,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.9999726">
Section 2 describes the similarity features used on
four cross levels in detail. Section 3 presents ex-
periments and the results of four cross levels on
training data and test data. Conclusions and future
work are given in Section 4.
</bodyText>
<sectionHeader confidence="0.823132" genericHeader="method">
2 Text Similarity Measurements
</sectionHeader>
<bodyText confidence="0.9999115">
To estimate the semantic similarity on P-S level
and S-Ph level, we treat the text pairs on both lev-
els as traditional semantic similarity computation
on sentence level and adopt 7 types of features,
i.e., string features, knowledge based features, cor-
pus based features, syntactic features, machine
translation based features, multi-level text features
and other features. All of them are borrowed
from previous work due to their superior perfor-
mance reported. For Ph-W level and W-Se level,
since word and sense alone cannot be treated as
sentence, we propose an information enrichment
method to extend original text with the help of
WordNet. Once the word or sense is enriched with
its synonym and its definition description, we can
thus adopt the previous features as well.
</bodyText>
<subsectionHeader confidence="0.991398">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999983363636364">
For P-S level and S-Ph level, we perform text pre-
processing before we extract semantic similarity
features. Firstly, the Stanford parser1 is used for
sentence tokenization and parsing. Specifically,
the tokens n’t and ’m are replaced with not and
am. Secondly, the Stanford POS Tagger2 is used
for POS tagging. Thirdly, we use Natural Lan-
guage Toolkit3 for WordNet based Lemmatiza-
tion, which lemmatizes the word to its nearest base
form that appears in WordNet, for example, was
is lemmatized as is rather than be.
</bodyText>
<subsectionHeader confidence="0.998467">
2.2 Features on P-S Level and S-Ph Level
</subsectionHeader>
<bodyText confidence="0.999940777777778">
We treat all text pairs of P-S level and S-Ph level
as sentences and then extract 7 types of similar-
ity features as below. Totally we get 52 similarity
features. Generally, these similarity features are
represented as numerical values.
String features. Intuitively, if two texts share
more strings, they are considered to be more se-
mantic similar. We extract 13 string based features
in consideration of the common sequence shared
</bodyText>
<footnote confidence="0.999916">
1http://nlp.stanford.edu/software/lex-parser.shtml
2http://nlp.stanford.edu/software/tagger.shtml
3http://nltk.org/
</footnote>
<bodyText confidence="0.998841509803922">
by two texts. We chose the Longest Common Se-
quence (LCS) feature (Zhu and Lan, 2013), the N-
gram Overlap feature (n=1,2,3) and the Weighted
Word Overlap feature (ˇSaric et al., 2012). All
these features are computed from original text
and from the processed text after lemmatization
as well. Besides, we also computed the N-gram
Overlap on character level, named Character N-
gram (n=2,3,4).
Knowledge based features. Knowledge based
similarity estimation relies on the semantic net-
work of words. In this work we used the knowl-
edge based features in our previous work (Zhu and
Lan, 2013), which include four word similarity
metrics based on WordNet: Path similarity (Banea
et al., 2012), WUP similarity (Wu and Palmer,
1994), LCH similarity (Leacock and Chodorow,
1998) and Lin similarity (Lin, 1998). Then two
strategies, i.e., the best alignment strategy and the
aggregation strategy, are employed to propagate
the word similarity to the text similarity. Totally
we get 8 knowledge based features.
Corpus based features. Latent Semantic Analy-
sis (LSA) (Landauer et al., 1997) is a widely used
corpus based measure when evaluating text simi-
larity. In this work we use the Vector Space Sen-
tence Similarity proposed by (ˇSaric et al., 2012),
which represents each sentence as a single distri-
butional vector by summing up the LSA vector of
each word in the sentence. Two corpora are used
to compute the LSA vector of words: New York
Times Annotated Corpus (NYT) and Wikipedia.
Besides, in consideration of different weights for
different words, they also calculated the weighted
LSA vector for each word. In addition, we use
the Co-occurrence Retrieval Model (CRM) feature
from our previous work (Zhu and Lan, 2013) as
another corpus-based feature. The CRM is calcu-
lated based on a notion of substitutability, that is,
the more appropriate it is to substitute word w1
in place of word w2 in a suitable natural language
task, the more semantically similar they are. At
last, 6 corpus based features are extracted.
Syntactic features. Dependency relations of sen-
tences often contain semantic information. In this
work we follow two syntactic dependency similar-
ity features presented in our previous work (Zhu
and Lan, 2013), i.e., Simple Dependency Overlap
and Special Dependency Overlap. The Simple De-
pendency Overlap measures all dependency rela-
tions while the Special Dependency Overlap fea-
</bodyText>
<page confidence="0.987802">
266
</page>
<bodyText confidence="0.999983621621621">
ture only focuses on the primary roles extracted
from several special dependency relations, i.e.,
subject, object and predict.
Machine Translation based features. Machine
translation (MT) evaluation metrics are designed
to assess whether the output of a MT system is
semantically equivalent to a set of reference trans-
lations. This type of feature has been proved to
be effective in our previous work (Zhu and Lan,
2013). As a result, we extend the original 6 lexical
level MT metrics to 10 metrics, i.e., WER, TER,
PER, BLEU, NIST, ROUGE-L, GTM-1,GTM-2,
GTM-3 and METEOR-ex. All these metrics are
calculated using the Asiya Open Toolkit for Auto-
matic Machine Translation (Meta-) Evaluation4.
Multi-level text Features. (Pilehvar et al., 2013)
presented a unified approach to semantic similar-
ity at multiple levels from word senses to text
documents through the semantic signature repre-
sentation of texts (e.g., sense, word or sentence).
Given initial nodes (senses), they performed ran-
dom walks on semantic network like WordNet,
then the resulting frequency distribution over all
nodes in WordNet served as semantic signature of
the text. By doing so the similarity of two texts
can be computed as the similarity of two seman-
tic signatures. In this work, we borrowed their
semantic signature method and adopted 3 similar-
ity measures to estimate two semantic signatures,
i.e., Cosine similarity, Weighted Overlap and Top-
k Jaccard (k=250, 500).
Other Features. Besides, other simple surface
features from texts, such as numbers, symbols and
length of texts, are extracted. Following (ˇSaric et
al., 2012) we adopt relative length difference, rela-
tive information content difference, numbers over-
lap, case match and stocks match.
</bodyText>
<subsectionHeader confidence="0.991677">
2.3 Features on Ph-W Level
</subsectionHeader>
<bodyText confidence="0.988429636363636">
For Ph-W level, since word and phrase no longer
share the property of sentence, most features used
for sentence similarity estimation are not applica-
ble for this level. Therefore, we adopt the follow-
ing features as the basic feature set for Ph-W level.
String features. This type contains two fea-
tures. The first is a boolean feature which records
whether the word appears in the phrase. The sec-
ond is the Weighted Word Overlap feature men-
tioned in Section 2.2.
Knowledge based features. As described in Sec-
</bodyText>
<footnote confidence="0.872711">
4http://nlp.lsi.upc.edu/asiya/
</footnote>
<bodyText confidence="0.999843212121212">
tion 2.2, we compute the averaged score and the
maximal score between word and phrase using the
four word similarity measures based on WordNet,
i.e., Path, WUP, LCH and Lin.
Corpus based features. We adopt the Vector
Space Similarity described in Section 2.2. Specif-
ically, for word the single distributional vector is
the LSA vector of itself.
Multi-level text Features. As described in Sec-
tion 2.2, since the semantic signatures are pro-
posed for various kinds of texts (e.g., sense, word
or sentence), they serve as one basic feature.
Obviously, the above features extracted from
the phrase-word pair is significantly less than the
features used in P-S level and S-Ph level. This is
because the information contained in phrase-word
pair is much less than that in sentences and para-
graphs. To overcome this information insufficient
problem, we propose an information enrichment
method based on WordNet to extend the initial
word in Ph-W level as below.
Word Expansion with Definition. For the word
part in Ph-W level, we extract its definition in
terms of its most common concept in WordNet and
then replace the initial word with this definition.
This gives a much richer set of initial single word.
Since a word may have many senses, not all of
this word definition expansion are correct. But we
show below empirically that using this expanded
set improves performance. By doing so we treat
the phrase and the definition of the original word
as two sentences, and thus, all features described
in Section 2.2 are calculated.
</bodyText>
<subsectionHeader confidence="0.966183">
2.4 Features on W-Se Level
</subsectionHeader>
<bodyText confidence="0.999807625">
For W-Se level, the information that a word and
a sense carry is less than other levels. Hence, the
basic features that can be extracted from the origi-
nal word-sense pair are even less than Ph-W level.
Therefore the basic features we use for W-Se level
are as follows.
String features. Two boolean string features
are used. One records whether the word-sense
pair shares the same POS tag and another records
whether the word-sense pair share the same word.
Knowledge based features. As described in Sec-
tion 2.2, four knowledge-based word similarity
measures based on WordNet are calculated.
Multi-level text Features. The multi-level text
features are the same as Ph-W level.
In consideration of the lack of contextual infor-
</bodyText>
<page confidence="0.991536">
267
</page>
<bodyText confidence="0.99995084">
mation between word-sense pair, we also propose
three information enrichment methods in order to
generate more effective information for word and
sense with the aid of WordNet.
Word Expansion with Synonyms. For the word
part in W-Se level, we extract its synonyms with
the help of WordNet, then update the values
of above basic features if its synonyms achieve
higher feature value than the original word itself.
Sense Expansion with Definition. For the sense
in W-Se level, we directly use its definition in
WordNet to enrich its information. By doing so
the similarity estimation of W-Se level can be con-
verted to that of word-phrase level, therefore we
use all basic features for Ph-W level described in
Section2.3.
Word-Sense Expansion with Definition. Un-
like the above two expansion methods which focus
only on one part of W-Se level, the third method is
to enrich information for word and sense together
by using their definitions in WordNet. As before
we extract the word definition in terms of its most
common concept in WordNet and then replace the
initial word with this definition. Then we use all
features in Section 2.2.
</bodyText>
<sectionHeader confidence="0.994068" genericHeader="evaluation">
3 Experiment and Results
</sectionHeader>
<bodyText confidence="0.985653153846154">
We adopt supervised regression model for each
cross level. In order to compare the performance
of different regression algorithms, we perform 5-
fold cross validation on training data for each cross
level. We used several regression algorithms in-
cluding Support Vector Regression (SVR) with
3 different kernels (i.e., linear, polynomial and
rbf), Random Forest, Stochastic Gradient Descent
(SGD) and Decision Tree implemented in the
scikit-learn toolkit (Pedregosa et al., 2011). The
system performance is evaluated in Pearson corre-
lation (r) (official measure) and Spearman’s rank
correlation (p).
</bodyText>
<subsectionHeader confidence="0.943795">
3.1 Results on Training Data
</subsectionHeader>
<bodyText confidence="0.999897583333333">
Table 1 and Table 2 show the averaged perfor-
mance of different regression algorithms in terms
of Pearson correlation (r) and Spearman’s rank
correlation (p) on the training data of P-S level and
S-Ph level using 5-fold cross validation, where the
standard deviation is given in brackets. The re-
sults show that Random Forest performs the best
both on P-S level and S-Ph level whether in (r) or
(p). We also find that the results of P-S level are
better than that of S-Ph level, and the reason may
be that paragraph and sentence pair contain more
information than the sentence and phrase pair.
</bodyText>
<table confidence="0.999935142857143">
Regression Algorithm r (%) p (%)
SVR, ker=rbf 80.70 (±1.47) 79.90 (±1.66)
SVR, ker=poly 73.78 (±1.57) 74.41 (±1.89)
SVR, ker=linear 80.43 (±1.13) 79.46 (±1.51)
Random Forest 80.92 (±1.40) 80.20 (±2.00)
SGD 77.61 (±0.76) 77.14 (±1.49)
Decision Tree 73.23 (±2.14) 71.84 (±2.55)
</table>
<tableCaption confidence="0.9980195">
Table 1: Results of different algorithms using 5-
fold cross validation on training data of P-S level
</tableCaption>
<table confidence="0.999982428571428">
Regression Algorithm r (%) p (%)
SVR, ker=rbf 66.14 (±5.14) 65.76 (±5.93)
SVR, ker=poly 58.93 (±2.29) 63.62 (±4.15)
SVR, ker=linear 66.78 (±4.51) 66.34 (±4.90)
Random Forest 73.18 (±5.23) 70.30 (±5.51)
SGD 63.18 (±3.61) 64.80 (±4.21)
Decision Tree 67.66 (±6.76) 66.03 (±6.64)
</table>
<tableCaption confidence="0.9377955">
Table 2: Results of different algorithms using 5-
fold cross validation on training data of S-Ph level
</tableCaption>
<bodyText confidence="0.988981448275862">
Table 3 shows the results of different regression
algorithms and different feature sets in terms of
r and p on the training data of Ph-W level us-
ing 5-fold cross validation, where the basic fea-
tures are denoted as Feature Set A and their com-
bination with word definition expansion features
are denoted as Feature Set B. The results show
that almost all algorithms performance have been
improved by using word definition expansion fea-
ture except Decision Tree. This proves the effec-
tiveness of the information enrichment method we
proposed in this level. Besides, Random Forest
achieves the best performance again with r=44%
and p=41%. However, in comparison with P-S
level and S-Ph level, all scores in Table 3 drop a
lot even with information enrichment method. The
possible reason may be two: the reduction of in-
formation on Ph-W level and our information en-
richment method brings in a certain noise as well.
For W-Se level, in order to examine the perfor-
mance of different information enrichment meth-
ods, we perform experiments on 4 different fea-
ture sets from A to D, where feature set A con-
tains the basic features, feature set B, C and D
add one information enrichment method based on
former feature set. Table 4 and 5 present the r
and p results of 4 feature sets using different re-
gression algorithms. From Table 4 and 5 we see
that most correlation scores are below 40% and
</bodyText>
<page confidence="0.992654">
268
</page>
<table confidence="0.9976037">
Regression Algorithm r (%) p (%)
Feature Set A1 Feature Set B2 Feature Set A Feature Set B
SVR, ker=rbf 34.67 (±4.34) 42.62 (±6.36) 33.26 (±4.24) 40.87 (±6.24)
SVR, ker=poly 19.00 (±4.26) 24.06 (±5.55) 21.13 (±4.86) 28.35 (±6.11)
SVR, ker=linear 34.87 (±4.65) 41.91 (±2.05) 35.42 (±5.05) 42.69 (±0.55)
Random Forest 43.17 (±7.72) 44.00 (±6.88) 40.34 (±5.71) 41.80 (±6.76)
SGD 26.20 (±3.37) 38.69 (±4.60) 23.55 (±5.01) 38.00 (±2.64)
Decision Tree 39.22 (±7.54) 32.22 (±12.74) 38.90 (±6.03) 31.64 (±10.47)
1 Feature Set A = basic feature set
2 Feature Set B = Feature Set A + Word Definition Expansion Features
</table>
<tableCaption confidence="0.99993">
Table 3: Results of different algorithms using 5-fold cross validation on training data of Ph-W level
</tableCaption>
<bodyText confidence="0.999906304347826">
the performance of W-Se level is the worst among
all these four levels. This illustrates that the less
information the texts contain, the worse perfor-
mance the model achieves. Again the Random
Forest algorithm performs the best among all algo-
rithms. Again almost all information enrichment
features perform better than Feature set A. This il-
lustrates that these information enrichment meth-
ods do help to improve performance. When we ob-
serve the three information enrichment methods,
we find that feature set C performs the best. In
comparison with feature set C, feature set B only
used word synonyms to expand information and
this expansion is quite limited. Feature set D per-
forms better than B but still worse than C. The rea-
son may be that when we extend sense with its def-
inition, the definition is accurate and exactly repre-
sents the meaning of sense. However since a word
often contains more than one concepts, and when
we use the definition of the most common concept
to extend word, such extension may not be correct
and the generated information may contain more
noise and/or change the original meaning of word.
</bodyText>
<subsectionHeader confidence="0.967118">
3.2 Results on Test Data
</subsectionHeader>
<bodyText confidence="0.999219833333333">
According to the experiments on training data, we
select Random Forest as the final regression algo-
rithm. The number of trees in Random Forest n is
optimized to 50 and the rest parameters are set to
be default. All features in Section 2.2 are used on
P-S level, S-Ph level and Ph-W level. For W-Se
level, we take all features except word-sense def-
inition expansion feature which has been shown
to impair the system performance. For each level,
all training examples are used to learn the corre-
sponding regression model. According to the offi-
cial results released by organizers, Table 6 and Ta-
ble 7 list the top 3 systems in terms of r (official)
and p. Our final systems rank the second both in
terms of r and p and also achieve the second place
on P-S level, S-Ph level and Ph-W level, as well
as the 4th place on W-Se level in terms of official
Pearson correlation.
</bodyText>
<table confidence="0.99736425">
Team P-S S-Ph Ph-W W-Se r Rank
SimCompass 0.811 0.742 0.415 0.356 1
ECNU 0.834 0.771 0.315 0.269 2
UNAL-NLP 0.837 0.738 0.274 0.256 3
</table>
<tableCaption confidence="0.979585">
Table 6: Pearson Correlation (official) on test data
</tableCaption>
<table confidence="0.99983925">
Team P-S S-Ph Ph-W W-Se p Rank
SimCompass 0.801 0.728 0.424 0.344 1
ECNU 0.821 0.757 0.306 0.263 2
UNAL-NLP 0.820 0.710 0.249 0.236 6
</table>
<tableCaption confidence="0.996657">
Table 7: Spearman Correlation on test data
</tableCaption>
<sectionHeader confidence="0.99752" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999973">
We build a supervised Random Forest regression
model for each cross level. For P-S and S-Ph level,
we adopt the ensemble of heterogeneous similar-
ity features, i.e., string features, knowledge based
features, corpus based features, syntactic features,
machine translation based features, multi-level
text features and other features to capture the se-
mantic similarity between two texts with distinc-
tively different lengths. For Ph-W and W-Se level,
we propose information enrichment methods to
lengthen original texts in order to generate more
semantic features, which has been proved to be ef-
fective. Our submitted final systems rank the 2nd
out of 18 teams both on Pearson Rank (official
rank) and Spearman Rank, and also rank the sec-
ond place on P-S level, S-Ph level and Ph-W level,
as well as the 4th place on W-Se level in terms of
Pearson correlation. In future work we will focus
on information enrichment methods which bring
in more accurate information and less noises.
</bodyText>
<sectionHeader confidence="0.997355" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<footnote confidence="0.6482965">
This research is supported by grants from Na-
tional Natural Science Foundation of China
</footnote>
<page confidence="0.988764">
269
</page>
<table confidence="0.989233727272727">
Regression Algorithm Feature Set A&apos; Feature Set B2 Feature Set C3 Feature Set D4
SVR, ker=rbf 29.85 (±7.29) 34.49 (±5.55) 36.80 (±6.46) 22.19 (±6.49)
SVR, ker=poly 24.62 (±3.63) 29.27 (±3.53) 26.55 (±1.27) 25.89 (±5.63)
SVR, ker=linear 29.58 (±5.88) 34.87 (±3.97) 35.96 (±1.75) 34.57 (±3.75)
Random Forest 22.87 (±5.59) 33.97 (±1.78) 40.43 (±3.00) 37.54 (±3.20)
SGD 26.32 (±7.31) 27.36 (±6.44) 32.50 (±6.02) 18.00 (±6.13)
Decision Tree 23.40 (±5.65) 26.33 (±3.86) 33.64 (±6.97) 31.86 (±3.95)
&apos; Feature Set A = basic feature set
2 Feature Set B = Feature Set A + Synonym Expansion
3 Feature Set C = Feature Set B + Sense Definition Expansion Features
4 Feature Set D = Feature Set C + Word-Sense Definition Expansion Features
</table>
<tableCaption confidence="0.997313">
Table 4: Results of different algorithms using 5-fold CV on training data of W-Se level (r (%))
</tableCaption>
<table confidence="0.999971142857143">
Regression Algorithm Feature Set A Feature Set B Feature Set C Feature Set D
SVR, ker=rbf 28.41 (±8.99) 29.61 (±6.23) 34.18 (±6.36) 22.90 (±6.78)
SVR, ker=poly 23.05 (±7.53) 22.47 (±4.47) 21.63 (±4.37) 25.37 (±7.25)
SVR, ker=linear 27.29 (±7.02) 31.79 (±4.00) 34.75 (±3.55) 34.19 (±3.06)
Random Forest 19.66 (±6.75) 31.98 (±3.21) 38.57 (±3.60) 37.56 (±3.15)
SGD 24.12 (±7.98) 24.62 (±6.36) 29.27 (±5.86) 23.05 (±11.23)
Decision Tree 22.30 (±5.25) 25.09 (±3.64) 31.99 (±7.81) 30.51 (±5.27)
</table>
<tableCaption confidence="0.998935">
Table 5: Results of different algorithms using 5-fold CV on training data of W-Se level (p (%))
</tableCaption>
<bodyText confidence="0.3847895">
(No.60903093) and Shanghai Knowledge Service
Platform Project (No. ZF1213).
</bodyText>
<sectionHeader confidence="0.968573" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99993884375">
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt: A supervised synergis-
tic approach to semantic text similarity. pages 635–
642. First Joint Conference on Lexical and Compu-
tational Semantics (*SEM).
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. pages 435–440. First Joint
Conference on Lexical and Computational Seman-
tics (*SEM).
Michael Heilman and Nitin Madnani. 2012. Ets:
Discriminative edit models for paraphrase scoring.
pages 529–535. First Joint Conference on Lexical
and Computational Semantics (*SEM).
Aminul Islam and Diana Inkpen. 2008. Semantic text
similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
Thomas K Landauer, Darrell Laham, Bob Rehder, and
Missy E Schreiner. 1997. How well can passage
meaning be derived without using word order? a
comparison of latent semantic analysis and humans.
In Proceedings of the 19th annual meeting of the
Cognitive Science Society, pages 412–417.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th in-
ternational conference on Machine Learning, vol-
ume 1, pages 296–304. San Francisco.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Fabian Pedregosa, Ga¨el. Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ´Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, disambiguate and
walk: A unified approach for measuring semantic
similarity. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2013).
Frane ˇSaric, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder,
and Bojana Dalbelo Baˇsic. 2012. Takelab: Systems
for measuring semantic text similarity. pages 441–
448. First Joint Conference on Lexical and Compu-
tational Semantics (*SEM).
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133–138. Association for Com-
putational Linguistics.
Tian Tian Zhu and Man Lan. 2013. Ecnucs: Measur-
ing short text semantic equivalence using multiple
similarity measurements. Atlanta, Georgia, USA,
page 124.
</reference>
<page confidence="0.996821">
270
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.530789">
<title confidence="0.999601">ECNU: Leveraging on Ensemble of Heterogeneous Features Information Enrichment for Cross Level Semantic Similarity Estimation</title>
<author confidence="0.999896">Tian Tian</author>
<affiliation confidence="0.9475415">Department of Computer Science East China Normal</affiliation>
<email confidence="0.662989">51111201046@ecnu.cn</email>
<abstract confidence="0.996266785714286">This paper reports our submissions to the Cross Level Semantic Similarity (CLSS) task in SemEval 2014. We submitted one Random Forest regression system on each cross level text pair, i.e., Paragraph to Sentence (P-S), Sentence to Phrase (S- Ph), Phrase to Word (Ph-W) and Word to Sense (W-Se). For text pairs on P-S level and S-Ph level, we consider them as sentences and extract heterogeneous types of similarity features, i.e., string features, knowledge based features, corpus based features, syntactic features, machine translation based features, multi-level text features, etc. For text pairs on Ph-W level and W-Se level, due to lack of information, most of these features are not applicable or available. To overcome this problem, we propose several information enrichment methods using WordNet synonym and definition. Our systems rank the 2nd out of 18 teams both on Pearson correlation (official rank) and Spearman rank correlation. Specifically, our systems take the second place on P-S level, S-Ph level and Ph-W level and the 4th place on W-Se level in terms of Pearson correlation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Samer Hassan</author>
<author>Michael Mohler</author>
<author>Rada Mihalcea</author>
</authors>
<title>Unt: A supervised synergistic approach to semantic text similarity.</title>
<date>2012</date>
<booktitle>First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<pages>635--642</pages>
<contexts>
<context position="6991" citStr="Banea et al., 2012" startWordPosition="1103" endWordPosition="1106"> (Zhu and Lan, 2013), the Ngram Overlap feature (n=1,2,3) and the Weighted Word Overlap feature (ˇSaric et al., 2012). All these features are computed from original text and from the processed text after lemmatization as well. Besides, we also computed the N-gram Overlap on character level, named Character Ngram (n=2,3,4). Knowledge based features. Knowledge based similarity estimation relies on the semantic network of words. In this work we used the knowledge based features in our previous work (Zhu and Lan, 2013), which include four word similarity metrics based on WordNet: Path similarity (Banea et al., 2012), WUP similarity (Wu and Palmer, 1994), LCH similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin, 1998). Then two strategies, i.e., the best alignment strategy and the aggregation strategy, are employed to propagate the word similarity to the text similarity. Totally we get 8 knowledge based features. Corpus based features. Latent Semantic Analysis (LSA) (Landauer et al., 1997) is a widely used corpus based measure when evaluating text similarity. In this work we use the Vector Space Sentence Similarity proposed by (ˇSaric et al., 2012), which represents each sentence as a single di</context>
</contexts>
<marker>Banea, Hassan, Mohler, Mihalcea, 2012</marker>
<rawString>Carmen Banea, Samer Hassan, Michael Mohler, and Rada Mihalcea. 2012. Unt: A supervised synergistic approach to semantic text similarity. pages 635– 642. First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. pages 435–440. First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Nitin Madnani</author>
</authors>
<title>Ets: Discriminative edit models for paraphrase scoring.</title>
<date>2012</date>
<booktitle>First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<pages>529--535</pages>
<contexts>
<context position="2045" citStr="Heilman and Madnani, 2012" startWordPosition="306" endWordPosition="309">ons in Natural Language Processing (NLP). Previous works often focus on text semantic similarity on the same level, i.e., paragraph to paragraph or sentence to sentence, and many effective text semantic measurements have been proposed (Islam and Inkpen, 2008), (B¨ar et al., 2012), This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Man Lan* Department of Computer Science and Technology East China Normal University mlan@cs.ecnu.edu.cn* (Heilman and Madnani, 2012). However, in many real world cases, the two texts may not always be on the same level. The Cross Level Semantic Similarity (CLSS) task in SemEval 2014 provides a universal platform to measure the degree of semantic equivalence between two texts across different levels. For each text pair on four cross levels, i.e., Paragraph to Sentence (P-S), Sentence to Phrase (S-Ph), Phrase to Word (Ph-W) and Word to Sense (W-Se), participants are required to return a similarity score which ranges from 0 (no relation) to 4 (semantic equivalence). We participate in all the four cross levels and take the sec</context>
</contexts>
<marker>Heilman, Madnani, 2012</marker>
<rawString>Michael Heilman and Nitin Madnani. 2012. Ets: Discriminative edit models for paraphrase scoring. pages 529–535. First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic text similarity using corpus-based word similarity and string similarity.</title>
<date>2008</date>
<journal>ACM Transactions on Knowledge Discovery from Data (TKDD),</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="1678" citStr="Islam and Inkpen, 2008" startWordPosition="258" endWordPosition="261"> and definition. Our systems rank the 2nd out of 18 teams both on Pearson correlation (official rank) and Spearman rank correlation. Specifically, our systems take the second place on P-S level, S-Ph level and Ph-W level and the 4th place on W-Se level in terms of Pearson correlation. 1 Introduction Semantic similarity is an essential component of many applications in Natural Language Processing (NLP). Previous works often focus on text semantic similarity on the same level, i.e., paragraph to paragraph or sentence to sentence, and many effective text semantic measurements have been proposed (Islam and Inkpen, 2008), (B¨ar et al., 2012), This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ Man Lan* Department of Computer Science and Technology East China Normal University mlan@cs.ecnu.edu.cn* (Heilman and Madnani, 2012). However, in many real world cases, the two texts may not always be on the same level. The Cross Level Semantic Similarity (CLSS) task in SemEval 2014 provides a universal platform to measure the degree of semantic equivalence betwe</context>
</contexts>
<marker>Islam, Inkpen, 2008</marker>
<rawString>Aminul Islam and Diana Inkpen. 2008. Semantic text similarity using corpus-based word similarity and string similarity. ACM Transactions on Knowledge Discovery from Data (TKDD), 2(2):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Darrell Laham</author>
<author>Bob Rehder</author>
<author>Missy E Schreiner</author>
</authors>
<title>How well can passage meaning be derived without using word order? a comparison of latent semantic analysis and humans.</title>
<date>1997</date>
<booktitle>In Proceedings of the 19th annual meeting of the Cognitive Science Society,</booktitle>
<pages>412--417</pages>
<contexts>
<context position="7382" citStr="Landauer et al., 1997" startWordPosition="1162" endWordPosition="1165"> relies on the semantic network of words. In this work we used the knowledge based features in our previous work (Zhu and Lan, 2013), which include four word similarity metrics based on WordNet: Path similarity (Banea et al., 2012), WUP similarity (Wu and Palmer, 1994), LCH similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin, 1998). Then two strategies, i.e., the best alignment strategy and the aggregation strategy, are employed to propagate the word similarity to the text similarity. Totally we get 8 knowledge based features. Corpus based features. Latent Semantic Analysis (LSA) (Landauer et al., 1997) is a widely used corpus based measure when evaluating text similarity. In this work we use the Vector Space Sentence Similarity proposed by (ˇSaric et al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. Two corpora are used to compute the LSA vector of words: New York Times Annotated Corpus (NYT) and Wikipedia. Besides, in consideration of different weights for different words, they also calculated the weighted LSA vector for each word. In addition, we use the Co-occurrence Retrieval Model (CRM) feature from o</context>
</contexts>
<marker>Landauer, Laham, Rehder, Schreiner, 1997</marker>
<rawString>Thomas K Landauer, Darrell Laham, Bob Rehder, and Missy E Schreiner. 1997. How well can passage meaning be derived without using word order? a comparison of latent semantic analysis and humans. In Proceedings of the 19th annual meeting of the Cognitive Science Society, pages 412–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>49--2</pages>
<contexts>
<context position="7074" citStr="Leacock and Chodorow, 1998" startWordPosition="1115" endWordPosition="1118">Word Overlap feature (ˇSaric et al., 2012). All these features are computed from original text and from the processed text after lemmatization as well. Besides, we also computed the N-gram Overlap on character level, named Character Ngram (n=2,3,4). Knowledge based features. Knowledge based similarity estimation relies on the semantic network of words. In this work we used the knowledge based features in our previous work (Zhu and Lan, 2013), which include four word similarity metrics based on WordNet: Path similarity (Banea et al., 2012), WUP similarity (Wu and Palmer, 1994), LCH similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin, 1998). Then two strategies, i.e., the best alignment strategy and the aggregation strategy, are employed to propagate the word similarity to the text similarity. Totally we get 8 knowledge based features. Corpus based features. Latent Semantic Analysis (LSA) (Landauer et al., 1997) is a widely used corpus based measure when evaluating text similarity. In this work we use the Vector Space Sentence Similarity proposed by (ˇSaric et al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. Two </context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th international conference on Machine Learning,</booktitle>
<volume>1</volume>
<pages>296--304</pages>
<location>San Francisco.</location>
<contexts>
<context position="7105" citStr="Lin, 1998" startWordPosition="1122" endWordPosition="1123">these features are computed from original text and from the processed text after lemmatization as well. Besides, we also computed the N-gram Overlap on character level, named Character Ngram (n=2,3,4). Knowledge based features. Knowledge based similarity estimation relies on the semantic network of words. In this work we used the knowledge based features in our previous work (Zhu and Lan, 2013), which include four word similarity metrics based on WordNet: Path similarity (Banea et al., 2012), WUP similarity (Wu and Palmer, 1994), LCH similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin, 1998). Then two strategies, i.e., the best alignment strategy and the aggregation strategy, are employed to propagate the word similarity to the text similarity. Totally we get 8 knowledge based features. Corpus based features. Latent Semantic Analysis (LSA) (Landauer et al., 1997) is a widely used corpus based measure when evaluating text similarity. In this work we use the Vector Space Sentence Similarity proposed by (ˇSaric et al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. Two corpora are used to compute the</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th international conference on Machine Learning, volume 1, pages 296–304. San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="3901" citStr="Miller, 1995" startWordPosition="621" endWordPosition="622">s system in the Semantic Textual Similarity (STS) task in *SEM Shared Task 2013 (Zhu and Lan, 2013). Others followed the previous work in (ˇSaric et al., 2012) and (Pilehvar et al., 2013). For Ph-W level and W-Se level, since the text pairs lack contextual information, for example, word or sense alone no longer shares the property of sentence, most features used in P-S level and S-Ph level are not applicable or available. To overcome the problem of insufficient information in word and sense level, we propose several information enrichment methods to extend information with the aid of WordNet (Miller, 1995), which significantly improved the system performance. The rest of this paper is organized as follows. 265 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 265–270, Dublin, Ireland, August 23-24, 2014. Section 2 describes the similarity features used on four cross levels in detail. Section 3 presents experiments and the results of four cross levels on training data and test data. Conclusions and future work are given in Section 4. 2 Text Similarity Measurements To estimate the semantic similarity on P-S level and S-Ph level, we treat the text pairs on </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Gramfort Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el. Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Taher Pilehvar</author>
<author>David Jurgens</author>
<author>Roberto Navigli</author>
</authors>
<title>Align, disambiguate and walk: A unified approach for measuring semantic similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="3475" citStr="Pilehvar et al., 2013" startWordPosition="550" endWordPosition="553">S-Ph level, we regard the paragraph of P-S as a long sentence, and the phrase of SPh as a short sentence. Then we use various types of text similarity features including string features, knowledge based features, corpus based features, syntactic features, machine translation based features, multi-level text features and so on, to capture the semantic similarity between two texts. Some of these features are borrowed from our previous system in the Semantic Textual Similarity (STS) task in *SEM Shared Task 2013 (Zhu and Lan, 2013). Others followed the previous work in (ˇSaric et al., 2012) and (Pilehvar et al., 2013). For Ph-W level and W-Se level, since the text pairs lack contextual information, for example, word or sense alone no longer shares the property of sentence, most features used in P-S level and S-Ph level are not applicable or available. To overcome the problem of insufficient information in word and sense level, we propose several information enrichment methods to extend information with the aid of WordNet (Miller, 1995), which significantly improved the system performance. The rest of this paper is organized as follows. 265 Proceedings of the 8th International Workshop on Semantic Evaluatio</context>
<context position="9441" citStr="Pilehvar et al., 2013" startWordPosition="1492" endWordPosition="1495">dict. Machine Translation based features. Machine translation (MT) evaluation metrics are designed to assess whether the output of a MT system is semantically equivalent to a set of reference translations. This type of feature has been proved to be effective in our previous work (Zhu and Lan, 2013). As a result, we extend the original 6 lexical level MT metrics to 10 metrics, i.e., WER, TER, PER, BLEU, NIST, ROUGE-L, GTM-1,GTM-2, GTM-3 and METEOR-ex. All these metrics are calculated using the Asiya Open Toolkit for Automatic Machine Translation (Meta-) Evaluation4. Multi-level text Features. (Pilehvar et al., 2013) presented a unified approach to semantic similarity at multiple levels from word senses to text documents through the semantic signature representation of texts (e.g., sense, word or sentence). Given initial nodes (senses), they performed random walks on semantic network like WordNet, then the resulting frequency distribution over all nodes in WordNet served as semantic signature of the text. By doing so the similarity of two texts can be computed as the similarity of two semantic signatures. In this work, we borrowed their semantic signature method and adopted 3 similarity measures to estima</context>
</contexts>
<marker>Pilehvar, Jurgens, Navigli, 2013</marker>
<rawString>Mohammad Taher Pilehvar, David Jurgens, and Roberto Navigli. 2013. Align, disambiguate and walk: A unified approach for measuring semantic similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSaric</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Baˇsic.</title>
<date></date>
<booktitle>First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<pages>441--448</pages>
<marker>ˇSaric, Glavaˇs, Karan, </marker>
<rawString>Frane ˇSaric, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsic. 2012. Takelab: Systems for measuring semantic text similarity. pages 441– 448. First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>133--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7029" citStr="Wu and Palmer, 1994" startWordPosition="1109" endWordPosition="1112">ap feature (n=1,2,3) and the Weighted Word Overlap feature (ˇSaric et al., 2012). All these features are computed from original text and from the processed text after lemmatization as well. Besides, we also computed the N-gram Overlap on character level, named Character Ngram (n=2,3,4). Knowledge based features. Knowledge based similarity estimation relies on the semantic network of words. In this work we used the knowledge based features in our previous work (Zhu and Lan, 2013), which include four word similarity metrics based on WordNet: Path similarity (Banea et al., 2012), WUP similarity (Wu and Palmer, 1994), LCH similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin, 1998). Then two strategies, i.e., the best alignment strategy and the aggregation strategy, are employed to propagate the word similarity to the text similarity. Totally we get 8 knowledge based features. Corpus based features. Latent Semantic Analysis (LSA) (Landauer et al., 1997) is a widely used corpus based measure when evaluating text similarity. In this work we use the Vector Space Sentence Similarity proposed by (ˇSaric et al., 2012), which represents each sentence as a single distributional vector by summing up the </context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 133–138. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tian Tian Zhu</author>
<author>Man Lan</author>
</authors>
<title>Ecnucs: Measuring short text semantic equivalence using multiple similarity measurements.</title>
<date>2013</date>
<pages>124</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3387" citStr="Zhu and Lan, 2013" startWordPosition="535" endWordPosition="538">t a supervised regression system for each cross level separately. For P-S level and S-Ph level, we regard the paragraph of P-S as a long sentence, and the phrase of SPh as a short sentence. Then we use various types of text similarity features including string features, knowledge based features, corpus based features, syntactic features, machine translation based features, multi-level text features and so on, to capture the semantic similarity between two texts. Some of these features are borrowed from our previous system in the Semantic Textual Similarity (STS) task in *SEM Shared Task 2013 (Zhu and Lan, 2013). Others followed the previous work in (ˇSaric et al., 2012) and (Pilehvar et al., 2013). For Ph-W level and W-Se level, since the text pairs lack contextual information, for example, word or sense alone no longer shares the property of sentence, most features used in P-S level and S-Ph level are not applicable or available. To overcome the problem of insufficient information in word and sense level, we propose several information enrichment methods to extend information with the aid of WordNet (Miller, 1995), which significantly improved the system performance. The rest of this paper is organ</context>
<context position="6392" citStr="Zhu and Lan, 2013" startWordPosition="1007" endWordPosition="1010">text pairs of P-S level and S-Ph level as sentences and then extract 7 types of similarity features as below. Totally we get 52 similarity features. Generally, these similarity features are represented as numerical values. String features. Intuitively, if two texts share more strings, they are considered to be more semantic similar. We extract 13 string based features in consideration of the common sequence shared 1http://nlp.stanford.edu/software/lex-parser.shtml 2http://nlp.stanford.edu/software/tagger.shtml 3http://nltk.org/ by two texts. We chose the Longest Common Sequence (LCS) feature (Zhu and Lan, 2013), the Ngram Overlap feature (n=1,2,3) and the Weighted Word Overlap feature (ˇSaric et al., 2012). All these features are computed from original text and from the processed text after lemmatization as well. Besides, we also computed the N-gram Overlap on character level, named Character Ngram (n=2,3,4). Knowledge based features. Knowledge based similarity estimation relies on the semantic network of words. In this work we used the knowledge based features in our previous work (Zhu and Lan, 2013), which include four word similarity metrics based on WordNet: Path similarity (Banea et al., 2012),</context>
<context position="8018" citStr="Zhu and Lan, 2013" startWordPosition="1269" endWordPosition="1272">corpus based measure when evaluating text similarity. In this work we use the Vector Space Sentence Similarity proposed by (ˇSaric et al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. Two corpora are used to compute the LSA vector of words: New York Times Annotated Corpus (NYT) and Wikipedia. Besides, in consideration of different weights for different words, they also calculated the weighted LSA vector for each word. In addition, we use the Co-occurrence Retrieval Model (CRM) feature from our previous work (Zhu and Lan, 2013) as another corpus-based feature. The CRM is calculated based on a notion of substitutability, that is, the more appropriate it is to substitute word w1 in place of word w2 in a suitable natural language task, the more semantically similar they are. At last, 6 corpus based features are extracted. Syntactic features. Dependency relations of sentences often contain semantic information. In this work we follow two syntactic dependency similarity features presented in our previous work (Zhu and Lan, 2013), i.e., Simple Dependency Overlap and Special Dependency Overlap. The Simple Dependency Overla</context>
</contexts>
<marker>Zhu, Lan, 2013</marker>
<rawString>Tian Tian Zhu and Man Lan. 2013. Ecnucs: Measuring short text semantic equivalence using multiple similarity measurements. Atlanta, Georgia, USA, page 124.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>