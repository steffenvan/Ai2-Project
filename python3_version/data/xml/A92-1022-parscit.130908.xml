<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023852">
<title confidence="0.99389">
Evaluating Parsing Strategies Using Standardized Parse Files
</title>
<author confidence="0.995263">
Ralph Grishman and Catherine Macleod and John Sterling*
</author>
<affiliation confidence="0.99692">
Computer Science Department
</affiliation>
<address confidence="0.854846333333333">
New York University
715 Broadway, 7th Floor
New York, New York 10003
</address>
<sectionHeader confidence="0.970067" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999774">
The availability of large files of manually-
reviewed parse trees from the University of
Pennsylvania &amp;quot;tree bank&amp;quot;, along with a pro-
gram for comparing system-generated parses
against these &amp;quot;standard&amp;quot; parses, provides a
new opportunity for evaluating different pars-
ing strategies. We discuss some of the restruc-
turing required to the output of our parser so
that it could be meaningfully compared with
these standard parses. We then describe several
heuristics for improving parsing accuracy and
coverage, such as closest attachment of mod-
ifiers, statistical grammars, and fitted parses,
and present a quantitative evaluation of the im-
provements obtained with each strategy.
</bodyText>
<sectionHeader confidence="0.99116" genericHeader="method">
1 The Problem
</sectionHeader>
<bodyText confidence="0.998776043478261">
The systematic improvement of parsing strategies re-
quires some way of evaluating competing strategies. We
need to understand their effect on both overall system
performance and the ability to parse sentences correctly.
In order to evaluate parser output, we will need a file of
standard &amp;quot;correct&amp;quot; parses and a mechanism for compar-
ing parser output with this standard. Preparing such a
file is a quite time-consuming operation. In addition, if
we would like our comparison of strategies to be mean-
ingful to other researchers and extendible to other sys-
tems, we would like a standard and a metric which are,
as much as possible, system independent.
One resource which is newly available to meet this
need is the &amp;quot;tree bank&amp;quot; at the University of Pennsyl-
vania. This bank includes a large number of parse trees
which have been prepared manually following a fairly de-
tailed standard. At first glance the comparison of parse
trees among systems based on different theories would
seem to be very difficult. However, based on an idea
proposed by Ezra Black [Black et al., 1991], a group or-
ganized by Phil Harrison has developed a metric and
a program for performing just such a comparison. A
preliminary assessment of this metric, based on a small
</bodyText>
<note confidence="0.60366825">
This paper is based upon work supported by the Defense
Advanced Research Projects Agency under Grant N00014-90-
J-1851 from the Office of Naval Research, and by the National
Science Foundation under Grant IRI-89-02304.
</note>
<bodyText confidence="0.999038357142857">
sample of short sentences (50 13-word sentences), has
been quite promising [Harrison et al., 19911.
Our goal, in the work described here, was to extend
this effort to larger samples and more complex sentences
and to use the metric to compare various parsing heuris-
tics which we have employed in our system. We describe
briefly in the next sections the nature of the task and
the text to which is was applied, the form of the tree
bank, and the comparison metric. We consider some of
the systematic mis-alignments between the standard and
our output, and describe how we have reduced these. Fi-
nally, we describe some of our parsing strategies and see
how they affect both the parse scores and the overall
system performance.
</bodyText>
<sectionHeader confidence="0.98921" genericHeader="method">
2 The Application Task
</sectionHeader>
<bodyText confidence="0.979009466666667">
The experiments described below were performed on a
version of the PROTEUS system as prepared for MUC-3,
the third Message Understanding Conference [Sundheim,
1991]. The task set before the participants at these Con-
ferences is one of information extraction: taking free-text
input on a particular subject matter, extracting specified
types of information, and filling a data base with this
information. For MUC-3, the texts consisted of short
news reports about terrorist incidents; most are in typ-
ical newspaper style, with complex sentences, although
transcripts of speeches and radio broadcasts are also in-
cluded.
The PROTEUS system has five basic stages of process-
ing: syntactic analysis, semantic analysis, reference res-
olution, discourse analysis, and data base creation. Syn-
tactic analysis consists of parsing with a broad-coverage
English grammar and dictionary, followed by syntactic
regularization. The regularized parse is then given to the
semantic analyzer, which produces a predicate-argument
structure. The semantic analyzer also provides feedback
to the parser as to whether the parse satisfies semantic
(selectional) constraints. Thus correct data base creation
is dependent on correct predicate-argument structures,
which are in turn dependent on at least locally correct
syntactic structures.
Because of the richness of the text, it is not possible to
provide a complete set of semantic patterns. The seman-
tic patterns included in the system are largely limited to
those relevant for filling data base entries. For the terror-
ist domain, this includes patterns for all sorts of terrorist
</bodyText>
<page confidence="0.998166">
156
</page>
<bodyText confidence="0.999445476190476">
incidents (attacks, bombings, ...), for the entities which
can be involved in these incidents (as perpetrators, tar-
gets, instruments, ...), and for other structures which
may affect truth value or certainty (claim, deny, allege,
...). We use a system of preference semantics which pe-
nalizes but does not exclude constructs not matching
any of these semantic patterns [Grishman and Sterling,
1990]. Our parser does a best-first search for the analysis
with the lowest penalty.
The information extraction task has the benefit of pro-
viding relatively clear measures of overall system perfor-
mance. As part of MUC-3, a set of standard (correct)
data base entries was prepared and a program was devel-
oped for scoring system responses against this standard.
Roughly speaking, the scoring program computes three
counts: Std, the number of data base fills in the standard
(correct) data base; Sys, the number of fills generated by
the system; and Cor, the number of correct fills gener-
ated by the system. System performance is then stated
in terms of recall (= Cor / Std) and precision (= Cor /
Sys).
</bodyText>
<sectionHeader confidence="0.980916" genericHeader="method">
3 The Tree Bank
</sectionHeader>
<bodyText confidence="0.999962352941177">
The goal of the &amp;quot;Treebank&amp;quot; at the University of Pennsyl-
vania is to construct a large data base of English anno-
tated with detailed grammatical structure. Among the
texts which they have annotated is a portion of the de-
velopment corpus used for MUC-3; they have annotated
356 of the 1300 articles in that corpus.
Sentences are annotated in a semi-automated proce-
dure in which sentences are first parsed automatically
and these parses are then manually revised. The result is
a file of labeled bracketings conforming in general terms
to the syntactic structures of transformational genera-
tive grammar (X-bar theory). The bracketing is shown
in considerable detail at the clause level; some of the de-
tails at the NP level are suppressed. The guidelines for
the bracketing are given in [Santorini, 1991]. A sample
sentence from the MUC-3 corpus, as bracketed for the
Treebank, is shown in Figure 1.
</bodyText>
<sectionHeader confidence="0.984442" genericHeader="method">
4 The Evaluation Metric
</sectionHeader>
<bodyText confidence="0.999971823529412">
The basic idea for the evaluation metric was developed
by Ezra Black. It was then refined and tested at a work-
shop (and through extensive subsequent electronic com-
munication) organized by Phil Harrison. This effort has
involved computational linguists from eight sites who
have applied this metric to compare — for a set of 50
short sentences — the Penn standard with the &amp;quot;ideal&amp;quot;
parses their systems would generate for these sentences.
At first glance the parses produced by different sys-
tems may seem so different as to be incomparable; node
labels in particular may be entirely different. The met-
ric therefore eliminates node labels and only compares
tree structures. Certain constituents are treated very
differently by different systems; for example, auxiliaries
are treated as main verbs by some systems but not oth-
ers. Several classes of constituents, such as auxiliaries,
pre-infinitival &amp;quot;to&amp;quot;, &amp;quot;not&amp;quot;, null elements, punctuation,
</bodyText>
<figure confidence="0.995981130434783">
( (S (PP in (NP cartagena))
(S (NP it)
was
(VP reported
(SBAR
(SBAR
that
(S (NP cast ellar)
(VP faced
(NP a
ft
revolutionary trial
(PP by
(NP the ELN))))))
and
(SEAR that
(S (NP he)
was
(VP (VP found
(ADJP guilty))
and
(VP executed))))))))
.)
</figure>
<figureCaption confidence="0.9259085">
Figure 1: A sample sentence from the U. of Pennsylvania
Tree Bank
</figureCaption>
<bodyText confidence="0.998325590909091">
etc., are therefore deleted before trees are compared. Af-
ter these eliminations, brackets with no elements inside,
brackets around single words, and multiple brackets sur-
rounding the same sequence of words are removed. Fi-
nally, these two bracketed sequences are compared.
We count the number of brackets in the standard out-
put (Std), the total number in the system output (Sys),
and the number of matching brackets (M). We then de-
fine —just as we did for system performance — measures
of recall (= M / Std) and precision (= M / Sys). We also
count the number of &amp;quot;crossings&amp;quot;: the number of cases
where a bracketed sequence from the standard overlaps
a bracketed sequence from the system output, but nei-
ther sequence is properly contained in the other. Most
automatic parsing systems generate considerably more
detailed bracketing than that produced by the Treebank;
in such cases precision would be reduced, but the recall
and crossing count would still accurately reflect the cor-
rectness of the system-generated parses.
Phil Harrison and Steven Abney have developed and
distributed a program which computes these various
measures.
</bodyText>
<sectionHeader confidence="0.985274" genericHeader="method">
5 Improving Alignment
</sectionHeader>
<bodyText confidence="0.999803">
Even with all the parse tree simplifications described in
the previous section, there can still be substantial sys-
tematic differences between the Treebank standard and
system output. The NYU system is a particularly good
(bad?) example in this regard, since it is based on Har-
ris&apos;s Linguistic String Theory and the trees produced are
therefore quite different in several respects from X-bar
</bodyText>
<page confidence="0.988666">
157
</page>
<bodyText confidence="0.9999409375">
trees. The result is a uniform degradation of recall and
precision.
To reduce these differences, we have implemented a
simple tree transducer to restructure the output of our
parser. This is implemented as a set of tree rewriting
rules which are applied at all levels of the parse tree.
Our primary goal has been to reduce crossings and in-
crease recall by producing trees which correspond more
closely to the standard; secondarily, we have also aimed
to increase precision by eliminating levels of bracketing
in our trees.
One of the principal differences is that our gram-
mar divides the sentence (ASSERTION) into SUBJECT,
VERB, and OBJECT, while the standard divides it into
NP (subject) and VP (verb -I- object). We therefore in-
clude a rule to insert a VP node into our trees:
</bodyText>
<equation confidence="0.9976646">
((ASSERTION (SA . ?sal)
(SUBJECT . ?subj)
(SA . ?sa2)
(VERB . ?verb)
(SA . ?sa3)
(OBJECT . ?obi)
(SA . ?sa4))
-&gt;
(S (SA . ?sal)
(S (SUBJECT . ?subj)
(SA . ?sa2)
(VP (VERB . ?verb)
(SA . ?sa3)
(OBJECT . ?obj))
(SA . ?sa4))))
</equation>
<bodyText confidence="0.999978025641026">
This rule is complicated by the presence of SAs (sentence
adjuncts), which may be empty. Depending on their
position, they are placed in the restructured tree beneath
VP, beneath S, or as a sister of the main S.
Differences in basic constituent structure have further
ramifications for the scoping of coordinate conjunction.
For example, &amp;quot;The terrorists attacked the village and
murdered the mayor.&amp;quot; will be analyzed as NP VP and
VP in the standard, while the PROTEUS system will ana-
lyze it as ASSERTION and ASSERTION, with the SUB-
JECT of the second ASSERTION empty. We therefore
include a rule to restructure this as VP and VP.
Rewrite rules are also required to handle differences in
the treatment of right modifiers of the noun. We have a
uniform structure for the NP, left modifiers + head noun
+ right modifiers. The standard, in contrast, uses rules
of the form NP NP VP, so that a verbal right modifier
is treated as a right sister of the structure consisting of
the head and left modifiers. Finally, since the standard
employs only minimal bracketing of left modifiers of the
noun, we have included rules to eliminate most of that
structure from our trees. All told, we currently have 26
rewriting rules.
Our set of restructuring rules is not complete. More
brackets would need to be eliminated to match the level
of bracketing used in the standard. Also, we treat certain
phrases (such as &amp;quot;according to&amp;quot;, &amp;quot;because of&amp;quot;, and &amp;quot;as
a consequence of&amp;quot;) as idioms, and so bracket them as
a single unit, whereas they are assigned a full phrase
structure in the standard. To gauge how closely our
restructured trees match the standard, we have taken
a set of 13 sentences which appear to parse correctly
with our system and scored them against the standard.
Without the restructuring rules we obtained an average
recall of 85.38%, an average precision of 76.55%, and
an average of 0.15 crossings per sentence. Applying the
restructuring rules improved these scores to an average
recall of 94.62%, an average precision of 92.48%, and an
average of 0.08 crossings per sentence.
</bodyText>
<sectionHeader confidence="0.999012" genericHeader="method">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999909">
For an evaluation of our different parsing heuristics, we
took the first 33 of the 356 news reports from the MUC-
3 corpus which had been bracketed by the University
of Pennsylvania. Three of these were deleted because
of differences in the way the sentence boundaries were
assigned by our system and UPenn. This left 30 reports
with 317 sentences for the evaluation runs.
Our syntactic analyzer uses an augmented-context-
free grammar: a context-free core plus a set of con-
straints. Some of these are stated as absolute con-
straints: if the constraint is violated, the analysis is re-
jected. Others are stated as preferences: associated with
each of these constraints is a penalty to be assessed if
the constraint is violated. Penalties from different con-
straints are combined additively, and the analysis with
the least penalty is selected as the final parse. The parser
uses a best-first search in which, at each point, the hy-
pothesis with the lowest penalty is pursued. Among
the constraints which are realized as preferences in the
system are: some grammatical constraints (including
adverb position, count noun, and comma constraints),
closest attachment of modifiers, statistical preference for
productions, and all semantic constraints.
</bodyText>
<subsectionHeader confidence="0.966051">
6.1 The base run
</subsectionHeader>
<bodyText confidence="0.99997172">
As a &amp;quot;base run&amp;quot;, we excluded almost all preferential con-
straints. We left in penalties on two constructs which,
while grammatical, lead to substantial extra computa-
tion — headless noun phrases and relative clauses with-
out relative pronouns. We also left in the ability to re-
lax (with penalty) a few grammatical constraints: con-
straints on adverb position, the constraint on count
nouns requiring determiners, and the constraints on
commas. Most sentences do not involve either of these
constructs or violate any of these constraints, and so do
not incur any penalty. In fact, most sentences get lots
of parses under these conditions: on the average we got
60 parses per sentence (these sentences are an average of
22.5 words long).
Since the parsing algorithm requires in the worst case
exponential time, we place some bound on the number of
hypotheses (edges in the chart) which may be generated
for each sentence; for the current evaluation, we used a
limit of 33000. For some sentences, all hypotheses can
be explored within this limit; for the others, parsing is
terminated when the edge limit is reached. Each set of
sentences may be further divided into those which obtain
one or more parses and those which do not obtain any.
Adding heuristics in the form of additional preferences
may reduce the search space, so that some sentences
</bodyText>
<page confidence="0.996843">
158
</page>
<bodyText confidence="0.999963703703704">
which previously hit the edge limit without getting a
parse will now get a parse. These preferences can thus
improve recall and precision in two distinct ways: by
selecting among parses produced by the base run in bet-
ter than random fashion, and by shifting some sentences
from the &amp;quot;no parse (edge limit)&amp;quot; column to the &amp;quot;parsed&amp;quot;
column. In order to separate these two effects, we also
compute the averages of crossings, recall, and precision
over the subset of 206 sentences which parsed in the base
run and did not reach the edge limit. Improvements in
these numbers will show the value of a particular prefer-
ence as a filter in selecting among parses, separate from
its value in guiding the parsing process.
For the base run on the subset of 206 sentences, we
computed two sets of statistics. For the first set, when
a sentence has N &gt; 1 parses, we weight each parse by
1/N in computing the average number of crossings and
the total recall and precision. This is in some sense a
fair evaluation of how well we would do if we were forced
to pick a single parse for each sentence and we did so
at random. The second set of statistics is computed by
selecting (from the parses for one sentence) the parse
with the minimal number of crossings and, among these
the parse with the highest recall. This represents how
well we could do — given the limitations of our base
grammar — if we had an ideal filter to select among the
parses generated.
</bodyText>
<subsectionHeader confidence="0.999969">
6.2 Parsing Heuristics
</subsectionHeader>
<bodyText confidence="0.999993833333333">
We describe below briefly the various heuristics used in
our experiments. The results computed over the entire
corpus are summarized in Table 1; results over the 206
sentences which parsed in the base run without hitting
the edge limit are summarized in Table 2. For sentences
with N &gt; 1 parses, we weight each parse by 1/N in com-
puting the average number of crossings and total recall
and precision (as we did for the first set of statistics for
the base run, described in the previous paragraph). In
addition, we have included in Table 2, line lm, the sec-
ond set of statistics for the base run, based on picking
the best parse for each sentence.
</bodyText>
<subsectionHeader confidence="0.955455">
6.2.1 Closest attachment
</subsectionHeader>
<bodyText confidence="0.999223428571429">
The simplest preference we tested was closest attach-
ment of modifiers. We penalized each modifier by the
number of words separating the modifier from the word
it modifies, with some additional penalty for successive
modifiers at the same level of the tree. This produced
an improvement in performance on all measures (line 2
in both tables).
</bodyText>
<subsectionHeader confidence="0.965627">
6.2.2 Statistical grammar
</subsectionHeader>
<bodyText confidence="0.999974590909091">
Using a sample of 260 sentence from the MUC-3 cor-
pus (disjoint from the set used here for evaluation),
we computed the probability of each production in
our context-free grammar using an iterative unsuper-
vised training scheme similar to the inside-outside al-
gorithm [Fujisaki, 1984; Chitrao and Grishman, 1990;
Chitrao, 1990]. We then used the logarithms of the prob-
abilities as penalties in applying the productions during
our analysis of the evaluation corpus. We used these sta-
tistical weights by themselves and in combination with
closest attachment. Note that statistical weighting by
itself is not particularly effective in focusing the search,
since longer hypotheses almost invariably have higher
penalties than shorter ones; as we have previously re-
ported, its effectiveness is greatly increased when com-
bined with a weighting scheme which prefers longer hy-
potheses. Statistical weighting by itself produced an im-
provement on all measures (line 3). Furthermore, the
combination of statistical weighting and closest attach-
ment (line 4) did better than either heuristic separately,
except for a slight loss in average recall over the entire
corpus.
</bodyText>
<subsectionHeader confidence="0.991559">
6.2.3 Merging
</subsectionHeader>
<bodyText confidence="0.99960575">
In our base parser, alternative hypotheses are never
merged. If X is a non-terminal symbol of our grammar,
we may generate several separate edges in the chart rep-
resenting X spanning words wi to w2, each representing
a different analysis. When merging is enabled, we re-
tain only the highest-scoring analysis, so there will be
only one edge for each non-terminal and span of words.1
Merging, when added to the statistical grammar and
closest attachment, had little effect on the performance
of sentences which parsed in the base run (Table 2, line
5). However, it did substantially increase the number of
sentences parsed (by reducing the number of hypotheses
to be followed, it allowed sentences which had previously
hit the edge limit to complete), and therefore increased
the average recall over the entire corpus (and the number
of crossings).
</bodyText>
<subsectionHeader confidence="0.922216">
6.2.4 Junk
</subsectionHeader>
<bodyText confidence="0.999957826086957">
Although our grammar has moderately broad cover-
age, there are certainly still a fair number of grammatical
structures which have not been included. In addition,
there are frequent passages not conforming to &amp;quot;standard
grammar&amp;quot;, particularly in the reports containing verba-
tim transcripts of speeches. Recognizing that most of
these sentences still include long grammatical sequences,
we have taken two different measures to attempt to an-
alyze them.
Our first measure involved a small modification to our
grammar. We introduced a new non-terminal, JUNK.
JUNK was permitted to occur wherever a sentence mod-
ifier, a post-nominal modifier, or a pre-nominal adjec-
tive could occur. With a fixed penalty, JUNK would
match a single word or any sequence of words delimited
by dashes, parentheses, or brackets. With an additional
penalty per word, it could match any sequence of two
or more words. This, roughly speaking, should find the
grammatical analysis skipping the minimum number of
words.
Adding JUNK has a slight effect on the set of sentences
which parsed in the base run.2 However, it substantially
increased the number of sentences parsed and thus the
</bodyText>
<footnote confidence="0.940292166666667">
1There are a few exceptions to this merging; in particu-
lar, we do not merge different complement structures because
there are subcategorization constraints which check for a spe-
cific complement.
20nly because the mix of penalties caused one sentence
which had previously parsed to hit the edge limit.
</footnote>
<page confidence="0.9986495">
159
160
</page>
<bodyText confidence="0.998298333333333">
overall recall (Table 1, line 6 vs. line 4). Combining
JUNK and merging produced even more dramatic im-
provements (line 7).
</bodyText>
<subsubsectionHeader confidence="0.434474">
6.2.5 Fitted parse
</subsubsectionHeader>
<bodyText confidence="0.999863090909091">
The second measure we introduced to process sen-
tences which still did not parse was the fitted parse. The
basic idea of a fitted parse is to &amp;quot;cover&amp;quot; the sentence
using constituents of a few types [Jensen et al., 19831.
In our implementation, we looked first for the longest
initial S and then repeatedly for the longest S or NP in
the remaining words. This managed to get some sort of
&amp;quot;parse&amp;quot; for all but two sentences in the corpus. Includ-
ing these in the scoring produced a further jump in total
average recall, to over 70%, but — not surprisingly —
with a loss of precision and an increase in crossings.
</bodyText>
<sectionHeader confidence="0.414864" genericHeader="method">
6.2.6 Overview
</sectionHeader>
<bodyText confidence="0.999808">
While all the heuristics do appreciably better than the
base run, we can see that, except for the base run, the
recall for the subset of 206 sentences (Table 2) varies
little (less than 3%). Most of the gain in recall, measured
over the entire corpus (Table 1), therefore reflects the
ability to analyze a larger number of sentences (without
a substantial degradation in parsing quality).
</bodyText>
<subsectionHeader confidence="0.993921">
6.3 System performance
</subsectionHeader>
<bodyText confidence="0.999953222222222">
Table 3 shows the performance of the entire extraction
system for the same 8 runs. For each run, the total recall
and precision (in terms of slots filled in the templates) is
tabulated. The precision is fairly constant; the system
recall correlates very roughly with the average parse tree
recall measured over the entire corpus. As we have just
noted, the average parse recall is in turn (except for the
base run) closely correlated to the total number of sen-
tences parsed.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999987586206896">
The availability of a substantial file of parses conforming
to a relatively well-specified standard affords a number
of opportunities for interesting computational linguistic
research. One such opportunity is the evaluation of both
grammars and parsing strategies, such as we have shown
in this paper. We believe that such evaluation will be
crucial to the further development of efficient, broad-
coverage syntactic analyzers. By restructuring the gen-
erated parse trees to conform to those of the standard,
and using agreed-upon comparison metrics, it should be
possible to make meaningful comparisons across systems,
even those using very different strategies.
We intend to apply our evaluation methods to addi-
tional parsing heuristics in the near future. In particular,
we noted earlier that selectional constraints are imple-
mented as penalties in our system (preference seman-
tics), and these evaluation methods can assist us in ob-
taining a proper balance of syntactic and semantic penal-
ties.
By extending our approach, it should be possible to
produce more detailed diagnostic information about the
parser output. For example, by stripping away some of
the structure from both the standard and the system-
generated parses, it would be possible to focus on the
system&apos;s performance on NP bracketing or on S bracket-
ing. Such an approach should also allow for meaningful
comparisons with some systems (e.g., some fast deter-
ministic parsers) which generate only more local struc-
tures and postpone attachment decisions.
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999750673469388">
[Black et al., 19911 Ezra Black, Steven Abney, Dan
Flickenger, Claudia Gdaniec, Ralph Grishman, Philip
Harrison, Donald Hindle, Robert Ingria, Fred Jelinek,
Judith Klavans, Mark Liberman, Mitch Marcus, Salim
Roukos, Beatrice Santorini, and Tomek Strzalkowski.
A procedure for quantitatively comparing the syntac-
tic coverage of English. In Proceedings of the Speech
and Natural Language Workshop, pages 306-311, Pa-
cific Grove, CA, February 1991. Morgan Kaufmann.
[Chitrao and Grishman, 1990] Mahesh
Chitrao and Ralph Grishman. Statistical parsing of
messages. In Proceedings of the Speech and Natural
Language Workshop, pages 263-266, Hidden Valley,
PA, June 1990. Morgan Kaufmann.
[Chitrao, 1990] Mahesh Chitrao. Statistical Techniques
for Parsing Messages. PhD thesis, New York Uni-
versity, 1990. Published as Proteus Project Memo-
randum No. 38, Computer Science Department, New
York University.
[Fujisaki, 1984] Tetsunosuke Fujisaki. A stochastic ap-
proach to sentence parsing. In Proc. 10th Intl Conf.
Computational Linguisitics and 22nd Annl. Meeting
Assn. Computational Linguistics, 1984.
[Grishman and Sterling, 1990] Ralph Grishman and
John Sterling. Information extraction and semantic
constraints. In Proc. 13th Int&apos;l Conf. Computational
Linguistics (COLING 90), 1990.
[Harrison et al., 1991] Philip Harrison, Steven Abney,
Ezra Black, Dan Flickinger, Claudia Gdaniec, Ralph
Grishman, Donald Hindle, Robert Ingria, Mitch Mar-
cus, Beatrice Santorini, and Tomek Strzalkowski.
Evaluating syntax performance of parser/grammars.
In Proceedings of the Natural Language Processing
Systems Evaluation Workshop, Berkeley, CA, June
1991. To be published as a Rome Laboratory Techni-
cal Report.
[Jensen et al., 1983] K. Jensen, G. E. Heidorn, L. A.
Miller, and Y. Ravin. Parse fitting and prose fixing:
getting a hold on ill-formedness. Am. J. Computa-
tional Linguistics, 9(3-4):147-160, 1983.
[Santorini, 1991] Beatrice Santorini. Bracketing guide-
lines for the penn treebank project. Department of
Computer Science, University of Pennsylvania, May
1991.
[Sundheim, 1991] Beth Sundheim. Third message un-
derstanding evaluation and conference (MUC-3):
Phase 1 status report. In Proceedings of the Speech
and Natural Language Workshop, pages 301-305, Pa-
cific Grove, CA, February 1991. Morgan Kaufmann.
</reference>
<page confidence="0.998222">
161
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000481">
<title confidence="0.999977">Evaluating Parsing Strategies Using Standardized Parse Files</title>
<author confidence="0.993307">Grishman Macleod Sterling</author>
<affiliation confidence="0.9995235">Computer Science Department New York University</affiliation>
<address confidence="0.999438">715 Broadway, 7th Floor York, New York</address>
<abstract confidence="0.99935525">The availability of large files of manuallyreviewed parse trees from the University of Pennsylvania &amp;quot;tree bank&amp;quot;, along with a program for comparing system-generated parses against these &amp;quot;standard&amp;quot; parses, provides a new opportunity for evaluating different parsing strategies. We discuss some of the restructuring required to the output of our parser so that it could be meaningfully compared with these standard parses. We then describe several heuristics for improving parsing accuracy and coverage, such as closest attachment of modifiers, statistical grammars, and fitted parses, and present a quantitative evaluation of the improvements obtained with each strategy. 1 The Problem The systematic improvement of parsing strategies requires some way of evaluating competing strategies. We need to understand their effect on both overall system performance and the ability to parse sentences correctly. In order to evaluate parser output, we will need a file of standard &amp;quot;correct&amp;quot; parses and a mechanism for comparing parser output with this standard. Preparing such a file is a quite time-consuming operation. In addition, if we would like our comparison of strategies to be meaningful to other researchers and extendible to other systems, we would like a standard and a metric which are, as much as possible, system independent. One resource which is newly available to meet this need is the &amp;quot;tree bank&amp;quot; at the University of Pennsylvania. This bank includes a large number of parse trees which have been prepared manually following a fairly detailed standard. At first glance the comparison of parse trees among systems based on different theories would seem to be very difficult. However, based on an idea by Ezra Black [Black al., a group organized by Phil Harrison has developed a metric and a program for performing just such a comparison. A preliminary assessment of this metric, based on a small</abstract>
<note confidence="0.8912305">This paper is based upon work supported by the Defense Advanced Research Projects Agency under Grant N00014-90- J-1851 from the Office of Naval Research, and by the National Science Foundation under Grant IRI-89-02304.</note>
<abstract confidence="0.986722863636365">sample of short sentences (50 13-word sentences), has quite promising [Harrison al., Our goal, in the work described here, was to extend this effort to larger samples and more complex sentences and to use the metric to compare various parsing heuristics which we have employed in our system. We describe briefly in the next sections the nature of the task and the text to which is was applied, the form of the tree bank, and the comparison metric. We consider some of the systematic mis-alignments between the standard and our output, and describe how we have reduced these. Finally, we describe some of our parsing strategies and see how they affect both the parse scores and the overall system performance. 2 The Application Task The experiments described below were performed on a version of the PROTEUS system as prepared for MUC-3, the third Message Understanding Conference [Sundheim, 1991]. The task set before the participants at these Conis one of extraction: free-text input on a particular subject matter, extracting specified types of information, and filling a data base with this information. For MUC-3, the texts consisted of short news reports about terrorist incidents; most are in typical newspaper style, with complex sentences, although transcripts of speeches and radio broadcasts are also included. The PROTEUS system has five basic stages of processing: syntactic analysis, semantic analysis, reference resolution, discourse analysis, and data base creation. Syntactic analysis consists of parsing with a broad-coverage English grammar and dictionary, followed by syntactic regularization. The regularized parse is then given to the semantic analyzer, which produces a predicate-argument structure. The semantic analyzer also provides feedback to the parser as to whether the parse satisfies semantic (selectional) constraints. Thus correct data base creation is dependent on correct predicate-argument structures, which are in turn dependent on at least locally correct syntactic structures. Because of the richness of the text, it is not possible to provide a complete set of semantic patterns. The semantic patterns included in the system are largely limited to those relevant for filling data base entries. For the terrorist domain, this includes patterns for all sorts of terrorist 156 incidents (attacks, bombings, ...), for the entities which can be involved in these incidents (as perpetrators, targets, instruments, ...), and for other structures which may affect truth value or certainty (claim, deny, allege, ...). We use a system of preference semantics which penalizes but does not exclude constructs not matching any of these semantic patterns [Grishman and Sterling, 1990]. Our parser does a best-first search for the analysis with the lowest penalty. The information extraction task has the benefit of providing relatively clear measures of overall system performance. As part of MUC-3, a set of standard (correct) data base entries was prepared and a program was developed for scoring system responses against this standard. Roughly speaking, the scoring program computes three counts: Std, the number of data base fills in the standard (correct) data base; Sys, the number of fills generated by the system; and Cor, the number of correct fills generated by the system. System performance is then stated in terms of recall (= Cor / Std) and precision (= Cor / Sys). 3 The Tree Bank The goal of the &amp;quot;Treebank&amp;quot; at the University of Pennsylvania is to construct a large data base of English annotated with detailed grammatical structure. Among the texts which they have annotated is a portion of the development corpus used for MUC-3; they have annotated 356 of the 1300 articles in that corpus. Sentences are annotated in a semi-automated procedure in which sentences are first parsed automatically and these parses are then manually revised. The result is a file of labeled bracketings conforming in general terms to the syntactic structures of transformational generative grammar (X-bar theory). The bracketing is shown in considerable detail at the clause level; some of the details at the NP level are suppressed. The guidelines for the bracketing are given in [Santorini, 1991]. A sample sentence from the MUC-3 corpus, as bracketed for the Treebank, is shown in Figure 1. 4 The Evaluation Metric The basic idea for the evaluation metric was developed by Ezra Black. It was then refined and tested at a workshop (and through extensive subsequent electronic communication) organized by Phil Harrison. This effort has involved computational linguists from eight sites who have applied this metric to compare — for a set of 50 short sentences — the Penn standard with the &amp;quot;ideal&amp;quot; parses their systems would generate for these sentences. At first glance the parses produced by different systems may seem so different as to be incomparable; node labels in particular may be entirely different. The metric therefore eliminates node labels and only compares tree structures. Certain constituents are treated very differently by different systems; for example, auxiliaries are treated as main verbs by some systems but not others. Several classes of constituents, such as auxiliaries, pre-infinitival &amp;quot;to&amp;quot;, &amp;quot;not&amp;quot;, null elements, punctuation, ( (S (PP in (NP cartagena)) (S (NP it) was (VP reported (SBAR (SBAR that (S (NP cast ellar) (VP faced (NP a ft revolutionary trial (PP by the and (SEAR that (S (NP he) was (VP (VP found (ADJP guilty)) and (VP executed)))))))) .) 1: sample sentence from the U. of Pennsylvania Tree Bank etc., are therefore deleted before trees are compared. After these eliminations, brackets with no elements inside, brackets around single words, and multiple brackets surrounding the same sequence of words are removed. Finally, these two bracketed sequences are compared. We count the number of brackets in the standard output (Std), the total number in the system output (Sys), and the number of matching brackets (M). We then define —just as we did for system performance — measures of recall (= M / Std) and precision (= M / Sys). We also count the number of &amp;quot;crossings&amp;quot;: the number of cases where a bracketed sequence from the standard overlaps a bracketed sequence from the system output, but neither sequence is properly contained in the other. Most automatic parsing systems generate considerably more detailed bracketing than that produced by the Treebank; in such cases precision would be reduced, but the recall and crossing count would still accurately reflect the correctness of the system-generated parses. Phil Harrison and Steven Abney have developed and distributed a program which computes these various measures. 5 Improving Alignment Even with all the parse tree simplifications described in the previous section, there can still be substantial systematic differences between the Treebank standard and system output. The NYU system is a particularly good (bad?) example in this regard, since it is based on Harris&apos;s Linguistic String Theory and the trees produced are therefore quite different in several respects from X-bar 157 trees. The result is a uniform degradation of recall and precision. To reduce these differences, we have implemented a simple tree transducer to restructure the output of our parser. This is implemented as a set of tree rewriting rules which are applied at all levels of the parse tree. Our primary goal has been to reduce crossings and increase recall by producing trees which correspond more closely to the standard; secondarily, we have also aimed to increase precision by eliminating levels of bracketing in our trees. One of the principal differences is that our grammar divides the sentence (ASSERTION) into SUBJECT, VERB, and OBJECT, while the standard divides it into NP (subject) and VP (verb -Iobject). We therefore include a rule to insert a VP node into our trees: ((ASSERTION (SA . ?sal) (SUBJECT . ?subj) (SA . ?sa2) (VERB . ?verb) (SA . ?sa3) (OBJECT . ?obi) (SA . ?sa4)) -&gt; (S (SA . ?sal) (S (SUBJECT . ?subj) (SA . ?sa2) (VP (VERB . ?verb) (SA . ?sa3) (OBJECT . ?obj)) (SA . ?sa4)))) This rule is complicated by the presence of SAs (sentence adjuncts), which may be empty. Depending on their position, they are placed in the restructured tree beneath VP, beneath S, or as a sister of the main S. Differences in basic constituent structure have further ramifications for the scoping of coordinate conjunction. For example, &amp;quot;The terrorists attacked the village and the mayor.&amp;quot; will be analyzed as VP and the standard, while the PROTEUS system will anait as and ASSERTION, the SUBthe second We therefore a rule to restructure this as and VP. Rewrite rules are also required to handle differences in the treatment of right modifiers of the noun. We have a structure for the NP, modifiers + head noun right modifiers. standard, in contrast, uses rules the form NP VP, that a verbal right modifier is treated as a right sister of the structure consisting of the head and left modifiers. Finally, since the standard employs only minimal bracketing of left modifiers of the noun, we have included rules to eliminate most of that structure from our trees. All told, we currently have 26 rewriting rules. Our set of restructuring rules is not complete. More brackets would need to be eliminated to match the level of bracketing used in the standard. Also, we treat certain phrases (such as &amp;quot;according to&amp;quot;, &amp;quot;because of&amp;quot;, and &amp;quot;as a consequence of&amp;quot;) as idioms, and so bracket them as a single unit, whereas they are assigned a full phrase structure in the standard. To gauge how closely our restructured trees match the standard, we have taken a set of 13 sentences which appear to parse correctly with our system and scored them against the standard. Without the restructuring rules we obtained an average recall of 85.38%, an average precision of 76.55%, and an average of 0.15 crossings per sentence. Applying the restructuring rules improved these scores to an average recall of 94.62%, an average precision of 92.48%, and an average of 0.08 crossings per sentence. For an evaluation of our different parsing heuristics, we took the first 33 of the 356 news reports from the MUC- 3 corpus which had been bracketed by the University of Pennsylvania. Three of these were deleted because of differences in the way the sentence boundaries were assigned by our system and UPenn. This left 30 reports with 317 sentences for the evaluation runs. Our syntactic analyzer uses an augmented-contextfree grammar: a context-free core plus a set of constraints. Some of these are stated as absolute constraints: if the constraint is violated, the analysis is rejected. Others are stated as preferences: associated with each of these constraints is a penalty to be assessed if the constraint is violated. Penalties from different constraints are combined additively, and the analysis with the least penalty is selected as the final parse. The parser uses a best-first search in which, at each point, the hypothesis with the lowest penalty is pursued. Among the constraints which are realized as preferences in the system are: some grammatical constraints (including adverb position, count noun, and comma constraints), closest attachment of modifiers, statistical preference for productions, and all semantic constraints. base run a &amp;quot;base excluded almost all preferential constraints. We left in penalties on two constructs which, while grammatical, lead to substantial extra computation — headless noun phrases and relative clauses without relative pronouns. We also left in the ability to relax (with penalty) a few grammatical constraints: constraints on adverb position, the constraint on count nouns requiring determiners, and the constraints on commas. Most sentences do not involve either of these constructs or violate any of these constraints, and so do not incur any penalty. In fact, most sentences get lots of parses under these conditions: on the average we got 60 parses per sentence (these sentences are an average of 22.5 words long). Since the parsing algorithm requires in the worst case exponential time, we place some bound on the number of hypotheses (edges in the chart) which may be generated for each sentence; for the current evaluation, we used a limit of 33000. For some sentences, all hypotheses can be explored within this limit; for the others, parsing is terminated when the edge limit is reached. Each set of sentences may be further divided into those which obtain one or more parses and those which do not obtain any. Adding heuristics in the form of additional preferences may reduce the search space, so that some sentences 158 which previously hit the edge limit without getting a parse will now get a parse. These preferences can thus improve recall and precision in two distinct ways: by selecting among parses produced by the base run in better than random fashion, and by shifting some sentences from the &amp;quot;no parse (edge limit)&amp;quot; column to the &amp;quot;parsed&amp;quot; column. In order to separate these two effects, we also compute the averages of crossings, recall, and precision over the subset of 206 sentences which parsed in the base run and did not reach the edge limit. Improvements in these numbers will show the value of a particular preference as a filter in selecting among parses, separate from its value in guiding the parsing process. For the base run on the subset of 206 sentences, we computed two sets of statistics. For the first set, when sentence has &gt; parses, we weight each parse by 1/N in computing the average number of crossings and the total recall and precision. This is in some sense a fair evaluation of how well we would do if we were forced to pick a single parse for each sentence and we did so at random. The second set of statistics is computed by selecting (from the parses for one sentence) the parse with the minimal number of crossings and, among these the parse with the highest recall. This represents how well we could do — given the limitations of our base grammar — if we had an ideal filter to select among the parses generated. 6.2 Parsing Heuristics We describe below briefly the various heuristics used in our experiments. The results computed over the entire corpus are summarized in Table 1; results over the 206 sentences which parsed in the base run without hitting the edge limit are summarized in Table 2. For sentences &gt; parses, we weight each parse by 1/N in computing the average number of crossings and total recall and precision (as we did for the first set of statistics for the base run, described in the previous paragraph). In addition, we have included in Table 2, line lm, the second set of statistics for the base run, based on picking the best parse for each sentence. 6.2.1 Closest attachment The simplest preference we tested was closest attachment of modifiers. We penalized each modifier by the number of words separating the modifier from the word it modifies, with some additional penalty for successive modifiers at the same level of the tree. This produced an improvement in performance on all measures (line 2 in both tables). 6.2.2 Statistical grammar Using a sample of 260 sentence from the MUC-3 corpus (disjoint from the set used here for evaluation), we computed the probability of each production in our context-free grammar using an iterative unsupervised training scheme similar to the inside-outside algorithm [Fujisaki, 1984; Chitrao and Grishman, 1990; Chitrao, 1990]. We then used the logarithms of the probabilities as penalties in applying the productions during our analysis of the evaluation corpus. We used these statistical weights by themselves and in combination with closest attachment. Note that statistical weighting by itself is not particularly effective in focusing the search, since longer hypotheses almost invariably have higher penalties than shorter ones; as we have previously reported, its effectiveness is greatly increased when combined with a weighting scheme which prefers longer hypotheses. Statistical weighting by itself produced an improvement on all measures (line 3). Furthermore, the combination of statistical weighting and closest attachment (line 4) did better than either heuristic separately, except for a slight loss in average recall over the entire corpus. 6.2.3 Merging In our base parser, alternative hypotheses are never If a non-terminal symbol of our grammar, we may generate several separate edges in the chart repwords to each representing a different analysis. When merging is enabled, we retain only the highest-scoring analysis, so there will be one edge for each non-terminal and span of Merging, when added to the statistical grammar and closest attachment, had little effect on the performance of sentences which parsed in the base run (Table 2, line 5). However, it did substantially increase the number of sentences parsed (by reducing the number of hypotheses to be followed, it allowed sentences which had previously hit the edge limit to complete), and therefore increased the average recall over the entire corpus (and the number of crossings). 6.2.4 Junk Although our grammar has moderately broad coverage, there are certainly still a fair number of grammatical structures which have not been included. In addition, there are frequent passages not conforming to &amp;quot;standard grammar&amp;quot;, particularly in the reports containing verbatim transcripts of speeches. Recognizing that most of these sentences still include long grammatical sequences, we have taken two different measures to attempt to analyze them. Our first measure involved a small modification to our We introduced a new non-terminal, permitted to occur wherever a sentence modifier, a post-nominal modifier, or a pre-nominal adjeccould occur. With a fixed penalty, match a single word or any sequence of words delimited by dashes, parentheses, or brackets. With an additional penalty per word, it could match any sequence of two or more words. This, roughly speaking, should find the grammatical analysis skipping the minimum number of words. a slight effect on the set of sentences parsed in the base However, it substantially increased the number of sentences parsed and thus the are a few exceptions to this merging; in particular, we do not merge different complement structures because there are subcategorization constraints which check for a specific complement. because the mix of penalties caused one sentence which had previously parsed to hit the edge limit. 159 160 overall recall (Table 1, line 6 vs. line 4). Combining merging produced even more dramatic improvements (line 7). 6.2.5 Fitted parse The second measure we introduced to process senwhich still did not parse was the parse. basic idea of a fitted parse is to &amp;quot;cover&amp;quot; the sentence constituents of a few types [Jensen al., In our implementation, we looked first for the longest initial S and then repeatedly for the longest S or NP in the remaining words. This managed to get some sort of &amp;quot;parse&amp;quot; for all but two sentences in the corpus. Including these in the scoring produced a further jump in total average recall, to over 70%, but — not surprisingly — with a loss of precision and an increase in crossings. 6.2.6 Overview While all the heuristics do appreciably better than the base run, we can see that, except for the base run, the recall for the subset of 206 sentences (Table 2) varies little (less than 3%). Most of the gain in recall, measured over the entire corpus (Table 1), therefore reflects the ability to analyze a larger number of sentences (without a substantial degradation in parsing quality). 6.3 System performance Table 3 shows the performance of the entire extraction system for the same 8 runs. For each run, the total recall and precision (in terms of slots filled in the templates) is tabulated. The precision is fairly constant; the system recall correlates very roughly with the average parse tree recall measured over the entire corpus. As we have just noted, the average parse recall is in turn (except for the base run) closely correlated to the total number of sentences parsed. 7 Conclusion The availability of a substantial file of parses conforming to a relatively well-specified standard affords a number of opportunities for interesting computational linguistic research. One such opportunity is the evaluation of both grammars and parsing strategies, such as we have shown in this paper. We believe that such evaluation will be crucial to the further development of efficient, broadcoverage syntactic analyzers. By restructuring the generated parse trees to conform to those of the standard, and using agreed-upon comparison metrics, it should be possible to make meaningful comparisons across systems, even those using very different strategies. We intend to apply our evaluation methods to additional parsing heuristics in the near future. In particular, we noted earlier that selectional constraints are implemented as penalties in our system (preference semantics), and these evaluation methods can assist us in obtaining a proper balance of syntactic and semantic penalties. By extending our approach, it should be possible to produce more detailed diagnostic information about the parser output. For example, by stripping away some of the structure from both the standard and the systemgenerated parses, it would be possible to focus on the system&apos;s performance on NP bracketing or on S bracketing. Such an approach should also allow for meaningful comparisons with some systems (e.g., some fast deterministic parsers) which generate only more local structures and postpone attachment decisions.</abstract>
<note confidence="0.550364">References</note>
<degree confidence="0.7514565">al., Ezra Black, Steven Abney, Dan Flickenger, Claudia Gdaniec, Ralph Grishman, Philip</degree>
<author confidence="0.871780666666667">Donald Hindle Harrison</author>
<author confidence="0.871780666666667">Robert Ingria</author>
<author confidence="0.871780666666667">Fred Jelinek</author>
<author confidence="0.871780666666667">Judith Klavans</author>
<author confidence="0.871780666666667">Mark Liberman</author>
<author confidence="0.871780666666667">Mitch Marcus</author>
<author confidence="0.871780666666667">Salim Roukos</author>
<author confidence="0.871780666666667">Beatrice Santorini</author>
<author confidence="0.871780666666667">Tomek Strzalkowski</author>
<note confidence="0.8258971">A procedure for quantitatively comparing the syntaccoverage of English. In of the Speech Natural Language Workshop, 306-311, Pacific Grove, CA, February 1991. Morgan Kaufmann. [Chitrao and Grishman, 1990] Mahesh Chitrao and Ralph Grishman. Statistical parsing of In of the Speech and Natural Workshop, 263-266, Hidden Valley, PA, June 1990. Morgan Kaufmann. 1990] Mahesh Chitrao. Techniques</note>
<affiliation confidence="0.867883">Parsing Messages. thesis, New York Uni-</affiliation>
<address confidence="0.97011">versity, 1990. Published as Proteus Project Memorandum No. 38, Computer Science Department, New</address>
<affiliation confidence="0.901571">York University.</affiliation>
<address confidence="0.512933">[Fujisaki, 1984] Tetsunosuke Fujisaki. A stochastic ap-</address>
<note confidence="0.989071285714286">to sentence parsing. In 10th Intl Conf. Computational Linguisitics and 22nd Annl. Meeting Computational Linguistics, [Grishman and Sterling, 1990] Ralph Grishman and John Sterling. Information extraction and semantic In 13th Int&apos;l Conf. Computational (COLING 90),</note>
<author confidence="0.844934">Philip Harrison al</author>
<author confidence="0.844934">Steven Abney</author>
<author confidence="0.844934">Ezra Black</author>
<author confidence="0.844934">Dan Flickinger</author>
<author confidence="0.844934">Claudia Gdaniec</author>
<author confidence="0.844934">Ralph Grishman</author>
<author confidence="0.844934">Donald Hindle</author>
<author confidence="0.844934">Robert Ingria</author>
<author confidence="0.844934">Mitch Mar-</author>
<abstract confidence="0.437731">cus, Beatrice Santorini, and Tomek Strzalkowski. Evaluating syntax performance of parser/grammars. of the Natural Language Processing</abstract>
<note confidence="0.945052235294118">Evaluation Workshop, CA, June 1991. To be published as a Rome Laboratory Technical Report. al., K. Jensen, G. E. Heidorn, L. A. Miller, and Y. Ravin. Parse fitting and prose fixing: a hold on ill-formedness. J. Computa- Linguistics, 1983. [Santorini, 1991] Beatrice Santorini. Bracketing guidelines for the penn treebank project. Department of Computer Science, University of Pennsylvania, May 1991. [Sundheim, 1991] Beth Sundheim. Third message understanding evaluation and conference (MUC-3): 1 status report. In of the Speech Natural Language Workshop, 301-305, Pacific Grove, CA, February 1991. Morgan Kaufmann. 161</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>A procedure for quantitatively comparing the syntactic coverage of English.</title>
<date>1991</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<pages>306--311</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Pacific Grove, CA,</location>
<marker>1991</marker>
<rawString>[Black et al., 19911 Ezra Black, Steven Abney, Dan Flickenger, Claudia Gdaniec, Ralph Grishman, Philip Harrison, Donald Hindle, Robert Ingria, Fred Jelinek, Judith Klavans, Mark Liberman, Mitch Marcus, Salim Roukos, Beatrice Santorini, and Tomek Strzalkowski. A procedure for quantitatively comparing the syntactic coverage of English. In Proceedings of the Speech and Natural Language Workshop, pages 306-311, Pacific Grove, CA, February 1991. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<title>Mahesh Chitrao and Ralph Grishman. Statistical parsing of messages.</title>
<date>1990</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<pages>263--266</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Hidden Valley, PA,</location>
<marker>1990</marker>
<rawString>[Chitrao and Grishman, 1990] Mahesh Chitrao and Ralph Grishman. Statistical parsing of messages. In Proceedings of the Speech and Natural Language Workshop, pages 263-266, Hidden Valley, PA, June 1990. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<title>Mahesh Chitrao. Statistical Techniques for Parsing Messages.</title>
<date>1990</date>
<booktitle>Published as Proteus Project Memorandum No.</booktitle>
<tech>PhD thesis,</tech>
<volume>38</volume>
<institution>New York University,</institution>
<marker>1990</marker>
<rawString>[Chitrao, 1990] Mahesh Chitrao. Statistical Techniques for Parsing Messages. PhD thesis, New York University, 1990. Published as Proteus Project Memorandum No. 38, Computer Science Department, New York University.</rawString>
</citation>
<citation valid="true">
<title>Tetsunosuke Fujisaki. A stochastic approach to sentence parsing.</title>
<date>1984</date>
<booktitle>In Proc. 10th Intl Conf. Computational Linguisitics and 22nd Annl. Meeting Assn. Computational Linguistics,</booktitle>
<marker>1984</marker>
<rawString>[Fujisaki, 1984] Tetsunosuke Fujisaki. A stochastic approach to sentence parsing. In Proc. 10th Intl Conf. Computational Linguisitics and 22nd Annl. Meeting Assn. Computational Linguistics, 1984.</rawString>
</citation>
<citation valid="true">
<title>Ralph Grishman and</title>
<date>1990</date>
<booktitle>In Proc. 13th Int&apos;l Conf. Computational Linguistics (COLING 90),</booktitle>
<marker>1990</marker>
<rawString>[Grishman and Sterling, 1990] Ralph Grishman and John Sterling. Information extraction and semantic constraints. In Proc. 13th Int&apos;l Conf. Computational Linguistics (COLING 90), 1990.</rawString>
</citation>
<citation valid="false">
<title>Evaluating syntax performance of parser/grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the Natural Language Processing Systems Evaluation Workshop,</booktitle>
<tech>Technical Report.</tech>
<location>Berkeley, CA,</location>
<marker>1991</marker>
<rawString>[Harrison et al., 1991] Philip Harrison, Steven Abney, Ezra Black, Dan Flickinger, Claudia Gdaniec, Ralph Grishman, Donald Hindle, Robert Ingria, Mitch Marcus, Beatrice Santorini, and Tomek Strzalkowski. Evaluating syntax performance of parser/grammars. In Proceedings of the Natural Language Processing Systems Evaluation Workshop, Berkeley, CA, June 1991. To be published as a Rome Laboratory Technical Report.</rawString>
</citation>
<citation valid="true">
<title>Parse fitting and prose fixing: getting a hold on ill-formedness.</title>
<date>1983</date>
<journal>Am. J. Computational Linguistics,</journal>
<pages>9--3</pages>
<marker>1983</marker>
<rawString>[Jensen et al., 1983] K. Jensen, G. E. Heidorn, L. A. Miller, and Y. Ravin. Parse fitting and prose fixing: getting a hold on ill-formedness. Am. J. Computational Linguistics, 9(3-4):147-160, 1983.</rawString>
</citation>
<citation valid="true">
<title>Beatrice Santorini. Bracketing guidelines for the penn treebank project.</title>
<date>1991</date>
<institution>Department of Computer Science, University of Pennsylvania,</institution>
<marker>1991</marker>
<rawString>[Santorini, 1991] Beatrice Santorini. Bracketing guidelines for the penn treebank project. Department of Computer Science, University of Pennsylvania, May 1991.</rawString>
</citation>
<citation valid="true">
<title>Beth Sundheim. Third message understanding evaluation and conference (MUC-3): Phase 1 status report.</title>
<date>1991</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<pages>301--305</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Pacific Grove, CA,</location>
<marker>1991</marker>
<rawString>[Sundheim, 1991] Beth Sundheim. Third message understanding evaluation and conference (MUC-3): Phase 1 status report. In Proceedings of the Speech and Natural Language Workshop, pages 301-305, Pacific Grove, CA, February 1991. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>