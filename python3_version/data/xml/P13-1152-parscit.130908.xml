<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.998821">
Combining Referring Expression Generation and Surface Realization:
A Corpus-Based Investigation of Architectures
</title>
<author confidence="0.983353">
Sina Zarrieß Jonas Kuhn
</author>
<affiliation confidence="0.9605975">
Institut f¨ur maschinelle Sprachverarbeitung
University of Stuttgart, Germany
</affiliation>
<email confidence="0.994646">
sina.zarriess,jonas.kuhn@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.99379" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999824307692308">
We suggest a generation task that inte-
grates discourse-level referring expression
generation and sentence-level surface re-
alization. We present a data set of Ger-
man articles annotated with deep syntax
and referents, including some types of im-
plicit referents. Our experiments compare
several architectures varying the order of a
set of trainable modules. The results sug-
gest that a revision-based pipeline, with in-
termediate linearization, significantly out-
performs standard pipelines or a parallel
architecture.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999864888888889">
Generating well-formed linguistic utterances from
an abstract non-linguistic input involves making
a multitude of conceptual, discourse-level as well
as sentence-level, lexical and syntactic decisions.
Work on rule-based natural language generation
(NLG) has explored a number of ways to com-
bine these decisions in an architecture, ranging
from integrated systems where all decisions hap-
pen jointly (Appelt, 1982) to strictly sequential
pipelines (Reiter and Dale, 1997). While inte-
grated or interactive systems typically face issues
with efficiency and scalability, they can directly
account for interactions between discourse-level
planning and linguistic realization. For instance,
Rubinoff (1992) mentions Example (1) where the
sentence planning component needs to have ac-
cess to the lexical knowledge that “order” and not
“home” can be realized as a verb in English.
</bodyText>
<listItem confidence="0.5799355">
(1) a. *John homed him with an order.
b. John ordered him home.
</listItem>
<bodyText confidence="0.999972318181818">
In recent data-driven generation research, the
focus has somewhat shifted from full data-to-text
systems to approaches that isolate well-defined
subproblems from the NLG pipeline. In particular,
the tasks of surface realization and referring ex-
pression generation (REG) have received increas-
ing attention using a number of available anno-
tated data sets (Belz and Kow, 2010; Belz et al.,
2011). While these single-task approaches have
given rise to many insights about algorithms and
corpus-based modelling for specific phenomena,
they can hardly deal with aspects of the architec-
ture and interaction between generation levels.
This paper suggests a middle ground between
full data-to-text and single-task generation, com-
bining two well-studied NLG problems. We in-
tegrate a discourse-level approach to REG with
sentence-level surface realization in a data-driven
framework. We address this integrated task with a
set of components that can be trained on flexible
inputs which allows us to systematically explore
different ways of arranging the components in a
generation architecture. Our main goal is to inves-
tigate how different architectural set-ups account
for interactions between generation decisions at
the level of referring expressions (REs), syntax
and word order.
Our basic set-up is inspired from the Generating
Referring Expressions in Context (GREC) tasks,
where candidate REs have to be assigned to in-
stances of a referent in a Wikipedia article (Belz
and Kow, 2010). We have created a dataset of Ger-
man texts with annotations that extend this stan-
dard in three substantial ways: (i) our domain con-
sists of articles about robbery events that mainly
involve two main referents, a victim and a per-
petrator (perp), (ii) annotations include deep and
shallow syntactic relations similar to the repre-
sentations used in (Belz et al., 2011) (iii) anno-
tations include empty referents, as e.g. in passives
and nominalizations directing attention to the phe-
nomenon of implicit reference, which is largely
understudied in NLG. Figure 1 presents an exam-
ple for a deep syntax tree with underspecified RE
</bodyText>
<page confidence="0.958067">
1547
</page>
<note confidence="0.917563">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1547–1557,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.993357714285714">
(Tree) be
agent mod mod
perp on because
pobj sub
trial attack
agent theme
perp victim
</figure>
<figureCaption confidence="0.99999">
Figure 1: Underspecified tree with RE candidates
</figureCaption>
<bodyText confidence="0.9792466">
slots and lists of candidates REs for each referent.
Applying a strictly sequential pipeline on our
data, we observe incoherent system output that
is related to an interaction of generation levels,
very similar to the interleaving between sentence
planning and lexicalization in Example (1). A
pipeline that first inserts REs into the underspec-
ified tree in Figure 1, then generates syntax and fi-
nally linearizes, produces inappropriate sentences
like (2-a).
</bodyText>
<listItem confidence="0.43461075">
(2) a. *[The two men]v are on trial because of an attack
by [two italians],, on [a young man],,.
b. [Two italians],, are on trial because of an attack on
[a young man],,.
</listItem>
<bodyText confidence="0.999508736842105">
Sentence (2-a) is incoherent because the syntac-
tic surface obscurs the intended meaning that “two
italians” and “the two men” refer to the same ref-
erent. In order to generate the natural Sentence
(2-b), the RE component needs information about
linear precedence of the two perp instances and the
nominalization of “attack”. These types of inter-
actions between referential and syntactic realiza-
tion have been thoroughly discussed in theoretical
accounts of textual coherence, as e.g. Centering
Theory (Grosz et al., 1995).
The integrated modelling of REG and surface
realization leads to a considerable expansion of
the choice space. In a sentence with 3 referents
that each have 10 RE candidates and can be freely
ordered, the number of surface realizations in-
creases from 6 to 6·103, assuming that the remain-
ing words can not be syntactically varied. Thus,
even when the generation problem is restricted to
these tasks, a fully integrated architecture faces
scalability issues on realistic corpus data.
In this work, we assume a modular set-up of
the generation system that allows for a flexible
ordering of the single components. Our experi-
ments vary 3 parameters of the generation archi-
tecture: 1) the sequential order of the modules,
2) parallelization of modules, 3) joint vs. sepa-
rate modelling of implicit referents. Our results
suggest that the interactions between RE and syn-
tax can be modelled in sequential generation ar-
chitecture where the RE component has access
to information about syntactic realization and an
approximative, intermediate linearization. Such
a system is reminiscent of earlier work in rule-
based generation that implements an interactive or
revision-based feedback between discourse-level
planning and linguistic realisation (Hovy, 1988;
Robin, 1993).
</bodyText>
<sectionHeader confidence="0.999813" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99977825">
Despite the common view of NLG as a pipeline
process, it is a well-known problem that high-
level, conceptual knowledge and low-level lin-
guistic knowledge are tightly interleaved (Danlos,
1984; Mellish et al., 2000). In rule-based, strictly
sequential generators these interactions can lead
to a so-called generation gap, where a down-
stream module cannot realize a text or sentence
plan generated by the preceding modules (Meteer,
1991; Wanner, 1994). For this reason, a num-
ber of other architectures has been proposed, see
De Smedt et al. (1996) for an overview. For rea-
sons of tractability and scalability, many prac-
tical NLG systems still have been designed as
sequential pipelines that follow the basic layout
of macroplanning-microplanning-linguistic real-
ization (Reiter, 1994; Cahill et al., 1999; Bateman
and Zock, 2003).
In recent data-driven research on NLG, many
single tasks have been addressed with corpus-
based methods. For surface realization, the stan-
dard set-up is to regenerate from syntactic rep-
resentations that have been produced for realis-
tic corpus sentences. The first widely known sta-
tistical approach by Langkilde and Knight (1998)
used language-model n-gram statistics on a word
lattice of candidate realisations to guide a ranker.
Subsequent work explored ways of exploiting lin-
guistically annotated data for trainable generation
models (Ratnaparkhi, 2000; Belz, 2005). Work on
data-driven approaches has led to insights about
the importance of linguistic features for sentence
</bodyText>
<figure confidence="0.9945801">
perp italians
two
men
the two
they &lt;empty&gt;
victim
the
victim man
a young
he &lt;empty&gt;
</figure>
<page confidence="0.991667">
1548
</page>
<bodyText confidence="0.99996384">
linearization decisions (Ringger et al., 2004; Filip-
pova and Strube, 2007; Cahill and Riester, 2009).
(Zarrieß et al., 2012) have recently argued that
the good performance of these linguistically mo-
tivated word order models, which exploit morpho-
syntactic features of noun phrases (i.e. refer-
ents), is related to the fact that these morpho-
syntactic features implicitly encode a lot of knowl-
edge about the underlying discourse or informa-
tion structure.
A considerable body of REG research has been
done in the paradigm established by Dale (1989;
1995). More closely related to our work are ap-
proaches in the line of Siddarthan and Copes-
take (2004) or Belz and Varges (2007) who gener-
ate contextually appropriate REs for instances of a
referent in a text. Belz and Varges (2007)’s GREC
data set includes annotations of implicit subjects
in coordinations. Zarrieß et al. (2011) deal with
implicit subjects in passives, proposing a set of
heuristics for adding these agents to the genera-
tion input. Roth and Frank (2012) acquire au-
tomatic annotations of implicit roles for the pur-
pose of studying coherence patterns in texts. Im-
plicit referents have also received attention for the
analysis of semantic roles (Gerber and Chai, 2010;
Ruppenhofer et al., 2010).
Statistical methods for data-to-text generation
have been explored only recently. Belz (2008)
trains a probabilistic CFG to generate weather
forecasts, Chen et al. (2010) induce a synchronous
grammar to generate sportcaster text. Both ad-
dress a restricted domain where a direct align-
ment between units in the non-linguistic represen-
tation and the linguistic utterance can be learned.
Marciniak and Strube (2005) propose an ILP
model for global optimization in a generation task
that is decomposed into a set of classifiers. Bohnet
et al. (2011) deal with multi-level generation in a
statistical framework and in a less restricted do-
main. They adopt a standard sequential pipeline
approach.
Recent corpus-based generation approaches
faced the problem that existing standard treebank
representations for parsing or other analysis tasks
do not necessarily fit the needs of generation
(Bohnet et al., 2010; Wanner et al., 2012). Zarrieß
et al. (2011) discuss the problem of an input rep-
resentation that is appropriately underspecified for
the realistic generation of voice alternations.
</bodyText>
<sectionHeader confidence="0.956797" genericHeader="method">
3 The Data Set
</sectionHeader>
<bodyText confidence="0.999957333333333">
The data set for our generation experiments con-
sists of 200 newspaper articles about robbery
events. The articles were extracted from a large
German newspaper corpus. A complete example
text with RE annotations is given in Figure 2, Ta-
ble 1 summarizes some data set statistics.
</bodyText>
<subsectionHeader confidence="0.999295">
3.1 RE annotation
</subsectionHeader>
<bodyText confidence="0.999961151515152">
The RE annotations mark explicit and implicit
mentions of referents involved in the robbery event
described in an article. Explicit mentions are
marked as spans on the surface sentence, labeled
with the referent’s role and an ID. We annotate the
following referential roles: (i) perpetrator (perp),
(ii) victim, (iii) source, according to the core roles
of the Robbery frame in English FrameNet. We
include source since some texts do not mention a
particular victim, but rather the location of the rob-
bery (e.g. a bank, a service station). The ID distin-
guishes referents that have the same role, e.g. “the
husband” and the “young family” in Sentences
(3-a) and (3-d) in Figure 2. Each RE is linked to
its syntactic head. This complies with the GREC
data sets, and is also useful for further annotation
of the deep syntax level (see Section 3.2).
The RE implicit mentions of victim, perp, and
source are annotated as attributes of their syntac-
tic heads in the surface sentence. We consider the
following types of implicit referents: (i) agents in
passives (e.g. “robbed” in (3-a)), (ii) arguments of
nominalizations (e.g. “resistance” in (3-e)), (iii)
possessives (e.g. “watch” in (3-f)), (iv) missing
subjects in coordinations. (e.g. “flee” in (3-f))
The brat tool (Stenetorp et al., 2012) was used
for annotation. We had 2 annotators with a compu-
tational linguistic background, provided with an-
notation guidelines. They were trained on a set of
20 texts. We measure a good agreement on another
set of 15 texts: the simple pairwise agreement for
explicit mentions is 95.14%-96.53% and 78.94%-
76.92% for implicit mentions.1
</bodyText>
<subsectionHeader confidence="0.999832">
3.2 Syntax annotation
</subsectionHeader>
<bodyText confidence="0.9998255">
The syntactic annotation of our data includes two
layers: shallow and deep, labeled dependencies,
similar to the representation used in surface real-
ization shared tasks (Belz et al., 2011). We use
</bodyText>
<footnote confidence="0.999678">
1Standard measures for the “above chance annotator
agreement” are only defined for task where the set of anno-
tated items is pre-defined.
</footnote>
<page confidence="0.996193">
1549
</page>
<figure confidence="0.999433974137931">
dem Heimwegposs:v ausgeraubtag:p
the way homeposs:v robbedag:p
zwei ungepflegt wirkenden jungen M¨annern im Alter von etwa 25 Jahren p:0.
two shabby-looking young men of about 25 years .
auf
on
✞ ☎
(3) a. ✝ Junge Familie v:0
✞ ☎ ✆
✝ Young family✆
b. Die Polizei sucht nach
The police looks for
c.
am
on
✞
✝✞
☎
✝a young family with their seven month old baby ✆
☎
eine junge Familie mit ihrem sieben Monate alten Baby v:0
✆
auf
on
Sie p:0
They
sollen
are said to
Montag
Monday
gegen 20 Uhr
around 20 o’clock
dem Heimwegposs:v von einem Einkaufsbummel ¨uberfallen und ausgeraubt haben.
the way homeposs:v from a shopping tour attacked and robbed have.
d. Wie
As
berichtet,
reports,
drohten
threatened
die zwei M¨anner p:0
the two men
✄
dem Ehemann v:1,
� ✁
the husband
✂
zusammenzuschlagen.
beat up.
✁
✄
✂✄
him
✂
�
ihn v:1
✁ �
✂✄
�
✁
die
the
Polizei
police
✄
e.
1
✁
Er v:1
✂✄
He
✂
deshalb
therefore
✂
gab
gave
✁
f. Anschließend nahmen
Afterwards took
✂
Brieftasche ohne
wallet without
die R¨auber p:0
the robbers
Gegenwehrag:v,the:p
resistanceag:v,the:p
Armbanduhrposs:v
watchposs:v
fl¨uchtetenag:p.
fleedag:p.
heraus.
out.
ab
off
und
and
✄
✂✄
his
✁
�
�
seine v:1
✁
✄
�
✂✄
ihm v:1
✁ �
him
✁
die
the
noch
also
</figure>
<figureCaption confidence="0.976925">
Figure 2: Example text with RE annotations, oval boxes mark victim mentions, square boxes mark perp
mentions, heads of implicit arguments are underlined
</figureCaption>
<bodyText confidence="0.999473583333334">
the Bohnet (2010) dependency parser to obtain an
automatic annotation of shallow or surface depen-
dencies for the corpus sentences.
The deep syntactic dependencies are derived
from the shallow layer by a set of hand-written
transformation rules. The goal is to link referents
to their main predicate in a uniform way, indepen-
dently of the surface-syntactic realization of the
verb. We address passives, nominalizations and
possessives corresponding to the contexts where
we annotated implicit referents (see above). The
transformations are defined as follows:
</bodyText>
<listItem confidence="0.9751064">
1. remove auxiliary nodes, verb morphology and finite-
ness, a tense feature distinguishes past and present, e.g.
“haben:AUX ¨uberfallen:VVINF” (have attacked) maps
to “¨uberfallen:VV:PAST” (attack:PAST)
2. map subjects in actives and oblique agents in passives
to “agents”; objects in actives and subjects in passive to
“themes”, e.g. victim/subj was attacked by perp/obl-ag
maps to perp/agent attack victim/theme
3. attach particles to verb lemma, e.g. “gab” ... “heraus”
in (3-e) is mapped to “herausgeben” (give to)
4. map nominalized to verbal lemmas, their prepositional
and genitive arguments to semantic subjects and ob-
jects, e.g. attack on victim is mapped to attack vic-
tim/theme
5. normalize prenominal and genitive postnominal poses-
</listItem>
<bodyText confidence="0.898518888888889">
sives, e.g. “seine Brieftasche” (his wallet) and “die
Brieftasche des Opfers” (the wallet of the victim) map
to “die Brieftasche POSS victim” (the wallet of victim),
only applies if possessive is an annotated RE
Nominalizations are mapped to their verbal
base forms on the basis of lexicalized rules for the
nominalized lemmas observed in the corpus. The
other transformations are defined on the shallow
dependency annotation.
</bodyText>
<tableCaption confidence="0.718212428571429">
# sentences 2030
# explicit REs
# implicit REs
# passives
# nominalizations
# possessives
Table 1: Basic annotation statistics
</tableCaption>
<subsectionHeader confidence="0.995303">
3.3 Multi-level Representation
</subsectionHeader>
<bodyText confidence="0.999980846153846">
In the final representation of our data set, we inte-
grate the RE and deep syntax annotation by replac-
ing subtrees corresponding to an RE span. The RE
slot in the tree of the sentence is labeled with its
referential role and its ID. All RE subtrees for a
referent in a text are collected in a candidate list
which is initialized with three default REs: (i) a
pronoun, (ii) a default nominal (e.g. “the victim”),
(iii) the empty RE. In contrast to the GREC data
sets, our RE candidates are not represented as the
original surface strings, but as non-linearized sub-
trees. The resulting multi-layer representation for
each text is structured as follows:
</bodyText>
<listItem confidence="0.9986425">
1. unordered deep trees with RE slots (deepSyn_re)
2. unorderd shallow trees with RE slots
(shallowSyn_re)
3. unordered RE subtrees
4. linearized, fully specified surface trees (linSyn+re)
5. alignments between nodes in 1., 2., 4.
</listItem>
<bodyText confidence="0.9702795">
The generation components in Section 4 also
use intermediate layers where REs are inserted
into the deep trees (deep5yn+re) or shallow trees
(shallow5yn+re).
Nodes in unordered trees are deterministically
sorted by their : 1. distance to the root, 2. label,
</bodyText>
<figure confidence="0.9646528">
3208
1778
383
393
1150
</figure>
<page confidence="0.903039">
1550
</page>
<bodyText confidence="0.810399">
3. PoS tag, 4. lemma. The generation components
traverse the nodes in this the order.
</bodyText>
<sectionHeader confidence="0.988552" genericHeader="method">
4 Generation Systems
</sectionHeader>
<bodyText confidence="0.999990916666667">
Our main goal is to investigate different architec-
tures for combined surface realization and refer-
ring expression generation. We assume that this
task is split into three main modules: a syntax gen-
erator, an REG component, and a linearizer. The
components are implemented in a way that they
can be trained and applied on varying inputs, de-
pending on the pipeline. Section 4.1 describes the
basic set-up of our components. Section 4.2 de-
fines the architectures that we will compare in our
experiments (Section 5). Section 4.3 presents the
implementation of the underlying feature models.
</bodyText>
<subsectionHeader confidence="0.636155">
4.1 Components
</subsectionHeader>
<subsubsectionHeader confidence="0.430037">
4.1.1 SYN: Deep to Shallow Syntax
</subsubsectionHeader>
<bodyText confidence="0.999983851851852">
For mapping deep to shallow dependency trees,
the syntax generator induces a probabilistic tree
transformation. The transformations are restricted
to verb nodes in the deep tree (possessives are
handled in the RE module) and extracted from
the alignments between the deep and shallow
layer in the training input. As an example, the
deep node “attack:VV” aligns to “have:AUX at-
tacked:VVINF”, “attacks:VVFIN”, “the:ART at-
tack:NN on:PRP”. The learner is implemented
as a ranking component, trained with SVMrank
(Joachims, 2006). During training, each instance
of a verb node has one optimal shallow depen-
dency alignment and a set of distractor candidates.
During testing, the module has to pick the best
shallow candidate according to its feature model.
In our crossvalidation set-up (see Section 5),
we extract, on average, 374 transformations from
the training sets. This set subdivides into non-
lexicalized and lexicalized transformations. The
mapping rule in (4-a) that simply rewrites the verb
underspecified PoS tag to the finite verb tag in the
shallow tree illustrates the non-lexicalized case.
Most transformation rules (335 out of 374 on aver-
age) are lexicalized for a specific verb lemma and
mostly transform nominalizations as in rule (4-b)
and particles (see Section 3.2).
</bodyText>
<equation confidence="0.950754333333333">
(4) a. (x,lemma,VV,y) → (x,lemma,VVFIN,y)
b. (x,¨uberfallen/attack,VV,y) → (x,bei/at,PREP,y),
(z, ¨Uberfall/attack,NN,x),(q,der/the,ART,z)
</equation>
<bodyText confidence="0.99903">
The baseline for the verb transformation com-
ponent is a two-step procedure: 1) pick a lexical-
ized rule if available for that verb lemma, 2) pick
the most frequent transformation.
</bodyText>
<subsubsectionHeader confidence="0.697833">
4.1.2 REG: Realizing Referring Expressions
</subsubsectionHeader>
<bodyText confidence="0.99998080952381">
Similar to the syntax component, the REG mod-
ule is implemented as a ranker that selects surface
RE subtrees for a given referential slot in a deep
or shallow dependency tree. The candidates for
the ranking correspond to the entire set of REs
used for that referential role in the original text
(see Section 3.1). The basic RE module is a joint
model of all RE types, i.e. nominal, pronominal
and empty realizations of the referent. For the ex-
periment in Section 5.4, we use an additional sep-
arate classifier for implicit referents, also trained
with SVMrank. It uses the same feature model
as the full ranking component, but learns a binary
distinction for implicit or explicit mentions of a
referent. The explicit mentions will be passed to
the RE ranking component.
The baseline for the REG component is defined
as follows: if the preceding and the current RE
slot are instances of the same referent, realize a
pronoun, else realize the longest nominal RE can-
didate that has not been used in the preceding text.
</bodyText>
<subsectionHeader confidence="0.711242">
4.1.3 LIN: Linearization
</subsectionHeader>
<bodyText confidence="0.9999645">
For linearization, we use the state-of-the-art
dependency linearizer described in Bohnet et
al. (2012). We train the linearizer on an auto-
matically parsed version of the German TIGER
treebank (Brants et al., 2002). This version
was produced with the dependency parser by
Bohnet (2010), trained on the dependency conver-
sion of TIGER by Seeker and Kuhn (2012).
</bodyText>
<subsectionHeader confidence="0.971193">
4.2 Architectures
</subsectionHeader>
<bodyText confidence="0.9999762">
Depending on the way the generation components
are combined in an architecture, they will have ac-
cess to different layers of the input representation.
The following definitions of architectures recur to
the layers introduced in Section 3.3.
</bodyText>
<subsectionHeader confidence="0.587993">
4.2.1 First Pipeline
</subsectionHeader>
<bodyText confidence="0.997475">
The first pipeline corresponds most closely to a
standard generation pipeline in the sense of (Reiter
and Dale, 1997). REG is carried out prior to sur-
face realization such that the RE component does
not have access to surface syntax or word order
whereas the SYN component has access to fully
specified RE slots.
</bodyText>
<listItem confidence="0.967612">
• training
</listItem>
<page confidence="0.765165">
1551
</page>
<listItem confidence="0.9993465">
1. train REG: (deepSyn_re, deepSyn+re)
2. train SYN: (deepSyn+re, shallowSyn+re)
• prediction
1. apply REG: deepSyn_re → deepSyn+re
2. apply SYN: deepSyn+re → shallowSyn+re
3. linearize: shallowSyn+re → linSyn+re
</listItem>
<subsubsectionHeader confidence="0.729217">
4.2.2 Second Pipeline
</subsubsectionHeader>
<bodyText confidence="0.9999582">
In the second pipeline, the order of the RE and
SYN component is switched. In this case, REG
has access to surface syntax without word order
but the surface realization is trained and applied
on trees with underspecified RE slots.
</bodyText>
<listItem confidence="0.999463875">
• training
1. train SYN: (deepSyn_re, shallowSyn_re)
2. train REG: (shallowSyn_re, shallowSyn+re)
• prediction
1. apply SYN: deepSyn_re → shallowSyn_re
2. apply REG: shallowSyn_re →
shallowSyn+re
3. linearize: shallowSyn+re → linSyn+re
</listItem>
<subsectionHeader confidence="0.86473">
4.2.3 Parallel System
</subsectionHeader>
<bodyText confidence="0.9999392">
A well-known problem with pipeline architectures
is the effect of error propagation. In our parallel
system, the components are trained independently
of each other and applied in parallel on the deep
syntactic input with underspecified REs.
</bodyText>
<listItem confidence="0.999252">
• training
1. train SYN: (deepSyn_re, shallowSyn_re)
2. train REG: (deepSyn_re, deepSyn+re)
• prediction
1. apply REG and SYN:
</listItem>
<bodyText confidence="0.500812">
deepSyn_re → shallowSyn+re
</bodyText>
<listItem confidence="0.956221">
2. linearize: shallowSyn+re → linSyn+re
</listItem>
<subsubsectionHeader confidence="0.498882">
4.2.4 Revision-based System
</subsubsectionHeader>
<bodyText confidence="0.999963285714286">
In the revision-based system, the RE component
has access to surface syntax and a preliminary lin-
earization, called prelinSyn. In this set-up, we ap-
ply the linearizer first on trees with underspeci-
fied RE slots. For this step, we insert the default
REs for the referent into the respective slots. After
REG, the tree is linearized once again.
</bodyText>
<listItem confidence="0.9994561">
• training
1. train SYN on gold pairs of
(deepSyn_re, shallowSyn_re)
2. train REG on gold pairs of
(prelinSyn_re, prelinSyn+re)
• prediction
1. apply SYN: deepSyn_re → shallowSyn_re
2. linearize: shallowSyn_re → prelinSyn_re
3. apply REG: prelinSyn_re → prelinSyn+re
4. linearize: prelinSyn+re → linSyn+re
</listItem>
<subsectionHeader confidence="0.994918">
4.3 Feature Models
</subsectionHeader>
<bodyText confidence="0.99999725">
The implementation of the feature models is based
on a general set of templates for the SYN and REG
component. The exact form of the models depends
on the input layer of a component in a given ar-
chitecture. For instance, when SYN is trained on
deep5yn−re, the properties of the children nodes
are less specific for verbs that have RE slots as
their dependents. When the SYN component is
trained on deep5yn+re, lemma and POS of the
children nodes are always specified.
The feature templates for SYN combine prop-
erties of the shallow candidate nodes (label, PoS
and lemma for top node and its children) with the
properties of the instance in the tree: (i) lemma,
tense, (ii) sentence is a header, (iii) label, PoS,
lemma of mother node, children and grandchil-
dren nodes (iv) number, lemmas of other verbs in
the sentence.
The feature templates for REG combine proper-
ties of the candidate RE (PoS and lemma for top
node and its children, length) with properties of
the RE slot in the tree: lemma, PoS and labels for
the (i) mother node, (ii) grandmother node, (iii)
uncle and sibling nodes. Additionally, we imple-
ment a small set of global properties of a referent
in a text: (i) identity is known, (ii) plural or sin-
gular referent, (iii) age is known, and a number of
contextual properties capturing the previous refer-
ents and their predicted REs: (i) role and realiza-
tion of the preceding referent, (ii) last mention of
the current referent, (iii) realization of the referent
in the header.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999983923076923">
In this experimental section, we provide a corpus-
based evaluation of the generation components
and architectures introduced in Section 4. In the
following, Section 5.1 presents the details of our
evaluation methodology. In Section 5.2, we dis-
cuss the first experiment that evaluates the pipeline
architectures and the single components on oracle
inputs. Section 5.3 describes an experiment which
compares the parallel and the revision-based ar-
chitecture against the pipeline. In Section 5.4, we
compare two methods for dealing with the implicit
referents in our data. Section 5.5 provides some
general discussion of the results.
</bodyText>
<page confidence="0.984836">
1552
</page>
<table confidence="0.998922625">
Sentence overlap SYN Accuracy RE Accuracy
Input System BLEU NIST BLEUr String Type String Type Impl
deepSyn_re Baseline 42.38 9.9 47.94 35.66 44.81 33.3 36.03 50.43
deepSyn_re 1st pipeline 54.65 11.30 59.95 57.09 68.15 54.61 71.51 84.72
deepSyn_re 2nd pipeline 54.28 11.25 59.62 59.14 68.58 52.24 68.2 82
gold deepSyn+re SYN→LIN 63.9 12.7 62.86 60.83 69.74 100 100 100
gold shallowSyn_re REG→LIN 60.57 11.87 68.06 100 100 60.53 75.86 88.86
gold shallowSyn+re LIN 79.17 13.91 72.7 100 100 100 100 100
</table>
<tableCaption confidence="0.999683">
Table 2: Evaluating pipeline architectures against the baseline and upper bounds
</tableCaption>
<subsectionHeader confidence="0.973965">
5.1 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.997026846153846">
We split our data set into 10 splits of 20 articles.
We use one split as the development set, and cross-
validate on the remaining splits. In each case,
the downstream modules of the pipeline will be
trained on the jackknifed training set.
Text normalization: We carry out automatic
evaluation calculated on lemmatized text with-
out punctuation, excluding additional effects that
would be introduced from a morphology genera-
tion component.
Measures: First, we use a number of evalua-
tion measures familiar from previous generation
shared tasks:
</bodyText>
<listItem confidence="0.9805757">
1. BLEU, sentence-level geometric mean of 1- to 4-gram
precision, as in (Belz et al., 2011)
2. NIST, sentence-level n-gram overlap weighted in
favour of less frequent n-grams, as in (Belz et al., 2011)
3. RE Accuracy on String, proportion of REs selected by
the system with a string identical to the RE string in the
original corpus, as in (Belz and Kow, 2010)
4. RE Accuracy on Type, proportion of REs selected by
the system with an RE type identical to the RE type in
the original corpus, as in (Belz and Kow, 2010)
</listItem>
<bodyText confidence="0.996916">
Second, we define a number of measures moti-
vated by our specific set-up of the task:
</bodyText>
<listItem confidence="0.990526785714286">
1. BLEUr, sentence-level BLEU computed on post-
processed output where predicted referring expressions
for victim and perp are replaced in the sentences (both
gold and predicted) by their original role label, this
score does not penalize lexical mismatches between
corpus and system REs
2. RE Accuracy on Impl, proportion of REs predicted cor-
rectly as implicit/non-implicit
3. SYN Accuracy on String, proportion of shallow verb
candidates selected by the system with a string identical
to the verb string in the original corpus
4. SYN Accuracy on Type, proportion of shallow verb
candidates selected by the system with a syntactic cat-
egory identical to the category in the original corpus
</listItem>
<subsectionHeader confidence="0.993114">
5.2 Pipelines and Upper Bounds
</subsectionHeader>
<bodyText confidence="0.999995523809524">
The first experiment addresses the first and sec-
ond pipeline introduced in Section 4.2.1 and 4.2.2.
The baseline combines the baseline version of
the SYN component (Section 4.1.1) and the REG
component (Section 4.1.2) respectively. As we re-
port in Table 2, both pipelines largely outperform
the baseline. Otherwise, they obtain very similar
scores in all measures with a small, weakly signif-
icant tendency for the first pipeline. The only re-
markable difference is that the accuracy of the in-
dividual components is, in each case, lower when
they are applied as the second step in the pipeline.
Thus, the RE accuracy suffers from mistakes from
the predicted syntax in the same way that the qual-
ity of syntax suffers from predicted REs.
The three bottom rows in Table 2 report the per-
formance of the individual components and lin-
earization when they are applied to inputs with an
REG and SYN oracle, providing upper bounds for
the pipelines applied on deepSyn_re. When REG
and linearization are applied on shallowSyn_re
with gold shallow trees, the BLEU score is
lower (60.57) as compared to the system that ap-
plies syntax and linearization on deepSyn+re,
deep trees with gold REs (BLEU score of 63.9).
However, the BLEUr score, which generalizes
over lexical RE mismatches, is higher for the
REG→LIN components than for SYN→LIN.
Moreover, the BLEUr score for the REG→LIN
system comes close to the upper bound that ap-
plies linearization on linSyn+re, gold shallow
trees with gold REs (BLEUr of 72.4), whereas
the difference in standard BLEU and NIST is
high. This effect indicates that the RE predic-
tion mostly decreases BLEU due to lexical mis-
matches, whereas the syntax prediction is more
likely to have a negative impact on final lineariza-
tion.
The error propagation effects that we find in the
first and second pipeline architecture clearly show
that decisions at the levels of syntax, reference
and word order interact, otherwise their predic-
</bodyText>
<page confidence="0.953058">
1553
</page>
<table confidence="0.99915825">
Input System BLEU NIST BLEUr
deepSyn_re 1st pipeline 54.65 11.30 59.95
deepSyn_re Parallel 54.78 11.30 60.05
deepSyn_re Revision 56.31 11.42 61.30
</table>
<tableCaption confidence="0.999587">
Table 3: Architecture evaluation
</tableCaption>
<bodyText confidence="0.999717833333333">
tion would not affect each other. In particular, the
REG module seems to be affected more seriously,
the String Accuracy decreases from 60.53 on gold
shallow trees to 52.24 on predicted shallow trees
whereas the Verb String Accuracy decreases from
60.83 on gold REs to 57.04 on predicted REs.
</bodyText>
<subsectionHeader confidence="0.994221">
5.3 Revision or parallelism?
</subsectionHeader>
<bodyText confidence="0.999676">
The second experiment compares the first pipeline
against the parallel and the revision-based ar-
chitecture introduced in Section 4.2.3 and 4.2.4.
The evaluation in Table 3 shows that the paral-
lel architecture improves only marginally over the
pipeline. By contrast, we obtain a clearly signifi-
cant improvement for the revision-based architec-
ture on all measures. The fact that this architec-
ture significantly improves the BLEU, NIST and
the BLEUr score of the parallel system indicates
that the REG benefits from the predicted syntax
when it is approximatively linearized. The fact
that also the BLEUr score improves shows that a
higher lexical quality of the REs leads to better fi-
nal linearizations.
Table 4 shows the performance of the REG
module on varying input layers, providing a more
detailed analysis of the interaction between RE,
syntax and word order. In order to produce the
deeplin5yn_re layer, deep syntax trees with ap-
proximative linearizations, we preprocessed the
deep trees by inserting a default surface trans-
formation for the verb nodes. We compare this
input for REG against the prelin5yn_re layer
used in the revision-based architecture, and the
deep5yn_re layer used in the pipeline and the par-
allel architecture. The REG module benefits from
the linearization in the case of deeplin5yn_re
and prelin5yn_re, outperforming the compo-
nent trained applied on the non-linearized deep
syntax trees. However, the REG module ap-
plied on prelin5yn_re, predicted shallow and lin-
earized trees, clearly outperforms the module ap-
plied on deeplin5yn_re. This shows that the
RE prediction can actually benefit from the pre-
dicted shallow syntax, but only when the predicted
trees are approximatively linearized. As an up-
per bound, we report the performance obtained on
</bodyText>
<table confidence="0.998366666666667">
RE Accuracy
Input System String Type Impl
deepSyn_re RE 54.61 71.51 84.72
deeplinSyn_re RE 56.78 72.23 84.71
prelinSyn_re RE 58.81 74.34 86.37
gold linSyn_re RE 68.63 83.63 94.74
</table>
<tableCaption confidence="0.999101">
Table 4: RE generation from different input layers
</tableCaption>
<bodyText confidence="0.996421285714286">
lin5yn_re, gold shallow trees with gold lineariza-
tions. This set-up corresponds to the GREC tasks.
The gold syntax leads to a huge increase in perfor-
mance.
These results strengthen the evidence from the
previous experiment that decisions at the level of
syntax, reference and word order are interleaved.
A parallel architecture that simply “circumvents”
error propagation effects by making decisions in-
dependent of each other is not optimal. Instead,
the automatic prediction of shallow syntax can
positively impact on RE generation if these shal-
low trees are additionally processed with an ap-
proximative linearization step.
</bodyText>
<subsectionHeader confidence="0.984681">
5.4 A joint treatment of implicit referents?
</subsectionHeader>
<bodyText confidence="0.999949310344827">
The previous experiments have pursued a joint
approach for modeling implicit referents. The
hypothesis for this experiment is that the SYN
component and the intermediate linearization in
a revision-based architecture could benefit from a
separate treatment of implicit referents since verb
alternations like passive or nominalization often
involve referent deletions.
The evaluation in Table 5 provides contradic-
tory results depending on the evaluation measure.
For the first pipeline, the system with a separate
treatment of implicit referents significantly outper-
forms the joint system in terms of BLEU. How-
ever, the BLEUr score does not improve. In the
revision-based architecture, we do not find a clear
result for or against a joint modelling approach.
The revision-based system with disjoint modelling
of implicits shows a slight, non-significant in-
crease in BLEU score. By contrast, the BLEUr
score is signficantly better for the joint approach.
We experimented with parallelization of syntax
generation and prediction of implicit referents in
a revision-based system. This has a small positive
effect on the BLEUr score and a small negative
effect on the plain BLEU and NIST score. These
contradictory scores might indicate that the auto-
matic evaluation measures cannot capture all as-
pects of text quality, an issue that we discuss in
the following.
</bodyText>
<page confidence="0.992829">
1554
</page>
<figure confidence="0.999821356321839">
(5) Generated by sequential system:
gab
gave
a. Deshalb
Therefore
daB
that
ohne
without
leistet
shows
heraus.
out.
Widerstand
resistance
H¨uchtete.
Heed.
anschlieBend
afterwards
dem Opfer � ✆
the victim
✁
Armbanduhr
watch
die
the
Er
He
nahm
takes
ab
off
und
and
dem T¨ater
to the robber
Brieftasche
wallet
das Opfer&apos; ✆ �
the victim
✁
seine
✂✄
his
der T¨ater
the robber
(6) Generated by revision-based system:
✞
✁
☎
Das Opfer ✆ �
The victim
✂
deshalb
therefore
✂
Brieftasche
wallet
ohne
without
Widerstand
resistance
heraus.
out.
gibt
gave
✝ ✄
b. AnschlieBend nahm
Afterwards took
the victim
✁
✂
Armbanduhr
watch
H¨uchtete.
Heed.
the robber
der T¨ater
die
the
dem Opfer � ✆
✞
✝
ab
off
und
and
</figure>
<figureCaption confidence="0.968264">
Figure 3: Two automatically generated outputs for the Sentences (3e-f) in Figure 2.
</figureCaption>
<figure confidence="0.9665245">
✄
�
seine
✂✄
✁
�
his
✁
zu
to
leisten
show
</figure>
<table confidence="0.973357666666667">
Joint System BLEU NIST BLEU,.
+ 1st pipeline 54.65 11.30 59.95
- 1st pipeline 55.38 11.48 59.52
+ Revision 56.31 11.42 61.30
- Revision 56.42 11.54 60.52
- Parallel+Revision 56.29 11.51 60.63
</table>
<tableCaption confidence="0.998587">
Table 5: Implicit reference and architectures
</tableCaption>
<sectionHeader confidence="0.533158" genericHeader="method">
5.5 Discussion
</sectionHeader>
<bodyText confidence="0.999978772727273">
The results presented in the preceding evaluations
consistenly show the tight connections between
decisions at the level of reference, syntax and word
order. These interactions entail highly interde-
pendent modelling steps: Although there is a di-
rect error propagation effect from predicted verb
transformation on RE accuracy, predicted syntax
still leads to informative intermediate lineariza-
tions that improve the RE prediction. Our optimal
generation architecture thus has a sequential set-
up, where the first linearization step can be seen
as an intermediate feedback that is revised in the
final linearization. This connects to work in, e.g.
(Hovy, 1988; Robin, 1993).
In Figure 3, we compare two system outputs for
the last two sentences of the text in Figure 2. The
output of the sequential system is severely inco-
herent and would probably be rejected by a hu-
man reader: In sentence (5a) the victim subject of
an active verb is deleted, and the relation between
the possessive and the embedded victim RE is not
clear. In sentence (5b) the first conjunct realizes
a pronominal perp RE and the second conjunct a
nominal perp RE. The output of the revision-based
system reads much more natural. This example
shows that the extension of the REG problem to
texts with more than one main referent (as in the
GREC data set) yields interesting inter-sentential
interactions that affect textual coherence.
We are aware of the fact that our automatic eval-
uation might only partially render certain effects,
especially with respect to textual coherence. It
is likely that the BLEU scores do not capture the
magnitude of the differences in text quality illus-
trated by the Examples (5-6). Ultimately, a hu-
man evaluation for this task is highly desirable.
We leave this for future work since our integrated
set-up rises a number of questions with respect to
evaluation design. In a preliminary analysis, we
noticed the problem that human readers find it dif-
ficult to judge discourse-level properties of a text
like coherence or naturalness when the generation
output is not perfectly grammatical or Huent at the
sentence level.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999962375">
We have presented a data-driven approach for in-
vestigating generation architectures that address
discourse-level reference and sentence-level syn-
tax and word order. The data set we created for our
experiments basically integrates standards from
previous research on REG and surface realization
and extends the annotations to further types of im-
plicit referents. Our results show that interactions
between the different generation levels are best
captured in a sequential, revision-based pipeline
where the REG component has access to predic-
tions from the syntax and the linearization mod-
ule. These empirical findings obtained from ex-
periments with generation architectures have clear
connections to theoretical accounts of textual co-
herence.
</bodyText>
<sectionHeader confidence="0.996518" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9531274">
This work was supported by the Deutsche
Forschungsgemeinschaft (German Research
Foundation) in SFB 732 Incremental Specification
in Context, project D2.
☎
</bodyText>
<page confidence="0.989211">
1555
</page>
<sectionHeader confidence="0.990092" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999801168224299">
Douglas Edmund Appelt. 1982. Planning natural lan-
guage utterances to satisfy multiple goals. Ph.D.
thesis, Stanford, CA, USA.
John Bateman and Michael Zock. 2003. Natural
Language Generation. In Ruslan Mitkov, editor,
The Oxford Handbook of Computational Linguis-
tics. Oxford University Press.
Anja Belz and Eric Kow. 2010. The GREC Challenges
2010: overview and evaluation results. In Proc. of
the 6th International Natural Language Generation
Conference, INLG ’10, pages 219–229, Strouds-
burg, PA, USA.
Anja Belz and Sebastian Varges. 2007. Generation of
repeated references to discourse entities. In Proc. of
the 11th European Workshop on Natural Language
Generation, ENLG ’07, pages 9–16, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Anja Belz, Mike White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and evalu-
ation results. In Proc. of the Generation Challenges
Session at the 13th European Workshop on Natu-
ral Language Generation, pages 217–226, Nancy,
France, September. Association for Computational
Linguistics.
Anja Belz. 2005. Statistical generation: Three meth-
ods compared and evaluated. In Proc. of the 10th
European Workshop on Natural Language Genera-
tion, pages 15–23.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive proba-
bilistic generation-space models. Nat. Lang. Eng.,
14(4):431–455, October.
Bernd Bohnet, Leo Wanner, Simon Milles, and Ali-
cia Burga. 2010. Broad coverage multilingual deep
sentence generation with a stochastic multi-level re-
alizer. In Proc. of the 23rd International Conference
on Computational Linguistics, Beijing, China.
Bernd Bohnet, Simon Mille, Benoit Favre, and Leo
Wanner. 2011. &lt;stumaba &gt;: From deep repre-
sentation to surface. In Proc. of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 232–235,
Nancy, France, September.
Bernd Bohnet, Anders Bj¨orkelund, Jonas Kuhn, Wolf-
gang Seeker, and Sina Zarriess. 2012. Generating
non-projective word order in statistical linearization.
In Proc. of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 928–
939, Jeju Island, Korea, July.
Bernd Bohnet. 2010. Top accuracy and fast de-
pendency parsing is not a contradiction. In Proc.
of the 23rd International Conference on Computa-
tional Linguistics, pages 89–97, Beijing, China, Au-
gust.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proc. of the Workshop on Treebanks
and Linguistic Theories.
Aoife Cahill and Arndt Riester. 2009. Incorporat-
ing Information Status into Generation Ranking. In
Proc. of the 47th Annual Meeting of the ACL, pages
817–825, Suntec, Singapore, August.
Lynne Cahill, Christy Doran, Roger Evans, Chris Mel-
lish, Daniel Paiva, Mike Reape, Donia Scott, and
Neil Tipper. 1999. In search of a reference architec-
ture for nlg systems. In Proc. of the European Work-
shop on Natural Language Generation (EWNLG),
pages 77–85.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. Journal
ofArtificial Intelligence Research, 37:397–435.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233–263.
Robert Dale. 1989. Cooking up referring expressions.
In Proc. of the 27th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 68–75,
Vancouver, British Columbia, Canada, June.
Laurence Danlos. 1984. Conceptual and linguistic de-
cisions in generation. In Proc. of the 10th Interna-
tional Conference on Computational Linguistics and
22nd Annual Meeting of the Association for Compu-
tational Linguistics, pages 501–504, Stanford, Cali-
fornia, USA, July.
Koenraad De Smedt, Helmut Horacek, and Michael
Zock. 1996. Architectures for natural language gen-
eration: Problems and perspectives. In Trends In
Natural Language Generation: An Artifical Intelli-
gence Perspective, pages 17–46. Springer-Verlag.
Katja Filippova and Michael Strube. 2007. Generating
constituent order in german clauses. In Proc. of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, Prague, Czech Republic.
Matthew Gerber and Joyce Chai. 2010. Beyond nom-
bank: A study of implicit arguments for nominal
predicates. In Proc. of the 48th Annual Meeting
of the Association for Computational Linguistics,
pages 1583–1592, Uppsala, Sweden, July.
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225.
</reference>
<page confidence="0.842584">
1556
</page>
<reference confidence="0.999843090909091">
Eduard H. Hovy. 1988. Planning coherent multisen-
tential text. In Proc. of the 26th Annual Meeting
of the Association for Computational Linguistics,
pages 163–169, Buffalo, New York, USA, June.
Thorsten Joachims. 2006. Training linear SVMs
in linear time. In Proc. of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
pages 217–226.
Irene Langkilde and Kevin Knight. 1998. Gener-
ation that exploits corpus-based statistical knowl-
edge. In Proc. of the 36th Annual Meeting of
the Association for Computational Linguistics and
17th International Conference on Computational
Linguistics, Volume 1, pages 704–710, Montreal,
Quebec, Canada, August. Association for Compu-
tational Linguistics.
Tomasz Marciniak and Michael Strube. 2005. Be-
yond the pipeline: discrete optimization in nlp. In
Proc. of the 9th Conference on Computational Nat-
ural Language Learning, CONLL ’05, pages 136–
143, Stroudsburg, PA, USA.
Chris Mellish, Roger Evans, Lynne Cahill, Christy Do-
ran, Daniel Paiva, Mike Reape, Donia Scott, and
Neil Tipper. 2000. A representation for complex
and evolving data dependencies in generation. In
Proc. of the 6th Conference on Applied Natural Lan-
guage Processing, pages 119–126, Seattle, Wash-
ington, USA, April.
Marie Meteer. 1991. Bridging the generation gap be-
tween text planning and linguistic realization. In
Computational Intelligence, volume 7 (4).
Adwait Ratnaparkhi. 2000. Trainable methods for
surface natural language generation. In Proc. of
the 1st North American chapter of the Association
for Computational Linguistics conference, NAACL
2000, pages 194–201, Stroudsburg, PA, USA.
Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. Nat. Lang.
Eng., 3(1):57–87, March.
Ehud Reiter. 1994. Has a Consensus NL Genera-
tion Architecture Appeared, and is it Psycholinguis-
tically Plausible? pages 163–170.
Eric K. Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically Informed Statisti-
cal Models of Constituent Structure for Ordering in
Sentence Realization. In Proc. of the 2004 Inter-
national Conference on Computational Linguistics,
Geneva, Switzerland.
Jacques Robin. 1993. A revision-based generation ar-
chitecture for reporting facts in their historical con-
text. In New Concepts in Natural Language Gener-
ation: Planning, Realization and Systems. Frances
Pinter, London and, pages 238–265. Pinter Publish-
ers.
Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
texts: A new corpus for a new task. In Proc. of the
1st Joint Conference on Lexical and Computational
Semantics (*SEM), Montreal, Canada.
Robert Rubinoff. 1992. Integrating text planning and
linguistic choice by annotating linguistic structures.
In Robert Dale, Eduard H. Hovy, Dietmar R¨osner,
and Oliviero Stock, editors, NLG, volume 587 of
Lecture Notes in Computer Science, pages 45–56.
Springer.
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2010.
Semeval-2010 task 10: Linking events and their
participants in discourse. In Proc. of the 5th
International Workshop on Semantic Evaluation,
pages 45–50, Uppsala, Sweden, July.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proc. of the 8th conference on
International Language Resources and Evaluation,
Istanbul, Turkey, May.
Advaith Siddharthan and Ann Copestake. 2004. Gen-
erating referring expressions in open domains. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL’04), Main Vol-
ume, pages 407–414, Barcelona, Spain, July.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c,
Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsu-
jii. 2012. brat: a web-based tool for nlp-assisted text
annotation. In Proc. of the Demonstrations at the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 102–
107, Avignon, France, April.
Leo Wanner, Simon Mille, and Bernd Bohnet. 2012.
Towards a surface realization-oriented corpus anno-
tation. In Proc. of the 7th International Natural Lan-
guage Generation Conference, pages 22–30, Utica,
IL, May.
Leo Wanner. 1994. Building another bridge over the
generation gap. In Proc. of the 7th International
Workshop on Natural Language Generation, INLG
’94, pages 137–144, Stroudsburg, PA, USA.
Sina Zarrieß, Aoife Cahill, and Jonas Kuhn. 2011. Un-
derspecifying and predicting voice for surface real-
isation ranking. In Proc. of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1007–
1017, Portland, Oregon, USA, June.
Sina Zarrieß, Aoife Cahill, and Jonas Kuhn. 2012.
To what extent does sentence-internal realisation re-
flect discourse context? a study on word order. In
Proc. of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 767–776, Avignon, France, April.
</reference>
<page confidence="0.994748">
1557
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.897259">
<title confidence="0.9997085">Combining Referring Expression Generation and Surface A Corpus-Based Investigation of Architectures</title>
<author confidence="0.999873">Sina Zarrieß Jonas Kuhn</author>
<affiliation confidence="0.9687965">Institut f¨ur maschinelle University of Stuttgart, Germany</affiliation>
<email confidence="0.998188">sina.zarriess,jonas.kuhn@ims.uni-stuttgart.de</email>
<abstract confidence="0.997000285714286">We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization. We present a data set of German articles annotated with deep syntax and referents, including some types of implicit referents. Our experiments compare several architectures varying the order of a set of trainable modules. The results suggest that a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas Edmund Appelt</author>
</authors>
<title>Planning natural language utterances to satisfy multiple goals.</title>
<date>1982</date>
<booktitle>Ph.D. thesis,</booktitle>
<location>Stanford, CA, USA.</location>
<contexts>
<context position="1211" citStr="Appelt, 1982" startWordPosition="157" endWordPosition="158"> set of trainable modules. The results suggest that a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture. 1 Introduction Generating well-formed linguistic utterances from an abstract non-linguistic input involves making a multitude of conceptual, discourse-level as well as sentence-level, lexical and syntactic decisions. Work on rule-based natural language generation (NLG) has explored a number of ways to combine these decisions in an architecture, ranging from integrated systems where all decisions happen jointly (Appelt, 1982) to strictly sequential pipelines (Reiter and Dale, 1997). While integrated or interactive systems typically face issues with efficiency and scalability, they can directly account for interactions between discourse-level planning and linguistic realization. For instance, Rubinoff (1992) mentions Example (1) where the sentence planning component needs to have access to the lexical knowledge that “order” and not “home” can be realized as a verb in English. (1) a. *John homed him with an order. b. John ordered him home. In recent data-driven generation research, the focus has somewhat shifted fro</context>
</contexts>
<marker>Appelt, 1982</marker>
<rawString>Douglas Edmund Appelt. 1982. Planning natural language utterances to satisfy multiple goals. Ph.D. thesis, Stanford, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bateman</author>
<author>Michael Zock</author>
</authors>
<title>Natural Language Generation.</title>
<date>2003</date>
<booktitle>The Oxford Handbook of Computational Linguistics.</booktitle>
<editor>In Ruslan Mitkov, editor,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="7420" citStr="Bateman and Zock, 2003" startWordPosition="1118" endWordPosition="1121"> al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches </context>
</contexts>
<marker>Bateman, Zock, 2003</marker>
<rawString>John Bateman and Michael Zock. 2003. Natural Language Generation. In Ruslan Mitkov, editor, The Oxford Handbook of Computational Linguistics. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Eric Kow</author>
</authors>
<title>The GREC Challenges 2010: overview and evaluation results.</title>
<date>2010</date>
<booktitle>In Proc. of the 6th International Natural Language Generation Conference, INLG ’10,</booktitle>
<pages>219--229</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2106" citStr="Belz and Kow, 2010" startWordPosition="290" endWordPosition="293">off (1992) mentions Example (1) where the sentence planning component needs to have access to the lexical knowledge that “order” and not “home” can be realized as a verb in English. (1) a. *John homed him with an order. b. John ordered him home. In recent data-driven generation research, the focus has somewhat shifted from full data-to-text systems to approaches that isolate well-defined subproblems from the NLG pipeline. In particular, the tasks of surface realization and referring expression generation (REG) have received increasing attention using a number of available annotated data sets (Belz and Kow, 2010; Belz et al., 2011). While these single-task approaches have given rise to many insights about algorithms and corpus-based modelling for specific phenomena, they can hardly deal with aspects of the architecture and interaction between generation levels. This paper suggests a middle ground between full data-to-text and single-task generation, combining two well-studied NLG problems. We integrate a discourse-level approach to REG with sentence-level surface realization in a data-driven framework. We address this integrated task with a set of components that can be trained on flexible inputs whi</context>
<context position="27123" citStr="Belz and Kow, 2010" startWordPosition="4281" endWordPosition="4284">valuation calculated on lemmatized text without punctuation, excluding additional effects that would be introduced from a morphology generation component. Measures: First, we use a number of evaluation measures familiar from previous generation shared tasks: 1. BLEU, sentence-level geometric mean of 1- to 4-gram precision, as in (Belz et al., 2011) 2. NIST, sentence-level n-gram overlap weighted in favour of less frequent n-grams, as in (Belz et al., 2011) 3. RE Accuracy on String, proportion of REs selected by the system with a string identical to the RE string in the original corpus, as in (Belz and Kow, 2010) 4. RE Accuracy on Type, proportion of REs selected by the system with an RE type identical to the RE type in the original corpus, as in (Belz and Kow, 2010) Second, we define a number of measures motivated by our specific set-up of the task: 1. BLEUr, sentence-level BLEU computed on postprocessed output where predicted referring expressions for victim and perp are replaced in the sentences (both gold and predicted) by their original role label, this score does not penalize lexical mismatches between corpus and system REs 2. RE Accuracy on Impl, proportion of REs predicted correctly as implici</context>
</contexts>
<marker>Belz, Kow, 2010</marker>
<rawString>Anja Belz and Eric Kow. 2010. The GREC Challenges 2010: overview and evaluation results. In Proc. of the 6th International Natural Language Generation Conference, INLG ’10, pages 219–229, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Sebastian Varges</author>
</authors>
<title>Generation of repeated references to discourse entities.</title>
<date>2007</date>
<booktitle>In Proc. of the 11th European Workshop on Natural Language Generation, ENLG ’07,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8857" citStr="Belz and Varges (2007)" startWordPosition="1345" endWordPosition="1348">lippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical meth</context>
</contexts>
<marker>Belz, Varges, 2007</marker>
<rawString>Anja Belz and Sebastian Varges. 2007. Generation of repeated references to discourse entities. In Proc. of the 11th European Workshop on Natural Language Generation, ENLG ’07, pages 9–16, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Mike White</author>
<author>Dominic Espinosa</author>
<author>Eric Kow</author>
<author>Deirdre Hogan</author>
<author>Amanda Stent</author>
</authors>
<title>The first surface realisation shared task: Overview and evaluation results.</title>
<date>2011</date>
<booktitle>In Proc. of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>217--226</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nancy, France,</location>
<contexts>
<context position="2126" citStr="Belz et al., 2011" startWordPosition="294" endWordPosition="297">Example (1) where the sentence planning component needs to have access to the lexical knowledge that “order” and not “home” can be realized as a verb in English. (1) a. *John homed him with an order. b. John ordered him home. In recent data-driven generation research, the focus has somewhat shifted from full data-to-text systems to approaches that isolate well-defined subproblems from the NLG pipeline. In particular, the tasks of surface realization and referring expression generation (REG) have received increasing attention using a number of available annotated data sets (Belz and Kow, 2010; Belz et al., 2011). While these single-task approaches have given rise to many insights about algorithms and corpus-based modelling for specific phenomena, they can hardly deal with aspects of the architecture and interaction between generation levels. This paper suggests a middle ground between full data-to-text and single-task generation, combining two well-studied NLG problems. We integrate a discourse-level approach to REG with sentence-level surface realization in a data-driven framework. We address this integrated task with a set of components that can be trained on flexible inputs which allows us to syst</context>
<context position="3580" citStr="Belz et al., 2011" startWordPosition="519" endWordPosition="522">ing expressions (REs), syntax and word order. Our basic set-up is inspired from the Generating Referring Expressions in Context (GREC) tasks, where candidate REs have to be assigned to instances of a referent in a Wikipedia article (Belz and Kow, 2010). We have created a dataset of German texts with annotations that extend this standard in three substantial ways: (i) our domain consists of articles about robbery events that mainly involve two main referents, a victim and a perpetrator (perp), (ii) annotations include deep and shallow syntactic relations similar to the representations used in (Belz et al., 2011) (iii) annotations include empty referents, as e.g. in passives and nominalizations directing attention to the phenomenon of implicit reference, which is largely understudied in NLG. Figure 1 presents an example for a deep syntax tree with underspecified RE 1547 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1547–1557, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Tree) be agent mod mod perp on because pobj sub trial attack agent theme perp victim Figure 1: Underspecified tree with RE candidates slots and lis</context>
<context position="12669" citStr="Belz et al., 2011" startWordPosition="1952" endWordPosition="1955">rdinations. (e.g. “flee” in (3-f)) The brat tool (Stenetorp et al., 2012) was used for annotation. We had 2 annotators with a computational linguistic background, provided with annotation guidelines. They were trained on a set of 20 texts. We measure a good agreement on another set of 15 texts: the simple pairwise agreement for explicit mentions is 95.14%-96.53% and 78.94%- 76.92% for implicit mentions.1 3.2 Syntax annotation The syntactic annotation of our data includes two layers: shallow and deep, labeled dependencies, similar to the representation used in surface realization shared tasks (Belz et al., 2011). We use 1Standard measures for the “above chance annotator agreement” are only defined for task where the set of annotated items is pre-defined. 1549 dem Heimwegposs:v ausgeraubtag:p the way homeposs:v robbedag:p zwei ungepflegt wirkenden jungen M¨annern im Alter von etwa 25 Jahren p:0. two shabby-looking young men of about 25 years . auf on ✞ ☎ (3) a. ✝ Junge Familie v:0 ✞ ☎ ✆ ✝ Young family✆ b. Die Polizei sucht nach The police looks for c. am on ✞ ✝✞ ☎ ✝a young family with their seven month old baby ✆ ☎ eine junge Familie mit ihrem sieben Monate alten Baby v:0 ✆ auf on Sie p:0 They sollen </context>
<context position="26854" citStr="Belz et al., 2011" startWordPosition="4233" endWordPosition="4236">ata set into 10 splits of 20 articles. We use one split as the development set, and crossvalidate on the remaining splits. In each case, the downstream modules of the pipeline will be trained on the jackknifed training set. Text normalization: We carry out automatic evaluation calculated on lemmatized text without punctuation, excluding additional effects that would be introduced from a morphology generation component. Measures: First, we use a number of evaluation measures familiar from previous generation shared tasks: 1. BLEU, sentence-level geometric mean of 1- to 4-gram precision, as in (Belz et al., 2011) 2. NIST, sentence-level n-gram overlap weighted in favour of less frequent n-grams, as in (Belz et al., 2011) 3. RE Accuracy on String, proportion of REs selected by the system with a string identical to the RE string in the original corpus, as in (Belz and Kow, 2010) 4. RE Accuracy on Type, proportion of REs selected by the system with an RE type identical to the RE type in the original corpus, as in (Belz and Kow, 2010) Second, we define a number of measures motivated by our specific set-up of the task: 1. BLEUr, sentence-level BLEU computed on postprocessed output where predicted referring</context>
</contexts>
<marker>Belz, White, Espinosa, Kow, Hogan, Stent, 2011</marker>
<rawString>Anja Belz, Mike White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The first surface realisation shared task: Overview and evaluation results. In Proc. of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 217–226, Nancy, France, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Statistical generation: Three methods compared and evaluated.</title>
<date>2005</date>
<booktitle>In Proc. of the 10th European Workshop on Natural Language Generation,</booktitle>
<pages>15--23</pages>
<contexts>
<context position="7987" citStr="Belz, 2005" startWordPosition="1204" endWordPosition="1205">Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence perp italians two men the two they &lt;empty&gt; victim the victim man a young he &lt;empty&gt; 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about t</context>
</contexts>
<marker>Belz, 2005</marker>
<rawString>Anja Belz. 2005. Statistical generation: Three methods compared and evaluated. In Proc. of the 10th European Workshop on Natural Language Generation, pages 15–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models.</title>
<date>2008</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="9534" citStr="Belz (2008)" startWordPosition="1453" endWordPosition="1454">rent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Rec</context>
</contexts>
<marker>Belz, 2008</marker>
<rawString>Anja Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Nat. Lang. Eng., 14(4):431–455, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Leo Wanner</author>
<author>Simon Milles</author>
<author>Alicia Burga</author>
</authors>
<title>Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer.</title>
<date>2010</date>
<booktitle>In Proc. of the 23rd International Conference on Computational Linguistics,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="10342" citStr="Bohnet et al., 2010" startWordPosition="1575" endWordPosition="1578">nment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do not necessarily fit the needs of generation (Bohnet et al., 2010; Wanner et al., 2012). Zarrieß et al. (2011) discuss the problem of an input representation that is appropriately underspecified for the realistic generation of voice alternations. 3 The Data Set The data set for our generation experiments consists of 200 newspaper articles about robbery events. The articles were extracted from a large German newspaper corpus. A complete example text with RE annotations is given in Figure 2, Table 1 summarizes some data set statistics. 3.1 RE annotation The RE annotations mark explicit and implicit mentions of referents involved in the robbery event described</context>
</contexts>
<marker>Bohnet, Wanner, Milles, Burga, 2010</marker>
<rawString>Bernd Bohnet, Leo Wanner, Simon Milles, and Alicia Burga. 2010. Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer. In Proc. of the 23rd International Conference on Computational Linguistics, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Simon Mille</author>
<author>Benoit Favre</author>
<author>Leo Wanner</author>
</authors>
<title>From deep representation to surface.</title>
<date>2011</date>
<booktitle>In Proc. of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>232--235</pages>
<location>Nancy, France,</location>
<contexts>
<context position="9985" citStr="Bohnet et al. (2011)" startWordPosition="1523" endWordPosition="1526">alysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do not necessarily fit the needs of generation (Bohnet et al., 2010; Wanner et al., 2012). Zarrieß et al. (2011) discuss the problem of an input representation that is appropriately underspecified for the realistic generation of voice alternations. 3 The Data Set The data set for our generation experiments co</context>
</contexts>
<marker>Bohnet, Mille, Favre, Wanner, 2011</marker>
<rawString>Bernd Bohnet, Simon Mille, Benoit Favre, and Leo Wanner. 2011. &lt;stumaba &gt;: From deep representation to surface. In Proc. of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 232–235, Nancy, France, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Anders Bj¨orkelund</author>
<author>Jonas Kuhn</author>
<author>Wolfgang Seeker</author>
<author>Sina Zarriess</author>
</authors>
<title>Generating non-projective word order in statistical linearization.</title>
<date>2012</date>
<booktitle>In Proc. of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>928--939</pages>
<location>Jeju Island, Korea,</location>
<marker>Bohnet, Bj¨orkelund, Kuhn, Seeker, Zarriess, 2012</marker>
<rawString>Bernd Bohnet, Anders Bj¨orkelund, Jonas Kuhn, Wolfgang Seeker, and Sina Zarriess. 2012. Generating non-projective word order in statistical linearization. In Proc. of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 928– 939, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proc. of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>89--97</pages>
<location>Beijing, China,</location>
<contexts>
<context position="14196" citStr="Bohnet (2010)" startWordPosition="2230" endWordPosition="2231">nd ✂ zusammenzuschlagen. beat up. ✁ ✄ ✂✄ him ✂ � ihn v:1 ✁ � ✂✄ � ✁ die the Polizei police ✄ e. 1 ✁ Er v:1 ✂✄ He ✂ deshalb therefore ✂ gab gave ✁ f. Anschließend nahmen Afterwards took ✂ Brieftasche ohne wallet without die R¨auber p:0 the robbers Gegenwehrag:v,the:p resistanceag:v,the:p Armbanduhrposs:v watchposs:v fl¨uchtetenag:p. fleedag:p. heraus. out. ab off und and ✄ ✂✄ his ✁ � � seine v:1 ✁ ✄ � ✂✄ ihm v:1 ✁ � him ✁ die the noch also Figure 2: Example text with RE annotations, oval boxes mark victim mentions, square boxes mark perp mentions, heads of implicit arguments are underlined the Bohnet (2010) dependency parser to obtain an automatic annotation of shallow or surface dependencies for the corpus sentences. The deep syntactic dependencies are derived from the shallow layer by a set of hand-written transformation rules. The goal is to link referents to their main predicate in a uniform way, independently of the surface-syntactic realization of the verb. We address passives, nominalizations and possessives corresponding to the contexts where we annotated implicit referents (see above). The transformations are defined as follows: 1. remove auxiliary nodes, verb morphology and finiteness,</context>
<context position="20940" citStr="Bohnet (2010)" startWordPosition="3290" endWordPosition="3291">icit mentions will be passed to the RE ranking component. The baseline for the REG component is defined as follows: if the preceding and the current RE slot are instances of the same referent, realize a pronoun, else realize the longest nominal RE candidate that has not been used in the preceding text. 4.1.3 LIN: Linearization For linearization, we use the state-of-the-art dependency linearizer described in Bohnet et al. (2012). We train the linearizer on an automatically parsed version of the German TIGER treebank (Brants et al., 2002). This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). 4.2 Architectures Depending on the way the generation components are combined in an architecture, they will have access to different layers of the input representation. The following definitions of architectures recur to the layers introduced in Section 3.3. 4.2.1 First Pipeline The first pipeline corresponds most closely to a standard generation pipeline in the sense of (Reiter and Dale, 1997). REG is carried out prior to surface realization such that the RE component does not have access to surface syntax or word orde</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proc. of the 23rd International Conference on Computational Linguistics, pages 89–97, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER Treebank.</title>
<date>2002</date>
<booktitle>In Proc. of the Workshop on Treebanks and Linguistic Theories.</booktitle>
<contexts>
<context position="20869" citStr="Brants et al., 2002" startWordPosition="3277" endWordPosition="3280">a binary distinction for implicit or explicit mentions of a referent. The explicit mentions will be passed to the RE ranking component. The baseline for the REG component is defined as follows: if the preceding and the current RE slot are instances of the same referent, realize a pronoun, else realize the longest nominal RE candidate that has not been used in the preceding text. 4.1.3 LIN: Linearization For linearization, we use the state-of-the-art dependency linearizer described in Bohnet et al. (2012). We train the linearizer on an automatically parsed version of the German TIGER treebank (Brants et al., 2002). This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). 4.2 Architectures Depending on the way the generation components are combined in an architecture, they will have access to different layers of the input representation. The following definitions of architectures recur to the layers introduced in Section 3.3. 4.2.1 First Pipeline The first pipeline corresponds most closely to a standard generation pipeline in the sense of (Reiter and Dale, 1997). REG is carried out prior to surface realization such th</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER Treebank. In Proc. of the Workshop on Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Arndt Riester</author>
</authors>
<title>Incorporating Information Status into Generation Ranking.</title>
<date>2009</date>
<booktitle>In Proc. of the 47th Annual Meeting of the ACL,</booktitle>
<pages>817--825</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="8286" citStr="Cahill and Riester, 2009" startWordPosition="1250" endWordPosition="1253">s sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence perp italians two men the two they &lt;empty&gt; victim the victim man a young he &lt;empty&gt; 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually ap</context>
</contexts>
<marker>Cahill, Riester, 2009</marker>
<rawString>Aoife Cahill and Arndt Riester. 2009. Incorporating Information Status into Generation Ranking. In Proc. of the 47th Annual Meeting of the ACL, pages 817–825, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynne Cahill</author>
<author>Christy Doran</author>
<author>Roger Evans</author>
<author>Chris Mellish</author>
<author>Daniel Paiva</author>
<author>Mike Reape</author>
<author>Donia Scott</author>
<author>Neil Tipper</author>
</authors>
<title>In search of a reference architecture for nlg systems.</title>
<date>1999</date>
<booktitle>In Proc. of the European Workshop on Natural Language Generation (EWNLG),</booktitle>
<pages>77--85</pages>
<contexts>
<context position="7395" citStr="Cahill et al., 1999" startWordPosition="1114" endWordPosition="1117">los, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work o</context>
</contexts>
<marker>Cahill, Doran, Evans, Mellish, Paiva, Reape, Scott, Tipper, 1999</marker>
<rawString>Lynne Cahill, Christy Doran, Roger Evans, Chris Mellish, Daniel Paiva, Mike Reape, Donia Scott, and Neil Tipper. 1999. In search of a reference architecture for nlg systems. In Proc. of the European Workshop on Natural Language Generation (EWNLG), pages 77–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Training a multilingual sportscaster: Using perceptual context to learn language.</title>
<date>2010</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>37--397</pages>
<contexts>
<context position="9611" citStr="Chen et al. (2010)" startWordPosition="1463" endWordPosition="1466">tions of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standa</context>
</contexts>
<marker>Chen, Kim, Mooney, 2010</marker>
<rawString>David L. Chen, Joohyun Kim, and Raymond J. Mooney. 2010. Training a multilingual sportscaster: Using perceptual context to learn language. Journal ofArtificial Intelligence Research, 37:397–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the gricean maxims in the generation of referring expressions. Cognitive Science, 19(2):233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Cooking up referring expressions.</title>
<date>1989</date>
<booktitle>In Proc. of the 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>68--75</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="8728" citStr="Dale (1989" startWordPosition="1323" endWordPosition="1324"> the two they &lt;empty&gt; victim the victim man a young he &lt;empty&gt; 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents ha</context>
</contexts>
<marker>Dale, 1989</marker>
<rawString>Robert Dale. 1989. Cooking up referring expressions. In Proc. of the 27th Annual Meeting of the Association for Computational Linguistics, pages 68–75, Vancouver, British Columbia, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence Danlos</author>
</authors>
<title>Conceptual and linguistic decisions in generation.</title>
<date>1984</date>
<booktitle>In Proc. of the 10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>501--504</pages>
<location>Stanford, California, USA,</location>
<contexts>
<context position="6785" citStr="Danlos, 1984" startWordPosition="1022" endWordPosition="1023"> and syntax can be modelled in sequential generation architecture where the RE component has access to information about syntactic realization and an approximative, intermediate linearization. Such a system is reminiscent of earlier work in rulebased generation that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et</context>
</contexts>
<marker>Danlos, 1984</marker>
<rawString>Laurence Danlos. 1984. Conceptual and linguistic decisions in generation. In Proc. of the 10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics, pages 501–504, Stanford, California, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koenraad De Smedt</author>
<author>Helmut Horacek</author>
<author>Michael Zock</author>
</authors>
<title>Architectures for natural language generation: Problems and perspectives. In Trends In Natural Language Generation: An Artifical Intelligence Perspective,</title>
<date>1996</date>
<pages>17--46</pages>
<publisher>Springer-Verlag.</publisher>
<marker>De Smedt, Horacek, Zock, 1996</marker>
<rawString>Koenraad De Smedt, Helmut Horacek, and Michael Zock. 1996. Architectures for natural language generation: Problems and perspectives. In Trends In Natural Language Generation: An Artifical Intelligence Perspective, pages 17–46. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Generating constituent order in german clauses.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="8259" citStr="Filippova and Strube, 2007" startWordPosition="1245" endWordPosition="1249">produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence perp italians two men the two they &lt;empty&gt; victim the victim man a young he &lt;empty&gt; 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) w</context>
</contexts>
<marker>Filippova, Strube, 2007</marker>
<rawString>Katja Filippova and Michael Strube. 2007. Generating constituent order in german clauses. In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gerber</author>
<author>Joyce Chai</author>
</authors>
<title>Beyond nombank: A study of implicit arguments for nominal predicates.</title>
<date>2010</date>
<booktitle>In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1583--1592</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="9412" citStr="Gerber and Chai, 2010" startWordPosition="1435" endWordPosition="1438"> line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level gene</context>
</contexts>
<marker>Gerber, Chai, 2010</marker>
<rawString>Matthew Gerber and Joyce Chai. 2010. Beyond nombank: A study of implicit arguments for nominal predicates. In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583–1592, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="5317" citStr="Grosz et al., 1995" startWordPosition="792" endWordPosition="795">s],, on [a young man],,. b. [Two italians],, are on trial because of an attack on [a young man],,. Sentence (2-a) is incoherent because the syntactic surface obscurs the intended meaning that “two italians” and “the two men” refer to the same referent. In order to generate the natural Sentence (2-b), the RE component needs information about linear precedence of the two perp instances and the nominalization of “attack”. These types of interactions between referential and syntactic realization have been thoroughly discussed in theoretical accounts of textual coherence, as e.g. Centering Theory (Grosz et al., 1995). The integrated modelling of REG and surface realization leads to a considerable expansion of the choice space. In a sentence with 3 referents that each have 10 RE candidates and can be freely ordered, the number of surface realizations increases from 6 to 6·103, assuming that the remaining words can not be syntactically varied. Thus, even when the generation problem is restricted to these tasks, a fully integrated architecture faces scalability issues on realistic corpus data. In this work, we assume a modular set-up of the generation system that allows for a flexible ordering of the single </context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
</authors>
<title>Planning coherent multisentential text.</title>
<date>1988</date>
<booktitle>In Proc. of the 26th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>163--169</pages>
<location>Buffalo, New York, USA,</location>
<contexts>
<context position="6564" citStr="Hovy, 1988" startWordPosition="987" endWordPosition="988">rameters of the generation architecture: 1) the sequential order of the modules, 2) parallelization of modules, 3) joint vs. separate modelling of implicit referents. Our results suggest that the interactions between RE and syntax can be modelled in sequential generation architecture where the RE component has access to information about syntactic realization and an approximative, intermediate linearization. Such a system is reminiscent of earlier work in rulebased generation that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For rea</context>
<context position="36301" citStr="Hovy, 1988" startWordPosition="5757" endWordPosition="5758">ions consistenly show the tight connections between decisions at the level of reference, syntax and word order. These interactions entail highly interdependent modelling steps: Although there is a direct error propagation effect from predicted verb transformation on RE accuracy, predicted syntax still leads to informative intermediate linearizations that improve the RE prediction. Our optimal generation architecture thus has a sequential setup, where the first linearization step can be seen as an intermediate feedback that is revised in the final linearization. This connects to work in, e.g. (Hovy, 1988; Robin, 1993). In Figure 3, we compare two system outputs for the last two sentences of the text in Figure 2. The output of the sequential system is severely incoherent and would probably be rejected by a human reader: In sentence (5a) the victim subject of an active verb is deleted, and the relation between the possessive and the embedded victim RE is not clear. In sentence (5b) the first conjunct realizes a pronominal perp RE and the second conjunct a nominal perp RE. The output of the revision-based system reads much more natural. This example shows that the extension of the REG problem to</context>
</contexts>
<marker>Hovy, 1988</marker>
<rawString>Eduard H. Hovy. 1988. Planning coherent multisentential text. In Proc. of the 26th Annual Meeting of the Association for Computational Linguistics, pages 163–169, Buffalo, New York, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear SVMs in linear time.</title>
<date>2006</date>
<booktitle>In Proc. of the ACM Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>217--226</pages>
<contexts>
<context position="18498" citStr="Joachims, 2006" startWordPosition="2902" endWordPosition="2903">ntation of the underlying feature models. 4.1 Components 4.1.1 SYN: Deep to Shallow Syntax For mapping deep to shallow dependency trees, the syntax generator induces a probabilistic tree transformation. The transformations are restricted to verb nodes in the deep tree (possessives are handled in the RE module) and extracted from the alignments between the deep and shallow layer in the training input. As an example, the deep node “attack:VV” aligns to “have:AUX attacked:VVINF”, “attacks:VVFIN”, “the:ART attack:NN on:PRP”. The learner is implemented as a ranking component, trained with SVMrank (Joachims, 2006). During training, each instance of a verb node has one optimal shallow dependency alignment and a set of distractor candidates. During testing, the module has to pick the best shallow candidate according to its feature model. In our crossvalidation set-up (see Section 5), we extract, on average, 374 transformations from the training sets. This set subdivides into nonlexicalized and lexicalized transformations. The mapping rule in (4-a) that simply rewrites the verb underspecified PoS tag to the finite verb tag in the shallow tree illustrates the non-lexicalized case. Most transformation rules</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear SVMs in linear time. In Proc. of the ACM Conference on Knowledge Discovery and Data Mining (KDD), pages 217–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proc. of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>704--710</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="7748" citStr="Langkilde and Knight (1998)" startWordPosition="1169" endWordPosition="1172">medt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence perp italians two men the two they &lt;empty&gt; victim the victim man a young he &lt;empty&gt; 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good pe</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proc. of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1, pages 704–710, Montreal, Quebec, Canada, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomasz Marciniak</author>
<author>Michael Strube</author>
</authors>
<title>Beyond the pipeline: discrete optimization in nlp.</title>
<date>2005</date>
<booktitle>In Proc. of the 9th Conference on Computational Natural Language Learning, CONLL ’05,</booktitle>
<pages>136--143</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9852" citStr="Marciniak and Strube (2005)" startWordPosition="1500" endWordPosition="1503">ations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do not necessarily fit the needs of generation (Bohnet et al., 2010; Wanner et al., 2012). Zarrieß et al. (2011) discuss the problem of an input representation that is appropria</context>
</contexts>
<marker>Marciniak, Strube, 2005</marker>
<rawString>Tomasz Marciniak and Michael Strube. 2005. Beyond the pipeline: discrete optimization in nlp. In Proc. of the 9th Conference on Computational Natural Language Learning, CONLL ’05, pages 136– 143, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Mellish</author>
<author>Roger Evans</author>
<author>Lynne Cahill</author>
<author>Christy Doran</author>
<author>Daniel Paiva</author>
<author>Mike Reape</author>
<author>Donia Scott</author>
<author>Neil Tipper</author>
</authors>
<title>A representation for complex and evolving data dependencies in generation.</title>
<date>2000</date>
<booktitle>In Proc. of the 6th Conference on Applied Natural Language Processing,</booktitle>
<pages>119--126</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="6808" citStr="Mellish et al., 2000" startWordPosition="1024" endWordPosition="1027">n be modelled in sequential generation architecture where the RE component has access to information about syntactic realization and an approximative, intermediate linearization. Such a system is reminiscent of earlier work in rulebased generation that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and</context>
</contexts>
<marker>Mellish, Evans, Cahill, Doran, Paiva, Reape, Scott, Tipper, 2000</marker>
<rawString>Chris Mellish, Roger Evans, Lynne Cahill, Christy Doran, Daniel Paiva, Mike Reape, Donia Scott, and Neil Tipper. 2000. A representation for complex and evolving data dependencies in generation. In Proc. of the 6th Conference on Applied Natural Language Processing, pages 119–126, Seattle, Washington, USA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Meteer</author>
</authors>
<title>Bridging the generation gap between text planning and linguistic realization.</title>
<date>1991</date>
<journal>In Computational Intelligence,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="7028" citStr="Meteer, 1991" startWordPosition="1059" endWordPosition="1060">lebased generation that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have </context>
</contexts>
<marker>Meteer, 1991</marker>
<rawString>Marie Meteer. 1991. Bridging the generation gap between text planning and linguistic realization. In Computational Intelligence, volume 7 (4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Trainable methods for surface natural language generation.</title>
<date>2000</date>
<booktitle>In Proc. of the 1st North American chapter of the Association for Computational Linguistics conference, NAACL</booktitle>
<pages>194--201</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7974" citStr="Ratnaparkhi, 2000" startWordPosition="1202" endWordPosition="1203">ion (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence perp italians two men the two they &lt;empty&gt; victim the victim man a young he &lt;empty&gt; 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of know</context>
</contexts>
<marker>Ratnaparkhi, 2000</marker>
<rawString>Adwait Ratnaparkhi. 2000. Trainable methods for surface natural language generation. In Proc. of the 1st North American chapter of the Association for Computational Linguistics conference, NAACL 2000, pages 194–201, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building applied natural language generation systems.</title>
<date>1997</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="1268" citStr="Reiter and Dale, 1997" startWordPosition="163" endWordPosition="166">hat a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture. 1 Introduction Generating well-formed linguistic utterances from an abstract non-linguistic input involves making a multitude of conceptual, discourse-level as well as sentence-level, lexical and syntactic decisions. Work on rule-based natural language generation (NLG) has explored a number of ways to combine these decisions in an architecture, ranging from integrated systems where all decisions happen jointly (Appelt, 1982) to strictly sequential pipelines (Reiter and Dale, 1997). While integrated or interactive systems typically face issues with efficiency and scalability, they can directly account for interactions between discourse-level planning and linguistic realization. For instance, Rubinoff (1992) mentions Example (1) where the sentence planning component needs to have access to the lexical knowledge that “order” and not “home” can be realized as a verb in English. (1) a. *John homed him with an order. b. John ordered him home. In recent data-driven generation research, the focus has somewhat shifted from full data-to-text systems to approaches that isolate we</context>
<context position="21412" citStr="Reiter and Dale, 1997" startWordPosition="3362" endWordPosition="3365">automatically parsed version of the German TIGER treebank (Brants et al., 2002). This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). 4.2 Architectures Depending on the way the generation components are combined in an architecture, they will have access to different layers of the input representation. The following definitions of architectures recur to the layers introduced in Section 3.3. 4.2.1 First Pipeline The first pipeline corresponds most closely to a standard generation pipeline in the sense of (Reiter and Dale, 1997). REG is carried out prior to surface realization such that the RE component does not have access to surface syntax or word order whereas the SYN component has access to fully specified RE slots. • training 1551 1. train REG: (deepSyn_re, deepSyn+re) 2. train SYN: (deepSyn+re, shallowSyn+re) • prediction 1. apply REG: deepSyn_re → deepSyn+re 2. apply SYN: deepSyn+re → shallowSyn+re 3. linearize: shallowSyn+re → linSyn+re 4.2.2 Second Pipeline In the second pipeline, the order of the RE and SYN component is switched. In this case, REG has access to surface syntax without word order but the surf</context>
</contexts>
<marker>Reiter, Dale, 1997</marker>
<rawString>Ehud Reiter and Robert Dale. 1997. Building applied natural language generation systems. Nat. Lang. Eng., 3(1):57–87, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>Has a Consensus NL Generation Architecture Appeared, and is it Psycholinguistically Plausible?</title>
<date>1994</date>
<pages>163--170</pages>
<contexts>
<context position="7374" citStr="Reiter, 1994" startWordPosition="1112" endWordPosition="1113">terleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000</context>
</contexts>
<marker>Reiter, 1994</marker>
<rawString>Ehud Reiter. 1994. Has a Consensus NL Generation Architecture Appeared, and is it Psycholinguistically Plausible? pages 163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric K Ringger</author>
<author>Michael Gamon</author>
<author>Robert C Moore</author>
<author>David Rojas</author>
<author>Martine Smets</author>
<author>Simon CorstonOliver</author>
</authors>
<title>Linguistically Informed Statistical Models of Constituent Structure for Ordering in Sentence Realization.</title>
<date>2004</date>
<booktitle>In Proc. of the 2004 International Conference on Computational Linguistics,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="8231" citStr="Ringger et al., 2004" startWordPosition="1241" endWordPosition="1244">ations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence perp italians two men the two they &lt;empty&gt; victim the victim man a young he &lt;empty&gt; 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004)</context>
</contexts>
<marker>Ringger, Gamon, Moore, Rojas, Smets, CorstonOliver, 2004</marker>
<rawString>Eric K. Ringger, Michael Gamon, Robert C. Moore, David Rojas, Martine Smets, and Simon CorstonOliver. 2004. Linguistically Informed Statistical Models of Constituent Structure for Ordering in Sentence Realization. In Proc. of the 2004 International Conference on Computational Linguistics, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Robin</author>
</authors>
<title>A revision-based generation architecture for reporting facts in their historical context. In New Concepts in Natural Language Generation: Planning, Realization and Systems. Frances Pinter, London and,</title>
<date>1993</date>
<pages>238--265</pages>
<publisher>Pinter Publishers.</publisher>
<contexts>
<context position="6578" citStr="Robin, 1993" startWordPosition="989" endWordPosition="990">the generation architecture: 1) the sequential order of the modules, 2) parallelization of modules, 3) joint vs. separate modelling of implicit referents. Our results suggest that the interactions between RE and syntax can be modelled in sequential generation architecture where the RE component has access to information about syntactic realization and an approximative, intermediate linearization. Such a system is reminiscent of earlier work in rulebased generation that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tracta</context>
<context position="36315" citStr="Robin, 1993" startWordPosition="5759" endWordPosition="5760">enly show the tight connections between decisions at the level of reference, syntax and word order. These interactions entail highly interdependent modelling steps: Although there is a direct error propagation effect from predicted verb transformation on RE accuracy, predicted syntax still leads to informative intermediate linearizations that improve the RE prediction. Our optimal generation architecture thus has a sequential setup, where the first linearization step can be seen as an intermediate feedback that is revised in the final linearization. This connects to work in, e.g. (Hovy, 1988; Robin, 1993). In Figure 3, we compare two system outputs for the last two sentences of the text in Figure 2. The output of the sequential system is severely incoherent and would probably be rejected by a human reader: In sentence (5a) the victim subject of an active verb is deleted, and the relation between the possessive and the embedded victim RE is not clear. In sentence (5b) the first conjunct realizes a pronominal perp RE and the second conjunct a nominal perp RE. The output of the revision-based system reads much more natural. This example shows that the extension of the REG problem to texts with mo</context>
</contexts>
<marker>Robin, 1993</marker>
<rawString>Jacques Robin. 1993. A revision-based generation architecture for reporting facts in their historical context. In New Concepts in Natural Language Generation: Planning, Realization and Systems. Frances Pinter, London and, pages 238–265. Pinter Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Roth</author>
<author>Anette Frank</author>
</authors>
<title>Aligning predicate argument structures in monolingual comparable texts: A new corpus for a new task.</title>
<date>2012</date>
<booktitle>In Proc. of the 1st Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="9201" citStr="Roth and Frank (2012)" startWordPosition="1401" endWordPosition="1404">about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic ut</context>
</contexts>
<marker>Roth, Frank, 2012</marker>
<rawString>Michael Roth and Anette Frank. 2012. Aligning predicate argument structures in monolingual comparable texts: A new corpus for a new task. In Proc. of the 1st Joint Conference on Lexical and Computational Semantics (*SEM), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Rubinoff</author>
</authors>
<title>Integrating text planning and linguistic choice by annotating linguistic structures.</title>
<date>1992</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>587</volume>
<pages>45--56</pages>
<editor>In Robert Dale, Eduard H. Hovy, Dietmar R¨osner, and Oliviero Stock, editors, NLG,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1498" citStr="Rubinoff (1992)" startWordPosition="194" endWordPosition="195">volves making a multitude of conceptual, discourse-level as well as sentence-level, lexical and syntactic decisions. Work on rule-based natural language generation (NLG) has explored a number of ways to combine these decisions in an architecture, ranging from integrated systems where all decisions happen jointly (Appelt, 1982) to strictly sequential pipelines (Reiter and Dale, 1997). While integrated or interactive systems typically face issues with efficiency and scalability, they can directly account for interactions between discourse-level planning and linguistic realization. For instance, Rubinoff (1992) mentions Example (1) where the sentence planning component needs to have access to the lexical knowledge that “order” and not “home” can be realized as a verb in English. (1) a. *John homed him with an order. b. John ordered him home. In recent data-driven generation research, the focus has somewhat shifted from full data-to-text systems to approaches that isolate well-defined subproblems from the NLG pipeline. In particular, the tasks of surface realization and referring expression generation (REG) have received increasing attention using a number of available annotated data sets (Belz and K</context>
</contexts>
<marker>Rubinoff, 1992</marker>
<rawString>Robert Rubinoff. 1992. Integrating text planning and linguistic choice by annotating linguistic structures. In Robert Dale, Eduard H. Hovy, Dietmar R¨osner, and Oliviero Stock, editors, NLG, volume 587 of Lecture Notes in Computer Science, pages 45–56. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
<author>Roser Morante</author>
<author>Collin Baker</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval-2010 task 10: Linking events and their participants in discourse.</title>
<date>2010</date>
<booktitle>In Proc. of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>45--50</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="9439" citStr="Ruppenhofer et al., 2010" startWordPosition="1439" endWordPosition="1442"> Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical fra</context>
</contexts>
<marker>Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2010</marker>
<rawString>Josef Ruppenhofer, Caroline Sporleder, Roser Morante, Collin Baker, and Martha Palmer. 2010. Semeval-2010 task 10: Linking events and their participants in discourse. In Proc. of the 5th International Workshop on Semantic Evaluation, pages 45–50, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Seeker</author>
<author>Jonas Kuhn</author>
</authors>
<title>Making Ellipses Explicit in Dependency Conversion for a German Treebank.</title>
<date>2012</date>
<booktitle>In Proc. of the 8th conference on International Language Resources and Evaluation,</booktitle>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="21013" citStr="Seeker and Kuhn (2012)" startWordPosition="3301" endWordPosition="3304">seline for the REG component is defined as follows: if the preceding and the current RE slot are instances of the same referent, realize a pronoun, else realize the longest nominal RE candidate that has not been used in the preceding text. 4.1.3 LIN: Linearization For linearization, we use the state-of-the-art dependency linearizer described in Bohnet et al. (2012). We train the linearizer on an automatically parsed version of the German TIGER treebank (Brants et al., 2002). This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). 4.2 Architectures Depending on the way the generation components are combined in an architecture, they will have access to different layers of the input representation. The following definitions of architectures recur to the layers introduced in Section 3.3. 4.2.1 First Pipeline The first pipeline corresponds most closely to a standard generation pipeline in the sense of (Reiter and Dale, 1997). REG is carried out prior to surface realization such that the RE component does not have access to surface syntax or word order whereas the SYN component has access to fully specified RE slots. • tra</context>
</contexts>
<marker>Seeker, Kuhn, 2012</marker>
<rawString>Wolfgang Seeker and Jonas Kuhn. 2012. Making Ellipses Explicit in Dependency Conversion for a German Treebank. In Proc. of the 8th conference on International Language Resources and Evaluation, Istanbul, Turkey, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
<author>Ann Copestake</author>
</authors>
<title>Generating referring expressions in open domains.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>407--414</pages>
<location>Barcelona, Spain,</location>
<marker>Siddharthan, Copestake, 2004</marker>
<rawString>Advaith Siddharthan and Ann Copestake. 2004. Generating referring expressions in open domains. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 407–414, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pontus Stenetorp</author>
<author>Sampo Pyysalo</author>
<author>Goran Topi´c</author>
<author>Tomoko Ohta</author>
<author>Sophia Ananiadou</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>brat: a web-based tool for nlp-assisted text annotation.</title>
<date>2012</date>
<booktitle>In Proc. of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>102--107</pages>
<location>Avignon, France,</location>
<marker>Stenetorp, Pyysalo, Topi´c, Ohta, Ananiadou, Tsujii, 2012</marker>
<rawString>Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c, Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsujii. 2012. brat: a web-based tool for nlp-assisted text annotation. In Proc. of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 102– 107, Avignon, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Wanner</author>
<author>Simon Mille</author>
<author>Bernd Bohnet</author>
</authors>
<title>Towards a surface realization-oriented corpus annotation.</title>
<date>2012</date>
<booktitle>In Proc. of the 7th International Natural Language Generation Conference,</booktitle>
<pages>22--30</pages>
<location>Utica, IL,</location>
<contexts>
<context position="10364" citStr="Wanner et al., 2012" startWordPosition="1579" endWordPosition="1582">n the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do not necessarily fit the needs of generation (Bohnet et al., 2010; Wanner et al., 2012). Zarrieß et al. (2011) discuss the problem of an input representation that is appropriately underspecified for the realistic generation of voice alternations. 3 The Data Set The data set for our generation experiments consists of 200 newspaper articles about robbery events. The articles were extracted from a large German newspaper corpus. A complete example text with RE annotations is given in Figure 2, Table 1 summarizes some data set statistics. 3.1 RE annotation The RE annotations mark explicit and implicit mentions of referents involved in the robbery event described in an article. Explic</context>
</contexts>
<marker>Wanner, Mille, Bohnet, 2012</marker>
<rawString>Leo Wanner, Simon Mille, and Bernd Bohnet. 2012. Towards a surface realization-oriented corpus annotation. In Proc. of the 7th International Natural Language Generation Conference, pages 22–30, Utica, IL, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Wanner</author>
</authors>
<title>Building another bridge over the generation gap.</title>
<date>1994</date>
<booktitle>In Proc. of the 7th International Workshop on Natural Language Generation, INLG ’94,</booktitle>
<pages>137--144</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7043" citStr="Wanner, 1994" startWordPosition="1061" endWordPosition="1062">tion that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced f</context>
</contexts>
<marker>Wanner, 1994</marker>
<rawString>Leo Wanner. 1994. Building another bridge over the generation gap. In Proc. of the 7th International Workshop on Natural Language Generation, INLG ’94, pages 137–144, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sina Zarrieß</author>
<author>Aoife Cahill</author>
<author>Jonas Kuhn</author>
</authors>
<title>Underspecifying and predicting voice for surface realisation ranking.</title>
<date>2011</date>
<booktitle>In Proc. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1007--1017</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="9059" citStr="Zarrieß et al. (2011)" startWordPosition="1377" endWordPosition="1380">ic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportc</context>
<context position="10387" citStr="Zarrieß et al. (2011)" startWordPosition="1583" endWordPosition="1586">epresentation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do not necessarily fit the needs of generation (Bohnet et al., 2010; Wanner et al., 2012). Zarrieß et al. (2011) discuss the problem of an input representation that is appropriately underspecified for the realistic generation of voice alternations. 3 The Data Set The data set for our generation experiments consists of 200 newspaper articles about robbery events. The articles were extracted from a large German newspaper corpus. A complete example text with RE annotations is given in Figure 2, Table 1 summarizes some data set statistics. 3.1 RE annotation The RE annotations mark explicit and implicit mentions of referents involved in the robbery event described in an article. Explicit mentions are marked </context>
</contexts>
<marker>Zarrieß, Cahill, Kuhn, 2011</marker>
<rawString>Sina Zarrieß, Aoife Cahill, and Jonas Kuhn. 2011. Underspecifying and predicting voice for surface realisation ranking. In Proc. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1007– 1017, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sina Zarrieß</author>
<author>Aoife Cahill</author>
<author>Jonas Kuhn</author>
</authors>
<title>To what extent does sentence-internal realisation reflect discourse context? a study on word order.</title>
<date>2012</date>
<booktitle>In Proc. of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>767--776</pages>
<location>Avignon, France,</location>
<contexts>
<context position="8310" citStr="Zarrieß et al., 2012" startWordPosition="1254" endWordPosition="1257">y known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence perp italians two men the two they &lt;empty&gt; victim the victim man a young he &lt;empty&gt; 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instan</context>
</contexts>
<marker>Zarrieß, Cahill, Kuhn, 2012</marker>
<rawString>Sina Zarrieß, Aoife Cahill, and Jonas Kuhn. 2012. To what extent does sentence-internal realisation reflect discourse context? a study on word order. In Proc. of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 767–776, Avignon, France, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>