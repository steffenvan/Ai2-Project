<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004956">
<title confidence="0.9959665">
Combining Orthogonal Monolingual and Multilingual Sources of
Evidence for All Words WSD
</title>
<author confidence="0.997874">
Weiwei Guo
</author>
<affiliation confidence="0.9958625">
Computer Science Department
Columbia University
</affiliation>
<address confidence="0.992769">
New York, NY, 10115
</address>
<email confidence="0.999466">
weiwei@cs.columbia.edu
</email>
<author confidence="0.99489">
Mona Diab
</author>
<affiliation confidence="0.9957695">
Center for Computational Learning Systems
Columbia University
</affiliation>
<address confidence="0.992757">
New York, NY, 10115
</address>
<email confidence="0.999628">
mdiab@ccls.columbia.edu
</email>
<sectionHeader confidence="0.993828" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998961136363636">
Word Sense Disambiguation remains one
of the most complex problems facing com-
putational linguists to date. In this pa-
per we present a system that combines
evidence from a monolingual WSD sys-
tem together with that from a multilingual
WSD system to yield state of the art per-
formance on standard All-Words data sets.
The monolingual system is based on a
modification of the graph based state of the
art algorithm In-Degree. The multilingual
system is an improvement over an All-
Words unsupervised approach, SALAAM.
SALAAM exploits multilingual evidence
as a means of disambiguation. In this
paper, we present modifications to both
of the original approaches and then their
combination. We finally report the highest
results obtained to date on the SENSEVAL
2 standard data set using an unsupervised
method, we achieve an overall F measure
of 64.58 using a voting scheme.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999833740740741">
Despite advances in natural language processing
(NLP), Word Sense Disambiguation (WSD) is still
considered one of the most challenging problems
in the field. Ever since the field’s inception, WSD
has been perceived as one of the central problems
in NLP. WSD is viewed as an enabling technology
that could potentially have far reaching impact on
NLP applications in general. We are starting to see
the beginnings of a positive effect of WSD in NLP
applications such as Machine Translation (Carpuat
and Wu, 2007; Chan et al., 2007).
Advances in WSD research in the current mil-
lennium can be attributed to several key factors:
the availability of large scale computational lexi-
cal resources such as WordNets (Fellbaum, 1998;
Miller, 1990), the availability of large scale cor-
pora, the existence and dissemination of standard-
ized data sets over the past 10 years through differ-
ent testbeds such as SENSEVAL and SEMEVAL
competitions,1 devising more robust computing
algorithms to handle large scale data sets, and sim-
ply advancement in hardware machinery.
In this paper, we address the problem of WSD
of all content words in a sentence, All-Words data.
In this framework, the task is to associate all to-
kens with their contextually relevant meaning defi-
nitions from some computational lexical resource.
Our work hinges upon combining two high qual-
ity WSD systems that rely on essentially differ-
ent sources of evidence. The two WSD systems
are a monolingual system RelCont and a multi-
lingual system TransCont. RelCont is an en-
hancement on an existing graph based algorithm,
In-Degree, first described in (Navigli and Lapata,
2007). TransCont is an enhancement over an
existing approach that leverages multilingual evi-
dence through projection, SALAAM, described in
detail in (Diab and Resnik, 2002). Similar to the
leveraged systems, the current combined approach
is unsupervised, namely it does not rely on training
data from the onset. We show that by combining
both sources of evidence, our approach yields the
highest performance for an unsupervised system
to date on standard All-Words data sets.
This paper is organized as follows: Section 2
delves into the problem of WSD in more detail;
Section 3 explores some of the relevant related
work; in Section 4, we describe the two WSD
systems in some detail emphasizing the improve-
ments to the basic systems in addition to a de-
scription of our combination approach; we present
our experimental set up and results in Section 5;
we discuss the results and our overall observations
with error analysis in Section 6; Finally, we con-
</bodyText>
<footnote confidence="0.995858">
1http://www.semeval.org
</footnote>
<page confidence="0.891429">
1542
</page>
<note confidence="0.85493">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1542–1551,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
clude in Section 7.
</note>
<sectionHeader confidence="0.801112" genericHeader="method">
2 Word Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.999951193548387">
The definition of WSD has taken on several differ-
ent practical meanings in recent years. In the latest
SEMEVAL 2010 workshop, there are 18 tasks de-
fined, several of which are on different languages,
however we recognize the widening of the defi-
nition of the task of WSD. In addition to the tra-
ditional All-Words and Lexical Sample tasks, we
note new tasks on word sense discrimination (no
sense inventory needed, the different senses are
merely distinguished), lexical substitution using
synonyms of words as substitutes both monolin-
gually and multilingually, as well as meaning def-
initions obtained from different languages namely
using words in translation.
Our paper is about the classical All-Words
(AW) task of WSD. In this task, all content bear-
ing words in running text are disambiguated from
a static lexical resource. For example a sen-
tence such as ‘I walked by the bank and saw
many beautiful plants there.’ will have the verbs
‘walked, saw’, the nouns ‘bank, plants’, the ad-
jectives ‘many, beautiful’, and the adverb ‘there’,
be disambiguated from a standard lexical resource.
Hence, using WordNet,2 ‘walked’ will be assigned
the corresponding meaning definitions of: to use
one’s feet to advance; to advance by steps, ‘saw’
will be assigned the meaning definition of: to per-
ceive by sight or have the power to perceive by
sight, the noun ‘bank’ will be assigned the mean-
ing definition of: sloping land especially the slope
beside a body of water, and so on.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="method">
3 Related Works
</sectionHeader>
<bodyText confidence="0.997656428571428">
Many systems over the years have been proposed
for the task. A thorough review of the state of
the art through the late 1990s (Ide and Veronis,
1998) and more recently in (Navigli, 2009). Sev-
eral techniques have been used to tackle the prob-
lem ranging from rule based/knowledge based
approaches to unsupervised and supervised ma-
chine learning techniques. To date, the best ap-
proaches that solve the AW WSD task are super-
vised as illustrated in the different SenseEval and
SEMEVAL AW task (Palmer et al., 2001; Snyder
and Palmer, 2004; Pradhan et al., 2007).
In this paper, we present an unsupervised com-
bination approach to the AW WSD problem that
</bodyText>
<footnote confidence="0.736868">
2http://wordnet.princeton.edu
</footnote>
<bodyText confidence="0.997433">
relies on WN similarity measures in conjunction
with evidence obtained through exploiting multi-
lingual evidence. We will review the closely rele-
vant related work on which this current investiga-
tion is based.3
</bodyText>
<sectionHeader confidence="0.973207" genericHeader="method">
4 Our Approach
</sectionHeader>
<bodyText confidence="0.99992275">
Our current investigation exploits two basic unsu-
pervised approaches that perform at state-of-the-
art for the AW WSD task in an unsupervised set-
ting. Crucially the two systems rely on differ-
ent sources of evidence allowing them to comple-
ment each other to a large extent leading to better
performance than for each system independently.
Given a target content word and co-occurring con-
textual clues, the monolingual system RelCont
attempts to assign the approporiate meaning def-
inition to the target word. Such words by defini-
tion are semantically related words. TransCont,
on the other hand, is the multilingual system.
TransCont defines the notion of context in the
translational space using a foreign word as a fil-
ter for defining the contextual content words for
a given target word. In this multilingual setting,
all the words that are mapped to (aligned with)
the same orthographic form in a foreign language
constitute the context. In the next subsections
we describe the two approaches RelCont and
TransCont in some detail, then we proceed to
describe two combination methods for the two ap-
proaches: MERGE and VOTE.
</bodyText>
<subsectionHeader confidence="0.997619">
4.1 Monolingual System RelCont
</subsectionHeader>
<bodyText confidence="0.999753428571428">
RelCont is based on an extension of a state-
of-the-art WSD approach by (Sinha and Mihal-
cea, 2007), henceforth (SM07). In the basic
SM07 work, the authors combine different seman-
tic similarity measures with different graph based
algorithms as an extension to work in (Mihal-
cea, 2005). Given a sequence of words W =
{fw1, w2 ... w�I, each word wi with several senses
lsi1, si2...si.I. A graph G = (V,E) is defined such
that there exists a vertex v for each sense. Two
senses of two different words may be connected by
an edge e, depending on their distance. That two
senses are connected suggests they should have
influence on each other, accordingly a maximum
</bodyText>
<footnote confidence="0.99289875">
3We acknowledge the existence of many research papers
that tackled the AW WSD problem using unsupervised ap-
proaches, yet for lack of space we will not be able to review
most of them.
</footnote>
<page confidence="0.98946">
1543
</page>
<bodyText confidence="0.984779582524273">
allowable distance is set. They explore 4 differ-
ent graph based algorithms. The highest yield-
ing algorithm in their work is the In-Degree al-
gorithm combining different WN similarity mea-
sures depending on POS. They used the Jiang
and Conrath (JCN) (Jiang and Conrath., 1997)
similarity measure within nouns, the Leacock &amp;
Chodorow (LCH) (Leacock and Chodorow, 1998)
similarity measure within verbs, and the Lesk
(Lesk, 1986) similarity measure within adjectives,
within adverbs, and among different POS tag pair-
ings. They evaluate their work against the SEN-
SEVAL 2 AW test data (SV2AW). They tune the
parameters of their algorithm – namely, the nor-
malization ratio for some of these measures – on
the SENSEVAL 3 data set. They report a state-of-
the-art unsupervised system that yields an overall
performance across all AW POS sets of 57.2%.
In our current work, we extend the SM07 work
in some interesting ways. A detailed narrative
of our approach is described in (Guo and Diab,
2009). Briefly, we focus on the In-Degree
graph based algorithm since it is the best per-
former in the SM07 work. The In-Degree al-
gorithm presents the problem as a weighted graph
with senses as nodes and the similarity between
senses as weights on edges. The In-Degree
of a vertex refers to the number of edges inci-
dent on that vertex. In the weighted graph, the
In-Degree for each vertex is calculated by sum-
ming the weights on the edges that are incident on
it. After all the In-Degree values for each sense
are computed, the sense with maximum value is
chosen as the final sense for that word.
In this paper, we use the In-Degree algo-
rithm while applying some modifications to the
basic similarity measures exploited and the WN
lexical resource tapped into. Similar to the orig-
inal In-Degree algorithm, we produce a prob-
abilistic ranked list of senses. Our modifications
are described as follows:
JCN for Verb-Verb Similarity In our imple-
mentation of the In-Degree algorithm, we use
the JCN similarity measure for both Noun-Noun
similarity calculation similar to SM07. However,
different from SM07, instead of using LCH for
Verb-Verb similarity, we use the JCN metric as it
yields better performance in our experimentations.
Expand Lesk Following the intuition in (Ped-
ersen et al., 2005), henceforth (PEA05), we ex-
pand the basic Lesk similarity measure to take into
account the glosses for all the relations for the
synsets on the contextual words and compare them
with the glosses of the target word senses, there-
fore going beyond the is-a relation. We exploit the
observation that WN senses are too fine-grained,
accordingly the neighbors would be slightly varied
while sharing significant semantic meaning con-
tent. To find similar senses, we use the relations:
hypernym, hyponym, similar attributes, similar
verb group, pertinym, holonym, and meronyms.4
The algorithm assumes that the words in the input
are POS tagged. In PEA05, the authors retrieve all
the relevant neighbors to form a bag of words for
both the target sense and the surrounding senses of
the context words, they specifically focus on the
Lesk similarity measure. In our current work, we
employ the neighbors in a disambiguation strategy
using different similarity measures one pair at a
time. Our algorithm takes as input a target sense
and a sense pertaining to a word in the surrounding
context, and returns a sense similarity score. We
do not apply the WN relations expansion to the
target sense. It is only applied to the contextual
word.5
For the monolingual system, we employ the
same normalization values used in SM07 for the
different similarity measures. Namely for the Lesk
and Expand-Lesk, we use the same cut-off value of
240, accordingly, if the Lesk or Expand-Lesk sim-
ilarity value returns 0 &lt;= 240 it is converted to
a real number in the interval [0,1], any similarity
over 240 is by default mapped to 1. We will refer
to the Expand-Lesk with this threshold as Lesk2.
We also experimented with different thresholds for
the Lesk and Expand-Lesk similarity measure us-
ing the SENSEVAL 3 data as a tuning set. We
found that a cut-off threshold of 40 was also use-
ful. We will refer to this variant of Expand-Lesk
with a cut off threshold of 40 as Lesk3. For JCN,
similar to SM07, the values are from 0.04 to 0.2,
we mapped them to the interval [0,1]. We did not
run any calibration studies beyond the what was
reported in SM07.
4In our experiments, we varied the number of relations to
employ and they all yielded relatively similar results. Hence
in this paper, we report results using all the relations listed
above.
5We experimented with expanding both the contextual
sense and the target sense and we found that the unreliabil-
ity of some of the relations is detrimental to the algorithm’s
performance. Hence we decided empirically to expand only
the contextual word.
</bodyText>
<page confidence="0.979182">
1544
</page>
<bodyText confidence="0.999922090909091">
SemCor Expansion of WN A part of the
RelCont approach relies on using the Lesk al-
gorithm. Accordingly, the availability of glosses
associated with the WN entries is extremely bene-
ficial. Therefore, we expand the number of glosses
available in WN by using the SemCor data set,
thereby adding more examples to compare. The
SemCor corpus is a corpus that is manually sense
tagged (Miller, 1990).6 In this expansion, depend-
ing on the version of WN, we use the sense-index
file in the WN Database to convert the SemCor
data to the appropriate version sense annotations.
We augment the sense entries for the different POS
WN databases with example usages from SemCor.
The augmentation is done as a look up table exter-
nal to WN proper since we did not want to dabble
with the WN offsets. We set a cap of 30 additional
examples per synset. We used the first 30 exam-
ples with no filtering criteria. Many of the synsets
had no additional examples. WN1.7.1 comprises a
total of 26875 synsets, of which 25940 synsets are
augmented with SemCor examples.7
</bodyText>
<subsectionHeader confidence="0.993855">
4.2 Multilingual System TransCont
</subsectionHeader>
<bodyText confidence="0.999951157894737">
TransCont is based on the WSD system
SALAAM (Diab and Resnik, 2002), henceforth
(DR02). The SALAAM system leverages word
alignments from parallel corpora to perform WSD.
The SALAAM algorithm exploits the word corre-
spondence cross linguistically to tag word senses
on words in running text. It relies on several un-
derlying assumptions. The first assumption is that
senses of polysemous words in one language could
be lexicalized differently in other languages. For
example, ‘bank’ in English would be translated as
banque or rive de fleuve in French, depending on
context. The other assumption is that if Language
1 (L1) words are translated to the same ortho-
graphic form in Language 2 (L2), then they share
the some element of meaning, they are semanti-
cally similar.8
The SALAAM algorithm can be described as
follows. Given a parallel corpus of L1-L2 that
</bodyText>
<footnote confidence="0.966972">
6Using SemCor in this setting to augment WN does hint
of using supervised data in the WSD process, however, since
our approach does not rely on training data and SemCor is not
used in our algorithm directly to tag data, but to augment a
rich knowledge resource, we contend that this does not affect
our system’s designation as an unsupervised system.
7Some example sentences are repeated across different
synsets and POS since the SemCor data is annotated as an
All-Words tagged data set.
8We implicitly make the underlying simplifying assump-
tion that the L2 words are less ambiguous than the L1 words.
</footnote>
<bodyText confidence="0.999955705882353">
is sentence and word aligned, group all the word
types in L1 that map to same word in L2 creat-
ing clusters referred to as typesets. Then perform
disambiguation on the typeset clusters using WN.
Once senses are identified for each word in the
cluster, the senses are propagated back to the origi-
nal word instances in the corpus. In the SALAAM
algorithm, the disambiguation step is carried out
as follows: within each of these target sets con-
sider all possible sense tags for each word and
choose sense tags informed by semantic similarity
with all the other words in the whole group. The
algorithm is a greedy algorithm that aims at maxi-
mizing the similarity of the chosen sense across all
the words in the set. The SALAAM disambigua-
tion algorithm used the noun groupings (Noun-
Groupings) algorithm described in DR02. The al-
gorithm applies disambiguation within POS tag.
The authors report only results on the nouns only
since NounGroupings heavily exploits the hierar-
chy structure of the WN noun taxonomy, which
does not exist for adjectives and adverbs, and is
very shallow for verbs.
Essentially SALAAM relies on variability in
translation as it is important to have multiple
words in a typeset to allow for disambiguation.
In the original SALAAM system, the authors au-
tomatically translated several balanced corpora in
order to render more variable data for the approach
to show it’s impact. The corpora that were trans-
lated are: the WSJ, the Brown corpus and all the
SENSEVAL data. The data were translated to dif-
ferent languages (Arabic, French and Spanish) us-
ing state of art MT systems. They employed the
automatic alignment system GIZA++ (Och and
Ney, 2003) to obtain word alignments in a single
direction from L1 to L2.
For TransCont we use the basic SALAAM
approach with some crucial modifications that
lead to better performance. We still rely on par-
allel corpora, we extract typesets based on the in-
tersection of word alignments in both alignment
directions using more advanced GIZA++ machin-
ery. In contrast to DR02, we experiment with
all four POS: Verbs (V), Nouns (N), Adjectives
(A) and Adverbs (R). Moreover, we modified the
underlying disambiguation method on the type-
sets. We still employ WN similarity, however, we
do not use the NounGroupings algorithm. Our
disambiguation method relies on calculating the
sense pair similarity exhaustively across all the
</bodyText>
<page confidence="0.967904">
1545
</page>
<bodyText confidence="0.999864133333333">
word types in a typeset and choosing the combi-
nation that yields the highest similarity. We exper-
imented with all the WN similarity measures in
the WN similarity package.9 We also experiment
with Lesk2 and Lesk3 as well as other measures,
however we do not use SemCor examples with
TransCont. We found that the best results are
yielded using the Lesk2/Lesk3 similarity measure
for N, A and R POS tagsets, while the Lin and JCN
measures yield the best performance for the verbs.
In contrast to the DR02 approach, we modify the
internal WSD process to use the In-Degree al-
gorithm on the typeset, so each sense obtains a
confidence, and the sense(s) with the highest con-
fidences are returned.
</bodyText>
<subsectionHeader confidence="0.999235">
4.3 Combining RelCont and TransCont
</subsectionHeader>
<bodyText confidence="0.9999094">
Our objective is to combine the different sources
of evidence for the purposes of producing an effec-
tive overall global WSD system that is able to dis-
ambiguate all content words in running text. We
combine the two systems in two different ways.
</bodyText>
<sectionHeader confidence="0.525308" genericHeader="method">
4.3.1 MERGE
</sectionHeader>
<bodyText confidence="0.999958230769231">
In this combination scheme, the words in the type-
set that result from the TransCont approach are
added to the context of the target word in the
RelCont approach. However the typeset words
are not treated the same as the words that come
from the surrounding context in the In-Degree
algorithm as we recognize that words that are
yielded in the typesets are semantically similar in
terms of content rather than being co-occurring
words as is the case for contextual words in Rel-
Cont. Heeding this difference, we proceed to
calculate similarity for words in the typesets us-
ing different similarity measures. In the case of
noun-noun similarity, in the original RelCont
experiments we use JCN, however with the words
present in the TransCont typesets we use one
of the Lesk variants, Lesk2 or Lesk3. Our obser-
vation is that the JCN measure is relatively coarser
grained, compared to Lesk measures, therefore it
is sufficient in case of lexical relatedness therefore
works well in case of the context words. Yet for
the words yielded in the TransCont typesets a
method that exploits the underlying rich relations
in the noun hierarchy captures the semantic sim-
ilarity more aptly. In the case of verbs we still
maintain the JCN similarity as it most effective
</bodyText>
<footnote confidence="0.80914">
9http://wn-similarity.sourceforge.net/
</footnote>
<bodyText confidence="0.999945333333333">
given the shallowness of the verb hierarchy and
the inherent nature of the verbal synsets which are
differentiated along syntactic rather than semantic
dimensions. We employ the Lesk algorithm still
with A-A and R-R similarity and when comparing
across different POS tag pairings.
</bodyText>
<subsectionHeader confidence="0.676932">
4.3.2 VOTE
</subsectionHeader>
<bodyText confidence="0.999936454545454">
In this combination scheme, the output of the
global disambiguation system is simply an inter-
section of the two outputs from the two underly-
ing systems RelCont and TransCont. Specif-
ically, we sum up the confidence ranging from
0 to 1 of the two system In-Degree algo-
rithm outputs to obtain a final confidence for each
sense, choosing the sense(s) that yields the high-
est confidences. The fact that TransCont uses
In-Degree internally allows for a seamless in-
tegration.
</bodyText>
<sectionHeader confidence="0.998069" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.939608">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.99966725">
The parallel data we experiment with are the
same standard data sets as in (Diab and Resnik,
2002), namely, Senseval 2 English AW data sets
(SV2AW) (Palmer et al., 2001), and Seneval 3 En-
glish AW (SV3AW) data set. We use the true POS
tag sets in the test data as rendered in the Penn
Tree Bank.10 We present our results on WordNet
1.7.1 for ease of comparison with previous results.
</bodyText>
<subsectionHeader confidence="0.995077">
5.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999741">
We use the scorer2 software to report fine-
grained (P)recision and (R)ecall and (F)-measure.
</bodyText>
<subsectionHeader confidence="0.997076">
5.3 Baselines
</subsectionHeader>
<bodyText confidence="0.9600145625">
We consider here several baselines. 1. A random
baseline (RAND) is the most appropriate base-
line for an unsupervised approach.2. We include
the most frequent sense baseline (MFBL), though
we note that we consider the most frequent sense
or first sense baseline to be a supervised baseline
since it depends crucially on SemCor in ranking
the senses within WN.11 3. The SM07 results as a
10We exclude the data points that have a tag of ”U” in the
gold standard for both baselines and our system.
11From an application standpoint, we do not find the first
sense baseline to be of interest since it introduces a strong
level of uniformity – removing semantic variability – which
is not desirable. Even if the first sense achieves higher results
in data sets, it is an artifact of the size of the data and the very
limited number of documents under investigation.
</bodyText>
<page confidence="0.988288">
1546
</page>
<bodyText confidence="0.9317885">
monolingual baseline. 4. The DR02 results as the
multilingual baseline.
</bodyText>
<subsectionHeader confidence="0.92387">
5.4 Experimental Results
5.4.1 RelCont
</subsectionHeader>
<bodyText confidence="0.995921260273973">
We present the results for 4 different experi-
mental conditions for RelCont: JCN-V which
uses JCN instead of LCH for verb-verb similar-
ity comparison, we consider this our base con-
dition; +ExpandL is adding the Lesk Expansion
to the base condition, namely Lesk2;12 +SemCor
adds the SemCor expansion to the base condi-
tion; and finally +ExpandL SemCor, adds the lat-
ter both conditions simultaneously. Table 1 illus-
trates the obtained results for the SV2AW using
WordNet 1.7.1 since it is the most studied data set
and for ease of comparison with previous studies.
We break the results down by POS tag (N)oun,
(V)erb, (A)djective, and Adve(R)b. The coverage
for SV2AW is 98.17% losing some of the verb and
adverb target words.
Our overall results on all the data sets clearly
outperform the baseline as well as state-of-the-
art performance using an unsupervised system
(SM07) in overall f-measure across all the data
sets. We are unable to beat the most frequent
baseline (MFBL) which is obtained using the first
sense. However MFBL is a supervised baseline
and our approach is unsupervised. Our implemen-
tation of SM07 is slightly higher than those re-
ported in (Sinha and Mihalcea, 2007) (57.12% )
is probably due to the fact that we do not consider
the items tagged as ”U” and also we resolve some
of the POS tag mismatches between the gold set
and the test data. We note that for the SV2AW data
set our coverage is not 100% due to some POS tag
mismatches that could not have been resolved au-
tomatically. These POS tag problems have to do
mainly with multiword expressions. In observing
the performance of the overall RelCont, we note
that using JCN for verbs clearly outperforms us-
ing the LCH similarity measure. Using SemCor to
augment WN examples seems to have the biggest
impact. Combining SemCor with ExpandL yields
the best results.
Observing the results yielded per POS in Ta-
ble 1, ExpandL seems to have the biggest impact
on the Nouns only. This is understandable since
the noun hierarchy has the most dense relations
and the most consistent ones. SemCor augmen-
12Using Lesk3 yields almost the same results
tation of WN seemed to benefit all POS signifi-
cantly except for nouns. In fact the performance
on the nouns deteriorated from the base condition
JCN-V from 68.7 to 68.3%. This maybe due to in-
consistencies in the annotations of nouns in Sem-
Cor or the very fine granularity of the nouns in
WN. We know that 72% of the nouns, 74% of
the verbs, 68.9% of the adjectives, and 81.9% of
the adverbs directly exploited the use of SemCor
augmented examples. Combining SemCor and
ExpandL seems to have a positive impact on the
verbs and adverbs, but not on the nouns and adjec-
tives. These trends are not held consistently across
data sets. For example, we see that SemCor aug-
mentation helps all POS tag sets over using Ex-
pandL alone or even when combined with Sem-
Cor. We note the similar trends in performance for
the SV3AW data.
Compared to state of the art systems, RelCont
with an overall F-measure performance of 62.13%
outperforms the best unsupervised system of
57.5% UNED-AW-U2 for SV2 (Navigli, 2009). It
is worth noting that it is higher than several of the
supervised systems. Moreover, RelCont yields
better overall results on SV3 at 59.87 compared to
the best unsupervised system IRST-DDD-U which
yielded an F-measure of 58.3% (Navigli, 2009).
</bodyText>
<subsubsectionHeader confidence="0.442423">
5.4.2 TransCont
</subsubsectionHeader>
<bodyText confidence="0.999964666666667">
For the TransCont results we illustrate the orig-
inal SALAAM results as our baseline. Simi-
lar to the DR02 work, we actually use the same
SALAAM parallel corpora comprising more than
5.5M English tokens translated using a single ma-
chine translation system GlobalLink. Therefore
our parallel corpus is the French English transla-
tion condition mentioned in DR02 work as FrGl.
We have 4 experimental conditions: FRGL using
Lesk2 for all POS tags in the typeset disambigua-
tion (Lesk2); FRGL using Lesk3 for all POS tags
(Lesk3); using Lesk3 for N, A and R but LIN simi-
larity measure for verbs (Lesk3 Lin); using Lesk3
for N, A and R but JCN for verbs (Lesk3 JCN).
In Table 3 we note the the Lesk3 JCN followed
immediately by Lesk3 Lin yield the best perfor-
mance. The trend holds for both SV2AW and
SV3AW. Essentially our new implementation of
the multilingual system significantly outperforms
the original DR02 implementation for all experi-
mental conditions.
</bodyText>
<page confidence="0.982429">
1547
</page>
<table confidence="0.99984325">
Condition N V A R Global F Measure
RAND 43.7 21 41.2 57.4 39.9
MFBL 71.8 41.45 67.7 81.8 65.35
SM07 68.7 33.01 65.2 63.1 59.2
JCN-V 68.7 35.46 65.2 63.1 59.72
+ExpandL 70.2 35.86 65.4 62.45 60.48
+SemCor 68.5 38.66 69.2 67.75 61.79
+ExpandL SemCor 69.0 38.66 68.8 69.45 62.13
</table>
<tableCaption confidence="0.99429">
Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
</tableCaption>
<table confidence="0.999939625">
Condition N V A R Global F Measure
RAND 39.67 19.34 41.85 92.31 32.97
MFBL 70.4 54.15 66.7 92.88 63.96
SM07 60.9 43.4 57 92.88 53.98
JCN-V 60.9 48.5 57 92.88 55.87
+ExpandL 59.9 48.55 57.95 92.88 55.62
+SemCor 66 48.95 65.55 92.88 59.87
+ExpandL SemCor 65 49.2 65.55 92.88 59.52
</table>
<tableCaption confidence="0.997982">
Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
</tableCaption>
<subsectionHeader confidence="0.866722">
5.4.3 Global Combined WSD
</subsectionHeader>
<bodyText confidence="0.9991235">
In this section we present the results of the global
combined WSD system. All the combined ex-
perimental conditions have the same percentage
coverage.13 We present the results combining us-
ing MERGE and using VOTE. We have chosen
4 baseline systems: (1) SM07; (2) the our base-
line monolingual system using JCN for verb-verb
comparisons (RelCont-BL), so as to distinguish
the level of improvement that could be attributed
to the multilingual system in the combination re-
sults; as well as (3) and (4) our best individual sys-
tem results from RelCont (ExpandL SemCor)
referred to in the tables below as (RelCont-Final)
and TransCont using the best experimental con-
dition (Lesk3 JCN). Table 5 and 6 illustrates the
overall performance of our combined approach.
In Table 5 we note that the combined conditions
outperform the two base systems independently,
using TransCont is always helpful for any of the
3 monolingual systems, no matter we use VOTE or
MERGE. In general the trend is that VOTE outper-
forms MERGE, however they exhibit different be-
haviors with respect to what works for each POS.
In Table 6 the combined result is not always
better than the corresponding monolingual sys-
tem. When applying to our baseline monolin-
13We do not back off in any of our systems to a default
sense, hence the coverage is not at a 100%.
gual system, the combined result is still bet-
ter. However, we observed worse results for Ex-
pandL Semcor, RelCont-Final. There may be 2
main reasons for the loss: (1) SV3 is the tuning
set in SM07, and we inherit the thresholds for
similarity metrics from that study. Accordingly,
an overfitting of the thresholds is probably hap-
pening in this case; (2) TransCont results are
not good enough on the SV3AW data. Compar-
ing the RelCont and TransCont system re-
sults, we find a drop in f-measure of −1.37%
in SV2AW, in contrast to a much larger drop in
performance for the SV3AW data set where the
drop in performance is −6.38% when comparing
RelCont-BL to TransCont and nearly −10%
comparing against RelCont-Final.
</bodyText>
<sectionHeader confidence="0.999768" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999827909090909">
We looked closely at the data in the combined con-
ditions attempting to get a feel for the data and
understand what was captured and what was not.
Some of the good examples that are captured in the
combined system that are not tagged in RelCont
is the case of ringer in Like most of the other 6,000
churches in Britain with sets of bells, St. Michael
once had its own “ band ” of ringers, who would
herald every Sunday morning and evening service
.. The RelCont answer is ringer sense number 4:
(horseshoes) the successful throw of a horseshoe
</bodyText>
<page confidence="0.968275">
1548
</page>
<table confidence="0.999702625">
Condition N V A R Global F Measure
RAND 43.7 21 41.2 57.4 39.9
DR02-FRGL 54.5
SALAAM 65.48 31.77 56.87 67.4 57.23
Lesk2 67.05 30 59.69 68.01 57.27
Lesk3 67.15 30 60.2 68.01 57.41
Lesk3 Lin 67.15 29.27 60.2 68.01 57.61
Lesk3 JCN 67.15 33.88 60.2 68.01 58.35
</table>
<tableCaption confidence="0.991211">
Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
</tableCaption>
<table confidence="0.999874571428571">
Condition N V A R Global F Measure
RAND 39.67 19.34 41.85 92.31 32.93
SALAAM 52.42 29.27 54.14 88.89 45.63
Lesk2 53.57 33.58 53.63 88.89 47
Lesk3 53.77 33.30 56.48 88.89 47.5
Lesk3 Lin 53.77 29.24 56.48 88.89 46.37
Lesk3 JCN 53.77 38.43 56.48 88.89 49.29
</table>
<tableCaption confidence="0.999666">
Table 4: TransCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
</tableCaption>
<bodyText confidence="0.999309423076923">
or quoit so as to encircle a stake or peg. When
the merged system is employed we see the cor-
rect sense being chosen as sense number 1 in the
MERGE condition: defined in WN as a person
who rings church bells (as for summoning the con-
gregation) resulting from a corresponding transla-
tion into French as sonneur.
We did some basic data analysis on the items
we are incapable of capturing. Several of them
are cases of metonymy in examples such as ”the
English are known...”, the sense of English here
is clearly in reference to the people of England,
however, our WSD system preferred the language
sense of the word. These cases are not gotten by
any of our systems. If it had access to syntac-
tic/semantic roles we assume it could capture that
this sense of the word entails volition for example.
Other types of errors resulted from the lack of a
way to explicitly identify multiwords.
Looking at the performance of TransCont we
note that much of the loss is a result of the lack of
variability in the translations which is a key factor
in the performance of the algorithm. For example
for the 157 adjective target test words in SV2AW,
there was a single word alignment for 51 of the
cases, losing any tagging for these words.
</bodyText>
<sectionHeader confidence="0.988581" genericHeader="conclusions">
7 Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.999549714285714">
In this paper we present a framework that com-
bines orthogonal sources of evidence to create a
state-of-the-art system for the task of WSD disam-
biguation for AW. Our approach yields an over-
all global F measure of 64.58 for the standard
SV2AW data set combining monolingual and mul-
tilingual evidence. The approach can be fur-
ther refined by adding other types of orthogo-
nal features such as syntactic features and seman-
tic role label features. Adding SemCor exam-
ples to TransCont should have a positive im-
pact on performance. Also adding more languages
as illustrated by the DR02 work should also yield
much better performance.
</bodyText>
<sectionHeader confidence="0.998441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995728222222222">
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61–72, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 33–40, Prague, Czech Republic,
June. Association for Computational Linguistics.
Mona Diab and Philip Resnik. 2002. An unsuper-
vised method for word sense tagging using parallel
corpora. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
</reference>
<page confidence="0.991386">
1549
</page>
<table confidence="0.999617666666667">
Condition N V A R Global F Measure
SM07 68.7 33.01 65.2 63.1 59.2
RelCont-BL 68.7 35.46 65.2 63.1 59.72
RelCont-Final 69.0 38.66 68.8 69.45 62.13
TransCont 67.15 33.88 60.2 68.01 58.35
MERGE: RelCont-BL+TransCont 69.3 36.91 66.7 64.45 60.82
VOTE: RelCont-BL+TransCont 71 37.71 66.5 66.1 61.92
MERGE: RelCont-Final+TransCont 70.7 38.66 69.5 70.45 63.14
VOTE: RelCont-Final+TransCont 74.2 38.26 68.6 71.45 64.58
</table>
<tableCaption confidence="0.993036">
Table 5: F-measure % for all Combined experimental conditions on SV2AW
</tableCaption>
<table confidence="0.999731333333333">
Condition N V A R Global F Measure
SM07 60.9 43.4 57 92.88 53.98
RelCont-BL 60.9 48.5 57 92.88 55.87
RelCont-Final 65 49.2 65.55 92.88 59.52
TransCont 53.77 38.43 56.48 88.89 49.29
MERGE: RelCont-BL+TransCont 60.6 49.5 58.85 92.88 56.47
VOTE: RelCont-BL+TransCont 59.3 49.5 59.1 92.88 55.92
MERGE: RelCont-Final+TransCont 63.2 50.3 65.25 92.88 59.07
VOTE: RelCont-Final+TransCont 62.4 49.65 65.25 92.88 58.47
</table>
<tableCaption confidence="0.992294">
Table 6: F-measure % for all Combined experimental conditions on SV3AW
</tableCaption>
<reference confidence="0.999630714285714">
pages 255–262, Philadelphia, Pennsylvania, USA,
July. Association for Computational Linguistics.
Christiane Fellbaum. 1998. ”wordnet: An electronic
lexical database”. MIT Press.
Weiwei Guo and Mona Diab. 2009. Improvements to
monolingual english word sense disambiguation. In
Proceedings of the Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions
(SEW-2009), pages 64–69, Boulder, Colorado, June.
Association for Computational Linguistics.
N. Ide and J. Veronis. 1998. Word sense disambigua-
tion: The state of the art. In Computational Linguis-
tics, pages 1–40, 24:1.
J. Jiang and D. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, Taiwan.
C. Leacock and M. Chodorow. 1998. Combining lo-
cal context and wordnet sense similarity for word
sense identification. In WordNet, An Electronic Lex-
ical Database. The MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In In Proceedings of
the SIGDOC Conference, Toronto, June.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 411–418, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
George A. Miller. 1990. Wordnet: a lexical database
for english. In Communications of the ACM, pages
39–41.
Roberto Navigli and Mirella Lapata. 2007. Graph
connectivity measures for unsupervised word sense
disambiguation. In Proceedings of the 201h Inter-
national Joint Conference on Artificial Intelligence
(IJCAI), pages 1683–1688, Hyderabad, India.
Roberto Navigli. 2009. Word sense disambiguation:
a survey. In ACM Computing Surveys, pages 1–69.
ACM Press.
Franz Joseph Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, , and
H. Dang. 2001. English tasks: all-words and verb
lexical sample. In In Proceedings of ACL/SIGLEX
Senseval-2, Toulouse, France, June.
Ted Pedersen, Satanjeev Banerjee, and Siddharth Pat-
wardhan. 2005. Maximizing semantic relatedness
to perform word sense disambiguation. In Univer-
sity of Minnesota Supercomputing Institute Research
Report UMSI 2005/25, Minnesotta, March.
</reference>
<page confidence="0.783625">
1550
</page>
<reference confidence="0.999591631578947">
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, srl and all words. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 87–92,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-based word sense disambiguation using mea-
sures of word semantic similarity. In Proceedings
of the IEEE International Conference on Semantic
Computing (ICSC 2007), Irvine, CA.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Rada Mihalcea and Phil
Edmonds, editors, Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, pages 41–43, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.994053">
1551
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.979558">
<title confidence="0.998421">Combining Orthogonal Monolingual and Multilingual Sources of Evidence for All Words WSD</title>
<author confidence="0.999634">Weiwei Guo</author>
<affiliation confidence="0.999953">Computer Science Department Columbia University</affiliation>
<address confidence="0.999519">New York, NY, 10115</address>
<email confidence="0.998857">weiwei@cs.columbia.edu</email>
<author confidence="0.999419">Mona Diab</author>
<affiliation confidence="0.9998535">Center for Computational Learning Systems Columbia University</affiliation>
<address confidence="0.999032">New York, NY, 10115</address>
<email confidence="0.999662">mdiab@ccls.columbia.edu</email>
<abstract confidence="0.999416739130435">Word Sense Disambiguation remains one of the most complex problems facing computational linguists to date. In this paper we present a system that combines evidence from a monolingual WSD system together with that from a multilingual WSD system to yield state of the art performance on standard All-Words data sets. The monolingual system is based on a modification of the graph based state of the art algorithm In-Degree. The multilingual system is an improvement over an All- Words unsupervised approach, SALAAM. SALAAM exploits multilingual evidence as a means of disambiguation. In this paper, we present modifications to both of the original approaches and then their combination. We finally report the highest results obtained to date on the SENSEVAL 2 standard data set using an unsupervised method, we achieve an overall F measure of 64.58 using a voting scheme.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>61--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1707" citStr="Carpuat and Wu, 2007" startWordPosition="262" endWordPosition="265">t using an unsupervised method, we achieve an overall F measure of 64.58 using a voting scheme. 1 Introduction Despite advances in natural language processing (NLP), Word Sense Disambiguation (WSD) is still considered one of the most challenging problems in the field. Ever since the field’s inception, WSD has been perceived as one of the central problems in NLP. WSD is viewed as an enabling technology that could potentially have far reaching impact on NLP applications in general. We are starting to see the beginnings of a positive effect of WSD in NLP applications such as Machine Translation (Carpuat and Wu, 2007; Chan et al., 2007). Advances in WSD research in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as WordNets (Fellbaum, 1998; Miller, 1990), the availability of large scale corpora, the existence and dissemination of standardized data sets over the past 10 years through different testbeds such as SENSEVAL and SEMEVAL competitions,1 devising more robust computing algorithms to handle large scale data sets, and simply advancement in hardware machinery. In this paper, we address the problem of WSD of all conten</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 61–72, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>33--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1727" citStr="Chan et al., 2007" startWordPosition="266" endWordPosition="269">d method, we achieve an overall F measure of 64.58 using a voting scheme. 1 Introduction Despite advances in natural language processing (NLP), Word Sense Disambiguation (WSD) is still considered one of the most challenging problems in the field. Ever since the field’s inception, WSD has been perceived as one of the central problems in NLP. WSD is viewed as an enabling technology that could potentially have far reaching impact on NLP applications in general. We are starting to see the beginnings of a positive effect of WSD in NLP applications such as Machine Translation (Carpuat and Wu, 2007; Chan et al., 2007). Advances in WSD research in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as WordNets (Fellbaum, 1998; Miller, 1990), the availability of large scale corpora, the existence and dissemination of standardized data sets over the past 10 years through different testbeds such as SENSEVAL and SEMEVAL competitions,1 devising more robust computing algorithms to handle large scale data sets, and simply advancement in hardware machinery. In this paper, we address the problem of WSD of all content words in a sentenc</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 33–40, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>255--262</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="2989" citStr="Diab and Resnik, 2002" startWordPosition="466" endWordPosition="469">e task is to associate all tokens with their contextually relevant meaning definitions from some computational lexical resource. Our work hinges upon combining two high quality WSD systems that rely on essentially different sources of evidence. The two WSD systems are a monolingual system RelCont and a multilingual system TransCont. RelCont is an enhancement on an existing graph based algorithm, In-Degree, first described in (Navigli and Lapata, 2007). TransCont is an enhancement over an existing approach that leverages multilingual evidence through projection, SALAAM, described in detail in (Diab and Resnik, 2002). Similar to the leveraged systems, the current combined approach is unsupervised, namely it does not rely on training data from the onset. We show that by combining both sources of evidence, our approach yields the highest performance for an unsupervised system to date on standard All-Words data sets. This paper is organized as follows: Section 2 delves into the problem of WSD in more detail; Section 3 explores some of the relevant related work; in Section 4, we describe the two WSD systems in some detail emphasizing the improvements to the basic systems in addition to a description of our co</context>
<context position="14406" citStr="Diab and Resnik, 2002" startWordPosition="2379" endWordPosition="2382"> the appropriate version sense annotations. We augment the sense entries for the different POS WN databases with example usages from SemCor. The augmentation is done as a look up table external to WN proper since we did not want to dabble with the WN offsets. We set a cap of 30 additional examples per synset. We used the first 30 examples with no filtering criteria. Many of the synsets had no additional examples. WN1.7.1 comprises a total of 26875 synsets, of which 25940 synsets are augmented with SemCor examples.7 4.2 Multilingual System TransCont TransCont is based on the WSD system SALAAM (Diab and Resnik, 2002), henceforth (DR02). The SALAAM system leverages word alignments from parallel corpora to perform WSD. The SALAAM algorithm exploits the word correspondence cross linguistically to tag word senses on words in running text. It relies on several underlying assumptions. The first assumption is that senses of polysemous words in one language could be lexicalized differently in other languages. For example, ‘bank’ in English would be translated as banque or rive de fleuve in French, depending on context. The other assumption is that if Language 1 (L1) words are translated to the same orthographic f</context>
<context position="21349" citStr="Diab and Resnik, 2002" startWordPosition="3537" endWordPosition="3540">airings. 4.3.2 VOTE In this combination scheme, the output of the global disambiguation system is simply an intersection of the two outputs from the two underlying systems RelCont and TransCont. Specifically, we sum up the confidence ranging from 0 to 1 of the two system In-Degree algorithm outputs to obtain a final confidence for each sense, choosing the sense(s) that yields the highest confidences. The fact that TransCont uses In-Degree internally allows for a seamless integration. 5 Experiments and Results 5.1 Data The parallel data we experiment with are the same standard data sets as in (Diab and Resnik, 2002), namely, Senseval 2 English AW data sets (SV2AW) (Palmer et al., 2001), and Seneval 3 English AW (SV3AW) data set. We use the true POS tag sets in the test data as rendered in the Penn Tree Bank.10 We present our results on WordNet 1.7.1 for ease of comparison with previous results. 5.2 Evaluation Metrics We use the scorer2 software to report finegrained (P)recision and (R)ecall and (F)-measure. 5.3 Baselines We consider here several baselines. 1. A random baseline (RAND) is the most appropriate baseline for an unsupervised approach.2. We include the most frequent sense baseline (MFBL), thoug</context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 255–262, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>wordnet: An electronic lexical database”.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1918" citStr="Fellbaum, 1998" startWordPosition="298" endWordPosition="299"> one of the most challenging problems in the field. Ever since the field’s inception, WSD has been perceived as one of the central problems in NLP. WSD is viewed as an enabling technology that could potentially have far reaching impact on NLP applications in general. We are starting to see the beginnings of a positive effect of WSD in NLP applications such as Machine Translation (Carpuat and Wu, 2007; Chan et al., 2007). Advances in WSD research in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as WordNets (Fellbaum, 1998; Miller, 1990), the availability of large scale corpora, the existence and dissemination of standardized data sets over the past 10 years through different testbeds such as SENSEVAL and SEMEVAL competitions,1 devising more robust computing algorithms to handle large scale data sets, and simply advancement in hardware machinery. In this paper, we address the problem of WSD of all content words in a sentence, All-Words data. In this framework, the task is to associate all tokens with their contextually relevant meaning definitions from some computational lexical resource. Our work hinges upon c</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. ”wordnet: An electronic lexical database”. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Improvements to monolingual english word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009),</booktitle>
<pages>64--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="9408" citStr="Guo and Diab, 2009" startWordPosition="1528" endWordPosition="1531"> similarity measure within verbs, and the Lesk (Lesk, 1986) similarity measure within adjectives, within adverbs, and among different POS tag pairings. They evaluate their work against the SENSEVAL 2 AW test data (SV2AW). They tune the parameters of their algorithm – namely, the normalization ratio for some of these measures – on the SENSEVAL 3 data set. They report a state-ofthe-art unsupervised system that yields an overall performance across all AW POS sets of 57.2%. In our current work, we extend the SM07 work in some interesting ways. A detailed narrative of our approach is described in (Guo and Diab, 2009). Briefly, we focus on the In-Degree graph based algorithm since it is the best performer in the SM07 work. The In-Degree algorithm presents the problem as a weighted graph with senses as nodes and the similarity between senses as weights on edges. The In-Degree of a vertex refers to the number of edges incident on that vertex. In the weighted graph, the In-Degree for each vertex is calculated by summing the weights on the edges that are incident on it. After all the In-Degree values for each sense are computed, the sense with maximum value is chosen as the final sense for that word. In this p</context>
</contexts>
<marker>Guo, Diab, 2009</marker>
<rawString>Weiwei Guo and Mona Diab. 2009. Improvements to monolingual english word sense disambiguation. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009), pages 64–69, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J Veronis</author>
</authors>
<title>Word sense disambiguation: The state of the art.</title>
<date>1998</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>1--40</pages>
<contexts>
<context position="5672" citStr="Ide and Veronis, 1998" startWordPosition="908" endWordPosition="911">and the adverb ‘there’, be disambiguated from a standard lexical resource. Hence, using WordNet,2 ‘walked’ will be assigned the corresponding meaning definitions of: to use one’s feet to advance; to advance by steps, ‘saw’ will be assigned the meaning definition of: to perceive by sight or have the power to perceive by sight, the noun ‘bank’ will be assigned the meaning definition of: sloping land especially the slope beside a body of water, and so on. 3 Related Works Many systems over the years have been proposed for the task. A thorough review of the state of the art through the late 1990s (Ide and Veronis, 1998) and more recently in (Navigli, 2009). Several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning techniques. To date, the best approaches that solve the AW WSD task are supervised as illustrated in the different SenseEval and SEMEVAL AW task (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised combination approach to the AW WSD problem that 2http://wordnet.princeton.edu relies on WN similarity measures in conjunction with evidence obtained </context>
</contexts>
<marker>Ide, Veronis, 1998</marker>
<rawString>N. Ide and J. Veronis. 1998. Word sense disambiguation: The state of the art. In Computational Linguistics, pages 1–40, 24:1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>D Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics,</booktitle>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. Jiang and D. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and wordnet sense similarity for word sense identification. In WordNet, An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="8789" citStr="Leacock and Chodorow, 1998" startWordPosition="1423" endWordPosition="1426">d suggests they should have influence on each other, accordingly a maximum 3We acknowledge the existence of many research papers that tackled the AW WSD problem using unsupervised approaches, yet for lack of space we will not be able to review most of them. 1543 allowable distance is set. They explore 4 different graph based algorithms. The highest yielding algorithm in their work is the In-Degree algorithm combining different WN similarity measures depending on POS. They used the Jiang and Conrath (JCN) (Jiang and Conrath., 1997) similarity measure within nouns, the Leacock &amp; Chodorow (LCH) (Leacock and Chodorow, 1998) similarity measure within verbs, and the Lesk (Lesk, 1986) similarity measure within adjectives, within adverbs, and among different POS tag pairings. They evaluate their work against the SENSEVAL 2 AW test data (SV2AW). They tune the parameters of their algorithm – namely, the normalization ratio for some of these measures – on the SENSEVAL 3 data set. They report a state-ofthe-art unsupervised system that yields an overall performance across all AW POS sets of 57.2%. In our current work, we extend the SM07 work in some interesting ways. A detailed narrative of our approach is described in (</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>C. Leacock and M. Chodorow. 1998. Combining local context and wordnet sense similarity for word sense identification. In WordNet, An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In</title>
<date>1986</date>
<booktitle>In Proceedings of the SIGDOC Conference,</booktitle>
<location>Toronto,</location>
<contexts>
<context position="8848" citStr="Lesk, 1986" startWordPosition="1434" endWordPosition="1435">3We acknowledge the existence of many research papers that tackled the AW WSD problem using unsupervised approaches, yet for lack of space we will not be able to review most of them. 1543 allowable distance is set. They explore 4 different graph based algorithms. The highest yielding algorithm in their work is the In-Degree algorithm combining different WN similarity measures depending on POS. They used the Jiang and Conrath (JCN) (Jiang and Conrath., 1997) similarity measure within nouns, the Leacock &amp; Chodorow (LCH) (Leacock and Chodorow, 1998) similarity measure within verbs, and the Lesk (Lesk, 1986) similarity measure within adjectives, within adverbs, and among different POS tag pairings. They evaluate their work against the SENSEVAL 2 AW test data (SV2AW). They tune the parameters of their algorithm – namely, the normalization ratio for some of these measures – on the SENSEVAL 3 data set. They report a state-ofthe-art unsupervised system that yields an overall performance across all AW POS sets of 57.2%. In our current work, we extend the SM07 work in some interesting ways. A detailed narrative of our approach is described in (Guo and Diab, 2009). Briefly, we focus on the In-Degree gra</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In In Proceedings of the SIGDOC Conference, Toronto, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>411--418</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="7860" citStr="Mihalcea, 2005" startWordPosition="1263" endWordPosition="1265"> the words that are mapped to (aligned with) the same orthographic form in a foreign language constitute the context. In the next subsections we describe the two approaches RelCont and TransCont in some detail, then we proceed to describe two combination methods for the two approaches: MERGE and VOTE. 4.1 Monolingual System RelCont RelCont is based on an extension of a stateof-the-art WSD approach by (Sinha and Mihalcea, 2007), henceforth (SM07). In the basic SM07 work, the authors combine different semantic similarity measures with different graph based algorithms as an extension to work in (Mihalcea, 2005). Given a sequence of words W = {fw1, w2 ... w�I, each word wi with several senses lsi1, si2...si.I. A graph G = (V,E) is defined such that there exists a vertex v for each sense. Two senses of two different words may be connected by an edge e, depending on their distance. That two senses are connected suggests they should have influence on each other, accordingly a maximum 3We acknowledge the existence of many research papers that tackled the AW WSD problem using unsupervised approaches, yet for lack of space we will not be able to review most of them. 1543 allowable distance is set. They exp</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>Rada Mihalcea. 2005. Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 411–418, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1990</date>
<booktitle>In Communications of the ACM,</booktitle>
<pages>39--41</pages>
<contexts>
<context position="1933" citStr="Miller, 1990" startWordPosition="300" endWordPosition="301"> challenging problems in the field. Ever since the field’s inception, WSD has been perceived as one of the central problems in NLP. WSD is viewed as an enabling technology that could potentially have far reaching impact on NLP applications in general. We are starting to see the beginnings of a positive effect of WSD in NLP applications such as Machine Translation (Carpuat and Wu, 2007; Chan et al., 2007). Advances in WSD research in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as WordNets (Fellbaum, 1998; Miller, 1990), the availability of large scale corpora, the existence and dissemination of standardized data sets over the past 10 years through different testbeds such as SENSEVAL and SEMEVAL competitions,1 devising more robust computing algorithms to handle large scale data sets, and simply advancement in hardware machinery. In this paper, we address the problem of WSD of all content words in a sentence, All-Words data. In this framework, the task is to associate all tokens with their contextually relevant meaning definitions from some computational lexical resource. Our work hinges upon combining two hi</context>
<context position="13654" citStr="Miller, 1990" startWordPosition="2249" endWordPosition="2250">h the contextual sense and the target sense and we found that the unreliability of some of the relations is detrimental to the algorithm’s performance. Hence we decided empirically to expand only the contextual word. 1544 SemCor Expansion of WN A part of the RelCont approach relies on using the Lesk algorithm. Accordingly, the availability of glosses associated with the WN entries is extremely beneficial. Therefore, we expand the number of glosses available in WN by using the SemCor data set, thereby adding more examples to compare. The SemCor corpus is a corpus that is manually sense tagged (Miller, 1990).6 In this expansion, depending on the version of WN, we use the sense-index file in the WN Database to convert the SemCor data to the appropriate version sense annotations. We augment the sense entries for the different POS WN databases with example usages from SemCor. The augmentation is done as a look up table external to WN proper since we did not want to dabble with the WN offsets. We set a cap of 30 additional examples per synset. We used the first 30 examples with no filtering criteria. Many of the synsets had no additional examples. WN1.7.1 comprises a total of 26875 synsets, of which </context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George A. Miller. 1990. Wordnet: a lexical database for english. In Communications of the ACM, pages 39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>Graph connectivity measures for unsupervised word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 201h International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>1683--1688</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="2822" citStr="Navigli and Lapata, 2007" startWordPosition="442" endWordPosition="445">sets, and simply advancement in hardware machinery. In this paper, we address the problem of WSD of all content words in a sentence, All-Words data. In this framework, the task is to associate all tokens with their contextually relevant meaning definitions from some computational lexical resource. Our work hinges upon combining two high quality WSD systems that rely on essentially different sources of evidence. The two WSD systems are a monolingual system RelCont and a multilingual system TransCont. RelCont is an enhancement on an existing graph based algorithm, In-Degree, first described in (Navigli and Lapata, 2007). TransCont is an enhancement over an existing approach that leverages multilingual evidence through projection, SALAAM, described in detail in (Diab and Resnik, 2002). Similar to the leveraged systems, the current combined approach is unsupervised, namely it does not rely on training data from the onset. We show that by combining both sources of evidence, our approach yields the highest performance for an unsupervised system to date on standard All-Words data sets. This paper is organized as follows: Section 2 delves into the problem of WSD in more detail; Section 3 explores some of the relev</context>
</contexts>
<marker>Navigli, Lapata, 2007</marker>
<rawString>Roberto Navigli and Mirella Lapata. 2007. Graph connectivity measures for unsupervised word sense disambiguation. In Proceedings of the 201h International Joint Conference on Artificial Intelligence (IJCAI), pages 1683–1688, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: a survey. In</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<pages>1--69</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="5709" citStr="Navigli, 2009" startWordPosition="916" endWordPosition="917"> a standard lexical resource. Hence, using WordNet,2 ‘walked’ will be assigned the corresponding meaning definitions of: to use one’s feet to advance; to advance by steps, ‘saw’ will be assigned the meaning definition of: to perceive by sight or have the power to perceive by sight, the noun ‘bank’ will be assigned the meaning definition of: sloping land especially the slope beside a body of water, and so on. 3 Related Works Many systems over the years have been proposed for the task. A thorough review of the state of the art through the late 1990s (Ide and Veronis, 1998) and more recently in (Navigli, 2009). Several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning techniques. To date, the best approaches that solve the AW WSD task are supervised as illustrated in the different SenseEval and SEMEVAL AW task (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised combination approach to the AW WSD problem that 2http://wordnet.princeton.edu relies on WN similarity measures in conjunction with evidence obtained through exploiting multilingual evide</context>
<context position="25844" citStr="Navigli, 2009" startWordPosition="4314" endWordPosition="4315">adverbs directly exploited the use of SemCor augmented examples. Combining SemCor and ExpandL seems to have a positive impact on the verbs and adverbs, but not on the nouns and adjectives. These trends are not held consistently across data sets. For example, we see that SemCor augmentation helps all POS tag sets over using ExpandL alone or even when combined with SemCor. We note the similar trends in performance for the SV3AW data. Compared to state of the art systems, RelCont with an overall F-measure performance of 62.13% outperforms the best unsupervised system of 57.5% UNED-AW-U2 for SV2 (Navigli, 2009). It is worth noting that it is higher than several of the supervised systems. Moreover, RelCont yields better overall results on SV3 at 59.87 compared to the best unsupervised system IRST-DDD-U which yielded an F-measure of 58.3% (Navigli, 2009). 5.4.2 TransCont For the TransCont results we illustrate the original SALAAM results as our baseline. Similar to the DR02 work, we actually use the same SALAAM parallel corpora comprising more than 5.5M English tokens translated using a single machine translation system GlobalLink. Therefore our parallel corpus is the French English translation condit</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: a survey. In ACM Computing Surveys, pages 1–69. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="17462" citStr="Och and Ney, 2003" startWordPosition="2891" endWordPosition="2894">dverbs, and is very shallow for verbs. Essentially SALAAM relies on variability in translation as it is important to have multiple words in a typeset to allow for disambiguation. In the original SALAAM system, the authors automatically translated several balanced corpora in order to render more variable data for the approach to show it’s impact. The corpora that were translated are: the WSJ, the Brown corpus and all the SENSEVAL data. The data were translated to different languages (Arabic, French and Spanish) using state of art MT systems. They employed the automatic alignment system GIZA++ (Och and Ney, 2003) to obtain word alignments in a single direction from L1 to L2. For TransCont we use the basic SALAAM approach with some crucial modifications that lead to better performance. We still rely on parallel corpora, we extract typesets based on the intersection of word alignments in both alignment directions using more advanced GIZA++ machinery. In contrast to DR02, we experiment with all four POS: Verbs (V), Nouns (N), Adjectives (A) and Adverbs (R). Moreover, we modified the underlying disambiguation method on the typesets. We still employ WN similarity, however, we do not use the NounGroupings a</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Joseph Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>C Fellbaum</author>
<author>S Cotton</author>
<author>L Delfs</author>
</authors>
<title>English tasks: all-words and verb lexical sample. In</title>
<date>2001</date>
<booktitle>In Proceedings of ACL/SIGLEX Senseval-2,</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="6031" citStr="Palmer et al., 2001" startWordPosition="968" endWordPosition="971">he meaning definition of: sloping land especially the slope beside a body of water, and so on. 3 Related Works Many systems over the years have been proposed for the task. A thorough review of the state of the art through the late 1990s (Ide and Veronis, 1998) and more recently in (Navigli, 2009). Several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning techniques. To date, the best approaches that solve the AW WSD task are supervised as illustrated in the different SenseEval and SEMEVAL AW task (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised combination approach to the AW WSD problem that 2http://wordnet.princeton.edu relies on WN similarity measures in conjunction with evidence obtained through exploiting multilingual evidence. We will review the closely relevant related work on which this current investigation is based.3 4 Our Approach Our current investigation exploits two basic unsupervised approaches that perform at state-of-theart for the AW WSD task in an unsupervised setting. Crucially the two systems rely on different sources of ev</context>
<context position="21420" citStr="Palmer et al., 2001" startWordPosition="3549" endWordPosition="3552">disambiguation system is simply an intersection of the two outputs from the two underlying systems RelCont and TransCont. Specifically, we sum up the confidence ranging from 0 to 1 of the two system In-Degree algorithm outputs to obtain a final confidence for each sense, choosing the sense(s) that yields the highest confidences. The fact that TransCont uses In-Degree internally allows for a seamless integration. 5 Experiments and Results 5.1 Data The parallel data we experiment with are the same standard data sets as in (Diab and Resnik, 2002), namely, Senseval 2 English AW data sets (SV2AW) (Palmer et al., 2001), and Seneval 3 English AW (SV3AW) data set. We use the true POS tag sets in the test data as rendered in the Penn Tree Bank.10 We present our results on WordNet 1.7.1 for ease of comparison with previous results. 5.2 Evaluation Metrics We use the scorer2 software to report finegrained (P)recision and (R)ecall and (F)-measure. 5.3 Baselines We consider here several baselines. 1. A random baseline (RAND) is the most appropriate baseline for an unsupervised approach.2. We include the most frequent sense baseline (MFBL), though we note that we consider the most frequent sense or first sense basel</context>
</contexts>
<marker>Palmer, Fellbaum, Cotton, Delfs, 2001</marker>
<rawString>M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, , and H. Dang. 2001. English tasks: all-words and verb lexical sample. In In Proceedings of ACL/SIGLEX Senseval-2, Toulouse, France, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Satanjeev Banerjee</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Maximizing semantic relatedness to perform word sense disambiguation.</title>
<date>2005</date>
<tech>Report UMSI 2005/25,</tech>
<institution>In University of Minnesota Supercomputing Institute Research</institution>
<location>Minnesotta,</location>
<contexts>
<context position="10692" citStr="Pedersen et al., 2005" startWordPosition="1744" endWordPosition="1748">ications to the basic similarity measures exploited and the WN lexical resource tapped into. Similar to the original In-Degree algorithm, we produce a probabilistic ranked list of senses. Our modifications are described as follows: JCN for Verb-Verb Similarity In our implementation of the In-Degree algorithm, we use the JCN similarity measure for both Noun-Noun similarity calculation similar to SM07. However, different from SM07, instead of using LCH for Verb-Verb similarity, we use the JCN metric as it yields better performance in our experimentations. Expand Lesk Following the intuition in (Pedersen et al., 2005), henceforth (PEA05), we expand the basic Lesk similarity measure to take into account the glosses for all the relations for the synsets on the contextual words and compare them with the glosses of the target word senses, therefore going beyond the is-a relation. We exploit the observation that WN senses are too fine-grained, accordingly the neighbors would be slightly varied while sharing significant semantic meaning content. To find similar senses, we use the relations: hypernym, hyponym, similar attributes, similar verb group, pertinym, holonym, and meronyms.4 The algorithm assumes that the</context>
</contexts>
<marker>Pedersen, Banerjee, Patwardhan, 2005</marker>
<rawString>Ted Pedersen, Satanjeev Banerjee, and Siddharth Patwardhan. 2005. Maximizing semantic relatedness to perform word sense disambiguation. In University of Minnesota Supercomputing Institute Research Report UMSI 2005/25, Minnesotta, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval-2007 task-17: English lexical sample, srl and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>87--92</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6079" citStr="Pradhan et al., 2007" startWordPosition="976" endWordPosition="979">lly the slope beside a body of water, and so on. 3 Related Works Many systems over the years have been proposed for the task. A thorough review of the state of the art through the late 1990s (Ide and Veronis, 1998) and more recently in (Navigli, 2009). Several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning techniques. To date, the best approaches that solve the AW WSD task are supervised as illustrated in the different SenseEval and SEMEVAL AW task (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised combination approach to the AW WSD problem that 2http://wordnet.princeton.edu relies on WN similarity measures in conjunction with evidence obtained through exploiting multilingual evidence. We will review the closely relevant related work on which this current investigation is based.3 4 Our Approach Our current investigation exploits two basic unsupervised approaches that perform at state-of-theart for the AW WSD task in an unsupervised setting. Crucially the two systems rely on different sources of evidence allowing them to complement each other to</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007. Semeval-2007 task-17: English lexical sample, srl and all words. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 87–92, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Sinha</author>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised graph-based word sense disambiguation using measures of word semantic similarity.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Conference on Semantic Computing (ICSC</booktitle>
<location>Irvine, CA.</location>
<contexts>
<context position="7675" citStr="Sinha and Mihalcea, 2007" startWordPosition="1232" endWordPosition="1236">Cont defines the notion of context in the translational space using a foreign word as a filter for defining the contextual content words for a given target word. In this multilingual setting, all the words that are mapped to (aligned with) the same orthographic form in a foreign language constitute the context. In the next subsections we describe the two approaches RelCont and TransCont in some detail, then we proceed to describe two combination methods for the two approaches: MERGE and VOTE. 4.1 Monolingual System RelCont RelCont is based on an extension of a stateof-the-art WSD approach by (Sinha and Mihalcea, 2007), henceforth (SM07). In the basic SM07 work, the authors combine different semantic similarity measures with different graph based algorithms as an extension to work in (Mihalcea, 2005). Given a sequence of words W = {fw1, w2 ... w�I, each word wi with several senses lsi1, si2...si.I. A graph G = (V,E) is defined such that there exists a vertex v for each sense. Two senses of two different words may be connected by an edge e, depending on their distance. That two senses are connected suggests they should have influence on each other, accordingly a maximum 3We acknowledge the existence of many </context>
<context position="23919" citStr="Sinha and Mihalcea, 2007" startWordPosition="3971" endWordPosition="3974">es. We break the results down by POS tag (N)oun, (V)erb, (A)djective, and Adve(R)b. The coverage for SV2AW is 98.17% losing some of the verb and adverb target words. Our overall results on all the data sets clearly outperform the baseline as well as state-of-theart performance using an unsupervised system (SM07) in overall f-measure across all the data sets. We are unable to beat the most frequent baseline (MFBL) which is obtained using the first sense. However MFBL is a supervised baseline and our approach is unsupervised. Our implementation of SM07 is slightly higher than those reported in (Sinha and Mihalcea, 2007) (57.12% ) is probably due to the fact that we do not consider the items tagged as ”U” and also we resolve some of the POS tag mismatches between the gold set and the test data. We note that for the SV2AW data set our coverage is not 100% due to some POS tag mismatches that could not have been resolved automatically. These POS tag problems have to do mainly with multiword expressions. In observing the performance of the overall RelCont, we note that using JCN for verbs clearly outperforms using the LCH similarity measure. Using SemCor to augment WN examples seems to have the biggest impact. Co</context>
</contexts>
<marker>Sinha, Mihalcea, 2007</marker>
<rawString>Ravi Sinha and Rada Mihalcea. 2007. Unsupervised graph-based word sense disambiguation using measures of word semantic similarity. In Proceedings of the IEEE International Conference on Semantic Computing (ICSC 2007), Irvine, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The english all-words task.</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>41--43</pages>
<editor>In Rada Mihalcea and Phil Edmonds, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="6056" citStr="Snyder and Palmer, 2004" startWordPosition="972" endWordPosition="975"> of: sloping land especially the slope beside a body of water, and so on. 3 Related Works Many systems over the years have been proposed for the task. A thorough review of the state of the art through the late 1990s (Ide and Veronis, 1998) and more recently in (Navigli, 2009). Several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning techniques. To date, the best approaches that solve the AW WSD task are supervised as illustrated in the different SenseEval and SEMEVAL AW task (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised combination approach to the AW WSD problem that 2http://wordnet.princeton.edu relies on WN similarity measures in conjunction with evidence obtained through exploiting multilingual evidence. We will review the closely relevant related work on which this current investigation is based.3 4 Our Approach Our current investigation exploits two basic unsupervised approaches that perform at state-of-theart for the AW WSD task in an unsupervised setting. Crucially the two systems rely on different sources of evidence allowing them to c</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The english all-words task. In Rada Mihalcea and Phil Edmonds, editors, Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 41–43, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>