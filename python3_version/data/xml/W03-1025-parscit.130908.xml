<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995943">
A Maximum Entropy Chinese Character-Based Parser
</title>
<author confidence="0.867458">
Xiaoqiang Luo
</author>
<address confidence="0.800752666666667">
1101 Kitchawan Road, 23-121
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
</address>
<email confidence="0.998266">
xiaoluo@us.ibm.com
</email>
<sectionHeader confidence="0.9926915" genericHeader="abstract">
Abstract
1 Introduction: Why Parsing Characters?
</sectionHeader>
<bodyText confidence="0.997315083333334">
After Linguistic Data Consortium (LDC) re-
leased the Chinese Treebank (CTB) developed at
UPenn (Xia et al., 2000), various statistical Chinese
parsers (Bikel and Chiang, 2000; Xu et al., 2002)
have been built. Techniques used in parsing En-
glish have been shown working fairly well when ap-
plied to parsing Chinese text. As there is no word
boundary in written Chinese text, CTB is manually
segmented into words and then labeled. Parsers de-
scribed in (Bikel and Chiang, 2000) and (Xu et al.,
2002) operate at word-level with the assumption that
input sentences are pre-segmented.
The paper studies the problem of parsing Chi-
nese unsegmented sentences. The first motivation
is that a character-based parser can be used directly
in natural language applications that operate at char-
acter level, whereas a word-based parser requires
a separate word-segmenter. The second and more
important reason is that the availability of CTB,
a large corpus with high quality syntactic annota-
tions, provides us with an opportunity to create a
highly-accurate word-segmenter. It is widely known
that Chinese word-segmentation is a hard problem.
There are multiple studies (Wu and Fung, 1994;
Sproat et al., 1996; Luo and Roukos, 1996) show-
ing that the agreement between two (untrained) na-
tive speakers is about upper to lower .
The agreement between multiple human subjects
is even lower (Wu and Fung, 1994). The rea-
son is that human subjects may differ in segment-
ing things like personal names (whether family and
given names should be one or two words), num-
ber and measure units and compound words, al-
though these ambiguities do not change a human
being’s understanding of a sentence. Low agree-
ment between humans affects directly evaluation of
machines’ performance (Wu and Fung, 1994) as it
is hard to define a gold standard. It does not nec-
essarily imply that machines cannot do better than
humans. Indeed, if we train a model with consis-
tently segmented data, a machine may do a bet-
ter job in “remembering” word segmentations. As
will be shown shortly, it is straightforward to en-
code word-segmentation information in a character-
The paper presents a maximum entropy
Chinese character-based parser trained on
the Chinese Treebank (“CTB” hence-
forth). Word-based parse trees in
CTB are first converted into character-
based trees, where word-level part-of-
speech (POS) tags become constituent
labels and character-level tags are de-
rived from word-level POS tags. A
maximum entropy parser is then trained
on the character-based corpus. The
parser does word-segmentation, POS-
tagging and parsing in a unified frame-
work. An average label F-measure
and word-segmentation F-measure
are achieved by the parser. Our re-
sults show that word-level POS tags can
improve significantly word-segmentation,
but higher-level syntactic strutures are of
little use to word segmentation in the max-
imum entropy parser. A word-dictionary
helps to improve both word-segmentation
and parsing accuracy.
based parse tree. Parsing Chinese character streams
therefore does effectively word-segmentation, part-
of-speech (POS) tagging and constituent labeling
at the same time. Since syntactical information
influences directly word-segmentation in the pro-
posed character-based parser, CTB allows us to test
whether or not syntactic information is useful for
word-segmentation. A third advantage of parsing
Chinese character streams is that Chinese words
are more or less an open concept and the out-of-
vocabulary (OOV) word rate is high. As morphol-
ogy of the Chinese language is limited, extra care
is needed to model unknown words when building
a word-based model. Xu et al. (2002), for example,
uses an independent corpus to derive word classes so
that unknown words can be parsed reliably. Chinese
characters, on the other hand, are almost closed. To
demonstrate the OOV problem, we collect a word
and character vocabulary from the first sen-
tences of CTB, and compute their coverages on the
corresponding word and character tokenization of
the last of the corpus. The word-based OOV
rate is while the character-based OOV rate is
only .
The first step of training a character-based parser
is to convert word-based parse trees into character-
based trees. We derive character-level tags from
word-level POS tags and encode word-boundary in-
formation with a positional tag. Word-level POSs
become a constituent label in character-based trees.
A maximum entropy parser (Ratnaparkhi, 1997)
parser is then built and tested. Many language-
independent feature templates in the English parser
can be reused. Lexical features, which are language-
dependent, are used to further improve the baseline
models trained with language-independent features
only. Word-segmentation results will be presented
and it will be shown that POSs are very helpful while
higher-level syntactic structures are of little use to
word-segmentation – at least in the way they are
used in the parser.
</bodyText>
<sectionHeader confidence="0.943096" genericHeader="method">
2 Word-Tree to Character-Tree
</sectionHeader>
<bodyText confidence="0.9672655">
CTB is manually segmented and is tokenized at
word level. To build a Chinese character parser,
we first need to convert word-based parse trees into
character trees. A few simple rules are employed in
this conversion to encode word boundary informa-
tion:
</bodyText>
<listItem confidence="0.792506714285714">
1. Word-level POS tags become labels in charac-
ter trees.
2. Character-level tags are inherited from word-
level POS tags after appending a positional tag;
3. For single-character words, the positional tag is
“s”; for multiple-character words, the first char-
acter is appended with a positional tag “b”, last
</listItem>
<bodyText confidence="0.945876">
character with a positional tag “e”, and all mid-
dle characters with a positional tag “m”.
An example will clarify any ambiguity of the
rules. For example, a word-parse tree
</bodyText>
<figure confidence="0.652724166666667">
“(IP (NP (NP /NR ) (NP /NN
/NN ) ) (VP /VV ) /PU )”
would become
“(IP (NP (NP (NR /nrb /nrm /nre ) ) (NP (NN
/nnb /nne ) (NN /nnb /nne ) ) ) (VP (VV
/vvb /vve ) ) (PU /pus ) ).” (1)
</figure>
<bodyText confidence="0.99901465">
Note that the word-level POS “NR” becomes a la-
bel of the constituent spanning the three characters “
”. The character-level tags of the constituent
“ ” are the lower-cased word-level POS tag
plus a positional letter. Thus, the first character “
” is assigned the tag “nrb” where “nr” is from
the word-level POS tag and “b” denotes the begin-
ning character; the second (middle) character “ ”
gets the positional letter “m”, signifying that it is in
the middle, and the last character “ ” gets the posi-
tional letter “e”, denoting the end of the word. Other
words in the sentence are mapped similarly. After
the mapping, the number of terminal tokens of the
character tree is larger than that of the word tree.
It is clear that character-level tags encode word
boundary information, and chunk-level&apos; labels are
word-level POS tags. Therefore, parsing a Chi-
nese character sentence is effectively doing word-
segmentation, POS-tagging and constructing syntac-
tic structure at the same time.
</bodyText>
<sectionHeader confidence="0.968472" genericHeader="method">
3 Model and Features
</sectionHeader>
<bodyText confidence="0.9735003">
The maximum entropy parser (Ratnaparkhi, 1997) is
used in this study, for it offers the flexibility of inte-
grating multiple sources of knowledge into a model.
The maximum entropy model decomposes ,
the probability of a parse tree given a sentence ,
into the product of probabilities of individual parse
&apos;A chunk is here defined as a constituent whose children are
all preterminals.
actions, i.e., . The parse ac-
tions are an ordered sequence, where is the
number of actions associated with the parse . The
mapping from a parse tree to its unique sequence of
actions is 1-to-1. Each parse action is either tag-
ging a word, chunking tagged words, extend-
ing an existing constituent to another constituent,
or checking whether an open constituent should
be closed. Each component model takes the expo-
nential form:
(2)
where is a normalization term to
ensure that is a probability,
is a feature function (often binary)
and is the weight of .
Given a set of features and a corpus of training
data, there exist efficient training algorithms (Dar-
roch and Ratcliff, 1972; Berger et al., 1996) to find
the optimal parameters . The art of building
a maximum entropy parser then reduces to choos-
ing “good” features. We break features used in this
study into two categories. The first set of features
are derived from predefined templates. When these
templates are applied to training data, features are
generated automatically. Since these templates can
be used in any language, features generated this way
are referred to language-independent features. The
second category of features incorporate lexical in-
formation into the model and are primarily designed
to improve word-segmentation. This set of features
are language-dependent since a Chinese word dic-
tionary is required.
</bodyText>
<subsectionHeader confidence="0.99995">
3.1 Language-Independent Feature Templates
</subsectionHeader>
<bodyText confidence="0.979570787878788">
The maximum entropy parser (Ratnaparkhi, 1997)
parses a sentence in three phases: (1) it first tags the
input sentence. Multiple tag sequences are kept in
the search heap for processing in later stages; (2)
Tagged tokens are grouped into chunks. It is pos-
sible that a tagged token is not in any chunk; (3)
A chunked sentence, consisting of a forest of many
subtrees, is then used to extend a subtree to a new
constituent or join an existing constituent. Each ex-
tending action is followed by a checking ac-
tion which decides whether or not to close the ex-
tended constituent. In general, when a parse action
is carried out, the context information, i.e., the in-
put sentence and preceding parse actions ,
is represented by a forest of subtrees. Feature func-
tions operate on the forest context and the next parse
action. They are all of the form:
(3)
where is a binary function on the con-
text.
Some notations are needed to present features.
We use to denote an input terminal token, its
tag (preterminal), a chunk, and a constituent
label, where the index is relative to the current
subtree: the subtree immediately left to the current
is indexed as , the second left to the current sub-
tree is indexed as , the subtree immediately to the
right is indexed as, so on and so forth. repre-
sents the root label of the -child of the subtree.
If , the child is counted from right.
With these notations, we are ready to introduce
language-independent features, which are broken
down as follows:
</bodyText>
<subsectionHeader confidence="0.849417">
Tag Features
</subsectionHeader>
<bodyText confidence="0.978878214285714">
In the tag model, the context consists of a win-
dow of five tokens – the token being tagged and
two tokens to its left and right – and two tags on
the left of the current word. The feature templates
are tabulated in Table 1 (to save space, templates are
grouped). At training time, feature templates are in-
stantiated by the training data. For example, when
the template “ ” is applied to the first charac-
ter of the sample sentence,
“(IP (NP (NP (NR /nrb /nrm /nre ) ) (NP (NN
/nnb /nne ) (NN /nnb /nne ) ) ) (VP (VV
/vvb /vve ) ) (PU /pus ) )”,
a feature *BOUNDARY*
.
</bodyText>
<subsectionHeader confidence="0.747843">
Chunk Features
</subsectionHeader>
<bodyText confidence="0.983049">
As character-level tags have encoded the chunk
label information and the uncertainly about a chunk
action is low given character-level tags, we limit the
chunk context to a window of three subtrees – the
current one plus its left and right subtree. in Ta-
ble 2 denotes the label of the subtree if it is not
is
generated. Note that is the token on the left
and in this case, the boundary of the sentence. The
template “ ” is instantiated similarly as
</bodyText>
<tableCaption confidence="0.992951">
Table 1: Tag feature templates:
</tableCaption>
<bodyText confidence="0.6869158">
: current token (if ) or to-
ken on the left (if ) or right (if ).
: tag.
a chunk, or the chunk label plus the tag of its right-
most child if it is a chunk.
</bodyText>
<table confidence="0.554409">
Index Template (context,future)
1
2
</table>
<tableCaption confidence="0.986916">
Table 2: Chunk feature templates:
</tableCaption>
<bodyText confidence="0.9507322">
is the chunk label plus the tag of its right most child
if the tree is a chunk; Otherwise is the con-
stituent label of the tree.
Again, we use the sentence (1) as an example. As-
sume that the current forest of subtrees is
(NR /nrb /nrm /nre ) /nnb /nne /nnb
/nne /vvb /vve /pus ,
and the current subtree is “ /nnb”, then instan-
tiating the template would result in a feature
.
</bodyText>
<subsectionHeader confidence="0.59111">
Extend Features
</subsectionHeader>
<bodyText confidence="0.999669125">
Extend features depend on previous subtree and
the two following subtrees. Some features uses child
labels of the previous subtree. For example, the in-
terpretation of the template on line 4 of Table 3 is
that is the root label of the previous subtree,
is the label of the right-most child of the
previous tree, and is the root label of the current
subtree.
</bodyText>
<sectionHeader confidence="0.610037" genericHeader="method">
Check Features
</sectionHeader>
<bodyText confidence="0.998605">
Most of check feature templates again use con-
stituent labels of the surrounding subtrees. The tem-
plate on line 1 of Table 4 is unique to the check
model. It essentially looks at children of the cur-
rent constituent, which is intuitively a strong indica-
tion whether or not the current constituent should be
closed.
</bodyText>
<tableCaption confidence="0.780630833333333">
Table 3: Extend feature templates:
is the root constituent label of the
subtree (relative to the current one);
is the label of the rightmost child of the
previous subtree.
Table 4: Check feature templates:
</tableCaption>
<bodyText confidence="0.975867333333333">
is the constituent label of the subtree
(relative to the current one). is the child la-
bel of the current constituent.
</bodyText>
<subsectionHeader confidence="0.998059">
3.2 Language-Dependent Features
</subsectionHeader>
<bodyText confidence="0.9998269">
The model described so far does not depend on any
Chinese word dictionary. All features derived from
templates in Section 3.1 are extracted from training
data. A problem is that words not seen in training
data may not have “good” features associated with
them. Fortunately, the maximum entropy framework
makes it relatively easy to incorporate other sources
of knowledge into the model. We present a set of
language-dependent features in this section, primar-
ily for Chinese word segmentation.
The language-dependent features are computed
from a word list and training data. Formerly, let be
a list of Chinese words, where characters are sepa-
rated by spaces. At the time of tagging characters
(recall word-segmentation information is encoded
in character-level tags), we test characters within a
window of five (that is, two characters to the left and
two to the right) and see if a character either starts,
occurs in any position of, or ends any word on the
list . This feature templates are summarized in Ta-
</bodyText>
<figure confidence="0.986169125">
Index
Template (context,future)
1
2
3
4
5
Index
Template (context,future)
1
2
3
4
5
6
Index
Template (context,future)
1
2
3
4
5
6
7
</figure>
<bodyText confidence="0.7240894">
ble 5. tests if the character starts any
word on the list . Similarly, tests if the
character occurs in any position of any word on
the list , and tests if the character is
the last position of any word on the list .
</bodyText>
<tableCaption confidence="0.946337">
Table 5: Language-dependent lexical features.
</tableCaption>
<bodyText confidence="0.999845363636364">
A word list can be collected to encode different
semantic or syntactic information. For example, a
list of location names or personal names may help
the model to identify unseen city or personal names;
Or a closed list of functional words can be collected
to represent a particular set of words sharing a POS.
This type of features would improve the model ro-
bustness since unseen words will share features fired
for seen words. We will show shortly that even a
relatively small word-list improves significantly the
word-segmentation accuracy.
</bodyText>
<sectionHeader confidence="0.99973" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.998466409090909">
All experiments reported here are conducted on the
latest LDC release of the Chinese Treebank, which
consists of about words. Word parse trees
are converted to character trees using the procedure
described in Section 2. All traces and functional
tags are stripped in training and testing. Two re-
sults are reported for the character-based parsers: the
F-measure of word segmentation and F-measure of
constituent labels. Formally, let be the
number of words of the reference sentence and its
parser output, respectively, and be the number
of common words in the sentence of test set, then
the word segmentation F-measure is
The F-measure of constituent labels is computed
similarly:
where and are the number of con-
stituents in the reference parse tree and parser
output, respectively, and is the number of
common constituents. Chunk-level labels converted
from POS tags (e.g., “NR”, “NN” and “VV” etc in
(1)) are included in computing label F-measures for
character-based parsers.
</bodyText>
<subsectionHeader confidence="0.999366">
4.1 Impact of Training Data
</subsectionHeader>
<bodyText confidence="0.9973113125">
The first question we have is whether CTB is large
enough in the sense that the performance saturates.
The first set of experiments are intended to answer
this question. In these experiments, the first
CTB is used as the training set and the rest as
the test set. We start with of the training set
and increase the training set each time by . Only
language-independent features are used in these ex-
periments.
Figure 1 shows the word segmentation F-measure
and label F-measure versus the amount of training
data. As can be seen, F-measures of both word
segmentation and constituent label increase mono-
tonically as the amount of training data increases.
If all training data is used, the word segmentation
F-measure is and label F-measure
</bodyText>
<subsectionHeader confidence="0.5246515">
Word seg F−measure and Label F−measure vs. training size
Percent of training data
</subsectionHeader>
<figureCaption confidence="0.994834666666667">
Figure 1: Learning curves: word-segmentation F-
measure and parsing label F-measure vs. percentage
of training data.
</figureCaption>
<figure confidence="0.9930190625">
Index
Template (context,future)
1
2
3
1
0.95
0.9
0.85
0.8
0.75
0.7
Segmentation
Label
0.650 20 40 60 80 100
F−measure
</figure>
<bodyText confidence="0.978401857142857">
.
These results show that language-independent fea-
tures work fairly well – a major advantage of data-
driven statistical approach. The learning curve also
shows that the current training size has not reached
a saturating point. This indicates that there is room
to improve our model by getting more training data.
</bodyText>
<subsectionHeader confidence="0.998979">
4.2 Effect of Lexical Features
</subsectionHeader>
<bodyText confidence="0.999904384615385">
In this section, we present the main parsing results.
As it has not been long since the second release of
CTB and there is no commonly-agreed training and
test set, we divide the entire corpus into 10 equal par-
titions and hold each partition as a test set while the
rest are used for training. For each training-test con-
figuration, a baseline model is trained with only lan-
guage independent features. Baseline word segmen-
tation and label F-measures are plotted with dotted-
line in Figure 2. We then add extra lexical features
described in Section 3.1 to the model. Lexical ques-
tions are derived from a 58K-entry word list. The
word list is broken into 4 sub-lists based on word
length, ranging from 2 to 5 characters. Lexical fea-
tures are computed by answering one of the three
questions in Table 5. Intuitively, these questions
would help the model to identify word boundaries,
which in turn ought to improve the parser. This is
confirmed by results shown in Figure 2. The solid
two lines represent results with enhanced lexical
questions. As can be seen, lexical questions improve
significantly both word segmentation and parsing
across all experiments. This is not surprising as lex-
ical features derived from the word list are comple-
mentary to language-independent features computed
from training sentences.
</bodyText>
<figure confidence="0.9316055">
Results of 10 experiments
Experiment Number
</figure>
<figureCaption confidence="0.9922884">
Figure 2: Parsing and word segmentation F-
measures vs. the experiment numbers. Lines with
triangles: segmentation; Lines with circles: label;
Dotted-lines: language-independent features only;
Solid lines: plus lexical features.
</figureCaption>
<bodyText confidence="0.997283631578948">
Another observation is that results vary greatly
across experiment configurations: for the model
trained with lexical features, the second exper-
iment has a label F-measure and word-
segmentation F-measure , while the sixth ex-
periment has a label F-measure and word-
segmentation F-measure . The large variances
justify multiple experiment runs. To reduce the vari-
ances, we report numbers averaged over the 10 ex-
periments in Table 6. Numbers on the row start-
ing with “WS” are word-segmentation results, while
numbers on the last row are F-measures of con-
stituent labels. The second column are average F-
measures for the baseline model trained with only
language-independent features. The third column
contains F-measures for the model trained with extra
lexical features. The last column are releative error
reduction. The best average word-segmentation F-
measure is and label F-measure is .
</bodyText>
<table confidence="0.99706225">
F-measure Relative(%)
baseline LexFeat
WS(%) 94.6 96.0 26
Label(%) 80.0 81.4 7
</table>
<tableCaption confidence="0.99954">
Table 6: WS: word-segmentation. Baseline:
</tableCaption>
<bodyText confidence="0.985874666666667">
language-independent features. LexFeat: plus lex-
ical features. Numbers are averaged over the 10 ex-
periments in Figure 2.
</bodyText>
<subsectionHeader confidence="0.9986675">
4.3 Effect of Syntactic Information on
Word-segmentation
</subsectionHeader>
<bodyText confidence="0.905147055555556">
Since CTB provides us with full parse trees, we want
to know how syntactic information affects word-
segmentation. To this end, we devise two sets of
experiments:
1. We strip all POS tags and labels in the Chinese
Treebank and retain only word boundary infor-
mation. To use the same maximum entropy
parser, we represent word boundary by dummy
constituent label “W”. For example, the sample
sentence (1) in Section 2 is represented as:
(W /wb /wm /we) (W /wb /we) (W
/wb /we ) (W /wb /we ) (W /ws ).
2. We remove all labels but retain word-level POS
information. The sample sentence above is rep-
resented as:
(NR /nrb /nrm /nre ) (NN /nnb /nne
) (NN /nnb /nne ) (VV /vvb /vve ) (PU
/pus ).
</bodyText>
<figure confidence="0.996684125">
1
0.95
F−measure
Segmentation (with LexFeat)
Segmentation (baseline)
Label (with LexFeat)
Label (baseline)
0.8
0.75
0.71 2 3 4 5 6 7 8 9 10
0.9
0.85
Note that positional tags are used in both setups.
Effect of Syntactic Info on Word Segmentation
0.931 2 3 4 5 6 7 8 9 10
Experiment Number
</figure>
<figureCaption confidence="0.998745">
Figure 3: Usefulness of syntactic information:
</figureCaption>
<bodyText confidence="0.93601065">
(black) dash-dotted line – word boundaries only,
(red) dashed line – POS info, and (blue) solid line
– full parse trees.
With these two representations of CTB, we re-
peat the 10 experiments of Section 4.2 using the
same lexical features. Word-segmentation results
are plotted in Figure 3. The model trained with word
boundary information has the worst performance,
which is not surprising as we would expect infor-
mation such as POS tags to help disambiguate word
boundaries. What is surprising is that syntactic in-
formation beyond POS tags has little effect on word-
segmentation – there is practically no difference be-
tween the solid line (for the model trained with
full parse trees) and the dashed-line (for the model
trained with POS information) in Figure 3. This re-
sult suggests that most ambiguities of Chinese word
boundaries can be resolved at lexical level, and high-
level syntactic information does not help much to
word segmentation in the current parser.
</bodyText>
<sectionHeader confidence="0.999989" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999749454545455">
Bikel and Chiang (2000) and Xu et al. (2002) con-
struct word-based statistical parsers on the first re-
lease of Chinese Treebank, which has about 100K
words, roughly half of the training data used in this
study. Bikel and Chiang (2000) in fact contains two
parsers: one is a lexicalized probabilistic context-
free grammar (PCFG) similar to (Collins, 1997);
the other is based on statistical TAG (Chiang, 2000).
About F-measure is reported in (Bikel and Chi-
ang, 2000). Xu et al. (2002) is also based on PCFG,
but enhanced with lexical features derived from the
ASBC corpus2. Xu et al. (2002) reports an overall
F-measure when the same training and test
set as (Bikel and Chiang, 2000) are used. Since our
parser operates at character level, and more training
data is used, the best results are not directly compa-
rable. The middle point of the learning curve in Fig-
ure 1, which is trained with roughly 100K words, is
at the same ballpark of (Xu et al., 2002). The con-
tribution of this work is that the proposed character-
based parser does word-segmentation, POS tagging
and parsing in a unified framework. It is the first at-
tempt to our knowledge that syntactic information is
used in word-segmentation.
Chinese word segmentation is a well-known prob-
lem that has been studied extensively (Wu and
Fung, 1994; Sproat et al., 1996; Luo and Roukos,
1996) and it is known that human agreement is
relatively low. Without knowing and control-
ling testing conditions, it is nearly impossible to
compare results in a meaningful way. There-
fore, we will compare our approach with some
related work only without commenting on seg-
mentation accuracy. Wu and Tseng (1993) con-
tains a good problem statement of Chinese word-
segmentation and also outlines a few segmentation
algorithms. Our method is supervised in that the
training data is manually labeled. Palmer (1997)
uses transform-based learning (TBL) to correct an
initial segmentation. Sproat et al. (1996) employs
stochastic finite state machines to find word bound-
aries. Luo and Roukos (1996) proposes to use a
language model to select from ambiguous word-
segmentations. All these work assume that a lexi-
con or some manually segmented data or both are
available. There are numerous work exploring semi-
supervised or unsupervised algorithms to segment
Chinese text. Ando and Lee (2003) uses a heuris-
tic method that does not require segmented training
data. Peng and Schuurmans (2001) learns a lexicon
and its unigram probability distribution. The auto-
matically learned lexicon is pruned using a mutual
information criterion. Peng and Schuurmans (2001)
requires a validation set and is therefore semi-
supervised.
</bodyText>
<figure confidence="0.9594255">
2Seehttp://godel.iis.sinica.edu.tw/ROCLING.
0.975
Word−boundary
POS
Full Tree
0.97
0.965
0.96
0.955
0.95
0.945
0.94
0.935
Word−seg F−measure
</figure>
<sectionHeader confidence="0.967775" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999556466666667">
We present a maximum entropy Chinese character-
based parser which does word-segmentation, POS
tagging and parsing in a unified framework. The
flexibility of maximum entropy model allows us
to integrate into the model knowledge from other
sources, together with features derived automat-
ically from training corpus. We have shown
that a relatively small word-list can reduce word-
segmentation error by as much as , and a word-
segmentation F-measure and label F-measure
are obtained by the character-based parser.
Our results also show that POS information is very
useful for Chinese word-segmentation, but higher-
level syntactic information benefits little to word-
segmentation.
</bodyText>
<sectionHeader confidence="0.999503" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999615">
Special thanks go to Hongyan Jing and Judith
Hochberg who proofread the paper and corrected
many typos and ungrammatical errors. The author is
also grateful to the anonymous reviewers for their in-
sightful comments and suggestions. This work was
partially supported by the Defense Advanced Re-
search Projects Agency and monitored by SPAWAR
under contract No. N66001-99-2-8916. The views
and findings contained in this material are those of
the authors and do not necessarily reflect the posi-
tion of policy of the Government and no official en-
dorsement should be inferred.
</bodyText>
<sectionHeader confidence="0.999354" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997978388888889">
Rie Kubota Ando and Lillian Lee. 2003. Mostly-
unsupervised statistical segmentation of Japanese
Kanji. Natural Language Engineering.
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39–71, March.
Daniel M. Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the chinese treebank.
In Proceedings of the Second Chinese Language Pro-
cessing Workshop, pages 1–6.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proc. Annual Meeting of ACL, pages 1–6.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proc. Annual Meet-
ing ofACL, pages 16–23.
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear model. Ann. Math. Statist.,
43:1470–1480.
Xiaoqiang Luo and Salim Roukos. 1996. An iterative al-
gorithm to build chinese language models. In Proc. of
the 34th Annual Meeting of the Association for Com-
putational Linguistics, pages 139–143.
David Palmer. 1997. A trainable rule-based algorithm
for word segmentation. In Proc. Annual Meeting of
ACL, Madrid.
Fuchun Peng and Dale Schuurmans. 2001. Self-
supervised Chinese word segmentation. In Advances
in Intelligent Data Analysis, pages 238–247.
Adwait Ratnaparkhi. 1997. A Linear Observed Time
Statistical Parser Based on Maximum Entropy Mod-
els. In Second Conference on Empirical Methods in
Natural Language Processing, pages 1 – 10.
Richard Sproat, Chilin Shih, William Gale, and Nancy
Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational
Linguistics, 22(3):377–404.
Dekai Wu and Pascale Fung. 1994. Improving chinese
tokenization with linguistic filters on statistical lexical
acquisition. In Fourth Conference on Applied Natural
Language Processing, pages 180–181, Stuttgart.
Zimin Wu and Gwyneth Tseng. 1993. Chinese text seg-
mentation for text retrieval: Achievements and prob-
lems. Journal of The American Society for Informa-
tion Science, 44(9):532–542.
F. Xia, M. Palmer, N. Xue, M.E. Okurowski, J. Kovarik,
F.D. Chiou, S. Huang, T. Kroch, and M. Marcus. 2000.
Developing guidelines and ensuring consistency for
Chinese text annotation. In Proc of the 2nd Intl. Conf.
on Language Resources and Evaluation (LREC 2000).
Jinxi Xu, Scott Miller, and Ralph Weischedel. 2002. A
statistical parser for Chinese. In Proc. Human Lan-
guage Technology Workshop.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.337906">
<title confidence="0.910934">A Maximum Entropy Chinese Character-Based Parser Xiaoqiang</title>
<author confidence="0.86918">Kitchawan Road</author>
<affiliation confidence="0.9004095">IBM T.J. Watson Research Yorktown Heights, NY</affiliation>
<email confidence="0.999072">xiaoluo@us.ibm.com</email>
<intro confidence="0.556613">Abstract</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Lillian Lee</author>
</authors>
<title>Mostlyunsupervised statistical segmentation of Japanese Kanji. Natural Language Engineering.</title>
<date>2003</date>
<contexts>
<context position="24541" citStr="Ando and Lee (2003)" startWordPosition="4114" endWordPosition="4117">dsegmentation and also outlines a few segmentation algorithms. Our method is supervised in that the training data is manually labeled. Palmer (1997) uses transform-based learning (TBL) to correct an initial segmentation. Sproat et al. (1996) employs stochastic finite state machines to find word boundaries. Luo and Roukos (1996) proposes to use a language model to select from ambiguous wordsegmentations. All these work assume that a lexicon or some manually segmented data or both are available. There are numerous work exploring semisupervised or unsupervised algorithms to segment Chinese text. Ando and Lee (2003) uses a heuristic method that does not require segmented training data. Peng and Schuurmans (2001) learns a lexicon and its unigram probability distribution. The automatically learned lexicon is pruned using a mutual information criterion. Peng and Schuurmans (2001) requires a validation set and is therefore semisupervised. 2Seehttp://godel.iis.sinica.edu.tw/ROCLING. 0.975 Word−boundary POS Full Tree 0.97 0.965 0.96 0.955 0.95 0.945 0.94 0.935 Word−seg F−measure 6 Conclusions We present a maximum entropy Chinese characterbased parser which does word-segmentation, POS tagging and parsing in a u</context>
</contexts>
<marker>Ando, Lee, 2003</marker>
<rawString>Rie Kubota Ando and Lillian Lee. 2003. Mostlyunsupervised statistical segmentation of Japanese Kanji. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="8158" citStr="Berger et al., 1996" startWordPosition="1326" endWordPosition="1329"> of actions associated with the parse . The mapping from a parse tree to its unique sequence of actions is 1-to-1. Each parse action is either tagging a word, chunking tagged words, extending an existing constituent to another constituent, or checking whether an open constituent should be closed. Each component model takes the exponential form: (2) where is a normalization term to ensure that is a probability, is a feature function (often binary) and is the weight of . Given a set of features and a corpus of training data, there exist efficient training algorithms (Darroch and Ratcliff, 1972; Berger et al., 1996) to find the optimal parameters . The art of building a maximum entropy parser then reduces to choosing “good” features. We break features used in this study into two categories. The first set of features are derived from predefined templates. When these templates are applied to training data, features are generated automatically. Since these templates can be used in any language, features generated this way are referred to language-independent features. The second category of features incorporate lexical information into the model and are primarily designed to improve word-segmentation. This </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>David Chiang</author>
</authors>
<title>Two statistical parsing models applied to the chinese treebank.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second Chinese Language Processing Workshop,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="690" citStr="Bikel and Chiang, 2000" startWordPosition="103" endWordPosition="106"> Kitchawan Road, 23-121 IBM T.J. Watson Research Center Yorktown Heights, NY 10598 xiaoluo@us.ibm.com Abstract 1 Introduction: Why Parsing Characters? After Linguistic Data Consortium (LDC) released the Chinese Treebank (CTB) developed at UPenn (Xia et al., 2000), various statistical Chinese parsers (Bikel and Chiang, 2000; Xu et al., 2002) have been built. Techniques used in parsing English have been shown working fairly well when applied to parsing Chinese text. As there is no word boundary in written Chinese text, CTB is manually segmented into words and then labeled. Parsers described in (Bikel and Chiang, 2000) and (Xu et al., 2002) operate at word-level with the assumption that input sentences are pre-segmented. The paper studies the problem of parsing Chinese unsegmented sentences. The first motivation is that a character-based parser can be used directly in natural language applications that operate at character level, whereas a word-based parser requires a separate word-segmenter. The second and more important reason is that the availability of CTB, a large corpus with high quality syntactic annotations, provides us with an opportunity to create a highly-accurate word-segmenter. It is widely kno</context>
<context position="22247" citStr="Bikel and Chiang (2000)" startWordPosition="3731" endWordPosition="3734">rprising as we would expect information such as POS tags to help disambiguate word boundaries. What is surprising is that syntactic information beyond POS tags has little effect on wordsegmentation – there is practically no difference between the solid line (for the model trained with full parse trees) and the dashed-line (for the model trained with POS information) in Figure 3. This result suggests that most ambiguities of Chinese word boundaries can be resolved at lexical level, and highlevel syntactic information does not help much to word segmentation in the current parser. 5 Related Work Bikel and Chiang (2000) and Xu et al. (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. Bikel and Chiang (2000) in fact contains two parsers: one is a lexicalized probabilistic contextfree grammar (PCFG) similar to (Collins, 1997); the other is based on statistical TAG (Chiang, 2000). About F-measure is reported in (Bikel and Chiang, 2000). Xu et al. (2002) is also based on PCFG, but enhanced with lexical features derived from the ASBC corpus2. Xu et al. (2002) reports an overall F-measure when t</context>
</contexts>
<marker>Bikel, Chiang, 2000</marker>
<rawString>Daniel M. Bikel and David Chiang. 2000. Two statistical parsing models applied to the chinese treebank. In Proceedings of the Second Chinese Language Processing Workshop, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In Proc. Annual Meeting of ACL,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="690" citStr="Chiang, 2000" startWordPosition="105" endWordPosition="106"> Road, 23-121 IBM T.J. Watson Research Center Yorktown Heights, NY 10598 xiaoluo@us.ibm.com Abstract 1 Introduction: Why Parsing Characters? After Linguistic Data Consortium (LDC) released the Chinese Treebank (CTB) developed at UPenn (Xia et al., 2000), various statistical Chinese parsers (Bikel and Chiang, 2000; Xu et al., 2002) have been built. Techniques used in parsing English have been shown working fairly well when applied to parsing Chinese text. As there is no word boundary in written Chinese text, CTB is manually segmented into words and then labeled. Parsers described in (Bikel and Chiang, 2000) and (Xu et al., 2002) operate at word-level with the assumption that input sentences are pre-segmented. The paper studies the problem of parsing Chinese unsegmented sentences. The first motivation is that a character-based parser can be used directly in natural language applications that operate at character level, whereas a word-based parser requires a separate word-segmenter. The second and more important reason is that the availability of CTB, a large corpus with high quality syntactic annotations, provides us with an opportunity to create a highly-accurate word-segmenter. It is widely kno</context>
<context position="22247" citStr="Chiang (2000)" startWordPosition="3733" endWordPosition="3734">s we would expect information such as POS tags to help disambiguate word boundaries. What is surprising is that syntactic information beyond POS tags has little effect on wordsegmentation – there is practically no difference between the solid line (for the model trained with full parse trees) and the dashed-line (for the model trained with POS information) in Figure 3. This result suggests that most ambiguities of Chinese word boundaries can be resolved at lexical level, and highlevel syntactic information does not help much to word segmentation in the current parser. 5 Related Work Bikel and Chiang (2000) and Xu et al. (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. Bikel and Chiang (2000) in fact contains two parsers: one is a lexicalized probabilistic contextfree grammar (PCFG) similar to (Collins, 1997); the other is based on statistical TAG (Chiang, 2000). About F-measure is reported in (Bikel and Chiang, 2000). Xu et al. (2002) is also based on PCFG, but enhanced with lexical features derived from the ASBC corpus2. Xu et al. (2002) reports an overall F-measure when t</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In Proc. Annual Meeting of ACL, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. Annual Meeting ofACL,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="22576" citStr="Collins, 1997" startWordPosition="3788" endWordPosition="3789"> trained with POS information) in Figure 3. This result suggests that most ambiguities of Chinese word boundaries can be resolved at lexical level, and highlevel syntactic information does not help much to word segmentation in the current parser. 5 Related Work Bikel and Chiang (2000) and Xu et al. (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. Bikel and Chiang (2000) in fact contains two parsers: one is a lexicalized probabilistic contextfree grammar (PCFG) similar to (Collins, 1997); the other is based on statistical TAG (Chiang, 2000). About F-measure is reported in (Bikel and Chiang, 2000). Xu et al. (2002) is also based on PCFG, but enhanced with lexical features derived from the ASBC corpus2. Xu et al. (2002) reports an overall F-measure when the same training and test set as (Bikel and Chiang, 2000) are used. Since our parser operates at character level, and more training data is used, the best results are not directly comparable. The middle point of the learning curve in Figure 1, which is trained with roughly 100K words, is at the same ballpark of (Xu et al., 2002</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proc. Annual Meeting ofACL, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear model.</title>
<date>1972</date>
<journal>Ann. Math. Statist.,</journal>
<pages>43--1470</pages>
<contexts>
<context position="8136" citStr="Darroch and Ratcliff, 1972" startWordPosition="1321" endWordPosition="1325">equence, where is the number of actions associated with the parse . The mapping from a parse tree to its unique sequence of actions is 1-to-1. Each parse action is either tagging a word, chunking tagged words, extending an existing constituent to another constituent, or checking whether an open constituent should be closed. Each component model takes the exponential form: (2) where is a normalization term to ensure that is a probability, is a feature function (often binary) and is the weight of . Given a set of features and a corpus of training data, there exist efficient training algorithms (Darroch and Ratcliff, 1972; Berger et al., 1996) to find the optimal parameters . The art of building a maximum entropy parser then reduces to choosing “good” features. We break features used in this study into two categories. The first set of features are derived from predefined templates. When these templates are applied to training data, features are generated automatically. Since these templates can be used in any language, features generated this way are referred to language-independent features. The second category of features incorporate lexical information into the model and are primarily designed to improve wo</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear model. Ann. Math. Statist., 43:1470–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Salim Roukos</author>
</authors>
<title>An iterative algorithm to build chinese language models.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>139--143</pages>
<contexts>
<context position="1432" citStr="Luo and Roukos, 1996" startWordPosition="218" endWordPosition="221">s the problem of parsing Chinese unsegmented sentences. The first motivation is that a character-based parser can be used directly in natural language applications that operate at character level, whereas a word-based parser requires a separate word-segmenter. The second and more important reason is that the availability of CTB, a large corpus with high quality syntactic annotations, provides us with an opportunity to create a highly-accurate word-segmenter. It is widely known that Chinese word-segmentation is a hard problem. There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower . The agreement between multiple human subjects is even lower (Wu and Fung, 1994). The reason is that human subjects may differ in segmenting things like personal names (whether family and given names should be one or two words), number and measure units and compound words, although these ambiguities do not change a human being’s understanding of a sentence. Low agreement between humans affects directly evaluation of machines’ performance (Wu and Fung, 1994) as it is hard to define a gold standard. It d</context>
<context position="23568" citStr="Luo and Roukos, 1996" startWordPosition="3959" endWordPosition="3962">racter level, and more training data is used, the best results are not directly comparable. The middle point of the learning curve in Figure 1, which is trained with roughly 100K words, is at the same ballpark of (Xu et al., 2002). The contribution of this work is that the proposed characterbased parser does word-segmentation, POS tagging and parsing in a unified framework. It is the first attempt to our knowledge that syntactic information is used in word-segmentation. Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low. Without knowing and controlling testing conditions, it is nearly impossible to compare results in a meaningful way. Therefore, we will compare our approach with some related work only without commenting on segmentation accuracy. Wu and Tseng (1993) contains a good problem statement of Chinese wordsegmentation and also outlines a few segmentation algorithms. Our method is supervised in that the training data is manually labeled. Palmer (1997) uses transform-based learning (TBL) to correct an initial segmentation. Sproat et al. (1996) empl</context>
</contexts>
<marker>Luo, Roukos, 1996</marker>
<rawString>Xiaoqiang Luo and Salim Roukos. 1996. An iterative algorithm to build chinese language models. In Proc. of the 34th Annual Meeting of the Association for Computational Linguistics, pages 139–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Palmer</author>
</authors>
<title>A trainable rule-based algorithm for word segmentation.</title>
<date>1997</date>
<booktitle>In Proc. Annual Meeting of ACL,</booktitle>
<location>Madrid.</location>
<contexts>
<context position="24070" citStr="Palmer (1997)" startWordPosition="4042" endWordPosition="4043">-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low. Without knowing and controlling testing conditions, it is nearly impossible to compare results in a meaningful way. Therefore, we will compare our approach with some related work only without commenting on segmentation accuracy. Wu and Tseng (1993) contains a good problem statement of Chinese wordsegmentation and also outlines a few segmentation algorithms. Our method is supervised in that the training data is manually labeled. Palmer (1997) uses transform-based learning (TBL) to correct an initial segmentation. Sproat et al. (1996) employs stochastic finite state machines to find word boundaries. Luo and Roukos (1996) proposes to use a language model to select from ambiguous wordsegmentations. All these work assume that a lexicon or some manually segmented data or both are available. There are numerous work exploring semisupervised or unsupervised algorithms to segment Chinese text. Ando and Lee (2003) uses a heuristic method that does not require segmented training data. Peng and Schuurmans (2001) learns a lexicon and its unigr</context>
</contexts>
<marker>Palmer, 1997</marker>
<rawString>David Palmer. 1997. A trainable rule-based algorithm for word segmentation. In Proc. Annual Meeting of ACL, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Dale Schuurmans</author>
</authors>
<title>Selfsupervised Chinese word segmentation.</title>
<date>2001</date>
<booktitle>In Advances in Intelligent Data Analysis,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="24639" citStr="Peng and Schuurmans (2001)" startWordPosition="4130" endWordPosition="4133">that the training data is manually labeled. Palmer (1997) uses transform-based learning (TBL) to correct an initial segmentation. Sproat et al. (1996) employs stochastic finite state machines to find word boundaries. Luo and Roukos (1996) proposes to use a language model to select from ambiguous wordsegmentations. All these work assume that a lexicon or some manually segmented data or both are available. There are numerous work exploring semisupervised or unsupervised algorithms to segment Chinese text. Ando and Lee (2003) uses a heuristic method that does not require segmented training data. Peng and Schuurmans (2001) learns a lexicon and its unigram probability distribution. The automatically learned lexicon is pruned using a mutual information criterion. Peng and Schuurmans (2001) requires a validation set and is therefore semisupervised. 2Seehttp://godel.iis.sinica.edu.tw/ROCLING. 0.975 Word−boundary POS Full Tree 0.97 0.965 0.96 0.955 0.95 0.945 0.94 0.935 Word−seg F−measure 6 Conclusions We present a maximum entropy Chinese characterbased parser which does word-segmentation, POS tagging and parsing in a unified framework. The flexibility of maximum entropy model allows us to integrate into the model k</context>
</contexts>
<marker>Peng, Schuurmans, 2001</marker>
<rawString>Fuchun Peng and Dale Schuurmans. 2001. Selfsupervised Chinese word segmentation. In Advances in Intelligent Data Analysis, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Linear Observed Time Statistical Parser Based on Maximum Entropy Models.</title>
<date>1997</date>
<booktitle>In Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="4651" citStr="Ratnaparkhi, 1997" startWordPosition="726" endWordPosition="727">the OOV problem, we collect a word and character vocabulary from the first sentences of CTB, and compute their coverages on the corresponding word and character tokenization of the last of the corpus. The word-based OOV rate is while the character-based OOV rate is only . The first step of training a character-based parser is to convert word-based parse trees into characterbased trees. We derive character-level tags from word-level POS tags and encode word-boundary information with a positional tag. Word-level POSs become a constituent label in character-based trees. A maximum entropy parser (Ratnaparkhi, 1997) parser is then built and tested. Many languageindependent feature templates in the English parser can be reused. Lexical features, which are languagedependent, are used to further improve the baseline models trained with language-independent features only. Word-segmentation results will be presented and it will be shown that POSs are very helpful while higher-level syntactic structures are of little use to word-segmentation – at least in the way they are used in the parser. 2 Word-Tree to Character-Tree CTB is manually segmented and is tokenized at word level. To build a Chinese character par</context>
<context position="7123" citStr="Ratnaparkhi, 1997" startWordPosition="1149" endWordPosition="1150">g that it is in the middle, and the last character “ ” gets the positional letter “e”, denoting the end of the word. Other words in the sentence are mapped similarly. After the mapping, the number of terminal tokens of the character tree is larger than that of the word tree. It is clear that character-level tags encode word boundary information, and chunk-level&apos; labels are word-level POS tags. Therefore, parsing a Chinese character sentence is effectively doing wordsegmentation, POS-tagging and constructing syntactic structure at the same time. 3 Model and Features The maximum entropy parser (Ratnaparkhi, 1997) is used in this study, for it offers the flexibility of integrating multiple sources of knowledge into a model. The maximum entropy model decomposes , the probability of a parse tree given a sentence , into the product of probabilities of individual parse &apos;A chunk is here defined as a constituent whose children are all preterminals. actions, i.e., . The parse actions are an ordered sequence, where is the number of actions associated with the parse . The mapping from a parse tree to its unique sequence of actions is 1-to-1. Each parse action is either tagging a word, chunking tagged words, ext</context>
<context position="8931" citStr="Ratnaparkhi, 1997" startWordPosition="1442" endWordPosition="1443">dy into two categories. The first set of features are derived from predefined templates. When these templates are applied to training data, features are generated automatically. Since these templates can be used in any language, features generated this way are referred to language-independent features. The second category of features incorporate lexical information into the model and are primarily designed to improve word-segmentation. This set of features are language-dependent since a Chinese word dictionary is required. 3.1 Language-Independent Feature Templates The maximum entropy parser (Ratnaparkhi, 1997) parses a sentence in three phases: (1) it first tags the input sentence. Multiple tag sequences are kept in the search heap for processing in later stages; (2) Tagged tokens are grouped into chunks. It is possible that a tagged token is not in any chunk; (3) A chunked sentence, consisting of a forest of many subtrees, is then used to extend a subtree to a new constituent or join an existing constituent. Each extending action is followed by a checking action which decides whether or not to close the extended constituent. In general, when a parse action is carried out, the context information, </context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A Linear Observed Time Statistical Parser Based on Maximum Entropy Models. In Second Conference on Empirical Methods in Natural Language Processing, pages 1 – 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A stochastic finite-state wordsegmentation algorithm for Chinese.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="1409" citStr="Sproat et al., 1996" startWordPosition="214" endWordPosition="217">ted. The paper studies the problem of parsing Chinese unsegmented sentences. The first motivation is that a character-based parser can be used directly in natural language applications that operate at character level, whereas a word-based parser requires a separate word-segmenter. The second and more important reason is that the availability of CTB, a large corpus with high quality syntactic annotations, provides us with an opportunity to create a highly-accurate word-segmenter. It is widely known that Chinese word-segmentation is a hard problem. There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower . The agreement between multiple human subjects is even lower (Wu and Fung, 1994). The reason is that human subjects may differ in segmenting things like personal names (whether family and given names should be one or two words), number and measure units and compound words, although these ambiguities do not change a human being’s understanding of a sentence. Low agreement between humans affects directly evaluation of machines’ performance (Wu and Fung, 1994) as it is hard to defin</context>
<context position="23545" citStr="Sproat et al., 1996" startWordPosition="3955" endWordPosition="3958">arser operates at character level, and more training data is used, the best results are not directly comparable. The middle point of the learning curve in Figure 1, which is trained with roughly 100K words, is at the same ballpark of (Xu et al., 2002). The contribution of this work is that the proposed characterbased parser does word-segmentation, POS tagging and parsing in a unified framework. It is the first attempt to our knowledge that syntactic information is used in word-segmentation. Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low. Without knowing and controlling testing conditions, it is nearly impossible to compare results in a meaningful way. Therefore, we will compare our approach with some related work only without commenting on segmentation accuracy. Wu and Tseng (1993) contains a good problem statement of Chinese wordsegmentation and also outlines a few segmentation algorithms. Our method is supervised in that the training data is manually labeled. Palmer (1997) uses transform-based learning (TBL) to correct an initial segmentation. Sp</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>Richard Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A stochastic finite-state wordsegmentation algorithm for Chinese. Computational Linguistics, 22(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Improving chinese tokenization with linguistic filters on statistical lexical acquisition.</title>
<date>1994</date>
<booktitle>In Fourth Conference on Applied Natural Language Processing,</booktitle>
<pages>180--181</pages>
<location>Stuttgart.</location>
<contexts>
<context position="1388" citStr="Wu and Fung, 1994" startWordPosition="210" endWordPosition="213">nces are pre-segmented. The paper studies the problem of parsing Chinese unsegmented sentences. The first motivation is that a character-based parser can be used directly in natural language applications that operate at character level, whereas a word-based parser requires a separate word-segmenter. The second and more important reason is that the availability of CTB, a large corpus with high quality syntactic annotations, provides us with an opportunity to create a highly-accurate word-segmenter. It is widely known that Chinese word-segmentation is a hard problem. There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower . The agreement between multiple human subjects is even lower (Wu and Fung, 1994). The reason is that human subjects may differ in segmenting things like personal names (whether family and given names should be one or two words), number and measure units and compound words, although these ambiguities do not change a human being’s understanding of a sentence. Low agreement between humans affects directly evaluation of machines’ performance (Wu and Fung, 1994) a</context>
<context position="23524" citStr="Wu and Fung, 1994" startWordPosition="3951" endWordPosition="3954">e used. Since our parser operates at character level, and more training data is used, the best results are not directly comparable. The middle point of the learning curve in Figure 1, which is trained with roughly 100K words, is at the same ballpark of (Xu et al., 2002). The contribution of this work is that the proposed characterbased parser does word-segmentation, POS tagging and parsing in a unified framework. It is the first attempt to our knowledge that syntactic information is used in word-segmentation. Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low. Without knowing and controlling testing conditions, it is nearly impossible to compare results in a meaningful way. Therefore, we will compare our approach with some related work only without commenting on segmentation accuracy. Wu and Tseng (1993) contains a good problem statement of Chinese wordsegmentation and also outlines a few segmentation algorithms. Our method is supervised in that the training data is manually labeled. Palmer (1997) uses transform-based learning (TBL) to correct an ini</context>
</contexts>
<marker>Wu, Fung, 1994</marker>
<rawString>Dekai Wu and Pascale Fung. 1994. Improving chinese tokenization with linguistic filters on statistical lexical acquisition. In Fourth Conference on Applied Natural Language Processing, pages 180–181, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zimin Wu</author>
<author>Gwyneth Tseng</author>
</authors>
<title>Chinese text segmentation for text retrieval: Achievements and problems.</title>
<date>1993</date>
<journal>Journal of The American Society for Information Science,</journal>
<volume>44</volume>
<issue>9</issue>
<contexts>
<context position="23873" citStr="Wu and Tseng (1993)" startWordPosition="4009" endWordPosition="4012">es word-segmentation, POS tagging and parsing in a unified framework. It is the first attempt to our knowledge that syntactic information is used in word-segmentation. Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low. Without knowing and controlling testing conditions, it is nearly impossible to compare results in a meaningful way. Therefore, we will compare our approach with some related work only without commenting on segmentation accuracy. Wu and Tseng (1993) contains a good problem statement of Chinese wordsegmentation and also outlines a few segmentation algorithms. Our method is supervised in that the training data is manually labeled. Palmer (1997) uses transform-based learning (TBL) to correct an initial segmentation. Sproat et al. (1996) employs stochastic finite state machines to find word boundaries. Luo and Roukos (1996) proposes to use a language model to select from ambiguous wordsegmentations. All these work assume that a lexicon or some manually segmented data or both are available. There are numerous work exploring semisupervised or </context>
</contexts>
<marker>Wu, Tseng, 1993</marker>
<rawString>Zimin Wu and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval: Achievements and problems. Journal of The American Society for Information Science, 44(9):532–542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
<author>M Palmer</author>
<author>N Xue</author>
<author>M E Okurowski</author>
<author>J Kovarik</author>
<author>F D Chiou</author>
<author>S Huang</author>
<author>T Kroch</author>
<author>M Marcus</author>
</authors>
<title>Developing guidelines and ensuring consistency for Chinese text annotation.</title>
<date>2000</date>
<booktitle>In Proc of the 2nd Intl. Conf. on Language Resources and Evaluation (LREC</booktitle>
<marker>Xia, Palmer, Xue, Okurowski, Kovarik, Chiou, Huang, Kroch, Marcus, 2000</marker>
<rawString>F. Xia, M. Palmer, N. Xue, M.E. Okurowski, J. Kovarik, F.D. Chiou, S. Huang, T. Kroch, and M. Marcus. 2000. Developing guidelines and ensuring consistency for Chinese text annotation. In Proc of the 2nd Intl. Conf. on Language Resources and Evaluation (LREC 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>Scott Miller</author>
<author>Ralph Weischedel</author>
</authors>
<title>A statistical parser for Chinese.</title>
<date>2002</date>
<booktitle>In Proc. Human Language Technology Workshop.</booktitle>
<contexts>
<context position="712" citStr="Xu et al., 2002" startWordPosition="108" endWordPosition="111">.J. Watson Research Center Yorktown Heights, NY 10598 xiaoluo@us.ibm.com Abstract 1 Introduction: Why Parsing Characters? After Linguistic Data Consortium (LDC) released the Chinese Treebank (CTB) developed at UPenn (Xia et al., 2000), various statistical Chinese parsers (Bikel and Chiang, 2000; Xu et al., 2002) have been built. Techniques used in parsing English have been shown working fairly well when applied to parsing Chinese text. As there is no word boundary in written Chinese text, CTB is manually segmented into words and then labeled. Parsers described in (Bikel and Chiang, 2000) and (Xu et al., 2002) operate at word-level with the assumption that input sentences are pre-segmented. The paper studies the problem of parsing Chinese unsegmented sentences. The first motivation is that a character-based parser can be used directly in natural language applications that operate at character level, whereas a word-based parser requires a separate word-segmenter. The second and more important reason is that the availability of CTB, a large corpus with high quality syntactic annotations, provides us with an opportunity to create a highly-accurate word-segmenter. It is widely known that Chinese word-s</context>
<context position="3849" citStr="Xu et al. (2002)" startWordPosition="598" endWordPosition="601">fore does effectively word-segmentation, partof-speech (POS) tagging and constituent labeling at the same time. Since syntactical information influences directly word-segmentation in the proposed character-based parser, CTB allows us to test whether or not syntactic information is useful for word-segmentation. A third advantage of parsing Chinese character streams is that Chinese words are more or less an open concept and the out-ofvocabulary (OOV) word rate is high. As morphology of the Chinese language is limited, extra care is needed to model unknown words when building a word-based model. Xu et al. (2002), for example, uses an independent corpus to derive word classes so that unknown words can be parsed reliably. Chinese characters, on the other hand, are almost closed. To demonstrate the OOV problem, we collect a word and character vocabulary from the first sentences of CTB, and compute their coverages on the corresponding word and character tokenization of the last of the corpus. The word-based OOV rate is while the character-based OOV rate is only . The first step of training a character-based parser is to convert word-based parse trees into characterbased trees. We derive character-level t</context>
<context position="22268" citStr="Xu et al. (2002)" startWordPosition="3736" endWordPosition="3739">information such as POS tags to help disambiguate word boundaries. What is surprising is that syntactic information beyond POS tags has little effect on wordsegmentation – there is practically no difference between the solid line (for the model trained with full parse trees) and the dashed-line (for the model trained with POS information) in Figure 3. This result suggests that most ambiguities of Chinese word boundaries can be resolved at lexical level, and highlevel syntactic information does not help much to word segmentation in the current parser. 5 Related Work Bikel and Chiang (2000) and Xu et al. (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. Bikel and Chiang (2000) in fact contains two parsers: one is a lexicalized probabilistic contextfree grammar (PCFG) similar to (Collins, 1997); the other is based on statistical TAG (Chiang, 2000). About F-measure is reported in (Bikel and Chiang, 2000). Xu et al. (2002) is also based on PCFG, but enhanced with lexical features derived from the ASBC corpus2. Xu et al. (2002) reports an overall F-measure when the same training and </context>
</contexts>
<marker>Xu, Miller, Weischedel, 2002</marker>
<rawString>Jinxi Xu, Scott Miller, and Ralph Weischedel. 2002. A statistical parser for Chinese. In Proc. Human Language Technology Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>