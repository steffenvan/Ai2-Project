<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021800">
<title confidence="0.891905">
CLaC-SentiPipe: SemEval2015 Subtasks 10 B,E, and Task 11
</title>
<author confidence="0.977649">
Canberk ¨Ozdemir and Sabine Bergler
</author>
<affiliation confidence="0.985914">
CLaC Labs, Concordia University
</affiliation>
<address confidence="0.6085825">
1455 de Maisonneuve Blvd West
Montreal, Quebec, Canada, H3G 1M8
</address>
<email confidence="0.997837">
ozdemir.berkin@gmail.com, bergler@cse.concordia.ca
</email>
<sectionHeader confidence="0.995635" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99720975">
CLaC Labs participated in two shared tasks
for SemEval2015, Task 10 (subtasks B and
E) and Task 11. The underlying system con-
figuration is nearly identical and consists of
two major components: a large Twitter lex-
icon compiled from tweets that carry cer-
tain selected hashtags (assumed to guaran-
tee a sentiment polarity) and then inducing
that same polarity for the words that occur
in the tweets. We also use standard senti-
ment lexica and combine the results. The lex-
ical sentiment features are further differenti-
ated according to some linguistic contexts in
which their triggers occur, including bigrams,
negation, modality, and dependency triples.
We studied feature combinations comprehen-
sively for their interoperability and effective-
ness on different datasets using the exhaustive
feature combination technique of (Shareghi
and Bergler, 2013a; Shareghi and Bergler,
2013b). For Subtask 10B we used a SVM, and
a decision tree regressor for Task 11. The re-
sulting systems ranked ninth for Subtask 10B,
fourth for Subtask 10E, and first for Task 11.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968033333333">
The field of Sentiment Analysis is in its second
phase: initially, the task was defined, annotation
standards, corpora, and feature resources were iden-
tified and provided to the research community (see
(Pang and Lee, 2008)). Now, we have regular com-
munity challenges such as the SemEval Twitter Sen-
timent shared tasks which allow us to compare dif-
ferent feature choice and combination across re-
search labs and across successive data sets. We de-
scribe here the systems we submitted to SemEval15
for Twitter Sentiment Analysis at the tweet level
(Task 10B) and Figurative Language in Twitter
(Task 11). The tasks and the design of the datasets
is described in detail in (Rosenthal et al., 2015) for
Task 10 and in (Ghosh et al., 2015) for Task 11. We
also submitted a sentiment lexicon transformed from
our in-house lexical resource for Task 10E.
Our system is based on a pipeline design in 5
major phases, described below. Following standard
text preprocessing, we use Stanford dependencies
(De Marneffe et al., 2006) and linguistic features
negation, modality and their scope in connection
with standard sentiment lexica from the literature
and an in-house lexical resource compiled with the
technique used for the NRC lexicon (Mohammad et
al., 2013). These features were successful in both
Task 10B (rank 9 on 40 for Twitter 2015 data, sev-
enth on 40 for Twitter 2015 sarcasm data) and Task
11 (rank 1 of 35 runs by 15 teams). Our sentiment
lexicon submitted to Task 10E ranked fourth of ten.
</bodyText>
<sectionHeader confidence="0.99488" genericHeader="introduction">
2 Pipeline Design
</sectionHeader>
<bodyText confidence="0.9914548">
CLaCSentiPipe is a pipeline system that attempts to
test the interoperability of different sentiment lexica
and a selected set of linguistic annotations.
The lexical resources used are aFinn (Nielsen,
2011), MPQA (Wilson et al., 2005), BingLiu (Hu
and Liu, 2004), and Gezi, our own lexical resource
described below.
Third party processing resources in our GATE en-
vironment (Cunningham et al., 2013) include a hy-
brid of Annie and CMU tokenizers (Cunningham
</bodyText>
<page confidence="0.988183">
479
</page>
<note confidence="0.9572495">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 479–485,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.989893441860465">
et al., 2002; Gimpel et al., 2011), named entity
recognition (Ritter et al., ), Stanford Parser Version
3.4.1 (Socher et al., 2013) and dependency module
(De Marneffe et al., 2006).
Linguistic notions used are negation and modal-
ity triggers (Kilicoglu, 2012; Rosenberg, 2013) and
scope (Rosenberg, 2013) as well as dependency re-
lations (De Marneffe et al., 2006).
Phase 1 Following tokenization, sentence split-
ting, POS tagging, and named entity recognition
(Ritter et al., ) (to fuse multi-word names into a sin-
gle token) and lookup in the sentiment lexica used,
we ignore Twitter-specific items (@name, URLs
... ) when parsing with the Stanford parser.
Phase 2 Using POS tags information for disam-
biguation, the prior polarity (value positive, neg-
ative, neutral and score where available) is deter-
mined for each token from each of the lexical re-
sources.
Phase 3 Based on the Stanford dependencies pro-
duced in Phase 1, we identify negation and modality
triggers and their scope (Rosenberg, 2013) and look
up PMI scores (Church and Hanks, 1990) for depen-
dency triples in the Gezi dependency resource.
Phase 4 The resulting features are the polarity
class according to each lexical resource, embedded-
ness in modality or negation, as well as sentiment
scores for each lexical token according to appro-
priate lexical resources; dependency score features
using PMI scores of dependency triples and their
types; dependency count features mapping PMI
scores into discrete polarity classes; ad hoc features
from specific annotations observed on training data.
Phase 5 The resulting feature space is grouped
into subsets of features in order to create fea-
ture combinations (Shareghi and Bergler, 2013a;
Shareghi and Bergler, 2013b) and processed with
Weka (Witten and Frank, 2011) libSVM (Chang
and Lin, 2011) with RBF kernel and parameters of
cost=5, gamma=0.001 and weights=[neutral=1; pos-
itive=2; negative=2.9] for Subtask 10B and M5P
(Wang and Witten, 1997), a decision tree regressor,
to predict continuos values1 for Task 11.
</bodyText>
<footnote confidence="0.995712">
1http://www.opentox.org/dev/
documentation/components/m5p
</footnote>
<sectionHeader confidence="0.984862" genericHeader="method">
3 Lexica
</sectionHeader>
<bodyText confidence="0.999997043478261">
In the past two years, the team that developed the
NRC lexicon (Mohammad et al., 2013) dominated
the Twitter sentiment task and our first question was:
is the NRC lexicon itself the ultimate resource, or is
the technique that derived it the essential lesson, and
can that technique be reused to similar effect. We
compiled a similar resource, Gezi, and compared it
with the NRC lexicon, but also much smaller tradi-
tional resources, namely Bing Liu’s dictionary (Hu
and Liu, 2004), MPQA (Wiebe et al., 2006), and
aFinn (Nielsen, 2011), a manually compiled dic-
tionary. Extensive ablation studies showed that all
the resulting dictionaries contributed to the best per-
forming feature combination, but that the contribu-
tion of the lexica was not proportional to size (sug-
gesting significant overlap). Surprisingly, aFinn, the
smallest lexicon, by itself performs better than any
of the other dictionaries by themselves and it is the
one stable component in all our top performing fea-
ture combinations. In our competition system, we
did not use the NRC lexicon, in order to assess
whether Gezi, derived in a similar manner, was per-
forming as well.
</bodyText>
<sectionHeader confidence="0.981003" genericHeader="method">
4 Gezi Lexical Resources
</sectionHeader>
<bodyText confidence="0.999846052631579">
Gezi corpus To assess whether the strong perfor-
mance of the NRC lexicon can be replicated and
enhanced, we used their technique to compile a
new resource, Gezi, by selecting positive and neg-
ative hashtags from the Twitter API from Decem-
ber 2013 to May 2014. The set of 35 positive and
34 negative seed hashtags were obtained from the
Oxford American Writer’s Thesaurus (Moody and
Lindberg, 2012) by expanding the adjectives good
and bad, resulting in nearly 20 million tweets, from
which unigram, bigram, and dependency triple in-
formation was collected.
After removing retweets, tweets with conflicting
hashtags, and tweets with little or no content words,
as well as all URLs in tweets, we annotate the re-
maining tweets with the polarity class of their seed
hashtag for our Gezi tweet corpus and project the
tweet polarity onto each token inside the tweet for
our unigram and bigram features.
</bodyText>
<page confidence="0.986283">
480
</page>
<bodyText confidence="0.999461861111111">
Data processing After applying Phase 1 to the
Gezi corpus the same way we use it in our main
pipeline, we also parse tweets and identify negation
triggers and their scopes. Then we record counts
of unigrams, bigrams and dependency triples (type-
head-modifier) in the context they occurred by also
taking negation scope into consideration. For in-
stance; if a term occurs in a positive-annotated tweet
where it is not in the scope of a negation, its positive
count is incremented; if it is in a positive-annotated
tweet and in the scope of negation, then its negated-
positive count is incremented. This reflects the dif-
ferent contexts in which the terms of the lexicon
were found and associates them with the resulting
sentiment. In addition, we keep terms with different
POS tags separate in the resources. The counts of
the terms in the positive, negative, negated-positive
and negated-negative categories for the entire col-
lection are then transformed into association scores
using pointwise mutual information.
NRC and Gezi A quick comparison of Gezi and
NRC unigrams and bigrams on three years of Sem-
Eval data in Table 1 shows their performance is
close, with a small advantage for the much larger
Gezi lexicon. Analyzing overlap of NRC (25721
unigrams) and Gezi (220399 unigrams), we find
they agree only on 13957 of 16868 shared entries
(both have higher agreement rates with aFinn!)
We interpret these findings as confirmation that
the NRC technique can profitably be replicated and
thus be used to create sentiment lexica that are big-
ger or smaller, that span a relevant period or con-
tain relevant topics. We also conclude that size alone
does not change results proportionally, as these large
lexica clearly expand into the long tail of infre-
quently used words.
</bodyText>
<table confidence="0.991891714285714">
SemEval Test data
2015 2014 2013
NRC unigrams 49.83 52.39 50.9
NRC bigrams 51.31 53.48 52.31
Gezi unigrams 54.65 60.81 57.86
Gezi bigrams 51.14 56.40 50.45
all four combined 56.07 64.26 59.60
</table>
<tableCaption confidence="0.999968">
Table 1: Comparison NRC and Gezi.
</tableCaption>
<sectionHeader confidence="0.959796" genericHeader="method">
5 Features and Feature Space
</sectionHeader>
<bodyText confidence="0.99995396969697">
Primary Features Lexicon features (aFinn, NRC,
... ) encode the prior polarity of the terms in a lexi-
con.
Recent work in our lab on embedding predication
(Kilicoglu, 2012), negation (Rosenberg, 2013), and
modality (Rosenberg et al., 2012) highlighted that
syntactically embedding constructions exert an in-
fluence over the meaning of constituents, so we ap-
plied this insight to sentiment values. On the 2013
dataset, most (of the 6822) tweets contained named
entities (6286), as expected, but surprisingly the sec-
ond most frequent feature was modality (1785), fol-
lowed by negation (1356). Thus these features have
the potential to influence the results to a measurable
degree.
These linguistic context features were encoded
as occurrences. The general schema of this in-
tegration for our system can be formulated as
polarityClass,lexicalResource,lin-
guisticScope, where polarityClass is
one of positive, negative, neutral, strong positive,
strong negative, lexicalResource represents a
lexical resource and linguisticScope is one of
none, negation, modality, negation+modality. For
each tweet token, its prior polarity and any scope
annotation is checked (a score feature is created if a
lexicon provides score information for its terms).
The features for each feature type are aggregated
into tweet-level aggregates, creating a compact fea-
ture space (94 features for Subtask 10B, 90 for Task
11).
Table 2 shows the primary features created from
the aFinn lexical resource for Example 1.
</bodyText>
<listItem confidence="0.5391">
(1) El Classico on a Sunday Night isn’t per-
fect for the Monday Morning !!
</listItem>
<bodyText confidence="0.998479">
This particular example has only one sentiment
trigger in aFinn, perfect, with aFinn score=3 and
positive-aFinn=1 (it is a strong positive sentiment
trigger in the lexicon). In the context of Example 1,
however, it occurs in the scope of a negation, thus
the score is multiplied by -0.5 and the count fea-
ture positive-aFinn-negated=1 is activated instead,
resulting in the feature assignment of Table 2.
Secondary Features The contrastive conjunction
but and a list of contrastive adverbs (although, etc)
</bodyText>
<page confidence="0.998911">
481
</page>
<tableCaption confidence="0.998873">
Table 2: aFinn features for Example 1.
</tableCaption>
<bodyText confidence="0.999531714285714">
each constitute a feature, as do named entities. Ad-
ditional ad hoc features are some special Twitter-
specific POS tags (i.e. emphasis from !!!!!), special
phrases indicative of sentiment (can’t wait). We also
found the first and last token in a tweet to carry po-
tentially special meaning, as well as the association
scores between the highest and lowest sentiment car-
riers in a tweet.
Feature Combinations We create feature spaces
for each combination of feature subsets described
above and we experiment on each combination. The
submitted feature combinations for Subtask 10B and
Task 11 were selected using the exhaustive feature
combination technique of (Shareghi and Bergler,
</bodyText>
<table confidence="0.998623266666667">
2013a; Shareghi and Bergler, 2013b). # feat’s
Primary Feature Subsets
aFinn 9
MPQA 12
BingLiu 8
NRC unigrams 17
NRC bigrams 17
Gezi unigrams 17
Gezi bigrams 17
dependency scores 13
dependency counts 8
Secondary Feature Subsets 9
pos tag based scores and counts
frequencies of specific annotations 12
position and top-lowest scores 6
</table>
<tableCaption confidence="0.876745">
Table 3: Feature subset bundles.
Table 3 lists the feature bundles used in our abla-
tion studies.
</tableCaption>
<sectionHeader confidence="0.8988265" genericHeader="method">
6 Subtask 10B: Polarity Classification of
Tweets
</sectionHeader>
<bodyText confidence="0.998430351351351">
The task is a 3-way classification problem of la-
belling a tweet as positive, neutral, or negative, see
(Rosenthal et al., 2015) for a detailed description.
We trained an SVM classifier for our experiments
using last year’s test sets for development. Perform-
ing manual feature selection, we selected not the fea-
ture combination that performed best on the train-
ing data but instead one that was close to the top
on 2015 training data and both, 2014 and 2013 test
data (for robustness) but that did not include NRC
data (to better assess Gezi). The competition system
included aFinn, MPQA, Bing Liu, Gezi unigrams
and dependency based features in addition to all sec-
ondary features listed above.
Results The task of assigning sentiment to a tweet
attracted the most participants. CLaC-SentiPipe
ranked 9 of 40, a very strong placement considering
less than 3% separated our results from the top rank-
ing one. A comparison of the competing systems
on the past two years’ data shows that our system
ranked 7 on 2013 Twitter data, 10 on 2014 Twitter
data, 6 on 2014 Live Journal data, 18 on SMS mes-
sages from 2013, and 10 on Twitter 2014 Sarcasm
data. This demonstrates robustness in performance.
The detailed official results are shown in Table 4.
The best performing system dips to rank 12 and
13 for the LiveJournal and Sarcasm tasks of the
previous years, which indicates that the different
datasets compared show a certain difference, but not
a big one. The very close performance of the sys-
tems in the top quarter on this task (less than 3%
difference) suggests that the different approaches are
drowned out by the constancy in the datasets: we
may have reached the beginning of the long tail at
this margin, where improvements contribute only
small amounts and are not individually measurable
in the general task.
</bodyText>
<sectionHeader confidence="0.975589" genericHeader="method">
7 Subtask 10E: Determining Strength of
</sectionHeader>
<subsectionHeader confidence="0.726286">
Association of Terms
</subsectionHeader>
<bodyText confidence="0.992422928571428">
SemEval 2015 Subtask 10E was a pilot task re-
questing association scores of terms extracted from
tweets. The test set consisted of words or phrases
that had to be associated with scores between 0 and
feature value
positive-aFinn 0
positive-aFinn-negated 1
positive-aFinn-mod 0
positive-aFinn-mod-negated 0
negative-aFinn 0
negative-aFinn-negated 0
negative-aFinn-mod 0
negative-aFinn-mod-negated 0
aFinn-score -1.5
</bodyText>
<page confidence="0.996087">
482
</page>
<table confidence="0.999571111111111">
dataset P positive P negative P neutral F1 overall
R F1 R F1 R F1
Twitter2015 75.58 63.20 68.84 43.51 75.34 55.17 66.63 60.08 63.19 62.00
Twitter2015-sarcasm 55.56 55.56 55.56 61.54 61.54 61.54 43.75 43.75 43.75 58.55
LiveJournal2014 79.33 66.51 72.36 68.39 82.57 74.81 67.87 68.86 68.36 73.59
SMS2013 59.26 68.29 63.46 54.39 73.86 62.65 83.55 68.60 75.34 63.05
Twitter2013 73.45 75.13 74.28 59.50 75.54 66.57 75.66 66.52 70.80 70.42
Twitter2014 78.76 70.98 74.67 58.53 74.75 65.65 63.10 66.97 64.97 70.16
Twitter2014Sarcasm 50.91 84.85 63.64 90.91 25.00 39.22 40.00 61.54 48.48 51.43
</table>
<tableCaption confidence="0.99973">
Table 4: Official CLaC-SentiPipe results for Task 10B: rank 9.
</tableCaption>
<bodyText confidence="0.9955525">
1 where 1 stands for maximum association with pos-
itive sentiment and 0 does for maximum association
with negative sentiment.
We followed a simple, rule-based approach:
</bodyText>
<listItem confidence="0.999257">
1. aFinn sentiment scores and Gezi (unigrams and
bigrams) PMI values are used
2. if a term is part of a bigram, the unigram sen-
timent trigger and negation annotations are re-
moved, if they exist
3. if a trigger is in negation scope, its prior senti-
ment score is multiplied with -0.5
4. if there is more than one sentiment trigger per
term, the triggers’ scores are summed up
5. each prior sentiment score is scaled to [0,1]
6. if there is no trigger for a term, score is 0.5
</listItem>
<bodyText confidence="0.9933905">
Results The evaluation metrics are Kendall and
Spearman rank correlation coefficients (Nelson,
2001) for subtask 10E between gold values of words
or phrases and predicted values. Gold values are hu-
man judgements from the compilation of the NRC
lexicon (Kiritchenko et al., 2014).
Our simple rule-based and lexica-driven system
submitted for Task 10E ranked 4th among 10 sub-
mitted systems in both correlation coefficient evalu-
ations. Our Kendall rank correlation coefficient re-
sult is 0.584 where all results range between 0.625
and 0.254, and our Spearman rank correlation coef-
ficient result is 0.777 where results range between
0.817 and 0.373.
</bodyText>
<sectionHeader confidence="0.745326" genericHeader="method">
8 Task 11: Figurative Language
</sectionHeader>
<bodyText confidence="0.999961942857143">
Figurative language permeates daily life and so-
cial media, conveying non-explicit meanings using
tropes such as irony, sarcasm, or metaphor. How-
ever, understanding these phenomena is not trivial
for sentiment analysis systems, that usually assume
that each word has only one (literal) meaning and an
a priori sentiment value.
SemEval 2015 Subtask 11 Sentiment Analysis of
Figurative Language in Twitter was organized for
the first time this year (Ghosh et al., 2015). The
challenge dataset contains tweets that contain at
least one instance of figurative language and non-
figurative tweets (labelled other). The labels are
in form of sentiment scores obtained from human
judgements. The dataset distinguished 3 types of
figurative language, Sarcasm, Irony and Metaphor.
The organizers made the tweet data available with
both integer-based and float-based scores.
We tested the robustness of our linguistic em-
bedding features by submitting the same pipeline
for text processing, feature creation and the exhaus-
tive feature combination evaluation technique of
(Shareghi and Bergler, 2013a; Shareghi and Bergler,
2013b) via 10-fold cross validation on the training
set with M5P (Wang and Witten, 1997), a deci-
sion tree regressor. We evaluated 10-fold cross val-
idation predictions by calculating correlation coeffi-
cients (Nelson, 2001).
The extracted features are the same as the features
we extracted for Subtask 10B. The only difference
is the gold labels since Task 11 requires continuous
classes while these are discrete in Subtask 10B.
We used float-based gold labels for training data
and treat the problem as a regression problem.
The output of our system’s predictions were then
</bodyText>
<page confidence="0.997952">
483
</page>
<table confidence="0.997492166666667">
MSE
Overall Sarcasm Irony Metaphor Other
2.117 1.023 0.779 3.155 3.411
Cosine
Overall Sarcasm Irony Metaphor Other
0.758 0.892 0.904 0.655 0.584
</table>
<tableCaption confidence="0.999762">
Table 5: CLaC-SentiPipe in Task 11: rank 1.
</tableCaption>
<bodyText confidence="0.989643416666667">
rounded to integer values, as required.
Results The single submission from CLaC ranked
first in both, cosine and mean squared error mea-
sures. There were wide margins between the first
three systems.
The different types of figurative language were
scored individually, see Table 5. In mean square
error, CLaC ranked first in the overall score, the
metaphor, and other categories. For the cosine mea-
sure, the third system of a competitor obtained best
performance in the other category, but with a high
mean squared error.
The second best system, interestingly, does not
hold best performance in a single category, which
demonstrates the good performance of a steady ap-
proach. The third ranked team obtained best per-
formance for irony both in cosine similarity and
least squared error, but not in their best performing
(ranked) submission.
Our system has shown robustness across tasks and
the linguistic features encoded have been validated
for their adaptability to figurative language.
Further analysis We compared our technique
with automatic forward feature selection, which in-
terestingly selected the following six features: Gezi
strong negative unigram, Gezi strong negative bi-
gram, NRC strong positive unigram, NRC strong
positive bigram: all four under scopes of both nega-
tion and modality; average scores of hashtag senti-
ment; counts of named entities. The results for this
feature set would have been 66.41, which places it
between the third and fourth-ranked systems in the
competition.
This reinforces the observation that negation and
modality contexts interoperate well with strong lex-
icon scores and are essential.
</bodyText>
<sectionHeader confidence="0.95876" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999883">
CLaCSentiPipe showed a strong top quarter perfor-
mance in sentiment annotation of tweets and in its
submitted lexicon, but it excelled at figurative lan-
guage. We claim that the use of linguistic fea-
tures negation, modality, embedding and depen-
dency triples provides a wider context to the a pri-
ori sentiment values found in a lexicon. We com-
bined our own large Twitter derived lexicon (Gezi)
with standard resources for a range of a priori values.
Gezi used the technique of extracting tweets with
hashtags that are believed to guarantee sentiment po-
larity and inducing sentiment values for the words
contained accordingly. This technique has been used
for the NRC lexicon and here we showed that it can
be reimplemented with good success. Our lexicon
was derived from a Twitter stream of two years ago.
The drastically lower performance of all systems on
2015 test data as compared to 2014 or 2013 data sug-
gests that some events or story lines in the 2015 data
use sentiment triggers differently.
Closeness of results suggest that the systems
largely cover common ground, and that their special-
izations now fall in the area of the long tail, where
incremental improvements become small and are
hard to detect and measure. This confirms the obser-
vation that sentiment carrying words form a fuzzy
set as demonstrated by (Andreevskaia and Bergler,
2006).
It is thus especially pleasing that the same system
performed best on Task 11, sentiment annotation
of tweets containing figurative language of various
forms: irony, sarcasm, metaphor, or other. Here, we
feel the explicit annotation of the embedding con-
structs has given the system the required degree of
freedom to adapt to the non-literal usage. We inter-
pret the fact that our features had not been designed
specifically for this task (but were repurposed from
Task 10 and merely retrained) as an indicator of ro-
bustness and a strong endorsement of our linguisti-
cally inspired features.
</bodyText>
<sectionHeader confidence="0.998294" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.93387975">
This work has been funded by a grant from Canada’s
Natural Science and Engineering Research Council
(NSERC) and has benefitted from collaboration with
Marc-Andr´e Faucher and Nasrin Baratalipour.
</bodyText>
<page confidence="0.998602">
484
</page>
<sectionHeader confidence="0.990109" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999169504672897">
Alina Andreevskaia and Sabine Bergler. 2006. Mining
WordNet for a fuzzy sentiment: Sentiment tag extrac-
tion from WordNet glosses. In Proceedings of the llth
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-06).
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. Transactions
on Intelligent Systems and Technology (TIST), 2(3).
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational linguistics, 16(1).
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. In Proceedings
of the 40th Anniversary Meeting of the Association for
Computational Linguistics (ACL’02).
Hamish Cunningham, Valentin Tablan, Angus Roberts,
and Kalina Bontcheva. 2013. Getting more out
of biomedical documents with GATE’s full lifecycle
open source text analytics. PLoS Computational Biol-
ogy, 9(2).
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC 2006).
Aniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso,
Ekaterina Shutova, Antonio Reyes, and John Barn-
den. 2015. SemEval-2015 task 11: Sentiment anal-
ysis of figurative language in Twitter. In Proceedings
of the International Workshop on Semantic Evaluation
(SemEval-2015).
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL/HLT 2011).
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Inter-
national Conference on Knowledge Discovery &amp; Data
Mining (KDD-2004).
Halil Kilicoglu. 2012. Embedding Predications. Ph.D.
thesis, Concordia University.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Moham-
mad. 2014. Sentiment analysis of short informal texts.
Journal of Artificial Intelligence Research, 50.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings of
the Seventh International Workshop on Semantic Eval-
uation (SemEval-2013).
Rick Moody and Christine A Lindberg. 2012. Oxford
American Writer’s Thesaurus. OUP.
Roger B. Nelson. 2001. Kendall tau metric. Encyclopae-
dia of Mathematics, 3.
Finn A. Nielsen. 2011. A new ANEW: Evaluation of a
word list for sentiment analysis in microblogs. In Pro-
ceedings of ESWC2011 Workshop on ‘Making Sense
of Microposts’: Big Things Come in Small Packages.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2).
Alan Ritter, Sam Clark, and Oren Etzion. Named entity
recognition in Tweets: an experimental study.
Sabine Rosenberg, Halil Kilicoglu, and Sabine Bergler.
2012. CLaC Labs: Processing modality and negation.
In Working Notes for QA4MRE Pilot Task at CLEF
2012.
Sabine Rosenberg. 2013. Negation triggers and their
scope. Master’s thesis, Concordia University.
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M. Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. SemEval-2015 Task 10: Sentiment anal-
ysis in Twitter. In Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval2015).
Ehsan Shareghi and Sabine Bergler. 2013a. CLaC-
CORE: Exhaustive feature combination for measur-
ing textual similarity. In Proceedings of *SEM 2013
Shared Task STS.
Ehsan Shareghi and Sabine Bergler. 2013b. Feature
combination for sentence similarity. In Proceedings
of the 26th Canadian Conference on Artificial Intelli-
gence (AI 2013). Springer Berlin. LNAI 7884.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with compositional
vector grammars. In Proceedings of the 51st Meeting
of the Association for Computational Linguistics (ACL
2013).
Yong Wang and Ian H. Witten. 1997. Induction of model
trees for predicting continuous classes. In Poster in
Proceedings of the 9th European Conference on Ma-
chine Learning. Faculty of Informatics and Statistics,
Prague.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2006.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT-EMNLP.
Ian H. Witten and Eibe Frank. 2011. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann Publishers, 3rd edition.
</reference>
<page confidence="0.999062">
485
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.147464">
<title confidence="0.463942">CLaC-SentiPipe: SemEval2015 Subtasks 10 B,E, and Task 11 and Sabine</title>
<address confidence="0.955667">CLaC Labs, Concordia 1455 de Maisonneuve Blvd Montreal, Quebec, Canada, H3G</address>
<email confidence="0.989034">ozdemir.berkin@gmail.com,bergler@cse.concordia.ca</email>
<abstract confidence="0.96408208">CLaC Labs participated in two shared tasks for SemEval2015, Task 10 (subtasks B and E) and Task 11. The underlying system configuration is nearly identical and consists of two major components: a large Twitter lexicon compiled from tweets that carry certain selected hashtags (assumed to guarantee a sentiment polarity) and then inducing that same polarity for the words that occur in the tweets. We also use standard sentiment lexica and combine the results. The lexical sentiment features are further differentiated according to some linguistic contexts in which their triggers occur, including bigrams, negation, modality, and dependency triples. We studied feature combinations comprehensively for their interoperability and effectiveness on different datasets using the exhaustive feature combination technique of (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b). For Subtask 10B we used a SVM, and a decision tree regressor for Task 11. The resulting systems ranked ninth for Subtask 10B, fourth for Subtask 10E, and first for Task 11.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alina Andreevskaia</author>
<author>Sabine Bergler</author>
</authors>
<title>Mining WordNet for a fuzzy sentiment: Sentiment tag extraction from WordNet glosses.</title>
<date>2006</date>
<booktitle>In Proceedings of the llth Conference of the European Chapter of the Association for Computational Linguistics (EACL-06).</booktitle>
<contexts>
<context position="22038" citStr="Andreevskaia and Bergler, 2006" startWordPosition="3527" endWordPosition="3530"> with good success. Our lexicon was derived from a Twitter stream of two years ago. The drastically lower performance of all systems on 2015 test data as compared to 2014 or 2013 data suggests that some events or story lines in the 2015 data use sentiment triggers differently. Closeness of results suggest that the systems largely cover common ground, and that their specializations now fall in the area of the long tail, where incremental improvements become small and are hard to detect and measure. This confirms the observation that sentiment carrying words form a fuzzy set as demonstrated by (Andreevskaia and Bergler, 2006). It is thus especially pleasing that the same system performed best on Task 11, sentiment annotation of tweets containing figurative language of various forms: irony, sarcasm, metaphor, or other. Here, we feel the explicit annotation of the embedding constructs has given the system the required degree of freedom to adapt to the non-literal usage. We interpret the fact that our features had not been designed specifically for this task (but were repurposed from Task 10 and merely retrained) as an indicator of robustness and a strong endorsement of our linguistically inspired features. Acknowled</context>
</contexts>
<marker>Andreevskaia, Bergler, 2006</marker>
<rawString>Alina Andreevskaia and Sabine Bergler. 2006. Mining WordNet for a fuzzy sentiment: Sentiment tag extraction from WordNet glosses. In Proceedings of the llth Conference of the European Chapter of the Association for Computational Linguistics (EACL-06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<booktitle>Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>2--3</pages>
<contexts>
<context position="5270" citStr="Chang and Lin, 2011" startWordPosition="831" endWordPosition="834"> resource, embeddedness in modality or negation, as well as sentiment scores for each lexical token according to appropriate lexical resources; dependency score features using PMI scores of dependency triples and their types; dependency count features mapping PMI scores into discrete polarity classes; ad hoc features from specific annotations observed on training data. Phase 5 The resulting feature space is grouped into subsets of features in order to create feature combinations (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b) and processed with Weka (Witten and Frank, 2011) libSVM (Chang and Lin, 2011) with RBF kernel and parameters of cost=5, gamma=0.001 and weights=[neutral=1; positive=2; negative=2.9] for Subtask 10B and M5P (Wang and Witten, 1997), a decision tree regressor, to predict continuos values1 for Task 11. 1http://www.opentox.org/dev/ documentation/components/m5p 3 Lexica In the past two years, the team that developed the NRC lexicon (Mohammad et al., 2013) dominated the Twitter sentiment task and our first question was: is the NRC lexicon itself the ultimate resource, or is the technique that derived it the essential lesson, and can that technique be reused to similar effect.</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. Transactions on Intelligent Systems and Technology (TIST), 2(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="4514" citStr="Church and Hanks, 1990" startWordPosition="715" endWordPosition="718">agging, and named entity recognition (Ritter et al., ) (to fuse multi-word names into a single token) and lookup in the sentiment lexica used, we ignore Twitter-specific items (@name, URLs ... ) when parsing with the Stanford parser. Phase 2 Using POS tags information for disambiguation, the prior polarity (value positive, negative, neutral and score where available) is determined for each token from each of the lexical resources. Phase 3 Based on the Stanford dependencies produced in Phase 1, we identify negation and modality triggers and their scope (Rosenberg, 2013) and look up PMI scores (Church and Hanks, 1990) for dependency triples in the Gezi dependency resource. Phase 4 The resulting features are the polarity class according to each lexical resource, embeddedness in modality or negation, as well as sentiment scores for each lexical token according to appropriate lexical resources; dependency score features using PMI scores of dependency triples and their types; dependency count features mapping PMI scores into discrete polarity classes; ad hoc features from specific annotations observed on training data. Phase 5 The resulting feature space is grouped into subsets of features in order to create f</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
<author>Diana Maynard</author>
<author>Kalina Bontcheva</author>
<author>Valentin Tablan</author>
</authors>
<title>GATE: A framework and graphical development environment for robust NLP tools and applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL’02).</booktitle>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: A framework and graphical development environment for robust NLP tools and applications. In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL’02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
<author>Valentin Tablan</author>
<author>Angus Roberts</author>
<author>Kalina Bontcheva</author>
</authors>
<title>Getting more out of biomedical documents with GATE’s full lifecycle open source text analytics.</title>
<date>2013</date>
<journal>PLoS Computational Biology,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="3225" citStr="Cunningham et al., 2013" startWordPosition="510" endWordPosition="513"> both Task 10B (rank 9 on 40 for Twitter 2015 data, seventh on 40 for Twitter 2015 sarcasm data) and Task 11 (rank 1 of 35 runs by 15 teams). Our sentiment lexicon submitted to Task 10E ranked fourth of ten. 2 Pipeline Design CLaCSentiPipe is a pipeline system that attempts to test the interoperability of different sentiment lexica and a selected set of linguistic annotations. The lexical resources used are aFinn (Nielsen, 2011), MPQA (Wilson et al., 2005), BingLiu (Hu and Liu, 2004), and Gezi, our own lexical resource described below. Third party processing resources in our GATE environment (Cunningham et al., 2013) include a hybrid of Annie and CMU tokenizers (Cunningham 479 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 479–485, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics et al., 2002; Gimpel et al., 2011), named entity recognition (Ritter et al., ), Stanford Parser Version 3.4.1 (Socher et al., 2013) and dependency module (De Marneffe et al., 2006). Linguistic notions used are negation and modality triggers (Kilicoglu, 2012; Rosenberg, 2013) and scope (Rosenberg, 2013) as well as dependency relations (De Marneffe et al.</context>
</contexts>
<marker>Cunningham, Tablan, Roberts, Bontcheva, 2013</marker>
<rawString>Hamish Cunningham, Valentin Tablan, Angus Roberts, and Kalina Bontcheva. 2013. Getting more out of biomedical documents with GATE’s full lifecycle open source text analytics. PLoS Computational Biology, 9(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniruddha Ghosh</author>
<author>Guofu Li</author>
<author>Tony Veale</author>
<author>Paolo Rosso</author>
<author>Ekaterina Shutova</author>
<author>Antonio Reyes</author>
<author>John Barnden</author>
</authors>
<title>SemEval-2015 task 11: Sentiment analysis of figurative language in Twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2015).</booktitle>
<contexts>
<context position="2048" citStr="Ghosh et al., 2015" startWordPosition="319" endWordPosition="322">rpora, and feature resources were identified and provided to the research community (see (Pang and Lee, 2008)). Now, we have regular community challenges such as the SemEval Twitter Sentiment shared tasks which allow us to compare different feature choice and combination across research labs and across successive data sets. We describe here the systems we submitted to SemEval15 for Twitter Sentiment Analysis at the tweet level (Task 10B) and Figurative Language in Twitter (Task 11). The tasks and the design of the datasets is described in detail in (Rosenthal et al., 2015) for Task 10 and in (Ghosh et al., 2015) for Task 11. We also submitted a sentiment lexicon transformed from our in-house lexical resource for Task 10E. Our system is based on a pipeline design in 5 major phases, described below. Following standard text preprocessing, we use Stanford dependencies (De Marneffe et al., 2006) and linguistic features negation, modality and their scope in connection with standard sentiment lexica from the literature and an in-house lexical resource compiled with the technique used for the NRC lexicon (Mohammad et al., 2013). These features were successful in both Task 10B (rank 9 on 40 for Twitter 2015 d</context>
<context position="17638" citStr="Ghosh et al., 2015" startWordPosition="2830" endWordPosition="2833">625 and 0.254, and our Spearman rank correlation coefficient result is 0.777 where results range between 0.817 and 0.373. 8 Task 11: Figurative Language Figurative language permeates daily life and social media, conveying non-explicit meanings using tropes such as irony, sarcasm, or metaphor. However, understanding these phenomena is not trivial for sentiment analysis systems, that usually assume that each word has only one (literal) meaning and an a priori sentiment value. SemEval 2015 Subtask 11 Sentiment Analysis of Figurative Language in Twitter was organized for the first time this year (Ghosh et al., 2015). The challenge dataset contains tweets that contain at least one instance of figurative language and nonfigurative tweets (labelled other). The labels are in form of sentiment scores obtained from human judgements. The dataset distinguished 3 types of figurative language, Sarcasm, Irony and Metaphor. The organizers made the tweet data available with both integer-based and float-based scores. We tested the robustness of our linguistic embedding features by submitting the same pipeline for text processing, feature creation and the exhaustive feature combination evaluation technique of (Shareghi</context>
</contexts>
<marker>Ghosh, Li, Veale, Rosso, Shutova, Reyes, Barnden, 2015</marker>
<rawString>Aniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso, Ekaterina Shutova, Antonio Reyes, and John Barnden. 2015. SemEval-2015 task 11: Sentiment analysis of figurative language in Twitter. In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2015).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL/HLT</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL/HLT 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Knowledge Discovery &amp; Data Mining (KDD-2004).</booktitle>
<contexts>
<context position="3089" citStr="Hu and Liu, 2004" startWordPosition="489" endWordPosition="492"> lexical resource compiled with the technique used for the NRC lexicon (Mohammad et al., 2013). These features were successful in both Task 10B (rank 9 on 40 for Twitter 2015 data, seventh on 40 for Twitter 2015 sarcasm data) and Task 11 (rank 1 of 35 runs by 15 teams). Our sentiment lexicon submitted to Task 10E ranked fourth of ten. 2 Pipeline Design CLaCSentiPipe is a pipeline system that attempts to test the interoperability of different sentiment lexica and a selected set of linguistic annotations. The lexical resources used are aFinn (Nielsen, 2011), MPQA (Wilson et al., 2005), BingLiu (Hu and Liu, 2004), and Gezi, our own lexical resource described below. Third party processing resources in our GATE environment (Cunningham et al., 2013) include a hybrid of Annie and CMU tokenizers (Cunningham 479 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 479–485, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics et al., 2002; Gimpel et al., 2011), named entity recognition (Ritter et al., ), Stanford Parser Version 3.4.1 (Socher et al., 2013) and dependency module (De Marneffe et al., 2006). Linguistic notions used are negation </context>
<context position="6039" citStr="Hu and Liu, 2004" startWordPosition="949" endWordPosition="952">, a decision tree regressor, to predict continuos values1 for Task 11. 1http://www.opentox.org/dev/ documentation/components/m5p 3 Lexica In the past two years, the team that developed the NRC lexicon (Mohammad et al., 2013) dominated the Twitter sentiment task and our first question was: is the NRC lexicon itself the ultimate resource, or is the technique that derived it the essential lesson, and can that technique be reused to similar effect. We compiled a similar resource, Gezi, and compared it with the NRC lexicon, but also much smaller traditional resources, namely Bing Liu’s dictionary (Hu and Liu, 2004), MPQA (Wiebe et al., 2006), and aFinn (Nielsen, 2011), a manually compiled dictionary. Extensive ablation studies showed that all the resulting dictionaries contributed to the best performing feature combination, but that the contribution of the lexica was not proportional to size (suggesting significant overlap). Surprisingly, aFinn, the smallest lexicon, by itself performs better than any of the other dictionaries by themselves and it is the one stable component in all our top performing feature combinations. In our competition system, we did not use the NRC lexicon, in order to assess whet</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the International Conference on Knowledge Discovery &amp; Data Mining (KDD-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
</authors>
<title>Embedding Predications.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Concordia University.</institution>
<contexts>
<context position="3727" citStr="Kilicoglu, 2012" startWordPosition="587" endWordPosition="588">ical resource described below. Third party processing resources in our GATE environment (Cunningham et al., 2013) include a hybrid of Annie and CMU tokenizers (Cunningham 479 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 479–485, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics et al., 2002; Gimpel et al., 2011), named entity recognition (Ritter et al., ), Stanford Parser Version 3.4.1 (Socher et al., 2013) and dependency module (De Marneffe et al., 2006). Linguistic notions used are negation and modality triggers (Kilicoglu, 2012; Rosenberg, 2013) and scope (Rosenberg, 2013) as well as dependency relations (De Marneffe et al., 2006). Phase 1 Following tokenization, sentence splitting, POS tagging, and named entity recognition (Ritter et al., ) (to fuse multi-word names into a single token) and lookup in the sentiment lexica used, we ignore Twitter-specific items (@name, URLs ... ) when parsing with the Stanford parser. Phase 2 Using POS tags information for disambiguation, the prior polarity (value positive, negative, neutral and score where available) is determined for each token from each of the lexical resources. P</context>
<context position="9819" citStr="Kilicoglu, 2012" startWordPosition="1579" endWordPosition="1580">riod or contain relevant topics. We also conclude that size alone does not change results proportionally, as these large lexica clearly expand into the long tail of infrequently used words. SemEval Test data 2015 2014 2013 NRC unigrams 49.83 52.39 50.9 NRC bigrams 51.31 53.48 52.31 Gezi unigrams 54.65 60.81 57.86 Gezi bigrams 51.14 56.40 50.45 all four combined 56.07 64.26 59.60 Table 1: Comparison NRC and Gezi. 5 Features and Feature Space Primary Features Lexicon features (aFinn, NRC, ... ) encode the prior polarity of the terms in a lexicon. Recent work in our lab on embedding predication (Kilicoglu, 2012), negation (Rosenberg, 2013), and modality (Rosenberg et al., 2012) highlighted that syntactically embedding constructions exert an influence over the meaning of constituents, so we applied this insight to sentiment values. On the 2013 dataset, most (of the 6822) tweets contained named entities (6286), as expected, but surprisingly the second most frequent feature was modality (1785), followed by negation (1356). Thus these features have the potential to influence the results to a measurable degree. These linguistic context features were encoded as occurrences. The general schema of this integ</context>
</contexts>
<marker>Kilicoglu, 2012</marker>
<rawString>Halil Kilicoglu. 2012. Embedding Predications. Ph.D. thesis, Concordia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Saif M Mohammad</author>
</authors>
<title>Sentiment analysis of short informal texts.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>50</volume>
<contexts>
<context position="16773" citStr="Kiritchenko et al., 2014" startWordPosition="2696" endWordPosition="2699">timent trigger and negation annotations are removed, if they exist 3. if a trigger is in negation scope, its prior sentiment score is multiplied with -0.5 4. if there is more than one sentiment trigger per term, the triggers’ scores are summed up 5. each prior sentiment score is scaled to [0,1] 6. if there is no trigger for a term, score is 0.5 Results The evaluation metrics are Kendall and Spearman rank correlation coefficients (Nelson, 2001) for subtask 10E between gold values of words or phrases and predicted values. Gold values are human judgements from the compilation of the NRC lexicon (Kiritchenko et al., 2014). Our simple rule-based and lexica-driven system submitted for Task 10E ranked 4th among 10 submitted systems in both correlation coefficient evaluations. Our Kendall rank correlation coefficient result is 0.584 where all results range between 0.625 and 0.254, and our Spearman rank correlation coefficient result is 0.777 where results range between 0.817 and 0.373. 8 Task 11: Figurative Language Figurative language permeates daily life and social media, conveying non-explicit meanings using tropes such as irony, sarcasm, or metaphor. However, understanding these phenomena is not trivial for se</context>
</contexts>
<marker>Kiritchenko, Zhu, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mohammad. 2014. Sentiment analysis of short informal texts. Journal of Artificial Intelligence Research, 50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval-2013).</booktitle>
<contexts>
<context position="2566" citStr="Mohammad et al., 2013" startWordPosition="399" endWordPosition="402"> the datasets is described in detail in (Rosenthal et al., 2015) for Task 10 and in (Ghosh et al., 2015) for Task 11. We also submitted a sentiment lexicon transformed from our in-house lexical resource for Task 10E. Our system is based on a pipeline design in 5 major phases, described below. Following standard text preprocessing, we use Stanford dependencies (De Marneffe et al., 2006) and linguistic features negation, modality and their scope in connection with standard sentiment lexica from the literature and an in-house lexical resource compiled with the technique used for the NRC lexicon (Mohammad et al., 2013). These features were successful in both Task 10B (rank 9 on 40 for Twitter 2015 data, seventh on 40 for Twitter 2015 sarcasm data) and Task 11 (rank 1 of 35 runs by 15 teams). Our sentiment lexicon submitted to Task 10E ranked fourth of ten. 2 Pipeline Design CLaCSentiPipe is a pipeline system that attempts to test the interoperability of different sentiment lexica and a selected set of linguistic annotations. The lexical resources used are aFinn (Nielsen, 2011), MPQA (Wilson et al., 2005), BingLiu (Hu and Liu, 2004), and Gezi, our own lexical resource described below. Third party processing </context>
<context position="5646" citStr="Mohammad et al., 2013" startWordPosition="884" endWordPosition="887">ase 5 The resulting feature space is grouped into subsets of features in order to create feature combinations (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b) and processed with Weka (Witten and Frank, 2011) libSVM (Chang and Lin, 2011) with RBF kernel and parameters of cost=5, gamma=0.001 and weights=[neutral=1; positive=2; negative=2.9] for Subtask 10B and M5P (Wang and Witten, 1997), a decision tree regressor, to predict continuos values1 for Task 11. 1http://www.opentox.org/dev/ documentation/components/m5p 3 Lexica In the past two years, the team that developed the NRC lexicon (Mohammad et al., 2013) dominated the Twitter sentiment task and our first question was: is the NRC lexicon itself the ultimate resource, or is the technique that derived it the essential lesson, and can that technique be reused to similar effect. We compiled a similar resource, Gezi, and compared it with the NRC lexicon, but also much smaller traditional resources, namely Bing Liu’s dictionary (Hu and Liu, 2004), MPQA (Wiebe et al., 2006), and aFinn (Nielsen, 2011), a manually compiled dictionary. Extensive ablation studies showed that all the resulting dictionaries contributed to the best performing feature combin</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-of-theart in sentiment analysis of tweets. In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval-2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rick Moody</author>
<author>Christine A Lindberg</author>
</authors>
<title>Oxford American Writer’s Thesaurus.</title>
<date>2012</date>
<publisher>OUP.</publisher>
<contexts>
<context position="7122" citStr="Moody and Lindberg, 2012" startWordPosition="1128" endWordPosition="1131">e component in all our top performing feature combinations. In our competition system, we did not use the NRC lexicon, in order to assess whether Gezi, derived in a similar manner, was performing as well. 4 Gezi Lexical Resources Gezi corpus To assess whether the strong performance of the NRC lexicon can be replicated and enhanced, we used their technique to compile a new resource, Gezi, by selecting positive and negative hashtags from the Twitter API from December 2013 to May 2014. The set of 35 positive and 34 negative seed hashtags were obtained from the Oxford American Writer’s Thesaurus (Moody and Lindberg, 2012) by expanding the adjectives good and bad, resulting in nearly 20 million tweets, from which unigram, bigram, and dependency triple information was collected. After removing retweets, tweets with conflicting hashtags, and tweets with little or no content words, as well as all URLs in tweets, we annotate the remaining tweets with the polarity class of their seed hashtag for our Gezi tweet corpus and project the tweet polarity onto each token inside the tweet for our unigram and bigram features. 480 Data processing After applying Phase 1 to the Gezi corpus the same way we use it in our main pipe</context>
</contexts>
<marker>Moody, Lindberg, 2012</marker>
<rawString>Rick Moody and Christine A Lindberg. 2012. Oxford American Writer’s Thesaurus. OUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger B Nelson</author>
</authors>
<title>Kendall tau metric.</title>
<date>2001</date>
<journal>Encyclopaedia of Mathematics,</journal>
<volume>3</volume>
<contexts>
<context position="16595" citStr="Nelson, 2001" startWordPosition="2668" endWordPosition="2669">ollowed a simple, rule-based approach: 1. aFinn sentiment scores and Gezi (unigrams and bigrams) PMI values are used 2. if a term is part of a bigram, the unigram sentiment trigger and negation annotations are removed, if they exist 3. if a trigger is in negation scope, its prior sentiment score is multiplied with -0.5 4. if there is more than one sentiment trigger per term, the triggers’ scores are summed up 5. each prior sentiment score is scaled to [0,1] 6. if there is no trigger for a term, score is 0.5 Results The evaluation metrics are Kendall and Spearman rank correlation coefficients (Nelson, 2001) for subtask 10E between gold values of words or phrases and predicted values. Gold values are human judgements from the compilation of the NRC lexicon (Kiritchenko et al., 2014). Our simple rule-based and lexica-driven system submitted for Task 10E ranked 4th among 10 submitted systems in both correlation coefficient evaluations. Our Kendall rank correlation coefficient result is 0.584 where all results range between 0.625 and 0.254, and our Spearman rank correlation coefficient result is 0.777 where results range between 0.817 and 0.373. 8 Task 11: Figurative Language Figurative language per</context>
<context position="18502" citStr="Nelson, 2001" startWordPosition="2959" endWordPosition="2960">f figurative language, Sarcasm, Irony and Metaphor. The organizers made the tweet data available with both integer-based and float-based scores. We tested the robustness of our linguistic embedding features by submitting the same pipeline for text processing, feature creation and the exhaustive feature combination evaluation technique of (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b) via 10-fold cross validation on the training set with M5P (Wang and Witten, 1997), a decision tree regressor. We evaluated 10-fold cross validation predictions by calculating correlation coefficients (Nelson, 2001). The extracted features are the same as the features we extracted for Subtask 10B. The only difference is the gold labels since Task 11 requires continuous classes while these are discrete in Subtask 10B. We used float-based gold labels for training data and treat the problem as a regression problem. The output of our system’s predictions were then 483 MSE Overall Sarcasm Irony Metaphor Other 2.117 1.023 0.779 3.155 3.411 Cosine Overall Sarcasm Irony Metaphor Other 0.758 0.892 0.904 0.655 0.584 Table 5: CLaC-SentiPipe in Task 11: rank 1. rounded to integer values, as required. Results The sin</context>
</contexts>
<marker>Nelson, 2001</marker>
<rawString>Roger B. Nelson. 2001. Kendall tau metric. Encyclopaedia of Mathematics, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn A Nielsen</author>
</authors>
<title>A new ANEW: Evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of ESWC2011 Workshop on ‘Making Sense of Microposts’: Big Things Come in Small Packages.</booktitle>
<contexts>
<context position="3033" citStr="Nielsen, 2011" startWordPosition="481" endWordPosition="482"> sentiment lexica from the literature and an in-house lexical resource compiled with the technique used for the NRC lexicon (Mohammad et al., 2013). These features were successful in both Task 10B (rank 9 on 40 for Twitter 2015 data, seventh on 40 for Twitter 2015 sarcasm data) and Task 11 (rank 1 of 35 runs by 15 teams). Our sentiment lexicon submitted to Task 10E ranked fourth of ten. 2 Pipeline Design CLaCSentiPipe is a pipeline system that attempts to test the interoperability of different sentiment lexica and a selected set of linguistic annotations. The lexical resources used are aFinn (Nielsen, 2011), MPQA (Wilson et al., 2005), BingLiu (Hu and Liu, 2004), and Gezi, our own lexical resource described below. Third party processing resources in our GATE environment (Cunningham et al., 2013) include a hybrid of Annie and CMU tokenizers (Cunningham 479 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 479–485, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics et al., 2002; Gimpel et al., 2011), named entity recognition (Ritter et al., ), Stanford Parser Version 3.4.1 (Socher et al., 2013) and dependency module (De Marne</context>
<context position="6093" citStr="Nielsen, 2011" startWordPosition="960" endWordPosition="961"> for Task 11. 1http://www.opentox.org/dev/ documentation/components/m5p 3 Lexica In the past two years, the team that developed the NRC lexicon (Mohammad et al., 2013) dominated the Twitter sentiment task and our first question was: is the NRC lexicon itself the ultimate resource, or is the technique that derived it the essential lesson, and can that technique be reused to similar effect. We compiled a similar resource, Gezi, and compared it with the NRC lexicon, but also much smaller traditional resources, namely Bing Liu’s dictionary (Hu and Liu, 2004), MPQA (Wiebe et al., 2006), and aFinn (Nielsen, 2011), a manually compiled dictionary. Extensive ablation studies showed that all the resulting dictionaries contributed to the best performing feature combination, but that the contribution of the lexica was not proportional to size (suggesting significant overlap). Surprisingly, aFinn, the smallest lexicon, by itself performs better than any of the other dictionaries by themselves and it is the one stable component in all our top performing feature combinations. In our competition system, we did not use the NRC lexicon, in order to assess whether Gezi, derived in a similar manner, was performing </context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn A. Nielsen. 2011. A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. In Proceedings of ESWC2011 Workshop on ‘Making Sense of Microposts’: Big Things Come in Small Packages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="1538" citStr="Pang and Lee, 2008" startWordPosition="230" endWordPosition="233">ombinations comprehensively for their interoperability and effectiveness on different datasets using the exhaustive feature combination technique of (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b). For Subtask 10B we used a SVM, and a decision tree regressor for Task 11. The resulting systems ranked ninth for Subtask 10B, fourth for Subtask 10E, and first for Task 11. 1 Introduction The field of Sentiment Analysis is in its second phase: initially, the task was defined, annotation standards, corpora, and feature resources were identified and provided to the research community (see (Pang and Lee, 2008)). Now, we have regular community challenges such as the SemEval Twitter Sentiment shared tasks which allow us to compare different feature choice and combination across research labs and across successive data sets. We describe here the systems we submitted to SemEval15 for Twitter Sentiment Analysis at the tweet level (Task 10B) and Figurative Language in Twitter (Task 11). The tasks and the design of the datasets is described in detail in (Rosenthal et al., 2015) for Task 10 and in (Ghosh et al., 2015) for Task 11. We also submitted a sentiment lexicon transformed from our in-house lexical </context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Oren Etzion</author>
</authors>
<title>Named entity recognition in Tweets: an experimental study.</title>
<marker>Ritter, Clark, Etzion, </marker>
<rawString>Alan Ritter, Sam Clark, and Oren Etzion. Named entity recognition in Tweets: an experimental study.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Rosenberg</author>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>CLaC Labs: Processing modality and negation.</title>
<date>2012</date>
<booktitle>In Working Notes for QA4MRE Pilot Task at CLEF</booktitle>
<contexts>
<context position="9886" citStr="Rosenberg et al., 2012" startWordPosition="1586" endWordPosition="1589">alone does not change results proportionally, as these large lexica clearly expand into the long tail of infrequently used words. SemEval Test data 2015 2014 2013 NRC unigrams 49.83 52.39 50.9 NRC bigrams 51.31 53.48 52.31 Gezi unigrams 54.65 60.81 57.86 Gezi bigrams 51.14 56.40 50.45 all four combined 56.07 64.26 59.60 Table 1: Comparison NRC and Gezi. 5 Features and Feature Space Primary Features Lexicon features (aFinn, NRC, ... ) encode the prior polarity of the terms in a lexicon. Recent work in our lab on embedding predication (Kilicoglu, 2012), negation (Rosenberg, 2013), and modality (Rosenberg et al., 2012) highlighted that syntactically embedding constructions exert an influence over the meaning of constituents, so we applied this insight to sentiment values. On the 2013 dataset, most (of the 6822) tweets contained named entities (6286), as expected, but surprisingly the second most frequent feature was modality (1785), followed by negation (1356). Thus these features have the potential to influence the results to a measurable degree. These linguistic context features were encoded as occurrences. The general schema of this integration for our system can be formulated as polarityClass,lexicalRes</context>
</contexts>
<marker>Rosenberg, Kilicoglu, Bergler, 2012</marker>
<rawString>Sabine Rosenberg, Halil Kilicoglu, and Sabine Bergler. 2012. CLaC Labs: Processing modality and negation. In Working Notes for QA4MRE Pilot Task at CLEF 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Rosenberg</author>
</authors>
<title>Negation triggers and their scope. Master’s thesis,</title>
<date>2013</date>
<institution>Concordia University.</institution>
<contexts>
<context position="3745" citStr="Rosenberg, 2013" startWordPosition="589" endWordPosition="590">cribed below. Third party processing resources in our GATE environment (Cunningham et al., 2013) include a hybrid of Annie and CMU tokenizers (Cunningham 479 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 479–485, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics et al., 2002; Gimpel et al., 2011), named entity recognition (Ritter et al., ), Stanford Parser Version 3.4.1 (Socher et al., 2013) and dependency module (De Marneffe et al., 2006). Linguistic notions used are negation and modality triggers (Kilicoglu, 2012; Rosenberg, 2013) and scope (Rosenberg, 2013) as well as dependency relations (De Marneffe et al., 2006). Phase 1 Following tokenization, sentence splitting, POS tagging, and named entity recognition (Ritter et al., ) (to fuse multi-word names into a single token) and lookup in the sentiment lexica used, we ignore Twitter-specific items (@name, URLs ... ) when parsing with the Stanford parser. Phase 2 Using POS tags information for disambiguation, the prior polarity (value positive, negative, neutral and score where available) is determined for each token from each of the lexical resources. Phase 3 Based on th</context>
<context position="9847" citStr="Rosenberg, 2013" startWordPosition="1582" endWordPosition="1583">ics. We also conclude that size alone does not change results proportionally, as these large lexica clearly expand into the long tail of infrequently used words. SemEval Test data 2015 2014 2013 NRC unigrams 49.83 52.39 50.9 NRC bigrams 51.31 53.48 52.31 Gezi unigrams 54.65 60.81 57.86 Gezi bigrams 51.14 56.40 50.45 all four combined 56.07 64.26 59.60 Table 1: Comparison NRC and Gezi. 5 Features and Feature Space Primary Features Lexicon features (aFinn, NRC, ... ) encode the prior polarity of the terms in a lexicon. Recent work in our lab on embedding predication (Kilicoglu, 2012), negation (Rosenberg, 2013), and modality (Rosenberg et al., 2012) highlighted that syntactically embedding constructions exert an influence over the meaning of constituents, so we applied this insight to sentiment values. On the 2013 dataset, most (of the 6822) tweets contained named entities (6286), as expected, but surprisingly the second most frequent feature was modality (1785), followed by negation (1356). Thus these features have the potential to influence the results to a measurable degree. These linguistic context features were encoded as occurrences. The general schema of this integration for our system can be</context>
</contexts>
<marker>Rosenberg, 2013</marker>
<rawString>Sabine Rosenberg. 2013. Negation triggers and their scope. Master’s thesis, Concordia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>SemEval-2015 Task 10: Sentiment analysis in Twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval2015).</booktitle>
<contexts>
<context position="2008" citStr="Rosenthal et al., 2015" startWordPosition="310" endWordPosition="313">e task was defined, annotation standards, corpora, and feature resources were identified and provided to the research community (see (Pang and Lee, 2008)). Now, we have regular community challenges such as the SemEval Twitter Sentiment shared tasks which allow us to compare different feature choice and combination across research labs and across successive data sets. We describe here the systems we submitted to SemEval15 for Twitter Sentiment Analysis at the tweet level (Task 10B) and Figurative Language in Twitter (Task 11). The tasks and the design of the datasets is described in detail in (Rosenthal et al., 2015) for Task 10 and in (Ghosh et al., 2015) for Task 11. We also submitted a sentiment lexicon transformed from our in-house lexical resource for Task 10E. Our system is based on a pipeline design in 5 major phases, described below. Following standard text preprocessing, we use Stanford dependencies (De Marneffe et al., 2006) and linguistic features negation, modality and their scope in connection with standard sentiment lexica from the literature and an in-house lexical resource compiled with the technique used for the NRC lexicon (Mohammad et al., 2013). These features were successful in both T</context>
<context position="13040" citStr="Rosenthal et al., 2015" startWordPosition="2081" endWordPosition="2084">ghi and Bergler, 2013a; Shareghi and Bergler, 2013b). # feat’s Primary Feature Subsets aFinn 9 MPQA 12 BingLiu 8 NRC unigrams 17 NRC bigrams 17 Gezi unigrams 17 Gezi bigrams 17 dependency scores 13 dependency counts 8 Secondary Feature Subsets 9 pos tag based scores and counts frequencies of specific annotations 12 position and top-lowest scores 6 Table 3: Feature subset bundles. Table 3 lists the feature bundles used in our ablation studies. 6 Subtask 10B: Polarity Classification of Tweets The task is a 3-way classification problem of labelling a tweet as positive, neutral, or negative, see (Rosenthal et al., 2015) for a detailed description. We trained an SVM classifier for our experiments using last year’s test sets for development. Performing manual feature selection, we selected not the feature combination that performed best on the training data but instead one that was close to the top on 2015 training data and both, 2014 and 2013 test data (for robustness) but that did not include NRC data (to better assess Gezi). The competition system included aFinn, MPQA, Bing Liu, Gezi unigrams and dependency based features in addition to all secondary features listed above. Results The task of assigning sent</context>
</contexts>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, Ritter, Stoyanov, 2015</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M. Mohammad, Alan Ritter, and Veselin Stoyanov. 2015. SemEval-2015 Task 10: Sentiment analysis in Twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval2015).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehsan Shareghi</author>
<author>Sabine Bergler</author>
</authors>
<title>CLaCCORE: Exhaustive feature combination for measuring textual similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of *SEM 2013 Shared Task STS.</booktitle>
<contexts>
<context position="1095" citStr="Shareghi and Bergler, 2013" startWordPosition="156" endWordPosition="159">mpiled from tweets that carry certain selected hashtags (assumed to guarantee a sentiment polarity) and then inducing that same polarity for the words that occur in the tweets. We also use standard sentiment lexica and combine the results. The lexical sentiment features are further differentiated according to some linguistic contexts in which their triggers occur, including bigrams, negation, modality, and dependency triples. We studied feature combinations comprehensively for their interoperability and effectiveness on different datasets using the exhaustive feature combination technique of (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b). For Subtask 10B we used a SVM, and a decision tree regressor for Task 11. The resulting systems ranked ninth for Subtask 10B, fourth for Subtask 10E, and first for Task 11. 1 Introduction The field of Sentiment Analysis is in its second phase: initially, the task was defined, annotation standards, corpora, and feature resources were identified and provided to the research community (see (Pang and Lee, 2008)). Now, we have regular community challenges such as the SemEval Twitter Sentiment shared tasks which allow us to compare different feature choice and combin</context>
<context position="5161" citStr="Shareghi and Bergler, 2013" startWordPosition="814" endWordPosition="817">es in the Gezi dependency resource. Phase 4 The resulting features are the polarity class according to each lexical resource, embeddedness in modality or negation, as well as sentiment scores for each lexical token according to appropriate lexical resources; dependency score features using PMI scores of dependency triples and their types; dependency count features mapping PMI scores into discrete polarity classes; ad hoc features from specific annotations observed on training data. Phase 5 The resulting feature space is grouped into subsets of features in order to create feature combinations (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b) and processed with Weka (Witten and Frank, 2011) libSVM (Chang and Lin, 2011) with RBF kernel and parameters of cost=5, gamma=0.001 and weights=[neutral=1; positive=2; negative=2.9] for Subtask 10B and M5P (Wang and Witten, 1997), a decision tree regressor, to predict continuos values1 for Task 11. 1http://www.opentox.org/dev/ documentation/components/m5p 3 Lexica In the past two years, the team that developed the NRC lexicon (Mohammad et al., 2013) dominated the Twitter sentiment task and our first question was: is the NRC lexicon itself the ultimate resource, </context>
<context position="12438" citStr="Shareghi and Bergler, 2013" startWordPosition="1982" endWordPosition="1985">al ad hoc features are some special Twitterspecific POS tags (i.e. emphasis from !!!!!), special phrases indicative of sentiment (can’t wait). We also found the first and last token in a tweet to carry potentially special meaning, as well as the association scores between the highest and lowest sentiment carriers in a tweet. Feature Combinations We create feature spaces for each combination of feature subsets described above and we experiment on each combination. The submitted feature combinations for Subtask 10B and Task 11 were selected using the exhaustive feature combination technique of (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b). # feat’s Primary Feature Subsets aFinn 9 MPQA 12 BingLiu 8 NRC unigrams 17 NRC bigrams 17 Gezi unigrams 17 Gezi bigrams 17 dependency scores 13 dependency counts 8 Secondary Feature Subsets 9 pos tag based scores and counts frequencies of specific annotations 12 position and top-lowest scores 6 Table 3: Feature subset bundles. Table 3 lists the feature bundles used in our ablation studies. 6 Subtask 10B: Polarity Classification of Tweets The task is a 3-way classification problem of labelling a tweet as positive, neutral, or negative, see (Rosenthal et al., 201</context>
<context position="18256" citStr="Shareghi and Bergler, 2013" startWordPosition="2920" endWordPosition="2923">., 2015). The challenge dataset contains tweets that contain at least one instance of figurative language and nonfigurative tweets (labelled other). The labels are in form of sentiment scores obtained from human judgements. The dataset distinguished 3 types of figurative language, Sarcasm, Irony and Metaphor. The organizers made the tweet data available with both integer-based and float-based scores. We tested the robustness of our linguistic embedding features by submitting the same pipeline for text processing, feature creation and the exhaustive feature combination evaluation technique of (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b) via 10-fold cross validation on the training set with M5P (Wang and Witten, 1997), a decision tree regressor. We evaluated 10-fold cross validation predictions by calculating correlation coefficients (Nelson, 2001). The extracted features are the same as the features we extracted for Subtask 10B. The only difference is the gold labels since Task 11 requires continuous classes while these are discrete in Subtask 10B. We used float-based gold labels for training data and treat the problem as a regression problem. The output of our system’s predictions were then 48</context>
</contexts>
<marker>Shareghi, Bergler, 2013</marker>
<rawString>Ehsan Shareghi and Sabine Bergler. 2013a. CLaCCORE: Exhaustive feature combination for measuring textual similarity. In Proceedings of *SEM 2013 Shared Task STS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehsan Shareghi</author>
<author>Sabine Bergler</author>
</authors>
<title>Feature combination for sentence similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 26th Canadian Conference on Artificial Intelligence (AI</booktitle>
<pages>7884</pages>
<publisher>Springer</publisher>
<location>Berlin. LNAI</location>
<contexts>
<context position="1095" citStr="Shareghi and Bergler, 2013" startWordPosition="156" endWordPosition="159">mpiled from tweets that carry certain selected hashtags (assumed to guarantee a sentiment polarity) and then inducing that same polarity for the words that occur in the tweets. We also use standard sentiment lexica and combine the results. The lexical sentiment features are further differentiated according to some linguistic contexts in which their triggers occur, including bigrams, negation, modality, and dependency triples. We studied feature combinations comprehensively for their interoperability and effectiveness on different datasets using the exhaustive feature combination technique of (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b). For Subtask 10B we used a SVM, and a decision tree regressor for Task 11. The resulting systems ranked ninth for Subtask 10B, fourth for Subtask 10E, and first for Task 11. 1 Introduction The field of Sentiment Analysis is in its second phase: initially, the task was defined, annotation standards, corpora, and feature resources were identified and provided to the research community (see (Pang and Lee, 2008)). Now, we have regular community challenges such as the SemEval Twitter Sentiment shared tasks which allow us to compare different feature choice and combin</context>
<context position="5161" citStr="Shareghi and Bergler, 2013" startWordPosition="814" endWordPosition="817">es in the Gezi dependency resource. Phase 4 The resulting features are the polarity class according to each lexical resource, embeddedness in modality or negation, as well as sentiment scores for each lexical token according to appropriate lexical resources; dependency score features using PMI scores of dependency triples and their types; dependency count features mapping PMI scores into discrete polarity classes; ad hoc features from specific annotations observed on training data. Phase 5 The resulting feature space is grouped into subsets of features in order to create feature combinations (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b) and processed with Weka (Witten and Frank, 2011) libSVM (Chang and Lin, 2011) with RBF kernel and parameters of cost=5, gamma=0.001 and weights=[neutral=1; positive=2; negative=2.9] for Subtask 10B and M5P (Wang and Witten, 1997), a decision tree regressor, to predict continuos values1 for Task 11. 1http://www.opentox.org/dev/ documentation/components/m5p 3 Lexica In the past two years, the team that developed the NRC lexicon (Mohammad et al., 2013) dominated the Twitter sentiment task and our first question was: is the NRC lexicon itself the ultimate resource, </context>
<context position="12438" citStr="Shareghi and Bergler, 2013" startWordPosition="1982" endWordPosition="1985">al ad hoc features are some special Twitterspecific POS tags (i.e. emphasis from !!!!!), special phrases indicative of sentiment (can’t wait). We also found the first and last token in a tweet to carry potentially special meaning, as well as the association scores between the highest and lowest sentiment carriers in a tweet. Feature Combinations We create feature spaces for each combination of feature subsets described above and we experiment on each combination. The submitted feature combinations for Subtask 10B and Task 11 were selected using the exhaustive feature combination technique of (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b). # feat’s Primary Feature Subsets aFinn 9 MPQA 12 BingLiu 8 NRC unigrams 17 NRC bigrams 17 Gezi unigrams 17 Gezi bigrams 17 dependency scores 13 dependency counts 8 Secondary Feature Subsets 9 pos tag based scores and counts frequencies of specific annotations 12 position and top-lowest scores 6 Table 3: Feature subset bundles. Table 3 lists the feature bundles used in our ablation studies. 6 Subtask 10B: Polarity Classification of Tweets The task is a 3-way classification problem of labelling a tweet as positive, neutral, or negative, see (Rosenthal et al., 201</context>
<context position="18256" citStr="Shareghi and Bergler, 2013" startWordPosition="2920" endWordPosition="2923">., 2015). The challenge dataset contains tweets that contain at least one instance of figurative language and nonfigurative tweets (labelled other). The labels are in form of sentiment scores obtained from human judgements. The dataset distinguished 3 types of figurative language, Sarcasm, Irony and Metaphor. The organizers made the tweet data available with both integer-based and float-based scores. We tested the robustness of our linguistic embedding features by submitting the same pipeline for text processing, feature creation and the exhaustive feature combination evaluation technique of (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b) via 10-fold cross validation on the training set with M5P (Wang and Witten, 1997), a decision tree regressor. We evaluated 10-fold cross validation predictions by calculating correlation coefficients (Nelson, 2001). The extracted features are the same as the features we extracted for Subtask 10B. The only difference is the gold labels since Task 11 requires continuous classes while these are discrete in Subtask 10B. We used float-based gold labels for training data and treat the problem as a regression problem. The output of our system’s predictions were then 48</context>
</contexts>
<marker>Shareghi, Bergler, 2013</marker>
<rawString>Ehsan Shareghi and Sabine Bergler. 2013b. Feature combination for sentence similarity. In Proceedings of the 26th Canadian Conference on Artificial Intelligence (AI 2013). Springer Berlin. LNAI 7884.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="3601" citStr="Socher et al., 2013" startWordPosition="566" endWordPosition="569">The lexical resources used are aFinn (Nielsen, 2011), MPQA (Wilson et al., 2005), BingLiu (Hu and Liu, 2004), and Gezi, our own lexical resource described below. Third party processing resources in our GATE environment (Cunningham et al., 2013) include a hybrid of Annie and CMU tokenizers (Cunningham 479 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 479–485, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics et al., 2002; Gimpel et al., 2011), named entity recognition (Ritter et al., ), Stanford Parser Version 3.4.1 (Socher et al., 2013) and dependency module (De Marneffe et al., 2006). Linguistic notions used are negation and modality triggers (Kilicoglu, 2012; Rosenberg, 2013) and scope (Rosenberg, 2013) as well as dependency relations (De Marneffe et al., 2006). Phase 1 Following tokenization, sentence splitting, POS tagging, and named entity recognition (Ritter et al., ) (to fuse multi-word names into a single token) and lookup in the sentiment lexica used, we ignore Twitter-specific items (@name, URLs ... ) when parsing with the Stanford parser. Phase 2 Using POS tags information for disambiguation, the prior polarity (v</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing with compositional vector grammars. In Proceedings of the 51st Meeting of the Association for Computational Linguistics (ACL 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yong Wang</author>
<author>Ian H Witten</author>
</authors>
<title>Induction of model trees for predicting continuous classes.</title>
<date>1997</date>
<booktitle>In Poster in Proceedings of the 9th European Conference on Machine Learning. Faculty of Informatics and Statistics,</booktitle>
<location>Prague.</location>
<contexts>
<context position="5422" citStr="Wang and Witten, 1997" startWordPosition="853" endWordPosition="856">ency score features using PMI scores of dependency triples and their types; dependency count features mapping PMI scores into discrete polarity classes; ad hoc features from specific annotations observed on training data. Phase 5 The resulting feature space is grouped into subsets of features in order to create feature combinations (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b) and processed with Weka (Witten and Frank, 2011) libSVM (Chang and Lin, 2011) with RBF kernel and parameters of cost=5, gamma=0.001 and weights=[neutral=1; positive=2; negative=2.9] for Subtask 10B and M5P (Wang and Witten, 1997), a decision tree regressor, to predict continuos values1 for Task 11. 1http://www.opentox.org/dev/ documentation/components/m5p 3 Lexica In the past two years, the team that developed the NRC lexicon (Mohammad et al., 2013) dominated the Twitter sentiment task and our first question was: is the NRC lexicon itself the ultimate resource, or is the technique that derived it the essential lesson, and can that technique be reused to similar effect. We compiled a similar resource, Gezi, and compared it with the NRC lexicon, but also much smaller traditional resources, namely Bing Liu’s dictionary (</context>
<context position="18369" citStr="Wang and Witten, 1997" startWordPosition="2938" endWordPosition="2941">rative tweets (labelled other). The labels are in form of sentiment scores obtained from human judgements. The dataset distinguished 3 types of figurative language, Sarcasm, Irony and Metaphor. The organizers made the tweet data available with both integer-based and float-based scores. We tested the robustness of our linguistic embedding features by submitting the same pipeline for text processing, feature creation and the exhaustive feature combination evaluation technique of (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b) via 10-fold cross validation on the training set with M5P (Wang and Witten, 1997), a decision tree regressor. We evaluated 10-fold cross validation predictions by calculating correlation coefficients (Nelson, 2001). The extracted features are the same as the features we extracted for Subtask 10B. The only difference is the gold labels since Task 11 requires continuous classes while these are discrete in Subtask 10B. We used float-based gold labels for training data and treat the problem as a regression problem. The output of our system’s predictions were then 483 MSE Overall Sarcasm Irony Metaphor Other 2.117 1.023 0.779 3.155 3.411 Cosine Overall Sarcasm Irony Metaphor Ot</context>
</contexts>
<marker>Wang, Witten, 1997</marker>
<rawString>Yong Wang and Ian H. Witten. 1997. Induction of model trees for predicting continuous classes. In Poster in Proceedings of the 9th European Conference on Machine Learning. Faculty of Informatics and Statistics, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2006</date>
<pages>39--2</pages>
<contexts>
<context position="6066" citStr="Wiebe et al., 2006" startWordPosition="954" endWordPosition="957">or, to predict continuos values1 for Task 11. 1http://www.opentox.org/dev/ documentation/components/m5p 3 Lexica In the past two years, the team that developed the NRC lexicon (Mohammad et al., 2013) dominated the Twitter sentiment task and our first question was: is the NRC lexicon itself the ultimate resource, or is the technique that derived it the essential lesson, and can that technique be reused to similar effect. We compiled a similar resource, Gezi, and compared it with the NRC lexicon, but also much smaller traditional resources, namely Bing Liu’s dictionary (Hu and Liu, 2004), MPQA (Wiebe et al., 2006), and aFinn (Nielsen, 2011), a manually compiled dictionary. Extensive ablation studies showed that all the resulting dictionaries contributed to the best performing feature combination, but that the contribution of the lexica was not proportional to size (suggesting significant overlap). Surprisingly, aFinn, the smallest lexicon, by itself performs better than any of the other dictionaries by themselves and it is the one stable component in all our top performing feature combinations. In our competition system, we did not use the NRC lexicon, in order to assess whether Gezi, derived in a simi</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2006</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2006. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="3061" citStr="Wilson et al., 2005" startWordPosition="484" endWordPosition="487"> the literature and an in-house lexical resource compiled with the technique used for the NRC lexicon (Mohammad et al., 2013). These features were successful in both Task 10B (rank 9 on 40 for Twitter 2015 data, seventh on 40 for Twitter 2015 sarcasm data) and Task 11 (rank 1 of 35 runs by 15 teams). Our sentiment lexicon submitted to Task 10E ranked fourth of ten. 2 Pipeline Design CLaCSentiPipe is a pipeline system that attempts to test the interoperability of different sentiment lexica and a selected set of linguistic annotations. The lexical resources used are aFinn (Nielsen, 2011), MPQA (Wilson et al., 2005), BingLiu (Hu and Liu, 2004), and Gezi, our own lexical resource described below. Third party processing resources in our GATE environment (Cunningham et al., 2013) include a hybrid of Annie and CMU tokenizers (Cunningham 479 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 479–485, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics et al., 2002; Gimpel et al., 2011), named entity recognition (Ritter et al., ), Stanford Parser Version 3.4.1 (Socher et al., 2013) and dependency module (De Marneffe et al., 2006). Linguisti</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2011</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques.</booktitle>
<publisher>Morgan Kaufmann Publishers,</publisher>
<note>3rd edition.</note>
<contexts>
<context position="5241" citStr="Witten and Frank, 2011" startWordPosition="826" endWordPosition="829"> class according to each lexical resource, embeddedness in modality or negation, as well as sentiment scores for each lexical token according to appropriate lexical resources; dependency score features using PMI scores of dependency triples and their types; dependency count features mapping PMI scores into discrete polarity classes; ad hoc features from specific annotations observed on training data. Phase 5 The resulting feature space is grouped into subsets of features in order to create feature combinations (Shareghi and Bergler, 2013a; Shareghi and Bergler, 2013b) and processed with Weka (Witten and Frank, 2011) libSVM (Chang and Lin, 2011) with RBF kernel and parameters of cost=5, gamma=0.001 and weights=[neutral=1; positive=2; negative=2.9] for Subtask 10B and M5P (Wang and Witten, 1997), a decision tree regressor, to predict continuos values1 for Task 11. 1http://www.opentox.org/dev/ documentation/components/m5p 3 Lexica In the past two years, the team that developed the NRC lexicon (Mohammad et al., 2013) dominated the Twitter sentiment task and our first question was: is the NRC lexicon itself the ultimate resource, or is the technique that derived it the essential lesson, and can that technique</context>
</contexts>
<marker>Witten, Frank, 2011</marker>
<rawString>Ian H. Witten and Eibe Frank. 2011. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann Publishers, 3rd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>