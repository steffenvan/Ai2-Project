<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.915232">
An Exploration of Eye Gaze in Spoken Language Processing for Multimodal
Conversational Interfaces
</title>
<author confidence="0.994446">
Shaolin Qu Joyce Y. Chai
</author>
<affiliation confidence="0.9962705">
Department of Computer Science and Engineering
Michigan State University
</affiliation>
<address confidence="0.986089">
East Lansing, MI 48824
</address>
<email confidence="0.999729">
{qushaoli,jchai}@cse.msu.edu
</email>
<sectionHeader confidence="0.994814" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990928">
Motivated by psycholinguistic findings,
we are currently investigating the role of
eye gaze in spoken language understand-
ing for multimodal conversational sys-
tems. Our assumption is that, during hu-
man machine conversation, a user’s eye
gaze on the graphical display indicates
salient entities on which the user’s atten-
tion is focused. The specific domain infor-
mation about the salient entities is likely
to be the content of communication and
therefore can be used to constrain speech
hypotheses and help language understand-
ing. Based on this assumption, this paper
describes an exploratory study that incor-
porates eye gaze in salience modeling for
spoken language processing. Our empiri-
cal results show that eye gaze has a poten-
tial in improving automated language pro-
cessing. Eye gaze is subconscious and in-
voluntary during human machine conver-
sation. Our work motivates more in-depth
investigation on eye gaze in attention pre-
diction and its implication in automated
language processing.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999830837837838">
Psycholinguistic experiments have shown that eye
gaze is tightly linked to human language process-
ing. Eye gaze is one of the reliable indicators of
what a person is “thinking about” (Henderson and
Ferreira, 2004). The direction of gaze carries infor-
mation about the focus of the users attention (Just
and Carpenter, 1976). The perceived visual context
influences spoken word recognition and mediates
syntactic processing (Tanenhaus et al., 1995; Roy
and Mukherjee, 2005). In addition, directly before
speaking a word, the eyes move to the mentioned
object (Griffin and Bock, 2000).
Motivated by these psycholinguistic findings, we
are currently investigating the role of eye gaze in
spoken language understanding during human ma-
chine conversation. Through multimodal interfaces,
a user can look at a graphic display and converse
with the system at the same time. Our assumption
is that, during human machine conversation, a user’s
eye gaze on the graphical display can indicate salient
entities on which the user’s attention is focused. The
specific domain information about the salient enti-
ties is likely linked to the content of communication
and therefore can be used to constrain speech hy-
potheses and influence language understanding.
Based on this assumption, we carried out an ex-
ploration study where eye gaze information is in-
corporated in a salience model to tailor a language
model for spoken language processing. Our prelim-
inary results show that eye gaze can be useful in im-
proving spoken language processing and the effect
of eye gaze varies among different users. Because
eye gaze is subconscious and involuntary in human
machine conversation, our work also motivates sys-
tematic investigations on how eye gaze contributes
to attention prediction and its implications in auto-
mated language processing.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99992875">
Eye gaze has been mainly used in human machine
interaction as a pointing mechanism in direct manip-
ulation interfaces (Jacob, 1990; Jacob, 1995; Zhai
et al., 1999), as a facilitator in computer supported
human human communication (Velichkovsky, 1995;
Vertegaal, 1999); or as an additional modality dur-
ing speech or multimodal communication (Starker
and Bolt, 1990; Campana et al., 2001; Kaur et al.,
</bodyText>
<page confidence="0.974559">
284
</page>
<note confidence="0.7985235">
Proceedings of NAACL HLT 2007, pages 284–291,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.9985675625">
2003; Qvarfordt and Zhai, 2005). This last area of
investigation is more related to our work.
In the context of speech and multimodal commu-
nication, studies have shown that speech and eye
gaze integration patterns can be modeled reliably for
users. For example, by studying patterns of eye gaze
and speech in the phrase “move it there”, researchers
found that the gaze fixation closest to the intended
object begins, with high probability, before the be-
ginning of the word “move” (Kaur et al., 2003). Re-
cent work has also shown that eye gaze has a poten-
tial to improve reference resolution in a spoken dia-
log system (Campana et al., 2001). Furthermore, eye
gaze also plays an important role in managing dia-
log in conversational systems (Qvarfordt and Zhai,
2005).
Salience modeling has been used in both natural
language and multimodal language processing. Lin-
guistic salience describes entities with their accessi-
bility in a hearer’s memory and their implications in
language production and interpretation. Linguistic
salience modeling has been used for language in-
terpretations such as reference resolution (Huls et
al., 1995; Eisenstein and Christoudias, 2004). Vi-
sual salience measures how much attention an en-
tity attracts from a user based on its visual proper-
ties. Visual salience can tailor users’ referring ex-
pressions and thus can be used for multimodal refer-
ence resolution (Kehler, 2000). Our recent work has
also investigated salience modeling based on deic-
tic gestures to improve spoken language understand-
ing (Chai and Qu, 2005; Qu and Chai, 2006).
</bodyText>
<sectionHeader confidence="0.985625" genericHeader="method">
3 Data Collection
</sectionHeader>
<bodyText confidence="0.999611076923077">
We conducted user studies to collect speech and eye
gaze data. In the experiments, a static 3D bedroom
scene was shown to the user. The system verbally
asked a user a list of questions one at a time about
the bedroom and the user answered the questions by
speaking to the system. Fig.1 shows the 14 questions
in the experiments. The user’s speech was recorded
through an open microphone and the user’s eye gaze
was captured by an Eye Link II eye tracker. From 7
users’ experiments, we collected 554 utterances with
a vocabulary of 489 words. Each utterance was tran-
scribed and annotated with entities that were being
talked about in the utterance.
</bodyText>
<figure confidence="0.895790055555555">
1 Describe this room.
2 What do you like/dislike about the arrangement?
3 Describe anything in the room that seems strange to
you.
4 Is there a bed in this room?
5 How big is the bed?
6 Describe the area around the bed.
7 Would you make any changes to the area around the
bed?
8 Describe the left wall.
9 How many paintings are there in this room?
10 Which is your favorite painting?
11 Which is your least favorite painting?
12 What is your favorite piece of furniture in the room?
13 What is your least favorite piece of furniture in the
room?
14 How would you change this piece of furniture to make
it better?
</figure>
<figureCaption confidence="0.999978">
Figure 1: Questions for users in experiments
</figureCaption>
<bodyText confidence="0.99998052">
The collected raw gaze data consists of the screen
coordinates of each gaze point sampled at 4 ms.
As shown in Fig.2a, this raw data is not very use-
ful for identifying fixated entities. The raw gaze
data are processed to eliminate invalid and saccadic
gaze points, leaving only pertinent eye fixations.
Invalid gaze points occur when users look off the
screen. Saccadic gaze points occur during ballis-
tic eye movements between fixations. Vision stud-
ies have shown that no visual processing occurs dur-
ing saccades (i.e., saccadic suppression). It is well
known that eyes do not stay still, but rather make
small, frequent jerky movements. In order to best
determine fixation locations, nearby gaze points are
averaged together to identify fixations. The pro-
cessed eye gaze fixations can be seen in Fig.2b.
Fig.3 shows an excerpt of the collected speech
and gaze fixation with fixated entities. In the speech
stream, each word starts at a particular timestamp. In
the gaze stream, each gaze fixation f has a starting
timestamp tf and a duration Tf. Gaze fixations can
have different durations. An entity e on the graphi-
cal display is fixated by gaze fixation f if the area of
e contains the fixation point of f. One gaze fixation
can fall on multiple entities or no entity.
</bodyText>
<sectionHeader confidence="0.98171" genericHeader="method">
4 Salience Driven Language Modeling
</sectionHeader>
<bodyText confidence="0.998628166666667">
Our goal is to use the domain specific information
about the salient entities on a graphical display, as
indicated by the user’s eye gaze, to help recognition
of the user’s utterances. In particular, we incorporate
this salient domain information in speech recogni-
tion via salience driven language modeling.
</bodyText>
<page confidence="0.996052">
285
</page>
<figure confidence="0.984673">
(a) Raw gaze points (b) Processed gaze fixations
</figure>
<figureCaption confidence="0.919125">
Figure 2: Gaze fixations on a scene
</figureCaption>
<figure confidence="0.993676307692308">
This room has a chandelier
f: gaze fixation speech stream
2572 2872 3170 3528 3736 (ms)
8 596 968 1668 2096 2692 3252
(ms)
tf Tf
[19] [ ] [17] [19] [22] [ ] [10]
[11]
gaze stream
[fixated entity]
[10] [10]
[11] [11]
( [19] – bed_8; [17] – lamp_2; [22] – door_1; [10] – bedroom; [11] – chandelier_1 )
</figure>
<figureCaption confidence="0.999472">
Figure 3: An excerpt of speech and gaze stream data
</figureCaption>
<bodyText confidence="0.99914675">
We first briefly introduce speech recognition. The
task of speech recognition is to, given an observed
spoken utterance O, find the word sequence W*
such that W* = arg max
</bodyText>
<equation confidence="0.648071">
W
</equation>
<bodyText confidence="0.999807666666667">
p(O|W) is the acoustic model and p(W) is the
language model. The acoustic model provides the
probability of observing the acoustic features given
hypothesized word sequences while the language
model provides the probability of a word sequence.
The language model is represented as:
</bodyText>
<equation confidence="0.9694365">
p(wk|wk−1
1 ) (1)
</equation>
<bodyText confidence="0.995045333333333">
Using first-order Markov assumption, the above lan-
guage model can be approximated by a bigram
model:
</bodyText>
<equation confidence="0.983999">
n
p(wn1) = H p(wk|wk−1) (2)
k=1
</equation>
<bodyText confidence="0.9999415">
In the following sections, we first introduce the
salience modeling based on eye gaze, then present
how the gaze-based salience models can be used to
tailor language models.
</bodyText>
<subsectionHeader confidence="0.998161">
4.1 Gaze-based Salience Modeling
</subsectionHeader>
<bodyText confidence="0.87832375">
We first define a gaze fixation set Ft0+T
t0 (e), which
contains all gaze fixations that fall on entity e within
a time window t0 — (t0 + T):
</bodyText>
<equation confidence="0.666333">
Ft0+T t0(e) = If|f falls on e within t0 — (t0 + T)}
</equation>
<bodyText confidence="0.976372">
We model gaze-based salience in two ways.
</bodyText>
<subsectionHeader confidence="0.749303">
4.1.1 Gaze Salience Model 1
</subsectionHeader>
<bodyText confidence="0.99993725">
Salience model 1 is based on the assumption that
when an entity has more gaze fixations on it than
other entities, this entity is more likely attended by
the user and thus has higher salience:
</bodyText>
<equation confidence="0.992067">
(e) _ #elements in Fto +T+T(e) (3)
pt0,T fie(#elements in Fto+T(e))
</equation>
<bodyText confidence="0.9966334">
Here, pt0,T (e) tells how likely it is that the user is
focusing on entity e within time period t0 — (t0+T)
based on how many gaze fixations are on e among
all gaze fixations that fall on entities within t0 —
(t0 + T).
</bodyText>
<subsectionHeader confidence="0.847998">
4.1.2 Gaze Salience Model 2
</subsectionHeader>
<bodyText confidence="0.999895">
Salience model 2 is based on the assumption that
when an entity has longer gaze fixations on it than
other entities, this entity is more likely attended by
the user and thus has higher salience:
</bodyText>
<equation confidence="0.9990652">
Dt0+T
t0 (e)
pt0,T (e) = E (4)
e Dt0+T
t0 (e)
</equation>
<bodyText confidence="0.880106">
where
</bodyText>
<equation confidence="0.9515582">
�
t0+T ( )
Dt0 e =
f�F t0+T
t0 (e)
</equation>
<bodyText confidence="0.994758">
Here, pt0,T (e) tells how likely it is that the user is
focusing on entity e within time period t0 — (t0 + t)
</bodyText>
<equation confidence="0.8913974">
p(O|W)p(W), where
n
p(W) = p(wn1� = H
k=1
Tf (5)
</equation>
<page confidence="0.987144">
286
</page>
<bodyText confidence="0.999985666666667">
based on how long e has been fixated by gaze fixa-
tions among the overall time length of all gaze fixa-
tions that fall on entities within t0 — (t0 + T).
</bodyText>
<subsectionHeader confidence="0.996847">
4.2 Salience Driven N-gram Model
</subsectionHeader>
<bodyText confidence="0.999696363636364">
Salience models can be incorporated in different lan-
guage models, such as bigram models, class-based
bigram models, and probabilistic context free gram-
mar. Among these language models, the salience
driven bigram model based on deictic gesture has
been shown to achieve best performance on speech
recognition (Qu and Chai, 2006). In our initial in-
vestigation of gaze-based salience, we incorporate
the gaze-based salience in a bigram model.
The salience driven bigram probability is given
by:
</bodyText>
<equation confidence="0.9988665">
ps(wi|wi−1) = (1 — A)p(wi|wi−1) +
A &amp; p(wi|wi−1, e)pto,T (e) (6)
</equation>
<bodyText confidence="0.999955210526316">
where pt0,T (e) is the salience distribution as mod-
eled in equations (3) and (4). In applying the
salience driven bigram model for speech recogni-
tion, we set t0 as the starting timestamp of the ut-
terance and T as the duration of the utterance. The
priming weight A decides how much the original
bigram probability will be tailored by the salient
entities indicated by eye gaze. Currently, we set
A = 0.67 empirically. We also tried learning the
priming weight with an EM algorithm. However,
we found out that the learned priming weight per-
formed worse than the empirical one in our exper-
iments. This is probably due to insufficient devel-
opment data. Bigram probabilities p(wi|wi−1) were
estimated by the maximum likelihood estimation us-
ing Katz’s backoff method (Katz, 1987) with a fre-
quency cutoff of 1. The same method was used to es-
timate p(wi|wi−1, e) from the users’ utterance tran-
scripts with entity annotation of e.
</bodyText>
<sectionHeader confidence="0.812197" genericHeader="method">
5 Application of Salience Driven LMs
</sectionHeader>
<bodyText confidence="0.999902285714286">
The salience driven language models can be inte-
grated into speech processing in two stages: an early
stage before a word lattice (n-best list) is generated
(Fig.4a), or in a late stage where the word lattice
(n-best list) is post-processed (Fig.4b).
For the early stage integration, the gaze-based
salience driven language model is used together with
</bodyText>
<figure confidence="0.999389">
(a) Early stage integration
(b) Late stage integration
</figure>
<figureCaption confidence="0.986809">
Figure 4: Integration of gaze-based salience driven
language model in speech processing
</figureCaption>
<bodyText confidence="0.90991">
the acoustic model to generate the word lattice, typ-
ically by Viterbi search.
For the late stage integration, the gaze-based
salience driven language model is used to rescore the
word lattice generated by a speech recognizer with
a basic language model not involving salience mod-
eling. A* search can be applied to find the n-best
paths in the word lattice.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999987">
The evaluations were conducted on data collected
from user studies (Sec. 3). We evaluated the gaze-
based salience driven bigram models when applied
for speech recognition at early and late stages.
</bodyText>
<subsectionHeader confidence="0.994982">
6.1 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999978">
Users’ speech was first segmented, then recognized
by the CMU Sphinx-4 speech recognizer using dif-
ferent language models. Evaluation was done by
a 14-fold cross validation. We compare the per-
formances of the early and late applications of two
gaze-based salience driven language models:
</bodyText>
<listItem confidence="0.996947545454545">
• S-Bigram1 – salience driven language model
based on salience modeling 1 (Sec. 4.1.1)
• S-Bigram2 – salience driven language model
based on salience modeling 2 (Sec. 4.1.2)
Table 1 and Table 2 show the results of early and
late application of the salience driven language mod-
els based on eye gaze. We can see that all word error
rates (WERs) are high. In the experiments, users
were instructed to only answer systems questions
one by one. There was no flow of a real conversa-
tion. In this setting, users were more free to express
</listItem>
<figure confidence="0.996155904761905">
eye gaze
Acoustic
Model
Speech Decoder
word lattice
(n-best list)
speech
Language
Model
Language
Model
word lattice
(n-best list) n-best list
Rescorer
Language
Model
Acoustic
Model
eye gaze
speech
Speech Decoder
</figure>
<page confidence="0.975569">
287
</page>
<bodyText confidence="0.9926505">
themselves than in the situation where users believed
they were conversing with a machine. Thus, we ob-
serve much longer sentences that often contain dis-
fluencies. Here is one example:
</bodyText>
<note confidence="0.697365">
System: “How big is the bed?”
</note>
<construct confidence="0.960152">
User: “I would to have to offer a guess that the bed,
if I look the chair that’s beside it [pause] in a rel-
ative angle to the bed, it’s probably six feet long,
possibly, or shorter, slightly shorter.”
</construct>
<bodyText confidence="0.999233444444444">
The high WER was mainly caused by the com-
plexity and disfluencies of users’ speech. Poor
speech recording quality is another reason for the
bad recognition performance. It was found that
the trigram model performed worse than the bigram
model in the experiment. This is probably due to the
sparseness of trigrams in the corpus. The amount of
data available is too small considering the vocabu-
lary size.
</bodyText>
<table confidence="0.999653">
Language Model Lattice-WER WER
Bigram 0.613 0.707
Trigram 0.643 0.719
S-Bigram 1 0.605 0.690
S-Bigram 2 0.604 0.689
</table>
<tableCaption confidence="0.991594">
Table 1: WER of early application of LMs
</tableCaption>
<table confidence="0.999627333333333">
Language Model Lattice-WER WER
S-Bigram 1 0.643 0.709
S-Bigram 2 0.643 0.710
</table>
<tableCaption confidence="0.999025">
Table 2: WER of late application of LMs
</tableCaption>
<bodyText confidence="0.99976096969697">
The S-Bigram1 and S-Bigram2 achieved similar
results in both early application (Table 1) and late
application (Table 2). In early application, the S-
Bigram1 model performed better than the trigram
model (t = 5.24, p &lt; 0.001, one-tailed) and the
bigram model (t = 3.31, p &lt; 0.001, one-tailed).
The S-Bigram2 model also performed better than the
trigram model (t = 5.15, p &lt; 0.001, one-tailed)
and the bigram model (t = 3.33, p &lt; 0.001, one-
tailed) in early application. In late application, the
S-Bigram1 model performed better than the trigram
model (t = 2.11, p &lt; 0.02, one-tailed), so did
the S-Bigram2 model (t = 1.99, p &lt; 0.025, one-
tailed). However, compared to the bigram model,
the S-Bigram1 model did not change the recogni-
tion performance significantly (t = 0.38, N.S., two-
tailed) in late application, neither did the S-Bigram2
model (t = 0.50, N.S., two-tailed).
We also compare performances of the salience
driven language models for individual users. In early
application (Fig.5a), both the S-Bigram1 and the S-
Bigram2 model performed better than the baselines
of the bigram and trigram models for all users except
user 2 and user 7. T-tests have shown that these are
significant improvements. For user 2, the S-Bigram1
model achieved the same WER as the bigram model.
For user 7, neither of the salience driven language
models improved recognition compared to the bi-
gram model. In late application (Fig.5b), only for
user 3 and user 4, both salience driven language
models performed better than the baselines of the bi-
gram and trigram models. These improvements have
also been confirmed by t-tests as significant.
</bodyText>
<figure confidence="0.919159">
User ID
(a) WER of early application
User ID
(b) WER of Late application
</figure>
<figureCaption confidence="0.999943">
Figure 5: WERs of LMs for individual users
</figureCaption>
<bodyText confidence="0.9999276">
Comparing early and late application of the
salience driven language models, it is observed that
early application performed better than late applica-
tion for all users except user 3 and user 4. T-tests
have confirmed that these differences are significant.
</bodyText>
<figure confidence="0.999312363636364">
1 2 3 4 5 6 7
WER
0.9
0.8
0.7
0.6
0.5
0.4
1
bigram trigram
s−bigram1 s−bigram2
1 2 3 4 5 6 7
WER
0.9
0.8
0.7
0.6
0.5
0.4
1
bigram trigram
s−bigram1 s−bigram2
</figure>
<page confidence="0.992855">
288
</page>
<bodyText confidence="0.999991764705882">
It is interesting to see that the effect of gaze-based
salience modeling is different among users. For
two users (i.e., user 3 and user 4), the gaze-based
salience driven language models consistently out-
performed the bigram and trigram models in both
early application and late application. However, for
some other users (e.g., user 7), this is not the case. In
fact, the gaze-based salience driven language mod-
els performed worse than the bigram model. This
observation indicates that during language produc-
tion, a user’s eye gaze is voluntary and unconscious.
This is different from deictic gesture, which is more
intentionally delivered by a user. Therefore, incor-
porating this “unconscious” mode of modality in
salience modeling requires more in-depth research
on the role of eye gaze in attention prediction during
multimodal human computer interaction.
</bodyText>
<subsectionHeader confidence="0.980986">
6.2 Discussion
</subsectionHeader>
<bodyText confidence="0.973065461538462">
Gaze-based salience driven language models are
built on the assumption that when a user is fixat-
ing on an entity, the user is saying something re-
lated to the entity. With this assumption, gaze-based
salience driven language models have the potential
to improve speech recognition by biasing the speech
decoder to favor the words that are consistent with
the entity indicated by the user’s eye gaze, especially
when the user’s utterance contains words describing
unique characteristics of the entity. These particular
characteristics could be the entity’s name or physical
properties (e.g., color, material, size).
Utterance: “a tree growing from the floor”
</bodyText>
<equation confidence="0.9443015">
Gaze salience:
p(bedroom) = 0.2414 p(plant willow) = 0.2414
p(chair soft) = 0.2414 p(door 1) = 0.1378
p(bed 8) = 0.1378
</equation>
<subsectionHeader confidence="0.75417">
Bigram n-best list:
</subsectionHeader>
<construct confidence="0.614026">
sheet growing from a four
sheet growing from a for
sheet growing from a floor
</construct>
<figure confidence="0.648730333333333">
� � �
S-Bigram2 n-best list:
a tree growing from the floor
a tree growing from the for
a tree growing from the floor a
� � �
</figure>
<figureCaption confidence="0.9457295">
Figure 6: N-best lists of utterance “a tree growing
from the floor”
</figureCaption>
<bodyText confidence="0.989733244897959">
Fig.6 shows an example where the S-Bigram2
model in early application improved recognition of
the utterance “a tree growing from the floor”. In
this example, the user’s gaze fixations accompany-
ing the utterance resulted in a list of candidate enti-
ties with fixating probabilities (cf. Eqn. (4)), among
which entities bedroom and plant willow were as-
signed higher probabilities. Two n-best lists, the Bi-
gram n-best list and the S-Bigram2 n-best list, were
generated by the speech recognizer when the bigram
model and the S-Bigram2 model were applied sep-
arately. The speech recognizer did not get the cor-
rect recognition when the bigram model was used,
but got the correct result when the S-Bigram2 model
was used.
Fig.7a and 7b show the word lattices of the ut-
terance generated by the speech recognizer using
the bigram model and the S-Bigram2 model respec-
tively. The n-best lists in Fig.6 were generated from
those word lattices. In the word lattices, each path
going from the start node &lt;s&gt; to the end node &lt;/s&gt;
forms a recognition hypothesis. The bigram proba-
bilities along the edges are in the logarithm of base
10. In the bigram case, the path “&lt;s&gt; a tree” has a
higher language score (summation of bigram prob-
abilities along the path) than “&lt;s&gt; sheet”, and “a
floor” has a higher language score than “a full”.
However, these correct paths “&lt;s&gt; a tree” and “a
floor” (not exactly correct, but better than “a full”)
do not appear in the best hypothesis in the result-
ing n-best list. This is because the system tries to
find an overall best hypothesis by considering both
language and acoustic score. Because of the noisy
speech, the incorrect hypotheses may happen to have
higher acoustic confidence than the correct ones. Af-
ter tailoring the bigram model with gaze salience,
the salient entity plant willow significantly increases
the probability of “a tree” (from -1.3594 to -0.9913)
and “tree growing” (from -3.1009 to -1.1887), while
it decreases the probability of “sheet growing” (from
-3.0962 to -3.4534). This probability change is made
by the entity conditional probability p(wi|wi_1,e)
in tailoring of bigram by salience (cf. Eqn. (6)).
Probability p(wi|wi_1, e), trained from the anno-
tated utterances, reflects what words are more likely
to be spoken by a user while talking about an entity
e. The increased probabilities of “a tree” and “tree
growing” show that word “tree” appears more likely
than “sheet” when the user is talking about entity
</bodyText>
<page confidence="0.994223">
289
</page>
<figure confidence="0.993306">
(b) Word lattice with S-Bigram 2
</figure>
<figureCaption confidence="0.991307">
Figure 7: Word lattices of utterance “a tree growing from the floor”
</figureCaption>
<figure confidence="0.998543166666667">
-3.2691
-3.5137
-2.4272
sheet
-3.9306
-3.5780
kind -0.2312
-3.3940
full
-3.0962
going
-1.5911
-2.2662
of
-1.0280
-3.6386
-3.0035
forest
i
-1.5043
-1.9490
&lt;s&gt;
-1.5987
-3.0275
-1.3594
tree -3.1009
growing
-3.2942
-3.1284
-0.8615
-1.9339
a -2.8274
-3.0035
from
a
floor
&lt;/s&gt;
-0.3552
-0.5322
-3.3376
-3.5137
-2.9066
four
-0.9768
-3.4168
for
(a) Word lattice with bigram model
-3.6233
-2.3655
-3.3477
-3.6009
for
-1.2683
-2.4966
a
-3.7353
-1.9934
-2.9278
-0.2570
-1.2861
&lt;/s&gt;
from
-0.1622
-3.9011
-1.2151
floor
-1.9165
a
-0.7782
the
-3.2468
-0.9913
forest
tree -1.1887
&lt;s&gt;
a
-4.2022
-1.5618
-3.8964
-3.4534
growing -3.4626
sheet
-3.6961
further
</figure>
<bodyText confidence="0.999019125">
“plant willow. This is in accordance with our com-
mon sense. Likewise, the salient entity bedroom, of
which floor is a component, makes the probability of
the correct hypothesis “the floor” much higher than
other hypotheses (“the for” and “the forest”). These
enlarged language score differences make the cor-
rect hypotheses “a tree” and “the floor” win out in
the searching procedure despite the noisy speech.
</bodyText>
<equation confidence="0.571821333333333">
Utterance: “I like the picture with like a forest in it”
Gaze salience:
p(bedroom) = 0.5960 p(chandelier 1) = 0.4040
</equation>
<subsectionHeader confidence="0.666036">
Bigram n-best list:
</subsectionHeader>
<construct confidence="0.79933325">
and i eight that picture rid like got five
and i eight that picture rid identifiable
and i eight that picture rid like got forest
� � �
</construct>
<footnote confidence="0.8580418">
S-Bigram2 n-best list:
and i that bedroom it like upside
and i that bedroom it like a five
and i that bedroom it like a forest
� � �
</footnote>
<figureCaption confidence="0.862239">
Figure 8: N-best lists of utterance “I like the picture
with like a forest in it”
</figureCaption>
<bodyText confidence="0.999974818181818">
Unlike the active input mode of deictic gesture,
eye gaze is a passive input mode. The salience in-
formation indicated by eye gaze is not as reliable
as the one indicated by deictic gesture. When the
salient entities indicated by eye gaze are not the
true entities the user is referring to, the salience
driven language model can worsen speech recogni-
tion. Fig.8 shows an example where the S-Bigram2
model in early application worsened the recogni-
tion of a user’s utterance “I like the picture with like
a forest in it” because of wrong salience informa-
tion. In this example, the user was talking about a
picture entity picture bamboo. However, this entity
was not salient, only entities bedroom and chande-
lier 1 were salient. As a result, the recognition with
the S-Bigram2 model becomes worse than the base-
line. The correct word “picture” is missing and the
wrong word “bedroom” appears in the result.
The failure to identify the actual referred entity
picture bamboo as salient in the above example can
also be caused by the visual properties of entities.
Smaller entities on the screen are harder to be fix-
</bodyText>
<page confidence="0.979119">
290
</page>
<bodyText confidence="0.999679333333333">
ated by eye gaze than larger entities. To address this
issue, more reliable salience modeling that takes into
account the visual features is needed.
</bodyText>
<sectionHeader confidence="0.996434" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999978793103448">
This paper presents an empirical exploration of in-
corporating eye gaze in spoken language processing
via salience driven language modeling. Our prelim-
inary results have shown the potential of eye gaze in
improving spoken language processing. Neverthe-
less, this exploratory study is only the first step in
our investigation. Many interesting research ques-
tions remain. During human machine conversation,
how is eye gaze aligned with speech production?
How reliable is eye gaze for attention prediction?
Are there any other factors such as interface design
and visual properties that will affect eye gaze behav-
ior and therefore attention prediction? The answers
to these questions will affect how eye gaze should be
appropriately modeled and used for language pro-
cessing.
Eye-tracking systems are no longer bulky, sta-
tionary systems that prevent natural human ma-
chine communication. Recently developed dis-
play mounted gaze-tracking systems (e.g., Tobii) are
completely non-intrusive, can tolerate head motion,
and provide high tracking quality. These features
have been demonstrated in several successful appli-
cations (Duchowski, 2002). Integrating eye tracking
with conversational interfaces is no longer beyond
reach. We believe it is time to conduct systematic
investigations and fully explore the additional chan-
nel provided by eye gaze in improving robustness of
human machine conversation.
</bodyText>
<sectionHeader confidence="0.998738" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999909833333333">
This work was supported by a Career Award IIS-
0347548 and IIS-0535112 from the National Sci-
ence Foundation. The authors would like to thank
Zahar Prasov for his contribution on data collection
and thank anonymous reviewers for their valuable
comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999273" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999959951612903">
E. Campana, J. Baldridge, J. Dowding, B. Hockey, R. Reming-
ton, and L. Stone. 2001. Using eye movements to determine
referents in a spoken dialogue system. In Proceedings of the
Workshop on Perceptive User Interface.
J. Chai and S. Qu. 2005. A salience driven approach to ro-
bust input interpretation in multimodal conversational sys-
tems. In Proceedings of HLT/EMNLP’05.
A. T. Duchowski. 2002. A breath-first survey of eye tracking
applications. Behavior Research methods, Instruments, and
Computers, 33(4).
J. Eisenstein and C. M. Christoudias. 2004. A salience-based
approach to gesture-speech alignment. In Proceedings of
HLT/NAACL’04.
Z. M. Griffin and K. Bock. 2000. What the eyes say about
speaking. Psychological Science, 11:274–279.
J. M. Henderson and F. Ferreira. 2004. The interface of lan-
guage, vision, and action: Eye movements and the visual
world. New York: Taylor &amp; Francis.
C. Huls, E. Bos, and W. Classen. 1995. Automatic referent res-
olution of deictic and anaphoric expressions. Computational
Linguistics, 21(1):59–79.
R. J. K. Jacob. 1990. What you look is what you get: Eye
movement-based interaction techniques. In Proceedings of
CHI’90.
R. J. K. Jacob. 1995. Eye tracking in advanced interface design.
In W. Barfield and T. Furness, editors, Advanced Interface
Design and Virtual Environments, pages 258–288. Oxford
University Press.
M. Just and P. Carpenter. 1976. Eye fixations and cognitive
processes. Cognitive Psychology, 8:441–480.
S. Katz. 1987. Estimation of probabilities from sparse data for
the language model component of a speech recogniser. IEEE
Trans. Acous., Speech and Sig. Processing, 35(3):400–401.
M. Kaur, M. Termaine, N. Huang, J. Wilder, Z. Gacovski,
F. Flippo, and C. S. Mantravadi. 2003. Where is “it”? event
synchronization in gaze-speech input systems. In Proceed-
ings of ICMI’03.
A. Kehler. 2000. Cognitive status and form of reference in
multimodal human-computer interaction. In Proceedings of
AAAI’00.
S. Qu and J. Chai. 2006. Salience modeling based on non-
verbal modalities for spoken language understanding. In
Proceedings of ICMI’06.
P. Qvarfordt and S. Zhai. 2005. Conversing with the user based
on eye-gaze patterns. In Proceedings of CHI’05.
D. Roy and N. Mukherjee. 2005. Towards situated speech
understanding: Visual context priming of language models.
Computer Speech and Language, 19(2):227–248.
I. Starker and R. A. Bolt. 1990. A gaze-responsive self-
disclosing display. In Proceedings of CHI’90.
M. K. Tanenhaus, M. J. Spivey-Knowlton, K. M. Eberhard,
and J. E. Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehension. Science,
268:1632–1634.
B. M. Velichkovsky. 1995. Communicating attention-gaze po-
sition transfer in cooperative problem solving. Pragmatics
and Cognition, 3:99–224.
R. Vertegaal. 1999. The gaze groupware system: Mediating
joint attention in multiparty communication and collabora-
tion. In Proceedings of CHI’99.
S. Zhai, C. Morimoto, and S. Ihde. 1999. Manual and gaze
input cascaded (magic) pointing. In Proceedings of CHI’99.
</reference>
<page confidence="0.997858">
291
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.710782">
<title confidence="0.9997065">An Exploration of Eye Gaze in Spoken Language Processing for Multimodal Conversational Interfaces</title>
<author confidence="0.999937">Shaolin Qu Joyce Y Chai</author>
<affiliation confidence="0.933412">Department of Computer Science and Michigan State</affiliation>
<address confidence="0.75865">East Lansing, MI</address>
<abstract confidence="0.999419153846154">Motivated by psycholinguistic findings, we are currently investigating the role of eye gaze in spoken language understanding for multimodal conversational systems. Our assumption is that, during human machine conversation, a user’s eye gaze on the graphical display indicates salient entities on which the user’s attention is focused. The specific domain information about the salient entities is likely to be the content of communication and therefore can be used to constrain speech hypotheses and help language understanding. Based on this assumption, this paper describes an exploratory study that incorporates eye gaze in salience modeling for spoken language processing. Our empirical results show that eye gaze has a potential in improving automated language processing. Eye gaze is subconscious and involuntary during human machine conversation. Our work motivates more in-depth investigation on eye gaze in attention prediction and its implication in automated language processing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Campana</author>
<author>J Baldridge</author>
<author>J Dowding</author>
<author>B Hockey</author>
<author>R Remington</author>
<author>L Stone</author>
</authors>
<title>Using eye movements to determine referents in a spoken dialogue system.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on Perceptive User Interface.</booktitle>
<contexts>
<context position="3477" citStr="Campana et al., 2001" startWordPosition="530" endWordPosition="533"> is subconscious and involuntary in human machine conversation, our work also motivates systematic investigations on how eye gaze contributes to attention prediction and its implications in automated language processing. 2 Related Work Eye gaze has been mainly used in human machine interaction as a pointing mechanism in direct manipulation interfaces (Jacob, 1990; Jacob, 1995; Zhai et al., 1999), as a facilitator in computer supported human human communication (Velichkovsky, 1995; Vertegaal, 1999); or as an additional modality during speech or multimodal communication (Starker and Bolt, 1990; Campana et al., 2001; Kaur et al., 284 Proceedings of NAACL HLT 2007, pages 284–291, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics 2003; Qvarfordt and Zhai, 2005). This last area of investigation is more related to our work. In the context of speech and multimodal communication, studies have shown that speech and eye gaze integration patterns can be modeled reliably for users. For example, by studying patterns of eye gaze and speech in the phrase “move it there”, researchers found that the gaze fixation closest to the intended object begins, with high probability, before the beginnin</context>
</contexts>
<marker>Campana, Baldridge, Dowding, Hockey, Remington, Stone, 2001</marker>
<rawString>E. Campana, J. Baldridge, J. Dowding, B. Hockey, R. Remington, and L. Stone. 2001. Using eye movements to determine referents in a spoken dialogue system. In Proceedings of the Workshop on Perceptive User Interface.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chai</author>
<author>S Qu</author>
</authors>
<title>A salience driven approach to robust input interpretation in multimodal conversational systems.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP’05.</booktitle>
<contexts>
<context position="5160" citStr="Chai and Qu, 2005" startWordPosition="799" endWordPosition="802"> hearer’s memory and their implications in language production and interpretation. Linguistic salience modeling has been used for language interpretations such as reference resolution (Huls et al., 1995; Eisenstein and Christoudias, 2004). Visual salience measures how much attention an entity attracts from a user based on its visual properties. Visual salience can tailor users’ referring expressions and thus can be used for multimodal reference resolution (Kehler, 2000). Our recent work has also investigated salience modeling based on deictic gestures to improve spoken language understanding (Chai and Qu, 2005; Qu and Chai, 2006). 3 Data Collection We conducted user studies to collect speech and eye gaze data. In the experiments, a static 3D bedroom scene was shown to the user. The system verbally asked a user a list of questions one at a time about the bedroom and the user answered the questions by speaking to the system. Fig.1 shows the 14 questions in the experiments. The user’s speech was recorded through an open microphone and the user’s eye gaze was captured by an Eye Link II eye tracker. From 7 users’ experiments, we collected 554 utterances with a vocabulary of 489 words. Each utterance was</context>
</contexts>
<marker>Chai, Qu, 2005</marker>
<rawString>J. Chai and S. Qu. 2005. A salience driven approach to robust input interpretation in multimodal conversational systems. In Proceedings of HLT/EMNLP’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A T Duchowski</author>
</authors>
<title>A breath-first survey of eye tracking applications.</title>
<date>2002</date>
<journal>Behavior Research methods, Instruments, and Computers,</journal>
<volume>33</volume>
<issue>4</issue>
<marker>Duchowski, 2002</marker>
<rawString>A. T. Duchowski. 2002. A breath-first survey of eye tracking applications. Behavior Research methods, Instruments, and Computers, 33(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>C M Christoudias</author>
</authors>
<title>A salience-based approach to gesture-speech alignment.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL’04.</booktitle>
<contexts>
<context position="4781" citStr="Eisenstein and Christoudias, 2004" startWordPosition="736" endWordPosition="739">hat eye gaze has a potential to improve reference resolution in a spoken dialog system (Campana et al., 2001). Furthermore, eye gaze also plays an important role in managing dialog in conversational systems (Qvarfordt and Zhai, 2005). Salience modeling has been used in both natural language and multimodal language processing. Linguistic salience describes entities with their accessibility in a hearer’s memory and their implications in language production and interpretation. Linguistic salience modeling has been used for language interpretations such as reference resolution (Huls et al., 1995; Eisenstein and Christoudias, 2004). Visual salience measures how much attention an entity attracts from a user based on its visual properties. Visual salience can tailor users’ referring expressions and thus can be used for multimodal reference resolution (Kehler, 2000). Our recent work has also investigated salience modeling based on deictic gestures to improve spoken language understanding (Chai and Qu, 2005; Qu and Chai, 2006). 3 Data Collection We conducted user studies to collect speech and eye gaze data. In the experiments, a static 3D bedroom scene was shown to the user. The system verbally asked a user a list of questi</context>
</contexts>
<marker>Eisenstein, Christoudias, 2004</marker>
<rawString>J. Eisenstein and C. M. Christoudias. 2004. A salience-based approach to gesture-speech alignment. In Proceedings of HLT/NAACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z M Griffin</author>
<author>K Bock</author>
</authors>
<title>What the eyes say about speaking.</title>
<date>2000</date>
<journal>Psychological Science,</journal>
<pages>11--274</pages>
<contexts>
<context position="1843" citStr="Griffin and Bock, 2000" startWordPosition="275" endWordPosition="278">omated language processing. 1 Introduction Psycholinguistic experiments have shown that eye gaze is tightly linked to human language processing. Eye gaze is one of the reliable indicators of what a person is “thinking about” (Henderson and Ferreira, 2004). The direction of gaze carries information about the focus of the users attention (Just and Carpenter, 1976). The perceived visual context influences spoken word recognition and mediates syntactic processing (Tanenhaus et al., 1995; Roy and Mukherjee, 2005). In addition, directly before speaking a word, the eyes move to the mentioned object (Griffin and Bock, 2000). Motivated by these psycholinguistic findings, we are currently investigating the role of eye gaze in spoken language understanding during human machine conversation. Through multimodal interfaces, a user can look at a graphic display and converse with the system at the same time. Our assumption is that, during human machine conversation, a user’s eye gaze on the graphical display can indicate salient entities on which the user’s attention is focused. The specific domain information about the salient entities is likely linked to the content of communication and therefore can be used to constr</context>
</contexts>
<marker>Griffin, Bock, 2000</marker>
<rawString>Z. M. Griffin and K. Bock. 2000. What the eyes say about speaking. Psychological Science, 11:274–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Henderson</author>
<author>F Ferreira</author>
</authors>
<title>The interface of language, vision, and action: Eye movements and the visual world.</title>
<date>2004</date>
<publisher>Taylor &amp; Francis.</publisher>
<location>New York:</location>
<contexts>
<context position="1475" citStr="Henderson and Ferreira, 2004" startWordPosition="219" endWordPosition="222">y study that incorporates eye gaze in salience modeling for spoken language processing. Our empirical results show that eye gaze has a potential in improving automated language processing. Eye gaze is subconscious and involuntary during human machine conversation. Our work motivates more in-depth investigation on eye gaze in attention prediction and its implication in automated language processing. 1 Introduction Psycholinguistic experiments have shown that eye gaze is tightly linked to human language processing. Eye gaze is one of the reliable indicators of what a person is “thinking about” (Henderson and Ferreira, 2004). The direction of gaze carries information about the focus of the users attention (Just and Carpenter, 1976). The perceived visual context influences spoken word recognition and mediates syntactic processing (Tanenhaus et al., 1995; Roy and Mukherjee, 2005). In addition, directly before speaking a word, the eyes move to the mentioned object (Griffin and Bock, 2000). Motivated by these psycholinguistic findings, we are currently investigating the role of eye gaze in spoken language understanding during human machine conversation. Through multimodal interfaces, a user can look at a graphic disp</context>
</contexts>
<marker>Henderson, Ferreira, 2004</marker>
<rawString>J. M. Henderson and F. Ferreira. 2004. The interface of language, vision, and action: Eye movements and the visual world. New York: Taylor &amp; Francis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Huls</author>
<author>E Bos</author>
<author>W Classen</author>
</authors>
<title>Automatic referent resolution of deictic and anaphoric expressions.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>1</issue>
<contexts>
<context position="4745" citStr="Huls et al., 1995" startWordPosition="732" endWordPosition="735">rk has also shown that eye gaze has a potential to improve reference resolution in a spoken dialog system (Campana et al., 2001). Furthermore, eye gaze also plays an important role in managing dialog in conversational systems (Qvarfordt and Zhai, 2005). Salience modeling has been used in both natural language and multimodal language processing. Linguistic salience describes entities with their accessibility in a hearer’s memory and their implications in language production and interpretation. Linguistic salience modeling has been used for language interpretations such as reference resolution (Huls et al., 1995; Eisenstein and Christoudias, 2004). Visual salience measures how much attention an entity attracts from a user based on its visual properties. Visual salience can tailor users’ referring expressions and thus can be used for multimodal reference resolution (Kehler, 2000). Our recent work has also investigated salience modeling based on deictic gestures to improve spoken language understanding (Chai and Qu, 2005; Qu and Chai, 2006). 3 Data Collection We conducted user studies to collect speech and eye gaze data. In the experiments, a static 3D bedroom scene was shown to the user. The system ve</context>
</contexts>
<marker>Huls, Bos, Classen, 1995</marker>
<rawString>C. Huls, E. Bos, and W. Classen. 1995. Automatic referent resolution of deictic and anaphoric expressions. Computational Linguistics, 21(1):59–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J K Jacob</author>
</authors>
<title>What you look is what you get: Eye movement-based interaction techniques.</title>
<date>1990</date>
<booktitle>In Proceedings of CHI’90.</booktitle>
<contexts>
<context position="3222" citStr="Jacob, 1990" startWordPosition="494" endWordPosition="495">a salience model to tailor a language model for spoken language processing. Our preliminary results show that eye gaze can be useful in improving spoken language processing and the effect of eye gaze varies among different users. Because eye gaze is subconscious and involuntary in human machine conversation, our work also motivates systematic investigations on how eye gaze contributes to attention prediction and its implications in automated language processing. 2 Related Work Eye gaze has been mainly used in human machine interaction as a pointing mechanism in direct manipulation interfaces (Jacob, 1990; Jacob, 1995; Zhai et al., 1999), as a facilitator in computer supported human human communication (Velichkovsky, 1995; Vertegaal, 1999); or as an additional modality during speech or multimodal communication (Starker and Bolt, 1990; Campana et al., 2001; Kaur et al., 284 Proceedings of NAACL HLT 2007, pages 284–291, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics 2003; Qvarfordt and Zhai, 2005). This last area of investigation is more related to our work. In the context of speech and multimodal communication, studies have shown that speech and eye gaze integration</context>
</contexts>
<marker>Jacob, 1990</marker>
<rawString>R. J. K. Jacob. 1990. What you look is what you get: Eye movement-based interaction techniques. In Proceedings of CHI’90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J K Jacob</author>
</authors>
<title>Eye tracking in advanced interface design.</title>
<date>1995</date>
<booktitle>Advanced Interface Design and Virtual Environments,</booktitle>
<pages>258--288</pages>
<editor>In W. Barfield and T. Furness, editors,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="3235" citStr="Jacob, 1995" startWordPosition="496" endWordPosition="497">del to tailor a language model for spoken language processing. Our preliminary results show that eye gaze can be useful in improving spoken language processing and the effect of eye gaze varies among different users. Because eye gaze is subconscious and involuntary in human machine conversation, our work also motivates systematic investigations on how eye gaze contributes to attention prediction and its implications in automated language processing. 2 Related Work Eye gaze has been mainly used in human machine interaction as a pointing mechanism in direct manipulation interfaces (Jacob, 1990; Jacob, 1995; Zhai et al., 1999), as a facilitator in computer supported human human communication (Velichkovsky, 1995; Vertegaal, 1999); or as an additional modality during speech or multimodal communication (Starker and Bolt, 1990; Campana et al., 2001; Kaur et al., 284 Proceedings of NAACL HLT 2007, pages 284–291, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics 2003; Qvarfordt and Zhai, 2005). This last area of investigation is more related to our work. In the context of speech and multimodal communication, studies have shown that speech and eye gaze integration patterns can</context>
</contexts>
<marker>Jacob, 1995</marker>
<rawString>R. J. K. Jacob. 1995. Eye tracking in advanced interface design. In W. Barfield and T. Furness, editors, Advanced Interface Design and Virtual Environments, pages 258–288. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Just</author>
<author>P Carpenter</author>
</authors>
<title>Eye fixations and cognitive processes.</title>
<date>1976</date>
<journal>Cognitive Psychology,</journal>
<pages>8--441</pages>
<contexts>
<context position="1584" citStr="Just and Carpenter, 1976" startWordPosition="237" endWordPosition="240">w that eye gaze has a potential in improving automated language processing. Eye gaze is subconscious and involuntary during human machine conversation. Our work motivates more in-depth investigation on eye gaze in attention prediction and its implication in automated language processing. 1 Introduction Psycholinguistic experiments have shown that eye gaze is tightly linked to human language processing. Eye gaze is one of the reliable indicators of what a person is “thinking about” (Henderson and Ferreira, 2004). The direction of gaze carries information about the focus of the users attention (Just and Carpenter, 1976). The perceived visual context influences spoken word recognition and mediates syntactic processing (Tanenhaus et al., 1995; Roy and Mukherjee, 2005). In addition, directly before speaking a word, the eyes move to the mentioned object (Griffin and Bock, 2000). Motivated by these psycholinguistic findings, we are currently investigating the role of eye gaze in spoken language understanding during human machine conversation. Through multimodal interfaces, a user can look at a graphic display and converse with the system at the same time. Our assumption is that, during human machine conversation,</context>
</contexts>
<marker>Just, Carpenter, 1976</marker>
<rawString>M. Just and P. Carpenter. 1976. Eye fixations and cognitive processes. Cognitive Psychology, 8:441–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recogniser.</title>
<date>1987</date>
<journal>IEEE Trans. Acous., Speech</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="12109" citStr="Katz, 1987" startWordPosition="2038" endWordPosition="2039"> the starting timestamp of the utterance and T as the duration of the utterance. The priming weight A decides how much the original bigram probability will be tailored by the salient entities indicated by eye gaze. Currently, we set A = 0.67 empirically. We also tried learning the priming weight with an EM algorithm. However, we found out that the learned priming weight performed worse than the empirical one in our experiments. This is probably due to insufficient development data. Bigram probabilities p(wi|wi−1) were estimated by the maximum likelihood estimation using Katz’s backoff method (Katz, 1987) with a frequency cutoff of 1. The same method was used to estimate p(wi|wi−1, e) from the users’ utterance transcripts with entity annotation of e. 5 Application of Salience Driven LMs The salience driven language models can be integrated into speech processing in two stages: an early stage before a word lattice (n-best list) is generated (Fig.4a), or in a late stage where the word lattice (n-best list) is post-processed (Fig.4b). For the early stage integration, the gaze-based salience driven language model is used together with (a) Early stage integration (b) Late stage integration Figure 4</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>S. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recogniser. IEEE Trans. Acous., Speech and Sig. Processing, 35(3):400–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kaur</author>
<author>M Termaine</author>
<author>N Huang</author>
<author>J Wilder</author>
<author>Z Gacovski</author>
<author>F Flippo</author>
<author>C S Mantravadi</author>
</authors>
<title>Where is “it”? event synchronization in gaze-speech input systems.</title>
<date>2003</date>
<booktitle>In Proceedings of ICMI’03.</booktitle>
<contexts>
<context position="4117" citStr="Kaur et al., 2003" startWordPosition="634" endWordPosition="637">ceedings of NAACL HLT 2007, pages 284–291, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics 2003; Qvarfordt and Zhai, 2005). This last area of investigation is more related to our work. In the context of speech and multimodal communication, studies have shown that speech and eye gaze integration patterns can be modeled reliably for users. For example, by studying patterns of eye gaze and speech in the phrase “move it there”, researchers found that the gaze fixation closest to the intended object begins, with high probability, before the beginning of the word “move” (Kaur et al., 2003). Recent work has also shown that eye gaze has a potential to improve reference resolution in a spoken dialog system (Campana et al., 2001). Furthermore, eye gaze also plays an important role in managing dialog in conversational systems (Qvarfordt and Zhai, 2005). Salience modeling has been used in both natural language and multimodal language processing. Linguistic salience describes entities with their accessibility in a hearer’s memory and their implications in language production and interpretation. Linguistic salience modeling has been used for language interpretations such as reference r</context>
</contexts>
<marker>Kaur, Termaine, Huang, Wilder, Gacovski, Flippo, Mantravadi, 2003</marker>
<rawString>M. Kaur, M. Termaine, N. Huang, J. Wilder, Z. Gacovski, F. Flippo, and C. S. Mantravadi. 2003. Where is “it”? event synchronization in gaze-speech input systems. In Proceedings of ICMI’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
</authors>
<title>Cognitive status and form of reference in multimodal human-computer interaction.</title>
<date>2000</date>
<booktitle>In Proceedings of AAAI’00.</booktitle>
<contexts>
<context position="5017" citStr="Kehler, 2000" startWordPosition="778" endWordPosition="779">been used in both natural language and multimodal language processing. Linguistic salience describes entities with their accessibility in a hearer’s memory and their implications in language production and interpretation. Linguistic salience modeling has been used for language interpretations such as reference resolution (Huls et al., 1995; Eisenstein and Christoudias, 2004). Visual salience measures how much attention an entity attracts from a user based on its visual properties. Visual salience can tailor users’ referring expressions and thus can be used for multimodal reference resolution (Kehler, 2000). Our recent work has also investigated salience modeling based on deictic gestures to improve spoken language understanding (Chai and Qu, 2005; Qu and Chai, 2006). 3 Data Collection We conducted user studies to collect speech and eye gaze data. In the experiments, a static 3D bedroom scene was shown to the user. The system verbally asked a user a list of questions one at a time about the bedroom and the user answered the questions by speaking to the system. Fig.1 shows the 14 questions in the experiments. The user’s speech was recorded through an open microphone and the user’s eye gaze was ca</context>
</contexts>
<marker>Kehler, 2000</marker>
<rawString>A. Kehler. 2000. Cognitive status and form of reference in multimodal human-computer interaction. In Proceedings of AAAI’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Qu</author>
<author>J Chai</author>
</authors>
<title>Salience modeling based on nonverbal modalities for spoken language understanding.</title>
<date>2006</date>
<booktitle>In Proceedings of ICMI’06.</booktitle>
<contexts>
<context position="5180" citStr="Qu and Chai, 2006" startWordPosition="803" endWordPosition="806">d their implications in language production and interpretation. Linguistic salience modeling has been used for language interpretations such as reference resolution (Huls et al., 1995; Eisenstein and Christoudias, 2004). Visual salience measures how much attention an entity attracts from a user based on its visual properties. Visual salience can tailor users’ referring expressions and thus can be used for multimodal reference resolution (Kehler, 2000). Our recent work has also investigated salience modeling based on deictic gestures to improve spoken language understanding (Chai and Qu, 2005; Qu and Chai, 2006). 3 Data Collection We conducted user studies to collect speech and eye gaze data. In the experiments, a static 3D bedroom scene was shown to the user. The system verbally asked a user a list of questions one at a time about the bedroom and the user answered the questions by speaking to the system. Fig.1 shows the 14 questions in the experiments. The user’s speech was recorded through an open microphone and the user’s eye gaze was captured by an Eye Link II eye tracker. From 7 users’ experiments, we collected 554 utterances with a vocabulary of 489 words. Each utterance was transcribed and ann</context>
<context position="11105" citStr="Qu and Chai, 2006" startWordPosition="1868" endWordPosition="1871">focusing on entity e within time period t0 — (t0 + t) p(O|W)p(W), where n p(W) = p(wn1� = H k=1 Tf (5) 286 based on how long e has been fixated by gaze fixations among the overall time length of all gaze fixations that fall on entities within t0 — (t0 + T). 4.2 Salience Driven N-gram Model Salience models can be incorporated in different language models, such as bigram models, class-based bigram models, and probabilistic context free grammar. Among these language models, the salience driven bigram model based on deictic gesture has been shown to achieve best performance on speech recognition (Qu and Chai, 2006). In our initial investigation of gaze-based salience, we incorporate the gaze-based salience in a bigram model. The salience driven bigram probability is given by: ps(wi|wi−1) = (1 — A)p(wi|wi−1) + A &amp; p(wi|wi−1, e)pto,T (e) (6) where pt0,T (e) is the salience distribution as modeled in equations (3) and (4). In applying the salience driven bigram model for speech recognition, we set t0 as the starting timestamp of the utterance and T as the duration of the utterance. The priming weight A decides how much the original bigram probability will be tailored by the salient entities indicated by ey</context>
</contexts>
<marker>Qu, Chai, 2006</marker>
<rawString>S. Qu and J. Chai. 2006. Salience modeling based on nonverbal modalities for spoken language understanding. In Proceedings of ICMI’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Qvarfordt</author>
<author>S Zhai</author>
</authors>
<title>Conversing with the user based on eye-gaze patterns.</title>
<date>2005</date>
<booktitle>In Proceedings of CHI’05.</booktitle>
<contexts>
<context position="3649" citStr="Qvarfordt and Zhai, 2005" startWordPosition="555" endWordPosition="558">nd its implications in automated language processing. 2 Related Work Eye gaze has been mainly used in human machine interaction as a pointing mechanism in direct manipulation interfaces (Jacob, 1990; Jacob, 1995; Zhai et al., 1999), as a facilitator in computer supported human human communication (Velichkovsky, 1995; Vertegaal, 1999); or as an additional modality during speech or multimodal communication (Starker and Bolt, 1990; Campana et al., 2001; Kaur et al., 284 Proceedings of NAACL HLT 2007, pages 284–291, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics 2003; Qvarfordt and Zhai, 2005). This last area of investigation is more related to our work. In the context of speech and multimodal communication, studies have shown that speech and eye gaze integration patterns can be modeled reliably for users. For example, by studying patterns of eye gaze and speech in the phrase “move it there”, researchers found that the gaze fixation closest to the intended object begins, with high probability, before the beginning of the word “move” (Kaur et al., 2003). Recent work has also shown that eye gaze has a potential to improve reference resolution in a spoken dialog system (Campana et al.</context>
</contexts>
<marker>Qvarfordt, Zhai, 2005</marker>
<rawString>P. Qvarfordt and S. Zhai. 2005. Conversing with the user based on eye-gaze patterns. In Proceedings of CHI’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roy</author>
<author>N Mukherjee</author>
</authors>
<title>Towards situated speech understanding: Visual context priming of language models.</title>
<date>2005</date>
<journal>Computer Speech and Language,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1733" citStr="Roy and Mukherjee, 2005" startWordPosition="257" endWordPosition="260">. Our work motivates more in-depth investigation on eye gaze in attention prediction and its implication in automated language processing. 1 Introduction Psycholinguistic experiments have shown that eye gaze is tightly linked to human language processing. Eye gaze is one of the reliable indicators of what a person is “thinking about” (Henderson and Ferreira, 2004). The direction of gaze carries information about the focus of the users attention (Just and Carpenter, 1976). The perceived visual context influences spoken word recognition and mediates syntactic processing (Tanenhaus et al., 1995; Roy and Mukherjee, 2005). In addition, directly before speaking a word, the eyes move to the mentioned object (Griffin and Bock, 2000). Motivated by these psycholinguistic findings, we are currently investigating the role of eye gaze in spoken language understanding during human machine conversation. Through multimodal interfaces, a user can look at a graphic display and converse with the system at the same time. Our assumption is that, during human machine conversation, a user’s eye gaze on the graphical display can indicate salient entities on which the user’s attention is focused. The specific domain information a</context>
</contexts>
<marker>Roy, Mukherjee, 2005</marker>
<rawString>D. Roy and N. Mukherjee. 2005. Towards situated speech understanding: Visual context priming of language models. Computer Speech and Language, 19(2):227–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Starker</author>
<author>R A Bolt</author>
</authors>
<title>A gaze-responsive selfdisclosing display.</title>
<date>1990</date>
<booktitle>In Proceedings of CHI’90.</booktitle>
<contexts>
<context position="3455" citStr="Starker and Bolt, 1990" startWordPosition="526" endWordPosition="529"> users. Because eye gaze is subconscious and involuntary in human machine conversation, our work also motivates systematic investigations on how eye gaze contributes to attention prediction and its implications in automated language processing. 2 Related Work Eye gaze has been mainly used in human machine interaction as a pointing mechanism in direct manipulation interfaces (Jacob, 1990; Jacob, 1995; Zhai et al., 1999), as a facilitator in computer supported human human communication (Velichkovsky, 1995; Vertegaal, 1999); or as an additional modality during speech or multimodal communication (Starker and Bolt, 1990; Campana et al., 2001; Kaur et al., 284 Proceedings of NAACL HLT 2007, pages 284–291, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics 2003; Qvarfordt and Zhai, 2005). This last area of investigation is more related to our work. In the context of speech and multimodal communication, studies have shown that speech and eye gaze integration patterns can be modeled reliably for users. For example, by studying patterns of eye gaze and speech in the phrase “move it there”, researchers found that the gaze fixation closest to the intended object begins, with high probabilit</context>
</contexts>
<marker>Starker, Bolt, 1990</marker>
<rawString>I. Starker and R. A. Bolt. 1990. A gaze-responsive selfdisclosing display. In Proceedings of CHI’90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M K Tanenhaus</author>
<author>M J Spivey-Knowlton</author>
<author>K M Eberhard</author>
<author>J E Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science,</journal>
<pages>268--1632</pages>
<contexts>
<context position="1707" citStr="Tanenhaus et al., 1995" startWordPosition="253" endWordPosition="256">man machine conversation. Our work motivates more in-depth investigation on eye gaze in attention prediction and its implication in automated language processing. 1 Introduction Psycholinguistic experiments have shown that eye gaze is tightly linked to human language processing. Eye gaze is one of the reliable indicators of what a person is “thinking about” (Henderson and Ferreira, 2004). The direction of gaze carries information about the focus of the users attention (Just and Carpenter, 1976). The perceived visual context influences spoken word recognition and mediates syntactic processing (Tanenhaus et al., 1995; Roy and Mukherjee, 2005). In addition, directly before speaking a word, the eyes move to the mentioned object (Griffin and Bock, 2000). Motivated by these psycholinguistic findings, we are currently investigating the role of eye gaze in spoken language understanding during human machine conversation. Through multimodal interfaces, a user can look at a graphic display and converse with the system at the same time. Our assumption is that, during human machine conversation, a user’s eye gaze on the graphical display can indicate salient entities on which the user’s attention is focused. The spe</context>
</contexts>
<marker>Tanenhaus, Spivey-Knowlton, Eberhard, Sedivy, 1995</marker>
<rawString>M. K. Tanenhaus, M. J. Spivey-Knowlton, K. M. Eberhard, and J. E. Sedivy. 1995. Integration of visual and linguistic information in spoken language comprehension. Science, 268:1632–1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B M Velichkovsky</author>
</authors>
<title>Communicating attention-gaze position transfer in cooperative problem solving. Pragmatics and Cognition,</title>
<date>1995</date>
<pages>3--99</pages>
<contexts>
<context position="3341" citStr="Velichkovsky, 1995" startWordPosition="511" endWordPosition="512">e gaze can be useful in improving spoken language processing and the effect of eye gaze varies among different users. Because eye gaze is subconscious and involuntary in human machine conversation, our work also motivates systematic investigations on how eye gaze contributes to attention prediction and its implications in automated language processing. 2 Related Work Eye gaze has been mainly used in human machine interaction as a pointing mechanism in direct manipulation interfaces (Jacob, 1990; Jacob, 1995; Zhai et al., 1999), as a facilitator in computer supported human human communication (Velichkovsky, 1995; Vertegaal, 1999); or as an additional modality during speech or multimodal communication (Starker and Bolt, 1990; Campana et al., 2001; Kaur et al., 284 Proceedings of NAACL HLT 2007, pages 284–291, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics 2003; Qvarfordt and Zhai, 2005). This last area of investigation is more related to our work. In the context of speech and multimodal communication, studies have shown that speech and eye gaze integration patterns can be modeled reliably for users. For example, by studying patterns of eye gaze and speech in the phrase “mo</context>
</contexts>
<marker>Velichkovsky, 1995</marker>
<rawString>B. M. Velichkovsky. 1995. Communicating attention-gaze position transfer in cooperative problem solving. Pragmatics and Cognition, 3:99–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Vertegaal</author>
</authors>
<title>The gaze groupware system: Mediating joint attention in multiparty communication and collaboration.</title>
<date>1999</date>
<booktitle>In Proceedings of CHI’99.</booktitle>
<contexts>
<context position="3359" citStr="Vertegaal, 1999" startWordPosition="513" endWordPosition="514"> in improving spoken language processing and the effect of eye gaze varies among different users. Because eye gaze is subconscious and involuntary in human machine conversation, our work also motivates systematic investigations on how eye gaze contributes to attention prediction and its implications in automated language processing. 2 Related Work Eye gaze has been mainly used in human machine interaction as a pointing mechanism in direct manipulation interfaces (Jacob, 1990; Jacob, 1995; Zhai et al., 1999), as a facilitator in computer supported human human communication (Velichkovsky, 1995; Vertegaal, 1999); or as an additional modality during speech or multimodal communication (Starker and Bolt, 1990; Campana et al., 2001; Kaur et al., 284 Proceedings of NAACL HLT 2007, pages 284–291, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics 2003; Qvarfordt and Zhai, 2005). This last area of investigation is more related to our work. In the context of speech and multimodal communication, studies have shown that speech and eye gaze integration patterns can be modeled reliably for users. For example, by studying patterns of eye gaze and speech in the phrase “move it there”, rese</context>
</contexts>
<marker>Vertegaal, 1999</marker>
<rawString>R. Vertegaal. 1999. The gaze groupware system: Mediating joint attention in multiparty communication and collaboration. In Proceedings of CHI’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zhai</author>
<author>C Morimoto</author>
<author>S Ihde</author>
</authors>
<title>Manual and gaze input cascaded (magic) pointing.</title>
<date>1999</date>
<booktitle>In Proceedings of CHI’99.</booktitle>
<contexts>
<context position="3255" citStr="Zhai et al., 1999" startWordPosition="498" endWordPosition="501"> a language model for spoken language processing. Our preliminary results show that eye gaze can be useful in improving spoken language processing and the effect of eye gaze varies among different users. Because eye gaze is subconscious and involuntary in human machine conversation, our work also motivates systematic investigations on how eye gaze contributes to attention prediction and its implications in automated language processing. 2 Related Work Eye gaze has been mainly used in human machine interaction as a pointing mechanism in direct manipulation interfaces (Jacob, 1990; Jacob, 1995; Zhai et al., 1999), as a facilitator in computer supported human human communication (Velichkovsky, 1995; Vertegaal, 1999); or as an additional modality during speech or multimodal communication (Starker and Bolt, 1990; Campana et al., 2001; Kaur et al., 284 Proceedings of NAACL HLT 2007, pages 284–291, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics 2003; Qvarfordt and Zhai, 2005). This last area of investigation is more related to our work. In the context of speech and multimodal communication, studies have shown that speech and eye gaze integration patterns can be modeled reliably</context>
</contexts>
<marker>Zhai, Morimoto, Ihde, 1999</marker>
<rawString>S. Zhai, C. Morimoto, and S. Ihde. 1999. Manual and gaze input cascaded (magic) pointing. In Proceedings of CHI’99.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>