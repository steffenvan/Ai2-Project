<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.991562">
Log-linear Models for Word Alignment
</title>
<author confidence="0.987095">
Yang Liu, Qun Liu and Shouxun Lin
</author>
<affiliation confidence="0.747897">
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<note confidence="0.9474605">
No. 6 Kexueyuan South Road, Haidian District
P. O. Box 2704, Beijing, 100080, China
</note>
<email confidence="0.805775">
{yliu, liuqun, sxlin}@ict.ac.cn
</email>
<sectionHeader confidence="0.977812" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998083133333333">
We present a framework for word align-
ment based on log-linear models. All
knowledge sources are treated as feature
functions, which depend on the source
langauge sentence, the target language
sentence and possible additional vari-
ables. Log-linear models allow statis-
tical alignment models to be easily ex-
tended by incorporating syntactic infor-
mation. In this paper, we use IBM Model
3 alignment probabilities, POS correspon-
dence, and bilingual dictionary cover-
age as features. Our experiments show
that log-linear models significantly out-
perform IBM translation models.
</bodyText>
<sectionHeader confidence="0.992444" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999853071428571">
Word alignment, which can be defined as an object
for indicating the corresponding words in a parallel
text, was first introduced as an intermediate result of
statistical translation models (Brown et al., 1993). In
statistical machine translation, word alignment plays
a crucial role as word-aligned corpora have been
found to be an excellent source of translation-related
knowledge.
Various methods have been proposed for finding
word alignments between parallel texts. There are
generally two categories of alignment approaches:
statistical approaches and heuristic approaches.
Statistical approaches, which depend on a set of
unknown parameters that are learned from training
</bodyText>
<page confidence="0.610268">
459
</page>
<bodyText confidence="0.998092882352941">
data, try to describe the relationship between a bilin-
gual sentence pair (Brown et al., 1993; Vogel and
Ney, 1996). Heuristic approaches obtain word align-
ments by using various similarity functions between
the types of the two languages (Smadja et al., 1996;
Ker and Chang, 1997; Melamed, 2000). The cen-
tral distinction between statistical and heuristic ap-
proaches is that statistical approaches are based on
well-founded probabilistic models while heuristic
ones are not. Studies reveal that statistical alignment
models outperform the simple Dice coefficient (Och
and Ney, 2003).
Finding word alignments between parallel texts,
however, is still far from a trivial work due to the di-
versity of natural languages. For example, the align-
ment of words within idiomatic expressions, free
translations, and missing content or function words
is problematic. When two languages widely differ
in word order, finding word alignments is especially
hard. Therefore, it is necessary to incorporate all
useful linguistic information to alleviate these prob-
lems.
Tiedemann (2003) introduced a word alignment
approach based on combination of association clues.
Clues combination is done by disjunction of single
clues, which are defined as probabilities of associa-
tions. The crucial assumption of clue combination
that clues are independent of each other, however,
is not always true. Och and Ney (2003) proposed
Model 6, a log-linear combination of IBM transla-
tion models and HMM model. Although Model 6
yields better results than naive IBM models, it fails
to include dependencies other than IBM models and
HMM model. Cherry and Lin (2003) developed a
</bodyText>
<note confidence="0.9973955">
Proceedings of the 43rd Annual Meeting of the ACL, pages 459–466,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999515333333334">
statistical model to find word alignments, which al-
low easy integration of context-specific features.
Log-linear models, which are very suitable to in-
corporate additional dependencies, have been suc-
cessfully applied to statistical machine translation
(Och and Ney, 2002). In this paper, we present a
framework for word alignment based on log-linear
models, allowing statistical models to be easily ex-
tended by incorporating additional syntactic depen-
dencies. We use IBM Model 3 alignment proba-
bilities, POS correspondence, and bilingual dictio-
nary coverage as features. Our experiments show
that log-linear models significantly outperform IBM
translation models.
We begin by describing log-linear models for
word alignment. The design of feature functions
is discussed then. Next, we present the training
method and the search algorithm for log-linear mod-
els. We will follow with our experimental results
and conclusion and close with a discussion of possi-
ble future directions.
</bodyText>
<sectionHeader confidence="0.996803" genericHeader="method">
2 Log-linear Models
</sectionHeader>
<bodyText confidence="0.996898545454545">
Formally, we use following definition for alignment.
Given a source (’English’) sentence e = eI1 = e1,
... , ei, ..., eI and a target language (’French’) sen-
tence f = fJ1 = f1, ..., fj, ..., fJ. We define a link
l = (i, j) to exist if ei and fj are translation (or part
of a translation) of one another. We define the null
link l = (i, 0) to exist if ei does not correspond to a
translation for any French word in f. The null link
l = (0, j) is defined similarly. An alignment a is
defined as a subset of the Cartesian product of the
word positions:
</bodyText>
<equation confidence="0.836813">
a C {(i,j) : i = 0,...,I;j = 0,...,J} (1)
</equation>
<bodyText confidence="0.999838888888889">
We define the alignment problem as finding the
alignment a that maximizes Pr(a  |e, f) given e and
f.
We directly model the probability Pr(a  |e, f).
An especially well-founded framework is maximum
entropy (Berger et al., 1996). In this framework, we
have a set of M feature functions hm(a, e, f), m =
1, ... , M. For each feature function, there exists
a model parameter Am, m = 1, ... , M. The direct
</bodyText>
<equation confidence="0.94511275">
alignment probability is given by:
exp[PmM 1 Amhm(a, e, f)]
Prae )
( I , f ) _ Ea, exp [PmM= 1 Amhm(al, e, f ]
</equation>
<bodyText confidence="0.979368833333333">
(2)
This approach has been suggested by (Papineni et
al., 1997) for a natural language understanding task
and successfully applied to statistical machine trans-
lation by (Och and Ney, 2002).
We obtain the following decision rule:
</bodyText>
<equation confidence="0.850617">
3/4 Amhm(a,e,f) (3)
</equation>
<bodyText confidence="0.998561176470588">
Typically, the source language sentence e and the
target sentence f are the fundamental knowledge
sources for the task of finding word alignments. Lin-
guistic data, which can be used to identify associ-
ations between lexical items are often ignored by
traditional word alignment approaches. Linguistic
tools such as part-of-speech taggers, parsers, named-
entity recognizers have become more and more ro-
bust and available for many languages by now. It
is important to make use of linguistic information
to improve alignment strategies. Treated as feature
functions, syntactic dependencies can be easily in-
corporated into log-linear models.
In order to incorporate a new dependency which
contains extra information other than the bilingual
sentence pair, we modify Eq.2 by adding a new vari-
able v:
</bodyText>
<equation confidence="0.9156315">
a e f v exp[PMm= 1 Amhm(a, e, f, v)]
Pr
( I , , ) _ Pa, exp[PMm=1 Amhm(al, e, f, v)]
(4)
Accordingly, we get a new decision rule:
3/4 Amhm(a, e, f, v) (5)
</equation>
<bodyText confidence="0.9999">
Note that our log-linear models are different from
Model 6 proposed by Och and Ney (2003), which
defines the alignment problem as finding the align-
ment a that maximizes Pr(f, a  |e) given e.
</bodyText>
<sectionHeader confidence="0.98275" genericHeader="method">
3 Feature Functions
</sectionHeader>
<bodyText confidence="0.9994185">
In this paper, we use IBM translation Model 3 as the
base feature of our log-linear models. In addition,
we also make use of syntactic information such as
part-of-speech tags and bilingual dictionaries.
</bodyText>
<equation confidence="0.925979111111111">
aˆ = argmax
a
1/2 XM
m=1
aˆ = argmax
a
M
1/2 X
m=1
</equation>
<page confidence="0.537755">
460
</page>
<subsectionHeader confidence="0.977057">
3.1 IBM Translation Models
</subsectionHeader>
<bodyText confidence="0.999518545454546">
Brown et al. (1993) proposed a series of statisti-
cal models of the translation process. IBM trans-
lation models try to model the translation probabil-
ity Pr(fJ1 |eI 1), which describes the relationship be-
tween a source language sentence eI1 and a target
language sentence fJ1 . In statistical alignment mod-
els Pr(fJ1 , aJ1 |eI1), a ’hidden’ alignment a = aJ1 is
introduced, which describes a mapping from a tar-
get position j to a source position i = aj. The
relationship between the translation model and the
alignment model is given by:
</bodyText>
<equation confidence="0.943291">
Pr(fJ1 , aJ1 |eI1) (6)
</equation>
<bodyText confidence="0.99956525">
Although IBM models are considered more co-
herent than heuristic models, they have two draw-
backs. First, IBM models are restricted in a way
such that each target word fj is assigned to exactly
one source word eaj. A more general way is to
model alignment as an arbitrary relation between
source and target language positions. Second, IBM
models are typically language-independent and may
fail to tackle problems occurred due to specific lan-
guages.
In this paper, we use Model 3 as our base feature
function, which is given by 1:
</bodyText>
<equation confidence="0.999034714285714">
h(a, e, f) = Pr(fJ1 , aJ1 |eI1)
l
(m − O0 p0m−2φ0p1φ0 7 7
00 i1=11
m
fj t(fj|eaj)d(j|aj, 1, m) (7)
j=1
</equation>
<bodyText confidence="0.9999135">
We distinguish between two translation directions
to use Model 3 as feature functions: treating English
as source language and French as target language or
vice versa.
</bodyText>
<subsectionHeader confidence="0.997192">
3.2 POS Tags Transition Model
</subsectionHeader>
<bodyText confidence="0.96898096">
The first linguistic information we adopt other than
the source language sentence e and the target lan-
guage sentence f is part-of-speech tags. The use
of POS information for improving statistical align-
ment quality of the HMM-based model is described
1If there is a target word which is assigned to more than one
source words, h(a, e, f) = 0.
in (Toutanova et al., 2002). They introduce addi-
tional lexicon probability for POS tags in both lan-
guages.
In IBM models as well as HMM models, when
one needs the model to take new information into
account, one must create an extended model which
can base its parameters on the previous model. In
log-linear models, however, new information can be
easily incorporated.
We use a POS Tags Transition Model as a fea-
ture function. This feature learns POS Tags tran-
sition probabilities from held-out data (via simple
counting) and then applies the learned distributions
to the ranking of various word alignments. We
define eT = eT1I = eT1, ... , eTi, ... , eTI and
fT = fT1J = fT1, ... , fTj, ... , fTJ as POS tag
sequences of the sentence pair e and f. POS Tags
Transition Model is formally described as:
</bodyText>
<equation confidence="0.9806385">
P r(fT|a, eT) = fj t(fTa(j)|eTa(i)) (8)
a
</equation>
<bodyText confidence="0.999971666666667">
where a is an element of a, a(i) is the corresponding
source position of a and a(j) is the target position.
Hence, the feature function is:
</bodyText>
<equation confidence="0.805628">
fjh(a, e, f, eT, fT) = t(fTa(j)|eTa(i)) (9)
a
</equation>
<bodyText confidence="0.999967">
We still distinguish between two translation direc-
tions to use POS tags Transition Model as feature
functions: treating English as source language and
French as target language or vice versa.
</bodyText>
<subsectionHeader confidence="0.999582">
3.3 Bilingual Dictionary
</subsectionHeader>
<bodyText confidence="0.999665714285714">
A conventional bilingual dictionary can be consid-
ered an additional knowledge source. We could use
a feature that counts how many entries of a conven-
tional lexicon co-occur in a given alignment between
the source sentence and the target sentence. There-
fore, the weight for the provided conventional dic-
tionary can be learned. The intuition is that the con-
ventional dictionary is expected to be more reliable
than the automatically trained lexicon and therefore
should get a larger weight.
We define a bilingual dictionary as a set of entries:
D = {(e, f, conf)}. e is a source language word,
f is a target langauge word, and conf is a positive
real-valued number (usually, conf = 1.0) assigned
</bodyText>
<equation confidence="0.9187855">
E
Pr(fJ 1 |eI 1) =
ai
1
Oi�n(Oi|ei) x
461
</equation>
<bodyText confidence="0.996852666666667">
by lexicographers to evaluate the validity of the en-
try. Therefore, the feature function using a bilingual
dictionary is:
</bodyText>
<equation confidence="0.945483">
�h(a, e, f, D) = occur(ea(i),fa(j),D) (10)
a
</equation>
<bodyText confidence="0.74513">
where
</bodyText>
<equation confidence="0.9750072">
�
conf if (e, f) occurs in D
occur(e, f, D) =
0 else
(11)
</equation>
<sectionHeader confidence="0.964086" genericHeader="method">
4 Training
</sectionHeader>
<bodyText confidence="0.999935181818182">
We use the GIS (Generalized Iterative Scaling) al-
gorithm (Darroch and Ratcliff, 1972) to train the
model parameters λM1 of the log-linear models ac-
cording to Eq. 4. By applying suitable transforma-
tions, the GIS algorithm is able to handle any type of
real-valued features. In practice, We use YASMET
2 written by Franz J. Och for performing training.
The renormalization needed in Eq. 4 requires a
sum over a large number of possible alignments. If
e has length l and f has length m, there are pos-
sible 2lm alignments between e and f (Brown et
al., 1993). It is unrealistic to enumerate all possi-
ble alignments when lm is very large. Hence, we
approximate this sum by sampling the space of all
possible alignments by a large set of highly proba-
ble alignments. The set of considered alignments are
also called n-best list of alignments.
We train model parameters on a development cor-
pus, which consists of hundreds of manually-aligned
bilingual sentence pairs. Using an n-best approx-
imation may result in the problem that the param-
eters trained with the GIS algorithm yield worse
alignments even on the development corpus. This
can happen because with the modified model scaling
factors the n-best list can change significantly and
can include alignments that have not been taken into
account in training. To avoid this problem, we iter-
atively combine n-best lists to train model parame-
ters until the resulting n-best list does not change,
as suggested by Och (2002). However, as this train-
ing procedure is based on maximum likelihood cri-
terion, there is only a loose relation to the final align-
ment quality on unseen bilingual texts. In practice,
</bodyText>
<footnote confidence="0.939037">
2Available at http://www.fjoch.com/YASMET.html
</footnote>
<bodyText confidence="0.999868857142857">
having a series of model parameters when the itera-
tion ends, we select the model parameters that yield
best alignments on the development corpus.
After the bilingual sentences in the develop-
ment corpus are tokenized (or segmented) and POS
tagged, they can be used to train POS tags transition
probabilities by counting relative frequencies:
</bodyText>
<equation confidence="0.987121">
p(fT  |eT) = N (fTT, eT)
</equation>
<bodyText confidence="0.988594666666667">
N(eHere, NA(fT, eT) is the frequency that the POS tag
fT is aligned to POS tag eT and N(eT) is the fre-
quency of eT in the development corpus.
</bodyText>
<sectionHeader confidence="0.984492" genericHeader="method">
5 Search
</sectionHeader>
<bodyText confidence="0.999713153846154">
We use a greedy search algorithm to search the
alignment with highest probability in the space of all
possible alignments. A state in this space is a partial
alignment. A transition is defined as the addition of
a single link to the current state. Our start state is
the empty alignment, where all words in e and f are
assigned to null. A terminal state is a state in which
no more links can be added to increase the probabil-
ity of the current alignment. Our task is to find the
terminal state with the highest probability.
We can compute gain, which is a heuristic func-
tion, instead of probability for efficiency. A gain is
defined as follows:
</bodyText>
<equation confidence="0.9936005">
exp[�M m=1 λmhm(a U l, e, f)]
gain(a, l) = (12)
</equation>
<bodyText confidence="0.834875">
exp[EMm= 1 λmhm(a, e, f)]
where l = (i, j) is a link added to a.
The greedy search algorithm for general log-
linear models is formally described as follows:
Input: e, f, eT, fT, and D
Output: a
</bodyText>
<listItem confidence="0.999119285714286">
1. Start with a = φ.
2. Do for each l = (i, j) and l V a:
Compute gain(a, l)
3. Terminate if bl, gain(a, l) &lt; 1.
4. Add the link l�with the maximal gain(a, l)
to a.
5. Goto 2.
</listItem>
<page confidence="0.943727">
462
</page>
<bodyText confidence="0.999742833333333">
The above search algorithm, however, is not effi-
cient for our log-linear models. It is time-consuming
for each feature to figure out a probability when
adding a new link, especially when the sentences
are very long. For our models, gain(a, l) can be
obtained in a more efficient way 3:
</bodyText>
<figure confidence="0.683249212121212">
gain(a, l) = M to Chm(a U l, e, f)1 13
E m g \ hm (a, e, f) J ( )
m=1
Note that we restrict that h(a, e, f) &gt; 0 for all fea-
ture functions.
The original terminational condition for greedy
search algorithm is:
exp[�M m=1 Amhm(a U l, e, f)]
gain(a, l) = 1.0
exp [EMm= 1 Amhm(a, e, f)]
That is:
M
E Am[hm(a U l, e, f) − hm(a, e, f)] G 0.0
m=1
By introducing gain threshold t, we obtain a new
terminational condition:
m=1
E
M Amlog(hm(a U l,e,f)1 &lt;t
\\ hm (a, e, f ) JJ
where
Am ( log hm (a U l, e, f)
t(hm(a,e,f) /
I−[hm(a U l, e, f) − hm(a, e, f)]
Note that we restrict h(a, e, f) &gt; 0 for all feature
functions. Gain threshold t is a real-valued number,
which can be optimized on the development corpus.
Therefore, we have a new search algorithm:
Input: e, f, eT, fT, D and t
Output: a
1. Start with a = �.
2. Do for each l = (i, j) and l V a:
Compute gain(a, l)
</figure>
<bodyText confidence="0.877472">
3We still call the new heuristic function gain to reduce no-
tational overhead, although the gain in Eq. 13 is not equivalent
to the one in Eq. 12.
</bodyText>
<equation confidence="0.974482">
M
t= E
m=1
</equation>
<listItem confidence="0.9984385">
3. Terminate if Vl, gain(a, l) G t.
4. Add the link lˆwith the maximal gain(a, l)
to a.
5. Goto 2.
</listItem>
<bodyText confidence="0.85642075">
The gain threshold t depends on the added link
l. We remove this dependency for simplicity when
using it in search algorithm by treating it as a fixed
real-valued number.
</bodyText>
<sectionHeader confidence="0.993766" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.9997295">
We present in this section results of experiments on
a parallel corpus of Chinese-English texts. Statis-
tics for the corpus are shown in Table 1. We use a
training corpus, which is used to train IBM transla-
tion models, a bilingual dictionary, a development
corpus, and a test corpus.
</bodyText>
<table confidence="0.999292666666667">
Chinese English
Train Sentences 108 925
Words 3 784 106 3 862 637
Vocabulary 49 962 55 698
Dict Entries 415 753
Vocabulary 206 616 203 497
Dev Sentences 435
Words 11 462 14 252
Ave. SentLen 26.35 32.76
Test Sentences 500
Words 13 891 15 291
Ave. SentLen 27.78 30.58
</table>
<tableCaption confidence="0.993584">
Table 1. Statistics of training corpus (Train), bilin-
</tableCaption>
<bodyText confidence="0.950555533333333">
gual dictionary (Dict), development corpus (Dev),
and test corpus (Test).
The Chinese sentences in both the development
and test corpus are segmented and POS tagged by
ICTCLAS (Zhang et al., 2003). The English sen-
tences are tokenized by a simple tokenizer of ours
and POS tagged by a rule-based tagger written by
Eric Brill (Brill, 1995). We manually aligned 935
sentences, in which we selected 500 sentences as
test corpus. The remaining 435 sentences are used
as development corpus to train POS tags transition
probabilities and to optimize the model parameters
and gain threshold.
Provided with human-annotated word-level align-
ment, we use precision, recall and AER (Och and
</bodyText>
<page confidence="0.892148">
463
</page>
<table confidence="0.99982025">
Size of Training Corpus
1K 5K 9K 39K 109K
Model 3 E → C 0.4497 0.4081 0.4009 0.3791 0.3745
Model 3 C → E 0.4688 0.4261 0.4221 0.3856 0.3469
Intersection 0.4588 0.4106 0.4044 0.3823 0.3687
Union 0.4596 0.4210 0.4157 0.3824 0.3703
Refined Method 0.4154 0.3586 0.3499 0.3153 0.3068
Model 3 E → C 0.4490 0.3987 0.3834 0.3639 0.3533
+ Model 3 C → E 0.3970 0.3317 0.3217 0.2949 0.2850
+ POS E → C 0.3828 0.3182 0.3082 0.2838 0.2739
+ POS C → E 0.3795 0.3160 0.3032 0.2821 0.2726
+ Dict 0.3650 0.3092 0.2982 0.2738 0.2685
</table>
<tableCaption confidence="0.999953">
Table 2. Comparison of AER for results of using IBM Model 3 (GIZA++) and log-linear models.
</tableCaption>
<bodyText confidence="0.960006">
Ney, 2003) for scoring the viterbi alignments of each
model against gold-standard annotated alignments:
</bodyText>
<equation confidence="0.998048">
recall = |A ∩ S|
|S|
AER = 1 − |A ∩ S |+ |A ∩ P|
|A |+ |S|
</equation>
<bodyText confidence="0.994355981481482">
where A is the set of word pairs aligned by word
alignment systems, S is the set marked in the gold
standard as ”sure” and P is the set marked as ”pos-
sible” (including the ”sure” pairs). In our Chinese-
English corpus, only one type of alignment was
marked, meaning that S = P.
In the following, we present the results of log-
linear models for word alignment. We used GIZA++
package (Och and Ney, 2003) to train IBM transla-
tion models. The training scheme is 15H535, which
means that Model 1 are trained for five iterations,
HMM model for five iterations and finally Model
3 for five iterations. Except for changing the iter-
ations for each model, we use default configuration
of GIZA++. After that, we used three types of meth-
ods for performing a symmetrization of IBM mod-
els: intersection, union, and refined methods (Och
and Ney , 2003).
The base feature of our log-linear models, IBM
Model 3, takes the parameters generated by GIZA++
as parameters for itself. In other words, our log-
linear models share GIZA++ with the same parame-
ters apart from POS transition probability table and
bilingual dictionary.
Table 2 compares the results of our log-linear
models with IBM Model 3. From row 3 to row 7
are results obtained by IBM Model 3. From row 8
to row 12 are results obtained by log-linear models.
As shown in Table 2, our log-linear models
achieve better results than IBM Model 3 in all train-
ing corpus sizes. Considering Model 3 E → C of
GIZA++ and ours alone, greedy search algorithm
described in Section 5 yields surprisingly better
alignments than hillclimbing algorithm in GIZA++.
Table 3 compares the results of log-linear mod-
els with IBM Model 5. The training scheme is
15H5354555. Our log-linear models still make use
of the parameters generated by GIZA++.
Comparing Table 3 with Table 2, we notice that
our log-linear models yield slightly better align-
ments by employing parameters generated by the
training scheme 15H5354555 rather than 15H535,
which can be attributed to improvement of param-
eters after further Model 4 and Model 5 training.
For log-linear models, POS information and an
additional dictionary are used, which is not the case
for GIZA++/IBM models. However, treated as a
method for performing symmetrization, log-linear
combination alone yields better results than intersec-
tion, union, and refined methods.
Figure 1 shows how gain threshold has an effect
on precision, recall and AER with fixed model scal-
ing factors.
Figure 2 shows the effect of number of features
</bodyText>
<figure confidence="0.693240666666667">
|A ∩ P|
precision =
|A|
</figure>
<page confidence="0.709092">
464
</page>
<table confidence="0.998561916666667">
Size of Training Corpus
1K 5K 9K 39K 109K
Model 5 E → C 0.4384 0.3934 0.3853 0.3573 0.3429
Model 5 C → E 0.4564 0.4067 0.3900 0.3423 0.3239
Intersection 0.4432 0.3916 0.3798 0.3466 0.3267
Union 0.4499 0.4051 0.3923 0.3516 0.3375
Refined Method 0.4106 0.3446 0.3262 0.2878 0.2748
Model 3 E → C 0.4372 0.3873 0.3724 0.3456 0.3334
+ Model 3 C → E 0.3920 0.3269 0.3167 0.2842 0.2727
+ POS E → C 0.3807 0.3122 0.3039 0.2732 0.2667
+ POS C → E 0.3731 0.3091 0.3017 0.2722 0.2657
+ Dict 0.3612 0.3046 0.2943 0.2658 0.2625
</table>
<tableCaption confidence="0.999793">
Table 3. Comparison of AER for results of using IBM Model 5 (GIZA++) and log-linear models.
</tableCaption>
<figure confidence="0.563209">
time consumed for searching (second)
</figure>
<figureCaption confidence="0.8765845">
Figure 1. Precision, recall and AER over different
gain thresholds with the same model scaling factors.
</figureCaption>
<bodyText confidence="0.9649506">
and size of training corpus on search efficiency for
log-linear models.
Table 4 shows the resulting normalized model
scaling factors. We see that adding new features also
has an effect on the other model scaling factors.
</bodyText>
<sectionHeader confidence="0.996433" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.996587166666667">
We have presented a framework for word alignment
based on log-linear models between parallel texts. It
allows statistical models easily extended by incor-
porating syntactic information. We take IBM Model
3 as base feature and use syntactic information such
as POS tags and bilingual dictionary. Experimental
</bodyText>
<figureCaption confidence="0.9789125">
Figure 2. Effect of number of features and size of
training corpus on search efficiency.
</figureCaption>
<table confidence="0.999157333333333">
MEC +MCE +PEC +PCE +Dict
A1 1.000 0.466 0.291 0.202 0.151
A2 - 0.534 0.312 0.212 0.167
A3 - - 0.397 0.270 0.257
A4 - - - 0.316 0.306
A5 - - - - 0.119
</table>
<tableCaption confidence="0.991114">
Table 4. Resulting model scaling factors: A1: Model
</tableCaption>
<bodyText confidence="0.697220333333333">
3 E → C (MEC); A2: Model 3 C → E (MCE); A3:
POS E → C (PEC); A4: POS C → E (PCE); A5: Dict
(normalized such that E5m_1 Am = 1).
results show that log-linear models for word align-
ment significantly outperform IBM translation mod-
els. However, the search algorithm we proposed is
</bodyText>
<page confidence="0.936184">
465
</page>
<bodyText confidence="0.999889583333333">
supervised, relying on a hand-aligned bilingual cor-
pus, while the baseline approach of IBM alignments
is unsupervised.
Currently, we only employ three types of knowl-
edge sources as feature functions. Syntax-based
translation models, such as tree-to-string model (Ya-
mada and Knight, 2001) and tree-to-tree model
(Gildea, 2003), may be very suitable to be added into
log-linear models.
It is promising to optimize the model parameters
directly with respect to AER as suggested in statisti-
cal machine translation (Och, 2003).
</bodyText>
<sectionHeader confidence="0.93015" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99529">
This work is supported by National High Technol-
ogy Research and Development Program contract
”Generally Technical Research and Basic Database
Establishment of Chinese Platform” (Subject No.
2004AA114010).
</bodyText>
<sectionHeader confidence="0.99329" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999443192307692">
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
DellaPietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39-72, March.
Eric Brill. 1995. Transformation-based-error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4), December.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263-311.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL), Sapporo, Japan.
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear models. Annals of Mathe-
matical Statistics, 43:1470-1480.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics (ACL), Sapporo, Japan.
Sue J. Ker and Jason S. Chang. 1997. A class-based ap-
proach to word alignment. Computational Linguistics,
23(2):313-343, June.
I. Dan Melamed 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221-249, June.
Franz J. Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 295-302, Philadelphia, PA,
July.
Franz J. Och. 2002. Statistical Machine Translation:
From Single-Word Models to Alignment Templates.
Ph.D. thesis, Computer Science Department, RWTH
Aachen, Germany, October.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics (ACL), pages: 160-167, Sapporo, Japan.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19-51, March.
Kishore A. Papineni, Salim Roukos, and Todd Ward.
1997. Feature-based language understanding. In Eu-
ropean Conf. on Speech Communication and Technol-
ogy, pages 1435-1438, Rhodes, Greece, September.
Frank Smadja, Vasileios Hatzivassiloglou, and Kathleen
R. McKeown 1996. Translating collocations for bilin-
gual lexicons: A statistical approach. Computational
Linguistics, 22(1):1-38, March.
Jorg Tiedemann. 2003. Combining clues for word align-
ment. In Proceedings of the 10th Conference of Euro-
pean Chapter of the ACL (EACL), Budapest, Hungary,
April.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2003. Extensions to HMM-based statistical
word alignment models. In Proceedings of Empirical
Methods in Natural Langauge Processing, Philadel-
phia, PA.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th Int. Conf. on Com-
putational Linguistics, pages 836-841, Copenhagen,
Denmark, August.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical machine translation model. In Pro-
ceedings of the 39th Annual Meeting ofthe Association
for Computational Linguistics (ACL), pages: 523-530,
Toulouse, France, July.
Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun Liu.
2003. HHMM-based Chinese lexical analyzer ICT-
CLAS. In Proceedings of the second SigHan Work-
shop affiliated with 41th ACL, pages: 184-187, Sap-
poro, Japan.
</reference>
<page confidence="0.984219">
466
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.833863">
<title confidence="0.996573">Log-linear Models for Word Alignment</title>
<author confidence="0.987595">Qun Liu Lin Liu</author>
<affiliation confidence="0.9872085">Institute of Computing Technology Chinese Academy of Sciences</affiliation>
<address confidence="0.9466355">No. 6 Kexueyuan South Road, Haidian District P. O. Box 2704, Beijing, 100080, China</address>
<email confidence="0.994339">liuqun,</email>
<abstract confidence="0.9963341875">We present a framework for word alignment based on log-linear models. All knowledge sources are treated as feature functions, which depend on the source langauge sentence, the target language sentence and possible additional variables. Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information. In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models significantly outperform IBM translation models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J DellaPietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="5120" citStr="Berger et al., 1996" startWordPosition="800" endWordPosition="803"> define a link l = (i, j) to exist if ei and fj are translation (or part of a translation) of one another. We define the null link l = (i, 0) to exist if ei does not correspond to a translation for any French word in f. The null link l = (0, j) is defined similarly. An alignment a is defined as a subset of the Cartesian product of the word positions: a C {(i,j) : i = 0,...,I;j = 0,...,J} (1) We define the alignment problem as finding the alignment a that maximizes Pr(a |e, f) given e and f. We directly model the probability Pr(a |e, f). An especially well-founded framework is maximum entropy (Berger et al., 1996). In this framework, we have a set of M feature functions hm(a, e, f), m = 1, ... , M. For each feature function, there exists a model parameter Am, m = 1, ... , M. The direct alignment probability is given by: exp[PmM 1 Amhm(a, e, f)] Prae ) ( I , f ) _ Ea, exp [PmM= 1 Amhm(al, e, f ] (2) This approach has been suggested by (Papineni et al., 1997) for a natural language understanding task and successfully applied to statistical machine translation by (Och and Ney, 2002). We obtain the following decision rule: 3/4 Amhm(a,e,f) (3) Typically, the source language sentence e and the target sentenc</context>
</contexts>
<marker>Berger, Pietra, DellaPietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. DellaPietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-72, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based-error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="16864" citStr="Brill, 1995" startWordPosition="2934" endWordPosition="2935">s 3 784 106 3 862 637 Vocabulary 49 962 55 698 Dict Entries 415 753 Vocabulary 206 616 203 497 Dev Sentences 435 Words 11 462 14 252 Ave. SentLen 26.35 32.76 Test Sentences 500 Words 13 891 15 291 Ave. SentLen 27.78 30.58 Table 1. Statistics of training corpus (Train), bilingual dictionary (Dict), development corpus (Dev), and test corpus (Test). The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS (Zhang et al., 2003). The English sentences are tokenized by a simple tokenizer of ours and POS tagged by a rule-based tagger written by Eric Brill (Brill, 1995). We manually aligned 935 sentences, in which we selected 500 sentences as test corpus. The remaining 435 sentences are used as development corpus to train POS tags transition probabilities and to optimize the model parameters and gain threshold. Provided with human-annotated word-level alignment, we use precision, recall and AER (Och and 463 Size of Training Corpus 1K 5K 9K 39K 109K Model 3 E → C 0.4497 0.4081 0.4009 0.3791 0.3745 Model 3 C → E 0.4688 0.4261 0.4221 0.3856 0.3469 Intersection 0.4588 0.4106 0.4044 0.3823 0.3687 Union 0.4596 0.4210 0.4157 0.3824 0.3703 Refined Method 0.4154 0.35</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based-error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4), December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<marker>Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A probability model to improve word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3154" citStr="Cherry and Lin (2003)" startWordPosition="470" endWordPosition="473">ic information to alleviate these problems. Tiedemann (2003) introduced a word alignment approach based on combination of association clues. Clues combination is done by disjunction of single clues, which are defined as probabilities of associations. The crucial assumption of clue combination that clues are independent of each other, however, is not always true. Och and Ney (2003) proposed Model 6, a log-linear combination of IBM translation models and HMM model. Although Model 6 yields better results than naive IBM models, it fails to include dependencies other than IBM models and HMM model. Cherry and Lin (2003) developed a Proceedings of the 43rd Annual Meeting of the ACL, pages 459–466, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics statistical model to find word alignments, which allow easy integration of context-specific features. Log-linear models, which are very suitable to incorporate additional dependencies, have been successfully applied to statistical machine translation (Och and Ney, 2002). In this paper, we present a framework for word alignment based on log-linear models, allowing statistical models to be easily extended by incorporating additional syntactic depen</context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>Colin Cherry and Dekang Lin. 2003. A probability model to improve word alignment. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>43--1470</pages>
<contexts>
<context position="11138" citStr="Darroch and Ratcliff, 1972" startWordPosition="1866" endWordPosition="1869">ed lexicon and therefore should get a larger weight. We define a bilingual dictionary as a set of entries: D = {(e, f, conf)}. e is a source language word, f is a target langauge word, and conf is a positive real-valued number (usually, conf = 1.0) assigned E Pr(fJ 1 |eI 1) = ai 1 Oi�n(Oi|ei) x 461 by lexicographers to evaluate the validity of the entry. Therefore, the feature function using a bilingual dictionary is: �h(a, e, f, D) = occur(ea(i),fa(j),D) (10) a where � conf if (e, f) occurs in D occur(e, f, D) = 0 else (11) 4 Training We use the GIS (Generalized Iterative Scaling) algorithm (Darroch and Ratcliff, 1972) to train the model parameters λM1 of the log-linear models according to Eq. 4. By applying suitable transformations, the GIS algorithm is able to handle any type of real-valued features. In practice, We use YASMET 2 written by Franz J. Och for performing training. The renormalization needed in Eq. 4 requires a sum over a large number of possible alignments. If e has length l and f has length m, there are possible 2lm alignments between e and f (Brown et al., 1993). It is unrealistic to enumerate all possible alignments when lm is very large. Hence, we approximate this sum by sampling the spac</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. Annals of Mathematical Statistics, 43:1470-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Loosely tree-based alignment for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Sapporo, Japan.</location>
<marker>Gildea, 2003</marker>
<rawString>Daniel Gildea. 2003. Loosely tree-based alignment for machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sue J Ker</author>
<author>Jason S Chang</author>
</authors>
<title>A class-based approach to word alignment.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--2</pages>
<contexts>
<context position="1804" citStr="Ker and Chang, 1997" startWordPosition="265" endWordPosition="268"> source of translation-related knowledge. Various methods have been proposed for finding word alignments between parallel texts. There are generally two categories of alignment approaches: statistical approaches and heuristic approaches. Statistical approaches, which depend on a set of unknown parameters that are learned from training 459 data, try to describe the relationship between a bilingual sentence pair (Brown et al., 1993; Vogel and Ney, 1996). Heuristic approaches obtain word alignments by using various similarity functions between the types of the two languages (Smadja et al., 1996; Ker and Chang, 1997; Melamed, 2000). The central distinction between statistical and heuristic approaches is that statistical approaches are based on well-founded probabilistic models while heuristic ones are not. Studies reveal that statistical alignment models outperform the simple Dice coefficient (Och and Ney, 2003). Finding word alignments between parallel texts, however, is still far from a trivial work due to the diversity of natural languages. For example, the alignment of words within idiomatic expressions, free translations, and missing content or function words is problematic. When two languages widel</context>
</contexts>
<marker>Ker, Chang, 1997</marker>
<rawString>Sue J. Ker and Jason S. Chang. 1997. A class-based approach to word alignment. Computational Linguistics, 23(2):313-343, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--2</pages>
<contexts>
<context position="1820" citStr="Melamed, 2000" startWordPosition="269" endWordPosition="270">n-related knowledge. Various methods have been proposed for finding word alignments between parallel texts. There are generally two categories of alignment approaches: statistical approaches and heuristic approaches. Statistical approaches, which depend on a set of unknown parameters that are learned from training 459 data, try to describe the relationship between a bilingual sentence pair (Brown et al., 1993; Vogel and Ney, 1996). Heuristic approaches obtain word alignments by using various similarity functions between the types of the two languages (Smadja et al., 1996; Ker and Chang, 1997; Melamed, 2000). The central distinction between statistical and heuristic approaches is that statistical approaches are based on well-founded probabilistic models while heuristic ones are not. Studies reveal that statistical alignment models outperform the simple Dice coefficient (Och and Ney, 2003). Finding word alignments between parallel texts, however, is still far from a trivial work due to the diversity of natural languages. For example, the alignment of words within idiomatic expressions, free translations, and missing content or function words is problematic. When two languages widely differ in word</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I. Dan Melamed 2000. Models of translational equivalence among words. Computational Linguistics, 26(2):221-249, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="3574" citStr="Och and Ney, 2002" startWordPosition="530" endWordPosition="533">ion of IBM translation models and HMM model. Although Model 6 yields better results than naive IBM models, it fails to include dependencies other than IBM models and HMM model. Cherry and Lin (2003) developed a Proceedings of the 43rd Annual Meeting of the ACL, pages 459–466, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics statistical model to find word alignments, which allow easy integration of context-specific features. Log-linear models, which are very suitable to incorporate additional dependencies, have been successfully applied to statistical machine translation (Och and Ney, 2002). In this paper, we present a framework for word alignment based on log-linear models, allowing statistical models to be easily extended by incorporating additional syntactic dependencies. We use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models significantly outperform IBM translation models. We begin by describing log-linear models for word alignment. The design of feature functions is discussed then. Next, we present the training method and the search algorithm for log-linear models. We will fo</context>
<context position="5595" citStr="Och and Ney, 2002" startWordPosition="894" endWordPosition="897"> f) given e and f. We directly model the probability Pr(a |e, f). An especially well-founded framework is maximum entropy (Berger et al., 1996). In this framework, we have a set of M feature functions hm(a, e, f), m = 1, ... , M. For each feature function, there exists a model parameter Am, m = 1, ... , M. The direct alignment probability is given by: exp[PmM 1 Amhm(a, e, f)] Prae ) ( I , f ) _ Ea, exp [PmM= 1 Amhm(al, e, f ] (2) This approach has been suggested by (Papineni et al., 1997) for a natural language understanding task and successfully applied to statistical machine translation by (Och and Ney, 2002). We obtain the following decision rule: 3/4 Amhm(a,e,f) (3) Typically, the source language sentence e and the target sentence f are the fundamental knowledge sources for the task of finding word alignments. Linguistic data, which can be used to identify associations between lexical items are often ignored by traditional word alignment approaches. Linguistic tools such as part-of-speech taggers, parsers, namedentity recognizers have become more and more robust and available for many languages by now. It is important to make use of linguistic information to improve alignment strategies. Treated</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 295-302, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Statistical Machine Translation: From Single-Word Models to Alignment Templates.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Department, RWTH</institution>
<location>Aachen, Germany,</location>
<contexts>
<context position="12518" citStr="Och (2002)" startWordPosition="2104" endWordPosition="2105">meters on a development corpus, which consists of hundreds of manually-aligned bilingual sentence pairs. Using an n-best approximation may result in the problem that the parameters trained with the GIS algorithm yield worse alignments even on the development corpus. This can happen because with the modified model scaling factors the n-best list can change significantly and can include alignments that have not been taken into account in training. To avoid this problem, we iteratively combine n-best lists to train model parameters until the resulting n-best list does not change, as suggested by Och (2002). However, as this training procedure is based on maximum likelihood criterion, there is only a loose relation to the final alignment quality on unseen bilingual texts. In practice, 2Available at http://www.fjoch.com/YASMET.html having a series of model parameters when the iteration ends, we select the model parameters that yield best alignments on the development corpus. After the bilingual sentences in the development corpus are tokenized (or segmented) and POS tagged, they can be used to train POS tags transition probabilities by counting relative frequencies: p(fT |eT) = N (fTT, eT) N(eHer</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>Franz J. Och. 2002. Statistical Machine Translation: From Single-Word Models to Alignment Templates. Ph.D. thesis, Computer Science Department, RWTH Aachen, Germany, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages: 160-167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="2106" citStr="Och and Ney, 2003" startWordPosition="307" endWordPosition="310">rs that are learned from training 459 data, try to describe the relationship between a bilingual sentence pair (Brown et al., 1993; Vogel and Ney, 1996). Heuristic approaches obtain word alignments by using various similarity functions between the types of the two languages (Smadja et al., 1996; Ker and Chang, 1997; Melamed, 2000). The central distinction between statistical and heuristic approaches is that statistical approaches are based on well-founded probabilistic models while heuristic ones are not. Studies reveal that statistical alignment models outperform the simple Dice coefficient (Och and Ney, 2003). Finding word alignments between parallel texts, however, is still far from a trivial work due to the diversity of natural languages. For example, the alignment of words within idiomatic expressions, free translations, and missing content or function words is problematic. When two languages widely differ in word order, finding word alignments is especially hard. Therefore, it is necessary to incorporate all useful linguistic information to alleviate these problems. Tiedemann (2003) introduced a word alignment approach based on combination of association clues. Clues combination is done by dis</context>
<context position="6693" citStr="Och and Ney (2003)" startWordPosition="1079" endWordPosition="1082">or many languages by now. It is important to make use of linguistic information to improve alignment strategies. Treated as feature functions, syntactic dependencies can be easily incorporated into log-linear models. In order to incorporate a new dependency which contains extra information other than the bilingual sentence pair, we modify Eq.2 by adding a new variable v: a e f v exp[PMm= 1 Amhm(a, e, f, v)] Pr ( I , , ) _ Pa, exp[PMm=1 Amhm(al, e, f, v)] (4) Accordingly, we get a new decision rule: 3/4 Amhm(a, e, f, v) (5) Note that our log-linear models are different from Model 6 proposed by Och and Ney (2003), which defines the alignment problem as finding the alignment a that maximizes Pr(f, a |e) given e. 3 Feature Functions In this paper, we use IBM translation Model 3 as the base feature of our log-linear models. In addition, we also make use of syntactic information such as part-of-speech tags and bilingual dictionaries. aˆ = argmax a 1/2 XM m=1 aˆ = argmax a M 1/2 X m=1 460 3.1 IBM Translation Models Brown et al. (1993) proposed a series of statistical models of the translation process. IBM translation models try to model the translation probability Pr(fJ1 |eI 1), which describes the relatio</context>
<context position="18378" citStr="Och and Ney, 2003" startWordPosition="3207" endWordPosition="3210">ts of using IBM Model 3 (GIZA++) and log-linear models. Ney, 2003) for scoring the viterbi alignments of each model against gold-standard annotated alignments: recall = |A ∩ S| |S| AER = 1 − |A ∩ S |+ |A ∩ P| |A |+ |S| where A is the set of word pairs aligned by word alignment systems, S is the set marked in the gold standard as ”sure” and P is the set marked as ”possible” (including the ”sure” pairs). In our ChineseEnglish corpus, only one type of alignment was marked, meaning that S = P. In the following, we present the results of loglinear models for word alignment. We used GIZA++ package (Och and Ney, 2003) to train IBM translation models. The training scheme is 15H535, which means that Model 1 are trained for five iterations, HMM model for five iterations and finally Model 3 for five iterations. Except for changing the iterations for each model, we use default configuration of GIZA++. After that, we used three types of methods for performing a symmetrization of IBM models: intersection, union, and refined methods (Och and Ney , 2003). The base feature of our log-linear models, IBM Model 3, takes the parameters generated by GIZA++ as parameters for itself. In other words, our loglinear models sh</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19-51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore A Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
</authors>
<title>Feature-based language understanding.</title>
<date>1997</date>
<booktitle>In European Conf. on Speech Communication and Technology,</booktitle>
<pages>1435--1438</pages>
<location>Rhodes, Greece,</location>
<contexts>
<context position="5470" citStr="Papineni et al., 1997" startWordPosition="875" endWordPosition="878">ons: a C {(i,j) : i = 0,...,I;j = 0,...,J} (1) We define the alignment problem as finding the alignment a that maximizes Pr(a |e, f) given e and f. We directly model the probability Pr(a |e, f). An especially well-founded framework is maximum entropy (Berger et al., 1996). In this framework, we have a set of M feature functions hm(a, e, f), m = 1, ... , M. For each feature function, there exists a model parameter Am, m = 1, ... , M. The direct alignment probability is given by: exp[PmM 1 Amhm(a, e, f)] Prae ) ( I , f ) _ Ea, exp [PmM= 1 Amhm(al, e, f ] (2) This approach has been suggested by (Papineni et al., 1997) for a natural language understanding task and successfully applied to statistical machine translation by (Och and Ney, 2002). We obtain the following decision rule: 3/4 Amhm(a,e,f) (3) Typically, the source language sentence e and the target sentence f are the fundamental knowledge sources for the task of finding word alignments. Linguistic data, which can be used to identify associations between lexical items are often ignored by traditional word alignment approaches. Linguistic tools such as part-of-speech taggers, parsers, namedentity recognizers have become more and more robust and availa</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1997</marker>
<rawString>Kishore A. Papineni, Salim Roukos, and Todd Ward. 1997. Feature-based language understanding. In European Conf. on Speech Communication and Technology, pages 1435-1438, Rhodes, Greece, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="1783" citStr="Smadja et al., 1996" startWordPosition="261" endWordPosition="264">nd to be an excellent source of translation-related knowledge. Various methods have been proposed for finding word alignments between parallel texts. There are generally two categories of alignment approaches: statistical approaches and heuristic approaches. Statistical approaches, which depend on a set of unknown parameters that are learned from training 459 data, try to describe the relationship between a bilingual sentence pair (Brown et al., 1993; Vogel and Ney, 1996). Heuristic approaches obtain word alignments by using various similarity functions between the types of the two languages (Smadja et al., 1996; Ker and Chang, 1997; Melamed, 2000). The central distinction between statistical and heuristic approaches is that statistical approaches are based on well-founded probabilistic models while heuristic ones are not. Studies reveal that statistical alignment models outperform the simple Dice coefficient (Och and Ney, 2003). Finding word alignments between parallel texts, however, is still far from a trivial work due to the diversity of natural languages. For example, the alignment of words within idiomatic expressions, free translations, and missing content or function words is problematic. Whe</context>
</contexts>
<marker>Smadja, Hatzivassiloglou, McKeown, 1996</marker>
<rawString>Frank Smadja, Vasileios Hatzivassiloglou, and Kathleen R. McKeown 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):1-38, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorg Tiedemann</author>
</authors>
<title>Combining clues for word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of European Chapter of the ACL (EACL),</booktitle>
<location>Budapest, Hungary,</location>
<contexts>
<context position="2593" citStr="Tiedemann (2003)" startWordPosition="382" endWordPosition="383">uristic ones are not. Studies reveal that statistical alignment models outperform the simple Dice coefficient (Och and Ney, 2003). Finding word alignments between parallel texts, however, is still far from a trivial work due to the diversity of natural languages. For example, the alignment of words within idiomatic expressions, free translations, and missing content or function words is problematic. When two languages widely differ in word order, finding word alignments is especially hard. Therefore, it is necessary to incorporate all useful linguistic information to alleviate these problems. Tiedemann (2003) introduced a word alignment approach based on combination of association clues. Clues combination is done by disjunction of single clues, which are defined as probabilities of associations. The crucial assumption of clue combination that clues are independent of each other, however, is not always true. Och and Ney (2003) proposed Model 6, a log-linear combination of IBM translation models and HMM model. Although Model 6 yields better results than naive IBM models, it fails to include dependencies other than IBM models and HMM model. Cherry and Lin (2003) developed a Proceedings of the 43rd An</context>
</contexts>
<marker>Tiedemann, 2003</marker>
<rawString>Jorg Tiedemann. 2003. Combining clues for word alignment. In Proceedings of the 10th Conference of European Chapter of the ACL (EACL), Budapest, Hungary, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>H Tolga Ilhan</author>
<author>Christopher D Manning</author>
</authors>
<title>Extensions to HMM-based statistical word alignment models.</title>
<date>2003</date>
<booktitle>In Proceedings of Empirical Methods in Natural Langauge Processing,</booktitle>
<location>Philadelphia, PA.</location>
<marker>Toutanova, Ilhan, Manning, 2003</marker>
<rawString>Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Manning. 2003. Extensions to HMM-based statistical word alignment models. In Proceedings of Empirical Methods in Natural Langauge Processing, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Int. Conf. on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the 16th Int. Conf. on Computational Linguistics, pages 836-841, Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical machine translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics (ACL),</booktitle>
<pages>523--530</pages>
<location>Toulouse, France,</location>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical machine translation model. In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics (ACL), pages: 523-530, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huaping Zhang</author>
<author>Hongkui Yu</author>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>HHMM-based Chinese lexical analyzer ICTCLAS.</title>
<date>2003</date>
<booktitle>In Proceedings of the second SigHan Workshop affiliated with 41th ACL,</booktitle>
<pages>184--187</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="16723" citStr="Zhang et al., 2003" startWordPosition="2907" endWordPosition="2910"> used to train IBM translation models, a bilingual dictionary, a development corpus, and a test corpus. Chinese English Train Sentences 108 925 Words 3 784 106 3 862 637 Vocabulary 49 962 55 698 Dict Entries 415 753 Vocabulary 206 616 203 497 Dev Sentences 435 Words 11 462 14 252 Ave. SentLen 26.35 32.76 Test Sentences 500 Words 13 891 15 291 Ave. SentLen 27.78 30.58 Table 1. Statistics of training corpus (Train), bilingual dictionary (Dict), development corpus (Dev), and test corpus (Test). The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS (Zhang et al., 2003). The English sentences are tokenized by a simple tokenizer of ours and POS tagged by a rule-based tagger written by Eric Brill (Brill, 1995). We manually aligned 935 sentences, in which we selected 500 sentences as test corpus. The remaining 435 sentences are used as development corpus to train POS tags transition probabilities and to optimize the model parameters and gain threshold. Provided with human-annotated word-level alignment, we use precision, recall and AER (Och and 463 Size of Training Corpus 1K 5K 9K 39K 109K Model 3 E → C 0.4497 0.4081 0.4009 0.3791 0.3745 Model 3 C → E 0.4688 0.</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun Liu. 2003. HHMM-based Chinese lexical analyzer ICTCLAS. In Proceedings of the second SigHan Workshop affiliated with 41th ACL, pages: 184-187, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>