<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.046288">
<title confidence="0.9972065">
Maximum Spanning Tree Algorithm for Non-projective Labeled
Dependency Parsing
</title>
<author confidence="0.995032">
Nobuyuki Shimizu
</author>
<affiliation confidence="0.9992125">
Dept. of Computer Science
State University of New York at Albany
</affiliation>
<address confidence="0.950341">
Albany, NY, 12222, USA
</address>
<email confidence="0.998844">
shimizu@cs.albany.edu
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99578375">
Following (McDonald et al., 2005), we
present an application of a maximum
spanning tree algorithm for a directed
graph to non-projective labeled depen-
dency parsing. Using a variant of the
voted perceptron (Collins, 2002; Collins
and Roark, 2004; Crammer and Singer,
2003), we discriminatively trained our
parser in an on-line fashion. After just one
epoch of training, we were generally able
to attain average results in the CoNLL
2006 Shared Task.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953692307692">
Recently, we have seen dependency parsing grow
more popular. It is not rare to see dependency re-
lations used as features, in tasks such as relation ex-
traction (Bunescu and Mooney, 2005) and machine
translation (Ding and Palmer, 2005). Although En-
glish dependency relations are mostly projective, in
other languages with more flexible word order, such
as Czech, non-projective dependencies are more fre-
quent. There are generally two methods for learn-
ing non-projective dependencies. You could map a
non-projective dependency tree to a projective one,
learn and predict the tree, then bring it back to the
non-projective dependency tree (Nivre and Nilsson,
2005). Non-projective dependency parsing can also
be represented as search for a maximum spanning
tree in a directed graph, and this technique has been
shown to perform well in Czech (McDonald et al.,
2005). In this paper, we investigate the effective-
ness of (McDonald et al., 2005) in the various lan-
guages given by the CoNLL 2006 shared task for
non-projective labeled dependency parsing.
The paper is structured as follows: in section 2
and 3, we review the decoding and learning aspects
of (McDonald et al., 2005), and in section 4, we de-
scribe the extension of the algorithm and the features
needed for the CoNLL 2006 shared task.
</bodyText>
<sectionHeader confidence="0.970445" genericHeader="method">
2 Non-Projective Dependency Parsing
</sectionHeader>
<subsectionHeader confidence="0.966528">
2.1 Dependency Structure
</subsectionHeader>
<bodyText confidence="0.999852714285714">
Let us define x to be a generic sequence of input to-
kens together with their POS tags and other morpho-
logical features, and y to be a generic dependency
structure, that is, a set of edges for x. We use the
terminology in (Taskar et al., 2004) for a generic
structured output prediction, and define apart.
Apart represents an edge together with its label.
A part is a tuple (DEPREL, i, j) where i is the start
point of the edge, j is the end point, and DEPREL is
the label of the edge. The token at i is the head of
the token at j.
Table 1 shows our formulation of building a non-
projective dependency tree as a prediction problem.
The task is to predict y, the set of parts (column 3,
Table 1), given x, the input tokens and their features
(column 1 and 2, Table 1).
In this paper we use the common method of fac-
toring the score of the dependency structure as the
sum of the scores of all the parts.
A dependency structure is characterized by its
features, and for each feature, we have a correspond-
</bodyText>
<page confidence="0.963941">
236
</page>
<note confidence="0.5323435">
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 236–240, New York City, June 2006. c�2006 Association for Computational Linguistics
</note>
<table confidence="0.99191175">
Token POS Edge Part
John NN (SUBJ, 2, 1)
saw VBD (PRED, 0, 2)
a DT (DET, 4,3)
dog NN (OBJ, 2, 4)
yesterday RB (ADJU, 2,5)
which WDT (MODWH, 7, 6)
was VBD (MODPRED, 4,7)
a DT (DET, 10, 8)
Yorkshire NN (MODN,10, 9)
Terrier NN (OBJ, 7, 10)
. .(., 10, 11)
</table>
<tableCaption confidence="0.999695">
Table 1: Example Parts
</tableCaption>
<bodyText confidence="0.999519">
ing weight. The score of a dependency structure
is the sum of these weights. Now, the dependency
structures are factored by the parts, so that each fea-
ture is some type of a specialization of a part. Each
part in a dependency structure maps to several fea-
tures. If we sum up the weights for these features,
we have the score for the part, and if we sum up the
scores of the parts, we have the score for the depen-
dency structure.
For example, let us say we would like to find the
score of the part (OBJ, 2, 4). This is the edge going
to the 4th token ”dog” in Table 1. Suppose there are
two features for this part.
</bodyText>
<listItem confidence="0.99023">
• There is an edge labeled with ”OBJ” that points
to the right. ( = DEPREL, dir(i, j) )
• There is an edge labeled with ”OBJ” starting at
the token ”saw” which points to the right. ( =
DEPREL, dir(i, j), wordz )
</listItem>
<bodyText confidence="0.994955875">
If a statement is never true during the training, the
weight for it will be 0. Otherwise there will be a
positive weight value. The score will be the sum of
all the weights of the features given by the part.
In the upcoming section, we explain a decoding
algorithm for the dependency structures, and later
we give a method for learning the weight vector used
in the decoding.
</bodyText>
<subsectionHeader confidence="0.999277">
2.2 Maximum Spanning Tree Algorithm
</subsectionHeader>
<bodyText confidence="0.99926308">
As in (McDonald et al., 2005), the decoding algo-
rithm we used is the Chu-Liu-Edmonds (CLE) al-
gorithm (Chu and Liu, 1965; Edmonds, 1967) for
finding the Maximum Spanning Tree in a directed
graph. The following is a nice summary by (Mc-
Donald et al., 2005).
Informally, the algorithm has each vertex
in the graph greedily select the incoming
edge with highest weight.
Note that the edge is coming from the parent to the
child. This means that given a child node word�, we
are finding the parent, or the head wordz such that
the edge (i, j) has the highest weight among all i,
i 7� j.
If a tree results, then this must be the max-
imum spanning tree. If not, there must be
a cycle. The procedure identifies a cycle
and contracts it into a single vertex and
recalculates edge weights going into and
out of the cycle. It can be shown that a
maximum spanning tree on the contracted
graph is equivalent to a maximum span-
ning tree in the original graph (Leonidas,
2003). Hence the algorithm can recur-
sively call itself on the new graph.
</bodyText>
<sectionHeader confidence="0.997164" genericHeader="method">
3 Online Learning
</sectionHeader>
<bodyText confidence="0.999954454545455">
Again following (McDonald et al., 2005), we have
used the single best MIRA (Crammer and Singer,
2003), which is a variant of the voted perceptron
(Collins, 2002; Collins and Roark, 2004) for struc-
tured prediction. In short, the update is executed
when the decoder fails to predict the correct parse,
and we compare the correct parse yt and the incor-
rect parse y′ suggested by the decoding algorithm.
The weights of the features in y′ will be lowered, and
the weights of the features in yt will be increased ac-
cordingly.
</bodyText>
<sectionHeader confidence="0.999309" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.998718444444445">
Our experiments were conducted on CoNLL-X
shared task, with various datasets (Hajiˇc et al., 2004;
Simov et al., 2005; Simov and Osenova, 2003; Chen
et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003;
van der Beek et al., 2002; Brants et al., 2002;
Kawata and Bartels, 2000; Afonso et al., 2002;
Dˇzeroski et al., 2006; Civit Torruella and MartiAn-
tonin, 2002; Nilsson et al., 2005; Oflazer et al.,
2003; Atalay et al., 2003) .
</bodyText>
<subsectionHeader confidence="0.983608">
4.1 Dependency Relation
</subsectionHeader>
<bodyText confidence="0.992428">
The CLE algorithm works on a directed graph with
unlabeled edges. Since the CoNLL-X shared task
</bodyText>
<page confidence="0.99038">
237
</page>
<construct confidence="0.946513724137931">
Given a part (DEPREL, i, j)
DEPREL, dir(i, j)
DEPREL, dir(i, j), wordi
DEPREL, dir(i, j), posi
DEPREL, dir(i, j), wordj
DEPREL, dir(i, j), posj
DEPREL, dir(i, j), wordi, posi
DEPREL, dir(i, j), wordj, posj
DEPREL, dir(i, j), wordi−1
DEPREL, dir(i, j), posi−1
DEPREL, dir(i, j), wordi−1, posi−1
DEPREL, dir(i, j), wordj−1
DEPREL, dir(i, j), posj−1
DEPREL, dir(i, j), wordj−1, posj−1
DEPREL, dir(i, j), wordi+1
DEPREL, dir(i, j), posi+1
DEPREL, dir(i, j), wordi+1, posi+1
DEPREL, dir(i, j), wordj+1
DEPREL, dir(i, j), posj+1
DEPREL, dir(i, j), wordj+1, posj+1
DEPREL, dir(i, j), posi−2
DEPREL, dir(i, j), posi+2
DEPREL, dir(i, j), distance = |j − i|
additional features
DEPREL, dir(i, j), wordi, wordj
DEPREL, dir(i, j), posi+1, posi, posi+1
DEPREL, dir(i, j), posi+1, wordi, posi+1
DEPREL, dir(i, j), wordi, posi, posj
DEPREL, dir(i, j), posi, wordj, posj
</construct>
<tableCaption confidence="0.962856">
Table 2: Binary Features for Each Part
</tableCaption>
<bodyText confidence="0.999922909090909">
requires the labeling of edges, as a preprocessing
stage, we created a directed complete graph with-
out multi-edges, that is, given two distinct nodes i
and j, exactly two edges exist between them, one
from i to j, and the other from j to i. There is no
self-pointing edge. Then we labeled each edge with
the highest scoring dependency relation. This com-
plete graph was given to the CLE algorithm and the
edge labels were never altered in the course of find-
ing the maximum spanning tree. The result is the
non-projective dependency tree with labeled edges.
</bodyText>
<subsectionHeader confidence="0.699617">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.999336173913044">
The features we used to score each part (edge)
(DEPREL, i, j) are shown in Table 2. The index i
is the position of the parent and j is that of the child.
wordj = the word token at the position j.
posj = the coarse part-of-speech at j.
dir(i, j) = R if i &lt; j, and L otherwise.
No other features were used beyond the combina-
tions of the CPOS tag and the word token in Table 2.
We have evaluated our parser on Arabic, Danish,
Slovene, Spanish, Turkish and Swedish, and used
the ”additional features” listed in Table 2 for all lan-
guages except for Danish and Swedish. The reason
for this is simply that the model with the additional
features did not fit in the 4 GB of memory used in
the training.
Although we could do batch learning by running
the online algorithm multiple times, we run the on-
line algorithm just once. The hardware used is an
Intel Pentinum D at 3.0 Ghz with 4 GB of memory,
and the software was written in C++. The training
time required was Arabic 204 min, Slovene 87 min,
Spanish 413 min, Swedish 1192 min, Turkish 410
min, Danish 381 min.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999625">
The results are shown in Table 3. Although our fea-
ture set is very simple, the results were around the
averages. We will do error analysis of three notable
languages: Arabic, Swedish and Turkish.
</bodyText>
<subsectionHeader confidence="0.962499">
5.1 Arabic
</subsectionHeader>
<bodyText confidence="0.999991461538461">
Of 4990 words in the test set, 800 are prepositions.
The prepositions are the most frequently found to-
kens after nouns in this set. On the other hand,
our head attachment error was 44% for prepositions.
Given the relatively large number of prepositions
found in the test set, it is important to get the prepo-
sition attachment right to achieve a higher mark in
this language. The obvious solution is to have a fea-
ture that connects the head of a preposition to the
child of the preposition. However, such a feature
effects the edge based factoring and the decoding al-
gorithm, and we will be forced to modify the MST
algorithm in some ways.
</bodyText>
<subsectionHeader confidence="0.995767">
5.2 Swedish
</subsectionHeader>
<bodyText confidence="0.999980875">
Due to the memory constraint on the computer, we
did not use the additional features for Swedish and
our feature heavily relied on the CPOS tag. At the
same time, we have noticed that relatively higher
performance of our parser compared to the average
coincides with the bigger tag set for CPOS for this
corpus. This suggests that we should be using more
fine grained POS in other languages.
</bodyText>
<subsectionHeader confidence="0.955176">
5.3 Turkish
</subsectionHeader>
<bodyText confidence="0.999432">
The difficulty with parsing Turkish stems from the
large unlabeled attachment error rate on the nouns
</bodyText>
<page confidence="0.994902">
238
</page>
<table confidence="0.999882285714286">
Language LAS AV SD
Arabic 62.83% 59.92% 6.53
Danish 75.81% 78.31% 5.45
Slovene 64.57% 65.61% 6.78
Spanish 73.17% 73.52% 8.41
Swedish 79.49% 76.44% 6.46
Turkish 54.23% 55.95% 7.71
Language UAS AV SD
Arabic 74.27% 73.48% 4.94
Danish 81.72% 84.52% 4.29
Slovene 74.88% 76.53% 4.67
Spanish 77.58% 77.76% 7.81
Swedish 86.62% 84.21% 5.45
Turkish 68.77% 69.35% 5.51
</table>
<tableCaption confidence="0.999827">
Table 3: Labeled and Unlabeled Attachment Score
</tableCaption>
<bodyText confidence="0.9998473">
(39%). Since the nouns are the most frequently oc-
curring words in the test set (2209 out of 5021 to-
tal), this seems to make Turkish the most challeng-
ing language for any system in the shared task. On
the average, there are 1.8 or so verbs per sentence,
and nouns have a difficult time attaching to the cor-
rect verb or postposition. This, we think, indicates
that there are morphological features or word order-
ing features that we really need in order to disam-
biguate them.
</bodyText>
<sectionHeader confidence="0.99857" genericHeader="evaluation">
6 Future Work
</sectionHeader>
<bodyText confidence="0.99997425">
As well as making use of fine-grained POS tags and
other morphological features, given the error analy-
sis on Arabic, we would like to add features that are
dependent on two or more edges.
</bodyText>
<subsectionHeader confidence="0.998255">
6.1 Bottom-Up Non-Projective Parsing
</subsectionHeader>
<bodyText confidence="0.996737157894737">
In order to incorporate features which depend on
other edges, we propose Bottom-Up Non-Projective
Parsing. It is often the case that dependency rela-
tions can be ordered by how close one relation is to
the root of dependency tree. For example, the de-
pendency relation between a determiner and a noun
should be decided before that between a preposition
and a noun, and that of a verb and a preposition, and
so on. We can use this information to do bottom-up
parsing.
Suppose all words have a POS tag assigned to
them, and every edge labeled with a dependency re-
lation is attached to a specific POS tag at the end
point. Also assume that there is an ordering of POS
tags such that the edge going to the POS tag needs
be decided before other edges. For example, (1) de-
terminer, (2) noun, (3) preposition, (4) verb would
be one such ordering. We propose the following al-
gorithm:
</bodyText>
<listItem confidence="0.819278791666667">
• Assume we have tokens as nodes in a graph and no edges
are present at first. For example, we have tokens ”I”,
”ate”, ”with”, ”a”, ”spoon”, and no edges between them.
• Take the POS tag that needs to be decided next. Find all
edges that go to each token labeled with this POS tag,
and put them in the graph. For example, if the POS is
noun, put edges from ”ate” to ”I”, from ”ate” to ”spoon”,
from ”with” to ”I”, from ”with” to ”spoon”, from ”I” to
”spoon”, and from ”spoon” to ”I”.
• Run the CLE algorithm on this graph. This selects the
highest incoming edge to each token with the POS tag we
are looking at, and remove cycles if any are present.
• Take the resulting forests and for each edge, bring the in-
formation on the child node to the parent node. For ex-
ample, if this time POS was noun, and there is an edge to
a preposition ”with” from a noun ”spoon”, then ”spoon”
is absorbed by ”with”. Note that since no remaining de-
pendency relation will attach to ”spoon”, we can safely
ignore ”spoon” from now on.
• Go back and repeat until no POS is remaining and we
have a dependency tree. Now in the next round, when
deciding the score of the edge from ”ate” to ”with”, we
can use the all information at the token ”with”, including
”spoon”.
</listItem>
<sectionHeader confidence="0.975299" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999992181818182">
We have extended non-projective unlabeled de-
pendency parsing (McDonald et al., 2005) to a
very simple non-projective labeled dependency and
showed that the parser performs reasonably well
with small number of features and just one itera-
tion of training. Based on the analysis of the Ara-
bic parsing results, we have proposed a bottom-
up non-projective labeled dependency parsing algo-
rithm that allows us to use features dependent on
more than one edge, with very little disadvantage
compared to the original algorithm.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996334625">
A. Abeill´e, editor. 2003. Treebanks: Building and Us-
ing Parsed Corpora, volume 20 of Text, Speech and
Language Technology. Kluwer Academic Publishers,
Dordrecht.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. “Flo-
resta sint´a(c)tica”: a treebank for Portuguese. In Proc.
of the Third Intern. Conf. on Language Resources and
Evaluation (LREC), pages 1698–1703.
</reference>
<page confidence="0.990979">
239
</page>
<reference confidence="0.999643451612903">
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annota-
tion process in the Turkish treebank. In Proc. of the 4th
Intern. Workshop on Linguistically Interpreteted Cor-
pora (LINC).
A. B¨ohmov´a, J. Hajiˇc, E. Hajiˇcov´a, and B. Hladk´a. 2003.
The PDT: a 3-level annotation scenario. In Abeill´e
(Abeill´e, 2003), chapter 7.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
R. Bunescu and R. Mooney. 2005. A shortest path de-
pendency kernel for relation extraction. In Proc. of
the Joint Conf. on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT/EMNLP).
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeill´e
(Abeill´e, 2003), chapter 13, pages 231–248.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. In Science Sinica, page
14:13961400.
M. Civit Torruella and Ma A. MartiAntonin. 2002. De-
sign principles for a Spanish treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. of the 42rd Annual
Meeting of the ACL.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of Empirical Methods
in Natural Language Processing (EMNLP).
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. In JMLR.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proc. of the 43rd Annual Meeting of the ACL.
S. Dˇzeroski, T. Erjavec, N. Ledinek, P. Pajas,
Z. ˇZabokrtsky, and A. ˇZele. 2006. Towards a Slovene
dependency treebank. In Proc. of the Fifth Intern.
Conf. on Language Resources and Evaluation (LREC).
J. Edmonds. 1967. Optimum branchings. In Journal of
Research of the National Bureau of Standards, page
71B:233240.
J. Hajiˇc, O. Smrˇz, P. Zem´anek, J. ˇSnaidauf, and E. Beˇska.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110–117.
Y. Kawata and J. Bartels. 2000. Stylebook for the
Japanese treebank in VERBMOBIL. Verbmobil-
Report 240, Seminar f¨ur Sprachwissenschaft, Univer-
sit¨at T¨ubingen.
M. T. Kromann. 2003. The Danish dependency treebank
and the underlying linguistic theory. In Proc. of the
Second Workshop on Treebanks and Linguistic Theo-
ries (TLT).
G. Leonidas. 2003. Arborescence optimization problems
solvable by edmonds algorithm. In Theoretical Com-
puter Science, page 301:427 437.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Joint Conf. on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing (HLT/EMNLP).
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets
TIGER: Reconstructing a Swedish treebank from an-
tiquity. In Proc. of the NODALIDA Special Session on
Treebanks.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. of the 43rd Annual Meeting of
the ACL.
K. Oflazer, B. Say, D. Zeynep Hakkani-T¨ur, and G. T¨ur.
2003. Building a Turkish treebank. In Abeill´e
(Abeill´e, 2003), chapter 15.
K. Simov and P. Osenova. 2003. Practical annotation
scheme for an HPSG treebank of Bulgarian. In Proc.
of the 4th Intern. Workshop on Linguistically Inter-
preteted Corpora (LINC), pages 17–24.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov.
2005. Design and implementation of the Bulgarian
HPSG-basedtreebank. In Journal ofResearch onLan-
guage and Computation – Special Issue, pages 495–
522. Kluwer Academic Publishers.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. of
Empirical Methods in Natural Language Processing
(EMNLP).
L. van der Beek, G. Bouma, R. Malouf, and G. van No-
ord. 2002. The Alpino dependency treebank. In Com-
putational Linguistics in the Netherlands (CLIN).
</reference>
<page confidence="0.996997">
240
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.361216">
<title confidence="0.994291">Maximum Spanning Tree Algorithm for Non-projective Dependency Parsing</title>
<author confidence="0.834463">Nobuyuki</author>
<affiliation confidence="0.998992">Dept. of Computer State University of New York at</affiliation>
<address confidence="0.988885">Albany, NY, 12222,</address>
<email confidence="0.999778">shimizu@cs.albany.edu</email>
<abstract confidence="0.991994">Following (McDonald et al., 2005), we present an application of a maximum spanning tree algorithm for a directed graph to non-projective labeled dependency parsing. Using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003), we discriminatively trained our parser in an on-line fashion. After just one epoch of training, we were generally able to attain average results in the CoNLL</abstract>
<address confidence="0.460925">2006 Shared Task.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Treebanks: Building and Using Parsed Corpora,</title>
<date>2003</date>
<booktitle>Text, Speech and Language Technology.</booktitle>
<volume>20</volume>
<editor>A. Abeill´e, editor.</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<marker>2003</marker>
<rawString>A. Abeill´e, editor. 2003. Treebanks: Building and Using Parsed Corpora, volume 20 of Text, Speech and Language Technology. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Afonso</author>
<author>E Bick</author>
<author>R Haber</author>
<author>D Santos</author>
</authors>
<title>Floresta sint´a(c)tica”: a treebank for Portuguese.</title>
<date>2002</date>
<booktitle>In Proc. of the Third Intern. Conf. on Language Resources and Evaluation (LREC),</booktitle>
<pages>1698--1703</pages>
<contexts>
<context position="6597" citStr="Afonso et al., 2002" startWordPosition="1168" endWordPosition="1171">on. In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordingly. 4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) . 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges. Since the CoNLL-X shared task 237 Given a part (DEPREL, i, j) DEPREL, dir(i, j) DEPREL, dir(i, j), wordi DEPREL, dir(i, j), posi DEPREL, dir(i, j), wordj DEPREL, dir(i, j), posj DEPREL, dir(i, j), wordi, posi DEPREL, dir(i, j), wordj, posj DEPREL, dir(i, j), wordi−1 DEPREL, dir(i, j), posi−1 DEPREL, dir(i, j), wordi−1, posi−1 DEPREL, dir(i, j), wordj−1 DEPREL, dir(i, j), pos</context>
</contexts>
<marker>Afonso, Bick, Haber, Santos, 2002</marker>
<rawString>S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. “Floresta sint´a(c)tica”: a treebank for Portuguese. In Proc. of the Third Intern. Conf. on Language Resources and Evaluation (LREC), pages 1698–1703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N B Atalay</author>
<author>K Oflazer</author>
<author>B Say</author>
</authors>
<title>The annotation process in the Turkish treebank.</title>
<date>2003</date>
<booktitle>In Proc. of the 4th Intern. Workshop on Linguistically Interpreteted Corpora (LINC).</booktitle>
<contexts>
<context position="6727" citStr="Atalay et al., 2003" startWordPosition="1190" endWordPosition="1193"> the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordingly. 4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) . 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges. Since the CoNLL-X shared task 237 Given a part (DEPREL, i, j) DEPREL, dir(i, j) DEPREL, dir(i, j), wordi DEPREL, dir(i, j), posi DEPREL, dir(i, j), wordj DEPREL, dir(i, j), posj DEPREL, dir(i, j), wordi, posi DEPREL, dir(i, j), wordj, posj DEPREL, dir(i, j), wordi−1 DEPREL, dir(i, j), posi−1 DEPREL, dir(i, j), wordi−1, posi−1 DEPREL, dir(i, j), wordj−1 DEPREL, dir(i, j), posj−1 DEPREL, dir(i, j), wordj−1, posj−1 DEPREL, dir(i, j), wordi+1 DEPREL, dir(i, j), posi+1 DEPREL, dir(i, j), wordi+1, posi+1 DEP</context>
</contexts>
<marker>Atalay, Oflazer, Say, 2003</marker>
<rawString>N. B. Atalay, K. Oflazer, and B. Say. 2003. The annotation process in the Turkish treebank. In Proc. of the 4th Intern. Workshop on Linguistically Interpreteted Corpora (LINC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B¨ohmov´a</author>
<author>J Hajiˇc</author>
<author>E Hajiˇcov´a</author>
<author>B Hladk´a</author>
</authors>
<title>The PDT: a 3-level annotation scenario.</title>
<date>2003</date>
<booktitle>In Abeill´e (Abeill´e,</booktitle>
<note>chapter 7.</note>
<marker>B¨ohmov´a, Hajiˇc, Hajiˇcov´a, Hladk´a, 2003</marker>
<rawString>A. B¨ohmov´a, J. Hajiˇc, E. Hajiˇcov´a, and B. Hladk´a. 2003. The PDT: a 3-level annotation scenario. In Abeill´e (Abeill´e, 2003), chapter 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brants</author>
<author>S Dipper</author>
<author>S Hansen</author>
<author>W Lezius</author>
<author>G Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proc. of the First Workshop on Treebanks and Linguistic Theories (TLT).</booktitle>
<contexts>
<context position="6550" citStr="Brants et al., 2002" startWordPosition="1160" endWordPosition="1163">ollins and Roark, 2004) for structured prediction. In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordingly. 4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) . 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges. Since the CoNLL-X shared task 237 Given a part (DEPREL, i, j) DEPREL, dir(i, j) DEPREL, dir(i, j), wordi DEPREL, dir(i, j), posi DEPREL, dir(i, j), wordj DEPREL, dir(i, j), posj DEPREL, dir(i, j), wordi, posi DEPREL, dir(i, j), wordj, posj DEPREL, dir(i, j), wordi−1 DEPREL, dir(i, j), posi−1 DEPREL, dir(i, j), wordi−1, posi−1 DE</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002. The TIGER treebank. In Proc. of the First Workshop on Treebanks and Linguistic Theories (TLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>R Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proc. of the Joint Conf. on Human Language Technology and Empirical Methods in Natural Language Processing (HLT/EMNLP).</booktitle>
<contexts>
<context position="863" citStr="Bunescu and Mooney, 2005" startWordPosition="129" endWordPosition="132">l., 2005), we present an application of a maximum spanning tree algorithm for a directed graph to non-projective labeled dependency parsing. Using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003), we discriminatively trained our parser in an on-line fashion. After just one epoch of training, we were generally able to attain average results in the CoNLL 2006 Shared Task. 1 Introduction Recently, we have seen dependency parsing grow more popular. It is not rare to see dependency relations used as features, in tasks such as relation extraction (Bunescu and Mooney, 2005) and machine translation (Ding and Palmer, 2005). Although English dependency relations are mostly projective, in other languages with more flexible word order, such as Czech, non-projective dependencies are more frequent. There are generally two methods for learning non-projective dependencies. You could map a non-projective dependency tree to a projective one, learn and predict the tree, then bring it back to the non-projective dependency tree (Nivre and Nilsson, 2005). Non-projective dependency parsing can also be represented as search for a maximum spanning tree in a directed graph, and th</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>R. Bunescu and R. Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proc. of the Joint Conf. on Human Language Technology and Empirical Methods in Natural Language Processing (HLT/EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Chen</author>
<author>C Luo</author>
<author>M Chang</author>
<author>F Chen</author>
<author>C Chen</author>
<author>C Huang</author>
<author>Z Gao</author>
</authors>
<title>Sinica treebank: Design criteria, representational issues and implementation.</title>
<date>2003</date>
<booktitle>In Abeill´e (Abeill´e,</booktitle>
<pages>231--248</pages>
<contexts>
<context position="6463" citStr="Chen et al., 2003" startWordPosition="1144" endWordPosition="1147">ammer and Singer, 2003), which is a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004) for structured prediction. In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordingly. 4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) . 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges. Since the CoNLL-X shared task 237 Given a part (DEPREL, i, j) DEPREL, dir(i, j) DEPREL, dir(i, j), wordi DEPREL, dir(i, j), posi DEPREL, dir(i, j), wordj DEPREL, dir(i, j), posj DEPREL, dir(i, j), wordi, posi DEPREL, dir(i, j), wordj, posj DEP</context>
</contexts>
<marker>Chen, Luo, Chang, Chen, Chen, Huang, Gao, 2003</marker>
<rawString>K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang, and Z. Gao. 2003. Sinica treebank: Design criteria, representational issues and implementation. In Abeill´e (Abeill´e, 2003), chapter 13, pages 231–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y J Chu</author>
<author>T H Liu</author>
</authors>
<title>On the shortest arborescence of a directed graph.</title>
<date>1965</date>
<booktitle>In Science Sinica,</booktitle>
<pages>14--13961400</pages>
<contexts>
<context position="4843" citStr="Chu and Liu, 1965" startWordPosition="853" endWordPosition="856">starting at the token ”saw” which points to the right. ( = DEPREL, dir(i, j), wordz ) If a statement is never true during the training, the weight for it will be 0. Otherwise there will be a positive weight value. The score will be the sum of all the weights of the features given by the part. In the upcoming section, we explain a decoding algorithm for the dependency structures, and later we give a method for learning the weight vector used in the decoding. 2.2 Maximum Spanning Tree Algorithm As in (McDonald et al., 2005), the decoding algorithm we used is the Chu-Liu-Edmonds (CLE) algorithm (Chu and Liu, 1965; Edmonds, 1967) for finding the Maximum Spanning Tree in a directed graph. The following is a nice summary by (McDonald et al., 2005). Informally, the algorithm has each vertex in the graph greedily select the incoming edge with highest weight. Note that the edge is coming from the parent to the child. This means that given a child node word�, we are finding the parent, or the head wordz such that the edge (i, j) has the highest weight among all i, i 7� j. If a tree results, then this must be the maximum spanning tree. If not, there must be a cycle. The procedure identifies a cycle and contra</context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a directed graph. In Science Sinica, page 14:13961400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Civit Torruella</author>
<author>Ma A MartiAntonin</author>
</authors>
<title>Design principles for a Spanish treebank.</title>
<date>2002</date>
<booktitle>In Proc. of the First Workshop on Treebanks and Linguistic Theories (TLT).</booktitle>
<contexts>
<context position="6661" citStr="Torruella and MartiAntonin, 2002" startWordPosition="1177" endWordPosition="1181">er fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordingly. 4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) . 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges. Since the CoNLL-X shared task 237 Given a part (DEPREL, i, j) DEPREL, dir(i, j) DEPREL, dir(i, j), wordi DEPREL, dir(i, j), posi DEPREL, dir(i, j), wordj DEPREL, dir(i, j), posj DEPREL, dir(i, j), wordi, posi DEPREL, dir(i, j), wordj, posj DEPREL, dir(i, j), wordi−1 DEPREL, dir(i, j), posi−1 DEPREL, dir(i, j), wordi−1, posi−1 DEPREL, dir(i, j), wordj−1 DEPREL, dir(i, j), posj−1 DEPREL, dir(i, j), wordj−1, posj−1 DEPREL, dir(i, j), wordi+</context>
</contexts>
<marker>Torruella, MartiAntonin, 2002</marker>
<rawString>M. Civit Torruella and Ma A. MartiAntonin. 2002. Design principles for a Spanish treebank. In Proc. of the First Workshop on Treebanks and Linguistic Theories (TLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>B Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proc. of the 42rd Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="5954" citStr="Collins and Roark, 2004" startWordPosition="1055" endWordPosition="1058">his must be the maximum spanning tree. If not, there must be a cycle. The procedure identifies a cycle and contracts it into a single vertex and recalculates edge weights going into and out of the cycle. It can be shown that a maximum spanning tree on the contracted graph is equivalent to a maximum spanning tree in the original graph (Leonidas, 2003). Hence the algorithm can recursively call itself on the new graph. 3 Online Learning Again following (McDonald et al., 2005), we have used the single best MIRA (Crammer and Singer, 2003), which is a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004) for structured prediction. In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordingly. 4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Ka</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>M. Collins and B. Roark. 2004. Incremental parsing with the perceptron algorithm. In Proc. of the 42rd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5928" citStr="Collins, 2002" startWordPosition="1053" endWordPosition="1054">results, then this must be the maximum spanning tree. If not, there must be a cycle. The procedure identifies a cycle and contracts it into a single vertex and recalculates edge weights going into and out of the cycle. It can be shown that a maximum spanning tree on the contracted graph is equivalent to a maximum spanning tree in the original graph (Leonidas, 2003). Hence the algorithm can recursively call itself on the new graph. 3 Online Learning Again following (McDonald et al., 2005), we have used the single best MIRA (Crammer and Singer, 2003), which is a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004) for structured prediction. In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordingly. 4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 200</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. of Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<booktitle>In JMLR.</booktitle>
<contexts>
<context position="5869" citStr="Crammer and Singer, 2003" startWordPosition="1041" endWordPosition="1044"> the edge (i, j) has the highest weight among all i, i 7� j. If a tree results, then this must be the maximum spanning tree. If not, there must be a cycle. The procedure identifies a cycle and contracts it into a single vertex and recalculates edge weights going into and out of the cycle. It can be shown that a maximum spanning tree on the contracted graph is equivalent to a maximum spanning tree in the original graph (Leonidas, 2003). Hence the algorithm can recursively call itself on the new graph. 3 Online Learning Again following (McDonald et al., 2005), we have used the single best MIRA (Crammer and Singer, 2003), which is a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004) for structured prediction. In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordingly. 4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨oh</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. In JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ding</author>
<author>M Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="911" citStr="Ding and Palmer, 2005" startWordPosition="136" endWordPosition="139">panning tree algorithm for a directed graph to non-projective labeled dependency parsing. Using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003), we discriminatively trained our parser in an on-line fashion. After just one epoch of training, we were generally able to attain average results in the CoNLL 2006 Shared Task. 1 Introduction Recently, we have seen dependency parsing grow more popular. It is not rare to see dependency relations used as features, in tasks such as relation extraction (Bunescu and Mooney, 2005) and machine translation (Ding and Palmer, 2005). Although English dependency relations are mostly projective, in other languages with more flexible word order, such as Czech, non-projective dependencies are more frequent. There are generally two methods for learning non-projective dependencies. You could map a non-projective dependency tree to a projective one, learn and predict the tree, then bring it back to the non-projective dependency tree (Nivre and Nilsson, 2005). Non-projective dependency parsing can also be represented as search for a maximum spanning tree in a directed graph, and this technique has been shown to perform well in C</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Y. Ding and M. Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proc. of the 43rd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dˇzeroski</author>
<author>T Erjavec</author>
<author>N Ledinek</author>
<author>P Pajas</author>
<author>Z ˇZabokrtsky</author>
<author>A ˇZele</author>
</authors>
<title>Towards a Slovene dependency treebank.</title>
<date>2006</date>
<booktitle>In Proc. of the Fifth Intern. Conf. on Language Resources and Evaluation (LREC).</booktitle>
<marker>Dˇzeroski, Erjavec, Ledinek, Pajas, ˇZabokrtsky, ˇZele, 2006</marker>
<rawString>S. Dˇzeroski, T. Erjavec, N. Ledinek, P. Pajas, Z. ˇZabokrtsky, and A. ˇZele. 2006. Towards a Slovene dependency treebank. In Proc. of the Fifth Intern. Conf. on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Edmonds</author>
</authors>
<title>Optimum branchings.</title>
<date>1967</date>
<journal>In Journal of Research of the National Bureau of Standards,</journal>
<pages>71--233240</pages>
<contexts>
<context position="4859" citStr="Edmonds, 1967" startWordPosition="857" endWordPosition="858">en ”saw” which points to the right. ( = DEPREL, dir(i, j), wordz ) If a statement is never true during the training, the weight for it will be 0. Otherwise there will be a positive weight value. The score will be the sum of all the weights of the features given by the part. In the upcoming section, we explain a decoding algorithm for the dependency structures, and later we give a method for learning the weight vector used in the decoding. 2.2 Maximum Spanning Tree Algorithm As in (McDonald et al., 2005), the decoding algorithm we used is the Chu-Liu-Edmonds (CLE) algorithm (Chu and Liu, 1965; Edmonds, 1967) for finding the Maximum Spanning Tree in a directed graph. The following is a nice summary by (McDonald et al., 2005). Informally, the algorithm has each vertex in the graph greedily select the incoming edge with highest weight. Note that the edge is coming from the parent to the child. This means that given a child node word�, we are finding the parent, or the head wordz such that the edge (i, j) has the highest weight among all i, i 7� j. If a tree results, then this must be the maximum spanning tree. If not, there must be a cycle. The procedure identifies a cycle and contracts it into a si</context>
</contexts>
<marker>Edmonds, 1967</marker>
<rawString>J. Edmonds. 1967. Optimum branchings. In Journal of Research of the National Bureau of Standards, page 71B:233240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>O Smrˇz</author>
<author>P Zem´anek</author>
<author>J ˇSnaidauf</author>
<author>E Beˇska</author>
</authors>
<title>Prague Arabic dependency treebank: Development in data and tools.</title>
<date>2004</date>
<booktitle>In Proc. of the NEMLAR Intern. Conf. on Arabic Language Resources and Tools,</booktitle>
<pages>110--117</pages>
<marker>Hajiˇc, Smrˇz, Zem´anek, ˇSnaidauf, Beˇska, 2004</marker>
<rawString>J. Hajiˇc, O. Smrˇz, P. Zem´anek, J. ˇSnaidauf, and E. Beˇska. 2004. Prague Arabic dependency treebank: Development in data and tools. In Proc. of the NEMLAR Intern. Conf. on Arabic Language Resources and Tools, pages 110–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Kawata</author>
<author>J Bartels</author>
</authors>
<title>Stylebook for the Japanese treebank in VERBMOBIL. VerbmobilReport 240, Seminar f¨ur Sprachwissenschaft, Universit¨at T¨ubingen.</title>
<date>2000</date>
<contexts>
<context position="6576" citStr="Kawata and Bartels, 2000" startWordPosition="1164" endWordPosition="1167">4) for structured prediction. In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordingly. 4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) . 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges. Since the CoNLL-X shared task 237 Given a part (DEPREL, i, j) DEPREL, dir(i, j) DEPREL, dir(i, j), wordi DEPREL, dir(i, j), posi DEPREL, dir(i, j), wordj DEPREL, dir(i, j), posj DEPREL, dir(i, j), wordi, posi DEPREL, dir(i, j), wordj, posj DEPREL, dir(i, j), wordi−1 DEPREL, dir(i, j), posi−1 DEPREL, dir(i, j), wordi−1, posi−1 DEPREL, dir(i, j), wordj−1 D</context>
</contexts>
<marker>Kawata, Bartels, 2000</marker>
<rawString>Y. Kawata and J. Bartels. 2000. Stylebook for the Japanese treebank in VERBMOBIL. VerbmobilReport 240, Seminar f¨ur Sprachwissenschaft, Universit¨at T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Kromann</author>
</authors>
<title>The Danish dependency treebank and the underlying linguistic theory.</title>
<date>2003</date>
<booktitle>In Proc. of the Second Workshop on Treebanks and Linguistic Theories (TLT).</booktitle>
<contexts>
<context position="6502" citStr="Kromann, 2003" startWordPosition="1152" endWordPosition="1153"> of the voted perceptron (Collins, 2002; Collins and Roark, 2004) for structured prediction. In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordingly. 4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) . 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges. Since the CoNLL-X shared task 237 Given a part (DEPREL, i, j) DEPREL, dir(i, j) DEPREL, dir(i, j), wordi DEPREL, dir(i, j), posi DEPREL, dir(i, j), wordj DEPREL, dir(i, j), posj DEPREL, dir(i, j), wordi, posi DEPREL, dir(i, j), wordj, posj DEPREL, dir(i, j), wordi−1 DEPREL, dir(i, </context>
</contexts>
<marker>Kromann, 2003</marker>
<rawString>M. T. Kromann. 2003. The Danish dependency treebank and the underlying linguistic theory. In Proc. of the Second Workshop on Treebanks and Linguistic Theories (TLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leonidas</author>
</authors>
<title>Arborescence optimization problems solvable by edmonds algorithm.</title>
<date>2003</date>
<booktitle>In Theoretical Computer Science,</booktitle>
<pages>301--427</pages>
<contexts>
<context position="5682" citStr="Leonidas, 2003" startWordPosition="1011" endWordPosition="1012"> with highest weight. Note that the edge is coming from the parent to the child. This means that given a child node word�, we are finding the parent, or the head wordz such that the edge (i, j) has the highest weight among all i, i 7� j. If a tree results, then this must be the maximum spanning tree. If not, there must be a cycle. The procedure identifies a cycle and contracts it into a single vertex and recalculates edge weights going into and out of the cycle. It can be shown that a maximum spanning tree on the contracted graph is equivalent to a maximum spanning tree in the original graph (Leonidas, 2003). Hence the algorithm can recursively call itself on the new graph. 3 Online Learning Again following (McDonald et al., 2005), we have used the single best MIRA (Crammer and Singer, 2003), which is a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004) for structured prediction. In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordi</context>
</contexts>
<marker>Leonidas, 2003</marker>
<rawString>G. Leonidas. 2003. Arborescence optimization problems solvable by edmonds algorithm. In Theoretical Computer Science, page 301:427 437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of the Joint Conf. on Human Language Technology and Empirical Methods in Natural Language Processing (HLT/EMNLP).</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proc. of the Joint Conf. on Human Language Technology and Empirical Methods in Natural Language Processing (HLT/EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nilsson</author>
<author>J Hall</author>
<author>J Nivre</author>
</authors>
<title>MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity.</title>
<date>2005</date>
<booktitle>In Proc. of the NODALIDA Special Session on Treebanks.</booktitle>
<contexts>
<context position="6683" citStr="Nilsson et al., 2005" startWordPosition="1182" endWordPosition="1185">rse, and we compare the correct parse yt and the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordingly. 4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) . 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges. Since the CoNLL-X shared task 237 Given a part (DEPREL, i, j) DEPREL, dir(i, j) DEPREL, dir(i, j), wordi DEPREL, dir(i, j), posi DEPREL, dir(i, j), wordj DEPREL, dir(i, j), posj DEPREL, dir(i, j), wordi, posi DEPREL, dir(i, j), wordj, posj DEPREL, dir(i, j), wordi−1 DEPREL, dir(i, j), posi−1 DEPREL, dir(i, j), wordi−1, posi−1 DEPREL, dir(i, j), wordj−1 DEPREL, dir(i, j), posj−1 DEPREL, dir(i, j), wordj−1, posj−1 DEPREL, dir(i, j), wordi+1 DEPREL, dir(i, j), p</context>
</contexts>
<marker>Nilsson, Hall, Nivre, 2005</marker>
<rawString>J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity. In Proc. of the NODALIDA Special Session on Treebanks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Nilsson</author>
</authors>
<title>Pseudo-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1338" citStr="Nivre and Nilsson, 2005" startWordPosition="200" endWordPosition="203">rsing grow more popular. It is not rare to see dependency relations used as features, in tasks such as relation extraction (Bunescu and Mooney, 2005) and machine translation (Ding and Palmer, 2005). Although English dependency relations are mostly projective, in other languages with more flexible word order, such as Czech, non-projective dependencies are more frequent. There are generally two methods for learning non-projective dependencies. You could map a non-projective dependency tree to a projective one, learn and predict the tree, then bring it back to the non-projective dependency tree (Nivre and Nilsson, 2005). Non-projective dependency parsing can also be represented as search for a maximum spanning tree in a directed graph, and this technique has been shown to perform well in Czech (McDonald et al., 2005). In this paper, we investigate the effectiveness of (McDonald et al., 2005) in the various languages given by the CoNLL 2006 shared task for non-projective labeled dependency parsing. The paper is structured as follows: in section 2 and 3, we review the decoding and learning aspects of (McDonald et al., 2005), and in section 4, we describe the extension of the algorithm and the features needed f</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency parsing. In Proc. of the 43rd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
<author>B Say</author>
<author>D Zeynep Hakkani-T¨ur</author>
<author>G T¨ur</author>
</authors>
<title>Building a Turkish treebank. In Abeill´e (Abeill´e,</title>
<date>2003</date>
<pages>15</pages>
<marker>Oflazer, Say, Hakkani-T¨ur, T¨ur, 2003</marker>
<rawString>K. Oflazer, B. Say, D. Zeynep Hakkani-T¨ur, and G. T¨ur. 2003. Building a Turkish treebank. In Abeill´e (Abeill´e, 2003), chapter 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Simov</author>
<author>P Osenova</author>
</authors>
<title>Practical annotation scheme for an HPSG treebank of Bulgarian.</title>
<date>2003</date>
<booktitle>In Proc. of the 4th Intern. Workshop on Linguistically Interpreteted Corpora (LINC),</booktitle>
<pages>17--24</pages>
<contexts>
<context position="6444" citStr="Simov and Osenova, 2003" startWordPosition="1140" endWordPosition="1143"> the single best MIRA (Crammer and Singer, 2003), which is a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004) for structured prediction. In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordingly. 4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) . 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges. Since the CoNLL-X shared task 237 Given a part (DEPREL, i, j) DEPREL, dir(i, j) DEPREL, dir(i, j), wordi DEPREL, dir(i, j), posi DEPREL, dir(i, j), wordj DEPREL, dir(i, j), posj DEPREL, dir(i, j), wordi, posi DEPREL, dir(i, </context>
</contexts>
<marker>Simov, Osenova, 2003</marker>
<rawString>K. Simov and P. Osenova. 2003. Practical annotation scheme for an HPSG treebank of Bulgarian. In Proc. of the 4th Intern. Workshop on Linguistically Interpreteted Corpora (LINC), pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Simov</author>
<author>P Osenova</author>
<author>A Simov</author>
<author>M Kouylekov</author>
</authors>
<title>Design and implementation of the Bulgarian HPSG-basedtreebank.</title>
<date>2005</date>
<booktitle>In Journal ofResearch onLanguage and Computation – Special Issue,</booktitle>
<pages>495--522</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="6419" citStr="Simov et al., 2005" startWordPosition="1136" endWordPosition="1139"> 2005), we have used the single best MIRA (Crammer and Singer, 2003), which is a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004) for structured prediction. In short, the update is executed when the decoder fails to predict the correct parse, and we compare the correct parse yt and the incorrect parse y′ suggested by the decoding algorithm. The weights of the features in y′ will be lowered, and the weights of the features in yt will be increased accordingly. 4 Experiments Our experiments were conducted on CoNLL-X shared task, with various datasets (Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) . 4.1 Dependency Relation The CLE algorithm works on a directed graph with unlabeled edges. Since the CoNLL-X shared task 237 Given a part (DEPREL, i, j) DEPREL, dir(i, j) DEPREL, dir(i, j), wordi DEPREL, dir(i, j), posi DEPREL, dir(i, j), wordj DEPREL, dir(i, j), posj DEPREL, dir(i, j), wo</context>
</contexts>
<marker>Simov, Osenova, Simov, Kouylekov, 2005</marker>
<rawString>K. Simov, P. Osenova, A. Simov, and M. Kouylekov. 2005. Design and implementation of the Bulgarian HPSG-basedtreebank. In Journal ofResearch onLanguage and Computation – Special Issue, pages 495– 522. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proc. of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2272" citStr="Taskar et al., 2004" startWordPosition="363" endWordPosition="366">e CoNLL 2006 shared task for non-projective labeled dependency parsing. The paper is structured as follows: in section 2 and 3, we review the decoding and learning aspects of (McDonald et al., 2005), and in section 4, we describe the extension of the algorithm and the features needed for the CoNLL 2006 shared task. 2 Non-Projective Dependency Parsing 2.1 Dependency Structure Let us define x to be a generic sequence of input tokens together with their POS tags and other morphological features, and y to be a generic dependency structure, that is, a set of edges for x. We use the terminology in (Taskar et al., 2004) for a generic structured output prediction, and define apart. Apart represents an edge together with its label. A part is a tuple (DEPREL, i, j) where i is the start point of the edge, j is the end point, and DEPREL is the label of the edge. The token at i is the head of the token at j. Table 1 shows our formulation of building a nonprojective dependency tree as a prediction problem. The task is to predict y, the set of parts (column 3, Table 1), given x, the input tokens and their features (column 1 and 2, Table 1). In this paper we use the common method of factoring the score of the depende</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. 2004. Max-margin parsing. In Proc. of Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L van der Beek</author>
<author>G Bouma</author>
<author>R Malouf</author>
<author>G van Noord</author>
</authors>
<title>The Alpino dependency treebank.</title>
<date>2002</date>
<booktitle>In Computational Linguistics in the Netherlands (CLIN).</booktitle>
<marker>van der Beek, Bouma, Malouf, van Noord, 2002</marker>
<rawString>L. van der Beek, G. Bouma, R. Malouf, and G. van Noord. 2002. The Alpino dependency treebank. In Computational Linguistics in the Netherlands (CLIN).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>