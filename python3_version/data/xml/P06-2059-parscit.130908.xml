<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.9955495">
Automatic Construction of Polarity-tagged Corpus from HTML
Documents
</title>
<author confidence="0.989915">
Nobuhiro Kaji and Masaru Kitsuregawa
</author>
<affiliation confidence="0.9997515">
Institute of Industrial Science
the University of Tokyo
</affiliation>
<address confidence="0.962686">
4-6-1 Komaba, Meguro-ku, Tokyo 153-8505 Japan
</address>
<email confidence="0.999787">
{kaji,kitsure}@tkl.iis.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.997402" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949833333333">
This paper proposes a novel method
of building polarity-tagged corpus from
HTML documents. The characteristics of
this method is that it is fully automatic and
can be applied to arbitrary HTML docu-
ments. The idea behind our method is
to utilize certain layout structures and lin-
guistic pattern. By using them, we can
automatically extract such sentences that
express opinion. In our experiment, the
method could construct a corpus consist-
ing of 126,610 sentences.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999875966666667">
Recently, there has been an increasing interest in
such applications that deal with opinions (a.k.a.
sentiment, reputation etc.). For instance, Mori-
naga et al. developed a system that extracts and
analyzes reputations on the Internet (Morinaga et
al., 2002). Pang et al. proposed a method of clas-
sifying movie reviews into positive and negative
ones (Pang et al., 2002).
In these applications, one of the most important
issue is how to determine the polarity (or semantic
orientation) of a given text. In other words, it is
necessary to decide whether a given text conveys
positive or negative content.
In order to solve this problem, we intend to
take statistical approach. More specifically, we
plan to learn the polarity of texts from a cor-
pus in which phrases, sentences or documents
are tagged with labels expressing the polarity
(polarity-tagged corpus).
So far, this approach has been taken by a lot of
researchers (Pang et al., 2002; Dave et al., 2003;
Wilson et al., 2005). In these previous works,
polarity-tagged corpus was built in either of the
following two ways. It is built manually, or created
from review sites such as AMAZON.COM. In some
review sites, the review is associated with meta-
data indicating its polarity. Those reviews can be
used as polarity-tagged corpus. In case of AMA-
ZON.COM, the review’s polarity is represented by
using 5-star scale.
However, both of the two approaches are not
appropriate for building large polarity-tagged cor-
pus. Since manual construction of tagged corpus
is time-consuming and expensive, it is difficult to
build large polarity-tagged corpus. The method
that relies on review sites can not be applied to
domains in which large amount of reviews are not
available. In addition, the corpus created from re-
views is often noisy as we discuss in Section 2.
This paper proposes a novel method of building
polarity-tagged corpus from HTML documents.
The idea behind our method is to utilize certain
layout structures and linguistic pattern. By using
them, we can automatically extract sentences that
express opinion (opinion sentences) from HTML
documents. Because this method is fully auto-
matic and can be applied to arbitrary HTML doc-
uments, it does not suffer from the same problems
as the previous methods.
In the experiment, we could construct a corpus
consisting of 126,610 sentences. To validate the
quality of the corpus, two human judges assessed
a part of the corpus and found that 92% opinion
sentences are appropriate ones. Furthermore, we
applied our corpus to opinion sentence classifica-
tion task. Naive Bayes classifier was trained on
our corpus and tested on three data sets. The re-
sult demonstrated that the classifier achieved more
than 80% accuracy in each data set.
The following of this paper is organized as fol-
</bodyText>
<page confidence="0.975751">
452
</page>
<note confidence="0.7320595">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 452–459,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.988028333333333">
lows. Section 2 shows the design of the corpus
constructed by our method. Section 3 gives an
overview of our method, and the detail follows in
Section 4. In Section 5, we discuss experimen-
tal results, and in Section 6 we examine related
works. Finally we conclude in Section 7.
one review rather than individual sentences in a
review. This is one serious problem in previous
works.
</bodyText>
<tableCaption confidence="0.911983">
Table 1: A part of automatically constructed
polarity-tagged corpus.
</tableCaption>
<sectionHeader confidence="0.929044" genericHeader="method">
2 Corpus Design
</sectionHeader>
<bodyText confidence="0.999276285714286">
This Section explains the design of our corpus that
is built automatically. Table 1 represents a part
of our corpus that was actually constructed in the
experiment. Note that this paper treats Japanese.
The sentences in the Table are translations, and the
original sentences are in Japanese.
The followings are characteristics of our corpus:
</bodyText>
<listItem confidence="0.96249375">
• Our corpus uses two labels, + and —. They
denote positive and negative sentences re-
spectively. Other labels such as ’neutral’ are
not used.
• Since we do not use ’neutral’ label, such sen-
tence that does not convey opinion is not
stored in our corpus.
• The label is assigned to not multiple sen-
</listItem>
<bodyText confidence="0.994410096774194">
tences (or document) but single sentence.
Namely, our corpus is tagged at sentence
level rather than document level.
It is important to discuss the reason that we in-
tend to build a corpus tagged at sentence level
rather than document level. The reason is that one
document often includes both positive and nega-
tive sentences, and hence it is difficult to learn
the polarity from the corpus tagged at document
level. Consider the following example (Pang et
al., 2002):
This film should be brilliant. It sounds
like a great plot, the actors are first
grade, and the supporting cast is good as
well, and Stallone is attempting to de-
liver a good performance. However, it
can’t hold up.
This document as a whole expresses negative
opinion, and should be labeled ’negative’ if it is
tagged at document level. However, it includes
several sentences that represent positive attitude.
We would like to point out that polarity-tagged
corpus created from reviews prone to be tagged at
document-level. This is because meta-data (e.g.
stars in AMAZON.COM) is usually associated with
label opinion sentence
+ It has high adaptability.
— The cost is expensive.
— The engine is powerless and noisy.
+ The usage is easy to understand.
+ Above all, the price is reasonable.
</bodyText>
<sectionHeader confidence="0.994967" genericHeader="method">
3 The Idea
</sectionHeader>
<bodyText confidence="0.999702454545455">
This Section briefly explains our basic idea, and
the detail of our corpus construction method is
represented in the next Section.
Our idea is to use certain layout structures and
linguistic pattern in order to extract opinion sen-
tences from HTML documents. More specifically,
we used two kinds of layout structures: the item-
ization and the table. In what follows, we ex-
plain examples where opinion sentences can be
extracted by using the itemization, table and lin-
guistic pattern.
</bodyText>
<subsectionHeader confidence="0.99232">
3.1 Itemization
</subsectionHeader>
<bodyText confidence="0.999974833333333">
The first idea is to extract opinion sentences from
the itemization (Figure 1). In this Figure, opinions
about a music player are itemized and these item-
izations have headers such as ’pros’ and ’cons’.
By using the headers, we can recognize that opin-
ion sentences are described in these itemizations.
</bodyText>
<listItem confidence="0.9466693">
Pros:
• The sound is natural.
• Music is easy to find.
• Can enjoy creating my favorite play-lists.
Cons:
• The remote controller does not have an LCD dis-
play.
• The body gets scratched and fingerprinted easily.
• The battery drains quickly when using the back-
light.
</listItem>
<figureCaption confidence="0.9720105">
Figure 1: Opinion sentences in itemization.
Hereafter, such phrases that indicate the pres-
</figureCaption>
<page confidence="0.998608">
453
</page>
<bodyText confidence="0.98258">
ence of opinion sentences are called indicators.
Indicators for positive sentences are called positive
indicators. ’Pros’ is an example of positive indi-
cator. Similarly, indicators for negative sentences
are called negative indicators.
</bodyText>
<subsectionHeader confidence="0.998457">
3.2 Table
</subsectionHeader>
<bodyText confidence="0.999836666666667">
The second idea is to use the table structure (Fig-
ure 2). In this Figure, a car review is summarized
in the table.
</bodyText>
<figure confidence="0.814123333333333">
Mileage(urban) 7.0km/litter
Mileage(highway) 9.0km/litter
Plus This is a four door car, but it’s
so cool.
Minus The seat is ragged and the light
is dark.
</figure>
<figureCaption confidence="0.998892">
Figure 2: Opinion sentences in table.
</figureCaption>
<bodyText confidence="0.999974">
We can predict that there are opinion sentences
in this table, because the left column acts as a
header and there are indicators (plus and minus)
in that column.
</bodyText>
<subsectionHeader confidence="0.999699">
3.3 Linguistic pattern
</subsectionHeader>
<bodyText confidence="0.999966571428572">
The third idea is based on linguistic pattern. Be-
cause we treat Japanese, the pattern that is dis-
cussed in this paper depends on Japanese gram-
mar although we think there are similar patterns in
other languages including English.
Consider the Japanese sentences attached with
English translations (Figure 3). Japanese sen-
tences are written in italics and ’-’ denotes that
the word is followed by postpositional particles.
For example, ’software-no’ means that ’software’
is followed by postpositional particle ’no’. Trans-
lations of each word and the entire sentence are
represented below the original Japanese sentence.
’-POST’ means postpositional particle.
In the examples, we focused on the singly un-
derlined phrases. Roughly speaking, they corre-
spond to ’the advantage/weakness is to’ in En-
glish. In these phrases, indicators (’riten (ad-
vantage)’ and ’ketten (weakness)’) are followed
by postpositional particle ’-ha’, which is topic
marker. And hence, we can recognize that some-
thing good (or bad) is the topic of the sentence.
Based on this observation, we crafted a linguis-
tic pattern that can detect the singly underlined
phrases. And then, we extracted doubly under-
lined phrases as opinions. They correspond to ’run
quickly’ and ’take too much time’. The detail of
this process is discussed in the next Section.
</bodyText>
<sectionHeader confidence="0.994742" genericHeader="method">
4 Automatic Corpus Construction
</sectionHeader>
<bodyText confidence="0.999326909090909">
This Section represents the detail of the corpus
construction procedure.
As shown in the previous Section, our idea uti-
lizes the indicator, and it is important to recognize
indicators in HTML documents. To do this, we
manually crafted lexicon, in which positive and
negative indicators are listed. This lexicon con-
sists of 303 positive and 433 negative indicators.
Using this lexicon, the polarity-tagged corpus is
constructed from HTML documents. The method
consists of the following three steps:
</bodyText>
<listItem confidence="0.862739">
1. Preprocessing
</listItem>
<bodyText confidence="0.9993384">
Before extracting opinion sentences, HTML
documents are preprocessed. This process
involves separating texts form HTML tags,
recognizing sentence boundary, and comple-
menting omitted HTML tags etc.
</bodyText>
<listItem confidence="0.622992">
2. Opinion sentence extraction
</listItem>
<bodyText confidence="0.987479333333333">
Opinion sentences are extracted from HTML
documents by using the itemization, table
and linguistic pattern.
</bodyText>
<sectionHeader confidence="0.493592" genericHeader="method">
3. Filtering
</sectionHeader>
<bodyText confidence="0.999699375">
Since HTML documents are noisy, some of
the extracted opinion sentences are not ap-
propriate. They are removed in this step.
For the preprocessing, we implemented simple
rule-based system. We cannot explain its detail
for lack of space. In the remainder of this Section,
we describe three extraction methods respectively,
and then examine filtering technique.
</bodyText>
<subsectionHeader confidence="0.999173">
4.1 Extraction based on itemization
</subsectionHeader>
<bodyText confidence="0.999994285714286">
The first method utilizes the itemization. In order
to extract opinion sentences, first of all, we have
to find such itemization as illustrated in Figure 1.
They are detected by using indicator lexicon and
HTML tags such as &lt;h1&gt; and &lt;ul&gt; etc.
After finding the itemizations, the sentences in
the items are extracted as opinion sentences. Their
polarity labels are assigned according to whether
the header is positive or negative indicator. From
the itemization in Figure 1, three positive sen-
tences and three negative ones are extracted.
The problem here is how to treat such item that
has more than one sentences (Figure 4). In this
itemization, there are two sentences in each of the
</bodyText>
<page confidence="0.992549">
454
</page>
<listItem confidence="0.911033">
(1) kono software-no riten-ha hayaku ugoku koto
</listItem>
<bodyText confidence="0.6970805">
this software-POST advantage-POST quickly run to
The advantage of this software is to run quickly.
</bodyText>
<listItem confidence="0.85265">
(2) ketten-ha jikan-ga kakarisugiru koto-desu
</listItem>
<bodyText confidence="0.947052">
weakness-POST time-POST take too much to-POST
The weakness is to take too much time.
</bodyText>
<figureCaption confidence="0.997683">
Figure 3: Instances of the linguistic pattern.
</figureCaption>
<bodyText confidence="0.99685225">
third and fourth item. It is hard to precisely pre-
dict the polarity of each sentence in such items,
because such item sometimes includes both posi-
tive and negative sentences. For example, in the
third item of the Figure, there are two sentences.
One (’Has high pixel...’) is positive and the other
(’I was not satisfied...’) is negative.
To get around this problem, we did not use such
items. From the itemization in Figure 4, only two
positive sentences are extracted (’the color is re-
ally good’ and ’this camera makes me happy while
taking pictures’).
</bodyText>
<listItem confidence="0.937735">
Pros:
• The color is really good.
• This camera makes me happy while taking pic-
tures.
• Has high pixel resolution with 4 million pixels. I
was not satisfied with 2 million.
• EVF is easy to see. But, compared with SLR, it’s
hard to see.
</listItem>
<figureCaption confidence="0.9964125">
Figure 4: Itemization where more than one sen-
tences are written in one item.
</figureCaption>
<subsectionHeader confidence="0.994851">
4.2 Extraction based on table
</subsectionHeader>
<bodyText confidence="0.999344214285714">
The second method extracts opinion sentences
from the table. Since the combination of &lt;table&gt;
and other tags can represent various kinds of ta-
bles, it is difficult to craft precise rules that can
deal with any table.
Therefore, we consider only two types of tables
in which opinion sentences are described (Figure
5). Type A is a table in which the leftmost column
acts as a header, and there are indicators in that
column. Similarly, type B is a table in which the
first row acts as a header. The table illustrated in
Figure 2 is categorized into type A.
The type of the table is decided as follows. The
table is categorized into type A if there are both
</bodyText>
<figure confidence="0.9803218">
type A
4 + + +
I_ � � �
I+:positive indicator +:positive sentence
1—:negative indicator —:negative sentence
</figure>
<figureCaption confidence="0.999789">
Figure 5: Two types of tables.
</figureCaption>
<bodyText confidence="0.999976923076923">
positive and negative indicators in the leftmost col-
umn. The table is categorized into type B if it is
not type A and there are both positive and negative
indicators in the first row.
After the type of the table is decided, we can
extract opinion sentences from the cells that cor-
respond to + and — in the Figure 5. It is obvi-
ous which label (positive or negative) should be
assigned to the extracted sentence.
We did not use such cell that contains more than
one sentences, because it is difficult to reliably
predict the polarity of each sentence. This is simi-
lar to the extraction from the itemization.
</bodyText>
<subsectionHeader confidence="0.998347">
4.3 Extraction based on linguistic pattern
</subsectionHeader>
<bodyText confidence="0.999910875">
The third method uses linguistic pattern. The char-
acteristic of this pattern is that it takes dependency
structure into consideration.
First of all, we explain Japanese dependency
structure. Figure 6 depicts the dependency rep-
resentations of the sentences in the Figure 3.
Japanese sentence is represented by a set of de-
pendencies between phrasal units called bunsetsu-
phrases. Broadly speaking, bunsetsu-phrase is an
unit similar to baseNP in English. In the Fig-
ure, square brackets enclose bunsetsu-phrase and
arrows show modifier —� head dependencies be-
tween bunsetsu-phrases.
In order to extract opinion sentences from these
dependency representations, we crafted the fol-
lowing dependency pattern.
</bodyText>
<figure confidence="0.918391692307692">
type B
455
[ kono ] [ software-no ] [ riten-ha ] [ hayaku ] [ ugoku ] [ koto ]
this software-POST
advantage-POST quickly run to
[ ketten-ha
] [ jikan-ga
] [ kakari sugiru
] [ koto-desu ]
weakness-POST
time-POST
take too much
to-POST
</figure>
<figureCaption confidence="0.999868">
Figure 6: Dependency representations.
</figureCaption>
<bodyText confidence="0.965749692307692">
[ INDICATOR-ha ] [ koto-POST* ]
This pattern matches the singly underlined
bunsetsu-phrases in the Figure 6. In the modi-
fier part of this pattern, the indicator is followed
by postpositional particle ’ha’, which is topic
marker1. In the head part, ’koto (to)’ is followed
by arbitrary numbers of postpositional particles.
If we find the dependency that matches this pat-
tern, a phrase between the two bunsetsu-phrases
is extracted as opinion sentence. In the Figure 6,
the doubly underlined phrases are extracted. This
heuristics is based on Japanese word order con-
straint.
</bodyText>
<subsectionHeader confidence="0.993404">
4.4 Filtering
</subsectionHeader>
<bodyText confidence="0.99993">
Sentences extracted by the above methods some-
times include noise text. Such texts have to be fil-
tered out. There are two cases that need filtering
process.
First, some of the extracted sentences do not ex-
press opinions. Instead, they represent objects to
which the writer’s opinion is directed (Table 7).
From this table, ’the overall shape’ and ’the shape
of the taillight’ are wrongly extracted as opinion
sentences. Since most of the objects are noun
phrases, we removed such sentences that have the
noun as the head.
</bodyText>
<figure confidence="0.36992875">
Mileage(urban) 10.0km/litter
Mileage(highway) 12.0km/litter
Plus The overall shape.
Minus The shape of the taillight.
</figure>
<figureCaption confidence="0.9961415">
Figure 7: A table describing only objects to which
the opinion is directed.
</figureCaption>
<bodyText confidence="0.969204">
Secondly, we have to treat duplicate opinion
sentences because there are mirror sites in the
</bodyText>
<footnote confidence="0.799049666666667">
1To be exact, some of the indicators such as ’strong point’
consists of more than one bunsetsu-phrase, and the modifier
part sometimes consists of more than one bunsetsu-phrase.
</footnote>
<bodyText confidence="0.990831333333333">
HTML documents. When there are more than one
sentences that are exactly the same, one of them is
held and the others are removed.
</bodyText>
<sectionHeader confidence="0.984446" genericHeader="method">
5 Experimental Results and Discussion
</sectionHeader>
<bodyText confidence="0.999900666666667">
This Section examines the results of corpus con-
struction experiment. To analyze Japanese sen-
tence we used Juman and KNP2.
</bodyText>
<subsectionHeader confidence="0.990166">
5.1 Corpus Construction
</subsectionHeader>
<bodyText confidence="0.999255555555556">
About 120 millions HTML documents were pro-
cessed, and 126,610 opinion sentences were ex-
tracted. Before the filtering, there were 224,002
sentences in our corpus. Table2 shows the statis-
tics of our corpus. The first column represents the
three extraction methods. The second and third
column shows the number of positive and nega-
tive sentences by extracted each method. Some
examples are illustrated in Table 3.
</bodyText>
<tableCaption confidence="0.995243">
Table 2: # of sentences in the corpus.
</tableCaption>
<table confidence="0.995323">
Positive Negative Total
Itemization 18,575 15,327 33,902
Table 12,103 11,016 23,119
Linguistic Pattern 34,282 35,307 69,589
Total 64,960 61,650 126,610
</table>
<bodyText confidence="0.9980222">
The result revealed that more than half of the
sentences are extracted by linguistic pattern (see
the fourth row). Our method turned out to be ef-
fective even in the case where only plain texts are
available.
</bodyText>
<subsectionHeader confidence="0.999149">
5.2 Quality assessment
</subsectionHeader>
<bodyText confidence="0.9960272">
In order to check the quality of our corpus,
500 sentences were randomly picked up and two
judges manually assessed whether appropriate la-
bels are assigned to the sentences.
The evaluation procedure is the followings.
</bodyText>
<footnote confidence="0.934926">
2http://www.kc.t.u-tokyo.ac.jp/nl-resource/top.html
</footnote>
<page confidence="0.999134">
456
</page>
<tableCaption confidence="0.999697">
Table 3: Examples of opinion sentences.
</tableCaption>
<table confidence="0.866515875">
label opinion sentence
cost keisan-ga yoininaru
+ cost computation-POST become easy
It becomes easy to compute cost.
kantan-de jikan-ga setsuyakudekiru
+ easy-POST time-POST can save
It’s easy and can save time.
soup-ha koku-ga ari oishii
+ soup-POST rich flavorful
The soup is rich and flavorful.
HTML keishiki-no mail-ni taioshitenai
HTML format-POST mail-POST cannot use
Cannot use mails in HTML format.
jugyo-ga hijoni tsumaranai
lecture-POST really boring
The lecture is really boring.
</table>
<subsubsectionHeader confidence="0.436268">
kokoro-ni nokoru ongaku-ga nai
</subsubsectionHeader>
<bodyText confidence="0.979825">
impressive music-POST there is no
There is no impressive music.
</bodyText>
<listItem confidence="0.945182444444444">
• Each of the 500 sentences are shown to the
two judges. Throughout this evaluation, We
did not present the label automatically tagged
by our method. Similarly, we did not show
HTML documents from which the opinion
sentences are extracted.
• The two judges individually categorized each
sentence into three groups: positive, negative
and neutral/ambiguous. The sentence is clas-
</listItem>
<bodyText confidence="0.966019043478261">
sified into the third group, if it does not ex-
press opinion (neutral) or if its polarity de-
pends on the context (ambiguous). Thus, two
goldstandard sets were created.
• The precision is estimated using the goldstan-
dard. In this evaluation, the precision refers
to the ratio of sentences where correct la-
bels are assigned by our method. Since we
have two goldstandard sets, we can report
two different precision values. A sentence
that is categorized into neutral/ambiguous by
the judge is interpreted as being assigned in-
correct label by our method, since our corpus
does not have a label that corresponds to neu-
tral/ambiguous.
We investigated the two goldstandard sets, and
found that the judges agree with each other in 467
out of 500 sentences (93.4%). The Kappa value
was 0.901. From this result, we can say that the
goldstandard was reliably created by the judges.
Then, we estimated the precision. The precision
was 459/500 (91.5%) when one goldstandard was
used, and 460/500 (92%) when the other was used.
Since these values are nearly equal to the agree-
ment between humans (467/500), we can conclude
that our method successfully constructed polarity-
tagged corpus.
After the evaluation, we analyzed errors and
found that most of them were caused by the lack
of context. The following is a typical example.
You see, there is much information.
In our corpus this sentence is categorized into pos-
itive one. The below is a part of the original docu-
ment from which this sentence was extracted.
I recommend this guide book. The Pros.
of this book is that, you see, there is
much information.
On the other hand, both of the two judges catego-
rized the above sentence into neutral/ambiguous,
probably because they can easily assume context
where much information is not desirable.
You see, there is much information. But,
it is not at all arranged, and makes me
confused.
In order to precisely treat this kind of sentences,
we think discourse analysis is inevitable.
</bodyText>
<subsectionHeader confidence="0.999477">
5.3 Application to opinion classification
</subsectionHeader>
<bodyText confidence="0.999986086956522">
Next, we applied our corpus to opinion sentence
classification. This is a task of classifying sen-
tences into positive and negative. We trained a
classifier on our corpus and investigated the result.
Classifier and data sets As a classifier, we
chose Naive Bayes with bag-of-words features,
because it is one of the most popular one in this
task. Negation was processed in a similar way as
previous works (Pang et al., 2002).
To validate the accuracy of the classifier, three
data sets were created from review pages in which
the review is associated with meta-data. To build
data sets tagged at sentence level, we used such re-
views that contain only one sentence. Table 4 rep-
resents the domains and the number of sentences
in each data set. Note that we confirmed there is
no duplicate between our corpus and the these data
sets.
The result and discussion Naive Bayes classi-
fier was trained on our corpus and tested on the
three data sets (Table 5). In the Table, the sec-
ond column represents the accuracy of the clas-
sification in each data set. The third and fourth
</bodyText>
<page confidence="0.999025">
457
</page>
<tableCaption confidence="0.998504">
Table 5: Classification result.
</tableCaption>
<table confidence="0.9968844">
Accuracy Positive Negative
Precision Recall Precision Recall
Computer 0.831 0.856 0.804 0.804 0.859
Restaurant 0.849 0.905 0.859 0.759 0.832
Car 0.833 0.860 0.844 0.799 0.819
</table>
<tableCaption confidence="0.99674">
Table 4: The data sets.
</tableCaption>
<table confidence="0.7138896">
Domain # of sentences
Positive Negative
Computer 933 910
Restaurant 753 409
Car 1,056 800
</table>
<bodyText confidence="0.998114272727273">
columns represent precision and recall of positive
sentences. The remaining two columns show those
of negative sentences. Naive Bayes achieved over
80% accuracy in all the three domains.
In order to compare our corpus with a small
domain specific corpus, we estimated accuracy in
each data set using 10 fold crossvalidation (Ta-
ble 6). In two domains, the result of our corpus
outperformed that of the crossvalidation. In the
other domain, our corpus is slightly better than the
crossvalidation.
</bodyText>
<tableCaption confidence="0.994252">
Table 6: Accuracy comparison.
</tableCaption>
<table confidence="0.863634">
Our corpus Crossvalidation
Computer 0.831 0.821
Restaurant 0.849 0.848
Car 0.833 0.808
</table>
<bodyText confidence="0.99966625">
One finding is that our corpus achieved good ac-
curacy, although it includes various domains and is
not accustomed to the target domain. Turney also
reported good result without domain customiza-
tion (Turney, 2002). We think these results can be
further improved by domain adaptation technique,
and it is one future work.
Furthermore, we examined the variance of the
accuracy between different domains. We trained
Naive Bayes on each data set and investigate the
accuracy in the other data sets (Table 7). For ex-
ample, when the classifier is trained on Computer
and tested on Restaurant, the accuracy was 0.757.
This result revealed that the accuracy is quite poor
when the training and test sets are in different do-
mains. On the other hand, when Naive Bayes is
trained on our corpus, there are little variance in
different domains (Table 5). This experiment in-
dicates that our corpus is relatively robust against
the change of the domain compared with small do-
main specific corpus. We think this is because our
corpus is large and balanced. Since we cannot al-
ways get domain specific corpus in real applica-
tion, this is the strength of our corpus.
</bodyText>
<tableCaption confidence="0.994278">
Table 7: Cross domain evaluation.
</tableCaption>
<table confidence="0.997796">
Computer Training Car
Restaurant
Computer 0.701 0.773
Test Restaurant 0.757 — 0.755
Car 0.751 0.711
</table>
<sectionHeader confidence="0.99996" genericHeader="method">
6 Related Works
</sectionHeader>
<subsectionHeader confidence="0.99975">
6.1 Learning the polarity of words
</subsectionHeader>
<bodyText confidence="0.999976777777778">
There are some works that discuss learning the po-
larity of words instead of sentences.
Hatzivassiloglou and McKeown proposed a
method of learning the polarity of adjectives from
corpus (Hatzivassiloglou and McKeown, 1997).
They hypothesized that if two adjectives are con-
nected with conjunctions such as ’and/but’, they
have the same/opposite polarity. Based on this hy-
pothesis, their method predicts the polarity of ad-
jectives by using a small set of adjectives labeled
with the polarity.
Other works rely on linguistic resources such
as WordNet (Kamps et al., 2004; Hu and Liu,
2004; Esuli and Sebastiani, 2005; Takamura et al.,
2005). For example, Kamps et al. used a graph
where nodes correspond to words in the Word-
Net, and edges connect synonymous words in the
WordNet. The polarity of an adjective is defined
by its shortest paths from the node corresponding
to ’good’ and ’bad’.
Although those researches are closely related to
our work, there is a striking difference. In those
researches, the target is limited to the polarity of
words and none of them discussed sentences. In
addition, most of the works rely on external re-
sources such as the WordNet, and cannot treat
words that are not in the resources.
</bodyText>
<page confidence="0.994804">
458
</page>
<subsectionHeader confidence="0.999585">
6.2 Learning subjective phrases
</subsectionHeader>
<bodyText confidence="0.99997936">
Some researchers examined the acquisition of sub-
jective phrases. The subjective phrase is more gen-
eral concept than opinion and includes both posi-
tive and negative expressions.
Wiebe learned subjective adjectives from a set
of seed adjectives. The idea is to automatically
identify the synonyms of the seed and to add them
to the seed adjectives (Wiebe, 2000). Riloff et
al. proposed a bootstrapping approach for learn-
ing subjective nouns (Riloff et al., 2003). Their
method learns subjective nouns and extraction pat-
terns in turn. First, given seed subjective nouns,
the method learns patterns that can extract sub-
jective nouns from corpus. And then, the pat-
terns extract new subjective nouns from corpus,
and they are added to the seed nouns. Although
this work aims at learning only nouns, in the sub-
sequent work, they also proposed a bootstrapping
method that can deal with phrases (Riloff and
Wiebe, 2003). Similarly, Wiebe also proposes a
bootstrapping approach to create subjective and
objective classifier (Wiebe and Riloff, 2005).
These works are different from ours in a sense
that they did not discuss how to determine the po-
larity of subjective words or phrases.
</bodyText>
<subsectionHeader confidence="0.992846">
6.3 Unsupervised sentiment classification
</subsectionHeader>
<bodyText confidence="0.9999836">
Turney proposed the unsupervised method for sen-
timent classification (Turney, 2002), and similar
method is utilized by many other researchers (Yu
and Hatzivassiloglou, 2003). The concept behind
Turney’s model is that positive/negative phrases
co-occur with words like ’excellent/poor’. The co-
occurrence statistic is measured by the result of
search engine. Since his method relies on search
engine, it is difficult to use rich linguistic informa-
tion such as dependencies.
</bodyText>
<sectionHeader confidence="0.999386" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999939727272727">
This paper proposed a fully automatic method of
building polarity-tagged corpus from HTML doc-
uments. In the experiment, we could build a cor-
pus consisting of 126,610 sentences.
As a future work, we intend to extract more
opinion sentences by applying this method to
larger HTML document sets and enhancing ex-
traction rules. Another important direction is to
investigate more precise model that can classify or
extract opinions, and learn its parameters from our
corpus.
</bodyText>
<sectionHeader confidence="0.998459" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999834711538461">
Kushal Dave, Steve Lawrence, and David M.Pennock.
2003. Mining the peanut gallery: Opinion extrac-
tion and semantic classification of product revews.
In Proceedings of the WWW, pages 519–528.
Andrea Esuli and Fabrizio Sebastiani. 2005. Deter-
mining the semantic orientation of terms throush
gloss classification. In Proceedings of the CIKM.
Vasileios Hatzivassiloglou and Katheleen R. McKe-
own. 1997. Predicting the semantic orientation of
adjectives. In Proceedings of the ACL, pages 174–
181.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
KDD, pages 168–177.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In Proceedings
of the LREC.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining product
reputations on the web. In Proceedings of the KDD.
Bo Pang, Lillian Lee, and Shivakumar Vaihyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
EMNLP.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the EMNLP.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the CoNLL.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientation of words us-
ing spin model. In Proceedings of the ACL, pages
133–140.
Peter D. Turney. 2002. Thumbs up or thumbs down?
senmantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings of the ACL,
pages 417–424.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of the CICLing.
Janyce M. Wiebe. 2000. Learning subjective adjec-
tives from corpora. In Proceedings of the AAAI.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the
HLT/EMNLP.
Hong Yu and Yasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opinion
sentences. In Proceedings of the EMNLP.
</reference>
<page confidence="0.999125">
459
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.894378">
<title confidence="0.9978345">Automatic Construction of Polarity-tagged Corpus from HTML Documents</title>
<author confidence="0.996067">Kaji Kitsuregawa</author>
<affiliation confidence="0.999786">Institute of Industrial Science the University of Tokyo</affiliation>
<address confidence="0.930071">4-6-1 Komaba, Meguro-ku, Tokyo 153-8505 Japan</address>
<abstract confidence="0.997594692307692">This paper proposes a novel method of building polarity-tagged corpus from HTML documents. The characteristics of this method is that it is fully automatic and can be applied to arbitrary HTML documents. The idea behind our method is to utilize certain layout structures and linguistic pattern. By using them, we can automatically extract such sentences that express opinion. In our experiment, the method could construct a corpus consisting of 126,610 sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product revews.</title>
<date>2003</date>
<booktitle>In Proceedings of the WWW,</booktitle>
<pages>519--528</pages>
<contexts>
<context position="1692" citStr="Dave et al., 2003" startWordPosition="260" endWordPosition="263">ve ones (Pang et al., 2002). In these applications, one of the most important issue is how to determine the polarity (or semantic orientation) of a given text. In other words, it is necessary to decide whether a given text conveys positive or negative content. In order to solve this problem, we intend to take statistical approach. More specifically, we plan to learn the polarity of texts from a corpus in which phrases, sentences or documents are tagged with labels expressing the polarity (polarity-tagged corpus). So far, this approach has been taken by a lot of researchers (Pang et al., 2002; Dave et al., 2003; Wilson et al., 2005). In these previous works, polarity-tagged corpus was built in either of the following two ways. It is built manually, or created from review sites such as AMAZON.COM. In some review sites, the review is associated with metadata indicating its polarity. Those reviews can be used as polarity-tagged corpus. In case of AMAZON.COM, the review’s polarity is represented by using 5-star scale. However, both of the two approaches are not appropriate for building large polarity-tagged corpus. Since manual construction of tagged corpus is time-consuming and expensive, it is difficu</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M.Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product revews. In Proceedings of the WWW, pages 519–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Determining the semantic orientation of terms throush gloss classification.</title>
<date>2005</date>
<booktitle>In Proceedings of the CIKM.</booktitle>
<contexts>
<context position="24834" citStr="Esuli and Sebastiani, 2005" startWordPosition="4048" endWordPosition="4051">rity of words There are some works that discuss learning the polarity of words instead of sentences. Hatzivassiloglou and McKeown proposed a method of learning the polarity of adjectives from corpus (Hatzivassiloglou and McKeown, 1997). They hypothesized that if two adjectives are connected with conjunctions such as ’and/but’, they have the same/opposite polarity. Based on this hypothesis, their method predicts the polarity of adjectives by using a small set of adjectives labeled with the polarity. Other works rely on linguistic resources such as WordNet (Kamps et al., 2004; Hu and Liu, 2004; Esuli and Sebastiani, 2005; Takamura et al., 2005). For example, Kamps et al. used a graph where nodes correspond to words in the WordNet, and edges connect synonymous words in the WordNet. The polarity of an adjective is defined by its shortest paths from the node corresponding to ’good’ and ’bad’. Although those researches are closely related to our work, there is a striking difference. In those researches, the target is limited to the polarity of words and none of them discussed sentences. In addition, most of the works rely on external resources such as the WordNet, and cannot treat words that are not in the resour</context>
</contexts>
<marker>Esuli, Sebastiani, 2005</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2005. Determining the semantic orientation of terms throush gloss classification. In Proceedings of the CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Katheleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>174--181</pages>
<contexts>
<context position="24443" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="3984" endWordPosition="3987">ange of the domain compared with small domain specific corpus. We think this is because our corpus is large and balanced. Since we cannot always get domain specific corpus in real application, this is the strength of our corpus. Table 7: Cross domain evaluation. Computer Training Car Restaurant Computer 0.701 0.773 Test Restaurant 0.757 — 0.755 Car 0.751 0.711 6 Related Works 6.1 Learning the polarity of words There are some works that discuss learning the polarity of words instead of sentences. Hatzivassiloglou and McKeown proposed a method of learning the polarity of adjectives from corpus (Hatzivassiloglou and McKeown, 1997). They hypothesized that if two adjectives are connected with conjunctions such as ’and/but’, they have the same/opposite polarity. Based on this hypothesis, their method predicts the polarity of adjectives by using a small set of adjectives labeled with the polarity. Other works rely on linguistic resources such as WordNet (Kamps et al., 2004; Hu and Liu, 2004; Esuli and Sebastiani, 2005; Takamura et al., 2005). For example, Kamps et al. used a graph where nodes correspond to words in the WordNet, and edges connect synonymous words in the WordNet. The polarity of an adjective is defined by it</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Katheleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the ACL, pages 174– 181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the KDD,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="24806" citStr="Hu and Liu, 2004" startWordPosition="4044" endWordPosition="4047"> Learning the polarity of words There are some works that discuss learning the polarity of words instead of sentences. Hatzivassiloglou and McKeown proposed a method of learning the polarity of adjectives from corpus (Hatzivassiloglou and McKeown, 1997). They hypothesized that if two adjectives are connected with conjunctions such as ’and/but’, they have the same/opposite polarity. Based on this hypothesis, their method predicts the polarity of adjectives by using a small set of adjectives labeled with the polarity. Other works rely on linguistic resources such as WordNet (Kamps et al., 2004; Hu and Liu, 2004; Esuli and Sebastiani, 2005; Takamura et al., 2005). For example, Kamps et al. used a graph where nodes correspond to words in the WordNet, and edges connect synonymous words in the WordNet. The polarity of an adjective is defined by its shortest paths from the node corresponding to ’good’ and ’bad’. Although those researches are closely related to our work, there is a striking difference. In those researches, the target is limited to the polarity of words and none of them discussed sentences. In addition, most of the works rely on external resources such as the WordNet, and cannot treat word</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the KDD, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Maarten Marx</author>
<author>Robert J Mokken</author>
<author>Maarten de Rijke</author>
</authors>
<title>Using wordnet to measure semantic orientations of adjectives.</title>
<date>2004</date>
<booktitle>In Proceedings of the LREC.</booktitle>
<marker>Kamps, Marx, Mokken, de Rijke, 2004</marker>
<rawString>Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten de Rijke. 2004. Using wordnet to measure semantic orientations of adjectives. In Proceedings of the LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Morinaga</author>
<author>Kenji Yamanishi</author>
<author>Kenji Tateishi</author>
<author>Toshikazu Fukushima</author>
</authors>
<title>Mining product reputations on the web.</title>
<date>2002</date>
<booktitle>In Proceedings of the KDD.</booktitle>
<contexts>
<context position="990" citStr="Morinaga et al., 2002" startWordPosition="140" endWordPosition="143">this method is that it is fully automatic and can be applied to arbitrary HTML documents. The idea behind our method is to utilize certain layout structures and linguistic pattern. By using them, we can automatically extract such sentences that express opinion. In our experiment, the method could construct a corpus consisting of 126,610 sentences. 1 Introduction Recently, there has been an increasing interest in such applications that deal with opinions (a.k.a. sentiment, reputation etc.). For instance, Morinaga et al. developed a system that extracts and analyzes reputations on the Internet (Morinaga et al., 2002). Pang et al. proposed a method of classifying movie reviews into positive and negative ones (Pang et al., 2002). In these applications, one of the most important issue is how to determine the polarity (or semantic orientation) of a given text. In other words, it is necessary to decide whether a given text conveys positive or negative content. In order to solve this problem, we intend to take statistical approach. More specifically, we plan to learn the polarity of texts from a corpus in which phrases, sentences or documents are tagged with labels expressing the polarity (polarity-tagged corpu</context>
</contexts>
<marker>Morinaga, Yamanishi, Tateishi, Fukushima, 2002</marker>
<rawString>Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and Toshikazu Fukushima. 2002. Mining product reputations on the web. In Proceedings of the KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaihyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="1102" citStr="Pang et al., 2002" startWordPosition="160" endWordPosition="163"> is to utilize certain layout structures and linguistic pattern. By using them, we can automatically extract such sentences that express opinion. In our experiment, the method could construct a corpus consisting of 126,610 sentences. 1 Introduction Recently, there has been an increasing interest in such applications that deal with opinions (a.k.a. sentiment, reputation etc.). For instance, Morinaga et al. developed a system that extracts and analyzes reputations on the Internet (Morinaga et al., 2002). Pang et al. proposed a method of classifying movie reviews into positive and negative ones (Pang et al., 2002). In these applications, one of the most important issue is how to determine the polarity (or semantic orientation) of a given text. In other words, it is necessary to decide whether a given text conveys positive or negative content. In order to solve this problem, we intend to take statistical approach. More specifically, we plan to learn the polarity of texts from a corpus in which phrases, sentences or documents are tagged with labels expressing the polarity (polarity-tagged corpus). So far, this approach has been taken by a lot of researchers (Pang et al., 2002; Dave et al., 2003; Wilson e</context>
<context position="5243" citStr="Pang et al., 2002" startWordPosition="845" endWordPosition="848">Since we do not use ’neutral’ label, such sentence that does not convey opinion is not stored in our corpus. • The label is assigned to not multiple sentences (or document) but single sentence. Namely, our corpus is tagged at sentence level rather than document level. It is important to discuss the reason that we intend to build a corpus tagged at sentence level rather than document level. The reason is that one document often includes both positive and negative sentences, and hence it is difficult to learn the polarity from the corpus tagged at document level. Consider the following example (Pang et al., 2002): This film should be brilliant. It sounds like a great plot, the actors are first grade, and the supporting cast is good as well, and Stallone is attempting to deliver a good performance. However, it can’t hold up. This document as a whole expresses negative opinion, and should be labeled ’negative’ if it is tagged at document level. However, it includes several sentences that represent positive attitude. We would like to point out that polarity-tagged corpus created from reviews prone to be tagged at document-level. This is because meta-data (e.g. stars in AMAZON.COM) is usually associated w</context>
<context position="21309" citStr="Pang et al., 2002" startWordPosition="3466" endWordPosition="3469">n. But, it is not at all arranged, and makes me confused. In order to precisely treat this kind of sentences, we think discourse analysis is inevitable. 5.3 Application to opinion classification Next, we applied our corpus to opinion sentence classification. This is a task of classifying sentences into positive and negative. We trained a classifier on our corpus and investigated the result. Classifier and data sets As a classifier, we chose Naive Bayes with bag-of-words features, because it is one of the most popular one in this task. Negation was processed in a similar way as previous works (Pang et al., 2002). To validate the accuracy of the classifier, three data sets were created from review pages in which the review is associated with meta-data. To build data sets tagged at sentence level, we used such reviews that contain only one sentence. Table 4 represents the domains and the number of sentences in each data set. Note that we confirmed there is no duplicate between our corpus and the these data sets. The result and discussion Naive Bayes classifier was trained on our corpus and tested on the three data sets (Table 5). In the Table, the second column represents the accuracy of the classifica</context>
</contexts>
<marker>Pang, Lee, Vaihyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaihyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="26385" citStr="Riloff and Wiebe, 2003" startWordPosition="4307" endWordPosition="4310">ynonyms of the seed and to add them to the seed adjectives (Wiebe, 2000). Riloff et al. proposed a bootstrapping approach for learning subjective nouns (Riloff et al., 2003). Their method learns subjective nouns and extraction patterns in turn. First, given seed subjective nouns, the method learns patterns that can extract subjective nouns from corpus. And then, the patterns extract new subjective nouns from corpus, and they are added to the seed nouns. Although this work aims at learning only nouns, in the subsequent work, they also proposed a bootstrapping method that can deal with phrases (Riloff and Wiebe, 2003). Similarly, Wiebe also proposes a bootstrapping approach to create subjective and objective classifier (Wiebe and Riloff, 2005). These works are different from ours in a sense that they did not discuss how to determine the polarity of subjective words or phrases. 6.3 Unsupervised sentiment classification Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003). The concept behind Turney’s model is that positive/negative phrases co-occur with words like ’excellent/poor’. The coo</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
</authors>
<title>Learning subjective nouns using extraction pattern bootstrapping.</title>
<date>2003</date>
<booktitle>In Proceedings of the CoNLL.</booktitle>
<contexts>
<context position="25935" citStr="Riloff et al., 2003" startWordPosition="4232" endWordPosition="4235">on, most of the works rely on external resources such as the WordNet, and cannot treat words that are not in the resources. 458 6.2 Learning subjective phrases Some researchers examined the acquisition of subjective phrases. The subjective phrase is more general concept than opinion and includes both positive and negative expressions. Wiebe learned subjective adjectives from a set of seed adjectives. The idea is to automatically identify the synonyms of the seed and to add them to the seed adjectives (Wiebe, 2000). Riloff et al. proposed a bootstrapping approach for learning subjective nouns (Riloff et al., 2003). Their method learns subjective nouns and extraction patterns in turn. First, given seed subjective nouns, the method learns patterns that can extract subjective nouns from corpus. And then, the patterns extract new subjective nouns from corpus, and they are added to the seed nouns. Although this work aims at learning only nouns, in the subsequent work, they also proposed a bootstrapping method that can deal with phrases (Riloff and Wiebe, 2003). Similarly, Wiebe also proposes a bootstrapping approach to create subjective and objective classifier (Wiebe and Riloff, 2005). These works are diff</context>
</contexts>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. Learning subjective nouns using extraction pattern bootstrapping. In Proceedings of the CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting semantic orientation of words using spin model.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>133--140</pages>
<contexts>
<context position="24858" citStr="Takamura et al., 2005" startWordPosition="4052" endWordPosition="4055"> works that discuss learning the polarity of words instead of sentences. Hatzivassiloglou and McKeown proposed a method of learning the polarity of adjectives from corpus (Hatzivassiloglou and McKeown, 1997). They hypothesized that if two adjectives are connected with conjunctions such as ’and/but’, they have the same/opposite polarity. Based on this hypothesis, their method predicts the polarity of adjectives by using a small set of adjectives labeled with the polarity. Other works rely on linguistic resources such as WordNet (Kamps et al., 2004; Hu and Liu, 2004; Esuli and Sebastiani, 2005; Takamura et al., 2005). For example, Kamps et al. used a graph where nodes correspond to words in the WordNet, and edges connect synonymous words in the WordNet. The polarity of an adjective is defined by its shortest paths from the node corresponding to ’good’ and ’bad’. Although those researches are closely related to our work, there is a striking difference. In those researches, the target is limited to the polarity of words and none of them discussed sentences. In addition, most of the works rely on external resources such as the WordNet, and cannot treat words that are not in the resources. 458 6.2 Learning su</context>
</contexts>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientation of words using spin model. In Proceedings of the ACL, pages 133–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? senmantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="23102" citStr="Turney, 2002" startWordPosition="3763" endWordPosition="3764">corpus with a small domain specific corpus, we estimated accuracy in each data set using 10 fold crossvalidation (Table 6). In two domains, the result of our corpus outperformed that of the crossvalidation. In the other domain, our corpus is slightly better than the crossvalidation. Table 6: Accuracy comparison. Our corpus Crossvalidation Computer 0.831 0.821 Restaurant 0.849 0.848 Car 0.833 0.808 One finding is that our corpus achieved good accuracy, although it includes various domains and is not accustomed to the target domain. Turney also reported good result without domain customization (Turney, 2002). We think these results can be further improved by domain adaptation technique, and it is one future work. Furthermore, we examined the variance of the accuracy between different domains. We trained Naive Bayes on each data set and investigate the accuracy in the other data sets (Table 7). For example, when the classifier is trained on Computer and tested on Restaurant, the accuracy was 0.757. This result revealed that the accuracy is quite poor when the training and test sets are in different domains. On the other hand, when Naive Bayes is trained on our corpus, there are little variance in </context>
<context position="26775" citStr="Turney, 2002" startWordPosition="4366" endWordPosition="4367"> nouns from corpus, and they are added to the seed nouns. Although this work aims at learning only nouns, in the subsequent work, they also proposed a bootstrapping method that can deal with phrases (Riloff and Wiebe, 2003). Similarly, Wiebe also proposes a bootstrapping approach to create subjective and objective classifier (Wiebe and Riloff, 2005). These works are different from ours in a sense that they did not discuss how to determine the polarity of subjective words or phrases. 6.3 Unsupervised sentiment classification Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003). The concept behind Turney’s model is that positive/negative phrases co-occur with words like ’excellent/poor’. The cooccurrence statistic is measured by the result of search engine. Since his method relies on search engine, it is difficult to use rich linguistic information such as dependencies. 7 Conclusion This paper proposed a fully automatic method of building polarity-tagged corpus from HTML documents. In the experiment, we could build a corpus consisting of 126,610 sentences. As a future work, we </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? senmantic orientation applied to unsupervised classification of reviews. In Proceedings of the ACL, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Ellen Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the CICLing.</booktitle>
<contexts>
<context position="26513" citStr="Wiebe and Riloff, 2005" startWordPosition="4324" endWordPosition="4327">arning subjective nouns (Riloff et al., 2003). Their method learns subjective nouns and extraction patterns in turn. First, given seed subjective nouns, the method learns patterns that can extract subjective nouns from corpus. And then, the patterns extract new subjective nouns from corpus, and they are added to the seed nouns. Although this work aims at learning only nouns, in the subsequent work, they also proposed a bootstrapping method that can deal with phrases (Riloff and Wiebe, 2003). Similarly, Wiebe also proposes a bootstrapping approach to create subjective and objective classifier (Wiebe and Riloff, 2005). These works are different from ours in a sense that they did not discuss how to determine the polarity of subjective words or phrases. 6.3 Unsupervised sentiment classification Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003). The concept behind Turney’s model is that positive/negative phrases co-occur with words like ’excellent/poor’. The cooccurrence statistic is measured by the result of search engine. Since his method relies on search engine, it is difficult to use</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>Janyce Wiebe and Ellen Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In Proceedings of the CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the AAAI.</booktitle>
<contexts>
<context position="25834" citStr="Wiebe, 2000" startWordPosition="4218" endWordPosition="4219">he target is limited to the polarity of words and none of them discussed sentences. In addition, most of the works rely on external resources such as the WordNet, and cannot treat words that are not in the resources. 458 6.2 Learning subjective phrases Some researchers examined the acquisition of subjective phrases. The subjective phrase is more general concept than opinion and includes both positive and negative expressions. Wiebe learned subjective adjectives from a set of seed adjectives. The idea is to automatically identify the synonyms of the seed and to add them to the seed adjectives (Wiebe, 2000). Riloff et al. proposed a bootstrapping approach for learning subjective nouns (Riloff et al., 2003). Their method learns subjective nouns and extraction patterns in turn. First, given seed subjective nouns, the method learns patterns that can extract subjective nouns from corpus. And then, the patterns extract new subjective nouns from corpus, and they are added to the seed nouns. Although this work aims at learning only nouns, in the subsequent work, they also proposed a bootstrapping method that can deal with phrases (Riloff and Wiebe, 2003). Similarly, Wiebe also proposes a bootstrapping </context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>Janyce M. Wiebe. 2000. Learning subjective adjectives from corpora. In Proceedings of the AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the HLT/EMNLP.</booktitle>
<contexts>
<context position="1714" citStr="Wilson et al., 2005" startWordPosition="264" endWordPosition="267">., 2002). In these applications, one of the most important issue is how to determine the polarity (or semantic orientation) of a given text. In other words, it is necessary to decide whether a given text conveys positive or negative content. In order to solve this problem, we intend to take statistical approach. More specifically, we plan to learn the polarity of texts from a corpus in which phrases, sentences or documents are tagged with labels expressing the polarity (polarity-tagged corpus). So far, this approach has been taken by a lot of researchers (Pang et al., 2002; Dave et al., 2003; Wilson et al., 2005). In these previous works, polarity-tagged corpus was built in either of the following two ways. It is built manually, or created from review sites such as AMAZON.COM. In some review sites, the review is associated with metadata indicating its polarity. Those reviews can be used as polarity-tagged corpus. In case of AMAZON.COM, the review’s polarity is represented by using 5-star scale. However, both of the two approaches are not appropriate for building large polarity-tagged corpus. Since manual construction of tagged corpus is time-consuming and expensive, it is difficult to build large pola</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Yasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="26865" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="4377" endWordPosition="4380">ork aims at learning only nouns, in the subsequent work, they also proposed a bootstrapping method that can deal with phrases (Riloff and Wiebe, 2003). Similarly, Wiebe also proposes a bootstrapping approach to create subjective and objective classifier (Wiebe and Riloff, 2005). These works are different from ours in a sense that they did not discuss how to determine the polarity of subjective words or phrases. 6.3 Unsupervised sentiment classification Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003). The concept behind Turney’s model is that positive/negative phrases co-occur with words like ’excellent/poor’. The cooccurrence statistic is measured by the result of search engine. Since his method relies on search engine, it is difficult to use rich linguistic information such as dependencies. 7 Conclusion This paper proposed a fully automatic method of building polarity-tagged corpus from HTML documents. In the experiment, we could build a corpus consisting of 126,610 sentences. As a future work, we intend to extract more opinion sentences by applying this method to larger HTML document s</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Yasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of the EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>