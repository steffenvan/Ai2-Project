<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001203">
<title confidence="0.8499015">
Confidence-Weighted Learning of Factored Discriminative Language
Models
</title>
<author confidence="0.754698">
Viet Ha-Thuc
</author>
<affiliation confidence="0.803948">
Computer Science Department
The University of Iowa
</affiliation>
<address confidence="0.670118">
Iowa City, IA 52241, USA
</address>
<email confidence="0.997887">
hviet@cs.uiowa.edu
</email>
<author confidence="0.395632">
Nicola Cancedda
</author>
<affiliation confidence="0.220557">
Xerox Research Centre Europe
</affiliation>
<address confidence="0.484518">
6, chemin de Maupertuis
38240 Meylan, France
</address>
<email confidence="0.994454">
Nicola.Cancedda@xrce.xerox.com
</email>
<sectionHeader confidence="0.997354" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997575">
Language models based on word surface
forms only are unable to benefit from avail-
able linguistic knowledge, and tend to suffer
from poor estimates for rare features. We pro-
pose an approach to overcome these two lim-
itations. We use factored features that can
flexibly capture linguistic regularities, and we
adopt confidence-weighted learning, a form of
discriminative online learning that can better
take advantage of a heavy tail of rare features.
Finally, we extend the confidence-weighted
learning to deal with label noise in training
data, a common case with discriminative lan-
guage modeling.
</bodyText>
<sectionHeader confidence="0.999511" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999736564102564">
Language Models (LMs) are key components in
most statistical machine translation systems, where
they play a crucial role in promoting output fluency.
Standard n-gram generative language models
have been extended in several ways. Generative
factored language models (Bilmes and Kirchhoff,
2003) represent each token by multiple factors –
such as part-of-speech, lemma and surface form–
and capture linguistic patterns in the target language
at the appropriate level of abstraction. Instead of
estimating likelihood, discriminative language mod-
els (Roark et al., 2004; Roark et al., 2007; Li and
Khudanpur, 2008) directly model fluency by casting
the task as a binary classification or a ranking prob-
lem. The method we propose combines advantages
of both directions mentioned above. We use factored
features to capture linguistic patterns and discrim-
inative learning for directly modeling fluency. We
define highly overlapping and correlated factored
features, and extend a robust learning algorithm to
handle them and cope with a high rate of label noise.
For discriminatively learning language models,
we use confidence-weighted learning (Dredze et al.,
2008), an extension of the perceptron-based on-
line learning used in previous work on discrimi-
native language models. Furthermore, we extend
confidence-weighted learning with soft margin to
handle the case where training data labels are noisy,
as is typically the case in discriminative language
modeling.
The rest of this paper is organized as follows. In
Section 2, we introduce factored features for dis-
criminative language models. Section 3 presents
confidence-weighted learning. Section 4 describes
its extension for the case where training data are
noisy. We present empirical results in Section 5
and differentiate our approach from previous ones
in Section 6. Finally, Section 7 presents some con-
cluding remarks.
</bodyText>
<sectionHeader confidence="0.989075" genericHeader="method">
2 Factored features
</sectionHeader>
<bodyText confidence="0.999758538461539">
Factored features are n-gram features where each
component in the n-gram can be characterized by
different linguistic dimensions of words such as sur-
face, lemma, part of speech (POS). Each of these
dimensions is conventionally referred to as a factor.
An example of a factored feature is “pick PRON
up”, where PRON is the part of speech (POS) tag
for pronouns. Appropriately weighted, this feature
can capture the fact that in English that pattern is of-
ten fluent. Compared to traditional surface n-gram
features like “pick her up”, “pick me up” etc., the
feature “pick PRON up” generalizes the pattern bet-
ter. On the other hand, this feature is more precise
</bodyText>
<page confidence="0.997192">
439
</page>
<note confidence="0.84958">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 439–444,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<table confidence="0.9990726">
POS Extended POS
Noun SingNoun, PlurNoun
Pronoun Sing3PPronoun, OtherPronoun
Verb InfVerb, ProgrVerb, SimplePastVerb,
PastPartVerb, Sing3PVerb, OtherVerb
</table>
<tableCaption confidence="0.995077">
Table 1: Extended tagset used for the third factor in the
proposed discriminative language model.
</tableCaption>
<bodyText confidence="0.975343857142857">
than the corresponding POS n-gram feature “VERB
PRON PREP” since the latter also promotes unde-
sirable patterns such as “pick PRON off” and “go
PRON in”. So, constructing features with compo-
nents from different abstraction levels allows better
capturing linguistic patterns.
In this study, we use tri-gram factored features to
learn a discriminative language model for English,
where each token is characterized by three factors
including surface, POS, and extended POS. In the
last factor, some POS tags are further refined (Table
1). In other words, we will use all possible trigrams
where each element is either a surface from, a POS,
or an extended POS.
</bodyText>
<sectionHeader confidence="0.99827" genericHeader="method">
3 Confidence-weighted Learning
</sectionHeader>
<bodyText confidence="0.9918472">
Online learning algorithms scale well to large
datasets, and are thus well adapted to discrimina-
tive language modeling. On the other hand, the
perceptron and Passive Aggressive (PA) algorithms1
(Crammer et al., 2006) can be ill-suited for learn-
ing tasks where there is a long tail of rare significant
features as in the case of language modeling.
Motivated by this, we adopt a simplified version
of the CW algorithm of (Dredze et al., 2008). We in-
troduce a score, based on the number of times a fea-
ture has been obseerved in training, indicating how
confident the algorithm is in the current estimate wi
for the weight of feature i. Instead of equally chang-
ing all feature weights upon a mistake, the algorithm
now changes more aggressively the weights it is less
confident in.
At iteration t, if the algorithm miss-ranks the pair
of positive and negative instances (pt, nt), it updates
the weight vector by solving the optimization in Eq.
(1):
</bodyText>
<subsectionHeader confidence="0.534389">
1The popular MIRA algorithm is a particular PA algorithm,
suitable for the linearly-separable case.
</subsectionHeader>
<bodyText confidence="0.9820915">
2(w − wt)TΛ2t(w − wt)(1)
s.t. wTΔt &gt; 1 (2)
where Δt = 0(pt) − 0(nt), 0(x) is the vector rep-
resentation of sentence x in factored feature space,
and Λt is a diagonal matrix with confidence scores.
The algorithm thus updates weights aggressively
enough to correctly rank the current pair of instances
(i.e. satisfying the constraint), and preserves as
much knowledge learned so far as possible (i.e. min-
imizing the weighted difference to wt). In the spe-
cial case when Λt = I this is the update of the
Passive-Aggressive algorithm of (Crammer et al.,
2006).
By introducing multiple confidence scores with
the diagonal matrix Λ, we take into account the
fact that feature weights that the algorithm has more
confidence in (because it has learned these weights
from more training instances) contribute more to
the knowledge the algorithm has accumulated so far
than feature weights it has less confidence in. A
change in the former is more risky than a change
with the same magnitude on the latter. So, to avoid
over-fitting to the current instance pair (thus gener-
alize better to the others), the difference between w
and wt is weighted by confidence matrix Λ in the
objective function.
To solve the quadratic optimization problem in
Eq. (1), we form the corresponding Lagrangian:
</bodyText>
<equation confidence="0.998331">
L(w,T) = 2(w−wt)TΛ2
1 t (w−wt)+T(1−wTΔ)
</equation>
<bodyText confidence="0.9790028">
(3)
where T is the Lagrange multiplier corresponding to
the constraint in Eq. (2). Setting the partial deriva-
tives of L with respect to w to zero, and then setting
the derivative of L with respect to T to zero, we get:
</bodyText>
<equation confidence="0.999836">
T = JJΛ−1ΔJJ2
1 − wtTΔ (4)
</equation>
<bodyText confidence="0.999968333333333">
Given this, we obtain Algorithm 1 for confidence-
weighted passive-aggressive learning (Figure 1). In
the algorithm, Pi and Ni are sets of fluent and non-
fluent sentences that can be contrasted, e.g. Pi is a
set of fluent translations and Ni is a set of non-fluent
translations of a same source sentence si.
</bodyText>
<equation confidence="0.977314333333333">
wt+1 = arg min
w
1
</equation>
<page confidence="0.966865">
440
</page>
<bodyText confidence="0.856898">
Algorithm 1 Confidence-weighted Passive-
Aggressive algorithm for re-ranking.
gin variant of confidence-weighted learning. The
optimization problem becomes:
</bodyText>
<equation confidence="0.608421333333333">
arg min 2(w − wt)&gt;Λ2
w 1 t (w − wt) + C2 (5)
s.t. w&gt;Δt &gt; 1 − (6)
</equation>
<bodyText confidence="0.87375325">
where C is a regularization parameter, controlling
the relative importance between the two terms in the
objective function. Solving the optimization prob-
lem, we obtain, for the Lagrange multiplier:
</bodyText>
<figure confidence="0.5531815">
Input: Tr = I(Pi, Ni),1 &lt; i &lt; K1
w0 +- 0,t +- 0
for a predefined number of iterations do
for i from 1 to K do
for all (pj, nj) E (Pi x Ni) do
Δt +- O(pj) − O(nj)
if w&gt;t Δt &lt; 1 then
1−wt Δt
</figure>
<equation confidence="0.9789585">
T +- Δt Λ�2
t Δt
wt+1 +- wt + TΛ−2
t Δt
Update Λ
t +- t + 1
return wt
1 − wt&gt;Δt
</equation>
<bodyText confidence="0.999953666666667">
The confidence matrix Λ is updated following the
intuition that the more often the algorithm has seen
a feature, the more confident the weight estimation
becomes. In our work, we set Λii to the logarithm of
the number of times the algorithm has seen feature
i, but alternative choices are possible.
</bodyText>
<sectionHeader confidence="0.991485" genericHeader="method">
4 Extension to soft margin
</sectionHeader>
<bodyText confidence="0.999990043478261">
In many practical situations, training data is noisy.
This is particularly true for language modeling,
where even human experts will argue about whether
a given sentence is fluent or not. Moreover, effective
language models must be trained on large datasets,
so the option of requiring extensive human annota-
tion is impractical. Instead, collecting fluency judg-
ments is often done by a less expensive and thus
even less reliable manner. One way is to rank trans-
lations in n-best lists by NIST or BLEU scores, then
take the top ones as fluent instances and bottom ones
as non-fluent instances. Nonetheless, neither NIST
nor BLEU are designed directly for measuring flu-
ency. For example, a translation could have low
NIST and BLEU scores just because it does not con-
vey the same information as the reference, despite
being perfectly fluent. Therefore, in our setting it is
crucial to be robust to noise in the training labels.
The update rule derived in the previous section al-
ways forces the new weights to satisfy the constraint
(Corrective updates): mislabeled training instances
could make feature weights change erratically. To
increase robustness to noise, we propose a soft mar-
</bodyText>
<equation confidence="0.98857875">
(7)
Δ&gt; t Λ−2
t Δt + 1
2C
</equation>
<bodyText confidence="0.999751666666667">
Thus, the training algorithm with soft-margins is the
same as Algorithm 1, but using Eq. 7 to update T
instead.
</bodyText>
<sectionHeader confidence="0.999601" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999963777777778">
We empirically validated our approach in two ways.
We first measured the effectiveness of the algorithms
in deciding, given a pair of candidate translations
for a same source sentence, whether the first candi-
date is more fluent than the second. In a second ex-
periment we used the score provided by the trained
DLM as an additional feature in an n-best list re-
ranking task and compared algorithms in terms of
impact on NIST and BLEU.
</bodyText>
<subsectionHeader confidence="0.948627">
5.1 Dataset
</subsectionHeader>
<bodyText confidence="0.988796833333333">
The dataset we use in our study is the Spanish-
English one from the shared task of the WMT-2007
workshop2.
Matrax, a phrase-based statistical machine trans-
lation system (Simard et al., 2005), including a tri-
gram generative language model with Kneser-Ney
smoothing. We then obtain training data for the dis-
criminative language model as follows. We take a
random subset of the parallel training set containing
50,000 sentence pairs. We use Matrax to generate
an n-best list for each source sentence. We define
(Pi, Ni), i = 1... 50, 000 as:
</bodyText>
<equation confidence="0.998146">
Pi = Is E nbestiINIST(s) &gt; NIST∗i − 11 (8)
Ni = Is E nbestiINIST(s) &lt; NIST∗i − 31 (9)
2http://www.statmt.org/wmt07/
T =
</equation>
<page confidence="0.994896">
441
</page>
<table confidence="0.939340166666667">
NIST BLEU
Baseline model 6.9683 0.2704
Baseline + DLM0 6.9804 0.2705
Baseline + DLM1 6.9857 0.2709
Baseline + DLM2 7.0288 0.2745
Baseline + DLM3 7.0815 0.2770
</table>
<tableCaption confidence="0.815704">
Table 3: NIST and BLEU scores upon n-best list re-
</tableCaption>
<figureCaption confidence="0.982905185185185">
ranking with the proposed discriminative language mod-
els.
(DLM 0) or factored features by standard percep-
tron (DLM 1), confidence-weighted learning (DLM
2) and confidence-weighted learning with soft mar-
gin (DLM 3). All discriminative language models
strongly reduce the error rate compared to the base-
line (9.1%, 11.4%, 15.1%, 19.4% relative reduc-
tion, respectively). Recall that the training set for
these discriminative language models is a relatively
small subset of the one used to train Matrax’s inte-
grated generative language model. Amongst the four
discriminative learning algorithms, we see that fac-
tored features are slightly better then POS features,
confidence-weighted learning is slightly better than
perceptron, and confidence-weighted learning with
soft margin is the best (9.08% and 5.04% better than
perceptron and confidence-weighted learning with
hard margin).
In the second experiment, we use standard NIST
and BLEU scores for evaluation. Results are in Ta-
ble 3. The relative quality of different methods in
terms of NIST and BLEU correlates well with er-
ror rate. Again, all three discriminative language
models could improve performances over the base-
line. Amongst the three, confidence-weighted learn-
ing with soft margin performs best.
</figureCaption>
<table confidence="0.999616166666667">
Error rate
Baseline model 0.4720
Baseline + DLM0 0.4290
Baseline + DLM1 0.4183
Baseline + DLM2 0.4005
Baseline + DLM3 0.3803
</table>
<tableCaption confidence="0.98919">
Table 2: Error rates for fluency ranking. See article body
for an explanation of the experiments.
</tableCaption>
<bodyText confidence="0.99997676">
where NISTI is the highest sentence-level NIST
score achieved in nbest, The size of n-best lists
was set to 10. Using this dataset, we trained dis-
criminative language models by standard percep-
tron, confidence-weighted learning and confidence-
weighted learning with soft margin.
We then trained the weights of a re-ranker using
eight features (seven from the baseline Matrax plus
one from the DLM) using a simple structured per-
ceptron algorithm on the development set.
For testing, we used the same trained Matrax
model to generate n-best lists of size 1,000 each for
each source sentence. Then, we used the trained dis-
criminative language model to compute a score for
each translation in the n-best list. The score is used
with seven standard Matrax features for re-ranking.
Finally, we measure the quality of the translations
re-ranked to the top.
In order to obtain the required factors for the
target-side tokens, we ran the morphological ana-
lyzer and POS-tagger integrated in the Xerox Incre-
mental Parser (XIP, Ait-Mokhtar et al. (2001)) on
the target side of the training corpus used for creat-
ing the phrase-table, and extended the phrase-table
format so as to record, for each token, all its factors.
</bodyText>
<subsectionHeader confidence="0.699193">
5.2 Results
</subsectionHeader>
<sectionHeader confidence="0.999242" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.99994247368421">
In the first experiment, we measure the quality of
the re-ranked n-best lists by classification error rate.
The error rate is computed as the fraction of pairs
from a test-set which is ranked correctly according
to its fluency score (approximated here by the NIST
score). Results are in Table 2.
For the baseline, we use the seven default Ma-
trax features, including a generative language model
score. DLM* are discriminative language mod-
els trained using, respectively, POS features only
This work is related to several existing directions:
generative factored language model, discriminative
language models, online passive-aggressive learning
and confidence-weighted learning.
Generative factored language models are pro-
posed by (Bilmes and Kirchhoff, 2003). In this
work, factors are used to define alternative back-
off paths in case surface-form n-grams are not ob-
served a sufficient number of times in the train-
</bodyText>
<page confidence="0.99675">
442
</page>
<bodyText confidence="0.999980840909091">
ing corpus. Unlike ours, this model cannot con-
sider simultaneously multiple factored features com-
ing from the same token n-gram, thus integrating all
possible available information sources.
Discriminative language models have also been
studied in speech recognition and statistical machine
translation (Roark et al., 2007; Li and Khudanpur,
2008). An attempt to combine factored features and
discriminative language modeling is presented in
(Mah´e and Cancedda, 2009). Unlike us, they com-
bine together instances from multiple n-best lists,
generally not comparable, in forming positive and
negative instances. Also, they use an SVM to train
the DLM, as opposed to the proposed online algo-
rithms.
Our approach stems from Passive-Aggressive al-
gorithms proposed by (Crammer et al., 2006) and
the CW online algorithm proposed by (Dredze et
al., 2008). In the former, Crammer et al. propose
an online learning algorithm with soft margins to
handle noise in training data. However, the work
does not consider the confidence associated with es-
timated feature weights. On the other hand, the CW
online algorithm in the later does not consider the
case where the training data is noisy.
While developed independently, our soft-margin
extension is closely related to the AROW(project)
algorithm of (Crammer et al., 2009; Crammer and
Lee, 2010). The cited work models classifiers as
non-correlated Gaussian distributions over weights,
while our approach uses point estimates for weights
coupled with confidence scores. Despite the differ-
ent conceptual modeling, though, in practice the al-
gorithms are similar, with point estimates playing
the same role as the mean vector, and our (squared)
confidence score matrix the same role as the preci-
sion (inverse covariance) matrix. Unlike in the cited
work, however, in our proposal, confidence scores
are updated also upon correct classification of train-
ing examples, and not only on mistakes. The ra-
tionale of this is that correctly classifying an exam-
ple could also increase the confidence on the current
model. Thus, the update formulas are also different
compared to the work cited above.
</bodyText>
<sectionHeader confidence="0.998729" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99993228">
We proposed a novel approach to discriminative lan-
guage models. First, we introduced the idea of us-
ing factored features in the discriminative language
modeling framework. Factored features allow the
language model to capture linguistic patterns at mul-
tiple levels of abstraction. Moreover, the discrimi-
native framework is appropriate for handling highly
overlapping features, which is the case of factored
features. While we did not experiment with this, a
natural extension consists in using all n-grams up
to a certain order, thus providing back-off features
and enabling the use of higher-order n-grams. Sec-
ond, for learning factored language models discrim-
inatively, we adopt a simple confidence-weighted
algorithm, limiting the problem of poor estimation
of weights for rare features. Finally, we extended
confidence-weighted learning with soft margins to
handle the case where labels of training data are
noisy. This is typically the case in discriminative
language modeling, where labels are obtained only
indirectly.
Our experiments show that combining all these el-
ements is important and achieves significant transla-
tion quality improvements already with a weak form
of integration: n-best list re-ranking.
</bodyText>
<sectionHeader confidence="0.999221" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999097380952381">
Salah Ait-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2001. A multi-input dependency parser. In
Proceedings of the Seventh International Workshop on
Parsing Technologies, Beijing, Cina.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Fac-
tored language models and generalized parallel back-
off. In Proceedings of HLT/NAACL, Edmonton, Al-
berta, Canada.
Koby Crammer and Daniel D. Lee. 2010. Learning via
gaussian herding. In Pre-proceeding of NIPS 2010.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal Of Machine Learning
Research, 7.
Koby Crammer, Alex Kulesza, and Mark Dredze. 2009.
Adaptive regularization of weight vectors. In Ad-
vances in Neural Processing Information Systems
(NIPS 2009).
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classifiers. In Pro-
ceedings of ICML, Helsinki, Finland.
</reference>
<page confidence="0.990764">
443
</page>
<reference confidence="0.998667833333333">
Zhifei Li and Sanjeev Khudanpur. 2008. Large-scale
discriminative n-gram language models for statistical
machine translation. In Proceedings of AMTA.
Pierre Mah´e and Nicola Cancedda. 2009. Linguisti-
cally enriched word-sequence kernels for discrimina-
tive language modeling. In Learning Machine Trans-
lation, NIPS Workshop Series. MIT Press, Cambridge,
Mass.
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language modeling
with conditional random fields and the perceptron al-
gorithm. In Proceedings of the annual meeting of
the Association for Computational Linguistics (ACL),
Barcelona, Spain.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech and Language, 21(2).
M. Simard, N. Cancedda, B. Cavestro, M. Dymetman,
E. Gaussier, C. Goutte, and K. Yamada. 2005. Trans-
lating with non-contiguous phrases. In Association
for Computational Linguistics, editor, Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language,
pages 755–762, October.
</reference>
<page confidence="0.998954">
444
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.415817">
<title confidence="0.9938895">Confidence-Weighted Learning of Factored Discriminative Language Models</title>
<author confidence="0.893945">Viet</author>
<affiliation confidence="0.998984">Computer Science The University of</affiliation>
<address confidence="0.53204">Iowa City, IA 52241,</address>
<email confidence="0.998924">hviet@cs.uiowa.edu</email>
<author confidence="0.978809">Nicola</author>
<affiliation confidence="0.995083">Xerox Research Centre</affiliation>
<address confidence="0.971843">6, chemin de 38240 Meylan,</address>
<email confidence="0.998161">Nicola.Cancedda@xrce.xerox.com</email>
<abstract confidence="0.9966598">Language models based on word surface forms only are unable to benefit from available linguistic knowledge, and tend to suffer from poor estimates for rare features. We propose an approach to overcome these two limitations. We use factored features that can flexibly capture linguistic regularities, and we adopt confidence-weighted learning, a form of discriminative online learning that can better take advantage of a heavy tail of rare features. Finally, we extend the confidence-weighted learning to deal with label noise in training data, a common case with discriminative language modeling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Salah Ait-Mokhtar</author>
<author>Jean-Pierre Chanod</author>
<author>Claude Roux</author>
</authors>
<title>A multi-input dependency parser.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh International Workshop on Parsing Technologies,</booktitle>
<location>Beijing, Cina.</location>
<contexts>
<context position="13737" citStr="Ait-Mokhtar et al. (2001)" startWordPosition="2237" endWordPosition="2240">uctured perceptron algorithm on the development set. For testing, we used the same trained Matrax model to generate n-best lists of size 1,000 each for each source sentence. Then, we used the trained discriminative language model to compute a score for each translation in the n-best list. The score is used with seven standard Matrax features for re-ranking. Finally, we measure the quality of the translations re-ranked to the top. In order to obtain the required factors for the target-side tokens, we ran the morphological analyzer and POS-tagger integrated in the Xerox Incremental Parser (XIP, Ait-Mokhtar et al. (2001)) on the target side of the training corpus used for creating the phrase-table, and extended the phrase-table format so as to record, for each token, all its factors. 5.2 Results 6 Related Work In the first experiment, we measure the quality of the re-ranked n-best lists by classification error rate. The error rate is computed as the fraction of pairs from a test-set which is ranked correctly according to its fluency score (approximated here by the NIST score). Results are in Table 2. For the baseline, we use the seven default Matrax features, including a generative language model score. DLM* </context>
</contexts>
<marker>Ait-Mokhtar, Chanod, Roux, 2001</marker>
<rawString>Salah Ait-Mokhtar, Jean-Pierre Chanod, and Claude Roux. 2001. A multi-input dependency parser. In Proceedings of the Seventh International Workshop on Parsing Technologies, Beijing, Cina.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff A Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<location>Edmonton, Alberta, Canada.</location>
<contexts>
<context position="1215" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="169" endWordPosition="172">guistic regularities, and we adopt confidence-weighted learning, a form of discriminative online learning that can better take advantage of a heavy tail of rare features. Finally, we extend the confidence-weighted learning to deal with label noise in training data, a common case with discriminative language modeling. 1 Introduction Language Models (LMs) are key components in most statistical machine translation systems, where they play a crucial role in promoting output fluency. Standard n-gram generative language models have been extended in several ways. Generative factored language models (Bilmes and Kirchhoff, 2003) represent each token by multiple factors – such as part-of-speech, lemma and surface form– and capture linguistic patterns in the target language at the appropriate level of abstraction. Instead of estimating likelihood, discriminative language models (Roark et al., 2004; Roark et al., 2007; Li and Khudanpur, 2008) directly model fluency by casting the task as a binary classification or a ranking problem. The method we propose combines advantages of both directions mentioned above. We use factored features to capture linguistic patterns and discriminative learning for directly modeling fluenc</context>
<context position="14689" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="2383" endWordPosition="2386">ted as the fraction of pairs from a test-set which is ranked correctly according to its fluency score (approximated here by the NIST score). Results are in Table 2. For the baseline, we use the seven default Matrax features, including a generative language model score. DLM* are discriminative language models trained using, respectively, POS features only This work is related to several existing directions: generative factored language model, discriminative language models, online passive-aggressive learning and confidence-weighted learning. Generative factored language models are proposed by (Bilmes and Kirchhoff, 2003). In this work, factors are used to define alternative backoff paths in case surface-form n-grams are not observed a sufficient number of times in the train442 ing corpus. Unlike ours, this model cannot consider simultaneously multiple factored features coming from the same token n-gram, thus integrating all possible available information sources. Discriminative language models have also been studied in speech recognition and statistical machine translation (Roark et al., 2007; Li and Khudanpur, 2008). An attempt to combine factored features and discriminative language modeling is presented in</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Proceedings of HLT/NAACL, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Daniel D Lee</author>
</authors>
<title>Learning via gaussian herding.</title>
<date>2010</date>
<booktitle>In Pre-proceeding of NIPS</booktitle>
<contexts>
<context position="16182" citStr="Crammer and Lee, 2010" startWordPosition="2617" endWordPosition="2620">ems from Passive-Aggressive algorithms proposed by (Crammer et al., 2006) and the CW online algorithm proposed by (Dredze et al., 2008). In the former, Crammer et al. propose an online learning algorithm with soft margins to handle noise in training data. However, the work does not consider the confidence associated with estimated feature weights. On the other hand, the CW online algorithm in the later does not consider the case where the training data is noisy. While developed independently, our soft-margin extension is closely related to the AROW(project) algorithm of (Crammer et al., 2009; Crammer and Lee, 2010). The cited work models classifiers as non-correlated Gaussian distributions over weights, while our approach uses point estimates for weights coupled with confidence scores. Despite the different conceptual modeling, though, in practice the algorithms are similar, with point estimates playing the same role as the mean vector, and our (squared) confidence score matrix the same role as the precision (inverse covariance) matrix. Unlike in the cited work, however, in our proposal, confidence scores are updated also upon correct classification of training examples, and not only on mistakes. The ra</context>
</contexts>
<marker>Crammer, Lee, 2010</marker>
<rawString>Koby Crammer and Daniel D. Lee. 2010. Learning via gaussian herding. In Pre-proceeding of NIPS 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal Of Machine Learning Research,</journal>
<volume>7</volume>
<contexts>
<context position="4836" citStr="Crammer et al., 2006" startWordPosition="719" endWordPosition="722">tudy, we use tri-gram factored features to learn a discriminative language model for English, where each token is characterized by three factors including surface, POS, and extended POS. In the last factor, some POS tags are further refined (Table 1). In other words, we will use all possible trigrams where each element is either a surface from, a POS, or an extended POS. 3 Confidence-weighted Learning Online learning algorithms scale well to large datasets, and are thus well adapted to discriminative language modeling. On the other hand, the perceptron and Passive Aggressive (PA) algorithms1 (Crammer et al., 2006) can be ill-suited for learning tasks where there is a long tail of rare significant features as in the case of language modeling. Motivated by this, we adopt a simplified version of the CW algorithm of (Dredze et al., 2008). We introduce a score, based on the number of times a feature has been obseerved in training, indicating how confident the algorithm is in the current estimate wi for the weight of feature i. Instead of equally changing all feature weights upon a mistake, the algorithm now changes more aggressively the weights it is less confident in. At iteration t, if the algorithm miss-</context>
<context position="6218" citStr="Crammer et al., 2006" startWordPosition="959" endWordPosition="962"> particular PA algorithm, suitable for the linearly-separable case. 2(w − wt)TΛ2t(w − wt)(1) s.t. wTΔt &gt; 1 (2) where Δt = 0(pt) − 0(nt), 0(x) is the vector representation of sentence x in factored feature space, and Λt is a diagonal matrix with confidence scores. The algorithm thus updates weights aggressively enough to correctly rank the current pair of instances (i.e. satisfying the constraint), and preserves as much knowledge learned so far as possible (i.e. minimizing the weighted difference to wt). In the special case when Λt = I this is the update of the Passive-Aggressive algorithm of (Crammer et al., 2006). By introducing multiple confidence scores with the diagonal matrix Λ, we take into account the fact that feature weights that the algorithm has more confidence in (because it has learned these weights from more training instances) contribute more to the knowledge the algorithm has accumulated so far than feature weights it has less confidence in. A change in the former is more risky than a change with the same magnitude on the latter. So, to avoid over-fitting to the current instance pair (thus generalize better to the others), the difference between w and wt is weighted by confidence matrix</context>
<context position="15633" citStr="Crammer et al., 2006" startWordPosition="2528" endWordPosition="2531">lable information sources. Discriminative language models have also been studied in speech recognition and statistical machine translation (Roark et al., 2007; Li and Khudanpur, 2008). An attempt to combine factored features and discriminative language modeling is presented in (Mah´e and Cancedda, 2009). Unlike us, they combine together instances from multiple n-best lists, generally not comparable, in forming positive and negative instances. Also, they use an SVM to train the DLM, as opposed to the proposed online algorithms. Our approach stems from Passive-Aggressive algorithms proposed by (Crammer et al., 2006) and the CW online algorithm proposed by (Dredze et al., 2008). In the former, Crammer et al. propose an online learning algorithm with soft margins to handle noise in training data. However, the work does not consider the confidence associated with estimated feature weights. On the other hand, the CW online algorithm in the later does not consider the case where the training data is noisy. While developed independently, our soft-margin extension is closely related to the AROW(project) algorithm of (Crammer et al., 2009; Crammer and Lee, 2010). The cited work models classifiers as non-correlat</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal Of Machine Learning Research, 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Mark Dredze</author>
</authors>
<title>Adaptive regularization of weight vectors.</title>
<date>2009</date>
<booktitle>In Advances in Neural Processing Information Systems (NIPS</booktitle>
<contexts>
<context position="16158" citStr="Crammer et al., 2009" startWordPosition="2613" endWordPosition="2616">ithms. Our approach stems from Passive-Aggressive algorithms proposed by (Crammer et al., 2006) and the CW online algorithm proposed by (Dredze et al., 2008). In the former, Crammer et al. propose an online learning algorithm with soft margins to handle noise in training data. However, the work does not consider the confidence associated with estimated feature weights. On the other hand, the CW online algorithm in the later does not consider the case where the training data is noisy. While developed independently, our soft-margin extension is closely related to the AROW(project) algorithm of (Crammer et al., 2009; Crammer and Lee, 2010). The cited work models classifiers as non-correlated Gaussian distributions over weights, while our approach uses point estimates for weights coupled with confidence scores. Despite the different conceptual modeling, though, in practice the algorithms are similar, with point estimates playing the same role as the mean vector, and our (squared) confidence score matrix the same role as the precision (inverse covariance) matrix. Unlike in the cited work, however, in our proposal, confidence scores are updated also upon correct classification of training examples, and not </context>
</contexts>
<marker>Crammer, Kulesza, Dredze, 2009</marker>
<rawString>Koby Crammer, Alex Kulesza, and Mark Dredze. 2009. Adaptive regularization of weight vectors. In Advances in Neural Processing Information Systems (NIPS 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Confidence-weighted linear classifiers.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML,</booktitle>
<location>Helsinki, Finland.</location>
<contexts>
<context position="2081" citStr="Dredze et al., 2008" startWordPosition="299" endWordPosition="302">models (Roark et al., 2004; Roark et al., 2007; Li and Khudanpur, 2008) directly model fluency by casting the task as a binary classification or a ranking problem. The method we propose combines advantages of both directions mentioned above. We use factored features to capture linguistic patterns and discriminative learning for directly modeling fluency. We define highly overlapping and correlated factored features, and extend a robust learning algorithm to handle them and cope with a high rate of label noise. For discriminatively learning language models, we use confidence-weighted learning (Dredze et al., 2008), an extension of the perceptron-based online learning used in previous work on discriminative language models. Furthermore, we extend confidence-weighted learning with soft margin to handle the case where training data labels are noisy, as is typically the case in discriminative language modeling. The rest of this paper is organized as follows. In Section 2, we introduce factored features for discriminative language models. Section 3 presents confidence-weighted learning. Section 4 describes its extension for the case where training data are noisy. We present empirical results in Section 5 an</context>
<context position="5060" citStr="Dredze et al., 2008" startWordPosition="760" endWordPosition="763">further refined (Table 1). In other words, we will use all possible trigrams where each element is either a surface from, a POS, or an extended POS. 3 Confidence-weighted Learning Online learning algorithms scale well to large datasets, and are thus well adapted to discriminative language modeling. On the other hand, the perceptron and Passive Aggressive (PA) algorithms1 (Crammer et al., 2006) can be ill-suited for learning tasks where there is a long tail of rare significant features as in the case of language modeling. Motivated by this, we adopt a simplified version of the CW algorithm of (Dredze et al., 2008). We introduce a score, based on the number of times a feature has been obseerved in training, indicating how confident the algorithm is in the current estimate wi for the weight of feature i. Instead of equally changing all feature weights upon a mistake, the algorithm now changes more aggressively the weights it is less confident in. At iteration t, if the algorithm miss-ranks the pair of positive and negative instances (pt, nt), it updates the weight vector by solving the optimization in Eq. (1): 1The popular MIRA algorithm is a particular PA algorithm, suitable for the linearly-separable c</context>
<context position="15695" citStr="Dredze et al., 2008" startWordPosition="2539" endWordPosition="2542">also been studied in speech recognition and statistical machine translation (Roark et al., 2007; Li and Khudanpur, 2008). An attempt to combine factored features and discriminative language modeling is presented in (Mah´e and Cancedda, 2009). Unlike us, they combine together instances from multiple n-best lists, generally not comparable, in forming positive and negative instances. Also, they use an SVM to train the DLM, as opposed to the proposed online algorithms. Our approach stems from Passive-Aggressive algorithms proposed by (Crammer et al., 2006) and the CW online algorithm proposed by (Dredze et al., 2008). In the former, Crammer et al. propose an online learning algorithm with soft margins to handle noise in training data. However, the work does not consider the confidence associated with estimated feature weights. On the other hand, the CW online algorithm in the later does not consider the case where the training data is noisy. While developed independently, our soft-margin extension is closely related to the AROW(project) algorithm of (Crammer et al., 2009; Crammer and Lee, 2010). The cited work models classifiers as non-correlated Gaussian distributions over weights, while our approach use</context>
</contexts>
<marker>Dredze, Crammer, Pereira, 2008</marker>
<rawString>Mark Dredze, Koby Crammer, and Fernando Pereira. 2008. Confidence-weighted linear classifiers. In Proceedings of ICML, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Large-scale discriminative n-gram language models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="1532" citStr="Li and Khudanpur, 2008" startWordPosition="217" endWordPosition="220">Introduction Language Models (LMs) are key components in most statistical machine translation systems, where they play a crucial role in promoting output fluency. Standard n-gram generative language models have been extended in several ways. Generative factored language models (Bilmes and Kirchhoff, 2003) represent each token by multiple factors – such as part-of-speech, lemma and surface form– and capture linguistic patterns in the target language at the appropriate level of abstraction. Instead of estimating likelihood, discriminative language models (Roark et al., 2004; Roark et al., 2007; Li and Khudanpur, 2008) directly model fluency by casting the task as a binary classification or a ranking problem. The method we propose combines advantages of both directions mentioned above. We use factored features to capture linguistic patterns and discriminative learning for directly modeling fluency. We define highly overlapping and correlated factored features, and extend a robust learning algorithm to handle them and cope with a high rate of label noise. For discriminatively learning language models, we use confidence-weighted learning (Dredze et al., 2008), an extension of the perceptron-based online learn</context>
<context position="15195" citStr="Li and Khudanpur, 2008" startWordPosition="2461" endWordPosition="2464">ng and confidence-weighted learning. Generative factored language models are proposed by (Bilmes and Kirchhoff, 2003). In this work, factors are used to define alternative backoff paths in case surface-form n-grams are not observed a sufficient number of times in the train442 ing corpus. Unlike ours, this model cannot consider simultaneously multiple factored features coming from the same token n-gram, thus integrating all possible available information sources. Discriminative language models have also been studied in speech recognition and statistical machine translation (Roark et al., 2007; Li and Khudanpur, 2008). An attempt to combine factored features and discriminative language modeling is presented in (Mah´e and Cancedda, 2009). Unlike us, they combine together instances from multiple n-best lists, generally not comparable, in forming positive and negative instances. Also, they use an SVM to train the DLM, as opposed to the proposed online algorithms. Our approach stems from Passive-Aggressive algorithms proposed by (Crammer et al., 2006) and the CW online algorithm proposed by (Dredze et al., 2008). In the former, Crammer et al. propose an online learning algorithm with soft margins to handle noi</context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. Large-scale discriminative n-gram language models for statistical machine translation. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Mah´e</author>
<author>Nicola Cancedda</author>
</authors>
<title>Linguistically enriched word-sequence kernels for discriminative language modeling.</title>
<date>2009</date>
<booktitle>In Learning Machine Translation, NIPS Workshop Series.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>Mah´e, Cancedda, 2009</marker>
<rawString>Pierre Mah´e and Nicola Cancedda. 2009. Linguistically enriched word-sequence kernels for discriminative language modeling. In Learning Machine Translation, NIPS Workshop Series. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
<author>Mark Johnson</author>
</authors>
<title>Discriminative language modeling with conditional random fields and the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the annual meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1487" citStr="Roark et al., 2004" startWordPosition="209" endWordPosition="212">ith discriminative language modeling. 1 Introduction Language Models (LMs) are key components in most statistical machine translation systems, where they play a crucial role in promoting output fluency. Standard n-gram generative language models have been extended in several ways. Generative factored language models (Bilmes and Kirchhoff, 2003) represent each token by multiple factors – such as part-of-speech, lemma and surface form– and capture linguistic patterns in the target language at the appropriate level of abstraction. Instead of estimating likelihood, discriminative language models (Roark et al., 2004; Roark et al., 2007; Li and Khudanpur, 2008) directly model fluency by casting the task as a binary classification or a ranking problem. The method we propose combines advantages of both directions mentioned above. We use factored features to capture linguistic patterns and discriminative learning for directly modeling fluency. We define highly overlapping and correlated factored features, and extend a robust learning algorithm to handle them and cope with a high rate of label noise. For discriminatively learning language models, we use confidence-weighted learning (Dredze et al., 2008), an e</context>
</contexts>
<marker>Roark, Saraclar, Collins, Johnson, 2004</marker>
<rawString>Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson. 2004. Discriminative language modeling with conditional random fields and the perceptron algorithm. In Proceedings of the annual meeting of the Association for Computational Linguistics (ACL), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
</authors>
<title>Discriminative n-gram language modeling.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1507" citStr="Roark et al., 2007" startWordPosition="213" endWordPosition="216">anguage modeling. 1 Introduction Language Models (LMs) are key components in most statistical machine translation systems, where they play a crucial role in promoting output fluency. Standard n-gram generative language models have been extended in several ways. Generative factored language models (Bilmes and Kirchhoff, 2003) represent each token by multiple factors – such as part-of-speech, lemma and surface form– and capture linguistic patterns in the target language at the appropriate level of abstraction. Instead of estimating likelihood, discriminative language models (Roark et al., 2004; Roark et al., 2007; Li and Khudanpur, 2008) directly model fluency by casting the task as a binary classification or a ranking problem. The method we propose combines advantages of both directions mentioned above. We use factored features to capture linguistic patterns and discriminative learning for directly modeling fluency. We define highly overlapping and correlated factored features, and extend a robust learning algorithm to handle them and cope with a high rate of label noise. For discriminatively learning language models, we use confidence-weighted learning (Dredze et al., 2008), an extension of the perc</context>
<context position="15170" citStr="Roark et al., 2007" startWordPosition="2457" endWordPosition="2460">ve-aggressive learning and confidence-weighted learning. Generative factored language models are proposed by (Bilmes and Kirchhoff, 2003). In this work, factors are used to define alternative backoff paths in case surface-form n-grams are not observed a sufficient number of times in the train442 ing corpus. Unlike ours, this model cannot consider simultaneously multiple factored features coming from the same token n-gram, thus integrating all possible available information sources. Discriminative language models have also been studied in speech recognition and statistical machine translation (Roark et al., 2007; Li and Khudanpur, 2008). An attempt to combine factored features and discriminative language modeling is presented in (Mah´e and Cancedda, 2009). Unlike us, they combine together instances from multiple n-best lists, generally not comparable, in forming positive and negative instances. Also, they use an SVM to train the DLM, as opposed to the proposed online algorithms. Our approach stems from Passive-Aggressive algorithms proposed by (Crammer et al., 2006) and the CW online algorithm proposed by (Dredze et al., 2008). In the former, Crammer et al. propose an online learning algorithm with s</context>
</contexts>
<marker>Roark, Saraclar, Collins, 2007</marker>
<rawString>Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. Computer Speech and Language, 21(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Simard</author>
<author>N Cancedda</author>
<author>B Cavestro</author>
<author>M Dymetman</author>
<author>E Gaussier</author>
<author>C Goutte</author>
<author>K Yamada</author>
</authors>
<title>Translating with non-contiguous phrases.</title>
<date>2005</date>
<booktitle>In Association for Computational Linguistics, editor, Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language,</booktitle>
<pages>755--762</pages>
<contexts>
<context position="10536" citStr="Simard et al., 2005" startWordPosition="1724" endWordPosition="1727"> validated our approach in two ways. We first measured the effectiveness of the algorithms in deciding, given a pair of candidate translations for a same source sentence, whether the first candidate is more fluent than the second. In a second experiment we used the score provided by the trained DLM as an additional feature in an n-best list reranking task and compared algorithms in terms of impact on NIST and BLEU. 5.1 Dataset The dataset we use in our study is the SpanishEnglish one from the shared task of the WMT-2007 workshop2. Matrax, a phrase-based statistical machine translation system (Simard et al., 2005), including a trigram generative language model with Kneser-Ney smoothing. We then obtain training data for the discriminative language model as follows. We take a random subset of the parallel training set containing 50,000 sentence pairs. We use Matrax to generate an n-best list for each source sentence. We define (Pi, Ni), i = 1... 50, 000 as: Pi = Is E nbestiINIST(s) &gt; NIST∗i − 11 (8) Ni = Is E nbestiINIST(s) &lt; NIST∗i − 31 (9) 2http://www.statmt.org/wmt07/ T = 441 NIST BLEU Baseline model 6.9683 0.2704 Baseline + DLM0 6.9804 0.2705 Baseline + DLM1 6.9857 0.2709 Baseline + DLM2 7.0288 0.274</context>
</contexts>
<marker>Simard, Cancedda, Cavestro, Dymetman, Gaussier, Goutte, Yamada, 2005</marker>
<rawString>M. Simard, N. Cancedda, B. Cavestro, M. Dymetman, E. Gaussier, C. Goutte, and K. Yamada. 2005. Translating with non-contiguous phrases. In Association for Computational Linguistics, editor, Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language, pages 755–762, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>