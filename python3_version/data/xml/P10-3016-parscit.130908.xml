<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.070849">
<title confidence="0.997202">
Adapting Self-training for Semantic Role Labeling
</title>
<author confidence="0.993393">
Rasoul Samad Zadeh Kaljahi
</author>
<affiliation confidence="0.998848">
FCSIT, University of Malaya
</affiliation>
<address confidence="0.573036">
50406, Kuala Lumpur, Malaysia.
</address>
<email confidence="0.991487">
rsk7945@perdana.um.edu.my
</email>
<sectionHeader confidence="0.993713" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999207444444445">
Supervised semantic role labeling (SRL) sys-
tems trained on hand-crafted annotated corpo-
ra have recently achieved state-of-the-art per-
formance. However, creating such corpora is
tedious and costly, with the resulting corpora
not sufficiently representative of the language.
This paper describes a part of an ongoing work
on applying bootstrapping methods to SRL to
deal with this problem. Previous work shows
that, due to the complexity of SRL, this task is
not straight forward. One major difficulty is
the propagation of classification noise into the
successive iterations. We address this problem
by employing balancing and preselection me-
thods for self-training, as a bootstrapping algo-
rithm. The proposed methods could achieve
improvement over the base line, which do not
use these methods.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963807017544">
Semantic role labeling has been an active re-
search field of computational linguistics since its
introduction by Gildea and Jurafsky (2002). It
reveals the event structure encoded in the sen-
tence, which is useful for other NLP tasks or ap-
plications such as information extraction, ques-
tion answering, and machine translation (Surdea-
nu et al., 2003). Several CoNLL shared tasks
(Carreras and Marquez, 2005; Surdeanu et al.,
2008) dedicated to semantic role labeling affirm
the increasing attention to this field.
One important supportive factor of studying
supervised statistical SRL has been the existence
of hand-annotated semantic corpora for training
SRL systems. FrameNet (Baker et al., 1998) was
the first such resource, which made the emer-
gence of this research field possible by the se-
minal work of Gildea and Jurafsky (2002). How-
ever, this corpus only exemplifies the semantic
role assignment by selecting some illustrative
examples for annotation. This questions its suita-
bility for statistical learning. Propbank was
started by Kingsbury and Palmer (2002) aiming
at developing a more representative resource of
English, appropriate for statistical SRL study.
Propbank has been used as the learning
framework by the majority of SRL work and
competitions like CoNLL shared tasks. However,
it only covers the newswire text from a specific
genre and also deals only with verb predicates.
All state-of-the-art SRL systems show a dra-
matic drop in performance when tested on a new
text domain (Punyakanok et al., 2008). This
evince the infeasibility of building a comprehen-
sive hand-crafted corpus of natural language use-
ful for training a robust semantic role labeler.
A possible relief for this problem is the utility
of semi-supervised learning methods along with
the existence of huge amount of natural language
text available at a low cost. Semi-supervised me-
thods compensate the scarcity of labeled data by
utilizing an additional and much larger amount
of unlabeled data via a variety of algorithms.
Self-training (Yarowsky, 1995) is a semi-
supervised algorithm which has been well stu-
died in the NLP area and gained promising re-
sult. It iteratively extend its training set by labe-
ling the unlabeled data using a base classifier
trained on the labeled data. Although the algo-
rithm is theoretically straightforward, it involves
a large number of parameters, highly influenced
by the specifications of the underlying task. Thus
to achieve the best-performing parameter set or
even to investigate the usefulness of these algo-
rithms for a learning task such as SRL, a tho-
rough experiment is required. This work investi-
gates its application to the SRL problem.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9990685">
The algorithm proposed by Yarowsky (1995) for
the problem of word sense disambiguation has
been cited as the origination of self-training. In
that work, he bootstrapped a ruleset from a
</bodyText>
<page confidence="0.991153">
91
</page>
<note confidence="0.8003955">
Proceedings of the ACL 2010 Student Research Workshop, pages 91–96,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.999106194444444">
Feature Name Description
Phrase Type Phrase type of the constitu-
ent
Position+Predicate Concatenation of constitu-
Voice ent position relative to verb
and verb voice
Predicate Lemma Lemma of the predicate
Predicate POS POS tag of the predicate
Path Tree path of non-terminals
from predicate to constitu-
ent
Head Word Lemma of the head word
Lemma of the constituent
Content Word Lemma of the content
Lemma word of the constituent
Head Word POS POS tag of the head word
of the constituent
Content Word POS POS tag of the head word
of the constituent
Governing Category The first VP or S ancestor
of a NP constituent
Predicate Rule expanding the predi-
Subcategorization cate&apos;s parent
Constituent Rule expanding the consti-
Subcategorization * tuent&apos;s parent
Clause+VP+NP Number of clauses, NPs
Count in Path and VPs in the path
Constituent and Number of words between
Predicate Distance constituent and predicate
Compound Verb Verb predicate structure
Identifier type: simple, compound, or
discontinuous compound
Head Word Loca- Location of head word in-
tion in Constituent * side the constituent based
on the number of words in
its right and left
</table>
<tableCaption confidence="0.998603">
Table 1: Features
</tableCaption>
<bodyText confidence="0.999782117647059">
small number of seed words extracted from
an online dictionary using a corpus of unan-
notated English text and gained a compara-
ble accuracy to fully supervised approaches.
Subsequently, several studies applied the algo-
rithm to other domains of NLP. Reference reso-
lution (Ng and Cardie 2003), POS tagging (Clark
et al., 2003), and parsing (McClosky et al., 2006)
were shown to be benefited from self-training.
These studies show that the performance of self-
training is tied with its several parameters and
the specifications of the underlying task.
In SRL field, He and Gildea (2006) used self-
training to address the problem of unseen frames
when using FrameNet as the underlying training
corpus. They generalized FrameNet frame ele-
ments to 15 thematic roles to control the com-
plexity of the process. The improvement gained
by the progress of self-training was small and
inconsistent. They reported that the NULL label
(non-argument) had often dominated other labels
in the examples added to the training set.
Lee et al. (2007) attacked another SRL learn-
ing problem using self-training. Using Propbank
instead of FrameNet, they aimed at increasing
the performance of supervised SRL system by
exploiting a large amount of unlabeled data
(about 7 times more than labeled data). The algo-
rithm variation was similar to that of He and Gil-
dea (2006), but it only dealt with core arguments
of the Propbank. They achieved a minor im-
provement too and credited it to the relatively
poor performance of their base classifier and the
insufficiency of the unlabeled data.
</bodyText>
<sectionHeader confidence="0.99238" genericHeader="method">
3 SRL System
</sectionHeader>
<bodyText confidence="0.99855964516129">
To have enough control over entire the system
and thus a flexible experimental framework, we
developed our own SRL system instead of using
a third-party system. The system works with
PropBank-style annotation and is described here.
Syntactic Formalism: A Penn Treebank con-
stituent-based approach for SRL is taken. Syn-
tactic parse trees are produced by the reranking
parser of Charniak and Johnson (2005).
Architecture: A two-stage pipeline architec-
ture is used, where in the first stage less-probable
argument candidates (samples) in the parse tree
are pruned, and in the next stage, final arguments
are identified and assigned a semantic role.
However, for unlabeled data, a preprocessing
stage identifies the verb predicates based on the
POS tag assigned by the parser. The joint argu-
ment identification and classification is chosen to
decrease the complexity of self-training process.
Features: Features are listed in table 1. We
tried to avoid features like named entity tags to
less depend on extra annotation. Features marked
with * are used in addition to common features
in the literature, due to their impact on the per-
formance in feature selection process.
Classifier: We chose a Maximum Entropy
classifier for its efficient training time and also
its built-in multi-classification capability. More-
over, the probability score that it assigns to labels
is useful in selection process in self-training. The
Maxent Toolkit1 was used for this purpose.
</bodyText>
<footnote confidence="0.965285">
1http://homepages.inf.ed.ac.uk/lzhang10/maxent_tool
kit.html
</footnote>
<page confidence="0.984022">
92
</page>
<listItem confidence="0.940216666666667">
1- Add the seed example set L to currently
empty training set T.
2- Train the base classifier C with training
set T.
3- Iterate the following steps until the stop
criterion S is met.
a- Select p examples from U into pool
P.
b- Label pool P with classifier C
c- Select n labeled examples with the
highest confidence score whose score
meets a certain threshold t and add to
training set T.
d- Retrain the classifier C with new
training set.
</listItem>
<figureCaption confidence="0.996206">
Figure 1: Self-training Algorithm
</figureCaption>
<table confidence="0.99902425">
WSJ Test Brown Test
P R F1 P R F1
Cur 77.43 68.15 72.50 69.14 57.01 62.49
Pun 82.28 76.78 79.44 73.38 62.93 67.75
</table>
<tableCaption confidence="0.9528245">
Table 2: Performances of the current system (Cur)
and the state-of-the-art (Punyakanok et al., 2008)
</tableCaption>
<sectionHeader confidence="0.74018" genericHeader="method">
4 Self-training
</sectionHeader>
<subsectionHeader confidence="0.99268">
4.1 The Algorithm
</subsectionHeader>
<bodyText confidence="0.999978425925926">
While the general theme of the self-training algo-
rithm is almost identical in different implementa-
tions, variations of it are developed based on the
characteristics of the task in hand, mainly by cus-
tomizing several involved parameters. Figure 1
shows the algorithm with highlighted parameters.
The size of seed labeled data set L and unla-
beled data U, and their ratio are the fundamental
parameters in any semi-supervised learning. The
data used in this work is explained in section 5.1.
In addition to performance, efficiency of the
classifier (C) is important for self-training, which
is computationally expensive. Our classifier is a
compromise between performance and efficien-
cy. Table 2 shows its performance compared to
the state-of-the-art (Punyakanok et al. 2008)
when trained on the whole labeled training set.
Stop criterion (S) can be set to a pre-
determined number of iterations, finishing all of
the unlabeled data, or convergence of the process
in terms of improvement. We use the second op-
tion for all experiments here.
In each iteration, one can label entire the
unlabeled data or only a portion of it. In the latter
case, a number of unlaleled examples (p) are
selected and loaded into a pool (P). The selection
can be based on a specific strategy, known as
preselection (Abney, 2008) or simply done
according to the original order of the unlabeled
data. We investigate preselection in this work.
After labeling the p unlabeled data, training
set is augmented by adding the newly labeled
data. Two main parameters are involved in this
step: selection of labeled examples to be added to
training set and addition of them to that set.
Selection is the crucial point of self-training,
in which the propagation of labeling noise into
upcoming iterations is the major concern. One
can select all of labeled examples, but usually
only a number of them (n), known as growth
size, based on a quality measure is selected. This
measure is often the confidence score assigned
by the classifier. To prevent poor labelings
diminishing the quality of training set, a
threshold (t) is set on this confidence score.
Selection is also influenced by other factors, one
of which being the balance between selected
labels, which is explored in this study and
explained in detail in the section 4.3.
The selected labeled examples can be retained
in unlabeled set to be labeled again in next
iterations (delibility) or moved so that they are
labeled only once (indelibility). We choose the
second approach here.
</bodyText>
<subsectionHeader confidence="0.966033">
4.2 Preselection
</subsectionHeader>
<bodyText confidence="0.999973583333333">
While using a pool can improve the efficiency of
the self-training process, there can be two other
motivations behind it, concerned with the per-
formance of the process.
One idea is that when all data is labeled, since
the growth size is often much smaller than the
labeled size, a uniform set of examples preferred
by the classifier is chosen in each iteration. This
leads to a biased classifier like the one discussed
in previous section. Limiting the labeling size to
a pool and at the same time (pre)selecting diver-
gence examples into it can remedy the problem.
The other motivation is originated from the
fact that the base classifier is relatively weak due
to small seed size, thus its predictions, as the
measure of confidence in selection process, may
not be reliable. Preselecting a set of unlabeled
examples more probable to be correctly labeled
by the classifier in initial steps seems to be a use-
ful strategy against this fact.
We examine both ideas here, by a random pre-
selection for the first case and a measure of sim-
plicity for the second case. Random preselection
is built into our system, since we use randomized
</bodyText>
<page confidence="0.99456">
93
</page>
<bodyText confidence="0.9995262">
training data. As the measure of simplicity, we
propose the number of samples extracted from
each sentence; that is we sort unlabeled sen-
tences in ascending order based on the number of
samples and load the pool from the beginning.
</bodyText>
<subsectionHeader confidence="0.999608">
4.3 Selection Balancing
</subsectionHeader>
<bodyText confidence="0.9999938">
Most of the previous self-training problems in-
volve a binary classification. Semantic role labe-
ling is a multi-class classification problem with
an unbalanced distribution of classes in a given
text. For example, the frequency of A1 as the
most frequent role in CoNLL training set is
84,917, while the frequency of 21 roles is less
than 20. The situation becomes worse when the
dominant label NULL (for non-arguments) is
added for argument identification purpose in a
joint architecture. This biases the classifiers to-
wards the frequent classes, and the impact is
magnified as self-training proceeds.
In previous work, although they used a re-
duced set of roles (yet not balanced), He and
Gildea (2006) and Lee et al. (2007), did not dis-
criminate between roles when selecting high-
confidence labeled samples. The former study
reports that the majority of labels assigned to
samples were NULL and argument labels ap-
peared only in last iterations.
To attack this problem, we propose a natural
way of balancing, in which instead of labeling
and selection based on argument samples, we
perform a sentence-based selection and labeling.
The idea is that argument roles are distributed
over the sentences. As the measure for selecting
a labeled sentence, the average of the probabili-
ties assigned by the classifier to all argument
samples extracted from the sentence is used.
</bodyText>
<sectionHeader confidence="0.996659" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9999782">
In these experiments, we target two main prob-
lems addressed by semi-supervised methods: the
performance of the algorithm in exploiting unla-
beled data when labeled data is scarce and the
domain-generalizability of the algorithm by us-
ing an out-of-domain unlabeled data.
We use the CoNLL 2005 shared task data and
setting for testing and evaluation purpose. The
evaluation metrics include precision, recall, and
their harmonic mean, F1.
</bodyText>
<subsectionHeader confidence="0.991787">
5.1 The Data
</subsectionHeader>
<bodyText confidence="0.99999472972973">
The labeled data are selected from Propbank
corpus prepared for CoNLL 2005 shared task.
Our learning curve experiments on varying size
of labeled data shows that the steepest increase in
F1 is achieved by 1/10th of CoNLL training data.
Therefore, for training a base classifier as high-
performance as possible, while simulating the
labeled data scarcity with a reasonably small
amount of it, 4000 sentence are selected random-
ly from the total 39,832 training sentences as
seed data (L). These sentences contain 71,400
argument samples covering 38 semantic roles out
of 52 roles present in the total training set.
We use one unlabeled training set (U) for in-
domain and another for out-of-domain experi-
ments. The former is the remaining portion of
CoNLL training data and contains 35,832 sen-
tences (698,567 samples). The out-of-domain set
was extracted from Open American National
Corpus 2 (OANC), a 14-million words multi-
genre corpus of American English. The whole
corpus was preprocessed to prune some proble-
matic sentences. We also excluded the biomed
section due to its large size to retain the domain
balance of the data. Finally, 304,711 sentences
with the length between 3 and 100 were parsed
by the syntactic parser. Out of these, 35,832 sen-
tences were randomly selected for the experi-
ments reported here (832,795 samples).
Two points are worth noting about the results
in advance. First, we do not exclude the argu-
ment roles not present in seed data when evaluat-
ing the results. Second, we observed that our
predicate-identification method is not reliable,
since it is solely based on POS tags assigned by
parser which is error-prone. Experiments with
gold predicates confirmed this conclusion.
</bodyText>
<subsectionHeader confidence="0.996578">
5.2 The Effect of Balanced Selection
</subsectionHeader>
<bodyText confidence="0.999255">
Figures 2 and 3 depict the results of using unba-
lanced and balanced selection with WSJ and
OANC data respectively. To be comparable with
previous work (He and Gildea, 2006), the growth
size (n) for unbalanced method is 7000 samples
and for balanced method is 350 sentences, since
each sentence roughly contains 20 samples. A
probability threshold (t) of 0.70 is used for both
cases. The F1 of base classifier, best-performed
classifier, and final classifier are marked.
When trained on WSJ unlabeled set, the ba-
lanced method outperforms the other in both
WSJ (68.53 vs. 67.96) and Brown test sets (59.62
vs. 58.95). A two-tail t-test based on different
random selection of training data confirms the
statistical significance of this improvement at
p&lt;=0.05 level. Also, the self-training trend is
</bodyText>
<footnote confidence="0.945744">
2 http://www.americannationalcorpus.org/OANC
</footnote>
<page confidence="0.997978">
94
</page>
<figure confidence="0.945138902439024">
WSJ test (U) WSJ test (B)
Brown test (U) Brown test (B)
WSJ test (U) WSJ test (B)
Brown test (U) Brown test (B)
F1
F1
67.96 67.7
59.62 59.
58.95 57.
68.26 68.15 67.1
5
59.41 58
58.68
55.
69
67.9
67
65
63
61
59
58.58
57
7
09
9
69
67.9
67
65
63
61
59
58.58
57
55
41
68.53 68.1
65 75
0 7000 14000 21000 28000 35000
Number of Unlabeled Sentences
</figure>
<figureCaption confidence="0.957832">
Figure 2: Balanced (B) and Unbalanced (U) Selection
with WSJ Unlabeled Data
</figureCaption>
<figure confidence="0.996737">
0 7000 14000 21000 28000 35000
Number of Unlabeled Sentences
</figure>
<figureCaption confidence="0.968812">
Figure 3: Balanced (B) and Unbalanced (U) Selection
with OANC Unlabeled Data
</figureCaption>
<figure confidence="0.979127710526316">
WSJ test (R) WSJ test (S)
Brown test (R) Brown test (S)
WSJ test (R) WSJ test (S)
Brown test (R) Brown test (S)
F1
F1
68.45 68
&apos; 68.25 68
59.77 59.
59.3 58.
68.33 67.45
5
68
67.39
59.27 59.1
59.38
69
67.9
67
65
63
61
59
58.58
57
.44 69
67.9
.14 67
65
63
61
4
59
5 58.58
57
59.0 8
0 5000 10000 15000 20000 25000 30000 35000
Number of Unlabeled Sentences
</figure>
<figureCaption confidence="0.998663">
Figure 4: Random (R) and Simplicity (S) Pre-selection
with WSJ Unlabeled Data
</figureCaption>
<bodyText confidence="0.999930695652174">
more promising with both test sets. When trained
on OANC, the F1 degrades with both methods as
self-training progress. However, for both test
sets, the best classifier is achieved by the ba-
lanced selection (68.26 vs. 68.15 and 59.41 vs.
58.68). Moreover, balanced selection shows a
more normal behavior, while the other degrades
the performance sharply in the last iterations
(due to a swift drop of recall).
Consistent with previous work, with unba-
lanced selection, non-NULL-labeled unlabeled
samples are selected only after the middle of the
process. But, with the balanced method, selection
is more evenly distributed over the roles.
A comparison between the results on Brown
test set with each of unlabeled sets shows that in-
domain data generalizes even better than out-of-
domain data (59.62 vs. 59.41 and also note the
trend). One apparent reason is that the classifier
cannot accurately label the out-of-domain unla-
beled data successively used for training. The
lower quality of our out-of-domain data can be
another reason for this behavior. Furthermore,
</bodyText>
<figure confidence="0.989857">
0 5000 10000 15000 20000 25000 30000 35000
Number of Unlabeled Sentences
</figure>
<figureCaption confidence="0.998331">
Figure 5: Random (R) and Simplicity (S) Pre-selection
with OANC Unlabeled Data
</figureCaption>
<bodyText confidence="0.996558333333333">
the parser we used was trained on WSJ, so it ne-
gatively affected the OANC parses and conse-
quently its SRL results.
</bodyText>
<subsectionHeader confidence="0.983656">
5.3 The Effect of Preselection
</subsectionHeader>
<bodyText confidence="0.999905">
Figures 4 and 5 show the results of using pool
with random and simplicity-based preselection
with WSJ and OANC data respectively. The pool
size (p) is 2000, and growth size (n) is 1000 sen-
tences. The probability threshold (t) used is 0.5.
Comparing these figures with the previous
figures shows that preselection improves the self-
training trend, so that more unlabeled data can
still be useful. This observation was consistent
with various random selection of training data.
Between the two strategies, simplicity-based
method outperforms the random method in both
self-training trend and best classifier F1 (68.45
vs. 68.25 and 59.77 vs. 59.3 with WSJ and 68.33
vs. 68 with OANC), though the t-test shows that
the F1 difference is not significant at p&lt;=0.05.
This improvement does not apply to the case of
using OANC data when tested with Brown data
</bodyText>
<page confidence="0.996139">
95
</page>
<bodyText confidence="0.993934">
(59.27 vs. 59.38), where, however, the differ-
ence is not statistically significant. The same
conclusion to the section 5.2 can be made here.
</bodyText>
<sectionHeader confidence="0.988775" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999992564102564">
This work studies the application of self-training
in learning semantic role labeling with the use of
unlabeled data. We used a balancing method for
selecting newly labeled examples for augmenting
the training set in each iteration of the self-
training process. The idea was to reduce the ef-
fect of unbalanced distribution of semantic roles
in training data. We also used a pool and ex-
amined two preselection methods for loading
unlabeled data into it.
These methods showed improvement in both
classifier performance and self-training trend.
However, using out-of-domain unlabeled data for
increasing the domain generalization ability of
the system was not more useful than using in-
domain data. Among possible reasons are the
low quality of the used data and the poor parses
of the out-of-domain data.
Another major factor that may affect the self-
training behavior here is the poor performance of
the base classifier compared to the state-of-the-
art (see Table 2), which exploits more compli-
cated SRL architecture. Due to high computa-
tional cost of self-training approach, bootstrap-
ping experiments with such complex SRL ap-
proaches are difficult and time-consuming.
Moreover, parameter tuning process shows
that other parameters such as pool-size, growth
number and probability threshold are very effec-
tive. Therefore, more comprehensive parameter
tuning experiments than what was done here is
required and may yield better results.
We are currently planning to port this setting
to co-training, another bootstrapping algorithm.
One direction for future work can be adapting the
architecture of the SRL system to better match
with the bootstrapping process. Another direction
can be adapting bootstrapping parameters to fit
the semantic role labeling complexity.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999610116666667">
Abney, S. 2008. Semisupervised Learning for Compu-
tational Linguistics. Chapman and Hall, London.
Baker, F., Fillmore, C. and Lowe, J. 1998. The Berke-
ley FrameNet project. In Proceedings of COLING-
ACL, pages 86-90.
Charniak, E. and Johnson, M. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173-180.
Carreras, X. and Marquez, L. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labe-
ling. In Proceedings of the 9th Conference on Nat-
ural Language Learning (CoNLL), pages. 152-164.
Clark S., Curran, R. J. and Osborne M. 2003. Boot-
strapping POS taggers using Unlabeled Data. In
Proceedings of the 7th Conference on Natural
Language Learning At HLT-NAACL 2003, pages
49-55.
Gildea, D. and Jurafsky, D. 2002. Automatic labeling
of semantic roles. CL, 28(3):245-288.
He, S. and Gildea, H. 2006. Self-training and Co-
training for Semantic Role Labeling: Primary Re-
port. TR 891, University of Colorado at Boulder
Kingsbury, P. and Palmer, M. 2002. From Treebank
to PropBank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and
Evaluation (LREC-2002).
Lee, J., Song, Y. and Rim, H. 2007. Investigation of
Weakly Supervised Learning for Semantic Role
Labeling. In Proceedings of the Sixth international
Conference on Advanced Language Processing
and Web information Technology (ALPIT 2007),
pages 165-170.
McClosky, D., Charniak, E., and Johnson, M. 2006.
Effective self-training for parsing. In Proceedings
of the Main Conference on Human Language
Technology Conference of the North American
Chapter of the ACL, pages 152-159.
Ng, V. and Cardie, C. 2003. Weakly supervised natu-
ral language learning without redundant views. In
Proceedings of the 2003 Conference of the North
American Chapter of the ACL on Human Lan-
guage Technology, pages 94-101.
Punyakanok, V., Roth, D. and Yi, W. 2008. The Im-
portance of Syntactic Parsing and Inference in Se-
mantic Role Labeling. CL, 34(2):257-287.
Surdeanu, M., Harabagiu, S., Williams, J. and Aar-
seth, P. 2003. Using predicate argument structures
for information extraction. In Proceedings of the
41st Annual Meeting of the ACL, pages 8-15.
Surdeanu, M., Johansson, R., Meyers, A., Marquez,
L. and Nivre, J. 2008. The CoNLL 2008 shared
task on joint parsing of syntactic and semantic de-
pendencies. In Proceedings of the 12th Conference
on Natural Language Learning (CoNLL), pages
159-177.
Yarowsky, E. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In pro-
ceeding of the 33rd Annual Meeting of ACL, pages
189-196.
</reference>
<page confidence="0.998416">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958910">
<title confidence="0.999876">Adapting Self-training for Semantic Role Labeling</title>
<author confidence="0.998359">Rasoul Samad Zadeh Kaljahi</author>
<affiliation confidence="0.999898">FCSIT, University of Malaya</affiliation>
<address confidence="0.998342">50406, Kuala Lumpur, Malaysia.</address>
<email confidence="0.964846">rsk7945@perdana.um.edu.my</email>
<abstract confidence="0.999842631578947">Supervised semantic role labeling (SRL) systems trained on hand-crafted annotated corpora have recently achieved state-of-the-art performance. However, creating such corpora is tedious and costly, with the resulting corpora not sufficiently representative of the language. This paper describes a part of an ongoing work on applying bootstrapping methods to SRL to deal with this problem. Previous work shows that, due to the complexity of SRL, this task is not straight forward. One major difficulty is the propagation of classification noise into the successive iterations. We address this problem employing methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Semisupervised Learning for Computational Linguistics.</title>
<date>2008</date>
<publisher>Chapman and Hall,</publisher>
<location>London.</location>
<contexts>
<context position="10267" citStr="Abney, 2008" startWordPosition="1636" endWordPosition="1637">e 2 shows its performance compared to the state-of-the-art (Punyakanok et al. 2008) when trained on the whole labeled training set. Stop criterion (S) can be set to a predetermined number of iterations, finishing all of the unlabeled data, or convergence of the process in terms of improvement. We use the second option for all experiments here. In each iteration, one can label entire the unlabeled data or only a portion of it. In the latter case, a number of unlaleled examples (p) are selected and loaded into a pool (P). The selection can be based on a specific strategy, known as preselection (Abney, 2008) or simply done according to the original order of the unlabeled data. We investigate preselection in this work. After labeling the p unlabeled data, training set is augmented by adding the newly labeled data. Two main parameters are involved in this step: selection of labeled examples to be added to training set and addition of them to that set. Selection is the crucial point of self-training, in which the propagation of labeling noise into upcoming iterations is the major concern. One can select all of labeled examples, but usually only a number of them (n), known as growth size, based on a </context>
</contexts>
<marker>Abney, 2008</marker>
<rawString>Abney, S. 2008. Semisupervised Learning for Computational Linguistics. Chapman and Hall, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Baker</author>
<author>C Fillmore</author>
<author>J Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL,</booktitle>
<pages>86--90</pages>
<contexts>
<context position="1676" citStr="Baker et al., 1998" startWordPosition="244" endWordPosition="247">nguistics since its introduction by Gildea and Jurafsky (2002). It reveals the event structure encoded in the sentence, which is useful for other NLP tasks or applications such as information extraction, question answering, and machine translation (Surdeanu et al., 2003). Several CoNLL shared tasks (Carreras and Marquez, 2005; Surdeanu et al., 2008) dedicated to semantic role labeling affirm the increasing attention to this field. One important supportive factor of studying supervised statistical SRL has been the existence of hand-annotated semantic corpora for training SRL systems. FrameNet (Baker et al., 1998) was the first such resource, which made the emergence of this research field possible by the seminal work of Gildea and Jurafsky (2002). However, this corpus only exemplifies the semantic role assignment by selecting some illustrative examples for annotation. This questions its suitability for statistical learning. Propbank was started by Kingsbury and Palmer (2002) aiming at developing a more representative resource of English, appropriate for statistical SRL study. Propbank has been used as the learning framework by the majority of SRL work and competitions like CoNLL shared tasks. However,</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Baker, F., Fillmore, C. and Lowe, J. 1998. The Berkeley FrameNet project. In Proceedings of COLINGACL, pages 86-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="7125" citStr="Charniak and Johnson (2005)" startWordPosition="1122" endWordPosition="1125"> it only dealt with core arguments of the Propbank. They achieved a minor improvement too and credited it to the relatively poor performance of their base classifier and the insufficiency of the unlabeled data. 3 SRL System To have enough control over entire the system and thus a flexible experimental framework, we developed our own SRL system instead of using a third-party system. The system works with PropBank-style annotation and is described here. Syntactic Formalism: A Penn Treebank constituent-based approach for SRL is taken. Syntactic parse trees are produced by the reranking parser of Charniak and Johnson (2005). Architecture: A two-stage pipeline architecture is used, where in the first stage less-probable argument candidates (samples) in the parse tree are pruned, and in the next stage, final arguments are identified and assigned a semantic role. However, for unlabeled data, a preprocessing stage identifies the verb predicates based on the POS tag assigned by the parser. The joint argument identification and classification is chosen to decrease the complexity of self-training process. Features: Features are listed in table 1. We tried to avoid features like named entity tags to less depend on extra</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, E. and Johnson, M. 2005. Coarse-to-fine nbest parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting of the ACL, pages 173-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L Marquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Conference on Natural Language Learning (CoNLL),</booktitle>
<pages>152--164</pages>
<contexts>
<context position="1384" citStr="Carreras and Marquez, 2005" startWordPosition="202" endWordPosition="205">his problem by employing balancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods. 1 Introduction Semantic role labeling has been an active research field of computational linguistics since its introduction by Gildea and Jurafsky (2002). It reveals the event structure encoded in the sentence, which is useful for other NLP tasks or applications such as information extraction, question answering, and machine translation (Surdeanu et al., 2003). Several CoNLL shared tasks (Carreras and Marquez, 2005; Surdeanu et al., 2008) dedicated to semantic role labeling affirm the increasing attention to this field. One important supportive factor of studying supervised statistical SRL has been the existence of hand-annotated semantic corpora for training SRL systems. FrameNet (Baker et al., 1998) was the first such resource, which made the emergence of this research field possible by the seminal work of Gildea and Jurafsky (2002). However, this corpus only exemplifies the semantic role assignment by selecting some illustrative examples for annotation. This questions its suitability for statistical </context>
</contexts>
<marker>Carreras, Marquez, 2005</marker>
<rawString>Carreras, X. and Marquez, L. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of the 9th Conference on Natural Language Learning (CoNLL), pages. 152-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>R J Curran</author>
<author>M Osborne</author>
</authors>
<title>Bootstrapping POS taggers using Unlabeled Data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th Conference on Natural Language Learning At HLT-NAACL</booktitle>
<pages>49--55</pages>
<contexts>
<context position="5475" citStr="Clark et al., 2003" startWordPosition="854" endWordPosition="857">redicate Distance constituent and predicate Compound Verb Verb predicate structure Identifier type: simple, compound, or discontinuous compound Head Word Loca- Location of head word intion in Constituent * side the constituent based on the number of words in its right and left Table 1: Features small number of seed words extracted from an online dictionary using a corpus of unannotated English text and gained a comparable accuracy to fully supervised approaches. Subsequently, several studies applied the algorithm to other domains of NLP. Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al., 2003), and parsing (McClosky et al., 2006) were shown to be benefited from self-training. These studies show that the performance of selftraining is tied with its several parameters and the specifications of the underlying task. In SRL field, He and Gildea (2006) used selftraining to address the problem of unseen frames when using FrameNet as the underlying training corpus. They generalized FrameNet frame elements to 15 thematic roles to control the complexity of the process. The improvement gained by the progress of self-training was small and inconsistent. They reported that the NULL label (non-a</context>
</contexts>
<marker>Clark, Curran, Osborne, 2003</marker>
<rawString>Clark S., Curran, R. J. and Osborne M. 2003. Bootstrapping POS taggers using Unlabeled Data. In Proceedings of the 7th Conference on Natural Language Learning At HLT-NAACL 2003, pages 49-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>CL,</journal>
<pages>28--3</pages>
<contexts>
<context position="1119" citStr="Gildea and Jurafsky (2002)" startWordPosition="159" endWordPosition="162"> applying bootstrapping methods to SRL to deal with this problem. Previous work shows that, due to the complexity of SRL, this task is not straight forward. One major difficulty is the propagation of classification noise into the successive iterations. We address this problem by employing balancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods. 1 Introduction Semantic role labeling has been an active research field of computational linguistics since its introduction by Gildea and Jurafsky (2002). It reveals the event structure encoded in the sentence, which is useful for other NLP tasks or applications such as information extraction, question answering, and machine translation (Surdeanu et al., 2003). Several CoNLL shared tasks (Carreras and Marquez, 2005; Surdeanu et al., 2008) dedicated to semantic role labeling affirm the increasing attention to this field. One important supportive factor of studying supervised statistical SRL has been the existence of hand-annotated semantic corpora for training SRL systems. FrameNet (Baker et al., 1998) was the first such resource, which made th</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, D. and Jurafsky, D. 2002. Automatic labeling of semantic roles. CL, 28(3):245-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S He</author>
<author>H Gildea</author>
</authors>
<title>Self-training and Cotraining for Semantic Role Labeling: Primary Report.</title>
<date>2006</date>
<tech>TR 891,</tech>
<institution>University of Colorado at Boulder</institution>
<contexts>
<context position="5733" citStr="He and Gildea (2006)" startWordPosition="896" endWordPosition="899">n its right and left Table 1: Features small number of seed words extracted from an online dictionary using a corpus of unannotated English text and gained a comparable accuracy to fully supervised approaches. Subsequently, several studies applied the algorithm to other domains of NLP. Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al., 2003), and parsing (McClosky et al., 2006) were shown to be benefited from self-training. These studies show that the performance of selftraining is tied with its several parameters and the specifications of the underlying task. In SRL field, He and Gildea (2006) used selftraining to address the problem of unseen frames when using FrameNet as the underlying training corpus. They generalized FrameNet frame elements to 15 thematic roles to control the complexity of the process. The improvement gained by the progress of self-training was small and inconsistent. They reported that the NULL label (non-argument) had often dominated other labels in the examples added to the training set. Lee et al. (2007) attacked another SRL learning problem using self-training. Using Propbank instead of FrameNet, they aimed at increasing the performance of supervised SRL s</context>
<context position="13580" citStr="He and Gildea (2006)" startWordPosition="2190" endWordPosition="2193">ntic role labeling is a multi-class classification problem with an unbalanced distribution of classes in a given text. For example, the frequency of A1 as the most frequent role in CoNLL training set is 84,917, while the frequency of 21 roles is less than 20. The situation becomes worse when the dominant label NULL (for non-arguments) is added for argument identification purpose in a joint architecture. This biases the classifiers towards the frequent classes, and the impact is magnified as self-training proceeds. In previous work, although they used a reduced set of roles (yet not balanced), He and Gildea (2006) and Lee et al. (2007), did not discriminate between roles when selecting highconfidence labeled samples. The former study reports that the majority of labels assigned to samples were NULL and argument labels appeared only in last iterations. To attack this problem, we propose a natural way of balancing, in which instead of labeling and selection based on argument samples, we perform a sentence-based selection and labeling. The idea is that argument roles are distributed over the sentences. As the measure for selecting a labeled sentence, the average of the probabilities assigned by the classi</context>
<context position="16630" citStr="He and Gildea, 2006" startWordPosition="2684" endWordPosition="2687">ments reported here (832,795 samples). Two points are worth noting about the results in advance. First, we do not exclude the argument roles not present in seed data when evaluating the results. Second, we observed that our predicate-identification method is not reliable, since it is solely based on POS tags assigned by parser which is error-prone. Experiments with gold predicates confirmed this conclusion. 5.2 The Effect of Balanced Selection Figures 2 and 3 depict the results of using unbalanced and balanced selection with WSJ and OANC data respectively. To be comparable with previous work (He and Gildea, 2006), the growth size (n) for unbalanced method is 7000 samples and for balanced method is 350 sentences, since each sentence roughly contains 20 samples. A probability threshold (t) of 0.70 is used for both cases. The F1 of base classifier, best-performed classifier, and final classifier are marked. When trained on WSJ unlabeled set, the balanced method outperforms the other in both WSJ (68.53 vs. 67.96) and Brown test sets (59.62 vs. 58.95). A two-tail t-test based on different random selection of training data confirms the statistical significance of this improvement at p&lt;=0.05 level. Also, the</context>
</contexts>
<marker>He, Gildea, 2006</marker>
<rawString>He, S. and Gildea, H. 2006. Self-training and Cotraining for Semantic Role Labeling: Primary Report. TR 891, University of Colorado at Boulder</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002).</booktitle>
<contexts>
<context position="2045" citStr="Kingsbury and Palmer (2002)" startWordPosition="302" endWordPosition="305"> to semantic role labeling affirm the increasing attention to this field. One important supportive factor of studying supervised statistical SRL has been the existence of hand-annotated semantic corpora for training SRL systems. FrameNet (Baker et al., 1998) was the first such resource, which made the emergence of this research field possible by the seminal work of Gildea and Jurafsky (2002). However, this corpus only exemplifies the semantic role assignment by selecting some illustrative examples for annotation. This questions its suitability for statistical learning. Propbank was started by Kingsbury and Palmer (2002) aiming at developing a more representative resource of English, appropriate for statistical SRL study. Propbank has been used as the learning framework by the majority of SRL work and competitions like CoNLL shared tasks. However, it only covers the newswire text from a specific genre and also deals only with verb predicates. All state-of-the-art SRL systems show a dramatic drop in performance when tested on a new text domain (Punyakanok et al., 2008). This evince the infeasibility of building a comprehensive hand-crafted corpus of natural language useful for training a robust semantic role l</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Kingsbury, P. and Palmer, M. 2002. From Treebank to PropBank. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>Y Song</author>
<author>H Rim</author>
</authors>
<title>Investigation of Weakly Supervised Learning for Semantic Role Labeling.</title>
<date>2007</date>
<booktitle>In Proceedings of the Sixth international Conference on Advanced Language Processing and Web information Technology (ALPIT</booktitle>
<pages>165--170</pages>
<contexts>
<context position="6177" citStr="Lee et al. (2007)" startWordPosition="968" endWordPosition="971">These studies show that the performance of selftraining is tied with its several parameters and the specifications of the underlying task. In SRL field, He and Gildea (2006) used selftraining to address the problem of unseen frames when using FrameNet as the underlying training corpus. They generalized FrameNet frame elements to 15 thematic roles to control the complexity of the process. The improvement gained by the progress of self-training was small and inconsistent. They reported that the NULL label (non-argument) had often dominated other labels in the examples added to the training set. Lee et al. (2007) attacked another SRL learning problem using self-training. Using Propbank instead of FrameNet, they aimed at increasing the performance of supervised SRL system by exploiting a large amount of unlabeled data (about 7 times more than labeled data). The algorithm variation was similar to that of He and Gildea (2006), but it only dealt with core arguments of the Propbank. They achieved a minor improvement too and credited it to the relatively poor performance of their base classifier and the insufficiency of the unlabeled data. 3 SRL System To have enough control over entire the system and thus </context>
<context position="13602" citStr="Lee et al. (2007)" startWordPosition="2195" endWordPosition="2198">ulti-class classification problem with an unbalanced distribution of classes in a given text. For example, the frequency of A1 as the most frequent role in CoNLL training set is 84,917, while the frequency of 21 roles is less than 20. The situation becomes worse when the dominant label NULL (for non-arguments) is added for argument identification purpose in a joint architecture. This biases the classifiers towards the frequent classes, and the impact is magnified as self-training proceeds. In previous work, although they used a reduced set of roles (yet not balanced), He and Gildea (2006) and Lee et al. (2007), did not discriminate between roles when selecting highconfidence labeled samples. The former study reports that the majority of labels assigned to samples were NULL and argument labels appeared only in last iterations. To attack this problem, we propose a natural way of balancing, in which instead of labeling and selection based on argument samples, we perform a sentence-based selection and labeling. The idea is that argument roles are distributed over the sentences. As the measure for selecting a labeled sentence, the average of the probabilities assigned by the classifier to all argument s</context>
</contexts>
<marker>Lee, Song, Rim, 2007</marker>
<rawString>Lee, J., Song, Y. and Rim, H. 2007. Investigation of Weakly Supervised Learning for Semantic Role Labeling. In Proceedings of the Sixth international Conference on Advanced Language Processing and Web information Technology (ALPIT 2007), pages 165-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the ACL,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="5512" citStr="McClosky et al., 2006" startWordPosition="860" endWordPosition="863">predicate Compound Verb Verb predicate structure Identifier type: simple, compound, or discontinuous compound Head Word Loca- Location of head word intion in Constituent * side the constituent based on the number of words in its right and left Table 1: Features small number of seed words extracted from an online dictionary using a corpus of unannotated English text and gained a comparable accuracy to fully supervised approaches. Subsequently, several studies applied the algorithm to other domains of NLP. Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al., 2003), and parsing (McClosky et al., 2006) were shown to be benefited from self-training. These studies show that the performance of selftraining is tied with its several parameters and the specifications of the underlying task. In SRL field, He and Gildea (2006) used selftraining to address the problem of unseen frames when using FrameNet as the underlying training corpus. They generalized FrameNet frame elements to 15 thematic roles to control the complexity of the process. The improvement gained by the progress of self-training was small and inconsistent. They reported that the NULL label (non-argument) had often dominated other la</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>McClosky, D., Charniak, E., and Johnson, M. 2006. Effective self-training for parsing. In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the ACL, pages 152-159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Weakly supervised natural language learning without redundant views.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the ACL on Human Language Technology,</booktitle>
<pages>94--101</pages>
<contexts>
<context position="5441" citStr="Ng and Cardie 2003" startWordPosition="848" endWordPosition="851">uent and Number of words between Predicate Distance constituent and predicate Compound Verb Verb predicate structure Identifier type: simple, compound, or discontinuous compound Head Word Loca- Location of head word intion in Constituent * side the constituent based on the number of words in its right and left Table 1: Features small number of seed words extracted from an online dictionary using a corpus of unannotated English text and gained a comparable accuracy to fully supervised approaches. Subsequently, several studies applied the algorithm to other domains of NLP. Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al., 2003), and parsing (McClosky et al., 2006) were shown to be benefited from self-training. These studies show that the performance of selftraining is tied with its several parameters and the specifications of the underlying task. In SRL field, He and Gildea (2006) used selftraining to address the problem of unseen frames when using FrameNet as the underlying training corpus. They generalized FrameNet frame elements to 15 thematic roles to control the complexity of the process. The improvement gained by the progress of self-training was small and inconsistent. They r</context>
</contexts>
<marker>Ng, Cardie, 2003</marker>
<rawString>Ng, V. and Cardie, C. 2003. Weakly supervised natural language learning without redundant views. In Proceedings of the 2003 Conference of the North American Chapter of the ACL on Human Language Technology, pages 94-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yi</author>
</authors>
<date>2008</date>
<booktitle>The Importance of Syntactic Parsing and Inference in Semantic Role Labeling. CL,</booktitle>
<pages>34--2</pages>
<contexts>
<context position="2501" citStr="Punyakanok et al., 2008" startWordPosition="376" endWordPosition="379">ent by selecting some illustrative examples for annotation. This questions its suitability for statistical learning. Propbank was started by Kingsbury and Palmer (2002) aiming at developing a more representative resource of English, appropriate for statistical SRL study. Propbank has been used as the learning framework by the majority of SRL work and competitions like CoNLL shared tasks. However, it only covers the newswire text from a specific genre and also deals only with verb predicates. All state-of-the-art SRL systems show a dramatic drop in performance when tested on a new text domain (Punyakanok et al., 2008). This evince the infeasibility of building a comprehensive hand-crafted corpus of natural language useful for training a robust semantic role labeler. A possible relief for this problem is the utility of semi-supervised learning methods along with the existence of huge amount of natural language text available at a low cost. Semi-supervised methods compensate the scarcity of labeled data by utilizing an additional and much larger amount of unlabeled data via a variety of algorithms. Self-training (Yarowsky, 1995) is a semisupervised algorithm which has been well studied in the NLP area and ga</context>
<context position="8931" citStr="Punyakanok et al., 2008" startWordPosition="1415" endWordPosition="1418">. 2- Train the base classifier C with training set T. 3- Iterate the following steps until the stop criterion S is met. a- Select p examples from U into pool P. b- Label pool P with classifier C c- Select n labeled examples with the highest confidence score whose score meets a certain threshold t and add to training set T. d- Retrain the classifier C with new training set. Figure 1: Self-training Algorithm WSJ Test Brown Test P R F1 P R F1 Cur 77.43 68.15 72.50 69.14 57.01 62.49 Pun 82.28 76.78 79.44 73.38 62.93 67.75 Table 2: Performances of the current system (Cur) and the state-of-the-art (Punyakanok et al., 2008) 4 Self-training 4.1 The Algorithm While the general theme of the self-training algorithm is almost identical in different implementations, variations of it are developed based on the characteristics of the task in hand, mainly by customizing several involved parameters. Figure 1 shows the algorithm with highlighted parameters. The size of seed labeled data set L and unlabeled data U, and their ratio are the fundamental parameters in any semi-supervised learning. The data used in this work is explained in section 5.1. In addition to performance, efficiency of the classifier (C) is important fo</context>
</contexts>
<marker>Punyakanok, Roth, Yi, 2008</marker>
<rawString>Punyakanok, V., Roth, D. and Yi, W. 2008. The Importance of Syntactic Parsing and Inference in Semantic Role Labeling. CL, 34(2):257-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>S Harabagiu</author>
<author>J Williams</author>
<author>P Aarseth</author>
</authors>
<title>Using predicate argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>8--15</pages>
<contexts>
<context position="1328" citStr="Surdeanu et al., 2003" startWordPosition="193" endWordPosition="197">n noise into the successive iterations. We address this problem by employing balancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods. 1 Introduction Semantic role labeling has been an active research field of computational linguistics since its introduction by Gildea and Jurafsky (2002). It reveals the event structure encoded in the sentence, which is useful for other NLP tasks or applications such as information extraction, question answering, and machine translation (Surdeanu et al., 2003). Several CoNLL shared tasks (Carreras and Marquez, 2005; Surdeanu et al., 2008) dedicated to semantic role labeling affirm the increasing attention to this field. One important supportive factor of studying supervised statistical SRL has been the existence of hand-annotated semantic corpora for training SRL systems. FrameNet (Baker et al., 1998) was the first such resource, which made the emergence of this research field possible by the seminal work of Gildea and Jurafsky (2002). However, this corpus only exemplifies the semantic role assignment by selecting some illustrative examples for ann</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Surdeanu, M., Harabagiu, S., Williams, J. and Aarseth, P. 2003. Using predicate argument structures for information extraction. In Proceedings of the 41st Annual Meeting of the ACL, pages 8-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L Marquez</author>
<author>J Nivre</author>
</authors>
<title>The CoNLL</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Natural Language Learning (CoNLL),</booktitle>
<pages>159--177</pages>
<contexts>
<context position="1408" citStr="Surdeanu et al., 2008" startWordPosition="206" endWordPosition="209">ancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods. 1 Introduction Semantic role labeling has been an active research field of computational linguistics since its introduction by Gildea and Jurafsky (2002). It reveals the event structure encoded in the sentence, which is useful for other NLP tasks or applications such as information extraction, question answering, and machine translation (Surdeanu et al., 2003). Several CoNLL shared tasks (Carreras and Marquez, 2005; Surdeanu et al., 2008) dedicated to semantic role labeling affirm the increasing attention to this field. One important supportive factor of studying supervised statistical SRL has been the existence of hand-annotated semantic corpora for training SRL systems. FrameNet (Baker et al., 1998) was the first such resource, which made the emergence of this research field possible by the seminal work of Gildea and Jurafsky (2002). However, this corpus only exemplifies the semantic role assignment by selecting some illustrative examples for annotation. This questions its suitability for statistical learning. Propbank was s</context>
</contexts>
<marker>Surdeanu, Johansson, Meyers, Marquez, Nivre, 2008</marker>
<rawString>Surdeanu, M., Johansson, R., Meyers, A., Marquez, L. and Nivre, J. 2008. The CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the 12th Conference on Natural Language Learning (CoNLL), pages 159-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>In proceeding of the 33rd Annual Meeting of ACL,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="3020" citStr="Yarowsky, 1995" startWordPosition="458" endWordPosition="459">s show a dramatic drop in performance when tested on a new text domain (Punyakanok et al., 2008). This evince the infeasibility of building a comprehensive hand-crafted corpus of natural language useful for training a robust semantic role labeler. A possible relief for this problem is the utility of semi-supervised learning methods along with the existence of huge amount of natural language text available at a low cost. Semi-supervised methods compensate the scarcity of labeled data by utilizing an additional and much larger amount of unlabeled data via a variety of algorithms. Self-training (Yarowsky, 1995) is a semisupervised algorithm which has been well studied in the NLP area and gained promising result. It iteratively extend its training set by labeling the unlabeled data using a base classifier trained on the labeled data. Although the algorithm is theoretically straightforward, it involves a large number of parameters, highly influenced by the specifications of the underlying task. Thus to achieve the best-performing parameter set or even to investigate the usefulness of these algorithms for a learning task such as SRL, a thorough experiment is required. This work investigates its applica</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, E. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In proceeding of the 33rd Annual Meeting of ACL, pages 189-196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>