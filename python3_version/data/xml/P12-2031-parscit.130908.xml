<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.165238">
<title confidence="0.990566">
Crowdsourcing Inference-Rule Evaluation
</title>
<author confidence="0.996902">
Naomi Zeichner Jonathan Berant Ido Dagan
</author>
<affiliation confidence="0.999343">
Bar-Ilan University Tel-Aviv University Bar-Ilan University
</affiliation>
<address confidence="0.678954">
Ramat-Gan, Israel Tel-Aviv, Israel Ramat-Gan, Israel
</address>
<email confidence="0.996236">
zeichner.naomi@gmail.com jonatha6@post.tau.ac.il dagan@cs.biu.ac.il
</email>
<sectionHeader confidence="0.993824" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999539375">
The importance of inference rules to semantic
applications has long been recognized and ex-
tensive work has been carried out to automat-
ically acquire inference-rule resources. How-
ever, evaluating such resources has turned out
to be a non-trivial task, slowing progress in the
field. In this paper, we suggest a framework
for evaluating inference-rule resources. Our
framework simplifies a previously proposed
“instance-based evaluation” method that in-
volved substantial annotator training, making
it suitable for crowdsourcing. We show that
our method produces a large amount of an-
notations with high inter-annotator agreement
for a low cost at a short period of time, without
requiring training expert annotators.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999481188679246">
Inference rules are an important component in se-
mantic applications, such as Question Answering
(QA) (Ravichandran and Hovy, 2002) and Informa-
tion Extraction (IE) (Shinyama and Sekine, 2006),
describing a directional inference relation between
two text patterns with variables. For example, to an-
swer the question ‘Where was Reagan raised?’ a
QA system can use the rule ‘X brought up in Y-4X
raised in Y’ to extract the answer from ‘Reagan was
brought up in Dixon’. Similarly, an IE system can
use the rule ‘X work as Y-4X hired as Y’ to ex-
tract the PERSON and ROLE entities in the “hiring”
event from ‘Bob worked as an analyst for Dell’.
The significance of inference rules has led to sub-
stantial effort into developing algorithms that au-
tomatically learn inference rules (Lin and Pantel,
2001; Sekine, 2005; Schoenmackers et al., 2010),
and generate knowledge resources for inference sys-
tems. However, despite their potential, utilization of
inference rule resources is currently somewhat lim-
ited. This is largely due to the fact that these al-
gorithms often produce invalid rules. Thus, evalu-
ation is necessary both for resource developers as
well as for inference system developers who want to
asses the quality of each resource. Unfortunately, as
evaluating inference rules is hard and costly, there is
no clear evaluation standard, and this has become a
slowing factor for progress in the field.
One option for evaluating inference rule resources
is to measure their impact on an end task, as that is
what ultimately interests an inference system devel-
oper. However, this is often problematic since infer-
ence systems have many components that address
multiple phenomena, and thus it is hard to assess the
effect of a single resource. An example is the Recog-
nizing Textual Entailment (RTE) framework (Dagan
et al., 2009), in which given a text T and a textual
hypothesis H, a system determines whether H can
be inferred from T. This type of evaluation was es-
tablished in RTE challenges by ablation tests (see
RTE ablation tests in ACLWiki) and showed that re-
sources’ impact can vary considerably from one sys-
tem to another. These issues have also been noted
by Sammons et al. (2010) and LoBue and Yates
(2011). A complementary application-independent
evaluation method is hence necessary.
Some attempts were made to let annotators judge
rule correctness directly, that is by asking them to
judge the correctness of a given rule (Shinyama et
al., 2002; Sekine, 2005). However, Szpektor et al.
(2007) observed that directly judging rules out of
context often results in low inter-annotator agree-
ment. To remedy that, Szpektor et al. (2007) and
</bodyText>
<page confidence="0.995693">
156
</page>
<note confidence="0.684995">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99988944117647">
Bhagat et al. (2007) proposed “instance-based eval-
uation”, in which annotators are presented with an
application of a rule in a particular context and
need to judge whether it results in a valid inference.
This simulates the utility of rules in an application
and yields high inter-annotator agreement. Unfortu-
nately, their method requires lengthy guidelines and
substantial annotator training effort, which are time
consuming and costly. Thus, a simple, robust and
replicable evaluation method is needed.
Recently, crowdsourcing services such as Ama-
zon Mechanical Turk (AMT) and CrowdFlower
(CF)1 have been employed for semantic inference
annotation (Snow et al., 2008; Wang and Callison-
Burch, 2010; Mehdad et al., 2010; Negri et al.,
2011). These works focused on generating and an-
notating RTE text-hypothesis pairs, but did not ad-
dress annotation and evaluation of inference rules.
In this paper, we propose a novel instance-based
evaluation framework for inference rules that takes
advantage of crowdsourcing. Our method substan-
tially simplifies annotation of rule applications and
avoids annotator training completely. The nov-
elty in our framework is two-fold: (1) We simplify
instance-based evaluation from a complex decision
scenario to two independent binary decisions. (2)
We apply methodological principles that efficiently
communicate the definition of the “inference” rela-
tion to untrained crowdsourcing workers (Turkers).
As a case study, we applied our method to evalu-
ate algorithms for learning inference rules between
predicates. We show that we can produce many an-
notations cheaply, quickly, at good quality, while
achieving high inter-annotator agreement.
</bodyText>
<sectionHeader confidence="0.904315" genericHeader="method">
2 Evaluating Rule Applications
</sectionHeader>
<bodyText confidence="0.999573">
As mentioned, in instance-based evaluation individ-
ual rule applications are judged rather than rules in
isolation, and the quality of a rule-resource is then
evaluated by the validity of a sample of applications
of its rules. Rule application is performed by finding
an instantiation of the rule left-hand-side in a cor-
pus (termed LHS extraction) and then applying the
rule on the extraction to produce an instantiation of
the rule right-hand-side (termed RHS instantiation).
For example, the rule ‘X observe Y→X celebrate Y’
</bodyText>
<footnote confidence="0.984528">
1https://www.mturk.com and http://crowdflower.com
</footnote>
<bodyText confidence="0.999103628571429">
can be applied on the LHS extraction ‘they observe
holidays’ to produce the RHS instantiation ‘they cel-
ebrate holidays’.
The target of evaluation is to judge whether each
rule application is valid or not. Following the stan-
dard RTE task definition, a rule application is con-
sidered valid if a human reading the LHS extrac-
tion is highly likely to infer that the RHS instanti-
ation is true (Dagan et al., 2009). In the aforemen-
tioned example, the annotator is expected to judge
that ‘they observe holidays’ entails ‘they celebrate
holidays’. In addition to this straightforward case,
two more subtle situations may arise. The first is
that the LHS extraction is meaningless. We regard
a proposition as meaningful if a human can easily
understand its meaning (despite some simple gram-
matical errors). A meaningless LHS extraction usu-
ally occurs due to a faulty extraction process (e.g.,
Table 1, Example 2) and was relatively rare in our
case study (4% of output, see Section 4). Such rule
applications can either be extracted from the sam-
ple so that the rule-base is not penalized (since the
problem is in the extraction procedure), or can be
used as examples of non-entailment, if we are in-
terested in overall performance. A second situation
is a meaningless RHS instantiation, usually caused
by rule application in a wrong context. This case is
tagged as non-entailment (for example, applying the
rule ‘X observe Y→X celebrate Y’ in the context of
the extraction ‘companies observe dress code’).
Each rule application therefore requires an answer
to the following three questions: 1) Is the LHS ex-
traction meaningful? 2) Is the RHS instantiation
meaningful? 3) If both are meaningful, does the
LHS extraction entail the RHS instantiation?
</bodyText>
<sectionHeader confidence="0.983748" genericHeader="method">
3 Crowdsourcing
</sectionHeader>
<bodyText confidence="0.9993557">
Previous works using crowdsourcing noted some
principles to help get the most out of the ser-
vice(Wang et al., 2012). In keeping with these find-
ings we employ the following principles: (a) Simple
tasks. The global task is split into simple sub-tasks,
each dealing with a single aspect of the problem. (b)
Do not assume linguistic knowledge by annota-
tors. Task descriptions avoid linguistic terms such
as “tense”, which confuse workers. (c) Gold stan-
dard validation. Using CF’s built-in methodology,
</bodyText>
<page confidence="0.976828">
157
</page>
<table confidence="0.9897175">
Phrase Meaningful Comments
1) Doctors be treat Mary Yes Annotators are instructed to ignore simple inflectional errors
2) A player deposit an No Bad extraction for the rule LHS ‘X deposit Y’
3) humans bring in bed No Wrong context, result of applying ‘X turn in Y--+X bring in Y’ on ‘humans turn in bed’
</table>
<tableCaption confidence="0.99988">
Table 1: Examples of phrase “meaningfulness” (Note that the comments are not presented to Turkers).
</tableCaption>
<bodyText confidence="0.996954789473684">
gold standard (GS) examples are combined with ac-
tual annotations to continuously validate annotator
reliability.
We split the annotation process into two tasks,
the first to judge phrase meaningfulness (Questions
1 and 2 above) and the second to judge entailment
(Question 3 above). In Task 1, the LHS extrac-
tions and RHS instantiations of all rule applications
are separated and presented to different Turkers in-
dependently of one another. This task is simple,
quick and cheap and allows Turkers to focus on
the single aspect of judging phrase meaningfulness.
Rule applications for which both the LHS extrac-
tion and RHS instantiation are judged as meaningful
are passed to Task 2, where Turkers need to decide
whether a given rule application is valid. If not for
Task 1, Turkers would need to distinguish in Task 2
between non-entailment due to (1) an incorrect rule
(2) a meaningless RHS instantiation (3) a meaning-
less LHS extraction. Thanks to Task 1, Turkers are
presented in Task 2 with two meaningful phrases and
need to decide only whether one entails the other.
To ensure high quality output, each example is
evaluated by three Turkers. Similarly to Mehdad et
al. (2010) we only use results for which the confi-
dence value provided by CF is greater than 70%.
We now describe the details of both tasks. Our
simplification contrasts with Szpektor et al. (2007),
whose judgments for each rule application are simi-
lar to ours, but had to be performed simultaneously
by annotators, which required substantial training.
Task 1: Is the phrase meaningful?
In keeping with the second principle above, the task
description is made up of a short verbal explana-
tion followed by positive and negative examples.
The definition of “meaningfulness” is conveyed via
examples pointing to properties of the automatic
phrase extraction process, as seen in Table 1.
</bodyText>
<subsubsectionHeader confidence="0.653792">
Task 2: Judge if one phrase is true given another.
</subsubsectionHeader>
<bodyText confidence="0.999959571428571">
As mentioned, rule applications for which both sides
were judged as meaningful are evaluated for entail-
ment. The challenge is to communicate the defini-
tion of “entailment” to Turkers. To that end the task
description begins with a short explanation followed
by “easy” and “hard” examples with explanations,
covering a variety of positive and negative entail-
ment “types” (Table 2).
Defining “entailment” is quite difficult when deal-
ing with expert annotators and still more with non-
experts, as was noted by Negri et al. (2011). We
therefore employ several additional mechanisms to
get the definition of entailment across to Turkers
and increase agreement with the GS. We run an
initial small test run and use its output to improve
annotation in two ways: First, we take examples
that were “confusing” for Turkers and add them to
the GS with explanatory feedback presented when
a Turker answers incorrectly. (E.g., the pair (‘The
owner be happy to help drivers’, ‘The owner assist
drivers’) was judged as entailing in the test run but
only achieved a confidence value of 0.53). Second,
we add examples that were annotated unanimously
by Turkers to the GS to increase its size, allowing
CF to better estimate Turker’s reliability (following
CF recommendations, we aim to have around 10%
GS examples in every run). In Section 4 we show
that these mechanisms improved annotation quality.
</bodyText>
<sectionHeader confidence="0.94382" genericHeader="method">
4 Case Study
</sectionHeader>
<bodyText confidence="0.999943454545455">
As a case study, we used our evaluation methodol-
ogy to compare four methods for learning entailment
rules between predicates: DIRT (Lin and Pantel,
2001), Cover (Weeds and Weir, 2003), BInc (Szpek-
tor and Dagan, 2008) and Berant et al. (2010). To
that end, we applied the methods on a set of one
billion extractions (generously provided by Fader
et al. (2011)) automatically extracted from the
ClueWeb09 web crawl2, where each extraction com-
prises a predicate and two arguments. This resulted
in four learned inference rule resources.
</bodyText>
<footnote confidence="0.970362">
2http://lemurproject.org/clueweb09.php/
</footnote>
<page confidence="0.994053">
158
</page>
<note confidence="0.995641571428571">
Example Entailed Explanation given to Turkers
LHS: The lawyer sign the contract Yes There is a chance the lawyer has not read the contract, but
RHS: The lawyer read the contract most likely that as he signed it, he must have read it.
LHS: John be related to Jerry No The LHS can be understood from the RHS, but not the
RHS: John be a close relative of Jerry other way around as the LHS is more general.
LHS: Women be at increased risk of cancer No Although the RHS is correct, it cannot be understood from
RHS: Women die of cancer the LHS.
</note>
<tableCaption confidence="0.998648">
Table 2: Examples given in the description of Task 2.
</tableCaption>
<bodyText confidence="0.999978784313725">
We randomly sampled 5,000 extractions, and for
each one sampled four rules whose LHS matches the
extraction from the union of the learned resources.
We then applied the rules, which resulted in 20,000
rule applications. We annotated rule applications
using our methodology and evaluated each learn-
ing method by comparing the rules learned by each
method with the annotation generated by CF.
In Task 1, 281 rule applications were annotated as
meaningless LHS extraction, and 1,012 were anno-
tated as meaningful LHS extraction but meaningless
RHS instantiation and so automatically annotated as
non-entailment. 8,264 rule applications were passed
on to Task 2, as both sides were judged meaning-
ful (the remaining 10,443 discarded due to low CF
confidence). In Task 2, 5,555 rule applications were
judged with a high confidence and supplied as out-
put, 2,447 of them as positive entailment and 3,108
as negative. Overall, 6,567 rule applications (dataset
of this paper) were annotated for a total cost of
$1000. The annotation process took about one week.
In tests run during development we experimented
with Task 2 wording and GS examples, seeking to
make the definition of entailment as clear as pos-
sible. To do so we randomly sampled and manu-
ally annotated 200 rule applications (from the initial
20,000), and had Turkers judge them. In our initial
test, Turkers tended to answer “yes” comparing to
our own annotation, with 0.79 agreement between
their annotation and ours, corresponding to a kappa
score of 0.54. After applying the mechanisms de-
scribed in Section 3, false-positive rate was reduced
from 18% to 6% while false-negative rate only in-
creased from 4% to 5%, corresponding to a high
agreement of 0.9 and kappa of 0.79.
In our test, 63% of the 200 rule applications were
annotated unanimously by the Turkers. Importantly,
all these examples were in perfect agreement with
our own annotation, reflecting their high reliability.
For the purpose of evaluating the resources learned
by the algorithms we used annotations with CF con-
fidence &gt; 0.7 for which kappa is 0.99.
Lastly, we computed the area under the recall-
precision curve (AUC) for DIRT, Cover, BInc and
Berant et al.’s method, resulting in an AUC of 0.4,
0.43, 0.44, and 0.52 respectively. We used the AUC
curve, with number of recall-precision points in the
order of thousands, to avoid tuning a threshold pa-
rameter. Overall, we demonstrated that our evalua-
tion framework allowed us to compare four different
learning methods in low costs and within one week.
</bodyText>
<sectionHeader confidence="0.998087" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9989715">
In this paper we have suggested a crowdsourcing
framework for evaluating inference rules. We have
shown that by simplifying the previously-proposed
instance-based evaluation framework we are able to
take advantage of crowdsourcing services to replace
trained expert annotators, resulting in good quality
large scale annotations, for reasonable time and cost.
We have presented the methodological principles we
developed to get the entailment decision across to
Turkers, achieving very high agreement both with
our annotations and between the annotators them-
selves. Using the CrowdFlower forms we provide
with this paper, the proposed methodology can be
beneficial for both resource developers evaluating
their output as well as inference system developers
wanting to assess the quality of existing resources.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998657">
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Communitys Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
</bodyText>
<page confidence="0.998412">
159
</page>
<sectionHeader confidence="0.990192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999436451612903">
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of the annual meeting of the Associa-
tion for Computational Linguistics (ACL).
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(Special Issue 04):i–xvii.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of the Conference of
Empirical Methods in Natural Language Processing
(EMNLP ’11).
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Peter LoBue and Alexander Yates. 2011. Types of
common-sense knowledge needed for recognizing tex-
tual entailment. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards cross-lingual textual entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (ACL).
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: Crowdsourcing the creation of
cross-lingual textual entailment corpora. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP ’11).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the annual meeting of the Associa-
tion for Computational Linguistics (ACL).
Mark Sammons, V. G. Vinod Vydiswaran, and Dan Roth.
2010. ”ask not what textual entailment can do for
you...”. In Proceedings of the annual meeting of the
Association for Computational Linguistics (ACL).
Stefan Schoenmackers, Oren Etzioni Jesse Davis, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ’10).
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of the Third International Workshop on
Paraphrasing (IWP2005).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics (HLT-NAACL ’06).
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of the second international
conference on Human Language Technology Research
(HLT ’02).
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast—but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP
’08).
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (Coling 2008).
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the annual meeting of the As-
sociation for Computational Linguistics (ACL).
Rui Wang and Chris Callison-Burch. 2010. Cheap facts
and counter-facts. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk.
Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan.
2012. Perspectives on crowdsourcing annotations for
natural language processing. Journal of Language Re-
sources and Evaluation).
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2003).
</reference>
<page confidence="0.997463">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978747">
<title confidence="0.99995">Crowdsourcing Inference-Rule Evaluation</title>
<author confidence="0.995929">Naomi Zeichner Jonathan Berant Ido Dagan</author>
<affiliation confidence="0.999994">Bar-Ilan University Tel-Aviv University Bar-Ilan University</affiliation>
<address confidence="0.998146">Ramat-Gan, Israel Tel-Aviv, Israel Ramat-Gan, Israel</address>
<email confidence="0.99044">zeichner.naomi@gmail.comjonatha6@post.tau.ac.ildagan@cs.biu.ac.il</email>
<abstract confidence="0.999642941176471">The importance of inference rules to semantic applications has long been recognized and extensive work has been carried out to automatically acquire inference-rule resources. However, evaluating such resources has turned out to be a non-trivial task, slowing progress in the field. In this paper, we suggest a framework for evaluating inference-rule resources. Our framework simplifies a previously proposed “instance-based evaluation” method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global learning of focused entailment graphs.</title>
<date>2010</date>
<booktitle>In Proceedings of the annual meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="12327" citStr="Berant et al. (2010)" startWordPosition="1955" endWordPosition="1958">he test run but only achieved a confidence value of 0.53). Second, we add examples that were annotated unanimously by Turkers to the GS to increase its size, allowing CF to better estimate Turker’s reliability (following CF recommendations, we aim to have around 10% GS examples in every run). In Section 4 we show that these mechanisms improved annotation quality. 4 Case Study As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates: DIRT (Lin and Pantel, 2001), Cover (Weeds and Weir, 2003), BInc (Szpektor and Dagan, 2008) and Berant et al. (2010). To that end, we applied the methods on a set of one billion extractions (generously provided by Fader et al. (2011)) automatically extracted from the ClueWeb09 web crawl2, where each extraction comprises a predicate and two arguments. This resulted in four learned inference rule resources. 2http://lemurproject.org/clueweb09.php/ 158 Example Entailed Explanation given to Turkers LHS: The lawyer sign the contract Yes There is a chance the lawyer has not read the contract, but RHS: The lawyer read the contract most likely that as he signed it, he must have read it. LHS: John be related to Jerry</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2010</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2010. Global learning of focused entailment graphs. In Proceedings of the annual meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>LEDIR: An unsupervised algorithm for learning directionality of inference rules.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<contexts>
<context position="3864" citStr="Bhagat et al. (2007)" startWordPosition="597" endWordPosition="600">ndependent evaluation method is hence necessary. Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005). However, Szpektor et al. (2007) observed that directly judging rules out of context often results in low inter-annotator agreement. To remedy that, Szpektor et al. (2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Bhagat et al. (2007) proposed “instance-based evaluation”, in which annotators are presented with an application of a rule in a particular context and need to judge whether it results in a valid inference. This simulates the utility of rules in an application and yields high inter-annotator agreement. Unfortunately, their method requires lengthy guidelines and substantial annotator training effort, which are time consuming and costly. Thus, a simple, robust and replicable evaluation method is needed. Recently, crowdsourcing services such as Amazon Mechanical Turk (AMT) and CrowdFlower (CF)1 have been employed for</context>
</contexts>
<marker>Bhagat, Pantel, Hovy, 2007</marker>
<rawString>Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007. LEDIR: An unsupervised algorithm for learning directionality of inference rules. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Bernardo Magnini</author>
<author>Dan Roth</author>
</authors>
<title>Recognizing textual entailment: Rational, evaluation and approaches.</title>
<date>2009</date>
<booktitle>Natural Language Engineering, 15(Special Issue 04):i–xvii.</booktitle>
<contexts>
<context position="2824" citStr="Dagan et al., 2009" startWordPosition="429" endWordPosition="432">e quality of each resource. Unfortunately, as evaluating inference rules is hard and costly, there is no clear evaluation standard, and this has become a slowing factor for progress in the field. One option for evaluating inference rule resources is to measure their impact on an end task, as that is what ultimately interests an inference system developer. However, this is often problematic since inference systems have many components that address multiple phenomena, and thus it is hard to assess the effect of a single resource. An example is the Recognizing Textual Entailment (RTE) framework (Dagan et al., 2009), in which given a text T and a textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources’ impact can vary considerably from one system to another. These issues have also been noted by Sammons et al. (2010) and LoBue and Yates (2011). A complementary application-independent evaluation method is hence necessary. Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a giv</context>
<context position="6532" citStr="Dagan et al., 2009" startWordPosition="1002" endWordPosition="1005"> the rule on the extraction to produce an instantiation of the rule right-hand-side (termed RHS instantiation). For example, the rule ‘X observe Y→X celebrate Y’ 1https://www.mturk.com and http://crowdflower.com can be applied on the LHS extraction ‘they observe holidays’ to produce the RHS instantiation ‘they celebrate holidays’. The target of evaluation is to judge whether each rule application is valid or not. Following the standard RTE task definition, a rule application is considered valid if a human reading the LHS extraction is highly likely to infer that the RHS instantiation is true (Dagan et al., 2009). In the aforementioned example, the annotator is expected to judge that ‘they observe holidays’ entails ‘they celebrate holidays’. In addition to this straightforward case, two more subtle situations may arise. The first is that the LHS extraction is meaningless. We regard a proposition as meaningful if a human can easily understand its meaning (despite some simple grammatical errors). A meaningless LHS extraction usually occurs due to a faulty extraction process (e.g., Table 1, Example 2) and was relatively rare in our case study (4% of output, see Section 4). Such rule applications can eith</context>
</contexts>
<marker>Dagan, Dolan, Magnini, Roth, 2009</marker>
<rawString>Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entailment: Rational, evaluation and approaches. Natural Language Engineering, 15(Special Issue 04):i–xvii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference of Empirical Methods in Natural Language Processing (EMNLP ’11).</booktitle>
<contexts>
<context position="12444" citStr="Fader et al. (2011)" startWordPosition="1976" endWordPosition="1979">Turkers to the GS to increase its size, allowing CF to better estimate Turker’s reliability (following CF recommendations, we aim to have around 10% GS examples in every run). In Section 4 we show that these mechanisms improved annotation quality. 4 Case Study As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates: DIRT (Lin and Pantel, 2001), Cover (Weeds and Weir, 2003), BInc (Szpektor and Dagan, 2008) and Berant et al. (2010). To that end, we applied the methods on a set of one billion extractions (generously provided by Fader et al. (2011)) automatically extracted from the ClueWeb09 web crawl2, where each extraction comprises a predicate and two arguments. This resulted in four learned inference rule resources. 2http://lemurproject.org/clueweb09.php/ 158 Example Entailed Explanation given to Turkers LHS: The lawyer sign the contract Yes There is a chance the lawyer has not read the contract, but RHS: The lawyer read the contract most likely that as he signed it, he must have read it. LHS: John be related to Jerry No The LHS can be understood from the RHS, but not the RHS: John be a close relative of Jerry other way around as th</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference of Empirical Methods in Natural Language Processing (EMNLP ’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT - discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="1794" citStr="Lin and Pantel, 2001" startWordPosition="264" endWordPosition="267">nd Sekine, 2006), describing a directional inference relation between two text patterns with variables. For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y-4X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’. Similarly, an IE system can use the rule ‘X work as Y-4X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’. The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems. However, despite their potential, utilization of inference rule resources is currently somewhat limited. This is largely due to the fact that these algorithms often produce invalid rules. Thus, evaluation is necessary both for resource developers as well as for inference system developers who want to asses the quality of each resource. Unfortunately, as evaluating inference rules is hard and costly, there is no clear evaluation standard, and this has become a slowing factor for progress in the </context>
<context position="12239" citStr="Lin and Pantel, 2001" startWordPosition="1939" endWordPosition="1942">owner be happy to help drivers’, ‘The owner assist drivers’) was judged as entailing in the test run but only achieved a confidence value of 0.53). Second, we add examples that were annotated unanimously by Turkers to the GS to increase its size, allowing CF to better estimate Turker’s reliability (following CF recommendations, we aim to have around 10% GS examples in every run). In Section 4 we show that these mechanisms improved annotation quality. 4 Case Study As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates: DIRT (Lin and Pantel, 2001), Cover (Weeds and Weir, 2003), BInc (Szpektor and Dagan, 2008) and Berant et al. (2010). To that end, we applied the methods on a set of one billion extractions (generously provided by Fader et al. (2011)) automatically extracted from the ClueWeb09 web crawl2, where each extraction comprises a predicate and two arguments. This resulted in four learned inference rule resources. 2http://lemurproject.org/clueweb09.php/ 158 Example Entailed Explanation given to Turkers LHS: The lawyer sign the contract Yes There is a chance the lawyer has not read the contract, but RHS: The lawyer read the contra</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT - discovery of inference rules from text. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter LoBue</author>
<author>Alexander Yates</author>
</authors>
<title>Types of common-sense knowledge needed for recognizing textual entailment.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT).</booktitle>
<contexts>
<context position="3213" citStr="LoBue and Yates (2011)" startWordPosition="500" endWordPosition="503">ematic since inference systems have many components that address multiple phenomena, and thus it is hard to assess the effect of a single resource. An example is the Recognizing Textual Entailment (RTE) framework (Dagan et al., 2009), in which given a text T and a textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources’ impact can vary considerably from one system to another. These issues have also been noted by Sammons et al. (2010) and LoBue and Yates (2011). A complementary application-independent evaluation method is hence necessary. Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005). However, Szpektor et al. (2007) observed that directly judging rules out of context often results in low inter-annotator agreement. To remedy that, Szpektor et al. (2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association</context>
</contexts>
<marker>LoBue, Yates, 2011</marker>
<rawString>Peter LoBue and Alexander Yates. 2011. Types of common-sense knowledge needed for recognizing textual entailment. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Towards cross-lingual textual entailment.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4564" citStr="Mehdad et al., 2010" startWordPosition="702" endWordPosition="705">application of a rule in a particular context and need to judge whether it results in a valid inference. This simulates the utility of rules in an application and yields high inter-annotator agreement. Unfortunately, their method requires lengthy guidelines and substantial annotator training effort, which are time consuming and costly. Thus, a simple, robust and replicable evaluation method is needed. Recently, crowdsourcing services such as Amazon Mechanical Turk (AMT) and CrowdFlower (CF)1 have been employed for semantic inference annotation (Snow et al., 2008; Wang and CallisonBurch, 2010; Mehdad et al., 2010; Negri et al., 2011). These works focused on generating and annotating RTE text-hypothesis pairs, but did not address annotation and evaluation of inference rules. In this paper, we propose a novel instance-based evaluation framework for inference rules that takes advantage of crowdsourcing. Our method substantially simplifies annotation of rule applications and avoids annotator training completely. The novelty in our framework is two-fold: (1) We simplify instance-based evaluation from a complex decision scenario to two independent binary decisions. (2) We apply methodological principles tha</context>
<context position="9965" citStr="Mehdad et al. (2010)" startWordPosition="1566" endWordPosition="1569"> Rule applications for which both the LHS extraction and RHS instantiation are judged as meaningful are passed to Task 2, where Turkers need to decide whether a given rule application is valid. If not for Task 1, Turkers would need to distinguish in Task 2 between non-entailment due to (1) an incorrect rule (2) a meaningless RHS instantiation (3) a meaningless LHS extraction. Thanks to Task 1, Turkers are presented in Task 2 with two meaningful phrases and need to decide only whether one entails the other. To ensure high quality output, each example is evaluated by three Turkers. Similarly to Mehdad et al. (2010) we only use results for which the confidence value provided by CF is greater than 70%. We now describe the details of both tasks. Our simplification contrasts with Szpektor et al. (2007), whose judgments for each rule application are similar to ours, but had to be performed simultaneously by annotators, which required substantial training. Task 1: Is the phrase meaningful? In keeping with the second principle above, the task description is made up of a short verbal explanation followed by positive and negative examples. The definition of “meaningfulness” is conveyed via examples pointing to p</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2010</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2010. Towards cross-lingual textual entailment. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Luisa Bentivogli</author>
<author>Yashar Mehdad</author>
<author>Danilo Giampiccolo</author>
<author>Alessandro Marchetti</author>
</authors>
<title>Divide and conquer: Crowdsourcing the creation of cross-lingual textual entailment corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP ’11).</booktitle>
<contexts>
<context position="4585" citStr="Negri et al., 2011" startWordPosition="706" endWordPosition="709"> in a particular context and need to judge whether it results in a valid inference. This simulates the utility of rules in an application and yields high inter-annotator agreement. Unfortunately, their method requires lengthy guidelines and substantial annotator training effort, which are time consuming and costly. Thus, a simple, robust and replicable evaluation method is needed. Recently, crowdsourcing services such as Amazon Mechanical Turk (AMT) and CrowdFlower (CF)1 have been employed for semantic inference annotation (Snow et al., 2008; Wang and CallisonBurch, 2010; Mehdad et al., 2010; Negri et al., 2011). These works focused on generating and annotating RTE text-hypothesis pairs, but did not address annotation and evaluation of inference rules. In this paper, we propose a novel instance-based evaluation framework for inference rules that takes advantage of crowdsourcing. Our method substantially simplifies annotation of rule applications and avoids annotator training completely. The novelty in our framework is two-fold: (1) We simplify instance-based evaluation from a complex decision scenario to two independent binary decisions. (2) We apply methodological principles that efficiently communi</context>
<context position="11215" citStr="Negri et al. (2011)" startWordPosition="1770" endWordPosition="1773">se extraction process, as seen in Table 1. Task 2: Judge if one phrase is true given another. As mentioned, rule applications for which both sides were judged as meaningful are evaluated for entailment. The challenge is to communicate the definition of “entailment” to Turkers. To that end the task description begins with a short explanation followed by “easy” and “hard” examples with explanations, covering a variety of positive and negative entailment “types” (Table 2). Defining “entailment” is quite difficult when dealing with expert annotators and still more with nonexperts, as was noted by Negri et al. (2011). We therefore employ several additional mechanisms to get the definition of entailment across to Turkers and increase agreement with the GS. We run an initial small test run and use its output to improve annotation in two ways: First, we take examples that were “confusing” for Turkers and add them to the GS with explanatory feedback presented when a Turker answers incorrectly. (E.g., the pair (‘The owner be happy to help drivers’, ‘The owner assist drivers’) was judged as entailing in the test run but only achieved a confidence value of 0.53). Second, we add examples that were annotated unani</context>
</contexts>
<marker>Negri, Bentivogli, Mehdad, Giampiccolo, Marchetti, 2011</marker>
<rawString>Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo Giampiccolo, and Alessandro Marchetti. 2011. Divide and conquer: Crowdsourcing the creation of cross-lingual textual entailment corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP ’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of the annual meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1130" citStr="Ravichandran and Hovy, 2002" startWordPosition="148" endWordPosition="151">a non-trivial task, slowing progress in the field. In this paper, we suggest a framework for evaluating inference-rule resources. Our framework simplifies a previously proposed “instance-based evaluation” method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators. 1 Introduction Inference rules are an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006), describing a directional inference relation between two text patterns with variables. For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y-4X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’. Similarly, an IE system can use the rule ‘X work as Y-4X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’. The significance of inference rules has led to substantial effort into developing algorithm</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of the annual meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Sammons</author>
<author>V G Vinod Vydiswaran</author>
<author>Dan Roth</author>
</authors>
<title>ask not what textual entailment can do for you...”.</title>
<date>2010</date>
<booktitle>In Proceedings of the annual meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="3186" citStr="Sammons et al. (2010)" startWordPosition="495" endWordPosition="498">wever, this is often problematic since inference systems have many components that address multiple phenomena, and thus it is hard to assess the effect of a single resource. An example is the Recognizing Textual Entailment (RTE) framework (Dagan et al., 2009), in which given a text T and a textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources’ impact can vary considerably from one system to another. These issues have also been noted by Sammons et al. (2010) and LoBue and Yates (2011). A complementary application-independent evaluation method is hence necessary. Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005). However, Szpektor et al. (2007) observed that directly judging rules out of context often results in low inter-annotator agreement. To remedy that, Szpektor et al. (2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, Jeju, Republic of Korea, 8-14 Ju</context>
</contexts>
<marker>Sammons, Vydiswaran, Roth, 2010</marker>
<rawString>Mark Sammons, V. G. Vinod Vydiswaran, and Dan Roth. 2010. ”ask not what textual entailment can do for you...”. In Proceedings of the annual meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni Jesse Davis</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning first-order horn clauses from web text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP ’10).</booktitle>
<contexts>
<context position="1837" citStr="Schoenmackers et al., 2010" startWordPosition="270" endWordPosition="273">tional inference relation between two text patterns with variables. For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y-4X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’. Similarly, an IE system can use the rule ‘X work as Y-4X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’. The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems. However, despite their potential, utilization of inference rule resources is currently somewhat limited. This is largely due to the fact that these algorithms often produce invalid rules. Thus, evaluation is necessary both for resource developers as well as for inference system developers who want to asses the quality of each resource. Unfortunately, as evaluating inference rules is hard and costly, there is no clear evaluation standard, and this has become a slowing factor for progress in the field. One option for evaluating inference </context>
</contexts>
<marker>Schoenmackers, Davis, Weld, 2010</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni Jesse Davis, and Daniel S. Weld. 2010. Learning first-order horn clauses from web text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>Automatic paraphrase discovery based on context and keywords between ne pairs.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).</booktitle>
<contexts>
<context position="1808" citStr="Sekine, 2005" startWordPosition="268" endWordPosition="269">ribing a directional inference relation between two text patterns with variables. For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y-4X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’. Similarly, an IE system can use the rule ‘X work as Y-4X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’. The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems. However, despite their potential, utilization of inference rule resources is currently somewhat limited. This is largely due to the fact that these algorithms often produce invalid rules. Thus, evaluation is necessary both for resource developers as well as for inference system developers who want to asses the quality of each resource. Unfortunately, as evaluating inference rules is hard and costly, there is no clear evaluation standard, and this has become a slowing factor for progress in the field. One opt</context>
<context position="3469" citStr="Sekine, 2005" startWordPosition="540" endWordPosition="541">textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources’ impact can vary considerably from one system to another. These issues have also been noted by Sammons et al. (2010) and LoBue and Yates (2011). A complementary application-independent evaluation method is hence necessary. Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005). However, Szpektor et al. (2007) observed that directly judging rules out of context often results in low inter-annotator agreement. To remedy that, Szpektor et al. (2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Bhagat et al. (2007) proposed “instance-based evaluation”, in which annotators are presented with an application of a rule in a particular context and need to judge whether it results in a valid inference. This simulates the </context>
</contexts>
<marker>Sekine, 2005</marker>
<rawString>Satoshi Sekine. 2005. Automatic paraphrase discovery based on context and keywords between ne pairs. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL ’06).</booktitle>
<contexts>
<context position="1190" citStr="Shinyama and Sekine, 2006" startWordPosition="157" endWordPosition="160">er, we suggest a framework for evaluating inference-rule resources. Our framework simplifies a previously proposed “instance-based evaluation” method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators. 1 Introduction Inference rules are an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006), describing a directional inference relation between two text patterns with variables. For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y-4X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’. Similarly, an IE system can use the rule ‘X work as Y-4X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’. The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, </context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL ’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
</authors>
<title>Automatic paraphrase acquisition from news articles.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research (HLT ’02).</booktitle>
<contexts>
<context position="3454" citStr="Shinyama et al., 2002" startWordPosition="536" endWordPosition="539">h given a text T and a textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources’ impact can vary considerably from one system to another. These issues have also been noted by Sammons et al. (2010) and LoBue and Yates (2011). A complementary application-independent evaluation method is hence necessary. Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005). However, Szpektor et al. (2007) observed that directly judging rules out of context often results in low inter-annotator agreement. To remedy that, Szpektor et al. (2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Bhagat et al. (2007) proposed “instance-based evaluation”, in which annotators are presented with an application of a rule in a particular context and need to judge whether it results in a valid inference. This</context>
</contexts>
<marker>Shinyama, Sekine, Sudo, 2002</marker>
<rawString>Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo. 2002. Automatic paraphrase acquisition from news articles. In Proceedings of the second international conference on Human Language Technology Research (HLT ’02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP ’08).</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP ’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<contexts>
<context position="12302" citStr="Szpektor and Dagan, 2008" startWordPosition="1949" endWordPosition="1953">) was judged as entailing in the test run but only achieved a confidence value of 0.53). Second, we add examples that were annotated unanimously by Turkers to the GS to increase its size, allowing CF to better estimate Turker’s reliability (following CF recommendations, we aim to have around 10% GS examples in every run). In Section 4 we show that these mechanisms improved annotation quality. 4 Case Study As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates: DIRT (Lin and Pantel, 2001), Cover (Weeds and Weir, 2003), BInc (Szpektor and Dagan, 2008) and Berant et al. (2010). To that end, we applied the methods on a set of one billion extractions (generously provided by Fader et al. (2011)) automatically extracted from the ClueWeb09 web crawl2, where each extraction comprises a predicate and two arguments. This resulted in four learned inference rule resources. 2http://lemurproject.org/clueweb09.php/ 158 Example Entailed Explanation given to Turkers LHS: The lawyer sign the contract Yes There is a chance the lawyer has not read the contract, but RHS: The lawyer read the contract most likely that as he signed it, he must have read it. LHS:</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Eyal Shnarch</author>
<author>Ido Dagan</author>
</authors>
<title>Instance-based evaluation of entailment rule acquisition.</title>
<date>2007</date>
<booktitle>In Proceedings of the annual meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="3502" citStr="Szpektor et al. (2007)" startWordPosition="543" endWordPosition="546">system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources’ impact can vary considerably from one system to another. These issues have also been noted by Sammons et al. (2010) and LoBue and Yates (2011). A complementary application-independent evaluation method is hence necessary. Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005). However, Szpektor et al. (2007) observed that directly judging rules out of context often results in low inter-annotator agreement. To remedy that, Szpektor et al. (2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Bhagat et al. (2007) proposed “instance-based evaluation”, in which annotators are presented with an application of a rule in a particular context and need to judge whether it results in a valid inference. This simulates the utility of rules in an applicatio</context>
<context position="10152" citStr="Szpektor et al. (2007)" startWordPosition="1599" endWordPosition="1602">s valid. If not for Task 1, Turkers would need to distinguish in Task 2 between non-entailment due to (1) an incorrect rule (2) a meaningless RHS instantiation (3) a meaningless LHS extraction. Thanks to Task 1, Turkers are presented in Task 2 with two meaningful phrases and need to decide only whether one entails the other. To ensure high quality output, each example is evaluated by three Turkers. Similarly to Mehdad et al. (2010) we only use results for which the confidence value provided by CF is greater than 70%. We now describe the details of both tasks. Our simplification contrasts with Szpektor et al. (2007), whose judgments for each rule application are similar to ours, but had to be performed simultaneously by annotators, which required substantial training. Task 1: Is the phrase meaningful? In keeping with the second principle above, the task description is made up of a short verbal explanation followed by positive and negative examples. The definition of “meaningfulness” is conveyed via examples pointing to properties of the automatic phrase extraction process, as seen in Table 1. Task 2: Judge if one phrase is true given another. As mentioned, rule applications for which both sides were judg</context>
</contexts>
<marker>Szpektor, Shnarch, Dagan, 2007</marker>
<rawString>Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. Instance-based evaluation of entailment rule acquisition. In Proceedings of the annual meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Wang</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Cheap facts and counter-facts.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<marker>Wang, Callison-Burch, 2010</marker>
<rawString>Rui Wang and Chris Callison-Burch. 2010. Cheap facts and counter-facts. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aobo Wang</author>
<author>Cong Duy Vu Hoang</author>
<author>Min-Yen Kan</author>
</authors>
<title>Perspectives on crowdsourcing annotations for natural language processing.</title>
<date>2012</date>
<journal>Journal of Language Resources and Evaluation).</journal>
<contexts>
<context position="7994" citStr="Wang et al., 2012" startWordPosition="1238" endWordPosition="1242">gless RHS instantiation, usually caused by rule application in a wrong context. This case is tagged as non-entailment (for example, applying the rule ‘X observe Y→X celebrate Y’ in the context of the extraction ‘companies observe dress code’). Each rule application therefore requires an answer to the following three questions: 1) Is the LHS extraction meaningful? 2) Is the RHS instantiation meaningful? 3) If both are meaningful, does the LHS extraction entail the RHS instantiation? 3 Crowdsourcing Previous works using crowdsourcing noted some principles to help get the most out of the service(Wang et al., 2012). In keeping with these findings we employ the following principles: (a) Simple tasks. The global task is split into simple sub-tasks, each dealing with a single aspect of the problem. (b) Do not assume linguistic knowledge by annotators. Task descriptions avoid linguistic terms such as “tense”, which confuse workers. (c) Gold standard validation. Using CF’s built-in methodology, 157 Phrase Meaningful Comments 1) Doctors be treat Mary Yes Annotators are instructed to ignore simple inflectional errors 2) A player deposit an No Bad extraction for the rule LHS ‘X deposit Y’ 3) humans bring in bed</context>
</contexts>
<marker>Wang, Hoang, Kan, 2012</marker>
<rawString>Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan. 2012. Perspectives on crowdsourcing annotations for natural language processing. Journal of Language Resources and Evaluation).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="12269" citStr="Weeds and Weir, 2003" startWordPosition="1944" endWordPosition="1947">’, ‘The owner assist drivers’) was judged as entailing in the test run but only achieved a confidence value of 0.53). Second, we add examples that were annotated unanimously by Turkers to the GS to increase its size, allowing CF to better estimate Turker’s reliability (following CF recommendations, we aim to have around 10% GS examples in every run). In Section 4 we show that these mechanisms improved annotation quality. 4 Case Study As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates: DIRT (Lin and Pantel, 2001), Cover (Weeds and Weir, 2003), BInc (Szpektor and Dagan, 2008) and Berant et al. (2010). To that end, we applied the methods on a set of one billion extractions (generously provided by Fader et al. (2011)) automatically extracted from the ClueWeb09 web crawl2, where each extraction comprises a predicate and two arguments. This resulted in four learned inference rule resources. 2http://lemurproject.org/clueweb09.php/ 158 Example Entailed Explanation given to Turkers LHS: The lawyer sign the contract Yes There is a chance the lawyer has not read the contract, but RHS: The lawyer read the contract most likely that as he sign</context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2003).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>