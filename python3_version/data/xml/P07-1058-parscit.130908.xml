<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017486">
<title confidence="0.983472">
Instance-based Evaluation of Entailment Rule Acquisition
</title>
<author confidence="0.996591">
Idan Szpektor, Eyal Shnarch, Ido Dagan
</author>
<affiliation confidence="0.990302">
Dept. of Computer Science
Bar Ilan University
</affiliation>
<address confidence="0.506049">
Ramat Gan, Israel
</address>
<email confidence="0.999045">
{szpekti,shey,dagan}@cs.biu.ac.il
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999611785714286">
Obtaining large volumes of inference knowl-
edge, such as entailment rules, has become
a major factor in achieving robust seman-
tic processing. While there has been sub-
stantial research on learning algorithms for
such knowledge, their evaluation method-
ology has been problematic, hindering fur-
ther research. We propose a novel evalua-
tion methodology for entailment rules which
explicitly addresses their semantic proper-
ties and yields satisfactory human agreement
levels. The methodology is used to compare
two state of the art learning algorithms, ex-
posing critical issues for future progress.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998014">
In many NLP applications, such as Question An-
swering (QA) and Information Extraction (IE), it is
crucial to recognize that a particular target mean-
ing can be inferred from different text variants. For
example, a QA system needs to identify that “As-
pirin lowers the risk ofheart attacks” can be inferred
from “Aspirin prevents heart attacks” in order to an-
swer the question “What lowers the risk of heart at-
tacks?”. This type of reasoning has been recognized
as a core semantic inference task by the generic tex-
tual entailment framework (Dagan et al., 2006).
A major obstacle for further progress in seman-
tic inference is the lack of broad-scale knowledge-
bases for semantic variability patterns (Bar-Haim et
al., 2006). One prominent type of inference knowl-
edge representation is inference rules such as para-
phrases and entailment rules. We define an entail-
ment rule to be a directional relation between two
templates, text patterns with variables, e.g. ‘X pre-
vent Y → X lower the risk of Y ’. The left-hand-
side template is assumed to entail the right-hand-
side template in certain contexts, under the same
variable instantiation. Paraphrases can be viewed
as bidirectional entailment rules. Such rules capture
basic inferences and are used as building blocks for
more complex entailment inference. For example,
given the above rule, the answer “Aspirin” can be
identified in the example above.
The need for large-scale inference knowledge-
bases triggered extensive research on automatic ac-
quisition of paraphrase and entailment rules. Yet the
current precision of acquisition algorithms is typ-
ically still mediocre, as illustrated in Table 1 for
DIRT (Lin and Pantel, 2001) and TEASE (Szpek-
tor et al., 2004), two prominent acquisition algo-
rithms whose outputs are publicly available. The
current performance level only stresses the obvious
need for satisfactory evaluation methodologies that
would drive future research.
The prominent approach in the literature for eval-
uating rules, termed here the rule-based approach, is
to present the rules to human judges asking whether
each rule is correct or not. However, it is difficult to
explicitly define when a learned rule should be con-
sidered correct under this methodology, and this was
mainly left undefined in previous works. As the cri-
terion for evaluating a rule is not well defined, using
this approach often caused low agreement between
human judges. Indeed, the standards for evaluation
in this field are lower than other fields: many papers
</bodyText>
<page confidence="0.985875">
456
</page>
<note confidence="0.92546">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 456–463,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999755055555556">
don’t report on human agreement at all and those
that do report rather low agreement levels. Yet it
is crucial to reliably assess rule correctness in or-
der to measure and compare the performance of dif-
ferent algorithms in a replicable manner. Lacking a
good evaluation methodology has become a barrier
for further advances in the field.
In order to provide a well-defined evaluation
methodology we first explicitly specify when entail-
ment rules should be considered correct, following
the spirit of their usage in applications. We then
propose a new instance-based evaluation approach.
Under this scheme, judges are not presented only
with the rule but rather with a sample of sentences
that match its left hand side. The judges then assess
whether the rule holds under each specific example.
A rule is considered correct only if the percentage of
examples assessed as correct is sufficiently high.
We have experimented with a sample of input
verbs for both DIRT and TEASE. Our results show
significant improvement in human agreement over
the rule-based approach. It is also the first compar-
ison between such two state-of-the-art algorithms,
which showed that they are comparable in precision
but largely complementary in their coverage.
Additionally, the evaluation showed that both al-
gorithms learn mostly one-directional rules rather
than (symmetric) paraphrases. While most NLP ap-
plications need directional inference, previous ac-
quisition works typically expected that the learned
rules would be paraphrases. Under such an expec-
tation, unidirectional rules were assessed as incor-
rect, underestimating the true potential of these algo-
rithms. In addition, we observed that many learned
rules are context sensitive, stressing the need to learn
contextual constraints for rule applications.
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="method">
2 Background: Entailment Rules and their
Evaluation
</sectionHeader>
<subsectionHeader confidence="0.997914">
2.1 Entailment Rules
</subsectionHeader>
<bodyText confidence="0.995536">
An entailment rule ‘L → R’ is a directional rela-
tion between two templates, L and R. For exam-
ple, ‘X acquire Y → X own Y ’ or ‘X beat Y →
X play against Y ’. Templates correspond to text
fragments with variables, and are typically either lin-
ear phrases or parse sub-trees.
The goal of entailment rules is to help applica-
</bodyText>
<table confidence="0.988861428571429">
Input Correct Incorrect
X change Y (↔) X modify Y X adopt Y
(DIRT) (←) X amend Y X create Y
(←) X revise Y X stick to Y
X change Y (↔) X alter Y X maintain Y
(TEASE) (→) X affect Y X follow Y
(←) X extend Y X use Y
</table>
<tableCaption confidence="0.99026">
Table 1: Examples of templates suggested by DIRT
</tableCaption>
<bodyText confidence="0.985558297297297">
and TEASE as having an entailment relation, in
some direction, with the input template ‘X change
Y ’. The entailment direction arrows were judged
manually and added for readability.
tions infer one text variant from another. A rule can
be applied to a given text only when L can be in-
ferred from it, with appropriate variable instantia-
tion. Then, using the rule, the application deduces
that R can also be inferred from the text under the
same variable instantiation. For example, the rule
‘X lose to Y →Y beat X’ can be used to infer “Liv-
erpool beat Chelsea” from “Chelsea lost to Liver-
pool in the semifinals”.
Entailment rules should typically be applied only
in specific contexts, which we term relevant con-
texts. For example, the rule ‘X acquire Y →
X buy Y ’ can be used in the context of ‘buying’
events. However, it shouldn’t be applied for “Stu-
dents acquired a new language”. In the same man-
ner, the rule ‘X acquire Y → X learn Y ’ should be
applied only when Y corresponds to some sort of
knowledge, as in the latter example.
Some existing entailment acquisition algorithms
can add contextual constraints to the learned rules
(Sekine, 2005), but most don’t. However, NLP ap-
plications usually implicitly incorporate some con-
textual constraints when applying a rule. For ex-
ample, when answering the question “Which com-
panies did IBM buy?” a QA system would apply
the rule ‘X acquire Y → X buy Y ’ correctly, since
the phrase “IBM acquire X” is likely to be found
mostly in relevant economic contexts. We thus ex-
pect that an evaluation methodology should consider
context relevance for entailment rules. For example,
we would like both ‘X acquire Y → X buy Y ’ and
‘X acquire Y → X learn Y ’ to be assessed as cor-
rect (the second rule should not be deemed incorrect
</bodyText>
<page confidence="0.997384">
457
</page>
<bodyText confidence="0.999631">
just because it is not applicable in frequent economic
contexts).
Finally, we highlight that the common notion of
“paraphrase rules” can be viewed as a special case
of entailment rules: a paraphrase ‘L H R’ holds if
both templates entail each other. Following the tex-
tual entailment formulation, we observe that many
applied inference settings require only directional
entailment, and a requirement for symmetric para-
phrase is usually unnecessary. For example, in or-
der to answer the question “Who owns Overture?”
it suffices to use a directional entailment rule whose
right hand side is ‘X own Y ’, such as ‘X acquire
Y —*X own Y ’, which is clearly not a paraphrase.
</bodyText>
<subsectionHeader confidence="0.999746">
2.2 Evaluation of Acquisition Algorithms
</subsectionHeader>
<bodyText confidence="0.999984242857143">
Many methods for automatic acquisition of rules
have been suggested in recent years, ranging from
distributional similarity to finding shared contexts
(Lin and Pantel, 2001; Ravichandran and Hovy,
2002; Shinyama et al., 2002; Barzilay and Lee,
2003; Szpektor et al., 2004; Sekine, 2005). How-
ever, there is still no common accepted framework
for their evaluation. Furthermore, all these methods
learn rules as pairs of templates {L, R} in a sym-
metric manner, without addressing rule directional-
ity. Accordingly, previous works (except (Szpektor
et al., 2004)) evaluated the learned rules under the
paraphrase criterion, which underestimates the prac-
tical utility of the learned rules (see Section 2.1).
One approach which was used for evaluating au-
tomatically acquired rules is to measure their contri-
bution to the performance of specific systems, such
as QA (Ravichandran and Hovy, 2002) or IE (Sudo
et al., 2003; Romano et al., 2006). While measuring
the impact of learned rules on applications is highly
important, it cannot serve as the primary approach
for evaluating acquisition algorithms for several rea-
sons. First, developers of acquisition algorithms of-
ten do not have access to the different applications
that will later use the learned rules as generic mod-
ules. Second, the learned rules may affect individual
systems differently, thus making observations that
are based on different systems incomparable. Third,
within a complex system it is difficult to assess the
exact quality of entailment rules independently of
effects of other system components.
Thus, as in many other NLP learning settings,
a direct evaluation is needed. Indeed, the promi-
nent approach for evaluating the quality of rule ac-
quisition algorithms is by human judgment of the
learned rules (Lin and Pantel, 2001; Shinyama et
al., 2002; Barzilay and Lee, 2003; Pang et al., 2003;
Szpektor et al., 2004; Sekine, 2005). In this evalua-
tion scheme, termed here the rule-based approach, a
sample of the learned rules is presented to the judges
who evaluate whether each rule is correct or not. The
criterion for correctness is not explicitly described in
most previous works. By the common view of con-
text relevance for rules (see Section 2.1), a rule was
considered correct if the judge could think of rea-
sonable contexts under which it holds.
We have replicated the rule-based methodology
but did not manage to reach a 0.6 Kappa agree-
ment level between pairs of judges. This approach
turns out to be problematic because the rule correct-
ness criterion is not sufficiently well defined and is
hard to apply. While some rules might obviously
be judged as correct or incorrect (see Table 1), judg-
ment is often more difficult due to context relevance.
One judge might come up with a certain context
that, to her opinion, justifies the rule, while another
judge might not imagine that context or think that
it doesn’t sufficiently support rule correctness. For
example, in our experiments one of the judges did
not identify the valid “religious holidays” context
for the correct rule ‘X observe Y —*X celebrate Y ’.
Indeed, only few earlier works reported inter-judge
agreement level, and those that did reported rather
low Kappa values, such as 0.54 (Barzilay and Lee,
2003) and 0.55 - 0.63 (Szpektor et al., 2004).
To conclude, the prominent rule-based methodol-
ogy for entailment rule evaluation is not sufficiently
well defined. It results in low inter-judge agreement
which prevents reliable and consistent assessments
of different algorithms.
</bodyText>
<sectionHeader confidence="0.998178" genericHeader="method">
3 Instance-based Evaluation Methodology
</sectionHeader>
<bodyText confidence="0.999827857142857">
As discussed in Section 2.1, an evaluation methodol-
ogy for entailment rules should reflect the expected
validity of their application within NLP systems.
Following that line, an entailment rule ‘L —* R’
should be regarded as correct if in all (or at least
most) relevant contexts in which the instantiated
template L is inferred from the given text, the instan-
</bodyText>
<page confidence="0.99261">
458
</page>
<table confidence="0.825645222222222">
Rule Sentence Judgment
1 X seek Y —*X disclose Y If he is arrested, he can immediately seek bail. Left not entailed
2 X clarify Y —*X prepare Y He didn’t clarify his position on the subject. Left not entailed
3 X hit Y —*X approach Y Other earthquakes have hit Lebanon since ’82. Irrelevant context
4 X lose Y —*X surrender Y Bread has recently lost its subsidy. Irrelevant context
5 X regulate Y —*X reform Y The SRA regulates the sale of sugar. No entailment
6 X resign Y —*X share Y Lopez resigned his post at VW last week. No entailment
7 X set Y —*X allow Y The committee set the following refunds. Entailment holds
8 X stress Y —*X state Y Ben Yahia also stressed the need for action. Entailment holds
</table>
<tableCaption confidence="0.992099">
Table 2: Rule evaluation examples and their judgment.
</tableCaption>
<bodyText confidence="0.999657545454546">
tiated template R is also inferred from the text. This
reasoning corresponds to the common definition of
entailment in semantics, which specifies that a text
L entails another text R if R is true in every circum-
stance (possible world) in which L is true (Chierchia
and McConnell-Ginet, 2000).
It follows that in order to assess if a rule is cor-
rect we should judge whether R is typically en-
tailed from those sentences that entail L (within rel-
evant contexts for the rule). We thus present a new
evaluation scheme for entailment rules, termed the
instance-based approach. At the heart of this ap-
proach, human judges are presented not only with
a rule but rather with a sample of examples of the
rule’s usage. Instead of thinking up valid contexts
for the rule the judges need to assess the rule’s va-
lidity under the given context in each example. The
essence of our proposal is a (apparently non-trivial)
protocol of a sequence of questions, which deter-
mines rule validity in a given sentence.
We shall next describe how we collect a sample of
examples for evaluation and the evaluation process.
</bodyText>
<subsectionHeader confidence="0.999684">
3.1 Sampling Examples
</subsectionHeader>
<bodyText confidence="0.999936333333334">
Given a rule ‘L—*R’, our goal is to generate evalua-
tion examples by finding a sample of sentences from
which L is entailed. We do that by automatically re-
trieving, from a given corpus, sentences that match
L and are thus likely to entail it, as explained below.
For each example sentence, we automatically ex-
tract the arguments that instantiate L and generate
two phrases, termed left phrase and right phrase,
which are constructed by instantiating the left tem-
plate L and the right template R with the extracted
arguments. For example, the left and right phrases
generated for example 1 in Table 2 are “he seek bail”
and “he disclose bail”, respectively.
Finding sentences that match L can be performed
at different levels. In this paper we match lexical-
syntactic templates by finding a sub-tree of the sen-
tence parse that is identical to the template structure.
Of course, this matching method is not perfect and
will sometimes retrieve sentences that do not entail
the left phrase for various reasons, such as incorrect
sentence analysis or semantic aspects like negation,
modality and conditionals. See examples 1-2 in Ta-
ble 2 for sentences that syntactically match L but
do not entail the instantiated left phrase. Since we
should assess R’s entailment only from sentences
that entail L, such sentences should be ignored by
the evaluation process.
</bodyText>
<subsectionHeader confidence="0.998941">
3.2 Judgment Questions
</subsectionHeader>
<bodyText confidence="0.9998458125">
For each example generated for a rule, the judges are
presented with the given sentence and the left and
right phrases. They primarily answer two questions
that assess whether entailment holds in this example,
following the semantics of entailment rule applica-
tion as discussed above:
Qle: Is the left phrase entailed from the sentence?
A positive/negative answer corresponds to a
‘Left entailed/not entailed’ judgment.
Qre: Is the right phrase entailed from the sentence?
A positive/negative answer corresponds to an
‘Entailment holds/No entailment’ judgment.
The first question identifies sentences that do not en-
tail the left phrase, and thus should be ignored when
evaluating the rule’s correctness. While inappropri-
ate matches of the rule left-hand-side may happen
</bodyText>
<page confidence="0.996497">
459
</page>
<bodyText confidence="0.992663259259259">
and harm an overall system precision, such errors
should be accounted for a system’s rule matching
module rather than for the rules’ precision. The sec-
ond question assesses whether the rule application is
valid or not for the current example. See examples
5-8 in Table 2 for cases where entailment does or
doesn’t hold.
Thus, the judges focus only on the given sentence
in each example, so the task is actually to evaluate
whether textual entailment holds between the sen-
tence (text) and each of the left and right phrases
(hypotheses). Following past experience in textual
entailment evaluation (Dagan et al., 2006) we expect
a reasonable agreement level between judges.
As discussed in Section 2.1, we may want to ig-
nore examples whose context is irrelevant for the
rule. To optionally capture this distinction, the
judges are asked another question:
Qrc: Is the right phrase a likely phrase in English?
A positive/negative answer corresponds to a
‘Relevant/Irrelevant context’ evaluation.
If the right phrase is not likely in English then the
given context is probably irrelevant for the rule, be-
cause it seems inherently incorrect to infer an im-
plausible phrase. Examples 3-4 in Table 2 demon-
strate cases of irrelevant contexts, which we may
choose to ignore when assessing rule correctness.
</bodyText>
<subsectionHeader confidence="0.958626">
3.3 Evaluation Process
</subsectionHeader>
<bodyText confidence="0.99995865">
For each example, the judges are presented with the
three questions above in the following order: (1) Qle
(2) Qrc (3) Qre. If the answer to a certain question
is negative then we do not need to present the next
questions to the judge: if the left phrase is not en-
tailed then we ignore the sentence altogether; and if
the context is irrelevant then the right phrase cannot
be entailed from the sentence and so the answer to
Qre is already known as negative.
The above entailment judgments assume that we
can actually ask whether the left or right phrases
are correct given the sentence, that is, we assume
that a truth value can be assigned to both phrases.
This is the case when the left and right templates
correspond, as expected, to semantic relations. Yet
sometimes learned templates are (erroneously) not
relational, e.g. ‘X, Y , IBM’ (representing a list).
We therefore let the judges initially mark rules that
include such templates as non-relational, in which
case their examples are not evaluated at all.
</bodyText>
<subsectionHeader confidence="0.96444">
3.4 Rule Precision
</subsectionHeader>
<bodyText confidence="0.999726076923077">
We compute the precision of a rule by the percent-
age of examples for which entailment holds out
of all “relevant” examples. We can calculate the
precision in two ways, as defined below, depending
on whether we ignore irrelevant contexts or not
(obtaining lower precision if we don’t). When
systems answer an information need, such as a
query or question, irrelevant contexts are sometimes
not encountered thanks to additional context which
is present in the given input (see Section 2.1). Thus,
the following two measures can be viewed as upper
and lower bounds for the expected precision of the
rule applications in actual systems:
</bodyText>
<figure confidence="0.9469005">
#Entailment holds
upper bound precision:
#Relevant context
#Entailment holds
</figure>
<figureCaption confidence="0.33447">
lower bound precision: #Left entailed
</figureCaption>
<bodyText confidence="0.999964666666667">
where # denotes the number of examples with
the corresponding judgment.
Finally, we consider a rule to be correct only if
its precision is at least 80%, which seems sensible
for typical applied settings. This yields two alterna-
tive sets of correct rules, corresponding to the upper
bound and lower bound precision measures. Even
though judges may disagree on specific examples for
a rule, their judgments may still agree overall on the
rule’s correctness. We therefore expect the agree-
ment level on rule correctness to be higher than the
agreement on individual examples.
</bodyText>
<sectionHeader confidence="0.997947" genericHeader="method">
4 Experimental Settings
</sectionHeader>
<bodyText confidence="0.999943">
We applied the instance-based methodology to eval-
uate two state-of-the-art unsupervised acquisition al-
gorithms, DIRT (Lin and Pantel, 2001) and TEASE
(Szpektor et al., 2004), whose output is publicly
available. DIRT identifies semantically related tem-
plates in a local corpus using distributional sim-
ilarity over the templates’ variable instantiations.
TEASE acquires entailment relations from the Web
for a given input template I by identifying charac-
teristic variable instantiations shared by I and other
templates.
</bodyText>
<page confidence="0.977728">
460
</page>
<bodyText confidence="0.984536569620253">
For the experiment we used the published DIRT explained in Section 3.1). A sample of 15 match-
and TEASE knowledge-bases1. For every given in- ing sentences was randomly selected, or all match-
put template I, each knowledge-base provides a list ing sentences if less than 15 were found. Finally,
of learned output templates {Oj}nI an example for judgment was generated from each
1 , where nI is the sampled sentence and its left and right phrases (see
number of output templates learned for I. Each out- Section 3.1). We did not find sentences for 108
put template is suggested as holding an entailment rules, and thus we ended up with 646 unique rules
relation with the input template I, but the algorithms that could be evaluated (with 8945 examples to be
do not specify the entailment direction(s). Thus, judged).
each pair {I, Oj} induces two candidate directional 4.2 Evaluating the Test-Set
entailment rules: ‘I →Oj’ and ‘Oj →I’. Two human judges evaluated the examples. We
randomly split the examples between the judges.
100 rules (1287 examples) were cross annotated for
agreement measurement. The judges followed the
procedure in Section 3.3 and the correctness of each
rule was assessed based on both its upper and lower
bound precision values (Section 3.4).
5 Methodology Evaluation Results
We assessed the instance-based methodology by
measuring the agreement level between judges. The
judges agreed on 75% of the 1287 shared exam-
ples, corresponding to a reasonable Kappa value of
0.64. A similar kappa value of 0.65 was obtained
for the examples that were judged as either entail-
ment holds/no entailment by both judges. Yet, our
evaluation target is to assess rules, and the Kappa
values for the final correctness judgments of the
shared rules were 0.74 and 0.68 for the lower and
upper bound evaluations. These Kappa scores are
regarded as ‘substantial agreement’ and are substan-
tially higher than published agreement scores and
those we managed to obtain using the standard rule-
based approach. As expected, the agreement on
rules is higher than on examples, since judges may
disagree on a certain example but their judgements
would still yield the same rule assessment.
Table 3 illustrates some disagreements that were
still exhibited within the instance-based evaluation.
The primary reason for disagreements was the dif-
ficulty to decide whether a context is relevant for
a rule or not, resulting in some confusion between
‘Irrelevant context’ and ‘No entailment’. This may
explain the lower agreement for the upper bound
precision, for which examples judged as ’Irrelevant
context’ are ignored, while for the lower bound both
4.1 Test Set Construction
The test set construction consists of three sampling
steps: selecting a set of input templates for the two
algorithms, selecting a sample of output rules to be
evaluated, and selecting a sample of sentences to be
judged for each rule.
First, we randomly selected 30 transitive verbs
out of the 1000 most frequent verbs in the Reuters
RCV1 corpus2. For each verb we manually
constructed a lexical-syntactic input template by
adding subject and object variables. For exam-
ple, for the verb ‘seek’ we constructed the template
‘X subj ←−−seek obj −−→ Y ’.
Next, for each input template I we considered
the learned templates {Oj}nI
1 from each knowledge-
base. Since DIRT has a long tail of templates with
a low score and very low precision, DIRT templates
whose score is below a threshold of 0.1 were filtered
out3. We then sampled 10% of the templates in each
output list, limiting the sample size to be between
5-20 templates for each list (thus balancing between
sufficient evaluation data and judgment load). For
each sampled template O we evaluated both direc-
tional rules, ‘I → O’ and ‘O → I’. In total, we sam-
pled 380 templates, inducing 760 directional rules
out of which 754 rules were unique.
Last, we randomly extracted a sample of example
sentences for each rule ‘L→R’ by utilizing a search
engine over the first CD of Reuters RCV1. First, we
retrieved all sentences containing all lexical terms
within L. The retrieved sentences were parsed using
the Minipar dependency parser (Lin, 1998), keep-
ing only sentences that syntactically match L (as
</bodyText>
<table confidence="0.928368">
1Available at http://aclweb.org/aclwiki/index.php?title=Te-
xtual Entailment Resource Pool
2http://about.reuters.com/researchandstandards/corpus/
3Following advice by Patrick Pantel, DIRT’s co-author.
461
Rule Sentence Judge 1 Judge 2
X sign Y —*X set Y Iraq and Turkey sign agreement Entailment holds Irrelevant context
to increase trade cooperation
X worsen Y —*X slow Y News of the strike worsened the Irrelevant context No entailment
situation
X get Y —*X want Y He will get his parade on Tuesday Entailment holds No entailment
</table>
<tableCaption confidence="0.998346">
Table 3: Examples for disagreement between the two judges.
</tableCaption>
<bodyText confidence="0.999920166666667">
judgments are conflated and represent no entailment.
Our findings suggest that better ways for distin-
guishing relevant contexts may be sought in future
research for further refinement of the instance-based
evaluation methodology.
About 43% of all examples were judged as ’Left
not entailed’. The relatively low matching precision
(57%) made us collect more examples than needed,
since ’Left not entailed’ examples are ignored. Bet-
ter matching capabilities will allow collecting and
judging fewer examples, thus improving the effi-
ciency of the evaluation process.
</bodyText>
<sectionHeader confidence="0.982638" genericHeader="evaluation">
6 DIRT and TEASE Evaluation Results
</sectionHeader>
<table confidence="0.99639375">
DIRT TEASE
P Y P Y
Rules:
Upper Bound 30.5% 33.5 28.4% 40.3
Lower Bound 18.6% 20.4 17% 24.1
Templates:
Upper Bound 44% 22.6 38% 26.9
Lower Bound 27.3% 14.1 23.6% 16.8
</table>
<tableCaption confidence="0.9655225">
Table 4: Average Precision (P) and Yield (Y) at the
rule and template levels.
</tableCaption>
<bodyText confidence="0.999529625">
We evaluated the quality of the entailment rules
produced by each algorithm using two scores: (1)
micro average Precision, the percentage of correct
rules out of all learned rules, and (2) average Yield,
the average number of correct rules learned for each
input template I, as extrapolated based on the sam-
ple4. Since DIRT and TEASE do not identify rule
directionality, we also measured these scores at the
</bodyText>
<footnote confidence="0.943832">
4Since the rules are matched against the full corpus (as in IR
evaluations), it is difficult to evaluate their true recall.
</footnote>
<bodyText confidence="0.999888394736842">
template level, where an output template O is con-
sidered correct if at least one of the rules ‘I —*O’ or
‘O —* I’ is correct. The results are presented in Ta-
ble 4. The major finding is that the overall quality of
DIRT and TEASE is very similar. Under the specific
DIRT cutoff threshold chosen, DIRT exhibits some-
what higher Precision while TEASE has somewhat
higher Yield (recall that there is no particular natural
cutoff point for DIRT’s output).
Since applications typically apply rules in a spe-
cific direction, the Precision for rules reflects their
expected performance better than the Precision for
templates. Obviously, future improvement in pre-
cision is needed for rule learning algorithms. Mean-
while, manual filtering of the learned rules can prove
effective within limited domains, where our evalua-
tion approach can be utilized for reliable filtering as
well. The substantial yield obtained by these algo-
rithms suggest that they are indeed likely to be valu-
able for recall increase in semantic applications.
In addition, we found that only about 15% of the
correct templates were learned by both algorithms,
which implies that the two algorithms largely com-
plement each other in terms of coverage. One ex-
planation may be that DIRT is focused on the do-
main of the local corpus used (news articles for the
published DIRT knowledge-base), whereas TEASE
learns from the Web, extracting rules from multiple
domains. Since Precision is comparable it may be
best to use both algorithms in tandem.
We also measured whether O is a paraphrase of
I, i.e. whether both ‘I —* O’ and ‘O —* I’ are cor-
rect. Only 20-25% of all correct templates were as-
sessed as paraphrases. This stresses the significance
of evaluating directional rules rather than only para-
phrases. Furthermore, it shows that in order to im-
prove precision, acquisition algorithms must iden-
tify rule directionality.
</bodyText>
<page confidence="0.997826">
462
</page>
<bodyText confidence="0.999670153846154">
About 28% of all ‘Left entailed’ examples were
evaluated as ‘Irrelevant context’, yielding the large
difference in precision between the upper and lower
precision bounds. This result shows that in order
to get closer to the upper bound precision, learning
algorithms and applications need to identify the rel-
evant contexts in which a rule should be applied.
Last, we note that the instance-based quality as-
sessment corresponds to the corpus from which the
example sentences were taken. It is therefore best to
evaluate the rules using a corpus of the same domain
from which they were learned, or the target applica-
tion domain for which the rules will be applied.
</bodyText>
<sectionHeader confidence="0.999807" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999821125">
Accurate learning of inference knowledge, such as
entailment rules, has become critical for further
progress of applied semantic systems. However,
evaluation of such knowledge has been problematic,
hindering further developments. The instance-based
evaluation approach proposed in this paper obtained
acceptable agreement levels, which are substantially
higher than those obtained for the common rule-
based approach.
We also conducted the first comparison between
two state-of-the-art acquisition algorithms, DIRT
and TEASE, using the new methodology. We found
that their quality is comparable but they effectively
complement each other in terms of rule coverage.
Also, we found that most learned rules are not para-
phrases but rather one-directional entailment rules,
and that many of the rules are context sensitive.
These findings suggest interesting directions for fu-
ture research, in particular learning rule direction-
ality and relevant contexts, issues that were hardly
explored till now. Such developments can be then
evaluated by the instance-based methodology, which
was designed to capture these two important aspects
of entailment rules.
</bodyText>
<sectionHeader confidence="0.998711" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999661">
The authors would like to thank Ephi Sachs and
Iddo Greental for their evaluation. This work was
partially supported by ISF grant 1095/05, the IST
Programme of the European Community under the
PASCAL Network of Excellence IST-2002-506778,
and the ITC-irst/University of Haifa collaboration.
</bodyText>
<sectionHeader confidence="0.998452" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999925822222222">
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Second PASCAL Challenge Work-
shop for Recognizing Textual Entailment.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings ofNAACL-HLT.
Gennaro Chierchia and Sally McConnell-Ginet. 2000.
Meaning and Grammar (2nd ed.): an introduction to
semantics. MIT Press, Cambridge, MA.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. Lecture Notes in Computer Science, 3944:177–
190.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343–360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings ofHLT-NAACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings ofACL.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido
Dagan, and Alberto Lavelli. 2006. Investigating a
generic paraphrase-based approach for relation extrac-
tion. In Proceedings of EACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings ofIWP.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings ofHLT.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic IE pattern acquisition. In Pro-
ceedings ofACL.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
</reference>
<page confidence="0.999714">
463
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.898536">
<title confidence="0.999719">Instance-based Evaluation of Entailment Rule Acquisition</title>
<author confidence="0.990925">Idan Szpektor</author>
<author confidence="0.990925">Eyal Shnarch</author>
<author confidence="0.990925">Ido Dagan</author>
<affiliation confidence="0.994396">Dept. of Computer Science Bar Ilan University</affiliation>
<address confidence="0.946361">Ramat Gan, Israel</address>
<abstract confidence="0.997825333333333">Obtaining large volumes of inference knowledge, such as entailment rules, has become a major factor in achieving robust semantic processing. While there has been substantial research on learning algorithms for such knowledge, their evaluation methodology has been problematic, hindering further research. We propose a novel evaluation methodology for entailment rules which explicitly addresses their semantic properties and yields satisfactory human agreement levels. The methodology is used to compare two state of the art learning algorithms, exposing critical issues for future progress.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The second pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Second PASCAL Challenge Workshop for Recognizing Textual Entailment.</booktitle>
<contexts>
<context position="1527" citStr="Bar-Haim et al., 2006" startWordPosition="231" endWordPosition="234">s crucial to recognize that a particular target meaning can be inferred from different text variants. For example, a QA system needs to identify that “Aspirin lowers the risk ofheart attacks” can be inferred from “Aspirin prevents heart attacks” in order to answer the question “What lowers the risk of heart attacks?”. This type of reasoning has been recognized as a core semantic inference task by the generic textual entailment framework (Dagan et al., 2006). A major obstacle for further progress in semantic inference is the lack of broad-scale knowledgebases for semantic variability patterns (Bar-Haim et al., 2006). One prominent type of inference knowledge representation is inference rules such as paraphrases and entailment rules. We define an entailment rule to be a directional relation between two templates, text patterns with variables, e.g. ‘X prevent Y → X lower the risk of Y ’. The left-handside template is assumed to entail the right-handside template in certain contexts, under the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences and are used as building blocks for more complex entailment inference. For example, given t</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising textual entailment challenge. In Second PASCAL Challenge Workshop for Recognizing Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings ofNAACL-HLT.</booktitle>
<contexts>
<context position="8681" citStr="Barzilay and Lee, 2003" startWordPosition="1426" endWordPosition="1429">ettings require only directional entailment, and a requirement for symmetric paraphrase is usually unnecessary. For example, in order to answer the question “Who owns Overture?” it suffices to use a directional entailment rule whose right hand side is ‘X own Y ’, such as ‘X acquire Y —*X own Y ’, which is clearly not a paraphrase. 2.2 Evaluation of Acquisition Algorithms Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005). However, there is still no common accepted framework for their evaluation. Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directionality. Accordingly, previous works (except (Szpektor et al., 2004)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1). One approach which was used for evaluating automatically acquired rules is to measure their contribution to the performance of specific systems, s</context>
<context position="10274" citStr="Barzilay and Lee, 2003" startWordPosition="1677" endWordPosition="1680"> will later use the learned rules as generic modules. Second, the learned rules may affect individual systems differently, thus making observations that are based on different systems incomparable. Third, within a complex system it is difficult to assess the exact quality of entailment rules independently of effects of other system components. Thus, as in many other NLP learning settings, a direct evaluation is needed. Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005). In this evaluation scheme, termed here the rule-based approach, a sample of the learned rules is presented to the judges who evaluate whether each rule is correct or not. The criterion for correctness is not explicitly described in most previous works. By the common view of context relevance for rules (see Section 2.1), a rule was considered correct if the judge could think of reasonable contexts under which it holds. We have replicated the rule-based methodology but did not manage to reach a 0.6 Kappa agreement level between pairs of </context>
<context position="11681" citStr="Barzilay and Lee, 2003" startWordPosition="1915" endWordPosition="1918">ed as correct or incorrect (see Table 1), judgment is often more difficult due to context relevance. One judge might come up with a certain context that, to her opinion, justifies the rule, while another judge might not imagine that context or think that it doesn’t sufficiently support rule correctness. For example, in our experiments one of the judges did not identify the valid “religious holidays” context for the correct rule ‘X observe Y —*X celebrate Y ’. Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 - 0.63 (Szpektor et al., 2004). To conclude, the prominent rule-based methodology for entailment rule evaluation is not sufficiently well defined. It results in low inter-judge agreement which prevents reliable and consistent assessments of different algorithms. 3 Instance-based Evaluation Methodology As discussed in Section 2.1, an evaluation methodology for entailment rules should reflect the expected validity of their application within NLP systems. Following that line, an entailment rule ‘L —* R’ should be regarded as correct if in all (or at least most) relevant contexts in whic</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiplesequence alignment. In Proceedings ofNAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gennaro Chierchia</author>
<author>Sally McConnell-Ginet</author>
</authors>
<title>Meaning and Grammar (2nd ed.): an introduction to semantics.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="13411" citStr="Chierchia and McConnell-Ginet, 2000" startWordPosition="2212" endWordPosition="2215"> regulates the sale of sugar. No entailment 6 X resign Y —*X share Y Lopez resigned his post at VW last week. No entailment 7 X set Y —*X allow Y The committee set the following refunds. Entailment holds 8 X stress Y —*X state Y Ben Yahia also stressed the need for action. Entailment holds Table 2: Rule evaluation examples and their judgment. tiated template R is also inferred from the text. This reasoning corresponds to the common definition of entailment in semantics, which specifies that a text L entails another text R if R is true in every circumstance (possible world) in which L is true (Chierchia and McConnell-Ginet, 2000). It follows that in order to assess if a rule is correct we should judge whether R is typically entailed from those sentences that entail L (within relevant contexts for the rule). We thus present a new evaluation scheme for entailment rules, termed the instance-based approach. At the heart of this approach, human judges are presented not only with a rule but rather with a sample of examples of the rule’s usage. Instead of thinking up valid contexts for the rule the judges need to assess the rule’s validity under the given context in each example. The essence of our proposal is a (apparently </context>
</contexts>
<marker>Chierchia, McConnell-Ginet, 2000</marker>
<rawString>Gennaro Chierchia and Sally McConnell-Ginet. 2000. Meaning and Grammar (2nd ed.): an introduction to semantics. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2006</date>
<journal>Lecture Notes in Computer Science,</journal>
<volume>3944</volume>
<pages>190</pages>
<contexts>
<context position="1366" citStr="Dagan et al., 2006" startWordPosition="206" endWordPosition="209">, exposing critical issues for future progress. 1 Introduction In many NLP applications, such as Question Answering (QA) and Information Extraction (IE), it is crucial to recognize that a particular target meaning can be inferred from different text variants. For example, a QA system needs to identify that “Aspirin lowers the risk ofheart attacks” can be inferred from “Aspirin prevents heart attacks” in order to answer the question “What lowers the risk of heart attacks?”. This type of reasoning has been recognized as a core semantic inference task by the generic textual entailment framework (Dagan et al., 2006). A major obstacle for further progress in semantic inference is the lack of broad-scale knowledgebases for semantic variability patterns (Bar-Haim et al., 2006). One prominent type of inference knowledge representation is inference rules such as paraphrases and entailment rules. We define an entailment rule to be a directional relation between two templates, text patterns with variables, e.g. ‘X prevent Y → X lower the risk of Y ’. The left-handside template is assumed to entail the right-handside template in certain contexts, under the same variable instantiation. Paraphrases can be viewed a</context>
<context position="17003" citStr="Dagan et al., 2006" startWordPosition="2805" endWordPosition="2808">59 and harm an overall system precision, such errors should be accounted for a system’s rule matching module rather than for the rules’ precision. The second question assesses whether the rule application is valid or not for the current example. See examples 5-8 in Table 2 for cases where entailment does or doesn’t hold. Thus, the judges focus only on the given sentence in each example, so the task is actually to evaluate whether textual entailment holds between the sentence (text) and each of the left and right phrases (hypotheses). Following past experience in textual entailment evaluation (Dagan et al., 2006) we expect a reasonable agreement level between judges. As discussed in Section 2.1, we may want to ignore examples whose context is irrelevant for the rule. To optionally capture this distinction, the judges are asked another question: Qrc: Is the right phrase a likely phrase in English? A positive/negative answer corresponds to a ‘Relevant/Irrelevant context’ evaluation. If the right phrase is not likely in English then the given context is probably irrelevant for the rule, because it seems inherently incorrect to infer an implausible phrase. Examples 3-4 in Table 2 demonstrate cases of irre</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. Lecture Notes in Computer Science, 3944:177– 190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="2481" citStr="Lin and Pantel, 2001" startWordPosition="382" endWordPosition="385">-handside template in certain contexts, under the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences and are used as building blocks for more complex entailment inference. For example, given the above rule, the answer “Aspirin” can be identified in the example above. The need for large-scale inference knowledgebases triggered extensive research on automatic acquisition of paraphrase and entailment rules. Yet the current precision of acquisition algorithms is typically still mediocre, as illustrated in Table 1 for DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al., 2004), two prominent acquisition algorithms whose outputs are publicly available. The current performance level only stresses the obvious need for satisfactory evaluation methodologies that would drive future research. The prominent approach in the literature for evaluating rules, termed here the rule-based approach, is to present the rules to human judges asking whether each rule is correct or not. However, it is difficult to explicitly define when a learned rule should be considered correct under this methodology, and this was mainly left undefined in previous wo</context>
<context position="8605" citStr="Lin and Pantel, 2001" startWordPosition="1414" endWordPosition="1417">e textual entailment formulation, we observe that many applied inference settings require only directional entailment, and a requirement for symmetric paraphrase is usually unnecessary. For example, in order to answer the question “Who owns Overture?” it suffices to use a directional entailment rule whose right hand side is ‘X own Y ’, such as ‘X acquire Y —*X own Y ’, which is clearly not a paraphrase. 2.2 Evaluation of Acquisition Algorithms Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005). However, there is still no common accepted framework for their evaluation. Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directionality. Accordingly, previous works (except (Szpektor et al., 2004)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1). One approach which was used for evaluating automatically acquired rule</context>
<context position="10227" citStr="Lin and Pantel, 2001" startWordPosition="1669" endWordPosition="1672">ave access to the different applications that will later use the learned rules as generic modules. Second, the learned rules may affect individual systems differently, thus making observations that are based on different systems incomparable. Third, within a complex system it is difficult to assess the exact quality of entailment rules independently of effects of other system components. Thus, as in many other NLP learning settings, a direct evaluation is needed. Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005). In this evaluation scheme, termed here the rule-based approach, a sample of the learned rules is presented to the judges who evaluate whether each rule is correct or not. The criterion for correctness is not explicitly described in most previous works. By the common view of context relevance for rules (see Section 2.1), a rule was considered correct if the judge could think of reasonable contexts under which it holds. We have replicated the rule-based methodology but did not manage to reac</context>
<context position="20222" citStr="Lin and Pantel, 2001" startWordPosition="3329" endWordPosition="3332">only if its precision is at least 80%, which seems sensible for typical applied settings. This yields two alternative sets of correct rules, corresponding to the upper bound and lower bound precision measures. Even though judges may disagree on specific examples for a rule, their judgments may still agree overall on the rule’s correctness. We therefore expect the agreement level on rule correctness to be higher than the agreement on individual examples. 4 Experimental Settings We applied the instance-based methodology to evaluate two state-of-the-art unsupervised acquisition algorithms, DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al., 2004), whose output is publicly available. DIRT identifies semantically related templates in a local corpus using distributional similarity over the templates’ variable instantiations. TEASE acquires entailment relations from the Web for a given input template I by identifying characteristic variable instantiations shared by I and other templates. 460 For the experiment we used the published DIRT explained in Section 3.1). A sample of 15 matchand TEASE knowledge-bases1. For every given in- ing sentences was randomly selected, or all matchput template I, each knowle</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of minipar.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Evaluation of Parsing Systems at LREC.</booktitle>
<contexts>
<context position="24764" citStr="Lin, 1998" startWordPosition="4074" endWordPosition="4075">ple size to be between 5-20 templates for each list (thus balancing between sufficient evaluation data and judgment load). For each sampled template O we evaluated both directional rules, ‘I → O’ and ‘O → I’. In total, we sampled 380 templates, inducing 760 directional rules out of which 754 rules were unique. Last, we randomly extracted a sample of example sentences for each rule ‘L→R’ by utilizing a search engine over the first CD of Reuters RCV1. First, we retrieved all sentences containing all lexical terms within L. The retrieved sentences were parsed using the Minipar dependency parser (Lin, 1998), keeping only sentences that syntactically match L (as 1Available at http://aclweb.org/aclwiki/index.php?title=Textual Entailment Resource Pool 2http://about.reuters.com/researchandstandards/corpus/ 3Following advice by Patrick Pantel, DIRT’s co-author. 461 Rule Sentence Judge 1 Judge 2 X sign Y —*X set Y Iraq and Turkey sign agreement Entailment holds Irrelevant context to increase trade cooperation X worsen Y —*X slow Y News of the strike worsened the Irrelevant context No entailment situation X get Y —*X want Y He will get his parade on Tuesday Entailment holds No entailment Table 3: Examp</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of minipar. In Proceedings of the Workshop on Evaluation of Parsing Systems at LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="10293" citStr="Pang et al., 2003" startWordPosition="1681" endWordPosition="1684">ned rules as generic modules. Second, the learned rules may affect individual systems differently, thus making observations that are based on different systems incomparable. Third, within a complex system it is difficult to assess the exact quality of entailment rules independently of effects of other system components. Thus, as in many other NLP learning settings, a direct evaluation is needed. Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005). In this evaluation scheme, termed here the rule-based approach, a sample of the learned rules is presented to the judges who evaluate whether each rule is correct or not. The criterion for correctness is not explicitly described in most previous works. By the common view of context relevance for rules (see Section 2.1), a rule was considered correct if the judge could think of reasonable contexts under which it holds. We have replicated the rule-based methodology but did not manage to reach a 0.6 Kappa agreement level between pairs of judges. This approa</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8634" citStr="Ravichandran and Hovy, 2002" startWordPosition="1418" endWordPosition="1421">ormulation, we observe that many applied inference settings require only directional entailment, and a requirement for symmetric paraphrase is usually unnecessary. For example, in order to answer the question “Who owns Overture?” it suffices to use a directional entailment rule whose right hand side is ‘X own Y ’, such as ‘X acquire Y —*X own Y ’, which is clearly not a paraphrase. 2.2 Evaluation of Acquisition Algorithms Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005). However, there is still no common accepted framework for their evaluation. Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directionality. Accordingly, previous works (except (Szpektor et al., 2004)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1). One approach which was used for evaluating automatically acquired rules is to measure their contrib</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorenza Romano</author>
<author>Milen Kouylekov</author>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
<author>Alberto Lavelli</author>
</authors>
<title>Investigating a generic paraphrase-based approach for relation extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="9367" citStr="Romano et al., 2006" startWordPosition="1535" endWordPosition="1538"> common accepted framework for their evaluation. Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directionality. Accordingly, previous works (except (Szpektor et al., 2004)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1). One approach which was used for evaluating automatically acquired rules is to measure their contribution to the performance of specific systems, such as QA (Ravichandran and Hovy, 2002) or IE (Sudo et al., 2003; Romano et al., 2006). While measuring the impact of learned rules on applications is highly important, it cannot serve as the primary approach for evaluating acquisition algorithms for several reasons. First, developers of acquisition algorithms often do not have access to the different applications that will later use the learned rules as generic modules. Second, the learned rules may affect individual systems differently, thus making observations that are based on different systems incomparable. Third, within a complex system it is difficult to assess the exact quality of entailment rules independently of effec</context>
</contexts>
<marker>Romano, Kouylekov, Szpektor, Dagan, Lavelli, 2006</marker>
<rawString>Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido Dagan, and Alberto Lavelli. 2006. Investigating a generic paraphrase-based approach for relation extraction. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>Automatic paraphrase discovery based on context and keywords between ne pairs.</title>
<date>2005</date>
<booktitle>In Proceedings ofIWP.</booktitle>
<contexts>
<context position="7096" citStr="Sekine, 2005" startWordPosition="1157" endWordPosition="1158">Liverpool beat Chelsea” from “Chelsea lost to Liverpool in the semifinals”. Entailment rules should typically be applied only in specific contexts, which we term relevant contexts. For example, the rule ‘X acquire Y → X buy Y ’ can be used in the context of ‘buying’ events. However, it shouldn’t be applied for “Students acquired a new language”. In the same manner, the rule ‘X acquire Y → X learn Y ’ should be applied only when Y corresponds to some sort of knowledge, as in the latter example. Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most don’t. However, NLP applications usually implicitly incorporate some contextual constraints when applying a rule. For example, when answering the question “Which companies did IBM buy?” a QA system would apply the rule ‘X acquire Y → X buy Y ’ correctly, since the phrase “IBM acquire X” is likely to be found mostly in relevant economic contexts. We thus expect that an evaluation methodology should consider context relevance for entailment rules. For example, we would like both ‘X acquire Y → X buy Y ’ and ‘X acquire Y → X learn Y ’ to be assessed as correct (the second rule should n</context>
<context position="8719" citStr="Sekine, 2005" startWordPosition="1434" endWordPosition="1435">d a requirement for symmetric paraphrase is usually unnecessary. For example, in order to answer the question “Who owns Overture?” it suffices to use a directional entailment rule whose right hand side is ‘X own Y ’, such as ‘X acquire Y —*X own Y ’, which is clearly not a paraphrase. 2.2 Evaluation of Acquisition Algorithms Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005). However, there is still no common accepted framework for their evaluation. Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directionality. Accordingly, previous works (except (Szpektor et al., 2004)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1). One approach which was used for evaluating automatically acquired rules is to measure their contribution to the performance of specific systems, such as QA (Ravichandran and Hovy, 2002</context>
<context position="10331" citStr="Sekine, 2005" startWordPosition="1689" endWordPosition="1690">learned rules may affect individual systems differently, thus making observations that are based on different systems incomparable. Third, within a complex system it is difficult to assess the exact quality of entailment rules independently of effects of other system components. Thus, as in many other NLP learning settings, a direct evaluation is needed. Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005). In this evaluation scheme, termed here the rule-based approach, a sample of the learned rules is presented to the judges who evaluate whether each rule is correct or not. The criterion for correctness is not explicitly described in most previous works. By the common view of context relevance for rules (see Section 2.1), a rule was considered correct if the judge could think of reasonable contexts under which it holds. We have replicated the rule-based methodology but did not manage to reach a 0.6 Kappa agreement level between pairs of judges. This approach turns out to be problematic because</context>
</contexts>
<marker>Sekine, 2005</marker>
<rawString>Satoshi Sekine. 2005. Automatic paraphrase discovery based on context and keywords between ne pairs. In Proceedings ofIWP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
<author>Ralph Grishman</author>
</authors>
<title>Automatic paraphrase acquisition from news articles.</title>
<date>2002</date>
<booktitle>In Proceedings ofHLT.</booktitle>
<contexts>
<context position="8657" citStr="Shinyama et al., 2002" startWordPosition="1422" endWordPosition="1425">any applied inference settings require only directional entailment, and a requirement for symmetric paraphrase is usually unnecessary. For example, in order to answer the question “Who owns Overture?” it suffices to use a directional entailment rule whose right hand side is ‘X own Y ’, such as ‘X acquire Y —*X own Y ’, which is clearly not a paraphrase. 2.2 Evaluation of Acquisition Algorithms Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005). However, there is still no common accepted framework for their evaluation. Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directionality. Accordingly, previous works (except (Szpektor et al., 2004)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1). One approach which was used for evaluating automatically acquired rules is to measure their contribution to the performanc</context>
<context position="10250" citStr="Shinyama et al., 2002" startWordPosition="1673" endWordPosition="1676">erent applications that will later use the learned rules as generic modules. Second, the learned rules may affect individual systems differently, thus making observations that are based on different systems incomparable. Third, within a complex system it is difficult to assess the exact quality of entailment rules independently of effects of other system components. Thus, as in many other NLP learning settings, a direct evaluation is needed. Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005). In this evaluation scheme, termed here the rule-based approach, a sample of the learned rules is presented to the judges who evaluate whether each rule is correct or not. The criterion for correctness is not explicitly described in most previous works. By the common view of context relevance for rules (see Section 2.1), a rule was considered correct if the judge could think of reasonable contexts under which it holds. We have replicated the rule-based methodology but did not manage to reach a 0.6 Kappa agreement</context>
</contexts>
<marker>Shinyama, Sekine, Sudo, Grishman, 2002</marker>
<rawString>Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and Ralph Grishman. 2002. Automatic paraphrase acquisition from news articles. In Proceedings ofHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Sudo</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>An improved extraction pattern representation model for automatic IE pattern acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="9345" citStr="Sudo et al., 2003" startWordPosition="1531" endWordPosition="1534">, there is still no common accepted framework for their evaluation. Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directionality. Accordingly, previous works (except (Szpektor et al., 2004)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1). One approach which was used for evaluating automatically acquired rules is to measure their contribution to the performance of specific systems, such as QA (Ravichandran and Hovy, 2002) or IE (Sudo et al., 2003; Romano et al., 2006). While measuring the impact of learned rules on applications is highly important, it cannot serve as the primary approach for evaluating acquisition algorithms for several reasons. First, developers of acquisition algorithms often do not have access to the different applications that will later use the learned rules as generic modules. Second, the learned rules may affect individual systems differently, thus making observations that are based on different systems incomparable. Third, within a complex system it is difficult to assess the exact quality of entailment rules </context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2003</marker>
<rawString>Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2003. An improved extraction pattern representation model for automatic IE pattern acquisition. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Hristo Tanev</author>
<author>Ido Dagan</author>
<author>Bonaventura Coppola</author>
</authors>
<title>Scaling web-based acquisition of entailment relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2515" citStr="Szpektor et al., 2004" startWordPosition="388" endWordPosition="392">texts, under the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences and are used as building blocks for more complex entailment inference. For example, given the above rule, the answer “Aspirin” can be identified in the example above. The need for large-scale inference knowledgebases triggered extensive research on automatic acquisition of paraphrase and entailment rules. Yet the current precision of acquisition algorithms is typically still mediocre, as illustrated in Table 1 for DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al., 2004), two prominent acquisition algorithms whose outputs are publicly available. The current performance level only stresses the obvious need for satisfactory evaluation methodologies that would drive future research. The prominent approach in the literature for evaluating rules, termed here the rule-based approach, is to present the rules to human judges asking whether each rule is correct or not. However, it is difficult to explicitly define when a learned rule should be considered correct under this methodology, and this was mainly left undefined in previous works. As the criterion for evaluati</context>
<context position="8704" citStr="Szpektor et al., 2004" startWordPosition="1430" endWordPosition="1433">ectional entailment, and a requirement for symmetric paraphrase is usually unnecessary. For example, in order to answer the question “Who owns Overture?” it suffices to use a directional entailment rule whose right hand side is ‘X own Y ’, such as ‘X acquire Y —*X own Y ’, which is clearly not a paraphrase. 2.2 Evaluation of Acquisition Algorithms Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005). However, there is still no common accepted framework for their evaluation. Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directionality. Accordingly, previous works (except (Szpektor et al., 2004)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1). One approach which was used for evaluating automatically acquired rules is to measure their contribution to the performance of specific systems, such as QA (Ravichandran</context>
<context position="10316" citStr="Szpektor et al., 2004" startWordPosition="1685" endWordPosition="1688">c modules. Second, the learned rules may affect individual systems differently, thus making observations that are based on different systems incomparable. Third, within a complex system it is difficult to assess the exact quality of entailment rules independently of effects of other system components. Thus, as in many other NLP learning settings, a direct evaluation is needed. Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005). In this evaluation scheme, termed here the rule-based approach, a sample of the learned rules is presented to the judges who evaluate whether each rule is correct or not. The criterion for correctness is not explicitly described in most previous works. By the common view of context relevance for rules (see Section 2.1), a rule was considered correct if the judge could think of reasonable contexts under which it holds. We have replicated the rule-based methodology but did not manage to reach a 0.6 Kappa agreement level between pairs of judges. This approach turns out to be prob</context>
<context position="11721" citStr="Szpektor et al., 2004" startWordPosition="1923" endWordPosition="1926"> judgment is often more difficult due to context relevance. One judge might come up with a certain context that, to her opinion, justifies the rule, while another judge might not imagine that context or think that it doesn’t sufficiently support rule correctness. For example, in our experiments one of the judges did not identify the valid “religious holidays” context for the correct rule ‘X observe Y —*X celebrate Y ’. Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 - 0.63 (Szpektor et al., 2004). To conclude, the prominent rule-based methodology for entailment rule evaluation is not sufficiently well defined. It results in low inter-judge agreement which prevents reliable and consistent assessments of different algorithms. 3 Instance-based Evaluation Methodology As discussed in Section 2.1, an evaluation methodology for entailment rules should reflect the expected validity of their application within NLP systems. Following that line, an entailment rule ‘L —* R’ should be regarded as correct if in all (or at least most) relevant contexts in which the instantiated template L is inferre</context>
<context position="20256" citStr="Szpektor et al., 2004" startWordPosition="3335" endWordPosition="3338"> 80%, which seems sensible for typical applied settings. This yields two alternative sets of correct rules, corresponding to the upper bound and lower bound precision measures. Even though judges may disagree on specific examples for a rule, their judgments may still agree overall on the rule’s correctness. We therefore expect the agreement level on rule correctness to be higher than the agreement on individual examples. 4 Experimental Settings We applied the instance-based methodology to evaluate two state-of-the-art unsupervised acquisition algorithms, DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al., 2004), whose output is publicly available. DIRT identifies semantically related templates in a local corpus using distributional similarity over the templates’ variable instantiations. TEASE acquires entailment relations from the Web for a given input template I by identifying characteristic variable instantiations shared by I and other templates. 460 For the experiment we used the published DIRT explained in Section 3.1). A sample of 15 matchand TEASE knowledge-bases1. For every given in- ing sentences was randomly selected, or all matchput template I, each knowledge-base provides a list ing sente</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura Coppola. 2004. Scaling web-based acquisition of entailment relations. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>