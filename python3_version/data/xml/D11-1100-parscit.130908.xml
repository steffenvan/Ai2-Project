<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.994145">
Harnessing WordNet Senses for Supervised Sentiment Classification
</title>
<author confidence="0.886077">
Balamurali A R12 Aditya Joshi2 Pushpak Bhattacharyya2
</author>
<affiliation confidence="0.983932">
1 IITB-Monash Research Academy, IIT Bombay
2Dept. of Computer Science and Engineering, IIT Bombay
</affiliation>
<address confidence="0.764772">
Mumbai, India - 400076
</address>
<email confidence="0.999302">
{balamurali,adityaj,pb}@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999728666666667">
Traditional approaches to sentiment classifica-
tion rely on lexical features, syntax-based fea-
tures or a combination of the two. We pro-
pose semantic features using word senses for
a supervised document-level sentiment classi-
fier. To highlight the benefit of sense-based
features, we compare word-based representa-
tion of documents with a sense-based repre-
sentation where WordNet senses of the words
are used as features. In addition, we highlight
the benefit of senses by presenting a part-of-
speech-wise effect on sentiment classification.
Finally, we show that even if a WSD engine
disambiguates between a limited set of words
in a document, a sentiment classifier still per-
forms better than what it does in absence of
sense annotation. Since word senses used as
features show promise, we also examine the
possibility of using similarity metrics defined
on WordNet to address the problem of not
finding a sense in the training corpus. We per-
form experiments using three popular similar-
ity metrics to mitigate the effect of unknown
synsets in a test corpus by replacing them with
similar synsets from the training corpus. The
results show promising improvement with re-
spect to the baseline.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99931972">
Sentiment Analysis (SA) is the task of prediction of
opinion in text. Sentiment classification deals with
tagging text as positive, negative or neutral from the
perspective of the speaker/writer with respect to a
topic. In this work, we follow the definition of Pang
et al. (2002) &amp; Turney (2002) and consider a binary
classification task for output labels as positive and
negative.
Traditional supervised approaches for SA have
explored lexeme and syntax-level units as features.
Approaches using lexeme-based features use bag-
of-words (Pang and Lee, 2008) or identify the
roles of different parts-of-speech (POS) like adjec-
tives (Pang et al., 2002; Whitelaw et al., 2005).
Approaches using syntax-based features construct
parse trees (Matsumoto et al., 2005) or use text
parsers to model valence shifters (Kennedy and
Inkpen, 2006).
Our work explores incorporation of semantics
in a supervised sentiment classifier. We use the
synsets in Wordnet as the feature space to represent
word senses. Thus, a document consisting of
words gets mapped to a document consisting of
corresponding word senses. Harnessing WordNet
senses as features helps us address two issues:
</bodyText>
<listItem confidence="0.9914944">
1. Impact of WordNet sense-based features on the
performance of supervised SA
2. Use of WordNet similarity metrics to solve the
problem of features unseen in the training cor-
pus
</listItem>
<bodyText confidence="0.999981285714286">
The first points deals with evaluating sense-based
features against word-based features. The second is-
sue that we address is in fact an opportunity to im-
prove the performance of SA that opens up because
of the choice of sense space. Since sense-based
features prove to generate superior sentiment clas-
sifiers, we get an opportunity to mitigate unknown
</bodyText>
<page confidence="0.963418">
1081
</page>
<note confidence="0.9577505">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1081–1091,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999967641025641">
synsets in the test corpus by replacing them with
known synsets in the training corpus. Note that such
replacement is not possible if word-based represen-
tation were used as it is not feasible to make such a
large number of similarity comparisons.
We use the corpus by Ye et al. (2009) that con-
sists of travel domain reviews marked as positive or
negative at the document level. Our experiments on
studying the impact of Wordnet sense-based features
deal with variants of this corpus manually or auto-
matically annotated with senses. Besides showing
the overall impact, we perform a POS-wise analysis
of the benefit to SA. In addition, we compare the ef-
fect of varying training samples on a sentiment clas-
sifier developed using word based features and sense
based features. Through empirical evidence, we also
show that disambiguating some words in a docu-
ment also provides a better accuracy as compared
to not disambiguating any words. These four sets of
experiments highlight our hypothesis that WordNet
senses are better features as compared to words.
Wordnet sense-based space allows us to mitigate
unknown features in the test corpus. Our synset re-
placement algorithm uses Wordnet similarity-based
metrics which replace an unknown synset in the test
corpus with the closest approximation in the training
corpus. Our results show that such a replacement
benefits the performance of SA.
The roadmap for the rest of the paper is as fol-
lows: Existing related work in SA and the differ-
entiating aspects of our work are explained in sec-
tion 2 Section 3 describes the sense-based features
that we use for this work. We explain the similarity-
based replacement technique using WordNet synsets
in section 4. Our experiments have been described
in section 5. In section 6, we present our results
and related discussions. Section 7 analyzes some of
the causes for erroneous classification. Finally, sec-
tion 8 concludes the paper and points to future work.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99988341509434">
This work studies the benefit of a word sense-based
feature space to supervised sentiment classification.
However, a word sense-based feature space is feasi-
ble subject to verification of the hypothesis that sen-
timent and word senses are related. Towards this,
Wiebe and Mihalcea (2006) conduct a study on hu-
man annotation of 354 words senses with polarity
and report a high inter-annotator agreement. The
work in sentiment analysis using sense-based fea-
tures, including ours, assumes this hypothesis that
sense decides the sentiment.
The novelty of our work lies in the following.
Firstly our approach is distinctly. Akkaya et al.
(2009) and Martn-Wanton et al. (2010) report per-
formance of rule-based sentiment classification us-
ing word senses. Instead of a rule-based implemen-
tation, We used supervised learning. The supervised
nature of our approach renders lexical resources un-
necessary as used in Martn-Wanton et al. (2010).
Rentoumi et al. (2009) suggest using word senses
to detect sentence level polarity of news headlines.
The authors use graph similarity to detect polarity of
senses. To predict sentence level polarity, a HMM
is trained on word sense and POS as the observa-
tion. The authors report that word senses partic-
ularly help understanding metaphors in these sen-
tences. Our work differs in terms of the corpus and
document sizes in addition to generating a general
purpose classifier.
Another supervised approach of creating an emo-
tional intensity classifier using concepts as features
has been reported by Carrillo de Albornoz et al.
(2010). This work is different based on the feature
space used. The concepts used for the purpose are
limited to affective classes. This restricts the size of
the feature space to a limited set of labels. As op-
posed to this, we construct feature vectors that map
to a larger sense-based space. In order to do so, we
use synset offsets as representation of sense-based
features.
Akkaya et al. (2009), Martn-Wanton et al. (2010)
and Carrillo de Albornoz et al. (2010) perform sen-
timent classification of individual sentences. How-
ever, we consider a document as a unit of sentiment
classification i.e. our goal is to predict a document
on the whole as positive or negative. This is different
from Pang and Lee (2004) which suggests that sen-
timent is associated only with subjective content. A
document in its entirety is represented using sense-
based features in our experiments. Carrillo de Al-
bornoz et al. (2010) suggests expansion using Word-
Net relations which we also follow. This is a benefit
that can be achieved only in a sense-based space.
</bodyText>
<page confidence="0.998256">
1082
</page>
<sectionHeader confidence="0.965613" genericHeader="method">
3 Features based on WordNet Senses
</sectionHeader>
<bodyText confidence="0.999915388888889">
In their original form, documents are said to be in
lexical space since they consist of words. When the
words are replaced by their corresponding senses,
the resultant document is said to be in semantic
space.
WordNet 2.1 (Fellbaum, 1998) has been used as
the sense repository. Each word/lexeme is mapped
to an appropriate synset in WordNet based on
its sense and represented using the corresponding
synset id of WordNet. Thus, the word love is dis-
ambiguated and replaced by the identifier 21758160
which consists of a POS category identifier 2 fol-
lowed by synset offset identifier 1758160. This pa-
per refers to synset offset as synset identifiers or sim-
ply, senses.
This section first gives the motivation for using
word senses and then, describes the approaches that
we use for our experiments.
</bodyText>
<subsectionHeader confidence="0.998141">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.9729355">
Consider the following sentences as the first sce-
nario.
</bodyText>
<listItem confidence="0.998377666666667">
1. “Her face fell when she heard that she had
been fired.”
2. “The fruit fell from the tree.”
</listItem>
<bodyText confidence="0.993472846153846">
The word ‘fell’ occurs in different senses in the
two sentences. In the first sentence, ‘fell’ has the
meaning of ‘assume a disappointed or sad expres-
sion, whereas in the second sentence, it has the
meaning of ‘descend in free fall under the influence
of gravity’. A user will infer the negative polarity of
the first sentence from the negative sense of ‘fell’ in
it while the user will state that the second sentence
does not carry any sentiment. This implies that there
is at least one sense of the word ‘fell’ that carries
sentiment and at least one that does not.
In the second scenario, consider the following ex-
amples.
</bodyText>
<listItem confidence="0.999056666666667">
1. “The snake bite proved to be deadly for the
young boy.”
2. “Shane Warne is a deadly spinner.”
</listItem>
<bodyText confidence="0.973996571428572">
The word deadly has senses which carry opposite
polarity in the two sentences and these senses as-
sign the polarity to the corresponding sentence. The
first sentence is negative while the second sentence
is positive.
Finally in the third scenario, consider the follow-
ing pair of sentences.
</bodyText>
<listItem confidence="0.998015">
1. “He speaks a vulgar language.”
2. “Now that’s real crude behavior!”
</listItem>
<bodyText confidence="0.999927071428571">
The words vulgar and crude occur as synonyms
in the synset that corresponds to the sense ‘conspic-
uously and tastelessly indecent’. The synonymous
nature of words can be identified only if they are
looked at as senses and not just words.
As one may observe, the first scenario shows that
a word may have some sentiment-bearing and some
non-sentiment-bearing senses. In the second sce-
nario, we show that there may be different senses
of a word that bear sentiments of opposite polarity.
Finally, in the third scenario, we show how a sense
can be manifested using different words, i.e., words
in a synset. The three scenarios motivate the use of
semantic space for sentiment prediction.
</bodyText>
<subsectionHeader confidence="0.964918">
3.2 Sense versus Lexeme-based Feature
Representation
</subsectionHeader>
<bodyText confidence="0.999614666666667">
We annotate the words in the corpus with their
senses using two sense disambiguation approaches.
As the first approach, manual sense annotation
of documents is carried out by two annotators on two
subsets of the corpus, the details of which are given
in Section 5.1. This is done to determine the ideal
case scenario- the skyline performance.
As the second approach, a state-of-art algorithm
for domain-specific WSD proposed by Khapra et
al. (2010) is used to obtain an automatically sense-
tagged corpus. This algorithm called iterative WSD
or IWSD iteratively disambiguates words by rank-
ing the candidate senses based on a scoring function.
The two types of sense-annotated corpus lead us
to four feature representations for a document:
</bodyText>
<listItem confidence="0.99708375">
1. Word senses that have been manually annotated
(M)
2. Word senses that have been annotated by an au-
tomatic WSD (I)
</listItem>
<page confidence="0.569909">
1083
</page>
<listItem confidence="0.965423333333333">
3. Manually annotated word senses and words
(both separately as features) (Words +
Sense(M))
4. Automatically annotated word senses and
words (both separately as features) (Words +
Sense(I))
</listItem>
<bodyText confidence="0.999791444444444">
Our first set of experiments compares the four
feature representations to find the feature represen-
tation with which sentiment classification gives the
best performance. W+S(M) and W+S(I) are used to
overcome non-coverage of WordNet for some noun
synsets. In addition to this, we also present a part-
of-speech-wise analysis of benefit to SA as well as
effect of varying the training samples on sentiment
classification accuracy.
</bodyText>
<subsectionHeader confidence="0.705536">
3.3 Partial disambiguation as opposed to no
disambiguation
</subsectionHeader>
<bodyText confidence="0.999940428571429">
The state-of-the-art automatic WSD engine that we
use performs (approximately) with 70% accuracy on
tourism domain (Khapra et al., 2010). This means
that the performance of SA depends on the perfor-
mance of WSD which is not very high in case of the
engine we use.
A partially disambiguated document is a docu-
ment which does not contain senses of all words.
Our hypothesis is that disambiguation of even few
words in a document can give better results than
no disambiguation. To verify this, we create differ-
ent variants of the corpus by disambiguating words
which have candidate senses within a threshold. For
example, a partially disambiguated variant of the
corpus with threshold 3 for candidate senses is cre-
ated by disambiguating words which have a maxi-
mum of three candidate senses. These synsets are
then used as features for classification along with
lexeme based features. We conduct multiple experi-
ments using this approach by varying the number of
candidate senses.
</bodyText>
<sectionHeader confidence="0.8886985" genericHeader="method">
4 Advantage of senses: Similarity Metrics
and Unknown Synsets
</sectionHeader>
<subsectionHeader confidence="0.999312">
4.1 Synset Replacement Algorithm
</subsectionHeader>
<bodyText confidence="0.999697076923077">
Using WordNet senses provides an opportunity to
use similarity-based metrics for WordNet to reduce
the effect of unknown features. If a synset encoun-
tered in a test document is not found in the training
corpus, it is replaced by one of the synsets present
in the training corpus. The substitute synset is deter-
mined on the basis of its similarity with the synset
in the test document. The synset that is replaced is
referred to as an unseen synset as it is not known to
the trained model.
For example, consider excerpts of two reviews,
the first of which occurs in the training corpus while
the second occurs in the test corpus.
</bodyText>
<listItem confidence="0.985046666666667">
1. “ In the night, it is a lovely city and... ”
2. “ The city has many beautiful hot spots for hon-
eymooners. ”
</listItem>
<bodyText confidence="0.9996646">
The synset of ‘beautiful’ is not present in the train-
ing corpus. We evaluate a similarity metric for all
synsets in the training corpus with respect to the
sense of beautiful and find that the sense of lovely is
closest to it. Hence, the sense of beautiful in the test
document is replaced by the sense of lovely which is
present in the training corpus.
The replacement algorithm is described in Algo-
rithm 1. The algorithm follows from the fact that the
similarity value for a synset with itself is maximum.
</bodyText>
<subsectionHeader confidence="0.991512">
4.2 Similarity metrics used
</subsectionHeader>
<bodyText confidence="0.999945">
We conduct different runs of the replacement
algorithm using three similarity metrics, namely
LIN’s similarity metric, Lesk similarity metric and
Leacock and Chodorow (LCH) similarity metric.
These runs generate three variants of the corpus.
We compare the benefit of each of these metrics by
studying their sentiment classification performance.
The metrics can be described as follows:
LIN: The metric by Lin (1998) uses the infor-
mation content individually possessed by two con-
cepts in addition to that shared by them. The infor-
mation content shared by two concepts A and B is
given by their most specific subsumer (lowest super-
ordinate(lso). Thus, this metric defines the similarity
between two concepts as
</bodyText>
<equation confidence="0.979378">
2 × log Pr(lso(A, B))
simLIN(A,B) = log Pr(A) + log Pr(B) (1)
</equation>
<page confidence="0.965245">
1084
</page>
<table confidence="0.84624875">
Input: Training Corpus, Test Corpus,
Similarity Metric
Output: New Test Corpus
T:= Training Corpus;
X:= Test Corpus;
S:= Similarity metric;
train concept list = get list concept(T) ;
test concept list = get list concept(X);
</table>
<figure confidence="0.624120214285714">
for each concept C in test concept list do
temp max similarity = 0 ;
temp concept = C ;
for each concept D in train concept list do
similarity value = get similarity value(C,D,S);
if (similarity value &gt; temp max similarity) then
temp max similarity= similarity value;
temp concept = D ;
end
end
C = temp concept;
replace synset corpus(C,X);
end
Return X ;
</figure>
<figureCaption confidence="0.4143025">
Algorithm 1: Synset replacement using similarity
metric
</figureCaption>
<bodyText confidence="0.994240916666667">
Lesk: Each concept in WordNet is defined
through gloss. To compute the Lesk similar-
ity (Banerjee and Pedersen, 2002) between A and
B, a scoring function based on the overlap of words
in their individual glosses is used.
Leacock and Chodorow (LCH): To measure
similarity between two concepts A and B, Leacock
and Chodorow (1998) compute the shortest path
through hypernymy relation between them under the
constraint that there exists such a path. The final
value is computed by scaling the path length by the
overall taxonomy depth (D).
</bodyText>
<equation confidence="0.8142725">
siMLCH(A, B) = − log Clen(A, B)1 (2)
2D
</equation>
<sectionHeader confidence="0.998927" genericHeader="method">
5 Experimentation
</sectionHeader>
<bodyText confidence="0.9997505">
We describe the variants of the corpus generated and
the experiments in this section.
</bodyText>
<subsectionHeader confidence="0.961631">
5.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.997285153846154">
We create different variants of the dataset by Ye et
al. (2009). This dataset contains 600 positive and
591 negative reviews about seven travel destinations.
Each review contains approximately 4-5 sentences
with an average number of words per review being
80-85.
To create the manually annotated corpus, two hu-
man annotators annotate words in the corpus with
senses for two disjoint subsets of the original cor-
pus by Ye et al. (2009). The inter-annotation agree-
ment for a subset of the corpus showed 91% sense
overlap. The manually annotated corpus consists of
34508 words with 6004 synsets.
</bodyText>
<table confidence="0.999877">
POS #Words P(%) R(%) F-Score(%)
Noun 12693 75.54 75.12 75.33
Adverb 4114 71.16 70.90 71.03
Adjective 6194 67.26 66.31 66.78
Verb 11507 68.28 67.97 68.12
Overall 34508 71.12 70.65 70.88
</table>
<tableCaption confidence="0.9631375">
Table 1: Annotation Statistics for IWSD; P- Precision,R-
Recall
</tableCaption>
<bodyText confidence="0.999964166666667">
The second variant of the corpus contains word
senses obtained from automatic disambiguation us-
ing IWSD. The evaluation statistics of the IWSD is
shown in Table 1. Table 1 shows that the F-score for
noun synsets is high while that for adjective synsets
is the lowest among all. The low recall for adjective
POS based synsets can be detrimental to classifica-
tion since adjectives are known to express direct sen-
timent (Pang et al., 2002). Hence, in the context of
sentiment classification, disambiguation of adjective
synsets is more critical as compared to disambigua-
tion of noun synsets.
</bodyText>
<subsectionHeader confidence="0.992849">
5.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999864272727273">
The experiments are performed using C-SVM (lin-
ear kernel with default parameters1) available as a
part of LibSVM2 package. We choose to use SVM
since it performs the best for sentiment classification
(Pang et al., 2002). All results reported are average
of five-fold cross-validation accuracies.
To conduct experiments on words as features, we
first perform stop-word removal. The words are not
stemmed since stemming is known to be detrimen-
tal to sentiment classification (Leopold and Kinder-
mann, 2002). To conduct the experiments based on
</bodyText>
<footnote confidence="0.9669195">
1C=0.0,e=0.0010
2http://www.csie.ntu.edu.tw/ cjlin/libsvm
</footnote>
<page confidence="0.858226">
1085
</page>
<table confidence="0.999761">
Feature Representations Accuracy(%) PF NF PP NP PR NR
Words (Baseline) 84.90 85.07 84.76 84.95 84.92 85.19 84.60
Sense (M) 89.10 88.22 89.11 91.50 87.07 85.18 91.24
Words + Sense (M) 90.20 89.81 90.43 92.02 88.55 87.71 92.39
Sense (I) 85.48 85.31 85.65 87.17 83.93 83.53 87.46
Words + Sense (I) 86.08 86.28 85.92 85.87 86.38 86.69 85.46
</table>
<tableCaption confidence="0.9561835">
Table 2: Classification Results; PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NP-
Negative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%)
</tableCaption>
<bodyText confidence="0.998509173913043">
the synset representation, words in the corpus are an-
notated with synset identifiers along with POS cat-
egory identifiers. For automatic sense disambigua-
tion, we used the trained IWSD engine from Khapra
et al. (2010). These synset identifiers along with
POS category identifiers are then used as features.
For replacement using semantic similarity measures,
we used WordNet::Similarity 2.05 package by Ped-
ersen et al. (2004).
To evaluate the result, we use accuracy, F-score,
recall and precision as the metrics. Classification
accuracy defines the ratio of the number of true in-
stances to the total number of instances. Recall is
calculated as a ratio of the true instances found to
the total number of false positives and true posi-
tives. Precision is defined as the number of true
instances divided by number of true positives and
false negatives. Positive Precision (PP) and Posi-
tive Recall (PR) are precision and recall for positive
documents while Negative Precision (NP) and Nega-
tive Recall (NR) are precision and recall for negative
documents. F-score is the weighted precision-recall
score.
</bodyText>
<sectionHeader confidence="0.999942" genericHeader="evaluation">
6 Results and Discussions
</sectionHeader>
<subsectionHeader confidence="0.722614">
6.1 Comparison of various feature
representations
</subsectionHeader>
<bodyText confidence="0.999952025">
Table 2 shows results of classification for different
feature representations. The baseline for our results
is the unigram bag-of-words model (Baseline).
An improvement of 4.2% is observed in the ac-
curacy of sentiment prediction when manually an-
notated sense-based features (M) are used in place
of word-based features (Words). The precision of
both the classes using features based on semantic
space is also better than one based on lexeme space.
While reported results suggest that it is more diffi-
cult to detect negative sentiment than positive senti-
ment (Gindl and Liegl, 2008), our results show that
negative recall increases by around 8% in case of
sense-based representation of documents.
The combined model of words and manually an-
notated senses (Words + Senses (M)) gives the best
performance with an accuracy of 90.2%. This leads
to an improvement of 5.3% over the baseline accu-
racy 3.
One of the reasons for improved performance is
the feature abstraction achieved due to the synset-
based features. The dimension of feature vector is
reduced by a factor of 82% when the document is
represented in synset space. The reduction in dimen-
sionality may also lead to reduction in noise (Cun-
ningham, 2008).
A comparison of accuracy of different sense rep-
resentations in Table 2 shows that manual disam-
biguation performs better than using automatic al-
gorithms like IWSD. Although overall classification
accuracy improvement of IWSD over baseline is
marginal, negative recall also improves. This bene-
fit is despite the fact that evaluation of IWSD engine
over manually annotated corpus gave an overall F-
score of 71% (refer Table 1). For a WSD engine
with a better accuracy, the performance of sense-
based SA can be boosted further.
Thus, in terms of feature representation of docu-
ments, sense-based features provide a better overall
performance as compared to word-based features.
</bodyText>
<page confidence="0.99748">
1086
</page>
<figureCaption confidence="0.994271">
Figure 1: POS-wise statistics of manually annotated se-
mantic space
</figureCaption>
<subsectionHeader confidence="0.998127">
6.2 POS-wise analysis
</subsectionHeader>
<bodyText confidence="0.99072">
For each POS, we compare the performance of two
models:
</bodyText>
<listItem confidence="0.999911">
• Model trained on words of only that POS
• Model trained on word senses of only that POS
</listItem>
<bodyText confidence="0.935159370370371">
Figure 1 shows the parts-of-speech-wise classifica-
tion accuracy of sentiment classification for senses
(manual) and words. In the lexeme space, adjectives
directly impact the classification performance. But it
can be seen that disambiguation of adverb and verb
synsets impact the performance of SA higher than
disambiguation of nouns and adjectives.
While it is believed that adjectives carry direct
sentiments, our results suggest that using adjectives
alone as features may not improve the accuracy. The
results prove that sentiment may be subtle at times
and not expressed directly through adjectives.
As manual sense annotation is an effort and cost
intensive process, the parts-of-speech-wise results
suggest improvements expected from an automatic
WSD engine so that it can aid sentiment classifica-
tion. Table 1 suggests that the WSD engine works
better for noun synsets compared to adjective and
adverb synsets. While this is expected in a typical
WSD setup, it is the adverbs and verbs that are more
important for detecting sentiment in semantics space
3The improvement in results of semantic space is found to
be statistically significant over the baseline at 95% confidence
level when tested using a paired t-test.
than nouns. The future WSD systems will have to
show an improvement in their accuracy with respect
to adverb and verb synsets.
</bodyText>
<table confidence="0.998329666666667">
Sense Words
POS Category PF NF PF NF
Adverb 79.65 80.45 70.25 73.68
Verb 75.50 79.28 62.23 63.12
Noun 73.39 75.40 69.77 72.55
Adjective 63.11 65.03 78.29 79.20
</table>
<tableCaption confidence="0.9514365">
Table 3: POS-wise F-score for sense (M) and Words;PF-
Positive F-score(%), NF- Negative F-score (%)
</tableCaption>
<bodyText confidence="0.998181">
Table 3 shows the positive and negative F-score
statistics with respect to different POS. Detection
of negative reviews using lexeme space is difficult.
POS-wise statistics also suggest the same. It should
be noted that adverb and verb synsets play an im-
portant role in negative class detection. Thus, an au-
tomatic WSD engine should give importance to the
correct disambiguation of these POS categories.
</bodyText>
<subsectionHeader confidence="0.905205">
6.3 Effect of size of training corpus
</subsectionHeader>
<table confidence="0.999381285714286">
#Training W M I W+S(M) W+S(I)
Documents
100 76.5 87 79.5 82.5 79.5
200 81.5 88.5 82 90 84
300 79.5 92 81 89.5 82
400 82 90.5 81 94 85.5
500 83.5 91 85 96 82.5
</table>
<tableCaption confidence="0.9793208">
Table 4: Accuracy (%) with respect to number of training
documents; W: Words, M: Manual Annotation, I: IWSD-
based sense annotation, W+S(M): Word+Senses (Manual
annotation), W+S(I): Word+Senses(IWSD-based sense
annotation)
</tableCaption>
<bodyText confidence="0.99984125">
From table 2, the benefit of sense disambigua-
tion to sentiment prediction is evident. In addition,
Table 4 shows variation of classification accuracy
with respect to different number of training sam-
ples based on different approaches of annotation ex-
plained in previous sections. The results are based
on a blind set of 90 test samples from both the po-
larity labels 4.
</bodyText>
<footnote confidence="0.600236">
4No cross validation is performed for this experiment
</footnote>
<page confidence="0.408211">
81.24
</page>
<figure confidence="0.999565166666667">
66.
Adver
90.00
80.00
70.00
60.00
50.00
40.00
30.00
20.00
10.00
0.00
</figure>
<page confidence="0.994896">
1087
</page>
<bodyText confidence="0.999708">
Compared to lexeme-based features, manually an-
notated sense based features give better performance
with lower number of training samples. IWSD is
also better than lexeme-based features. A SA sys-
tem trained on 100 training samples using manually
annotated senses gives an accuracy of 87%. Word-
based features never achieve this accuracy. An
IWSD-based system requires lesser samples when
compared to lexeme space for an equivalent accu-
racy. Note that model based on words + senses(M)
features achieve an accuracy of 96% on this test set.
This implies that the synset space, in addition
to benefit to sentiment prediction in general, re-
quires lesser number of training samples in order to
achieve the accuracy that lexeme space can achieve
with a larger number of samples.
</bodyText>
<subsectionHeader confidence="0.99126">
6.4 Effect of Partial disambiguation
</subsectionHeader>
<bodyText confidence="0.999801961538462">
Figure 2 shows the accuracy, positive F-score and
negative F-score with respect to different thresholds
of candidate senses for partially disambiguated doc-
uments as described in Section 3.3. We compare the
performance of these documents with word-based
features (B) and sense-based features based on man-
ually (M) or automatically obtained senses (I). Note
that Sense (I) and Sense (M) correspond to com-
pletely disambiguated documents.
In case of partial disambiguation using manual
annotation, disambiguating words with less than
three candidate senses performs better than others.
For partial disambiguation that relies on an auto-
matic WSD engine, a comparable performance to
full disambiguation can be obtained by disambiguat-
ing words which have a maximum of four candidate
senses.
As expected, completely disambiguated docu-
ments provide the best F-score and accuracy fig-
ures5. However, a performance comparable to com-
plete disambiguation can be attained by disam-
biguating selective words.
Our results show that even if highly ambiguous
(in terms of senses) words are not disambiguated by
a WSD engine, the performance of sentiment classi-
fication improves.
</bodyText>
<footnote confidence="0.958074">
5All results are statistically significant with respect to base-
line
</footnote>
<figure confidence="0.9757526">
Negative Fscore
3 senses (I)
3 senses (M)
2 senses (I)
2 senses (M)
Sense(I)
Sense(M)
Words (B)
81.00 82.00 83.00 84.00
Fs
</figure>
<figureCaption confidence="0.995892">
Figure 2: Partial disambiguation statistics: Accu-
racy,Positive F-score, Negative F-score variation with re-
spect to sense disambiguation difficult level is shown.
Words(B): baseline system
</figureCaption>
<subsubsectionHeader confidence="0.418313">
6.5 Synset replacement using similarity metrics
</subsubsectionHeader>
<bodyText confidence="0.99089565">
Table 5 shows the results of synset replacement ex-
periments performed using similarity metrics de-
fined in section 4. The similarity metric value NA
shown in the table indicates that synset replacement
is not performed for the specific run of experiment.
For this set of experiments, we use the combina-
tion of sense and words as features (indicated by
Senses+Words (M)).
Synset replacement using a similarity metric
shows an improvement over using words alone.
However, the improvement in classification accu-
racy is marginal compared to sense-based represen-
tation without synset replacement (Similarity Met-
ric=NA).
Replacement using LIN and LCH metrics gives
marginally better results compared to the vanilla set-
ting in a manually annotated corpus. The same phe-
nomenon is seen in the case of IWSD based ap-
proach6. The limited improvement can be due to
the fact that since LCH and LIN consider only IS-A
</bodyText>
<footnote confidence="0.937137333333333">
6Results based on LCH and LIN similarity metric for auto-
matic sense disambiguation is not statistically significant with
α=0.05
</footnote>
<figure confidence="0.998651833333333">
4 senses (M)
6 senses (M)
5 senses (M)
4 senses (I)
6 senses (I)
5 senses (I)
</figure>
<page confidence="0.963077">
1088
</page>
<table confidence="0.99938">
Feature Representation Similarity Accuracy PF NF PP NP PR NR
Metric
Words (Baseline) NA 84.90 85.07 84.76 84.95 84.92 85.19 84.60
Words + Sense(M) NA 90.20 89.81 90.43 92.02 88.55 87.71 92.39
Words + Sense(I) NA 86.08 86.28 85.92 85.87 86.38 86.69 85.46
Words + Sense (M) LCH 90.60 90.20 90.85 92.85 88.61 87.70 93.21
Words + Sense(M) LIN 90.70 90.26 90.97 93.17 88.50 87.53 93.57
Words + Sense (M) Lesk 91.12 90.70 91.38 93.55 88.97 88.03 93.92
Words + Sense (I) LCH 85.66 85.85 85.52 85.67 85.76 86.02 85.28
Words + Sense(I) LIN 86.16 86.37 86.00 86.06 86.40 86.69 85.61
Words + Sense (I) Lesk 86.25 86.41 86.10 86.31 86.26 86.52 85.95
</table>
<tableCaption confidence="0.810641">
Table 5: Similarity Metric Analysis using different similarity metrics with synsets and a combinations of synset and
</tableCaption>
<table confidence="0.9753318">
words;PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NP-Negative Precision (%), PR-
Positive Recall (%), NR-Negative Recall (%)
Top information IWSD Manual Match Match Unmatched
content features synset # synsets # synset # Synsets (%) Synset(%)
(in %)
10 601 722 288 39.89 60.11
20 1199 1443 650 45.05 54.95
30 1795 2165 1005 46.42 53.58
40 2396 2889 1375 47.59 52.41
50 2997 3613 1730 47.88 52.12
</table>
<tableCaption confidence="0.99518">
Table 6: Comparison of top information gain-based features of manually annotated corpora and automatically anno-
tated corpora
</tableCaption>
<bodyText confidence="0.999916352941176">
relationship in WordNet, the replacement happens
only for verbs and nouns. This excludes adverb
synsets which we have shown to be the best features
for a sense-based SA system.
Among all similarity metrics, the best classifica-
tion accuracy is achieved using Lesk. The system
performs with an overall classification accuracy of
91.12%, which is a substantial improvement of 6.2%
over baseline. Again, it is only 1% over the vanilla
setting that uses combination of synset and words.
However, the similarity metric is not sophisticated
as LIN or LCH.
Thus, we observe a marginal improvement by us-
ing similarity-based metrics for WordNet. A good
metric which covers all POS categories can provide
substantial improvement in the classification accu-
racy.
</bodyText>
<sectionHeader confidence="0.999633" genericHeader="evaluation">
7 Error Analysis
</sectionHeader>
<bodyText confidence="0.99982375">
For sentiment classification based on semantic
space, we classify the errors into four categories.
The examples quoted are from manual evaluation of
the results.
</bodyText>
<listItem confidence="0.589896">
1. Effect of low disambiguation accuracy ofIWSD
</listItem>
<bodyText confidence="0.984081307692308">
engine: SA using automatic sense annotation
depends on the annotation system used. To as-
sess the impact of IWSD system on sentiment
classification, we compare the feature set based
on manually annotated senses with the feature
set based on automatically annotated senses.
We compare the most informative features of
the two classifiers. Table 6 shows the number
of top informative features (synset) selected as
the percentage of total synset features present
when the semantic representation of documen-
tation is used. The matched synset column rep-
resents the number of IWSD synsets that match
</bodyText>
<page confidence="0.992659">
1089
</page>
<bodyText confidence="0.992630666666667">
with manually annotated synsets.
The number of top performing features is more
in case of manually annotated synsets. This
can be attributed to the total number of synsets
tagged in the two variant of the corpus. The re-
duction in the performance of SA for automati-
cally annotated senses is because of the number
of unmatched synsets.
Thus, although the accuracy of IWSD is cur-
rently 70%, the table indicates that IWSD can
match the performance of manually annotated
senses for SA if IWSD is able to tag correctly
those top information content synsets. This as-
pect needs to be investigated further.
2. Negation Handling: For the purpose of this
work, we concentrate on words as units for sen-
timent determination. Syntax and its contri-
bution in understanding sentiment is neglected
and hence, positive documents which con-
tain negations are wrongly classified as nega-
tive. Negation may be direct as in the excerpt
‘....what is there not to like about Vegas.’ or
may be double as in the excerpt‘...that aren’t
insecure’.
</bodyText>
<listItem confidence="0.452181">
3. Interjections and WordNet coverage: Recent
</listItem>
<bodyText confidence="0.898016235294118">
informal words are not covered in WordNet and
hence, do not get disambiguated. The same
is the case for interjections like ‘wow’,‘duh’
which sometimes carry direct sentiment. Lex-
ical resources which include them can be used
to incorporate information about these lexical
units.
4. Document Speci�city: The assumption under-
lying our analysis is that a document contains
description of only one topic. However, re-
views are generic in nature and tend to express
contrasting sentiment about sub-topics . For
example, a travel review about Paris can talk
about restaurants in Paris, traffic in Paris, pub-
lic behaviour, etc. with opposing sentiments.
Assigning an overall sentiment to a document
is subjective in such cases.
</bodyText>
<sectionHeader confidence="0.991586" genericHeader="conclusions">
8 Conclusion &amp; Future Work
</sectionHeader>
<bodyText confidence="0.99998952173913">
This work presents an empirical benefit of WSD to
sentiment analysis. The study shows that supervised
sentiment classifier modeled on wordNet senses per-
form better than word-based features. We show how
the performance impact differs for different auto-
matic and manual techniques, parts-of-speech, dif-
ferent training sample size and different levels of
disambiguation. In addition, we also show the bene-
fit of using WordNet based similarity metrics for re-
placing unknown features in the test set. Our results
support the fact that not only does sense space im-
prove the performance of a sentiment classification
system, but also opens opportunities for improve-
ment using better similarity metrics.
Incorporation of syntactical information along
with semantics can be an interesting area of work.
More sophisticated features which include the two
need to be explored. Another line of work is in the
context of cross-lingual sentiment analysis. Current
solutions are based on machine translation which is
very resource-intensive. Using a bi-lingual dictio-
nary which maps WordNet across languages, such a
machine translation sub-system can be avoided.
</bodyText>
<sectionHeader confidence="0.978204" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9978464">
We thank Jaya Saraswati and Rajita Shukla from
CFILT Lab, IIT Bombay for annotating the dataset
used for this work. We also thank Mitesh Khapra
and Salil Joshi, IIT Bombay for providing us with
the IWSD engine for the required experiments.
</bodyText>
<sectionHeader confidence="0.999416" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998310615384615">
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Proc. of
EMNLP ’09, pages 190–199, Singapore.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
lesk algorithm for word sense disambiguation using
wordnet. In Proc. of CICLing’02, pages 136–145,
London, UK.
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gervs. 2010. Improving emotional intensity clas-
sification using word sense disambiguation. Special
issue: Natural Language Processing and its Appli-
cations. Journal on Research in Computing Science,
46:131–142.
</reference>
<page confidence="0.71459">
1090
</page>
<reference confidence="0.999851323943662">
Pdraig Cunningham. 2008. Dimension reduction. In
Machine Learning Techniques for Multimedia, Cogni-
tive Technologies, pages 91–112.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Stefan Gindl and Johannes Liegl, 2008. Evaluation of
different sentiment detection methods for polarity clas-
sification on web-based reviews, pages 35–43.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie reviews using contextual va-
lence shifters. Computational Intelligence, 22(2):110–
125.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak
Bhattacharyya. 2010. Domain-specific word sense
disambiguation combining corpus basedand wordnet
based parameters. In Proc. of GWC’10, Mumbai, In-
dia.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context with wordnet similarity for word
sense identification. In WordNet: A Lexical Reference
System and its Application.
Edda Leopold and J¨org Kindermann. 2002. Text catego-
rization with support vector machines. how to repre-
sent texts in input space? Machine Learning, 46:423–
444.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In In Proc. of the 15th International Con-
ference on Machine Learning, pages 296–304.
Tamara Martn-Wanton, Alexandra Balahur-Dobrescu,
Andres Montoyo-Guijarro, and Aurora Pons-Porrata.
2010. Word sense disambiguation in opinion mining:
Pros and cons. In Proc. of CICLing’10, Madrid,Spain.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. In Proc.
of PAKDD’05,, Lecture Notes in Computer Science,
pages 301–311.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of ACL’04, pages
271–278, Barcelona, Spain.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2:1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. volume 10, pages 79–86.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the relat-
edness of concepts. In Demonstration Papers at HLT-
NAACL’04, pages 38–41.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Sen-
timent analysis of figurative language using a word
sense disambiguation approach. In Proc. of the In-
ternational Conference RANLP’09, pages 370–375,
Borovets, Bulgaria.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of ACL’02, pages 417–424,
Philadelphia, US.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proc. of CIKM ’05, pages 625–631, New York, NY,
USA.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proc. of COLING-ACL’06, pages
1065–1072.
Qiang Ye, Ziqiong Zhang, and Rob Law. 2009. Senti-
ment classification of online reviews to travel destina-
tions by supervised machine learning approaches. Ex-
pert Systems with Applications, 36(3):6527 – 6535.
</reference>
<page confidence="0.992196">
1091
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.188958">
<title confidence="0.999786">Harnessing WordNet Senses for Supervised Sentiment Classification</title>
<author confidence="0.532387">A Aditya Pushpak</author>
<affiliation confidence="0.545917666666667">1IITB-Monash Research Academy, IIT of Computer Science and Engineering, IIT Mumbai, India -</affiliation>
<abstract confidence="0.998386535714286">Traditional approaches to sentiment classification rely on lexical features, syntax-based features or a combination of the two. We propose semantic features using word senses for a supervised document-level sentiment classifier. To highlight the benefit of sense-based features, we compare word-based representation of documents with a sense-based representation where WordNet senses of the words are used as features. In addition, we highlight the benefit of senses by presenting a part-ofspeech-wise effect on sentiment classification. Finally, we show that even if a WSD engine disambiguates between a limited set of words in a document, a sentiment classifier still performs better than what it does in absence of sense annotation. Since word senses used as features show promise, we also examine the possibility of using similarity metrics defined on WordNet to address the problem of not finding a sense in the training corpus. We perform experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with from the training corpus. The results show promising improvement with respect to the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cem Akkaya</author>
<author>Janyce Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Subjectivity word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP ’09,</booktitle>
<pages>190--199</pages>
<contexts>
<context position="5989" citStr="Akkaya et al. (2009)" startWordPosition="938" endWordPosition="941"> the benefit of a word sense-based feature space to supervised sentiment classification. However, a word sense-based feature space is feasible subject to verification of the hypothesis that sentiment and word senses are related. Towards this, Wiebe and Mihalcea (2006) conduct a study on human annotation of 354 words senses with polarity and report a high inter-annotator agreement. The work in sentiment analysis using sense-based features, including ours, assumes this hypothesis that sense decides the sentiment. The novelty of our work lies in the following. Firstly our approach is distinctly. Akkaya et al. (2009) and Martn-Wanton et al. (2010) report performance of rule-based sentiment classification using word senses. Instead of a rule-based implementation, We used supervised learning. The supervised nature of our approach renders lexical resources unnecessary as used in Martn-Wanton et al. (2010). Rentoumi et al. (2009) suggest using word senses to detect sentence level polarity of news headlines. The authors use graph similarity to detect polarity of senses. To predict sentence level polarity, a HMM is trained on word sense and POS as the observation. The authors report that word senses particularl</context>
<context position="7303" citStr="Akkaya et al. (2009)" startWordPosition="1153" endWordPosition="1156"> document sizes in addition to generating a general purpose classifier. Another supervised approach of creating an emotional intensity classifier using concepts as features has been reported by Carrillo de Albornoz et al. (2010). This work is different based on the feature space used. The concepts used for the purpose are limited to affective classes. This restricts the size of the feature space to a limited set of labels. As opposed to this, we construct feature vectors that map to a larger sense-based space. In order to do so, we use synset offsets as representation of sense-based features. Akkaya et al. (2009), Martn-Wanton et al. (2010) and Carrillo de Albornoz et al. (2010) perform sentiment classification of individual sentences. However, we consider a document as a unit of sentiment classification i.e. our goal is to predict a document on the whole as positive or negative. This is different from Pang and Lee (2004) which suggests that sentiment is associated only with subjective content. A document in its entirety is represented using sensebased features in our experiments. Carrillo de Albornoz et al. (2010) suggests expansion using WordNet relations which we also follow. This is a benefit that</context>
</contexts>
<marker>Akkaya, Wiebe, Mihalcea, 2009</marker>
<rawString>Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009. Subjectivity word sense disambiguation. In Proc. of EMNLP ’09, pages 190–199, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted lesk algorithm for word sense disambiguation using wordnet.</title>
<date>2002</date>
<booktitle>In Proc. of CICLing’02,</booktitle>
<pages>136--145</pages>
<location>London, UK.</location>
<contexts>
<context position="16182" citStr="Banerjee and Pedersen, 2002" startWordPosition="2642" endWordPosition="2645">ilarity metric; train concept list = get list concept(T) ; test concept list = get list concept(X); for each concept C in test concept list do temp max similarity = 0 ; temp concept = C ; for each concept D in train concept list do similarity value = get similarity value(C,D,S); if (similarity value &gt; temp max similarity) then temp max similarity= similarity value; temp concept = D ; end end C = temp concept; replace synset corpus(C,X); end Return X ; Algorithm 1: Synset replacement using similarity metric Lesk: Each concept in WordNet is defined through gloss. To compute the Lesk similarity (Banerjee and Pedersen, 2002) between A and B, a scoring function based on the overlap of words in their individual glosses is used. Leacock and Chodorow (LCH): To measure similarity between two concepts A and B, Leacock and Chodorow (1998) compute the shortest path through hypernymy relation between them under the constraint that there exists such a path. The final value is computed by scaling the path length by the overall taxonomy depth (D). siMLCH(A, B) = − log Clen(A, B)1 (2) 2D 5 Experimentation We describe the variants of the corpus generated and the experiments in this section. 5.1 Data Preparation We create diffe</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted lesk algorithm for word sense disambiguation using wordnet. In Proc. of CICLing’02, pages 136–145, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Carrillo de Albornoz</author>
<author>Laura Plaza</author>
<author>Pablo Gervs</author>
</authors>
<title>Improving emotional intensity classification using word sense disambiguation. Special issue: Natural Language Processing and its Applications.</title>
<date>2010</date>
<journal>Journal on Research in Computing Science,</journal>
<pages>46--131</pages>
<marker>de Albornoz, Plaza, Gervs, 2010</marker>
<rawString>Jorge Carrillo de Albornoz, Laura Plaza, and Pablo Gervs. 2010. Improving emotional intensity classification using word sense disambiguation. Special issue: Natural Language Processing and its Applications. Journal on Research in Computing Science, 46:131–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pdraig Cunningham</author>
</authors>
<title>Dimension reduction.</title>
<date>2008</date>
<booktitle>In Machine Learning Techniques for Multimedia, Cognitive Technologies,</booktitle>
<pages>91--112</pages>
<contexts>
<context position="21722" citStr="Cunningham, 2008" startWordPosition="3527" endWordPosition="3529">ults show that negative recall increases by around 8% in case of sense-based representation of documents. The combined model of words and manually annotated senses (Words + Senses (M)) gives the best performance with an accuracy of 90.2%. This leads to an improvement of 5.3% over the baseline accuracy 3. One of the reasons for improved performance is the feature abstraction achieved due to the synsetbased features. The dimension of feature vector is reduced by a factor of 82% when the document is represented in synset space. The reduction in dimensionality may also lead to reduction in noise (Cunningham, 2008). A comparison of accuracy of different sense representations in Table 2 shows that manual disambiguation performs better than using automatic algorithms like IWSD. Although overall classification accuracy improvement of IWSD over baseline is marginal, negative recall also improves. This benefit is despite the fact that evaluation of IWSD engine over manually annotated corpus gave an overall Fscore of 71% (refer Table 1). For a WSD engine with a better accuracy, the performance of sensebased SA can be boosted further. Thus, in terms of feature representation of documents, sense-based features </context>
</contexts>
<marker>Cunningham, 2008</marker>
<rawString>Pdraig Cunningham. 2008. Dimension reduction. In Machine Learning Techniques for Multimedia, Cognitive Technologies, pages 91–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="8227" citStr="Fellbaum, 1998" startWordPosition="1310" endWordPosition="1311">(2004) which suggests that sentiment is associated only with subjective content. A document in its entirety is represented using sensebased features in our experiments. Carrillo de Albornoz et al. (2010) suggests expansion using WordNet relations which we also follow. This is a benefit that can be achieved only in a sense-based space. 1082 3 Features based on WordNet Senses In their original form, documents are said to be in lexical space since they consist of words. When the words are replaced by their corresponding senses, the resultant document is said to be in semantic space. WordNet 2.1 (Fellbaum, 1998) has been used as the sense repository. Each word/lexeme is mapped to an appropriate synset in WordNet based on its sense and represented using the corresponding synset id of WordNet. Thus, the word love is disambiguated and replaced by the identifier 21758160 which consists of a POS category identifier 2 followed by synset offset identifier 1758160. This paper refers to synset offset as synset identifiers or simply, senses. This section first gives the motivation for using word senses and then, describes the approaches that we use for our experiments. 3.1 Motivation Consider the following sen</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Gindl</author>
<author>Johannes Liegl</author>
</authors>
<title>Evaluation of different sentiment detection methods for polarity classification on web-based reviews,</title>
<date>2008</date>
<pages>35--43</pages>
<contexts>
<context position="21096" citStr="Gindl and Liegl, 2008" startWordPosition="3420" endWordPosition="3423">rison of various feature representations Table 2 shows results of classification for different feature representations. The baseline for our results is the unigram bag-of-words model (Baseline). An improvement of 4.2% is observed in the accuracy of sentiment prediction when manually annotated sense-based features (M) are used in place of word-based features (Words). The precision of both the classes using features based on semantic space is also better than one based on lexeme space. While reported results suggest that it is more difficult to detect negative sentiment than positive sentiment (Gindl and Liegl, 2008), our results show that negative recall increases by around 8% in case of sense-based representation of documents. The combined model of words and manually annotated senses (Words + Senses (M)) gives the best performance with an accuracy of 90.2%. This leads to an improvement of 5.3% over the baseline accuracy 3. One of the reasons for improved performance is the feature abstraction achieved due to the synsetbased features. The dimension of feature vector is reduced by a factor of 82% when the document is represented in synset space. The reduction in dimensionality may also lead to reduction i</context>
</contexts>
<marker>Gindl, Liegl, 2008</marker>
<rawString>Stefan Gindl and Johannes Liegl, 2008. Evaluation of different sentiment detection methods for polarity classification on web-based reviews, pages 35–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Kennedy</author>
<author>Diana Inkpen</author>
</authors>
<title>Sentiment classification of movie reviews using contextual valence shifters.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>125</pages>
<contexts>
<context position="2325" citStr="Kennedy and Inkpen, 2006" startWordPosition="350" endWordPosition="353"> to a topic. In this work, we follow the definition of Pang et al. (2002) &amp; Turney (2002) and consider a binary classification task for output labels as positive and negative. Traditional supervised approaches for SA have explored lexeme and syntax-level units as features. Approaches using lexeme-based features use bagof-words (Pang and Lee, 2008) or identify the roles of different parts-of-speech (POS) like adjectives (Pang et al., 2002; Whitelaw et al., 2005). Approaches using syntax-based features construct parse trees (Matsumoto et al., 2005) or use text parsers to model valence shifters (Kennedy and Inkpen, 2006). Our work explores incorporation of semantics in a supervised sentiment classifier. We use the synsets in Wordnet as the feature space to represent word senses. Thus, a document consisting of words gets mapped to a document consisting of corresponding word senses. Harnessing WordNet senses as features helps us address two issues: 1. Impact of WordNet sense-based features on the performance of supervised SA 2. Use of WordNet similarity metrics to solve the problem of features unseen in the training corpus The first points deals with evaluating sense-based features against word-based features. </context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>Alistair Kennedy and Diana Inkpen. 2006. Sentiment classification of movie reviews using contextual valence shifters. Computational Intelligence, 22(2):110– 125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh Khapra</author>
<author>Sapan Shah</author>
<author>Piyush Kedia</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Domain-specific word sense disambiguation combining corpus basedand wordnet based parameters.</title>
<date>2010</date>
<booktitle>In Proc. of GWC’10,</booktitle>
<location>Mumbai, India.</location>
<contexts>
<context position="11218" citStr="Khapra et al. (2010)" startWordPosition="1814" endWordPosition="1817">erent words, i.e., words in a synset. The three scenarios motivate the use of semantic space for sentiment prediction. 3.2 Sense versus Lexeme-based Feature Representation We annotate the words in the corpus with their senses using two sense disambiguation approaches. As the first approach, manual sense annotation of documents is carried out by two annotators on two subsets of the corpus, the details of which are given in Section 5.1. This is done to determine the ideal case scenario- the skyline performance. As the second approach, a state-of-art algorithm for domain-specific WSD proposed by Khapra et al. (2010) is used to obtain an automatically sensetagged corpus. This algorithm called iterative WSD or IWSD iteratively disambiguates words by ranking the candidate senses based on a scoring function. The two types of sense-annotated corpus lead us to four feature representations for a document: 1. Word senses that have been manually annotated (M) 2. Word senses that have been annotated by an automatic WSD (I) 1083 3. Manually annotated word senses and words (both separately as features) (Words + Sense(M)) 4. Automatically annotated word senses and words (both separately as features) (Words + Sense(I)</context>
<context position="12443" citStr="Khapra et al., 2010" startWordPosition="2004" endWordPosition="2007"> first set of experiments compares the four feature representations to find the feature representation with which sentiment classification gives the best performance. W+S(M) and W+S(I) are used to overcome non-coverage of WordNet for some noun synsets. In addition to this, we also present a partof-speech-wise analysis of benefit to SA as well as effect of varying the training samples on sentiment classification accuracy. 3.3 Partial disambiguation as opposed to no disambiguation The state-of-the-art automatic WSD engine that we use performs (approximately) with 70% accuracy on tourism domain (Khapra et al., 2010). This means that the performance of SA depends on the performance of WSD which is not very high in case of the engine we use. A partially disambiguated document is a document which does not contain senses of all words. Our hypothesis is that disambiguation of even few words in a document can give better results than no disambiguation. To verify this, we create different variants of the corpus by disambiguating words which have candidate senses within a threshold. For example, a partially disambiguated variant of the corpus with threshold 3 for candidate senses is created by disambiguating wor</context>
<context position="19556" citStr="Khapra et al. (2010)" startWordPosition="3179" endWordPosition="3182">ense (M) 89.10 88.22 89.11 91.50 87.07 85.18 91.24 Words + Sense (M) 90.20 89.81 90.43 92.02 88.55 87.71 92.39 Sense (I) 85.48 85.31 85.65 87.17 83.93 83.53 87.46 Words + Sense (I) 86.08 86.28 85.92 85.87 86.38 86.69 85.46 Table 2: Classification Results; PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NPNegative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%) the synset representation, words in the corpus are annotated with synset identifiers along with POS category identifiers. For automatic sense disambiguation, we used the trained IWSD engine from Khapra et al. (2010). These synset identifiers along with POS category identifiers are then used as features. For replacement using semantic similarity measures, we used WordNet::Similarity 2.05 package by Pedersen et al. (2004). To evaluate the result, we use accuracy, F-score, recall and precision as the metrics. Classification accuracy defines the ratio of the number of true instances to the total number of instances. Recall is calculated as a ratio of the true instances found to the total number of false positives and true positives. Precision is defined as the number of true instances divided by number of tr</context>
</contexts>
<marker>Khapra, Shah, Kedia, Bhattacharyya, 2010</marker>
<rawString>Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak Bhattacharyya. 2010. Domain-specific word sense disambiguation combining corpus basedand wordnet based parameters. In Proc. of GWC’10, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context with wordnet similarity for word sense identification. In WordNet: A Lexical Reference System and its Application.</title>
<date>1998</date>
<contexts>
<context position="16393" citStr="Leacock and Chodorow (1998)" startWordPosition="2678" endWordPosition="2681">ain concept list do similarity value = get similarity value(C,D,S); if (similarity value &gt; temp max similarity) then temp max similarity= similarity value; temp concept = D ; end end C = temp concept; replace synset corpus(C,X); end Return X ; Algorithm 1: Synset replacement using similarity metric Lesk: Each concept in WordNet is defined through gloss. To compute the Lesk similarity (Banerjee and Pedersen, 2002) between A and B, a scoring function based on the overlap of words in their individual glosses is used. Leacock and Chodorow (LCH): To measure similarity between two concepts A and B, Leacock and Chodorow (1998) compute the shortest path through hypernymy relation between them under the constraint that there exists such a path. The final value is computed by scaling the path length by the overall taxonomy depth (D). siMLCH(A, B) = − log Clen(A, B)1 (2) 2D 5 Experimentation We describe the variants of the corpus generated and the experiments in this section. 5.1 Data Preparation We create different variants of the dataset by Ye et al. (2009). This dataset contains 600 positive and 591 negative reviews about seven travel destinations. Each review contains approximately 4-5 sentences with an average num</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context with wordnet similarity for word sense identification. In WordNet: A Lexical Reference System and its Application.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edda Leopold</author>
<author>J¨org Kindermann</author>
</authors>
<title>Text categorization with support vector machines. how to represent texts in input space?</title>
<date>2002</date>
<booktitle>Machine Learning,</booktitle>
<pages>46--423</pages>
<contexts>
<context position="18721" citStr="Leopold and Kindermann, 2002" startWordPosition="3053" endWordPosition="3057">n, disambiguation of adjective synsets is more critical as compared to disambiguation of noun synsets. 5.2 Experimental setup The experiments are performed using C-SVM (linear kernel with default parameters1) available as a part of LibSVM2 package. We choose to use SVM since it performs the best for sentiment classification (Pang et al., 2002). All results reported are average of five-fold cross-validation accuracies. To conduct experiments on words as features, we first perform stop-word removal. The words are not stemmed since stemming is known to be detrimental to sentiment classification (Leopold and Kindermann, 2002). To conduct the experiments based on 1C=0.0,e=0.0010 2http://www.csie.ntu.edu.tw/ cjlin/libsvm 1085 Feature Representations Accuracy(%) PF NF PP NP PR NR Words (Baseline) 84.90 85.07 84.76 84.95 84.92 85.19 84.60 Sense (M) 89.10 88.22 89.11 91.50 87.07 85.18 91.24 Words + Sense (M) 90.20 89.81 90.43 92.02 88.55 87.71 92.39 Sense (I) 85.48 85.31 85.65 87.17 83.93 83.53 87.46 Words + Sense (I) 86.08 86.28 85.92 85.87 86.38 86.69 85.46 Table 2: Classification Results; PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NPNegative Precision (%), PR-Positive Recall (%), NR-</context>
</contexts>
<marker>Leopold, Kindermann, 2002</marker>
<rawString>Edda Leopold and J¨org Kindermann. 2002. Text categorization with support vector machines. how to represent texts in input space? Machine Learning, 46:423– 444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity. In</title>
<date>1998</date>
<booktitle>In Proc. of the 15th International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<contexts>
<context position="15069" citStr="Lin (1998)" startWordPosition="2454" endWordPosition="2455">ing corpus. The replacement algorithm is described in Algorithm 1. The algorithm follows from the fact that the similarity value for a synset with itself is maximum. 4.2 Similarity metrics used We conduct different runs of the replacement algorithm using three similarity metrics, namely LIN’s similarity metric, Lesk similarity metric and Leacock and Chodorow (LCH) similarity metric. These runs generate three variants of the corpus. We compare the benefit of each of these metrics by studying their sentiment classification performance. The metrics can be described as follows: LIN: The metric by Lin (1998) uses the information content individually possessed by two concepts in addition to that shared by them. The information content shared by two concepts A and B is given by their most specific subsumer (lowest superordinate(lso). Thus, this metric defines the similarity between two concepts as 2 × log Pr(lso(A, B)) simLIN(A,B) = log Pr(A) + log Pr(B) (1) 1084 Input: Training Corpus, Test Corpus, Similarity Metric Output: New Test Corpus T:= Training Corpus; X:= Test Corpus; S:= Similarity metric; train concept list = get list concept(T) ; test concept list = get list concept(X); for each concep</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In In Proc. of the 15th International Conference on Machine Learning, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Martn-Wanton</author>
<author>Alexandra Balahur-Dobrescu</author>
<author>Andres Montoyo-Guijarro</author>
<author>Aurora Pons-Porrata</author>
</authors>
<title>Word sense disambiguation in opinion mining: Pros and cons.</title>
<date>2010</date>
<booktitle>In Proc. of CICLing’10,</booktitle>
<location>Madrid,Spain.</location>
<contexts>
<context position="6020" citStr="Martn-Wanton et al. (2010)" startWordPosition="943" endWordPosition="946">nse-based feature space to supervised sentiment classification. However, a word sense-based feature space is feasible subject to verification of the hypothesis that sentiment and word senses are related. Towards this, Wiebe and Mihalcea (2006) conduct a study on human annotation of 354 words senses with polarity and report a high inter-annotator agreement. The work in sentiment analysis using sense-based features, including ours, assumes this hypothesis that sense decides the sentiment. The novelty of our work lies in the following. Firstly our approach is distinctly. Akkaya et al. (2009) and Martn-Wanton et al. (2010) report performance of rule-based sentiment classification using word senses. Instead of a rule-based implementation, We used supervised learning. The supervised nature of our approach renders lexical resources unnecessary as used in Martn-Wanton et al. (2010). Rentoumi et al. (2009) suggest using word senses to detect sentence level polarity of news headlines. The authors use graph similarity to detect polarity of senses. To predict sentence level polarity, a HMM is trained on word sense and POS as the observation. The authors report that word senses particularly help understanding metaphors </context>
<context position="7331" citStr="Martn-Wanton et al. (2010)" startWordPosition="1157" endWordPosition="1160">ition to generating a general purpose classifier. Another supervised approach of creating an emotional intensity classifier using concepts as features has been reported by Carrillo de Albornoz et al. (2010). This work is different based on the feature space used. The concepts used for the purpose are limited to affective classes. This restricts the size of the feature space to a limited set of labels. As opposed to this, we construct feature vectors that map to a larger sense-based space. In order to do so, we use synset offsets as representation of sense-based features. Akkaya et al. (2009), Martn-Wanton et al. (2010) and Carrillo de Albornoz et al. (2010) perform sentiment classification of individual sentences. However, we consider a document as a unit of sentiment classification i.e. our goal is to predict a document on the whole as positive or negative. This is different from Pang and Lee (2004) which suggests that sentiment is associated only with subjective content. A document in its entirety is represented using sensebased features in our experiments. Carrillo de Albornoz et al. (2010) suggests expansion using WordNet relations which we also follow. This is a benefit that can be achieved only in a s</context>
</contexts>
<marker>Martn-Wanton, Balahur-Dobrescu, Montoyo-Guijarro, Pons-Porrata, 2010</marker>
<rawString>Tamara Martn-Wanton, Alexandra Balahur-Dobrescu, Andres Montoyo-Guijarro, and Aurora Pons-Porrata. 2010. Word sense disambiguation in opinion mining: Pros and cons. In Proc. of CICLing’10, Madrid,Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shotaro Matsumoto</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Sentiment classification using word sub-sequences and dependency sub-trees.</title>
<date>2005</date>
<booktitle>In Proc. of PAKDD’05,, Lecture Notes in Computer Science,</booktitle>
<pages>301--311</pages>
<contexts>
<context position="2252" citStr="Matsumoto et al., 2005" startWordPosition="338" endWordPosition="341">tive or neutral from the perspective of the speaker/writer with respect to a topic. In this work, we follow the definition of Pang et al. (2002) &amp; Turney (2002) and consider a binary classification task for output labels as positive and negative. Traditional supervised approaches for SA have explored lexeme and syntax-level units as features. Approaches using lexeme-based features use bagof-words (Pang and Lee, 2008) or identify the roles of different parts-of-speech (POS) like adjectives (Pang et al., 2002; Whitelaw et al., 2005). Approaches using syntax-based features construct parse trees (Matsumoto et al., 2005) or use text parsers to model valence shifters (Kennedy and Inkpen, 2006). Our work explores incorporation of semantics in a supervised sentiment classifier. We use the synsets in Wordnet as the feature space to represent word senses. Thus, a document consisting of words gets mapped to a document consisting of corresponding word senses. Harnessing WordNet senses as features helps us address two issues: 1. Impact of WordNet sense-based features on the performance of supervised SA 2. Use of WordNet similarity metrics to solve the problem of features unseen in the training corpus The first points</context>
</contexts>
<marker>Matsumoto, Takamura, Okumura, 2005</marker>
<rawString>Shotaro Matsumoto, Hiroya Takamura, and Manabu Okumura. 2005. Sentiment classification using word sub-sequences and dependency sub-trees. In Proc. of PAKDD’05,, Lecture Notes in Computer Science, pages 301–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proc. of ACL’04,</booktitle>
<pages>271--278</pages>
<location>Barcelona,</location>
<contexts>
<context position="7618" citStr="Pang and Lee (2004)" startWordPosition="1206" endWordPosition="1209">ose are limited to affective classes. This restricts the size of the feature space to a limited set of labels. As opposed to this, we construct feature vectors that map to a larger sense-based space. In order to do so, we use synset offsets as representation of sense-based features. Akkaya et al. (2009), Martn-Wanton et al. (2010) and Carrillo de Albornoz et al. (2010) perform sentiment classification of individual sentences. However, we consider a document as a unit of sentiment classification i.e. our goal is to predict a document on the whole as positive or negative. This is different from Pang and Lee (2004) which suggests that sentiment is associated only with subjective content. A document in its entirety is represented using sensebased features in our experiments. Carrillo de Albornoz et al. (2010) suggests expansion using WordNet relations which we also follow. This is a benefit that can be achieved only in a sense-based space. 1082 3 Features based on WordNet Senses In their original form, documents are said to be in lexical space since they consist of words. When the words are replaced by their corresponding senses, the resultant document is said to be in semantic space. WordNet 2.1 (Fellba</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proc. of ACL’04, pages 271–278, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<journal>Found. Trends Inf. Retr.,</journal>
<pages>2--1</pages>
<contexts>
<context position="2049" citStr="Pang and Lee, 2008" startWordPosition="308" endWordPosition="311">ising improvement with respect to the baseline. 1 Introduction Sentiment Analysis (SA) is the task of prediction of opinion in text. Sentiment classification deals with tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic. In this work, we follow the definition of Pang et al. (2002) &amp; Turney (2002) and consider a binary classification task for output labels as positive and negative. Traditional supervised approaches for SA have explored lexeme and syntax-level units as features. Approaches using lexeme-based features use bagof-words (Pang and Lee, 2008) or identify the roles of different parts-of-speech (POS) like adjectives (Pang et al., 2002; Whitelaw et al., 2005). Approaches using syntax-based features construct parse trees (Matsumoto et al., 2005) or use text parsers to model valence shifters (Kennedy and Inkpen, 2006). Our work explores incorporation of semantics in a supervised sentiment classifier. We use the synsets in Wordnet as the feature space to represent word senses. Thus, a document consisting of words gets mapped to a document consisting of corresponding word senses. Harnessing WordNet senses as features helps us address two</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2:1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<volume>10</volume>
<pages>79--86</pages>
<contexts>
<context position="1773" citStr="Pang et al. (2002)" startWordPosition="268" endWordPosition="271"> address the problem of not finding a sense in the training corpus. We perform experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline. 1 Introduction Sentiment Analysis (SA) is the task of prediction of opinion in text. Sentiment classification deals with tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic. In this work, we follow the definition of Pang et al. (2002) &amp; Turney (2002) and consider a binary classification task for output labels as positive and negative. Traditional supervised approaches for SA have explored lexeme and syntax-level units as features. Approaches using lexeme-based features use bagof-words (Pang and Lee, 2008) or identify the roles of different parts-of-speech (POS) like adjectives (Pang et al., 2002; Whitelaw et al., 2005). Approaches using syntax-based features construct parse trees (Matsumoto et al., 2005) or use text parsers to model valence shifters (Kennedy and Inkpen, 2006). Our work explores incorporation of semantics i</context>
<context position="18042" citStr="Pang et al., 2002" startWordPosition="2951" endWordPosition="2954">rb 4114 71.16 70.90 71.03 Adjective 6194 67.26 66.31 66.78 Verb 11507 68.28 67.97 68.12 Overall 34508 71.12 70.65 70.88 Table 1: Annotation Statistics for IWSD; P- Precision,RRecall The second variant of the corpus contains word senses obtained from automatic disambiguation using IWSD. The evaluation statistics of the IWSD is shown in Table 1. Table 1 shows that the F-score for noun synsets is high while that for adjective synsets is the lowest among all. The low recall for adjective POS based synsets can be detrimental to classification since adjectives are known to express direct sentiment (Pang et al., 2002). Hence, in the context of sentiment classification, disambiguation of adjective synsets is more critical as compared to disambiguation of noun synsets. 5.2 Experimental setup The experiments are performed using C-SVM (linear kernel with default parameters1) available as a part of LibSVM2 package. We choose to use SVM since it performs the best for sentiment classification (Pang et al., 2002). All results reported are average of five-fold cross-validation accuracies. To conduct experiments on words as features, we first perform stop-word removal. The words are not stemmed since stemming is kno</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. volume 10, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity: measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Demonstration Papers at HLTNAACL’04,</booktitle>
<pages>38--41</pages>
<contexts>
<context position="19764" citStr="Pedersen et al. (2004)" startWordPosition="3208" endWordPosition="3212">6.38 86.69 85.46 Table 2: Classification Results; PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NPNegative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%) the synset representation, words in the corpus are annotated with synset identifiers along with POS category identifiers. For automatic sense disambiguation, we used the trained IWSD engine from Khapra et al. (2010). These synset identifiers along with POS category identifiers are then used as features. For replacement using semantic similarity measures, we used WordNet::Similarity 2.05 package by Pedersen et al. (2004). To evaluate the result, we use accuracy, F-score, recall and precision as the metrics. Classification accuracy defines the ratio of the number of true instances to the total number of instances. Recall is calculated as a ratio of the true instances found to the total number of false positives and true positives. Precision is defined as the number of true instances divided by number of true positives and false negatives. Positive Precision (PP) and Positive Recall (PR) are precision and recall for positive documents while Negative Precision (NP) and Negative Recall (NR) are precision and reca</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity: measuring the relatedness of concepts. In Demonstration Papers at HLTNAACL’04, pages 38–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vassiliki Rentoumi</author>
<author>George Giannakopoulos</author>
<author>Vangelis Karkaletsis</author>
<author>George A Vouros</author>
</authors>
<title>Sentiment analysis of figurative language using a word sense disambiguation approach.</title>
<date>2009</date>
<booktitle>In Proc. of the International Conference RANLP’09,</booktitle>
<pages>370--375</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="6304" citStr="Rentoumi et al. (2009)" startWordPosition="986" endWordPosition="989">ords senses with polarity and report a high inter-annotator agreement. The work in sentiment analysis using sense-based features, including ours, assumes this hypothesis that sense decides the sentiment. The novelty of our work lies in the following. Firstly our approach is distinctly. Akkaya et al. (2009) and Martn-Wanton et al. (2010) report performance of rule-based sentiment classification using word senses. Instead of a rule-based implementation, We used supervised learning. The supervised nature of our approach renders lexical resources unnecessary as used in Martn-Wanton et al. (2010). Rentoumi et al. (2009) suggest using word senses to detect sentence level polarity of news headlines. The authors use graph similarity to detect polarity of senses. To predict sentence level polarity, a HMM is trained on word sense and POS as the observation. The authors report that word senses particularly help understanding metaphors in these sentences. Our work differs in terms of the corpus and document sizes in addition to generating a general purpose classifier. Another supervised approach of creating an emotional intensity classifier using concepts as features has been reported by Carrillo de Albornoz et al.</context>
</contexts>
<marker>Rentoumi, Giannakopoulos, Karkaletsis, Vouros, 2009</marker>
<rawString>Vassiliki Rentoumi, George Giannakopoulos, Vangelis Karkaletsis, and George A. Vouros. 2009. Sentiment analysis of figurative language using a word sense disambiguation approach. In Proc. of the International Conference RANLP’09, pages 370–375, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proc. of ACL’02,</booktitle>
<pages>417--424</pages>
<location>Philadelphia, US.</location>
<contexts>
<context position="1789" citStr="Turney (2002)" startWordPosition="273" endWordPosition="274">of not finding a sense in the training corpus. We perform experiments using three popular similarity metrics to mitigate the effect of unknown synsets in a test corpus by replacing them with similar synsets from the training corpus. The results show promising improvement with respect to the baseline. 1 Introduction Sentiment Analysis (SA) is the task of prediction of opinion in text. Sentiment classification deals with tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic. In this work, we follow the definition of Pang et al. (2002) &amp; Turney (2002) and consider a binary classification task for output labels as positive and negative. Traditional supervised approaches for SA have explored lexeme and syntax-level units as features. Approaches using lexeme-based features use bagof-words (Pang and Lee, 2008) or identify the roles of different parts-of-speech (POS) like adjectives (Pang et al., 2002; Whitelaw et al., 2005). Approaches using syntax-based features construct parse trees (Matsumoto et al., 2005) or use text parsers to model valence shifters (Kennedy and Inkpen, 2006). Our work explores incorporation of semantics in a supervised s</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proc. of ACL’02, pages 417–424, Philadelphia, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
<author>Navendu Garg</author>
<author>Shlomo Argamon</author>
</authors>
<title>Using appraisal groups for sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proc. of CIKM ’05,</booktitle>
<pages>625--631</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="2165" citStr="Whitelaw et al., 2005" startWordPosition="327" endWordPosition="330">of opinion in text. Sentiment classification deals with tagging text as positive, negative or neutral from the perspective of the speaker/writer with respect to a topic. In this work, we follow the definition of Pang et al. (2002) &amp; Turney (2002) and consider a binary classification task for output labels as positive and negative. Traditional supervised approaches for SA have explored lexeme and syntax-level units as features. Approaches using lexeme-based features use bagof-words (Pang and Lee, 2008) or identify the roles of different parts-of-speech (POS) like adjectives (Pang et al., 2002; Whitelaw et al., 2005). Approaches using syntax-based features construct parse trees (Matsumoto et al., 2005) or use text parsers to model valence shifters (Kennedy and Inkpen, 2006). Our work explores incorporation of semantics in a supervised sentiment classifier. We use the synsets in Wordnet as the feature space to represent word senses. Thus, a document consisting of words gets mapped to a document consisting of corresponding word senses. Harnessing WordNet senses as features helps us address two issues: 1. Impact of WordNet sense-based features on the performance of supervised SA 2. Use of WordNet similarity </context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 2005. Using appraisal groups for sentiment analysis. In Proc. of CIKM ’05, pages 625–631, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Word sense and subjectivity.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL’06,</booktitle>
<pages>1065--1072</pages>
<contexts>
<context position="5637" citStr="Wiebe and Mihalcea (2006)" startWordPosition="882" endWordPosition="885"> the similaritybased replacement technique using WordNet synsets in section 4. Our experiments have been described in section 5. In section 6, we present our results and related discussions. Section 7 analyzes some of the causes for erroneous classification. Finally, section 8 concludes the paper and points to future work. 2 Related Work This work studies the benefit of a word sense-based feature space to supervised sentiment classification. However, a word sense-based feature space is feasible subject to verification of the hypothesis that sentiment and word senses are related. Towards this, Wiebe and Mihalcea (2006) conduct a study on human annotation of 354 words senses with polarity and report a high inter-annotator agreement. The work in sentiment analysis using sense-based features, including ours, assumes this hypothesis that sense decides the sentiment. The novelty of our work lies in the following. Firstly our approach is distinctly. Akkaya et al. (2009) and Martn-Wanton et al. (2010) report performance of rule-based sentiment classification using word senses. Instead of a rule-based implementation, We used supervised learning. The supervised nature of our approach renders lexical resources unnece</context>
</contexts>
<marker>Wiebe, Mihalcea, 2006</marker>
<rawString>Janyce Wiebe and Rada Mihalcea. 2006. Word sense and subjectivity. In Proc. of COLING-ACL’06, pages 1065–1072.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Ye</author>
<author>Ziqiong Zhang</author>
<author>Rob Law</author>
</authors>
<title>Sentiment classification of online reviews to travel destinations by supervised machine learning approaches. Expert Systems with Applications,</title>
<date>2009</date>
<volume>36</volume>
<issue>3</issue>
<pages>6535</pages>
<contexts>
<context position="3674" citStr="Ye et al. (2009)" startWordPosition="565" endWordPosition="568">nse space. Since sense-based features prove to generate superior sentiment classifiers, we get an opportunity to mitigate unknown 1081 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1081–1091, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics synsets in the test corpus by replacing them with known synsets in the training corpus. Note that such replacement is not possible if word-based representation were used as it is not feasible to make such a large number of similarity comparisons. We use the corpus by Ye et al. (2009) that consists of travel domain reviews marked as positive or negative at the document level. Our experiments on studying the impact of Wordnet sense-based features deal with variants of this corpus manually or automatically annotated with senses. Besides showing the overall impact, we perform a POS-wise analysis of the benefit to SA. In addition, we compare the effect of varying training samples on a sentiment classifier developed using word based features and sense based features. Through empirical evidence, we also show that disambiguating some words in a document also provides a better acc</context>
<context position="16830" citStr="Ye et al. (2009)" startWordPosition="2752" endWordPosition="2755">tion based on the overlap of words in their individual glosses is used. Leacock and Chodorow (LCH): To measure similarity between two concepts A and B, Leacock and Chodorow (1998) compute the shortest path through hypernymy relation between them under the constraint that there exists such a path. The final value is computed by scaling the path length by the overall taxonomy depth (D). siMLCH(A, B) = − log Clen(A, B)1 (2) 2D 5 Experimentation We describe the variants of the corpus generated and the experiments in this section. 5.1 Data Preparation We create different variants of the dataset by Ye et al. (2009). This dataset contains 600 positive and 591 negative reviews about seven travel destinations. Each review contains approximately 4-5 sentences with an average number of words per review being 80-85. To create the manually annotated corpus, two human annotators annotate words in the corpus with senses for two disjoint subsets of the original corpus by Ye et al. (2009). The inter-annotation agreement for a subset of the corpus showed 91% sense overlap. The manually annotated corpus consists of 34508 words with 6004 synsets. POS #Words P(%) R(%) F-Score(%) Noun 12693 75.54 75.12 75.33 Adverb 411</context>
</contexts>
<marker>Ye, Zhang, Law, 2009</marker>
<rawString>Qiang Ye, Ziqiong Zhang, and Rob Law. 2009. Sentiment classification of online reviews to travel destinations by supervised machine learning approaches. Expert Systems with Applications, 36(3):6527 – 6535.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>