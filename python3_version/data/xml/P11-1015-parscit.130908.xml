<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000567">
<title confidence="0.994887">
Learning Word Vectors for Sentiment Analysis
</title>
<author confidence="0.97321">
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
Andrew Y. Ng, and Christopher Potts
</author>
<affiliation confidence="0.971697">
Stanford University
</affiliation>
<address confidence="0.850183">
Stanford, CA 94305
</address>
<email confidence="0.96715">
[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu
</email>
<sectionHeader confidence="0.995372" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999730863636364">
Unsupervised vector-based approaches to se-
mantics can model rich lexical meanings, but
they largely fail to capture sentiment informa-
tion that is central to many word meanings and
important for a wide range of NLP tasks. We
present a model that uses a mix of unsuper-
vised and supervised techniques to learn word
vectors capturing semantic term–document in-
formation as well as rich sentiment content.
The proposed model can leverage both con-
tinuous and multi-dimensional sentiment in-
formation as well as non-sentiment annota-
tions. We instantiate the model to utilize the
document-level sentiment polarity annotations
present in many online documents (e.g. star
ratings). We evaluate the model using small,
widely used sentiment and subjectivity cor-
pora and find it out-performs several previ-
ously introduced methods for sentiment clas-
sification. We also introduce a large dataset
of movie reviews to serve as a more robust
benchmark for work in this area.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995236704545455">
Word representations are a critical component of
many natural language processing systems. It is
common to represent words as indices in a vocab-
ulary, but this fails to capture the rich relational
structure of the lexicon. Vector-based models do
much better in this regard. They encode continu-
ous similarities between words as distance or angle
between word vectors in a high-dimensional space.
The general approach has proven useful in tasks
such as word sense disambiguation, named entity1
recognition, part of speech tagging, and document
retrieval (Turney and Pantel, 2010; Collobert and
Weston, 2008; Turian et al., 2010).
In this paper, we present a model to capture both
semantic and sentiment similarities among words.
The semantic component of our model learns word
vectors via an unsupervised probabilistic model of
documents. However, in keeping with linguistic and
cognitive research arguing that expressive content
and descriptive semantic content are distinct (Ka-
plan, 1999; Jay, 2000; Potts, 2007), we find that
this basic model misses crucial sentiment informa-
tion. For example, while it learns that wonderful
and amazing are semantically close, it doesn’t cap-
ture the fact that these are both very strong positive
sentiment words, at the opposite end of the spectrum
from terrible and awful.
Thus, we extend the model with a supervised
sentiment component that is capable of embracing
many social and attitudinal aspects of meaning (Wil-
son et al., 2004; Alm et al., 2005; Andreevskaia
and Bergler, 2006; Pang and Lee, 2005; Goldberg
and Zhu, 2006; Snyder and Barzilay, 2007). This
component of the model uses the vector represen-
tation of words to predict the sentiment annotations
on contexts in which the words appear. This causes
words expressing similar sentiment to have similar
vector representations. The full objective function
of the model thus learns semantic vectors that are
imbued with nuanced sentiment information. In our
experiments, we show how the model can leverage
document-level sentiment annotations of a sort that
are abundant online in the form of consumer reviews
42 for movies, products, etc. The technique is suffi-
</bodyText>
<note confidence="0.868871666666667">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 142–150,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
1 43
</note>
<bodyText confidence="0.998124529411765">
ciently general to work also with continuous and
multi-dimensional notions of sentiment as well as
non-sentiment annotations (e.g., political affiliation,
speaker commitment).
After presenting the model in detail, we pro-
vide illustrative examples of the vectors it learns,
and then we systematically evaluate the approach
on document-level and sentence-level classification
tasks. Our experiments involve the small, widely
used sentiment and subjectivity corpora of Pang and
Lee (2004), which permits us to make comparisons
with a number of related approaches and published
results. We also show that this dataset contains many
correlations between examples in the training and
testing sets. This leads us to evaluate on, and make
publicly available, a large dataset of informal movie
reviews from the Internet Movie Database (IMDB).
</bodyText>
<sectionHeader confidence="0.99966" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999870575757576">
The model we present in the next section draws in-
spiration from prior work on both probabilistic topic
modeling and vector-spaced models for word mean-
ings.
Latent Dirichlet Allocation (LDA; (Blei et al.,
2003)) is a probabilistic document model that as-
sumes each document is a mixture of latent top-
ics. For each latent topic T, the model learns a
conditional distribution p(wIT) for the probability
that word w occurs in T. One can obtain a k-
dimensional vector representation of words by first
training a k-topic model and then filling the matrix
with the p(wIT) values (normalized to unit length).
The result is a word–topic matrix in which the rows
are taken to represent word meanings. However,
because the emphasis in LDA is on modeling top-
ics, not word meanings, there is no guarantee that
the row (word) vectors are sensible as points in a
k-dimensional space. Indeed, we show in section
4 that using LDA in this way does not deliver ro-
bust word vectors. The semantic component of our
model shares its probabilistic foundation with LDA,
but is factored in a manner designed to discover
word vectors rather than latent topics. Some recent
work introduces extensions of LDA to capture sen-
timent in addition to topical information (Li et al.,
2010; Lin and He, 2009; Boyd-Graber and Resnik,
2010). Like LDA, these methods focus on model-
ing sentiment-imbued topics rather than embedding
words in a vector space.
Vector space models (VSMs) seek to model words
directly (Turney and Pantel, 2010). Latent Seman-
tic Analysis (LSA), perhaps the best known VSM,
explicitly learns semantic word vectors by apply-
ing singular value decomposition (SVD) to factor a
term–document co-occurrence matrix. It is typical
to weight and normalize the matrix values prior to
SVD. To obtain a k-dimensional representation for a
given word, only the entries corresponding to the k
largest singular values are taken from the word’s ba-
sis in the factored matrix. Such matrix factorization-
based approaches are extremely successful in prac-
tice, but they force the researcher to make a number
of design choices (weighting, normalization, dimen-
sionality reduction algorithm) with little theoretical
guidance to suggest which to prefer.
Using term frequency (tf) and inverse document
frequency (idf) weighting to transform the values
in a VSM often increases the performance of re-
trieval and categorization systems. Delta idf weight-
ing (Martineau and Finin, 2009) is a supervised vari-
ant of idf weighting in which the idf calculation is
done for each document class and then one value
is subtracted from the other. Martineau and Finin
present evidence that this weighting helps with sen-
timent classification, and Paltoglou and Thelwall
(2010) systematically explore a number of weight-
ing schemes in the context of sentiment analysis.
The success of delta idf weighting in previous work
suggests that incorporating sentiment information
into VSM values via supervised methods is help-
ful for sentiment analysis. We adopt this insight,
but we are able to incorporate it directly into our
model’s objective function. (Section 4 compares
our approach with a representative sample of such
weighting schemes.)
</bodyText>
<sectionHeader confidence="0.999423" genericHeader="method">
3 Our Model
</sectionHeader>
<bodyText confidence="0.9998052">
To capture semantic similarities among words, we
derive a probabilistic model of documents which
learns word representations. This component does
not require labeled data, and shares its foundation
with probabilistic topic models such as LDA. The
sentiment component of our model uses sentiment
annotations to constrain words expressing similar
sentiment to have similar representations. We can
efficiently learn parameters for the joint objective
function using alternating maximization.
</bodyText>
<subsectionHeader confidence="0.999947">
3.1 Capturing Semantic Similarities
</subsectionHeader>
<bodyText confidence="0.9997804">
We build a probabilistic model of a document us-
ing a continuous mixture distribution over words in-
dexed by a multi-dimensional random variable θ.
We assume words in a document are conditionally
independent given the mixture variable θ. We assign
a probability to a document d using a joint distribu-
tion over the document and θ. The model assumes
each word wi E d is conditionally independent of
the other words given θ. The probability of a docu-
ment is thus
</bodyText>
<equation confidence="0.980642">
N
p(d) =Zp(d, θ)dθ =Zp(θ) Y p(wi|θ)dθ. (1)
i=1
</equation>
<bodyText confidence="0.999525066666667">
Where N is the number of words in d and wi is
the ith word in d. We use a Gaussian prior on θ.
We define the conditional distribution p(wi|θ) us-
ing a log-linear model with parameters R and b.
The energy function uses a word representation ma-
trix R E R(β � |V |) where each word w (represented
as a one-on vector) in the vocabulary V has a β-
dimensional vector representation φw = Rw corre-
sponding to that word’s column in R. The random
variable θ is also a β-dimensional vector, θ E Rβ
which weights each of the β dimensions of words’
representation vectors. We additionally introduce a
bias bw for each word to capture differences in over-
all word frequencies. The energy assigned to a word
w given these model parameters is
</bodyText>
<equation confidence="0.988769">
E(w; θ, φw, bw) = −θTφw − bw. (2)
</equation>
<bodyText confidence="0.63147">
To obtain the distribution p(w|θ) we use a softmax,
</bodyText>
<equation confidence="0.99552775">
exp(−E(w; θ, φw, bw))
p(w|θ; R, b) =
Pw′EV exp(−E(wl; θ, φw′, bw′))
(3)
</equation>
<bodyText confidence="0.999980136363636">
how closely its representation vector φw matches the
scaling direction of θ. This idea is similar to the
word vector inner product used in the log-bilinear
language model of Mnih and Hinton (2007).
Equation 1 resembles the probabilistic model of
LDA (Blei et al., 2003), which models documents
as mixtures of latent topics. One could view the en-
tries of a word vector φ as that word’s association
strength with respect to each latent topic dimension.
The random variable θ then defines a weighting over
topics. However, our model does not attempt to
model individual topics, but instead directly models
word probabilities conditioned on the topic mixture
variable θ. Because of the log-linear formulation of
the conditional distribution, θ is a vector in Rβ and
not restricted to the unit simplex as it is in LDA.
We now derive maximum likelihood learning for
this model when given a set of unlabeled documents
D. In maximum likelihood learning we maximize
the probability of the observed data given the model
parameters. We assume documents dk E D are i.i.d.
samples. Thus the learning problem becomes
</bodyText>
<equation confidence="0.97417975">
Nk
p(D; R, b) = YZp(θ) Y p(wi|θ; R, b)dθ.
dkED i=1
(5)
</equation>
<bodyText confidence="0.9915975">
Using maximum a posteriori (MAP) estimates for θ,
we approximate this learning problem as
</bodyText>
<equation confidence="0.931606">
p(wi|ˆθk; R, b), (6)
</equation>
<bodyText confidence="0.99982175">
where ˆθk denotes the MAP estimate of θ for dk.
We introduce a Frobenious norm regularization term
for the word representation matrix R. The word bi-
ases b are not regularized reflecting the fact that we
want the biases to capture whatever overall word fre-
quency statistics are present in the data. By taking
the logarithm and simplifying we obtain the final ob-
jective,
</bodyText>
<equation confidence="0.8885201">
max
R,b
Ymax
R,b dkED
YNk
i=1
p(ˆθk)
exp(θT φw + bw)
Pw′EV exp(θTφw′ + bw′).
(4)
</equation>
<bodyText confidence="0.999280166666667">
The number of terms in the denominator’s sum-
mation grows linearly in |V |, making exact com-
putation of the distribution possible. For a given
θ, a word w’s occurrence probability is related to144
which is maximized with respect to R and b. The
hyper-parameters in the model are the regularization
</bodyText>
<equation confidence="0.991596333333333">
ν||R||2F + X λ||ˆθk||22 + XNk log p(wi |ˆθk; R, b),
dkED i=1
(7)
</equation>
<bodyText confidence="0.8205475">
weights (λ and ν), and the word vector dimension-
ality β.
</bodyText>
<subsectionHeader confidence="0.999628">
3.2 Capturing Word Sentiment
</subsectionHeader>
<bodyText confidence="0.999977863636364">
The model presented so far does not explicitly cap-
ture sentiment information. Applying this algorithm
to documents will produce representations where
words that occur together in documents have sim-
ilar representations. However, this unsupervised
approach has no explicit way of capturing which
words are predictive of sentiment as opposed to
content-related. Much previous work in natural lan-
guage processing achieves better representations by
learning from multiple tasks (Collobert and Weston,
2008; Finkel and Manning, 2009). Following this
theme we introduce a second task to utilize labeled
documents to improve our model’s word representa-
tions.
Sentiment is a complex, multi-dimensional con-
cept. Depending on which aspects of sentiment we
wish to capture, we can give some body of text a
sentiment label s which can be categorical, continu-
ous, or multi-dimensional. To leverage such labels,
we introduce an objective that the word vectors of
our model should predict the sentiment label using
some appropriate predictor,
</bodyText>
<equation confidence="0.993949">
s = f(φw). (8)
</equation>
<bodyText confidence="0.999976125">
Using an appropriate predictor function f(x) we
map a word vector φw to a predicted sentiment label
s. We can then improve our word vector φw to better
predict the sentiment labels of contexts in which that
word occurs.
For simplicity we consider the case where the sen-
timent label s is a scalar continuous value repre-
senting sentiment polarity of a document. This cap-
tures the case of many online reviews where doc-
uments are associated with a label on a star rating
scale. We linearly map such star values to the inter-
val s E [0, 1] and treat them as a probability of pos-
itive sentiment polarity. Using this formulation, we
employ a logistic regression as our predictor f(x).
We use w’s vector representation φw and regression
weights ψ to express this as
</bodyText>
<equation confidence="0.917009">
p(s = 1|w; R, ψ) = σ(ψT φw + bc), (9)
</equation>
<page confidence="0.96015">
145
</page>
<bodyText confidence="0.999994235294118">
where σ(x) is the logistic function and ψ E Rβ is the
logistic regression weight vector. We additionally
introduce a scalar bias bc for the classifier.
The logistic regression weights ψ and bc define
a linear hyperplane in the word vector space where
a word vector’s positive sentiment probability de-
pends on where it lies with respect to this hyper-
plane. Learning over a collection of documents re-
sults in words residing different distances from this
hyperplane based on the average polarity of docu-
ments in which the words occur.
Given a set of labeled documents D where sk is
the sentiment label for document dk, we wish to
maximize the probability of document labels given
the documents. We assume documents in the collec-
tion and words within a document are i.i.d. samples.
By maximizing the log-objective we obtain,
</bodyText>
<equation confidence="0.923497">
log p(sk|wi; R, ψ, bc). (10)
</equation>
<bodyText confidence="0.9174645">
The conditional probability p(sk|wi; R, ψ, bc) is
easily obtained from equation 9.
</bodyText>
<subsectionHeader confidence="0.997266">
3.3 Learning
</subsectionHeader>
<bodyText confidence="0.996380666666667">
The full learning objective maximizes a sum of the
two objectives presented. This produces a final ob-
jective function of,
|Sk |denotes the number of documents in the dataset
with the same rounded value of sk (i.e. sk &lt; 0.5
and sk &gt; 0.5). We introduce the weighting 1
</bodyText>
<subsectionHeader confidence="0.385518">
|Sk |to
</subsectionHeader>
<bodyText confidence="0.9998461">
combat the well-known imbalance in ratings present
in review collections. This weighting prevents the
overall distribution of document ratings from affect-
ing the estimate of document ratings in which a par-
ticular word occurs. The hyper-parameters of the
model are the regularization weights (λ and ν), and
the word vector dimensionality β.
Maximizing the objective function with respect to
R, b, ψ, and bc is a non-convex problem. We use
alternating maximization, which first optimizes the
</bodyText>
<equation confidence="0.996103722222222">
|D|
X
k=1
max
R,ψ,b�
XNk
i=1
ν||R||2F + |D |λ||�θk||22 + XNk log p(wi |�θk; R, b)
X i=1
k=1
log p(sk|wi; R, ψ, bc). (11)
1
+
|Sk|
X |D|
k=1
XNk
i=1
</equation>
<bodyText confidence="0.99931475">
word representations (R, b, 0, and b,) while leav-
ing the MAP estimates (B) fixed. Then we find the
new MAP estimate for each document while leav-
ing the word representations fixed, and continue this
process until convergence. The optimization algo-
rithm quickly finds a global solution for each Bk be-
cause we have a low-dimensional, convex problem
in each Bk. Because the MAP estimation problems
for different documents are independent, we can
solve them on separate machines in parallel. This
facilitates scaling the model to document collections
with hundreds of thousands of documents.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999990555555556">
We evaluate our model with document-level and
sentence-level categorization tasks in the domain of
online movie reviews. For document categoriza-
tion, we compare our method to previously pub-
lished results on a standard dataset, and introduce
a new dataset for the task. In both tasks we com-
pare our model’s word representations with several
bag of words weighting methods, and alternative ap-
proaches to word vector induction.
</bodyText>
<subsectionHeader confidence="0.997344">
4.1 Word Representation Learning
</subsectionHeader>
<bodyText confidence="0.998212227272727">
We induce word representations with our model us-
ing 25,000 movie reviews from IMDB. Because
some movies receive substantially more reviews
than others, we limited ourselves to including at
most 30 reviews from any movie in the collection.
We build a fixed dictionary of the 5,000 most fre-
quent tokens, but ignore the 50 most frequent terms
from the original full vocabulary. Traditional stop
word removal was not used because certain stop
words (e.g. negating words) are indicative of senti-
ment. Stemming was not applied because the model
learns similar representations for words of the same
stem when the data suggests it. Additionally, be-
cause certain non-word tokens (e.g. “!” and “:-)” )
are indicative of sentiment, we allow them in our vo-
cabulary. Ratings on IMDB are given as star values
(E 11, 2, ...,10}), which we linearly map to [0, 1] to
use as document labels when training our model.
The semantic component of our model does not
require document labels. We train a variant of our
model which uses 50,000 unlabeled reviews in addi-
tion to the labeled set of 25,000 reviews. The unla-
</bodyText>
<page confidence="0.99569">
146
</page>
<bodyText confidence="0.9811704">
beled set of reviews contains neutral reviews as well
as those which are polarized as found in the labeled
set. Training the model with additional unlabeled
data captures a common scenario where the amount
of labeled data is small relative to the amount of un-
labeled data available. For all word vector models,
we use 50-dimensional vectors.
As a qualitative assessment of word represen-
tations, we visualize the words most similar to a
query word using vector similarity of the learned
representations. Given a query word w and an-
other word w′ we obtain their vector representations
0,,, and 0,,,′, and evaluate their cosine similarity as
5(�,,,, ,,,′ — ,w�w′
By assessing the simi-
</bodyText>
<equation confidence="0.918034">
larity
) ||�w||·||�w′||
</equation>
<bodyText confidence="0.999927458333333">
larity of w with all other words w′, we can find the
words deemed most similar by the model.
Table 1 shows the most similar words to given
query words using our model’s word representations
as well as those of LSA. All of these vectors cap-
ture broad semantic similarities. However, both ver-
sions of our model seem to do better than LSA in
avoiding accidental distributional similarities (e.g.,
screwball and grant as similar to romantic) A com-
parison of the two versions of our model also begins
to highlight the importance of adding sentiment in-
formation. In general, words indicative of sentiment
tend to have high similarity with words of the same
sentiment polarity, so even the purely unsupervised
model’s results look promising. However, they also
show more genre and content effects. For exam-
ple, the sentiment enriched vectors for ghastly are
truly semantic alternatives to that word, whereas the
vectors without sentiment also contain some content
words that tend to have ghastly predicated of them.
Of course, this is only an impressionistic analysis of
a few cases, but it is helpful in understanding why
the sentiment-enriched model proves superior at the
sentiment classification results we report next.
</bodyText>
<subsectionHeader confidence="0.963535">
4.2 Other Word Representations
</subsectionHeader>
<bodyText confidence="0.903859714285714">
For comparison, we implemented several alternative
vector space models that are conceptually similar to
our own, as discussed in section 2:
Latent Semantic Analysis (LSA; Deerwester et
al., 1990) We apply truncated SVD to a tf.idf
weighted, cosine normalized count matrix, which
is a standard weighting and smoothing scheme for
</bodyText>
<table confidence="0.931763318181818">
Our model Our model
Sentiment + Semantic Semantic only LSA
melancholy bittersweet thoughtful poetic
heartbreaking warmth lyrical
happiness layer poetry
tenderness gentle profound
compassionate loneliness vivid
embarrassingly predators hideous
trite hideous inept
ghastly laughably tube severely
atrocious baffled grotesque
appalling smack unsuspecting
lame passable uninspired
laughable unconvincing flat
lackluster unimaginative amateurish bland
uninspired clich´ed forgettable
awful insipid mediocre
romantic romance romance romance
love charming screwball
sweet delightful grant
beautiful sweet comedies
relationship chemistry comedy
</table>
<tableCaption confidence="0.991836">
Table 1: Similarity of learned word vectors. Each target word is given with its five most similar words using cosine
</tableCaption>
<bodyText confidence="0.947264714285714">
similarity of the vectors determined by each model. The full version of our model (left) captures both lexical similarity
as well as similarity of sentiment strength and orientation. Our unsupervised semantic component (center) and LSA
(right) capture semantic relations.
VSM induction (Turney and Pantel, 2010). of such weighting variants for sentiment tasks.
Latent Dirichlet Allocation (LDA; Blei et 4.3 Document Polarity Classification
al., 2003) We use the method described in sec- Our first evaluation task is document-level senti-
tion 2 for inducing word representations from the ment polarity classification. A classifier must pre-
topic matrix. To train the 50-topic LDA model we dict whether a given review is positive or negative
use code released by Blei et al. (2003). We use the given the review text.
same 5,000 term vocabulary for LDA as is used for Given a document’s bag of words vector v, we
training word vector models. We leave the LDA obtain features from our model using a matrix-
hyperparameters at their default values, though vector product Rv, where v can have arbitrary tf.idf
some work suggests optimizing over priors for LDA weighting. We do not cosine normalize v, instead
is important (Wallach et al., 2009). applying cosine normalization to the final feature
Weighting Variants We evaluate both binary (b) vector Rv. This procedure is also used to obtain
term frequency weighting with smoothed delta idf features from the LDA and LSA word vectors. In
(At’) and no idf (n) because these variants worked preliminary experiments, we found ‘bnn’ weighting
well in previous experiments in sentiment (Mar- to work best for v when generating document fea-
tineau and Finin, 2009; Pang et al., 2002). In all tures via the product Rv. In all experiments, we
cases, we use cosine normalization (c). Paltoglou use this weighting to get multi-word representations
and Thelwall (2010) perform an extensive analysis147
</bodyText>
<table confidence="0.9997856">
Features PL04 Our Dataset Subjectivity
Bag of Words (bnc) 85.45 87.80 87.77
Bag of Words (bAt’c) 85.80 88.23 85.65
LDA 66.70 67.42 66.65
LSA 84.55 83.96 82.82
Our Semantic Only 87.10 87.30 86.65
Our Full 84.65 87.44 86.19
Our Full, Additional Unlabeled 87.05 87.99 87.22
Our Semantic + Bag of Words (bnc) 88.30 88.28 88.58
Our Full + Bag of Words (bnc) 87.85 88.33 88.45
Our Full, Add’l Unlabeled + Bag of Words (bnc) 88.90 88.89 88.13
Bag of Words SVM (Pang and Lee, 2004) 87.15 N/A 90.00
Contextual Valence Shifters (Kennedy and Inkpen, 2006) 86.20 N/A N/A
tf.Didf Weighting (Martineau and Finin, 2009) 88.10 N/A N/A
Appraisal Taxonomy (Whitelaw et al., 2005) 90.20 N/A N/A
</table>
<tableCaption confidence="0.951380666666667">
Table 2: Classification accuracy on three tasks. From left to right the datasets are: A collection of 2,000 movie reviews
often used as a benchmark of sentiment classification (Pang and Lee, 2004), 50,000 reviews we gathered from IMDB,
and the sentence subjectivity dataset also released by (Pang and Lee, 2004). All tasks are balanced two-class problems.
</tableCaption>
<bodyText confidence="0.71707">
from word vectors.
</bodyText>
<subsectionHeader confidence="0.919627">
4.3.1 Pang and Lee Movie Review Dataset
</subsectionHeader>
<bodyText confidence="0.999916708333333">
The polarity dataset version 2.0 introduced by Pang
and Lee (2004) 1 consists of 2,000 movie reviews,
where each is associated with a binary sentiment po-
larity label. We report 10-fold cross validation re-
sults using the authors’ published folds to make our
results comparable with others in the literature. We
use a linear support vector machine (SVM) classifier
trained with LIBLINEAR (Fan et al., 2008), and set
the SVM regularization parameter to the same value
used by Pang and Lee (2004).
Table 2 shows the classification performance of
our method, other VSMs we implemented, and pre-
viously reported results from the literature. Bag of
words vectors are denoted by their weighting nota-
tion. Features from word vector learner are denoted
by the learner name. As a control, we trained ver-
sions of our model with only the unsupervised se-
mantic component, and the full model (semantic and
sentiment). We also include results for a version of
our full model trained with 50,000 additional unla-
beled examples. Finally, to test whether our mod-
els’ representations complement a standard bag of
words, we evaluate performance of the two feature
representations concatenated.
</bodyText>
<footnote confidence="0.988449">
1http://www.cs.cornell.edu/people/pabo/movie-review-data 148
</footnote>
<bodyText confidence="0.99984359375">
Our method’s features clearly outperform those of
other VSMs, and perform best when combined with
the original bag of words representation. The vari-
ant of our model trained with additional unlabeled
data performed best, suggesting the model can effec-
tively utilize large amounts of unlabeled data along
with labeled examples. Our method performs com-
petitively with previously reported results in spite of
our restriction to a vocabulary of only 5,000 words.
We extracted the movie title associated with each
review and found that 1,299 of the 2,000 reviews in
the dataset have at least one other review of the same
movie in the dataset. Of 406 movies with multiple
reviews, 249 have the same polarity label for all of
their reviews. Overall, these facts suggest that, rela-
tive to the size of the dataset, there are highly corre-
lated examples with correlated labels. This is a nat-
ural and expected property of this kind of document
collection, but it can have a substantial impact on
performance in datasets of this scale. In the random
folds distributed by the authors, approximately 50%
of reviews in each validation fold’s test set have a
review of the same movie with the same label in the
training set. Because the dataset is small, a learner
may perform well by memorizing the association be-
tween label and words unique to a particular movie
(e.g., character names or plot terms).
We introduce a substantially larger dataset, which
uses disjoint sets of movies for training and testing.
These steps minimize the ability of a learner to rely
on idiosyncratic word–class associations, thereby
focusing attention on genuine sentiment features.
</bodyText>
<subsectionHeader confidence="0.420187">
4.3.2 IMDB Review Dataset
</subsectionHeader>
<bodyText confidence="0.999979103448276">
We constructed a collection of 50,000 reviews from
IMDB, allowing no more than 30 reviews per movie.
The constructed dataset contains an even number of
positive and negative reviews, so randomly guessing
yields 50% accuracy. Following previous work on
polarity classification, we consider only highly po-
larized reviews. A negative review has a score &lt; 4
out of 10, and a positive review has a score &gt; 7
out of 10. Neutral reviews are not included in the
dataset. In the interest of providing a benchmark for
future work in this area, we release this dataset to
the public.2
We evenly divided the dataset into training and
test sets. The training set is the same 25,000 la-
beled reviews used to induce word vectors with our
model. We evaluate classifier performance after
cross-validating classifier parameters on the training
set, again using a linear SVM in all cases. Table 2
shows classification performance on our subset of
IMDB reviews. Our model showed superior per-
formance to other approaches, and performed best
when concatenated with bag of words representa-
tion. Again the variant of our model which utilized
extra unlabeled data during training performed best.
Differences in accuracy are small, but, because
our test set contains 25,000 examples, the variance
of the performance estimate is quite low. For ex-
ample, an accuracy increase of 0.1% corresponds to
correctly classifying an additional 25 reviews.
</bodyText>
<subsectionHeader confidence="0.996046">
4.4 Subjectivity Detection
</subsectionHeader>
<bodyText confidence="0.99997225">
As a second evaluation task, we performed sentence-
level subjectivity classification. In this task, a clas-
sifier is trained to decide whether a given sentence is
subjective, expressing the writer’s opinions, or ob-
jective, expressing purely facts. We used the dataset
of Pang and Lee (2004), which contains subjective
sentences from movie review summaries and objec-
tive sentences from movie plot summaries. This task
</bodyText>
<footnote confidence="0.9157095">
2Dataset and further details are available online at:
http://www.andrew-maas.net/data/sentiment 149
</footnote>
<bodyText confidence="0.9999">
is substantially different from the review classifica-
tion task because it uses sentences as opposed to en-
tire documents and the target concept is subjectivity
instead of opinion polarity. We randomly split the
10,000 examples into 10 folds and report 10-fold
cross validation accuracy using the SVM training
protocol of Pang and Lee (2004).
Table 2 shows classification accuracies from the
sentence subjectivity experiment. Our model again
provided superior features when compared against
other VSMs. Improvement over the bag-of-words
baseline is obtained by concatenating the two feature
vectors.
</bodyText>
<sectionHeader confidence="0.999773" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999981451612903">
We presented a vector space model that learns word
representations captuing semantic and sentiment in-
formation. The model’s probabilistic foundation
gives a theoretically justified technique for word
vector induction as an alternative to the overwhelm-
ing number of matrix factorization-based techniques
commonly used. Our model is parametrized as a
log-bilinear model following recent success in us-
ing similar techniques for language models (Bengio
et al., 2003; Collobert and Weston, 2008; Mnih and
Hinton, 2007), and it is related to probabilistic latent
topic models (Blei et al., 2003; Steyvers and Grif-
fiths, 2006). We parametrize the topical component
of our model in a manner that aims to capture word
representations instead of latent topics. In our ex-
periments, our method performed better than LDA,
which models latent topics directly.
We extended the unsupervised model to incor-
porate sentiment information and showed how this
extended model can leverage the abundance of
sentiment-labeled texts available online to yield
word representations that capture both sentiment
and semantic relations. We demonstrated the util-
ity of such representations on two tasks of senti-
ment classification, using existing datasets as well
as a larger one that we release for future research.
These tasks involve relatively simple sentiment in-
formation, but the model is highly flexible in this
regard; it can be used to characterize a wide variety
of annotations, and thus is broadly applicable in the
growing areas of sentiment analysis and retrieval.
</bodyText>
<sectionHeader confidence="0.995488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99962825">
This work is supported by the DARPA Deep Learn-
ing program under contract number FA8650-10-C-
7020, an NSF Graduate Fellowship awarded to AM,
and ONR grant No. N00014-10-1-0109 to CP.
</bodyText>
<sectionHeader confidence="0.990305" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999950565656566">
C. O. Alm, D. Roth, and R. Sproat. 2005. Emotions from
text: machine learning for text-based emotion predic-
tion. In Proceedings ofHLT/EMNLP, pages 579–586.
A. Andreevskaia and S. Bergler. 2006. Mining Word-
Net for fuzzy sentiment: sentiment tag extraction from
WordNet glosses. In Proceedings of the European
ACL, pages 209–216.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
a neural probabilistic language model. Journal ofMa-
chine Learning Research, 3:1137–1155, August.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet allocation. Journal ofMachine Learning Re-
search, 3:993–1022, May.
J. Boyd-Graber and P. Resnik. 2010. Holistic sentiment
analysis across languages: multilingual supervised la-
tent Dirichlet allocation. In Proceedings of EMNLP,
pages 45–55.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing. In Proceedings of the
ICML, pages 160–167.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41:391–407, September.
R. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, and
C. J. Lin. 2008. LIBLINEAR: A library for large lin-
ear classification. The Journal of Machine Learning
Research, 9:1871–1874, August.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In Proceedings of NAACL,
pages 326–334.
A. B. Goldberg and J. Zhu. 2006. Seeing stars when
there aren’t many stars: graph-based semi-supervised
learning for sentiment categorization. In TextGraphs:
HLT/NAACL Workshop on Graph-based Algorithms
for Natural Language Processing, pages 45–52.
T. Jay. 2000. Why We Curse: A Neuro-Psycho-
Social Theory of Speech. John Benjamins, Philadel-
phia/Amsterdam.
D. Kaplan. 1999. What is meaning? Explorations in the
theory of Meaning as Use. Brief version — draft 1.
Ms., UCLA.
A. Kennedy and D. Inkpen. 2006. Sentiment clas-
sification of movie reviews using contextual valence
shifters. Computational Intelligence, 22:110–125,
May. 150
F. Li, M. Huang, and X. Zhu. 2010. Sentiment analysis
with global topics and local dependency. In Proceed-
ings ofAAAI, pages 1371–1376.
C. Lin and Y. He. 2009. Joint sentiment/topic model for
sentiment analysis. In Proceeding of the 18th ACM
Conference on Information and Knowledge Manage-
ment, pages 375–384.
J. Martineau and T. Finin. 2009. Delta tfidf: an improved
feature space for sentiment analysis. In Proceedings
of the 3rd AAAI International Conference on Weblogs
and Social Media, pages 258–261.
A. Mnih and G. E. Hinton. 2007. Three new graphical
models for statistical language modelling. In Proceed-
ings of the ICML, pages 641–648.
G. Paltoglou and M. Thelwall. 2010. A study of informa-
tion retrieval weighting schemes for sentiment analy-
sis. In Proceedings of the ACL, pages 1386–1395.
B. Pang and L. Lee. 2004. A sentimental education:
sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the ACL,
pages 271–278.
B. Pang and L. Lee. 2005. Seeing stars: exploiting class
relationships for sentiment categorization with respect
to rating scales. In Proceedings of ACL, pages 115–
124.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? sentiment classification using machine learning
techniques. In Proceedings of EMNLP, pages 79–86.
C. Potts. 2007. The expressive dimension. Theoretical
Linguistics, 33:165–197.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In Proceedings of
NAACL, pages 300–307.
M. Steyvers and T. L. Griffiths. 2006. Probabilistic topic
models. In T. Landauer, D McNamara, S. Dennis, and
W. Kintsch, editors, Latent Semantic Analysis: A Road
to Meaning.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. In Proceedings of the ACL, page
384394.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: vector space models of semantics. Journal
ofArtificial Intelligence Research, 37:141–188.
H. Wallach, D. Mimno, and A. McCallum. 2009. Re-
thinking LDA: why priors matter. In Proceedings of
NIPS, pages 1973–1981.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Using ap-
praisal groups for sentiment analysis. In Proceedings
of CIKM, pages 625–631.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? Finding strong and weak opinion clauses. In
Proceedings ofAAAI, pages 761–769.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.603737">
<title confidence="0.99999">Learning Word Vectors for Sentiment Analysis</title>
<author confidence="0.928434">Andrew L Maas</author>
<author confidence="0.928434">Raymond E Daly</author>
<author confidence="0.928434">Peter T Pham</author>
<author confidence="0.928434">Dan Y Ng</author>
<affiliation confidence="0.674673">Stanford</affiliation>
<address confidence="0.973643">Stanford, CA</address>
<email confidence="0.997102">[amaas,rdaly,ptpham,yuze,ang,cgpotts]@stanford.edu</email>
<abstract confidence="0.998795260869565">Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term–document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C O Alm</author>
<author>D Roth</author>
<author>R Sproat</author>
</authors>
<title>Emotions from text: machine learning for text-based emotion prediction.</title>
<date>2005</date>
<booktitle>In Proceedings ofHLT/EMNLP,</booktitle>
<pages>579--586</pages>
<contexts>
<context position="2701" citStr="Alm et al., 2005" startWordPosition="417" endWordPosition="420">c and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This component of the model uses the vector representation of words to predict the sentiment annotations on contexts in which the words appear. This causes words expressing similar sentiment to have similar vector representations. The full objective function of the model thus learns semantic vectors that are imbued with nuanced sentiment information. In our experiments, we show how the model can leverage document-level sentiment annotations of a sort that are abundant online in the form of</context>
</contexts>
<marker>Alm, Roth, Sproat, 2005</marker>
<rawString>C. O. Alm, D. Roth, and R. Sproat. 2005. Emotions from text: machine learning for text-based emotion prediction. In Proceedings ofHLT/EMNLP, pages 579–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Andreevskaia</author>
<author>S Bergler</author>
</authors>
<title>Mining WordNet for fuzzy sentiment: sentiment tag extraction from WordNet glosses.</title>
<date>2006</date>
<booktitle>In Proceedings of the European ACL,</booktitle>
<pages>209--216</pages>
<contexts>
<context position="2733" citStr="Andreevskaia and Bergler, 2006" startWordPosition="421" endWordPosition="424">search arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This component of the model uses the vector representation of words to predict the sentiment annotations on contexts in which the words appear. This causes words expressing similar sentiment to have similar vector representations. The full objective function of the model thus learns semantic vectors that are imbued with nuanced sentiment information. In our experiments, we show how the model can leverage document-level sentiment annotations of a sort that are abundant online in the form of consumer reviews 42 for movies,</context>
</contexts>
<marker>Andreevskaia, Bergler, 2006</marker>
<rawString>A. Andreevskaia and S. Bergler. 2006. Mining WordNet for fuzzy sentiment: sentiment tag extraction from WordNet glosses. In Proceedings of the European ACL, pages 209–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>a neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="29532" citStr="Bengio et al., 2003" startWordPosition="4761" endWordPosition="4764">rior features when compared against other VSMs. Improvement over the bag-of-words baseline is obtained by concatenating the two feature vectors. 5 Discussion We presented a vector space model that learns word representations captuing semantic and sentiment information. The model’s probabilistic foundation gives a theoretically justified technique for word vector induction as an alternative to the overwhelming number of matrix factorization-based techniques commonly used. Our model is parametrized as a log-bilinear model following recent success in using similar techniques for language models (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007), and it is related to probabilistic latent topic models (Blei et al., 2003; Steyvers and Griffiths, 2006). We parametrize the topical component of our model in a manner that aims to capture word representations instead of latent topics. In our experiments, our method performed better than LDA, which models latent topics directly. We extended the unsupervised model to incorporate sentiment information and showed how this extended model can leverage the abundance of sentiment-labeled texts available online to yield word representations that ca</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. a neural probabilistic language model. Journal ofMachine Learning Research, 3:1137–1155, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="4623" citStr="Blei et al., 2003" startWordPosition="706" endWordPosition="709">d subjectivity corpora of Pang and Lee (2004), which permits us to make comparisons with a number of related approaches and published results. We also show that this dataset contains many correlations between examples in the training and testing sets. This leads us to evaluate on, and make publicly available, a large dataset of informal movie reviews from the Internet Movie Database (IMDB). 2 Related work The model we present in the next section draws inspiration from prior work on both probabilistic topic modeling and vector-spaced models for word meanings. Latent Dirichlet Allocation (LDA; (Blei et al., 2003)) is a probabilistic document model that assumes each document is a mixture of latent topics. For each latent topic T, the model learns a conditional distribution p(wIT) for the probability that word w occurs in T. One can obtain a kdimensional vector representation of words by first training a k-topic model and then filling the matrix with the p(wIT) values (normalized to unit length). The result is a word–topic matrix in which the rows are taken to represent word meanings. However, because the emphasis in LDA is on modeling topics, not word meanings, there is no guarantee that the row (word)</context>
<context position="9785" citStr="Blei et al., 2003" startWordPosition="1563" endWordPosition="1566"> representation vectors. We additionally introduce a bias bw for each word to capture differences in overall word frequencies. The energy assigned to a word w given these model parameters is E(w; θ, φw, bw) = −θTφw − bw. (2) To obtain the distribution p(w|θ) we use a softmax, exp(−E(w; θ, φw, bw)) p(w|θ; R, b) = Pw′EV exp(−E(wl; θ, φw′, bw′)) (3) how closely its representation vector φw matches the scaling direction of θ. This idea is similar to the word vector inner product used in the log-bilinear language model of Mnih and Hinton (2007). Equation 1 resembles the probabilistic model of LDA (Blei et al., 2003), which models documents as mixtures of latent topics. One could view the entries of a word vector φ as that word’s association strength with respect to each latent topic dimension. The random variable θ then defines a weighting over topics. However, our model does not attempt to model individual topics, but instead directly models word probabilities conditioned on the topic mixture variable θ. Because of the log-linear formulation of the conditional distribution, θ is a vector in Rβ and not restricted to the unit simplex as it is in LDA. We now derive maximum likelihood learning for this mode</context>
<context position="21352" citStr="Blei et al. (2003)" startWordPosition="3459" endWordPosition="3462">rength and orientation. Our unsupervised semantic component (center) and LSA (right) capture semantic relations. VSM induction (Turney and Pantel, 2010). of such weighting variants for sentiment tasks. Latent Dirichlet Allocation (LDA; Blei et 4.3 Document Polarity Classification al., 2003) We use the method described in sec- Our first evaluation task is document-level sentition 2 for inducing word representations from the ment polarity classification. A classifier must pretopic matrix. To train the 50-topic LDA model we dict whether a given review is positive or negative use code released by Blei et al. (2003). We use the given the review text. same 5,000 term vocabulary for LDA as is used for Given a document’s bag of words vector v, we training word vector models. We leave the LDA obtain features from our model using a matrixhyperparameters at their default values, though vector product Rv, where v can have arbitrary tf.idf some work suggests optimizing over priors for LDA weighting. We do not cosine normalize v, instead is important (Wallach et al., 2009). applying cosine normalization to the final feature Weighting Variants We evaluate both binary (b) vector Rv. This procedure is also used to o</context>
<context position="29659" citStr="Blei et al., 2003" startWordPosition="4782" endWordPosition="4785">eature vectors. 5 Discussion We presented a vector space model that learns word representations captuing semantic and sentiment information. The model’s probabilistic foundation gives a theoretically justified technique for word vector induction as an alternative to the overwhelming number of matrix factorization-based techniques commonly used. Our model is parametrized as a log-bilinear model following recent success in using similar techniques for language models (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007), and it is related to probabilistic latent topic models (Blei et al., 2003; Steyvers and Griffiths, 2006). We parametrize the topical component of our model in a manner that aims to capture word representations instead of latent topics. In our experiments, our method performed better than LDA, which models latent topics directly. We extended the unsupervised model to incorporate sentiment information and showed how this extended model can leverage the abundance of sentiment-labeled texts available online to yield word representations that capture both sentiment and semantic relations. We demonstrated the utility of such representations on two tasks of sentiment clas</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent dirichlet allocation. Journal ofMachine Learning Research, 3:993–1022, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boyd-Graber</author>
<author>P Resnik</author>
</authors>
<title>Holistic sentiment analysis across languages: multilingual supervised latent Dirichlet allocation.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>45--55</pages>
<contexts>
<context position="5713" citStr="Boyd-Graber and Resnik, 2010" startWordPosition="894" endWordPosition="897">ent word meanings. However, because the emphasis in LDA is on modeling topics, not word meanings, there is no guarantee that the row (word) vectors are sensible as points in a k-dimensional space. Indeed, we show in section 4 that using LDA in this way does not deliver robust word vectors. The semantic component of our model shares its probabilistic foundation with LDA, but is factored in a manner designed to discover word vectors rather than latent topics. Some recent work introduces extensions of LDA to capture sentiment in addition to topical information (Li et al., 2010; Lin and He, 2009; Boyd-Graber and Resnik, 2010). Like LDA, these methods focus on modeling sentiment-imbued topics rather than embedding words in a vector space. Vector space models (VSMs) seek to model words directly (Turney and Pantel, 2010). Latent Semantic Analysis (LSA), perhaps the best known VSM, explicitly learns semantic word vectors by applying singular value decomposition (SVD) to factor a term–document co-occurrence matrix. It is typical to weight and normalize the matrix values prior to SVD. To obtain a k-dimensional representation for a given word, only the entries corresponding to the k largest singular values are taken from</context>
</contexts>
<marker>Boyd-Graber, Resnik, 2010</marker>
<rawString>J. Boyd-Graber and P. Resnik. 2010. Holistic sentiment analysis across languages: multilingual supervised latent Dirichlet allocation. In Proceedings of EMNLP, pages 45–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing.</title>
<date>2008</date>
<booktitle>In Proceedings of the ICML,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="1818" citStr="Collobert and Weston, 2008" startWordPosition="276" endWordPosition="279"> area. 1 Introduction Word representations are a critical component of many natural language processing systems. It is common to represent words as indices in a vocabulary, but this fails to capture the rich relational structure of the lexicon. Vector-based models do much better in this regard. They encode continuous similarities between words as distance or angle between word vectors in a high-dimensional space. The general approach has proven useful in tasks such as word sense disambiguation, named entity1 recognition, part of speech tagging, and document retrieval (Turney and Pantel, 2010; Collobert and Weston, 2008; Turian et al., 2010). In this paper, we present a model to capture both semantic and sentiment similarities among words. The semantic component of our model learns word vectors via an unsupervised probabilistic model of documents. However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are</context>
<context position="12183" citStr="Collobert and Weston, 2008" startWordPosition="1964" endWordPosition="1967"> XNk log p(wi |ˆθk; R, b), dkED i=1 (7) weights (λ and ν), and the word vector dimensionality β. 3.2 Capturing Word Sentiment The model presented so far does not explicitly capture sentiment information. Applying this algorithm to documents will produce representations where words that occur together in documents have similar representations. However, this unsupervised approach has no explicit way of capturing which words are predictive of sentiment as opposed to content-related. Much previous work in natural language processing achieves better representations by learning from multiple tasks (Collobert and Weston, 2008; Finkel and Manning, 2009). Following this theme we introduce a second task to utilize labeled documents to improve our model’s word representations. Sentiment is a complex, multi-dimensional concept. Depending on which aspects of sentiment we wish to capture, we can give some body of text a sentiment label s which can be categorical, continuous, or multi-dimensional. To leverage such labels, we introduce an objective that the word vectors of our model should predict the sentiment label using some appropriate predictor, s = f(φw). (8) Using an appropriate predictor function f(x) we map a word</context>
<context position="29560" citStr="Collobert and Weston, 2008" startWordPosition="4765" endWordPosition="4768">mpared against other VSMs. Improvement over the bag-of-words baseline is obtained by concatenating the two feature vectors. 5 Discussion We presented a vector space model that learns word representations captuing semantic and sentiment information. The model’s probabilistic foundation gives a theoretically justified technique for word vector induction as an alternative to the overwhelming number of matrix factorization-based techniques commonly used. Our model is parametrized as a log-bilinear model following recent success in using similar techniques for language models (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007), and it is related to probabilistic latent topic models (Blei et al., 2003; Steyvers and Griffiths, 2006). We parametrize the topical component of our model in a manner that aims to capture word representations instead of latent topics. In our experiments, our method performed better than LDA, which models latent topics directly. We extended the unsupervised model to incorporate sentiment information and showed how this extended model can leverage the abundance of sentiment-labeled texts available online to yield word representations that capture both sentiment and sem</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing. In Proceedings of the ICML, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--391</pages>
<contexts>
<context position="19688" citStr="Deerwester et al., 1990" startWordPosition="3225" endWordPosition="3228">nriched vectors for ghastly are truly semantic alternatives to that word, whereas the vectors without sentiment also contain some content words that tend to have ghastly predicated of them. Of course, this is only an impressionistic analysis of a few cases, but it is helpful in understanding why the sentiment-enriched model proves superior at the sentiment classification results we report next. 4.2 Other Word Representations For comparison, we implemented several alternative vector space models that are conceptually similar to our own, as discussed in section 2: Latent Semantic Analysis (LSA; Deerwester et al., 1990) We apply truncated SVD to a tf.idf weighted, cosine normalized count matrix, which is a standard weighting and smoothing scheme for Our model Our model Sentiment + Semantic Semantic only LSA melancholy bittersweet thoughtful poetic heartbreaking warmth lyrical happiness layer poetry tenderness gentle profound compassionate loneliness vivid embarrassingly predators hideous trite hideous inept ghastly laughably tube severely atrocious baffled grotesque appalling smack unsuspecting lame passable uninspired laughable unconvincing flat lackluster unimaginative amateurish bland uninspired clich´ed </context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41:391–407, September. R. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, and C. J. Lin. 2008. LIBLINEAR: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
</authors>
<title>Joint parsing and named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>326--334</pages>
<contexts>
<context position="12210" citStr="Finkel and Manning, 2009" startWordPosition="1968" endWordPosition="1971">kED i=1 (7) weights (λ and ν), and the word vector dimensionality β. 3.2 Capturing Word Sentiment The model presented so far does not explicitly capture sentiment information. Applying this algorithm to documents will produce representations where words that occur together in documents have similar representations. However, this unsupervised approach has no explicit way of capturing which words are predictive of sentiment as opposed to content-related. Much previous work in natural language processing achieves better representations by learning from multiple tasks (Collobert and Weston, 2008; Finkel and Manning, 2009). Following this theme we introduce a second task to utilize labeled documents to improve our model’s word representations. Sentiment is a complex, multi-dimensional concept. Depending on which aspects of sentiment we wish to capture, we can give some body of text a sentiment label s which can be categorical, continuous, or multi-dimensional. To leverage such labels, we introduce an objective that the word vectors of our model should predict the sentiment label using some appropriate predictor, s = f(φw). (8) Using an appropriate predictor function f(x) we map a word vector φw to a predicted s</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>J. R. Finkel and C. D. Manning. 2009. Joint parsing and named entity recognition. In Proceedings of NAACL, pages 326–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Goldberg</author>
<author>J Zhu</author>
</authors>
<title>Seeing stars when there aren’t many stars: graph-based semi-supervised learning for sentiment categorization.</title>
<date>2006</date>
<booktitle>In TextGraphs: HLT/NAACL Workshop on Graph-based Algorithms for Natural Language Processing,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="2777" citStr="Goldberg and Zhu, 2006" startWordPosition="429" endWordPosition="432">ve semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This component of the model uses the vector representation of words to predict the sentiment annotations on contexts in which the words appear. This causes words expressing similar sentiment to have similar vector representations. The full objective function of the model thus learns semantic vectors that are imbued with nuanced sentiment information. In our experiments, we show how the model can leverage document-level sentiment annotations of a sort that are abundant online in the form of consumer reviews 42 for movies, products, etc. The technique is suffiProcee</context>
</contexts>
<marker>Goldberg, Zhu, 2006</marker>
<rawString>A. B. Goldberg and J. Zhu. 2006. Seeing stars when there aren’t many stars: graph-based semi-supervised learning for sentiment categorization. In TextGraphs: HLT/NAACL Workshop on Graph-based Algorithms for Natural Language Processing, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Jay</author>
</authors>
<title>Why We Curse: A Neuro-PsychoSocial Theory of Speech.</title>
<date>2000</date>
<publisher>John Benjamins, Philadelphia/Amsterdam.</publisher>
<contexts>
<context position="2212" citStr="Jay, 2000" startWordPosition="337" endWordPosition="338">pace. The general approach has proven useful in tasks such as word sense disambiguation, named entity1 recognition, part of speech tagging, and document retrieval (Turney and Pantel, 2010; Collobert and Weston, 2008; Turian et al., 2010). In this paper, we present a model to capture both semantic and sentiment similarities among words. The semantic component of our model learns word vectors via an unsupervised probabilistic model of documents. However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This </context>
</contexts>
<marker>Jay, 2000</marker>
<rawString>T. Jay. 2000. Why We Curse: A Neuro-PsychoSocial Theory of Speech. John Benjamins, Philadelphia/Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kaplan</author>
</authors>
<title>What is meaning? Explorations in the theory of Meaning as Use. Brief version — draft 1.</title>
<date>1999</date>
<location>Ms., UCLA.</location>
<contexts>
<context position="2201" citStr="Kaplan, 1999" startWordPosition="334" endWordPosition="336">-dimensional space. The general approach has proven useful in tasks such as word sense disambiguation, named entity1 recognition, part of speech tagging, and document retrieval (Turney and Pantel, 2010; Collobert and Weston, 2008; Turian et al., 2010). In this paper, we present a model to capture both semantic and sentiment similarities among words. The semantic component of our model learns word vectors via an unsupervised probabilistic model of documents. However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2</context>
</contexts>
<marker>Kaplan, 1999</marker>
<rawString>D. Kaplan. 1999. What is meaning? Explorations in the theory of Meaning as Use. Brief version — draft 1. Ms., UCLA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kennedy</author>
<author>D Inkpen</author>
</authors>
<title>Sentiment classification of movie reviews using contextual valence shifters.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<pages>22--110</pages>
<contexts>
<context position="23050" citStr="Kennedy and Inkpen, 2006" startWordPosition="3743" endWordPosition="3746"> multi-word representations and Thelwall (2010) perform an extensive analysis147 Features PL04 Our Dataset Subjectivity Bag of Words (bnc) 85.45 87.80 87.77 Bag of Words (bAt’c) 85.80 88.23 85.65 LDA 66.70 67.42 66.65 LSA 84.55 83.96 82.82 Our Semantic Only 87.10 87.30 86.65 Our Full 84.65 87.44 86.19 Our Full, Additional Unlabeled 87.05 87.99 87.22 Our Semantic + Bag of Words (bnc) 88.30 88.28 88.58 Our Full + Bag of Words (bnc) 87.85 88.33 88.45 Our Full, Add’l Unlabeled + Bag of Words (bnc) 88.90 88.89 88.13 Bag of Words SVM (Pang and Lee, 2004) 87.15 N/A 90.00 Contextual Valence Shifters (Kennedy and Inkpen, 2006) 86.20 N/A N/A tf.Didf Weighting (Martineau and Finin, 2009) 88.10 N/A N/A Appraisal Taxonomy (Whitelaw et al., 2005) 90.20 N/A N/A Table 2: Classification accuracy on three tasks. From left to right the datasets are: A collection of 2,000 movie reviews often used as a benchmark of sentiment classification (Pang and Lee, 2004), 50,000 reviews we gathered from IMDB, and the sentence subjectivity dataset also released by (Pang and Lee, 2004). All tasks are balanced two-class problems. from word vectors. 4.3.1 Pang and Lee Movie Review Dataset The polarity dataset version 2.0 introduced by Pang a</context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>A. Kennedy and D. Inkpen. 2006. Sentiment classification of movie reviews using contextual valence shifters. Computational Intelligence, 22:110–125, May. 150</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Li</author>
<author>M Huang</author>
<author>X Zhu</author>
</authors>
<title>Sentiment analysis with global topics and local dependency.</title>
<date>2010</date>
<booktitle>In Proceedings ofAAAI,</booktitle>
<pages>1371--1376</pages>
<contexts>
<context position="5664" citStr="Li et al., 2010" startWordPosition="886" endWordPosition="889"> which the rows are taken to represent word meanings. However, because the emphasis in LDA is on modeling topics, not word meanings, there is no guarantee that the row (word) vectors are sensible as points in a k-dimensional space. Indeed, we show in section 4 that using LDA in this way does not deliver robust word vectors. The semantic component of our model shares its probabilistic foundation with LDA, but is factored in a manner designed to discover word vectors rather than latent topics. Some recent work introduces extensions of LDA to capture sentiment in addition to topical information (Li et al., 2010; Lin and He, 2009; Boyd-Graber and Resnik, 2010). Like LDA, these methods focus on modeling sentiment-imbued topics rather than embedding words in a vector space. Vector space models (VSMs) seek to model words directly (Turney and Pantel, 2010). Latent Semantic Analysis (LSA), perhaps the best known VSM, explicitly learns semantic word vectors by applying singular value decomposition (SVD) to factor a term–document co-occurrence matrix. It is typical to weight and normalize the matrix values prior to SVD. To obtain a k-dimensional representation for a given word, only the entries correspondin</context>
</contexts>
<marker>Li, Huang, Zhu, 2010</marker>
<rawString>F. Li, M. Huang, and X. Zhu. 2010. Sentiment analysis with global topics and local dependency. In Proceedings ofAAAI, pages 1371–1376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>Y He</author>
</authors>
<title>Joint sentiment/topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceeding of the 18th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>375--384</pages>
<contexts>
<context position="5682" citStr="Lin and He, 2009" startWordPosition="890" endWordPosition="893">re taken to represent word meanings. However, because the emphasis in LDA is on modeling topics, not word meanings, there is no guarantee that the row (word) vectors are sensible as points in a k-dimensional space. Indeed, we show in section 4 that using LDA in this way does not deliver robust word vectors. The semantic component of our model shares its probabilistic foundation with LDA, but is factored in a manner designed to discover word vectors rather than latent topics. Some recent work introduces extensions of LDA to capture sentiment in addition to topical information (Li et al., 2010; Lin and He, 2009; Boyd-Graber and Resnik, 2010). Like LDA, these methods focus on modeling sentiment-imbued topics rather than embedding words in a vector space. Vector space models (VSMs) seek to model words directly (Turney and Pantel, 2010). Latent Semantic Analysis (LSA), perhaps the best known VSM, explicitly learns semantic word vectors by applying singular value decomposition (SVD) to factor a term–document co-occurrence matrix. It is typical to weight and normalize the matrix values prior to SVD. To obtain a k-dimensional representation for a given word, only the entries corresponding to the k largest</context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>C. Lin and Y. He. 2009. Joint sentiment/topic model for sentiment analysis. In Proceeding of the 18th ACM Conference on Information and Knowledge Management, pages 375–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Martineau</author>
<author>T Finin</author>
</authors>
<title>Delta tfidf: an improved feature space for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the 3rd AAAI International Conference on Weblogs and Social Media,</booktitle>
<pages>258--261</pages>
<contexts>
<context position="6850" citStr="Martineau and Finin, 2009" startWordPosition="1069" endWordPosition="1072">iven word, only the entries corresponding to the k largest singular values are taken from the word’s basis in the factored matrix. Such matrix factorizationbased approaches are extremely successful in practice, but they force the researcher to make a number of design choices (weighting, normalization, dimensionality reduction algorithm) with little theoretical guidance to suggest which to prefer. Using term frequency (tf) and inverse document frequency (idf) weighting to transform the values in a VSM often increases the performance of retrieval and categorization systems. Delta idf weighting (Martineau and Finin, 2009) is a supervised variant of idf weighting in which the idf calculation is done for each document class and then one value is subtracted from the other. Martineau and Finin present evidence that this weighting helps with sentiment classification, and Paltoglou and Thelwall (2010) systematically explore a number of weighting schemes in the context of sentiment analysis. The success of delta idf weighting in previous work suggests that incorporating sentiment information into VSM values via supervised methods is helpful for sentiment analysis. We adopt this insight, but we are able to incorporate</context>
<context position="23110" citStr="Martineau and Finin, 2009" startWordPosition="3752" endWordPosition="3755">extensive analysis147 Features PL04 Our Dataset Subjectivity Bag of Words (bnc) 85.45 87.80 87.77 Bag of Words (bAt’c) 85.80 88.23 85.65 LDA 66.70 67.42 66.65 LSA 84.55 83.96 82.82 Our Semantic Only 87.10 87.30 86.65 Our Full 84.65 87.44 86.19 Our Full, Additional Unlabeled 87.05 87.99 87.22 Our Semantic + Bag of Words (bnc) 88.30 88.28 88.58 Our Full + Bag of Words (bnc) 87.85 88.33 88.45 Our Full, Add’l Unlabeled + Bag of Words (bnc) 88.90 88.89 88.13 Bag of Words SVM (Pang and Lee, 2004) 87.15 N/A 90.00 Contextual Valence Shifters (Kennedy and Inkpen, 2006) 86.20 N/A N/A tf.Didf Weighting (Martineau and Finin, 2009) 88.10 N/A N/A Appraisal Taxonomy (Whitelaw et al., 2005) 90.20 N/A N/A Table 2: Classification accuracy on three tasks. From left to right the datasets are: A collection of 2,000 movie reviews often used as a benchmark of sentiment classification (Pang and Lee, 2004), 50,000 reviews we gathered from IMDB, and the sentence subjectivity dataset also released by (Pang and Lee, 2004). All tasks are balanced two-class problems. from word vectors. 4.3.1 Pang and Lee Movie Review Dataset The polarity dataset version 2.0 introduced by Pang and Lee (2004) 1 consists of 2,000 movie reviews, where each </context>
</contexts>
<marker>Martineau, Finin, 2009</marker>
<rawString>J. Martineau and T. Finin. 2009. Delta tfidf: an improved feature space for sentiment analysis. In Proceedings of the 3rd AAAI International Conference on Weblogs and Social Media, pages 258–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G E Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the ICML,</booktitle>
<pages>641--648</pages>
<contexts>
<context position="9712" citStr="Mnih and Hinton (2007)" startWordPosition="1551" endWordPosition="1554">β-dimensional vector, θ E Rβ which weights each of the β dimensions of words’ representation vectors. We additionally introduce a bias bw for each word to capture differences in overall word frequencies. The energy assigned to a word w given these model parameters is E(w; θ, φw, bw) = −θTφw − bw. (2) To obtain the distribution p(w|θ) we use a softmax, exp(−E(w; θ, φw, bw)) p(w|θ; R, b) = Pw′EV exp(−E(wl; θ, φw′, bw′)) (3) how closely its representation vector φw matches the scaling direction of θ. This idea is similar to the word vector inner product used in the log-bilinear language model of Mnih and Hinton (2007). Equation 1 resembles the probabilistic model of LDA (Blei et al., 2003), which models documents as mixtures of latent topics. One could view the entries of a word vector φ as that word’s association strength with respect to each latent topic dimension. The random variable θ then defines a weighting over topics. However, our model does not attempt to model individual topics, but instead directly models word probabilities conditioned on the topic mixture variable θ. Because of the log-linear formulation of the conditional distribution, θ is a vector in Rβ and not restricted to the unit simplex</context>
<context position="29584" citStr="Mnih and Hinton, 2007" startWordPosition="4769" endWordPosition="4772">mprovement over the bag-of-words baseline is obtained by concatenating the two feature vectors. 5 Discussion We presented a vector space model that learns word representations captuing semantic and sentiment information. The model’s probabilistic foundation gives a theoretically justified technique for word vector induction as an alternative to the overwhelming number of matrix factorization-based techniques commonly used. Our model is parametrized as a log-bilinear model following recent success in using similar techniques for language models (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007), and it is related to probabilistic latent topic models (Blei et al., 2003; Steyvers and Griffiths, 2006). We parametrize the topical component of our model in a manner that aims to capture word representations instead of latent topics. In our experiments, our method performed better than LDA, which models latent topics directly. We extended the unsupervised model to incorporate sentiment information and showed how this extended model can leverage the abundance of sentiment-labeled texts available online to yield word representations that capture both sentiment and semantic relations. We demo</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>A. Mnih and G. E. Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the ICML, pages 641–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Paltoglou</author>
<author>M Thelwall</author>
</authors>
<title>A study of information retrieval weighting schemes for sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>1386--1395</pages>
<contexts>
<context position="7129" citStr="Paltoglou and Thelwall (2010)" startWordPosition="1115" endWordPosition="1118">(weighting, normalization, dimensionality reduction algorithm) with little theoretical guidance to suggest which to prefer. Using term frequency (tf) and inverse document frequency (idf) weighting to transform the values in a VSM often increases the performance of retrieval and categorization systems. Delta idf weighting (Martineau and Finin, 2009) is a supervised variant of idf weighting in which the idf calculation is done for each document class and then one value is subtracted from the other. Martineau and Finin present evidence that this weighting helps with sentiment classification, and Paltoglou and Thelwall (2010) systematically explore a number of weighting schemes in the context of sentiment analysis. The success of delta idf weighting in previous work suggests that incorporating sentiment information into VSM values via supervised methods is helpful for sentiment analysis. We adopt this insight, but we are able to incorporate it directly into our model’s objective function. (Section 4 compares our approach with a representative sample of such weighting schemes.) 3 Our Model To capture semantic similarities among words, we derive a probabilistic model of documents which learns word representations. T</context>
</contexts>
<marker>Paltoglou, Thelwall, 2010</marker>
<rawString>G. Paltoglou and M. Thelwall. 2010. A study of information retrieval weighting schemes for sentiment analysis. In Proceedings of the ACL, pages 1386–1395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="4050" citStr="Pang and Lee (2004)" startWordPosition="614" endWordPosition="617">r Computational Linguistics, pages 142–150, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 1 43 ciently general to work also with continuous and multi-dimensional notions of sentiment as well as non-sentiment annotations (e.g., political affiliation, speaker commitment). After presenting the model in detail, we provide illustrative examples of the vectors it learns, and then we systematically evaluate the approach on document-level and sentence-level classification tasks. Our experiments involve the small, widely used sentiment and subjectivity corpora of Pang and Lee (2004), which permits us to make comparisons with a number of related approaches and published results. We also show that this dataset contains many correlations between examples in the training and testing sets. This leads us to evaluate on, and make publicly available, a large dataset of informal movie reviews from the Internet Movie Database (IMDB). 2 Related work The model we present in the next section draws inspiration from prior work on both probabilistic topic modeling and vector-spaced models for word meanings. Latent Dirichlet Allocation (LDA; (Blei et al., 2003)) is a probabilistic docume</context>
<context position="22979" citStr="Pang and Lee, 2004" startWordPosition="3733" endWordPosition="3736">use cosine normalization (c). Paltoglou use this weighting to get multi-word representations and Thelwall (2010) perform an extensive analysis147 Features PL04 Our Dataset Subjectivity Bag of Words (bnc) 85.45 87.80 87.77 Bag of Words (bAt’c) 85.80 88.23 85.65 LDA 66.70 67.42 66.65 LSA 84.55 83.96 82.82 Our Semantic Only 87.10 87.30 86.65 Our Full 84.65 87.44 86.19 Our Full, Additional Unlabeled 87.05 87.99 87.22 Our Semantic + Bag of Words (bnc) 88.30 88.28 88.58 Our Full + Bag of Words (bnc) 87.85 88.33 88.45 Our Full, Add’l Unlabeled + Bag of Words (bnc) 88.90 88.89 88.13 Bag of Words SVM (Pang and Lee, 2004) 87.15 N/A 90.00 Contextual Valence Shifters (Kennedy and Inkpen, 2006) 86.20 N/A N/A tf.Didf Weighting (Martineau and Finin, 2009) 88.10 N/A N/A Appraisal Taxonomy (Whitelaw et al., 2005) 90.20 N/A N/A Table 2: Classification accuracy on three tasks. From left to right the datasets are: A collection of 2,000 movie reviews often used as a benchmark of sentiment classification (Pang and Lee, 2004), 50,000 reviews we gathered from IMDB, and the sentence subjectivity dataset also released by (Pang and Lee, 2004). All tasks are balanced two-class problems. from word vectors. 4.3.1 Pang and Lee Mov</context>
<context position="28233" citStr="Pang and Lee (2004)" startWordPosition="4577" endWordPosition="4580">zed extra unlabeled data during training performed best. Differences in accuracy are small, but, because our test set contains 25,000 examples, the variance of the performance estimate is quite low. For example, an accuracy increase of 0.1% corresponds to correctly classifying an additional 25 reviews. 4.4 Subjectivity Detection As a second evaluation task, we performed sentencelevel subjectivity classification. In this task, a classifier is trained to decide whether a given sentence is subjective, expressing the writer’s opinions, or objective, expressing purely facts. We used the dataset of Pang and Lee (2004), which contains subjective sentences from movie review summaries and objective sentences from movie plot summaries. This task 2Dataset and further details are available online at: http://www.andrew-maas.net/data/sentiment 149 is substantially different from the review classification task because it uses sentences as opposed to entire documents and the target concept is subjectivity instead of opinion polarity. We randomly split the 10,000 examples into 10 folds and report 10-fold cross validation accuracy using the SVM training protocol of Pang and Lee (2004). Table 2 shows classification acc</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang and L. Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the ACL, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="2753" citStr="Pang and Lee, 2005" startWordPosition="425" endWordPosition="428">ontent and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This component of the model uses the vector representation of words to predict the sentiment annotations on contexts in which the words appear. This causes words expressing similar sentiment to have similar vector representations. The full objective function of the model thus learns semantic vectors that are imbued with nuanced sentiment information. In our experiments, we show how the model can leverage document-level sentiment annotations of a sort that are abundant online in the form of consumer reviews 42 for movies, products, etc. The </context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>B. Pang and L. Lee. 2005. Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of ACL, pages 115– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="22292" citStr="Pang et al., 2002" startWordPosition="3616" endWordPosition="3619"> some work suggests optimizing over priors for LDA weighting. We do not cosine normalize v, instead is important (Wallach et al., 2009). applying cosine normalization to the final feature Weighting Variants We evaluate both binary (b) vector Rv. This procedure is also used to obtain term frequency weighting with smoothed delta idf features from the LDA and LSA word vectors. In (At’) and no idf (n) because these variants worked preliminary experiments, we found ‘bnn’ weighting well in previous experiments in sentiment (Mar- to work best for v when generating document featineau and Finin, 2009; Pang et al., 2002). In all tures via the product Rv. In all experiments, we cases, we use cosine normalization (c). Paltoglou use this weighting to get multi-word representations and Thelwall (2010) perform an extensive analysis147 Features PL04 Our Dataset Subjectivity Bag of Words (bnc) 85.45 87.80 87.77 Bag of Words (bAt’c) 85.80 88.23 85.65 LDA 66.70 67.42 66.65 LSA 84.55 83.96 82.82 Our Semantic Only 87.10 87.30 86.65 Our Full 84.65 87.44 86.19 Our Full, Additional Unlabeled 87.05 87.99 87.22 Our Semantic + Bag of Words (bnc) 88.30 88.28 88.58 Our Full + Bag of Words (bnc) 87.85 88.33 88.45 Our Full, Add’l</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of EMNLP, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Potts</author>
</authors>
<title>The expressive dimension. Theoretical Linguistics,</title>
<date>2007</date>
<pages>33--165</pages>
<contexts>
<context position="2226" citStr="Potts, 2007" startWordPosition="339" endWordPosition="340">eneral approach has proven useful in tasks such as word sense disambiguation, named entity1 recognition, part of speech tagging, and document retrieval (Turney and Pantel, 2010; Collobert and Weston, 2008; Turian et al., 2010). In this paper, we present a model to capture both semantic and sentiment similarities among words. The semantic component of our model learns word vectors via an unsupervised probabilistic model of documents. However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This component of t</context>
</contexts>
<marker>Potts, 2007</marker>
<rawString>C. Potts. 2007. The expressive dimension. Theoretical Linguistics, 33:165–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>R Barzilay</author>
</authors>
<title>Multiple aspect ranking using the good grief algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>300--307</pages>
<contexts>
<context position="2805" citStr="Snyder and Barzilay, 2007" startWordPosition="433" endWordPosition="436">distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This component of the model uses the vector representation of words to predict the sentiment annotations on contexts in which the words appear. This causes words expressing similar sentiment to have similar vector representations. The full objective function of the model thus learns semantic vectors that are imbued with nuanced sentiment information. In our experiments, we show how the model can leverage document-level sentiment annotations of a sort that are abundant online in the form of consumer reviews 42 for movies, products, etc. The technique is suffiProceedings of the 49th Annual Mee</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>B. Snyder and R. Barzilay. 2007. Multiple aspect ranking using the good grief algorithm. In Proceedings of NAACL, pages 300–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steyvers</author>
<author>T L Griffiths</author>
</authors>
<title>Probabilistic topic models. In</title>
<date>2006</date>
<booktitle>Latent Semantic Analysis: A Road to Meaning.</booktitle>
<editor>T. Landauer, D McNamara, S. Dennis, and W. Kintsch, editors,</editor>
<contexts>
<context position="29690" citStr="Steyvers and Griffiths, 2006" startWordPosition="4786" endWordPosition="4790">iscussion We presented a vector space model that learns word representations captuing semantic and sentiment information. The model’s probabilistic foundation gives a theoretically justified technique for word vector induction as an alternative to the overwhelming number of matrix factorization-based techniques commonly used. Our model is parametrized as a log-bilinear model following recent success in using similar techniques for language models (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007), and it is related to probabilistic latent topic models (Blei et al., 2003; Steyvers and Griffiths, 2006). We parametrize the topical component of our model in a manner that aims to capture word representations instead of latent topics. In our experiments, our method performed better than LDA, which models latent topics directly. We extended the unsupervised model to incorporate sentiment information and showed how this extended model can leverage the abundance of sentiment-labeled texts available online to yield word representations that capture both sentiment and semantic relations. We demonstrated the utility of such representations on two tasks of sentiment classification, using existing data</context>
</contexts>
<marker>Steyvers, Griffiths, 2006</marker>
<rawString>M. Steyvers and T. L. Griffiths. 2006. Probabilistic topic models. In T. Landauer, D McNamara, S. Dennis, and W. Kintsch, editors, Latent Semantic Analysis: A Road to Meaning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: A simple and general method for semisupervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>384394</pages>
<contexts>
<context position="1840" citStr="Turian et al., 2010" startWordPosition="280" endWordPosition="283">epresentations are a critical component of many natural language processing systems. It is common to represent words as indices in a vocabulary, but this fails to capture the rich relational structure of the lexicon. Vector-based models do much better in this regard. They encode continuous similarities between words as distance or angle between word vectors in a high-dimensional space. The general approach has proven useful in tasks such as word sense disambiguation, named entity1 recognition, part of speech tagging, and document retrieval (Turney and Pantel, 2010; Collobert and Weston, 2008; Turian et al., 2010). In this paper, we present a model to capture both semantic and sentiment similarities among words. The semantic component of our model learns word vectors via an unsupervised probabilistic model of documents. However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong posi</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: A simple and general method for semisupervised learning. In Proceedings of the ACL, page 384394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: vector space models of semantics.</title>
<date>2010</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1790" citStr="Turney and Pantel, 2010" startWordPosition="272" endWordPosition="275">enchmark for work in this area. 1 Introduction Word representations are a critical component of many natural language processing systems. It is common to represent words as indices in a vocabulary, but this fails to capture the rich relational structure of the lexicon. Vector-based models do much better in this regard. They encode continuous similarities between words as distance or angle between word vectors in a high-dimensional space. The general approach has proven useful in tasks such as word sense disambiguation, named entity1 recognition, part of speech tagging, and document retrieval (Turney and Pantel, 2010; Collobert and Weston, 2008; Turian et al., 2010). In this paper, we present a model to capture both semantic and sentiment similarities among words. The semantic component of our model learns word vectors via an unsupervised probabilistic model of documents. However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t cap</context>
<context position="5909" citStr="Turney and Pantel, 2010" startWordPosition="925" endWordPosition="928">ed, we show in section 4 that using LDA in this way does not deliver robust word vectors. The semantic component of our model shares its probabilistic foundation with LDA, but is factored in a manner designed to discover word vectors rather than latent topics. Some recent work introduces extensions of LDA to capture sentiment in addition to topical information (Li et al., 2010; Lin and He, 2009; Boyd-Graber and Resnik, 2010). Like LDA, these methods focus on modeling sentiment-imbued topics rather than embedding words in a vector space. Vector space models (VSMs) seek to model words directly (Turney and Pantel, 2010). Latent Semantic Analysis (LSA), perhaps the best known VSM, explicitly learns semantic word vectors by applying singular value decomposition (SVD) to factor a term–document co-occurrence matrix. It is typical to weight and normalize the matrix values prior to SVD. To obtain a k-dimensional representation for a given word, only the entries corresponding to the k largest singular values are taken from the word’s basis in the factored matrix. Such matrix factorizationbased approaches are extremely successful in practice, but they force the researcher to make a number of design choices (weightin</context>
<context position="20886" citStr="Turney and Pantel, 2010" startWordPosition="3385" endWordPosition="3388">bland uninspired clich´ed forgettable awful insipid mediocre romantic romance romance romance love charming screwball sweet delightful grant beautiful sweet comedies relationship chemistry comedy Table 1: Similarity of learned word vectors. Each target word is given with its five most similar words using cosine similarity of the vectors determined by each model. The full version of our model (left) captures both lexical similarity as well as similarity of sentiment strength and orientation. Our unsupervised semantic component (center) and LSA (right) capture semantic relations. VSM induction (Turney and Pantel, 2010). of such weighting variants for sentiment tasks. Latent Dirichlet Allocation (LDA; Blei et 4.3 Document Polarity Classification al., 2003) We use the method described in sec- Our first evaluation task is document-level sentition 2 for inducing word representations from the ment polarity classification. A classifier must pretopic matrix. To train the 50-topic LDA model we dict whether a given review is positive or negative use code released by Blei et al. (2003). We use the given the review text. same 5,000 term vocabulary for LDA as is used for Given a document’s bag of words vector v, we tra</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>P. D. Turney and P. Pantel. 2010. From frequency to meaning: vector space models of semantics. Journal ofArtificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wallach</author>
<author>D Mimno</author>
<author>A McCallum</author>
</authors>
<title>Rethinking LDA: why priors matter.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1973--1981</pages>
<contexts>
<context position="21809" citStr="Wallach et al., 2009" startWordPosition="3538" endWordPosition="3541">n. A classifier must pretopic matrix. To train the 50-topic LDA model we dict whether a given review is positive or negative use code released by Blei et al. (2003). We use the given the review text. same 5,000 term vocabulary for LDA as is used for Given a document’s bag of words vector v, we training word vector models. We leave the LDA obtain features from our model using a matrixhyperparameters at their default values, though vector product Rv, where v can have arbitrary tf.idf some work suggests optimizing over priors for LDA weighting. We do not cosine normalize v, instead is important (Wallach et al., 2009). applying cosine normalization to the final feature Weighting Variants We evaluate both binary (b) vector Rv. This procedure is also used to obtain term frequency weighting with smoothed delta idf features from the LDA and LSA word vectors. In (At’) and no idf (n) because these variants worked preliminary experiments, we found ‘bnn’ weighting well in previous experiments in sentiment (Mar- to work best for v when generating document featineau and Finin, 2009; Pang et al., 2002). In all tures via the product Rv. In all experiments, we cases, we use cosine normalization (c). Paltoglou use this </context>
</contexts>
<marker>Wallach, Mimno, McCallum, 2009</marker>
<rawString>H. Wallach, D. Mimno, and A. McCallum. 2009. Rethinking LDA: why priors matter. In Proceedings of NIPS, pages 1973–1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Whitelaw</author>
<author>N Garg</author>
<author>S Argamon</author>
</authors>
<title>Using appraisal groups for sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>625--631</pages>
<contexts>
<context position="23167" citStr="Whitelaw et al., 2005" startWordPosition="3761" endWordPosition="3764">Bag of Words (bnc) 85.45 87.80 87.77 Bag of Words (bAt’c) 85.80 88.23 85.65 LDA 66.70 67.42 66.65 LSA 84.55 83.96 82.82 Our Semantic Only 87.10 87.30 86.65 Our Full 84.65 87.44 86.19 Our Full, Additional Unlabeled 87.05 87.99 87.22 Our Semantic + Bag of Words (bnc) 88.30 88.28 88.58 Our Full + Bag of Words (bnc) 87.85 88.33 88.45 Our Full, Add’l Unlabeled + Bag of Words (bnc) 88.90 88.89 88.13 Bag of Words SVM (Pang and Lee, 2004) 87.15 N/A 90.00 Contextual Valence Shifters (Kennedy and Inkpen, 2006) 86.20 N/A N/A tf.Didf Weighting (Martineau and Finin, 2009) 88.10 N/A N/A Appraisal Taxonomy (Whitelaw et al., 2005) 90.20 N/A N/A Table 2: Classification accuracy on three tasks. From left to right the datasets are: A collection of 2,000 movie reviews often used as a benchmark of sentiment classification (Pang and Lee, 2004), 50,000 reviews we gathered from IMDB, and the sentence subjectivity dataset also released by (Pang and Lee, 2004). All tasks are balanced two-class problems. from word vectors. 4.3.1 Pang and Lee Movie Review Dataset The polarity dataset version 2.0 introduced by Pang and Lee (2004) 1 consists of 2,000 movie reviews, where each is associated with a binary sentiment polarity label. We </context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>C. Whitelaw, N. Garg, and S. Argamon. 2005. Using appraisal groups for sentiment analysis. In Proceedings of CIKM, pages 625–631.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>R Hwa</author>
</authors>
<title>Just how mad are you? Finding strong and weak opinion clauses.</title>
<date>2004</date>
<booktitle>In Proceedings ofAAAI,</booktitle>
<pages>761--769</pages>
<contexts>
<context position="2683" citStr="Wilson et al., 2004" startWordPosition="412" endWordPosition="416">eeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Kaplan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment information. For example, while it learns that wonderful and amazing are semantically close, it doesn’t capture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful. Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wilson et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This component of the model uses the vector representation of words to predict the sentiment annotations on contexts in which the words appear. This causes words expressing similar sentiment to have similar vector representations. The full objective function of the model thus learns semantic vectors that are imbued with nuanced sentiment information. In our experiments, we show how the model can leverage document-level sentiment annotations of a sort that are abundant onl</context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are you? Finding strong and weak opinion clauses. In Proceedings ofAAAI, pages 761–769.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>