<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.99929">
A Clustered Global Phrase Reordering Model
for Statistical Machine Translation
</title>
<author confidence="0.682918">
Masaaki Nagata Kuniko Saito
</author>
<note confidence="0.723119333333333">
NTT Communication Science Laboratories NTT Cyber Space Laboratories
2-4 Hikaridai, Seika-cho, Souraku-gun 1-1 Hikarinooka, Yokoshuka-shi
Kyoto, 619-0237 Japan Kanagawa, 239-0847 Japan
</note>
<email confidence="0.967485">
nagata.masaaki@labs.ntt.co.jp, saito.kuniko@labs.ntt.co.jp
</email>
<author confidence="0.991192">
Kazuhide Yamamoto, Kazuteru Ohashi
</author>
<affiliation confidence="0.997182">
Nagaoka University of Technology
</affiliation>
<address confidence="0.7951125">
1603-1, Kamitomioka, Nagaoka City
Niigata, 940-2188 Japan
</address>
<email confidence="0.99323">
ykaz@nlp.nagaokaut.ac.jp, ohashi@nlp.nagaokaut.ac.jp
</email>
<sectionHeader confidence="0.982807" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992475">
In this paper, we present a novel global re-
ordering model that can be incorporated
into standard phrase-based statistical ma-
chine translation. Unlike previous local
reordering models that emphasize the re-
ordering of adjacent phrase pairs (Till-
mann and Zhang, 2005), our model ex-
plicitly models the reordering of long dis-
tances by directly estimating the parame-
ters from the phrase alignments of bilin-
gual training sentences. In principle, the
global phrase reordering model is condi-
tioned on the source and target phrases
that are currently being translated, and
the previously translated source and tar-
get phrases. To cope with sparseness, we
use N-best phrase alignments and bilin-
gual phrase clustering, and investigate a
variety of combinations of conditioning
factors. Through experiments, we show,
that the global reordering model signifi-
cantly improves the translation accuracy
of a standard Japanese-English translation
task.
</bodyText>
<sectionHeader confidence="0.995157" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9903254">
Global reordering is essential to the translation of
languages with different word orders. Ideally, a
model should allow the reordering of any distance,
because if we are to translate from Japanese to En-
glish, the verb in the Japanese sentence must be
moved from the end of the sentence to the begin-
ning just after the subject in the English sentence.
✁ Graduated in March 2006
Standard phrase-based translation systems use
a word distance-based reordering model in which
non-monotonic phrase alignment is penalized
based on the word distance between successively
translated source phrases without considering the
orientation of the phrase alignment or the identi-
ties of the source and target phrases (Koehn et al.,
2003; Och and Ney, 2004). (Tillmann and Zhang,
2005) introduced the notion of a block (a pair of
source and target phrases that are translations of
each other), and proposed the block orientation
bigram in which the local reordering of adjacent
blocks are expressed as a three-valued orienta-
tion, namely Right (monotone), Left (swapped),
or Neutral. A block with neutral orientation is sup-
posed to be less strongly linked to its predecessor
block: thus in their model, the global reordering is
not explicitly modeled.
In this paper, we present a global reordering
model that explicitly models long distance re-
ordering&apos;. It predicts four type of reordering
patterns, namely MA (monotone adjacent), MG
(monotone gap), RA (reverse adjacent), and RG
(reverse gap). There are based on the identities of
the source and target phrases currently being trans-
lated, and the previously translated source and tar-
get phrases. The parameters of the reordering
model are estimated from the phrase alignments of
training bilingual sentences. To cope with sparse-
ness, we use N-best phrase alignments and bilin-
gual phrase clustering.
In the following sections, we first describe the
global phrase reordering model and its param-
&apos;It might be misleading to call our reordering model
“global” since it is at most considers two phrases. A truly
global reordering model would take the entire sentence struc-
ture into account.
</bodyText>
<page confidence="0.461792">
713
</page>
<bodyText confidence="0.985871111111111">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 713–720,
Sydney, July 2006. c�2006 Association for Computational Linguistics
eter estimation method including N-best phrase
alignments and bilingual phrase clustering. Next,
through an experiment, we show that the global
phrase reordering model significantly improves
the translation accuracy of the IWSLT-2005
Japanese-English translation task (Eck and Hori,
2005).
</bodyText>
<figure confidence="0.989705142857143">
b4
RA
b3
RA
MG b2
b1
p f 0) 3 1-A t &amp; )7.)
commu-
nication
of
means
a
is
language
</figure>
<sectionHeader confidence="0.979356" genericHeader="method">
2 Baseline Translation Model
</sectionHeader>
<bodyText confidence="0.995457357142857">
In statistical machine translation, the translation of
a source (foreign) sentence is formulated as the
search for a target (English) sentence that max-
imizes the conditional probability , which
can be rewritten using the Bayes rule as,
where is a translation model and is a
target language model.
In phrase-based statistical machine translation,
the source sentence is segmented into a sequence
of phrases , and each source phrase is trans-
lated into a target phrase . Target phrases may be
reordered. The translation model used in (Koehn
et al., 2003) is the product of translation probabil-
ity and distortion probability
</bodyText>
<equation confidence="0.968775">
(1)
</equation>
<bodyText confidence="0.972153866666667">
where denotes the start position of the source
phrase translated into the -th target phrase, and
denotes the end position of the source phrase
translated into the -th target phrase.
The translation probability is calculated from
the relative frequency as,
(2)
where is the frequency of alignments
between the source phrase and the target phrase
.
(Koehn et al., 2003) used the following dis-
tortion model, which simply penalizes non-
monotonic phrase alignments based on the word
distance of successively translated source phrases
with an appropriate value for the parameter ,
</bodyText>
<figure confidence="0.697886">
(3)
</figure>
<figureCaption confidence="0.999882">
Figure 1: Phrase alignment and reordering
Figure 2: Four types of reordering patterns
</figureCaption>
<sectionHeader confidence="0.838629" genericHeader="method">
3 The Global Phrase Reordering Model
</sectionHeader>
<bodyText confidence="0.9996436">
Figure 1 shows an example of Japanese-English
phrase alignment that consists of four phrase pairs.
Note that the Japanese verb phrase “ ” at
the the end of the sentence is aligned to the English
verb “is” at the beginning of the sentence just after
the subject. Such reordering is typical in Japanese-
English translations.
Motivated by the three-valued orientation for lo-
cal reordering in (Tillmann and Zhang, 2005), we
define the following four types of reordering pat-
terns, as shown in Figure 2,
monotone adjacent (MA): The two source
phrases are adjacent, and are in the same or-
der as the two target phrases.
monotone gap (MG): The two source phrases
are not adjacent, but are in the same order as
the two target phrases.
reverse adjacent (RA): The two source
phrases are adjacent, but are in the reverse or-
der of the two target phrases.
</bodyText>
<figure confidence="0.999424365853659">
bi_1
bi_1
source
source
fi_1 fi
fi_1 fi
bi
target target
d=MA d=MG
bi
ei
ei_1
ei
ei_1
source fi_1 fi source
fi_1 fi
target target
d=RA d=RG
bi
bi_1
bi
bi_1
ei_1
ei
ei
ei_1
,
714
���� 0) MW
�� � � �
commu-
nication
of
means
4 5
6
1
2
7
8
3
</figure>
<figureCaption confidence="0.999966">
Figure 3: Expansion of a phrase pair
</figureCaption>
<bodyText confidence="0.910132">
clustering. We also investigated various approx-
imations of Equation (4) by reducing the condi-
tional factors.
</bodyText>
<table confidence="0.9984736">
J-to-E C-to-E
Monotone Adjacent 0.441 0.828
Monotone Gap 0.281 0.106
Reverse Adjacent 0.206 0.033
Reverse Gap 0.072 0.033
</table>
<tableCaption confidence="0.999907">
Table 1: Percentage of reordering patterns
</tableCaption>
<bodyText confidence="0.99766592">
reverse gap (RG): The two source phrases are
not adjacent, and are in the reverse order as
the two target phrases.
For the global reordering model, we only con-
sider the cases in which the two target phrases
are adjacent because, in decoding, the target sen-
tence is generated from left to right and phrase by
phrase. If we are to generate the -th target phrase
from the source phrase , we call and the
current block , and and the previous
block
.
Table 1 shows the percentage of each reorder-
ing pattern that appeared in the N-best phrase
alignments of the training bilingual sentences for
the IWSLT 2005 Japanese-English and Chinese-
English translation tasks (Eck and Hori, 2005).
Since non-local reorderings such as monotone gap
and reverse gap are more frequent in Japanese to
English translations, they are worth modeling ex-
plicitly in this reordering model.
Since the probability of reordering pattern (in-
tended to stand for ‘distortion’) is conditioned on
the current and previous blocks, the global phrase
reordering model is formalized as follows:
</bodyText>
<equation confidence="0.566229">
(4)
</equation>
<bodyText confidence="0.9972712">
We can replace the conventional word distance-
based distortion probability in Equa-
tion (1) with the global phrase reordering model
in Equation (4) with minimal modification of the
underlying phrase-based decoding algorithm.
</bodyText>
<sectionHeader confidence="0.988305" genericHeader="method">
4 Parameter Estimation Method
</sectionHeader>
<bodyText confidence="0.999954">
In principle, the parameters of the global phrase
reordering model in Equation (4) can be estimated
from the relative frequencies of respective events
in the Viterbi phrase alignment of the training
bilingual sentences. This straightforward estima-
tion method, however, often suffers from sparse
data problem. To cope with this sparseness, we
used N-best phrase alignment and bilingual phrase
</bodyText>
<subsectionHeader confidence="0.993358">
4.1 N-best Phrase Alignment
</subsectionHeader>
<bodyText confidence="0.998372173913044">
In order to obtain the Viterbi phrase alignment
of a bilingual sentence pair, we search for the
phrase segmentation and phrase alignment that
maximizes the product of the phrase translation
probabilities ,
Phrase translation probabilities are approximated
using word translation probabilities and
as follows,
where and are words in the target and source
phrases.
The phrase alignment based on Equation (5) can
be thought of as an extension of word alignment
based on the IBM Model 1 to phrase alignment.
Note that bilingual phrase segmentation (phrase
extraction) is also done using the same criteria.
The approximation in Equation (6) is motivated
by (Vogel et al., 2003). Here, we added the second
term to cope with the asymmetry between
and . The word translation proba-
bilities are estimated using the GIZA++ (Och and
Ney, 2003).
The above search is implemented in the follow-
ing way:
</bodyText>
<listItem confidence="0.9834769375">
1. All source word and target word pairs are
considered to be initial phrase pairs.
715
2. If the phrase translation probability of the
phrase pair is less than the threshold, it is
deleted.
3. Each phrase pair is expanded toward the eight
neighboring directions as shown in Figure 3.
4. If the phrase translation probability of the ex-
panded phrase pair is less than the threshold,
it is deleted.
5. The process of expansion and deletion is re-
peated until no further expansion is possible.
6. The consistent N-best phrase alignment are
searched from all combinations of the above
phrase pairs.
</listItem>
<bodyText confidence="0.973616130434783">
The search for consistent Viterbi phrase align-
ments can be implemented as a phrase-based de-
coder using a beam search whose outputs are con-
strained only to the target sentence. The consistent
N-best phrase alignment can be obtained by using
A* search as described in (Ueffing et al., 2002).
We did not use any reordering constraints, such as
IBM constraint and ITG constraint in the search
for the N-best phrase alignment (Zens et al., 2004).
The thresholds used in the search are the fol-
lowing: the minimum phrase translation probabil-
ity is 0.0001. The maximum number of translation
candidates for each phrase is 20. The beam width
is 1e-10, the stack size (for each target candidate
word length) is 1000. We found that, compared
with the decoding of sentence translation, we have
to search significantly larger space for the N-best
phrase alignment.
Figure 3 shows an example of phrase pair
expansion toward eight neighbors. If the cur-
rent phrase pair is ( , of), the expanded phrase
pairs are ( , means of), ( ,
means of), ( , means of), (
</bodyText>
<listItem confidence="0.7225005">
, of), ( , of), (
, of communication), ( , of communication),
</listItem>
<bodyText confidence="0.994232727272727">
and ( , of communication).
Figure 4 shows an example of the best three
phrase alignments for a Japanese-English bilin-
gual sentence. For the estimation of the global
phrase reordering model, preliminary tests have
shown that the appropriate N-best number is 20.
In counting the events for the relative frequency
estimation, we treat all N-best phrase alignments
equally.
For comparison, we also implemented a dif-
ferent N-best phrase alignment method, where
</bodyText>
<figure confidence="0.882915333333333">
____
the_light_was_red
_ __
the_light was_red
_  _
(3) the_light was red
</figure>
<figureCaption confidence="0.999574">
Figure 4: N-best phrase alignments
</figureCaption>
<bodyText confidence="0.999975">
phrase pairs are extracted using the standard
phrase extraction method described in (Koehn et
al., 2003). We call this conventional phrase ex-
traction method “grow-diag-final”, and the pro-
posed phrase extraction method “ppicker” (this is
intended to stand for phrase picker).
</bodyText>
<subsectionHeader confidence="0.993432">
4.2 Bilingual Phrase Clustering
</subsectionHeader>
<bodyText confidence="0.9997982">
The second approach to cope with the sparseness
in Equation (4) is to group the phrases into equiv-
alence classes. We used a bilingual word cluster-
ing tool, mkcls (Och et al., 1999) for this purpose.
It forms partitions of the vocabulary of the two
languages to maximize the joint probability of the
training bilingual corpus.
In order to perform bilingual phrase clustering,
all words in a phrase are concatenated by an under-
score ’ ’ to form a pseudo word. We then use the
modified bilingual sentences as the input to mk-
cls. We treat all N-best phrase alignments equally.
Thus, the phrase alignments in Figure 4 are con-
verted to the following three bilingual sentence
pairs.
</bodyText>
<equation confidence="0.930040166666667">
_ _ _ _
the_light_was_red
_ _ _
the_light was_red
_ _
the_light was red
</equation>
<bodyText confidence="0.99993175">
Preliminary tests have shown that the appropriate
number of classes for the estimation of the global
phrase reordering model is 20.
As a comparison, we also tried two phrase clas-
sification methods based on the part of speech of
the head word (Ohashi et al., 2005). We defined
(arguably) the first word of each English phrase
and the last word of each Japanese phrase as the
</bodyText>
<figure confidence="0.940944727272727">
716
shorthand reordering model
baseline
e[0]
f[0]
e[0]f[0]
e[-1]f[0]
e[0]f[-1,0]
e[-1]f[-1,0]
e[-1,0]f[0]
e[-1,0]f[-1,0]
</figure>
<tableCaption confidence="0.72502">
Table 2: All reordering models tried in the experi-
</tableCaption>
<bodyText confidence="0.951952375">
ments
head word. We then used the part of speech of
the head word as the phrase class. We call this
method “1pos”. Since we are not sure whether it is
appropriate to introduce asymmetry in head word
selection, we also tried a “2pos” method, where
the parts of speech of both the first and the last
words are used for phrase classification.
</bodyText>
<subsectionHeader confidence="0.999522">
4.3 Conditioning Factor of Reordering
</subsectionHeader>
<bodyText confidence="0.9963503125">
The third approach to cope with sparseness in
Equation (4) is to approximate the equation by re-
ducing the conditioning factors.
Other than the baseline word distance-based
reordering model and the Equation (4) itself,
we tried eight different approximations of Equa-
tion (4) as shown in Table 2, where, the symbol in
the left column is the shorthand for the reordering
model in the right column.
The approximations are designed based on two
intuitions. The current block ( and ) would
probably be more important than the previous
block ( and ). The previous target phrase
( ) might be more important than the current
target phrase ( ) because the distortion model of
IBM 4 is conditioned on
</bodyText>
<sectionHeader confidence="0.999186" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999218">
5.1 Corpus and Tools
</subsectionHeader>
<bodyText confidence="0.992672">
We used the IWSLT-2005 Japanese-English trans-
lation task (Eck and Hori, 2005) for evaluating the
proposed global phrase reordering model. We re-
port results using the well-known automatic eval-
uation metrics Bleu (Papineni et al., 2002).
</bodyText>
<table confidence="0.82717575">
IWSLT (International Workshop on Spoken
Sentences Words Vocabulary
Japanese 20,000 198,453 9,277
English 20,000 183,452 6,956
</table>
<tableCaption confidence="0.989637">
Table 3: IWSLT 2005 Japanese-English training
</tableCaption>
<bodyText confidence="0.989091785714286">
data
Language Translation) 2005 is an evaluation cam-
paign for spoken language translation.Its task
domain encompasses basic travel conversations.
20,000 bilingual sentences are provided for train-
ing. Table 3 shows the number of words and the
size of vocabulary of the training data. The av-
erage sentence length of Japanese is 9.9 words,
while that of English is 9.2 words.
Two development sets, each containing 500
source sentences, are also provided and each
development sentence comes with 16 reference
translations. We used the second development set
(devset2) for the experiments described in this pa-
per. This 20,000 sentence corpus allows for fast
experimentation and enables us to study different
aspects of the proposed global phrase reordering
model.
Japanese word segmentation was done using
ChaSen2 and English tokenization was done using
a tool provided by LDC3. For the phrase classi-
fication based on the parts of speech of the head
word, we used the first two layers of the Chasen’s
part of speech tag for Japanese. For English part
of speech tagging, we used MXPOST4.
Word translation probabilities are obtained by
using GIZA++ (Och and Ney, 2003). For training,
all English words are made in lower case. We used
a back-off word trigram model as the language
model. It is trained from the lowercased English
side of the training corpus using a statistical lan-
guage modeling toolkit, Palmkit 5.
We implemented our own decoder based on the
algorithm described in (Ueffing et al., 2002). For
decoding, we used phrase translation probability,
lexical translation probability, word penalty, and
distortion (phrase reordering) probability. Mini-
mum error rate training was not used for weight
optimization.
The thresholds used in the decoding are the fol-
lowing: the minimum phrase translation probabil-
ity is 0.01. The maximum number of translation
</bodyText>
<footnote confidence="0.99859725">
2http://chasen.aist-nara.ac.jp/
3http://www.cis.upenn.edu/˜treebank/tokenizer.sed
4http://www.cis.upenn.edu/˜adwait/statnlp.html
5http://palmkit.sourceforge.net/
</footnote>
<bodyText confidence="0.666067666666667">
, and . The
appropriate form of the global phrase reordering
model is decided through experimentation.
</bodyText>
<page confidence="0.406899">
717
</page>
<table confidence="0.9994952">
ppicker grow-diag-final
class lex class lex
baseline 0.400 0.400 0.343 0.343
0.407 0.407 0.350 0.350
f[0] 0.417 0.410 0.362 0.356
e[0] 0.422 0.416 0.356 0.360
e[0]f[0] 0.422 0.404 0.355 0.353
e[0]f[-1,0] 0.407 0.381 0.346 0.327
e[-1,0]f[0] 0.410 0.392 0.348 0.341
e[-1,0]f[-1,0] 0.394 0.387 0.339 0.340
</table>
<tableCaption confidence="0.836617">
Table 4: BLEU score of reordering models with
different phrase extraction methods
</tableCaption>
<bodyText confidence="0.991508">
candidates for each phrase is 10. The beam width
is 1e-5, the stack size (for each target candidate
word length) is 100.
</bodyText>
<subsectionHeader confidence="0.989273">
5.2 Clustered and Lexicalized Model
</subsectionHeader>
<bodyText confidence="0.999991952380953">
Figure 5 shows the BLEU score of clustered and
lexical reordering model with different condition-
ing factors. Here, “class” shows the accuracy
when the identity of each phrase is represented by
its class, which is obtained by the bilingual phrase
clustering, while “lex” shows the accuracy when
the identity of each phrases is represented by its
lexical form.
The clustered reordering model “class” is gen-
erally better than the lexicalized reordering model
“lex”. The accuracy of “lex” drops rapidly as the
number of conditioning factors increases. The re-
ordering models using the part of speech of the
head word for phrase classification such as “1pos”
and “2pos” are somewhere in between.
The best score is achieved by the clustered
model when the phrase reordering pattern is con-
ditioned on either the current target phrase or
the current block, namely phrase pair and .
They are significantly better than the baseline of
the word distance-based reordering model.
</bodyText>
<subsectionHeader confidence="0.905837">
5.3 Interaction between Phrase Extraction
and Phrase Alignment
</subsectionHeader>
<bodyText confidence="0.999966733333334">
Table 4 shows the BLEU score of reordering mod-
els with different phrase extraction methods. Here,
“ppicker” shows the accuracy when phrases are
extracted by using the N-best phrase alignment
method described in Section 4.1, while “grow-
diag-final” shows the accuracy when phrases are
extracted using the standard phrase extraction al-
gorithm described in (Koehn et al., 2003).
It is obvious that, for building the global phrase
reordering model, our phrase extraction method is
significantly better than the conventional phrase
extraction method. We assume this is because the
proposed N-best phrase alignment method opti-
mizes the combination of phrase extraction (seg-
mentation) and phrase alignment in a sentence.
</bodyText>
<subsectionHeader confidence="0.96752">
5.4 Global and Local Reordering Model
</subsectionHeader>
<bodyText confidence="0.99998080952381">
In order to show the advantages of explicitly mod-
eling global phrase reordering, we implemented a
different reordering model where the reordering
pattern is classified into three values: monotone
adjacent, reverse adjacent and neutral. By collaps-
ing monotone gap and reverse gap into neutral, it
can be thought of as a local reordering model sim-
ilar to the block orientation bigram (Tillmann and
Zhang, 2005).
Figure 6 shows the BLEU score of the local
and global reordering models. Here, “class3”
and “lex3”represent the three-valued local reorder-
ing model, while “class4” and “lex4”represent the
four-valued global reordering model. “Class” and
“lex” represent clustered and lexical models, re-
spectively. We used “grow-diag-final” for phrase
extraction in this experiment.
It is obvious that the four-valued global reorder-
ing model consistently outperformed the three-
valued local reordering model under various con-
ditioning factors.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999777142857143">
As shown in Figure 5, the reordering model of
Equation (4) (indicated as e[-1,0]f[-1,0] in short-
hand) suffers from a sparse data problem even if
phrase clustering is used. The empirically justifi-
able global reordering model seems to be the fol-
lowing, conditioned on the classes of source and
target phrases:
</bodyText>
<equation confidence="0.656939">
(7)
</equation>
<bodyText confidence="0.9999566">
which is similar to the block orientation bigram
(Tillmann and Zhang, 2005). We should note,
however, that the block orientation bigram is a
joint probability model for the sequence of blocks
(source and target phrases) as well as their orien-
tations (reordering pattern) whose purpose is very
different from our global phrase reordering model.
The advantage of the reordering model is that it
can better model global phrase reordering using a
four-valued reordering pattern, and it can be easily
</bodyText>
<page confidence="0.627454">
718
</page>
<figureCaption confidence="0.997881">
Figure 5: BLEU score for the clustered and lexical reordering model with different conditioning factors
</figureCaption>
<bodyText confidence="0.9999195">
incorporated into a standard phrase-based transla-
tion decoder.
The problem of the global phrase reordering
model is the cost of parameter estimation. In
particular, the N-best phrase alignment described
in Section 4.1 is computationally expensive. We
must devise a more efficient phrase alignment al-
gorithm that can globally optimize both phrase
segmentation (phrase extraction) and phrase align-
ment.
</bodyText>
<sectionHeader confidence="0.997638" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999665">
In this paper, we presented a novel global phrase
reordering model, that is estimated from the N-
best phrase alignment of training bilingual sen-
tences. Through experiments, we were able to
show that our reordering model offers improved
translation accuracy over the baseline method.
</bodyText>
<sectionHeader confidence="0.996372" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999648195121951">
Matthias Eck and Chiori Hori. 2005. Overview of
the IWSLT 2005 evaluation campaign. In Proceed-
ings ofInternational Workshop on Spoken Language
Translation (IWSLT 2005), pages 11–32.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of
the North American Chapter of the Association of
Computational Linguistics (HLT-NAACL-03), pages
127–133.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417–449.
Franz Josef Och, Christoph Tillman, and Hermann
Ney. 1999. Improved alignment models for statisti-
cal machine translation. In Proceedings of the 1999
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora (EMNLP/WVLC-99), pages 20–28.
Kazuteru Ohashi, Kazuhide Yamamoto, Kuniko Saito,
and Masaaki Nagata. 2005. NUT-NTT statistical
machine translation system for IWSLT 2005. In
Proceedings of International Workshop on Spoken
Language Translation, pages 128–133.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Lnguistics (ACL-02), pages 311–318.
Christoph Tillmann and Tong Zhang. 2005. A local-
ized prediction model for statistical machine trans-
lation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05), pages 557–564.
Nicola Ueffing, Franz Josef Och, and Hermann Ney.
2002. Generation of word graphs in statistical ma-
chine translation. In Proceedings of the Conference
</reference>
<page confidence="0.831207">
719
</page>
<figureCaption confidence="0.959286">
Figure 6: BLEU score of local and global reordering model
</figureCaption>
<reference confidence="0.997767583333333">
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-02), pages 156–163.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble,
Ashish Venugopal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical machine translation sys-
tem. In Proceedings ofMT Summit IX.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering constraints for
phrase-based statistical machine translation. In Pro-
ceedings of 20th International Conference on Com-
putational Linguistics (COLING-04), pages 205–
211.
</reference>
<page confidence="0.847682">
720
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.621296">
<title confidence="0.999728">A Clustered Global Phrase Reordering Model for Statistical Machine Translation</title>
<author confidence="0.996243">Masaaki Nagata Kuniko Saito</author>
<affiliation confidence="0.996117">NTT Communication Science Laboratories NTT Cyber Space Laboratories</affiliation>
<address confidence="0.9088395">2-4 Hikaridai, Seika-cho, Souraku-gun 1-1 Hikarinooka, Yokoshuka-shi Kyoto, 619-0237 Japan Kanagawa, 239-0847 Japan</address>
<email confidence="0.953798">nagata.masaaki@labs.ntt.co.jp,saito.kuniko@labs.ntt.co.jp</email>
<author confidence="0.85671">Kazuhide Yamamoto</author>
<author confidence="0.85671">Kazuteru Ohashi</author>
<affiliation confidence="0.999996">Nagaoka University of Technology</affiliation>
<address confidence="0.995635">1603-1, Kamitomioka, Nagaoka City Niigata, 940-2188 Japan</address>
<email confidence="0.984241">ykaz@nlp.nagaokaut.ac.jp,ohashi@nlp.nagaokaut.ac.jp</email>
<abstract confidence="0.9973482">In this paper, we present a novel global reordering model that can be incorporated into standard phrase-based statistical machine translation. Unlike previous local reordering models that emphasize the reordering of adjacent phrase pairs (Tillmann and Zhang, 2005), our model explicitly models the reordering of long distances by directly estimating the parameters from the phrase alignments of bilingual training sentences. In principle, the global phrase reordering model is conditioned on the source and target phrases that are currently being translated, and the previously translated source and target phrases. To cope with sparseness, we use N-best phrase alignments and bilingual phrase clustering, and investigate a variety of combinations of conditioning factors. Through experiments, we show, that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Chiori Hori</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2005</date>
<booktitle>In Proceedings ofInternational Workshop on Spoken Language Translation (IWSLT</booktitle>
<pages>11--32</pages>
<contexts>
<context position="4092" citStr="Eck and Hori, 2005" startWordPosition="597" endWordPosition="600">since it is at most considers two phrases. A truly global reordering model would take the entire sentence structure into account. 713 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 713–720, Sydney, July 2006. c�2006 Association for Computational Linguistics eter estimation method including N-best phrase alignments and bilingual phrase clustering. Next, through an experiment, we show that the global phrase reordering model significantly improves the translation accuracy of the IWSLT-2005 Japanese-English translation task (Eck and Hori, 2005). b4 RA b3 RA MG b2 b1 p f 0) 3 1-A t &amp; )7.) communication of means a is language 2 Baseline Translation Model In statistical machine translation, the translation of a source (foreign) sentence is formulated as the search for a target (English) sentence that maximizes the conditional probability , which can be rewritten using the Bayes rule as, where is a translation model and is a target language model. In phrase-based statistical machine translation, the source sentence is segmented into a sequence of phrases , and each source phrase is translated into a target phrase . Target phrases may be</context>
<context position="7594" citStr="Eck and Hori, 2005" startWordPosition="1206" endWordPosition="1209"> and are in the reverse order as the two target phrases. For the global reordering model, we only consider the cases in which the two target phrases are adjacent because, in decoding, the target sentence is generated from left to right and phrase by phrase. If we are to generate the -th target phrase from the source phrase , we call and the current block , and and the previous block . Table 1 shows the percentage of each reordering pattern that appeared in the N-best phrase alignments of the training bilingual sentences for the IWSLT 2005 Japanese-English and ChineseEnglish translation tasks (Eck and Hori, 2005). Since non-local reorderings such as monotone gap and reverse gap are more frequent in Japanese to English translations, they are worth modeling explicitly in this reordering model. Since the probability of reordering pattern (intended to stand for ‘distortion’) is conditioned on the current and previous blocks, the global phrase reordering model is formalized as follows: (4) We can replace the conventional word distancebased distortion probability in Equation (1) with the global phrase reordering model in Equation (4) with minimal modification of the underlying phrase-based decoding algorith</context>
<context position="14597" citStr="Eck and Hori, 2005" startWordPosition="2363" endWordPosition="2366">model and the Equation (4) itself, we tried eight different approximations of Equation (4) as shown in Table 2, where, the symbol in the left column is the shorthand for the reordering model in the right column. The approximations are designed based on two intuitions. The current block ( and ) would probably be more important than the previous block ( and ). The previous target phrase ( ) might be more important than the current target phrase ( ) because the distortion model of IBM 4 is conditioned on 5 Experiments 5.1 Corpus and Tools We used the IWSLT-2005 Japanese-English translation task (Eck and Hori, 2005) for evaluating the proposed global phrase reordering model. We report results using the well-known automatic evaluation metrics Bleu (Papineni et al., 2002). IWSLT (International Workshop on Spoken Sentences Words Vocabulary Japanese 20,000 198,453 9,277 English 20,000 183,452 6,956 Table 3: IWSLT 2005 Japanese-English training data Language Translation) 2005 is an evaluation campaign for spoken language translation.Its task domain encompasses basic travel conversations. 20,000 bilingual sentences are provided for training. Table 3 shows the number of words and the size of vocabulary of the t</context>
</contexts>
<marker>Eck, Hori, 2005</marker>
<rawString>Matthias Eck and Chiori Hori. 2005. Overview of the IWSLT 2005 evaluation campaign. In Proceedings ofInternational Workshop on Spoken Language Translation (IWSLT 2005), pages 11–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL-03),</booktitle>
<pages>127--133</pages>
<contexts>
<context position="2206" citStr="Koehn et al., 2003" startWordPosition="309" endWordPosition="312">model should allow the reordering of any distance, because if we are to translate from Japanese to English, the verb in the Japanese sentence must be moved from the end of the sentence to the beginning just after the subject in the English sentence. ✁ Graduated in March 2006 Standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases (Koehn et al., 2003; Och and Ney, 2004). (Tillmann and Zhang, 2005) introduced the notion of a block (a pair of source and target phrases that are translations of each other), and proposed the block orientation bigram in which the local reordering of adjacent blocks are expressed as a three-valued orientation, namely Right (monotone), Left (swapped), or Neutral. A block with neutral orientation is supposed to be less strongly linked to its predecessor block: thus in their model, the global reordering is not explicitly modeled. In this paper, we present a global reordering model that explicitly models long distan</context>
<context position="4754" citStr="Koehn et al., 2003" startWordPosition="713" endWordPosition="716">) communication of means a is language 2 Baseline Translation Model In statistical machine translation, the translation of a source (foreign) sentence is formulated as the search for a target (English) sentence that maximizes the conditional probability , which can be rewritten using the Bayes rule as, where is a translation model and is a target language model. In phrase-based statistical machine translation, the source sentence is segmented into a sequence of phrases , and each source phrase is translated into a target phrase . Target phrases may be reordered. The translation model used in (Koehn et al., 2003) is the product of translation probability and distortion probability (1) where denotes the start position of the source phrase translated into the -th target phrase, and denotes the end position of the source phrase translated into the -th target phrase. The translation probability is calculated from the relative frequency as, (2) where is the frequency of alignments between the source phrase and the target phrase . (Koehn et al., 2003) used the following distortion model, which simply penalizes nonmonotonic phrase alignments based on the word distance of successively translated source phrase</context>
<context position="11921" citStr="Koehn et al., 2003" startWordPosition="1908" endWordPosition="1911"> phrase alignments for a Japanese-English bilingual sentence. For the estimation of the global phrase reordering model, preliminary tests have shown that the appropriate N-best number is 20. In counting the events for the relative frequency estimation, we treat all N-best phrase alignments equally. For comparison, we also implemented a different N-best phrase alignment method, where ____ the_light_was_red _ __ the_light was_red _  _ (3) the_light was red Figure 4: N-best phrase alignments phrase pairs are extracted using the standard phrase extraction method described in (Koehn et al., 2003). We call this conventional phrase extraction method “grow-diag-final”, and the proposed phrase extraction method “ppicker” (this is intended to stand for phrase picker). 4.2 Bilingual Phrase Clustering The second approach to cope with the sparseness in Equation (4) is to group the phrases into equivalence classes. We used a bilingual word clustering tool, mkcls (Och et al., 1999) for this purpose. It forms partitions of the vocabulary of the two languages to maximize the joint probability of the training bilingual corpus. In order to perform bilingual phrase clustering, all words in a phrase </context>
<context position="18996" citStr="Koehn et al., 2003" startWordPosition="3030" endWordPosition="3033">tioned on either the current target phrase or the current block, namely phrase pair and . They are significantly better than the baseline of the word distance-based reordering model. 5.3 Interaction between Phrase Extraction and Phrase Alignment Table 4 shows the BLEU score of reordering models with different phrase extraction methods. Here, “ppicker” shows the accuracy when phrases are extracted by using the N-best phrase alignment method described in Section 4.1, while “growdiag-final” shows the accuracy when phrases are extracted using the standard phrase extraction algorithm described in (Koehn et al., 2003). It is obvious that, for building the global phrase reordering model, our phrase extraction method is significantly better than the conventional phrase extraction method. We assume this is because the proposed N-best phrase alignment method optimizes the combination of phrase extraction (segmentation) and phrase alignment in a sentence. 5.4 Global and Local Reordering Model In order to show the advantages of explicitly modeling global phrase reordering, we implemented a different reordering model where the reordering pattern is classified into three values: monotone adjacent, reverse adjacent</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL-03), pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9478" citStr="Och and Ney, 2003" startWordPosition="1496" endWordPosition="1499">translation probabilities are approximated using word translation probabilities and as follows, where and are words in the target and source phrases. The phrase alignment based on Equation (5) can be thought of as an extension of word alignment based on the IBM Model 1 to phrase alignment. Note that bilingual phrase segmentation (phrase extraction) is also done using the same criteria. The approximation in Equation (6) is motivated by (Vogel et al., 2003). Here, we added the second term to cope with the asymmetry between and . The word translation probabilities are estimated using the GIZA++ (Och and Ney, 2003). The above search is implemented in the following way: 1. All source word and target word pairs are considered to be initial phrase pairs. 715 2. If the phrase translation probability of the phrase pair is less than the threshold, it is deleted. 3. Each phrase pair is expanded toward the eight neighboring directions as shown in Figure 3. 4. If the phrase translation probability of the expanded phrase pair is less than the threshold, it is deleted. 5. The process of expansion and deletion is repeated until no further expansion is possible. 6. The consistent N-best phrase alignment are searched</context>
<context position="16089" citStr="Och and Ney, 2003" startWordPosition="2595" endWordPosition="2598">opment set (devset2) for the experiments described in this paper. This 20,000 sentence corpus allows for fast experimentation and enables us to study different aspects of the proposed global phrase reordering model. Japanese word segmentation was done using ChaSen2 and English tokenization was done using a tool provided by LDC3. For the phrase classification based on the parts of speech of the head word, we used the first two layers of the Chasen’s part of speech tag for Japanese. For English part of speech tagging, we used MXPOST4. Word translation probabilities are obtained by using GIZA++ (Och and Ney, 2003). For training, all English words are made in lower case. We used a back-off word trigram model as the language model. It is trained from the lowercased English side of the training corpus using a statistical language modeling toolkit, Palmkit 5. We implemented our own decoder based on the algorithm described in (Ueffing et al., 2002). For decoding, we used phrase translation probability, lexical translation probability, word penalty, and distortion (phrase reordering) probability. Minimum error rate training was not used for weight optimization. The thresholds used in the decoding are the fol</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Herman Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="2226" citStr="Och and Ney, 2004" startWordPosition="313" endWordPosition="316">he reordering of any distance, because if we are to translate from Japanese to English, the verb in the Japanese sentence must be moved from the end of the sentence to the beginning just after the subject in the English sentence. ✁ Graduated in March 2006 Standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases (Koehn et al., 2003; Och and Ney, 2004). (Tillmann and Zhang, 2005) introduced the notion of a block (a pair of source and target phrases that are translations of each other), and proposed the block orientation bigram in which the local reordering of adjacent blocks are expressed as a three-valued orientation, namely Right (monotone), Left (swapped), or Neutral. A block with neutral orientation is supposed to be less strongly linked to its predecessor block: thus in their model, the global reordering is not explicitly modeled. In this paper, we present a global reordering model that explicitly models long distance reordering&apos;. It p</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Herman Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillman</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/WVLC-99),</booktitle>
<pages>20--28</pages>
<contexts>
<context position="12304" citStr="Och et al., 1999" startWordPosition="1970" endWordPosition="1973"> ____ the_light_was_red _ __ the_light was_red _  _ (3) the_light was red Figure 4: N-best phrase alignments phrase pairs are extracted using the standard phrase extraction method described in (Koehn et al., 2003). We call this conventional phrase extraction method “grow-diag-final”, and the proposed phrase extraction method “ppicker” (this is intended to stand for phrase picker). 4.2 Bilingual Phrase Clustering The second approach to cope with the sparseness in Equation (4) is to group the phrases into equivalence classes. We used a bilingual word clustering tool, mkcls (Och et al., 1999) for this purpose. It forms partitions of the vocabulary of the two languages to maximize the joint probability of the training bilingual corpus. In order to perform bilingual phrase clustering, all words in a phrase are concatenated by an underscore ’ ’ to form a pseudo word. We then use the modified bilingual sentences as the input to mkcls. We treat all N-best phrase alignments equally. Thus, the phrase alignments in Figure 4 are converted to the following three bilingual sentence pairs. _ _ _ _ the_light_was_red _ _ _ the_light was_red _ _ the_light was red Preliminary tests have shown tha</context>
</contexts>
<marker>Och, Tillman, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillman, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/WVLC-99), pages 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuteru Ohashi</author>
<author>Kazuhide Yamamoto</author>
<author>Kuniko Saito</author>
<author>Masaaki Nagata</author>
</authors>
<title>NUT-NTT statistical machine translation system for IWSLT</title>
<date>2005</date>
<booktitle>In Proceedings of International Workshop on Spoken Language Translation,</booktitle>
<pages>128--133</pages>
<contexts>
<context position="13135" citStr="Ohashi et al., 2005" startWordPosition="2116" endWordPosition="2119">hrase are concatenated by an underscore ’ ’ to form a pseudo word. We then use the modified bilingual sentences as the input to mkcls. We treat all N-best phrase alignments equally. Thus, the phrase alignments in Figure 4 are converted to the following three bilingual sentence pairs. _ _ _ _ the_light_was_red _ _ _ the_light was_red _ _ the_light was red Preliminary tests have shown that the appropriate number of classes for the estimation of the global phrase reordering model is 20. As a comparison, we also tried two phrase classification methods based on the part of speech of the head word (Ohashi et al., 2005). We defined (arguably) the first word of each English phrase and the last word of each Japanese phrase as the 716 shorthand reordering model baseline e[0] f[0] e[0]f[0] e[-1]f[0] e[0]f[-1,0] e[-1]f[-1,0] e[-1,0]f[0] e[-1,0]f[-1,0] Table 2: All reordering models tried in the experiments head word. We then used the part of speech of the head word as the phrase class. We call this method “1pos”. Since we are not sure whether it is appropriate to introduce asymmetry in head word selection, we also tried a “2pos” method, where the parts of speech of both the first and the last words are used for p</context>
</contexts>
<marker>Ohashi, Yamamoto, Saito, Nagata, 2005</marker>
<rawString>Kazuteru Ohashi, Kazuhide Yamamoto, Kuniko Saito, and Masaaki Nagata. 2005. NUT-NTT statistical machine translation system for IWSLT 2005. In Proceedings of International Workshop on Spoken Language Translation, pages 128–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Lnguistics (ACL-02),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="14754" citStr="Papineni et al., 2002" startWordPosition="2387" endWordPosition="2390">e shorthand for the reordering model in the right column. The approximations are designed based on two intuitions. The current block ( and ) would probably be more important than the previous block ( and ). The previous target phrase ( ) might be more important than the current target phrase ( ) because the distortion model of IBM 4 is conditioned on 5 Experiments 5.1 Corpus and Tools We used the IWSLT-2005 Japanese-English translation task (Eck and Hori, 2005) for evaluating the proposed global phrase reordering model. We report results using the well-known automatic evaluation metrics Bleu (Papineni et al., 2002). IWSLT (International Workshop on Spoken Sentences Words Vocabulary Japanese 20,000 198,453 9,277 English 20,000 183,452 6,956 Table 3: IWSLT 2005 Japanese-English training data Language Translation) 2005 is an evaluation campaign for spoken language translation.Its task domain encompasses basic travel conversations. 20,000 bilingual sentences are provided for training. Table 3 shows the number of words and the size of vocabulary of the training data. The average sentence length of Japanese is 9.9 words, while that of English is 9.2 words. Two development sets, each containing 500 source sent</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Lnguistics (ACL-02), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A localized prediction model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05),</booktitle>
<pages>557--564</pages>
<contexts>
<context position="802" citStr="Tillmann and Zhang, 2005" startWordPosition="91" endWordPosition="95">4 Hikaridai, Seika-cho, Souraku-gun 1-1 Hikarinooka, Yokoshuka-shi Kyoto, 619-0237 Japan Kanagawa, 239-0847 Japan nagata.masaaki@labs.ntt.co.jp, saito.kuniko@labs.ntt.co.jp Kazuhide Yamamoto, Kazuteru Ohashi Nagaoka University of Technology 1603-1, Kamitomioka, Nagaoka City Niigata, 940-2188 Japan ykaz@nlp.nagaokaut.ac.jp, ohashi@nlp.nagaokaut.ac.jp Abstract In this paper, we present a novel global reordering model that can be incorporated into standard phrase-based statistical machine translation. Unlike previous local reordering models that emphasize the reordering of adjacent phrase pairs (Tillmann and Zhang, 2005), our model explicitly models the reordering of long distances by directly estimating the parameters from the phrase alignments of bilingual training sentences. In principle, the global phrase reordering model is conditioned on the source and target phrases that are currently being translated, and the previously translated source and target phrases. To cope with sparseness, we use N-best phrase alignments and bilingual phrase clustering, and investigate a variety of combinations of conditioning factors. Through experiments, we show, that the global reordering model significantly improves the t</context>
<context position="2254" citStr="Tillmann and Zhang, 2005" startWordPosition="317" endWordPosition="320">distance, because if we are to translate from Japanese to English, the verb in the Japanese sentence must be moved from the end of the sentence to the beginning just after the subject in the English sentence. ✁ Graduated in March 2006 Standard phrase-based translation systems use a word distance-based reordering model in which non-monotonic phrase alignment is penalized based on the word distance between successively translated source phrases without considering the orientation of the phrase alignment or the identities of the source and target phrases (Koehn et al., 2003; Och and Ney, 2004). (Tillmann and Zhang, 2005) introduced the notion of a block (a pair of source and target phrases that are translations of each other), and proposed the block orientation bigram in which the local reordering of adjacent blocks are expressed as a three-valued orientation, namely Right (monotone), Left (swapped), or Neutral. A block with neutral orientation is supposed to be less strongly linked to its predecessor block: thus in their model, the global reordering is not explicitly modeled. In this paper, we present a global reordering model that explicitly models long distance reordering&apos;. It predicts four type of reorder</context>
<context position="5943" citStr="Tillmann and Zhang, 2005" startWordPosition="904" endWordPosition="907">uccessively translated source phrases with an appropriate value for the parameter , (3) Figure 1: Phrase alignment and reordering Figure 2: Four types of reordering patterns 3 The Global Phrase Reordering Model Figure 1 shows an example of Japanese-English phrase alignment that consists of four phrase pairs. Note that the Japanese verb phrase “ ” at the the end of the sentence is aligned to the English verb “is” at the beginning of the sentence just after the subject. Such reordering is typical in JapaneseEnglish translations. Motivated by the three-valued orientation for local reordering in (Tillmann and Zhang, 2005), we define the following four types of reordering patterns, as shown in Figure 2, monotone adjacent (MA): The two source phrases are adjacent, and are in the same order as the two target phrases. monotone gap (MG): The two source phrases are not adjacent, but are in the same order as the two target phrases. reverse adjacent (RA): The two source phrases are adjacent, but are in the reverse order of the two target phrases. bi_1 bi_1 source source fi_1 fi fi_1 fi bi target target d=MA d=MG bi ei ei_1 ei ei_1 source fi_1 fi source fi_1 fi target target d=RA d=RG bi bi_1 bi bi_1 ei_1 ei ei ei_1 , </context>
<context position="19782" citStr="Tillmann and Zhang, 2005" startWordPosition="3151" endWordPosition="3154">on method. We assume this is because the proposed N-best phrase alignment method optimizes the combination of phrase extraction (segmentation) and phrase alignment in a sentence. 5.4 Global and Local Reordering Model In order to show the advantages of explicitly modeling global phrase reordering, we implemented a different reordering model where the reordering pattern is classified into three values: monotone adjacent, reverse adjacent and neutral. By collapsing monotone gap and reverse gap into neutral, it can be thought of as a local reordering model similar to the block orientation bigram (Tillmann and Zhang, 2005). Figure 6 shows the BLEU score of the local and global reordering models. Here, “class3” and “lex3”represent the three-valued local reordering model, while “class4” and “lex4”represent the four-valued global reordering model. “Class” and “lex” represent clustered and lexical models, respectively. We used “grow-diag-final” for phrase extraction in this experiment. It is obvious that the four-valued global reordering model consistently outperformed the threevalued local reordering model under various conditioning factors. 6 Discussion As shown in Figure 5, the reordering model of Equation (4) (</context>
</contexts>
<marker>Tillmann, Zhang, 2005</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2005. A localized prediction model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05), pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Generation of word graphs in statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-02),</booktitle>
<pages>156--163</pages>
<contexts>
<context position="10416" citStr="Ueffing et al., 2002" startWordPosition="1657" endWordPosition="1660">s as shown in Figure 3. 4. If the phrase translation probability of the expanded phrase pair is less than the threshold, it is deleted. 5. The process of expansion and deletion is repeated until no further expansion is possible. 6. The consistent N-best phrase alignment are searched from all combinations of the above phrase pairs. The search for consistent Viterbi phrase alignments can be implemented as a phrase-based decoder using a beam search whose outputs are constrained only to the target sentence. The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al., 2002). We did not use any reordering constraints, such as IBM constraint and ITG constraint in the search for the N-best phrase alignment (Zens et al., 2004). The thresholds used in the search are the following: the minimum phrase translation probability is 0.0001. The maximum number of translation candidates for each phrase is 20. The beam width is 1e-10, the stack size (for each target candidate word length) is 1000. We found that, compared with the decoding of sentence translation, we have to search significantly larger space for the N-best phrase alignment. Figure 3 shows an example of phrase p</context>
<context position="16425" citStr="Ueffing et al., 2002" startWordPosition="2652" endWordPosition="2655">r the phrase classification based on the parts of speech of the head word, we used the first two layers of the Chasen’s part of speech tag for Japanese. For English part of speech tagging, we used MXPOST4. Word translation probabilities are obtained by using GIZA++ (Och and Ney, 2003). For training, all English words are made in lower case. We used a back-off word trigram model as the language model. It is trained from the lowercased English side of the training corpus using a statistical language modeling toolkit, Palmkit 5. We implemented our own decoder based on the algorithm described in (Ueffing et al., 2002). For decoding, we used phrase translation probability, lexical translation probability, word penalty, and distortion (phrase reordering) probability. Minimum error rate training was not used for weight optimization. The thresholds used in the decoding are the following: the minimum phrase translation probability is 0.01. The maximum number of translation 2http://chasen.aist-nara.ac.jp/ 3http://www.cis.upenn.edu/˜treebank/tokenizer.sed 4http://www.cis.upenn.edu/˜adwait/statnlp.html 5http://palmkit.sourceforge.net/ , and . The appropriate form of the global phrase reordering model is decided th</context>
</contexts>
<marker>Ueffing, Och, Ney, 2002</marker>
<rawString>Nicola Ueffing, Franz Josef Och, and Hermann Ney. 2002. Generation of word graphs in statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-02), pages 156–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Ying Zhang</author>
<author>Fei Huang</author>
<author>Alicia Tribble</author>
<author>Ashish Venugopal</author>
<author>Bing Zhao</author>
<author>Alex Waibel</author>
</authors>
<title>The CMU statistical machine translation system.</title>
<date>2003</date>
<booktitle>In Proceedings ofMT Summit IX.</booktitle>
<contexts>
<context position="9319" citStr="Vogel et al., 2003" startWordPosition="1468" endWordPosition="1471">bilingual sentence pair, we search for the phrase segmentation and phrase alignment that maximizes the product of the phrase translation probabilities , Phrase translation probabilities are approximated using word translation probabilities and as follows, where and are words in the target and source phrases. The phrase alignment based on Equation (5) can be thought of as an extension of word alignment based on the IBM Model 1 to phrase alignment. Note that bilingual phrase segmentation (phrase extraction) is also done using the same criteria. The approximation in Equation (6) is motivated by (Vogel et al., 2003). Here, we added the second term to cope with the asymmetry between and . The word translation probabilities are estimated using the GIZA++ (Och and Ney, 2003). The above search is implemented in the following way: 1. All source word and target word pairs are considered to be initial phrase pairs. 715 2. If the phrase translation probability of the phrase pair is less than the threshold, it is deleted. 3. Each phrase pair is expanded toward the eight neighboring directions as shown in Figure 3. 4. If the phrase translation probability of the expanded phrase pair is less than the threshold, it </context>
</contexts>
<marker>Vogel, Zhang, Huang, Tribble, Venugopal, Zhao, Waibel, 2003</marker>
<rawString>Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble, Ashish Venugopal, Bing Zhao, and Alex Waibel. 2003. The CMU statistical machine translation system. In Proceedings ofMT Summit IX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Reordering constraints for phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of 20th International Conference on Computational Linguistics (COLING-04),</booktitle>
<pages>205--211</pages>
<contexts>
<context position="10568" citStr="Zens et al., 2004" startWordPosition="1683" endWordPosition="1686">xpansion and deletion is repeated until no further expansion is possible. 6. The consistent N-best phrase alignment are searched from all combinations of the above phrase pairs. The search for consistent Viterbi phrase alignments can be implemented as a phrase-based decoder using a beam search whose outputs are constrained only to the target sentence. The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al., 2002). We did not use any reordering constraints, such as IBM constraint and ITG constraint in the search for the N-best phrase alignment (Zens et al., 2004). The thresholds used in the search are the following: the minimum phrase translation probability is 0.0001. The maximum number of translation candidates for each phrase is 20. The beam width is 1e-10, the stack size (for each target candidate word length) is 1000. We found that, compared with the decoding of sentence translation, we have to search significantly larger space for the N-best phrase alignment. Figure 3 shows an example of phrase pair expansion toward eight neighbors. If the current phrase pair is ( , of), the expanded phrase pairs are ( , means of), ( , means of), ( , means of), </context>
</contexts>
<marker>Zens, Ney, Watanabe, Sumita, 2004</marker>
<rawString>Richard Zens, Hermann Ney, Taro Watanabe, and Eiichiro Sumita. 2004. Reordering constraints for phrase-based statistical machine translation. In Proceedings of 20th International Conference on Computational Linguistics (COLING-04), pages 205– 211.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>