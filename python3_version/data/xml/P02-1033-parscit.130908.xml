<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001829">
<note confidence="0.9044625">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 255-262.
</note>
<bodyText confidence="0.999927037735849">
second, achieving sense tagging using that same
sense inventory for the second language, thus
creating a sense-tagged corpus and automati-
cally making a connection to the first language&apos;s
sense inventory. In this paper we focus primarily
ort the first goal.
The crux of this research is the observation
that translations can serve as a source of sense
distinctions (Brown et al., 1991; Dagan, 1991;
Dagan and Itai, 1994; Dyvik, 1998; Ide, 2000;
Resnik and Yarowsky, 1999). A word that has
multiple senses in one language is often trans-
lated as distinct words in another language, with
the particular choice depending ort the transla-
tor and the contextualized meaning; thus the
corresponding translation can be thought of as
a sense indicator for the instance of the word in
its context. Looking at parallel translations, it
becomes evident that two factors are at play. On
the one hand, instances of a word/sense combi-
nation are translated with some consistency into
a relatively small handful of words in the second
language. On the other hand, that handful of
words is rarely a singleton set evert for a single
word/sense, because the preferences of different
translators and the demands of context produce
semantically similar words that differ in their
nuances.
For example, in a French-English parallel cor-
pus, the French word catastrophe could be found
in correspondence to English disaster in one in-
stance, and to tragedy in another. Each of those
English words is itself ambiguous e.g., tragedy
can refer to a kind of play (as opposed to com-
edy) but we can take advantage of the fact
that both English word instances appeared in
correspondence with catastrophe to infer that
they share some common element of meaning,
and we can use that inference in deciding which
of the English senses was intended. Having done
so, we can go further: we can project the English
word sense chosen for this instance of tragedy
to the French word catastrophe in this context,
thus tagging the two languages in tandem with
a single sense inventory.
The remainder of this paper is organized as
follows. Section 2 describes the approach. Sec-
tion 3 lays out evaluation experiments, using
SENSEVAL-2 data, showing the results of several
different variations of the approach and com-
paring performance with other SENSEVAL-2 sys-
tems. Section 4 contains discussion and we con-
clude in Section 5.
</bodyText>
<sectionHeader confidence="0.967715" genericHeader="abstract">
2 Approach
</sectionHeader>
<bodyText confidence="0.999605076923077">
For the sake of exposition, let us assume that
we are working with art English-French parallel
corpus and that we are using art English sense
inventory.&apos; Although there is no necessary as-
sumption of directionality in translation, we will
sometimes refer to the English language cor-
pus as the target corpus and the French corpus
as the source corpus, which corresponds to the
characterization, in the previous section, of the
French word (catastrophe) being translated into
two different words (disaster and tragedy) in two
diferent contexts. The process we described can
be viewed more abstractly as follows:
</bodyText>
<listItem confidence="0.998812">
1. Identify words in the target (English) cor-
pus and their corresponding translations in
the source (French) corpus.
2. Group the words of the target language
</listItem>
<bodyText confidence="0.770932333333333">
forming target sets that were trans-
lated into the same orthographic form in
the source corpus.
</bodyText>
<listItem confidence="0.650399">
3. Within each of these target sets, consider
all the possible sense tags for each word and
select sense tags informed by semantic sim-
ilarity with the other words in the group.
4. Project the sense tags from the target side
to the source side of the parallel corpus.
</listItem>
<bodyText confidence="0.999428222222222">
The first step of the process assumes a
sentence- or segment-aligned parallel corpus;
suitable data are now available for many lan-
guages via organizations such LDC and ELRA
and the Web is a promising source of data in
new language pairs and in new genres (Nie et
al., 1999; Resnik, 1999a). After identifying and
tokenizing sentences, we obtain word-level align-
ments for the parallel corpus using the GIZA++
</bodyText>
<footnote confidence="0.909552666666667">
1The method has little dependence on language; in
our evaluation section we report on work using English-
French and English-Spanish.
</footnote>
<bodyText confidence="0.975731696078432">
implementation of the IBM statistical MT mod-
els (Och and Ney, 2000). For each French word
instance f, we collect the word instance e with
which it is aligned. Positions of the word in-
stances are recorded so that in later stages we
can project the eventual semantic annotation ort
e to f. For example, the alignment of The ac-
cident was a tragedy with L &apos;accident etait une
catastrophe might associate these two instances
of catastrophe and tragedy.
lit the second step, we collect for each word
type F the set of all English word types with
which it is aligned anywhere in the corpus, which
we call the target set for F. For example, the
target set for French catastrophe might con-
tain English word types disaster, tragedy, and
situation, the last of these arising because some
translator chose to render la catastrophe in En-
glish as the awful situation. In extracting cor-
respondences we take advantage of WordNet to
identify English nominal compounds in order to
help reduce the number of ambiguous terms in
the target set.2 For example, without nomi-
nal compound identification on the English side,
the target set for French abeille will contain bee,
winch is ambiguous (SPELLING-BEE VS. INSECT).
With compound identification, the target set
for abeille still contains bee, but it is also rich
in unambiguous terms like alkali_bee, honey_bee,
and gueen_bee. In the semantic similarity com-
putation, the presence of these monosemous
words provides strong reinforcement for the IN-
SECT sense of bee. Moreover, it enables us to
tag instances of bee with their more specific
compound-noun senses when they appear within
a compound that is known to the sense inven-
tory.
lit the third step, the target set is treated
as a problem of monolingual sense disam-
biguation with respect to the target-language
sense inventory. Consider the target set
{disaster, tragedy, situation}: to the human
reader, the juxtaposition of these words within
a single set automatically brings certain senses
2We used a small set of compound-matching rules con-
sidering a window of two tokens to the right and left, and
also used the &amp;quot;satellite&amp;quot; annotations in SENSEVAL data as
part of our preprocessing.
to the foreground. The same intuitive idea is ex-
ploited by Resnik&apos;s (1999b) algorithm for disam-
biguating groups of related nouns, which we ap-
ply here. For a target set {el, ..., en}, the algo-
rithm considers each pair of words (e,, ei)(j
and identifies which senses of the two words are
most similar semantically. Those senses are then
reinforced by an amount corresponding to that
degree of similarity.3 After comparison across
all pairs, each word sense s,j, of word e, ends up
having associated with it a confidence c(s,j,) E
[0, 1] that reflects how much reinforcement sense
s,j, received based on the other words in the set.
In our example, the KIND-OF-DRAMA sense of
tragedy would have received little support from
the senses of the other two words in the set;
on the other hand, the CALAMITY sense would
have been reinforced and therefore would receive
higher confidence.
At the end of the third step, we highlight the
significance of variability in translation: since
the method relies on semantic similarities be-
tween multiple items in a target set, the tar-
get set must contain at least two members. If
throughout the parallel corpus the translator al-
ways chose to translate the French word catas-
trophe to tragedy, the target set for catastrophe
will contain only a single element. Our algo-
rithm will have no basis for assigning reinforce-
ment differently to different senses, and as a re-
sult, none of these instances of tragedy the
ones corresponding to catastrophe will be
tagged.
At this point we take advantage of the book-
keeping information recorded earlier. We know
which instances of tragedy are associated with
the target set {disaster, tragedy, situation} , and
so those instances can be labeled with the most
confident sense (CALAMITY) or, for that mat-
ter, with the confidence distribution over all pos-
sible senses as determined by the noun-group
disambiguation algorithm.
In the fourth and final step, we take advan-
tage of the English-side tagging and the word-
level alignment to project the sense tags on
3Since we use WordNet as our sense inventory, we
also adopt the information-theoretic measure of semantic
similarity based on that taxonomy.
English to the corresponding words in French.
For example, the tagging The accident was a
tragedy/cALAmnv would yield L&apos;accident etait
une catastrophe/CALAMITY. As a result, a large
number of French words will receive tags from
the English sense inventory.
</bodyText>
<sectionHeader confidence="0.995753" genericHeader="keywords">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999986333333333">
In order to provide a useful formal evaluation of
this approach for English sense disambiguation,
there were three requirements. We needed:
</bodyText>
<listItem confidence="0.98300175">
â€¢ a parallel corpus with English on one side,
large enough to train stochastic translation
models,
â€¢ gold-standard sense tags on the English side
for some subset of the corpus,
â€¢ performance figures for other systems on
the same subset, in order to compare re-
sults.
</listItem>
<bodyText confidence="0.9990887">
Meeting all three requirements simultaneously
presented something of a challenge. There are
a few human-tagged English corpora available
for word sense disambiguation, but most are rel-
atively small by model-training standards and
none have associated translations in other lan-
guages. Conversely, there are some parallel cor-
pora large enough for training alignment mod-
els, but to our knowledge none of these have
been even partially sense tagged.
</bodyText>
<subsectionHeader confidence="0.998262">
3.1 Corpora and Sense Inventory
</subsectionHeader>
<bodyText confidence="0.999941615384615">
To solve this problem, we adopted a &amp;quot;pseudo-
translation&amp;quot; approach (Diab, 2000). A suitably
large English corpus is constructed, containing
as a subset an English corpus for which we have
art existing set of associated gold-standard sense
tags. The entire corpus, including the sub-
set, is translated using commercial MT tech-
nology, producing an artificial parallel corpus.
This corpus is then used as described in Sec-
tion 2, and the quality of sense tagging on the
English gold-standard subset is assessed using
community-wide evaluation standards, with re-
sults suitable for inter-system comparison with
</bodyText>
<table confidence="0.884266166666667">
Corpus Tokens Lines
BC-SV1 2498405 101841
SV2-AW 5815 242
SV2-LS 1760522 74552
WSJ 1290297 49679
Total 5555039 226314
</table>
<tableCaption confidence="0.999079">
Table 1: Sizes of corpora used in experiments
</tableCaption>
<bodyText confidence="0.978236586666667">
other algorithms that have been tested ort the
same data.
The pseudo-translation approach has advan-
tages and disadvantages. On the one hand,
using commercial MT systems does not neces-
sarily result in performance figures representing
what could be obtained with better quality hu-
man translations. On the other hand, a pseudo-
translated corpus is far easier to produce, and
this approach to evaluation allows for controlled
experimentation using English paired with mul-
tiple languages.
We used the the English &amp;quot;all words&amp;quot; portion
of the SENSEVAL-2 test data (henceforth 5V2-
AW) as our gold-standard English subset. The
corpus comprises three documents from the Wall
Street Journal, totaling 242 lines with 5826 to-
kens in all. To fill out this English-side cor-
pus, we added the raw unannotated texts of the
Brown Corpus (BC) (Francis and KuCera, 1982),
the SENSEVAL-1 Corpus (SV1), the SENSEVAL-
2 English Lexical Sample test, trial and train-
ing corpora (5V2-LS), and Wall Street Jour-
nal (WSJ) sections 18-24 from the Penn Tree-
bank. We will refer to this unwieldy merged
corpus with the unwieldy but informative label
BCSV1SV2WSJ. Table 1 shows the sizes of the
component corpora.
Two different commercially available MT sys-
tems were used for the pseudo-translations:
Globalink Pro 6.4 (GL) and Systran Profes-
sional Premium (SYS). The motivation behind
using two MT systems stems from a desire to
more closely approximate the variability of hu-
man translation in a very large corpus, where
one translator would be unlikely to have per-
formed the entire task, and to help offset the
possible tendency of any single MT system to
be unnaturally consistent in its lexical selection.
The English BCSV1SV2WSJ was translated
into French and Spanish, resulting in four par-
allel corpora: BCSV1SV2WSJ paired with the
French GL translation (yielding parallel corpus
FRGL), with French SYS translation (FRSYS),
with Spanish GL (SPGL), and with Spanish
SYS (SPSYS).4
Each of the four parallel corpora just de-
scribed (FRGL, FRSYS, SPGL, SPSYS) repre-
sents a separate experimental variant. Consis-
tent with Diab (2000), we added one more vari-
ant for each language in order to more closely
approach the variability associated with multi-
ple translations: in Step 2 we combined the tar-
get sets from the two MT systems. For example,
if the word types shore, bank are in the target
set of orilla in SPGL, and coast, bank, and shore
are in the target set for orilla in SPSYS, the
union of the target sets is taken and the result
is a merged target set for orilla containing {bank,
coast, shore}. These last two variations are la-
beled MFRGLSYS and MSPGLSYS.
We restricted our experiments to disambigua-
tion of nouns, for which there were 1071 in-
stances in 5V2-AW not marked &amp;quot;unassignable&amp;quot;
by SENSEVAL&apos;S human annotators. Nouns were
identified on the basis of human-assigned part-
of-speech tags where available (BC, WSJ and
5V2-AW) and using the Brill tagger elsewhere
(Brill, 1993). The choice of 5V2-AW as our
gold standard corpus determined our choice of
sense inventory: SENSEVAL-2 produced a gold
standard for the English &amp;quot;all words&amp;quot; task using
a pre-release of WordNet 1.7 (Fellbaum, 1998),
and we restricted our attention to the noun tax-
onomy.
</bodyText>
<subsectionHeader confidence="0.99787">
3.2 Sense Selection Criterion
</subsectionHeader>
<bodyText confidence="0.998242">
Because the algorithm for disambiguating noun
groupings returns a confidence value for every
sense of a word, some threshold or other crite-
rion is needed to decide which sense or senses
to actually assign. We simply assign the sense
</bodyText>
<footnote confidence="0.936312">
4The choice of languages was partly a question of
available software for reasonably high quality translation,
and partly motivated by the longer-term aim of perform-
ing evaluation of sense tags propagated back into the
source languages via comparison with EuroWordNet.
</footnote>
<table confidence="0.999898428571428">
Variant Precision Recall
FRG&apos;: 58.1 50.9
FRSYS 58.0 49.0
MFRGLSYS 59.4 54.5
SPGL 57.9 48.6
SPSYS 60.0 51.5
MSPGLSYS 59.4 53.3
</table>
<tableCaption confidence="0.999875">
Table 2: Results on SENSEVAL-2 nouns (Vo)
</tableCaption>
<bodyText confidence="0.999906071428572">
tag that scored the maximum confidence level,
or all such tags, equally weighted, if there is a
tie. (The SENSEVAL evaluation measures allow
for partial credit.)
This criterion is fairly sensitive to noise in tar-
get sets; for example, in a real corpus the French
catastrophe is aligned with English {catastrophe,
disaster, shocker, tragedy}. Shocker is art outlier
in this set and its presence affects the overall
confidence score assignment for all the words in
the set. We observed that this is similar to what
happens when the French word underlying the
target set is homonymous; such cases are part
of our discussion in Section 4.
</bodyText>
<subsectionHeader confidence="0.628121">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.997792823529412">
We evaluated the algorithm&apos;s performance us-
ing the standard SENSEVAL-2 evaluation soft-
ware, obtaining figures for precision and recall
for sense tagging the nouns in our gold standard.
In this evaluation, partial credit is given in cases
where a system assigns multiple sense tags.5 We
report results using the &amp;quot;fine-grained&amp;quot; scoring
variant; this is the strictest variant, which some-
times requires systems to discern among Word-
Net senses that even linguists have a difficult
time distinguishing.
Table 2 summarizes the results, and Figure 1
shows our algorithm&apos;s results (triangles) com-
pared to the performance of the 21 SENSEVAL-2
English All Words participants, when the eval-
uation is restricted to the same set of noun test
instances.6 Hollow circles represent supervised
</bodyText>
<footnote confidence="0.984762">
6The scorer2 program, disseminated by Rada Mihal-
cea in conjunction with the SENSEVAL-2 exercise, imple-
ments a version of Melamed and Resnik&apos;s (2000) frame-
work for tagger evaluation given hierarchical tag sets. For
discussion see Kilgarriff and Rosenzweig (2000).
6We computed results for other systems on our only-
nouns subset of the task by subsetting those systems&apos;
</footnote>
<figure confidence="0.9853526875">
90
80
70 o
o
60 n e
eilk, â€¢
. 50 â€¢ â€¢ â€¢
a;
40
w
30 421,
20 â€¢
10
0 â€¢ â€¢
0 20 4060 80 100
Precision
</figure>
<figureCaption confidence="0.999982">
Figure 1: Comparison with SENSEVAL-2 systems
</figureCaption>
<bodyText confidence="0.9897336">
systems and filled circles represent unsupervised
systems.&apos; Of the systems that are unsupervised,
and can therefore be included in a fair compari-
son, only one is clearly better ort both precision
and recall.
</bodyText>
<sectionHeader confidence="0.998055" genericHeader="introduction">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999991">
The results show that the performance of our
approach is comparable or superior to most
other unsupervised systems, even though it is
based on cross-language lexical correspondences,
a radically different source of evidence, and even
though those correspondences were derived from
machine translations rather than clean human
translations. Here we briefly consider issues that
bear on recall and precision, respectively.
</bodyText>
<subsectionHeader confidence="0.994586">
4.1 Considerations Affecting Recall
</subsectionHeader>
<bodyText confidence="0.999673666666667">
Some of the sentences in the test corpus could
not be automatically aligned because our aligner
discards sentence pairs that are longer than a
</bodyText>
<footnote confidence="0.939566454545454">
answers from the full all-words task, which are avail-
able at http://www.cis.upenn.eduh,cottonisensevali
answers+misc.tgz. A scatterplot of the results
for the 21 systems including all parts of speech
appears at http://www.sle.sharp.co.ukisenseva12/
Results/all_graphs.htm.
&apos;We classified systems based on their descrip-
tions at http://www.cogs.susx.ac.ukilabinlpi mc-
carthy/SEVALsystems.html. Per the SENSEVAL
reporting guidelines, we do not identify individual
systems.
</footnote>
<bodyText confidence="0.998235083333333">
pre-defined limit. For these sentences, therefore,
no attempt could be made at disambiguation.
Future experiments will attempt to increase the
acceptable sentence length, as limited by real
memory, and to break longer sentence pairs into
logical sub-parts for alignment.
A second issue that affects recall is the lack
of variability in pseudo-translations. Of the
English nouns that are aligned with source-
language words, approximately 35% are always
aligned with the same word, rendering them un-
taggable using an approach based ort semantic
similarity within target sets. Some cases may
reflect preserved ambiguity in the language pair
e.g. French interet and English interest are
ambiguous in similar ways and others may
simply reflect the fact that commercial MT sys-
tems are just not very creative or context sensi-
tive in their lexical choices. It should be possible
to increase variability by extending the corpus
to include human-translated parallel text, or by
combining evidence from multiple or more dis-
tantly related source languages in the spirit of
Resnik and Yarowsky (1999).
</bodyText>
<subsectionHeader confidence="0.998201">
4.2 Considerations Affecting Precision
</subsectionHeader>
<bodyText confidence="0.953619780487805">
On inspecting the target sets qualitatively, we
find that they contain many outliers, largely
owing to noisy alignment. The problem wors-
ens when the outliers are monosemous, since a
monosemous word with a misleading sense will
erroneously bias the sense tag assignment for
the other target set words. For example, the
word types adolescence, idol, teen, and teenager
form a target set for the French source word
adolescence, and the presence of idol has a neg-
ative impact ort the sense assignment for the
other members of the set. In addition, seman-
tically distant words can align with the same
source word; e.g., amorce in French may align
with initiation, bait, and cap, which are all cor-
rect translations in suitable contexts but provide
Ito suitable basis for semantic reinforcement.
These problems reflect the algorithm&apos;s im-
plicit assumption that the source words are
monosemous, reflected in its attempt to have ev-
ery word in a target set influence the semantics
of every other word. Inspecting the data pro-
duces many counterexamples, e.g. French canon
(cannon, cannonball, canon, theologian) bandes
(band, gang, mob, strip, streak, tape), and baie
(bay, berry, cove).
A sensible alternative would be apply auto-
matic clustering techniques to the target sets
(e.g. (Diab and Finch, 2000; Schiitze, 1992)),
providing target sub-clusters of words that
should be treated as related, with no cross-
cluster reinforcement. For example, the tar-
get set for French canon would have two co-
herent sub-clusters containing {cannon, can-
nonball} and {canon, theologian)}, respectively.
Manual inspection of target sets in our exper-
iments suggests that when target sets are se-
mantically coherent e.g. adversaires (antag-
onists, opponents, contestants), accident: (acci-
dent, crash, wreck) sense assignment is gen-
erally highly accurate.
</bodyText>
<sectionHeader confidence="0.995372" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999482">
This paper presents art unsupervised approach
to word sense disambiguation that exploits
translations as a proxy for semantic annotation
across languages. The observation behind the
approach, that words having the same trans-
lation often share some dimension of meaning,
leads to art algorithm in which the correct sense
of a word is reinforced by the semantic similar-
ity of other words with which it shares those
dimensions of meaning.
Performance using this algorithm has been
rigorously evaluated and is comparable with
other unsupervised WSD systems, based ori fair
comparison using community-wide test data.
Because it achieves this performance using cross-
language data alone, it is likely that improved
results can be obtained by also taking advan-
tage of monolingual contextual evidence. Al-
though in the end all unsupervised systems are
likely to produce precision results inferior to the
best supervised algorithms, they are often more
practical to apply in a broad-vocabulary setting.
Moreover, noisy annotations can serve as seeds
both for monolingual supervised methods and
for bootstrapping cross-linguistic sense disam-
biguation and sense inventories, complementing
other research ori the complex problem of map-
ping sense tags cross linguistically (e.g. (Alonge
et al., 1998; Rodriguez et al., 1998; Vossen et
al., 1999)).
</bodyText>
<sectionHeader confidence="0.995269" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9933915">
This work has been supported, in part, by ONR MiTRI
Contract FCP0.810548265, NSA RD-02-5700, and
DARPA/ITO Cooperative Agreement N660010028910.
The authors would like to thank the anonymous review-
ers for their comments, Rebecca Hwa and Okan Kolak
for helpful assistance and discussion, Franz Josef Och for
his help with GIZA++, Adwait Ratnaparkhi for the use
of MXTERMINATOR, and our collaborators at Johns
Hopkins for the use of their computing facilities in parts
of this work.
</bodyText>
<sectionHeader confidence="0.98869" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.973744906976744">
R. Agirre, L. Padro, and J. Atserias. 2000. Combin-
ing supervised and unsupervised lexical knowledge
methods for word sense disambiguation. Comput-
ers and the Humanities: Special issue on SENSE-
VAL, 34:103-108.
A. Alonge, N. Calzolari, P. Vossen, L. Loksma,
I. Casrellon, M. A. Marti, and W. Peters. 1998.
The linguistic design of the eurowordnet database.
Computers and the Humanities: Special issue on
Euro WordNet, 32(2-3).
Eric Brill. 1993. A Corpus-Based Approach to
Language Learning. Ph.D. thesis, Computer and
Information Science, University of Pennsylvania,
June.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1991. A statistical approach to
sense disambiguation in machine translation. In
Proc. of the Speech and Natural Language Work-
shop, pages 146-151, Pacific Grove, CA.
Rebecca Bruce and Janyce Wiebe. 1994. A new
approach to sense identification. In ARPA Work-
shop on human Language Technology, Plainsboro,
NJ, March.
Ido Dagan and Alon Itai. 1994. Word sense dis-
ambiguation using a second language monolingual
corpus.
Ido Dagan. 1991. Lexical disambiguation: sources
of information and their statistical realization. In
Proceedings of the 29th Annual Meeting of the
Association for Computational Linguistics, June.
Berkeley, California.
Mona Diab and Steven Finch. 2000. A statistical
word level translation model for comparable cor-
pora. In Proceedings of Conference on Content
based multimedia information Access RI40&apos;00,
Paris, France. Content Based Multimedia Infor-
mation Access.
Mona Diab. 2000. An unsupervised method for mul-
tilingual word sense tagging using parallel corpora:
A preliminary investigation. In SIGLEX2000:
Word Senses and Multi-linguality, Hong Kong,
October.
Helge Dyvik. 1998. Translations as semantic mir-
rors. In Proceedings of Workshop W13: Multilin-
guality in the lexicon II, pages 24-44, Brighton,
UK. The 13th biennial European Conference on
Artificial Intelligence ECAI 98.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
W. Francis and H. Kueera. 1982. Frequency Analy-
sis of English Usage. Houghton Mifflin Co.: New
York.
Nancy Ide. 2000. Cross-lingual sense determination:
Can it work? Computers and the Humanities:
Special issue on SENSE VAL, 34:223-234.
Adam Kilgarriff and Joseph Rosenzweig. 2000.
Framework and results for english SENSEVAL.
Computers and the Humanities: Special issue on
SENSE VAL, 34:15-48.
Dekang Lin. 1999. A case-base algorithm for word
sense disambiguation. In Proceedings of Confer-
ence Pacific Association for Computational Lin-
guistics, Waterloo, Canada. Pacific Association for
Computational Linguistics.
Dekang Lin. 2000. Word sense disambiguation with
a similarity based smoothed library. Computers
and the Humanities: Special issue on SENSEVAL,
34:147-152.
K. Litkowski. 2000. SENSEVAL: The el-research ex-
perience. Computers and the Humanities: Special
issue on SENSE VAL, 34:153-158.
I. Dan Melamed and Philip Resnik. 2000. Evalua-
tion of sense disambiguation given hierarchical tag
sets. Computers and the Humanities, 34(1-2).
J. Nie, P. Isabelle, M. Simard, and R. Durand. 1999.
Cross-language information retrieval based on par-
allel texts and automatic mining of parallel texts
from the web. In Proceedings of ACM-SIGIR con-
ference, pages 74-81, Berkeley, CA.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of
37th Annual Meeting of the Association for Com-
putational Linguistics (ACL &apos;00), Hong Kong, Oc-
tober.
Philip Resnik and David Yarowsky. 1999. Distin-
guishing systems and distinguishing senses: New
evaluation methods for word sense disambigua-
tion. Natural Language Engineering, 5(2):113-
133.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In ANLP Workshop on Tagging
Text with Lexical Semantics, Washington, D.C.,
April.
Philip Resnik. 1999a. Mining the Web for bilingual
text. In 37th Annual Meeting of the Association
for Computational Linguistics (ACL &apos;99), College
Park, Maryland, June.
Philip Resnik. 1999b. Semantic similarity in a tax-
onomy: An information-based measure and its ap-
plication to problems of ambiguity in natural lan-
guage. Journal of Artificial Intelligence Research
11:95-130.
H. Rodriguez, S. Climent, P. Vossen, L. Loksma,
W. Peters, A. Alonge, F. Bertagna, and A. Hoven-
tini. 1998. The top-down strategy for building
eurowordnet: Vocabulary coverage, base concepts
and top ontology. Computers and the Humanities:
Special issue on EuroWordNet, 32(2-3).
Hinrich Schiitze. 1992. Dimensions of meaning. In
Proceedings of Supercomputing &apos;92.
Pick Vossen, Wim Peters, and Julio Gonzalo. 1999.
Towards a universal index of meaning. In Proceed-
ings of the ACL SIGLEX workshop, Maryland,
MD, USA.
David Yarowsky. 1992. Word-sense disambigua-
tion using statistical models of Roget&apos;s categories
trained on large corpora. In Proceedings of the
14th International Conference on Computational
Linguistics (COLING-92), pages 454-460, Nantes,
France, July.
David Yarowsky. 1993. One sense per collocation.
ARPA Workshop on Human Language Technol-
ogy, March. Princeton.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics, pages
189-196, Cambridge, MA. Association for Com-
putational Linguistics.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.920881">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 255-262.</note>
<abstract confidence="0.988995462598426">second, achieving sense tagging using that same sense inventory for the second language, thus creating a sense-tagged corpus and automatically making a connection to the first language&apos;s sense inventory. In this paper we focus primarily ort the first goal. The crux of this research is the observation that translations can serve as a source of sense distinctions (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Dyvik, 1998; Ide, 2000; Resnik and Yarowsky, 1999). A word that has multiple senses in one language is often translated as distinct words in another language, with the particular choice depending ort the translator and the contextualized meaning; thus the corresponding translation can be thought of as a sense indicator for the instance of the word in its context. Looking at parallel translations, it becomes evident that two factors are at play. On the one hand, instances of a word/sense combination are translated with some consistency into a relatively small handful of words in the second language. On the other hand, that handful of words is rarely a singleton set evert for a single word/sense, because the preferences of different translators and the demands of context produce semantically similar words that differ in their nuances. For example, in a French-English parallel corthe French word be found correspondence to English one inand to another. Each of those words is itself ambiguous e.g., tragedy refer to a kind of play (as opposed to comwe can take advantage of the fact that both English word instances appeared in with infer that they share some common element of meaning, and we can use that inference in deciding which of the English senses was intended. Having done so, we can go further: we can project the English word sense chosen for this instance of tragedy the French word this context, thus tagging the two languages in tandem with a single sense inventory. The remainder of this paper is organized as follows. Section 2 describes the approach. Section 3 lays out evaluation experiments, using showing the results of several different variations of the approach and comperformance with other systems. Section 4 contains discussion and we conclude in Section 5. For the sake of exposition, let us assume that we are working with art English-French parallel corpus and that we are using art English sense inventory.&apos; Although there is no necessary assumption of directionality in translation, we will sometimes refer to the English language corpus as the target corpus and the French corpus as the source corpus, which corresponds to the characterization, in the previous section, of the word translated into different words two diferent contexts. The process we described can be viewed more abstractly as follows: 1. Identify words in the target (English) corpus and their corresponding translations in the source (French) corpus. 2. Group the words of the target language target sets that were lated into the same orthographic form in the source corpus. 3. Within each of these target sets, consider all the possible sense tags for each word and select sense tags informed by semantic similarity with the other words in the group. 4. Project the sense tags from the target side to the source side of the parallel corpus. The first step of the process assumes a sentenceor segment-aligned parallel corpus; suitable data are now available for many languages via organizations such LDC and ELRA and the Web is a promising source of data in new language pairs and in new genres (Nie et al., 1999; Resnik, 1999a). After identifying and tokenizing sentences, we obtain word-level alignments for the parallel corpus using the GIZA++ method has little dependence on language; in our evaluation section we report on work using English- French and English-Spanish. implementation of the IBM statistical MT models (Och and Ney, 2000). For each French word collect the word instance e with which it is aligned. Positions of the word instances are recorded so that in later stages we can project the eventual semantic annotation ort example, the alignment of accident was a tragedy with L &apos;accident etait une associate these two instances lit the second step, we collect for each word set of word types with which it is aligned anywhere in the corpus, which call the set example, the set for French con- English word types tragedy, last of these arising because some chose to render catastrophe Enas awful situation. extracting correspondences we take advantage of WordNet to identify English nominal compounds in order to help reduce the number of ambiguous terms in target For example, without nominal compound identification on the English side, target set for French contain ambiguous VS. INSECT). With compound identification, the target set contains it is also rich unambiguous terms like honey_bee, the semantic similarity computation, the presence of these monosemous provides strong reinforcement for the INof it enables us to instances of their more specific compound-noun senses when they appear within a compound that is known to the sense inventory. lit the third step, the target set is treated as a problem of monolingual sense disambiguation with respect to the target-language sense inventory. Consider the target set tragedy, situation}: the human reader, the juxtaposition of these words within a single set automatically brings certain senses used a small set of compound-matching rules considering a window of two tokens to the right and left, and used the &amp;quot;satellite&amp;quot; annotations in as part of our preprocessing. to the foreground. The same intuitive idea is exploited by Resnik&apos;s (1999b) algorithm for disambiguating groups of related nouns, which we aphere. For a target set ..., the algoconsiders each pair of words and identifies which senses of the two words are most similar semantically. Those senses are then reinforced by an amount corresponding to that of After comparison across pairs, each word sense word up associated with it a confidence 1] reflects how much reinforcement sense based on the other words in the set. our example, the of have received little support from the senses of the other two words in the set; the other hand, the would have been reinforced and therefore would receive higher confidence. At the end of the third step, we highlight the significance of variability in translation: since the method relies on semantic similarities between multiple items in a target set, the target set must contain at least two members. If the parallel corpus the translator alto translate the French word catasto tragedy, target set for will contain only a single element. Our algorithm will have no basis for assigning reinforcedifferently to different senses, and as a renone of these instances of corresponding to be tagged. At this point we take advantage of the bookkeeping information recorded earlier. We know instances of associated with target set tragedy, situation} , so those instances can be labeled with the most sense for that ter, with the confidence distribution over all possible senses as determined by the noun-group disambiguation algorithm. In the fourth and final step, we take advantage of the English-side tagging and the wordlevel alignment to project the sense tags on we use WordNet as our sense inventory, we also adopt the information-theoretic measure of semantic similarity based on that taxonomy. English to the corresponding words in French. example, the tagging accident was a would yield etait catastrophe/CALAMITY. a result, a large number of French words will receive tags from the English sense inventory. 3 Evaluation In order to provide a useful formal evaluation of this approach for English sense disambiguation, there were three requirements. We needed: â€¢ a parallel corpus with English on one side, large enough to train stochastic translation models, â€¢ gold-standard sense tags on the English side for some subset of the corpus, â€¢ performance figures for other systems on the same subset, in order to compare results. Meeting all three requirements simultaneously presented something of a challenge. There are a few human-tagged English corpora available for word sense disambiguation, but most are relatively small by model-training standards and none have associated translations in other languages. Conversely, there are some parallel corpora large enough for training alignment models, but to our knowledge none of these have been even partially sense tagged. 3.1 Corpora and Sense Inventory To solve this problem, we adopted a &amp;quot;pseudoapproach (Diab, suitably large English corpus is constructed, containing as a subset an English corpus for which we have art existing set of associated gold-standard sense tags. The entire corpus, including the subset, is translated using commercial MT technology, producing an artificial parallel corpus. This corpus is then used as described in Section 2, and the quality of sense tagging on the English gold-standard subset is assessed using community-wide evaluation standards, with results suitable for inter-system comparison with Corpus Tokens Lines BC-SV1 2498405 101841 SV2-AW 5815 242 SV2-LS 1760522 74552 WSJ 1290297 49679 Total 5555039 226314 Table 1: Sizes of corpora used in experiments other algorithms that have been tested ort the same data. The pseudo-translation approach has advantages and disadvantages. On the one hand, using commercial MT systems does not necessarily result in performance figures representing what could be obtained with better quality human translations. On the other hand, a pseudotranslated corpus is far easier to produce, and this approach to evaluation allows for controlled experimentation using English paired with multiple languages. We used the the English &amp;quot;all words&amp;quot; portion the data (henceforth 5V2- AW) as our gold-standard English subset. The corpus comprises three documents from the Wall Street Journal, totaling 242 lines with 5826 tokens in all. To fill out this English-side corpus, we added the raw unannotated texts of the Brown Corpus (BC) (Francis and KuCera, 1982), Corpus the SENSEVAL- 2 English Lexical Sample test, trial and training corpora (5V2-LS), and Wall Street Journal (WSJ) sections 18-24 from the Penn Treebank. We will refer to this unwieldy merged corpus with the unwieldy but informative label BCSV1SV2WSJ. Table 1 shows the sizes of the component corpora. Two different commercially available MT systems were used for the pseudo-translations: Globalink Pro 6.4 (GL) and Systran Professional Premium (SYS). The motivation behind using two MT systems stems from a desire to more closely approximate the variability of human translation in a very large corpus, where one translator would be unlikely to have performed the entire task, and to help offset the possible tendency of any single MT system to be unnaturally consistent in its lexical selection. The English BCSV1SV2WSJ was translated into French and Spanish, resulting in four parallel corpora: BCSV1SV2WSJ paired with the French GL translation (yielding parallel corpus FRGL), with French SYS translation (FRSYS), with Spanish GL (SPGL), and with Spanish Each of the four parallel corpora just described (FRGL, FRSYS, SPGL, SPSYS) represents a separate experimental variant. Consiswith Diab added one more variant for each language in order to more closely approach the variability associated with multiple translations: in Step 2 we combined the target sets from the two MT systems. For example, the word types bank in the target of SPGL, and bank, in the target set for SPSYS, the union of the target sets is taken and the result a merged target set for shore}. last two variations are labeled MFRGLSYS and MSPGLSYS. We restricted our experiments to disambiguation of nouns, for which there were 1071 instances in 5V2-AW not marked &amp;quot;unassignable&amp;quot; annotators. Nouns were identified on the basis of human-assigned partof-speech tags where available (BC, WSJ and 5V2-AW) and using the Brill tagger elsewhere (Brill, 1993). The choice of 5V2-AW as our gold standard corpus determined our choice of inventory: a gold standard for the English &amp;quot;all words&amp;quot; task using a pre-release of WordNet 1.7 (Fellbaum, 1998), and we restricted our attention to the noun taxonomy. 3.2 Sense Selection Criterion Because the algorithm for disambiguating noun groupings returns a confidence value for every sense of a word, some threshold or other criterion is needed to decide which sense or senses to actually assign. We simply assign the sense choice of languages was partly a question of available software for reasonably high quality translation, and partly motivated by the longer-term aim of performing evaluation of sense tags propagated back into the source languages via comparison with EuroWordNet. Variant Precision Recall FRG&apos;: 58.1 50.9 FRSYS 58.0 49.0 MFRGLSYS 59.4 54.5 SPGL 57.9 48.6 SPSYS 60.0 51.5 MSPGLSYS 59.4 53.3 2: Results on (Vo) tag that scored the maximum confidence level, or all such tags, equally weighted, if there is a (The measures allow for partial credit.) This criterion is fairly sensitive to noise in target sets; for example, in a real corpus the French aligned with English shocker, tragedy}. Shocker art outlier in this set and its presence affects the overall confidence score assignment for all the words in the set. We observed that this is similar to what happens when the French word underlying the target set is homonymous; such cases are part of our discussion in Section 4. 3.3 Results We evaluated the algorithm&apos;s performance usthe standard software, obtaining figures for precision and recall for sense tagging the nouns in our gold standard. In this evaluation, partial credit is given in cases a system assigns multiple sense We report results using the &amp;quot;fine-grained&amp;quot; scoring variant; this is the strictest variant, which sometimes requires systems to discern among Word- Net senses that even linguists have a difficult time distinguishing. Table 2 summarizes the results, and Figure 1 shows our algorithm&apos;s results (triangles) comto the performance of the 21 English All Words participants, when the evaluation is restricted to the same set of noun test Hollow circles represent supervised disseminated by Rada Mihalin conjunction with the implements a version of Melamed and Resnik&apos;s (2000) framework for tagger evaluation given hierarchical tag sets. For discussion see Kilgarriff and Rosenzweig (2000). computed results for other systems on our onlynouns subset of the task by subsetting those systems&apos; 90 80 70 o o 60 n e eilk, â€¢ . 50 a; â€¢ â€¢ â€¢ 40 w 30 421, 20 â€¢ 10 0 â€¢ â€¢ 0 20 Precision 80 100 1: Comparison with systems and filled circles represent unsupervised systems.&apos; Of the systems that are unsupervised, and can therefore be included in a fair comparison, only one is clearly better ort both precision and recall. The results show that the performance of our approach is comparable or superior to most other unsupervised systems, even though it is based on cross-language lexical correspondences, a radically different source of evidence, and even though those correspondences were derived from machine translations rather than clean human translations. Here we briefly consider issues that bear on recall and precision, respectively. 4.1 Considerations Affecting Recall Some of the sentences in the test corpus could not be automatically aligned because our aligner discards sentence pairs that are longer than a answers from the full all-words task, which are availat answers+misc.tgz. A scatterplot of the results for the 21 systems including all parts of speech appears at http://www.sle.sharp.co.ukisenseva12/ Results/all_graphs.htm. systems based on their descripat http://www.cogs.susx.ac.ukilabinlpi mc- Per the SENSEVAL reporting guidelines, we do not identify individual systems. pre-defined limit. For these sentences, therefore, no attempt could be made at disambiguation. Future experiments will attempt to increase the acceptable sentence length, as limited by real memory, and to break longer sentence pairs into logical sub-parts for alignment. A second issue that affects recall is the lack of variability in pseudo-translations. Of the English nouns that are aligned with sourcelanguage words, approximately 35% are always with the rendering them untaggable using an approach based ort semantic similarity within target sets. Some cases may reflect preserved ambiguity in the language pair e.g. French English ambiguous in similar ways and others may simply reflect the fact that commercial MT systems are just not very creative or context sensitive in their lexical choices. It should be possible to increase variability by extending the corpus to include human-translated parallel text, or by combining evidence from multiple or more distantly related source languages in the spirit of Resnik and Yarowsky (1999). 4.2 Considerations Affecting Precision On inspecting the target sets qualitatively, we find that they contain many outliers, largely owing to noisy alignment. The problem worsens when the outliers are monosemous, since a monosemous word with a misleading sense will erroneously bias the sense tag assignment for the other target set words. For example, the types idol, teen, form a target set for the French source word the presence of a negative impact ort the sense assignment for the other members of the set. In addition, semantically distant words can align with the same word; e.g., French may align bait, are all correct translations in suitable contexts but provide Ito suitable basis for semantic reinforcement. These problems reflect the algorithm&apos;s implicit assumption that the source words are monosemous, reflected in its attempt to have every word in a target set influence the semantics every other word. Inspecting the data promany counterexamples, e.g. French (cannon, cannonball, canon, theologian) bandes gang, mob, strip, streak, tape), (bay, berry, cove). A sensible alternative would be apply automatic clustering techniques to the target sets (Diab and Finch, 1992)), providing target sub-clusters of words that should be treated as related, with no crosscluster reinforcement. For example, the tarset for French have two cosub-clusters containing cantheologian)}, Manual inspection of target sets in our expersuggests that when target sets are secoherent e.g. opponents, contestants), accident: (accicrash, wreck) assignment is erally highly accurate. 5 Conclusions This paper presents art unsupervised approach to word sense disambiguation that exploits translations as a proxy for semantic annotation across languages. The observation behind the approach, that words having the same translation often share some dimension of meaning, leads to art algorithm in which the correct sense of a word is reinforced by the semantic similarity of other words with which it shares those dimensions of meaning. Performance using this algorithm has been rigorously evaluated and is comparable with other unsupervised WSD systems, based ori fair comparison using community-wide test data. Because it achieves this performance using crosslanguage data alone, it is likely that improved results can be obtained by also taking advantage of monolingual contextual evidence. Although in the end all unsupervised systems are likely to produce precision results inferior to the best supervised algorithms, they are often more practical to apply in a broad-vocabulary setting. Moreover, noisy annotations can serve as seeds both for monolingual supervised methods and for bootstrapping cross-linguistic sense disambiguation and sense inventories, complementing other research ori the complex problem of mapping sense tags cross linguistically (e.g. (Alonge et al., 1998; Rodriguez et al., 1998; Vossen et al., 1999)).</abstract>
<note confidence="0.8937612">Acknowledgments This work has been supported, in part, by ONR MiTRI Contract FCP0.810548265, NSA RD-02-5700, and DARPA/ITO Cooperative Agreement N660010028910. The authors would like to thank the anonymous review-</note>
<abstract confidence="0.834761166666667">ers for their comments, Rebecca Hwa and Okan Kolak for helpful assistance and discussion, Franz Josef Och for his help with GIZA++, Adwait Ratnaparkhi for the use of MXTERMINATOR, and our collaborators at Johns Hopkins for the use of their computing facilities in parts of this work.</abstract>
<note confidence="0.919644714285714">References R. Agirre, L. Padro, and J. Atserias. 2000. Combining supervised and unsupervised lexical knowledge for word sense disambiguation. Computers and the Humanities: Special issue on SENSE- VAL, 34:103-108. A. Alonge, N. Calzolari, P. Vossen, L. Loksma, I. Casrellon, M. A. Marti, and W. Peters. 1998. The linguistic design of the eurowordnet database. Computers and the Humanities: Special issue on WordNet, Brill. 1993. A Approach to Learning. thesis, Computer and Information Science, University of Pennsylvania,</note>
<author confidence="0.719248">P F Brown</author>
<author confidence="0.719248">S A Della Pietra</author>
<author confidence="0.719248">V J Della Pietra</author>
<author confidence="0.719248">R L Mercer</author>
<affiliation confidence="0.9119075">sense disambiguation in machine translation. In Proc. of the Speech and Natural Language Work-</affiliation>
<address confidence="0.95754">146-151, Pacific Grove, CA.</address>
<author confidence="0.8711775">A new to sense identification In Work-</author>
<affiliation confidence="0.777581">on human Language Technology,</affiliation>
<address confidence="0.908636">NJ, March.</address>
<abstract confidence="0.908915666666667">Ido Dagan and Alon Itai. 1994. Word sense disambiguation using a second language monolingual corpus.</abstract>
<note confidence="0.539829333333333">Ido Dagan. 1991. Lexical disambiguation: sources of information and their statistical realization. In Proceedings of the 29th Annual Meeting of the</note>
<affiliation confidence="0.796966">for Computational Linguistics,</affiliation>
<address confidence="0.972829">Berkeley, California.</address>
<author confidence="0.73938">A statistical</author>
<abstract confidence="0.855840090909091">word level translation model for comparable cor- In of Conference on Content based multimedia information Access RI40&apos;00, Paris, France. Content Based Multimedia Information Access. Mona Diab. 2000. An unsupervised method for multilingual word sense tagging using parallel corpora: preliminary investigation. In Senses and Multi-linguality, Kong, October. Helge Dyvik. 1998. Translations as semantic mir-</abstract>
<note confidence="0.887561142857143">In of Workshop W13: Multilinin the lexicon II, 24-44, Brighton, UK. The 13th biennial European Conference on Artificial Intelligence ECAI 98. Fellbaum, editor. 1998. Lexical Database. Press. Francis and H. Kueera. 1982. Analyof English Usage. Mifflin Co.: New York. Nancy Ide. 2000. Cross-lingual sense determination: it work? and the Humanities: issue on SENSE 34:223-234. Adam Kilgarriff and Joseph Rosenzweig. 2000. Framework and results for english SENSEVAL. Computers and the Humanities: Special issue on 34:15-48. Dekang Lin. 1999. A case-base algorithm for word disambiguation. In of Conference Pacific Association for Computational Lin- Canada. Pacific Association for Computational Linguistics.</note>
<abstract confidence="0.965063">Dekang Lin. 2000. Word sense disambiguation with similarity based smoothed library. and the Humanities: Special issue on SENSEVAL, 34:147-152. K. Litkowski. 2000. SENSEVAL: The el-research exand the Humanities: Special on SENSE 34:153-158. I. Dan Melamed and Philip Resnik. 2000. Evaluation of sense disambiguation given hierarchical tag and the Humanities, J. Nie, P. Isabelle, M. Simard, and R. Durand. 1999. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts the web. In of ACM-SIGIR con-</abstract>
<address confidence="0.600828">74-81, Berkeley, CA.</address>
<note confidence="0.838446">Franz Josef Och and Hermann Ney. 2000. Improved alignment models. In of 37th Annual Meeting of the Association for Com- Linguistics (ACL &apos;00), Kong, Oc-</note>
<abstract confidence="0.7999012">tober. Philip Resnik and David Yarowsky. 1999. Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambigua- Language Engineering, 133. Philip Resnik. 1997. Selectional preference and sense In Workshop on Tagging with Lexical Semantics, D.C., April.</abstract>
<author confidence="0.728907">a Mining the Web for bilingual</author>
<affiliation confidence="0.77333">In Annual Meeting of the Association Computational Linguistics (ACL &apos;99),</affiliation>
<address confidence="0.796623">Park, Maryland, June.</address>
<author confidence="0.608911">b Semantic similarity in a tax-</author>
<abstract confidence="0.8637547">onomy: An information-based measure and its application to problems of ambiguity in natural lanof Artificial Intelligence Research 11:95-130. H. Rodriguez, S. Climent, P. Vossen, L. Loksma, W. Peters, A. Alonge, F. Bertagna, and A. Hoventini. 1998. The top-down strategy for building eurowordnet: Vocabulary coverage, base concepts top ontology. and the Humanities: issue on EuroWordNet,</abstract>
<note confidence="0.8336094">Hinrich Schiitze. 1992. Dimensions of meaning. In Proceedings of Supercomputing &apos;92. Pick Vossen, Wim Peters, and Julio Gonzalo. 1999. a universal index of meaning. In Proceedof the ACL SIGLEX workshop,</note>
<address confidence="0.908097">MD, USA.</address>
<author confidence="0.584323">Word-sense disambigua-</author>
<abstract confidence="0.7360635">tion using statistical models of Roget&apos;s categories on large corpora. In of the</abstract>
<note confidence="0.876789583333333">14th International Conference on Computational (COLING-92), 454-460, Nantes, France, July. David Yarowsky. 1993. One sense per collocation. ARPA Workshop on Human Language Technology, March. Princeton. David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the for Computational Linguistics, 189-196, Cambridge, MA. Association for Computational Linguistics.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Agirre</author>
<author>L Padro</author>
<author>J Atserias</author>
</authors>
<title>Combining supervised and unsupervised lexical knowledge methods for word sense disambiguation. Computers and the Humanities: Special issue on SENSEVAL,</title>
<date>2000</date>
<pages>34--103</pages>
<marker>Agirre, Padro, Atserias, 2000</marker>
<rawString>R. Agirre, L. Padro, and J. Atserias. 2000. Combining supervised and unsupervised lexical knowledge methods for word sense disambiguation. Computers and the Humanities: Special issue on SENSEVAL, 34:103-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Alonge</author>
<author>N Calzolari</author>
<author>P Vossen</author>
<author>L Loksma</author>
<author>I Casrellon</author>
<author>M A Marti</author>
<author>W Peters</author>
</authors>
<date>1998</date>
<booktitle>The linguistic design of the eurowordnet database. Computers and the Humanities: Special issue on Euro WordNet,</booktitle>
<pages>32--2</pages>
<marker>Alonge, Calzolari, Vossen, Loksma, Casrellon, Marti, Peters, 1998</marker>
<rawString>A. Alonge, N. Calzolari, P. Vossen, L. Loksma, I. Casrellon, M. A. Marti, and W. Peters. 1998. The linguistic design of the eurowordnet database. Computers and the Humanities: Special issue on Euro WordNet, 32(2-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A Corpus-Based Approach to Language Learning.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer and Information Science, University of Pennsylvania,</institution>
<contexts>
<context position="13393" citStr="Brill, 1993" startWordPosition="2197" endWordPosition="2198">re in the target set of orilla in SPGL, and coast, bank, and shore are in the target set for orilla in SPSYS, the union of the target sets is taken and the result is a merged target set for orilla containing {bank, coast, shore}. These last two variations are labeled MFRGLSYS and MSPGLSYS. We restricted our experiments to disambiguation of nouns, for which there were 1071 instances in 5V2-AW not marked &amp;quot;unassignable&amp;quot; by SENSEVAL&apos;S human annotators. Nouns were identified on the basis of human-assigned partof-speech tags where available (BC, WSJ and 5V2-AW) and using the Brill tagger elsewhere (Brill, 1993). The choice of 5V2-AW as our gold standard corpus determined our choice of sense inventory: SENSEVAL-2 produced a gold standard for the English &amp;quot;all words&amp;quot; task using a pre-release of WordNet 1.7 (Fellbaum, 1998), and we restricted our attention to the noun taxonomy. 3.2 Sense Selection Criterion Because the algorithm for disambiguating noun groupings returns a confidence value for every sense of a word, some threshold or other criterion is needed to decide which sense or senses to actually assign. We simply assign the sense 4The choice of languages was partly a question of available software</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Eric Brill. 1993. A Corpus-Based Approach to Language Learning. Ph.D. thesis, Computer and Information Science, University of Pennsylvania, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>A statistical approach to sense disambiguation in machine translation.</title>
<date>1991</date>
<booktitle>In Proc. of the Speech and Natural Language Workshop,</booktitle>
<pages>146--151</pages>
<location>Pacific Grove, CA.</location>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1991. A statistical approach to sense disambiguation in machine translation. In Proc. of the Speech and Natural Language Workshop, pages 146-151, Pacific Grove, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Bruce</author>
<author>Janyce Wiebe</author>
</authors>
<title>A new approach to sense identification.</title>
<date>1994</date>
<booktitle>In ARPA Workshop on human Language Technology,</booktitle>
<location>Plainsboro, NJ,</location>
<marker>Bruce, Wiebe, 1994</marker>
<rawString>Rebecca Bruce and Janyce Wiebe. 1994. A new approach to sense identification. In ARPA Workshop on human Language Technology, Plainsboro, NJ, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
</authors>
<title>Word sense disambiguation using a second language monolingual corpus.</title>
<date>1994</date>
<marker>Dagan, Itai, 1994</marker>
<rawString>Ido Dagan and Alon Itai. 1994. Word sense disambiguation using a second language monolingual corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
</authors>
<title>Lexical disambiguation: sources of information and their statistical realization.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Berkeley, California.</location>
<marker>Dagan, 1991</marker>
<rawString>Ido Dagan. 1991. Lexical disambiguation: sources of information and their statistical realization. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, June. Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Steven Finch</author>
</authors>
<title>A statistical word level translation model for comparable corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of Conference on Content based multimedia information Access RI40&apos;00,</booktitle>
<location>Paris,</location>
<contexts>
<context position="20000" citStr="Diab and Finch, 2000" startWordPosition="3232" endWordPosition="3235">, which are all correct translations in suitable contexts but provide Ito suitable basis for semantic reinforcement. These problems reflect the algorithm&apos;s implicit assumption that the source words are monosemous, reflected in its attempt to have every word in a target set influence the semantics of every other word. Inspecting the data produces many counterexamples, e.g. French canon (cannon, cannonball, canon, theologian) bandes (band, gang, mob, strip, streak, tape), and baie (bay, berry, cove). A sensible alternative would be apply automatic clustering techniques to the target sets (e.g. (Diab and Finch, 2000; Schiitze, 1992)), providing target sub-clusters of words that should be treated as related, with no crosscluster reinforcement. For example, the target set for French canon would have two coherent sub-clusters containing {cannon, cannonball} and {canon, theologian)}, respectively. Manual inspection of target sets in our experiments suggests that when target sets are semantically coherent e.g. adversaires (antagonists, opponents, contestants), accident: (accident, crash, wreck) sense assignment is generally highly accurate. 5 Conclusions This paper presents art unsupervised approach to word s</context>
</contexts>
<marker>Diab, Finch, 2000</marker>
<rawString>Mona Diab and Steven Finch. 2000. A statistical word level translation model for comparable corpora. In Proceedings of Conference on Content based multimedia information Access RI40&apos;00, Paris, France. Content Based Multimedia Information Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
</authors>
<title>An unsupervised method for multilingual word sense tagging using parallel corpora: A preliminary investigation.</title>
<date>2000</date>
<booktitle>In SIGLEX2000: Word Senses and Multi-linguality,</booktitle>
<location>Hong Kong,</location>
<contexts>
<context position="9756" citStr="Diab, 2000" startWordPosition="1606" endWordPosition="1607">ystems on the same subset, in order to compare results. Meeting all three requirements simultaneously presented something of a challenge. There are a few human-tagged English corpora available for word sense disambiguation, but most are relatively small by model-training standards and none have associated translations in other languages. Conversely, there are some parallel corpora large enough for training alignment models, but to our knowledge none of these have been even partially sense tagged. 3.1 Corpora and Sense Inventory To solve this problem, we adopted a &amp;quot;pseudotranslation&amp;quot; approach (Diab, 2000). A suitably large English corpus is constructed, containing as a subset an English corpus for which we have art existing set of associated gold-standard sense tags. The entire corpus, including the subset, is translated using commercial MT technology, producing an artificial parallel corpus. This corpus is then used as described in Section 2, and the quality of sense tagging on the English gold-standard subset is assessed using community-wide evaluation standards, with results suitable for inter-system comparison with Corpus Tokens Lines BC-SV1 2498405 101841 SV2-AW 5815 242 SV2-LS 1760522 74</context>
<context position="12539" citStr="Diab (2000)" startWordPosition="2048" endWordPosition="2049"> one translator would be unlikely to have performed the entire task, and to help offset the possible tendency of any single MT system to be unnaturally consistent in its lexical selection. The English BCSV1SV2WSJ was translated into French and Spanish, resulting in four parallel corpora: BCSV1SV2WSJ paired with the French GL translation (yielding parallel corpus FRGL), with French SYS translation (FRSYS), with Spanish GL (SPGL), and with Spanish SYS (SPSYS).4 Each of the four parallel corpora just described (FRGL, FRSYS, SPGL, SPSYS) represents a separate experimental variant. Consistent with Diab (2000), we added one more variant for each language in order to more closely approach the variability associated with multiple translations: in Step 2 we combined the target sets from the two MT systems. For example, if the word types shore, bank are in the target set of orilla in SPGL, and coast, bank, and shore are in the target set for orilla in SPSYS, the union of the target sets is taken and the result is a merged target set for orilla containing {bank, coast, shore}. These last two variations are labeled MFRGLSYS and MSPGLSYS. We restricted our experiments to disambiguation of nouns, for which</context>
</contexts>
<marker>Diab, 2000</marker>
<rawString>Mona Diab. 2000. An unsupervised method for multilingual word sense tagging using parallel corpora: A preliminary investigation. In SIGLEX2000: Word Senses and Multi-linguality, Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helge Dyvik</author>
</authors>
<title>Translations as semantic mirrors.</title>
<date>1998</date>
<booktitle>In Proceedings of Workshop W13: Multilinguality in the lexicon II,</booktitle>
<pages>24--44</pages>
<location>Brighton, UK.</location>
<marker>Dyvik, 1998</marker>
<rawString>Helge Dyvik. 1998. Translations as semantic mirrors. In Proceedings of Workshop W13: Multilinguality in the lexicon II, pages 24-44, Brighton, UK. The 13th biennial European Conference on Artificial Intelligence ECAI 98.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Francis</author>
<author>H Kueera</author>
</authors>
<title>Frequency Analysis of English Usage.</title>
<date>1982</date>
<location>Houghton Mifflin Co.: New York.</location>
<marker>Francis, Kueera, 1982</marker>
<rawString>W. Francis and H. Kueera. 1982. Frequency Analysis of English Usage. Houghton Mifflin Co.: New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
</authors>
<title>Cross-lingual sense determination: Can it work?</title>
<date>2000</date>
<journal>Computers and the Humanities: Special issue on SENSE VAL,</journal>
<pages>34--223</pages>
<marker>Ide, 2000</marker>
<rawString>Nancy Ide. 2000. Cross-lingual sense determination: Can it work? Computers and the Humanities: Special issue on SENSE VAL, 34:223-234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>Framework and results for english SENSEVAL.</title>
<date>2000</date>
<journal>Computers and the Humanities: Special issue on SENSE VAL,</journal>
<pages>34--15</pages>
<contexts>
<context position="16045" citStr="Kilgarriff and Rosenzweig (2000)" startWordPosition="2617" endWordPosition="2620">s to discern among WordNet senses that even linguists have a difficult time distinguishing. Table 2 summarizes the results, and Figure 1 shows our algorithm&apos;s results (triangles) compared to the performance of the 21 SENSEVAL-2 English All Words participants, when the evaluation is restricted to the same set of noun test instances.6 Hollow circles represent supervised 6The scorer2 program, disseminated by Rada Mihalcea in conjunction with the SENSEVAL-2 exercise, implements a version of Melamed and Resnik&apos;s (2000) framework for tagger evaluation given hierarchical tag sets. For discussion see Kilgarriff and Rosenzweig (2000). 6We computed results for other systems on our onlynouns subset of the task by subsetting those systems&apos; 90 80 70 o o 60 n e eilk, â€¢ . 50 â€¢ â€¢ â€¢ a; 40 w 30 421, 20 â€¢ 10 0 â€¢ â€¢ 0 20 4060 80 100 Precision Figure 1: Comparison with SENSEVAL-2 systems systems and filled circles represent unsupervised systems.&apos; Of the systems that are unsupervised, and can therefore be included in a fair comparison, only one is clearly better ort both precision and recall. 4 Discussion The results show that the performance of our approach is comparable or superior to most other unsupervised systems, even though it i</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>Adam Kilgarriff and Joseph Rosenzweig. 2000. Framework and results for english SENSEVAL. Computers and the Humanities: Special issue on SENSE VAL, 34:15-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A case-base algorithm for word sense disambiguation.</title>
<date>1999</date>
<booktitle>In Proceedings of Conference Pacific Association for Computational Linguistics,</booktitle>
<location>Waterloo, Canada. Pacific</location>
<marker>Lin, 1999</marker>
<rawString>Dekang Lin. 1999. A case-base algorithm for word sense disambiguation. In Proceedings of Conference Pacific Association for Computational Linguistics, Waterloo, Canada. Pacific Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Word sense disambiguation with a similarity based smoothed library.</title>
<date>2000</date>
<journal>Computers and the Humanities: Special issue on SENSEVAL,</journal>
<pages>34--147</pages>
<marker>Lin, 2000</marker>
<rawString>Dekang Lin. 2000. Word sense disambiguation with a similarity based smoothed library. Computers and the Humanities: Special issue on SENSEVAL, 34:147-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Litkowski</author>
</authors>
<title>SENSEVAL: The el-research experience.</title>
<date>2000</date>
<booktitle>Computers and the Humanities: Special issue on SENSE VAL,</booktitle>
<pages>34--153</pages>
<marker>Litkowski, 2000</marker>
<rawString>K. Litkowski. 2000. SENSEVAL: The el-research experience. Computers and the Humanities: Special issue on SENSE VAL, 34:153-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Philip Resnik</author>
</authors>
<title>Evaluation of sense disambiguation given hierarchical tag sets. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--1</pages>
<marker>Melamed, Resnik, 2000</marker>
<rawString>I. Dan Melamed and Philip Resnik. 2000. Evaluation of sense disambiguation given hierarchical tag sets. Computers and the Humanities, 34(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nie</author>
<author>P Isabelle</author>
<author>M Simard</author>
<author>R Durand</author>
</authors>
<title>Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the web.</title>
<date>1999</date>
<booktitle>In Proceedings of ACM-SIGIR conference,</booktitle>
<pages>74--81</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="3922" citStr="Nie et al., 1999" startWordPosition="647" endWordPosition="650">s that were translated into the same orthographic form in the source corpus. 3. Within each of these target sets, consider all the possible sense tags for each word and select sense tags informed by semantic similarity with the other words in the group. 4. Project the sense tags from the target side to the source side of the parallel corpus. The first step of the process assumes a sentence- or segment-aligned parallel corpus; suitable data are now available for many languages via organizations such LDC and ELRA and the Web is a promising source of data in new language pairs and in new genres (Nie et al., 1999; Resnik, 1999a). After identifying and tokenizing sentences, we obtain word-level alignments for the parallel corpus using the GIZA++ 1The method has little dependence on language; in our evaluation section we report on work using EnglishFrench and English-Spanish. implementation of the IBM statistical MT models (Och and Ney, 2000). For each French word instance f, we collect the word instance e with which it is aligned. Positions of the word instances are recorded so that in later stages we can project the eventual semantic annotation ort e to f. For example, the alignment of The accident wa</context>
</contexts>
<marker>Nie, Isabelle, Simard, Durand, 1999</marker>
<rawString>J. Nie, P. Isabelle, M. Simard, and R. Durand. 1999. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the web. In Proceedings of ACM-SIGIR conference, pages 74-81, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of 37th Annual Meeting of the Association for Computational Linguistics (ACL &apos;00),</booktitle>
<location>Hong Kong,</location>
<contexts>
<context position="4256" citStr="Och and Ney, 2000" startWordPosition="698" endWordPosition="701">lel corpus. The first step of the process assumes a sentence- or segment-aligned parallel corpus; suitable data are now available for many languages via organizations such LDC and ELRA and the Web is a promising source of data in new language pairs and in new genres (Nie et al., 1999; Resnik, 1999a). After identifying and tokenizing sentences, we obtain word-level alignments for the parallel corpus using the GIZA++ 1The method has little dependence on language; in our evaluation section we report on work using EnglishFrench and English-Spanish. implementation of the IBM statistical MT models (Och and Ney, 2000). For each French word instance f, we collect the word instance e with which it is aligned. Positions of the word instances are recorded so that in later stages we can project the eventual semantic annotation ort e to f. For example, the alignment of The accident was a tragedy with L &apos;accident etait une catastrophe might associate these two instances of catastrophe and tragedy. lit the second step, we collect for each word type F the set of all English word types with which it is aligned anywhere in the corpus, which we call the target set for F. For example, the target set for French catastro</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of 37th Annual Meeting of the Association for Computational Linguistics (ACL &apos;00), Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>David Yarowsky</author>
</authors>
<title>Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<pages>5--2</pages>
<contexts>
<context position="18668" citStr="Resnik and Yarowsky (1999)" startWordPosition="3019" endWordPosition="3022">ith the same word, rendering them untaggable using an approach based ort semantic similarity within target sets. Some cases may reflect preserved ambiguity in the language pair e.g. French interet and English interest are ambiguous in similar ways and others may simply reflect the fact that commercial MT systems are just not very creative or context sensitive in their lexical choices. It should be possible to increase variability by extending the corpus to include human-translated parallel text, or by combining evidence from multiple or more distantly related source languages in the spirit of Resnik and Yarowsky (1999). 4.2 Considerations Affecting Precision On inspecting the target sets qualitatively, we find that they contain many outliers, largely owing to noisy alignment. The problem worsens when the outliers are monosemous, since a monosemous word with a misleading sense will erroneously bias the sense tag assignment for the other target set words. For example, the word types adolescence, idol, teen, and teenager form a target set for the French source word adolescence, and the presence of idol has a negative impact ort the sense assignment for the other members of the set. In addition, semantically di</context>
</contexts>
<marker>Resnik, Yarowsky, 1999</marker>
<rawString>Philip Resnik and David Yarowsky. 1999. Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation. Natural Language Engineering, 5(2):113-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In ANLP Workshop on Tagging Text with Lexical Semantics,</booktitle>
<location>Washington, D.C.,</location>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In ANLP Workshop on Tagging Text with Lexical Semantics, Washington, D.C., April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Mining the Web for bilingual text.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association for Computational Linguistics (ACL &apos;99),</booktitle>
<location>College Park, Maryland,</location>
<contexts>
<context position="3936" citStr="Resnik, 1999" startWordPosition="651" endWordPosition="652">ated into the same orthographic form in the source corpus. 3. Within each of these target sets, consider all the possible sense tags for each word and select sense tags informed by semantic similarity with the other words in the group. 4. Project the sense tags from the target side to the source side of the parallel corpus. The first step of the process assumes a sentence- or segment-aligned parallel corpus; suitable data are now available for many languages via organizations such LDC and ELRA and the Web is a promising source of data in new language pairs and in new genres (Nie et al., 1999; Resnik, 1999a). After identifying and tokenizing sentences, we obtain word-level alignments for the parallel corpus using the GIZA++ 1The method has little dependence on language; in our evaluation section we report on work using EnglishFrench and English-Spanish. implementation of the IBM statistical MT models (Och and Ney, 2000). For each French word instance f, we collect the word instance e with which it is aligned. Positions of the word instances are recorded so that in later stages we can project the eventual semantic annotation ort e to f. For example, the alignment of The accident was a tragedy wi</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999a. Mining the Web for bilingual text. In 37th Annual Meeting of the Association for Computational Linguistics (ACL &apos;99), College Park, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research</journal>
<pages>11--95</pages>
<contexts>
<context position="3936" citStr="Resnik, 1999" startWordPosition="651" endWordPosition="652">ated into the same orthographic form in the source corpus. 3. Within each of these target sets, consider all the possible sense tags for each word and select sense tags informed by semantic similarity with the other words in the group. 4. Project the sense tags from the target side to the source side of the parallel corpus. The first step of the process assumes a sentence- or segment-aligned parallel corpus; suitable data are now available for many languages via organizations such LDC and ELRA and the Web is a promising source of data in new language pairs and in new genres (Nie et al., 1999; Resnik, 1999a). After identifying and tokenizing sentences, we obtain word-level alignments for the parallel corpus using the GIZA++ 1The method has little dependence on language; in our evaluation section we report on work using EnglishFrench and English-Spanish. implementation of the IBM statistical MT models (Och and Ney, 2000). For each French word instance f, we collect the word instance e with which it is aligned. Positions of the word instances are recorded so that in later stages we can project the eventual semantic annotation ort e to f. For example, the alignment of The accident was a tragedy wi</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999b. Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. Journal of Artificial Intelligence Research 11:95-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rodriguez</author>
<author>S Climent</author>
<author>P Vossen</author>
<author>L Loksma</author>
<author>W Peters</author>
<author>A Alonge</author>
<author>F Bertagna</author>
<author>A Hoventini</author>
</authors>
<title>The top-down strategy for building eurowordnet: Vocabulary coverage, base concepts and top ontology. Computers and the Humanities: Special issue on</title>
<date>1998</date>
<booktitle>EuroWordNet,</booktitle>
<pages>32--2</pages>
<marker>Rodriguez, Climent, Vossen, Loksma, Peters, Alonge, Bertagna, Hoventini, 1998</marker>
<rawString>H. Rodriguez, S. Climent, P. Vossen, L. Loksma, W. Peters, A. Alonge, F. Bertagna, and A. Hoventini. 1998. The top-down strategy for building eurowordnet: Vocabulary coverage, base concepts and top ontology. Computers and the Humanities: Special issue on EuroWordNet, 32(2-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schiitze</author>
</authors>
<title>Dimensions of meaning.</title>
<date>1992</date>
<booktitle>In Proceedings of Supercomputing &apos;92.</booktitle>
<contexts>
<context position="20017" citStr="Schiitze, 1992" startWordPosition="3236" endWordPosition="3237">t translations in suitable contexts but provide Ito suitable basis for semantic reinforcement. These problems reflect the algorithm&apos;s implicit assumption that the source words are monosemous, reflected in its attempt to have every word in a target set influence the semantics of every other word. Inspecting the data produces many counterexamples, e.g. French canon (cannon, cannonball, canon, theologian) bandes (band, gang, mob, strip, streak, tape), and baie (bay, berry, cove). A sensible alternative would be apply automatic clustering techniques to the target sets (e.g. (Diab and Finch, 2000; Schiitze, 1992)), providing target sub-clusters of words that should be treated as related, with no crosscluster reinforcement. For example, the target set for French canon would have two coherent sub-clusters containing {cannon, cannonball} and {canon, theologian)}, respectively. Manual inspection of target sets in our experiments suggests that when target sets are semantically coherent e.g. adversaires (antagonists, opponents, contestants), accident: (accident, crash, wreck) sense assignment is generally highly accurate. 5 Conclusions This paper presents art unsupervised approach to word sense disambiguati</context>
</contexts>
<marker>Schiitze, 1992</marker>
<rawString>Hinrich Schiitze. 1992. Dimensions of meaning. In Proceedings of Supercomputing &apos;92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pick Vossen</author>
<author>Wim Peters</author>
<author>Julio Gonzalo</author>
</authors>
<title>Towards a universal index of meaning.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL SIGLEX workshop,</booktitle>
<location>Maryland, MD, USA.</location>
<marker>Vossen, Peters, Gonzalo, 1999</marker>
<rawString>Pick Vossen, Wim Peters, and Julio Gonzalo. 1999. Towards a universal index of meaning. In Proceedings of the ACL SIGLEX workshop, Maryland, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92),</booktitle>
<pages>454--460</pages>
<location>Nantes, France,</location>
<marker>Yarowsky, 1992</marker>
<rawString>David Yarowsky. 1992. Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora. In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), pages 454-460, Nantes, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>ARPA Workshop on Human Language Technology,</booktitle>
<location>March. Princeton.</location>
<marker>Yarowsky, 1993</marker>
<rawString>David Yarowsky. 1993. One sense per collocation. ARPA Workshop on Human Language Technology, March. Princeton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Cambridge, MA.</location>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189-196, Cambridge, MA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>