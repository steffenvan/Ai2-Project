<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.979542">
Syntactic Constraints on Paraphrases Extracted from Parallel Corpora
</title>
<author confidence="0.979405">
Chris Callison-Burch
</author>
<affiliation confidence="0.921876">
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.54445">
Baltimore, Maryland
</address>
<email confidence="0.269327">
ccb cs jhu edu
</email>
<sectionHeader confidence="0.977866" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999775166666667">
We improve the quality of paraphrases ex-
tracted from parallel corpora by requiring that
phrases and their paraphrases be the same syn-
tactic type. This is achieved by parsing the En-
glish side of a parallel corpus and altering the
phrase extraction algorithm to extract phrase
labels alongside bilingual phrase pairs. In or-
der to retain broad coverage of non-constituent
phrases, complex syntactic labels are intro-
duced. A manual evaluation indicates a 19%
absolute improvement in paraphrase quality
over the baseline method.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999804081081081">
Paraphrases are alternative ways of expressing the
same information. Being able to identify or gen-
erate paraphrases automatically is useful in a wide
range of natural language applications. Recent work
has shown how paraphrases can improve question
answering through query expansion (Riezler et al.,
2007), automatic evaluation of translation and sum-
marization by modeling alternative lexicalization
(Kauchak and Barzilay, 2006; Zhou et al., 2006;
Owczarzak et al., 2006), and machine translation
both by dealing with out of vocabulary words and
phrases (Callison-Burch et al., 2006) and by expand-
ing the set of reference translations for minimum er-
ror rate training (Madnani et al., 2007). While all ap-
plications require the preservation of meaning when
a phrase is replaced by its paraphrase, some addi-
tionally require the resulting sentence to be gram-
matical.
In this paper we examine the effectiveness of
placing syntactic constraints on a commonly used
paraphrasing technique that extracts paraphrases
from parallel corpora (Bannard and Callison-Burch,
2005). The paraphrasing technique employs various
aspects of phrase-based statistical machine transla-
tion including phrase extraction heuristics to obtain
bilingual phrase pairs from word alignments. En-
glish phrases are considered to be potential para-
phrases of each other if they share a common for-
eign language phrase among their translations. Mul-
tiple paraphrases are frequently extracted for each
phrase and can be ranked using a paraphrase proba-
bility based on phrase translation probabilities.
We find that the quality of the paraphrases that
are generated in this fashion improves significantly
when they are required to be the same syntactic type
as the phrase that they are paraphrasing. This con-
straint:
</bodyText>
<listItem confidence="0.9620114">
• Eliminates a trivial but pervasive error that
arises from the interaction of unaligned words
with phrase extraction heuristics.
• Refines the results for phrases that can take on
different syntactic labels.
• Applies both to phrases which are linguistically
coherent and to arbitrary sequences of words.
• Results in much more grammatical output
when phrases are replaced with their para-
phrases.
</listItem>
<bodyText confidence="0.9995285">
A thorough manual evaluation of the refined para-
phrasing technique finds a 19% absolute improve-
</bodyText>
<page confidence="0.984252">
196
</page>
<note confidence="0.9621425">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 196–205,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.99932725">
ment in the number of paraphrases that are judged
to be correct.
This paper is structured as follows: Section 2
describes related work in syntactic constraints on
phrase-based SMT and work utilizing syntax in
paraphrase discovery. Section 3 details the prob-
lems with extracting paraphrases from parallel cor-
pora and our improvements to the technique. Sec-
tion 4 describes our experimental design and evalu-
ation methodology. Section 5 gives the results of our
experiments, and Section 6 discusses their implica-
tions.
</bodyText>
<sectionHeader confidence="0.99966" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999984327272727">
A number of research efforts have focused on em-
ploying syntactic constraints in statistical machine
translation. Wu (1997) introduced the inversion
transduction grammar formalism which treats trans-
lation as a process of parallel parsing of the source
and target language via a synchronized grammar.
The synchronized grammar places constraints on
which words can be aligned across bilingual sen-
tence pairs. To achieve computational efficiency, the
original proposal used only a single non-terminal la-
bel rather than a linguistic grammar.
Subsequent work used more articulated parses
to improve alignment quality by applying cohesion
constraints (Fox, 2002; Lin and Cherry, 2002). If
two English phrases are in disjoint subtrees in the
parse, then the phrasal cohesion constraint prevents
them from being aligned to overlapping sequences
in the foreign sentence. Other recent work has incor-
porated constituent and dependency subtrees into the
translation rules used by phrase-based systems (Gal-
ley et al., 2004; Quirk et al., 2005). Phrase-based
rules have also been replaced with synchronous con-
text free grammars (Chiang, 2005) and with tree
fragments (Huang and Knight, 2006).
A number of techniques for generating para-
phrases have employed syntactic information, either
in the process of extracting paraphrases from mono-
lingual texts or in the extracted patterns themselves.
Lin and Pantel (2001) derived paraphrases based
on the distributional similarity of paths in depen-
dency trees. Barzilay and McKeown (2001) incor-
porated part-of-speech information and other mor-
phosyntactic clues into their co-training algorithm.
They extracted paraphrase patterns that incorporate
this information. Ibrahim et al. (2003) generated
structural paraphrases capable of capturing long-
distance dependencies. Pang et al. (2003) employed
a syntax-based algorithm to align equivalent English
sentences by merging corresponding nodes in parse
trees and compressing them down into a word lat-
tice.
Perhaps the most closely related work is a recent
extension to Bannard and Callison-Burch’s para-
phrasing method. Zhao et al. (2008b) extended the
method so that it is capable of generating richer
paraphrase patterns that include part-of-speech slots,
rather than simple lexical and phrasal paraphrases.
For example, they extracted patterns such as con-
sider NN → take NN into consideration. To ac-
complish this, Zhao el al. used dependency parses
on the English side of the parallel corpus. Their
work differs from the work presented in this paper
because their syntactic constraints applied to slots
within paraphrase patters, and our constraints apply
to the paraphrases themselves.
</bodyText>
<sectionHeader confidence="0.767474" genericHeader="method">
3 Paraphrasing with parallel corpora
</sectionHeader>
<bodyText confidence="0.9994614">
Bannard and Callison-Burch (2005) extract para-
phrases from bilingual parallel corpora. They give
a probabilistic formation of paraphrasing which nat-
urally falls out of the fact that they use techniques
from phrase-based statistical machine translation:
</bodyText>
<equation confidence="0.935708">
e2 = arg max p(e2|e1) (1)
e2:e2�e1
where
�p(e2|e1) = p(f|e1)p(e2|f, e1) (2)
f
E≈ p(f|e1)p(e2|f) (3)
f
</equation>
<bodyText confidence="0.997184">
Phrase translation probabilities p(f|e1) and p(e2|f)
are commonly calculated using maximum likelihood
estimation (Koehn et al., 2003):
</bodyText>
<equation confidence="0.991914333333333">
count(e, f)
p(f|e) = (4)
E f count(e, f)
</equation>
<bodyText confidence="0.9990875">
where the counts are collected by enumerating all
bilingual phrase pairs that are consistent with the
</bodyText>
<page confidence="0.997879">
197
</page>
<figureCaption confidence="0.978334">
Figure 1: The interaction of the phrase extraction heuristic with unaligned English words means that the Spanish
phrase la igualdad aligns with equal, create equal, and to create equal.
</figureCaption>
<figure confidence="0.993455545454545">
el
proyecto europeo no ha
the
european
conseguido
la igualdad
de oportunidades
project has failed
to create
opportunities.
equal
</figure>
<bodyText confidence="0.87754325">
word alignments for sentence pairs in a bilingual
parallel corpus. Various phrase extraction heuristics
are possible. Och and Ney (2004) defined consistent
bilingual phrase pairs as follows:
</bodyText>
<equation confidence="0.85615575">
J I j+m i+n
BP(f1 ,e1,A) _ {(fj Fei ) :
d(i0,j0) E A : j &lt; j0 &lt; j + m — i &lt; i0 &lt; i + n
∧EI(i0,j0) E A : j &lt; j0 &lt; j + m∧ — i &lt; i0 &lt; i + n}
</equation>
<table confidence="0.9522174">
equal
equal .35 equally .02
same .07 the .02
equality .03 fair .01
equals .02 equal rights .01
</table>
<tableCaption confidence="0.9908485">
Table 1: The baseline method’s paraphrases of equal and
their probabilities (excluding items with p &lt; .01).
</tableCaption>
<figure confidence="0.996396090909091">
create equal
create equal
equal
to create a
create
to create equality
.42 same .03
.06 created .02
.05 conditions .02
.04 playing .02
.03 creating .01
</figure>
<bodyText confidence="0.999497724137931">
where f1 is a foreign sentence, ei is an English sen-
tence and A is a set of word alignment points.
The heuristic allows unaligned words to be in-
cluded at the boundaries of the source or target lan-
guage phrases. For example, when enumerating the
consistent phrase pairs for the sentence pair given in
Figure 1, la igualdad would align not only to equal,
but also to create equal, and to create equal. In SMT
these alternative translations are ranked by the trans-
lation probabilities and other feature functions dur-
ing decoding.
The interaction between the phrase extraction
heuristic and unaligned words results in an unde-
sirable effect for paraphrasing. By Bannard and
Callison-Burch’s definition, equal, create equal, and
to create equal would be considered paraphrases be-
cause they are aligned to the same foreign phrase.
Tables 1 and 2 show how sub- and super-phrases can
creep into the paraphrases: equal can be paraphrased
as equal rights and create equal can be paraphrased
as equal. Obviously when e2 is substituted for e1 the
resulting sentence will generally be ungrammatical.
The first case could result in equal equal rights, and
the second would drop the verb.
This problem is pervasive. To test its extent we at-
tempted to generate paraphrases for 900,000 phrases
using Bannard and Callison-Burch’s method trained
on the Europarl corpora (as described in Section 4).
It generated a total of 3.7 million paraphrases for
</bodyText>
<tableCaption confidence="0.520704333333333">
Table 2: The baseline’s paraphrases of create equal. Most
are clearly bad, and the most probable e2 # e1 is a sub-
string of e1.
</tableCaption>
<bodyText confidence="0.998853714285714">
400,000 phrases in the list.1 We observed that 34%
of the paraphrases (excluding the phrase itself) were
super- or sub-strings of the original phrase. The
most probable paraphrase was a super- or sub-string
of the phrase 73% of the time.
There are a number of strategies that might be
adopted to alleviate this problem:
</bodyText>
<listItem confidence="0.934612833333333">
• Bannard and Callison-Burch (2005) rank their
paraphrases with a language model when the
paraphrases are substituted into a sentence.
• Bannard and Callison-Burch (2005) sum over
multiple parallel corpora C to reduce the prob-
lems associated with systematic errors in the
</listItem>
<footnote confidence="0.983729666666667">
1The remaining 500,000 phrases could not be paraphrased
either because e2 # e1 or because they were not consistently
aligned to any foreign phrases.
</footnote>
<page confidence="0.959452">
198
</page>
<equation confidence="0.9411915">
word alignments in one language pair:
p(f|e1)p(e2|f) (5)
</equation>
<bodyText confidence="0.901126428571429">
• We could change the phrase extraction heuris-
tic’s treatment of unaligned words, or we could
attempt to ensure that we have fewer unaligned
items in our word alignments.
• The paraphrase criterion could be changed
from being e2 =� e1 to specifying that e2 is not
sub- or super-string of e1.
In this paper we adopt a different strategy. The
essence of our strategy is to constrain paraphrases
to be the same syntactic type as the phrases that they
are paraphrasing. Syntactic constraints can apply in
two places: during phrase extraction and when sub-
stituting paraphrases into sentences. These are de-
scribed in sections 3.1 and 3.2.
</bodyText>
<subsectionHeader confidence="0.999215">
3.1 Syntactic constraints on phrase extraction
</subsectionHeader>
<bodyText confidence="0.999996333333333">
When we apply syntactic constraints to the phrase
extraction heuristic, we change how bilingual phrase
pairs are enumerated and how the component proba-
bilities of the paraphrase probability are calculated.
We use the syntactic type s of e1 in a refined ver-
sion of the paraphrase probability:
</bodyText>
<equation confidence="0.9947575">
ˆe2 = arg max p(e2|e1, s(e1)) (6)
e2:e2=,4e1ns(e2)=s(e1)
</equation>
<bodyText confidence="0.9934925">
where p(e2|e1, s(e1)) can be approximated as:
We define a new phrase extraction algorithm that op-
erates on an English parse tree P along with foreign
sentence f1 , English sentence ei, and word align-
ment A. We dub this SBP for syntactic bilingual
phrases:
</bodyText>
<equation confidence="0.5125425">
SBPeI A P) _ {(f�+� z i+n))
e+n s(e
(fJ1 , i, , , z , i :
d(i0,j0) E A : j &lt; j0 &lt; j + m — i &lt; i0 &lt; i + n
nE1(i0,j0) E A : j &lt; j0 &lt; j + mn — i &lt; i0 &lt; i + n
nE1 subtree E P with label s spanning words (i, i + n)}
</equation>
<table confidence="0.968013375">
equal
JJ equal .60 similar .02
same .14 equivalent .01
fair .02
ADJP equal .79 the same .01
necessary .02 equal in law .01
similar .02 equivalent .01
identical .02
</table>
<tableCaption confidence="0.9931765">
Table 3: Syntactically constrained paraphrases for equal
when it is labeled as an adjective or adjectival phrase.
</tableCaption>
<bodyText confidence="0.9998438">
The SBP phrase extraction algorithm produces tu-
ples containing a foreign phrase, an English phrase
and a syntactic label (f, e, s). After enumerating
these for all phrase pairs in a parallel corpus, we can
calculate p(f|e1, s(e1)) and p(e2|f, s(e1)) as:
</bodyText>
<equation confidence="0.999458666666667">
count(f, e1, s(e1))
p(f|e1, s(e1)) =
P f count(f, e1, s(e1))
count(f, e2, s(e1))
p(e2|f, s(e1)) =
Pe2 count(f, e2, s(e1))
</equation>
<bodyText confidence="0.999976115384616">
By redefining the probabilities in this way we parti-
tion the space of possible paraphrases by their syn-
tactic categories.
In order to enumerate all phrase pairs with their
syntactic labels we need to parse the English side of
the parallel corpus (but not the foreign side). This
limits the potential applicability of our refined para-
phrasing method to languages which have parsers.
Table 3 gives an example of the refined para-
phrases for equal when it occurs as an adjective or
adjectival phrase. Note that most of the paraphrases
that were possible under the baseline model (Table
1) are now excluded. We no longer get the noun
equality, the verb equals, the adverb equally, the de-
termier the or the NP equal rights. The paraphrases
seem to be higher quality, especially if one considers
their fidelity when they replace the original phrase in
the context of some sentence.
We tested the rate of paraphrases that were sub-
and super-strings when we constrain paraphrases
based on non-terminal nodes in parse trees. The
percent of the best paraphrases being substrings
dropped from 73% to 24%, and the overall percent
of paraphrases subsuming or being subsumed by the
original phrase dropped from 34% to 12%. How-
ever, the number of phrases for which we were able
</bodyText>
<equation confidence="0.997208">
ˆe2 = arg max
e2
X
f
X
cEC
X
cEC
P
f p(f|e1, s(e1))p(e2|f, s(e1))
(7)
|C|
</equation>
<page confidence="0.994649">
199
</page>
<figureCaption confidence="0.99471075">
Figure 2: In addition to extracting phrases that are domi-
nated by a node in the parse tree, we also generate labels
for non-syntactic constituents. Three labels are possible
for create equal.
</figureCaption>
<bodyText confidence="0.999889071428571">
to generated paraphrases dropped from 400,000 to
90,000, since we limited ourselves to phrases that
were valid syntactic constituents. The number of
unique paraphrases dropped from several million to
800,000.
The fact that we are able to produce paraphrases
for a much smaller set of phrases is a downside to
using syntactic constraints as we have initially pro-
posed. It means that we would not be able to gen-
erate paraphrases for phrases such as create equal.
Many NLP tasks, such as SMT, which could benefit
from paraphrases require broad coverage and may
need to paraphrases for phrases which are not syn-
tactic constituents.
</bodyText>
<sectionHeader confidence="0.394044" genericHeader="method">
Complex syntactic labels
</sectionHeader>
<bodyText confidence="0.999971125">
To generate paraphrases for a wider set of phrases,
we change our phrase extraction heuristic again so
that it produces phrase pairs for arbitrary spans in
the sentence, including spans that aren’t syntactic
constituents. We assign every span in a sentence a
syntactic label using CCG-style notation (Steedman,
1999), which gives a syntactic role with elements
missing on the left and/or right hand sides.
</bodyText>
<equation confidence="0.8505305">
J Ij+&apos;in. i+�n,
SBP (f1 , e1, A, P) _ {(fj , ei , s) :
d(i0,j0) E A : j &lt; j0 &lt; j + m H i &lt; i0 &lt; i + n
nE](i0,j0) E A : j &lt; j0 &lt; j + mn H i &lt; i0 &lt; i + n
nE]s E CCG-labels(ei+n
i , P)}
</equation>
<bodyText confidence="0.9993285">
The function CCG-labels describes the set of CCG-
labels for the phrase spanning positions i to i + n in
</bodyText>
<table confidence="0.9601265">
create equal
VP/(NP/NNS) create equal .92
creating equal .08
VP/(NP/NNS) PP create equal .96
promote equal .03
establish fair .01
VP/(NP/NNS) PP PP create equal .80
creating equal .10
provide equal .06
create genuinely fair .04
VP/(NP/(NP/NN) PP) create equal .83
create a level playing .17
VP/(NP/(NP/NNS) PP) create equal .83
creating equal .17
</table>
<tableCaption confidence="0.998443">
Table 4: Paraphrases and syntactic labels for the non-
constituent phrase create equal.
</tableCaption>
<bodyText confidence="0.993107666666667">
a parse tree P. It generates three complex syntactic
labels for the non-syntactic constituent phrase create
equal in the parse tree given in Figure 2:
</bodyText>
<listItem confidence="0.926138285714286">
1. VP/(NP/NNS) – This label corresponds to the in-
nermost circle. It indicates that create equal is
a verb phrase missing a noun phrase to its right.
That noun phrase in turn missing a plural noun
(NNS) to its right.
2. SQ\VBP NP/(VP/(NP/NNS)) – This label corre-
sponds to the middle circle. It indicates that
create equal is an SQ missing a VBP and a NP
to its left, and the complex VP to its right.
3. SBARQ\WHADVP (SQ\VBP NP/(VP/(NP/NNS)))/. –
This label corresponds to the outermost cir-
cle. It indicates that create equal is an SBARQ
missing a WHADVP and the complex SQ to its
left, and a punctuation mark to its right.
</listItem>
<bodyText confidence="0.999899545454546">
We can use these complex labels instead of atomic
non-terminal symbols to handle non-constituent
phrases. For example, Table 4 shows the para-
phrases and syntactic labels that are generated for
the non-constituent phrase create equal. The para-
phrases are significantly better than the paraphrases
generated for the phrase by the baseline method (re-
fer back to Table 2).
The labels shown in the figure are a fraction of
those that can be derived for the phrase in the paral-
lel corpus. Each of these corresponds to a different
</bodyText>
<figure confidence="0.9978852">
create
NNS
rights
JJ
equal
do
PRP
we
SBARQ
WHADVP
WRB
How
SQ
.
VBP
NP
VP
?
VB
NP
</figure>
<page confidence="0.983507">
200
</page>
<bodyText confidence="0.999636">
syntactic context, and each has its own set of associ-
ated paraphrases.
We increase the number of phrases that are para-
phrasable from the 90,000 in our initial definition
of SBP to 250,000 when we use complex CCG la-
bels. The number of unique paraphrases increases
from 800,000 to 3.5 million, which is nearly as
many paraphrases that were produced by the base-
line method for the sample.
</bodyText>
<subsectionHeader confidence="0.9980505">
3.2 Syntactic constraints when substituting
paraphrases into a test sentence
</subsectionHeader>
<bodyText confidence="0.9999465">
In addition to applying syntactic constraints to our
phrase extraction algorithm, we can also apply them
when we substitute a paraphrase into a sentence. To
do so, we limit the paraphrases to be the same syn-
tactic type as the phrase that it is replacing, based on
the syntactic labels that are derived from the phrase
tree for a test sentence. Since each phrase normally
has a set of different CCG labels (instead of a sin-
gle non-termal symbol) we need a way of choosing
which label to use when applying the constraint.
There are several different possibilities for choos-
ing among labels. We could simultaneously choose
the best paraphrase and the best label for the phrase
in the parse tree of the test sentence:
</bodyText>
<equation confidence="0.995757">
e2 = arg max arg max P(e2|e1, s) (8)
e2:e2�e1 sECCG-labels(e1,P)
</equation>
<bodyText confidence="0.999483">
Alternately, we could average over all of the labels
that are generated for the phrase in the parse tree:
</bodyText>
<equation confidence="0.9913195">
e2 = arg max
e2:e2�e1
</equation>
<bodyText confidence="0.999934736842105">
The potential drawback of using Equations 8 and
9 is that the CCG labels for a particular sentence sig-
nificantly reduces the paraphrases that can be used.
For instance, VP/(NP/NNS) is the only label for the
paraphrases in Table 4 that is compatible with the
parse tree given in Figure 2.
Because the CCG labels for a given sentence are
so specific, many times there are no matches. There-
fore we also investigated a looser constraint. We
choose the highest probability paraphrase with any
label (i.e. the set of labels extracted from all parse
trees in our parallel corpus):
Equation 10 only applies syntactic constraints dur-
ing phrase extraction and ignores them during sub-
stitution.
In our experiments, we evaluate the quality of the
paraphrases that are generated using Equations 8, 9
and 10. We compare their quality against the Ban-
nard and Callison-Burch (2005) baseline.
</bodyText>
<sectionHeader confidence="0.995082" genericHeader="method">
4 Experimental design
</sectionHeader>
<bodyText confidence="0.9999948">
We conducted a manual evaluation to evaluate para-
phrase quality. We evaluated whether paraphrases
retained the meaning of their original phrases and
whether they remained grammatical when they re-
placed the original phrase in a sentence.
</bodyText>
<subsectionHeader confidence="0.995703">
4.1 Training materials
</subsectionHeader>
<bodyText confidence="0.999977941176471">
Our paraphrase model was trained using the Eu-
roparl corpus (Koehn, 2005). We used ten par-
allel corpora between English and (each of) Dan-
ish, Dutch, Finnish, French, German, Greek, Ital-
ian, Portuguese, Spanish, and Swedish, with approx-
imately 30 million words per language for a total of
315 million English words. Automatic word align-
ments were created for these using Giza++ (Och and
Ney, 2003). The English side of each parallel corpus
was parsed using the Bikel parser (Bikel, 2002). A
total of 1.6 million unique sentences were parsed.
A trigram language model was trained on these En-
glish sentences using the SRI language modeling
toolkit (Stolcke, 2002).
The paraphrase model and language model for the
Bannard and Callison-Burch (2005) baseline were
trained on the same data to ensure a fair comparison.
</bodyText>
<subsectionHeader confidence="0.999749">
4.2 Test phrases
</subsectionHeader>
<bodyText confidence="0.999806692307692">
The test set was the English portion of test sets
used in the shared translation task of the ACL-
2007 Workshop on Statistical Machine Translation
(Callison-Burch et al., 2007). The test sentences
were also parsed with the Bikel parser.
The phrases to be evaluated were selected such
that there was an even balance of phrase lengths
(from one word long up to five words long), with
half of the phrases being valid syntactic constituents
and half being arbitrary sequences of words. 410
phrases were selected at random for evaluation. 30
items were excluded from our results subsequent
to evaluation on the grounds that they consisted
</bodyText>
<equation confidence="0.99891825">
� P(e2|e1, s) (9)
sECCG-labels(e1,P)
e2 = arg max arg max P(e2|e1, s) (10)
e2:e2�e1 sEnVT in CCCG-labels(e1,T)
</equation>
<page confidence="0.991999">
201
</page>
<bodyText confidence="0.952036666666667">
solely of punctuation and stop words like determin-
ers, prepositions and pronouns. This left a total of
380 unique phrases.
</bodyText>
<subsectionHeader confidence="0.997184">
4.3 Experimental conditions
</subsectionHeader>
<bodyText confidence="0.951932857142857">
We produced paraphrases under the following eight
conditions:
1. Baseline – The paraphrase probability defined
by Bannard and Callison-Burch (2005). Calcu-
lated over multiple parallel corpora as given in
Equation 5. Note that under this condition the
best paraphrase is the same for each occurrence
of the phrase irrespective of which sentence it
occurs in.
2. Baseline + LM – The paraphrase probability
(as above) combined with the language model
probability calculated for the sentence with the
phrase replaced with the paraphrase.
3. Extraction Constraints – This condition se-
lected the best paraphrase according to Equa-
tion 10. It chooses the single best paraphrase
over all labels. Conditions 3 and 5 only apply
the syntactic constraints at the phrase extraction
stage, and do not require that the paraphrase
have the same syntactic label as the phrase in
the sentence that it is being subtituted into.
</bodyText>
<listItem confidence="0.65188525">
4. Extraction Constraints + LM – As above, but
the paraphrases are also ranked with a language
model probability.
5. Substitution Constraints – This condition
</listItem>
<bodyText confidence="0.797295833333333">
corresponds to Equation 8, which selects the
highest probability paraphrase which matches
at least one of the syntactic labels of the phrase
in the test sentence. Conditions 5–8 apply the
syntactic constraints both and the phrase ex-
traction and at the substitution stages.
</bodyText>
<listItem confidence="0.973453666666667">
6. Syntactic Constraints + LM – As above, but
including a language model probability as well.
7. Averaged Substitution Constraints – This
</listItem>
<bodyText confidence="0.67650725">
condition corresponds to Equation 9, which av-
erages over all of the syntactic labels for the
phrase in the sentence, instead of choosing the
single one which maximizes the probability.
</bodyText>
<note confidence="0.664485">
MEANING
</note>
<tableCaption confidence="0.91906475">
5 All of the meaning of the original phrase is re-
tained, and nothing is added
4 The meaning of the original phrase is retained, al-
though some additional information may be added
but does not transform the meaning
3 The meaning of the original phrase is retained, al-
though some information may be deleted without
too great a loss in the meaning
</tableCaption>
<table confidence="0.927218">
2 Substantial amount of the meaning is different
1 The paraphrase doesn’t mean anything close to
the original phrase
GRAMMAR
</table>
<tableCaption confidence="0.961332833333333">
5 The sentence with the paraphrase inserted is per-
fectly grammatical
4 The sentence is grammatical, but might sound
slightly awkward
3 The sentence has an agreement error (such as be-
tween its subject and verb, or between a plural
noun and singular determiner)
2 The sentence has multiple errors or omits words
that would be required to make it grammatical
1 The sentence is totally ungrammatical
Table 5: Annotators rated paraphrases along two 5-point
scales.
</tableCaption>
<bodyText confidence="0.870330333333333">
8. Averaged Substitution Constraints + LM –
As above, but including a language model
probability.
</bodyText>
<subsectionHeader confidence="0.999271">
4.4 Manual evaluation
</subsectionHeader>
<bodyText confidence="0.926472111111111">
We evaluated the paraphrase quality through a sub-
stitution test. We retrieved a number of sentences
which contained each test phrase and substituted the
phrase with automatically-generated paraphrases.
Annotators judged whether the paraphrases had the
same meaning as the original and whether the re-
sulting sentences were grammatical. They assigned
two values to each sentence using the 5-point scales
given in Table 5. We considered an item to have
the same meaning if it was assigned a score of 3 or
greater, and to be grammatical if it was assigned a
score of 4 or 5.
We evaluated several instances of a phrase when
it occurred multiple times in the test corpus,
since paraphrase quality can vary based on context
(Szpektor et al., 2007). There were an average of
3.1 instances for each phrase, with a maximum of
6. There were a total of 1,195 sentences that para-
</bodyText>
<page confidence="0.99595">
202
</page>
<bodyText confidence="0.99996025">
phrases were substituted into, with a total of 8,422
judgements collected. Note that 7 different para-
phrases were judged on average for every instance.
This is because annotators judged paraphrases for
eight conditions, and because we collected judg-
ments for the 5-best paraphrases for many of the
conditions.
We measured inter-annotator agreement with the
Kappa statistic (Carletta, 1996) using the 1,391
items that two annotators scored in common. The
two annotators assigned the same absolute score
47% of the time. If we consider chance agreement to
be 20% for 5-point scales, then K = 0.33, which is
commonly interpreted as “fair” (Landis and Koch,
1977). If we instead measure agreement in terms
of how often the annotators both judged an item to
be above or below the thresholds that we set, then
their rate of agreement was 80%. In this case chance
agreement would be 50%, so K = 0.61, which is
“substantial”.
</bodyText>
<subsectionHeader confidence="0.999024">
4.5 Data and code
</subsectionHeader>
<bodyText confidence="0.903288444444445">
In order to allow other researchers to recreate our re-
sults or extend our work, we have prepared the fol-
lowing materials for download2:
• The complete set of paraphrases generated for
the test set. This includes the 3.7 million para-
phrases generated by the baseline method and
the 3.5 million paraphrases generated with syn-
tactic constraints.
• The code that we used to produce these para-
phrases and the complete data sets (including
all 10 word-aligned parallel corpora along with
their English parses), so that researchers can
extract paraphrases for new sets of phrases.
• The manual judgments about paraphrase qual-
ity. These may be useful as development ma-
terial for setting the weights of a log-linear for-
mulation of paraphrasing, as suggested in Zhao
et al. (2008a).
</bodyText>
<sectionHeader confidence="0.999955" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.969886">
Table 6 summarizes the results of the manual eval-
uation. We can observe a strong trend in the syn-
tactically constrained approaches performing better
</bodyText>
<footnote confidence="0.974072">
2Available from http://cs.jhu.edu/˜ccb/.
</footnote>
<table confidence="0.9995166">
correct correct both
meaning grammar correct
Baseline .56 .35 .30
Baseline+LM .46 .44 .36
Extraction Constraints .62 .57 .46
Extraction Const+LM .60 .65 .50
Substitution Constraints .60 .60 .50
Substitution Const+LM .61 .68 .54
Avg Substitution Const .62 .61 .51
Avg Substit Const+LM .61 .68 .55
</table>
<tableCaption confidence="0.986661">
Table 6: The results of the manual evaluation for each
</tableCaption>
<bodyText confidence="0.949132222222222">
of the eight conditions. Correct meaning is the percent of
time that a condition was assigned a 3, 4, or 5, and correct
grammar is the percent of time that it was given a 4 or 5,
using the scales from Table 5.
than the baseline. They retain the correct meaning
more often (ranging from 4% to up to 15%). They
are judged to be grammatical far more frequently
(up to 26% more often without the language model,
and 24% with the language model) . They perform
nearly 20% better when both meaning and grammat-
icality are used as criteria.3
Another trend that can be observed is that incor-
porating a language model probability tends to result
in more grammatical output (a 7–9% increase), but
meaning suffers as a result in some cases. When
the LM is applied there is a drop of 12% in correct
meaning for the baseline, but only a slight dip of 1-
2% for the syntactically-constrained phrases.
Note that for the conditions where the paraphrases
were required to have the same syntactic type as the
phrase in the parse tree, there was a reduction in the
number of paraphrases that could be applied. For
the first two conditions, paraphrases were posited for
1194 sentences, conditions 3 and 4 could be applied
to 1142 of those sentences, but conditions 5–8 could
only be applied to 876 sentences. The substitution
constraints reduce coverage to 73% of the test sen-
tences. Given that the extraction constraints have
better coverage and nearly identical performance on
3Our results show a significantly lower score for the base-
line than reported in Bannard and Callison-Burch (2005). This
is potentially due to the facts that in this work we evaluated
on out-of-domain news commentary data, and we randomly se-
lected phrases. In the pervious work the test phrases were drawn
from WordNet, and they were evaluated solely on in-domain
European parliament data.
</bodyText>
<page confidence="0.996578">
203
</page>
<bodyText confidence="0.9935875">
the meaning criterion, they might be more suitable
in some circumstances.
</bodyText>
<sectionHeader confidence="0.99857" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99986671875">
In this paper we have presented a novel refinement
to paraphrasing with bilingual parallel corpora. We
illustrated that a significantly higher performance
can be achieved by constraining paraphrases to have
the same syntactic type as the original phrase. A
thorough manual evaluation found an absolute im-
provement in quality of 19% using strict criteria
about paraphrase accuracy when comparing against
a strong baseline. The syntactically enhanced para-
phrases are judged to be grammatically correct over
two thirds of the time, as opposed to the baseline
method which was grammatically correct under half
of the time.
This paper proposed constraints on paraphrases at
two stages: when deriving them from parsed paral-
lel corpora and when substituting them into parsed
test sentences. These constraints produce para-
phrases that are better than the baseline and which
are less commonly affected by problems due to un-
aligned words. Furthermore, by introducing com-
plex syntactic labels instead of solely relying on
non-terminal symbols in the parse trees, we are able
to keep the broad coverage of the baseline method.
Syntactic constraints significantly improve the
quality of this paraphrasing method, and their use
opens the question about whether analogous con-
straints can be usefully applied to paraphrases gen-
erated from purely monolingual corpora. Our im-
provements to the extraction of paraphrases from
parallel corpora suggests that it may be usefully ap-
plied to other NLP applications, such as generation,
which require grammatical output.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998874">
Thanks go to Sally Blatz, Emily Hinchcliff and
Michelle Bland for conducting the manual evalua-
tion and to Michelle Bland and Omar Zaidan for
proofreading and commenting on a draft of this pa-
per.
This work was supported by the National Science
Foundation under Grant No. 0713448. The views
and findings are the author’s alone.
</bodyText>
<sectionHeader confidence="0.996372" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999884547169811">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL.
Dan Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings
of HLT.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of HLT/NAACL.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249–254.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings of HLT/NAACL.
Bryant Huang and Kevin Knight. 2006. Relabeling syn-
tax trees to improve syntax-based machine translation
quality. In Proceedings of HLT/NAACL.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual
corpora. In Proceedings of the Second International
Workshop on Paraphrasing (ACL 2003).
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
EMNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit,
Phuket, Thailand.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159–174.
Dekang Lin and Colin Cherry. 2002. Word align-
ment with cohesion constraint. In Proceedings of
HLT/NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering,
7(3):343–360.
</reference>
<page confidence="0.984369">
204
</page>
<reference confidence="0.999869428571429">
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parame-
ter tuning in statistical machine translation. In Pro-
ceedings of the ACL Workshop on Statistical Machine
Translation.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417–449.
Karolina Owczarzak, Declan Groves, Josef Van Gen-
abith, and Andy Way. 2006. Contextual bitext-derived
paraphrases in automatic MT evaluation. In Proceed-
ings of the SMT Workshop at HLT-NAACL.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of ACL.
Mark Steedman. 1999. Alternative quantier scope in ccg.
In Proceedings of ACL.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, Denver,
Colorado, September.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of ACL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng
Li. 2008a. Combining multiple resources to improve
SMT-based paraphrasing model. In Proceedings of
ACL/HLT.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot approach for extracting paraphrase
patterns from bilingual corpora. In Proceedings of
ACL/HLT.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Re-
evaluating machine translation results with paraphrase
support. In Proceedings of EMNLP.
</reference>
<page confidence="0.998872">
205
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.543848">
<title confidence="0.99492">Syntactic Constraints on Paraphrases Extracted from Parallel Corpora</title>
<author confidence="0.951251">Chris</author>
<affiliation confidence="0.7972225">Center for Language and Speech Johns Hopkins</affiliation>
<address confidence="0.932705">Baltimore,</address>
<email confidence="0.979567">ccbcsjhuedu</email>
<abstract confidence="0.999454230769231">We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1803" citStr="Bannard and Callison-Burch, 2005" startWordPosition="265" endWordPosition="268">hou et al., 2006; Owczarzak et al., 2006), and machine translation both by dealing with out of vocabulary words and phrases (Callison-Burch et al., 2006) and by expanding the set of reference translations for minimum error rate training (Madnani et al., 2007). While all applications require the preservation of meaning when a phrase is replaced by its paraphrase, some additionally require the resulting sentence to be grammatical. In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpora (Bannard and Callison-Burch, 2005). The paraphrasing technique employs various aspects of phrase-based statistical machine translation including phrase extraction heuristics to obtain bilingual phrase pairs from word alignments. English phrases are considered to be potential paraphrases of each other if they share a common foreign language phrase among their translations. Multiple paraphrases are frequently extracted for each phrase and can be ranked using a paraphrase probability based on phrase translation probabilities. We find that the quality of the paraphrases that are generated in this fashion improves significantly whe</context>
<context position="6446" citStr="Bannard and Callison-Burch (2005)" startWordPosition="960" endWordPosition="963"> al. (2008b) extended the method so that it is capable of generating richer paraphrase patterns that include part-of-speech slots, rather than simple lexical and phrasal paraphrases. For example, they extracted patterns such as consider NN → take NN into consideration. To accomplish this, Zhao el al. used dependency parses on the English side of the parallel corpus. Their work differs from the work presented in this paper because their syntactic constraints applied to slots within paraphrase patters, and our constraints apply to the paraphrases themselves. 3 Paraphrasing with parallel corpora Bannard and Callison-Burch (2005) extract paraphrases from bilingual parallel corpora. They give a probabilistic formation of paraphrasing which naturally falls out of the fact that they use techniques from phrase-based statistical machine translation: e2 = arg max p(e2|e1) (1) e2:e2�e1 where �p(e2|e1) = p(f|e1)p(e2|f, e1) (2) f E≈ p(f|e1)p(e2|f) (3) f Phrase translation probabilities p(f|e1) and p(e2|f) are commonly calculated using maximum likelihood estimation (Koehn et al., 2003): count(e, f) p(f|e) = (4) E f count(e, f) where the counts are collected by enumerating all bilingual phrase pairs that are consistent with the </context>
<context position="9961" citStr="Bannard and Callison-Burch (2005)" startWordPosition="1559" endWordPosition="1562">g Bannard and Callison-Burch’s method trained on the Europarl corpora (as described in Section 4). It generated a total of 3.7 million paraphrases for Table 2: The baseline’s paraphrases of create equal. Most are clearly bad, and the most probable e2 # e1 is a substring of e1. 400,000 phrases in the list.1 We observed that 34% of the paraphrases (excluding the phrase itself) were super- or sub-strings of the original phrase. The most probable paraphrase was a super- or sub-string of the phrase 73% of the time. There are a number of strategies that might be adopted to alleviate this problem: • Bannard and Callison-Burch (2005) rank their paraphrases with a language model when the paraphrases are substituted into a sentence. • Bannard and Callison-Burch (2005) sum over multiple parallel corpora C to reduce the problems associated with systematic errors in the 1The remaining 500,000 phrases could not be paraphrased either because e2 # e1 or because they were not consistently aligned to any foreign phrases. 198 word alignments in one language pair: p(f|e1)p(e2|f) (5) • We could change the phrase extraction heuristic’s treatment of unaligned words, or we could attempt to ensure that we have fewer unaligned items in our</context>
<context position="19484" citStr="Bannard and Callison-Burch (2005)" startWordPosition="3235" endWordPosition="3239">at is compatible with the parse tree given in Figure 2. Because the CCG labels for a given sentence are so specific, many times there are no matches. Therefore we also investigated a looser constraint. We choose the highest probability paraphrase with any label (i.e. the set of labels extracted from all parse trees in our parallel corpus): Equation 10 only applies syntactic constraints during phrase extraction and ignores them during substitution. In our experiments, we evaluate the quality of the paraphrases that are generated using Equations 8, 9 and 10. We compare their quality against the Bannard and Callison-Burch (2005) baseline. 4 Experimental design We conducted a manual evaluation to evaluate paraphrase quality. We evaluated whether paraphrases retained the meaning of their original phrases and whether they remained grammatical when they replaced the original phrase in a sentence. 4.1 Training materials Our paraphrase model was trained using the Europarl corpus (Koehn, 2005). We used ten parallel corpora between English and (each of) Danish, Dutch, Finnish, French, German, Greek, Italian, Portuguese, Spanish, and Swedish, with approximately 30 million words per language for a total of 315 million English </context>
<context position="21650" citStr="Bannard and Callison-Burch (2005)" startWordPosition="3582" endWordPosition="3585">syntactic constituents and half being arbitrary sequences of words. 410 phrases were selected at random for evaluation. 30 items were excluded from our results subsequent to evaluation on the grounds that they consisted � P(e2|e1, s) (9) sECCG-labels(e1,P) e2 = arg max arg max P(e2|e1, s) (10) e2:e2�e1 sEnVT in CCCG-labels(e1,T) 201 solely of punctuation and stop words like determiners, prepositions and pronouns. This left a total of 380 unique phrases. 4.3 Experimental conditions We produced paraphrases under the following eight conditions: 1. Baseline – The paraphrase probability defined by Bannard and Callison-Burch (2005). Calculated over multiple parallel corpora as given in Equation 5. Note that under this condition the best paraphrase is the same for each occurrence of the phrase irrespective of which sentence it occurs in. 2. Baseline + LM – The paraphrase probability (as above) combined with the language model probability calculated for the sentence with the phrase replaced with the paraphrase. 3. Extraction Constraints – This condition selected the best paraphrase according to Equation 10. It chooses the single best paraphrase over all labels. Conditions 3 and 5 only apply the syntactic constraints at th</context>
<context position="28923" citStr="Bannard and Callison-Burch (2005)" startWordPosition="4799" endWordPosition="4802">ere required to have the same syntactic type as the phrase in the parse tree, there was a reduction in the number of paraphrases that could be applied. For the first two conditions, paraphrases were posited for 1194 sentences, conditions 3 and 4 could be applied to 1142 of those sentences, but conditions 5–8 could only be applied to 876 sentences. The substitution constraints reduce coverage to 73% of the test sentences. Given that the extraction constraints have better coverage and nearly identical performance on 3Our results show a significantly lower score for the baseline than reported in Bannard and Callison-Burch (2005). This is potentially due to the facts that in this work we evaluated on out-of-domain news commentary data, and we randomly selected phrases. In the pervious work the test phrases were drawn from WordNet, and they were evaluated solely on in-domain European parliament data. 203 the meaning criterion, they might be more suitable in some circumstances. 6 Conclusion In this paper we have presented a novel refinement to paraphrasing with bilingual parallel corpora. We illustrated that a significantly higher performance can be achieved by constraining paraphrases to have the same syntactic type as</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5229" citStr="Barzilay and McKeown (2001)" startWordPosition="782" endWordPosition="785">porated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005). Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated structural paraphrases capable of capturing longdistance dependencies. Pang et al. (2003) employed a syntax-based algorithm to align equivalent English sentences by merging corresponding nodes in parse trees and compressing them down into a word lattice. Perhaps the most closely related work is a recent extension to Bannard and Callison-Burch’s paraphrasing method. Zhao et al. (2008b) ext</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Bikel</author>
</authors>
<title>Design of a multi-lingual, parallelprocessing statistical parsing engine.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT.</booktitle>
<contexts>
<context position="20262" citStr="Bikel, 2002" startWordPosition="3362" endWordPosition="3363">nal phrases and whether they remained grammatical when they replaced the original phrase in a sentence. 4.1 Training materials Our paraphrase model was trained using the Europarl corpus (Koehn, 2005). We used ten parallel corpora between English and (each of) Danish, Dutch, Finnish, French, German, Greek, Italian, Portuguese, Spanish, and Swedish, with approximately 30 million words per language for a total of 315 million English words. Automatic word alignments were created for these using Giza++ (Och and Ney, 2003). The English side of each parallel corpus was parsed using the Bikel parser (Bikel, 2002). A total of 1.6 million unique sentences were parsed. A trigram language model was trained on these English sentences using the SRI language modeling toolkit (Stolcke, 2002). The paraphrase model and language model for the Bannard and Callison-Burch (2005) baseline were trained on the same data to ensure a fair comparison. 4.2 Test phrases The test set was the English portion of test sets used in the shared translation task of the ACL2007 Workshop on Statistical Machine Translation (Callison-Burch et al., 2007). The test sentences were also parsed with the Bikel parser. The phrases to be eval</context>
</contexts>
<marker>Bikel, 2002</marker>
<rawString>Dan Bikel. 2002. Design of a multi-lingual, parallelprocessing statistical parsing engine. In Proceedings of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="1323" citStr="Callison-Burch et al., 2006" startWordPosition="191" endWordPosition="194">ity over the baseline method. 1 Introduction Paraphrases are alternative ways of expressing the same information. Being able to identify or generate paraphrases automatically is useful in a wide range of natural language applications. Recent work has shown how paraphrases can improve question answering through query expansion (Riezler et al., 2007), automatic evaluation of translation and summarization by modeling alternative lexicalization (Kauchak and Barzilay, 2006; Zhou et al., 2006; Owczarzak et al., 2006), and machine translation both by dealing with out of vocabulary words and phrases (Callison-Burch et al., 2006) and by expanding the set of reference translations for minimum error rate training (Madnani et al., 2007). While all applications require the preservation of meaning when a phrase is replaced by its paraphrase, some additionally require the resulting sentence to be grammatical. In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpora (Bannard and Callison-Burch, 2005). The paraphrasing technique employs various aspects of phrase-based statistical machine translation including phrase ex</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(Meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="20779" citStr="Callison-Burch et al., 2007" startWordPosition="3444" endWordPosition="3447">+ (Och and Ney, 2003). The English side of each parallel corpus was parsed using the Bikel parser (Bikel, 2002). A total of 1.6 million unique sentences were parsed. A trigram language model was trained on these English sentences using the SRI language modeling toolkit (Stolcke, 2002). The paraphrase model and language model for the Bannard and Callison-Burch (2005) baseline were trained on the same data to ensure a fair comparison. 4.2 Test phrases The test set was the English portion of test sets used in the shared translation task of the ACL2007 Workshop on Statistical Machine Translation (Callison-Burch et al., 2007). The test sentences were also parsed with the Bikel parser. The phrases to be evaluated were selected such that there was an even balance of phrase lengths (from one word long up to five words long), with half of the phrases being valid syntactic constituents and half being arbitrary sequences of words. 410 phrases were selected at random for evaluation. 30 items were excluded from our results subsequent to evaluation on the grounds that they consisted � P(e2|e1, s) (9) sECCG-labels(e1,P) e2 = arg max arg max P(e2|e1, s) (10) e2:e2�e1 sEnVT in CCCG-labels(e1,T) 201 solely of punctuation and s</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (Meta-) evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="25481" citStr="Carletta, 1996" startWordPosition="4214" endWordPosition="4215">s in the test corpus, since paraphrase quality can vary based on context (Szpektor et al., 2007). There were an average of 3.1 instances for each phrase, with a maximum of 6. There were a total of 1,195 sentences that para202 phrases were substituted into, with a total of 8,422 judgements collected. Note that 7 different paraphrases were judged on average for every instance. This is because annotators judged paraphrases for eight conditions, and because we collected judgments for the 5-best paraphrases for many of the conditions. We measured inter-annotator agreement with the Kappa statistic (Carletta, 1996) using the 1,391 items that two annotators scored in common. The two annotators assigned the same absolute score 47% of the time. If we consider chance agreement to be 20% for 5-point scales, then K = 0.33, which is commonly interpreted as “fair” (Landis and Koch, 1977). If we instead measure agreement in terms of how often the annotators both judged an item to be above or below the thresholds that we set, then their rate of agreement was 80%. In this case chance agreement would be 50%, so K = 0.61, which is “substantial”. 4.5 Data and code In order to allow other researchers to recreate our r</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4841" citStr="Chiang, 2005" startWordPosition="726" endWordPosition="727"> a linguistic grammar. Subsequent work used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002). If two English phrases are in disjoint subtrees in the parse, then the phrasal cohesion constraint prevents them from being aligned to overlapping sequences in the foreign sentence. Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005). Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated str</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4368" citStr="Fox, 2002" startWordPosition="653" endWordPosition="654">employing syntactic constraints in statistical machine translation. Wu (1997) introduced the inversion transduction grammar formalism which treats translation as a process of parallel parsing of the source and target language via a synchronized grammar. The synchronized grammar places constraints on which words can be aligned across bilingual sentence pairs. To achieve computational efficiency, the original proposal used only a single non-terminal label rather than a linguistic grammar. Subsequent work used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002). If two English phrases are in disjoint subtrees in the parse, then the phrasal cohesion constraint prevents them from being aligned to overlapping sequences in the foreign sentence. Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005). Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic in</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="4722" citStr="Galley et al., 2004" startWordPosition="705" endWordPosition="709"> sentence pairs. To achieve computational efficiency, the original proposal used only a single non-terminal label rather than a linguistic grammar. Subsequent work used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002). If two English phrases are in disjoint subtrees in the parse, then the phrasal cohesion constraint prevents them from being aligned to overlapping sequences in the foreign sentence. Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005). Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-traini</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryant Huang</author>
<author>Kevin Knight</author>
</authors>
<title>Relabeling syntax trees to improve syntax-based machine translation quality.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="4890" citStr="Huang and Knight, 2006" startWordPosition="732" endWordPosition="735">used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002). If two English phrases are in disjoint subtrees in the parse, then the phrasal cohesion constraint prevents them from being aligned to overlapping sequences in the foreign sentence. Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005). Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated structural paraphrases capable of capturing longdist</context>
</contexts>
<marker>Huang, Knight, 2006</marker>
<rawString>Bryant Huang and Kevin Knight. 2006. Relabeling syntax trees to improve syntax-based machine translation quality. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Ibrahim</author>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
</authors>
<title>Extracting structural paraphrases from aligned monolingual corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second International Workshop on Paraphrasing (ACL</booktitle>
<contexts>
<context position="5427" citStr="Ibrahim et al. (2003)" startWordPosition="807" endWordPosition="810">ontext free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated structural paraphrases capable of capturing longdistance dependencies. Pang et al. (2003) employed a syntax-based algorithm to align equivalent English sentences by merging corresponding nodes in parse trees and compressing them down into a word lattice. Perhaps the most closely related work is a recent extension to Bannard and Callison-Burch’s paraphrasing method. Zhao et al. (2008b) extended the method so that it is capable of generating richer paraphrase patterns that include part-of-speech slots, rather than simple lexical and phrasal paraphrases. For example, they extracted pat</context>
</contexts>
<marker>Ibrahim, Katz, Lin, 2003</marker>
<rawString>Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extracting structural paraphrases from aligned monolingual corpora. In Proceedings of the Second International Workshop on Paraphrasing (ACL 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1167" citStr="Kauchak and Barzilay, 2006" startWordPosition="166" endWordPosition="169"> coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method. 1 Introduction Paraphrases are alternative ways of expressing the same information. Being able to identify or generate paraphrases automatically is useful in a wide range of natural language applications. Recent work has shown how paraphrases can improve question answering through query expansion (Riezler et al., 2007), automatic evaluation of translation and summarization by modeling alternative lexicalization (Kauchak and Barzilay, 2006; Zhou et al., 2006; Owczarzak et al., 2006), and machine translation both by dealing with out of vocabulary words and phrases (Callison-Burch et al., 2006) and by expanding the set of reference translations for minimum error rate training (Madnani et al., 2007). While all applications require the preservation of meaning when a phrase is replaced by its paraphrase, some additionally require the resulting sentence to be grammatical. In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpor</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="6901" citStr="Koehn et al., 2003" startWordPosition="1026" endWordPosition="1029"> to slots within paraphrase patters, and our constraints apply to the paraphrases themselves. 3 Paraphrasing with parallel corpora Bannard and Callison-Burch (2005) extract paraphrases from bilingual parallel corpora. They give a probabilistic formation of paraphrasing which naturally falls out of the fact that they use techniques from phrase-based statistical machine translation: e2 = arg max p(e2|e1) (1) e2:e2�e1 where �p(e2|e1) = p(f|e1)p(e2|f, e1) (2) f E≈ p(f|e1)p(e2|f) (3) f Phrase translation probabilities p(f|e1) and p(e2|f) are commonly calculated using maximum likelihood estimation (Koehn et al., 2003): count(e, f) p(f|e) = (4) E f count(e, f) where the counts are collected by enumerating all bilingual phrase pairs that are consistent with the 197 Figure 1: The interaction of the phrase extraction heuristic with unaligned English words means that the Spanish phrase la igualdad aligns with equal, create equal, and to create equal. el proyecto europeo no ha the european conseguido la igualdad de oportunidades project has failed to create opportunities. equal word alignments for sentence pairs in a bilingual parallel corpus. Various phrase extraction heuristics are possible. Och and Ney (2004)</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT-Summit,</booktitle>
<location>Phuket, Thailand.</location>
<contexts>
<context position="19849" citStr="Koehn, 2005" startWordPosition="3293" endWordPosition="3294">ts during phrase extraction and ignores them during substitution. In our experiments, we evaluate the quality of the paraphrases that are generated using Equations 8, 9 and 10. We compare their quality against the Bannard and Callison-Burch (2005) baseline. 4 Experimental design We conducted a manual evaluation to evaluate paraphrase quality. We evaluated whether paraphrases retained the meaning of their original phrases and whether they remained grammatical when they replaced the original phrase in a sentence. 4.1 Training materials Our paraphrase model was trained using the Europarl corpus (Koehn, 2005). We used ten parallel corpora between English and (each of) Danish, Dutch, Finnish, French, German, Greek, Italian, Portuguese, Spanish, and Swedish, with approximately 30 million words per language for a total of 315 million English words. Automatic word alignments were created for these using Giza++ (Och and Ney, 2003). The English side of each parallel corpus was parsed using the Bikel parser (Bikel, 2002). A total of 1.6 million unique sentences were parsed. A trigram language model was trained on these English sentences using the SRI language modeling toolkit (Stolcke, 2002). The paraphr</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. A parallel corpus for statistical machine translation. In Proceedings of MT-Summit, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<pages>33--159</pages>
<contexts>
<context position="25751" citStr="Landis and Koch, 1977" startWordPosition="4259" endWordPosition="4262"> total of 8,422 judgements collected. Note that 7 different paraphrases were judged on average for every instance. This is because annotators judged paraphrases for eight conditions, and because we collected judgments for the 5-best paraphrases for many of the conditions. We measured inter-annotator agreement with the Kappa statistic (Carletta, 1996) using the 1,391 items that two annotators scored in common. The two annotators assigned the same absolute score 47% of the time. If we consider chance agreement to be 20% for 5-point scales, then K = 0.33, which is commonly interpreted as “fair” (Landis and Koch, 1977). If we instead measure agreement in terms of how often the annotators both judged an item to be above or below the thresholds that we set, then their rate of agreement was 80%. In this case chance agreement would be 50%, so K = 0.61, which is “substantial”. 4.5 Data and code In order to allow other researchers to recreate our results or extend our work, we have prepared the following materials for download2: • The complete set of paraphrases generated for the test set. This includes the 3.7 million paraphrases generated by the baseline method and the 3.5 million paraphrases generated with syn</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33:159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Colin Cherry</author>
</authors>
<title>Word alignment with cohesion constraint.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="4391" citStr="Lin and Cherry, 2002" startWordPosition="655" endWordPosition="658">yntactic constraints in statistical machine translation. Wu (1997) introduced the inversion transduction grammar formalism which treats translation as a process of parallel parsing of the source and target language via a synchronized grammar. The synchronized grammar places constraints on which words can be aligned across bilingual sentence pairs. To achieve computational efficiency, the original proposal used only a single non-terminal label rather than a linguistic grammar. Subsequent work used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002). If two English phrases are in disjoint subtrees in the parse, then the phrasal cohesion constraint prevents them from being aligned to overlapping sequences in the foreign sentence. Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005). Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic information, either in th</context>
</contexts>
<marker>Lin, Cherry, 2002</marker>
<rawString>Dekang Lin and Colin Cherry. 2002. Word alignment with cohesion constraint. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules from text.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="5112" citStr="Lin and Pantel (2001)" startWordPosition="765" endWordPosition="768"> prevents them from being aligned to overlapping sequences in the foreign sentence. Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005). Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated structural paraphrases capable of capturing longdistance dependencies. Pang et al. (2003) employed a syntax-based algorithm to align equivalent English sentences by merging corresponding nodes in parse trees and compressing them down into a word lattice. Perhaps the most cl</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules from text. Natural Language Engineering, 7(3):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Necip Fazil Ayan</author>
<author>Philip Resnik</author>
<author>Bonnie Dorr</author>
</authors>
<title>Using paraphrases for parameter tuning in statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1429" citStr="Madnani et al., 2007" startWordPosition="210" endWordPosition="213"> Being able to identify or generate paraphrases automatically is useful in a wide range of natural language applications. Recent work has shown how paraphrases can improve question answering through query expansion (Riezler et al., 2007), automatic evaluation of translation and summarization by modeling alternative lexicalization (Kauchak and Barzilay, 2006; Zhou et al., 2006; Owczarzak et al., 2006), and machine translation both by dealing with out of vocabulary words and phrases (Callison-Burch et al., 2006) and by expanding the set of reference translations for minimum error rate training (Madnani et al., 2007). While all applications require the preservation of meaning when a phrase is replaced by its paraphrase, some additionally require the resulting sentence to be grammatical. In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpora (Bannard and Callison-Burch, 2005). The paraphrasing technique employs various aspects of phrase-based statistical machine translation including phrase extraction heuristics to obtain bilingual phrase pairs from word alignments. English phrases are considered </context>
</contexts>
<marker>Madnani, Ayan, Resnik, Dorr, 2007</marker>
<rawString>Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and Bonnie Dorr. 2007. Using paraphrases for parameter tuning in statistical machine translation. In Proceedings of the ACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="20172" citStr="Och and Ney, 2003" startWordPosition="3345" endWordPosition="3348">valuate paraphrase quality. We evaluated whether paraphrases retained the meaning of their original phrases and whether they remained grammatical when they replaced the original phrase in a sentence. 4.1 Training materials Our paraphrase model was trained using the Europarl corpus (Koehn, 2005). We used ten parallel corpora between English and (each of) Danish, Dutch, Finnish, French, German, Greek, Italian, Portuguese, Spanish, and Swedish, with approximately 30 million words per language for a total of 315 million English words. Automatic word alignments were created for these using Giza++ (Och and Ney, 2003). The English side of each parallel corpus was parsed using the Bikel parser (Bikel, 2002). A total of 1.6 million unique sentences were parsed. A trigram language model was trained on these English sentences using the SRI language modeling toolkit (Stolcke, 2002). The paraphrase model and language model for the Bannard and Callison-Burch (2005) baseline were trained on the same data to ensure a fair comparison. 4.2 Test phrases The test set was the English portion of test sets used in the shared translation task of the ACL2007 Workshop on Statistical Machine Translation (Callison-Burch et al.</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="7501" citStr="Och and Ney (2004)" startWordPosition="1120" endWordPosition="1123">oehn et al., 2003): count(e, f) p(f|e) = (4) E f count(e, f) where the counts are collected by enumerating all bilingual phrase pairs that are consistent with the 197 Figure 1: The interaction of the phrase extraction heuristic with unaligned English words means that the Spanish phrase la igualdad aligns with equal, create equal, and to create equal. el proyecto europeo no ha the european conseguido la igualdad de oportunidades project has failed to create opportunities. equal word alignments for sentence pairs in a bilingual parallel corpus. Various phrase extraction heuristics are possible. Och and Ney (2004) defined consistent bilingual phrase pairs as follows: J I j+m i+n BP(f1 ,e1,A) _ {(fj Fei ) : d(i0,j0) E A : j &lt; j0 &lt; j + m — i &lt; i0 &lt; i + n ∧EI(i0,j0) E A : j &lt; j0 &lt; j + m∧ — i &lt; i0 &lt; i + n} equal equal .35 equally .02 same .07 the .02 equality .03 fair .01 equals .02 equal rights .01 Table 1: The baseline method’s paraphrases of equal and their probabilities (excluding items with p &lt; .01). create equal create equal equal to create a create to create equality .42 same .03 .06 created .02 .05 conditions .02 .04 playing .02 .03 creating .01 where f1 is a foreign sentence, ei is an English sent</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Declan Groves</author>
<author>Josef Van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Contextual bitext-derived paraphrases in automatic MT evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the SMT Workshop at HLT-NAACL.</booktitle>
<marker>Owczarzak, Groves, Van Genabith, Way, 2006</marker>
<rawString>Karolina Owczarzak, Declan Groves, Josef Van Genabith, and Andy Way. 2006. Contextual bitext-derived paraphrases in automatic MT evaluation. In Proceedings of the SMT Workshop at HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="5527" citStr="Pang et al. (2003)" startWordPosition="820" endWordPosition="823">iques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated structural paraphrases capable of capturing longdistance dependencies. Pang et al. (2003) employed a syntax-based algorithm to align equivalent English sentences by merging corresponding nodes in parse trees and compressing them down into a word lattice. Perhaps the most closely related work is a recent extension to Bannard and Callison-Burch’s paraphrasing method. Zhao et al. (2008b) extended the method so that it is capable of generating richer paraphrase patterns that include part-of-speech slots, rather than simple lexical and phrasal paraphrases. For example, they extracted patterns such as consider NN → take NN into consideration. To accomplish this, Zhao el al. used depende</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4743" citStr="Quirk et al., 2005" startWordPosition="710" endWordPosition="713">chieve computational efficiency, the original proposal used only a single non-terminal label rather than a linguistic grammar. Subsequent work used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002). If two English phrases are in disjoint subtrees in the parse, then the phrasal cohesion constraint prevents them from being aligned to overlapping sequences in the foreign sentence. Other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrase-based systems (Galley et al., 2004; Quirk et al., 2005). Phrase-based rules have also been replaced with synchronous context free grammars (Chiang, 2005) and with tree fragments (Huang and Knight, 2006). A number of techniques for generating paraphrases have employed syntactic information, either in the process of extracting paraphrases from monolingual texts or in the extracted patterns themselves. Lin and Pantel (2001) derived paraphrases based on the distributional similarity of paths in dependency trees. Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. They ex</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
<author>Ioannis Tsochantaridis</author>
<author>Vibhu Mittal</author>
<author>Yi Liu</author>
</authors>
<title>Statistical machine translation for query expansion in answer retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1045" citStr="Riezler et al., 2007" startWordPosition="151" endWordPosition="154">g the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method. 1 Introduction Paraphrases are alternative ways of expressing the same information. Being able to identify or generate paraphrases automatically is useful in a wide range of natural language applications. Recent work has shown how paraphrases can improve question answering through query expansion (Riezler et al., 2007), automatic evaluation of translation and summarization by modeling alternative lexicalization (Kauchak and Barzilay, 2006; Zhou et al., 2006; Owczarzak et al., 2006), and machine translation both by dealing with out of vocabulary words and phrases (Callison-Burch et al., 2006) and by expanding the set of reference translations for minimum error rate training (Madnani et al., 2007). While all applications require the preservation of meaning when a phrase is replaced by its paraphrase, some additionally require the resulting sentence to be grammatical. In this paper we examine the effectiveness</context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007. Statistical machine translation for query expansion in answer retrieval. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Alternative quantier scope in ccg.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="15058" citStr="Steedman, 1999" startWordPosition="2445" endWordPosition="2446">lly proposed. It means that we would not be able to generate paraphrases for phrases such as create equal. Many NLP tasks, such as SMT, which could benefit from paraphrases require broad coverage and may need to paraphrases for phrases which are not syntactic constituents. Complex syntactic labels To generate paraphrases for a wider set of phrases, we change our phrase extraction heuristic again so that it produces phrase pairs for arbitrary spans in the sentence, including spans that aren’t syntactic constituents. We assign every span in a sentence a syntactic label using CCG-style notation (Steedman, 1999), which gives a syntactic role with elements missing on the left and/or right hand sides. J Ij+&apos;in. i+�n, SBP (f1 , e1, A, P) _ {(fj , ei , s) : d(i0,j0) E A : j &lt; j0 &lt; j + m H i &lt; i0 &lt; i + n nE](i0,j0) E A : j &lt; j0 &lt; j + mn H i &lt; i0 &lt; i + n nE]s E CCG-labels(ei+n i , P)} The function CCG-labels describes the set of CCGlabels for the phrase spanning positions i to i + n in create equal VP/(NP/NNS) create equal .92 creating equal .08 VP/(NP/NNS) PP create equal .96 promote equal .03 establish fair .01 VP/(NP/NNS) PP PP create equal .80 creating equal .10 provide equal .06 create genuinely fair </context>
</contexts>
<marker>Steedman, 1999</marker>
<rawString>Mark Steedman. 1999. Alternative quantier scope in ccg. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="20436" citStr="Stolcke, 2002" startWordPosition="3390" endWordPosition="3391">roparl corpus (Koehn, 2005). We used ten parallel corpora between English and (each of) Danish, Dutch, Finnish, French, German, Greek, Italian, Portuguese, Spanish, and Swedish, with approximately 30 million words per language for a total of 315 million English words. Automatic word alignments were created for these using Giza++ (Och and Ney, 2003). The English side of each parallel corpus was parsed using the Bikel parser (Bikel, 2002). A total of 1.6 million unique sentences were parsed. A trigram language model was trained on these English sentences using the SRI language modeling toolkit (Stolcke, 2002). The paraphrase model and language model for the Bannard and Callison-Burch (2005) baseline were trained on the same data to ensure a fair comparison. 4.2 Test phrases The test set was the English portion of test sets used in the shared translation task of the ACL2007 Workshop on Statistical Machine Translation (Callison-Burch et al., 2007). The test sentences were also parsed with the Bikel parser. The phrases to be evaluated were selected such that there was an even balance of phrase lengths (from one word long up to five words long), with half of the phrases being valid syntactic constitue</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, Denver, Colorado, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Eyal Shnarch</author>
<author>Ido Dagan</author>
</authors>
<title>Instance-based evaluation of entailment rule acquisition.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="24962" citStr="Szpektor et al., 2007" startWordPosition="4129" endWordPosition="4132">ch test phrase and substituted the phrase with automatically-generated paraphrases. Annotators judged whether the paraphrases had the same meaning as the original and whether the resulting sentences were grammatical. They assigned two values to each sentence using the 5-point scales given in Table 5. We considered an item to have the same meaning if it was assigned a score of 3 or greater, and to be grammatical if it was assigned a score of 4 or 5. We evaluated several instances of a phrase when it occurred multiple times in the test corpus, since paraphrase quality can vary based on context (Szpektor et al., 2007). There were an average of 3.1 instances for each phrase, with a maximum of 6. There were a total of 1,195 sentences that para202 phrases were substituted into, with a total of 8,422 judgements collected. Note that 7 different paraphrases were judged on average for every instance. This is because annotators judged paraphrases for eight conditions, and because we collected judgments for the 5-best paraphrases for many of the conditions. We measured inter-annotator agreement with the Kappa statistic (Carletta, 1996) using the 1,391 items that two annotators scored in common. The two annotators a</context>
</contexts>
<marker>Szpektor, Shnarch, Dagan, 2007</marker>
<rawString>Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. Instance-based evaluation of entailment rule acquisition. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="3836" citStr="Wu (1997)" startWordPosition="576" endWordPosition="577">judged to be correct. This paper is structured as follows: Section 2 describes related work in syntactic constraints on phrase-based SMT and work utilizing syntax in paraphrase discovery. Section 3 details the problems with extracting paraphrases from parallel corpora and our improvements to the technique. Section 4 describes our experimental design and evaluation methodology. Section 5 gives the results of our experiments, and Section 6 discusses their implications. 2 Related work A number of research efforts have focused on employing syntactic constraints in statistical machine translation. Wu (1997) introduced the inversion transduction grammar formalism which treats translation as a process of parallel parsing of the source and target language via a synchronized grammar. The synchronized grammar places constraints on which words can be aligned across bilingual sentence pairs. To achieve computational efficiency, the original proposal used only a single non-terminal label rather than a linguistic grammar. Subsequent work used more articulated parses to improve alignment quality by applying cohesion constraints (Fox, 2002; Lin and Cherry, 2002). If two English phrases are in disjoint subt</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Cheng Niu</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Combining multiple resources to improve SMT-based paraphrasing model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT.</booktitle>
<contexts>
<context position="5823" citStr="Zhao et al. (2008" startWordPosition="866" endWordPosition="869">ilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated structural paraphrases capable of capturing longdistance dependencies. Pang et al. (2003) employed a syntax-based algorithm to align equivalent English sentences by merging corresponding nodes in parse trees and compressing them down into a word lattice. Perhaps the most closely related work is a recent extension to Bannard and Callison-Burch’s paraphrasing method. Zhao et al. (2008b) extended the method so that it is capable of generating richer paraphrase patterns that include part-of-speech slots, rather than simple lexical and phrasal paraphrases. For example, they extracted patterns such as consider NN → take NN into consideration. To accomplish this, Zhao el al. used dependency parses on the English side of the parallel corpus. Their work differs from the work presented in this paper because their syntactic constraints applied to slots within paraphrase patters, and our constraints apply to the paraphrases themselves. 3 Paraphrasing with parallel corpora Bannard an</context>
<context position="26797" citStr="Zhao et al. (2008" startWordPosition="4440" endWordPosition="4443">et of paraphrases generated for the test set. This includes the 3.7 million paraphrases generated by the baseline method and the 3.5 million paraphrases generated with syntactic constraints. • The code that we used to produce these paraphrases and the complete data sets (including all 10 word-aligned parallel corpora along with their English parses), so that researchers can extract paraphrases for new sets of phrases. • The manual judgments about paraphrase quality. These may be useful as development material for setting the weights of a log-linear formulation of paraphrasing, as suggested in Zhao et al. (2008a). 5 Results Table 6 summarizes the results of the manual evaluation. We can observe a strong trend in the syntactically constrained approaches performing better 2Available from http://cs.jhu.edu/˜ccb/. correct correct both meaning grammar correct Baseline .56 .35 .30 Baseline+LM .46 .44 .36 Extraction Constraints .62 .57 .46 Extraction Const+LM .60 .65 .50 Substitution Constraints .60 .60 .50 Substitution Const+LM .61 .68 .54 Avg Substitution Const .62 .61 .51 Avg Substit Const+LM .61 .68 .55 Table 6: The results of the manual evaluation for each of the eight conditions. Correct meaning is t</context>
</contexts>
<marker>Zhao, Niu, Zhou, Liu, Li, 2008</marker>
<rawString>Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng Li. 2008a. Combining multiple resources to improve SMT-based paraphrasing model. In Proceedings of ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Pivot approach for extracting paraphrase patterns from bilingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT.</booktitle>
<contexts>
<context position="5823" citStr="Zhao et al. (2008" startWordPosition="866" endWordPosition="869">ilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. They extracted paraphrase patterns that incorporate this information. Ibrahim et al. (2003) generated structural paraphrases capable of capturing longdistance dependencies. Pang et al. (2003) employed a syntax-based algorithm to align equivalent English sentences by merging corresponding nodes in parse trees and compressing them down into a word lattice. Perhaps the most closely related work is a recent extension to Bannard and Callison-Burch’s paraphrasing method. Zhao et al. (2008b) extended the method so that it is capable of generating richer paraphrase patterns that include part-of-speech slots, rather than simple lexical and phrasal paraphrases. For example, they extracted patterns such as consider NN → take NN into consideration. To accomplish this, Zhao el al. used dependency parses on the English side of the parallel corpus. Their work differs from the work presented in this paper because their syntactic constraints applied to slots within paraphrase patters, and our constraints apply to the paraphrases themselves. 3 Paraphrasing with parallel corpora Bannard an</context>
<context position="26797" citStr="Zhao et al. (2008" startWordPosition="4440" endWordPosition="4443">et of paraphrases generated for the test set. This includes the 3.7 million paraphrases generated by the baseline method and the 3.5 million paraphrases generated with syntactic constraints. • The code that we used to produce these paraphrases and the complete data sets (including all 10 word-aligned parallel corpora along with their English parses), so that researchers can extract paraphrases for new sets of phrases. • The manual judgments about paraphrase quality. These may be useful as development material for setting the weights of a log-linear formulation of paraphrasing, as suggested in Zhao et al. (2008a). 5 Results Table 6 summarizes the results of the manual evaluation. We can observe a strong trend in the syntactically constrained approaches performing better 2Available from http://cs.jhu.edu/˜ccb/. correct correct both meaning grammar correct Baseline .56 .35 .30 Baseline+LM .46 .44 .36 Extraction Constraints .62 .57 .46 Extraction Const+LM .60 .65 .50 Substitution Constraints .60 .60 .50 Substitution Const+LM .61 .68 .54 Avg Substitution Const .62 .61 .51 Avg Substit Const+LM .61 .68 .55 Table 6: The results of the manual evaluation for each of the eight conditions. Correct meaning is t</context>
</contexts>
<marker>Zhao, Wang, Liu, Li, 2008</marker>
<rawString>Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li. 2008b. Pivot approach for extracting paraphrase patterns from bilingual corpora. In Proceedings of ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Reevaluating machine translation results with paraphrase support.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1186" citStr="Zhou et al., 2006" startWordPosition="170" endWordPosition="173"> phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method. 1 Introduction Paraphrases are alternative ways of expressing the same information. Being able to identify or generate paraphrases automatically is useful in a wide range of natural language applications. Recent work has shown how paraphrases can improve question answering through query expansion (Riezler et al., 2007), automatic evaluation of translation and summarization by modeling alternative lexicalization (Kauchak and Barzilay, 2006; Zhou et al., 2006; Owczarzak et al., 2006), and machine translation both by dealing with out of vocabulary words and phrases (Callison-Burch et al., 2006) and by expanding the set of reference translations for minimum error rate training (Madnani et al., 2007). While all applications require the preservation of meaning when a phrase is replaced by its paraphrase, some additionally require the resulting sentence to be grammatical. In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpora (Bannard and Call</context>
</contexts>
<marker>Zhou, Lin, Hovy, 2006</marker>
<rawString>Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Reevaluating machine translation results with paraphrase support. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>