<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018551">
<title confidence="0.99983">
A Comparative Study of Word Co-occurrence for Term Clustering
in Language Model-based Sentence Retrieval
</title>
<author confidence="0.971199">
Saeedeh Momtazi
</author>
<affiliation confidence="0.912297333333333">
Spoken Language Systems
Saarland University
saeedeh.momtazi
</affiliation>
<email confidence="0.970991">
@lsv.uni-saarland.de
</email>
<author confidence="0.970238">
Sanjeev Khudanpur
</author>
<affiliation confidence="0.900252333333333">
Center for Language
and Speech Processing
Johns Hopkins University
</affiliation>
<email confidence="0.993285">
khudanpur@jhu.edu
</email>
<author confidence="0.990232">
Dietrich Klakow
</author>
<affiliation confidence="0.891491333333333">
Spoken Language Systems
Saarland University
dietrich.klakow
</affiliation>
<email confidence="0.984828">
@lsv.uni-saarland.de
</email>
<sectionHeader confidence="0.99544" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999880555555556">
Sentence retrieval is a very important part of
question answering systems. Term clustering,
in turn, is an effective approach for improving
sentence retrieval performance: the more simi-
lar the terms in each cluster, the better the per-
formance of the retrieval system. A key step in
obtaining appropriate word clusters is accurate
estimation of pairwise word similarities, based
on their tendency to co-occur in similar con-
texts. In this paper, we compare four differ-
ent methods for estimating word co-occurrence
frequencies from two different corpora. The re-
sults show that different, commonly-used con-
texts for defining word co-occurrence differ
significantly in retrieval performance. Using an
appropriate co-occurrence criterion and corpus
is shown to improve the mean average preci-
sion of sentence retrieval form 36.8% to 42.1%.
</bodyText>
<sectionHeader confidence="0.969255" genericHeader="method">
1 Corpus-Driven Clustering of Terms
</sectionHeader>
<bodyText confidence="0.999973945945946">
Since the search in Question Answering (QA) is con-
ducted over smaller segments of text than in docu-
ment retrieval, the problems of data sparsity and ex-
act matching become more critical. The idea of using
class-based language model by applying term clus-
tering, proposed by Momtazi and Klakow (2009), is
found to be effective in overcoming these problems.
Term clustering has a very long history in natu-
ral language processing. The idea was introduced
by Brown et al. (1992) and used in different appli-
cations, including speech recognition, named entity
tagging, machine translation, query expansion, text
categorization, and word sense disambiguation. In
most of the studies in term clustering, one of several
well-know notions of co-occurrence—appearing in
the same document, in the same sentence or follow-
ing the same word—has been used to estimate term
similarity. However, to the best of our knowledge,
none of them explored the relationship between dif-
ferent notions of co-occurrence and the effectiveness
of their resulting clusters in an end task.
In this research, we present a comprehensive study
of how different notions of co-occurrence impact re-
trieval performance. To this end, the Brown algo-
rithm (Brown et al., 1992) is applied to pairwise word
co-occurrence statistics based on different definitions
of word co-occurrence. Then, the word clusters are
used in a class-based language model for sentence
retrieval. Additionally, impact of corpus size and do-
main on co-occurrence estimation is studied.
The paper is organized as follows. In Section 2,
we give a brief description of class-based language
model for sentence retrieval and the Brown word
clustering algorithm. Section 3 presents different
methods for estimating the word co-occurrence. In
Section 4, experimental results are presented. Fi-
nally, Section 5 summarizes the paper.
</bodyText>
<sectionHeader confidence="0.961603" genericHeader="method">
2 Term Clustering Method and Application
</sectionHeader>
<bodyText confidence="0.966925666666667">
In language model-based sentence retrieval, the prob-
ability P(QIS) of generating query Q conditioned on
a candidate sentence S is first calculated. Thereafter
sentences in the search collection are ranked in de-
scending order of this probability. For word-based
unigram, P(QIS) is estimated as
</bodyText>
<equation confidence="0.994884">
P(Q|S) = � P(qi|S), (1)
i=1...M
</equation>
<bodyText confidence="0.9774625">
where M is the number of query terms, qi denotes the
ith query term in Q, and S is the sentence model.
</bodyText>
<page confidence="0.988146">
325
</page>
<subsubsectionHeader confidence="0.578225">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 325–328,
</subsubsectionHeader>
<subsectionHeader confidence="0.276546">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.996695">
For class-based unigrams, P(Q|S) is computed
using only the cluster labels of the query terms as
</bodyText>
<equation confidence="0.999394">
P(Q|S) = � P(qi|Cqi, S)P(Cqi|S), (2)
i=1...M
</equation>
<bodyText confidence="0.999440612903226">
where Cqi is the cluster that contains qi and
P(qi|Cqi, S) is the emission probability of the
ith query term given its cluster and the sen-
tence. P(Cqi|S) is analogous to the sentence model
P(qi|S) in (1), but is based on clusters instead of
terms. To calculate P(Cqi|S), each cluster is con-
sidered an atomic entity, with Q and S interpreted as
sequences of such entities.
In order to cluster lexical items, we use the al-
gorithm proposed by Brown et al (1992), as imple-
mented in the SRILM toolkit (Stolcke, 2002). The al-
gorithm requires an input corpus statistics in the form
(w, w0, fww0), where fww0 is the number of times the
word w0 is seen in the context w. Both w and w0 are
assumed to come from a common vocabulary. Be-
ginning with each vocabulary item in a separate clus-
ter, a bottom-up approach is used to merge the pair of
clusters that minimizes the loss in Average Mutual In-
formation (AMI) between the word cluster Cw0 and
its context cluster Cw. Different words seen in the
same contexts are good candidates for merger, as are
different contexts in which the same words are seen.
While originally proposed with bigram statistics,
the algorithm is agnostic to the definition of co-
occurrence. E.g. if (w, w0) are verb-object pairs,
the algorithm clusters verbs based on their selectional
preferences, if fww0 is the number of times w and w0
appear in the same document, it will produce seman-
tically (or topically) related word-clusters, etc.
Several notions of co-occurrence have been used
in the literature to cluster words, as described next.
</bodyText>
<sectionHeader confidence="0.872801" genericHeader="method">
3 Notions of Word Co-occurrence
</sectionHeader>
<subsectionHeader confidence="0.935537">
Co-occurrence in a Document
</subsectionHeader>
<bodyText confidence="0.99996319047619">
If two content words w and w0 are seen in the
same document, they are usually topically related. In
this notion of co-occurrence, how near or far away
from each other they are in the document is irrele-
vant, as is their order of appearance in the document.
Document-wise co-occurrence has been successfully
used in many NLP applications such as automatic
thesaurus generation (Manning et al., 2008)
Statistics of document-wise co-occurrence may be
collected in two different ways. In the first case,
fww0 = fw0w is simply the number of documents that
contain both w and w0. This is usually the notion
used in ad hoc retrieval. Alternatively, we may want
to treat each instance of w0 in a document that con-
tains an instance of w to be a co-occurrence event.
Therefore if w0 appears three times in a document
that contains two instances of w, the former method
counts it as one co-occurrence, while the latter as six
co-occurrences. We use the latter statistic, since we
are concerned with retrieving sentence sized “docu-
ments,” wherein a repeated word is more significant.
</bodyText>
<subsectionHeader confidence="0.921421">
Co-occurrence in a Sentence
</subsectionHeader>
<bodyText confidence="0.983081277777778">
Since topic changes sometimes happen within a
single document, and our end task is sentence re-
trieval, we also investigate the notion of word co-
occurrence in a smaller segment of text such as a
sentence. In contrast to the document-wise model,
sentence-wise co-occurrence does not consider whole
documents, and only concerns itself with the number
of times that two words occur in the same sentence.
Co-occurrence in a Window of Text
The window-wise co-occurrence statistic is an even
narrower notion of context, considering only terms in
a window surrounding w0. Specifically, a window of
a fixed size is moved along the text, and fww0 is set
as the number of times both w and w0 appear in the
window. Since the window size is a free parameter,
different sizes may be applied. In our experiments we
use two window sizes, 2 and 5, that have been studied
in related research (Church and Hanks, 1990).
</bodyText>
<subsectionHeader confidence="0.9187">
Co-occurrence in a Syntactic Relationship
</subsectionHeader>
<bodyText confidence="0.999796357142857">
Another notion of word similarity derives from
having the same syntactic relationship with the con-
text w. This syntax-wise co-occurrence statistic is
similar to the sentence-wise co-occurrence, in that
co-occurrence is defined at the sentence level. How-
ever, in contrast to the sentence-wise model, w and
w0 are said to co-occur only if there is a syntactic re-
lation between them in that sentence. E.g., this type
of co-occurrence can help cluster nouns that are used
as objects of same verb, such as ‘tea’, ‘water’, and
‘cola,’ which all are used with the verb ‘drink’.
To gather such statistics, all sentences in the corpus
must be syntactically parsed. We found that a depen-
dency parser is an appropriate tool for our goal: it
</bodyText>
<page confidence="0.99571">
326
</page>
<bodyText confidence="0.999854625">
directly captures dependencies between words with-
out the mediation of any virtual (nonterminal) nodes.
Having all sentences in the parsed format, f,,,,,,, is de-
fined as the number of times that the words w and w&apos;
have a parent-child relationship of any syntactic type
in the dependency parse tree. For our experiments we
use MINIPAR (Lin, 1998) to parse the whole corpus
due to its robustness and speed.
</bodyText>
<sectionHeader confidence="0.996578" genericHeader="method">
4 Sentence Retrieval Experiments
</sectionHeader>
<subsectionHeader confidence="0.999869">
4.1 Derivatives of the TREC QA Data Sets
</subsectionHeader>
<bodyText confidence="0.999790476190476">
The set of questions from the TREC 2006 QA track1
was used as the test data to evaluate our models,
while the TREC 2005 set was used for development.
The TREC 2006 QA task contains 75 question-
series, each on one topic, for a total of 403 factoid
questions which is used as queries for sentence re-
trieval. For sentence-level relevance judgments, the
Question Answer Sentence Pair corpus of Kaisser
and Lowe (2008) was used. All the documents
that contain relevant sentences are from the NIST
AQUAINT1 corpus.
QA systems typically employ sentence retrieval af-
ter initial, high quality document retrieval. To simu-
late this, we created a separate search collection for
each question using all sentences from all documents
relevant to the topic (question-series) from which the
question was derived. On average, there are 17 rel-
evant documents per topic, many not relevant to the
question itself: they may be relevant to another ques-
tion. So the sentence search collection is realistic,
even if somewhat optimistic.
</bodyText>
<subsectionHeader confidence="0.840506">
4.2 Corpora for Term Clustering
</subsectionHeader>
<bodyText confidence="0.9998101">
We investigated two different corpora2, AQUAINT1
and Google n-grams, to obtain word co-occurrence
statistics for term clustering. Based on this we can
also evaluate the impact of corpus size and corpus
domain on the result of term clustering.
AQUAINT1 consists of English newswire text ex-
tracted from the Xinhua, the New York Times and the
Associated Press Worldstream News Services.
The Google n-gram counts were generated from
publicly accessible English web pages. Since there is
</bodyText>
<footnote confidence="0.998705333333333">
1See http://trec.nist.gov.
2See catalog numbers LDC2002T31 and LDC2006T13 re-
spectively at http://www.ldc.upenn.edu/Catalog.
</footnote>
<table confidence="0.99975625">
Corpus Co-occurrence # Word Pairs
AQUAINT1 document 368,109,133
AQUAINT1 sentence 104,084,473
AQUAINT1 syntax 12,343,947
AQUAINT1 window-5 46,307,650
AQUAINT1 window-2 14,093,661
Google n-grams window-5 12,005,479
Google n-grams window-2 328,431,792
</table>
<tableCaption confidence="0.999157">
Table 1: Statistics for different notions of co-occurrence.
</tableCaption>
<bodyText confidence="0.999466777777778">
no possibility of extracting document-wise, sentence-
wise or syntax-wise co-occurrence statistics from the
Google n-gram corpus, we only collect window-wise
statistics to the extent available in the corpus.
Table 1 shows the number of word pairs extracted
from the two corpora with different definitions of co-
occurrence. The statistics only include word pairs
for which both constituent words are present in the
35,000 word vocabulary of our search collection.
</bodyText>
<subsectionHeader confidence="0.999912">
4.3 Sentence Retrieval Results and Discussion
</subsectionHeader>
<bodyText confidence="0.965188074074074">
Sentence retrieval performance for term clustering
using different definitions of word co-occurrence is
shown in Figure 1. Since the Brown algorithm re-
quires specifying the number of clusters, tests were
conducted for 50, 100, 200, 500, and 1000 clusters
of the term vocabulary. The baseline system is the
word-based sentence retrieval model of Equation (1).
Figure 1(a) shows the Mean Average Precision
(MAP) for class-based sentence retrieval of Equation
(2) using clusters based on different co-occurrence
statistics from AQUAINT1. Note that
(i) the best result achieved by sentence-wise co-
occurence is better the best result of document-
wise, perhaps due to more local and relevant in-
formation that it captures;
(ii) all the results achieved by syntax-wise co-
occurrence are better than sentence-wise, indi-
cating that merely co-occurring in a sentence
is not very indicative of word similarity, while
relations extracted from syntactic structure im-
prove system performance significantly;
(iii) window-2 significantly outperforms all other
notions of co-occurrence; i.e., the bigram statis-
tics achieve the best clustering results. In com-
parison, window-5 has the worst results, with
performance very close to baseline.
Although window-5 co-occurrence has been reported
</bodyText>
<page confidence="0.983392">
327
</page>
<figure confidence="0.99817044">
document sentence window2 window5 syntax base-line
AQUAINT-window2 Google-window2 Google-window5 base-line
0.43
0.42
0.41
0.40
0.39
0.38
0.37
0.36
0.35
MAP
0.43
0.42
0.41
0.40
0.39
0.38
0.37
0.36
0.35
50 500 50 500
log of number of clusters log of number of clusters
(a) (b)
MAP
</figure>
<figureCaption confidence="0.999997">
Figure 1: MAP of sentence retrieval for different word co-occurrence statistics from AQUAINT1 and Google n-grams.
</figureCaption>
<bodyText confidence="0.997982925925926">
to be effective in other applications, it is not helpful
in sentence retrieval.
Figure 1(b) shows the MAP for class-based sen-
tence retrieval of Equation (2) when window-wise
co-occurrence statistics from the Google n-grams are
used. For better visualization, we repeated the MAP
results using AQUAINT1 window-2 co-occurrence
statistics from Figure 1(a) in 1(b). Note that
(iv) window-2 co-occurrence statistics significantly
outperform window-5 for the Google n-grams,
consistent with results from AQUAINT1;
(v) Google n-gram window-2 co-occurrence statis-
tics consistently result in better MAP than
AQUAINT window-2.
The last result indicates that even though the Google
n-grams are from a different (and much broader) do-
main than the test data, they significantly improve the
system performance due to sheer size. Finally
(vi) Google n-gram window-2 MAP curve is flatter
than AQUAINT window-2; i.e., performance is
not very sensitive to the number of clusters.
The best overall result is from Google window-2
co-occurrence statistics with 100 clusters, achiev-
ing 42.1% MAP while the best result derived
from AQUAINT1 is 41.7% MAP for window-2 co-
occurrence with 100 clusters, and the MAP of the
word-based model (baseline) is 36.8%.
</bodyText>
<sectionHeader confidence="0.994581" genericHeader="conclusions">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999989928571429">
We compared different notions of word co-
occurrence for clustering terms, using document-
wise, sentence-wise, window-wise, and syntax-wise
co-occurrence statistics derived from AQUAINT1.
We found that different notions of co-occurrence sig-
nificantly change the behavior of a sentence retrieval
system, in which window-wise model with size 2
achieves the best result. In addition, Google n-grams
were used for window-wise model to study the im-
pact of corpus size and domain on the clustering re-
sult. The result showed that although the domain of
the Google n-grams is dissimilar to the test set, it
outperforms models derived from AQUAINT1 due to
sheer size.
</bodyText>
<sectionHeader confidence="0.998202" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998835">
Saeedeh Momtazi is funded by the German research
foundation DFG through the International Research
Training Group (IRTG 715).
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998999">
P.F. Brown, V.J.D. Pietra, P.V. Souza, J.C. Lai, and R.L.
Mercer. 1992. Class-based n-gram models of natural
language. Computational Linguistics, 18(4):467–479.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22–29.
M. Kaisser and J.B. Lowe. 2008. Creating a research
collection of question answer sentence pairs with Ama-
zon’s mechanical turk. In Proc. of LREC.
D. Lin. 1998. Dependency-based evaluation of MINI-
PAR. In Proc. of the Evaluation of Parsing Systems
Workshop.
C.D. Manning, P. Raghavan, and H. Sch¨utze. 2008. Intro-
duction to Information Retrieval. Cambridge Univer-
sity Press.
S. Momtazi and D. Klakow. 2009. A word clustering
approach for language model-based sentence retrieval
in question answering systems. In Proc. ofACMCIKM.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. of ICSLP.
</reference>
<page confidence="0.998355">
328
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000104">
<title confidence="0.8835635">A Comparative Study of Word Co-occurrence for Term in Language Model-based Sentence Retrieval Saeedeh Spoken Language</title>
<author confidence="0.463373">Saarland</author>
<email confidence="0.988085">@lsv.uni-saarland.de</email>
<author confidence="0.51609">Sanjeev</author>
<affiliation confidence="0.926253">Center for</affiliation>
<title confidence="0.851433">and Speech</title>
<author confidence="0.928662">Johns Hopkins</author>
<email confidence="0.999601">khudanpur@jhu.edu</email>
<author confidence="0.705574">Dietrich</author>
<affiliation confidence="0.976103">Spoken Language</affiliation>
<address confidence="0.683797">Saarland</address>
<email confidence="0.996681">@lsv.uni-saarland.de</email>
<abstract confidence="0.959336126482213">Sentence retrieval is a very important part of question answering systems. Term clustering, in turn, is an effective approach for improving sentence retrieval performance: the more similar the terms in each cluster, the better the performance of the retrieval system. A key step in obtaining appropriate word clusters is accurate of similarities, based on their tendency to co-occur in similar contexts. In this paper, we compare four different methods for estimating word co-occurrence frequencies from two different corpora. The results show that different, commonly-used contexts for defining word co-occurrence differ significantly in retrieval performance. Using an appropriate co-occurrence criterion and corpus is shown to improve the mean average precision of sentence retrieval form 36.8% to 42.1%. 1 Corpus-Driven Clustering of Terms Since the search in Question Answering (QA) is conducted over smaller segments of text than in document retrieval, the problems of data sparsity and exact matching become more critical. The idea of using class-based language model by applying term clustering, proposed by Momtazi and Klakow (2009), is found to be effective in overcoming these problems. Term clustering has a very long history in natural language processing. The idea was introduced by Brown et al. (1992) and used in different applications, including speech recognition, named entity tagging, machine translation, query expansion, text categorization, and word sense disambiguation. In most of the studies in term clustering, one of several of in the same document, in the same sentence or following the same word—has been used to estimate term similarity. However, to the best of our knowledge, none of them explored the relationship between different notions of co-occurrence and the effectiveness of their resulting clusters in an end task. In this research, we present a comprehensive study of how different notions of co-occurrence impact retrieval performance. To this end, the Brown algorithm (Brown et al., 1992) is applied to pairwise word statistics based on different of word co-occurrence. Then, the word clusters are used in a class-based language model for sentence retrieval. Additionally, impact of corpus size and domain on co-occurrence estimation is studied. The paper is organized as follows. In Section 2, we give a brief description of class-based language model for sentence retrieval and the Brown word clustering algorithm. Section 3 presents different methods for estimating the word co-occurrence. In Section 4, experimental results are presented. Finally, Section 5 summarizes the paper. 2 Term Clustering Method and Application In language model-based sentence retrieval, the probgenerating query on candidate sentence first calculated. Thereafter sentences in the search collection are ranked in descending order of this probability. For word-based estimated as = � i=1...M the number of query terms, the term in and the sentence model. 325 Language Technologies: The 2010 Annual Conference of the North American Chapter of the pages Angeles, California, June 2010. Association for Computational Linguistics class-based unigrams, computed only the labels the query terms as = � i=1...M the cluster that contains S) the emission probability of the term given its cluster and the senanalogous to the sentence model (1), but is based on clusters instead of To calculate each cluster is conan atomic entity, with as sequences of such entities. In order to cluster lexical items, we use the algorithm proposed by Brown et al (1992), as implemented in the SRILM toolkit (Stolcke, 2002). The algorithm requires an input corpus statistics in the form where is the number of timesthe wordis seen in the contextBoth are assumed to come from a common vocabulary. Beginning with each vocabulary item in a separate cluster, a bottom-up approach is used to merge the pair of that minimizes the loss in Mutual Inbetween the word cluster context cluster Different words seen in the same contexts are good candidates for merger, as are different contexts in which the same words are seen. While originally proposed with bigram statistics, algorithm is the definition of co- E.g. if verb-object pairs, the algorithm clusters verbs based on their selectional if is the number of times appear in the same document, it will produce semantically (or topically) related word-clusters, etc. Several notions of co-occurrence have been used in the literature to cluster words, as described next. 3 Notions of Word Co-occurrence Co-occurrence in a Document two content words are seen in the same document, they are usually topically related. In this notion of co-occurrence, how near or far away from each other they are in the document is irrelevant, as is their order of appearance in the document. has been successfully used in many NLP applications such as automatic thesaurus generation (Manning et al., 2008) Statistics of document-wise co-occurrence may be collected in two different ways. In the first case, = simply the number of documents that both This is usually the notion used in ad hoc retrieval. Alternatively, we may want treat each in a document that conan instance of be a co-occurrence event. if appears three times in a document contains two instances of the former method counts it as one co-occurrence, while the latter as six co-occurrences. We use the latter statistic, since we are concerned with retrieving sentence sized “documents,” wherein a repeated word is more significant. Co-occurrence in a Sentence Since topic changes sometimes happen within a single document, and our end task is sentence retrieval, we also investigate the notion of word cooccurrence in a smaller segment of text such as a sentence. In contrast to the document-wise model, does not consider whole documents, and only concerns itself with the number of times that two words occur in the same sentence. Co-occurrence in a Window of Text statistic is an even narrower notion of context, considering only terms in window surrounding Specifically, a window of fixed size is moved along the text, and is set the number of times both appear in the window. Since the window size is a free parameter, different sizes may be applied. In our experiments we use two window sizes, 2 and 5, that have been studied in related research (Church and Hanks, 1990). Co-occurrence in a Syntactic Relationship Another notion of word similarity derives from having the same syntactic relationship with the con- This statistic is similar to the sentence-wise co-occurrence, in that co-occurrence is defined at the sentence level. Howin contrast to the sentence-wise model, said to co-occur only if there is a syntactic relation between them in that sentence. E.g., this type of co-occurrence can help cluster nouns that are used as objects of same verb, such as ‘tea’, ‘water’, and ‘cola,’ which all are used with the verb ‘drink’. To gather such statistics, all sentences in the corpus must be syntactically parsed. We found that a dependency parser is an appropriate tool for our goal: it 326 directly captures dependencies between words without the mediation of any virtual (nonterminal) nodes. all sentences in the parsed format, deas the number of times that the words a parent-child relationship of syntactic type in the dependency parse tree. For our experiments we use MINIPAR (Lin, 1998) to parse the whole corpus due to its robustness and speed. 4 Sentence Retrieval Experiments 4.1 Derivatives of the TREC QA Data Sets set of questions from the TREC 2006 QA was used as the test data to evaluate our models, while the TREC 2005 set was used for development. The TREC 2006 QA task contains 75 questionseries, each on one topic, for a total of 403 factoid questions which is used as queries for sentence retrieval. For sentence-level relevance judgments, the Answer Sentence Pair of Kaisser and Lowe (2008) was used. All the documents that contain relevant sentences are from the NIST AQUAINT1 corpus. QA systems typically employ sentence retrieval after initial, high quality document retrieval. To simuthis, we created a separate collection each question using all sentences from all documents relevant to the topic (question-series) from which the question was derived. On average, there are 17 reldocuments per topic, many to the question itself: they may be relevant to another question. So the sentence search collection is realistic, even if somewhat optimistic. 4.2 Corpora for Term Clustering investigated two different to obtain word co-occurrence statistics for term clustering. Based on this we can also evaluate the impact of corpus size and corpus domain on the result of term clustering. AQUAINT1 consists of English newswire text extracted from the Xinhua, the New York Times and the Associated Press Worldstream News Services. Google counts were generated from publicly accessible English web pages. Since there is catalog numbers LDC2002T31 and LDC2006T13 reat Corpus Co-occurrence # Word Pairs AQUAINT1 document 368,109,133 AQUAINT1 sentence 104,084,473 AQUAINT1 syntax 12,343,947 AQUAINT1 window-5 46,307,650 AQUAINT1 window-2 14,093,661 window-5 12,005,479 window-2 328,431,792 Table 1: Statistics for different notions of co-occurrence. no possibility of extracting document-wise, sentencewise or syntax-wise co-occurrence statistics from the corpus, we only collect window-wise statistics to the extent available in the corpus. Table 1 shows the number of word pairs extracted from the two corpora with different definitions of cooccurrence. The statistics only include word pairs for which both constituent words are present in the 35,000 word vocabulary of our search collection. 4.3 Sentence Retrieval Results and Discussion Sentence retrieval performance for term clustering using different definitions of word co-occurrence is shown in Figure 1. Since the Brown algorithm requires specifying the number of clusters, tests were conducted for 50, 100, 200, 500, and 1000 clusters of the term vocabulary. The baseline system is the word-based sentence retrieval model of Equation (1). 1(a) shows the Average Precision (MAP) for class-based sentence retrieval of Equation (2) using clusters based on different co-occurrence statistics from AQUAINT1. Note that (i) the best result achieved by sentence-wise cooccurence is better the best result of documentwise, perhaps due to more local and relevant information that it captures; (ii) all the results achieved by syntax-wise cooccurrence are better than sentence-wise, indicating that merely co-occurring in a sentence is not very indicative of word similarity, while relations extracted from syntactic structure improve system performance significantly; (iii) window-2 significantly outperforms all other notions of co-occurrence; i.e., the bigram statistics achieve the best clustering results. In comparison, window-5 has the worst results, with performance very close to baseline.</abstract>
<note confidence="0.745528565217391">Although window-5 co-occurrence has been reported 327 document sentence window2 window5 syntax base-line AQUAINT-window2 Google-window2 Google-window5 base-line 0.43 0.42 0.41 0.40 0.39 0.38 0.37 0.36 0.35 MAP 0.43 0.42 0.41 0.40 0.39 0.38 0.37 0.36 0.35</note>
<phone confidence="0.793015">500 500</phone>
<abstract confidence="0.910839514285714">of number of clusters of number of clusters (a) (b) MAP 1: MAP of sentence retrieval for different word co-occurrence statistics from AQUAINT1 and Google to be effective in other applications, it is not helpful in sentence retrieval. Figure 1(b) shows the MAP for class-based sentence retrieval of Equation (2) when window-wise statistics from the Google are used. For better visualization, we repeated the MAP results using AQUAINT1 window-2 co-occurrence statistics from Figure 1(a) in 1(b). Note that (iv) window-2 co-occurrence statistics significantly window-5 for the Google consistent with results from AQUAINT1; Google window-2 co-occurrence statistics consistently result in better MAP than AQUAINT window-2. The last result indicates that even though the Google are from a different (and much broader) domain than the test data, they significantly improve the system performance due to sheer size. Finally Google window-2 MAP curve is flatter than AQUAINT window-2; i.e., performance is not very sensitive to the number of clusters. The best overall result is from Google window-2 co-occurrence statistics with 100 clusters, achieving 42.1% MAP while the best result derived from AQUAINT1 is 41.7% MAP for window-2 cooccurrence with 100 clusters, and the MAP of the word-based model (baseline) is 36.8%. 5 Concluding Remarks We compared different notions of word cooccurrence for clustering terms, using documentwise, sentence-wise, window-wise, and syntax-wise co-occurrence statistics derived from AQUAINT1. We found that different notions of co-occurrence significantly change the behavior of a sentence retrieval system, in which window-wise model with size 2 the best result. In addition, Google were used for window-wise model to study the impact of corpus size and domain on the clustering result. The result showed that although the domain of Google is dissimilar to the test set, it outperforms models derived from AQUAINT1 due to sheer size. Acknowledgments Saeedeh Momtazi is funded by the German research foundation DFG through the International Research Training Group (IRTG 715). References P.F. Brown, V.J.D. Pietra, P.V. Souza, J.C. Lai, and R.L. Mercer. 1992. Class-based n-gram models of natural 18(4):467–479. K.W. Church and P. Hanks. 1990. Word association mutual information, and lexicography. Com- 16(1):22–29. M. Kaisser and J.B. Lowe. 2008. Creating a research collection of question answer sentence pairs with Amamechanical turk. In of D. Lin. 1998. Dependency-based evaluation of MINI- In of the Evaluation of Parsing Systems Manning, P. Raghavan, and H. Sch¨utze. 2008. Introto Information Cambridge University Press. S. Momtazi and D. Klakow. 2009. A word clustering approach for language model-based sentence retrieval question answering systems. In A. Stolcke. 2002. SRILM an extensible language modtoolkit. In of</abstract>
<intro confidence="0.46579">328</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J D Pietra</author>
<author>P V Souza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="1752" citStr="Brown et al. (1992)" startWordPosition="252" endWordPosition="255">criterion and corpus is shown to improve the mean average precision of sentence retrieval form 36.8% to 42.1%. 1 Corpus-Driven Clustering of Terms Since the search in Question Answering (QA) is conducted over smaller segments of text than in document retrieval, the problems of data sparsity and exact matching become more critical. The idea of using class-based language model by applying term clustering, proposed by Momtazi and Klakow (2009), is found to be effective in overcoming these problems. Term clustering has a very long history in natural language processing. The idea was introduced by Brown et al. (1992) and used in different applications, including speech recognition, named entity tagging, machine translation, query expansion, text categorization, and word sense disambiguation. In most of the studies in term clustering, one of several well-know notions of co-occurrence—appearing in the same document, in the same sentence or following the same word—has been used to estimate term similarity. However, to the best of our knowledge, none of them explored the relationship between different notions of co-occurrence and the effectiveness of their resulting clusters in an end task. In this research, </context>
<context position="4403" citStr="Brown et al (1992)" startWordPosition="672" endWordPosition="675">ional Linguistics For class-based unigrams, P(Q|S) is computed using only the cluster labels of the query terms as P(Q|S) = � P(qi|Cqi, S)P(Cqi|S), (2) i=1...M where Cqi is the cluster that contains qi and P(qi|Cqi, S) is the emission probability of the ith query term given its cluster and the sentence. P(Cqi|S) is analogous to the sentence model P(qi|S) in (1), but is based on clusters instead of terms. To calculate P(Cqi|S), each cluster is considered an atomic entity, with Q and S interpreted as sequences of such entities. In order to cluster lexical items, we use the algorithm proposed by Brown et al (1992), as implemented in the SRILM toolkit (Stolcke, 2002). The algorithm requires an input corpus statistics in the form (w, w0, fww0), where fww0 is the number of times the word w0 is seen in the context w. Both w and w0 are assumed to come from a common vocabulary. Beginning with each vocabulary item in a separate cluster, a bottom-up approach is used to merge the pair of clusters that minimizes the loss in Average Mutual Information (AMI) between the word cluster Cw0 and its context cluster Cw. Different words seen in the same contexts are good candidates for merger, as are different contexts i</context>
</contexts>
<marker>Brown, Pietra, Souza, Lai, Mercer, 1992</marker>
<rawString>P.F. Brown, V.J.D. Pietra, P.V. Souza, J.C. Lai, and R.L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="7556" citStr="Church and Hanks, 1990" startWordPosition="1212" endWordPosition="1215">s not consider whole documents, and only concerns itself with the number of times that two words occur in the same sentence. Co-occurrence in a Window of Text The window-wise co-occurrence statistic is an even narrower notion of context, considering only terms in a window surrounding w0. Specifically, a window of a fixed size is moved along the text, and fww0 is set as the number of times both w and w0 appear in the window. Since the window size is a free parameter, different sizes may be applied. In our experiments we use two window sizes, 2 and 5, that have been studied in related research (Church and Hanks, 1990). Co-occurrence in a Syntactic Relationship Another notion of word similarity derives from having the same syntactic relationship with the context w. This syntax-wise co-occurrence statistic is similar to the sentence-wise co-occurrence, in that co-occurrence is defined at the sentence level. However, in contrast to the sentence-wise model, w and w0 are said to co-occur only if there is a syntactic relation between them in that sentence. E.g., this type of co-occurrence can help cluster nouns that are used as objects of same verb, such as ‘tea’, ‘water’, and ‘cola,’ which all are used with the</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K.W. Church and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kaisser</author>
<author>J B Lowe</author>
</authors>
<title>Creating a research collection of question answer sentence pairs with Amazon’s mechanical turk.</title>
<date>2008</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="9224" citStr="Kaisser and Lowe (2008)" startWordPosition="1494" endWordPosition="1497"> in the dependency parse tree. For our experiments we use MINIPAR (Lin, 1998) to parse the whole corpus due to its robustness and speed. 4 Sentence Retrieval Experiments 4.1 Derivatives of the TREC QA Data Sets The set of questions from the TREC 2006 QA track1 was used as the test data to evaluate our models, while the TREC 2005 set was used for development. The TREC 2006 QA task contains 75 questionseries, each on one topic, for a total of 403 factoid questions which is used as queries for sentence retrieval. For sentence-level relevance judgments, the Question Answer Sentence Pair corpus of Kaisser and Lowe (2008) was used. All the documents that contain relevant sentences are from the NIST AQUAINT1 corpus. QA systems typically employ sentence retrieval after initial, high quality document retrieval. To simulate this, we created a separate search collection for each question using all sentences from all documents relevant to the topic (question-series) from which the question was derived. On average, there are 17 relevant documents per topic, many not relevant to the question itself: they may be relevant to another question. So the sentence search collection is realistic, even if somewhat optimistic. 4</context>
</contexts>
<marker>Kaisser, Lowe, 2008</marker>
<rawString>M. Kaisser and J.B. Lowe. 2008. Creating a research collection of question answer sentence pairs with Amazon’s mechanical turk. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Proc. of the Evaluation of Parsing Systems Workshop.</booktitle>
<contexts>
<context position="8678" citStr="Lin, 1998" startWordPosition="1400" endWordPosition="1401">as objects of same verb, such as ‘tea’, ‘water’, and ‘cola,’ which all are used with the verb ‘drink’. To gather such statistics, all sentences in the corpus must be syntactically parsed. We found that a dependency parser is an appropriate tool for our goal: it 326 directly captures dependencies between words without the mediation of any virtual (nonterminal) nodes. Having all sentences in the parsed format, f,,,,,,, is defined as the number of times that the words w and w&apos; have a parent-child relationship of any syntactic type in the dependency parse tree. For our experiments we use MINIPAR (Lin, 1998) to parse the whole corpus due to its robustness and speed. 4 Sentence Retrieval Experiments 4.1 Derivatives of the TREC QA Data Sets The set of questions from the TREC 2006 QA track1 was used as the test data to evaluate our models, while the TREC 2005 set was used for development. The TREC 2006 QA task contains 75 questionseries, each on one topic, for a total of 403 factoid questions which is used as queries for sentence retrieval. For sentence-level relevance judgments, the Question Answer Sentence Pair corpus of Kaisser and Lowe (2008) was used. All the documents that contain relevant sen</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Dependency-based evaluation of MINIPAR. In Proc. of the Evaluation of Parsing Systems Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>P Raghavan</author>
<author>H Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>C.D. Manning, P. Raghavan, and H. Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Momtazi</author>
<author>D Klakow</author>
</authors>
<title>A word clustering approach for language model-based sentence retrieval in question answering systems.</title>
<date>2009</date>
<booktitle>In Proc. ofACMCIKM.</booktitle>
<contexts>
<context position="1577" citStr="Momtazi and Klakow (2009)" startWordPosition="222" endWordPosition="225">t corpora. The results show that different, commonly-used contexts for defining word co-occurrence differ significantly in retrieval performance. Using an appropriate co-occurrence criterion and corpus is shown to improve the mean average precision of sentence retrieval form 36.8% to 42.1%. 1 Corpus-Driven Clustering of Terms Since the search in Question Answering (QA) is conducted over smaller segments of text than in document retrieval, the problems of data sparsity and exact matching become more critical. The idea of using class-based language model by applying term clustering, proposed by Momtazi and Klakow (2009), is found to be effective in overcoming these problems. Term clustering has a very long history in natural language processing. The idea was introduced by Brown et al. (1992) and used in different applications, including speech recognition, named entity tagging, machine translation, query expansion, text categorization, and word sense disambiguation. In most of the studies in term clustering, one of several well-know notions of co-occurrence—appearing in the same document, in the same sentence or following the same word—has been used to estimate term similarity. However, to the best of our kn</context>
</contexts>
<marker>Momtazi, Klakow, 2009</marker>
<rawString>S. Momtazi and D. Klakow. 2009. A word clustering approach for language model-based sentence retrieval in question answering systems. In Proc. ofACMCIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="4456" citStr="Stolcke, 2002" startWordPosition="683" endWordPosition="684">puted using only the cluster labels of the query terms as P(Q|S) = � P(qi|Cqi, S)P(Cqi|S), (2) i=1...M where Cqi is the cluster that contains qi and P(qi|Cqi, S) is the emission probability of the ith query term given its cluster and the sentence. P(Cqi|S) is analogous to the sentence model P(qi|S) in (1), but is based on clusters instead of terms. To calculate P(Cqi|S), each cluster is considered an atomic entity, with Q and S interpreted as sequences of such entities. In order to cluster lexical items, we use the algorithm proposed by Brown et al (1992), as implemented in the SRILM toolkit (Stolcke, 2002). The algorithm requires an input corpus statistics in the form (w, w0, fww0), where fww0 is the number of times the word w0 is seen in the context w. Both w and w0 are assumed to come from a common vocabulary. Beginning with each vocabulary item in a separate cluster, a bottom-up approach is used to merge the pair of clusters that minimizes the loss in Average Mutual Information (AMI) between the word cluster Cw0 and its context cluster Cw. Different words seen in the same contexts are good candidates for merger, as are different contexts in which the same words are seen. While originally pro</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proc. of ICSLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>