<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.049689">
<title confidence="0.980392">
Adaptive Tutorial Dialogue Systems Using Deep NLP Techniques
</title>
<author confidence="0.9432395">
Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow,
Manuel Marques-Pita, Colin Matheson and Johanna D. Moore
</author>
<affiliation confidence="0.9967015">
ICCS-HCRC, School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.954916">
Edinburgh, EH8 9LW, United Kingdom
</address>
<email confidence="0.997297">
(mdzikovs,ccallawa,efarrow,mmpita,colin,jmoore)@inf.ed.ac.uk*
</email>
<sectionHeader confidence="0.995599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964444444444">
We present tutorial dialogue systems in
two different domains that demonstrate
the use of dialogue management and deep
natural language processing techniques.
Generation techniques are used to produce
natural sounding feedback adapted to stu-
dent performance and the dialogue his-
tory, and context is used to interpret ten-
tative answers phrased as questions.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.981105947368421">
Intelligent tutoring systems help students improve
learning compared to reading textbooks, though not
quite as much as human tutors (Anderson et al.,
1995). The specific properties of human-human di-
alogue that help students learn are still being stud-
ied, but the proposed features important for learn-
ing include allowing students to explain their actions
(Chi et al., 1994), adapting tutorial feedback to the
learner’s level, and engagement/affect. Some tuto-
rial dialogue systems use NLP techniques to analyze
student responses to “why” questions. (Aleven et al.,
2001; Jordan et al., 2006). However, for remediation
they revert to scripted dialogue, relying on short-
answer questions and canned feedback. The result-
ing dialogue may be redundant in ways detrimental
to student understanding (Jordan et al., 2005) and
allows for only limited adaptivity (Jordan, 2004).
Ihis work was supported under the 6th Framework Pro-
gramme of the European Commission, Ref. IST-507826, and
by a grant from The Office of Naval Research N000149910165.
We demonstrate two tutorial dialogue systems
that use techniques from task-oriented dialogue sys-
tems to improve the interaction. The systems are
built using the Information State Update approach
(Larsson and Traum, 2000) for dialogue manage-
ment and generic components for deep natural lan-
guage understanding and generation. Tutorial feed-
back is generated adaptively based on the student
model, and the interpretation is used to process
explanations and to differentiate between student
queries and hedged answers phrased as questions.
The systems are intended for testing hypotheses
about tutoring. By comparing student learning gains
between versions of the same system using different
tutoring strategies, as well as between the systems
and human tutors, we can test hypotheses about the
role of factors such as free natural language input,
adaptivity and student affect.
</bodyText>
<sectionHeader confidence="0.62858" genericHeader="method">
2 The BEEDIFF Tutor
</sectionHeader>
<bodyText confidence="0.999895">
The BEEDIFF tutor helps students solve symbolic
differentiation problems, a procedural task. Solu-
tion graphs generated by a domain reasoner are used
to interpret student actions and to generate feed-
back.1 Student input is relatively limited and con-
sists mostly of mathematical formulas, but the sys-
tem generates adaptive feedback based on the notion
of student performance and on the dialogue history.
For example, if an average student asks for a hint
on differentiating sin(x2), the first level of feedback
may be “Think about which rule to apply”, which
</bodyText>
<footnote confidence="0.73563">
1Solution graphs are generated automatically for arbitrary
expressions, with no limit on the complexity of expressions ex-
cept for possible efficiency considerations.
</footnote>
<page confidence="0.815917">
5
</page>
<author confidence="0.2153">
NAACL HLT Demonstration Program, pages 5–6,
</author>
<affiliation confidence="0.71838">
Rochester, New York, USA, April 2007. c�2007 Association for Computational Linguistics
</affiliation>
<bodyText confidence="0.9998855">
can then be specialized to “Use the chain rule” and
then to giving away the complete answer. For stu-
dents with low performance, more specific feed-
back can be given from the start. The same strat-
egy (based on an initial corpus analysis) is used in
producing feedback after incorrect answers, and we
intend to use the system to evaluate its effectiveness.
The feedback is generated automatically from a
single diagnosis and generation techniques are used
to produce appropriate discourse cues. For example,
when a student repeats the same mistake, the feed-
back may be “You’ve differentiated the inner layer
correctly, but you’re still missing the minus sign”.
The two clauses are joined by a contrast relationship,
and the second indicates that an error was repeated
by using the adverbial “still”.
</bodyText>
<sectionHeader confidence="0.990143" genericHeader="method">
3 The BEETLE Tutor
</sectionHeader>
<bodyText confidence="0.99998366">
The BEETLE tutor is designed to teach students ba-
sic electricity and electronics concepts. Unlike the
BEEDIFF tutor, the BEETLE tutor is built around
a pre-planned course where the students alternate
reading with exercises involving answering “why”
questions and interacting with a circuit simulator.
Since this is a conceptual domain, for most exer-
cises there is no structured sequence of steps that the
students should follow, but students need to name a
correct set of objects and relationships in their re-
sponse. We model the process of building an answer
to an exercise as co-constructing a solution, where
the student and tutor may contribute parts of the an-
swer. For example, consider the question “For each
circuit, which components are in a closed path”.
The solution can be built up gradually, with the stu-
dent naming different components, and the system
providing feedback until the list is complete. This
generic process of gradually building up a solution is
also applied to giving explanations. For example, in
answer to the question “What is required for a light
bulb to light” the student may say “The bulb must be
in a closed path”, which is correct but not complete.
The system may then say “Correct, but is that every-
thing?” to prompt the student towards mentioning
the battery as well. The diagnosis of the student an-
swer is represented as a set of correctly given objects
or relationships, incorrect parts, and objects and re-
lationships that have yet to be mentioned, and the
system uses the same dialogue strategy of eliciting
the missing parts for all types of questions.
Students often phrase their answers tentatively,
for example “Is the bulb in a closed path?”. In the
context of a tutor question the interpretation process
treats yes-no questions from the student as poten-
tially hedged answers. The dialogue manager at-
tempts to match the objects and relationships in the
student input with those in the question. If a close
match can be found, then the student utterance is
interpreted as giving an answer rather than a true
query. In contrast, if the student said “Is the bulb
connected to the battery?”, this would be interpreted
as a proper query and the system would attempt to
answer it.
Conclusion We demonstrate two tutorial dialogue
systems in different domains built by adapting di-
alogue techniques from task-oriented dialogue sys-
tems. Improved interpretation and generation help
support adaptivity and a wider range of inputs than
possible in scripted dialogue.
</bodyText>
<sectionHeader confidence="0.996806" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992253913043478">
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cognitive
tutor. In Proc. AI-ED 2001.
J. R. Anderson, A. T. Corbett, K. R. Koedinger, and
R. Pelletier. 1995. Cognitive tutors: Lessons learned.
The Journal of the Learning Sciences, 4(2):167–207.
M. T. H. Chi, N. de Leeuw, M.-H. Chiu, and C. La-
Vancher. 1994. Eliciting self-explanations improves
understanding. Cognitive Science, 18(3):439–477.
P. Jordan, P. Albacete, and K. VanLehn. 2005. Taking
control of redundancy in scripted tutorial dialogue. In
Proc. of AIED2005, pages 314–321.
P. Jordan, M. Makatchev, U. Pappuswamy, K. VanLehn,
and P. Albacete. 2006. A natural language tutorial
dialogue system for physics. In Proc. of FLAIRS-06.
P. W. Jordan. 2004. Using student explanations as mod-
els for adapting tutorial dialogue. In V. Barr and
Z. Markov, editors, FLAIRS Conference. AAAI Press.
S. Larsson and D. Traum. 2000. Information state and
dialogue management in the TRINDI Dialogue Move
Engine Toolkit. Natural Language Engineering, 6(3-
4):323–340.
</reference>
<page confidence="0.998782">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.561848">
<title confidence="0.99966">Adaptive Tutorial Dialogue Systems Using Deep NLP Techniques</title>
<author confidence="0.9724175">Myroslava O Dzikovska</author>
<author confidence="0.9724175">Charles B Callaway</author>
<author confidence="0.9724175">Elaine Manuel Marques-Pita</author>
<author confidence="0.9724175">Colin Matheson</author>
<author confidence="0.9724175">D Johanna</author>
<affiliation confidence="0.9825185">ICCS-HCRC, School of University of</affiliation>
<address confidence="0.611556">Edinburgh, EH8 9LW, United</address>
<abstract confidence="0.9984126">We present tutorial dialogue systems in two different domains that demonstrate the use of dialogue management and deep natural language processing techniques. Generation techniques are used to produce natural sounding feedback adapted to student performance and the dialogue history, and context is used to interpret tentative answers phrased as questions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>V Aleven</author>
<author>O Popescu</author>
<author>K R Koedinger</author>
</authors>
<title>Towards tutorial dialog to support self-explanation: Adding natural language understanding to a cognitive tutor.</title>
<date>2001</date>
<booktitle>In Proc. AI-ED</booktitle>
<contexts>
<context position="1281" citStr="Aleven et al., 2001" startWordPosition="175" endWordPosition="178">ative answers phrased as questions. 1 Introduction Intelligent tutoring systems help students improve learning compared to reading textbooks, though not quite as much as human tutors (Anderson et al., 1995). The specific properties of human-human dialogue that help students learn are still being studied, but the proposed features important for learning include allowing students to explain their actions (Chi et al., 1994), adapting tutorial feedback to the learner’s level, and engagement/affect. Some tutorial dialogue systems use NLP techniques to analyze student responses to “why” questions. (Aleven et al., 2001; Jordan et al., 2006). However, for remediation they revert to scripted dialogue, relying on shortanswer questions and canned feedback. The resulting dialogue may be redundant in ways detrimental to student understanding (Jordan et al., 2005) and allows for only limited adaptivity (Jordan, 2004). Ihis work was supported under the 6th Framework Programme of the European Commission, Ref. IST-507826, and by a grant from The Office of Naval Research N000149910165. We demonstrate two tutorial dialogue systems that use techniques from task-oriented dialogue systems to improve the interaction. The s</context>
</contexts>
<marker>Aleven, Popescu, Koedinger, 2001</marker>
<rawString>V. Aleven, O. Popescu, and K. R. Koedinger. 2001. Towards tutorial dialog to support self-explanation: Adding natural language understanding to a cognitive tutor. In Proc. AI-ED 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Anderson</author>
<author>A T Corbett</author>
<author>K R Koedinger</author>
<author>R Pelletier</author>
</authors>
<title>Cognitive tutors: Lessons learned.</title>
<date>1995</date>
<journal>The Journal of the Learning Sciences,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="868" citStr="Anderson et al., 1995" startWordPosition="112" endWordPosition="115">EH8 9LW, United Kingdom (mdzikovs,ccallawa,efarrow,mmpita,colin,jmoore)@inf.ed.ac.uk* Abstract We present tutorial dialogue systems in two different domains that demonstrate the use of dialogue management and deep natural language processing techniques. Generation techniques are used to produce natural sounding feedback adapted to student performance and the dialogue history, and context is used to interpret tentative answers phrased as questions. 1 Introduction Intelligent tutoring systems help students improve learning compared to reading textbooks, though not quite as much as human tutors (Anderson et al., 1995). The specific properties of human-human dialogue that help students learn are still being studied, but the proposed features important for learning include allowing students to explain their actions (Chi et al., 1994), adapting tutorial feedback to the learner’s level, and engagement/affect. Some tutorial dialogue systems use NLP techniques to analyze student responses to “why” questions. (Aleven et al., 2001; Jordan et al., 2006). However, for remediation they revert to scripted dialogue, relying on shortanswer questions and canned feedback. The resulting dialogue may be redundant in ways de</context>
</contexts>
<marker>Anderson, Corbett, Koedinger, Pelletier, 1995</marker>
<rawString>J. R. Anderson, A. T. Corbett, K. R. Koedinger, and R. Pelletier. 1995. Cognitive tutors: Lessons learned. The Journal of the Learning Sciences, 4(2):167–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T H Chi</author>
<author>N de Leeuw</author>
<author>M-H Chiu</author>
<author>C LaVancher</author>
</authors>
<title>Eliciting self-explanations improves understanding.</title>
<date>1994</date>
<journal>Cognitive Science,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>Chi, de Leeuw, Chiu, LaVancher, 1994</marker>
<rawString>M. T. H. Chi, N. de Leeuw, M.-H. Chiu, and C. LaVancher. 1994. Eliciting self-explanations improves understanding. Cognitive Science, 18(3):439–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jordan</author>
<author>P Albacete</author>
<author>K VanLehn</author>
</authors>
<title>Taking control of redundancy in scripted tutorial dialogue.</title>
<date>2005</date>
<booktitle>In Proc. of AIED2005,</booktitle>
<pages>314--321</pages>
<contexts>
<context position="1524" citStr="Jordan et al., 2005" startWordPosition="212" endWordPosition="215">uman dialogue that help students learn are still being studied, but the proposed features important for learning include allowing students to explain their actions (Chi et al., 1994), adapting tutorial feedback to the learner’s level, and engagement/affect. Some tutorial dialogue systems use NLP techniques to analyze student responses to “why” questions. (Aleven et al., 2001; Jordan et al., 2006). However, for remediation they revert to scripted dialogue, relying on shortanswer questions and canned feedback. The resulting dialogue may be redundant in ways detrimental to student understanding (Jordan et al., 2005) and allows for only limited adaptivity (Jordan, 2004). Ihis work was supported under the 6th Framework Programme of the European Commission, Ref. IST-507826, and by a grant from The Office of Naval Research N000149910165. We demonstrate two tutorial dialogue systems that use techniques from task-oriented dialogue systems to improve the interaction. The systems are built using the Information State Update approach (Larsson and Traum, 2000) for dialogue management and generic components for deep natural language understanding and generation. Tutorial feedback is generated adaptively based on th</context>
</contexts>
<marker>Jordan, Albacete, VanLehn, 2005</marker>
<rawString>P. Jordan, P. Albacete, and K. VanLehn. 2005. Taking control of redundancy in scripted tutorial dialogue. In Proc. of AIED2005, pages 314–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jordan</author>
<author>M Makatchev</author>
<author>U Pappuswamy</author>
<author>K VanLehn</author>
<author>P Albacete</author>
</authors>
<title>A natural language tutorial dialogue system for physics.</title>
<date>2006</date>
<booktitle>In Proc. of FLAIRS-06.</booktitle>
<contexts>
<context position="1303" citStr="Jordan et al., 2006" startWordPosition="179" endWordPosition="182"> as questions. 1 Introduction Intelligent tutoring systems help students improve learning compared to reading textbooks, though not quite as much as human tutors (Anderson et al., 1995). The specific properties of human-human dialogue that help students learn are still being studied, but the proposed features important for learning include allowing students to explain their actions (Chi et al., 1994), adapting tutorial feedback to the learner’s level, and engagement/affect. Some tutorial dialogue systems use NLP techniques to analyze student responses to “why” questions. (Aleven et al., 2001; Jordan et al., 2006). However, for remediation they revert to scripted dialogue, relying on shortanswer questions and canned feedback. The resulting dialogue may be redundant in ways detrimental to student understanding (Jordan et al., 2005) and allows for only limited adaptivity (Jordan, 2004). Ihis work was supported under the 6th Framework Programme of the European Commission, Ref. IST-507826, and by a grant from The Office of Naval Research N000149910165. We demonstrate two tutorial dialogue systems that use techniques from task-oriented dialogue systems to improve the interaction. The systems are built using</context>
</contexts>
<marker>Jordan, Makatchev, Pappuswamy, VanLehn, Albacete, 2006</marker>
<rawString>P. Jordan, M. Makatchev, U. Pappuswamy, K. VanLehn, and P. Albacete. 2006. A natural language tutorial dialogue system for physics. In Proc. of FLAIRS-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Jordan</author>
</authors>
<title>Using student explanations as models for adapting tutorial dialogue.</title>
<date>2004</date>
<editor>In V. Barr and Z. Markov, editors, FLAIRS Conference.</editor>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1578" citStr="Jordan, 2004" startWordPosition="222" endWordPosition="223">d, but the proposed features important for learning include allowing students to explain their actions (Chi et al., 1994), adapting tutorial feedback to the learner’s level, and engagement/affect. Some tutorial dialogue systems use NLP techniques to analyze student responses to “why” questions. (Aleven et al., 2001; Jordan et al., 2006). However, for remediation they revert to scripted dialogue, relying on shortanswer questions and canned feedback. The resulting dialogue may be redundant in ways detrimental to student understanding (Jordan et al., 2005) and allows for only limited adaptivity (Jordan, 2004). Ihis work was supported under the 6th Framework Programme of the European Commission, Ref. IST-507826, and by a grant from The Office of Naval Research N000149910165. We demonstrate two tutorial dialogue systems that use techniques from task-oriented dialogue systems to improve the interaction. The systems are built using the Information State Update approach (Larsson and Traum, 2000) for dialogue management and generic components for deep natural language understanding and generation. Tutorial feedback is generated adaptively based on the student model, and the interpretation is used to pro</context>
</contexts>
<marker>Jordan, 2004</marker>
<rawString>P. W. Jordan. 2004. Using student explanations as models for adapting tutorial dialogue. In V. Barr and Z. Markov, editors, FLAIRS Conference. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Larsson</author>
<author>D Traum</author>
</authors>
<title>Information state and dialogue management in the TRINDI Dialogue Move Engine Toolkit.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<pages>6--3</pages>
<contexts>
<context position="1967" citStr="Larsson and Traum, 2000" startWordPosition="279" endWordPosition="282">o scripted dialogue, relying on shortanswer questions and canned feedback. The resulting dialogue may be redundant in ways detrimental to student understanding (Jordan et al., 2005) and allows for only limited adaptivity (Jordan, 2004). Ihis work was supported under the 6th Framework Programme of the European Commission, Ref. IST-507826, and by a grant from The Office of Naval Research N000149910165. We demonstrate two tutorial dialogue systems that use techniques from task-oriented dialogue systems to improve the interaction. The systems are built using the Information State Update approach (Larsson and Traum, 2000) for dialogue management and generic components for deep natural language understanding and generation. Tutorial feedback is generated adaptively based on the student model, and the interpretation is used to process explanations and to differentiate between student queries and hedged answers phrased as questions. The systems are intended for testing hypotheses about tutoring. By comparing student learning gains between versions of the same system using different tutoring strategies, as well as between the systems and human tutors, we can test hypotheses about the role of factors such as free n</context>
</contexts>
<marker>Larsson, Traum, 2000</marker>
<rawString>S. Larsson and D. Traum. 2000. Information state and dialogue management in the TRINDI Dialogue Move Engine Toolkit. Natural Language Engineering, 6(3-4):323–340.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>