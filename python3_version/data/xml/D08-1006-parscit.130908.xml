<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.868439">
Refining Generative Language Models using Discriminative Learning
</title>
<author confidence="0.974435">
Ben Sandbank
</author>
<affiliation confidence="0.9584075">
Blavatnik School of Computer Science
Tel-Aviv University
</affiliation>
<address confidence="0.604026">
Tel-Aviv 69978, Israel
</address>
<email confidence="0.991145">
sandban@post.tau.ac.il
</email>
<sectionHeader confidence="0.99656" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994968125">
We propose a new approach to language mod-
eling which utilizes discriminative learning
methods. Our approach is an iterative one:
starting with an initial language model, in
each iteration we generate &apos;false&apos; sentences
from the current model, and then train a clas-
sifier to discriminate between them and sen-
tences from the training corpus. To the extent
that this succeeds, the classifier is incorpo-
rated into the model by lowering the probabil-
ity of sentences classified as false, and the
process is repeated. We demonstrate the effec-
tiveness of this approach on a natural lan-
guage corpus and show it provides an 11.4%
improvement in perplexity over a modified
kneser-ney smoothed trigram.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999886071428571">
Language modeling is a fundamental task in natu-
ral language processing and is routinely employed
in a wide range of applications, such as speech
recognition, machine translation, etc’. Tradition-
ally, a language model is a probabilistic model
which assigns a probability value to a sentence or a
sequence of words. We refer to these as generative
language models. A very popular example of a
generative language model is the n-gram, which
conditions the probability of the next word on the
previous (n-1)-words.
Although simple and widely-applicable, it has
proven difficult to allow n-grams, and other forms
of generative language models as well, to take ad-
</bodyText>
<page confidence="0.986216">
51
</page>
<bodyText confidence="0.999672709677419">
vantage of non-local and overlapping features.1
These sorts of features, however, pose no problem
for standard discriminative learning methods, e.g.
large-margin classifiers. For this reason, a new
class of language model, the discriminative lan-
guage model, has been proposed recently to aug-
ment generative language models (Gao et al.,
2005; Roark et al., 2007). Instead of providing
probability values, discriminative language models
directly classify sentences as either correct or in-
correct, where the definition of correctness de-
pends on the application (e.g. grammatical /
ungrammatical, correct translation / incorrect trans-
lation, etc&apos;).
Discriminative learning methods require
negative samples. Given that the corpora used for
training language models contain only real
sentences, i.e. positive samples, obtaining these
can be problematic. In most work on
discriminative language modeling this was not a
major issue as the work was concerned with
specific applications, and these provided a natural
definition of negative samples. For instance,
(Roark et al., 2007) proposed a discriminative
language model for a speech recognition task.
Given an acoustic sequence, a baseline recognizer
was used to generate a set of possible
transcriptions. The correct transcription was taken
as a positive sample, while the rest were taken as
negative samples. More recently, however,
Okanohara and Tsujii (2007) showed that a
</bodyText>
<note confidence="0.959425833333333">
1 Conditional maximum entropy models (Rosenfeld, 1996)
provide somewhat of a counter-example, but there, too, many
kinds of global and non-local features are difficult to use
(Rosenfeld, 1997).
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 51–58,
Honolulu, October 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.99989571875">
discriminative language model can be trained
independently of a specific application by using a
generative language model to obtain the negative
samples. Using a non-linear large-margin learning
algorithm, they successfully trained a classifier to
discriminate real sentences from sentences
generated by a trigram.
In this paper we extend this line of work to
study the extent to which discriminative learning
methods can lead to better generative language
models per-se. The basic intuition is the following:
if a classifier can be used to discriminate real sen-
tences from &apos;false&apos; sentences generated by a lan-
guage model, then it can also be used to improve
that language model by taking probability mass
away from sentences classified as false and trans-
ferring it to sentences classified as real. If the re-
sulting language model can be efficiently sampled
from, then this process can be repeated, until gen-
erated sentences can no longer be distinguished
from real ones.
The remainder of the paper is structured as
follows: In the next section we formally develop
this intuition, providing a quick overview of the
whole-sentence maximum-entropy model and of
self-supervised boosting, two previous works on
which we rely. We also present the method we use
for sampling from the current model, which for the
present work is far more efficient than the classical
Gibbs sampling. Our experimental results are
presented in section 3, and section 4 concludes
with a discussion and a future outlook.
</bodyText>
<sectionHeader confidence="0.991886" genericHeader="method">
2 Learning Framework
</sectionHeader>
<subsectionHeader confidence="0.998545">
2.1 Whole-sentence maximum-entropy model
</subsectionHeader>
<bodyText confidence="0.99944975">
The vast majority of statistical language models
estimate the probability of a given sentence as a
product of conditional probabilities via the chain
rule:
</bodyText>
<equation confidence="0.8720196">
P(s) = P(w1...wn) = ∏ P(wi  |hi)
def def n
(1)
1
def
</equation>
<bodyText confidence="0.983406">
where hi = w wi− is called the history of the
</bodyText>
<equation confidence="0.476059">
1 ... 1
</equation>
<bodyText confidence="0.998975111111111">
word wi. Most work on language modeling
therefore is directed at the estimation of
P(wi  |hi) . While this is theoretically correct, it
makes it difficult to incorporate global information
about the sentence into the model, e.g. length,
grammaticality, etc&apos;. For this reason, the whole-
sentence maximum-entropy model was proposed
in (Rosenfeld, 1997). In the WSME model the
probability of a sentence is defined directly as:
</bodyText>
<equation confidence="0.998622">
P(s) = Z P0 (s) • exp(Z Ai f (s))
i
Where P0(s) is some baseline model,
def
Z = Z P0 (s) • exp(Z Ai f (s)) is a normalizations i
</equation>
<bodyText confidence="0.942784714285714">
constant and the {fi}&apos;s are features encoding some
information about the sentence. Most generally, a
feature is a function from the set of word
sequences to R, the set of real numbers. However,
in most applications, as in our work, the features
are taken to be binary. Lastly, the {λi}&apos;s are real
coefficients encoding the relative importance of
their corresponding features. In the WSME
framework the set of features {fi} is given ahead of
training by the modeler, and learning consists of
estimating the coefficients {λi}. This is done by
stipulating the constraints
Ep (Ji)=Ep(f)= ZJi(sj)
/� def 1 N
</bodyText>
<equation confidence="0.991806">
N =
j 1 (3)
</equation>
<bodyText confidence="0.999293111111111">
where pɶ is the empirical distribution defined by
the training set {s1, ... sN}.2 If these constraints are
consistent then there is a unique solution in {λi}
that satisfies them. This solution is guaranteed to
be the one closest to P0 in the Kullback-Leblier
sense among all solutions satisfying (3). It is also
guaranteed to be the maximum likelihood solution
for the exponential family. For more details, see
(Chen and Rosenfeld, 1999a).
</bodyText>
<subsectionHeader confidence="0.999139">
2.2 Self-supervised boosting
</subsectionHeader>
<bodyText confidence="0.999996">
A different approach to learning the same sort
of model as in (2) was proposed in (Welling et al.,
2003). Here, instead of having all the features pre-
given, they are learned one at a time along with
their corresponding coefficients. Welling et al.
show that adding a new feature to (2) can be
</bodyText>
<footnote confidence="0.60418">
2 Sometimes a smoothed version of (3) is used instead (e.g.
Chen and Rosenfeld, 1999b).
</footnote>
<equation confidence="0.850175666666667">
i
=
(2)
</equation>
<page confidence="0.972302">
52
</page>
<bodyText confidence="0.9999795">
interpreted as gradient ascent on the log-likelihood
function, and show that the optimal feature is the
one that best discriminates real data from data
sampled from the current model. To see this, let
</bodyText>
<equation confidence="0.986715666666667">
E s = P s +∑λf s
( ) ln( 0 ( )) i i ( ) (4)
i
</equation>
<bodyText confidence="0.893142">
denote the energy associated with sentence s.3
Equation (2) can now be rewritten as -
</bodyText>
<equation confidence="0.991642">
P(s) = 1 exp(E(s)) (5)
Z
</equation>
<bodyText confidence="0.997032666666667">
where Z is a normalization constant as before. The
derivative of the log-likelihood with respect to an
arbitrary parameter β is then –
</bodyText>
<equation confidence="0.8670852">
N
1 ∂E s
( ) ∂ E s
( )
∑ ∑
i + P s
( ) (6)
∂β N = β ∈
∂ ∂β
i 1 s S
</equation>
<bodyText confidence="0.999976428571429">
where {s1, ... sN} is once again the training corpus,
and the second sum runs over the set of all word
sequences.
Now, suppose we change the energy function by
adding an infinitesimal multiple of a new feature
f*. The log-likelihood after adding the feature can
be approximated by –
</bodyText>
<equation confidence="0.9771695">
(7)
∂ ε
</equation>
<bodyText confidence="0.979000173913044">
where the derivative of L is taken at ε = 0.
Because the optimal feature is the one that
maximizes the increase in log-likelihood, we are
searching for a feature that maximizes this
derivative. Using equation (6) and noting that
we have –
This expression cannot be computed in practice,
because the set of all word sequences S is infinite.
The second term however can approximated using
samples {ui} from the current model –
3 In (Welling et al., 2003) the term for P0 does not appear,
which is equivalent to taking the uniform distribution as the
baseline model.
In other words, given a set of N samples {ui}
from the model, the optimal feature to add is one
that gives high scores to sampled sentences and
low ones to real sentences. By labeling real
sentences with 0 and sampled sentences with 1, the
task of learning the feature translates into the task
of training a classifier to discriminate between
these two classes of sentences.
In the remainder of the paper we will use feature
and classifier interchangeably.
</bodyText>
<subsectionHeader confidence="0.999741">
2.3 Rejection sampling
</subsectionHeader>
<bodyText confidence="0.999988578947369">
Self-supervised boosting was presented as a
general method for density estimation, and was not
tested in the context of language modeling. Rather,
Welling at al. demonstrated its effectiveness in
modeling hand-written digits and on synthetic data.
Đn both cases essentially linear classifiers were
used as features. As these are computationally very
efficient, the authors could use a variant of Gibbs
sampling for generating negative samples.
Unfortunately, as shown in (Okanohara and Tsujii,
2007), with the represetation of sentences that we
use, linear classifiers cannot discriminate real
sentences from sentences sampled from a trigram,
which is the model we use as a baseline, so here
we resort to a non-linear large-margin classifier
(see section 3 for details). While large-margin
classifiers consistently out-perform other learning
algorithms in many NLP tasks, their non-linear
variations are also notoriously slow when it comes
to computing their decision function – taking time
that can be linear in the size of their training data.
This means that MCMC techniques like Gibbs
sampling quickly become intractable, even for
small corpora, as they require performing very
large numbers of classifications. For this reason we
use a different sampling scheme which we refer to
as rejection sampling. This allows us to sample
from the true model distribution while requiring a
drastically smaller number of classifications, as
long as the current model isn&apos;t too far removed
from the baseline.
We will start by describing the sampling
process, and then show that the probability
distribution it samples from has the form of
equation (2). To sample a sentence from the cur-
rent model, we generate one from the baseline
model, and then pass it through each of the classi-
fiers in the model. If a given classifier classifies the
</bodyText>
<equation confidence="0.996756909090909">
∂L
= −
L(E(s) + ε f * (s)) ≈ L(E(s)) + ε ∂L
∂ =
E f
∂ ε
*
∂ε (8)
∂L
= −
N
1 N ∑ f *(si)+∑ P(s)f *(s)
1 N * 1 N *
f s
∑ ( ) + ∑ f u
( )
N =
i = 1 i 1
∂L ≈−
i i
(9)
∂ε N
</equation>
<page confidence="0.986197">
53
</page>
<bodyText confidence="0.997990153846154">
sentence as a model sentence, then it is rejected
with a certain probability associated with this clas-
sifier. Only if a sentence is accepted by all classifi-
ers is it taken as a sample sentence. Otherwise, the
sampling process is restarted.
Let us derive an expression for the probability of
a sentence s generated in this manner. To simplify
notation, assume that at this point we added but a
single feature f to the baseline model P0, and
let prej stand for the rejection probability
associated with it. Furthermore, let p- stand for the
accuracy of f in classifying sentences sampled
from P0 (negative samples). Formally,
</bodyText>
<equation confidence="0.995219">
p− = EP0 (f) (10)
</equation>
<bodyText confidence="0.990802375">
First let&apos;s assume that f (s) =1. The probability
for generating s is a sum of the probabilities of two
disjoint outcomes – the probability of generating s
as the first sentence and having it survive the
rejection, plus the probability of generating in the
first iteration some sentence s&apos; such that f (s&apos;) =1,
rejecting that, and then generating s in one of the
subsequent iterations. Formally, this means that –
</bodyText>
<equation confidence="0.940231333333334">
P (s)=(1−prej)P0(s)+p−prejP 1 (s) (11)
1
Rearranging, we have –
1
P s
( ) = P s
( )
1 01 −p−prej
p E P f i
− = −
i ( ) (16)
i 1
</equation>
<bodyText confidence="0.999897545454545">
stand for fi&apos;s accuracy in classifying sentences
generated from Pi-1, and let i
prej be the rejection
probability associated with the i&apos;th feature.
Sampling from the model then proceeds by
sampling a sentence s from P0. For each 1≤ i≤ N,
in order, if fi (s) =1, then we attempt to reject s
with probability i
prej . If s survives all the rejection
attempts, it is returned as the next sample. Using
similar arguments as before it&apos;s possible to show
</bodyText>
<equation confidence="0.938371555555556">
that if we take ln(1 )
λ i = − prej and –
i
N
Z (1 p − p
i i
= ∏ − ) (17)
rej
1
</equation>
<bodyText confidence="0.931438571428571">
This process can betrivially generali
then the probability of a sentence s sampled by this
process is given by equation (2). Conversely, this
shows that rejection sampling can be used for
obtaining negative samples from the model given
i
in (2) by taking p rej = − λi , as long as
</bodyText>
<sectionHeader confidence="0.906539" genericHeader="method">
1 exp( )
</sectionHeader>
<bodyText confidence="0.9922796">
0 &lt; exp(λi) ≤ 1. In section 3 we show that in our
experimental setup, rejection sampling brings
about enormous savings in the number of
classifications necessary during training, as
compared with Gibbs sampling.
</bodyText>
<subsectionHeader confidence="0.989519">
2.4 Adding a new feature
</subsectionHeader>
<equation confidence="0.825878842105263">
P(s) = − prej P0 (s)
1− p− prej
1
(12)
i
=
new feature is therefore to set i 1 such that the
prej +
constraint:
E Pi f i
( 1 ) ( 1 )
+ = ɶ + (18)
E p f i
+ 1
1
P (s) P (s) exp( f ( s ))
= ⋅ λ ⋅ (15)
1 0
Z
</equation>
<bodyText confidence="0.982753428571429">
Similarly, the probability for a sentence s for
which f (s) = 0 is the probability of generating s
as the first sentence, plus the probability of
generating someother sentences&apos; for which
f
=1, rejecting it, and then generating s in a
future iteration. Formall
</bodyText>
<equation confidence="0.443223928571429">
(s&apos;)
y,
P1 (s) =P0(s)+p−prejP1(s) (13)
an
d hence –
(14)
Letting Z
, andletting
, we have, for all
=1−p−prej
λ=ln(1−prej
s –
zed for N
features. Let –
</equation>
<bodyText confidence="0.875466166666667">
Given the current model
and a new feature
wewish to findtheoptimal
or
equivalently its optimal rejection probability
In the WSME fr
</bodyText>
<equation confidence="0.823065">
Pi
fi+1,
λi+1,
i1
prej +
</equation>
<bodyText confidence="0.9992795">
amework, the weights of the
features are set in such a way that the expected
value of the features on sentences sampled from
the model equals their expected value on real
sentences. A possible way to set the weight of a
is satistfied, where pɶ is once again the empirical
distribution defined by the training set. Intuitively,
this means that the new feature could no longer be
used to discriminate between sentences sampled
from Pi+1 and real sentences. However, setting
</bodyText>
<page confidence="0.996215">
54
</page>
<bodyText confidence="0.968398533333333">
associated with the features already existing in the
model, thus hampering the model&apos;s performance.
Therefore, we set the new feature&apos;s rejection
probability by directly searching for the one that
minimizes an estimate of Pi+1&apos;s perplexity on a set
of held out real sentences. To do this, we first
sample a new set of sentences from Pi,
independently of the set that was used for training
fi+1 , and use it to estimate i 1
p − + . For any
arbitrarily determined i 1
prej + , this enables us to
calculate an estimate for the normalization constant
Z (equation 17), and therefore an estimate for Pi+1.
We do this for a range of possible values for i 1
</bodyText>
<equation confidence="0.74525">
prej +
</equation>
<bodyText confidence="0.9993075">
and pick the one that leads to the largest reduction
in perplexity on the held out data.4
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="method">
3 Experimental work
</sectionHeader>
<bodyText confidence="0.878866194444444">
We tested our approach on the ATIS natural
language corpus (Hemphill et al., 1990). We split
the corpus into a training set of 11,000 sentences, a
held-out set containing 1,045 sentences, and a test
set containing 1,000 sentences which were
reserved for measuring perplexity. The corpus was
pre-processed so that every word appearing less
than three times was replaced by a special UNK
symbol. The resulting lexicon contained 603 word
types.
Our learning framework leaves open a number
of design choices:
1. Baseline language model: For P0 we used a
trigram with modified kneser-ney smoothing
[Chen and Goodman, 1998], which is still
considered one of the best smoothing methods for
n-gram language models.
2. Sentence representation: Each sentence was
represented as the collection of unigrams, bigrams
and trigrams it contained. A coordinate was
reserved for each such n-gram which appeared in
the data, whether real or sampled. The value of the
n&apos;th coordinate in the vector representation of
4 Interestingly, in practice both methods result in near identical
rejection probabilities, within a precision of 0.0001. This
indicates that satisfying the constraint (18) for the new feature
is more important, in terms of perplexity, than preserving the
constraints of the previous features, insofar as those get
violated.
sentence s was set to the number of times the
corresponding n-gram appeared in s.
3. Type of classifiers: For our features we used
large-margin classifiers trained using the online
algorithm described in (Crammer et al., 2006). The
code for the classifier was generously provided by
Daisuke Okanohara. This code was extensively
optimized to take advantage of the very sparse
sentence representation described above. As shown
in (Okanohara and Tsujii, 2007), using this
representation, a linear classifier cannot distinguish
sentences sampled from a trigram and real
sentences. Therefore, we used a 3rd order
polynomial kernel, which was found to give good
results. No special effort was otherwise made in
order to optimize the parameters of the classifiers.
4. Stopping criterion: The process of adding
features to the model was continued until the
classification performance of the next feature was
within 2% of chance performance.
We refer to the language model obtained by this
approach as the boosted model to distinguish it
from the baseline model. To estimate the boosted
model&apos;s perplexity we needed to estimate the
normalization constant Z in equation (2). Since this
constant is equal to P0 (exp( i i ))
E E A f it can be
i
estimated from a large-enough sample from P0. We
used 10,000,000 sentences generated from the
baseline trigram and took the upper bound of the
95% confidence interval of the sample mean as an
upper bound for Z. This means the perplexity
estimates we report are upper bounds for the real
model perplexity with 95% confidence.5
The algorithm converged after 21 features were
added to the model. Figure 1 presents the model&apos;s
perplexity on the test set estimated after each
iteration. The perplexity of the final model is 9.02.
In comparison, the perplexity of the modified
kneser-ney smoothed trigram on this corpus is
10.18. This is an 11.4% improvement relative to
the baseline model.
</bodyText>
<footnote confidence="0.542753333333333">
5 Alternatively we could have used our estimate for PN(s) de-
scribed in section 2.4. A large sample of sentences would still
be necessary though, to get a good estimate for equation (16).
</footnote>
<table confidence="0.4171915">
+ in this manner may violate the constraints (18)
1
i
prej
</table>
<page confidence="0.93203">
55
</page>
<figureCaption confidence="0.998084166666667">
Figure 1. Model perplexity during training. The x-
axis denotes the number of features added to the
model. The final perplexity after 21 features is 9.02.
Figure 2. Classifier accuracy during training,
assessed on held-out data. 0.5 signifies chance
performance.
</figureCaption>
<bodyText confidence="0.995045548387097">
Figure 2 shows the accuracy of the trained
features on held-out data. The held-out data was
composed of equal parts real and model sentences,
so 50% accuracy is chance performance. As might
have been expected, the classifiers start out with a
relatively high accuracy of 68%, which dwindles
down to little over 50% as more features are added
to the model. Not surprisingly, there is a strong
correlation between the accuracy of a feature and
the reduction in perplexity it engenders (spearman
correlation coefficient r=0.89, p&lt;10-5.)
In tables 1 and 2 we show a representative
sample of sentences from the baseline model and
from the final model. As the baseline model is a
trigram, it cannot capture dependencies that span a
range longer than two words. Hence sentences that
start out seemingly in one topic and then veer off
to another are common. The global information
available to the features used by the boosted model
greatly reduces this phenomenon. To get a
quantitative sense of this, we generated 200
sentences from each model and submitted them for
grammaticality testing by a proficient (though non-
native) English speaker. Of the trigram-generated
please list costs in at pittsburgh
what type of airplane is have an early morning
what types of aircraft is that a meal
what not nineteen forty two
between boston and atlanta on august fifteenth
which airlines fly american flying on
what is the flight leaving pittsburgh after six p m
</bodyText>
<tableCaption confidence="0.990078666666667">
Table 1. A sample of sentences generated by the
baseline model, a trigram smoothed with modified
kneser-ney smoothing.
</tableCaption>
<bodyText confidence="0.9996916">
what is the cost of flight d l three seventeen
sixty five
what time does flight at eight thirty eight a m
and six p m
what does fare code q w mean
what kind of aircraft will i be flying on
flights from philadelphia on saturday
what is the fare for flight two nine six
what is the cost of coach transcontinental flight
u a three oh two from denver to san francisco
</bodyText>
<tableCaption confidence="0.9604805">
Table 2. A sample of sentences generated by the final
model
</tableCaption>
<bodyText confidence="0.999090208333334">
sentences, 86 were deemed grammatical (43%),
while of those generated by the boosted model 132
were grammatical (66%). This difference is
statistically significant with p&lt;10-5.
Finally, let us quantify the computational
savings obtained from using rejection sampling.
Let |V |stand for the lexicon size (here |V|=603)
and |L |for the average sentence length (|L|=14). In
Gibbs sampling, a sentence is sampled by starting
out with a random sequence of words. For each
word position, the current word is replaced with
each word in the lexicon, and the probability of the
resulting sentence is calculated. Then one of the
words is randomly selected for this position in
proportion to the calculated probabilities. The
sentence has to be scanned in this manner several
times for the sample to approximate the model
distribution. Assuming we perform only 3 scans
for each sentence, Gibbs sampling would have thus
required us to classify 3 |V  ||L |≈ 25,000
sentences per sampled sentence. Given that in each
iteration we generate 12,045 sentences, and that in
the n&apos;th iteration each sentence has to be classified
by n features, this gives a total of roughly
</bodyText>
<page confidence="0.978698">
56
</page>
<figure confidence="0.711747">
7 ⋅ 1 0 classifications after 21 iterations. In
10
contrast, using rejection sampling, we used only
6.7 ⋅ 1 0 classifications in total – a difference of
7
over three orders of magnitude.
</figure>
<sectionHeader confidence="0.990304" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999929365079365">
In this work we presented a method that enables
using discriminative learning methods for refining
generative language models. Utilizing large-
margin classifiers that are trained to discriminate
real sentences from model sentences we showed
that sizeable improvements in perplexity over a
state-of-the-art smoothed trigram are possible.
Our method bears some similarity to the recently
developed Contrastive Estimation method (Smith
and Eisner, 2004). Contrastive estimation (CE) was
proposed as a means for training log-linear prob-
abilistic models. As all training methods, contras-
tive estimation pushes probability mass unto
positive samples. Unlike other methods, CE takes
this probability mass from the &apos;neighborhood&apos; of
each positive sample. For example, given a real
sentence s, CE might give it more probability by
taking away probability from similar sentences
which are likely to be ungrammatical, for instance
sentences that are formed by taking s and switch-
ing the order of two adjacent words in it. This is
intuitively similar to our approach – effectively,
our model gives probability mass to positive sam-
ples, taking it away from sentences classified as
model sentences. A major difference between the
two approaches, however, is that in CE the defini-
tion of the sentence&apos;s neighborhood must be speci-
fied in advance by the modeler. In our work, the
&apos;neighborhood&apos; is determined automatically and
dynamically as learning proceeds, according to the
capabilities of the classifiers used.
The sentence representation we chose for this
work is rather simple, and was intended primarily
to demonstrate the efficacy of our approach. In
future work we plan to experiment with richer
representations, e.g. including long-range n-grams
(Rosenfeld, 1996), class n-grams (Brown et al.,
1992), grammatical features (Amaya and Benedy,
2001), etc&apos;.
The main computational bottleneck in our
approach is the generation of negative samples
from the current model. Rejection sampling
allowed us to use computationally intensive
classifiers as our features by reducing the number
of classifications that had to be performed during
the sampling process. However, if the boosted
model strays too far from the baseline P0, these
savings will be negated by the very large sentence
rejection probabilities that will ensue. This is likely
to be the case when richer representations as
suggested above are used, necessitating a return to
Gibbs sampling. Therefore, in future work we plan
to experiment with classifiers whose decision
function is cheaper to compute, such as neural
networks and decision trees. Another possible
direction would be using the recently proposed
Deep Belief Network formalism (Hinton et al.,
2006). DBNs utilize semi-linear features which are
stacked recursively and thus very efficiently model
non-linearities in their data. These have been used
in the past for language modeling (Mnih and
Hinton, 2007), but not within the whole-sentence
framework.
</bodyText>
<sectionHeader confidence="0.996516" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9983856">
We would like to thank Daisuke Okanohara for
his generosity in providing the code for the large-
margin classifier. The author is supported by the
Yeshaya Horowitz association through the center
of Complexity Science.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999248333333333">
Fredy Amaya and Jose Miguel Benedi, 2001, Improve-
ment of a whole sentence maximum entropy model us-
ing grammatical features. In Proceedings of the 39th
Annual Meeting of the Association of Computational
Linguistics.
Peter. F. Brown, Vincent. J. Della Pietra, Peter. V.
deSouza, Jenifer. C. Lai and Robert. L. Mercer. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467–479, Dec. 1992.
Stanley F. Chen and Joshua. Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report TR–10–98, Center for Re-
search in Computing Technology, Harvard University
Stanley F. Chen and Ronald Rosenfeld. 1999a. Efficient
sampling and feature selection in whole sentence maxi-
mum entropy language models. In Proceedings of
ICASSP’99. IEEE.
Stanley F. Chen and Ronald Rosenfeld. 1999b. A Gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMUCS-99-108, Carnegie Mellon
University.
</reference>
<page confidence="0.980347">
57
</page>
<reference confidence="0.999670820512821">
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online pas-
sive-aggressive algorithms. Journal of Machine Learn-
ing Research.
Jianfeng Gao, Hao Yu, Wei Yuan, and Peng Xu. 2005.
Minimum sample risk methods for language modeling.
In Proc. of HLT/EMNLP.
Charles T. Hemphill, John J. Godfrey and George R.
Doddington. 1990. The ATIS Spoken Language Sys-
tems Pilot Corpus. The workshop on speech and natural
language, Morgan Kaufmann.
Geoffrey E. Hinton, Simon Osindero and Yee-Whye
Teh, 2006. A fast learning algorithm for deep belief
nets, Neural Computation, 18(7):1527–1554.
Andriy Mnih and Geoffrey E. Hinton. 2007. Three new
graphical models for statistical language modeling. In
Proceedings of the 24th international conference on
Machine Learning.
Daisuke Okanohara and Jun&apos;ichi Tsujii. 2007. A dis-
criminative language model with pseudo-negative sam-
ples. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics.
Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling. Com-
puter Speech and Language, 21(2):373–392.
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
speech and language, 10:187–228
Ronald Rosenfeld. 1997. A whole sentence maximum
entropy language model. In Proc. of the IEEE Workshop
on Automatic Speech Recognition and Understanding.
Noah A. Smith and Jason Eisner. 2005. Contrastive es-
timation: training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the Asso-
ciation of Computational Linguistics.
Max Welling., Richard Zemel and Geoffrey E. Hinton.
2003. Self-Supervised Boosting. Advances in Neural
Information Processing Systems, 15, MIT Press, Cam-
bridge, MA
</reference>
<page confidence="0.999258">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.461192">
<title confidence="0.999906">Refining Generative Language Models using Discriminative Learning</title>
<author confidence="0.98893">Ben Sandbank</author>
<affiliation confidence="0.7080555">Blavatnik School of Computer Tel-Aviv</affiliation>
<address confidence="0.714499">Tel-Aviv 69978,</address>
<email confidence="0.994034">sandban@post.tau.ac.il</email>
<abstract confidence="0.998848823529412">We propose a new approach to language modeling which utilizes discriminative learning methods. Our approach is an iterative one: starting with an initial language model, in each iteration we generate &apos;false&apos; sentences from the current model, and then train a classifier to discriminate between them and sentences from the training corpus. To the extent that this succeeds, the classifier is incorporated into the model by lowering the probability of sentences classified as false, and the process is repeated. We demonstrate the effectiveness of this approach on a natural language corpus and show it provides an 11.4% improvement in perplexity over a modified kneser-ney smoothed trigram.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fredy Amaya</author>
<author>Jose Miguel Benedi</author>
</authors>
<title>Improvement of a whole sentence maximum entropy model using grammatical features.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<marker>Amaya, Benedi, 2001</marker>
<rawString>Fredy Amaya and Jose Miguel Benedi, 2001, Improvement of a whole sentence maximum entropy model using grammatical features. In Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="24230" citStr="Brown et al., 1992" startWordPosition="4113" endWordPosition="4116">sentences. A major difference between the two approaches, however, is that in CE the definition of the sentence&apos;s neighborhood must be specified in advance by the modeler. In our work, the &apos;neighborhood&apos; is determined automatically and dynamically as learning proceeds, according to the capabilities of the classifiers used. The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach. In future work we plan to experiment with richer representations, e.g. including long-range n-grams (Rosenfeld, 1996), class n-grams (Brown et al., 1992), grammatical features (Amaya and Benedy, 2001), etc&apos;. The main computational bottleneck in our approach is the generation of negative samples from the current model. Rejection sampling allowed us to use computationally intensive classifiers as our features by reducing the number of classifications that had to be performed during the sampling process. However, if the boosted model strays too far from the baseline P0, these savings will be negated by the very large sentence rejection probabilities that will ensue. This is likely to be the case when richer representations as suggested above are </context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter. F. Brown, Vincent. J. Della Pietra, Peter. V. deSouza, Jenifer. C. Lai and Robert. L. Mercer. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467–479, Dec. 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR–10–98,</tech>
<institution>Center for Research in Computing Technology, Harvard University</institution>
<contexts>
<context position="15960" citStr="Goodman, 1998" startWordPosition="2771" endWordPosition="2772">tested our approach on the ATIS natural language corpus (Hemphill et al., 1990). We split the corpus into a training set of 11,000 sentences, a held-out set containing 1,045 sentences, and a test set containing 1,000 sentences which were reserved for measuring perplexity. The corpus was pre-processed so that every word appearing less than three times was replaced by a special UNK symbol. The resulting lexicon contained 603 word types. Our learning framework leaves open a number of design choices: 1. Baseline language model: For P0 we used a trigram with modified kneser-ney smoothing [Chen and Goodman, 1998], which is still considered one of the best smoothing methods for n-gram language models. 2. Sentence representation: Each sentence was represented as the collection of unigrams, bigrams and trigrams it contained. A coordinate was reserved for each such n-gram which appeared in the data, whether real or sampled. The value of the n&apos;th coordinate in the vector representation of 4 Interestingly, in practice both methods result in near identical rejection probabilities, within a precision of 0.0001. This indicates that satisfying the constraint (18) for the new feature is more important, in terms</context>
</contexts>
<marker>Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR–10–98, Center for Research in Computing Technology, Harvard University</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Efficient sampling and feature selection in whole sentence maximum entropy language models.</title>
<date>1999</date>
<booktitle>In Proceedings of ICASSP’99.</booktitle>
<publisher>IEEE.</publisher>
<contexts>
<context position="6766" citStr="Chen and Rosenfeld, 1999" startWordPosition="1066" endWordPosition="1069">iven ahead of training by the modeler, and learning consists of estimating the coefficients {λi}. This is done by stipulating the constraints Ep (Ji)=Ep(f)= ZJi(sj) /� def 1 N N = j 1 (3) where pɶ is the empirical distribution defined by the training set {s1, ... sN}.2 If these constraints are consistent then there is a unique solution in {λi} that satisfies them. This solution is guaranteed to be the one closest to P0 in the Kullback-Leblier sense among all solutions satisfying (3). It is also guaranteed to be the maximum likelihood solution for the exponential family. For more details, see (Chen and Rosenfeld, 1999a). 2.2 Self-supervised boosting A different approach to learning the same sort of model as in (2) was proposed in (Welling et al., 2003). Here, instead of having all the features pregiven, they are learned one at a time along with their corresponding coefficients. Welling et al. show that adding a new feature to (2) can be 2 Sometimes a smoothed version of (3) is used instead (e.g. Chen and Rosenfeld, 1999b). i = (2) 52 interpreted as gradient ascent on the log-likelihood function, and show that the optimal feature is the one that best discriminates real data from data sampled from the curren</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999a. Efficient sampling and feature selection in whole sentence maximum entropy language models. In Proceedings of ICASSP’99. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMUCS-99-108,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="6766" citStr="Chen and Rosenfeld, 1999" startWordPosition="1066" endWordPosition="1069">iven ahead of training by the modeler, and learning consists of estimating the coefficients {λi}. This is done by stipulating the constraints Ep (Ji)=Ep(f)= ZJi(sj) /� def 1 N N = j 1 (3) where pɶ is the empirical distribution defined by the training set {s1, ... sN}.2 If these constraints are consistent then there is a unique solution in {λi} that satisfies them. This solution is guaranteed to be the one closest to P0 in the Kullback-Leblier sense among all solutions satisfying (3). It is also guaranteed to be the maximum likelihood solution for the exponential family. For more details, see (Chen and Rosenfeld, 1999a). 2.2 Self-supervised boosting A different approach to learning the same sort of model as in (2) was proposed in (Welling et al., 2003). Here, instead of having all the features pregiven, they are learned one at a time along with their corresponding coefficients. Welling et al. show that adding a new feature to (2) can be 2 Sometimes a smoothed version of (3) is used instead (e.g. Chen and Rosenfeld, 1999b). i = (2) 52 interpreted as gradient ascent on the log-likelihood function, and show that the optimal feature is the one that best discriminates real data from data sampled from the curren</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999b. A Gaussian prior for smoothing maximum entropy models. Technical Report CMUCS-99-108, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="16891" citStr="Crammer et al., 2006" startWordPosition="2913" endWordPosition="2916">led. The value of the n&apos;th coordinate in the vector representation of 4 Interestingly, in practice both methods result in near identical rejection probabilities, within a precision of 0.0001. This indicates that satisfying the constraint (18) for the new feature is more important, in terms of perplexity, than preserving the constraints of the previous features, insofar as those get violated. sentence s was set to the number of times the corresponding n-gram appeared in s. 3. Type of classifiers: For our features we used large-margin classifiers trained using the online algorithm described in (Crammer et al., 2006). The code for the classifier was generously provided by Daisuke Okanohara. This code was extensively optimized to take advantage of the very sparse sentence representation described above. As shown in (Okanohara and Tsujii, 2007), using this representation, a linear classifier cannot distinguish sentences sampled from a trigram and real sentences. Therefore, we used a 3rd order polynomial kernel, which was found to give good results. No special effort was otherwise made in order to optimize the parameters of the classifiers. 4. Stopping criterion: The process of adding features to the model w</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Hao Yu</author>
<author>Wei Yuan</author>
<author>Peng Xu</author>
</authors>
<title>Minimum sample risk methods for language modeling.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP.</booktitle>
<contexts>
<context position="1896" citStr="Gao et al., 2005" startWordPosition="287" endWordPosition="290">e of a generative language model is the n-gram, which conditions the probability of the next word on the previous (n-1)-words. Although simple and widely-applicable, it has proven difficult to allow n-grams, and other forms of generative language models as well, to take ad51 vantage of non-local and overlapping features.1 These sorts of features, however, pose no problem for standard discriminative learning methods, e.g. large-margin classifiers. For this reason, a new class of language model, the discriminative language model, has been proposed recently to augment generative language models (Gao et al., 2005; Roark et al., 2007). Instead of providing probability values, discriminative language models directly classify sentences as either correct or incorrect, where the definition of correctness depends on the application (e.g. grammatical / ungrammatical, correct translation / incorrect translation, etc&apos;). Discriminative learning methods require negative samples. Given that the corpora used for training language models contain only real sentences, i.e. positive samples, obtaining these can be problematic. In most work on discriminative language modeling this was not a major issue as the work was </context>
</contexts>
<marker>Gao, Yu, Yuan, Xu, 2005</marker>
<rawString>Jianfeng Gao, Hao Yu, Wei Yuan, and Peng Xu. 2005. Minimum sample risk methods for language modeling. In Proc. of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles T Hemphill</author>
<author>John J Godfrey</author>
<author>George R Doddington</author>
</authors>
<title>The ATIS Spoken Language Systems Pilot Corpus. The workshop on speech and natural language,</title>
<date>1990</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="15426" citStr="Hemphill et al., 1990" startWordPosition="2684" endWordPosition="2687">s perplexity on a set of held out real sentences. To do this, we first sample a new set of sentences from Pi, independently of the set that was used for training fi+1 , and use it to estimate i 1 p − + . For any arbitrarily determined i 1 prej + , this enables us to calculate an estimate for the normalization constant Z (equation 17), and therefore an estimate for Pi+1. We do this for a range of possible values for i 1 prej + and pick the one that leads to the largest reduction in perplexity on the held out data.4 3 Experimental work We tested our approach on the ATIS natural language corpus (Hemphill et al., 1990). We split the corpus into a training set of 11,000 sentences, a held-out set containing 1,045 sentences, and a test set containing 1,000 sentences which were reserved for measuring perplexity. The corpus was pre-processed so that every word appearing less than three times was replaced by a special UNK symbol. The resulting lexicon contained 603 word types. Our learning framework leaves open a number of design choices: 1. Baseline language model: For P0 we used a trigram with modified kneser-ney smoothing [Chen and Goodman, 1998], which is still considered one of the best smoothing methods for</context>
</contexts>
<marker>Hemphill, Godfrey, Doddington, 1990</marker>
<rawString>Charles T. Hemphill, John J. Godfrey and George R. Doddington. 1990. The ATIS Spoken Language Systems Pilot Corpus. The workshop on speech and natural language, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Simon Osindero and Yee-Whye Teh,</title>
<date>2006</date>
<journal>Neural Computation,</journal>
<volume>18</volume>
<issue>7</issue>
<marker>Hinton, 2006</marker>
<rawString>Geoffrey E. Hinton, Simon Osindero and Yee-Whye Teh, 2006. A fast learning algorithm for deep belief nets, Neural Computation, 18(7):1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Three new graphical models for statistical language modeling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine Learning.</booktitle>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey E. Hinton. 2007. Three new graphical models for statistical language modeling. In Proceedings of the 24th international conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Okanohara</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>A discriminative language model with pseudo-negative samples.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="2963" citStr="Okanohara and Tsujii (2007)" startWordPosition="442" endWordPosition="445">ntences, i.e. positive samples, obtaining these can be problematic. In most work on discriminative language modeling this was not a major issue as the work was concerned with specific applications, and these provided a natural definition of negative samples. For instance, (Roark et al., 2007) proposed a discriminative language model for a speech recognition task. Given an acoustic sequence, a baseline recognizer was used to generate a set of possible transcriptions. The correct transcription was taken as a positive sample, while the rest were taken as negative samples. More recently, however, Okanohara and Tsujii (2007) showed that a 1 Conditional maximum entropy models (Rosenfeld, 1996) provide somewhat of a counter-example, but there, too, many kinds of global and non-local features are difficult to use (Rosenfeld, 1997). Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 51–58, Honolulu, October 2008. c�2008 Association for Computational Linguistics discriminative language model can be trained independently of a specific application by using a generative language model to obtain the negative samples. Using a non-linear large-margin learning algorithm, they succes</context>
<context position="9585" citStr="Okanohara and Tsujii, 2007" startWordPosition="1576" endWordPosition="1579"> classes of sentences. In the remainder of the paper we will use feature and classifier interchangeably. 2.3 Rejection sampling Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling. Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data. Đn both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately, as shown in (Okanohara and Tsujii, 2007), with the represetation of sentences that we use, linear classifiers cannot discriminate real sentences from sentences sampled from a trigram, which is the model we use as a baseline, so here we resort to a non-linear large-margin classifier (see section 3 for details). While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function – taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling q</context>
<context position="17121" citStr="Okanohara and Tsujii, 2007" startWordPosition="2947" endWordPosition="2950">he constraint (18) for the new feature is more important, in terms of perplexity, than preserving the constraints of the previous features, insofar as those get violated. sentence s was set to the number of times the corresponding n-gram appeared in s. 3. Type of classifiers: For our features we used large-margin classifiers trained using the online algorithm described in (Crammer et al., 2006). The code for the classifier was generously provided by Daisuke Okanohara. This code was extensively optimized to take advantage of the very sparse sentence representation described above. As shown in (Okanohara and Tsujii, 2007), using this representation, a linear classifier cannot distinguish sentences sampled from a trigram and real sentences. Therefore, we used a 3rd order polynomial kernel, which was found to give good results. No special effort was otherwise made in order to optimize the parameters of the classifiers. 4. Stopping criterion: The process of adding features to the model was continued until the classification performance of the next feature was within 2% of chance performance. We refer to the language model obtained by this approach as the boosted model to distinguish it from the baseline model. To</context>
</contexts>
<marker>Okanohara, Tsujii, 2007</marker>
<rawString>Daisuke Okanohara and Jun&apos;ichi Tsujii. 2007. A discriminative language model with pseudo-negative samples. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
</authors>
<title>Discriminative n-gram language modeling.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1917" citStr="Roark et al., 2007" startWordPosition="291" endWordPosition="294">language model is the n-gram, which conditions the probability of the next word on the previous (n-1)-words. Although simple and widely-applicable, it has proven difficult to allow n-grams, and other forms of generative language models as well, to take ad51 vantage of non-local and overlapping features.1 These sorts of features, however, pose no problem for standard discriminative learning methods, e.g. large-margin classifiers. For this reason, a new class of language model, the discriminative language model, has been proposed recently to augment generative language models (Gao et al., 2005; Roark et al., 2007). Instead of providing probability values, discriminative language models directly classify sentences as either correct or incorrect, where the definition of correctness depends on the application (e.g. grammatical / ungrammatical, correct translation / incorrect translation, etc&apos;). Discriminative learning methods require negative samples. Given that the corpora used for training language models contain only real sentences, i.e. positive samples, obtaining these can be problematic. In most work on discriminative language modeling this was not a major issue as the work was concerned with specif</context>
</contexts>
<marker>Roark, Saraclar, Collins, 2007</marker>
<rawString>Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. Computer Speech and Language, 21(2):373–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling. Computer speech and language,</title>
<date>1996</date>
<pages>10--187</pages>
<contexts>
<context position="3032" citStr="Rosenfeld, 1996" startWordPosition="454" endWordPosition="455"> on discriminative language modeling this was not a major issue as the work was concerned with specific applications, and these provided a natural definition of negative samples. For instance, (Roark et al., 2007) proposed a discriminative language model for a speech recognition task. Given an acoustic sequence, a baseline recognizer was used to generate a set of possible transcriptions. The correct transcription was taken as a positive sample, while the rest were taken as negative samples. More recently, however, Okanohara and Tsujii (2007) showed that a 1 Conditional maximum entropy models (Rosenfeld, 1996) provide somewhat of a counter-example, but there, too, many kinds of global and non-local features are difficult to use (Rosenfeld, 1997). Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 51–58, Honolulu, October 2008. c�2008 Association for Computational Linguistics discriminative language model can be trained independently of a specific application by using a generative language model to obtain the negative samples. Using a non-linear large-margin learning algorithm, they successfully trained a classifier to discriminate real sentences from sente</context>
<context position="24194" citStr="Rosenfeld, 1996" startWordPosition="4109" endWordPosition="4110">om sentences classified as model sentences. A major difference between the two approaches, however, is that in CE the definition of the sentence&apos;s neighborhood must be specified in advance by the modeler. In our work, the &apos;neighborhood&apos; is determined automatically and dynamically as learning proceeds, according to the capabilities of the classifiers used. The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach. In future work we plan to experiment with richer representations, e.g. including long-range n-grams (Rosenfeld, 1996), class n-grams (Brown et al., 1992), grammatical features (Amaya and Benedy, 2001), etc&apos;. The main computational bottleneck in our approach is the generation of negative samples from the current model. Rejection sampling allowed us to use computationally intensive classifiers as our features by reducing the number of classifications that had to be performed during the sampling process. However, if the boosted model strays too far from the baseline P0, these savings will be negated by the very large sentence rejection probabilities that will ensue. This is likely to be the case when richer rep</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Ronald Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer speech and language, 10:187–228</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A whole sentence maximum entropy language model.</title>
<date>1997</date>
<booktitle>In Proc. of the IEEE Workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="3170" citStr="Rosenfeld, 1997" startWordPosition="475" endWordPosition="476"> natural definition of negative samples. For instance, (Roark et al., 2007) proposed a discriminative language model for a speech recognition task. Given an acoustic sequence, a baseline recognizer was used to generate a set of possible transcriptions. The correct transcription was taken as a positive sample, while the rest were taken as negative samples. More recently, however, Okanohara and Tsujii (2007) showed that a 1 Conditional maximum entropy models (Rosenfeld, 1996) provide somewhat of a counter-example, but there, too, many kinds of global and non-local features are difficult to use (Rosenfeld, 1997). Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 51–58, Honolulu, October 2008. c�2008 Association for Computational Linguistics discriminative language model can be trained independently of a specific application by using a generative language model to obtain the negative samples. Using a non-linear large-margin learning algorithm, they successfully trained a classifier to discriminate real sentences from sentences generated by a trigram. In this paper we extend this line of work to study the extent to which discriminative learning methods can le</context>
<context position="5512" citStr="Rosenfeld, 1997" startWordPosition="846" endWordPosition="847">model The vast majority of statistical language models estimate the probability of a given sentence as a product of conditional probabilities via the chain rule: P(s) = P(w1...wn) = ∏ P(wi |hi) def def n (1) 1 def where hi = w wi− is called the history of the 1 ... 1 word wi. Most work on language modeling therefore is directed at the estimation of P(wi |hi) . While this is theoretically correct, it makes it difficult to incorporate global information about the sentence into the model, e.g. length, grammaticality, etc&apos;. For this reason, the wholesentence maximum-entropy model was proposed in (Rosenfeld, 1997). In the WSME model the probability of a sentence is defined directly as: P(s) = Z P0 (s) • exp(Z Ai f (s)) i Where P0(s) is some baseline model, def Z = Z P0 (s) • exp(Z Ai f (s)) is a normalizations i constant and the {fi}&apos;s are features encoding some information about the sentence. Most generally, a feature is a function from the set of word sequences to R, the set of real numbers. However, in most applications, as in our work, the features are taken to be binary. Lastly, the {λi}&apos;s are real coefficients encoding the relative importance of their corresponding features. In the WSME framework</context>
</contexts>
<marker>Rosenfeld, 1997</marker>
<rawString>Ronald Rosenfeld. 1997. A whole sentence maximum entropy language model. In Proc. of the IEEE Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association of Computational Linguistics.</booktitle>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Welling</author>
<author>Richard Zemel</author>
<author>Geoffrey E Hinton</author>
</authors>
<date>2003</date>
<booktitle>Self-Supervised Boosting. Advances in Neural Information Processing Systems, 15,</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA</location>
<contexts>
<context position="6903" citStr="Welling et al., 2003" startWordPosition="1089" endWordPosition="1092">s Ep (Ji)=Ep(f)= ZJi(sj) /� def 1 N N = j 1 (3) where pɶ is the empirical distribution defined by the training set {s1, ... sN}.2 If these constraints are consistent then there is a unique solution in {λi} that satisfies them. This solution is guaranteed to be the one closest to P0 in the Kullback-Leblier sense among all solutions satisfying (3). It is also guaranteed to be the maximum likelihood solution for the exponential family. For more details, see (Chen and Rosenfeld, 1999a). 2.2 Self-supervised boosting A different approach to learning the same sort of model as in (2) was proposed in (Welling et al., 2003). Here, instead of having all the features pregiven, they are learned one at a time along with their corresponding coefficients. Welling et al. show that adding a new feature to (2) can be 2 Sometimes a smoothed version of (3) is used instead (e.g. Chen and Rosenfeld, 1999b). i = (2) 52 interpreted as gradient ascent on the log-likelihood function, and show that the optimal feature is the one that best discriminates real data from data sampled from the current model. To see this, let E s = P s +∑λf s ( ) ln( 0 ( )) i i ( ) (4) i denote the energy associated with sentence s.3 Equation (2) can n</context>
<context position="8494" citStr="Welling et al., 2003" startWordPosition="1402" endWordPosition="1405">pose we change the energy function by adding an infinitesimal multiple of a new feature f*. The log-likelihood after adding the feature can be approximated by – (7) ∂ ε where the derivative of L is taken at ε = 0. Because the optimal feature is the one that maximizes the increase in log-likelihood, we are searching for a feature that maximizes this derivative. Using equation (6) and noting that we have – This expression cannot be computed in practice, because the set of all word sequences S is infinite. The second term however can approximated using samples {ui} from the current model – 3 In (Welling et al., 2003) the term for P0 does not appear, which is equivalent to taking the uniform distribution as the baseline model. In other words, given a set of N samples {ui} from the model, the optimal feature to add is one that gives high scores to sampled sentences and low ones to real sentences. By labeling real sentences with 0 and sampled sentences with 1, the task of learning the feature translates into the task of training a classifier to discriminate between these two classes of sentences. In the remainder of the paper we will use feature and classifier interchangeably. 2.3 Rejection sampling Self-sup</context>
</contexts>
<marker>Welling, Zemel, Hinton, 2003</marker>
<rawString>Max Welling., Richard Zemel and Geoffrey E. Hinton. 2003. Self-Supervised Boosting. Advances in Neural Information Processing Systems, 15, MIT Press, Cambridge, MA</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>