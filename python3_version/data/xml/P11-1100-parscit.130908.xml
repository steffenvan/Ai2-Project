<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003166">
<title confidence="0.989731">
Automatically Evaluating Text Coherence Using Discourse Relations
</title>
<author confidence="0.998481">
Ziheng Lin, Hwee Tou Ng and Min-Yen Kan
</author>
<affiliation confidence="0.999894">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.971445">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.999576">
{linzihen,nght,kanmy}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999368555555556">
We present a novel model to represent and
assess the discourse coherence of text. Our
model assumes that coherent text implicitly
favors certain types of discourse relation tran-
sitions. We implement this model and apply it
towards the text ordering ranking task, which
aims to discern an original text from a per-
muted ordering of its sentences. The experi-
mental results demonstrate that our model is
able to significantly outperform the state-of-
the-art coherence model by Barzilay and Lap-
ata (2005), reducing the error rate of the previ-
ous approach by an average of 29% over three
data sets against human upper bounds. We fur-
ther show that our model is synergistic with
the previous approach, demonstrating an error
reduction of 73% when the features from both
models are combined for the task.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999806764705882">
The coherence of a text is usually reflected by its dis-
course structure and relations. In Rhetorical Struc-
ture Theory (RST), Mann and Thompson (1988) ob-
served that certain RST relations tend to favor one
of two possible canonical orderings. Some rela-
tions (e.g., Concessive and Conditional) favor ar-
ranging their satellite span before the nucleus span.
In contrast, other relations (e.g., Elaboration and Ev-
idence) usually order their nucleus before the satel-
lite. If a text that uses non-canonical relation order-
ings is rewritten to use canonical orderings, it often
improves text quality and coherence.
This notion of preferential ordering of discourse
relations is observed in natural language in general,
and generalizes to other discourse frameworks aside
from RST. The following example shows a Contrast
relation between the two sentences.
</bodyText>
<listItem confidence="0.892876333333333">
(1) [ Everyone agrees that most of the nation’s old
bridges need to be repaired or replaced. ]S, [ But
there’s disagreement over how to do it. ]S2
</listItem>
<bodyText confidence="0.98415262962963">
Here the second sentence provides contrasting infor-
mation to the first. If this order is violated without
rewording (i.e., if the two sentences are swapped), it
produces an incoherent text (Marcu, 1996).
In addition to the intra-relation ordering, such
preferences also extend to inter-relation ordering:
(2) [ The Constitution does not expressly give the
president such power. ]S, [ However, the president
does have a duty not to violate the Constitution. ]S2
[ The question is whether his only means of
defense is the veto. ]S.
The second sentence above provides a contrast to the
previous sentence and an explanation for the next
one. This pattern of Contrast-followed-by-Cause is
rather common in text (Pitler et al., 2008). Ordering
the three sentences differently results in incoherent,
cryptic text.
Thus coherent text exhibits measurable prefer-
ences for specific intra- and inter-discourse relation
ordering. Our key idea is to use the converse of this
phenomenon to assess the coherence of a text. In
this paper, we detail our model to capture the coher-
ence of a text based on the statistical distribution of
the discourse structure and relations. Our method
specifically focuses on the discourse relation transi-
tions between adjacent sentences, modeling them in
a discourse role matrix.
</bodyText>
<page confidence="0.950108">
997
</page>
<note confidence="0.979664">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 997–1006,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.993996267605634">
Our study makes additional contributions. We im- constraints and rules.
plement and validate our model on three data sets, To implement our proposal, we need to identify
which show robust improvements over the current the text’s discourse relations. This task, discourse
state-of-the-art for coherence assessment. We also parsing, has been a recent focus of study in the nat-
provide the first assessment of the upper-bound of ural language processing (NLP) community, largely
human performance on the standard task of distin- enabled by the availability of large-scale discourse
guishing coherent from incoherent orderings. To the annotated corpora (Wellner and Pustejovsky, 2007;
best our knowledge, this is also the first study in Elwell and Baldridge, 2008; Lin et al., 2009; Pitler
which we show output from an automatic discourse et al., 2009; Pitler and Nenkova, 2009; Lin et al.,
parser helps in coherence modeling. 2010; Wang et al., 2010). The Penn Discourse Tree-
2 Related Work bank (PDTB) (Prasad et al., 2008) is such a cor-
The study of coherence in discourse has led to many pus which provides a discourse-level annotation on
linguistic theories, of which we only discuss algo- top of the Penn Treebank, following a predicate-
rithms that have been reduced to practice. argument approach (Webber, 2004). Crucially, the
Barzilay and Lapata (2005; 2008) proposed an PDTB provides annotations not only on explicit (i.e.,
entity-based model to represent and assess local tex- signaled by discourse connectives such as because)
tual coherence. The model is motivated by Center- discourse relations, but also implicit (i.e., inferred
ing Theory (Grosz et al., 1995), which states that by readers) ones.
subsequent sentences in a locally coherent text are 3 Using Discourse Relations
likely to continue to focus on the same entities as To utilize discourse relations of a text, we first ap-
in previous sentences. Barzilay and Lapata op- ply automatic discourse parsing on the input text.
erationalized Centering Theory by creating an en- While any discourse framework, such as the Rhetor-
tity grid model to capture discourse entity transi- ical Structure Theory (RST), could be applied in our
tions at the sentence-to-sentence level, and demon- work to encode discourse information, we have cho-
strated their model’s ability to discern coherent texts sen to work with the Discourse Lexicalized Tree Ad-
from incoherent ones. Barzilay and Lee (2004) pro- joining Grammar (D-LTAG) by Webber (2004) as
posed a domain-dependent HMM model to capture embodied in the PDTB, as a PDTB-styled discourse
topic shift in a text, where topics are represented by parser1 developed by Lin et al. (2010) has recently
hidden states and sentences are observations. The become freely available.
global coherence of a text can then be summarized This parser tags each explicit/implicit relation
by the overall probability of topic shift from the first with two levels of relation types. In this work,
sentence to the last. Following these two directions, we utilize the four PDTB Level-1 types: Temporal
Soricut and Marcu (2006) and Elsner et al. (2007) (Temp), Contingency (Cont), Comparison (Comp),
combined the entity-based and HMM-based models and Expansion (Exp). This parser automatically
and demonstrated that these two models are comple- identifies the discourse relations, labels the argu-
mentary to each other in coherence assessment. ment spans, and classifies the relation types, includ-
Our approach differs from these models in that ing identifying common entity and no relation (En-
it introduces and operationalizes another indicator tRel and NoRel) as types.
of discourse coherence, by modeling a text’s dis- A simple approach to directly model the connec-
course relation transitions. Karamanis (2007) has tions among discourse relations is to use the se-
tried to integrate local discourse relations into the quence of discourse relation transitions. Text (2) in
Centering-based coherence metrics for the task of ont
information ordering, but was not able to obtain im- Section 1 can be represented by 51 C ) 52 C −→
provement over the baseline method, which is partly 53, for instance, when we use Level-1 types. In
due to the much smaller data set and the way the such a basic approach, we can compile a distribu-
discourse relation information is utilized in heuristic
998
1http://wing.comp.nus.edu.sg/˜linzihen/
parser/
tion of the n-gram discourse relation transition se-
quences in gold standard coherent text, and a similar
one for incoherent text. For example, the above text
would generate the transition bigram Comp→Cont.
We can build a classifier to distinguish one from the
other through learned examples or using a suitable
distribution distance measure (e.g., KL Divergence).
In our pilot work where we implemented such a
basic model with n-gram features for relation tran-
sitions, the performance was very poor. Our analy-
sis revealed a serious shortcoming: as the discourse
relation transitions in short texts are few in num-
ber, we have very little data to base the coherence
judgment on. However, when faced with even short
text excerpts, humans can distinguish coherent texts
from incoherent ones, as exemplified in our exam-
ple texts. The basic approach also does not model
the intra-relation preference. In Text (1), a Com-
parison (Comp) relation would be recorded between
the two sentences, irregardless of whether S1 or S2
comes first. However, it is clear that the ordering of
(S1 ≺ S2) is more coherent.
</bodyText>
<sectionHeader confidence="0.980266" genericHeader="method">
4 A Refined Approach
</sectionHeader>
<bodyText confidence="0.999958071428572">
The central problem with the basic approach is in its
sparse modeling of discourse relations. In develop-
ing an improved model, we need to better exploit the
discourse parser’s output to provide more circum-
stantial evidence to support the system’s coherence
decision.
In this section, we introduce the concept of a dis-
course role matrix which aims to capture an ex-
panded set of discourse relation transition patterns.
We describe how to represent the coherence of a text
with its discourse relations and how to transform
such information into a matrix representation. We
then illustrate how we use the matrix to formulate a
preference ranking problem.
</bodyText>
<subsectionHeader confidence="0.996384">
4.1 Discourse Role Matrix
</subsectionHeader>
<bodyText confidence="0.9934105">
Figure 1 shows a text and its gold standard PDTB
discourse relations. When a term appears in a dis-
course relation, the discourse role of this term is
defined as the discourse relation type plus the argu-
ment span in which the term is located (i.e., the argu-
ment tag). For instance, consider the term “cananea”
in the first relation. Since the relation type is a
[ Japan normally depends heavily on the Highland
Valley and Cananea mines as well as the Bougainville
mine in Papua New Guinea. ]S1 [ Recently, Japan
has been buying copper elsewhere. ]S2 [ [ But as
Highland Valley and Cananea begin operating, ]C3.1
[ they are expected to resume their roles as Japan’s
suppliers. ]C3.2 ]S3 [ [ According to Fred Demler,
metals economist for Drexel Burnham Lambert, New
York, ]C4.1 [ “Highland Valley has already started
operating ]C4.2 [ and Cananea is expected to do so
soon.” ]C4.3 ]S4
</bodyText>
<listItem confidence="0.947962090909091">
5 discourse relations are present in the above text:
1. Implicit Comparison between S1 as Arg1, and S2
as Arg2
2. Explicit Comparison using “but” between S2 as
Arg1, and S3 as Arg2
3. Explicit Temporal using “as” within S3 (Clause
C3.1 as Arg1, and C3.2 as Arg2)
4. Implicit Expansion between S3 as Arg1, and S4
as Arg2
5. Explicit Expansion using “and” within S4
(Clause C4.2 as Arg1, and C4.3 as Arg2)
</listItem>
<figureCaption confidence="0.990547333333333">
Figure 1: An excerpt with four contiguous sentences from
wsj 0437, showing five gold standard discourse relations.
“Cananea” is highlighted for illustration.
</figureCaption>
<table confidence="0.9989487">
S# Terms
copper cananea operat depend ...
S1 nil Comp.Arg1 nil Comp.Arg1
S2 Comp . Arg1nil nil nil
rg
S3 nil Comp.Arg2 Comp.Arg2 nil
Temp.Arg1 Temp.Arg1
Exp.Arg1 Exp.Arg1
S4 nil Exp.Arg2 Exp.Arg1 nil
Exp.Arg2
</table>
<tableCaption confidence="0.997503">
Table 1: Discourse role matrix fragment for Figure 1.
Rows correspond to sentences, columns to stemmed
terms, and cells contain extracted discourse roles.
</tableCaption>
<bodyText confidence="0.999980333333333">
Comparison and “cananea” is found in the Arg1
span, the discourse role of “cananea” is defined as
Comp.Arg1. When terms appear in different rela-
tions and/or argument spans, they obtain different
discourse roles in the text. For instance, “cananea”
plays a different discourse role of Temp.Arg1 in the
third relation in Figure 1. In the fourth relation,
since “cananea” appears in both argument spans, it
has two additional discourse roles, Exp.Arg1 and
</bodyText>
<page confidence="0.99328">
999
</page>
<bodyText confidence="0.999968905882354">
Exp.Arg2. The discourse role matrix thus represents
the different discourse roles of the terms across the
continuous text units. We use sentences as the text
units, and define terms to be the stemmed forms
of the open class words: nouns, verbs, adjectives,
and adverbs. We formulate the discourse role matrix
such that it encodes the discourse roles of the terms
across adjacent sentences.
Table 1 shows a fragment of the matrix represen-
tation of the text in Figure 1. Columns correspond to
the extracted terms; rows, the contiguous sentences.
A cell CT,,Sj then contains the set of the discourse
roles of the term Ti that appears in sentence 5j. For
example, the term “cananea” from 51 takes part in
the first relation, so the cell Ccananea,S1 contains the
role Comp.Arg1. A cell may be empty (nil, as in
Ccananea,S2) or contain multiple discourse roles (as
in Ccananea,S3, as “cananea” in 53 participates in
the second, third, and fourth relations). Given these
discourse relations, building the matrix is straight-
forward: we note down the relations that a term Ti
from a sentence 5j participates in, and record its dis-
course roles in the respective cell.
We hypothesize that the sequence of discourse
role transitions in a coherent text provides clues that
distinguish it from an incoherent text. The discourse
role matrix thus provides the foundation for com-
puting such role transitions, on a per term basis. In
fact, each column of the matrix corresponds to a
lexical chain (Morris and Hirst, 1991) for a partic-
ular term across the whole text. The key differences
from the traditional lexical chains are that our chain
nodes’ entities are simplified (they share the same
stemmed form, instead being connected by WordNet
relations), but are further enriched by being typed
with discourse relations.
We compile the set of sub-sequences of discourse
role transitions for every term in the matrix. These
transitions tell us how the discourse role of a term
varies through the progression of the text. For in-
stance, “cananea” functions as Comp.Arg1 in 51 and
Comp.Arg2 in 53, and plays the role of Exp.Arg1
and Exp.Arg2 in 53 and 54, respectively. As we
have six relation types (Temp(oral), Cont(ingency),
Comp(arison), Exp(ansion), EntRel and NoRel) and
two argument tags (Arg1 and Arg2) for each type,
we have a total of 6 x 2 = 12 possible dis-
course roles, plus a nil value. We define a dis-
course role transition as the sub-sequence of dis-
course roles for a term in multiple consecutive sen-
tences. For example, the discourse role transition of
“cananea” from 51 to 52 is Comp.Arg1—*nil. As a
cell may contain multiple discourse roles, a transi-
tion may produce multiple sub-sequences. For ex-
ample, the length 2 sub-sequences for “cananea”
from 53 to 54, are Comp.Arg2—*Exp.Arg2,
Temp.Arg1—*Exp.Arg2, and Exp.Arg1—*Exp.Arg2.
Each sub-sequence has a probability that can be
computed from the matrix. To illustrate the calcu-
lation, suppose the matrix fragment in Table 1 is
the entire discourse role matrix. Then since there
are in total 25 length 2 sub-sequences and the sub-
sequence Comp.Arg2—*Exp.Arg2 has a count of
two, its probability is 2/25 = 0.08. A key prop-
erty of our approach is that, while discourse tran-
sitions are captured locally on a per-term basis, the
probabilities of the discourse transitions are aggre-
gated globally, across all terms. We believe that the
overall distribution of discourse role transitions for
a coherent text is distinguishable from that for an in-
coherent text. Our model captures the distributional
differences of such sub-sequences in coherent and
incoherent text in training to determine an unseen
text’s coherence. To evaluate the coherence of a text,
we extract sub-sequences with various lengths from
the discourse role matrix as features2 and compute
the sub-sequence probabilities as the feature values.
To further refine the computation of the sub-
sequence distribution, we follow (Barzilay and Za-
pata, 2005) and divide the matrix into a salient ma-
trix and a non-salient matrix. Terms (columns) with
a frequency greater than a threshold form the salient
matrix, while the rest form the non-salient matrix.
The sub-sequence distributions are then calculated
separately for these two matrices.
</bodyText>
<subsectionHeader confidence="0.99857">
4.2 Preference Ranking
</subsectionHeader>
<bodyText confidence="0.999959">
While some texts can be said to be simply coherent
or incoherent, often it is a matter of degree. A text
can be less coherent when compared to one text, but
more coherent when compared to another. As such,
since the notion of coherence is relative, we feel
that coherence assessment is better represented as
</bodyText>
<footnote confidence="0.472078">
2Sub-sequences consisting of only nil values are not used as
features.
</footnote>
<page confidence="0.963058">
1000
</page>
<bodyText confidence="0.999751533333333">
a ranking problem rather than a classification prob-
lem. Given a pair of texts, the system ranks them
based on how coherent they are. Applications of
such a system include differentiating a text from its
permutation (i.e., the sentence ordering of the text
is shuffled) and identifying a more well-written es-
say from a pair. Such a system can easily generalize
from pairwise ranking into listwise, suitable for the
ordinal ranking of a set of texts. Coherence scoring
equations can also be deduced (Lapata and Barzilay,
2005) from such a model, yielding coherence scores.
To induce a model for preference ranking, we use
the SVMlight package3 by (Joachims, 1999) with
the preference ranking configuration for training and
testing. All parameters are set to their default values.
</bodyText>
<sectionHeader confidence="0.999844" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99986775">
We evaluate our coherence model on the task of text
ordering ranking, a standard coherence evaluation
task used in both (Barzilay and Lapata, 2005) and
(Elsner et al., 2007). In this task, the system is
asked to decide which of two texts is more coherent.
The pair of texts consists of a source text and one
of its permutations (i.e., the text’s sentence order is
randomized). Assuming that the original text is al-
ways more discourse-coherent than its permutation,
an ideal system will prefer the original to the per-
muted text. A system’s accuracy is thus the number
of times the system correctly chooses the original
divided by the total number of test pairs.
In order to acquire a large data set for training and
testing, we follow the approach in (Barzilay and La-
pata, 2005) to create a collection of synthetic data
from Wall Street Journal (WSJ) articles in the Penn
Treebank. All of the WSJ articles are randomly split
into a training and a testing set; 40 articles are held
out from the training set for development. For each
article, its sentences are permuted up to 20 times to
create a set of permutations4. Each permutation is
paired with its source text to form a pair.
We also evaluate on two other data collections
(cf. Table 2), provided by (Barzilay and Lapata,
2005), for a direct comparison with their entity-
based model. These two data sets consist of Associ-
ated Press articles about earthquakes from the North
</bodyText>
<footnote confidence="0.9805535">
3http://svmlight.joachims.org/
4Short articles may produce less than 20 permutations.
</footnote>
<table confidence="0.999497666666667">
WSJ Earthquakes Accidents
Train # Articles 1040 97 100
# Pairs 19120 1862 1996
Avg. # Sents 22.0 10.4 11.5
Test # Articles 1079 99 100
# Pairs 19896 1956 1986
</table>
<tableCaption confidence="0.991572">
Table 2: Details of the WSJ, Earthquakes, and Accidents
</tableCaption>
<bodyText confidence="0.976807648648649">
data sets, showing the number of training/testing articles,
number of pairs of articles, and average length of an arti-
cle (in sentences).
American News Corpus, and narratives from the Na-
tional Transportation Safety Board. These collec-
tions are much smaller than the WSJ data, as each
training/testing set contains only up to 100 source
articles. Similar to the WSJ data, we construct pairs
by permuting each source article up to 20 times.
Our model has two parameters: (1) the term fre-
quency (TF) that is used as a threshold to iden-
tify salient terms, and (2) the lengths of the sub-
sequences that are extracted as features. These pa-
rameters are tuned on the development set, and the
best ones that produce the optimal accuracy are
TF &gt;= 2 and lengths of the sub-sequences &lt;= 3.
We must also be careful in using the automatic
discourse parser. We note that the discourse parser
of Lin et al. (2010) comes trained on the PDTB,
which provides annotations on top of the whole WSJ
data. As we also use the WSJ data for evaluation,
we must avoid parsing an article that has already
been used in training the parser to prevent training
on the test data. We re-train the parser with 24 WSJ
sections and use the trained parser to parse the sen-
tences in our WSJ collection from the remaining
section. We repeat this re-training/parsing process
for all 25 sections. Because the Earthquakes and
Accidents data do not overlap with the WSJ training
data, we use the parser as distributed to parse these
two data sets. Since the discourse parser utilizes
paragraph boundaries but a permuted text does not
have such boundaries, we ignore paragraph bound-
aries and treat the source text as if it has only one
paragraph. This is to make sure that we do not give
the system extra information because of this differ-
ence between the source and permuted text.
</bodyText>
<page confidence="0.954772">
1001
</page>
<subsectionHeader confidence="0.969246">
5.1 Human Evaluation
</subsectionHeader>
<bodyText confidence="0.999958633333333">
While the text ordering ranking task has been used
in previous studies, two key questions about this task
have remained unaddressed in the previous work:
(1) to what extent is the assumption that the source
text is more coherent than its permutation correct?
and (2) how well do humans perform on this task?
The answer to the first is needed to validate the cor-
rectness of this synthetic task, while the second aims
to obtain the upper bound for evaluation. We con-
duct a human evaluation to answer these questions.
We randomly select 50 source text/permutation
pairs from each of the WSJ, Earthquakes, and Ac-
cidents training sets. We observe that some of the
source texts have formulaic structures in their ini-
tial sentences that give away the correct ordering.
Sources from the Earthquakes data always begin
with a headline sentence and a location-newswire
sentence, and many sources from the Accidents data
start with two sentences of “This is preliminary
... errors. Any errors ... completed.” We remove
these sentences from the source and permuted texts,
to avoid the subjects judging based on these clues in-
stead of textual coherence. For each set of 50 pairs,
we assigned two human subjects (who are not au-
thors of this paper) to perform the ranking. The sub-
jects are told to identify the source text from the pair.
When both subjects rank a source text higher than its
permutation, we interpret it as the subjects agreeing
that the source text is more coherent than the permu-
tation. Table 3 shows the inter-subject agreements.
</bodyText>
<table confidence="0.962102">
WSJ Earthquakes Accidents Overall
90.0 90.0 94.0 91.3
</table>
<tableCaption confidence="0.999754">
Table 3: Inter-subject agreements on the three data sets.
</tableCaption>
<bodyText confidence="0.9997183">
While our study is limited and only indicative, we
conclude from these results that the task is tractable.
Also, since our subjects’ judgments correlate highly
with the gold standard, the assumption that the orig-
inal text is always more coherent than the permuted
text is supported. Importantly though, human per-
formance is not perfect, suggesting fair upper bound
limits on system performance. We note that the Ac-
cidents data set is relatively easier to rank, as it has
a higher upper bound than the other two.
</bodyText>
<subsectionHeader confidence="0.986402">
5.2 Baseline
</subsectionHeader>
<bodyText confidence="0.999883777777777">
Barzilay and Lapata (2005) showed that their entity-
based model is able to distinguish a source text from
its permutation accurately. Thus, it can serve as a
good comparison point for our discourse relation-
based model. We compare against their Syn-
tax+Salience setting. Since they did not automat-
ically determine the coreferential information of a
permuted text but obtained that from its correspond-
ing source text, we do not perform automatic coref-
erence resolution in our reimplementation of their
system. For fair comparison, we follow their experi-
ment settings as closely as possible. We re-use their
Earthquakes and Accidents dataset as is, using their
exact permutations and pre-processing. For the WSJ
data, we need to perform our own pre-processing,
thus we employed the Stanford parser5 to perform
sentence segmentation and constituent parsing, fol-
lowed by entity extraction.
</bodyText>
<subsectionHeader confidence="0.606798">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999478">
We perform a series of experiments to answer the
following four questions:
</bodyText>
<listItem confidence="0.967344625">
1. Does our model outperform the baseline?
2. How do the different features derived from us-
ing relation types, argument tags, and salience
information affect performance?
3. Can the combination of the baseline and our
model outperform the single models?
4. How does system performance of these models
compare with human performance on the task?
</listItem>
<bodyText confidence="0.998931181818182">
Baseline results are shown in the first row of Ta-
ble 4. The results on the Earthquakes and Accidents
data are quite similar to those published in (Barzilay
and Lapata, 2005) (they reported 83.4% on Earth-
quakes and 89.7% on Accidents), validating the cor-
rectness of our reimplementation of their method.
Row 2 in Table 4 shows the overall performance
of the proposed refined model, answering Question
1. The model setting of Type+Arg+Sal means that
the model makes use of the discourse roles consist-
ing of 1) relation types and 2) argument tags (e.g.,
</bodyText>
<footnote confidence="0.965445">
5http://nlp.stanford.edu/software/
lex-parser.shtml
</footnote>
<page confidence="0.978832">
1002
</page>
<table confidence="0.999427625">
WSJ Earthquakes Accidents
Baseline 85.71 83.59 89.93
Type+Arg+Sal 88.06** 86.50** 89.38
Arg+Sal 88.28** 85.89* 87.06
Type+Sal 87.06** 82.98 86.05
Type+Arg 85.98 82.67 87.87
Baseline &amp; 89.25** 89.72** 91.64**
Type+Arg+Sal
</table>
<tableCaption confidence="0.533795714285714">
Table 4: Test set ranking accuracy. The first row shows
the baseline performance, the next four show our model
with different settings, and the last row is a combined
model. Double (**) and single (*) asterisks indicate that
the respective model significantly outperforms the base-
line at p &lt; 0.01 and p &lt; 0.05, respectively. We follow
Barzilay and Lapata (2008) and use the Fisher Sign test.
</tableCaption>
<bodyText confidence="0.999986315068494">
the discourse role Comp.Arg2 consists of the type
Comp(arison) and the tag Arg2), and 3) two dis-
tinct feature sets from salient and non-salient terms.
Comparing these accuracies to the baseline, our
model significantly outperforms the baseline with
p &lt; 0.01 in the WSJ and Earthquakes data sets
with accuracy increments of 2.35% and 2.91%, re-
spectively. In Accidents, our model’s performance
is slightly lower than the baseline, but the difference
is not statistically significant.
To answer Question 2, we perform feature abla-
tion testing. We eliminate each of the information
sources from the full model. In Row 3, we first
delete relation types from the discourse roles, which
causes discourse roles to only contain the argument
tags. A discourse role such as Comp.Arg2 becomes
Arg2 after deleting the relation type. Comparing
Row 3 to Row 2, we see performance reductions on
the Earthquakes and Accidents data after eliminat-
ing type information. Row 4 measures the effect of
omitting argument tags (Type+Sal). In this setting,
the discourse role Comp.Arg2 reduces to Comp. We
see a large reduction in performance across all three
data sets. This model is also most similar to the ba-
sic naive model in Section 3. These results suggest
that the argument tag information plays an impor-
tant role in our discourse role transition model. Row
5 omits the salience information (Type+Arg), which
also markedly reduces performance. This result sup-
ports the use of salience, in line with the conclusion
drawn in (Barzilay and Lapata, 2005).
To answer Question 3, we train and test a com-
bined model using features from both the baseline
and our model (shown as Row 6 in Table 4). The
entity-based model of Barzilay and Lapata (2005)
connects the local entity transition with textual co-
herence, while our model looks at the patterns of
discourse relation transitions. As these two models
focus on different aspects of coherence, we expect
that they are complementary to each other. The com-
bined model in all three data sets gives the highest
performance in comparison to all single models, and
it significantly outperforms the baseline model with
p &lt; 0.01. This confirms that the combined model is
linguistically richer than the single models as it inte-
grates different information together, and the entity-
based model and our model are synergistic.
To answer Question 4, when compared to the hu-
man upper bound (Table 3), the performance gaps
for the baseline model are relatively large, while
those for our full model are more acceptable in
the WSJ and Earthquakes data. For the combined
model, the error rates are significantly reduced in
all three data sets. The average error rate reduc-
tions against 100% are 9.57% for the full model and
26.37% for the combined model. If we compute the
average error rate reductions against the human up-
per bounds (rather than an oracular 100%), the aver-
age error rate reduction for the full model is 29% and
that for the combined model is 73%. While these are
only indicative results, they do highlight the signifi-
cant gains that our model is making towards reach-
ing human performance levels.
We further note that some of the permuted texts
may read as coherently as the original text. This phe-
nomenon has been observed in several natural lan-
guage synthesis tasks such as generation and sum-
marization, in which a single gold standard is inade-
quate to fully assess performance. As such, both au-
tomated systems and humans may actually perform
better than our performance measures indicate. We
leave it to future work to measure the impact of this
phenomenon.
</bodyText>
<sectionHeader confidence="0.931729" genericHeader="evaluation">
6 Analysis and Discussion
</sectionHeader>
<bodyText confidence="0.994901333333333">
When we compare the accuracies of the full model
in the three data sets (Row 2), the accuracy in the
Accidents data is the highest (89.38%), followed by
</bodyText>
<page confidence="0.969657">
1003
</page>
<bodyText confidence="0.999958">
that in the WSJ (88.06%), with Earthquakes at the
lowest (86.50%). To explain the variation, we exam-
ine the ratio between the number of the relations in
the article and the article length (i.e., number of sen-
tences). This ratio is 1.22 for the Accidents source
articles, 1.2 for the WSJ, and 1.08 for Earthquakes.
The relation/length ratio gives us an idea of how of-
ten a sentence participates in discourse relations. A
high ratio means that the article is densely intercon-
nected by discourse relations, and may make dis-
tinguishing this article from its permutation easier
compared to that for a loosely connected article.
We expect that when a text contains more dis-
course relation types (i.e., Temporal, Contingency,
Comparison, Expansion) and less EntRel and NoRel
types, it is easier to compute how coherent this text
is. This is because compared to EntRel and NoRel,
these four discourse relations can combine to pro-
duce meaningful transitions, such as the example
Text (2). To examine how this affects performance,
we calculate the average ratio between the number
of the four discourse relations in the permuted text
and the length for the permuted text. The ratio is
0.58 for those that are correctly ranked by our sys-
tem, and 0.48 for those that are incorrectly ranked,
which supports our hypothesis.
We also examined the learning curves for our
Type+Arg+Sal model, the baseline model, and the
combined model on the data sets, as shown in Fig-
ure 2(a)–2(c). In the WSJ data, the accuracies for
all three models increase rapidly as more pairs are
added to the training set. After 2,000 pairs, the in-
crease slows until 8,000 pairs, after which the curve
is nearly flat. From the curves, our model consis-
tently performs better than the baseline with a signif-
icant gap, and the combined model also consistently
and significantly outperforms the other two. Only
about half of the total training data is needed to reach
optimal performance for all three models. The learn-
ing curves in the Earthquakes data show that the per-
formance for all models is always increasing as more
training pairs are utilized. The Type+Arg+Sal and
combined models start with lower accuracies than
the baseline, but catch up with it at 1,000 and 400
pairs, respectively, and consistently outperform the
baseline beyond this point. On the other hand, the
learning curves for the Type+Arg+Sal and baseline
models in Accidents do not show any one curve con-
</bodyText>
<figure confidence="0.998845875">
0 4000 8000 12000 16000 20000
Number of pairs in training data
0 400 800 1200 1600 2000
Number of pairs in training data
(b) Earthquakes
0 400 800 1200 1600 2000
Number of pairs in training data
(c) Accidents
</figure>
<figureCaption confidence="0.9736195">
Figure 2: Learning curves for the Type+Arg+Sal, the
baseline, and the combined models on the three data sets.
</figureCaption>
<bodyText confidence="0.9997361">
sistently better than the other: our model outper-
forms in the middle segment but underperforms in
the first and last segments. The curve for the com-
bined model shows a consistently significant gap be-
tween it and the other two curves after the point at
400 pairs.
With the performance of the model as it is, how
can future work improve upon it? We point out one
weakness that we plan to explore. We use the full
Type+Arg+Sal model trained on the WSJ training
</bodyText>
<figure confidence="0.994231540540541">
75
70
90
85
80
65
60
55
Combined
Type+Arg+Sal
Baseline
(a) WSJ
Accuracy (%)
75
70
90
85
80
65
60
55
Combined
Type+Arg+Sal
Baseline
Accuracy (%)
75
70
90
85
80
65
60
55
Combined
Type+Arg+Sal
Baseline
Accuracy (%)
</figure>
<page confidence="0.987099">
1004
</page>
<bodyText confidence="0.998891630434783">
data to test Text (2) from the introduction. As (2) the entity-based local coherence model. While the
has 3 sentences, permuting it gives rise to 5 permu- entity-based model captures repetitive mentions of
tations. The model is able to correctly rank four entities, our discourse relation-based model gleans
of these 5 pairs. The only permutation it fails on its evidence from the argumentative and discourse
is (S3 ≺ S1 ≺ S2), when the last sentence is structure of the text. Our model is complementary to
moved to the beginning. A very good clue of co- the entity-based model, as it tackles the same prob-
herence in Text (2) is the explicit Comp relation lem from a different perspective. Experiments vali-
between S1 and S2. Since this clue is retained in date our claim, with a combined model outperform-
(S3 ≺ S1 ≺ S2), it is difficult for the system to dis- ing both single models.
tinguish this ordering from the source. In contrast, The idea of modeling coherence with discourse
as this clue is not present in the other four permuta- relations and formulating it in a discourse role ma-
tions, it is easier to distinguish them as incoherent. trix can also be applied to other NLP tasks. We
By modeling longer range discourse relation transi- plan to apply our methodology to other tasks, such
tions, we may be able to discern these two cases. as summarization, text generation and essay scoring,
While performance on identifying explicit dis- which also need to produce and assess discourse co-
course relations in the PDTB is as high as herence.
93% (Pitler et al., 2008), identifying implicit ones References
has been shown to be a difficult task with accuracy Regina Barzilay and Mirella Lapata. 2005. Modeling
of 40% at Level-2 types (Lin et al., 2009). As the local coherence: an entity-based approach. In Pro-
overall performance of the PDTB parser is still less ceedings of the 43rd Annual Meeting of the Associa-
accurate than we hope it to be, we expect that our tion for Computational Linguistics (ACL 2005), pages
proposed model will give better performance than 141–148, Morristown, NJ, USA. Association for Com-
it does now, when the current PDTB parser perfor- putational Linguistics.
mance is improved. Regina Barzilay and Mirella Lapata. 2008. Modeling
7 Conclusion local coherence: An entity-based approach. Computa-
We have proposed a new model for discourse co- tional Linguistics, 34:1–34, March.
herence that leverages the observation that coherent Regina Barzilay and Lillian Lee. 2004. Catching the
texts preferentially follow certain discourse struc- drift: Probabilistic content models, with applications
tures. We posit that these structures can be cap- to generation and summarization. In Proceedings of
tured in and represented by the patterns of discourse the Human Language Technology Conference /North
relation transitions. We first demonstrate that sim- American Chapter of the Association for Computa-
ply using the sequence of discourse relation tran- tional Linguistics Annual Meeting 2004.
sition leads to sparse features and is insufficient to Micha Elsner, Joseph Austerweil, and Eugene Charniak.
distinguish coherent from incoherent text. To ad- 2007. A unified local and global model for dis-
dress this, our method transforms the discourse re- course coherence. In Proceedings of the Conference
lation transitions into a discourse role matrix. The on Human Language Technology and North American
matrix schematically represents term occurrences in Chapter ofthe Association for Computational Linguis-
text units and associates each occurrence with its tics (HLT-NAACL 2007), Rochester, New York, USA,
discourse roles in the text units. In our approach, April.
n-gram sub-sequences of transitions per term in the Robert Elwell and Jason Baldridge. 2008. Discourse
discourse role matrix then constitute the more fine- connective argument identification with connective
grained evidence used in our model to distinguish specific rankers. In Proceedings of the IEEE Inter-
coherence from incoherence. national Conference on Semantic Computing (ICSC
When applied to distinguish a source text from 2010), Washington, DC, USA.
a sentence-reordered permutation, our model sig- Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.
nificantly outperforms the previous state-of-the-art, 1995. Centering: a framework for modeling the lo-
</bodyText>
<reference confidence="0.977230782051282">
1005 cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225, June.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
Schlkopf, Christopher J. C. Burges, and Alexander J.
Smola, editors, Advances in Kernel Methods – Support
Vector Learning, pages 169–184. MIT Press, Cam-
bridge, MA, USA.
Nikiforos Karamanis. 2007. Supplementing entity co-
herence with local rhetorical relations for information
ordering. Journal of Logic, Language and Informa-
tion, 16:445–464, October.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In Leslie Pack Kaelbling and Alessandro Saf-
fiotti, editors, Proceedings of the Nineteenth Interna-
tional Joint Conference on Artificial Intelligence, Ed-
inburgh, Scotland, UK.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2009), Singapore.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
PDTB-styled end-to-end discourse parser. Technical
Report TRB8/10, School of Computing, National Uni-
versity of Singapore, August.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.
Daniel Marcu. 1996. Distinguishing between coher-
ent and incoherent texts. In The Proceedings of the
Student Conference on Computational Linguistics in
Montreal, pages 136–143.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indicator of the
structure of text. Computational Linguistics, 17:21–
48, March.
Emily Pitler and Ani Nenkova. 2009. Using syntax
to disambiguate explicit discourse connectives in text.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, Singapore.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind Joshi. 2008. Easily
identifiable discourse relations. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING 2008) Short Papers, Manchester,
UK.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Au-
tomatic sense prediction for implicit discourse rela-
tions in text. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP (ACL-IJCNLP 2009), Sin-
gapore.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings ofthe COLING/ACL Main Conference Poster
Sessions, pages 803–810, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
WenTing Wang, Jian Su, and Chew Lim Tan. 2010. Ker-
nel based discourse relation recognition with tempo-
ral ordering information. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL 2010), Uppsala, Sweden, July.
Bonnie Webber. 2004. D-LTAG: Extending lexicalized
TAG to discourse. Cognitive Science, 28(5):751–779.
Ben Wellner and James Pustejovsky. 2007. Automati-
cally identifying the arguments of discourse connec-
tives. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), Prague, Czech Republic.
</reference>
<page confidence="0.989397">
1006
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.620326">
<title confidence="0.997824">Automatically Evaluating Text Coherence Using Discourse Relations</title>
<author confidence="0.999221">Hwee Tou Ng Lin</author>
<affiliation confidence="0.900524">Department of Computer National University of 13 Computing</affiliation>
<address confidence="0.846248">Singapore</address>
<abstract confidence="0.999794315789474">We present a novel model to represent and assess the discourse coherence of text. Our model assumes that coherent text implicitly favors certain types of discourse relation transitions. We implement this model and apply it towards the text ordering ranking task, which aims to discern an original text from a permuted ordering of its sentences. The experimental results demonstrate that our model is able to significantly outperform the state-ofthe-art coherence model by Barzilay and Lapata (2005), reducing the error rate of the previous approach by an average of 29% over three data sets against human upper bounds. We further show that our model is synergistic with the previous approach, demonstrating an error reduction of 73% when the features from both models are combined for the task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>