<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000053">
<title confidence="0.987708">
Relaxed Cross-lingual Projection of Constituent Syntax
</title>
<author confidence="0.978592">
Wenbin Jiang and Qun Liu and Yajuan L¨u
</author>
<affiliation confidence="0.984237666666667">
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<email confidence="0.992961">
{jiangwenbin, liuqun, lvyajuan}@ict.ac.cn
</email>
<sectionHeader confidence="0.996647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997">
We propose a relaxed correspondence as-
sumption for cross-lingual projection of con-
stituent syntax, which allows a supposed
constituent of the target sentence to corre-
spond to an unrestricted treelet in the source
parse. Such a relaxed assumption fundamen-
tally tolerates the syntactic non-isomorphism
between languages, and enables us to learn
the target-language-specific syntactic idiosyn-
crasy rather than a strained grammar di-
rectly projected from the source language syn-
tax. Based on this assumption, a novel con-
stituency projection method is also proposed
in order to induce a projected constituent tree-
bank from the source-parsed bilingual cor-
pus. Experiments show that, the parser trained
on the projected treebank dramatically out-
performs previous projected and unsupervised
parsers.
</bodyText>
<sectionHeader confidence="0.998886" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999924340425532">
For languages with treebanks, supervised models
give the state-of-the-art performance in dependency
parsing (McDonald and Pereira, 2006; Nivre et al.,
2006; Koo and Collins, 2010; Martins et al., 2010)
and constituent parsing (Collins, 2003; Charniak
and Johnson, 2005; Petrov et al., 2006). To break the
restriction of the treebank scale, lots of works have
been devoted to the unsupervised methods (Klein
and Manning, 2004; Bod, 2006; Seginer, 2007; Co-
hen and Smith, 2009) and the semi-supervised meth-
ods (Sarkar, 2001; Steedman et al., 2003; McClosky
et al., 2006; Koo et al., 2008) to utilize the unan-
notated text. In recent years, researchers have also
conducted many investigations on syntax projection
(Hwa et al., 2005; Ganchev et al., 2009; Smith and
Eisner, 2009; Jiang et al., 2010), in order to borrow
syntactic knowledge from another language.
Different from the bilingual parsing (Smith and
Smith, 2004; Burkett and Klein, 2008; Zhao et al.,
2009; Huang et al., 2009; Chen et al., 2010) that
improves parsing performance with bilingual con-
straints, and the bilingual grammar induction (Wu,
1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et
al., 2009) that induces grammar from parallel text,
the syntax projection aims to project the syntac-
tic knowledge from one language to another. This
seems especially promising for the languages that
have bilingual corpora parallel to resource-rich lan-
guages with large treebanks. Previous works mainly
focus on dependency projection. The dependency
relationship between words in the parsed source sen-
tences can be directly projected across the word
alignment to words in the target sentences, follow-
ing the direct correspondence assumption (DCA)
(Hwa et al., 2005). Due to the syntactic non-
isomorphism between languages, DCA assumption
usually leads to conflicting or incomplete projection.
Researchers have to adopt strategies to tackle this
problem, such as designing rules to handle language
non-isomorphism (Hwa et al., 2005), and resorting
to the quasi-synchronous grammar (Smith and Eis-
ner, 2009).
For constituency projection, however, the lack of
isomorphism becomes much more serious, since a
constituent grammar describes a language in a more
detailed way. In this paper we propose a relaxed
correspondence assumption (RCA) for constituency
</bodyText>
<page confidence="0.959339">
1192
</page>
<note confidence="0.788048333333333">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1192–1201,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
Z஍ԅ
</note>
<figureCaption confidence="0.899039333333333">
Figure 1: An example for constituency projection based on the RCA assumption. The projection is from English
to Chinese. A dash dot line links a projected constituent to its corresponding treelet, which is marked with gray
background; An Arabic numeral relates adirectly-projected constituent to its counter-part in the source parse.
</figureCaption>
<figure confidence="0.999637302325581">
Through
he veri
fied
[VP]
3
൐
PP
NP
[PP]
TOP TOP
DT JJ NN
1
[S]
1S
2 3
PP NP
VP
.
[S-PP-NP-VP-*]
PU
NP
PRP
4
5
VBD NP
.
PN
[S-PP-*-VP-*]
d
NP
a series of
NNS
experiments
ඹڶ ಬི
5[NP]
ᄃಬ ॴ
iFXಁ
VV AS
[NP-DT-JJ-*]
NN
esis
4 [VBD]
P NN
</figure>
<footnote confidence="0.7698556">
treebank fr
om the FBIS Chinese-English parallel
corpus with English sentences parsed by the Char-
niak parser. The Berkeley Parser trained on the pro-
1193 jected treebank dramatically outperforms the previ-
</footnote>
<bodyText confidence="0.9967192">
ous projected and unsupervised parsers. This pro-
vides an promising substitute for unsupervised pars-
ing methods, to the resource-scarce languages that
have bilingual corpora parallel to resource-rich lan-
guages with human-annotated treebanks.
In the rest of this paper we first presents the RCA
assumption, and the algorithm used to determine the
corresponding treelet in the source parse fora can-
didate constituent in the target sentence. Then we
word-aligned source-parsed parallel corpus. After
giving experimental results and the comparison with
d point out several as-
pects to be improved in the future work.
describe the induction of the projected PCFG gram-
mar and the projected constituent treebank from the
</bodyText>
<sectionHeader confidence="0.979668" genericHeader="method">
2 Relaxed Correspondence Assumption
</sectionHeader>
<equation confidence="0.9232794">
IN
2
the previous hypoth
DT NN IN
LC DEG
</equation>
<bodyText confidence="0.999858709677419">
projection. It allows a supposed constituent of
the target sentence to correspond to an unrestricted
treelet in the source parse. Such a relaxed as-
sumption fundamentally tolerates the syntactic non-
isomorphism between languages, and enables us to
learn the target-language-specific syntactic idiosyn-
crasy, rather than induce a strained grammar directly
projected from the source language syntax. We also
propose a novel cross-lingual projection method for
constituent syntax based on the RCA assumption.
Given aword-aligned source-parsed bilingual cor-
pus, aPCFG grammar can be induced for the target
language by maximum likelihood estimation on the
exhaustive enumeration of candidate projected pro-
ductions, where each nonterminal in a production
is an unrestricted treelet extracted from the source
parse. The projected PCFG grammar is then used
to parse each target sentence under the guidance of
the corresponding source tree, so as to produce an
optimized projected constituent tree.
Experiments validate the effectiveness of the
RCA assumption and the constituency projection
method. We induce a projected Chinese constituent
previous unsupervised and projected parsers, we fi-
nally conclude our work an
The DCA assumption (Hwa et al., 2005) works well
in dependency projection. A dependency grammar
describes a sentence in a compact manner where the
syntactic information is carried by the dependency
relationships between pairs of words. It is reason-
able to audaciously assume that the relationship of
</bodyText>
<listItem confidence="0.968948391304348">
Algorithm 1 Treelet Extraction Algorithm.
1: Input: Tf: parse tree of source sentence f
2: e: target sentence
3: A: word alignment of e and f
4: for i, j s.t. 1 &lt; i &lt; j &lt; |e |do &gt; all spans
5: t +- EXTTREELET(e, i, j, Tf, A)
6: T(i,j) +- PRUNETREE(t)
7: Output: treelet set T for all spans of e
8: function EXTTREELET(e, i, j, T, A)
9: if T aligns totally outside ei:j then
10: return 0
11: if T aligns totally inside ei:j then
12: return IT · root}
13: t +- IT · root} &gt; partly aligned inside ei:j
14: for each subtree s of T do
15: t +- t U EXTTREELET(e, i, j, s, A)
16: return t
17: function PRUNETREE(T)
18: for each node n in T do
19: merge n’s successive empty children
20: t + -T
21: while t has only one non-empty subtree do
22: t +- the non-empty subtree of t
</listItem>
<figure confidence="0.9708125">
23: return t
laxed correspondence.
(a) [TOP-[S-*-*-[VP-*-[NP-DT-JJ-*]]-*]] [NP-DT-JJ-*]
TOP NP
* *
DT JJ *
S
VP
*
* NP
DT JJ *
(b) [TOP-[S-*-[NP-[NP-DT-JJ-*]-*]-*-*]] [NP-DT-JJ-*]
TOP NP
DT JJ *
* NP
*
*
NP *
DT JJ *
S
</figure>
<figureCaption confidence="0.985391333333333">
Figure 2: Two examples for treelet pruning. Asterisks
indicate eliminated subtrees, which are represented as
empty children of their parent nodes.
</figureCaption>
<bodyText confidence="0.9999435">
a word pair in the source sentence also holds for
the corresponding word pair in the target sentence.
Compared with dependency grammar, constituent
grammar depicts syntax in a more complex way that
gives a sentence a hierarchically branched structure.
Therefore the lack of syntactic isomorphism for con-
stituency projection becomes much more serious, it
will be hard and inappropriate to directly project the
complex constituent structure from one language to
another.
For constituency projection, we propose a relaxed
corresponding assumption (RCA) to eliminate the
influence of syntactic non-isomorphism between the
source- and target languages. This assumption al-
lows a supposed constituent of the target sentence to
correspond to an unrestricted treelet in the source
parse. A treelet is a connected subgraph in the
source constituent tree, which covers a discontigu-
ous sequence of words of the source sentence. This
property enables a supposed constituent of the tar-
get sentence not necessarily to correspond to exactly
a constituent of the source parse, so as to funda-
mentally tolerate the syntactic non-isomorphism be-
tween languages. Figure 1 gives an example of re-
</bodyText>
<subsectionHeader confidence="0.998587">
2.1 Corresponding Treelet Extraction
</subsectionHeader>
<bodyText confidence="0.999993590909091">
According to the word alignment between the source
and target sentences, we can extract the treelet out of
the source parse for any possible constituent span of
the target sentence. Algorithm 1 shows the treelet
extraction algorithm.
Given the target sentence e, the parse tree Tf of
the source sentence f, and the word alignment A
between e and f, the algorithm extracts the corre-
sponding treelet out of Tf for each candidate span
of e (line 4-6). For a given span (i, j), its corre-
sponding treelet in Tf can be extracted by a recur-
sive top-down traversal in the tree. If all nodes in
the current subtree T align outside of source subse-
quence ei:j, the recursion stops and returns an empty
tree 0, indicating that the subtree is eliminated from
the final treelet (line 9-10). And, if all nodes in T
align inside ei:j, the root of T is returned as the con-
cise representation of the whole subtree (line 11-12).
For the third situation, that is to say T aligns partly
inside ei:j, the recursion has to continue to investi-
gate the subtrees of T (line 14-15). The recursive
traversal finally returns a treelet t that exactly corre-
</bodyText>
<page confidence="0.974104">
1194
</page>
<bodyText confidence="0.99997675">
sponds to the candidate constituent span hi, ji of the
source sentence.
We can find that even for a smaller span, the recur-
sive extraction procedure still starts from the root of
the source tree. This leads to a expatiatory treelet
with some redundant nodes on the top. Function
PRUNETREE takes charge of the treelet pruning (line
6). It traverses the treelet to merge the successive
empty sibling nodes (marked with asterisks) into one
(line 18-19), then conducts a top-down pruning to
delete the redundant branches until meeting a branch
with more than one non-empty subtrees (line 20-22).
Figure 2 shows the effect of the pruning operation
with two examples. The pruning operation maps the
two original treelets into the same simplified ver-
sion, that is, the pruned treelet. The branches pruned
out of the original treelet serve as the context of the
pruned treelet. The bracketed representations of the
pruned treelets, as shown above the treelet graphs,
are used as the nonterminals of the projected target
parses.
Since the overall complexity of the algorithm is
O(|e|3), it seems inefficient to collect the treelets
for all spans in the target sentence. But in fact it
runs fast on the realistic corpus in our experiments,
we assume that the function EXTTREELET doesn’t
always consume O(|e|) because of the more or less
isomorphism between two languages.
</bodyText>
<sectionHeader confidence="0.78673" genericHeader="method">
3 Projected Grammar and Treebank
</sectionHeader>
<bodyText confidence="0.997881">
This section describes how to build a projected con-
stituent treebank based on the RCA assumption. Ac-
cording to the last section, each span of the target
sentence could correspond to a treelet in the source
parse. If a span hi, ji has a corresponding treelet t,
a candidate projected constituent can be defined as a
triple hi, j, ti. For an n-way partition of this span,
</bodyText>
<equation confidence="0.794641">
hi, k1i, hk1 + 1, k2i, .., hkn−1 + 1, ji
</equation>
<bodyText confidence="0.992216379310345">
if each sub-span hkp−1+1, kpi corresponds to a can-
didate constituent hkp−1+1, kp, tpi, a candidate pro-
jected production can then be defined, denoted as
hi, j,ti → hi, k1, t1ihk1+1, k2, t2i..hkn−1+1, j, tni
There may be many candidate projected constituents
because of arbitrary combination, the tree projec-
tion procedure aims to find the optimum tree from
the parse forest determined by these candidate con-
stituents. Each production in the optimum tree
should satisfy this principle: the rule used in this
production appears in the whole corpus as frequently
as possible.
However, due to translation diversity and word
alignment error, the real constituent tree of the target
sentence may not be contained in the candidate pro-
jected constituents. We propose a relaxed and fault-
tolerant tree projection strategy to tackle this prob-
lem. First, based on the distribution of candidate
projected constituents over each single sentence, we
estimate the distribution over the whole corpus for
the rules used in these constituents, so as to obtain
a projected PCFG grammar. Then, using a PCFG
parser and this grammar, we parse each target sen-
tence under the guidance of the candidate projected
constituent set of the target sentence, so as to ob-
tain the optimum projected tree as far as possible.
In the following, we first describe the estimation of
the projected PCFG grammar and then show the tree
projection procedure.
</bodyText>
<subsectionHeader confidence="0.983993">
3.1 Projected PCFG Grammar
</subsectionHeader>
<bodyText confidence="0.999942875">
From a human-annotated treebank, we can induce a
PCFG grammar by estimating the frequency of the
production rules, which are contained in the produc-
tions of the trees. But for each target sentence we
don’t know which candidate productions consist the
correct constituent tree, so we can’t estimate the fre-
quency of the production rules directly.
A reasonable hypothesis is, if a candidate pro-
jected production for a target sentence happens to be
in the correct parse of the sentence, the rule used in
this production will appear frequently in the whole
corpus. We assume that each candidate projected
production may be a part of the correct parse, but
with different probabilities. If we give each candi-
date projected production an appropriate probabil-
ity and use this probability as the appearance fre-
quency of this production in the correct parse, we
can achieve an approximation of the PCFG gram-
mar hidden in the target sentences. In this work,
we restrict the productions to be binarized to reduce
the computational complexity. It results in a bina-
rized PCFG grammar, similar to previous unsuper-
vised works.
To estimate the frequencies of the candidate pro-
</bodyText>
<page confidence="0.964396">
1195
</page>
<bodyText confidence="0.999725">
ductions in the correct parse of the target sentence,
we need first estimate the frequencies of the candi-
date spans, which are described as follows:
</bodyText>
<equation confidence="0.9618375">
p((i, j)|e) = # of trees including (i, j) (1)
# of all trees
</equation>
<bodyText confidence="0.9991498">
The count of all binary trees of a target sentence e
can be calculated similar to the 0 value calculation
in the inside-outside algorithm. Without confusion,
we adopt the symbol 0(i, j) to denote the count of
binary tree for span (i, j):
</bodyText>
<equation confidence="0.999838">
0(i,j) =
0(i,k) &apos; 0(k + 1,j) i &lt; j
</equation>
<bodyText confidence="0.9522196">
0(1, |e|) is the count of binary trees of target sen-
tence e. We also need to calculate the count of bi-
nary tree fragments that cover the nodes outside span
(i, j). This is similar to the calculation of the α value
in the inside-outside algorithm. We also adopt the
</bodyText>
<equation confidence="0.918258333333333">
symbol α(i, j) here:
1 i = 1,j = |e|
α(i, j) =
+
�i − 1 α(k, j) &apos; 0(k, j − 1) else
k=1
</equation>
<bodyText confidence="0.998695666666667">
For simplicity we omit some conditions in above for-
mulas. The count of trees containing span (i, j) is
α(i, j) &apos; 0(i, j). Equation 1 can be rewritten as
</bodyText>
<equation confidence="0.995436333333333">
α(i,j) &apos; 0(i,j)
p((i,j)|e) = (4)
0(1, |e|)
</equation>
<bodyText confidence="0.9295275">
On condition that (i, j) is a span in the parse of e,
the probability that (i, j) has two children (i, k) and
</bodyText>
<equation confidence="0.980238666666667">
(k + 1,j) is
p((i,k)(k + 1,j)|(i,j)) = 0(i,k) &apos; 0(k + 1, j) (5)
0(i, j)
</equation>
<bodyText confidence="0.979241">
Therefore, the probability that (i, j) is a span in the
parse of e and has two children (i, k) and (k + 1, j)
can be calculated as follows:
</bodyText>
<equation confidence="0.99605425">
p((i,j) —� (i, k)(k + 1, j)|e)
= p((i, j)|e) &apos; p((i, k)(k + 1,j)|(i, j))
α(i, j) &apos; 0(i, k) &apos; 0(k + 1, j)
0(1, |e|)
</equation>
<bodyText confidence="0.99977115">
Since each candidate projected span aligns to one
treelet at most, this probability is also the frequency
of the candidate projected production related to the
three spans.
The counting approach above is based on the as-
sumption that there is a uniform distribution over the
projected trees for every target sentence. The inside
and outside algorithms and the other counting for-
mulae are used to calculate the expected counts un-
der this assumption. This looks like a single iteration
of EM.
A binarized projected PCFG grammar can then be
easily induced by maximum likelihood estimation.
Due to word alignment errors, free translation, and
exhaustive enumeration of possible projected pro-
ductions, such a PCFG grammar may contain too
much noisy nonterminals and production rules. We
introduce a threshold bRULE to filter the grammar. A
production rule can be reserved only if its frequency
is larger than bRULE.
</bodyText>
<subsectionHeader confidence="0.996576">
3.2 Relaxed Tree Projection
</subsectionHeader>
<bodyText confidence="0.9999866">
The projected PCFG grammar is used in the pro-
cedure of constituency projection. Such a gram-
mar, as a kind of global syntactic knowledge, can
attenuate the negative effect of word alignment er-
ror, free translation and syntactic non-isomorphism
for the constituency projection between each sin-
gle sentence pair. To obtain as optimal a projected
constituency tree as possible, we have to integrate
two kinds of knowledge: the local knowledge in
the candidate projected production set of the target
sentence, and the global knowledge in the projected
PCFG grammar.
The integrated projection strategy can be con-
ducted as follows. We parse each target sentence
with the projected PCFG grammar G, and use the
candidate projected production set D to guide the
PCFG parsing. The parsing procedure aims to find
an optimum projected tree, which maximizes both
the PCFG tree probability and the count of produc-
tions that also appear in the candidate projected pro-
</bodyText>
<equation confidence="0.988270636363637">
1
{
j−1 �
k=i
i = j
(2)
{
� |e |α(i, k) &apos; 0(k + 1, |e|) (3)
k=j+1
(6)
=
</equation>
<page confidence="0.959545">
1196
</page>
<figure confidence="0.826105">
# selected NTs
</figure>
<figureCaption confidence="0.996709666666667">
Figure 3: Rule counts corresponding to selected nonter-
minal sets, and their frequency summation proportions to
the whole rule set.
</figureCaption>
<figure confidence="0.99612">
1 2 4 8 16 32 64 128 256 512 1024
# selected NTs
</figure>
<figureCaption confidence="0.994638333333333">
Figure 4: Performance curve of the projected PCFG
grammars corresponding to different sizes of nontermi-
nal sets.
</figureCaption>
<figure confidence="0.998571965517241">
1 2 4 8 16 32 64 128 256 512 1024
16000
14000
12000
10000
4000
2000
8000
6000
0
# reserved rules
Percentage in all rules
40
20
90
80
70
60
50
30
0
10
Percentage in all rules Unlabeled F1 (%) 35
30
25
20
15
10
# reserved rules
</figure>
<bodyText confidence="0.811297">
duction set. The two optimization objectives can be
coordinated as follows:
</bodyText>
<equation confidence="0.981259">
y˜ = argmax ri (p(d|G) · eλ·δ(d,D)) (7)
y d∈y
</equation>
<bodyText confidence="0.9990716">
Here, d represents a production; S is a boolean func-
tion that returns 1 if d appears in D and returns 0
otherwise; A is a weight coefficient that needs to be
tuned to maximize the quality of the projected tree-
bank.
</bodyText>
<sectionHeader confidence="0.99958" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99982808">
Our work focuses on the constituency projection
from English to Chinese. The FBIS Chinese-English
parallel corpus is used to obtain a projected con-
stituent treebank. It contains 239 thousand sentence
pairs, with about 6.9/8.9 million Chinese/English
words. We parse the English sentences with the
Charniak Parser (Charniak and Johnson, 2005), and
tag the Chinese sentences with a POS tagger imple-
mented faithfully according to (Collins, 2002) and
trained on the Penn Chinese Treebank 5.0 (Xue et
al., 2005). We perform word alignment by runing
GIZA++ (Och and Ney, 2000), and then use the
alignment results for constituency projection.
Following the previous works of unsupervised
constituent parsing, we evaluate the projected parser
on the subsets of CTB 1.0 and CTB 5.0, which con-
tain no more than 10 or 40 words after the removal
of punctuation. The gold-standard POS tags are di-
rectly used for testing. The evaluation for unsu-
pervised parsing differs slightly from the standard
PARSEVAL metrics, it ignores the multiplicity of
brackets, brackets of span one, and the bracket la-
bels. In all experiments we report the unlabeled F1
value which is the harmonic mean of the unlabeled
precision and recall.
</bodyText>
<subsectionHeader confidence="0.965771">
4.1 Projected PCFG Grammar
</subsectionHeader>
<bodyText confidence="0.9986204">
An initial projected PCFG grammar can be induced
from the word-aligned and source-parsed parallel
corpus according to section 3.1. Such an initial
grammar is huge and contains a large amount of
projected nonterminals and production rules, where
many of them come from free translation and word
alignment errors. We conservatively set the filtra-
tion threshold bRULE as 1.0 to discard the rules with
frequency less than one, the rule count falls dramat-
ically from 3.3 millions to 92 thousands.
Figure 3 shows the statistics of the remained pro-
duction rules. We sort the projected nonterminals
according to their frequencies and select the top 2N
(1 &lt; N &lt; 10) best ones, and then discard the rules
that fall out of the selected nonterminal set. The fre-
quency summation of the rule set corresponding to
32 best nonterminals accounts for nearly 90% of the
frequency summation of the whole rule set.
We use the developing set of CTB 1.0 (chapter
301-325) to evaluate the performance of a series of
filtered grammars. Figure 4 gives the unlabeled F1
value of each grammar on all trees in the developing
set. The filtered grammar corresponding to the set
of top 32 nonterminals achieves the highest perfor-
mance. We denote this grammar as G32 and use it
</bodyText>
<page confidence="0.98298">
1197
</page>
<table confidence="0.7934992">
Unlabeled F1 (%) 40 Unlabeled F1 (%) 49.5
39 49
38 48.5
37 48
36 47.5
</table>
<page confidence="0.451964">
47
46.5
46
45.5
45
</page>
<figure confidence="0.798086">
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
Weight coefficient
</figure>
<figureCaption confidence="0.983118666666667">
Figure 5: Performance curve of the Berkeley Parser
trained on 5 thousand projected trees. The weight co-
efficient A ranges from 0 to 5.
</figureCaption>
<bodyText confidence="0.955651">
in the following tree projection procedure.
</bodyText>
<subsectionHeader confidence="0.99531">
4.2 Projected Treebank and Parser
</subsectionHeader>
<bodyText confidence="0.999950517241379">
The projected grammar G32 provides global syn-
tactic knowledge for constituency projection. Such
global knowledge and the local knowledge carried
by the candidate projected production set are inte-
grated in a linear weighted manner as in Formula
7. The weight coefficient A is tuned to maximize
the quality of the projected treebank, which is in-
directly measured by evaluating the performance of
the parser trained on it.
We select the first 5 thousand sentence pairs from
the Chinese-English FBIS corpus, and induce a se-
ries of projected treebanks using different A, ranging
from 0 to 5. Then we train the Berkeley Parser on
each projected treebank, and test it on the develop-
ing set of CTB 1.0. Figure 5 gives the performance
curve, which reports the unlabeled F1 values of the
projected parsers on all sentences of the developing
set. We find that the best performance is achieved
with A between 1 and 2.5, with slight fluctuation
in this range. It can be concluded that, the pro-
jected PCFG grammar and the candidate projected
production set do represent two different kinds of
constraints, and we can effectively coordinate them
by tuning the weight coefficient. Since different A
values in this range result in slight performance fluc-
tuation of the projected parser, we simply set it to 1
for the constituency projection on the whole FBIS
corpus.
There are more than 200 thousand projected trees
</bodyText>
<figure confidence="0.7123495">
5000 10000 20000 40000 80000 160000
Scale of treebank
</figure>
<figureCaption confidence="0.98138">
Figure 6: Performance curve of the Berkeley Parser
trained on different amounts of best project trees. The
scale of the selected treebank ranges from 5000 to
160000.
</figureCaption>
<bodyText confidence="0.976602285714286">
induced from the Chinese-English FBIS corpus. It
is a heavy burden for a parser to train on so large a
treebank. And on the other hand, the free translation
and word alignment errors result in many projected
trees of poor-quality. We design a criteria to approx-
imate the quality of the projected tree y for the target
sentence x:
</bodyText>
<equation confidence="0.961077333333333">
��
˜Q(y) = |x|−1 (p(d|G) · eA·δ(d,D)) (8)
d∈y
</equation>
<bodyText confidence="0.9896395">
and use an amount of best projected trees instead of
the whole projected treebank to train the parser. Fig-
ure 6 shows the performance of the Berkeley Parser
trained on different amounts of selected trees. The
performance of the Berkeley Parser constantly im-
proves along with the increment of selected trees.
However, treebanks containing more than 40 thou-
sand projected trees can not brings significant im-
provement. The parser trained on 160 thousand trees
only achieves an F1 increment of 0.4 points over the
one trained on 40 thousand trees. This indicates that
the newly added trees do not give the parser more
information due to their projection quality, and a
larger parallel corpus may lead to better parsing per-
formance.
The Berkeley Parser trained on 160 thousand best
projected trees is used in the final test. Table 1
gives the experimental results and the comparison
with related works. This is a sparse table since the
experiments of previous researchers focused on dif-
ferent data sets. Our projected parser significan
tly
</bodyText>
<page confidence="0.96428">
1198
</page>
<table confidence="0.999403">
System CTB-TEST-40 CTB1-ALL-10 CTB5-ALL-10 CTB5-ALL-40
(Klein and Manning, 2004) — 46.7 — —
(Bod, 2006) — 47.2 — —
(Seginer, 2007) — — 54.6 38.0
(Jiang et al., 2010) 40.4 — — —
our work 52.1 54.4 54.5 49.2
</table>
<tableCaption confidence="0.98938825">
Table 1: The performance of the Berkeley Parser trained on 160 thousand best projected trees, compared with previous
works on constituency projection and unsupervised parsing. CTB-TEST-40: sentences &lt; 40 words from CTB standard
test set (chapter 271-300); CTB1-ALL-10/CTB5-ALL-10: sentences &lt; 10 words from CTB 1.0/CTB 5.0 after the
removal of punctuation; CTB5-ALL-40: sentences &lt; 40 words from CTB 5.0 after the removal of punctuation.
</tableCaption>
<bodyText confidence="0.999860566666667">
outperforms the parser of Jiang et al. (2010), where
they directly adapt the DCA assumption of (Hwa
et al., 2005) from dependency projection to con-
stituency projection and resort to a better word align-
ment and a more complicated tree projection algo-
rithm. This indicates that the RCA assumption is
more suitable for constituency projection than the
DCA assumption, and can induce a better grammar
that much more reflects the language-specific syn-
tactic idiosyncrasy of the target language.
Our projected parser also obviously surpasses ex-
isting unsupervised parsers. The parser of Seginer
(2007) performs slightly better on CTB 5.0 sen-
tences no more than 10 words, but obviously falls
behind on sentences no more than 40 words. Fig-
ure 7 shows the unlabeled F1 of our parser on
a series of subsets of CTB 5.0 with different sen-
tence length upper limits. We find that even on the
whole treebank, our parser still gives a promising
result. Compared with unsupervised parsing, con-
stituency projection can make use of the syntactic
information of another language, so that it proba-
bly induce a better grammar. Although compar-
ing a syntax projection technique to supervised or
semi-supervised techniques seems unfair, it still sug-
gests that if a resource-poor language has a bilingual
corpus parallel to a resource-rich language with a
human-annotated treebank, the constituency projec-
tion based on RCA assumption is a promising sub-
stitute for unsupervised parsing.
</bodyText>
<sectionHeader confidence="0.995642" genericHeader="conclusions">
5 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.999907">
This paper describes a relaxed correspondence as-
sumption (RCA) for constituency projection. Un-
der this assumption a supposed constituent in the
target sentence can correspond to an unrestricted
</bodyText>
<figure confidence="0.974326153846154">
55
54
53
52
51
50
49
48
47
46
45
10 20 30 40 50 60 70 80 90 100+
Upper limit of sentence length
</figure>
<figureCaption confidence="0.992864">
Figure 7: Performance of the Berkeley Parser on subsets
of CTB 5.0 with different sentence length upper limits.
100+ indicates the whole treebank.
</figureCaption>
<bodyText confidence="0.999981190476191">
treelet in the parse of the source sentence. Different
from the direct correspondence assumption (DCA)
widely used in dependency projection, the RCA as-
sumption is more suitable for constituency projec-
tion, since it fundamentally tolerates the syntactic
non-isomorphism between the source and target lan-
guages. According to the RCA assumption we pro-
pose a novel constituency projection method. First, a
projected PCFG grammar is induced from the word-
aligned source-parsed parallel corpus. Then, the tree
projection is conducted on each sentence pair by a
PCFG parsing procedure, which integrates both the
global knowledge in the projected PCFG grammar
and the local knowledge in the set of candidate pro-
jected productions.
Experiments show that the parser trained on
the projected treebank significantly outperforms the
projected parsers based on the DCA assumption.
This validates the effectiveness of the RCA assump-
tion and the constituency projection method, and
indicates that the RCA assumption is more suit-
</bodyText>
<equation confidence="0.817731">
Unlabeled F1 (%)
</equation>
<page confidence="0.991117">
1199
</page>
<bodyText confidence="0.999965821428571">
able for constituency projection than the DCA as-
sumption. The projected parser also obviously sur-
passes the unsupervised parsers. This suggests
that if a resource-poor language has a bilingual
corpus parallel to a resource-rich language with a
human-annotated treebank, the constituency projec-
tion based on RCA assumption is an promising sub-
stitute for unsupervised methods.
Although achieving appealing results, our current
work is quite coarse and has many aspects to be im-
proved. First, the word alignment is the fundamental
precondition for projected grammar induction and
the following constituency projection, we can adopt
the better word alignment strategies to improve the
word alignment quality. Second, the PCFG grammar
is too weak due to its context free assumption, we
can adopt more complicated grammars such as TAG
(Joshi et al., 1975), in order to provide a more pow-
erful global syntactic constraints for the tree projec-
tion procedure. Third, the current tree projection
algorithm is too simple, more bilingual constraints
could lead to better projected trees. Last but not
least, the constituency projection and the unsuper-
vised parsing make use of different kinds of knowl-
edge, therefore the unsupervised methods can be in-
tegrated into the constituency projection framework
to achieve better projected grammars, treebanks, and
parsers.
</bodyText>
<sectionHeader confidence="0.998436" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997986">
The authors were supported by National Natural
Science Foundation of China Contract 90920004,
60736014 and 60873167. We are grateful to the
anonymous reviewers for their thorough reviewing
and valuable suggestions.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999925919354839">
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Pro-
ceedings of the NIPS.
Rens Bod. 2006. An all-subtrees approach to unsuper-
vised parsing. In Proceedings of the COLING-ACL.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of the EMNLP.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine-grained n-best parsing and discriminative rerank-
ing. In Proceedings of the ACL.
Wenliang Chen, Jun&apos;ichi Kazama, and Kentaro Tori-
sawa. 2010. Bitext dependency parsing with bilingual
subtree constraints. In Proceedings of the ACL.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
the NAACL-HLT.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the EMNLP.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of the 47th ACL.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the EMNLP.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
In Natural Language Engineering.
Wenbin Jiang, Yajuan L¨u, Yang Liu, and Qun Liu. 2010.
Effective constituent projection across languages. In
Proceedings of the COLING.
A. K. Joshi, L. S. Levy, and M. Takahashi. 1975. Tree
adjunct grammars. Journal Computer Systems Sci-
ence.
Dan Klein and Christopher D. Manning. 2004. Cor-
pusbased induction of syntactic structure: Models of
dependency and constituency. In Proceedings of the
ACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of the ACL.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the ACL.
Andr´e F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and M´ario A. T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings ofEMNLP.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of the ACL.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81–88.
</reference>
<page confidence="0.674076">
1200
</page>
<reference confidence="0.999860432432432">
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Labeled pseudoprojec-
tive dependency parsing with support vector machines.
In Proceedings of CoNLL, pages 221–225.
Franz J. Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of the ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the ACL.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings ofNAACL.
Yoav Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings of the ACL.
David Smith and Jason Eisner. 2009. Parser adaptation
and projection with quasi-synchronous grammar fea-
tures. In Proceedings of EMNLP.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using english to
parse korean. In Proceedings of the EMNLP.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proceedings of the ACL.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the EACL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of the ACL-IJCNLP.
</reference>
<page confidence="0.990076">
1201
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.606098">
<title confidence="0.998179">Relaxed Cross-lingual Projection of Constituent Syntax</title>
<author confidence="0.992898">Jiang Liu L¨u</author>
<affiliation confidence="0.972045">Key Laboratory of Intelligent Information Institute of Computing</affiliation>
<address confidence="0.720958">Chinese Academy of</address>
<email confidence="0.938039">liuqun,</email>
<abstract confidence="0.9950884">We propose a relaxed correspondence assumption for cross-lingual projection of constituent syntax, which allows a supposed constituent of the target sentence to correspond to an unrestricted treelet in the source parse. Such a relaxed assumption fundamentally tolerates the syntactic non-isomorphism between languages, and enables us to learn the target-language-specific syntactic idiosyncrasy rather than a strained grammar directly projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>Bayesian synchronous grammar induction.</title>
<date>2008</date>
<booktitle>In Proceedings of the NIPS.</booktitle>
<contexts>
<context position="2209" citStr="Blunsom et al., 2008" startWordPosition="327" endWordPosition="330">Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly projected across the word alignment to words in the target sentences, following the direct correspondence assumption (DCA) (Hwa et al., 2005). Due to the syntactic n</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. Bayesian synchronous grammar induction. In Proceedings of the NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An all-subtrees approach to unsupervised parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING-ACL.</booktitle>
<contexts>
<context position="1502" citStr="Bod, 2006" startWordPosition="214" endWordPosition="215">ed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance </context>
<context position="24822" citStr="Bod, 2006" startWordPosition="4168" endWordPosition="4169">and trees. This indicates that the newly added trees do not give the parser more information due to their projection quality, and a larger parallel corpus may lead to better parsing performance. The Berkeley Parser trained on 160 thousand best projected trees is used in the final test. Table 1 gives the experimental results and the comparison with related works. This is a sparse table since the experiments of previous researchers focused on different data sets. Our projected parser significan tly 1198 System CTB-TEST-40 CTB1-ALL-10 CTB5-ALL-10 CTB5-ALL-40 (Klein and Manning, 2004) — 46.7 — — (Bod, 2006) — 47.2 — — (Seginer, 2007) — — 54.6 38.0 (Jiang et al., 2010) 40.4 — — — our work 52.1 54.4 54.5 49.2 Table 1: The performance of the Berkeley Parser trained on 160 thousand best projected trees, compared with previous works on constituency projection and unsupervised parsing. CTB-TEST-40: sentences &lt; 40 words from CTB standard test set (chapter 271-300); CTB1-ALL-10/CTB5-ALL-10: sentences &lt; 10 words from CTB 1.0/CTB 5.0 after the removal of punctuation; CTB5-ALL-40: sentences &lt; 40 words from CTB 5.0 after the removal of punctuation. outperforms the parser of Jiang et al. (2010), where they d</context>
</contexts>
<marker>Bod, 2006</marker>
<rawString>Rens Bod. 2006. An all-subtrees approach to unsupervised parsing. In Proceedings of the COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="2008" citStr="Burkett and Klein, 2008" startWordPosition="294" endWordPosition="297">f the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the pars</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine-grained n-best parsing and discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1335" citStr="Charniak and Johnson, 2005" startWordPosition="184" endWordPosition="187">m the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different f</context>
<context position="19189" citStr="Charniak and Johnson, 2005" startWordPosition="3212" endWordPosition="3215">s follows: y˜ = argmax ri (p(d|G) · eλ·δ(d,D)) (7) y d∈y Here, d represents a production; S is a boolean function that returns 1 if d appears in D and returns 0 otherwise; A is a weight coefficient that needs to be tuned to maximize the quality of the projected treebank. 4 Experiments Our work focuses on the constituency projection from English to Chinese. The FBIS Chinese-English parallel corpus is used to obtain a projected constituent treebank. It contains 239 thousand sentence pairs, with about 6.9/8.9 million Chinese/English words. We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). We perform word alignment by runing GIZA++ (Och and Ney, 2000), and then use the alignment results for constituency projection. Following the previous works of unsupervised constituent parsing, we evaluate the projected parser on the subsets of CTB 1.0 and CTB 5.0, which contain no more than 10 or 40 words after the removal of punctuation. The gold-standard POS tags are directly used for testing. The evaluation for unsupervised p</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine-grained n-best parsing and discriminative reranking. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Jun&apos;ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Bitext dependency parsing with bilingual subtree constraints.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="2067" citStr="Chen et al., 2010" startWordPosition="306" endWordPosition="309">upervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly projected across the wo</context>
</contexts>
<marker>Chen, Kazama, Torisawa, 2010</marker>
<rawString>Wenliang Chen, Jun&apos;ichi Kazama, and Kentaro Torisawa. 2010. Bitext dependency parsing with bilingual subtree constraints. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL-HLT.</booktitle>
<contexts>
<context position="1541" citStr="Cohen and Smith, 2009" startWordPosition="218" endWordPosition="222">iments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bil</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proceedings of the NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="19290" citStr="Collins, 2002" startWordPosition="3230" endWordPosition="3231">that returns 1 if d appears in D and returns 0 otherwise; A is a weight coefficient that needs to be tuned to maximize the quality of the projected treebank. 4 Experiments Our work focuses on the constituency projection from English to Chinese. The FBIS Chinese-English parallel corpus is used to obtain a projected constituent treebank. It contains 239 thousand sentence pairs, with about 6.9/8.9 million Chinese/English words. We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). We perform word alignment by runing GIZA++ (Och and Ney, 2000), and then use the alignment results for constituency projection. Following the previous works of unsupervised constituent parsing, we evaluate the projected parser on the subsets of CTB 1.0 and CTB 5.0, which contain no more than 10 or 40 words after the removal of punctuation. The gold-standard POS tags are directly used for testing. The evaluation for unsupervised parsing differs slightly from the standard PARSEVAL metrics, it ignores the multiplicity of brackets, </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing. Computational Linguistics.</title>
<date>2003</date>
<contexts>
<context position="1307" citStr="Collins, 2003" startWordPosition="182" endWordPosition="183">y projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from a</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th ACL.</booktitle>
<contexts>
<context position="1815" citStr="Ganchev et al., 2009" startWordPosition="264" endWordPosition="267">a, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages </context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projection constraints. In Proceedings of the 47th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Bilingually-constrained (monolingual) shift-reduce parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="2047" citStr="Huang et al., 2009" startWordPosition="302" endWordPosition="305">n devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly pro</context>
</contexts>
<marker>Huang, Jiang, Liu, 2009</marker>
<rawString>Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>In Natural Language Engineering.</journal>
<contexts>
<context position="1793" citStr="Hwa et al., 2005" startWordPosition="260" endWordPosition="263">cDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promis</context>
<context position="3050" citStr="Hwa et al., 2005" startWordPosition="452" endWordPosition="455">l corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly projected across the word alignment to words in the target sentences, following the direct correspondence assumption (DCA) (Hwa et al., 2005). Due to the syntactic nonisomorphism between languages, DCA assumption usually leads to conflicting or incomplete projection. Researchers have to adopt strategies to tackle this problem, such as designing rules to handle language non-isomorphism (Hwa et al., 2005), and resorting to the quasi-synchronous grammar (Smith and Eisner, 2009). For constituency projection, however, the lack of isomorphism becomes much more serious, since a constituent grammar describes a language in a more detailed way. In this paper we propose a relaxed correspondence assumption (RCA) for constituency 1192 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1192–1201, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Zԅ Figure 1: An example for constituency projection based on the RCA assumpt</context>
<context position="6379" citStr="Hwa et al., 2005" startWordPosition="968" endWordPosition="971">od estimation on the exhaustive enumeration of candidate projected productions, where each nonterminal in a production is an unrestricted treelet extracted from the source parse. The projected PCFG grammar is then used to parse each target sentence under the guidance of the corresponding source tree, so as to produce an optimized projected constituent tree. Experiments validate the effectiveness of the RCA assumption and the constituency projection method. We induce a projected Chinese constituent previous unsupervised and projected parsers, we finally conclude our work an The DCA assumption (Hwa et al., 2005) works well in dependency projection. A dependency grammar describes a sentence in a compact manner where the syntactic information is carried by the dependency relationships between pairs of words. It is reasonable to audaciously assume that the relationship of Algorithm 1 Treelet Extraction Algorithm. 1: Input: Tf: parse tree of source sentence f 2: e: target sentence 3: A: word alignment of e and f 4: for i, j s.t. 1 &lt; i &lt; j &lt; |e |do &gt; all spans 5: t +- EXTTREELET(e, i, j, Tf, A) 6: T(i,j) +- PRUNETREE(t) 7: Output: treelet set T for all spans of e 8: function EXTTREELET(e, i, j, T, A) 9: i</context>
<context position="25476" citStr="Hwa et al., 2005" startWordPosition="4274" endWordPosition="4277">38.0 (Jiang et al., 2010) 40.4 — — — our work 52.1 54.4 54.5 49.2 Table 1: The performance of the Berkeley Parser trained on 160 thousand best projected trees, compared with previous works on constituency projection and unsupervised parsing. CTB-TEST-40: sentences &lt; 40 words from CTB standard test set (chapter 271-300); CTB1-ALL-10/CTB5-ALL-10: sentences &lt; 10 words from CTB 1.0/CTB 5.0 after the removal of punctuation; CTB5-ALL-40: sentences &lt; 40 words from CTB 5.0 after the removal of punctuation. outperforms the parser of Jiang et al. (2010), where they directly adapt the DCA assumption of (Hwa et al., 2005) from dependency projection to constituency projection and resort to a better word alignment and a more complicated tree projection algorithm. This indicates that the RCA assumption is more suitable for constituency projection than the DCA assumption, and can induce a better grammar that much more reflects the language-specific syntactic idiosyncrasy of the target language. Our projected parser also obviously surpasses existing unsupervised parsers. The parser of Seginer (2007) performs slightly better on CTB 5.0 sentences no more than 10 words, but obviously falls behind on sentences no more </context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. In Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Yajuan L¨u</author>
<author>Yang Liu</author>
<author>Qun Liu</author>
</authors>
<title>Effective constituent projection across languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the COLING.</booktitle>
<marker>Jiang, L¨u, Liu, Liu, 2010</marker>
<rawString>Wenbin Jiang, Yajuan L¨u, Yang Liu, and Qun Liu. 2010. Effective constituent projection across languages. In Proceedings of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal Computer Systems Science.</journal>
<contexts>
<context position="29172" citStr="Joshi et al., 1975" startWordPosition="4858" endWordPosition="4861">nguage with a human-annotated treebank, the constituency projection based on RCA assumption is an promising substitute for unsupervised methods. Although achieving appealing results, our current work is quite coarse and has many aspects to be improved. First, the word alignment is the fundamental precondition for projected grammar induction and the following constituency projection, we can adopt the better word alignment strategies to improve the word alignment quality. Second, the PCFG grammar is too weak due to its context free assumption, we can adopt more complicated grammars such as TAG (Joshi et al., 1975), in order to provide a more powerful global syntactic constraints for the tree projection procedure. Third, the current tree projection algorithm is too simple, more bilingual constraints could lead to better projected trees. Last but not least, the constituency projection and the unsupervised parsing make use of different kinds of knowledge, therefore the unsupervised methods can be integrated into the constituency projection framework to achieve better projected grammars, treebanks, and parsers. Acknowledgments The authors were supported by National Natural Science Foundation of China Contr</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>A. K. Joshi, L. S. Levy, and M. Takahashi. 1975. Tree adjunct grammars. Journal Computer Systems Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1491" citStr="Klein and Manning, 2004" startWordPosition="210" endWordPosition="213">bank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing p</context>
<context position="24799" citStr="Klein and Manning, 2004" startWordPosition="4160" endWordPosition="4163">ints over the one trained on 40 thousand trees. This indicates that the newly added trees do not give the parser more information due to their projection quality, and a larger parallel corpus may lead to better parsing performance. The Berkeley Parser trained on 160 thousand best projected trees is used in the final test. Table 1 gives the experimental results and the comparison with related works. This is a sparse table since the experiments of previous researchers focused on different data sets. Our projected parser significan tly 1198 System CTB-TEST-40 CTB1-ALL-10 CTB5-ALL-10 CTB5-ALL-40 (Klein and Manning, 2004) — 46.7 — — (Bod, 2006) — 47.2 — — (Seginer, 2007) — — 54.6 38.0 (Jiang et al., 2010) 40.4 — — — our work 52.1 54.4 54.5 49.2 Table 1: The performance of the Berkeley Parser trained on 160 thousand best projected trees, compared with previous works on constituency projection and unsupervised parsing. CTB-TEST-40: sentences &lt; 40 words from CTB standard test set (chapter 271-300); CTB1-ALL-10/CTB5-ALL-10: sentences &lt; 10 words from CTB 1.0/CTB 5.0 after the removal of punctuation; CTB5-ALL-40: sentences &lt; 40 words from CTB 5.0 after the removal of punctuation. outperforms the parser of Jiang et a</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1245" citStr="Koo and Collins, 2010" startWordPosition="171" endWordPosition="174">specific syntactic idiosyncrasy rather than a strained grammar directly projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jian</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1652" citStr="Koo et al., 2008" startWordPosition="238" endWordPosition="241">ervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonas Kuhn</author>
</authors>
<title>Experiments in parallel-text based grammar induction.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="2187" citStr="Kuhn, 2004" startWordPosition="325" endWordPosition="326">rkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly projected across the word alignment to words in the target sentences, following the direct correspondence assumption (DCA) (Hwa et al., 2005). </context>
</contexts>
<marker>Kuhn, 2004</marker>
<rawString>Jonas Kuhn. 2004. Experiments in parallel-text based grammar induction. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
<author>Pedro M Q Aguiar</author>
<author>M´ario A T Figueiredo</author>
</authors>
<title>Turbo parsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="1268" citStr="Martins et al., 2010" startWordPosition="175" endWordPosition="178">syncrasy rather than a strained grammar directly projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in ord</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, Eric P. Xing, Pedro M. Q. Aguiar, and M´ario A. T. Figueiredo. 2010. Turbo parsers: Dependency parsing by approximate variational inference. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1633" citStr="McClosky et al., 2006" startWordPosition="234" endWordPosition="237">ous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) t</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="1202" citStr="McDonald and Pereira, 2006" startWordPosition="163" endWordPosition="166">es, and enables us to learn the target-language-specific syntactic idiosyncrasy rather than a strained grammar directly projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Gulsen Eryigit</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled pseudoprojective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>221--225</pages>
<contexts>
<context position="1222" citStr="Nivre et al., 2006" startWordPosition="167" endWordPosition="170">the target-language-specific syntactic idiosyncrasy rather than a strained grammar directly projected from the source language syntax. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Eryigit, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit, and Svetoslav Marinov. 2006. Labeled pseudoprojective dependency parsing with support vector machines. In Proceedings of CoNLL, pages 221–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="19418" citStr="Och and Ney, 2000" startWordPosition="3251" endWordPosition="3254">uality of the projected treebank. 4 Experiments Our work focuses on the constituency projection from English to Chinese. The FBIS Chinese-English parallel corpus is used to obtain a projected constituent treebank. It contains 239 thousand sentence pairs, with about 6.9/8.9 million Chinese/English words. We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). We perform word alignment by runing GIZA++ (Och and Ney, 2000), and then use the alignment results for constituency projection. Following the previous works of unsupervised constituent parsing, we evaluate the projected parser on the subsets of CTB 1.0 and CTB 5.0, which contain no more than 10 or 40 words after the removal of punctuation. The gold-standard POS tags are directly used for testing. The evaluation for unsupervised parsing differs slightly from the standard PARSEVAL metrics, it ignores the multiplicity of brackets, brackets of span one, and the bracket labels. In all experiments we report the unlabeled F1 value which is the harmonic mean of </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz J. Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1357" citStr="Petrov et al., 2006" startWordPosition="188" endWordPosition="191">. Based on this assumption, a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual pars</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="1587" citStr="Sarkar, 2001" startWordPosition="228" endWordPosition="229">eebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Fast unsupervised incremental parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1517" citStr="Seginer, 2007" startWordPosition="216" endWordPosition="217">l corpus. Experiments show that, the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual </context>
<context position="24849" citStr="Seginer, 2007" startWordPosition="4174" endWordPosition="4175">es that the newly added trees do not give the parser more information due to their projection quality, and a larger parallel corpus may lead to better parsing performance. The Berkeley Parser trained on 160 thousand best projected trees is used in the final test. Table 1 gives the experimental results and the comparison with related works. This is a sparse table since the experiments of previous researchers focused on different data sets. Our projected parser significan tly 1198 System CTB-TEST-40 CTB1-ALL-10 CTB5-ALL-10 CTB5-ALL-40 (Klein and Manning, 2004) — 46.7 — — (Bod, 2006) — 47.2 — — (Seginer, 2007) — — 54.6 38.0 (Jiang et al., 2010) 40.4 — — — our work 52.1 54.4 54.5 49.2 Table 1: The performance of the Berkeley Parser trained on 160 thousand best projected trees, compared with previous works on constituency projection and unsupervised parsing. CTB-TEST-40: sentences &lt; 40 words from CTB standard test set (chapter 271-300); CTB1-ALL-10/CTB5-ALL-10: sentences &lt; 10 words from CTB 1.0/CTB 5.0 after the removal of punctuation; CTB5-ALL-40: sentences &lt; 40 words from CTB 5.0 after the removal of punctuation. outperforms the parser of Jiang et al. (2010), where they directly adapt the DCA assum</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer. 2007. Fast unsupervised incremental parsing. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1839" citStr="Smith and Eisner, 2009" startWordPosition="268" endWordPosition="271"> 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corp</context>
<context position="3123" citStr="Smith and Eisner, 2009" startWordPosition="462" endWordPosition="466"> Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly projected across the word alignment to words in the target sentences, following the direct correspondence assumption (DCA) (Hwa et al., 2005). Due to the syntactic nonisomorphism between languages, DCA assumption usually leads to conflicting or incomplete projection. Researchers have to adopt strategies to tackle this problem, such as designing rules to handle language non-isomorphism (Hwa et al., 2005), and resorting to the quasi-synchronous grammar (Smith and Eisner, 2009). For constituency projection, however, the lack of isomorphism becomes much more serious, since a constituent grammar describes a language in a more detailed way. In this paper we propose a relaxed correspondence assumption (RCA) for constituency 1192 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1192–1201, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Zԅ Figure 1: An example for constituency projection based on the RCA assumption. The projection is from English to Chinese. A dash dot line links a p</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>David Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Bilingual parsing with factored estimation: Using english to parse korean.</title>
<date>2004</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="1983" citStr="Smith and Smith, 2004" startWordPosition="290" endWordPosition="293">break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship </context>
</contexts>
<marker>Smith, Smith, 2004</marker>
<rawString>David A. Smith and Noah A. Smith. 2004. Bilingual parsing with factored estimation: Using english to parse korean. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="2231" citStr="Snyder et al., 2009" startWordPosition="331" endWordPosition="334"> McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly projected across the word alignment to words in the target sentences, following the direct correspondence assumption (DCA) (Hwa et al., 2005). Due to the syntactic nonisomorphism between </context>
</contexts>
<marker>Snyder, Naseem, Barzilay, 2009</marker>
<rawString>Benjamin Snyder, Tahira Naseem, and Regina Barzilay. 2009. Unsupervised multilingual grammar induction. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Miles Osborne</author>
<author>Anoop Sarkar</author>
<author>Stephen Clark</author>
<author>Rebecca Hwa</author>
<author>Julia Hockenmaier</author>
<author>Paul Ruhlen</author>
<author>Steven Baker</author>
<author>Jeremiah Crim</author>
</authors>
<title>Bootstrapping statistical parsers from small datasets.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL.</booktitle>
<contexts>
<context position="1610" citStr="Steedman et al., 2003" startWordPosition="230" endWordPosition="233">cally outperforms previous projected and unsupervised parsers. 1 Introduction For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). To break the restriction of the treebank scale, lots of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008;</context>
</contexts>
<marker>Steedman, Osborne, Sarkar, Clark, Hwa, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In Proceedings of the EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="2175" citStr="Wu, 1997" startWordPosition="323" endWordPosition="324">ethods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences can be directly projected across the word alignment to words in the target sentences, following the direct correspondence assumption (DCA) (Hwa et </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>In Natural Language Engineering.</journal>
<contexts>
<context position="19354" citStr="Xue et al., 2005" startWordPosition="3240" endWordPosition="3243">s a weight coefficient that needs to be tuned to maximize the quality of the projected treebank. 4 Experiments Our work focuses on the constituency projection from English to Chinese. The FBIS Chinese-English parallel corpus is used to obtain a projected constituent treebank. It contains 239 thousand sentence pairs, with about 6.9/8.9 million Chinese/English words. We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). We perform word alignment by runing GIZA++ (Och and Ney, 2000), and then use the alignment results for constituency projection. Following the previous works of unsupervised constituent parsing, we evaluate the projected parser on the subsets of CTB 1.0 and CTB 5.0, which contain no more than 10 or 40 words after the removal of punctuation. The gold-standard POS tags are directly used for testing. The evaluation for unsupervised parsing differs slightly from the standard PARSEVAL metrics, it ignores the multiplicity of brackets, brackets of span one, and the bracket labels. In all experiments</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. In Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Yan Song</author>
<author>Chunyu Kit</author>
<author>Guodong Zhou</author>
</authors>
<title>Cross language dependency parsing using a bilingual lexicon.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP.</booktitle>
<contexts>
<context position="2027" citStr="Zhao et al., 2009" startWordPosition="298" endWordPosition="301">s of works have been devoted to the unsupervised methods (Klein and Manning, 2004; Bod, 2006; Seginer, 2007; Cohen and Smith, 2009) and the semi-supervised methods (Sarkar, 2001; Steedman et al., 2003; McClosky et al., 2006; Koo et al., 2008) to utilize the unannotated text. In recent years, researchers have also conducted many investigations on syntax projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2010), in order to borrow syntactic knowledge from another language. Different from the bilingual parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009; Chen et al., 2010) that improves parsing performance with bilingual constraints, and the bilingual grammar induction (Wu, 1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et al., 2009) that induces grammar from parallel text, the syntax projection aims to project the syntactic knowledge from one language to another. This seems especially promising for the languages that have bilingual corpora parallel to resource-rich languages with large treebanks. Previous works mainly focus on dependency projection. The dependency relationship between words in the parsed source sentences</context>
</contexts>
<marker>Zhao, Song, Kit, Zhou, 2009</marker>
<rawString>Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou. 2009. Cross language dependency parsing using a bilingual lexicon. In Proceedings of the ACL-IJCNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>