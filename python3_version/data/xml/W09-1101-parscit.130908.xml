<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.149426">
<title confidence="0.991895">
Joint Inference for Natural Language Processing
</title>
<author confidence="0.997547">
Andrew McCallum
</author>
<affiliation confidence="0.9983995">
Department of Computer Science
University of Massachusetts Amherst
</affiliation>
<address confidence="0.438113">
Amherst, MA 01002
</address>
<email confidence="0.979971">
mccallum@cs.umass.edu
</email>
<sectionHeader confidence="0.512454" genericHeader="abstract">
Abstract of the Invited Talk
</sectionHeader>
<bodyText confidence="0.999980297297297">
In recent decades, researchers in natural language
processing have made great progress on well-
defined subproblems such as part-of-speech tagging,
phrase chunking, syntactic parsing, named-entity
recognition, coreference and semantic-role label-
ing. Better models, features, and learning algorithms
have allowed systems to perform many of these tasks
with 90% accuracy or better. However, success in in-
tegrated, end-to-end natural language understanding
remains elusive.
I contend that the chief reason for this failure
is that errors cascade and accumulate through a
pipeline of naively chained components. For exam-
ple, if we naively use the single most likely output
of a part-of-speech tagger as the input to a syntactic
parser, and those parse trees as the input to a coref-
erence system, and so on, errors in each step will
propagate to later ones: each components 90% ac-
curacy multiplied through six components becomes
only 53%.
Consider, for instance, the sentence “I know you
like your mother.” If a part-of-speech tagger de-
terministically labels “like” as a verb, then certain
later syntactic and semantic analysis will be blocked
from alternative interpretations, such as “I know you
like your mother (does).” The part-of-speech tag-
ger needs more syntactic and semantic information
to make this choice. Consider also the classic exam-
ple “The boy saw the man with the telescope.” No
single correct syntactic parse of this sentence is pos-
sible in isolation. Correct interpretation requires the
integration of these syntactic decisions with seman-
tics and context.
Humans manage and resolve ambiguity by uni-
fied, simultaneous consideration of morphology,
syntax, semantics, pragmatics and other contextual
information. In statistical modeling such unified
</bodyText>
<page confidence="0.808616">
1
</page>
<bodyText confidence="0.998288314285714">
consideration is known as joint inference. The need
for joint inference appears not only in natural lan-
guage processing, but also in information integra-
tion, computer vision, robotics and elsewhere. All of
these applications require integrating evidence from
multiple sources, at multiple levels of abstraction. I
believe that joint inference is one of the most fun-
damentally central issues in all of artificial intelli-
gence.
In this talk I will describe work in probabilistic
models that perform joint inference across multiple
components of an information processing pipeline
in order to avoid the brittle accumulation of errors.
I will survey work in exact inference, variational
inference and Markov-chain Monte Carlo methods.
We will discuss various approaches that have been
applied to natural language processing, and hypoth-
esize about why joint inference has helped in some
cases, and not in others.
I will then focus on our recent work at Univer-
sity of Massachusetts in large-scale conditional ran-
dom fields with complex relational structure. In a
single factor graph we seamlessly integrate multiple
subproblems, using our new probabilistic program-
ming language to compactly express complex, muta-
ble variable-factor structure both in first-order logic
as well as in more expressive Turing-complete im-
perative procedures. We avoid unrolling this graph-
ical model by using Markov-chain Monte Carlo for
inference, and make inference more efficient with
learned proposal distributions. Parameter estimation
is performed by SampleRank, which avoids com-
plete inference as a subroutine by learning simply
to correctly rank successive states of the Markov-
chain.
</bodyText>
<reference confidence="0.9612802">
Joint work with Aron Culotta, Michael Wick,
Rob Hall, Khashayar Rohanimanesh, Karl Schultz,
Sameer Singh, Charles Sutton and David Smith.
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), page 1,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.320894">
<title confidence="0.999792">Joint Inference for Natural Language Processing</title>
<author confidence="0.99448">Andrew</author>
<affiliation confidence="0.99997">Department of Computer University of Massachusetts</affiliation>
<address confidence="0.981922">Amherst, MA</address>
<email confidence="0.999555">mccallum@cs.umass.edu</email>
<abstract confidence="0.999056864864865">of the Invited Talk In recent decades, researchers in natural language processing have made great progress on welldefined subproblems such as part-of-speech tagging, phrase chunking, syntactic parsing, named-entity recognition, coreference and semantic-role labeling. Better models, features, and learning algorithms have allowed systems to perform many of these tasks with 90% accuracy or better. However, success in integrated, end-to-end natural language understanding remains elusive. I contend that the chief reason for this failure is that errors cascade and accumulate through a pipeline of naively chained components. For example, if we naively use the single most likely output of a part-of-speech tagger as the input to a syntactic parser, and those parse trees as the input to a coreference system, and so on, errors in each step will propagate to later ones: each components 90% accuracy multiplied through six components becomes only 53%. Consider, for instance, the sentence “I know you like your mother.” If a part-of-speech tagger deterministically labels “like” as a verb, then certain later syntactic and semantic analysis will be blocked from alternative interpretations, such as “I know you like your mother (does).” The part-of-speech tagger needs more syntactic and semantic information to make this choice. Consider also the classic example “The boy saw the man with the telescope.” No single correct syntactic parse of this sentence is possible in isolation. Correct interpretation requires the integration of these syntactic decisions with semantics and context. Humans manage and resolve ambiguity by unified, simultaneous consideration of morphology, syntax, semantics, pragmatics and other contextual information. In statistical modeling such unified 1 consideration is known as joint inference. The need for joint inference appears not only in natural language processing, but also in information integration, computer vision, robotics and elsewhere. All of these applications require integrating evidence from multiple sources, at multiple levels of abstraction. I believe that joint inference is one of the most fundamentally central issues in all of artificial intelligence. In this talk I will describe work in probabilistic models that perform joint inference across multiple components of an information processing pipeline in order to avoid the brittle accumulation of errors. I will survey work in exact inference, variational inference and Markov-chain Monte Carlo methods. We will discuss various approaches that have been applied to natural language processing, and hypothesize about why joint inference has helped in some cases, and not in others. I will then focus on our recent work at University of Massachusetts in large-scale conditional random fields with complex relational structure. In a single factor graph we seamlessly integrate multiple subproblems, using our new probabilistic programming language to compactly express complex, mutable variable-factor structure both in first-order logic as well as in more expressive Turing-complete imperative procedures. We avoid unrolling this graphical model by using Markov-chain Monte Carlo for inference, and make inference more efficient with learned proposal distributions. Parameter estimation is performed by SampleRank, which avoids complete inference as a subroutine by learning simply to correctly rank successive states of the Markovchain.</abstract>
<author confidence="0.853219666666667">Joint work with Aron Culotta</author>
<author confidence="0.853219666666667">Michael Wick</author>
<author confidence="0.853219666666667">Rob Hall</author>
<author confidence="0.853219666666667">Khashayar Rohanimanesh</author>
<author confidence="0.853219666666667">Karl Schultz</author>
<author confidence="0.853219666666667">Sameer Singh</author>
<author confidence="0.853219666666667">Charles Sutton</author>
<author confidence="0.853219666666667">David Smith</author>
<note confidence="0.659974">of the Thirteenth Conference on Computational Natural Language Learning page 1, Colorado, June 2009. Association for Computational Linguistics</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Joint work with Aron Culotta</author>
<author>Michael Wick</author>
<author>Rob Hall</author>
<author>Khashayar Rohanimanesh</author>
<author>Karl Schultz</author>
<author>Sameer Singh</author>
<author>Charles Sutton</author>
<author>David Smith</author>
</authors>
<marker>Culotta, Wick, Hall, Rohanimanesh, Schultz, Singh, Sutton, Smith, </marker>
<rawString>Joint work with Aron Culotta, Michael Wick, Rob Hall, Khashayar Rohanimanesh, Karl Schultz, Sameer Singh, Charles Sutton and David Smith.</rawString>
</citation>
<citation valid="true">
<date>2009</date>
<booktitle>Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<volume>1</volume>
<pages>page</pages>
<location>Boulder, Colorado,</location>
<marker>2009</marker>
<rawString>Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), page 1, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>