<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000090">
<title confidence="0.865097">
Parsing Time: Learning to Interpret Time Expressions
</title>
<author confidence="0.988927">
Gabor Angeli Christopher D. Manning Daniel Jurafsky
</author>
<affiliation confidence="0.998356">
Stanford University Stanford University Stanford University
</affiliation>
<address confidence="0.853287">
Stanford, CA 94305 Stanford, CA 94305 Stanford, CA 94305
</address>
<email confidence="0.999651">
angeli@stanford.edu manning@stanford.edu jurafsky@stanford.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99963305">
We present a probabilistic approach for learn-
ing to interpret temporal phrases given only a
corpus of utterances and the times they ref-
erence. While most approaches to the task
have used regular expressions and similar lin-
ear pattern interpretation rules, the possibil-
ity of phrasal embedding and modification in
time expressions motivates our use of a com-
positional grammar of time expressions. This
grammar is used to construct a latent parse
which evaluates to the time the phrase would
represent, as a logical parse might evaluate to
a concrete entity. In this way, we can employ
a loosely supervised EM-style bootstrapping
approach to learn these latent parses while
capturing both syntactic uncertainty and prag-
matic ambiguity in a probabilistic framework.
We achieve an accuracy of 72% on an adapted
TempEval-2 task – comparable to state of the
art systems.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998805960000001">
Temporal resolution is the task of mapping from
a textual phrase describing a potentially complex
time, date, or duration to a normalized (grounded)
temporal representation. For example, possibly
complex phrases such as the week before last are
often more useful in their grounded form – e.g.,
January 1 - January 7.
The dominant approach to this problem in previ-
ous work has been to use rule-based methods, gen-
erally a combination of regular-expression matching
followed by hand-written interpretation functions.
In general, it is appealing to learn the interpre-
tation of temporal expressions, rather than hand-
building systems. Moreover, complex hierarchical
temporal expressions, such as the Tuesday before
last or the third Wednesday of each month, and am-
biguous expressions, such as last Friday, are diffi-
cult to handle using deterministic rules and would
benefit from a recursive and probabilistic phrase
structure representation. Therefore, we attempt to
learn a temporal interpretation system where tempo-
ral phrases are parsed by a grammar, but this gram-
mar and its semantic interpretation rules are latent,
with only the input phrase and its grounded interpre-
tation given to the learning system.
Employing probabilistic techniques allows us to
capture ambiguity in temporal phrases in two impor-
tant respects. In part, it captures syntactic ambigu-
ity – e.g., last Friday the 13th bracketing as either
[last Friday] [the 13th], or last [Friday the 13th].
This also includes examples of lexical ambiguity –
e.g., two meanings of last in last week of November
versus last week. In addition, temporal expressions
often carry a pragmatic ambiguity. For instance, a
speaker may refer to either the next or previous Fri-
day when he utters Friday on a Sunday. Similarly,
next week can refer to either the coming week or the
week thereafter.
Probabilistic systems furthermore allow propaga-
tion of uncertainty to higher-level components – for
example recognizing that May could have a num-
ber of non-temporal meanings and allowing a sys-
tem with a broader contextual scope to make the fi-
nal judgment. We implement a CRF to detect tem-
poral expressions, and show our model’s ability to
act as a component in such a system.
We describe our temporal representation, fol-
lowed by the learning algorithm; we conclude with
experimental results showing our approach to be
competitive with state of the art systems.
</bodyText>
<page confidence="0.988646">
446
</page>
<note confidence="0.5279415">
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 446–455,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.999407" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999930436363637">
Our approach draws inspiration from a large body of
work on parsing expressions into a logical form. The
latent parse parallels the formal semantics in previ-
ous work, e.g., Montague semantics. Like these rep-
resentations, a parse – in conjunction with the refer-
ence time – defines a set of matching entities, in this
case the grounded time. The matching times can be
thought of as analogous to the entities in a logical
model which satisfy a given expression.
Supervised approaches to logical parsing promi-
nently include Zelle and Mooney (1996), Zettle-
moyer and Collins (2005), Kate et al. (2005), Zettle-
moyer and Collins (2007), inter alia. For exam-
ple, Zettlemoyer and Collins (2007) learn a mapping
from textual queries to a logical form. This logical
form importantly contains all the predicates and en-
tities used in their parse. We loosen the supervision
required in these systems by allowing the parse to be
entirely latent; the annotation of the grounded time
neither defines, nor gives any direct cues about the
elements of the parse, since many parses evaluate to
the same grounding. To demonstrate, the grounding
for a week ago could be described by specifying a
month and day, or as a week ago, or as last x – sub-
stituting today’s day of the week for x. Each of these
correspond to a completely different parse.
Recent work by Clarke et al. (2010) and Liang et
al. (2011) similarly relax supervision to require only
annotated answers rather than full logical forms. For
example, Liang et al. (2011) constructs a latent parse
similar in structure to a dependency grammar, but
representing a logical form. Our proposed lexi-
cal entries and grammar combination rules can be
thought of as paralleling the lexical entries and pred-
icates, and the implicit combination rules respec-
tively in this framework. Rather than querying from
a finite database, however, our system must com-
pare temporal expression within an infinite timeline.
Furthermore, our system is run using neither lexical
cues nor intelligent initialization.
Related work on interpreting temporal expres-
sions has focused on constructing hand-crafted in-
terpretation rules (Mani and Wilson, 2000; Saquete
et al., 2003; Puscasu, 2004; Grover et al., 2010). Of
these, HeidelTime (Str¨otgen and Gertz, 2010) and
SUTime (Chang and Manning, 2012) provide par-
ticularly strong competition.
Recent probabilistic approaches to temporal reso-
lution include UzZaman and Allen (2010), who em-
ploy a parser to produce deep logical forms, in con-
junction with a CRF classifier. In a similar vein,
Kolomiyets and Moens (2010) employ a maximum
entropy classifier to detect the location and temporal
type of expressions; the grounding is then done via
deterministic rules.
</bodyText>
<sectionHeader confidence="0.997039" genericHeader="method">
3 Representation
</sectionHeader>
<bodyText confidence="0.99990125">
We define a compositional representation of time;
a type system is described in Section 3.1 while the
grammar is outlined in Section 3.2 and described in
detail in Sections 3.3 and 3.4.
</bodyText>
<subsectionHeader confidence="0.997242">
3.1 Temporal Expression Types
</subsectionHeader>
<bodyText confidence="0.9943859">
We represent temporal expressions as either a
Range, Sequence, or Duration. We describe these,
the Function type, and the miscellaneous Number
and Nil types below:
Range [and Instant] A period between two dates
(or times). This includes entities such as Today,
1987, or Now. We denote a range by the variable
r. We maintain a consistent interval-based theory of
time (Allen, 1981) and represent instants as intervals
with zero span.
</bodyText>
<listItem confidence="0.822894055555556">
Sequence A sequence of Ranges, not necessarily
occurring at regular intervals. This includes enti-
ties such as Friday, November 27th, or last
Friday. A Sequence is a tuple of three elements
s = (rs, As, Ps):
1. rs(i): The ith element of a sequence, of type
Range. In the case of the sequence Friday,
rs(0) corresponds to the Friday in the current
week; rs(1) is the Friday in the following week,
etc.
2. As: The distance between two elements in the
sequence – approximated if this distance is not
constant. In the case of Friday, this distance
would be a week.
3. Ps: The containing unit of an element of a se-
quence. For example, PFriday would be the
Range corresponding to the current week. The
sequence index i E Z, from rs(i), is defined
</listItem>
<page confidence="0.998107">
447
</page>
<bodyText confidence="0.999689857142857">
relative to rs(0) – the element in the same con-
taining unit as the reference time.
We define the reference time t (Reichenbach,
1947) to be the instant relative to which times are
evaluated. For the TempEval-2 corpus, we approxi-
mate this as the publication time of the article. While
this is conflating Reichenbach’s reference time with
speech time, it is a useful approximation.
To contrast with Ranges, a Sequence can rep-
resent a number of grounded times. Nonetheless,
pragmatically, not all of these are given equal weight
– an utterance of last Friday may mean either of the
previous two Fridays, but is unlikely to ground to
anything else. We represent this ambiguity by defin-
ing a distribution over the elements of the Sequence.
While this could be any distribution, we chose to ap-
proximate it as a Gaussian.
In order to allow sharing parameters between any
sequence, we define the domain in terms of the index
of the sequence rather than of a constant unit of time
(e.g., seconds). To illustrate, the distribution over
April would have a much larger variance than the
distribution over Sunday, were the domains fixed.
The probability of the ith element of a sequence thus
depends on the beginning of the range rs(i), the ref-
erence time t, and the distance between elements of
the sequence Δs. We summarize this in the equation
below, with learned parameters p and Q:
</bodyText>
<equation confidence="0.951857">
P 160=−0.5
.5 Nµ,σ 1 rs(i) t Δs /
</equation>
<bodyText confidence="0.998647428571428">
Figure 1 shows an example of such a distribution;
importantly, note that moving the reference time be-
tween two elements dynamically changes the prob-
ability assigned to each.
Duration A period of time. This includes entities
like Week, Month, and 7 days. We denote a du-
ration with the variable d.
We define a special case of the Duration type to
represent approximate durations, identified by their
canonical unit (week, month, etc). These are used
to represent expressions such as a few years or some
days.
Function A function of arity less than or equal to
two representing some general modification to one
</bodyText>
<figureCaption confidence="0.990311555555556">
Figure 1: An illustration of a temporal distribution, e.g.,
Sunday. The reference time is labeled as time t between
Nov 20 and Nov 27; the probability that this sequence
is referring to Nov 20 is the integral of the marked area.
The domain of the graph are the indices of the sequence;
the distribution is overlaid with mean at the (normalized)
reference time t/Δ3; in our case Δ3 is a week. Note
that the probability of an index changes depending on the
exact location of the reference time.
</figureCaption>
<bodyText confidence="0.9998262">
of the above types. This captures semantic entities
such as those implied in last x, the third x [of y],
or x days ago. The particular functions and their
application are enumerated in Table 2.
Other Types Two other types bear auxiliary roles
in representing temporal expressions, though they
are not directly temporal concepts. In the grammar,
these appear as preterminals only.
The first of these types is Number – denoting
a number without any temporal meaning attached.
This comes into play representing expressions such
as 2 weeks. The other is the Nil type – denoting
terms which are not directly contributing to the se-
mantic meaning of the expression. This is intended
for words such as a or the, which serve as cues with-
out bearing temporal content themselves. The Nil
type is lexicalized with the word it generates.
Omitted Phenomena The representation de-
scribed is a simplification of the complexities of
time. Notably, a body of work has focused on
reasoning about events or states relative to temporal
expressions. Moens and Steedman (1988) describes
temporal expressions relating to changes of state;
Condoravdi (2010) explores NPI licensing in
temporal expressions. Broader context is also not
</bodyText>
<figure confidence="0.98295515">
-2 -1 -0.3 1 2
11/13 11/20 t 12/4 12/11
Δ,
�
�
0
11/27
Reference time
P(11/20) = f −0.5
−1.5 f(x)
448
Range
f(Duration) : Range
catRight
next
(a)
catRight(t, 2D )
catRight(t, −)
next
(b)
</figure>
<figureCaption confidence="0.963466333333333">
Figure 2: The grammar – (a) describes the CFG parse of
the temporal types. Words are tagged with their nontermi-
nal entry, above which only the types of the expressions
are maintained; (b) describes the corresponding combi-
nation of the temporal instances. The parse in (b) is de-
terministic given the grammar combination rules in (a).
</figureCaption>
<bodyText confidence="0.999493666666667">
directly modeled, but rather left to systems in which
the model would be embedded. Furthermore, vague
times (e.g., in the 90’s) represent a notable chunk
of temporal expressions uttered. In contrast, NLP
evaluations have generally not handled such vague
time expressions.
</bodyText>
<subsectionHeader confidence="0.999277">
3.2 Grammar Formalism
</subsectionHeader>
<bodyText confidence="0.999052432432433">
Our approach builds on the assumption that natural
language descriptions of time are compositional in
nature. Each word attached to a temporal phrase is
usually compositionally modifying the meaning of
the phrase. To demonstrate, we consider the expres-
sion the week before last week. We can construct a
meaning by applying the modifier last to week – cre-
ating the previous week; and then applying before to
week and last week.
We construct a paradigm for parsing temporal
phrases consisting of a standard PCFG over tempo-
ral types with each parse rule defining a function to
apply to the child nodes, or the word being gener-
ated. At the root of the tree, we recursively apply
the functions in the parse tree to obtain a final tem-
poral value. One can view this formalism as a rule-
to-rule translation (Bach, 1976; Allen, 1995, p. 263),
or a constrained Synchronous PCFG (Yamada and
Knight, 2001).
Our approach contrasts with common approaches,
such as CCG grammars (Steedman, 2000; Bos et
al., 2004; Kwiatkowski et al., 2011), giving us more
flexibility in the composition rules. Figure 2 shows
an example of the grammar.
Formally, we define our temporal grammar
G = (E, 5, V, W, R, 0). The alphabet E and start
symbol 5 retain their usual interpretations. We de-
fine a set V to be the set of types, as described in
Section 3.1 – these act as our nonterminals. For each
v E V we define an (infinite) set Wv corresponding
to the possible instances of type v. Concretely, if
v = Sequence, our set Wv E W could contain el-
ements corresponding to Friday, last Friday, Nov.
27th, etc. Each node in the tree defines a pair (v, w)
such that w E Wv, with combination rules defined
over v and function applications performed on w.
A rule R E R is defined as a pair
</bodyText>
<equation confidence="0.770683">
R = (vi — vivk, f : (Wv,, Wvk) -* Wvj. The
</equation>
<bodyText confidence="0.990896307692308">
first term is our conventional PCFG rule over the
types V. The second term defines the function to
apply to the values returned recursively by the child
nodes. Note that this definition is trivially adapted
for the case of unary rules.
The last term in our grammar formalism denotes
the rule probabilities 0. In line with the usual in-
terpretation, this defines a probability of applying a
particular rule r E R. Importantly, note that the
distribution over possible groundings of a temporal
expression are not included in the grammar formal-
ism. The learning of these probabilities is detailed
in Section 4.
</bodyText>
<subsectionHeader confidence="0.997206">
3.3 Preterminals
</subsectionHeader>
<bodyText confidence="0.999968">
We define a set of preterminals, specifying their
eventual type, as well as the temporal instance it pro-
duces when its function is evaluated on the word it
generates (e.g., f(day) = Day). A distinction is
made in our description between entities with con-
tent roles versus entities with a functional role.
The first – consisting of Ranges, Sequences, and
Durations – are listed in Table 1. A total of 62 such
preterminals are defined in the implemented system,
corresponding to primitive entities often appearing
in newswire, although this list is easily adaptable to
</bodyText>
<figure confidence="0.995813583333333">
Duration
Number
Num.∗100
2
Duration
Day
days
2D
Num(2)
2
1D
days
</figure>
<page confidence="0.991013">
449
</page>
<table confidence="0.998380363636364">
Function Description Signature(s)
shiftLeft Shift a Range or Sequence left by a Duration f : S, D —* S; f : R, D —* R
shiftRight Shift a Range or Sequence right by a Duration f : S, D —* S; f : R, D —* R
shrinkBegin Take the first Duration of a Range/Sequence f : S, D —* S; f : R, D —* R
shrinkEnd Take the last Duration of a Range/Sequence f : S, D —* S; f : R, D —* R
catLeft Take Duration units after the end of a Range f : R, D —* R
catRight Take Duration units before the start of a Range f : R, D —* R
moveLeft1 Move the origin of a sequence left by 1 f : S —* S
moveRight1 Move the origin of a sequence right by 1 f : S —* S
nth x of y Take the nth Sequence in y (Day of Week, etc) f : Number —* S
approximate Make a Duration approximate f : D —* D
</table>
<tableCaption confidence="0.983422666666667">
Table 2: The functional preterminals of the grammar; R, S, and D denote Ranges Sequences and Durations respec-
tively. The name, a brief description, and the type signature of the function (as used in parsing) are given. Described
in more detail in Section 3.4, the functions are most easily interpreted as operations on either an interval or sequence.
</tableCaption>
<table confidence="0.9997813">
Type Instances
Range Past, Future, Yesterday,
Tomorrow,Today,Reference,
Year(n), Century(n)
Sequence Friday, January, ...
DayOfMonth, DayOfWeek, ...
EveryDay, EveryWeek, ...
Duration Second, Minute, Hour,
Day, Week, Month, Quarter,
Year,Decade,Century
</table>
<tableCaption confidence="0.999648">
Table 1: The content-bearing preterminals of the gram-
</tableCaption>
<bodyText confidence="0.9929196">
mar, arranged by their types. Note that the Sequence
type contains more elements than enumerated here; how-
ever, only a few of each characteristic type are shown here
for brevity.
fit other domains. It should be noted that the expres-
sions, represented in Typewriter, have no a pri-
ori association with words, denoted by italics; this
correspondence must be learned. Furthermore, enti-
ties which are subject to interpretation – for example
Quarter or Season – are given a concrete inter-
pretation. The nth quarter is defined by evenly split-
ting a year into four; the seasons are defined in the
same way but with winter beginning in December.
The functional entities are described in Table 2,
and correspond to the Function type. The majority
of these mirror generic operations on intervals on a
timeline, or manipulations of a sequence. Notably,
like intervals, times can be moved (3 weeks ago) or
their size changed (the first two days of the month),
or a new interval can be started from one of the end-
points (the last 2 days). Additionally, a sequence can
be modified by shifting its origin (last Friday), or
taking the nth element of the sequence within some
bound (fourth Sunday in November).
The lexical entry for the Nil type is tagged with the
word it generates, producing entries such as Nil(a),
Nil(November), etc. The lexical entry for the Num-
ber type is parameterized by the order of magnitude
and ordinality of the number; e.g., 27th becomes
Number(101,ordinal).
</bodyText>
<subsectionHeader confidence="0.982649">
3.4 Combination Rules
</subsectionHeader>
<bodyText confidence="0.978735333333333">
As mentioned earlier, our grammar defines both
combination rules over types (in V) as well as a
method for combining temporal instances (in W„ E
W). This method is either a function application of
one of the functions in Table 2, a function which is
implicit in the text (intersection and multiplication),
</bodyText>
<listItem confidence="0.9236349">
or an identity operation (for Nils). These cases are
detailed below:
• Function application, e.g., last week. We apply
(or partially apply) a function to an argument
on either the left or the right: f(x, y)Ox or xO
f(x, y). Furthermore, for functions of arity 2
taking a Range as an argument, we define a rule
treating it as a unary function with the reference
time taking the place of the second argument.
• Intersecting two ranges or sequences, e.g.,
</listItem>
<page confidence="0.955542">
450
</page>
<figure confidence="0.888623">
Input (w,t) ( Last Friday the 13 th , May 16 2011 )
moveLeft1( FRI ) ∩ 13th
Output T∗ May 13 2011
</figure>
<figureCaption confidence="0.999046">
Figure 3: An overview of the system architecture. Note
that the parse is latent – that is, it is not annotated in the
training data.
</figureCaption>
<bodyText confidence="0.99636925">
November 27th. The intersect function treats
both arguments as intervals, and will return an
interval (Range or Sequence) corresponding to
the overlap between the two.1
</bodyText>
<listItem confidence="0.999727333333333">
• Multiplying a Number with a Duration, e.g., 5
weeks.
• Combining a non-Nil and Nil element with no
</listItem>
<bodyText confidence="0.807150666666667">
change to the temporal expression, e.g., a week.
The lexicalization of the Nil type allows the
algorithm to take hints from these supporting
words.
We proceed to describe learning the parameters of
this grammar.
</bodyText>
<sectionHeader confidence="0.997497" genericHeader="method">
4 Learning
</sectionHeader>
<bodyText confidence="0.999886666666667">
We present a system architecture, described in Fig-
ure 3. We detail the inference procedure in Sec-
tion 4.1 and training in Section 4.2.
</bodyText>
<subsectionHeader confidence="0.732615">
4.1 Inference
</subsectionHeader>
<bodyText confidence="0.999908428571429">
To provide a list of candidate expressions with their
associated probabilities, we employ a k-best CKY
parser. Specifically, we implement Algorithm 3 de-
scribed in Huang and Chiang (2005), providing an
0(Gn3k log k) algorithm with respect to the gram-
mar size G, phrase length n, and beam size k. We
set the beam size to 2000.
</bodyText>
<footnote confidence="0.834463">
1In the case of complex sequences (e.g., Friday the 13th) an
A* search is performed to find overlapping ranges in the two
sequences; the origin r3(0) is updated to refer to the closest
such match to the reference time.
</footnote>
<bodyText confidence="0.999445666666667">
Revisiting the notion of pragmatic ambiguity, in
a sense the most semantically complete output of
the system would be a distribution – an utterance of
Friday would give a distribution over Fridays rather
than a best guess of its grounding. However, it is of-
ten advantageous to ground to a concrete expression
with a corresponding probability. The CKY k-best
beam and the temporal distribution – capturing syn-
tactic and pragmatic ambiguity – can be combined to
provide a Viterbi decoding, as well as its associated
probability.
We define the probability of a syntactic parse
y making use of rules R C_ R as P(y) =
P(w1, ... wn; R) = l i,j,kER P(j, k  |i). As de-
scribed in Section 3.1, we define the probability of
a grounding relative to reference time t and a par-
ticular syntactic interpretation Pt(i|y). The prod-
uct of these two terms provides the probability of
a grounded temporal interpretation; we can obtain a
Viterbi decoding by maximizing this joint probabil-
ity:
</bodyText>
<equation confidence="0.997113">
Pt(i, y) = P(y) X Pt(i|y) (2)
</equation>
<bodyText confidence="0.999878333333333">
This provides us with a framework for obtaining
grounded times from a temporal phrase – in line with
the annotations provided during training time.
</bodyText>
<subsectionHeader confidence="0.971046">
4.2 Training
</subsectionHeader>
<bodyText confidence="0.998361052631579">
We present an EM-style bootstrapping approach to
training the parameters of our grammar jointly with
the parameters of our Gaussian temporal distribu-
tion.
Our TimEM algorithm for learning the parame-
ters for the grammar (0), jointly with the temporal
distribution (µ and a) is given in Algorithm 1. The
inputs to the algorithm are the initial parameters 0,
µ, and a, and a set of training instances D. Further-
more, the algorithm makes use of a Dirichlet prior a
on the grammar parameters 0, as well as a Gaussian
prior N on the mean of the temporal distribution µ.
The algorithm outputs the final parameters 0*, µ*
and a*.
Each training instance is a tuple consisting of
the words in the temporal phrase w, the annotated
grounded time T*, and the reference time of the ut-
terance t. The input phrase is tokenized according
to Penn Treebank guidelines, except we additionally
</bodyText>
<figure confidence="0.998482846153846">
moveLeft1( FRI )
13th
13th
13th
FRI
friday
Nilthe
the
moveLeft1(−)
last
Latent
parse
R
</figure>
<page confidence="0.597774">
451
</page>
<figure confidence="0.99416912">
Algorithm 1: TimEM
Input: Initial parameters θ, µ, σ; data
D = {(w, τ*, t)}; Dirichlet prior α,
Gaussian prior N
Output: Optimal parameters θ*, µ*, σ*
1 while not converged do
2
3
4 end
5 return (θs, µ, σ)
6 begin E-Step(D,θ,µ,σ)
7 Mθ = []; Mµ,σ = []
8 for (w, τ*, t) E D do
9 ¯mθ = []; ¯mµ,σ = []
10 for y E k-bestCKY(w, θ) do
11 if p = Pµ,σ(τ*  |y, t) &gt; 0 then
12 ¯mθ += (y, p); ¯mµ,σ += (i, p)
13 end
14 end
15 M += normalize( ¯mθ)
16 Mµ,σ += normalize( ¯mµ,σ)
17 end
18 return M
19 end
20 begin M-Step(
</figure>
<equation confidence="0.8218378">
θ&apos; := bayesianPosterior(
σ&apos; := mlePosterior(
µ&apos; := bayesianPosterior( Mµ,σ, σ&apos;, N)
return (θ&apos;, µ&apos;, σ&apos;)
25 end
</equation>
<bodyText confidence="0.999979375">
split on the characters ‘-’ and ‘/,’ which often de-
limit a boundary between temporal entities. Beyond
this preprocessing, no language-specific information
about the meanings of the words are introduced, in-
cluding syntactic parses, POS tags, etc.
The algorithm operates similarly to the EM algo-
rithms used for grammar induction (Klein and Man-
ning, 2004; Carroll and Charniak, 1992). How-
ever, unlike grammar induction, we are allowed a
certain amount of supervision by requiring that the
predicted temporal expression match the annotation.
Our expected statistics are therefore more accurately
our normalized expected counts of valid parses.
Note that in conventional grammar induction, the
expected sufficient statistics can be gathered analyt-
ically from reading off the chart scores of a parse.
This does not work in our case for two reasons. In
part, we would like to incorporate the probability
of the temporal grounding in our feedback probabil-
ity. Additionally, we are only using parses which are
valid candidates – that is, the parses which ground to
the correct time τ* – which we cannot establish until
the entire expression is parsed. The expected statis-
tics are thus computed non-analytically via a beam
on both the possible parses (line 10) and the pos-
sible temporal groundings of a given interpretation
(line 11).
The particular EM updates are the standard up-
dates for multinomial and Gaussian distributions
given fully observed data. In the multinomial case,
our (unnormalized) parameter updates, with Dirich-
let prior α, are:
</bodyText>
<equation confidence="0.993337">
� ✶ (vjk|i = vmn|l) p (3)
(y,p)E ¯Mg vjkjiEy
</equation>
<bodyText confidence="0.999766333333333">
In the Gaussian case, the parameter update for σ
is the maximum likelihood update; while the update
for µ incorporates a Bayesian prior N(µ0, σ0):
</bodyText>
<equation confidence="0.996931666666667">
E
σ&apos;2µ0 + σ2 0 (i,p)E ¯Mµ,o i · p µ&apos; =5
σ&apos;2 + σ20 E(i,p)E¯Mµ,o p
</equation>
<bodyText confidence="0.999958181818182">
As the parameters improve, the parser more effi-
ciently prunes incorrect parses and the beam incor-
porates valid parses for longer and longer phrases.
For instance, in the first iteration the model must
learn the meaning of both words in last Friday; once
the parser learns the meaning of one of them – e.g.,
Friday appears elsewhere in the corpus – subsequent
iterations focus on proposing candidate meanings
for last. In this way, a progressively larger percent-
age of the data is available to be learned from at each
iteration.
</bodyText>
<sectionHeader confidence="0.997648" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9994035">
We evaluate our model against current state-of-the
art systems for temporal resolution on the English
</bodyText>
<figure confidence="0.98284365">
�
p (i,p)E¯Mµ,o
(i − µ&apos;)2 · p (4)
1
σ&apos;
E
d
(i,p)E
Mµ,o
( Mθ, Mµ,σ) := E-Step (D,θ,µ,σ)
(θ, µ, σ) := M-Step ( Mθ, Mµ,σ)
Mθ,
Mµ,σ)
21
22
23
24
Mµ,σ)
Mθ, α)
�θ&apos;mn|l = α+
</figure>
<page confidence="0.994773">
452
</page>
<table confidence="0.999617666666667">
System Type Train Type Test
Value Value
GUTime 0.72 0.46 0.80 0.42
SUTime 0.85 0.69 0.94 0.71
HeidelTime 0.80 0.67 0.85 0.71
OurSystem 0.90 0.72 0.88 0.72
</table>
<tableCaption confidence="0.9548975">
Table 3: TempEval-2 Attribute scores for our system and
three previous systems. The scores are calculated us-
ing gold extents, forcing a guessed interpretation for each
parse.
</tableCaption>
<figure confidence="0.999360416666667">
0 0.2 0.4 0.6 0.8 1
Extent recall
OurSystem
HeidelTime1
HeidelTime2
SUTime
Value accuracy 1
0.8
0.6
0.4
0.2
0
</figure>
<bodyText confidence="0.99028">
portion of the TempEval-2 Task A dataset (Verhagen
et al., 2010).
</bodyText>
<subsectionHeader confidence="0.974811">
5.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999989166666667">
The TempEval-2 dataset is relatively small, contain-
ing 162 documents and 1052 temporal phrases in the
training set and an additional 20 documents and 156
phrases in the evaluation set. Each temporal phrase
was annotated as a TIMEX32 tag around an adver-
bial or prepositional phrase
</bodyText>
<subsectionHeader confidence="0.956019">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999960285714286">
In the TempEval-2 A Task, system performance is
evaluated on detection and resolution of expressions.
Since we perform only the second of these, we eval-
uate our system assuming gold detection.
Similarly, the original TempEval-2 scoring
scheme gave a precision and recall for detection,
and an accuracy for only the temporal expressions
attempted. Since our system is able to produce a
guess for every expression, we produce a precision-
recall curve on which competing systems are plotted
(see Figure 4). Note that the downward slope of the
curve indicates that the probabilities returned by the
system are indicative of its confidence – the prob-
ability of a parse correlates with the probability of
that parse being correct.
Additionally, and perhaps more accurately, we
compare to previous system scores when con-
strained to make a prediction on every example; if
no guess is made, the output is considered incorrect.
This in general yields lower results, as the system
is not allowed to abstain on expressions it does not
</bodyText>
<footnote confidence="0.9895495">
2See http://www.timeml.org for details on the
TimeML format and TIMEX3 tag.
</footnote>
<figureCaption confidence="0.999201142857143">
Figure 4: A precision-recall curve for our system, com-
pared to prior work. The data points are obtained by set-
ting a threshold minimum probability at which to guess
a time creating different extent recall values. The curve
falls below HeidelTime1 and SUTime in part from lack
of context, and in part since our system was not trained
to optimize this curve.
</figureCaption>
<bodyText confidence="0.993143">
recognize. Results are summarized in Table 3.
We compare to three previous rule-based sys-
tems. GUTime (Mani and Wilson, 2000) presents an
older but widely used baseline.3 More recently, SU-
Time (Chang and Manning, 2012) provides a much
stronger comparison. We also compare to Heidel-
Time (Str¨otgen and Gertz, 2010), which represents
the state-of-the-art system at the TempEval-2 task.
</bodyText>
<subsectionHeader confidence="0.982503">
5.3 Detection
</subsectionHeader>
<bodyText confidence="0.977815583333333">
One of the advantages of our model is that it can pro-
vide candidate groundings for any expression. We
explore this ability by building a detection model to
find candidate temporal expressions, which we then
ground. The detection model is implemented as a
Conditional Random Field (Lafferty et al., 2001),
with features over the morphology and context. Par-
ticularly, we define the following features:
• The word and lemma within 2 of the current
word.
• The word shape4 and part of speech of the cur-
rent word.
</bodyText>
<footnote confidence="0.980538714285714">
3Due to discrepancies in output formats, the output of
GUTime was heuristically patched and manually checked to
conform to the expected format.
4Word shape is calculated by mapping each character to one
of uppercase, lowercase, number, or punctuation. The first four
characters are mapped verbatim; subsequent sequences of sim-
ilar characters are collapsed.
</footnote>
<page confidence="0.994683">
453
</page>
<table confidence="0.999834714285714">
System P Extent F1 Attribute
R Typ Val
GUTime 0.89 0.79 0.84 0.95 0.68
SUTime 0.88 0.96 0.92 0.96 0.82
HeidelTime1 0.90 0.82 0.86 0.96 0.85
HeidelTime2 0.82 0.91 0.86 0.92 0.77
OurSystem 0.89 0.84 0.86 0.91 0.72
</table>
<tableCaption confidence="0.680032714285714">
Table 4: TempEval-2 Extent scores for our system and
three previous systems. Note that the attribute scores are
now relatively low compared to previous work; unlike
rule-based approaches, our model can guess a temporal
interpretation for any phrase, meaning that a good pro-
portion of the phrases not detected would have been in-
terpreted correctly.
</tableCaption>
<listItem confidence="0.9854525">
• Whether the current word is a number, along
with its ordinality and order of magnitude
• Prefixes and suffixes up to length 5, along with
their word shape.
</listItem>
<bodyText confidence="0.999983">
We summarize our results in Table 4, noting that
the performance indicates that the CRF and interpre-
tation model find somewhat different phrases hard to
detect and interpret respectively. Many errors made
in detection are attributable to the small size of the
training corpus (63,000 tokens).
</bodyText>
<subsectionHeader confidence="0.810333">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999991103448276">
Our system performs well above the GUTime base-
line and is competitive with both of the more recent
systems. In part, this is from more sophisticated
modeling of syntactic ambiguity: e.g., the past few
weeks has a clause the past – which, alone, should
be parsed as PAST – yet the system correctly dis-
prefers incorporating this interpretation and returns
the approximate duration 1 week. Furthermore,
we often capture cases of pragmatic ambiguity – for
example, empirically, August tends to refers to the
previous August when mentioned in February.
Compared to rule-based systems, we attribute
most errors the system makes to either data spar-
sity or missing lexical primitives. For example –
illustrating sparsity – we have trouble recognizing
Nov. as corresponding to November (e.g., Nov. 13),
since the publication time of the articles happen to
often be near November and we prefer tagging the
word as Nil (analogous to the 13th). Missing lexi-
cal primitives, in turn, include tags for 1990s, or half
(in minute and a half); as well as missing functions,
such as or (in weeks or months).
Remaining errors can be attributed to causes such
as providing the wrong Viterbi grounding to the
evaluation script (e.g., last rather than this Friday),
differences in annotation (e.g., 24 hours is marked
wrong against a day), or missing context (e.g., the
publication time is not the true reference time),
among others.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99835616">
We present a new approach to resolving temporal ex-
pressions, based on synchronous parsing of a fixed
grammar with learned parameters and a composi-
tional representation of time. The system allows
for output which captures uncertainty both with re-
spect to the syntactic structure of the phrase and the
pragmatic ambiguity of temporal utterances. We
also note that the approach is theoretically better
adapted for phrases more complex than those found
in TempEval-2.
Furthermore, the system makes very few
language-specific assumptions, and the algorithm
could be adapted to domains beyond temporal
resolution. We hope to improve detection and
explore system performance on multilingual and
complex datasets in future work.
Acknowledgements The authors would like to thank Valentin
Spitkovsky, David McClosky, and Angel Chang for valuable
discussion and insights. We gratefully acknowledge the support
of the Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the view of DARPA, AFRL, or the US government.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999804857142857">
James F. Allen. 1981. An interval-based representa-
tion of temporal knowledge. In Proceedings of the
7th international joint conference on Artificial intelli-
gence, pages 221–226, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
James Allen. 1995. Natural Language Understanding.
Benjamin/Cummings, Redwood City, CA.
</reference>
<page confidence="0.990722">
454
</page>
<reference confidence="0.9999075625">
E. Bach. 1976. An extension of classical transforma-
tional grammar. In Problems of Linguistic Metatheory
(Proceedings of the 1976 Conference), Michigan State
University.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In
Proceedings of Coling, pages 1240–1246, Geneva,
Switzerland. COLING.
Glenn Carroll and Eugene Charniak. 1992. Two experi-
ments on learning probabilistic dependency grammars
from corpora. Technical report, Providence, RI, USA.
Angel Chang and Chris Manning. 2012. SUTIME: a
library for recognizing and normalizing time expres-
sions. In Language Resources and Evaluation.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world’s response. In CoNLL, pages 18–27, Uppsala,
Sweden.
Cleo Condoravdi. 2010. NPI licensing in temporal
clauses. Natural Language and Linguistic Theory,
28:877–910.
Claire Grover, Richard Tobin, Beatrice Alex, and Kate
Byrne. 2010. Edinburgh-LTG: TempEval-2 system
description. In Proceedings of the 5th International
Workshop on Semantic Evaluation, Sem-Eval, pages
333–336.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technology, Parsing, pages 53–
64.
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney.
2005. Learning to transform natural to formal lan-
guages. In AAAI, pages 1062–1068, Pittsburgh, PA.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL.
Oleksandr Kolomiyets and Marie-Francine Moens. 2010.
KUL: recognition and normalization of temporal ex-
pressions. In Proceedings of the 5th International
Workshop on Semantic Evaluation, Sem-Eval ’10,
pages 325–328.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generalization
in CCG grammar induction for semantic parsing. In
EMNLP, pages 1512–1523, Edinburgh, Scotland, UK.
J. Lafferty, A. McCallum, and F Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In International
Conference on Machine Learning (ICML).
P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In ACL.
Inderjeet Mani and George Wilson. 2000. Robust tem-
poral processing of news. In ACL, pages 69–76, Hong
Kong.
Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational Lin-
guistics, 14:15–28.
G. Puscasu. 2004. A framework for temporal resolution.
In LREC, pages 1901–1904.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
Macmillan, New York.
E. Saquete, R. Muoz, and P. Martnez-Barco. 2003.
Terseo: Temporal expression resolution system ap-
plied to event ordering. In Text, Speech and Dialogue,
pages 220–228.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Jannik Str¨otgen and Michael Gertz. 2010. Heideltime:
High quality rule-based extraction and normalization
of temporal expressions. In Proceedings of the 5th In-
ternational Workshop on Semantic Evaluation, Sem-
Eval, pages 321–324.
Naushad UzZaman and James F. Allen. 2010. TRIPS
and TRIOS system for TempEval-2: Extracting tem-
poral information from text. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Sem-
Eval, pages 276–283.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
TempEval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57–62, Up-
psala, Sweden.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL, pages 523–530.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic pro-
gramming. In AAAI/IAAI, pages 1050–1055, Portland,
OR.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
UAI, pages 658–666. AUAI Press.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing to
logical form. In EMNLP-CoNLL, pages 678–687.
</reference>
<page confidence="0.999054">
455
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986981">
<title confidence="0.999928">Parsing Time: Learning to Interpret Time Expressions</title>
<author confidence="0.997187">Gabor Angeli Christopher D Manning Daniel Jurafsky</author>
<affiliation confidence="0.999982">Stanford University Stanford University Stanford University</affiliation>
<address confidence="0.999642">Stanford, CA 94305 Stanford, CA 94305 Stanford, CA 94305</address>
<email confidence="0.999799">angeli@stanford.edumanning@stanford.edujurafsky@stanford.edu</email>
<abstract confidence="0.99918119047619">We present a probabilistic approach for learning to interpret temporal phrases given only a corpus of utterances and the times they reference. While most approaches to the task have used regular expressions and similar linear pattern interpretation rules, the possibility of phrasal embedding and modification in time expressions motivates our use of a comof time This is used to construct a parse which evaluates to the time the phrase would represent, as a logical parse might evaluate to a concrete entity. In this way, we can employ a loosely supervised EM-style bootstrapping approach to learn these latent parses while capturing both syntactic uncertainty and pragmatic ambiguity in a probabilistic framework. We achieve an accuracy of 72% on an adapted TempEval-2 task – comparable to state of the art systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>An interval-based representation of temporal knowledge.</title>
<date>1981</date>
<booktitle>In Proceedings of the 7th international joint conference on Artificial intelligence,</booktitle>
<pages>221--226</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="7148" citStr="Allen, 1981" startWordPosition="1131" endWordPosition="1132"> Representation We define a compositional representation of time; a type system is described in Section 3.1 while the grammar is outlined in Section 3.2 and described in detail in Sections 3.3 and 3.4. 3.1 Temporal Expression Types We represent temporal expressions as either a Range, Sequence, or Duration. We describe these, the Function type, and the miscellaneous Number and Nil types below: Range [and Instant] A period between two dates (or times). This includes entities such as Today, 1987, or Now. We denote a range by the variable r. We maintain a consistent interval-based theory of time (Allen, 1981) and represent instants as intervals with zero span. Sequence A sequence of Ranges, not necessarily occurring at regular intervals. This includes entities such as Friday, November 27th, or last Friday. A Sequence is a tuple of three elements s = (rs, As, Ps): 1. rs(i): The ith element of a sequence, of type Range. In the case of the sequence Friday, rs(0) corresponds to the Friday in the current week; rs(1) is the Friday in the following week, etc. 2. As: The distance between two elements in the sequence – approximated if this distance is not constant. In the case of Friday, this distance woul</context>
</contexts>
<marker>Allen, 1981</marker>
<rawString>James F. Allen. 1981. An interval-based representation of temporal knowledge. In Proceedings of the 7th international joint conference on Artificial intelligence, pages 221–226, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Natural Language Understanding. Benjamin/Cummings,</title>
<date>1995</date>
<location>Redwood City, CA.</location>
<contexts>
<context position="13298" citStr="Allen, 1995" startWordPosition="2189" endWordPosition="2190">e. To demonstrate, we consider the expression the week before last week. We can construct a meaning by applying the modifier last to week – creating the previous week; and then applying before to week and last week. We construct a paradigm for parsing temporal phrases consisting of a standard PCFG over temporal types with each parse rule defining a function to apply to the child nodes, or the word being generated. At the root of the tree, we recursively apply the functions in the parse tree to obtain a final temporal value. One can view this formalism as a ruleto-rule translation (Bach, 1976; Allen, 1995, p. 263), or a constrained Synchronous PCFG (Yamada and Knight, 2001). Our approach contrasts with common approaches, such as CCG grammars (Steedman, 2000; Bos et al., 2004; Kwiatkowski et al., 2011), giving us more flexibility in the composition rules. Figure 2 shows an example of the grammar. Formally, we define our temporal grammar G = (E, 5, V, W, R, 0). The alphabet E and start symbol 5 retain their usual interpretations. We define a set V to be the set of types, as described in Section 3.1 – these act as our nonterminals. For each v E V we define an (infinite) set Wv corresponding to th</context>
</contexts>
<marker>Allen, 1995</marker>
<rawString>James Allen. 1995. Natural Language Understanding. Benjamin/Cummings, Redwood City, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bach</author>
</authors>
<title>An extension of classical transformational grammar.</title>
<date>1976</date>
<booktitle>In Problems of Linguistic Metatheory (Proceedings of the 1976 Conference),</booktitle>
<institution>State University.</institution>
<location>Michigan</location>
<contexts>
<context position="13285" citStr="Bach, 1976" startWordPosition="2187" endWordPosition="2188">of the phrase. To demonstrate, we consider the expression the week before last week. We can construct a meaning by applying the modifier last to week – creating the previous week; and then applying before to week and last week. We construct a paradigm for parsing temporal phrases consisting of a standard PCFG over temporal types with each parse rule defining a function to apply to the child nodes, or the word being generated. At the root of the tree, we recursively apply the functions in the parse tree to obtain a final temporal value. One can view this formalism as a ruleto-rule translation (Bach, 1976; Allen, 1995, p. 263), or a constrained Synchronous PCFG (Yamada and Knight, 2001). Our approach contrasts with common approaches, such as CCG grammars (Steedman, 2000; Bos et al., 2004; Kwiatkowski et al., 2011), giving us more flexibility in the composition rules. Figure 2 shows an example of the grammar. Formally, we define our temporal grammar G = (E, 5, V, W, R, 0). The alphabet E and start symbol 5 retain their usual interpretations. We define a set V to be the set of types, as described in Section 3.1 – these act as our nonterminals. For each v E V we define an (infinite) set Wv corres</context>
</contexts>
<marker>Bach, 1976</marker>
<rawString>E. Bach. 1976. An extension of classical transformational grammar. In Problems of Linguistic Metatheory (Proceedings of the 1976 Conference), Michigan State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Wide-coverage semantic representations from a CCG parser.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>1240--1246</pages>
<location>Geneva, Switzerland. COLING.</location>
<contexts>
<context position="13471" citStr="Bos et al., 2004" startWordPosition="2214" endWordPosition="2217">and then applying before to week and last week. We construct a paradigm for parsing temporal phrases consisting of a standard PCFG over temporal types with each parse rule defining a function to apply to the child nodes, or the word being generated. At the root of the tree, we recursively apply the functions in the parse tree to obtain a final temporal value. One can view this formalism as a ruleto-rule translation (Bach, 1976; Allen, 1995, p. 263), or a constrained Synchronous PCFG (Yamada and Knight, 2001). Our approach contrasts with common approaches, such as CCG grammars (Steedman, 2000; Bos et al., 2004; Kwiatkowski et al., 2011), giving us more flexibility in the composition rules. Figure 2 shows an example of the grammar. Formally, we define our temporal grammar G = (E, 5, V, W, R, 0). The alphabet E and start symbol 5 retain their usual interpretations. We define a set V to be the set of types, as described in Section 3.1 – these act as our nonterminals. For each v E V we define an (infinite) set Wv corresponding to the possible instances of type v. Concretely, if v = Sequence, our set Wv E W could contain elements corresponding to Friday, last Friday, Nov. 27th, etc. Each node in the tre</context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>Johan Bos, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Wide-coverage semantic representations from a CCG parser. In Proceedings of Coling, pages 1240–1246, Geneva, Switzerland. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Eugene Charniak</author>
</authors>
<title>Two experiments on learning probabilistic dependency grammars from corpora.</title>
<date>1992</date>
<tech>Technical report,</tech>
<location>Providence, RI, USA.</location>
<contexts>
<context position="23737" citStr="Carroll and Charniak, 1992" startWordPosition="4050" endWordPosition="4053">+= (y, p); ¯mµ,σ += (i, p) 13 end 14 end 15 M += normalize( ¯mθ) 16 Mµ,σ += normalize( ¯mµ,σ) 17 end 18 return M 19 end 20 begin M-Step( θ&apos; := bayesianPosterior( σ&apos; := mlePosterior( µ&apos; := bayesianPosterior( Mµ,σ, σ&apos;, N) return (θ&apos;, µ&apos;, σ&apos;) 25 end split on the characters ‘-’ and ‘/,’ which often delimit a boundary between temporal entities. Beyond this preprocessing, no language-specific information about the meanings of the words are introduced, including syntactic parses, POS tags, etc. The algorithm operates similarly to the EM algorithms used for grammar induction (Klein and Manning, 2004; Carroll and Charniak, 1992). However, unlike grammar induction, we are allowed a certain amount of supervision by requiring that the predicted temporal expression match the annotation. Our expected statistics are therefore more accurately our normalized expected counts of valid parses. Note that in conventional grammar induction, the expected sufficient statistics can be gathered analytically from reading off the chart scores of a parse. This does not work in our case for two reasons. In part, we would like to incorporate the probability of the temporal grounding in our feedback probability. Additionally, we are only us</context>
</contexts>
<marker>Carroll, Charniak, 1992</marker>
<rawString>Glenn Carroll and Eugene Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. Technical report, Providence, RI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angel Chang</author>
<author>Chris Manning</author>
</authors>
<title>SUTIME: a library for recognizing and normalizing time expressions. In Language Resources and Evaluation.</title>
<date>2012</date>
<contexts>
<context position="6123" citStr="Chang and Manning, 2012" startWordPosition="965" endWordPosition="968">ought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide particularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of time; a type system is described in Section 3.1 while the grammar is outlined in Section 3.2 and described in detail in Section</context>
<context position="28450" citStr="Chang and Manning, 2012" startWordPosition="4845" endWordPosition="4848">for details on the TimeML format and TIMEX3 tag. Figure 4: A precision-recall curve for our system, compared to prior work. The data points are obtained by setting a threshold minimum probability at which to guess a time creating different extent recall values. The curve falls below HeidelTime1 and SUTime in part from lack of context, and in part since our system was not trained to optimize this curve. recognize. Results are summarized in Table 3. We compare to three previous rule-based systems. GUTime (Mani and Wilson, 2000) presents an older but widely used baseline.3 More recently, SUTime (Chang and Manning, 2012) provides a much stronger comparison. We also compare to HeidelTime (Str¨otgen and Gertz, 2010), which represents the state-of-the-art system at the TempEval-2 task. 5.3 Detection One of the advantages of our model is that it can provide candidate groundings for any expression. We explore this ability by building a detection model to find candidate temporal expressions, which we then ground. The detection model is implemented as a Conditional Random Field (Lafferty et al., 2001), with features over the morphology and context. Particularly, we define the following features: • The word and lemma</context>
</contexts>
<marker>Chang, Manning, 2012</marker>
<rawString>Angel Chang and Chris Manning. 2012. SUTIME: a library for recognizing and normalizing time expressions. In Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Ming-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In CoNLL,</booktitle>
<pages>18--27</pages>
<location>Uppsala,</location>
<contexts>
<context position="5174" citStr="Clarke et al. (2010)" startWordPosition="821" endWordPosition="824">rm. This logical form importantly contains all the predicates and entities used in their parse. We loosen the supervision required in these systems by allowing the parse to be entirely latent; the annotation of the grounded time neither defines, nor gives any direct cues about the elements of the parse, since many parses evaluate to the same grounding. To demonstrate, the grounding for a week ago could be described by specifying a month and day, or as a week ago, or as last x – substituting today’s day of the week for x. Each of these correspond to a completely different parse. Recent work by Clarke et al. (2010) and Liang et al. (2011) similarly relax supervision to require only annotated answers rather than full logical forms. For example, Liang et al. (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our syst</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world’s response. In CoNLL, pages 18–27, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cleo Condoravdi</author>
</authors>
<title>NPI licensing in temporal clauses.</title>
<date>2010</date>
<booktitle>Natural Language and Linguistic Theory,</booktitle>
<pages>28--877</pages>
<contexts>
<context position="11585" citStr="Condoravdi (2010)" startWordPosition="1897" endWordPosition="1898">sions such as 2 weeks. The other is the Nil type – denoting terms which are not directly contributing to the semantic meaning of the expression. This is intended for words such as a or the, which serve as cues without bearing temporal content themselves. The Nil type is lexicalized with the word it generates. Omitted Phenomena The representation described is a simplification of the complexities of time. Notably, a body of work has focused on reasoning about events or states relative to temporal expressions. Moens and Steedman (1988) describes temporal expressions relating to changes of state; Condoravdi (2010) explores NPI licensing in temporal expressions. Broader context is also not -2 -1 -0.3 1 2 11/13 11/20 t 12/4 12/11 Δ, � � 0 11/27 Reference time P(11/20) = f −0.5 −1.5 f(x) 448 Range f(Duration) : Range catRight next (a) catRight(t, 2D ) catRight(t, −) next (b) Figure 2: The grammar – (a) describes the CFG parse of the temporal types. Words are tagged with their nonterminal entry, above which only the types of the expressions are maintained; (b) describes the corresponding combination of the temporal instances. The parse in (b) is deterministic given the grammar combination rules in (a). dir</context>
</contexts>
<marker>Condoravdi, 2010</marker>
<rawString>Cleo Condoravdi. 2010. NPI licensing in temporal clauses. Natural Language and Linguistic Theory, 28:877–910.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Richard Tobin</author>
<author>Beatrice Alex</author>
<author>Kate Byrne</author>
</authors>
<title>Edinburgh-LTG: TempEval-2 system description.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, Sem-Eval,</booktitle>
<pages>333--336</pages>
<contexts>
<context position="6036" citStr="Grover et al., 2010" startWordPosition="952" endWordPosition="955"> logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide particularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of time; a type system is described in Sect</context>
</contexts>
<marker>Grover, Tobin, Alex, Byrne, 2010</marker>
<rawString>Claire Grover, Richard Tobin, Beatrice Alex, and Kate Byrne. 2010. Edinburgh-LTG: TempEval-2 system description. In Proceedings of the 5th International Workshop on Semantic Evaluation, Sem-Eval, pages 333–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology, Parsing,</booktitle>
<pages>53--64</pages>
<contexts>
<context position="20258" citStr="Huang and Chiang (2005)" startWordPosition="3423" endWordPosition="3426">on, e.g., 5 weeks. • Combining a non-Nil and Nil element with no change to the temporal expression, e.g., a week. The lexicalization of the Nil type allows the algorithm to take hints from these supporting words. We proceed to describe learning the parameters of this grammar. 4 Learning We present a system architecture, described in Figure 3. We detail the inference procedure in Section 4.1 and training in Section 4.2. 4.1 Inference To provide a list of candidate expressions with their associated probabilities, we employ a k-best CKY parser. Specifically, we implement Algorithm 3 described in Huang and Chiang (2005), providing an 0(Gn3k log k) algorithm with respect to the grammar size G, phrase length n, and beam size k. We set the beam size to 2000. 1In the case of complex sequences (e.g., Friday the 13th) an A* search is performed to find overlapping ranges in the two sequences; the origin r3(0) is updated to refer to the closest such match to the reference time. Revisiting the notion of pragmatic ambiguity, in a sense the most semantically complete output of the system would be a distribution – an utterance of Friday would give a distribution over Fridays rather than a best guess of its grounding. Ho</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the Ninth International Workshop on Parsing Technology, Parsing, pages 53– 64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages. In</title>
<date>2005</date>
<booktitle>AAAI,</booktitle>
<pages>1062--1068</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="4412" citStr="Kate et al. (2005)" startWordPosition="687" endWordPosition="690">uistics 2 Related Work Our approach draws inspiration from a large body of work on parsing expressions into a logical form. The latent parse parallels the formal semantics in previous work, e.g., Montague semantics. Like these representations, a parse – in conjunction with the reference time – defines a set of matching entities, in this case the grounded time. The matching times can be thought of as analogous to the entities in a logical model which satisfy a given expression. Supervised approaches to logical parsing prominently include Zelle and Mooney (1996), Zettlemoyer and Collins (2005), Kate et al. (2005), Zettlemoyer and Collins (2007), inter alia. For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. This logical form importantly contains all the predicates and entities used in their parse. We loosen the supervision required in these systems by allowing the parse to be entirely latent; the annotation of the grounded time neither defines, nor gives any direct cues about the elements of the parse, since many parses evaluate to the same grounding. To demonstrate, the grounding for a week ago could be described by specifying a month and day, or as a </context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005. Learning to transform natural to formal languages. In AAAI, pages 1062–1068, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="23708" citStr="Klein and Manning, 2004" startWordPosition="4045" endWordPosition="4049">* |y, t) &gt; 0 then 12 ¯mθ += (y, p); ¯mµ,σ += (i, p) 13 end 14 end 15 M += normalize( ¯mθ) 16 Mµ,σ += normalize( ¯mµ,σ) 17 end 18 return M 19 end 20 begin M-Step( θ&apos; := bayesianPosterior( σ&apos; := mlePosterior( µ&apos; := bayesianPosterior( Mµ,σ, σ&apos;, N) return (θ&apos;, µ&apos;, σ&apos;) 25 end split on the characters ‘-’ and ‘/,’ which often delimit a boundary between temporal entities. Beyond this preprocessing, no language-specific information about the meanings of the words are introduced, including syntactic parses, POS tags, etc. The algorithm operates similarly to the EM algorithms used for grammar induction (Klein and Manning, 2004; Carroll and Charniak, 1992). However, unlike grammar induction, we are allowed a certain amount of supervision by requiring that the predicted temporal expression match the annotation. Our expected statistics are therefore more accurately our normalized expected counts of valid parses. Note that in conventional grammar induction, the expected sufficient statistics can be gathered analytically from reading off the chart scores of a parse. This does not work in our case for two reasons. In part, we would like to incorporate the probability of the temporal grounding in our feedback probability.</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: models of dependency and constituency. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oleksandr Kolomiyets</author>
<author>Marie-Francine Moens</author>
</authors>
<title>KUL: recognition and normalization of temporal expressions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, Sem-Eval ’10,</booktitle>
<pages>325--328</pages>
<contexts>
<context position="6389" citStr="Kolomiyets and Moens (2010)" startWordPosition="1007" endWordPosition="1010">re, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide particularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of time; a type system is described in Section 3.1 while the grammar is outlined in Section 3.2 and described in detail in Sections 3.3 and 3.4. 3.1 Temporal Expression Types We represent temporal expressions as either a Range, Sequence, or Duration. We describe these, the Function type, and the miscellaneous Number and Nil types below: Range [and Instant] A period between two dates (or times)</context>
</contexts>
<marker>Kolomiyets, Moens, 2010</marker>
<rawString>Oleksandr Kolomiyets and Marie-Francine Moens. 2010. KUL: recognition and normalization of temporal expressions. In Proceedings of the 5th International Workshop on Semantic Evaluation, Sem-Eval ’10, pages 325–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Lexical generalization in CCG grammar induction for semantic parsing.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>1512--1523</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="13498" citStr="Kwiatkowski et al., 2011" startWordPosition="2218" endWordPosition="2221">before to week and last week. We construct a paradigm for parsing temporal phrases consisting of a standard PCFG over temporal types with each parse rule defining a function to apply to the child nodes, or the word being generated. At the root of the tree, we recursively apply the functions in the parse tree to obtain a final temporal value. One can view this formalism as a ruleto-rule translation (Bach, 1976; Allen, 1995, p. 263), or a constrained Synchronous PCFG (Yamada and Knight, 2001). Our approach contrasts with common approaches, such as CCG grammars (Steedman, 2000; Bos et al., 2004; Kwiatkowski et al., 2011), giving us more flexibility in the composition rules. Figure 2 shows an example of the grammar. Formally, we define our temporal grammar G = (E, 5, V, W, R, 0). The alphabet E and start symbol 5 retain their usual interpretations. We define a set V to be the set of types, as described in Section 3.1 – these act as our nonterminals. For each v E V we define an (infinite) set Wv corresponding to the possible instances of type v. Concretely, if v = Sequence, our set Wv E W could contain elements corresponding to Friday, last Friday, Nov. 27th, etc. Each node in the tree defines a pair (v, w) suc</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical generalization in CCG grammar induction for semantic parsing. In EMNLP, pages 1512–1523, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="28933" citStr="Lafferty et al., 2001" startWordPosition="4921" endWordPosition="4924">-based systems. GUTime (Mani and Wilson, 2000) presents an older but widely used baseline.3 More recently, SUTime (Chang and Manning, 2012) provides a much stronger comparison. We also compare to HeidelTime (Str¨otgen and Gertz, 2010), which represents the state-of-the-art system at the TempEval-2 task. 5.3 Detection One of the advantages of our model is that it can provide candidate groundings for any expression. We explore this ability by building a detection model to find candidate temporal expressions, which we then ground. The detection model is implemented as a Conditional Random Field (Lafferty et al., 2001), with features over the morphology and context. Particularly, we define the following features: • The word and lemma within 2 of the current word. • The word shape4 and part of speech of the current word. 3Due to discrepancies in output formats, the output of GUTime was heuristically patched and manually checked to conform to the expected format. 4Word shape is calculated by mapping each character to one of uppercase, lowercase, number, or punctuation. The first four characters are mapped verbatim; subsequent sequences of similar characters are collapsed. 453 System P Extent F1 Attribute R Ty</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5198" citStr="Liang et al. (2011)" startWordPosition="826" endWordPosition="829">ortantly contains all the predicates and entities used in their parse. We loosen the supervision required in these systems by allowing the parse to be entirely latent; the annotation of the grounded time neither defines, nor gives any direct cues about the elements of the parse, since many parses evaluate to the same grounding. To demonstrate, the grounding for a week ago could be described by specifying a month and day, or as a week ago, or as last x – substituting today’s day of the week for x. Each of these correspond to a completely different parse. Recent work by Clarke et al. (2010) and Liang et al. (2011) similarly relax supervision to require only annotated answers rather than full logical forms. For example, Liang et al. (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither </context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2011. Learning dependency-based compositional semantics. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>George Wilson</author>
</authors>
<title>Robust temporal processing of news.</title>
<date>2000</date>
<booktitle>In ACL,</booktitle>
<pages>69--76</pages>
<location>Hong Kong.</location>
<contexts>
<context position="5977" citStr="Mani and Wilson, 2000" startWordPosition="942" endWordPosition="945">lar in structure to a dependency grammar, but representing a logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide particularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional</context>
<context position="28357" citStr="Mani and Wilson, 2000" startWordPosition="4830" endWordPosition="4833">the system is not allowed to abstain on expressions it does not 2See http://www.timeml.org for details on the TimeML format and TIMEX3 tag. Figure 4: A precision-recall curve for our system, compared to prior work. The data points are obtained by setting a threshold minimum probability at which to guess a time creating different extent recall values. The curve falls below HeidelTime1 and SUTime in part from lack of context, and in part since our system was not trained to optimize this curve. recognize. Results are summarized in Table 3. We compare to three previous rule-based systems. GUTime (Mani and Wilson, 2000) presents an older but widely used baseline.3 More recently, SUTime (Chang and Manning, 2012) provides a much stronger comparison. We also compare to HeidelTime (Str¨otgen and Gertz, 2010), which represents the state-of-the-art system at the TempEval-2 task. 5.3 Detection One of the advantages of our model is that it can provide candidate groundings for any expression. We explore this ability by building a detection model to find candidate temporal expressions, which we then ground. The detection model is implemented as a Conditional Random Field (Lafferty et al., 2001), with features over the</context>
</contexts>
<marker>Mani, Wilson, 2000</marker>
<rawString>Inderjeet Mani and George Wilson. 2000. Robust temporal processing of news. In ACL, pages 69–76, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Moens</author>
<author>Mark Steedman</author>
</authors>
<title>Temporal ontology and temporal reference.</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<pages>14--15</pages>
<contexts>
<context position="11506" citStr="Moens and Steedman (1988)" startWordPosition="1885" endWordPosition="1888"> number without any temporal meaning attached. This comes into play representing expressions such as 2 weeks. The other is the Nil type – denoting terms which are not directly contributing to the semantic meaning of the expression. This is intended for words such as a or the, which serve as cues without bearing temporal content themselves. The Nil type is lexicalized with the word it generates. Omitted Phenomena The representation described is a simplification of the complexities of time. Notably, a body of work has focused on reasoning about events or states relative to temporal expressions. Moens and Steedman (1988) describes temporal expressions relating to changes of state; Condoravdi (2010) explores NPI licensing in temporal expressions. Broader context is also not -2 -1 -0.3 1 2 11/13 11/20 t 12/4 12/11 Δ, � � 0 11/27 Reference time P(11/20) = f −0.5 −1.5 f(x) 448 Range f(Duration) : Range catRight next (a) catRight(t, 2D ) catRight(t, −) next (b) Figure 2: The grammar – (a) describes the CFG parse of the temporal types. Words are tagged with their nonterminal entry, above which only the types of the expressions are maintained; (b) describes the corresponding combination of the temporal instances. Th</context>
</contexts>
<marker>Moens, Steedman, 1988</marker>
<rawString>Marc Moens and Mark Steedman. 1988. Temporal ontology and temporal reference. Computational Linguistics, 14:15–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Puscasu</author>
</authors>
<title>A framework for temporal resolution. In</title>
<date>2004</date>
<booktitle>LREC,</booktitle>
<pages>1901--1904</pages>
<contexts>
<context position="6014" citStr="Puscasu, 2004" startWordPosition="950" endWordPosition="951"> representing a logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide particularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of time; a type syste</context>
</contexts>
<marker>Puscasu, 2004</marker>
<rawString>G. Puscasu. 2004. A framework for temporal resolution. In LREC, pages 1901–1904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Reichenbach</author>
</authors>
<date>1947</date>
<booktitle>Elements of Symbolic Logic.</booktitle>
<publisher>Macmillan,</publisher>
<location>New York.</location>
<contexts>
<context position="8078" citStr="Reichenbach, 1947" startWordPosition="1299" endWordPosition="1300">ge. In the case of the sequence Friday, rs(0) corresponds to the Friday in the current week; rs(1) is the Friday in the following week, etc. 2. As: The distance between two elements in the sequence – approximated if this distance is not constant. In the case of Friday, this distance would be a week. 3. Ps: The containing unit of an element of a sequence. For example, PFriday would be the Range corresponding to the current week. The sequence index i E Z, from rs(i), is defined 447 relative to rs(0) – the element in the same containing unit as the reference time. We define the reference time t (Reichenbach, 1947) to be the instant relative to which times are evaluated. For the TempEval-2 corpus, we approximate this as the publication time of the article. While this is conflating Reichenbach’s reference time with speech time, it is a useful approximation. To contrast with Ranges, a Sequence can represent a number of grounded times. Nonetheless, pragmatically, not all of these are given equal weight – an utterance of last Friday may mean either of the previous two Fridays, but is unlikely to ground to anything else. We represent this ambiguity by defining a distribution over the elements of the Sequence</context>
</contexts>
<marker>Reichenbach, 1947</marker>
<rawString>Hans Reichenbach. 1947. Elements of Symbolic Logic. Macmillan, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Saquete</author>
<author>R Muoz</author>
<author>P Martnez-Barco</author>
</authors>
<title>Terseo: Temporal expression resolution system applied to event ordering.</title>
<date>2003</date>
<booktitle>In Text, Speech and Dialogue,</booktitle>
<pages>220--228</pages>
<contexts>
<context position="5999" citStr="Saquete et al., 2003" startWordPosition="946" endWordPosition="949">ependency grammar, but representing a logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide particularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of tim</context>
</contexts>
<marker>Saquete, Muoz, Martnez-Barco, 2003</marker>
<rawString>E. Saquete, R. Muoz, and P. Martnez-Barco. 2003. Terseo: Temporal expression resolution system applied to event ordering. In Text, Speech and Dialogue, pages 220–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The syntactic process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="13453" citStr="Steedman, 2000" startWordPosition="2212" endWordPosition="2213"> previous week; and then applying before to week and last week. We construct a paradigm for parsing temporal phrases consisting of a standard PCFG over temporal types with each parse rule defining a function to apply to the child nodes, or the word being generated. At the root of the tree, we recursively apply the functions in the parse tree to obtain a final temporal value. One can view this formalism as a ruleto-rule translation (Bach, 1976; Allen, 1995, p. 263), or a constrained Synchronous PCFG (Yamada and Knight, 2001). Our approach contrasts with common approaches, such as CCG grammars (Steedman, 2000; Bos et al., 2004; Kwiatkowski et al., 2011), giving us more flexibility in the composition rules. Figure 2 shows an example of the grammar. Formally, we define our temporal grammar G = (E, 5, V, W, R, 0). The alphabet E and start symbol 5 retain their usual interpretations. We define a set V to be the set of types, as described in Section 3.1 – these act as our nonterminals. For each v E V we define an (infinite) set Wv corresponding to the possible instances of type v. Concretely, if v = Sequence, our set Wv E W could contain elements corresponding to Friday, last Friday, Nov. 27th, etc. Ea</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The syntactic process. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jannik Str¨otgen</author>
<author>Michael Gertz</author>
</authors>
<title>Heideltime: High quality rule-based extraction and normalization of temporal expressions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval,</booktitle>
<pages>321--324</pages>
<marker>Str¨otgen, Gertz, 2010</marker>
<rawString>Jannik Str¨otgen and Michael Gertz. 2010. Heideltime: High quality rule-based extraction and normalization of temporal expressions. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval, pages 321–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naushad UzZaman</author>
<author>James F Allen</author>
</authors>
<title>TRIPS and TRIOS system for TempEval-2: Extracting temporal information from text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="6252" citStr="UzZaman and Allen (2010)" startWordPosition="982" endWordPosition="985">ther than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide particularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of time; a type system is described in Section 3.1 while the grammar is outlined in Section 3.2 and described in detail in Sections 3.3 and 3.4. 3.1 Temporal Expression Types We represent temporal expressions as either a Range, Sequence, or Duration. We descr</context>
</contexts>
<marker>UzZaman, Allen, 2010</marker>
<rawString>Naushad UzZaman and James F. Allen. 2010. TRIPS and TRIOS system for TempEval-2: Extracting temporal information from text. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval, pages 276–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Roser Sauri</author>
<author>Tommaso Caselli</author>
<author>James Pustejovsky</author>
</authors>
<date>2010</date>
<booktitle>Semeval-2010 task 13: TempEval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>57--62</pages>
<location>Uppsala,</location>
<contexts>
<context position="26470" citStr="Verhagen et al., 2010" startWordPosition="4520" endWordPosition="4523">o ( Mθ, Mµ,σ) := E-Step (D,θ,µ,σ) (θ, µ, σ) := M-Step ( Mθ, Mµ,σ) Mθ, Mµ,σ) 21 22 23 24 Mµ,σ) Mθ, α) �θ&apos;mn|l = α+ 452 System Type Train Type Test Value Value GUTime 0.72 0.46 0.80 0.42 SUTime 0.85 0.69 0.94 0.71 HeidelTime 0.80 0.67 0.85 0.71 OurSystem 0.90 0.72 0.88 0.72 Table 3: TempEval-2 Attribute scores for our system and three previous systems. The scores are calculated using gold extents, forcing a guessed interpretation for each parse. 0 0.2 0.4 0.6 0.8 1 Extent recall OurSystem HeidelTime1 HeidelTime2 SUTime Value accuracy 1 0.8 0.6 0.4 0.2 0 portion of the TempEval-2 Task A dataset (Verhagen et al., 2010). 5.1 Dataset The TempEval-2 dataset is relatively small, containing 162 documents and 1052 temporal phrases in the training set and an additional 20 documents and 156 phrases in the evaluation set. Each temporal phrase was annotated as a TIMEX32 tag around an adverbial or prepositional phrase 5.2 Results In the TempEval-2 A Task, system performance is evaluated on detection and resolution of expressions. Since we perform only the second of these, we evaluate our system assuming gold detection. Similarly, the original TempEval-2 scoring scheme gave a precision and recall for detection, and an </context>
</contexts>
<marker>Verhagen, Sauri, Caselli, Pustejovsky, 2010</marker>
<rawString>Marc Verhagen, Roser Sauri, Tommaso Caselli, and James Pustejovsky. 2010. Semeval-2010 task 13: TempEval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 57–62, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In ACL,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="13368" citStr="Yamada and Knight, 2001" startWordPosition="2198" endWordPosition="2201">fore last week. We can construct a meaning by applying the modifier last to week – creating the previous week; and then applying before to week and last week. We construct a paradigm for parsing temporal phrases consisting of a standard PCFG over temporal types with each parse rule defining a function to apply to the child nodes, or the word being generated. At the root of the tree, we recursively apply the functions in the parse tree to obtain a final temporal value. One can view this formalism as a ruleto-rule translation (Bach, 1976; Allen, 1995, p. 263), or a constrained Synchronous PCFG (Yamada and Knight, 2001). Our approach contrasts with common approaches, such as CCG grammars (Steedman, 2000; Bos et al., 2004; Kwiatkowski et al., 2011), giving us more flexibility in the composition rules. Figure 2 shows an example of the grammar. Formally, we define our temporal grammar G = (E, 5, V, W, R, 0). The alphabet E and start symbol 5 retain their usual interpretations. We define a set V to be the set of types, as described in Section 3.1 – these act as our nonterminals. For each v E V we define an (infinite) set Wv corresponding to the possible instances of type v. Concretely, if v = Sequence, our set W</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In ACL, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In AAAI/IAAI,</booktitle>
<pages>1050--1055</pages>
<location>Portland, OR.</location>
<contexts>
<context position="4360" citStr="Zelle and Mooney (1996)" startWordPosition="678" endWordPosition="681">June 3-8, 2012. c�2012 Association for Computational Linguistics 2 Related Work Our approach draws inspiration from a large body of work on parsing expressions into a logical form. The latent parse parallels the formal semantics in previous work, e.g., Montague semantics. Like these representations, a parse – in conjunction with the reference time – defines a set of matching entities, in this case the grounded time. The matching times can be thought of as analogous to the entities in a logical model which satisfy a given expression. Supervised approaches to logical parsing prominently include Zelle and Mooney (1996), Zettlemoyer and Collins (2005), Kate et al. (2005), Zettlemoyer and Collins (2007), inter alia. For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. This logical form importantly contains all the predicates and entities used in their parse. We loosen the supervision required in these systems by allowing the parse to be entirely latent; the annotation of the grounded time neither defines, nor gives any direct cues about the elements of the parse, since many parses evaluate to the same grounding. To demonstrate, the grounding for a week ago could </context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In AAAI/IAAI, pages 1050–1055, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In UAI,</booktitle>
<pages>658--666</pages>
<publisher>AUAI Press.</publisher>
<contexts>
<context position="4392" citStr="Zettlemoyer and Collins (2005)" startWordPosition="682" endWordPosition="686">sociation for Computational Linguistics 2 Related Work Our approach draws inspiration from a large body of work on parsing expressions into a logical form. The latent parse parallels the formal semantics in previous work, e.g., Montague semantics. Like these representations, a parse – in conjunction with the reference time – defines a set of matching entities, in this case the grounded time. The matching times can be thought of as analogous to the entities in a logical model which satisfy a given expression. Supervised approaches to logical parsing prominently include Zelle and Mooney (1996), Zettlemoyer and Collins (2005), Kate et al. (2005), Zettlemoyer and Collins (2007), inter alia. For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. This logical form importantly contains all the predicates and entities used in their parse. We loosen the supervision required in these systems by allowing the parse to be entirely latent; the annotation of the grounded time neither defines, nor gives any direct cues about the elements of the parse, since many parses evaluate to the same grounding. To demonstrate, the grounding for a week ago could be described by specifying a mon</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI, pages 658–666. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form. In EMNLP-CoNLL,</title>
<date>2007</date>
<pages>678--687</pages>
<contexts>
<context position="4444" citStr="Zettlemoyer and Collins (2007)" startWordPosition="691" endWordPosition="695">rk Our approach draws inspiration from a large body of work on parsing expressions into a logical form. The latent parse parallels the formal semantics in previous work, e.g., Montague semantics. Like these representations, a parse – in conjunction with the reference time – defines a set of matching entities, in this case the grounded time. The matching times can be thought of as analogous to the entities in a logical model which satisfy a given expression. Supervised approaches to logical parsing prominently include Zelle and Mooney (1996), Zettlemoyer and Collins (2005), Kate et al. (2005), Zettlemoyer and Collins (2007), inter alia. For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. This logical form importantly contains all the predicates and entities used in their parse. We loosen the supervision required in these systems by allowing the parse to be entirely latent; the annotation of the grounded time neither defines, nor gives any direct cues about the elements of the parse, since many parses evaluate to the same grounding. To demonstrate, the grounding for a week ago could be described by specifying a month and day, or as a week ago, or as last x – substit</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In EMNLP-CoNLL, pages 678–687.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>