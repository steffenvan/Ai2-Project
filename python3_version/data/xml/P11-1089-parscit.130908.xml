<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000830">
<title confidence="0.998934">
A Discriminative Model for Joint Morphological Disambiguation and
Dependency Parsing
</title>
<author confidence="0.998446">
John Lee
</author>
<affiliation confidence="0.988209333333333">
Department of Chinese,
Translation and Linguistics
City University of Hong Kong
</affiliation>
<email confidence="0.99195">
jsylee@cityu.edu.hk
</email>
<author confidence="0.995801">
Jason Naradowsky, David A. Smith
</author>
<affiliation confidence="0.9987495">
Department of Computer Science
University of Massachusetts, Amherst
</affiliation>
<email confidence="0.998519">
{narad,dasmith}@cs.umass.edu
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912368421053">
Most previous studies of morphological dis-
ambiguation and dependency parsing have
been pursued independently. Morphological
taggers operate on n-grams and do not take
into account syntactic relations; parsers use
the “pipeline” approach, assuming that mor-
phological information has been separately
obtained.
However, in morphologically-rich languages,
there is often considerable interaction between
morphology and syntax, such that neither can
be disambiguated without the other. In this pa-
per, we propose a discriminative model that
jointly infers morphological properties and
syntactic structures. In evaluations on various
highly-inflected languages, this joint model
outperforms both a baseline tagger in morpho-
logical disambiguation, and a pipeline parser
in head selection.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979444444445">
To date, studies of morphological analysis and
dependency parsing have been pursued more or
less independently. Morphological taggers dis-
ambiguate morphological attributes such as part-
of-speech (POS) or case, without taking syntax
into account (Hakkani-T¨ur et al., 2000; Hajiˇc et
al., 2001); dependency parsers commonly assume
the “pipeline” approach, relying on morphologi-
cal information as part of the input (Buchholz and
Marsi, 2006; Nivre et al., 2007). This approach
serves many languages well, especially those with
less morphological ambiguity. In English, for ex-
ample, accuracy of POS tagging has risen above
97% (Toutanova et al., 2003), and that of depen-
dency parsing has reached the low nineties (Nivre
et al., 2007). For these languages, there may be little
to be gained to justify the computational cost of in-
corporating syntactic inference during the morpho-
logical tagging task; conversely, it is doubtful that
errorful morphological information is a main cause
of errors in English dependency parsing.
However, the pipeline approach seems more prob-
lematic for morphologically-rich languages with
substantial interactions between morphology and
syntax (Tsarfaty, 2006). Consider the Latin sen-
tence, Una dies omnis potuit praecurrere amantis,
‘One day was able to make up for all the lovers’1. As
shown in Table 1, the adjective omnis (‘all’) is am-
biguous in number, gender, and case; there are seven
valid analyses. From the perspective of a finite-
state morphological tagger, the most attractive anal-
ysis is arguably the singular nominative, since omnis
is immediately followed by the singular verb potuit
(‘could’). Indeed, the baseline tagger used in this
study did make this decision. Given its nominative
case, the pipeline parser assigned the verb potuit to
be its head; the two words form the typical subject-
verb relation, agreeing in number.
Unfortunately, as shown in Figure 1, the word om-
nis in fact modifies the noun amantis, at the end of
the sentence. As a result, despite the distance be-
tween them, they must agree in number, gender and
case, i.e., both must be plural masculine (or femi-
nine) accusative. The pipeline parser, acting on the
input that omnis is nominative, naturally did not see
</bodyText>
<footnote confidence="0.881329">
1Taken from poem 1.13 by Sextus Propertius, English trans-
lation by Katz (2004).
</footnote>
<page confidence="0.948197">
885
</page>
<note confidence="0.936439285714286">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 885–894,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
Latin Una dies omnis potuit praecurrere amantis
English one day all could to surpass lovers
Number sg pl sg pl sg sg pl sg - sg pl
Gender f n m/f m/f m/f m/f/n m/f - - m/f/n m/f
Case nom/ab nom/acc nom nom/acc nom gen acc - - gen acc
</note>
<tableCaption confidence="0.72249375">
Table 1: The Latin sentence “Una dies omnis potuit praecurrere amantis”, meaning ‘One day was able to make up
for all the lovers’, shown with glosses and possible morphological analyses. The correct analyses are shown in bold.
The word omnis has 7 possible combinations of number, gender and case, while amantis has 5. Disambiguation partly
depends on establishing amantis as the head of omnis, and so the two must agree in all three attributes.
</tableCaption>
<bodyText confidence="0.99973747826087">
this agreement, and therefore did not consider this
syntactic relation likely.
Such a dilemma is not uncommon in languages
with relatively free word order. On the one hand,
it appears difficult to improve morphological tag-
ging accuracy on words like omnis without syntactic
knowledge; on the other hand, a parser cannot reli-
ably disambiguate syntax unless it has accurate mor-
phological information, in this example the agree-
ment in number, gender, and case.
In this paper we propose to attack this chicken-
and-egg problem with a discriminative model that
jointly infers morphological and syntactic properties
of a sentence, given its words as input. In eval-
uations on various highly-inflected languages, the
model outperforms both a baseline tagger in mor-
phological disambiguation, and a pipeline parser in
head selection.
After a description of previous work (§2), the
joint model (§3) will be contrasted with the base-
line pipeline model (§4). Experimental results (§5-
6) will then be presented, followed by conclusions
and future directions.
</bodyText>
<sectionHeader confidence="0.993986" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999957833333333">
Since space does not allow a full review of the vast
literature on morphological analysis and parsing, we
focus only on past research involving joint morpho-
logical and syntactic inference (§2.1); we then dis-
cuss Latin (§2.2), a language representative of the
challenges that motivated our approach.
</bodyText>
<subsectionHeader confidence="0.9800645">
2.1 Joint Morphological and Syntactic
Inference
</subsectionHeader>
<bodyText confidence="0.99976">
Most previous work in morphological disambigua-
tion, even when applied on morphologically com-
plex languages with relatively free word order,
</bodyText>
<equation confidence="0.784643285714286">
potuit
could
praecurrere
to surpass
amantis
lovers
omnis
</equation>
<figureCaption confidence="0.7826876">
Figure 1: Dependency tree for the sentence “Una dies
omnis potuit praecurrere amantis”. The word omnis is
an adjective modifying the noun amantis. This informa-
tion is key to the morphological disambiguation of both
words, as shown in Table 1.
</figureCaption>
<bodyText confidence="0.96684235">
such as Turkish (Hakkani-T¨ur et al., 2000) and
Czech (Hajiˇc et al., 2001), did not consider syn-
tactic relationships between words. In the litera-
ture on data-driven parsing, two recent studies at-
tempted joint inference on morphology and syntax,
and both considered phrase-structure trees for Mod-
ern Hebrew (Cohen and Smith, 2007; Goldberg and
Tsarfaty, 2008).
The primary focus of morphological processing in
Modern Hebrew is splitting orthographic words into
morphemes: clitics such as prepositions, pronouns,
and the definite article must be separated from the
core word. Each of the resulting morphemes is then
tagged with an atomic “part-of-speech” to indicate
word class and some morphological features. Sim-
ilarly, the English POS tags in the Penn Treebank
combine word class information with morphologi-
dies
day
una
</bodyText>
<page confidence="0.995774">
886
</page>
<bodyText confidence="0.9995562">
cal attributes such as “plural” or “past tense”.
Cohen and Smith (2007) separately train a dis-
criminative conditional random field (CRF) for seg-
mentation and tagging, and a generative probabilis-
tic context-free grammar (PCFG) for parsing. At de-
coding time, the two models are combined as a prod-
uct of experts. Goldberg and Tsarfaty (2008) pro-
pose a generative joint model. This paper is the first
to use a fully discriminative model for joint morpho-
logical and syntactic inference on dependency trees.
</bodyText>
<subsectionHeader confidence="0.998815">
2.2 Latin
</subsectionHeader>
<bodyText confidence="0.999832428571429">
Unlike Modern Hebrew, Latin does not require ex-
tensive morpheme segmentation2. However, it does
have a relatively free word order, and is also highly
inflected, with each word having up to nine morpho-
logical attributes, listed in Table 2. In addition to its
absolute numbers of cases, moods, and tenses, Latin
morphology is fusional. For instance, the suffix
−is in omnis cannot be segmented into morphemes
that separately indicate gender, number, and case.
According to the Latin morphological database en-
coded in MORPHEUS (Crane, 1991), 30% of Latin
nouns can be parsed as another part-of-speech, and
on average each has 3.8 possible morphological in-
terpretations.
We know of only one previous attempt in data-
driven dependency parsing for Latin (Bamman and
Crane, 2008), with the goal of constructing a dy-
namic lexicon for a digital library. Parsing is per-
formed using the usual pipeline approach, first with
the TreeTagger analyzer (Schmid, 1994) and then
with a state-of-the-art dependency parser (McDon-
ald et al., 2005). Head selection accuracy was
61.49%, and rose to 64.99% with oracle morpho-
logical tags. Of the nine morphological attributes,
gender and especially case had the lowest accu-
racy. This observation echoes the findings for
Czech (Smith et al., 2005), where case was also the
most difficult to disambiguate.
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="method">
3 Joint Model
</sectionHeader>
<bodyText confidence="0.999919">
This section describes a model that jointly infers
morphological and syntactic properties of a sen-
tence. It will be presented as a graphical model,
</bodyText>
<footnote confidence="0.736171">
2Except for enclitics such as -que, -ve, and -ne, but their
segmentation is rather straightforward compared to Modern He-
brew or other Semitic languages.
</footnote>
<table confidence="0.999480111111111">
Attribute Values
Part-of- noun, verb, participle, adjective,
speech adverb, conjunction, preposition,
(POS) pronoun, numeral, interjection,
exclamation, punctuation
Person first, second, third
Number singular, plural
Tense present, imperfect, perfect,
pluperfect, future perfect, future
Mood indicative, subjunctive, infinitive,
imperative, participle, gerund,
gerundive, supine
Voice active, passive
Gender masculine, feminine, neuter
Case nominative, genitive, dative,
accusative, ablative, vocative,
locative
Degree comparative, superlative
</table>
<tableCaption confidence="0.982281">
Table 2: Morphological attributes and values for Latin.
</tableCaption>
<bodyText confidence="0.899815">
Ancient Greek has the same attributes; Czech and Hun-
garian lack some of them. In all categories except POS,
a value of null (‘-’) may also be assigned. For example, a
noun has ‘-’ for the tense attribute.
starting with the variables and then the factors,
which represents constraints on the variables. Let
n be the number of words and m be the number of
possible values for a morphological attribute. The
variables are:
</bodyText>
<listItem confidence="0.997377833333333">
• WORD: the n words w1,...,wn of the input sen-
tence, all observed.
• TAG: O(nm) boolean variables3 Ta,i,v, corre-
sponding to each value of the morphological at-
tributes listed in Table 2. Ta,i,v = true when
the word wi has value v as its morphological
attribute a. In Figure 2, CASE3,acc is the short-
hand representing the variable Tcase,3,acc. It is
set to true since the word w3 has the accusative
case.
• LINK: O(n2) boolean variables Li,j corre-
sponding to a possible link between each pair
</listItem>
<bodyText confidence="0.504997">
3The TAG variables were actually implemented as multino-
mials, but are presented here as booleans for ease of understand-
ing.
</bodyText>
<page confidence="0.993419">
887
</page>
<figureCaption confidence="0.998847166666667">
Figure 2: The joint model (§3) depicted as a graphical model. The variables, all boolean, are represented by circles and
are bolded if their correct values are true. Factors are represented by rectangles and are bolded if they fire. For clarity,
this graph shows only those variables and factors associated with one pair of words (i.e., w3=omnis and w6=amantis)
and with one morphological attribute (i.e., case). The variables L3,6, CASE3,a,, and CASE6,a,, are bolded, indicating
that w3 and w6 are linked and both have the accusative case. The ternary factor CASE-LINK, that connects to these
three variable, therefore fires.
</figureCaption>
<figure confidence="0.999901769230769">
WORD
LINK
TREE
WORD−
LINK
CASE−
BIGRAM
CASE 2,...
CASE−
UNIGRAM
CASE−
BIGRAM
CASE−
UNIGRAM
L3,6 L4,6
CASE
3,acc
CASE
LINK
CASE
6,acc
CASE 5,...
CASE−
LINK
CASE−
UNIGRAM
CASE−
LINK
CASE 6,gen
CASE−
UNIGRAM
CASE 3,nom
CASE−
LINK
CASE−
UNIGRAM
CASE 3,gen
CASE−
LINK
</figure>
<figureCaption confidence="0.844892">
of words4. Li,j = true when there is a depen-
dency link from the word wi to the word wj. In
Figure 2, the variable L3,6 is set to true since
there is a dependency link between the words
w3 and w6.
</figureCaption>
<bodyText confidence="0.9999525">
We define a probability distribution over all joint as-
signments A to the above variables,
</bodyText>
<equation confidence="0.997481333333333">
1 fl Fk(A) (1)
p(A) = Z
k
</equation>
<bodyText confidence="0.999395125">
where Z is a normalizing constant. The assign-
ment A is subject to a hard constraint, represented
in Figure 2 as TREE, requiring that the values of
the LINK variables must yield a tree, which may
be non-projective. The factors Fk(A) represent soft
constraints evaluating various aspects of the “good-
ness” of the tree structure implied by A. We say a
factor “fires” when all its neighboring variables are
</bodyText>
<footnote confidence="0.9522745">
4Variables for link labels can be integrated in a straightfor-
ward manner, if desired.
</footnote>
<bodyText confidence="0.9995192">
true and it evaluates to a non-negative real num-
ber; otherwise, it evaluates to 1 and has no effect
on the product in equation (1). Soft constraints in
the model are divided into local and link factors, to
which we now turn.
</bodyText>
<subsectionHeader confidence="0.99815">
3.1 Local Factors
</subsectionHeader>
<bodyText confidence="0.9955542">
The local factors consult either one word or two
neighboring words, and their morphological at-
tributes. These factors express the desirability of the
assignments of morphological attributes based on lo-
cal context. There are three types:
</bodyText>
<listItem confidence="0.997985777777778">
• TAG-UNIGRAM: There are O(nm) such unary
factors, each instance of which is connected to
a TAG variable. The factor fires when Ta,i,v
is true. The features consist of the value v
of the morphological attribute concerned, com-
bined with the word identity of wi, with back-
off using all suffixes of the word. The CASE-
UNIGRAM factors shown in Figure 2 are ex-
amples of this family of factors.
</listItem>
<page confidence="0.853745">
888
</page>
<listItem confidence="0.915690357142857">
• TAG-BIGRAM: There are O(nm2) of such bi-
nary factors, each connected to the TAG vari-
ables of a pair of neighboring words. The factor
fires when Ta,i,v1 and Ta,i+1,v2 are both true.
The CASE-BIGRAM factors shown in Figure 2
are examples of this family of factors.
• TAG-CONSISTENCY: For each word, the TAG
variables representing the possible POS val-
ues are connected to those representing the val-
ues of other morphological attributes, yield-
ing O(nm2) binary factors. They fire when
Tpos,i,v1 and Ta,i,v2 are both true. These fac-
tors are intended to discourage inconsistent as-
signments, such as a non-null tense for a noun.
</listItem>
<bodyText confidence="0.9999">
It is clear that so far, none of these factors are aware
of the morphological agreement between omnis and
amantis, crucial for inferring their syntactic relation.
We now turn our attention to link factors, which
serve this purpose.
</bodyText>
<subsectionHeader confidence="0.999667">
3.2 Link Factors
</subsectionHeader>
<bodyText confidence="0.9953206">
The link factors consult all pairs of words, possibly
separated by a long distance, that may have a de-
pendency link. These factors model the likelihood
of such a link based on the word identities and their
morphological attributes:
</bodyText>
<listItem confidence="0.985881857142857">
• WORD-LINK: There are O(n2) such unary fac-
tors, each connected to a LINK variable, as
shown in Figure 2. The factor fires when Li,j
is true. Features include various combina-
tions of the word identities of the parent wi and
child wj, and 5-letter prefixes of these words,
replicating the so-called “basic features” used
by McDonald et al. (2005).
• POS-LINK: There are O(n2m2) such ternary
factors, each connected to the variables Li,j,
Ti,pos,vi and Tj,pos,vj. It fires when all three are
true or, in other words, when the parent word
wi has POS vi, and the child wj has POS vj.
Features replicate all the so-called “basic fea-
</listItem>
<bodyText confidence="0.986300259259259">
tures” used by McDonald et al. (2005) that in-
volve POS. These factors are not shown in Fig-
ure 2, but would have exactly the same struc-
ture as the CASE-LINK factors.
Beyond these basic features, McDonald et al.
(2005) also utilize POS trigrams and POS 4-
grams. Both include the POS of two linked
words, wi and wj. The third component in the
trigrams is the POS of each word wk located
between wi and wj, i &lt; k &lt; j. The two ad-
ditional components that make up the 4-grams
are subsets of the POS of words located to the
immediate left and right of wi and wj.
If fully implemented in our joint model, these
features would necessitate two separate fami-
lies of link factors: O(n3m3) factors for the
POS trigrams, and O(n2m4) factors for the
POS 4-grams. To avoid this substantial in-
crease in model complexity, these features are
instead approximated: the POS of all words
involved in the trigrams and 4-grams, except
those of wi and wj, are regarded as fixed, their
values being taken from the output of a mor-
phological tagger (§4.1), rather than connected
to the appropriate TAG variables. This approxi-
mation allows these features to be incorporated
in the POS-LINK factors.
</bodyText>
<listItem confidence="0.979211">
• MORPH-LINK: There are O(n2m2) such
</listItem>
<bodyText confidence="0.8795077">
ternary factors, each connected to the variables
Li,j, Ti,a,vi and Tj,a,vj, for every attribute a
other than POS. The factor fires when all three
variables are true, and both vi and vj are non-
null; i.e., it fires when the parent word wi has
vi as its morphological attribute a, and the child
wj has vj. Features include the combination of
vi and vj themselves, and agreement between
them. The CASE-LINK factors in Figure 2 are
an example of this family of factors.
</bodyText>
<sectionHeader confidence="0.995254" genericHeader="method">
4 Baselines
</sectionHeader>
<bodyText confidence="0.9999883">
To ensure a meaningful comparison with the joint
model, our two baselines are both implemented in
the same graphical model framework, and trained
with the same machine-learning algorithm. Roughly
speaking, they divide up the variables and factors of
the joint model and train them separately. For mor-
phological disambiguation, we use the baseline tag-
ger described in §4.1. For dependency parsing, our
baseline is a “pipeline” parser (§4.2) that infers syn-
tax upon the output of the baseline tagger.
</bodyText>
<page confidence="0.996679">
889
</page>
<subsectionHeader confidence="0.94987">
4.1 Baseline Morphological Tagger
</subsectionHeader>
<bodyText confidence="0.99975525">
The tagger is a graphical model with the WORD
and TAG variables, connected by the local fac-
tors TAG-UNIGRAM, TAG-BIGRAM, and TAG-
CONSISTENCY, all used in the joint model (§3).
</bodyText>
<subsectionHeader confidence="0.993579">
4.2 Baseline Dependency Parser
</subsectionHeader>
<bodyText confidence="0.999927928571429">
The parser has no local factors, but has the same
variables as the joint model and the same features
from all three families of link factors (§3). However,
since it takes as input the morphological attributes
predicted by the tagger, the TAG variables are now
observed. This leads to a change in the structure
of the link factors — all features from the POS-
LINK factors now belong to the WORD-LINK fac-
tors, since the POS of all words are observed. In
short, the features of the parser are a replication of
(McDonald et al., 2005), but also extended beyond
POS to the other morphological attributes, with the
features in the MORPH-LINK factors incorporated
into WORD-LINK for similar reasons.
</bodyText>
<sectionHeader confidence="0.999161" genericHeader="method">
5 Experimental Set-up
</sectionHeader>
<subsectionHeader confidence="0.899195">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999931">
Our evaluation focused on the Latin Dependency
Treebank (Bamman and Crane, 2006), created at
the Perseus Digital Library by tailoring the Prague
Dependency Treebank guidelines for the Latin lan-
guage. It consists of excerpts from works by eight
Latin authors. We randomly divided the 53K-word
treebank into 10 folds of roughly equal sizes, with an
average of 5314 words (347 sentences) per fold. We
used one fold as the development set and performed
cross-validation on the other nine.
To measure how well our model generalizes
to other highly-inflected, relatively free-word-order
languages, we considered Ancient Greek, Hungar-
ian, and Czech. Their respective datasets consist of
8000 sentences from the Ancient Greek Dependency
Treebank (Bamman et al., 2009), 5800 from the
Hungarian Szeged Dependency Treebank (Vincze et
al., 2010), and a subset of 3100 from the Prague De-
pendency Treebank (B¨ohmov´a et al., 2003).
</bodyText>
<subsectionHeader confidence="0.995957">
5.2 Training
</subsectionHeader>
<bodyText confidence="0.999886">
We define each factor in (1) as a log-linear function:
</bodyText>
<equation confidence="0.9940325">
Fk(A) = exp � Bhfh(A, W, k) (2)
h
</equation>
<bodyText confidence="0.9999642">
Given an assignment A and words W, fh is an
indicator function describing the presence or ab-
sence of the feature, and Bh is the corresponding set
of weights learned using stochastic gradient ascent,
with the gradients inferred by loopy belief propaga-
tion (Smith and Eisner, 2008). The variance of the
Gaussian prior is set to 1. The other two parameters
in the training process, the number of belief propa-
gation iterations and the number of training rounds,
were tuned on the development set.
</bodyText>
<subsectionHeader confidence="0.98707">
5.3 Decoding
</subsectionHeader>
<bodyText confidence="0.999991642857143">
The output of the joint model is the assignment to
the TAG and LINK variables. Loopy belief propaga-
tion (BP) was used to calculate the posterior proba-
bilities of these variables. For TAG, we emit the tag
with the highest posterior probability as computed
by sum-product BP. We produced head attachments
by first calculating the posteriors of the LINK vari-
ables with BP and then passing them to an edge-
factored tree decoder. This is equivalent to mini-
mum Bayes risk decoding (Goodman, 1996), which
is used by Cohen and Smith (2007) and Smith and
Eisner (2008). This MBR decoding procedure en-
forces the hard constraint that the output be a tree
but sums over possible morphological assignments.5
</bodyText>
<subsectionHeader confidence="0.855342">
5.4 Reducing Model Complexity
</subsectionHeader>
<bodyText confidence="0.9999452">
In principle, the joint model should consider every
possible combination of morphological attributes for
every word. In practice, to reduce the complexity
of the model, we used a pre-existing morphological
database, MORPHEUS (Crane, 1991), to constrain
the range of possible values of the attributes listed
in Table 2; more precisely, we add a hard constraint,
requiring that assignments to the TAG variables be
compatible with MORPHEUS. This constraint signif-
icantly reduces the value of m in the big-O notation
</bodyText>
<footnote confidence="0.7789035">
5This approach to nuisance variables has also been used
effectively for parsing with tree-substitution grammars, where
several derived trees may correspond to each derivation tree,
and parsing with PCFGs with latent annotations.
</footnote>
<page confidence="0.985551">
890
</page>
<table confidence="0.999627416666667">
Model Tagger Joint Tagger Joint
Attr. all all non-null non-null
POS 94.4 94.5 94.4 94.5
Person 99.4 99.5 97.1 97.6
Number 95.3 95.9 93.7 94.5
Tense 98.0 98.2 93.2 93.9
Mood 98.1 98.3 93.8 94.4
Voice 98.5 98.6 95.3 95.7
Gender 93.1 93.9 87.7 89.1
Case 89.3 90.0 79.9 81.2
Degree 99.9 99.9 86.4 90.8
UAS 61.0 61.9 — —
</table>
<tableCaption confidence="0.9970794">
Table 3: Latin morphological disambiguation and pars-
ing. For some attributes, such as degree, a substan-
tial portion of words have the null value. The non-null
columns provides a sharper picture by excluding these
“easy” cases. Note that POS is never null.
</tableCaption>
<bodyText confidence="0.99996555">
for the number of variables and factors described in
§3. To illustrate the effect, the graphical model of
the sentence in Table 1, whose six words are all cov-
ered by the database, has 1,866 factors; without the
benefit of the database, the full model would have
31,901 factors.
The MORPHEUS database was automatically gen-
erated from a list of stems, inflections, irregular
forms and morphological rules. It covers about 99%
of the distinct words in the Latin Dependency Tree-
bank. At decoding time, for each fold, the database
is further augmented with tags seen in training data.
After this augmentation, an average of 44 words are
“unseen” in each fold.
Similarly, we constructed morphological dictio-
naries for Czech, Ancient Greek, and Hungarian
from words that occurred at least five times in the
training data; words that occurred fewer times were
unrestricted in the morphological attributes they
could take on.
</bodyText>
<sectionHeader confidence="0.992708" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.988191333333333">
We compare the performance of the pipeline model
(§4) and the joint model (§3) on morphological dis-
ambiguation and unlabeled dependency parsing.
</bodyText>
<table confidence="0.999217545454545">
Model Tagger Joint Tagger Joint
Attr. all all non-null non-null
POS 95.5 95.7 95.5 95.7
Person 98.4 98.8 93.5 95.6
Number 91.2 92.3 87.0 88.4
Tense 98.4 98.8 92.7 96.1
Voice 98.5 98.7 93.2 95.8
Gender 86.6 87.9 75.6 78.0
Case 84.1 85.6 74.3 76.5
Degree 97.9 98.0 90.1 90.1
UAS 67.4 68.7 — —
</table>
<tableCaption confidence="0.9364875">
Table 4: Czech morphological disambiguation and pars-
ing. As with Latin, the model is least accurate with
noun/adjective categories of gender number, and case,
particularly when considering only words whose true
value is non-null for those attributes. Joint inference with
syntactic features improves accuracy across the board.
</tableCaption>
<table confidence="0.999939916666667">
Model Tagger Joint Tagger Joint
Attr. all all non-null non-null
POS 94.9 95.7 94.9 95.7
Person 98.7 99.0 92.2 94.6
Number 97.4 97.9 96.5 97.1
Tense 96.8 97.2 84.1 86.8
Mood 97.9 98.3 91.4 93.2
Voice 97.8 98.0 91.3 92.4
Gender 95.4 96.1 90.7 91.9
Case 95.9 96.3 92.0 92.6
Degree 99.8 99.9 33.3 55.6
UAS 68.0 70.5 — —
</table>
<tableCaption confidence="0.976255666666667">
Table 5: Ancient Greek morphological disambiguation
and parsing. Noun/adjective morphology is more accu-
rate, but verbal morphology is more problematic.
</tableCaption>
<table confidence="0.9999317">
Model Tagger Joint Tagger Joint
Attr. all all non-null non-null
POS 95.8 95.8 95.8 95.8
Person 98.5 98.6 94.9 94.1
Number 97.4 97.5 96.8 96.6
Tense 98.9 99.3 97.2 97.3
Mood 98.7 99.2 95.8 97.3
Case 96.7 97.0 94.5 94.9
Degree 97.9 98.1 87.5 88.6
UAS 78.2 78.8 — —
</table>
<tableCaption confidence="0.98816725">
Table 6: Hungarian morphological disambiguation and
parsing. The agglutinative morphological system makes
local cues more effective, but syntactic information helps
in almost all categories.
</tableCaption>
<page confidence="0.996488">
891
</page>
<subsectionHeader confidence="0.996429">
6.1 Morphological Disambiguation
</subsectionHeader>
<bodyText confidence="0.999990371428571">
As seen in Table 3, the joint model outperforms6
the baseline tagger in all attributes in Latin morpho-
logical disambiguation. Among words not covered
by the morphological database, accuracy in POS is
slightly better, but lower for case, gender and num-
ber.
The joint model made the most gains on adjec-
tives and participles. Both parts-of-speech are par-
ticularly ambiguous: according to MORPHEUS, 43%
of the adjectives can be interpreted as another POS,
most frequently nouns; while participles have an av-
erage of 5.5 morphological interpretations. Both
also often have identical forms for different genders,
numbers and cases. In these situations, syntactic
considerations help nudge the joint model to the cor-
rect interpretations.
Experiments on the other three languages bear out
similar results: the joint model improves morpho-
logical disambiguation. The performance of Czech
(Table 4) exhibits the closest analogue to Latin: gen-
der, number, and case are much less accurately pre-
dicted than are the other morphological attributes.
Like Latin, Czech lacks definite and indefinite arti-
cles to provide high-confidence cues for noun phrase
boundaries.
The Ancient Greek treebank comprises both ar-
chaic texts, before the development of a definite ar-
ticle, and later classic Greek, which has a definite
article; Hungarian has both a definite and an indefi-
nite article. In both languages (Tables 5 and 6), noun
and adjective gender, number, and case are more
accurately predicted than in Czech and Latin. The
verbal system of ancient Greek, in contrast, is more
complex than that of the other languages, so mood,
voice, and tense accuracy are lower.
</bodyText>
<subsectionHeader confidence="0.996668">
6.2 Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999941166666667">
In addition to morphological disambiguation, we
also measured the performance of the joint model
on dependency parsing of Latin and the other lan-
guages. The baseline pipeline parser (§4.2) yielded
61.00% head selection accuracy (i.e., unlabeled at-
tachment score, UAS), outperformed7 by the joint
</bodyText>
<footnote confidence="0.981437">
6The differences are statistically significant in all (p &lt; 0.01
by McNemar’s Test) but POS (p = 0.5).
7Significant at p &lt; e−11 by McNemar’s Test.
</footnote>
<bodyText confidence="0.999874191489362">
model at 61.88%. The joint model showed simi-
lar improvements in Ancient Greek, Hungarian, and
Czech.
Wrong decisions made by the baseline tagger of-
ten misled the pipeline parser. For adjectives, the ex-
ample shown in Table 1 and Figure 1 is a typical sce-
nario, where an accusative adjective was tagged as
nominative, and was then misanalyzed by the parser
as modifying a verb (as a subject) rather than mod-
ifying an accusative noun. For participles modify-
ing a noun, the wrong noun was often chosen based
on inaccurate morphological information. In these
cases, the joint model, entertaining all morpholog-
ical possibilities, was able to find the combination
of links and morphological analyses that are collec-
tively more likely.
The accuracy figures of our baselines are compa-
rable, but not identical, to their counterparts reported
in (Bamman and Crane, 2008). The differences may
partially be attributed to the different morphologi-
cal tagger used, and the different learning algorithm,
namely Margin Infused Relaxed Algorithm (MIRA)
in (McDonald et al., 2005) rather than maximum
likelihood. More importantly, the Latin Dependency
Treebank has grown from about 30K at the time of
the previous work to 53K at present, resulting in sig-
nificantly different training and testing material.
Gold Pipeline Parser When given perfect mor-
phological information, the Latin parser performs at
65.28% accuracy in head selection. Despite the or-
acle morphology, the head selection accuracy is still
below other languages. This is hardly surprising,
given the relatively small training set, and that the
“the most difficult languages are those that combine
a relatively free word order with a high degree of in-
flection”, as observed at the recent dependency pars-
ing shared task (Nivre et al., 2007); both of these are
characteristics of Latin.
A particularly troublesome structure is coordina-
tion; the most frequent link errors all involve either a
parent or a child as a conjunction. In a list of words,
all words and coordinators depend on the final coor-
dinator. Since the factors in our model consult only
one link at a time, they do not sufficiently capture
this kind of structures. Higher-order features, partic-
ularly those concerned with links with grandparents
and siblings, have been shown to benefit dependency
</bodyText>
<page confidence="0.993189">
892
</page>
<bodyText confidence="0.9365995">
parsing (Smith and Eisner, 2008) and may be able to
address this issue.
</bodyText>
<sectionHeader confidence="0.989424" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999967769230769">
We have proposed a discriminative model that
jointly infers morphological properties and syntactic
structures. In evaluations on various highly-inflected
languages, this joint model outperforms both a base-
line tagger in morphological disambiguation, and a
pipeline parser in head selection.
This model may be refined by incorporating richer
features and improved decoding. In particular, we
would like to experiment with higher-order features
(§6), and with maximum a posteriori decoding, via
max-product BP or (relaxed) integer linear program-
ming. Further evaluation on other morphological
systems would also be desirable.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999959368421053">
We thank David Bamman and Gregory Crane for
their feedback and support. Part of this research
was performed by the first author while visiting
Perseus Digital Library at Tufts University, un-
der the grants A Reading Environment for Ara-
bic and Islamic Culture, Department of Education
(P017A060068-08) and The Dynamic Lexicon: Cy-
berinfrastructure and the Automatic Analysis of His-
torical Languages, National Endowment for the Hu-
manities (PR-50013-08). The latter two authors
were supported by Army prime contract #W911NF-
07-1-0216 and University of Pennsylvania subaward
#103-548106; by SRI International subcontract #27-
001338 and ARFL prime contract #FA8750-09-C-
0181; and by the Center for Intelligent Information
Retrieval. Any opinions, findings, and conclusions
or recommendations expressed in this material are
the authors’ and do not necessarily reflect those of
the sponsors.
</bodyText>
<sectionHeader confidence="0.999452" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999728220338983">
David Bamman and Gregory Crane. 2006. The Design
and Use of a Latin Dependency Treebank. Proc. Work-
shop on Treebanks and Linguistic Theories (TLT).
Prague, Czech Republic.
David Bamman and Gregory Crane. 2008. Building a
Dynamic Lexicon from a Digital Library. Proc. 8th
ACM/IEEE-CS Joint Conference on Digital Libraries
(JCDL 2008). Pittsburgh, PA.
David Bamman, Francesco Mambrini, and Gregory
Crane. 2009. An Ownership Model of Anno-
tation: The Ancient Greek Dependency Treebank.
Proc. Workshop on Treebanks and Linguistic Theories
(TLT).
A. B¨ohmov´a, J. Hajiˇc, E. Hajiˇcov´a, and B. Hladk´a.
2003. The PDT: a 3-level Annotation Scenario. In
Treebanks: Building and Using Parsed Corpora, A.
Abeill´e (ed). Kluwer.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency Parsing.
Proc. CoNLL. New York, NY.
Shay B. Cohen and Noah A. Smith. 2007. Joint Morpho-
logical and Syntactic Disambiguation. Proc. EMNLP-
CoNLL. Prague, Czech Republic.
Gregory Crane. 1991. Generating and Parsing Classical
Greek. Literary and Linguistic Computing 6(4):243–
245.
Yoav Goldberg and Reut Tsarfaty. 2008. A Single Gen-
erative Model for Joint Morphological Segmentation
and Syntactic Parsing. Proc. ACL. Columbus, OH.
Joshua Goodman. 1996. Parsing Algorithms and Met-
rics. Proc. ACL.
J. Hajiˇc, P. Krbec, P. Kvˇetoˇn, K. Oliva, and V. Petkeviˇc.
2001. Serial Combination of Rules and Statistics: A
Case Study in Czech Tagging. Proc. ACL.
D. Z. Hakkani-T¨ur, K. Oflazer, and G. T¨ur. 2000. Statis-
tical Morphological Disambiguation for Agglutinative
Languages. Proc. COLING.
Vincent Katz. 2004. The Complete Elegies of Sextus
Propertius. Princeton University Press, Princeton, NJ.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jana Hajiˇc. 2005. Non-projective Dependency
Parsing using Spanning Tree Algorithms. Proc.
HLT/EMNLP.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. Proc. ACL.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. Proc. CoNLL Shared Task Session
of EMNLP-CoNLL. Prague, Czech Republic.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging using Decision Trees. Proc. International
Conference on New Methods in Language Processing.
Manchester, UK.
Noah A. Smith, David A. Smith and Roy W. Tromble.
2005. Context-Based Morphological Disambiguation
with Random Fields. Proc. HLT/EMNLP. Vancouver,
Canada.
</reference>
<page confidence="0.99097">
893
</page>
<reference confidence="0.999764153846154">
David Smith and Jason Eisner. 2008. Dependency Pars-
ing by Belief Propagation. Proc. EMNLP. Honolulu,
Hawaii.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
Proc. HLT-NAACL. Edmonton, Canada.
Reut Tsarfaty. 2006. Integrated Morphological and
Syntactic Disambiguation for Modern Hebrew. Proc.
COLING-ACL Student Research Workshop.
Veronika Vincze, D´ora Szauter, Attila Alm´asi, Gy¨orgy
M´ora, Zolt´an Alexin, and J´anos Csirik. 2010. Hun-
garian Dependency Treebank. Proc. LREC.
</reference>
<page confidence="0.998884">
894
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.585606">
<title confidence="0.9866205">A Discriminative Model for Joint Morphological Disambiguation Dependency Parsing</title>
<author confidence="0.956655">John</author>
<affiliation confidence="0.783533666666667">Department of Translation and City University of Hong</affiliation>
<email confidence="0.828265">jsylee@cityu.edu.hk</email>
<author confidence="0.968981">Jason Naradowsky</author>
<author confidence="0.968981">A David</author>
<affiliation confidence="0.999878">Department of Computer University of Massachusetts,</affiliation>
<abstract confidence="0.99950525">Most previous studies of morphological disambiguation and dependency parsing have been pursued independently. Morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the “pipeline” approach, assuming that morphological information has been separately obtained. However, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. In this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures. In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Gregory Crane</author>
</authors>
<title>The Design and Use of a Latin Dependency Treebank.</title>
<date>2006</date>
<booktitle>Proc. Workshop on Treebanks and Linguistic Theories (TLT).</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="18296" citStr="Bamman and Crane, 2006" startWordPosition="2962" endWordPosition="2965">the morphological attributes predicted by the tagger, the TAG variables are now observed. This leads to a change in the structure of the link factors — all features from the POSLINK factors now belong to the WORD-LINK factors, since the POS of all words are observed. In short, the features of the parser are a replication of (McDonald et al., 2005), but also extended beyond POS to the other morphological attributes, with the features in the MORPH-LINK factors incorporated into WORD-LINK for similar reasons. 5 Experimental Set-up 5.1 Data Our evaluation focused on the Latin Dependency Treebank (Bamman and Crane, 2006), created at the Perseus Digital Library by tailoring the Prague Dependency Treebank guidelines for the Latin language. It consists of excerpts from works by eight Latin authors. We randomly divided the 53K-word treebank into 10 folds of roughly equal sizes, with an average of 5314 words (347 sentences) per fold. We used one fold as the development set and performed cross-validation on the other nine. To measure how well our model generalizes to other highly-inflected, relatively free-word-order languages, we considered Ancient Greek, Hungarian, and Czech. Their respective datasets consist of </context>
</contexts>
<marker>Bamman, Crane, 2006</marker>
<rawString>David Bamman and Gregory Crane. 2006. The Design and Use of a Latin Dependency Treebank. Proc. Workshop on Treebanks and Linguistic Theories (TLT). Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Gregory Crane</author>
</authors>
<title>Building a Dynamic Lexicon from a Digital Library.</title>
<date>2008</date>
<booktitle>Proc. 8th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="8261" citStr="Bamman and Crane, 2008" startWordPosition="1278" endWordPosition="1281">y inflected, with each word having up to nine morphological attributes, listed in Table 2. In addition to its absolute numbers of cases, moods, and tenses, Latin morphology is fusional. For instance, the suffix −is in omnis cannot be segmented into morphemes that separately indicate gender, number, and case. According to the Latin morphological database encoded in MORPHEUS (Crane, 1991), 30% of Latin nouns can be parsed as another part-of-speech, and on average each has 3.8 possible morphological interpretations. We know of only one previous attempt in datadriven dependency parsing for Latin (Bamman and Crane, 2008), with the goal of constructing a dynamic lexicon for a digital library. Parsing is performed using the usual pipeline approach, first with the TreeTagger analyzer (Schmid, 1994) and then with a state-of-the-art dependency parser (McDonald et al., 2005). Head selection accuracy was 61.49%, and rose to 64.99% with oracle morphological tags. Of the nine morphological attributes, gender and especially case had the lowest accuracy. This observation echoes the findings for Czech (Smith et al., 2005), where case was also the most difficult to disambiguate. 3 Joint Model This section describes a mode</context>
<context position="27415" citStr="Bamman and Crane, 2008" startWordPosition="4444" endWordPosition="4447"> is a typical scenario, where an accusative adjective was tagged as nominative, and was then misanalyzed by the parser as modifying a verb (as a subject) rather than modifying an accusative noun. For participles modifying a noun, the wrong noun was often chosen based on inaccurate morphological information. In these cases, the joint model, entertaining all morphological possibilities, was able to find the combination of links and morphological analyses that are collectively more likely. The accuracy figures of our baselines are comparable, but not identical, to their counterparts reported in (Bamman and Crane, 2008). The differences may partially be attributed to the different morphological tagger used, and the different learning algorithm, namely Margin Infused Relaxed Algorithm (MIRA) in (McDonald et al., 2005) rather than maximum likelihood. More importantly, the Latin Dependency Treebank has grown from about 30K at the time of the previous work to 53K at present, resulting in significantly different training and testing material. Gold Pipeline Parser When given perfect morphological information, the Latin parser performs at 65.28% accuracy in head selection. Despite the oracle morphology, the head se</context>
</contexts>
<marker>Bamman, Crane, 2008</marker>
<rawString>David Bamman and Gregory Crane. 2008. Building a Dynamic Lexicon from a Digital Library. Proc. 8th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL 2008). Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Francesco Mambrini</author>
<author>Gregory Crane</author>
</authors>
<title>An Ownership Model of Annotation: The Ancient Greek Dependency Treebank.</title>
<date>2009</date>
<booktitle>Proc. Workshop on Treebanks and Linguistic Theories (TLT).</booktitle>
<contexts>
<context position="18975" citStr="Bamman et al., 2009" startWordPosition="3066" endWordPosition="3069">rague Dependency Treebank guidelines for the Latin language. It consists of excerpts from works by eight Latin authors. We randomly divided the 53K-word treebank into 10 folds of roughly equal sizes, with an average of 5314 words (347 sentences) per fold. We used one fold as the development set and performed cross-validation on the other nine. To measure how well our model generalizes to other highly-inflected, relatively free-word-order languages, we considered Ancient Greek, Hungarian, and Czech. Their respective datasets consist of 8000 sentences from the Ancient Greek Dependency Treebank (Bamman et al., 2009), 5800 from the Hungarian Szeged Dependency Treebank (Vincze et al., 2010), and a subset of 3100 from the Prague Dependency Treebank (B¨ohmov´a et al., 2003). 5.2 Training We define each factor in (1) as a log-linear function: Fk(A) = exp � Bhfh(A, W, k) (2) h Given an assignment A and words W, fh is an indicator function describing the presence or absence of the feature, and Bh is the corresponding set of weights learned using stochastic gradient ascent, with the gradients inferred by loopy belief propagation (Smith and Eisner, 2008). The variance of the Gaussian prior is set to 1. The other </context>
</contexts>
<marker>Bamman, Mambrini, Crane, 2009</marker>
<rawString>David Bamman, Francesco Mambrini, and Gregory Crane. 2009. An Ownership Model of Annotation: The Ancient Greek Dependency Treebank. Proc. Workshop on Treebanks and Linguistic Theories (TLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B¨ohmov´a</author>
<author>J Hajiˇc</author>
<author>E Hajiˇcov´a</author>
<author>B Hladk´a</author>
</authors>
<title>The PDT: a 3-level Annotation Scenario. In Treebanks: Building and Using Parsed Corpora, A. Abeill´e (ed).</title>
<date>2003</date>
<publisher>Kluwer.</publisher>
<marker>B¨ohmov´a, Hajiˇc, Hajiˇcov´a, Hladk´a, 2003</marker>
<rawString>A. B¨ohmov´a, J. Hajiˇc, E. Hajiˇcov´a, and B. Hladk´a. 2003. The PDT: a 3-level Annotation Scenario. In Treebanks: Building and Using Parsed Corpora, A. Abeill´e (ed). Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<date>2006</date>
<booktitle>CoNLLX Shared Task on Multilingual Dependency Parsing. Proc. CoNLL.</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="1566" citStr="Buchholz and Marsi, 2006" startWordPosition="205" endWordPosition="208">ations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection. 1 Introduction To date, studies of morphological analysis and dependency parsing have been pursued more or less independently. Morphological taggers disambiguate morphological attributes such as partof-speech (POS) or case, without taking syntax into account (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001); dependency parsers commonly assume the “pipeline” approach, relying on morphological information as part of the input (Buchholz and Marsi, 2006; Nivre et al., 2007). This approach serves many languages well, especially those with less morphological ambiguity. In English, for example, accuracy of POS tagging has risen above 97% (Toutanova et al., 2003), and that of dependency parsing has reached the low nineties (Nivre et al., 2007). For these languages, there may be little to be gained to justify the computational cost of incorporating syntactic inference during the morphological tagging task; conversely, it is doubtful that errorful morphological information is a main cause of errors in English dependency parsing. However, the pipel</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLLX Shared Task on Multilingual Dependency Parsing. Proc. CoNLL. New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<date>2007</date>
<booktitle>Joint Morphological and Syntactic Disambiguation. Proc. EMNLPCoNLL.</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6481" citStr="Cohen and Smith, 2007" startWordPosition="996" endWordPosition="999">ould praecurrere to surpass amantis lovers omnis Figure 1: Dependency tree for the sentence “Una dies omnis potuit praecurrere amantis”. The word omnis is an adjective modifying the noun amantis. This information is key to the morphological disambiguation of both words, as shown in Table 1. such as Turkish (Hakkani-T¨ur et al., 2000) and Czech (Hajiˇc et al., 2001), did not consider syntactic relationships between words. In the literature on data-driven parsing, two recent studies attempted joint inference on morphology and syntax, and both considered phrase-structure trees for Modern Hebrew (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). The primary focus of morphological processing in Modern Hebrew is splitting orthographic words into morphemes: clitics such as prepositions, pronouns, and the definite article must be separated from the core word. Each of the resulting morphemes is then tagged with an atomic “part-of-speech” to indicate word class and some morphological features. Similarly, the English POS tags in the Penn Treebank combine word class information with morphologidies day una 886 cal attributes such as “plural” or “past tense”. Cohen and Smith (2007) separately train a discriminati</context>
<context position="20272" citStr="Cohen and Smith (2007)" startWordPosition="3291" endWordPosition="3294">erations and the number of training rounds, were tuned on the development set. 5.3 Decoding The output of the joint model is the assignment to the TAG and LINK variables. Loopy belief propagation (BP) was used to calculate the posterior probabilities of these variables. For TAG, we emit the tag with the highest posterior probability as computed by sum-product BP. We produced head attachments by first calculating the posteriors of the LINK variables with BP and then passing them to an edgefactored tree decoder. This is equivalent to minimum Bayes risk decoding (Goodman, 1996), which is used by Cohen and Smith (2007) and Smith and Eisner (2008). This MBR decoding procedure enforces the hard constraint that the output be a tree but sums over possible morphological assignments.5 5.4 Reducing Model Complexity In principle, the joint model should consider every possible combination of morphological attributes for every word. In practice, to reduce the complexity of the model, we used a pre-existing morphological database, MORPHEUS (Crane, 1991), to constrain the range of possible values of the attributes listed in Table 2; more precisely, we add a hard constraint, requiring that assignments to the TAG variabl</context>
</contexts>
<marker>Cohen, Smith, 2007</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2007. Joint Morphological and Syntactic Disambiguation. Proc. EMNLPCoNLL. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Crane</author>
</authors>
<title>Generating and Parsing Classical Greek.</title>
<date>1991</date>
<journal>Literary and Linguistic Computing</journal>
<volume>6</volume>
<issue>4</issue>
<pages>245</pages>
<contexts>
<context position="8027" citStr="Crane, 1991" startWordPosition="1242" endWordPosition="1243">joint morphological and syntactic inference on dependency trees. 2.2 Latin Unlike Modern Hebrew, Latin does not require extensive morpheme segmentation2. However, it does have a relatively free word order, and is also highly inflected, with each word having up to nine morphological attributes, listed in Table 2. In addition to its absolute numbers of cases, moods, and tenses, Latin morphology is fusional. For instance, the suffix −is in omnis cannot be segmented into morphemes that separately indicate gender, number, and case. According to the Latin morphological database encoded in MORPHEUS (Crane, 1991), 30% of Latin nouns can be parsed as another part-of-speech, and on average each has 3.8 possible morphological interpretations. We know of only one previous attempt in datadriven dependency parsing for Latin (Bamman and Crane, 2008), with the goal of constructing a dynamic lexicon for a digital library. Parsing is performed using the usual pipeline approach, first with the TreeTagger analyzer (Schmid, 1994) and then with a state-of-the-art dependency parser (McDonald et al., 2005). Head selection accuracy was 61.49%, and rose to 64.99% with oracle morphological tags. Of the nine morphologica</context>
<context position="20704" citStr="Crane, 1991" startWordPosition="3357" endWordPosition="3358"> variables with BP and then passing them to an edgefactored tree decoder. This is equivalent to minimum Bayes risk decoding (Goodman, 1996), which is used by Cohen and Smith (2007) and Smith and Eisner (2008). This MBR decoding procedure enforces the hard constraint that the output be a tree but sums over possible morphological assignments.5 5.4 Reducing Model Complexity In principle, the joint model should consider every possible combination of morphological attributes for every word. In practice, to reduce the complexity of the model, we used a pre-existing morphological database, MORPHEUS (Crane, 1991), to constrain the range of possible values of the attributes listed in Table 2; more precisely, we add a hard constraint, requiring that assignments to the TAG variables be compatible with MORPHEUS. This constraint significantly reduces the value of m in the big-O notation 5This approach to nuisance variables has also been used effectively for parsing with tree-substitution grammars, where several derived trees may correspond to each derivation tree, and parsing with PCFGs with latent annotations. 890 Model Tagger Joint Tagger Joint Attr. all all non-null non-null POS 94.4 94.5 94.4 94.5 Pers</context>
</contexts>
<marker>Crane, 1991</marker>
<rawString>Gregory Crane. 1991. Generating and Parsing Classical Greek. Literary and Linguistic Computing 6(4):243– 245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
</authors>
<title>A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing.</title>
<date>2008</date>
<booktitle>Proc. ACL.</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="6511" citStr="Goldberg and Tsarfaty, 2008" startWordPosition="1000" endWordPosition="1003">pass amantis lovers omnis Figure 1: Dependency tree for the sentence “Una dies omnis potuit praecurrere amantis”. The word omnis is an adjective modifying the noun amantis. This information is key to the morphological disambiguation of both words, as shown in Table 1. such as Turkish (Hakkani-T¨ur et al., 2000) and Czech (Hajiˇc et al., 2001), did not consider syntactic relationships between words. In the literature on data-driven parsing, two recent studies attempted joint inference on morphology and syntax, and both considered phrase-structure trees for Modern Hebrew (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). The primary focus of morphological processing in Modern Hebrew is splitting orthographic words into morphemes: clitics such as prepositions, pronouns, and the definite article must be separated from the core word. Each of the resulting morphemes is then tagged with an atomic “part-of-speech” to indicate word class and some morphological features. Similarly, the English POS tags in the Penn Treebank combine word class information with morphologidies day una 886 cal attributes such as “plural” or “past tense”. Cohen and Smith (2007) separately train a discriminative conditional random field (C</context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Yoav Goldberg and Reut Tsarfaty. 2008. A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing. Proc. ACL. Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing Algorithms and Metrics.</title>
<date>1996</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="20231" citStr="Goodman, 1996" startWordPosition="3285" endWordPosition="3286">e number of belief propagation iterations and the number of training rounds, were tuned on the development set. 5.3 Decoding The output of the joint model is the assignment to the TAG and LINK variables. Loopy belief propagation (BP) was used to calculate the posterior probabilities of these variables. For TAG, we emit the tag with the highest posterior probability as computed by sum-product BP. We produced head attachments by first calculating the posteriors of the LINK variables with BP and then passing them to an edgefactored tree decoder. This is equivalent to minimum Bayes risk decoding (Goodman, 1996), which is used by Cohen and Smith (2007) and Smith and Eisner (2008). This MBR decoding procedure enforces the hard constraint that the output be a tree but sums over possible morphological assignments.5 5.4 Reducing Model Complexity In principle, the joint model should consider every possible combination of morphological attributes for every word. In practice, to reduce the complexity of the model, we used a pre-existing morphological database, MORPHEUS (Crane, 1991), to constrain the range of possible values of the attributes listed in Table 2; more precisely, we add a hard constraint, requ</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Parsing Algorithms and Metrics. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>P Krbec</author>
<author>P Kvˇetoˇn</author>
<author>K Oliva</author>
<author>V Petkeviˇc</author>
</authors>
<date>2001</date>
<booktitle>Serial Combination of Rules and Statistics: A Case Study in Czech Tagging. Proc. ACL.</booktitle>
<marker>Hajiˇc, Krbec, Kvˇetoˇn, Oliva, Petkeviˇc, 2001</marker>
<rawString>J. Hajiˇc, P. Krbec, P. Kvˇetoˇn, K. Oliva, and V. Petkeviˇc. 2001. Serial Combination of Rules and Statistics: A Case Study in Czech Tagging. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Z Hakkani-T¨ur</author>
<author>K Oflazer</author>
<author>G T¨ur</author>
</authors>
<title>Statistical Morphological Disambiguation for Agglutinative Languages.</title>
<date>2000</date>
<booktitle>Proc. COLING.</booktitle>
<marker>Hakkani-T¨ur, Oflazer, T¨ur, 2000</marker>
<rawString>D. Z. Hakkani-T¨ur, K. Oflazer, and G. T¨ur. 2000. Statistical Morphological Disambiguation for Agglutinative Languages. Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Katz</author>
</authors>
<title>The Complete Elegies of Sextus Propertius.</title>
<date>2004</date>
<publisher>Princeton University Press,</publisher>
<location>Princeton, NJ.</location>
<contexts>
<context position="3430" citStr="Katz (2004)" startWordPosition="509" endWordPosition="510">his decision. Given its nominative case, the pipeline parser assigned the verb potuit to be its head; the two words form the typical subjectverb relation, agreeing in number. Unfortunately, as shown in Figure 1, the word omnis in fact modifies the noun amantis, at the end of the sentence. As a result, despite the distance between them, they must agree in number, gender and case, i.e., both must be plural masculine (or feminine) accusative. The pipeline parser, acting on the input that omnis is nominative, naturally did not see 1Taken from poem 1.13 by Sextus Propertius, English translation by Katz (2004). 885 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 885–894, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Latin Una dies omnis potuit praecurrere amantis English one day all could to surpass lovers Number sg pl sg pl sg sg pl sg - sg pl Gender f n m/f m/f m/f m/f/n m/f - - m/f/n m/f Case nom/ab nom/acc nom nom/acc nom gen acc - - gen acc Table 1: The Latin sentence “Una dies omnis potuit praecurrere amantis”, meaning ‘One day was able to make up for all the lovers’, shown with glosses and possible morpholo</context>
</contexts>
<marker>Katz, 2004</marker>
<rawString>Vincent Katz. 2004. The Complete Elegies of Sextus Propertius. Princeton University Press, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jana Hajiˇc</author>
</authors>
<title>Non-projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>Proc. HLT/EMNLP.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jana Hajiˇc. 2005. Non-projective Dependency Parsing using Spanning Tree Algorithms. Proc. HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Large-Margin Training of Dependency Parsers.</title>
<date>2005</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="8514" citStr="McDonald et al., 2005" startWordPosition="1318" endWordPosition="1322">emes that separately indicate gender, number, and case. According to the Latin morphological database encoded in MORPHEUS (Crane, 1991), 30% of Latin nouns can be parsed as another part-of-speech, and on average each has 3.8 possible morphological interpretations. We know of only one previous attempt in datadriven dependency parsing for Latin (Bamman and Crane, 2008), with the goal of constructing a dynamic lexicon for a digital library. Parsing is performed using the usual pipeline approach, first with the TreeTagger analyzer (Schmid, 1994) and then with a state-of-the-art dependency parser (McDonald et al., 2005). Head selection accuracy was 61.49%, and rose to 64.99% with oracle morphological tags. Of the nine morphological attributes, gender and especially case had the lowest accuracy. This observation echoes the findings for Czech (Smith et al., 2005), where case was also the most difficult to disambiguate. 3 Joint Model This section describes a model that jointly infers morphological and syntactic properties of a sentence. It will be presented as a graphical model, 2Except for enclitics such as -que, -ve, and -ne, but their segmentation is rather straightforward compared to Modern Hebrew or other </context>
<context position="14784" citStr="McDonald et al. (2005)" startWordPosition="2357" endWordPosition="2360">, which serve this purpose. 3.2 Link Factors The link factors consult all pairs of words, possibly separated by a long distance, that may have a dependency link. These factors model the likelihood of such a link based on the word identities and their morphological attributes: • WORD-LINK: There are O(n2) such unary factors, each connected to a LINK variable, as shown in Figure 2. The factor fires when Li,j is true. Features include various combinations of the word identities of the parent wi and child wj, and 5-letter prefixes of these words, replicating the so-called “basic features” used by McDonald et al. (2005). • POS-LINK: There are O(n2m2) such ternary factors, each connected to the variables Li,j, Ti,pos,vi and Tj,pos,vj. It fires when all three are true or, in other words, when the parent word wi has POS vi, and the child wj has POS vj. Features replicate all the so-called “basic features” used by McDonald et al. (2005) that involve POS. These factors are not shown in Figure 2, but would have exactly the same structure as the CASE-LINK factors. Beyond these basic features, McDonald et al. (2005) also utilize POS trigrams and POS 4- grams. Both include the POS of two linked words, wi and wj. The </context>
<context position="18022" citStr="McDonald et al., 2005" startWordPosition="2922" endWordPosition="2925">BIGRAM, and TAGCONSISTENCY, all used in the joint model (§3). 4.2 Baseline Dependency Parser The parser has no local factors, but has the same variables as the joint model and the same features from all three families of link factors (§3). However, since it takes as input the morphological attributes predicted by the tagger, the TAG variables are now observed. This leads to a change in the structure of the link factors — all features from the POSLINK factors now belong to the WORD-LINK factors, since the POS of all words are observed. In short, the features of the parser are a replication of (McDonald et al., 2005), but also extended beyond POS to the other morphological attributes, with the features in the MORPH-LINK factors incorporated into WORD-LINK for similar reasons. 5 Experimental Set-up 5.1 Data Our evaluation focused on the Latin Dependency Treebank (Bamman and Crane, 2006), created at the Perseus Digital Library by tailoring the Prague Dependency Treebank guidelines for the Latin language. It consists of excerpts from works by eight Latin authors. We randomly divided the 53K-word treebank into 10 folds of roughly equal sizes, with an average of 5314 words (347 sentences) per fold. We used one</context>
<context position="27616" citStr="McDonald et al., 2005" startWordPosition="4473" endWordPosition="4476">rticiples modifying a noun, the wrong noun was often chosen based on inaccurate morphological information. In these cases, the joint model, entertaining all morphological possibilities, was able to find the combination of links and morphological analyses that are collectively more likely. The accuracy figures of our baselines are comparable, but not identical, to their counterparts reported in (Bamman and Crane, 2008). The differences may partially be attributed to the different morphological tagger used, and the different learning algorithm, namely Margin Infused Relaxed Algorithm (MIRA) in (McDonald et al., 2005) rather than maximum likelihood. More importantly, the Latin Dependency Treebank has grown from about 30K at the time of the previous work to 53K at present, resulting in significantly different training and testing material. Gold Pipeline Parser When given perfect morphological information, the Latin parser performs at 65.28% accuracy in head selection. Despite the oracle morphology, the head selection accuracy is still below other languages. This is hardly surprising, given the relatively small training set, and that the “the most difficult languages are those that combine a relatively free </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online Large-Margin Training of Dependency Parsers. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>Shared Task on Dependency Parsing. Proc. CoNLL Shared Task Session of EMNLP-CoNLL.</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 Shared Task on Dependency Parsing. Proc. CoNLL Shared Task Session of EMNLP-CoNLL. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging using Decision Trees.</title>
<date>1994</date>
<booktitle>Proc. International Conference on New Methods in Language Processing.</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="8439" citStr="Schmid, 1994" startWordPosition="1309" endWordPosition="1310">r instance, the suffix −is in omnis cannot be segmented into morphemes that separately indicate gender, number, and case. According to the Latin morphological database encoded in MORPHEUS (Crane, 1991), 30% of Latin nouns can be parsed as another part-of-speech, and on average each has 3.8 possible morphological interpretations. We know of only one previous attempt in datadriven dependency parsing for Latin (Bamman and Crane, 2008), with the goal of constructing a dynamic lexicon for a digital library. Parsing is performed using the usual pipeline approach, first with the TreeTagger analyzer (Schmid, 1994) and then with a state-of-the-art dependency parser (McDonald et al., 2005). Head selection accuracy was 61.49%, and rose to 64.99% with oracle morphological tags. Of the nine morphological attributes, gender and especially case had the lowest accuracy. This observation echoes the findings for Czech (Smith et al., 2005), where case was also the most difficult to disambiguate. 3 Joint Model This section describes a model that jointly infers morphological and syntactic properties of a sentence. It will be presented as a graphical model, 2Except for enclitics such as -que, -ve, and -ne, but their</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging using Decision Trees. Proc. International Conference on New Methods in Language Processing. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>David A Smith</author>
<author>Roy W Tromble</author>
</authors>
<title>Context-Based Morphological Disambiguation with Random Fields.</title>
<date>2005</date>
<booktitle>Proc. HLT/EMNLP.</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="8760" citStr="Smith et al., 2005" startWordPosition="1358" endWordPosition="1361">nterpretations. We know of only one previous attempt in datadriven dependency parsing for Latin (Bamman and Crane, 2008), with the goal of constructing a dynamic lexicon for a digital library. Parsing is performed using the usual pipeline approach, first with the TreeTagger analyzer (Schmid, 1994) and then with a state-of-the-art dependency parser (McDonald et al., 2005). Head selection accuracy was 61.49%, and rose to 64.99% with oracle morphological tags. Of the nine morphological attributes, gender and especially case had the lowest accuracy. This observation echoes the findings for Czech (Smith et al., 2005), where case was also the most difficult to disambiguate. 3 Joint Model This section describes a model that jointly infers morphological and syntactic properties of a sentence. It will be presented as a graphical model, 2Except for enclitics such as -que, -ve, and -ne, but their segmentation is rather straightforward compared to Modern Hebrew or other Semitic languages. Attribute Values Part-of- noun, verb, participle, adjective, speech adverb, conjunction, preposition, (POS) pronoun, numeral, interjection, exclamation, punctuation Person first, second, third Number singular, plural Tense pres</context>
</contexts>
<marker>Smith, Smith, Tromble, 2005</marker>
<rawString>Noah A. Smith, David A. Smith and Roy W. Tromble. 2005. Context-Based Morphological Disambiguation with Random Fields. Proc. HLT/EMNLP. Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Dependency Parsing by Belief Propagation.</title>
<date>2008</date>
<booktitle>Proc. EMNLP.</booktitle>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="19515" citStr="Smith and Eisner, 2008" startWordPosition="3160" endWordPosition="3163"> of 8000 sentences from the Ancient Greek Dependency Treebank (Bamman et al., 2009), 5800 from the Hungarian Szeged Dependency Treebank (Vincze et al., 2010), and a subset of 3100 from the Prague Dependency Treebank (B¨ohmov´a et al., 2003). 5.2 Training We define each factor in (1) as a log-linear function: Fk(A) = exp � Bhfh(A, W, k) (2) h Given an assignment A and words W, fh is an indicator function describing the presence or absence of the feature, and Bh is the corresponding set of weights learned using stochastic gradient ascent, with the gradients inferred by loopy belief propagation (Smith and Eisner, 2008). The variance of the Gaussian prior is set to 1. The other two parameters in the training process, the number of belief propagation iterations and the number of training rounds, were tuned on the development set. 5.3 Decoding The output of the joint model is the assignment to the TAG and LINK variables. Loopy belief propagation (BP) was used to calculate the posterior probabilities of these variables. For TAG, we emit the tag with the highest posterior probability as computed by sum-product BP. We produced head attachments by first calculating the posteriors of the LINK variables with BP and </context>
<context position="28897" citStr="Smith and Eisner, 2008" startWordPosition="4679" endWordPosition="4682">at the recent dependency parsing shared task (Nivre et al., 2007); both of these are characteristics of Latin. A particularly troublesome structure is coordination; the most frequent link errors all involve either a parent or a child as a conjunction. In a list of words, all words and coordinators depend on the final coordinator. Since the factors in our model consult only one link at a time, they do not sufficiently capture this kind of structures. Higher-order features, particularly those concerned with links with grandparents and siblings, have been shown to benefit dependency 892 parsing (Smith and Eisner, 2008) and may be able to address this issue. 7 Conclusions and Future Work We have proposed a discriminative model that jointly infers morphological properties and syntactic structures. In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection. This model may be refined by incorporating richer features and improved decoding. In particular, we would like to experiment with higher-order features (§6), and with maximum a posteriori decoding, via max-product BP or (relaxed) integer l</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>David Smith and Jason Eisner. 2008. Dependency Parsing by Belief Propagation. Proc. EMNLP. Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>Proc. HLT-NAACL.</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1776" citStr="Toutanova et al., 2003" startWordPosition="238" endWordPosition="241">hological analysis and dependency parsing have been pursued more or less independently. Morphological taggers disambiguate morphological attributes such as partof-speech (POS) or case, without taking syntax into account (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001); dependency parsers commonly assume the “pipeline” approach, relying on morphological information as part of the input (Buchholz and Marsi, 2006; Nivre et al., 2007). This approach serves many languages well, especially those with less morphological ambiguity. In English, for example, accuracy of POS tagging has risen above 97% (Toutanova et al., 2003), and that of dependency parsing has reached the low nineties (Nivre et al., 2007). For these languages, there may be little to be gained to justify the computational cost of incorporating syntactic inference during the morphological tagging task; conversely, it is doubtful that errorful morphological information is a main cause of errors in English dependency parsing. However, the pipeline approach seems more problematic for morphologically-rich languages with substantial interactions between morphology and syntax (Tsarfaty, 2006). Consider the Latin sentence, Una dies omnis potuit praecurrer</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network. Proc. HLT-NAACL. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
</authors>
<title>Integrated Morphological and Syntactic Disambiguation for Modern Hebrew.</title>
<date>2006</date>
<booktitle>Proc. COLING-ACL Student Research Workshop.</booktitle>
<contexts>
<context position="2313" citStr="Tsarfaty, 2006" startWordPosition="319" endWordPosition="320"> example, accuracy of POS tagging has risen above 97% (Toutanova et al., 2003), and that of dependency parsing has reached the low nineties (Nivre et al., 2007). For these languages, there may be little to be gained to justify the computational cost of incorporating syntactic inference during the morphological tagging task; conversely, it is doubtful that errorful morphological information is a main cause of errors in English dependency parsing. However, the pipeline approach seems more problematic for morphologically-rich languages with substantial interactions between morphology and syntax (Tsarfaty, 2006). Consider the Latin sentence, Una dies omnis potuit praecurrere amantis, ‘One day was able to make up for all the lovers’1. As shown in Table 1, the adjective omnis (‘all’) is ambiguous in number, gender, and case; there are seven valid analyses. From the perspective of a finitestate morphological tagger, the most attractive analysis is arguably the singular nominative, since omnis is immediately followed by the singular verb potuit (‘could’). Indeed, the baseline tagger used in this study did make this decision. Given its nominative case, the pipeline parser assigned the verb potuit to be it</context>
</contexts>
<marker>Tsarfaty, 2006</marker>
<rawString>Reut Tsarfaty. 2006. Integrated Morphological and Syntactic Disambiguation for Modern Hebrew. Proc. COLING-ACL Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
</authors>
<title>D´ora Szauter, Attila Alm´asi, Gy¨orgy M´ora, Zolt´an Alexin, and J´anos Csirik.</title>
<date>2010</date>
<booktitle>Proc. LREC.</booktitle>
<marker>Vincze, 2010</marker>
<rawString>Veronika Vincze, D´ora Szauter, Attila Alm´asi, Gy¨orgy M´ora, Zolt´an Alexin, and J´anos Csirik. 2010. Hungarian Dependency Treebank. Proc. LREC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>