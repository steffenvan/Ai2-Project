<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.971275">
Stochastic Attribute-Value Grammars
</title>
<author confidence="0.899352">
Steven P. Abney*
</author>
<sectionHeader confidence="0.815865" genericHeader="abstract">
AT&amp;T Laboratories
</sectionHeader>
<bodyText confidence="0.993570916666667">
Probabilistic analogues of regular and context-free grammars are well known in computational
linguistics, and currently the subject of intensive research. To date, however, no satisfactory
probabilistic analogue of attribute-value grammars has been proposed: previous attempts have
failed to define an adequate parameter-estimation algorithm.
In the present paper, I define stochastic attribute-value grammars and give an algorithm
for computing the maximum-likelihood estimate of their parameters. The estimation algorithm
is adapted from Della Pietra, Della Pietra, and Lafferty (1995). To estimate model parameters, it
is necessary to compute the expectations of certain functions under random fields. In the appli-
cation discussed by Della Pietra, Della Pietra, and Lafferty (representing English orthographic
constraints), Gibbs sampling can be used to estimate the needed expectations. The fact that
attribute-value grammars generate constrained languages makes Gibbs sampling inapplicable,
but I show that sampling can be done using the more general Metropolis-Hastings algorithm.
</bodyText>
<sectionHeader confidence="0.990249" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999329166666667">
Stochastic versions of regular grammars and context-free grammars have received a
great deal of attention in computational linguistics for the last several years, and ba-
sic techniques of stochastic parsing and parameter estimation have been known for
decades. However, regular and context-free grammars are widely deemed linguisti-
cally inadequate; standard grammars in computational linguistics are attribute-value
(AV) grammars of some variety. Before the advent of statistical methods, regular and
context-free grammars were considered too inexpressive for serious consideration, and
even now the reliance on stochastic versions of the less-expressive grammars is often
seen as an expedient necessitated by the lack of an adequate stochastic version of
attribute-value grammars.
Proposals have been made for extending stochastic models developed for the reg-
ular and context-free cases to grammars with constraints.&apos; Brew (1995) sketches a
probabilistic version of Head-Driven Phrase Structure Grammar (HPSG). He proposes
a stochastic process for generating attribute-value structures, that is, directed acyclic
graphs (dags). A dag is generated starting from a single node labeled with the (unique)
most general type. Each type S has a set of maximal subtypes T1, . . , Tn. To expand
a node labeled S, one chooses a maximal subtype T stochastically. One then considers
equating the current node with other nodes of type T, making a stochastic yes/no de-
</bodyText>
<footnote confidence="0.986579666666667">
* AT&amp;T Laboratories, Rm. A249, 180 Park Avenue, Florham Park, NJ 07932
1 I confine my discussion here to Brew and Eisele because they aim to describe parametric models of
probability distributions over the languages of constraint-based grammars, and to estimate the
parameters of those models. Other authors have assigned weights or preferences to constraint-based
grammars but not discussed parameter estimation. One approach of the latter sort that I find of
particular interest is that of Stefan Riezler (Riezler 1996), who describes a weighted logic for
constraint-based grammars that characterizes the languages of the grammars as fuzzy sets. This
interpretation avoids the need for normalization that Brew and Eisele face, though parameter
estimation still remains to be addressed.
</footnote>
<note confidence="0.85713">
© 1997 Association for Computational Linguistics
Computational Linguistics Volume 23, Number 4
</note>
<bodyText confidence="0.999766">
cision for each. Equating two nodes creates a re-entrancy. If the current node is equated
with no other node, one proceeds to expand it. Each maximal type introduces types
U1,...,Un, corresponding to values of attributes; one creates a child node for each
introduced type, and then expands each child in turn. A limitation of this approach is
that it permits one to specify only the average rate of re-entrancies; it does not permit
one to specify more complex context dependencies.
Eisele (1994) takes a logic-programming approach to constraint grammars. He
assigns probabilities to proof trees by attaching parameters to logic program clauses.
He presents the following logic program as an example:
</bodyText>
<figure confidence="0.7867678">
1. p(X,Y,Z) &lt;-1 q(X,Y), r(Y,Z).
2. q(a,b)
3. q(X ,c) 4-0.6
4. r(b,d) 4-0.5
5. r(X,e)
</figure>
<bodyText confidence="0.998858333333333">
The probability of a proof tree is defined to be proportional to the product of the
probabilities of clauses used in the proof. Normalization is necessary because some
derivations lead to invalid proof trees. For example, the derivation
</bodyText>
<equation confidence="0.95113">
bY 1 b Y 3 b y 4
P(X,Y,Z) q(X,Y) r(Y,Z) r(c,Z) : Y=c : Y=c b=c Z=d
</equation>
<bodyText confidence="0.995069571428571">
is invalid because of the illegal assignment b = c.
Both Brew and Eisele associate weights with analogues of rewrite rules. In Brew&apos;s
case, we can view type expansion as a stochastic choice from a finite set of rules of form
X where X is the type to expand and each 6 is a sequence of introduced child
types. A re-entrancy decision is a stochastic choice between two rules, X —&gt; yes and
X no, where X is the type of the node being considered for re-entrancy. In Eisele&apos;s
case, expanding a goal term can be viewed as a stochastic choice among a finite set of
rules X —&gt; where X is the predicate of the goal term and each 6 is a program clause
whose head has predicate X. The parameters of the models are essentially weights
on such rules, representing the probability of choosing 6 when making a choice of
type X.
In these terms, Brew and Eisele propose estimating parameters as the empiri-
cal relative frequency of the corresponding rules. That is, the weight of the rule
X —&gt; is obtained by counting the number of times X rewrites as 6 in the train-
ing corpus, divided by the total number of times X is rewritten in the training cor-
pus. For want of a standard term, let us call these estimates Empirical Relative Fre-
quency (ERF) estimates. To deal with incomplete data, both Brew and Eisele appeal
to the Expectation-Maximization (EM) algorithm, applied however to ERF rather than
maximum-likelihood estimates.
Under certain independence conditions, ERF estimates are maximum-likelihood
estimates. Unfortunately, these conditions are violated when there are context depen-
dencies of the sort found in attribute-value grammars, as will be shown below. As a
consequence, applying the ERF method to attribute-value grammars does not gener-
ally yield maximum-likelihood estimates. This is true whether one uses EM or not—a
method that yields the &amp;quot;wrong&amp;quot; estimates on complete data does not improve when
EM is used to extend the method to incomplete data.
Eisele identifies an important symptom that something is amiss with ERF esti-
mates: the probability distribution over proof trees that one obtains does not agree
</bodyText>
<page confidence="0.99578">
598
</page>
<note confidence="0.81299">
Abney Stochastic Attribute-Value Grammars
</note>
<bodyText confidence="0.999743068965517">
with the frequency of proof trees in the training corpus. Eisele recognizes that this
problem arises only where there are context dependencies.
Fortunately, solutions to the context-dependency problem have been described
(and indeed are currently enjoying a surge of interest) in statistics, machine learn-
ing, and statistical pattern recognition, particularly image processing. The models of
interest are known as random fields. Random fields can be seen as a generalization
of Markov chains and stochastic branching processes. Markov chains are stochas-
tic processes corresponding to regular grammars and random branching processes
are stochastic processes corresponding to context-free grammars. The evolution of a
Markov chain describes a line, in which each stochastic choice depends only on the
state at the immediately preceding time-point. The evolution of a random branching
process describes a tree in which a finite-state process may spawn multiple child pro-
cesses at the next time-step, but the number of processes and their states depend only
on the state of the unique parent process at the preceding time-step. In particular,
stochastic choices are independent of other choices at the same time-step: each process
evolves independently. If we permit re-entrancies, that is, if we permit processes to
re-merge, we generally introduce context-sensitivity. In order to re-merge, processes
must be &amp;quot;in synch,&amp;quot; which is to say, they cannot evolve in complete independence of
one another. Random fields are a particular class of multidimensional random pro-
cesses, that is, processes corresponding to probability distributions over an arbitrary
graph. The theory of random fields can be traced back to Gibbs (1902); indeed, the
probability distributions involved are known as Gibbs distributions.
To my knowledge, the first application of random fields to natural language was
Mark et al. (1992). The problem of interest was how to combine a stochastic context-
free grammar with n-gram language models. In the resulting structures, the probability
of choosing a particular word is constrained simultaneously by the syntactic tree in
which it appears and the choices of words at the n preceding positions. The context-
sensitive constraints introduced by the n-gram model are reflected in re-entrancies in
the structure of statistical dependencies, as in Figure 1.
</bodyText>
<figure confidence="0.6830524">
NP VP
I /\
there was NP
&amp;quot;
no response
</figure>
<figureCaption confidence="0.937817">
Figure 1
</figureCaption>
<bodyText confidence="0.988194833333333">
Statistical dependencies under the model of Mark et al. (1992).
In this diagram, the choice of label on a node z with parent x and preceding word y is
dependent on the label of x and y, but conditionally independent of the label on any
other node.
Della Pietra, Della Pietra, and Lafferty (1995, henceforth, DD&amp;L) also apply ran-
dom fields to natural language processing. The application they consider is the in-
duction of English orthographic constraints—inducing a grammar of possible English
words. DD&amp;L describe an algorithm called Improved Iterative Scaling (IIS) for se-
lecting informative features of words to construct a random field, and for setting the
parameters of the field optimally for a given set of features, to model an empirical
word distribution.
It is not immediately obvious how to use the ITS algorithm to equip attribute-value
</bodyText>
<page confidence="0.99495">
599
</page>
<note confidence="0.449147">
Computational Linguistics Volume 23, Number 4
</note>
<bodyText confidence="0.999467611111111">
grammars with probabilities. In brief, the difficulty is that the ITS algorithm requires
the computation of the expectations, under random fields, of certain functions; in
general, computing these expectations involves summing over all configurations (all
possible character sequences, in the orthography application), which is not possible
when the configuration space is large. Instead, DD&amp;L use Gibbs sampling to estimate
the needed expectations.
Gibbs sampling is possible for the application that DD&amp;L consider. A prerequisite
for Gibbs sampling is that the configuration space be closed under relabeling of graph
nodes. In the orthography application, the configuration space is the set of possible
English words, represented as finite linear graphs labeled with ASCII characters. Every
way of changing a label, that is, every substitution of one ASCII character for a different
one, yields a possible English word.
By contrast, the set of graphs admitted by an attribute-value grammar G is highly
constrained. If one changes an arbitrary node label in a dag admitted by G, one does not
necessarily obtain a new dag admitted by G. Hence, Gibbs sampling is not applicable.
However, I will show that a more general sampling method, the Metropolis-Hastings
algorithm, can be used to compute the maximum-likelihood estimate of the parameters
of AV grammars.
</bodyText>
<sectionHeader confidence="0.737276" genericHeader="introduction">
2. Stochastic Context-Free Grammars
</sectionHeader>
<bodyText confidence="0.9999846">
Let us begin by examining stochastic context-free grammars (SCFGs) and asking why
the natural extension of SCFG parameter estimation to attribute-value grammars fails.
A point of terminology: I will use the term grammar to refer to an unweighted gram-
mar, be it a context-free grammar or attribute-value grammar. A grammar equipped
with weights (and other periphenalia as necessary) I will refer to as a model. Occa-
sionally I will also use model to refer to the weights themselves, or the probability
distribution they define.
Throughout we will use the following stochastic context-free grammar for illus-
trative purposes. Let us call the underlying grammar G1 and the grammar equipped
with weights as shown, Mi.:
</bodyText>
<listItem confidence="0.999032">
1. S A A ,31 = 1/2
2. S B 02 = 1/2
3. A a 03 = 2/3
4. A —&gt; b 04 = 1/3
5. B--4 aa 03 1/2
6. B b b 06 = 1/2
</listItem>
<bodyText confidence="0.999850666666667">
The probability of a given tree is computed as the product of probabilities of rules
used in it. For example: Let x be the tree in Figure 2 and let qi be the probability
distribution over trees defined by model M1. Then:
</bodyText>
<equation confidence="0.990492">
qi (x) =
0 1 2 2 2
03 P3 = -2 3- • -3 = 9-
</equation>
<bodyText confidence="0.931995">
In parsing, we use the probability distribution qi (x) defined by model M1 to dis-
ambiguate: the grammar assigns some set of trees {xi, , x,,} to a sentence a, and we
</bodyText>
<page confidence="0.975588">
600
</page>
<figure confidence="0.98381025">
Abney Stochastic Attribute-Value Grammars
AC1:
03 ... ... .1-/&apos;
a , a
</figure>
<figureCaption confidence="0.682191">
Figure 2
Computing the probability of a parse tree.
</figureCaption>
<bodyText confidence="0.9999595">
choose that tree xi that has greatest probability qi (x,). The issue of efficiently comput-
ing the most-probable parse for a given sentence has been thoroughly addressed in the
literature. The standard parsing techniques can be readily adapted to the random-field
models to be discussed below, so I simply refer the reader to the literature. Instead, I
concentrate on parameter estimation, which, for attribute-value grammars, cannot be
accomplished by standard techniques.
By parameter estimation we mean determining values for the weights 0. In order
for a stochastic grammar to be useful, we must be able to compute the correct weights,
where by correct weights we mean the weights that best account for a training corpus.
The degree to which a given set of weights accounts for a training corpus is measured
by the similarity between the distribution q(x) determined by the weights /3 and the
distribution of trees x in the training corpus.
</bodyText>
<subsectionHeader confidence="0.998997">
2.1 The Goodness of a Model
</subsectionHeader>
<bodyText confidence="0.999765333333333">
The distribution determined by the training corpus is known as the empirical distri-
bution. For example, suppose we have a training corpus containing twelve trees of
the four types from L(G1) shown in Figure 3, where c(x) is the count of how often the
</bodyText>
<equation confidence="0.735820846153846">
X I x2 X3 X4
S s s s
I I
A A A A
I I I I B B
a a b b a a b b
c= 4x 2x 3x 3x =12
4/12 2/12 3/12 3/12
Figure 3
An empirical distribution. There are twelve parse trees of four distinct types.
tree (type) x appears in the corpus, and /3(.) is the empirical distribution, defined as:
c(x)
p(x) N N = c(x)
</equation>
<bodyText confidence="0.987345">
In comparing a distribution q to the empirical distribution /3, we shall actually mea-
sure dissimilarity rather than similarity. Our measure for dissimilarity of distributions
</bodyText>
<page confidence="0.997035">
601
</page>
<note confidence="0.402719">
Computational Linguistics Volume 23, Number 4
</note>
<tableCaption confidence="0.997563">
Table 1
</tableCaption>
<table confidence="0.975824285714286">
Computing the divergence of qi from I).
-13 q111) 191n(Plqi)
xi 2/9 1/3 0.67 0.14
X2 1/18 1/6 0.33 0.18
X3 1/4 1/4 1.00 0.00
X4 1/4 1/4 1.00 0.00
0.32
</table>
<tableCaption confidence="0.994696">
Table 2
</tableCaption>
<table confidence="0.760873571428571">
Computing the divergence of q&apos; from 13.
q, 19 q&apos;/13 pin(p/q&apos;)
Xi 1/8 1/3 0.38 0.33
X2 1/8 1/6 0.75 0.05
X3 1/4 1/4 1.00 0.00
X4 1/4 1/4 1.00 0.00
0.38
</table>
<equation confidence="0.968632">
is the Kullback-Leibler (KL) divergence, defined as:
D(-*) = E Nx) P(x)
x q(x)
</equation>
<bodyText confidence="0.999919">
The divergence between /5 and q at point x is the log of the ratio of p(x) to q(x). The
overall divergence between p and q is the average divergence, where the averaging is
over tree (tokens) in the corpus; i.e., point divergences 1n03(x)/q(x)) are weighted by
/5(x) and summed.
For example, let qi be, as before, the distribution determined by model M1. Table 1
shows qi, 17, the ratio qi (x)/13(x), and the weighted point divergence /3(x) ln(f 9(x) /q1(x)).
The sum of the fourth column is the KL divergence D(Pliqi) between /3 and qi. The
third column contains qi (x)/(x) rather than 17(x)/qi(x) so that one can see at a glance
whether qi (x) is too large (&gt; 1) or too small (&lt; 1). The total divergence D(7&apos;311(11) = 0.32.
One set of weights is better than another if its divergence from the empirical
distribution is less. For example, let us consider a different set of weights for grammar
G1. Let M&apos; be G1 with weights (1/2, 1/2, 1/2, 1/2, 1/2, 1/2), and let q&apos; be the probability
distribution determined by M&apos;. Then the computation of the KL divergence is as in
Table 2. The fit for x2 improves, but that is more than offset by a poorer fit for x1. The
distribution qi is a better distribution than q&apos;, in the sense that qi is more similar (less
dissimilar) to the empirical distribution than q&apos; is.
One reason for adopting minimal KL divergence as a measure of goodness is that
minimizing KL divergence maximizes likelihood. The likelihood of distribution q is
the probability of the training corpus according to q:
</bodyText>
<equation confidence="0.98934">
L(q) = fi q(x)
x in training
H q(x),(x)
</equation>
<page confidence="0.821151">
602
</page>
<note confidence="0.35711">
Abney Stochastic Attribute-Value Grammars
</note>
<equation confidence="0.9445855">
Since log is monotone increasing, maximizing likelihood is equivalent to maximizing
log likelihood:
ln L(q) = E c(x) ln q(x)
= NE13(x)lnq(x)
</equation>
<bodyText confidence="0.999548">
The expression on the right-hand side is —1/N times the cross entropy of q with
respect top, hence maximizing log likelihood is equivalent to minimizing cross entropy.
Finally, D(1q) is equal to the cross entropy of q less the entropy of 15, and the entropy of
/3 is constant with respect to q; hence minimizing cross entropy (maximizing likelihood)
is equivalent to minimizing divergence.
</bodyText>
<subsectionHeader confidence="0.999468">
2.2 The ERF Method
</subsectionHeader>
<bodyText confidence="0.999843">
For stochastic context-free grammars, it can be shown that the ERF method yields the
best model for a given training corpus. First, let us introduce some terminology and
notation. With each rule i in a stochastic context-free grammar is associated a weight
A and a function f1 (x) that returns the number of times rule i is used in the derivation
of tree x. For example, consider the tree in Figure 2, repeated here in Figure 4 for
convenience: Rule 1 is used once and rule 3 is used twice; accordingly fi (x) = 1,
</bodyText>
<equation confidence="0.71058875">
P3 33
Figure 4
Rule applications in a parse tree.
f3(x) =-- 2, and fi(x) = 0 for i E {2, 4, 5, 6}.
</equation>
<bodyText confidence="0.999880944444445">
We use the notation p[f] to represent the expectation of f under probability distri-
bution p; that is, p[f] = Ex p(x)f(x). The ERF method instructs us to choose the weight
for rule i proportional to its empirical expectation fo[f]. Algorithmically, we compute
the expectation of each rule&apos;s frequency, and normalize among rules with the same
left-hand side.
To illustrate, let us consider corpus (2.1) again. The expectation of each rule fre-
quency fi is a sum of terms f9(x)f,(x). These terms are shown for each tree, in Table 3.
For example, in tree xl, rule 1 is used once and rule 3 is used twice. The empirical
probability of x1 is 1 / 3, so X1&apos;s contribution to f4fd is 1/3 • 1, and its contribution to
/3[f3] is 1/3 • 2. The weight is obtained from p[f] by normalizing among rules with
the same left-hand side. For example, the expected rule frequencies 13[fd and [f2] of
rules with left-hand side S already sum to 1, so they are adopted without change as /31
and 02. On the other hand, the expected rule frequencies 13 [f5] and p[f6] for rules with
left-hand side B sum to 1/2, not 1, so they are doubled to yield weights Os and /36. It
should be observed that the resulting weights are precisely the weights of model A41.
It can be proven that the ERF weights are the best weights for a given context-
free grammar, in the sense that they define the distribution that is most similar to
the empirical distribution. That is, if /3 are the ERF weights (for a given grammar),
</bodyText>
<page confidence="0.999134">
603
</page>
<tableCaption confidence="0.866655">
Computational Linguistics Volume 23, Number 4
Table 3
Parameter estimation using the ERF method.
</tableCaption>
<table confidence="0.7458555">
ij S.--AA S—+B A--4a A-13 B—*aa B--bb
pfi 13f2 Pf3 13f4 13f5 13f6
Xi [s [A a] [A a]] 1/3 1/3 2/3
X2 [s [A b] [A b]E 1/6 1/6 2/6
.x3 Es [13 a all 1/4 1/4 1/4
X4 [s [B b IA 1/4 1/4 1/4
13Ef] = 1/2 1/2 2/3 1/3 1/4 1/4
0 = 1/2 1/2 2/3 1/3 1/2 1/2
</table>
<bodyText confidence="0.99731425">
defining distribution q, and 0&apos; defining q&apos; is any set of weights such that q q&apos;, then
D(13iiq) &lt; D(13iiqi)-
One might expect the best weights to yield D(1311q) = 0, but such is not the case. We
have just seen, for example, that the best weights for grammar G1 yield distribution
(ID yet D(311q1) = 0.32 &gt; 0. A closer inspection of the divergence calculation in Table 1
reveals that qi is sometimes less than 19, but never greater than /5. Could we improve
the fit by increasing qi? For that matter, how can it be that qi is never greater than /5?
As probability distributions, qi and /3 should have the same total mass, namely, one.
Where is the missing mass for qi?
The answer is of course that qi and /5 are probability distributions over L(Gi),
but not all of L(G1) appears in the corpus. Two trees are missing, and they account
for the missing mass. These two trees are given in Figure 5. Each of these trees has
</bodyText>
<figure confidence="0.994604">
A A A A
a b b a
</figure>
<figureCaption confidence="0.989049">
Figure 5
</figureCaption>
<bodyText confidence="0.9806765">
The trees from L(G1) that are missing in the training corpus.
probability 0 according to /5 (hence they can be ignored in the divergence calculation),
but probability 1/9 according to qi.
Intuitively, the problem is this: The distribution qi assigns too little weight to trees
x1 and x2, and too much weight to the &amp;quot;missing&amp;quot; trees; call them x5 and x6. Yet exactly
the same rules are used in x5 and x6 as are used in x1 and x2. Hence there is no way to
increase the weight for trees x1 and x2, improving their fit to /5, without simultaneously
increasing the weight for x5 and x6, making their fit to 13 worse. The distribution qi is
the best compromise possible.
To say it another way, our assumption that the corpus was generated by a context-
free grammar means that any context dependencies in the corpus must be accidental,
the result of sampling noise. There is indeed a dependency in the corpus in Figure 3:
in the trees where there are two A&apos;s, the A&apos;s always rewrite the same way. If the
corpus was generated by a stochastic context-free grammar, then this dependency is
accidental.
This does not mean that the context-free assumption is wrong. If we generate
twelve trees at random from qi, it would not be too surprising if we got the corpus in
Figure 3. More extremely, if we generate a random corpus of size 1 from qi, it is quite
</bodyText>
<page confidence="0.99041">
604
</page>
<note confidence="0.437785">
Abney Stochastic Attribute-Value Grammars
</note>
<bodyText confidence="0.9993105">
impossible for the resulting empirical distribution to match the distribution qi. But as
the corpus size increases, the fit between 15 and qi becomes ever better.
</bodyText>
<sectionHeader confidence="0.780275" genericHeader="method">
3. Attribute-Value Grammars
</sectionHeader>
<bodyText confidence="0.997597">
But what if the dependency in corpus (3) is not accidental? What if we wish to adopt
a grammar that imposes the constraint that both A&apos;s rewrite the same way? We can
impose such a constraint by means of an attribute-value grammar.
We may formalize an attribute-value grammar as a context-free grammar with
attribute labels and path equations. An example is the following grammar; let us call
it G2:
</bodyText>
<listItem confidence="0.998441333333333">
1. S —&gt; 1:A 2:A (1 1) = (21)
2. S 1:B
3. A -4 1:a
4. A —&gt; 1:b
5. B 1:a
6. B —&gt; 1:b
</listItem>
<figureCaption confidence="0.943983">
Figure 6 illustrates how a dag is generated from G2. We begin in (a) with a single
</figureCaption>
<figure confidence="0.9995645">
3 3
(a) (b) (c) (d)
</figure>
<figureCaption confidence="0.987531">
Figure 6
</figureCaption>
<bodyText confidence="0.981958722222222">
Generating a dag. The grammar used is G2.
node labeled with the start category of G2, namely, S. A node x is expanded by choosing
a rule that rewrites the category of x. In this case, we choose rule 1 to expand the root
node. Rule 1 instructs us to create two children, both labeled A. The edge to the first
child is labeled / and the edge to the second child is labeled 2. The constraint (1 1)
= (2 1) indicates that the / child of the / child of x is identical to the 1 child of the 2
child of x. We create an unlabeled node to represent this grandchild of x and direct
appropriately labeled edges from the children, yielding (b).
We proceed to expand the newly introduced nodes. We choose rule 3 to expand
the first A node. In this case, a child with edge labeled I already exists, so we use it
rather than creating a new one. Rule 3 instructs us to label this child a, yielding (c).
Now we expand the second A node. Again we choose rule 3. We are instructed to
label the 1 child a, but it already has that label, so we do not need to do anything.
Finally, in (d), the only remaining node is the bottom-most node, labeled a. Since its
label is a terminal category, it does not need to be expanded, and we are done.
Let us back up to (c) again. Here we were free to choose rule 4 instead of rule 3 to
expand the right-hand A node. Rule 4 instructs us to label the 1 child b, but we cannot,
inasmuch as it is already labeled a. The derivation fails, and no dag is generated.
</bodyText>
<page confidence="0.991855">
605
</page>
<note confidence="0.623947">
Computational Linguistics Volume 23, Number 4
</note>
<bodyText confidence="0.9703745">
The language L(G2) is the set of dags produced by successful derivations, as shown
in Figure 7. (The edges of the dags should actually be labeled with l&apos;s and 2&apos;s, but I
</bodyText>
<figure confidence="0.9247895">
x, X2 X3 X4
A A A A
NZ
a
</figure>
<figureCaption confidence="0.999502">
Figure 7
</figureCaption>
<bodyText confidence="0.9626615">
The language generated by G2.
have suppressed the edge labels for the sake of perspicuity.)
</bodyText>
<subsectionHeader confidence="0.999902">
3.1 AV Grammars and the ERF Method
</subsectionHeader>
<bodyText confidence="0.963930166666667">
Now we face the question of how to attach probabilities to grammar G2. The natural
extension of the method we used for context-free grammars is the following: Associate
a weight with each of the six rules of grammar G2. For example, let M2 be the model
consisting of G2 plus weights (i3,. .436) = (1 /2, 1 /2, 2/3, 1 /3, 1 /2, 1 /2). Let 02(x) be
the weight that M2 assigns to dag x; it is defined to be the product of the weights
of the rules used to generate x. For example, the weight 02 (xi) assigned to tree xi of
</bodyText>
<figureCaption confidence="0.87238">
Figure 7 is 2/9, computed as in Figure 8. Rule 1 is used once and rule 3 is used twice;
</figureCaption>
<figure confidence="0.604049">
X = ./A
</figure>
<figureCaption confidence="0.750775">
Figure 8
</figureCaption>
<bodyText confidence="0.741802714285714">
Rule applications in a dag generated by G2. The weight of the dag is the product of the
weights of rule applications.
hence 02(xi) = 010303 = 1/2 • 2/3 • 2/3 = 2/9.
Observe that 02(xi) = Ov3i, which is to say, 13(1(x1)4(x1). Moreover, since /3° = 1,
it does not hurt to include additional factors /ex&apos;) for those i where f, (xi) = 0. That
is, we can define the dag weight 0 corresponding to rule weights =- On)
generally as:
</bodyText>
<sectionHeader confidence="0.414924" genericHeader="method">
o(x)= Hex)
</sectionHeader>
<bodyText confidence="0.83509125">
i=1
The next question is how to estimate weights. Let us consider what happens when
we use the ERF method. Let us assume a corpus distribution for the dags in Figure 7
analogous to the distribution in Figure 3:
</bodyText>
<equation confidence="0.888932">
XI X2 X3 X4 (1)
1-) = 1/3 1/6 1/4 1/4
</equation>
<bodyText confidence="0.726764666666667">
Using the ERF method, we estimate rule weights as in Table 4. This table is identical
to the one given earlier in the context-free case. We arrive at the same weights M2 we
considered above, defining dag weights ;.2(x).
</bodyText>
<page confidence="0.991478">
606
</page>
<table confidence="0.638089">
Abney Stochastic Attribute-Value Grammars
</table>
<tableCaption confidence="0.995132">
Table 4
</tableCaption>
<table confidence="0.984570375">
Estimating the parameters of G2 using the ERF method.
Pfi Pf2 Pf3 PA if PA
xi 1/3 1/3 2/3
X2 1/6 1/6 2/6
x3 1/4 1/4 1/4
X4 1/4 1/4 1/4
P [J1 = 1/2 1/2 2/3 1/3 1/4 1/4
&apos;3= 1/2 1/2 2/3 1/3 1/2 1/2
</table>
<subsectionHeader confidence="0.969628">
3.2 Why the ERF Method Fails
</subsectionHeader>
<bodyText confidence="0.9997902">
But at this point a problem arises: 02 is not a probability distribution. Unlike in the
context-free case, the four dags in Figure 7 constitute the entirety of L(G2). This time,
there are no missing dags to account for the missing probability mass.
There is an obvious &amp;quot;fix&amp;quot; for this problem: we can simply normalize (b2. We might
define the distribution q for an AV grammar with weight function 0 as:
</bodyText>
<equation confidence="0.979796">
q(x)= yo(x)
where Z is the normalizing constant:
Z= E q)(x)
xEL(G)
</equation>
<bodyText confidence="0.977409">
In particular, for 02, we have Z = 2/9 + 1/18 + 1/4 + 1/4 = 7/9. Dividing 02 by 7/9
yields the ERF distribution:
</bodyText>
<equation confidence="0.995971">
Xi X2 X3 X4
q2(X) = 2/7 1/14 9/28 9/28
</equation>
<bodyText confidence="0.99955">
On the face of it, then, we can transplant the methods we used in the context-free
case to the AV case and nothing goes wrong. The only problem that arises (.15 not
summing to one) has an obvious fix (normalization).
However, something has actually gone very wrong. The ERF method yields the
best weights only under certain conditions that we inadvertently violated by chang-
ing L(G) and re-apportioning probability via normalization. In point of fact, we can
easily see that the ERF weights in Table 4 are not the best weights for our example
grammar. Consider the alternative model MK given in Figure 9, defining probability
distribution 11*.
</bodyText>
<figure confidence="0.555551666666667">
S—AA S—&gt;I3 A—&gt;a A—&gt;b B—&gt;a B—&gt;b
3+24 3 -4 1 1 1
6+24 6+24 1+4 1-1-fi f 2
</figure>
<figureCaption confidence="0.981251">
Figure 9
</figureCaption>
<bodyText confidence="0.961499">
An alternative model, M*.
These weights are proper, in the sense that weights for rules with the same left-hand
</bodyText>
<page confidence="0.980252">
607
</page>
<note confidence="0.60375">
Computational Linguistics Volume 23, Number 4
</note>
<bodyText confidence="0.768931">
side sum to one. The reader can verify that 0* sums to Z = 3+3`n and that q* is:
</bodyText>
<equation confidence="0.9685125">
X1 X2 X3 x4
q*(x) = 1/3 1/6 1/4 1/4
That is, q* = /9. Comparing q2 (the ERF distribution) and q* to /3, we observe that
D(11q2) = 0.07 but D(/311q*) = 0.
</equation>
<bodyText confidence="0.998525833333333">
In short, in the AV case, the ERF weights do not yield the best weights. This
means that the ERF method does not converge to the correct weights as the corpus
size increases. If there are genuine dependencies in the grammar, the ERF method
converges systematically to the wrong weights. Fortunately, there are methods that
do converge to the right weights. These are methods that have been developed for
random fields.
</bodyText>
<sectionHeader confidence="0.976865" genericHeader="method">
4. Random Fields
</sectionHeader>
<bodyText confidence="0.9999235">
A random field defines a probability distribution over a set of labeled graphs SZ called
configurations. In our case, the configurations are the dags generated by the grammar,
i.e., C2 = L(G). The weight assigned to a configuration is the product of the weights
assigned to selected features of the configuration. We use the notation:
</bodyText>
<equation confidence="0.990124">
o(x) = ie
</equation>
<bodyText confidence="0.999962909090909">
where ,(3, is the weight for feature i and J(.) is its frequency function, that is, f, (x) is
the number of times that feature i occurs in configuration x. (For most purposes, a
feature can be identified with its frequency function; I will not always make a careful
distinction between them.)
I use the term feature here as it is used in the machine learning and statistical
pattern recognition literature, not as in the constraint grammar literature, where feature
is synonymous with attribute. In my usage, dag edges are labeled with attributes, not
features. Features are rather like geographic features of dags: a feature is some larger
or smaller piece of structure that occurs—possibly at more than one place—in a dag.
The probability of a configuration (that is, a dag) is proportional to its weight, and
is obtained by normalizing the weight distribution.
</bodyText>
<equation confidence="0.978115">
q(x) = 0(x) Z = ExE,2 0(x)
</equation>
<bodyText confidence="0.999755076923077">
If we identify the features of a configuration with local trees—equivalently, with
applications of rewrite rules—the random field model is almost identical to the model
we considered in the previous section. There are two important differences. First,
we no longer require weights to sum to one for rules with the same left-hand side.
Second, the model does not require features to be identified with rewrite rules. We use
the grammar to define the set of configurations S2 = L(G), but in defining a probability
distribution over L(G), we can choose features of dags however we wish.
Let us consider an example. Let us continue to assume grammar G2 generating
the language in Figure 7, and let us continue to assume the empirical distribution
in (1). But now rather than taking rule applications to be features, let us adopt the
two features in Figure 10. For purpose of illustration, take feature 1 to have weight
)(31 = v--2- and feature 2 to have weight 02 = 3/2. The functions fi and f2 represent the
frequencies of features 1 and 2, respectively, as in Figure 11. We are able to exactly
</bodyText>
<page confidence="0.982649">
608
</page>
<figure confidence="0.869155">
Abney Stochastic Attribute-Value Grammars
• 2. (-14)
Figure 10
Two features.
S
A A
</figure>
<equation confidence="0.717136">
fi = 2 0 0 0
f2= o o 1 1
4= - \ / . 4 1 3/2 3/2 Z = 6
q= 1/3 1/6 1/4 1/4
Figure 11
</equation>
<bodyText confidence="0.982186454545455">
The frequencies (number of instances) of features 1 and 2 in dags generated by G2, and the
computation of dag weights 0 and dag probabilities q.
recreate the empirical distribution using fewer features than before. Intuitively, we
need only use as many features as are necessary to distinguish among trees that have
different empirical probabilities.
This added flexibility is welcome, but it does make parameter estimation more
involved. Now we must not only choose values for weights, we must also choose the
features that weights are to be associated with. We would like to do both in a way
that permits us to find the best model, in the sense of the model that minimizes the
Kullback-Leibler distance with respect to the empirical distribution. The ITS algorithm
(Della Pietra, Della Pietra, and Lafferty 1995) provides a method to do precisely that.
</bodyText>
<sectionHeader confidence="0.893556" genericHeader="method">
5. Field Induction
</sectionHeader>
<bodyText confidence="0.951203">
In outline, the ITS algorithm is as follows:
</bodyText>
<listItem confidence="0.9993745">
1. Start (t =-- 0) with the null field, containing no features.
2. Feature Selection. Consider every feature that might be added to field
A and choose the best one.
3. Weight Adjustment. Readjust weights for all features. The result is field
Mt+i.
4. Iterate until the field cannot be improved.
</listItem>
<bodyText confidence="0.762227625">
For the sake of concreteness, let us take features to be labeled subdags. In step 2
of the algorithm we do not consider every conceivable labeled subdag, but only the
atomic (i.e., single-node) subdags and those complex subdags that can be constructed
by combining features already in the field or by combining a feature in the field with
some atomic feature. We also limit our attention to features that actually occur in the
training corpus.
In our running example, the atomic features are as shown in Figure 12. Features
can be combined by adding connecting arcs, as shown in Figure 13, for example.
</bodyText>
<page confidence="0.989395">
609
</page>
<figure confidence="0.967909833333333">
Computational Linguistics Volume 23, Number 4
®
Figure 12
The atomic features arising in dags generated by G2.
(-A\ 0 = rs-1 +0=
0 + 0 = I
</figure>
<figureCaption confidence="0.903678">
Figure 13
</figureCaption>
<bodyText confidence="0.623476">
Combining features to create more complex features.
</bodyText>
<subsectionHeader confidence="0.913252">
5.1 The Null Field
</subsectionHeader>
<bodyText confidence="0.961988">
Field induction begins with the null field. With the corpus we have been assuming, the
null field takes the form in Figure 14. No dag x has any features, so 0(x) = n, i3( is a
</bodyText>
<equation confidence="0.842003666666667">
S
Fc/A AA BB
i
a b a b
(x)= 1 1 1 1 Z = 4
q(x) = 1/4 1/4 1/4 1/4
</equation>
<figureCaption confidence="0.70238">
Figure 14
</figureCaption>
<bodyText confidence="0.989488518518519">
The null field for G2.
product of zero terms, and hence has value 1. As a result, q is the uniform distribution.
The Kullback-Leibler divergence D(pllq) is 0.03. The aim of feature selection is to choose
a feature that reduces this divergence as much as possible.
The astute reader will note that there is a problem with the null field if L(G) is
infinite. Namely, it is not possible to have a uniform probability mass distribution over
an infinite set. If each dag in an infinite set of dags is assigned a constant nonzero
probability E, then the total probability is infinite, no matter how small E is. There are
a couple of ways of dealing with the problem. The approach that DD&amp;L adopt is to
assume a consistent prior distribution p(k) over graph sizes k, and a family of random
fields qk representing the conditional probability q(x I k); the probability of a tree is
then p(k)q(x I k). All the random fields have the same features and weights, differing
only in their normalizing constants.
I will take a somewhat different approach here. As sketched at the beginning of
section 3, we can generate dags from an AV grammar much as proposed by Brew
and Eisele. If we ignore failed derivations, the process of dag generation is completely
analogous to the process of tree generation from a stochastic CFG—indeed, in the
limiting case in which none of the rules contain constraints, the grammar is a CFG.
To obtain an initial distribution, we associate a weight with each rule, the weights
for rules with a common left-hand side summing to one. The probability of a dag is
proportional to the product of weights of rules used to generate it. (Renormalization
is necessary because of the failed derivations.) We estimate weights using the ERF
method: we estimate the weight of a rule as the relative frequency of the rule in the
training corpus, among rules with the same left-hand side.
The resulting initial distribution (the ERF distribution) is not the maximum-likeli-
hood distribution, as we know. But it can be taken as a useful first approximation.
Intuitively, we begin with the ERF distribution and construct a random field to take
</bodyText>
<page confidence="0.993658">
610
</page>
<note confidence="0.634202">
Abney Stochastic Attribute-Value Grammars
</note>
<bodyText confidence="0.999498666666667">
account of context dependencies that the ERF distribution fails to capture, incremen-
tally improving the fit to the empirical distribution.
In this framework, a model consists of: (1) An AV grammar G whose purpose
is to define a set of dags L(G). (2) A set of initial weights 0 attached to the rules
of G. The weight of a dag is the product of weights of rules used in generating it.
Discarding failed derivations and renormalizing yields the initial distribution po (x).
</bodyText>
<listItem confidence="0.851487">
(3) A set of features f1,. with weights 01, ... ,13,, to define the field distribution
</listItem>
<equation confidence="0.972777">
q(x) = .po(x) -fi(x).
</equation>
<subsectionHeader confidence="0.968431">
5.2 Feature Selection
</subsectionHeader>
<bodyText confidence="0.961651181818182">
At each iteration, we select a new feature f by considering all atomic features, and all
complex features that can be constructed from features already in the field. Holding
the weights constant for all old features in the field, we choose the best weight 0 for f
)
(how 13 is chosen will be discussed shortly), yielding a new distribution q1. The score
for feature f is the reduction it permits in D(11 gold ), where gold is the old field. That
is, the score for f is D(Igold) — D(311q0/). We compute the score for each candidate
feature and add to the field that feature with the highest score.
To illustrate, consider the two atomic features a and B. Given the null field as old
field, the best weight for a is 0 = 7/5, and the best weight for B is 0 = 1. This yields q
and D(1.711f) as in Figure 15. The better feature is a, and a would be added to the field
</bodyText>
<figure confidence="0.8403302">
S S
A A
a
1/3
7/5
7/24
ln 0.04
OB 1
qB _ 1/4
jo ln -IP; 0.10
</figure>
<figureCaption confidence="0.633725">
Figure 15
</figureCaption>
<bodyText confidence="0.976968928571429">
Comparing features. qa is the best (minimum-divergence) distribution that can be generated by
adding the feature &amp;quot;a&amp;quot; to the field, and qB is the best distribution generable by adding the
feature &amp;quot;B&amp;quot;.
if these were the only two choices.
Intuitively, a is better than B because a permits us to distinguish the set {xi, x3}
from the set {x2, x4}; the empirical probability of the former is 1/3+1/4 = 7/12 whereas
the empirical probability of the latter is 5/12. Distinguishing these sets permits us to
model the empirical distribution better (since the old field assigns them equal prob-
ability, counter to the empirical distribution). By contrast, the feature B distinguishes
the set {xi, x2} from {x3, x4}. The empirical probability of the former is 1/3+1/6 = 1/2
and the empirical probability of the latter is also 1/2. The old field models these prob-
abilities exactly correctly, so making the distinction does not permit us to improve on
the old field. As a result, the best weight we can choose for B is 1, which is equivalent
to not having the feature B at all.
</bodyText>
<table confidence="0.995689625">
A A a 1/4
1/6 1/4
1 7/5 1 Z = 24/5
5/24 7/24 5/24
—0.04 —0.04 0.05 D = 0.01
1 1 1 Z = 4
1/4 1/4 1/4
—0.07 0 0 D = 0.03
</table>
<page confidence="0.873994">
611
</page>
<note confidence="0.642941">
Computational Linguistics Volume 23, Number 4
</note>
<subsectionHeader confidence="0.999566">
5.3 Selecting the Initial Weight
</subsectionHeader>
<bodyText confidence="0.99981725">
DD&amp;L show that there is a unique weight ,a that maximizes the score for a new
feature f (provided that the score for f is not constant for all weights). Writing qo for
the distribution that results from assigning weight 13 to feature f, j is the solution to
the equation
</bodyText>
<equation confidence="0.989821">
go[f] = 1-9{fl (2)
</equation>
<bodyText confidence="0.999217684210526">
Intuitively, we choose the weight such that the expectation of f under the resulting
new field is equal to its empirical expectation.
Solving equation (2) for 3 is easy if L(G) is small enough to enumerate. Then the
sum over L(G) that is implicit in q,3 [f] can be expanded out, and solving for 3 is simply
a matter of arithmetic. Things are a bit trickier if L(G) is too large to enumerate. DD&amp;L
show that we can solve equation (2) if we can estimate gold [f = ic] for k from 0 to the
maximum value of f in the training corpus. (See Appendix 1 for details.)
We can estimate gold if = k] by means of random sampling. The idea is actually
rather simple: to estimate how often the feature appears in &amp;quot;the average dag,&amp;quot; we
generate a representative mini-corpus from the distribution iloid and count. That is, we
generate dags at random in such a way that the relative frequency of dag x is gold (x)
(in the limit), and we count how often the feature of interest appears in dags in our
generated mini-corpus.
The application that DD&amp;L consider is the induction of English orthographic con-
straints, that is, inducing a field that assigns high probability to &amp;quot;English-sounding&amp;quot;
words and low probability to non-English-sounding words. For this application, Gibbs
sampling is appropriate. Gibbs sampling does not work for the application to AV gram-
mars, however. Fortunately, there is an alternative random sampling method we can
use: Metropolis-Hastings sampling. We will discuss the issue in some detail shortly.
</bodyText>
<subsectionHeader confidence="0.999829">
5.4 Readjusting Weights
</subsectionHeader>
<bodyText confidence="0.99321925">
When a new feature is added to the field, the best value for its initial weight is chosen,
but the weights for the old features are held constant. In general, however, adding the
new feature may make it necessary to readjust weights for all features. The second
half of the IIS algorithm involves finding the best weights for a given set of features.
The method is very similar to the method for selecting the initial weight for a new
feature. Let (i31, , On) be the old weights for the features. We wish to compute &amp;quot;in-
crements&amp;quot; (61, . . SO to determine a new field with weights (601, • • • , n13n). Consider
the equation
gold efi] = P[fil (3)
where f# (x) = Epx) is the total number of features of dag x. The reason for the
factor et is a bit involved. Very roughly, we would like to choose weights so that the
expectation of f, under the new field is equal to p[f]. Now qn,(x) is:
</bodyText>
<equation confidence="0.946894666666667">
1
cinew(x) = 2p0(x)110,00x)
= —z6ciold(x)ll 6if
</equation>
<bodyText confidence="0.998194">
where we factor Z as Z6Zo, for Zo the normalization constant in q.m. Hence, qnew [f]
</bodyText>
<footnote confidence="0.685196">
r 1 f]x
gold [2f if nj bj I. Now there are two problems with this expression: it requires us to
compute Z6, which we are not able to do, and it requires us to determine weights
</footnote>
<page confidence="0.986958">
612
</page>
<note confidence="0.725859">
Abney Stochastic Attribute-Value Grammars
</note>
<bodyText confidence="0.997576647058824">
45j for all the features simultaneously, not just the weight 6, for feature i. We might
consider approximating qnew Ef,J by ignoring the normalization factor and assuming
that all features have the same weight as feature i. Since ft 6;6(x) = 6,4(x), we arrive at
the expression on the left-hand side of equation (3).
One might expect the approximation just described to be rather poor, but it is
proven in Della Pietra, Della Pietra, and Lafferty (1995) that solving equation (3) for
6, (for each i) and setting the new weight for feature i to SiOi is guaranteed to improve
the model. This is the real justification for equation (3), and the reader is referred to
Della Pietra, Della Pietra, and Lafferty (1995) for details.
Solving (3) yields improved weights, but it does not necessarily immediately yield
the globally best weights. We can obtain the globally best weights by iterating. Set
A 6,,A, for all i, and solve equation (3) again. Repeat until the weights no longer
change.
As with equation (2), solving equation (3) is straightforward if L(G) is small enough
to enumerate, but not if L(G) is large. In that case, we must use random sampling. We
generate a representative mini-corpus and estimate expectations by counting in the
mini-corpus. (See Appendix 2.)
</bodyText>
<subsectionHeader confidence="0.997368">
5.5 Random Sampling
</subsectionHeader>
<bodyText confidence="0.999902290322581">
We have seen that random sampling is necessary both to set the initial weight for
features under consideration and to adjust all weights after a new feature is adopted.
Random sampling involves creating a corpus that is representative of a given model
distribution q(x). To take a very simple example, a fair coin can be seen as a method
for sampling from the distribution q in which q(H) = 1/2, q(T) = 1/2. Saying that
a corpus is representative is actually not a comment about the corpus itself but the
method by which it was generated: a corpus representative of distribution q is one
generated by a process that samples from q. Saying that a process M samples from q is
to say that the empirical distributions of corpora generated by M converge to q in the
limit. For example, if we flip a fair coin once, the resulting empirical distribution over
(H, T) is either (1,0) or (0, 1), not the fair-coin distribution (1/2, 1/2). But as we take
larger and larger corpora, the resulting empirical distributions converge to (1/2, 1/2).
An advantage of SCFGs that random fields lack is the transparent relationship be-
tween an SCFG defining a distribution q and a sampler for q. We can sample from q by
performing stochastic derivations: each time we have a choice among rules expanding
a category X, we choose rule X with probability 13„ where A is the weight of rule
–÷ e,.
Now we can sample from the initial distribution pa by performing stochastic
derivations. At the beginning of Section 3, we sketched how to generate dags from an
AV grammar G via nondeterministic derivations. We defined the initial distribution
in terms of weights 0 attached to the rules of G. We can convert the nondeterminis-
tic derivations discussed at the beginning of Section 3 into stochastic derivations by
choosing rule X with probability 0, when expanding a node labeled X. Some
derivations fail, but throwing away failed derivations has the effect of renormalizing
the weight function, so that we generate a dag x with probability po(x), as desired.
The Metropolis-Hastings algorithm provides us with a means of converting the
sampler for the initial distribution po(x) into a sampler for the field distribution q(x).
Generally, let p(.) be a distribution for which we have a sampler. We wish to construct
a sample xl, , xN from a different distribution q(.). Assume that items xl, , xt, are
already in the sample, and we wish to choose x1. The sampler for p(.) proposes a
new item y. We do not simply add y to the sample—that would give us a sample
</bodyText>
<page confidence="0.996336">
613
</page>
<note confidence="0.713649">
Computational Linguistics Volume 23, Number 4
</note>
<bodyText confidence="0.9992356">
from p(.)—but rather we make a stochastic decision whether to accept the proposal y
or reject it. If we accept y, it is added to the sample (xn±i = y), and if we reject y, then
xn is repeated (xn+i = xn).
The acceptance decision is made as follows: If p(y) &gt; q(y), then y is overrep-
resented among the proposals. We can quantify the degree of overrepresentation as
p(y)/q(y). The idea is to reject y with a probability corresponding to its degree of
overrepresentation. However, we do not consider the absolute degree of overrepre-
sentation, but rather the degree of overrepresentation relative to xn. (If y and xr, are
equally overrepresented, there is no reason to reject y in favor of xn.) That is, we
consider the value
</bodyText>
<equation confidence="0.852616">
P(xii)/q(xn) P(xn)q(Y)
</equation>
<bodyText confidence="0.993477">
If r &lt; 1, then y is underrepresented relative to xn, and we accept y with probability one.
If r&gt; 1, then we accept y with a probability that diminishes as r increases: specifically,
with probability 1/r. In brief, the acceptance probability of y is A(y I xn) = min(1, 1/r).
It can be shown that proposing items with probability pH and accepting them with
probability A( I xn) yields a sampler for q(.). (See, for example, Winkler [1995]).2
The acceptance probability A(y I xn) reduces in our case to a particularly simple
form. If r &lt; 1 then A(y I x) = 1. Otherwise, writing 0(x) for the &amp;quot;field weight&amp;quot; ni 0{,(x),
we have:
</bodyText>
<equation confidence="0.885591666666667">
z -10(y )Po(y)pn (x„
A(y I xn I Z-10(xn)Po(x0Po(Y) (4)
0(Y)/0(xn)
</equation>
<sectionHeader confidence="0.972427" genericHeader="method">
6. Final Remarks
</sectionHeader>
<bodyText confidence="0.99994325">
In summary, we cannot simply transplant CF methods to the AV grammar case. In par-
ticular, the ERF method yields correct weights only for SCFGs, not for AV grammars.
We can define a probabilistic version of AV grammars with a correct weight-selection
method by going to random fields. Feature selection and weight adjustment can be
accomplished using the IIS algorithm. In feature selection, we need to use random
sampling to find the initial weight for a candidate feature, and in weight adjustment
we need to use random sampling to solve the weight equation. The random sampling
method that DD&amp;L used is not appropriate for sets of dags, but we can solve that
problem by using the Metropolis-Hastings method instead.
Open questions remain. First, random sampling is notorious for being slow, and it
remains to be shown whether the approach proposed here will be practicable. I expect
practicability to be quite sensitive to the choice of grammar—the more the grammar&apos;s
</bodyText>
<sectionHeader confidence="0.486037" genericHeader="method">
2 The Metropolis-Hastings acceptance probability is usually given in the form
</sectionHeader>
<equation confidence="0.929671">
. 7r(y)g(y, x)
7(x)g(x,y)
</equation>
<bodyText confidence="0.9999448">
in which 71- is the distribution we wish to sample from (q, in our notation) and g(x,y) is the proposal
probability: the probability that the input sampler will propose y if the previous configuration was x.
The case we consider is a special case in which the proposal probability is independent of x: the
proposal probability g(x, y) is, in our notation, p(y).
The original Metropolis algorithm is also a special case of the Metropolis-Hastings algorithm, in
which the proposal probability is symmetric, that is, g(x,y) g(y, x). The acceptance function then
reduces to min(1,71-(y)/ir(x)), which is min(1,q(y)/q(x)) in our notation. I mention this only to point
out that it is a different special case. Our proposal probability is not symmetric, but rather independent
of the previous configuration, and though our acceptance function reduces to a form (4) that is similar
to the original Metropolis acceptance function, it is not the same: in general, 49(y) / 0(x) q(y)/q(x).
</bodyText>
<equation confidence="0.995801">
r = p(y)/q(y) p(y)q(x„,)
A(y I x) = mm 1,
</equation>
<page confidence="0.997295">
614
</page>
<note confidence="0.643304">
Abney Stochastic Attribute-Value Grammars
</note>
<bodyText confidence="0.999845692307692">
distribution diverges from the initial context-free approximation, the more features
will be necessary to &amp;quot;correct&amp;quot; it, and the more random sampling will be called on.
A second issue is incomplete data. The approach described here assumes complete
data (a parsed training corpus). Fortunately, an extension of the method to handle
incomplete data (unparsed training corpora) is described in Riezler (1997), and I refer
readers to that paper.
As a closing note, it should be pointed out explicitly that the random field tech-
niques described here can be profitably applied to context-free grammars, as well. As
Stanley Peters nicely put it, there is a distinction between possibilistic and probabilistic
context-sensitivity. Even if the language described by the grammar of interest—that
is, the set of possible trees—is context-free, there may well be context-sensitive sta-
tistical dependencies. Random fields can be readily applied to capture such statistical
dependencies whether or not L(G) is context-sensitive.
</bodyText>
<sectionHeader confidence="0.906763" genericHeader="method">
Appendix A: Initial Weight Estimation
</sectionHeader>
<bodyText confidence="0.9987345">
In the feature selection step, we choose an initial weight 3 for each candidate feature
f so as to maximize the gain G = D(-)11 gold) — D(i)-11cfr,o) of adding f to the field. It is
actually more convenient to consider log weights a = ln /3. For a given feature f, the
log weight et that maximizes gain is the solution to the equation:
</bodyText>
<equation confidence="0.940674">
q[f] = ij[f]
</equation>
<bodyText confidence="0.995034">
where q„ is the distribution that results from adding f to the field with log weight a.
This equation can be solved using Newton&apos;s method. Define
</bodyText>
<equation confidence="0.95175">
F(a) =p[f] – q[f] (5)
</equation>
<bodyText confidence="0.922186571428571">
To find the value of a for which F(a) = 0, we begin at a convenient point ao (the
&amp;quot;null&amp;quot; weight ao = 0 recommends itself) and iteratively compute:
F(at)
Della Pietra, Della Pietra, and Lafferty (1995) show that F&apos;(at) is equal to the negative
of the variance off under the new field, which I will write –17„[f].
To compute the iteration (6) we need to be able to compute F(at) and F&apos;(at). For
F(at) we require p[f] and q„[f], and F&apos;(at) can be expressed as [f
</bodyText>
<equation confidence="0.811704">
]
2
q€ [f2].
p[f] is
</equation>
<bodyText confidence="0.938038">
simply the average value of f in the training corpus. The remaining terms are all of
the form qa [FL We can re-express this expectation in terms of the old field -gold
</bodyText>
<figure confidence="0.783182">
q,[11 =&gt; fr(x)q„(x)
Exfr (X)eaf (x) (bid (x)
Ex eaf(x)goid (x)
gold [frecifi
cioid [ef
</figure>
<bodyText confidence="0.9083985">
The expectations qoid fief] can be obtained by generating a random sample (z1, . • • , zN)
of size N from gold and computing the average value of leaf. That is, gold [ref]
</bodyText>
<equation confidence="0.788704333333333">
at+i = at
F&apos;(at)
(6)
</equation>
<page confidence="0.924739">
615
</page>
<note confidence="0.610568">
Computational Linguistics Volume 23, Number 4
</note>
<equation confidence="0.888444">
(1/N)sr(a), where:
Sr() = fr (zk)eq (zo
Ecountk[f (zk) = u]ure&apos;
This yields:
[ Sr(a)
j j so(a)
</equation>
<bodyText confidence="0.846851">
and the Newton iteration (6) reduces to:
</bodyText>
<equation confidence="0.999451">
at+1 = at +
so(at)s2(at) — 51(at)2
S(at)p[f] — so (at)si (at)
</equation>
<bodyText confidence="0.996924333333333">
To compare candidates, we also need to know the gain D(Pliq old) — DQ311c16) for each
candidate. This can be expressed as follows (Della Pietra, Della Pietra, and Lafferty
1995):
</bodyText>
<equation confidence="0.9657395">
G(f, a) = 13[f] In — in gold [eaf]
[f]1n&amp; — lnso(a) +1nN
</equation>
<bodyText confidence="0.9942095">
Putting everything together, the algorithm for feature selection has the following
form. The array E[f] is assumed to have been initialized with the empirical expectations
</bodyText>
<figure confidence="0.795168826086957">
[f I.
procedure SelectFeature 0 begin
Fill array C[f, , u] = countk[f (zk) = u]
by sampling from old field
G 0, g 4- none
for each f in candidates do
a 0
until a is accurate enough do
So
4- Si +--- S2 4- 0
for u from 0 to umax do
x C[f , u]e&amp;quot;
a +
s2E[f]—s0si
° 2
sos2-s,
end
G +— aE[f] — ln so + ln N
if G&gt; a then a G , g f , 131 4- a
end
return g, a, a
end
a
</figure>
<page confidence="0.918913">
616
</page>
<note confidence="0.528741">
Abney Stochastic Attribute-Value Grammars
</note>
<sectionHeader confidence="0.640143" genericHeader="conclusions">
Appendix B: Adjusting Field Weights
</sectionHeader>
<bodyText confidence="0.9998684">
The procedure for adjusting field weights has much the same structure as the pro-
cedure for choosing initial weights. In terms of log weights, we wish to compute
increments (61, • • • , 6) such that the new field, with log weights (al + 61, • • • , an + e5n)
has a lower divergence than the old field (al, , an). We choose each 6, as the solution
to the equation:
</bodyText>
<equation confidence="0.630237">
P[fil = gold [fie6if#i
</equation>
<bodyText confidence="0.722503">
Again, we use Newton&apos;s method. We wish to find 6 such that F1(6) = 0, where:
</bodyText>
<equation confidence="0.843333333333333">
F(8) = P[fil — qoid[fief]
As Della Pietra, Della Pietra, and Lafferty (1995) show, the first derivative is:
.C(6) = —qoki[fif#ef# ]
</equation>
<bodyText confidence="0.9967795">
We see that the expectations we need to compute by sampling from gold are of form
q0ld[ffie6f#]. We generate a random sample (z1, ••• , zN) and define:
</bodyText>
<equation confidence="0.9634404">
6) = Efi(za# (Zk)r e6f# (zk)
= E E countk[f;(zk) = u A f# (zk) = miumre6m
111 U
= E mre6n, E fi(zk)
k[t#(zk)=m
</equation>
<bodyText confidence="0.999796666666667">
As we generate the sample we update the array C[i, mI = Ek (zk)= m 1;(4) . We estimate
qo1c[ffte6f#] as the average value of ftae6f# in the sample, namely, (1/N)5r(i,6). This
permits us to compute F;(6) and F;(6). The resulting Newton iteration is:
</bodyText>
<equation confidence="0.995806">
6t+i = 6t +
si(i, 6)
Np[fd — so(i,6;)
</equation>
<bodyText confidence="0.863904666666667">
The estimation procedure is:
procedure Adjust Weights (ai, , an) begin
until the field converges do
</bodyText>
<figure confidence="0.54283325">
Fill array C[i, m]
by sampling from qa
for i from 1 to n
8-0
until 6 is sufficiently accurate do
So Si 4-- 0
for m from 0 to mrnax do
x &lt;— C[i, m]e6m
</figure>
<reference confidence="0.4203202">
So &lt;— So ± X
Si *— Si XM
end
6— 8+ NEtfd—so
si
</reference>
<page confidence="0.929078">
617
</page>
<note confidence="0.282825">
Computational Linguistics Volume 23, Number 4
</note>
<reference confidence="0.462828833333333">
end
a, a, +
end
end
return (ai, • • •, an)
end
</reference>
<sectionHeader confidence="0.878024" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.965972411764706">
This work has greatly profited from the
comments, criticism, and suggestions of a
number of people, including Yoav Freund,
John Lafferty, Stanley Peters, Hans
Uszkoreit, and members of the audience at
talks I gave at Saarbrucken and Tubingen.
Michael Miller and Kevin Mark introduced
me to random fields as a way of dealing
with context-sensitivities in language,
planting the idea that led (much later) to
this paper. Finally, I would especially like to
thank Marc Light and Stefan Riezler for
extended discussions of the issues
addressed here and helpful criticism of my
first attempts to present this material.
All responsibility for flaws and errors of
course remains with me.
</bodyText>
<sectionHeader confidence="0.909534" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999800032258064">
Brew, Chris. 1995. Stochastic HPSG. In
Proceedings of EACL-95.
Della Pietra, Stephen, Vincent Della Pietra,
and John Lafferty. 1995. Inducing features
of random fields. Technical Report
CMU-CS-95-144, CMU.
Eisele, Andreas. 1994. Towards probabilistic
extensions of constraint-based grammars.
Technical Report Deliverable RI .2.B,
DYANA-2.
Gibbs, W. 1902. Elementary Principles of
Statistical Mechanics. Yale University Press,
New Haven, CT.
Mark, Kevin, Michael Miller, Ulf Grenander,
and Steve Abney. 1992. Parameter
estimation for constrained context-free
language models. In Proceedings of the Fifth
Darpa Workshop on Speech and Natural
Language, San Mateo, CA. Morgan
Kaufman.
Riezler, Stefan. 1996. Quantitative constraint
logic programming for weighted
grammar applications. Talk given at
LACL, September.
Riezler, Stefan. 1997. Probabilistic constraint
logic programming. Arbeitspapiere des
Sonderforschungsbereichs 340, Bericht Nr.
117, Universitat Tubingen.
Winkler, Gerhard. 1995. Image Analysis,
Random Fields and Dynamic Monte Carlo
Methods. Springer.
</reference>
<page confidence="0.994054">
618
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.657930">
<title confidence="0.99997">Stochastic Attribute-Value Grammars</title>
<author confidence="0.999984">Steven P Abney</author>
<affiliation confidence="0.996033">AT&amp;T Laboratories</affiliation>
<abstract confidence="0.970582583333333">Probabilistic analogues of regular and context-free grammars are well known in computational linguistics, and currently the subject of intensive research. To date, however, no satisfactory probabilistic analogue of attribute-value grammars has been proposed: previous attempts have failed to define an adequate parameter-estimation algorithm. In the present paper, I define stochastic attribute-value grammars and give an algorithm for computing the maximum-likelihood estimate of their parameters. The estimation algorithm is adapted from Della Pietra, Della Pietra, and Lafferty (1995). To estimate model parameters, it is necessary to compute the expectations of certain functions under random fields. In the application discussed by Della Pietra, Della Pietra, and Lafferty (representing English orthographic constraints), Gibbs sampling can be used to estimate the needed expectations. The fact that attribute-value grammars generate constrained languages makes Gibbs sampling inapplicable, but I show that sampling can be done using the more general Metropolis-Hastings algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>X Si</author>
</authors>
<note>XM end 6— 8+ NEtfd—so si</note>
<marker>Si, </marker>
<rawString>So &lt;— So ± X Si *— Si XM end 6— 8+ NEtfd—so si</rawString>
</citation>
<citation valid="false">
<note>end a, a, + end</note>
<marker></marker>
<rawString>end a, a, + end</rawString>
</citation>
<citation valid="false">
<authors>
<author>end return</author>
</authors>
<note>an) end</note>
<marker>return, </marker>
<rawString>end return (ai, • • •, an) end</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brew</author>
</authors>
<title>Stochastic HPSG.</title>
<date>1995</date>
<booktitle>In Proceedings of EACL-95.</booktitle>
<contexts>
<context position="2103" citStr="Brew (1995)" startWordPosition="284" endWordPosition="285">ed linguistically inadequate; standard grammars in computational linguistics are attribute-value (AV) grammars of some variety. Before the advent of statistical methods, regular and context-free grammars were considered too inexpressive for serious consideration, and even now the reliance on stochastic versions of the less-expressive grammars is often seen as an expedient necessitated by the lack of an adequate stochastic version of attribute-value grammars. Proposals have been made for extending stochastic models developed for the regular and context-free cases to grammars with constraints.&apos; Brew (1995) sketches a probabilistic version of Head-Driven Phrase Structure Grammar (HPSG). He proposes a stochastic process for generating attribute-value structures, that is, directed acyclic graphs (dags). A dag is generated starting from a single node labeled with the (unique) most general type. Each type S has a set of maximal subtypes T1, . . , Tn. To expand a node labeled S, one chooses a maximal subtype T stochastically. One then considers equating the current node with other nodes of type T, making a stochastic yes/no de* AT&amp;T Laboratories, Rm. A249, 180 Park Avenue, Florham Park, NJ 07932 1 I </context>
</contexts>
<marker>Brew, 1995</marker>
<rawString>Brew, Chris. 1995. Stochastic HPSG. In Proceedings of EACL-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>Vincent Della Pietra Stephen</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1995</date>
<tech>Technical Report CMU-CS-95-144, CMU.</tech>
<marker>Pietra, Stephen, Lafferty, 1995</marker>
<rawString>Della Pietra, Stephen, Vincent Della Pietra, and John Lafferty. 1995. Inducing features of random fields. Technical Report CMU-CS-95-144, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Eisele</author>
</authors>
<title>Towards probabilistic extensions of constraint-based grammars.</title>
<date>1994</date>
<tech>Technical Report Deliverable RI .2.B,</tech>
<pages>2</pages>
<contexts>
<context position="4007" citStr="Eisele (1994)" startWordPosition="582" endWordPosition="583">ains to be addressed. © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 cision for each. Equating two nodes creates a re-entrancy. If the current node is equated with no other node, one proceeds to expand it. Each maximal type introduces types U1,...,Un, corresponding to values of attributes; one creates a child node for each introduced type, and then expands each child in turn. A limitation of this approach is that it permits one to specify only the average rate of re-entrancies; it does not permit one to specify more complex context dependencies. Eisele (1994) takes a logic-programming approach to constraint grammars. He assigns probabilities to proof trees by attaching parameters to logic program clauses. He presents the following logic program as an example: 1. p(X,Y,Z) &lt;-1 q(X,Y), r(Y,Z). 2. q(a,b) 3. q(X ,c) 4-0.6 4. r(b,d) 4-0.5 5. r(X,e) The probability of a proof tree is defined to be proportional to the product of the probabilities of clauses used in the proof. Normalization is necessary because some derivations lead to invalid proof trees. For example, the derivation bY 1 b Y 3 b y 4 P(X,Y,Z) q(X,Y) r(Y,Z) r(c,Z) : Y=c : Y=c b=c Z=d is inv</context>
</contexts>
<marker>Eisele, 1994</marker>
<rawString>Eisele, Andreas. 1994. Towards probabilistic extensions of constraint-based grammars. Technical Report Deliverable RI .2.B, DYANA-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gibbs</author>
</authors>
<title>Elementary Principles of Statistical Mechanics.</title>
<date>1902</date>
<publisher>Yale University Press,</publisher>
<location>New Haven, CT.</location>
<contexts>
<context position="8476" citStr="Gibbs (1902)" startWordPosition="1305" endWordPosition="1306">ding time-step. In particular, stochastic choices are independent of other choices at the same time-step: each process evolves independently. If we permit re-entrancies, that is, if we permit processes to re-merge, we generally introduce context-sensitivity. In order to re-merge, processes must be &amp;quot;in synch,&amp;quot; which is to say, they cannot evolve in complete independence of one another. Random fields are a particular class of multidimensional random processes, that is, processes corresponding to probability distributions over an arbitrary graph. The theory of random fields can be traced back to Gibbs (1902); indeed, the probability distributions involved are known as Gibbs distributions. To my knowledge, the first application of random fields to natural language was Mark et al. (1992). The problem of interest was how to combine a stochastic contextfree grammar with n-gram language models. In the resulting structures, the probability of choosing a particular word is constrained simultaneously by the syntactic tree in which it appears and the choices of words at the n preceding positions. The contextsensitive constraints introduced by the n-gram model are reflected in re-entrancies in the structur</context>
</contexts>
<marker>Gibbs, 1902</marker>
<rawString>Gibbs, W. 1902. Elementary Principles of Statistical Mechanics. Yale University Press, New Haven, CT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Mark</author>
<author>Michael Miller</author>
<author>Ulf Grenander</author>
<author>Steve Abney</author>
</authors>
<title>Parameter estimation for constrained context-free language models.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fifth Darpa Workshop on Speech and Natural Language,</booktitle>
<publisher>Morgan Kaufman.</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="8657" citStr="Mark et al. (1992)" startWordPosition="1330" endWordPosition="1333">is, if we permit processes to re-merge, we generally introduce context-sensitivity. In order to re-merge, processes must be &amp;quot;in synch,&amp;quot; which is to say, they cannot evolve in complete independence of one another. Random fields are a particular class of multidimensional random processes, that is, processes corresponding to probability distributions over an arbitrary graph. The theory of random fields can be traced back to Gibbs (1902); indeed, the probability distributions involved are known as Gibbs distributions. To my knowledge, the first application of random fields to natural language was Mark et al. (1992). The problem of interest was how to combine a stochastic contextfree grammar with n-gram language models. In the resulting structures, the probability of choosing a particular word is constrained simultaneously by the syntactic tree in which it appears and the choices of words at the n preceding positions. The contextsensitive constraints introduced by the n-gram model are reflected in re-entrancies in the structure of statistical dependencies, as in Figure 1. NP VP I /\ there was NP &amp;quot; no response Figure 1 Statistical dependencies under the model of Mark et al. (1992). In this diagram, the ch</context>
</contexts>
<marker>Mark, Miller, Grenander, Abney, 1992</marker>
<rawString>Mark, Kevin, Michael Miller, Ulf Grenander, and Steve Abney. 1992. Parameter estimation for constrained context-free language models. In Proceedings of the Fifth Darpa Workshop on Speech and Natural Language, San Mateo, CA. Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
</authors>
<title>Quantitative constraint logic programming for weighted grammar applications. Talk given at LACL,</title>
<date>1996</date>
<contexts>
<context position="3149" citStr="Riezler 1996" startWordPosition="450" endWordPosition="451">onsiders equating the current node with other nodes of type T, making a stochastic yes/no de* AT&amp;T Laboratories, Rm. A249, 180 Park Avenue, Florham Park, NJ 07932 1 I confine my discussion here to Brew and Eisele because they aim to describe parametric models of probability distributions over the languages of constraint-based grammars, and to estimate the parameters of those models. Other authors have assigned weights or preferences to constraint-based grammars but not discussed parameter estimation. One approach of the latter sort that I find of particular interest is that of Stefan Riezler (Riezler 1996), who describes a weighted logic for constraint-based grammars that characterizes the languages of the grammars as fuzzy sets. This interpretation avoids the need for normalization that Brew and Eisele face, though parameter estimation still remains to be addressed. © 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 cision for each. Equating two nodes creates a re-entrancy. If the current node is equated with no other node, one proceeds to expand it. Each maximal type introduces types U1,...,Un, corresponding to values of attributes; one creates a ch</context>
</contexts>
<marker>Riezler, 1996</marker>
<rawString>Riezler, Stefan. 1996. Quantitative constraint logic programming for weighted grammar applications. Talk given at LACL, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
</authors>
<title>Probabilistic constraint logic programming. Arbeitspapiere des Sonderforschungsbereichs 340, Bericht Nr.</title>
<date>1997</date>
<tech>117,</tech>
<institution>Universitat Tubingen.</institution>
<contexts>
<context position="49246" citStr="Riezler (1997)" startWordPosition="8623" endWordPosition="8624"> that is similar to the original Metropolis acceptance function, it is not the same: in general, 49(y) / 0(x) q(y)/q(x). r = p(y)/q(y) p(y)q(x„,) A(y I x) = mm 1, 614 Abney Stochastic Attribute-Value Grammars distribution diverges from the initial context-free approximation, the more features will be necessary to &amp;quot;correct&amp;quot; it, and the more random sampling will be called on. A second issue is incomplete data. The approach described here assumes complete data (a parsed training corpus). Fortunately, an extension of the method to handle incomplete data (unparsed training corpora) is described in Riezler (1997), and I refer readers to that paper. As a closing note, it should be pointed out explicitly that the random field techniques described here can be profitably applied to context-free grammars, as well. As Stanley Peters nicely put it, there is a distinction between possibilistic and probabilistic context-sensitivity. Even if the language described by the grammar of interest—that is, the set of possible trees—is context-free, there may well be context-sensitive statistical dependencies. Random fields can be readily applied to capture such statistical dependencies whether or not L(G) is context-s</context>
</contexts>
<marker>Riezler, 1997</marker>
<rawString>Riezler, Stefan. 1997. Probabilistic constraint logic programming. Arbeitspapiere des Sonderforschungsbereichs 340, Bericht Nr. 117, Universitat Tubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Winkler</author>
</authors>
<title>Image Analysis, Random Fields and Dynamic Monte Carlo Methods.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<marker>Winkler, 1995</marker>
<rawString>Winkler, Gerhard. 1995. Image Analysis, Random Fields and Dynamic Monte Carlo Methods. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>