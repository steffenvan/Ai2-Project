<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993639">
Syntax-Driven Sentence Revision for Broadcast News Summarization
</title>
<author confidence="0.9692965">
Hideki Tanaka, Akinori Kinoshita, Takeshi Kobayakawa,
Tadashi Kumano and Naoto Kato
</author>
<affiliation confidence="0.960581">
NHK Science and Technology Research Labs.
</affiliation>
<address confidence="0.930753">
1-10-11, Kinuta, Setagaya-ku, Tokyo, Japan
</address>
<email confidence="0.999086">
{tanaka.h-ja,kinoshita.a-ek,kobayakawa-t.ko,kumano.t-eq,kato.n-ga}@nhk.or.jp
</email>
<sectionHeader confidence="0.993906" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999847571428571">
We propose a method of revising lead sentences in
a news broadcast. Unlike many other methods pro-
posed so far, this method does not use the corefer-
ence relation of noun phrases (NPs) but rather,
insertion and substitution of the phrases modifying
the same head chunk in lead and other sentences.
The method borrows an idea from the sentence
fusion methods and is more general than those
using NP coreferencing as ours includes them. We
show in experiments the method was able to find
semantically appropriate revisions thus demon-
strating its basic feasibility. We also show that that
parsing errors mainly degraded the sentential com-
pleteness such as grammaticality and redundancy.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973571428572">
We address the problem of revising the lead sen-
tence in a broadcast news text to increase the
amount of background information in the lead.
This is one of the draft and revision approaches
to summarization, which has received keen atten-
tion in the research community. Unlike many
other methods that directly utilize noun phrase
(NP) coreference (Nenkova 2008; Mani et al.
1999), we propose a method that employs inser-
tion and substitution of phrases that modify the
same chunk in the lead and other sentences. We
also show its effectiveness in a revision experi-
ment.
As is well known, the extractive summary that
has been extensively studied from the early days
of summarization history (Luhn, 1958) suffers
from various drawbacks. These include the prob-
lems of a break in cohesion in the summary text
such as dangling anaphora and a sudden shift in
topic.
To ameliorate these problems, the idea of revis-
ing the extracted sentences was proposed in a
single document summarization study. Jing and
McKeown (1999; 2000) found that human sum-
marization can be traced back to six cut-and-
paste operations of a text and proposed a revision
method consisting of sentence reduction and
combination modules with a sentence extraction
part. Mani and colleagues (1999) proposed a
summarization system based on “draft and revi-
sion” together with sentence extraction. The re-
vision part is achieved with the sentence aggre-
gation and smoothing modules.
The cohesion break problem becomes particu-
larly conspicuous in multi-document summariza-
tion. To ameliorate this, revision of the extracted
sentences is also thought to be effective, and
many ideas and methods have been proposed so
far. For example, Otterbacher and colleagues
(2002) analyzed manually revised extracts and
factored out cohesion problems. Nenkova (2008)
proposed a revision idea that utilizes noun
coreference with linguistic quality improvements
in mind.
Other than the break in cohesion, multi-
document summarization faces the problem of
information overlap particularly when the docu-
ment set consists of similar sentences. Barzilay
and McKeown (2005) proposed an idea called
sentence fusion that integrates information in
overlapping sentences to produce a non-
overlapping summary sentence. Their algorithm
firstly analyzes the sentences to obtain the de-
pendency trees and sets a basis tree by finding
the centroid of the dependency trees. It next
augments the basis tree with the sub-trees in oth-
er sentences and finally prunes the predefined
constituents. Their algorithm was further modi-
fied and applied to the German biographies by
Filippova and Strube (2008).
Like the work of Jing and McKeown (2000) and
Mani et al. (1999), our work was inspired by the
summarization method used by human abstrac-
tors. Actually, our abstractors first extract impor-
tant sentences, which is called lead identification,
and then revise them, which is referred to as
phrase elaboration or specification. In this paper,
we concentrate on the revision part.
Our work can be viewed as an application of the
sentence fusion method to the draft and revision
</bodyText>
<page confidence="0.99332">
39
</page>
<note confidence="0.99899">
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 39–47,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999788076923077">
approach to a single Japanese news document
summarization. Actually, our dependency struc-
ture alignment is almost the same as that of
Filippova and Strube (2008), and our lead sen-
tence plays the role of a basis tree in the Barzilay
and McKeown approach (2005). Though the idea
of sentence fusion was developed mainly for
suppressing the overlap in multi-document sum-
marization, we consider this effective in aug-
menting the extracts in a single-document sum-
marization task where we face less overlap
among sentences.
Before explaining the method in detail, we will
briefly introduce the Japanese dependency 1
structure on which our idea is based. The de-
pendency structure is constructed based on the
bunsetsu chunk, which we call “chunk” for sim-
plicity. The chunk usually consists of one con-
tent-bearing word and a series of function words.
All the chunks in a sentence except for the last
one modify a chunk in the right direction. We
call the modifying chunk the modifier and the
modified chunk the head. We usually span a di-
rected edge from a modifier chunk to the head
chunk2. Our dependency tree has no syntactic
information such as subject or object.
</bodyText>
<sectionHeader confidence="0.788441" genericHeader="method">
2 Broadcast news summarization
</sectionHeader>
<bodyText confidence="0.9999844">
Tanaka et al. (2005) showed that most Japanese
broadcast news texts are written with a three-part
structure, i.e., the lead, body, and supplement.
The most important information is succinctly
mentioned in the lead, which is the opening sen-
tence(s) of a news story, referred to as an “arti-
cle” here. Proper names and details are some-
times avoided in favor of more abstract expres-
sions such as “big insurance company.” The lead
is then detailed in the body by answering who,
what, when, where, why, and how, and proper
names only alluded to in the lead appear here.
Necessary information that was not covered in
the lead or the body is placed in the supplement.
The research also reports that professional news
abstractors who are hired for digital text services
summarize articles in a two-step approach. First,
they identify the lead sentences and set it (them)
as the starting point of the summary. As the av-
erage lead length is 95 characters and the al-
</bodyText>
<footnote confidence="0.998451">
1 This is the kakari-uke (modifier-modifiee) relation of
Japanese, which differs from the conventional dependency
relation. We use the term dependency for convenience in
this paper.
2 This is the other way around compared to the English de-
pendency such as in Barzilay and McKeown (2005).
</footnote>
<bodyText confidence="0.998384652173913">
lowed summary length is about 115 characters
(or 150 characters depending on the screen de-
sign), they revise the lead sentences using ex-
pressions from the remainder of the story.
We see here that the extraction and revision
strategy that has been extensively studied by
many researchers for various reasons was actu-
ally applied by human abstractors, and therefore,
the strategy can be used as a real summarization
model. Inspired by this, we decided to study a
news summarization system based on the above
approach. To develop a complete summarization
system, we have to solve three problems: 1)
identifying the lead, body, and supplement struc-
ture in each article, 2) finding the lead revision
candidates, and 3) generating a final summary by
selecting and combining the candidates.
We have already studied problem 1) and showed
that automatic recognition of three tags with a
decision tree algorithm reached a precision over
92% (Tanaka et al. 2007). We then moved to
problem 2), which we discuss extensively in the
rest of this paper.
</bodyText>
<sectionHeader confidence="0.969182" genericHeader="method">
3 Manual lead revision experiment
</sectionHeader>
<bodyText confidence="0.999955793103448">
To see how problem 2) in the previous section
could be solved, we conducted a manual lead-
revision experiment. We asked a native Japanese
speaker to revise the lead sentences of 15 news
articles using expressions from the body section
of each article with cut-and-paste operations (in-
sertion and substitution) of bunsetsu chunk se-
quences. We refer to chunk sequences as phrases.
We also asked the reviser to find as many revi-
sions as possible.
In the interview with her, we found that she took
advantage of the syntactic structure to revise the
lead sentences. Actually, she first searched for
the “same” chunks in the lead and the body and
checked whether the modifier phrases to these
chunks could be used for revision. To see what
makes these chunks the “same,” we compared
the syntactic head chunk of the lead and body
phrases used for substitution and insertion.
Table 1 summarizes the results of the compari-
son in three categories: perfect match, partial
match (content word match), and different.
The table indicates that nearly half of the head
chunks were exactly the same, and the rest con-
tained some differences. The second row shows
the number where the syntactic heads had the
same content words but not the same function
words. The pair �AL kaidan-shi ‘talked’ and
�ALILfes. kaidan-shi-mashi-ta ‘talked’ is an
</bodyText>
<page confidence="0.998097">
40
</page>
<table confidence="0.9976576">
Ins. Sub. Total
Perfect 9 6 15
Partial 6 6 12
Different 1 6 7
Total 16 18 34
</table>
<tableCaption confidence="0.999703">
Table 1. Degree of syntactic head agreement
</tableCaption>
<bodyText confidence="0.999943642857143">
example. These are the syntactic and aspectual
variants of the same verb 会談する kaidan-suru
‘talk.’
The third row represents cases where the syntac-
tic heads had no common surface words. We
found that even in this case, though, the syntactic
heads were close in some way. In one example,
there was accordance in the distant heads, for
instance, in the pair 見つ かった mitsuka-tta
‘found’ and 一部の ichibu-no ‘part of.’ In this
case, we can find the chunk 見つかった mit-
suka-tta ‘found’ at a short edge distance from 一
部の ichibu-no ‘part of.’ Based on the findings,
we devised a lead sentence revision algorithm.
</bodyText>
<sectionHeader confidence="0.991914" genericHeader="method">
4 Revision algorithm
</sectionHeader>
<subsectionHeader confidence="0.972797">
4.1 Concept
</subsectionHeader>
<bodyText confidence="0.938307068965517">
We explain here the concept of our algorithm
and show an example in Figure 1. We have a
lead sentence and a body sentence, both of which
have the “same” syntactic head chunk, 到着しま
した, touchaku-shima-shi-ta, ‘arrived.’
The head chunk of the lead has two phrases (un-
derlined with thick lines in Figure 1) that directly
modify the head. We call such a phrase a maxi-
mum phrase of a head3. Like the lead sentence,
the body sentence also has two maximum phras-
es. In the following part, we use the term phrase
to refer to a maximum phrase for simplicity.
By comparing the phrases in Figure 1, we notice
that the following operations can add useful in-
formation to the lead sentence; 1) inserting the
first phrase of the body will supply the fact the
visit was on the 4th, 2) substituting the first
phrase of the lead with the second one in the
body adds the detail of the IAEA team. This re-
vision strategy was employed by the human re-
viser mentioned in section 2, and we consider
this to be effective because our target document
has a so-called inverse pyramid structure (Robin
and McKeown 1996), in which the first sentence
is elaborated by the following sentences.
3 To be more precise, a maximum phrase is defined as the
maximum chunk sequence on a dependency path of a head.
チームが 韓CiL 到着しました
the team at Korea arrived
</bodyText>
<figure confidence="0.710563333333333">
到着しました
arrived
maximum phrase
</figure>
<figureCaption confidence="0.999345">
Figure 1. Concept of revision algorithm
</figureCaption>
<bodyText confidence="0.996315833333333">
Further analyzing the above fact, we devised the
lead sentence revision algorithm below. We pre-
sent the outline here and discuss the details in the
next section. We suppose an input pair of a lead
and a body sentence that are syntactically ana-
lyzed.
</bodyText>
<listItem confidence="0.411803">
1) Trigger search
</listItem>
<bodyText confidence="0.938013">
We search for the “same” chunks in the lead
and body sentences. We call the “same”
chunks triggers as they give the starting point
to the revision.
</bodyText>
<listItem confidence="0.741497">
2) Phrase alignment
</listItem>
<bodyText confidence="0.997749">
We identify the maximum phrases of each
trigger, and these phrases are aligned according
to a similarity metric.
</bodyText>
<listItem confidence="0.571929">
3) Substitution
</listItem>
<bodyText confidence="0.999699">
If a body phrase has a corresponding phrase in
the lead, and the body phrase is richer in in-
formation, we substitute the body phrase for
the lead phrase.
</bodyText>
<sectionHeader confidence="0.683021" genericHeader="method">
4) Insertion
</sectionHeader>
<bodyText confidence="0.999933416666667">
If a body phrase has no counterpart in the lead,
that is, the phrase is floating, we insert it into
the lead sentence.
Our method inserts and substitutes any type of
phrase that modifies the trigger and therefore has
no limitation in syntactic type. Although NP
elaboration such as in (Nenkova 2008) is of great
importance, there are other useful syntactic types
for revision. An example is the adverbial phrase
insertion of time and location. The insertion of
the phrase 4 日 yokka ‘on the 4th’ in figure 1 in-
deed adds useful information to the lead sentence.
</bodyText>
<subsectionHeader confidence="0.979178">
4.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.99996925">
The overall flow of the revision algorithm is
shown in Algorithm 1. The inputs are a lead and
a body sentence that are syntactically parsed,
which are denoted by L and B respectively.
The whole algorithm starts with the all-trigger
search in step 1. Revision candidates are then
found for each trigger pair in the main loop from
steps 2 to 6. The revision for each trigger pair is
</bodyText>
<figure confidence="0.936094111111111">
Lead
IAEAD
of the IAEA
Body
4p,
on the 4th
insertion substitution
IAEAD 査察官 5人が
of the IAEA inspectors five
</figure>
<page confidence="0.96564">
41
</page>
<construct confidence="0.394767">
Algorithm 14 (Left figures are the step numbers.)
</construct>
<listItem confidence="0.9478045">
1: find all trigger pairs between L and B and
store them in T.
T={(l, b) ; l≈b, l∈L and b∈B }
2: for all (l, b) ∈ T do
find l’s max phrases and store in Pl.
Pl={pl ; pl ∈ max phrase of l}
3: do the same for trigger b
Pb={pb ; pb ∈ max phrase of b}
4: align phrases in Pl and Pb and store
result in A
</listItem>
<equation confidence="0.555318">
A={( pl, pb) ; pl ↔ pb ,
pl ∈ Pl, pb ∈ Pb }
</equation>
<listItem confidence="0.996898">
5: for all (pl, pb) ∈ A do
follow Table 2
end for
6: end for
</listItem>
<equation confidence="0.845642">
Body
pb =∅ pb ≠ ∅
</equation>
<tableCaption confidence="0.787137">
Table 2. Operations for step 5
</tableCaption>
<bodyText confidence="0.99924">
found based on the idea in the previous section in
steps 4 and 5. Now we explain the main parts.
</bodyText>
<listItem confidence="0.997782">
• Step 1: trigger chunk pair search
</listItem>
<bodyText confidence="0.999589526315789">
We first detect the trigger pairs in step 1 that are
the base of the revision process. What then can
be a trigger pair that yields correct revisions? We
roughly define trigger pairs as the “coreferential”
chunk pairs of all parts of speech, i.e., the parts
of speech that point to the same entity, event,
action, change, and so on.
Notice that the term coreferential is used in an
extended way as it is usually used to describe the
phenomena in noun group pairs (Mitkov, 2002).
The chunk X11*LILfes. touchaku-shimashita
‘arrived’ and IAEA 0D IAEA-no ‘of the IAEA’
in Figure 1 are examples.
Identifying our coreferential chunks is even
harder than the conventional coreference resolu-
tion, and we made a simplifying assumption as in
Nenkova (2008) with some additional conditions
that were obtained through our preliminary ex-
periments.
</bodyText>
<listItem confidence="0.998321">
(1) Assumption: Two chunks having the same
surface forms are coreferential.
(2) Conditions for light verb (noun) chunks:
Agreement of modifying verbal nous is fur-
</listItem>
<page confidence="0.375042">
4 The sign a b means the chunk “a” and “b” are triggers.
</page>
<bodyText confidence="0.905442416666667">
≈
The sign p q means the phrases “p” and “q” are aligned.
↔
ther required for chunks whose content
words consist only of light verbs such as &amp;5
6 aru ‘be’ and 4�6 naru ‘become’: these
chunks themselves have little lexical mean-
ing. The agreement is checked with the
hand-crafted rules. Similar checks are ap-
plied to chunks whose content words consist
only of light nouns such as �L koto (‘koto’
makes the previous verb a noun) .
</bodyText>
<listItem confidence="0.588235222222222">
(3) Conditions for verb inflections: a chunk that
contains a verb usually ends with a function
word series that indicates a variety of infor-
mation such as inflection type, dependency
type, tense, aspect, and modality. Some in-
formation such as tense and aspect is vital to
decide the coreference relation (exchanging
the modifier phrases “arrive” and “will ar-
rive” will likely bring about inconsistency in
meaning), although some is not. We are in
the process of categorizing function words
that do not affect the coreference relation and
temporally adopted the empirically obtained
rule: the difference in verb inflection be-
tween the te-form (predicate modifying
form) and dictionary form (sentence end
form) can be ignored.
• Step 4: phrase alignment
</listItem>
<bodyText confidence="0.981988666666667">
We used the surface form agreement for similar-
ity evaluation. We applied several metrics and
explain them one by one.
</bodyText>
<listItem confidence="0.802938">
1) Chunk similarity t, s
t, s : x, y∈ chunk → [0, 1].
</listItem>
<bodyText confidence="0.85494925">
Function t is the Dice coefficient between the
set of content words in x and those in y. The
same coefficient calculated with all words
(function and content words) is denoted as s.
</bodyText>
<listItem confidence="0.7230005">
2) Phrase absorption ratio
a : px, py∈ phrases → [0, 1]
</listItem>
<bodyText confidence="0.998532666666667">
This is the function that indicates how many
chunks in phrase px is represented in py and is
calculated with t as in,
</bodyText>
<equation confidence="0.952998909090909">
1
a p p
( , ) : = ∑ max( ( , ))
t x y .
x y
x∈ px y∈ py
px
αa px py
( , ) (1 ) ( ,
+ − α s x y
α∈
</equation>
<bodyText confidence="0.82447675">
where the shorter phrase is set to px so that
3) Alignment quality
With the above two functions, the alignment qual-
lity is evaluated by the function
</bodyText>
<figure confidence="0.550359047619048">
g : px, py
phrases
[0,
∈
→
1]
0,1
&lt; py
variables x an
px
. The
d y are the last
Lead
pl≠ ∅
4: no op. 1: insertion
3: no op. 2: substitution
pl =∅
g px py
( , ):=
),
[ ]
</figure>
<page confidence="0.982784">
42
</page>
<bodyText confidence="0.999530375">
chunks in px and py, respectively. Intuitively,
the function evaluates how many chunks in the
shorter phrase px are represented in py and how
similar the last chunks are. The last chunk in a
phrase, especially the function words in the
chunk, determines the syntactic character of
the phrase, and we measured this value with
the second term of the alignment quality. The
parameter α is decided empirically, which was
set at 0.375 in this paper.
In alignment, we calculated the score for all
possible phrase combinations and then greed-
ily selected the pair with the highest score. We
set the minimum alignment score at 0.185;
those pairs with scores lower than this value
were not aligned.
</bodyText>
<listItem confidence="0.980134">
• Step 5 (Table 2, case 1): insertion
</listItem>
<bodyText confidence="0.971494">
Step 5 starts either an insertion or substitution
process, as in Table 2. If pb (body phrase is
</bodyText>
<equation confidence="0.519004666666667">
≠ ∅
not null) and pl = (lead phrase is null) in Table
∅
</equation>
<bodyText confidence="0.8820745">
2, the insertion process starts.
In this process, we check the following.
</bodyText>
<listItem confidence="0.554835">
1) Redundancy check
</listItem>
<bodyText confidence="0.9915062">
Insertion may cause redundancy in informa-
tion. As a matter of fact, redundancy often
happens when there is an error in syntactic
analysis. Suppose there are the same lead and
body phrases that modify the same chunks in
the lead and body sentences. If the lead phrase
fails to modify the correct chunk because of an
error, the body phrase loses the chance to be
aligned to the lead phrase since they belong to
different trigger chunks. As a result, the body
phrase becomes a floating phrase and is in-
serted into the lead chunk, which duplicates
the same phrase.
To prevent this, we evaluate the degree of du-
plication with the phrase absorption ratio a
and allow phrase insertion when the score is
below a predefined threshold B : we allow in-
sertion when
a(pb , L) &lt; B, pb E phrase, L : lead sentence,
is satisfied.
</bodyText>
<listItem confidence="0.747632">
2) Discourse coherence check
</listItem>
<bodyText confidence="0.978316517241379">
Blind phrase insertion may invite a break in
cohesion in a lead sentence. This frequently
happens when the inserted phrase has words
that require an antecedent. We then prepared a
list of words that contain such context-
requiring words and forbid phrase insertions
that contain words that are on the list. This list
contains the pronoun family such as この ko-
kono ‘this’ and special adjectives such as 違う
chigau ‘different.’
3) Insertion point decision
The body phrase should be inserted at the
proper position in the lead sentence to main-
tain the syntactic consistency. Because we
dealt with single-phrase insertion here, we
employed a simple heuristics.
Since the Japanese dependency edge spans
from left to right as we mentioned in section 1,
we considered that the right phrase of the in-
serted phrase is important to keep the new de-
pendency from the inserted phrase to the trig-
ger chunk. Because we already know the
phrase alignment status at this stage, we fol-
low the next steps to determine the insertion
position in the lead of the insertion phrase.
A) In the body sentence, find the nearest right
substitution phrase pr of the insertion
phrase.
B) Find the pr’s aligned phrase in the lead prL.
</bodyText>
<listItem confidence="0.83213775">
C) Insert the phrase to the left of the prL.
D) If there is no pr, insert the phrase to the left
to the trigger.
• Step 5 (Table 2, case 2): substitution
</listItem>
<bodyText confidence="0.99809795">
If pb ≠ ∅ and pl ≠ ∅ in Table 2, the substitu-
tion process starts. This process first checks if
each aligned phrase pair contains the same chunk
other than the present trigger. If there is such a
chunk, the substitution phrase is reduced to the
subtree from the present trigger to the identical
chunk. The newly found identical chunks are in
trigger table T, and the remaining part will be
evaluated later in the main loop. Owing to the
phrase partitioning, we can avoid phrase substi-
tutions which are in an inclusive relation.
The substitution candidate goes through three
checks: information increase, redundancy, and
discourse cohesion. As the latter two are almost
the same as those in the insertion, we explain
here the information increase. This involves
checking whether the number of chunks in the
body phrase is greater than that in the aligned
lead phrase. This is based on the simple assump-
tion that elaboration requires more words.
</bodyText>
<sectionHeader confidence="0.995723" genericHeader="method">
5 Revision experiments
</sectionHeader>
<subsectionHeader confidence="0.824584">
5.1 Data and evaluation steps
</subsectionHeader>
<listItem confidence="0.956906">
• Purpose
</listItem>
<bodyText confidence="0.995932">
We conducted a lead revision experiment with
three purposes. The first one was to empirically
evaluate the validity of our simplified assump-
</bodyText>
<page confidence="0.999509">
43
</page>
<bodyText confidence="0.99972325">
tions: trigger identification and concreteness in-
crease evaluation. For trigger identification, we
basically viewed the identical chunks as triggers
and added some amendments for light verbs
(nouns) and verb inflections. For the check of an
increase in concreteness, we assumed that
phrases with more chunks were more concrete.
However, these simplifications should be veri-
fied in experiments.
The second purpose was to check the validity of
using the revision phrases only in body sentences
and not in the supplemental sentences.
The last one was to determine how ineffective
the result is if the syntactic parsing fails. With
these purposes in mind, we designed our experi-
ment as follows.
</bodyText>
<listItem confidence="0.982203">
• Data
</listItem>
<bodyText confidence="0.989556428571429">
A total of 257 articles from news programs
broadcast on 20 Jan., 20 Apr., and 20 July in
2004 were tagged with lead, body, and supple-
ment tags by a native Japanese evaluator. The
articles were morphologically analyzed by Me-
cab (Kudo et al., 2003) and syntactically parsed
by Cabocha (Kudo and Matsumoto, 2002).
</bodyText>
<listItem confidence="0.989045">
• Evaluator and evaluation detail
</listItem>
<bodyText confidence="0.9998784">
We prepared an evaluation interface that presents
a lead with one revision point (insertion or sub-
stitution) that was obtained using the body and
supplemental sentences to an evaluator.
A Japanese native speaker evaluated the results
one by one with the above interface. We planned
a linguistic evaluation like DUC2005 (Hoa Trang,
2005). Since their five-type evaluation is in-
tended for multi-document summarization,
whereas our task is single-document summariza-
tion, and we are interested in evaluating our
questions mentioned above, we carried out the
evaluation as follows. In future, we plan to in-
crease the number of evaluation items and the
number of evaluators.
</bodyText>
<tableCaption confidence="0.998884">
Table 3. Evaluation of increased concreteness
</tableCaption>
<table confidence="0.97233075">
Completeness Required operations Score
Poor More than 2 0
Acceptable One 1
Perfect None 2
</table>
<tableCaption confidence="0.999499">
Table 4. Sentential completeness
</tableCaption>
<bodyText confidence="0.9921834375">
E1) The evaluator judged if the revision was ob-
tained from the lead and body sentences with
or without parsing errors. Here, errors that did
not affect the revision were not considered.
E2) Second, she checked whether the revision
was semantically correct or revised informa-
tion matching the fact described in the lead
sentence. Here, she did not care about the
grammaticality or the improvements in con-
creteness of the revision; if the revision was
problematic but manually correctable, it was
judged as OK. This step evaluated the correct-
ness of the trigger selection; wrong triggers,
i.e., those referring to different facts produce
semantically inconsistent revisions as they mix
up different facts.
The following evaluation was done for those
judged correct in evaluation step E2, as we found
that revisions that were semantically inconsistent
with the lead’s facts were often too difficult to
evaluate further.
E3) Third, she evaluated the change in concrete-
ness after revision with the revisions that
passed evaluation E2. She judged whether or
not the revision increased the concreteness of
the lead in three categories (Table 3).
Notice that original lead sentences are sup-
posed to have an average score of 1.
E4) Last, she checked the sentential complete-
ness of the revision result that passed evalua-
tion E2. They still contained problems such as
grammatical errors and improper insertion po-
sition. Rather than evaluating these items sepa-
rately, we measured them together for senten-
tial completeness. At this time, we measured in
terms of the number of operations (insertion,
deletion, substitution) needed to make the sen-
tence complete5.
As shown in Table 4, revisions requiring more
than two operations are categorized as “poor,”
those requiring one operation are “acceptable,”
and those requiring no operations are “perfect.”
We employed this measure because we found
that grading detailed items such as grammatical-
ity and insertion positions at fine levels was
rather difficult. We also found that native Japa-
nese speakers can correct errors easily. Notice
the lead sentences are perfect and are supposed
</bodyText>
<footnote confidence="0.533435666666667">
5 This was not an automatic process and may not be perfect.
The evaluator simulated the correction in mind and judged
whether it was done with one action.
</footnote>
<figure confidence="0.993254714285714">
Concreteness
Score
Decreased
Unchanged
Increased
0
1
</figure>
<page confidence="0.8035295">
2
44
</page>
<bodyText confidence="0.99974725">
to have an average score of 2 in sentential com-
pleteness. Since the revision does not improve
the completeness further but elicits defects such
as grammatical errors, it usually produces a score
below 2. Some examples of the results with their
scores are shown below. The underlined parts are
the inserted body chunk phrases, and the paren-
thesized parts are the deleted lead chunks.
</bodyText>
<figure confidence="0.58415625">
1) Concreteness 2, Completeness 2
民間団体の「コリア・ソサエティ」な
どが主催する「朝鮮半島平和フォーラ
ム」に(催しに)出席する...
</figure>
<listItem confidence="0.776464235294118">
minkan-dantai-no ‘private
organization’, korea-
society-nado-ga ‘Korea Soci-
ety and others’, shusai-suru
‘sponsored’, chousen-hantou-
heiwa-forumu-ni ‘Peace Fo-
rum in Korean Peninsula’,
(moyooshi-ni ‘event’),
shusseki-suru ‘attend’
2) Concreteness 1, Completeness 2
部品に亀裂が入っているのが()見つ
かった...
buhin-ni ‘to the parts’ ki-
retsu-ga ‘cracks’, haitte-
iru-no-ga ‘being there’ (),
mitsuka-tta ‘found’
3) Concreteness 2, Completeness 0
</listItem>
<bodyText confidence="0.980045166666666">
ヘリコプターから地上二十メートルの
高さから()落下し死亡しました。
Herikoputa-kara ‘from a hel-
icopter’, chijou-niju-
metoru-no-takasa-kara ‘from
20 meters high’ (), rakka-
shi ‘fell and’, shibou-
shima-shita ‘killed’
Example 1 is the perfect substitution and had
scores of 2 for both concreteness increase and
completeness. Actually, the originally vaguely
mentioned term ‘event’ was replaced by a more
concrete phrase with proper names, ‘Korean Pen-
insula Peace Forum sponsored by Korea Society
and others.’ Notice that this can be achieved by
NP coreference based methods if they can iden-
tify that these two different phrases are corefer-
ential. Our method does this through the depend-
ency on the same trigger ,&apos;i; r ,t;5 shusseki-suru
‘attend.’
Example 2 is a perfect sentence, but its concrete-
ness stayed at the same level. As a result, the
scores were 1 for concreteness increase and 2 for
completeness.
</bodyText>
<table confidence="0.9995272">
Incorrect Correct Cor. Ratio
Parse Succ. 70 353 0.83
Fail. 31 149 0.83
Sent. Body 50 464 0.90
Supp. 51 38 0.43
</table>
<tableCaption confidence="0.996306">
Table 5. Results of semantic correctness
</tableCaption>
<table confidence="0.9904764">
Score 0 1 2 Ave.
Parse Succ. 0 55 298 1.84
Fail. 1 19 129 1.86
Sent. Body 1 61 402 1.86
Supp. 0 13 25 1.66
</table>
<tableCaption confidence="0.842497">
Table 6. Results of concreteness increase
</tableCaption>
<table confidence="0.9986148">
Score 0 1 2 Ave.
Parse Succ. 78 60 215 1.39
Fail. 66 55 28 0.74
Sent. Body 120 110 234 1.25
Supp. 24 5 9 0.61
</table>
<tableCaption confidence="0.999562">
Table 7. Results of sentential completeness
</tableCaption>
<bodyText confidence="0.999766470588235">
Actually, the original sentence that meant “They
found a crack in the parts” was revised to “They
found there was a crack in the parts,” which did
not add useful information. Example 3 has a
grammatical problem although the revision sup-
plied useful information.As a result, it had scores
of 2 for concreteness increase and 0 for com-
pleteness. The added kara-case phrase (from
phrase) WE—+,� l ,11-0)AL-7�&apos;6 chijou-
niju-metoru-no-takasa-kara ‘from 20 meters
high’ is useful, but since the original sentence
already has the kara-case --1-:373 �6
herikoputa-kara ‘from helicopter,’ the insertion
invited a double kara-case, which is forbidden in
Japanese. To correct the error, we need at least
two operations, and thus, a completeness score of
0 was assigned.
</bodyText>
<subsectionHeader confidence="0.991961">
5.2 Results of experiments
</subsectionHeader>
<bodyText confidence="0.999952071428572">
Table 5 presents the results of evaluation E2, the
semantic correctness with the parsing status of
evaluation E1 and the source sentence category
from which the phrases for revision were ob-
tained. Columns 2 and 3 list the number of revi-
sions (insertions and substitutions) that were cor-
rect and incorrect and column 4 shows the cor-
rectness ratio. We obtained a total of 603 revi-
sions and found that 30% (180/603) of them
were derived with syntactic errors.
The semantic correctness ratio was unchanged
regardless of the parsing success. On the contrary,
it was affected by the source sentence type. The
correctness ratio with the supplemental sentence
</bodyText>
<page confidence="0.997608">
45
</page>
<bodyText confidence="0.99987874">
was significantly6 lower than that with the body
sentence. Table 6 lists the results of the con-
creteness improvements with the parsing status
and the source sentence type. Columns 2, 3 and 4
list the number of revisions that fell in the scores
(0-2) listed in the first row. The average score in
this table again was not affected by the parsing
failure but was significantly affected by the
source sentence category. The result with the
supplement sentences was significantly worse
than that with body sentences.
Table 7 lists the results of the sentential com-
pleteness in the same fashion as Table 6. The
sentential completeness was significantly wors-
ened by both the parsing failure and source sen-
tence category.
These results indicate that the answers to the
questions posed at the beginning of this section
are as follows. From the semantic correctness
evaluation, we infer that our trigger selection
strategy worked well especially when the source
sentence category was limited to the body.
From the concreteness-increase evaluation, the
assumption that we made also worked reasonably
well when the source sentence category was lim-
ited to the body.
The effect of parsing was much more limited
than we had anticipated in that it did not degrade
either the semantic correctness or the concrete-
ness improvements. Parsing failure, however,
degraded the sentential completeness of the re-
vised sentences. This seems quite reasonable:
parsing errors elicit problems such as wrong
phrase attachment and wrong maximum phrase
identification. The revisions with these errors
invite incomplete sentences that need corrections.
It is worth noting that cases sometimes occurred
where a parsing error did not cause any problem
in the revision. We found that the phrases gov-
erned by a trigger pair in many cases were quite
similar, and therefore, the parser makes the same
error. In that case, the errors are often offset and
cause no problems superficially.
We consider that the sentential completeness
needs further improvements to make an auto-
matic summarization system, although the se-
mantic correctness and concreteness increase are
at an almost satisfactory level. Our dependency-
based revision is expected to be potentially use-
ful to develop a summarization system.
</bodyText>
<footnote confidence="0.862183666666667">
6 In this section, the “significance” was tested with the
Mann-Whitney U test with Fisher’s exact probability. We
set the significance level at 5%.
</footnote>
<sectionHeader confidence="0.99695" genericHeader="method">
6 Future work
</sectionHeader>
<bodyText confidence="0.999979457142857">
Several problems remain to be solved, which will
be addressed in future work. Obviously, we need
to improve the parsing accuracy that degraded
the sentential completeness in our experiments.
Although we did not quantitatively evaluate the
errors in phrase insertion position and redun-
dancy, we could see these happening in the re-
vised sentences because of the inaccurate parsing.
Apart from this, we need to further refine the
following problems.
Regarding the trigger selection, one particular
problem we faced was the mixture of statements
of different politicians in a news article. The
statements were often included as direct quota-
tions that end with the chunk 述べまLた nobe-
mashi-ta ‘said.’ Our system takes the chunk as
the trigger and does not care whose statements
they are; thus, it ended up mixing them up. A
similar problem happened when we had two dif-
ferent female victims of an incident in an article.
Since our system has no means to distinguish
them, the modifier phrases about these women
were mixed up.
We think that we can improve our method by
applying more general language generation tech-
niques. An example is the kara-case collision that
we explained in example 3 in section 5.1. The
essence of the problem is that the added content
is useful, but there is a grammatical problem. In
other words, “what to say” is ok but “how to
say” needs refinement. This particular problem
can be solved by doing the case-collision check,
and by synthesizing the colliding phrases into
one. These can be better treated in the generation
framework.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999876230769231">
We proposed a lead sentence revision method
based on the operations of phrases that have the
same head in the lead and other sentences. This
method is a type of sentence fusion and is more
general than methods that use noun phrase
coreferencing in that it can add phrases of any
syntactic type. We described the algorithm and
the rules extensively, conducted a lead revision
experiment, and showed that the algorithm was
able to find semantically appropriate revisions.
We also showed that parsing errors mainly de-
grade the sentential completeness such as gram-
maticality and repetition.
</bodyText>
<page confidence="0.999075">
46
</page>
<sectionHeader confidence="0.936542" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.99975929787234">
Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence Fusion for Multidocument News Summa-
rization. Computational Linguistics. 31(3): 298-
327.
Katja Filippova and Michael Strube. 2008. Sentence
Fusion via Dependency Graph Compression. proc.
of the EMNLP 2008: 177-185
Hongyan Jing and Kathleen R. McKeown. 1999. The
Decomposition of Human-Written Summary Sen-
tences. proc. of the 22nd International Conference
on Research and Development in Information Re-
trieval SIGIR 99: 129-136.
Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and Paste Based Text Summarization, proc. of the
1st meeting of the North American Chapter of the
Association for Computational Linguistics: 178-
185.
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. Proc.
of the 6th Conference on Natural Language Learn-
ing 2002: 63-69.
Taku Kudo, Kaoru Yamamoto and Yuji Matsumoto.
2004. Applying Conditional Random Fields to Jap-
anese Morphological Analysis, proc. of the
EMNLP 2004: 230-237.
H. P. Luhn. 1958. The Automatic Creation of Litera-
ture Abstracts. Advances in Automatic Text Sum-
marization. The MIT Press: 15-21.
Inderjeet Mani, Barbara Gates, and Eric Bloedorn.
1999. Improving Summaries by Revising Them.
Proc. of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics.: 558-565.
Ruslan Mitkov 2002, Anaphora Resolution, Pearson
Education.
Ani Nenkova. 2008. Entity-driven Rewrite for Multi-
document Summarization, proc. of the 3rd Interna-
tional Joint Conference on Natural Language Gen-
eration: 118-125.
Jahna C. Otterbacher, Dragomir R. Radev, and Airong
Luo 2002, Revisions that Improve Cohesion in
Multi-document Summaries: A Preliminary Study.
Proc. of the ACL-02 Workshop on Automatic
Summarization: 27-36.
Jacques Robin and Kathleen McKeown. 1996. Em-
pirically designing and evaluating a new revision-
based model for summary generation. Artificial In-
telligence. 85: 135-179.
</reference>
<page confidence="0.999491">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.520551">
<title confidence="0.999896">Syntax-Driven Sentence Revision for Broadcast News Summarization</title>
<author confidence="0.780754">Hideki Tanaka</author>
<author confidence="0.780754">Akinori Kinoshita</author>
<author confidence="0.780754">Takeshi Tadashi Kumano</author>
<author confidence="0.780754">Naoto</author>
<affiliation confidence="0.998446">NHK Science and Technology Research</affiliation>
<address confidence="0.996215">1-10-11, Kinuta, Setagaya-ku, Tokyo, Japan</address>
<email confidence="0.979694">tanaka.h-ja@nhk.or.jp</email>
<email confidence="0.979694">kinoshita.a-ek@nhk.or.jp</email>
<email confidence="0.979694">kobayakawa-t.ko@nhk.or.jp</email>
<email confidence="0.979694">kumano.t-eq@nhk.or.jp</email>
<email confidence="0.979694">kato.n-ga@nhk.or.jp</email>
<abstract confidence="0.993529333333333">We propose a method of revising lead sentences in a news broadcast. Unlike many other methods proposed so far, this method does not use the coreference relation of noun phrases (NPs) but rather, insertion and substitution of the phrases modifying the same head chunk in lead and other sentences. The method borrows an idea from the sentence fusion methods and is more general than those using NP coreferencing as ours includes them. We show in experiments the method was able to find semantically appropriate revisions thus demonstrating its basic feasibility. We also show that that parsing errors mainly degraded the sentential completeness such as grammaticality and redundancy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence Fusion for Multidocument News Summarization. Computational Linguistics.</title>
<date>2005</date>
<volume>31</volume>
<issue>3</issue>
<pages>298--327</pages>
<contexts>
<context position="3120" citStr="Barzilay and McKeown (2005)" startWordPosition="472" endWordPosition="475">cularly conspicuous in multi-document summarization. To ameliorate this, revision of the extracted sentences is also thought to be effective, and many ideas and methods have been proposed so far. For example, Otterbacher and colleagues (2002) analyzed manually revised extracts and factored out cohesion problems. Nenkova (2008) proposed a revision idea that utilizes noun coreference with linguistic quality improvements in mind. Other than the break in cohesion, multidocument summarization faces the problem of information overlap particularly when the document set consists of similar sentences. Barzilay and McKeown (2005) proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a nonoverlapping summary sentence. Their algorithm firstly analyzes the sentences to obtain the dependency trees and sets a basis tree by finding the centroid of the dependency trees. It next augments the basis tree with the sub-trees in other sentences and finally prunes the predefined constituents. Their algorithm was further modified and applied to the German biographies by Filippova and Strube (2008). Like the work of Jing and McKeown (2000) and Mani et al. (1999), our work was inspired</context>
<context position="6696" citStr="Barzilay and McKeown (2005)" startWordPosition="1065" endWordPosition="1068"> lead or the body is placed in the supplement. The research also reports that professional news abstractors who are hired for digital text services summarize articles in a two-step approach. First, they identify the lead sentences and set it (them) as the starting point of the summary. As the average lead length is 95 characters and the al1 This is the kakari-uke (modifier-modifiee) relation of Japanese, which differs from the conventional dependency relation. We use the term dependency for convenience in this paper. 2 This is the other way around compared to the English dependency such as in Barzilay and McKeown (2005). lowed summary length is about 115 characters (or 150 characters depending on the screen design), they revise the lead sentences using expressions from the remainder of the story. We see here that the extraction and revision strategy that has been extensively studied by many researchers for various reasons was actually applied by human abstractors, and therefore, the strategy can be used as a real summarization model. Inspired by this, we decided to study a news summarization system based on the above approach. To develop a complete summarization system, we have to solve three problems: 1) id</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence Fusion for Multidocument News Summarization. Computational Linguistics. 31(3): 298-327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Sentence Fusion via Dependency Graph Compression.</title>
<date>2008</date>
<booktitle>proc. of the EMNLP</booktitle>
<pages>177--185</pages>
<contexts>
<context position="3632" citStr="Filippova and Strube (2008)" startWordPosition="552" endWordPosition="555">f information overlap particularly when the document set consists of similar sentences. Barzilay and McKeown (2005) proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a nonoverlapping summary sentence. Their algorithm firstly analyzes the sentences to obtain the dependency trees and sets a basis tree by finding the centroid of the dependency trees. It next augments the basis tree with the sub-trees in other sentences and finally prunes the predefined constituents. Their algorithm was further modified and applied to the German biographies by Filippova and Strube (2008). Like the work of Jing and McKeown (2000) and Mani et al. (1999), our work was inspired by the summarization method used by human abstractors. Actually, our abstractors first extract important sentences, which is called lead identification, and then revise them, which is referred to as phrase elaboration or specification. In this paper, we concentrate on the revision part. Our work can be viewed as an application of the sentence fusion method to the draft and revision 39 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 39–47, Suntec, Singapore,</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Katja Filippova and Michael Strube. 2008. Sentence Fusion via Dependency Graph Compression. proc. of the EMNLP 2008: 177-185</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen R McKeown</author>
</authors>
<title>The Decomposition of Human-Written Summary Sentences.</title>
<date>1999</date>
<booktitle>proc. of the 22nd International Conference on Research and Development in Information Retrieval SIGIR 99:</booktitle>
<pages>129--136</pages>
<contexts>
<context position="2026" citStr="Jing and McKeown (1999" startWordPosition="309" endWordPosition="312">se a method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences. We also show its effectiveness in a revision experiment. As is well known, the extractive summary that has been extensively studied from the early days of summarization history (Luhn, 1958) suffers from various drawbacks. These include the problems of a break in cohesion in the summary text such as dangling anaphora and a sudden shift in topic. To ameliorate these problems, the idea of revising the extracted sentences was proposed in a single document summarization study. Jing and McKeown (1999; 2000) found that human summarization can be traced back to six cut-andpaste operations of a text and proposed a revision method consisting of sentence reduction and combination modules with a sentence extraction part. Mani and colleagues (1999) proposed a summarization system based on “draft and revision” together with sentence extraction. The revision part is achieved with the sentence aggregation and smoothing modules. The cohesion break problem becomes particularly conspicuous in multi-document summarization. To ameliorate this, revision of the extracted sentences is also thought to be ef</context>
</contexts>
<marker>Jing, McKeown, 1999</marker>
<rawString>Hongyan Jing and Kathleen R. McKeown. 1999. The Decomposition of Human-Written Summary Sentences. proc. of the 22nd International Conference on Research and Development in Information Retrieval SIGIR 99: 129-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Cut and Paste Based Text Summarization,</title>
<date>2000</date>
<booktitle>proc. of the 1st meeting of the North American Chapter of the Association for Computational Linguistics:</booktitle>
<pages>178--185</pages>
<contexts>
<context position="3674" citStr="Jing and McKeown (2000)" startWordPosition="560" endWordPosition="563">cument set consists of similar sentences. Barzilay and McKeown (2005) proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a nonoverlapping summary sentence. Their algorithm firstly analyzes the sentences to obtain the dependency trees and sets a basis tree by finding the centroid of the dependency trees. It next augments the basis tree with the sub-trees in other sentences and finally prunes the predefined constituents. Their algorithm was further modified and applied to the German biographies by Filippova and Strube (2008). Like the work of Jing and McKeown (2000) and Mani et al. (1999), our work was inspired by the summarization method used by human abstractors. Actually, our abstractors first extract important sentences, which is called lead identification, and then revise them, which is referred to as phrase elaboration or specification. In this paper, we concentrate on the revision part. Our work can be viewed as an application of the sentence fusion method to the draft and revision 39 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 39–47, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP appro</context>
</contexts>
<marker>Jing, McKeown, 2000</marker>
<rawString>Hongyan Jing and Kathleen R. McKeown. 2000. Cut and Paste Based Text Summarization, proc. of the 1st meeting of the North American Chapter of the Association for Computational Linguistics: 178-185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese Dependency Analysis using Cascaded Chunking.</title>
<date>2002</date>
<booktitle>Proc. of the 6th Conference on Natural Language Learning</booktitle>
<pages>63--69</pages>
<contexts>
<context position="22172" citStr="Kudo and Matsumoto, 2002" startWordPosition="3833" endWordPosition="3836">xperiments. The second purpose was to check the validity of using the revision phrases only in body sentences and not in the supplemental sentences. The last one was to determine how ineffective the result is if the syntactic parsing fails. With these purposes in mind, we designed our experiment as follows. • Data A total of 257 articles from news programs broadcast on 20 Jan., 20 Apr., and 20 July in 2004 were tagged with lead, body, and supplement tags by a native Japanese evaluator. The articles were morphologically analyzed by Mecab (Kudo et al., 2003) and syntactically parsed by Cabocha (Kudo and Matsumoto, 2002). • Evaluator and evaluation detail We prepared an evaluation interface that presents a lead with one revision point (insertion or substitution) that was obtained using the body and supplemental sentences to an evaluator. A Japanese native speaker evaluated the results one by one with the above interface. We planned a linguistic evaluation like DUC2005 (Hoa Trang, 2005). Since their five-type evaluation is intended for multi-document summarization, whereas our task is single-document summarization, and we are interested in evaluating our questions mentioned above, we carried out the evaluation</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese Dependency Analysis using Cascaded Chunking. Proc. of the 6th Conference on Natural Language Learning 2002: 63-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<date>2004</date>
<booktitle>Applying Conditional Random Fields to Japanese Morphological Analysis, proc. of the EMNLP 2004:</booktitle>
<pages>230--237</pages>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto and Yuji Matsumoto. 2004. Applying Conditional Random Fields to Japanese Morphological Analysis, proc. of the EMNLP 2004: 230-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The Automatic Creation of Literature Abstracts. Advances in Automatic Text Summarization.</title>
<date>1958</date>
<pages>15--21</pages>
<publisher>The MIT Press:</publisher>
<contexts>
<context position="1716" citStr="Luhn, 1958" startWordPosition="259" endWordPosition="260">e the amount of background information in the lead. This is one of the draft and revision approaches to summarization, which has received keen attention in the research community. Unlike many other methods that directly utilize noun phrase (NP) coreference (Nenkova 2008; Mani et al. 1999), we propose a method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences. We also show its effectiveness in a revision experiment. As is well known, the extractive summary that has been extensively studied from the early days of summarization history (Luhn, 1958) suffers from various drawbacks. These include the problems of a break in cohesion in the summary text such as dangling anaphora and a sudden shift in topic. To ameliorate these problems, the idea of revising the extracted sentences was proposed in a single document summarization study. Jing and McKeown (1999; 2000) found that human summarization can be traced back to six cut-andpaste operations of a text and proposed a revision method consisting of sentence reduction and combination modules with a sentence extraction part. Mani and colleagues (1999) proposed a summarization system based on “d</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H. P. Luhn. 1958. The Automatic Creation of Literature Abstracts. Advances in Automatic Text Summarization. The MIT Press: 15-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Barbara Gates</author>
<author>Eric Bloedorn</author>
</authors>
<title>Improving Summaries by Revising Them.</title>
<date>1999</date>
<booktitle>Proc. of the 37th Annual Meeting of the Association for Computational Linguistics.:</booktitle>
<pages>558--565</pages>
<contexts>
<context position="1394" citStr="Mani et al. 1999" startWordPosition="203" endWordPosition="206">he method was able to find semantically appropriate revisions thus demonstrating its basic feasibility. We also show that that parsing errors mainly degraded the sentential completeness such as grammaticality and redundancy. 1 Introduction We address the problem of revising the lead sentence in a broadcast news text to increase the amount of background information in the lead. This is one of the draft and revision approaches to summarization, which has received keen attention in the research community. Unlike many other methods that directly utilize noun phrase (NP) coreference (Nenkova 2008; Mani et al. 1999), we propose a method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences. We also show its effectiveness in a revision experiment. As is well known, the extractive summary that has been extensively studied from the early days of summarization history (Luhn, 1958) suffers from various drawbacks. These include the problems of a break in cohesion in the summary text such as dangling anaphora and a sudden shift in topic. To ameliorate these problems, the idea of revising the extracted sentences was proposed in a single document summarizati</context>
<context position="3697" citStr="Mani et al. (1999)" startWordPosition="565" endWordPosition="568">ar sentences. Barzilay and McKeown (2005) proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a nonoverlapping summary sentence. Their algorithm firstly analyzes the sentences to obtain the dependency trees and sets a basis tree by finding the centroid of the dependency trees. It next augments the basis tree with the sub-trees in other sentences and finally prunes the predefined constituents. Their algorithm was further modified and applied to the German biographies by Filippova and Strube (2008). Like the work of Jing and McKeown (2000) and Mani et al. (1999), our work was inspired by the summarization method used by human abstractors. Actually, our abstractors first extract important sentences, which is called lead identification, and then revise them, which is referred to as phrase elaboration or specification. In this paper, we concentrate on the revision part. Our work can be viewed as an application of the sentence fusion method to the draft and revision 39 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 39–47, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP approach to a single Japanes</context>
</contexts>
<marker>Mani, Gates, Bloedorn, 1999</marker>
<rawString>Inderjeet Mani, Barbara Gates, and Eric Bloedorn. 1999. Improving Summaries by Revising Them. Proc. of the 37th Annual Meeting of the Association for Computational Linguistics.: 558-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Anaphora Resolution,</title>
<date>2002</date>
<publisher>Pearson Education.</publisher>
<contexts>
<context position="14143" citStr="Mitkov, 2002" startWordPosition="2406" endWordPosition="2407">p 5 found based on the idea in the previous section in steps 4 and 5. Now we explain the main parts. • Step 1: trigger chunk pair search We first detect the trigger pairs in step 1 that are the base of the revision process. What then can be a trigger pair that yields correct revisions? We roughly define trigger pairs as the “coreferential” chunk pairs of all parts of speech, i.e., the parts of speech that point to the same entity, event, action, change, and so on. Notice that the term coreferential is used in an extended way as it is usually used to describe the phenomena in noun group pairs (Mitkov, 2002). The chunk X11*LILfes. touchaku-shimashita ‘arrived’ and IAEA 0D IAEA-no ‘of the IAEA’ in Figure 1 are examples. Identifying our coreferential chunks is even harder than the conventional coreference resolution, and we made a simplifying assumption as in Nenkova (2008) with some additional conditions that were obtained through our preliminary experiments. (1) Assumption: Two chunks having the same surface forms are coreferential. (2) Conditions for light verb (noun) chunks: Agreement of modifying verbal nous is fur4 The sign a b means the chunk “a” and “b” are triggers. ≈ The sign p q means th</context>
</contexts>
<marker>Mitkov, 2002</marker>
<rawString>Ruslan Mitkov 2002, Anaphora Resolution, Pearson Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
</authors>
<title>Entity-driven Rewrite for Multidocument Summarization,</title>
<date>2008</date>
<booktitle>proc. of the 3rd International Joint Conference on Natural Language Generation:</booktitle>
<pages>118--125</pages>
<contexts>
<context position="1375" citStr="Nenkova 2008" startWordPosition="201" endWordPosition="202"> experiments the method was able to find semantically appropriate revisions thus demonstrating its basic feasibility. We also show that that parsing errors mainly degraded the sentential completeness such as grammaticality and redundancy. 1 Introduction We address the problem of revising the lead sentence in a broadcast news text to increase the amount of background information in the lead. This is one of the draft and revision approaches to summarization, which has received keen attention in the research community. Unlike many other methods that directly utilize noun phrase (NP) coreference (Nenkova 2008; Mani et al. 1999), we propose a method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences. We also show its effectiveness in a revision experiment. As is well known, the extractive summary that has been extensively studied from the early days of summarization history (Luhn, 1958) suffers from various drawbacks. These include the problems of a break in cohesion in the summary text such as dangling anaphora and a sudden shift in topic. To ameliorate these problems, the idea of revising the extracted sentences was proposed in a single d</context>
<context position="2821" citStr="Nenkova (2008)" startWordPosition="431" endWordPosition="432">dules with a sentence extraction part. Mani and colleagues (1999) proposed a summarization system based on “draft and revision” together with sentence extraction. The revision part is achieved with the sentence aggregation and smoothing modules. The cohesion break problem becomes particularly conspicuous in multi-document summarization. To ameliorate this, revision of the extracted sentences is also thought to be effective, and many ideas and methods have been proposed so far. For example, Otterbacher and colleagues (2002) analyzed manually revised extracts and factored out cohesion problems. Nenkova (2008) proposed a revision idea that utilizes noun coreference with linguistic quality improvements in mind. Other than the break in cohesion, multidocument summarization faces the problem of information overlap particularly when the document set consists of similar sentences. Barzilay and McKeown (2005) proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a nonoverlapping summary sentence. Their algorithm firstly analyzes the sentences to obtain the dependency trees and sets a basis tree by finding the centroid of the dependency trees. It next augm</context>
<context position="12263" citStr="Nenkova 2008" startWordPosition="2031" endWordPosition="2032"> Phrase alignment We identify the maximum phrases of each trigger, and these phrases are aligned according to a similarity metric. 3) Substitution If a body phrase has a corresponding phrase in the lead, and the body phrase is richer in information, we substitute the body phrase for the lead phrase. 4) Insertion If a body phrase has no counterpart in the lead, that is, the phrase is floating, we insert it into the lead sentence. Our method inserts and substitutes any type of phrase that modifies the trigger and therefore has no limitation in syntactic type. Although NP elaboration such as in (Nenkova 2008) is of great importance, there are other useful syntactic types for revision. An example is the adverbial phrase insertion of time and location. The insertion of the phrase 4 日 yokka ‘on the 4th’ in figure 1 indeed adds useful information to the lead sentence. 4.2 Algorithm The overall flow of the revision algorithm is shown in Algorithm 1. The inputs are a lead and a body sentence that are syntactically parsed, which are denoted by L and B respectively. The whole algorithm starts with the all-trigger search in step 1. Revision candidates are then found for each trigger pair in the main loop f</context>
<context position="14412" citStr="Nenkova (2008)" startWordPosition="2446" endWordPosition="2447">correct revisions? We roughly define trigger pairs as the “coreferential” chunk pairs of all parts of speech, i.e., the parts of speech that point to the same entity, event, action, change, and so on. Notice that the term coreferential is used in an extended way as it is usually used to describe the phenomena in noun group pairs (Mitkov, 2002). The chunk X11*LILfes. touchaku-shimashita ‘arrived’ and IAEA 0D IAEA-no ‘of the IAEA’ in Figure 1 are examples. Identifying our coreferential chunks is even harder than the conventional coreference resolution, and we made a simplifying assumption as in Nenkova (2008) with some additional conditions that were obtained through our preliminary experiments. (1) Assumption: Two chunks having the same surface forms are coreferential. (2) Conditions for light verb (noun) chunks: Agreement of modifying verbal nous is fur4 The sign a b means the chunk “a” and “b” are triggers. ≈ The sign p q means the phrases “p” and “q” are aligned. ↔ ther required for chunks whose content words consist only of light verbs such as &amp;5 6 aru ‘be’ and 4�6 naru ‘become’: these chunks themselves have little lexical meaning. The agreement is checked with the hand-crafted rules. Similar</context>
</contexts>
<marker>Nenkova, 2008</marker>
<rawString>Ani Nenkova. 2008. Entity-driven Rewrite for Multidocument Summarization, proc. of the 3rd International Joint Conference on Natural Language Generation: 118-125.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jahna C Otterbacher</author>
<author>Dragomir R Radev</author>
</authors>
<title>and Airong Luo 2002, Revisions that Improve Cohesion in Multi-document Summaries: A Preliminary Study.</title>
<booktitle>Proc. of the ACL-02 Workshop on Automatic Summarization:</booktitle>
<pages>27--36</pages>
<marker>Otterbacher, Radev, </marker>
<rawString>Jahna C. Otterbacher, Dragomir R. Radev, and Airong Luo 2002, Revisions that Improve Cohesion in Multi-document Summaries: A Preliminary Study. Proc. of the ACL-02 Workshop on Automatic Summarization: 27-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Robin</author>
<author>Kathleen McKeown</author>
</authors>
<title>Empirically designing and evaluating a new revisionbased model for summary generation.</title>
<date>1996</date>
<journal>Artificial Intelligence.</journal>
<volume>85</volume>
<pages>135--179</pages>
<contexts>
<context position="10935" citStr="Robin and McKeown 1996" startWordPosition="1798" endWordPosition="1801">following part, we use the term phrase to refer to a maximum phrase for simplicity. By comparing the phrases in Figure 1, we notice that the following operations can add useful information to the lead sentence; 1) inserting the first phrase of the body will supply the fact the visit was on the 4th, 2) substituting the first phrase of the lead with the second one in the body adds the detail of the IAEA team. This revision strategy was employed by the human reviser mentioned in section 2, and we consider this to be effective because our target document has a so-called inverse pyramid structure (Robin and McKeown 1996), in which the first sentence is elaborated by the following sentences. 3 To be more precise, a maximum phrase is defined as the maximum chunk sequence on a dependency path of a head. チームが 韓CiL 到着しました the team at Korea arrived 到着しました arrived maximum phrase Figure 1. Concept of revision algorithm Further analyzing the above fact, we devised the lead sentence revision algorithm below. We present the outline here and discuss the details in the next section. We suppose an input pair of a lead and a body sentence that are syntactically analyzed. 1) Trigger search We search for the “same” chunks in </context>
</contexts>
<marker>Robin, McKeown, 1996</marker>
<rawString>Jacques Robin and Kathleen McKeown. 1996. Empirically designing and evaluating a new revisionbased model for summary generation. Artificial Intelligence. 85: 135-179.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>