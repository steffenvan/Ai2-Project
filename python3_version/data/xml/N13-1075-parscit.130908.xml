<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000826">
<title confidence="0.994791">
Translation Acquisition Using Synonym Sets
</title>
<author confidence="0.985234">
Daniel Andrade Masaaki Tsuchida Takashi Onishi Kai Ishikawa
</author>
<affiliation confidence="0.931422">
Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan
</affiliation>
<address confidence="0.369284">
{s-andrade@cj, m-tsuchida@cq,
</address>
<email confidence="0.568379">
t-onishi@bq, k-ishikawa@dq}.jp.nec.com
</email>
<sectionHeader confidence="0.994517" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992665">
We propose a new method for translation ac-
quisition which uses a set of synonyms to ac-
quire translations from comparable corpora.
The motivation is that, given a certain query
term, it is often possible for a user to specify
one or more synonyms. Using the resulting
set of query terms has the advantage that we
can overcome the problem that a single query
term’s context vector does not always reliably
represent a terms meaning due to the context
vector’s sparsity. Our proposed method uses
a weighted average of the synonyms’ context
vectors, that is derived by inferring the mean
vector of the von Mises-Fisher distribution.
We evaluate our method, using the synsets
from the cross-lingually aligned Japanese and
English WordNet. The experiments show that
our proposed method significantly improves
translation accuracy when compared to a pre-
vious method for smoothing context vectors.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986220956521739">
Automatic translation acquisition is an important
task for various applications. For example, finding
term translations can be used to automatically up-
date existing bilingual dictionaries, which are an in-
dispensable resource for tasks such as cross-lingual
information retrieval and text mining.
Various previous research like (Rapp, 1999; Fung,
1998) has shown that it is possible to acquire word
translations from comparable corpora.
We suggest here an extension of this approach
which uses several query terms instead of a single
query term. A user who searches a translation for
a query term that is not listed in an existing bilin-
gual dictionary, might first try to find a synonym
of that term. For example, the user might look up
a synonym in a thesaurus) or might use methods
for automatic synonym acquisition like described
in (Grefenstette, 1994). If the synonym is listed in
the bilingual dictionary, we can consider the syn-
onym’s translations as the translations of the query
term. Otherwise, if the synonym is not listed in the
dictionary either, we use the synonym together with
the original query term to find a translation.
We claim that using a set of synonymous query
terms to find a translation is better than using a single
query term. The reason is that a single query term’s
context vector is, in general, unreliable due to spar-
sity. For example, a low frequent query term tends to
have many zero entries in its context vector. To mit-
igate this problem it has been proposed to smooth
a query’s context vector by its nearest neighbors
(Pekar et al., 2006). However, nearest neighbors,
which context vectors are close the query’s context
vector, can have different meanings and therefore
might introduce noise.
The contributions of this paper are two-fold. First,
we confirm experimentally that smoothing a query’s
context vector with its synonyms leads in deed to
higher translation accuracy, compared to smoothing
with nearest neighbors. Second, we propose a sim-
ple method to combine a set of context vectors that
performs in this setting better than a method previ-
ously proposed by (Pekar et al., 2006).
Our approach to combine a set of context vec-
&apos;Monolingual thesauri are, arguably, easier to construct than
bilingual dictionaries.
</bodyText>
<page confidence="0.986203">
655
</page>
<subsectionHeader confidence="0.29278">
Proceedings of NAACL-HLT 2013, pages 655–660,
</subsectionHeader>
<bodyText confidence="0.967362818181818">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
tors is derived by learning the mean vector of a von
Mises-Fisher distribution. The combined context
vector is a weighted-average of the original context-
vectors, where the weights are determined by the
word occurrence frequencies.
In the following section we briefly show the rela-
tion to other previous work. In Section 3, we explain
our method in detail, followed by an empirical eval-
uation in Section 4. We summarize our results in
Section 6.
</bodyText>
<sectionHeader confidence="0.999726" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999894818181818">
There are several previous works on extract-
ing translations from comparable corpora ranging
from (Rapp, 1999; Fung, 1998), and more re-
cently (Haghighi et al., 2008; Laroche and Langlais,
2010), among others. Essentially, all these meth-
ods calculate the similarity of a query term’s context
vector with each translation candidate’s context vec-
tor. The context vectors are extracted from the com-
parable corpora, and mapped to a common vector
space with the help of an existing bilingual dictio-
nary.
The work in (D´ejean et al., 2002) uses cross-
lingually aligned classes in a multilingual thesaurus
to improve the translation accuracy. Their method
uses the probability that the query term and a trans-
lation candidate are assigned to the same class. In
contrast, our method does not need cross-lingually
aligned classes.
Ismail and Manandhar (2010) proposes a method
that tries to improve a query’s context vector by us-
ing in-domain terms. In-domain terms are the terms
that are highly associated to the query, as well as
highly associated to one of the query’s highly asso-
ciated terms. Their method makes it necessary that
the query term has enough highly associated context
terms.2 However, a low-frequent query term might
not have enough highly associated terms.
In general if a query term has a low-frequency in
the corpus, then its context vector is sparse. In that
case, the chance of finding a correct translation is
reduced (Pekar et al., 2006). Therefore, Pekar et al.
(2006) suggest to use distance-based averaging to
smooth the context vector of a low-frequent query
</bodyText>
<footnote confidence="0.731795">
2In their experiments, they require that a query word has at
least 100 associated terms.
</footnote>
<bodyText confidence="0.996202833333333">
term. Their smoothing strategy is dependent on the
occurrence frequency of a query term and its close
neighbors. Let us denote q the context vector of the
query word, and K be the set of its close neighbors.
The smoothed context vector q′ is then derived by
using:
</bodyText>
<equation confidence="0.4769978">
� wx · x, (1)
q′ = -y-q+(1--y)-
xcK
7-q+(1-7)-
xEK
</equation>
<bodyText confidence="0.999713">
where wx is the weight of neighbor x, and all
weights sum to one. The context vectors q and x
are interpreted as probability vectors and therefore
L1-normalized. The weight wx is a function of the
distance between neighbor x and query q. The pa-
rameter ry determines the degree of smoothing, and
is a function of the frequency of the query term and
its neighbors:
</bodyText>
<equation confidence="0.9575175">
_ log f (q) 2
ry log maxx∈K∪{q} f (x) ( )
</equation>
<bodyText confidence="0.999877">
where f(x) is the frequency of term x. Their method
forms the baseline for our proposed method.
</bodyText>
<sectionHeader confidence="0.98618" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.999982913043478">
Our goal is to combine the context vectors to one
context vector which is less sparse and more reli-
able than the original context vector of query word
q. We assume that for each occurrence of a word,
its corresponding context vector was generated by
a probabilistic model. Furthermore, we assume that
synonyms are generated by the same probability dis-
tribution. Finally we use the mean vector of that dis-
tribution to represent the combined context vector.
By using the assumption that each occurrence of a
word corresponds to one sample of the probability
distribution, our model places more weight on syn-
onyms that are highly-frequent than synonyms that
occur infrequently. This is motivated by the assump-
tion that context vectors of synonyms that occur with
high frequency in the corpus, are more reliable than
the ones of low-frequency synonyms.
When comparing context vectors, work
like Laroche and Langlais (2010) observed
that often the cosine similarity performs superior
to other distance-measures, like, for example, the
euclidean distance. This suggests that context
vectors tend to lie in the spherical vector space,
</bodyText>
<page confidence="0.995808">
656
</page>
<bodyText confidence="0.9845059">
and therefore the von Mises-Fisher distribution is
a natural choice for our probabilistic model. The
von Mises-Fisher distribution was also successfully
used in the work of (Basu et al., 2004) to cluster
text data.
The von Mises-Fisher distribution with location
parameter µ, and concentration parameter r. is de-
fined as:
Ax|µ, r.) = c(r.) · e
where c(r.) is a normalization constant, and ||x ||=
||µ ||= 1, and r. ≥ 0.  ||denotes here the L2-norm.
The cosine-similarity measures the angle between
two vectors, and the von Mises distribution defines
a probability distribution over the possible angles.
The parameter µ of the von Mises distribution is es-
timated as follows (Jammalamadaka and Sengupta,
2001): Given the words x1, ..., xn, we denote the
corresponding context vectors as x1, ..., xn, and as-
sume that each context vector is L2-normalized.
Then, the mean vector µ is calculated as:
</bodyText>
<equation confidence="0.998137666666667">
1 n xi
µ = Z n
i=1
</equation>
<bodyText confidence="0.999737375">
where Z ensures that the resulting context vector is
L2-normalized, i.e. Z is   ||En 1 i ||. For our pur-
pose, r. is irrelevant and is assumed to be any fixed
positive constant.
Since we assume that each occurrence of a word x
in the corpus corresponds to one observation of the
corresponding word’s context vector x, we get the
following formula:
</bodyText>
<equation confidence="0.9981115">
f(xi) · x
Enj=1 f(xj) i
</equation>
<bodyText confidence="0.5597045">
where
is now
</bodyText>
<equation confidence="0.995644">
Z′
||
f(xi) xi||. We then
x-1 ∑nj=1 f(xj)
</equation>
<bodyText confidence="0.9985758">
use the vector µ as the combined vector of the
words’ context vectors xi.
Our proposed procedure to combine the context
vector of query word q and its synonyms can be sum-
marized as follows:
</bodyText>
<listItem confidence="0.987899714285714">
1. Denote the context vectors of q and its syn-
onyms as x1, ..., xn, and L2-normalize each
context vector.
2. Calculate the weighted average of the vectors
x1, ..., xn, whereas the weights correspond to
the frequencies of each word xi.
3. L2-normalize the weighted average.
</listItem>
<sectionHeader confidence="0.831045" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999774">
As source and target language corpora we use a cor-
pus extracted from a collection of complaints con-
cerning automobiles compiled by the Japanese Min-
istry of Land, Infrastructure, Transport and Tourism
(MLIT)3 and the USA National Highway Traffic
Safety Administration (NHTSA)4, respectively. The
Japanese corpus contains 24090 sentences that were
POS tagged using MeCab (Kudo et al., 2004). The
English corpus contains 47613 sentences, that were
POS tagged using Stepp Tagger (Tsuruoka et al.,
2005), and use the Lemmatizer (Okazaki et al.,
2008) to extract and stem content words (nouns,
verbs, adjectives, adverbs).
For creating the context vectors, we calculate the
association between two content words occurring
in the same sentence, using the log-odds-ratio (Ev-
ert, 2004). It was shown in (Laroche and Langlais,
2010) that the log-odds-ratio in combination with
the cosine-similarity performs superior to several
other methods like PMIS and LLR6. For comparing
two context vectors we use the cosine similarity.
To transform the Japanese and English context
vectors into the same vector space, we use a bilin-
gual dictionary with around 1.6 million entries.7
To express all context vectors in the same vector
space, we map the context vectors in English to con-
text vectors in Japanese.8 First, for all the words
which are listed in the bilingual dictionary we calcu-
late word translation probabilities. These translation
probabilities are calculated using the EM-algorithm
described in (Koehn and Knight, 2000). We then
create a translation matrix T which contains in each
</bodyText>
<footnote confidence="0.834254571428572">
3http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html
4http://www-odi.nhtsa.dot.gov/downloads/index.cfm
5point-wise mutual information
6log-likelihood ratio
7The bilingual dictionary was developed in the course of our
Japanese language processing efforts described in (Sato et al.,
2003).
</footnote>
<bodyText confidence="0.939676666666667">
8Alternatively, we could, for example, use canonical corre-
lation analysis to match the vectors to a common latent vector
space, like described in (Haghighi et al., 2008).
</bodyText>
<equation confidence="0.995589">
κ·x·µT ,
1 ·
µ =
Z′
n
i=1
Ei
</equation>
<page confidence="0.990376">
657
</page>
<bodyText confidence="0.999970181818182">
column the translation probabilities for a word in
English into any word in Japanese. Each context
vector in English is then mapped into Japanese us-
ing the linear transformation described by the trans-
lation matrix T. For word x with context vector x in
English, let x′ be its context vector after transforma-
tion into Japanese, i.e. x′ = T · x.
The gold-standard was created by considering
all nouns in the Japanese and English WordNet
where synsets are aligned cross-lingually. This way
we were able to create a gold-standard with 215
Japanese nouns, and their respective English trans-
lations that occur in our comparable corpora.9 Note
that the cross-lingual alignment is needed only for
evaluation. For evaluation, we consider only the
translations that occur in the corresponding English
synset as correct.
Because all methods return a ranked list of trans-
lation candidates, the accuracy is measured using the
rank of the translation listed in the gold-standard.
The inverse rank is the sum of the inverse ranks of
each translation in the gold-standard.
In Table 1, the first row shows the results when us-
ing no smoothing. Next, we smooth the query’s con-
text vector by using Equation (1) and (2). The set of
neighbors K is defined as the k-terms in the source
language that are closest to the query word, with re-
spect to the cosine similarity (sim). The weight wx
for a neighbor x is set to wx = 100.13·sim(x,4) in
accordance to (Pekar et al., 2006). For k we tried
values between 1 and 100, and got the best inverse
rank when using k=19. The resulting method (Top-
k Smoothing) performs consistently better than the
method using no smoothing, see Table 1, second
row. Next, instead of smoothing the query word with
its nearest neighbors, we use as the set K the set of
synonyms of the query word (Syn Smoothing). Ta-
ble 1 shows a clear improvement over the method
that uses nearest neighbor-smoothing. This confirms
our claim that using synonyms for smoothing can
lead to better translation accuracy than using nearest
neighbors. In the last row of Table 1, we compare
our proposed method to combine context vectors of
synonyms (Syn Mises-Combination), with the pre-
</bodyText>
<footnote confidence="0.978254">
9The resulting synsets in Japanese and English, contain in
average 2.2 and 2.8 words, respectively. The ambiguity of a
query term in our gold-standard is low, since, in average, a
query term belongs to only 1.2 different synsets.
</footnote>
<bodyText confidence="0.999577">
vious method (Syn Smoothing). A pair-wise com-
parison of our proposed method with Syn Smooth-
ing shows a statistically significant improvement (p
&lt; 0.01).10
Finally, we also show the result when simply
adding each synonym vector to the query’s context
vector to form a new combined context vector (Syn
Sum).11 Even though, this approach does not use the
frequency information of a word, it performs bet-
ter than Syn Smoothing. We suppose that this is
due to the fact that it actually indirectly uses fre-
quency information, since the log-odds-ratio tends
to be higher for words which occur with high fre-
quency in the corpus.
</bodyText>
<table confidence="0.999905">
Method Top1 Top5 Top10 MIR
No Smoothing 0.14 0.30 0.36 0.23
Top-k Smoothing 0.16 0.33 0.43 0.26
Syn Smoothing 0.18 0.35 0.46 0.28
Syn Sum 0.23 0.46 0.57 0.35
Syn Mises-Combination 0.31 0.46 0.55 0.40
</table>
<tableCaption confidence="0.966371">
Table 1: Shows Top-n accuracy and mean inverse rank
(MIR) for baseline methods which use no synonyms
(No Smoothing, Top-k Smoothing), the proposed method
(Syn Mises-Combination) which uses synonyms, and al-
ternative methods that also use synonyms (Syn Smooth-
ing, Syn Sum).
</tableCaption>
<sectionHeader confidence="0.996523" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9999582">
We first discuss an example where the query terms
are rI)L—X (cruise) and A)R (cruise). Both words
can have the same meaning. The resulting trans-
lation candidates suggested by the baseline meth-
ods and the proposed method is shown in Table 2.
Using no smoothing, the baseline method outputs
the correct translation for rI)L—X (cruise) and A
)R (cruise) at rank 10 and 15, respectively. When
combining both queries to form one context vector
our proposed method (Syn Mises-Combination) re-
trieves the correct translation at rank 2. Note that we
considered all nouns that occur three or more times
as possible translation candidates. As can be seen
in Table 2, this also includes spelling mistakes like
”sevice” and ”infromation”.
</bodyText>
<footnote confidence="0.9773415">
10We use the sign-test (Wilcox, 2009) to test the hypothesis
that the proposed method ranks higher than the baseline.
11No normalization is performed before adding the context
vectors.
</footnote>
<page confidence="0.988462">
658
</page>
<table confidence="0.998735333333333">
Method Query Output Rank
No Smoothing クルーズ ..., affinity, delco, cruise, sevice, sentrum,... 10
No Smoothing 巡航 ..., denali, attendant, cruise, abs, tactic,... 15
Top-k Smoothing クルーズ pillar, multi, cruise, star, affinity,... 3
Top-k Smoothing 巡航 ..., burnout, dipstick, cruise, infromation, speed,... 8
Syn Smoothing クルーズ smoothed with 巡航 ..., affinity, delco, cruise, sevice, sentrum,... 10
Syn Smoothing 巡航 smoothed with クルーズ ..., alldata, mode, cruise, expectancy, mph,... 8
Syn Sum クルーズ, 巡航 assumption, level, cruise, reimbursment, infromation,... 3
Syn Mises-Combination クルーズ, 巡航 pillar, cruise, assumption, level, speed,... 2
</table>
<tableCaption confidence="0.945047">
Table 2: Shows the results for -1AL X and AC which both have the same meaning ”cruise”. The third column
</tableCaption>
<figureCaption confidence="0.835102">
shows part of the ranked translation candidates separated by comma. The last column shows the rank of the correct
translation ”cruise”. Syn Smoothing uses Equation (1) with q corresponding to the context vector of the query word,
and K contains only the context vector of the term that is used for smoothing.
</figureCaption>
<bodyText confidence="0.999955153846154">
Finally, we note that some terms in our test set
are ambiguous, and the ambiguity is not resolved by
using the synonyms of only one synset. For exam-
ple, the term 40V (steering, guidance) belongs to
the synset ”steering, guidance” which includes the
terms VXX9 (steering, guidance) and )1�F (guid-
ance), N (guidance). Despite this conflation of
senses in one synset, our proposed method can im-
prove the finding of (one) correct translation. The
baseline system using only 40V (steering, guidance)
outputs the correct translation ”steering” at rank 4,
whereas our method using all four terms outputs it
at rank 2.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999990235294118">
We proposed a new method for translation acquisi-
tion which uses a set of synonyms to acquire transla-
tions. Our approach combines the query term’s con-
text vector with all the context vectors of its syn-
onyms. In order to combine the vectors we use a
weighted average of each context vector, where the
weights are determined by a term’s occurrence fre-
quency.
Our experiments, using the Japanese and English
WordNet (Bond et al., 2009; Fellbaum, 1998), show
that our proposed method can increase the transla-
tion accuracy, when compared to using only a single
query term, or smoothing with nearest neighbours.
Our results suggest that instead of directly search-
ing for a translation, it is worth first looking for syn-
onyms, for example by considering spelling varia-
tions or monolingual resources.
</bodyText>
<sectionHeader confidence="0.998354" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999321945945946">
S. Basu, M. Bilenko, and R.J. Mooney. 2004. A prob-
abilistic framework for semi-supervised clustering. In
Proceedings of the ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 59–68.
F. Bond, H. Isahara, S. Fujita, K. Uchimoto, T. Kurib-
ayashi, and K. Kanzaki. 2009. Enhancing the
japanese wordnet. In Proceedings of the 7th Workshop
on Asian Language Resources, pages 1–8. Association
for Computational Linguistics.
H. D´ejean, ´E. Gaussier, and F. Sadat. 2002. An approach
based on multilingual thesauri and model combination
for bilingual lexicon extraction. In Proceedings of the
International Conference on Computational Linguis-
tics, pages 1–7. International Committee on Computa-
tional Linguistics.
S. Evert. 2004. The statistics of word cooccurrences:
word pairs and collocations. Doctoral dissertation, In-
stitut f¨ur maschinelle Sprachverarbeitung, Universit¨at
Stuttgart.
C. Fellbaum. 1998. Wordnet: an electronic lexical
database. Cambrige, MIT Press, Language, Speech,
and Communication.
P. Fung. 1998. A statistical view on bilingual lexicon ex-
traction: from parallel corpora to non-parallel corpora.
Lecture Notes in Computer Science, 1529:1–17.
G. Grefenstette. 1994. Explorations in automatic the-
saurus discovery. Springer.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, pages
771–779. Association for Computational Linguistics.
A. Ismail and S. Manandhar. 2010. Bilingual lexicon
extraction from comparable corpora using in-domain
terms. In Proceedings of the International Conference
on Computational Linguistics, pages 481 – 489.
</reference>
<page confidence="0.987999">
659
</page>
<reference confidence="0.999631297872341">
S.R. Jammalamadaka and A. Sengupta. 2001. Topics in
circular statistics, volume 5. World Scientific Pub Co
Inc.
P. Koehn and K. Knight. 2000. Estimating word trans-
lation probabilities from unrelated monolingual cor-
pora using the em algorithm. In Proceedings of the
National Conference on Artificial Intelligence, pages
711–715. Association for the Advancement of Artifi-
cial Intelligence.
T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Ap-
plying conditional random fields to Japanese morpho-
logical analysis. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 230–237. Association for Computational Lin-
guistics.
A. Laroche and P. Langlais. 2010. Revisiting context-
based projection methods for term-translation spotting
in comparable corpora. In Proceedings of the In-
ternational Conference on Computational Linguistics,
pages 617 – 625.
N. Okazaki, Y. Tsuruoka, S. Ananiadou, and J. Tsujii.
2008. A discriminative candidate generator for string
transformations. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 447–456. Association for Computational Lin-
guistics.
V. Pekar, R. Mitkov, D. Blagoev, and A. Mulloni. 2006.
Finding translations for low-frequency words in com-
parable corpora. Machine Translation, 20(4):247–
266.
R. Rapp. 1999. Automatic identification of word transla-
tions from unrelated English and German corpora. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, pages 519–526. Asso-
ciation for Computational Linguistics.
K. Sato, T. Ikeda, T. Nakata, and S. Osada. 2003. In-
troduction of a Japanese language processing mid-
dleware used for CRM. In Annual Meeting of the
Japanese Association for Natural Language Process-
ing (in Japanese), pages 109–112.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii. 2005. Developing a ro-
bust part-of-speech tagger for biomedical text. Lecture
Notes in Computer Science, 3746:382–392.
R.R. Wilcox. 2009. Basic Statistics: Understanding
Conventional Methods and Modern Insights. Oxford
University Press.
</reference>
<page confidence="0.996824">
660
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.720582">
<title confidence="0.999943">Translation Acquisition Using Synonym Sets</title>
<author confidence="0.999554">Daniel Andrade Masaaki Tsuchida Takashi Onishi Kai Ishikawa</author>
<affiliation confidence="0.769226">Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan</affiliation>
<email confidence="0.976684">m-tsuchida@cq,</email>
<abstract confidence="0.998207095238095">We propose a new method for translation acquisition which uses a set of synonyms to acquire translations from comparable corpora. The motivation is that, given a certain query term, it is often possible for a user to specify one or more synonyms. Using the resulting set of query terms has the advantage that we can overcome the problem that a single query term’s context vector does not always reliably represent a terms meaning due to the context vector’s sparsity. Our proposed method uses a weighted average of the synonyms’ context vectors, that is derived by inferring the mean vector of the von Mises-Fisher distribution. We evaluate our method, using the synsets from the cross-lingually aligned Japanese and English WordNet. The experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Basu</author>
<author>M Bilenko</author>
<author>R J Mooney</author>
</authors>
<title>A probabilistic framework for semi-supervised clustering.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>59--68</pages>
<contexts>
<context position="7830" citStr="Basu et al., 2004" startWordPosition="1268" endWordPosition="1271">tion that context vectors of synonyms that occur with high frequency in the corpus, are more reliable than the ones of low-frequency synonyms. When comparing context vectors, work like Laroche and Langlais (2010) observed that often the cosine similarity performs superior to other distance-measures, like, for example, the euclidean distance. This suggests that context vectors tend to lie in the spherical vector space, 656 and therefore the von Mises-Fisher distribution is a natural choice for our probabilistic model. The von Mises-Fisher distribution was also successfully used in the work of (Basu et al., 2004) to cluster text data. The von Mises-Fisher distribution with location parameter µ, and concentration parameter r. is defined as: Ax|µ, r.) = c(r.) · e where c(r.) is a normalization constant, and ||x ||= ||µ ||= 1, and r. ≥ 0. ||denotes here the L2-norm. The cosine-similarity measures the angle between two vectors, and the von Mises distribution defines a probability distribution over the possible angles. The parameter µ of the von Mises distribution is estimated as follows (Jammalamadaka and Sengupta, 2001): Given the words x1, ..., xn, we denote the corresponding context vectors as x1, ...,</context>
</contexts>
<marker>Basu, Bilenko, Mooney, 2004</marker>
<rawString>S. Basu, M. Bilenko, and R.J. Mooney. 2004. A probabilistic framework for semi-supervised clustering. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 59–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Bond</author>
<author>H Isahara</author>
<author>S Fujita</author>
<author>K Uchimoto</author>
<author>T Kuribayashi</author>
<author>K Kanzaki</author>
</authors>
<title>Enhancing the japanese wordnet.</title>
<date>2009</date>
<booktitle>In Proceedings of the 7th Workshop on Asian Language Resources,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Bond, Isahara, Fujita, Uchimoto, Kuribayashi, Kanzaki, 2009</marker>
<rawString>F. Bond, H. Isahara, S. Fujita, K. Uchimoto, T. Kuribayashi, and K. Kanzaki. 2009. Enhancing the japanese wordnet. In Proceedings of the 7th Workshop on Asian Language Resources, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H D´ejean</author>
<author>´E Gaussier</author>
<author>F Sadat</author>
</authors>
<title>An approach based on multilingual thesauri and model combination for bilingual lexicon extraction.</title>
<date>2002</date>
<journal>International Committee on Computational Linguistics.</journal>
<booktitle>In Proceedings of the International Conference on Computational Linguistics,</booktitle>
<pages>1--7</pages>
<marker>D´ejean, Gaussier, Sadat, 2002</marker>
<rawString>H. D´ejean, ´E. Gaussier, and F. Sadat. 2002. An approach based on multilingual thesauri and model combination for bilingual lexicon extraction. In Proceedings of the International Conference on Computational Linguistics, pages 1–7. International Committee on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Evert</author>
</authors>
<title>The statistics of word cooccurrences: word pairs and collocations. Doctoral dissertation, Institut f¨ur maschinelle Sprachverarbeitung,</title>
<date>2004</date>
<location>Universit¨at Stuttgart.</location>
<contexts>
<context position="10227" citStr="Evert, 2004" startWordPosition="1676" endWordPosition="1678">frastructure, Transport and Tourism (MLIT)3 and the USA National Highway Traffic Safety Administration (NHTSA)4, respectively. The Japanese corpus contains 24090 sentences that were POS tagged using MeCab (Kudo et al., 2004). The English corpus contains 47613 sentences, that were POS tagged using Stepp Tagger (Tsuruoka et al., 2005), and use the Lemmatizer (Okazaki et al., 2008) to extract and stem content words (nouns, verbs, adjectives, adverbs). For creating the context vectors, we calculate the association between two content words occurring in the same sentence, using the log-odds-ratio (Evert, 2004). It was shown in (Laroche and Langlais, 2010) that the log-odds-ratio in combination with the cosine-similarity performs superior to several other methods like PMIS and LLR6. For comparing two context vectors we use the cosine similarity. To transform the Japanese and English context vectors into the same vector space, we use a bilingual dictionary with around 1.6 million entries.7 To express all context vectors in the same vector space, we map the context vectors in English to context vectors in Japanese.8 First, for all the words which are listed in the bilingual dictionary we calculate wor</context>
</contexts>
<marker>Evert, 2004</marker>
<rawString>S. Evert. 2004. The statistics of word cooccurrences: word pairs and collocations. Doctoral dissertation, Institut f¨ur maschinelle Sprachverarbeitung, Universit¨at Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>Wordnet: an electronic lexical database. Cambrige,</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Language, Speech, and Communication.</location>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. Wordnet: an electronic lexical database. Cambrige, MIT Press, Language, Speech, and Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
</authors>
<title>A statistical view on bilingual lexicon extraction: from parallel corpora to non-parallel corpora.</title>
<date>1998</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>1529--1</pages>
<contexts>
<context position="1509" citStr="Fung, 1998" startWordPosition="218" endWordPosition="219">method, using the synsets from the cross-lingually aligned Japanese and English WordNet. The experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors. 1 Introduction Automatic translation acquisition is an important task for various applications. For example, finding term translations can be used to automatically update existing bilingual dictionaries, which are an indispensable resource for tasks such as cross-lingual information retrieval and text mining. Various previous research like (Rapp, 1999; Fung, 1998) has shown that it is possible to acquire word translations from comparable corpora. We suggest here an extension of this approach which uses several query terms instead of a single query term. A user who searches a translation for a query term that is not listed in an existing bilingual dictionary, might first try to find a synonym of that term. For example, the user might look up a synonym in a thesaurus) or might use methods for automatic synonym acquisition like described in (Grefenstette, 1994). If the synonym is listed in the bilingual dictionary, we can consider the synonym’s translatio</context>
<context position="4128" citStr="Fung, 1998" startWordPosition="649" endWordPosition="650">for Computational Linguistics tors is derived by learning the mean vector of a von Mises-Fisher distribution. The combined context vector is a weighted-average of the original contextvectors, where the weights are determined by the word occurrence frequencies. In the following section we briefly show the relation to other previous work. In Section 3, we explain our method in detail, followed by an empirical evaluation in Section 4. We summarize our results in Section 6. 2 Related Work There are several previous works on extracting translations from comparable corpora ranging from (Rapp, 1999; Fung, 1998), and more recently (Haghighi et al., 2008; Laroche and Langlais, 2010), among others. Essentially, all these methods calculate the similarity of a query term’s context vector with each translation candidate’s context vector. The context vectors are extracted from the comparable corpora, and mapped to a common vector space with the help of an existing bilingual dictionary. The work in (D´ejean et al., 2002) uses crosslingually aligned classes in a multilingual thesaurus to improve the translation accuracy. Their method uses the probability that the query term and a translation candidate are as</context>
</contexts>
<marker>Fung, 1998</marker>
<rawString>P. Fung. 1998. A statistical view on bilingual lexicon extraction: from parallel corpora to non-parallel corpora. Lecture Notes in Computer Science, 1529:1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in automatic thesaurus discovery.</title>
<date>1994</date>
<publisher>Springer.</publisher>
<contexts>
<context position="2013" citStr="Grefenstette, 1994" startWordPosition="305" endWordPosition="306">uch as cross-lingual information retrieval and text mining. Various previous research like (Rapp, 1999; Fung, 1998) has shown that it is possible to acquire word translations from comparable corpora. We suggest here an extension of this approach which uses several query terms instead of a single query term. A user who searches a translation for a query term that is not listed in an existing bilingual dictionary, might first try to find a synonym of that term. For example, the user might look up a synonym in a thesaurus) or might use methods for automatic synonym acquisition like described in (Grefenstette, 1994). If the synonym is listed in the bilingual dictionary, we can consider the synonym’s translations as the translations of the query term. Otherwise, if the synonym is not listed in the dictionary either, we use the synonym together with the original query term to find a translation. We claim that using a set of synonymous query terms to find a translation is better than using a single query term. The reason is that a single query term’s context vector is, in general, unreliable due to sparsity. For example, a low frequent query term tends to have many zero entries in its context vector. To mit</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>G. Grefenstette. 1994. Explorations in automatic thesaurus discovery. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>P Liang</author>
<author>T Berg-Kirkpatrick</author>
<author>D Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>771--779</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4170" citStr="Haghighi et al., 2008" startWordPosition="655" endWordPosition="658">rs is derived by learning the mean vector of a von Mises-Fisher distribution. The combined context vector is a weighted-average of the original contextvectors, where the weights are determined by the word occurrence frequencies. In the following section we briefly show the relation to other previous work. In Section 3, we explain our method in detail, followed by an empirical evaluation in Section 4. We summarize our results in Section 6. 2 Related Work There are several previous works on extracting translations from comparable corpora ranging from (Rapp, 1999; Fung, 1998), and more recently (Haghighi et al., 2008; Laroche and Langlais, 2010), among others. Essentially, all these methods calculate the similarity of a query term’s context vector with each translation candidate’s context vector. The context vectors are extracted from the comparable corpora, and mapped to a common vector space with the help of an existing bilingual dictionary. The work in (D´ejean et al., 2002) uses crosslingually aligned classes in a multilingual thesaurus to improve the translation accuracy. Their method uses the probability that the query term and a translation candidate are assigned to the same class. In contrast, our</context>
<context position="11485" citStr="Haghighi et al., 2008" startWordPosition="1855" endWordPosition="1858">ranslation probabilities are calculated using the EM-algorithm described in (Koehn and Knight, 2000). We then create a translation matrix T which contains in each 3http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html 4http://www-odi.nhtsa.dot.gov/downloads/index.cfm 5point-wise mutual information 6log-likelihood ratio 7The bilingual dictionary was developed in the course of our Japanese language processing efforts described in (Sato et al., 2003). 8Alternatively, we could, for example, use canonical correlation analysis to match the vectors to a common latent vector space, like described in (Haghighi et al., 2008). κ·x·µT , 1 · µ = Z′ n i=1 Ei 657 column the translation probabilities for a word in English into any word in Japanese. Each context vector in English is then mapped into Japanese using the linear transformation described by the translation matrix T. For word x with context vector x in English, let x′ be its context vector after transformation into Japanese, i.e. x′ = T · x. The gold-standard was created by considering all nouns in the Japanese and English WordNet where synsets are aligned cross-lingually. This way we were able to create a gold-standard with 215 Japanese nouns, and their resp</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 771–779. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ismail</author>
<author>S Manandhar</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora using in-domain terms.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics,</booktitle>
<pages>481--489</pages>
<contexts>
<context position="4852" citStr="Ismail and Manandhar (2010)" startWordPosition="763" endWordPosition="766">y, all these methods calculate the similarity of a query term’s context vector with each translation candidate’s context vector. The context vectors are extracted from the comparable corpora, and mapped to a common vector space with the help of an existing bilingual dictionary. The work in (D´ejean et al., 2002) uses crosslingually aligned classes in a multilingual thesaurus to improve the translation accuracy. Their method uses the probability that the query term and a translation candidate are assigned to the same class. In contrast, our method does not need cross-lingually aligned classes. Ismail and Manandhar (2010) proposes a method that tries to improve a query’s context vector by using in-domain terms. In-domain terms are the terms that are highly associated to the query, as well as highly associated to one of the query’s highly associated terms. Their method makes it necessary that the query term has enough highly associated context terms.2 However, a low-frequent query term might not have enough highly associated terms. In general if a query term has a low-frequency in the corpus, then its context vector is sparse. In that case, the chance of finding a correct translation is reduced (Pekar et al., 2</context>
</contexts>
<marker>Ismail, Manandhar, 2010</marker>
<rawString>A. Ismail and S. Manandhar. 2010. Bilingual lexicon extraction from comparable corpora using in-domain terms. In Proceedings of the International Conference on Computational Linguistics, pages 481 – 489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Jammalamadaka</author>
<author>A Sengupta</author>
</authors>
<title>Topics in circular statistics, volume 5. World Scientific Pub Co Inc.</title>
<date>2001</date>
<contexts>
<context position="8344" citStr="Jammalamadaka and Sengupta, 2001" startWordPosition="1352" endWordPosition="1355">ur probabilistic model. The von Mises-Fisher distribution was also successfully used in the work of (Basu et al., 2004) to cluster text data. The von Mises-Fisher distribution with location parameter µ, and concentration parameter r. is defined as: Ax|µ, r.) = c(r.) · e where c(r.) is a normalization constant, and ||x ||= ||µ ||= 1, and r. ≥ 0. ||denotes here the L2-norm. The cosine-similarity measures the angle between two vectors, and the von Mises distribution defines a probability distribution over the possible angles. The parameter µ of the von Mises distribution is estimated as follows (Jammalamadaka and Sengupta, 2001): Given the words x1, ..., xn, we denote the corresponding context vectors as x1, ..., xn, and assume that each context vector is L2-normalized. Then, the mean vector µ is calculated as: 1 n xi µ = Z n i=1 where Z ensures that the resulting context vector is L2-normalized, i.e. Z is ||En 1 i ||. For our purpose, r. is irrelevant and is assumed to be any fixed positive constant. Since we assume that each occurrence of a word x in the corpus corresponds to one observation of the corresponding word’s context vector x, we get the following formula: f(xi) · x Enj=1 f(xj) i where is now Z′ || f(xi) </context>
</contexts>
<marker>Jammalamadaka, Sengupta, 2001</marker>
<rawString>S.R. Jammalamadaka and A. Sengupta. 2001. Topics in circular statistics, volume 5. World Scientific Pub Co Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Estimating word translation probabilities from unrelated monolingual corpora using the em algorithm.</title>
<date>2000</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>711--715</pages>
<contexts>
<context position="10963" citStr="Koehn and Knight, 2000" startWordPosition="1790" endWordPosition="1793">performs superior to several other methods like PMIS and LLR6. For comparing two context vectors we use the cosine similarity. To transform the Japanese and English context vectors into the same vector space, we use a bilingual dictionary with around 1.6 million entries.7 To express all context vectors in the same vector space, we map the context vectors in English to context vectors in Japanese.8 First, for all the words which are listed in the bilingual dictionary we calculate word translation probabilities. These translation probabilities are calculated using the EM-algorithm described in (Koehn and Knight, 2000). We then create a translation matrix T which contains in each 3http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html 4http://www-odi.nhtsa.dot.gov/downloads/index.cfm 5point-wise mutual information 6log-likelihood ratio 7The bilingual dictionary was developed in the course of our Japanese language processing efforts described in (Sato et al., 2003). 8Alternatively, we could, for example, use canonical correlation analysis to match the vectors to a common latent vector space, like described in (Haghighi et al., 2008). κ·x·µT , 1 · µ = Z′ n i=1 Ei 657 column the translation probabilities for a </context>
</contexts>
<marker>Koehn, Knight, 2000</marker>
<rawString>P. Koehn and K. Knight. 2000. Estimating word translation probabilities from unrelated monolingual corpora using the em algorithm. In Proceedings of the National Conference on Artificial Intelligence, pages 711–715. Association for the Advancement of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>K Yamamoto</author>
<author>Y Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>230--237</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9839" citStr="Kudo et al., 2004" startWordPosition="1616" endWordPosition="1619">, ..., xn, and L2-normalize each context vector. 2. Calculate the weighted average of the vectors x1, ..., xn, whereas the weights correspond to the frequencies of each word xi. 3. L2-normalize the weighted average. 4 Experiments As source and target language corpora we use a corpus extracted from a collection of complaints concerning automobiles compiled by the Japanese Ministry of Land, Infrastructure, Transport and Tourism (MLIT)3 and the USA National Highway Traffic Safety Administration (NHTSA)4, respectively. The Japanese corpus contains 24090 sentences that were POS tagged using MeCab (Kudo et al., 2004). The English corpus contains 47613 sentences, that were POS tagged using Stepp Tagger (Tsuruoka et al., 2005), and use the Lemmatizer (Okazaki et al., 2008) to extract and stem content words (nouns, verbs, adjectives, adverbs). For creating the context vectors, we calculate the association between two content words occurring in the same sentence, using the log-odds-ratio (Evert, 2004). It was shown in (Laroche and Langlais, 2010) that the log-odds-ratio in combination with the cosine-similarity performs superior to several other methods like PMIS and LLR6. For comparing two context vectors we</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 230–237. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Laroche</author>
<author>P Langlais</author>
</authors>
<title>Revisiting contextbased projection methods for term-translation spotting in comparable corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics,</booktitle>
<pages>617--625</pages>
<contexts>
<context position="4199" citStr="Laroche and Langlais, 2010" startWordPosition="659" endWordPosition="662">ng the mean vector of a von Mises-Fisher distribution. The combined context vector is a weighted-average of the original contextvectors, where the weights are determined by the word occurrence frequencies. In the following section we briefly show the relation to other previous work. In Section 3, we explain our method in detail, followed by an empirical evaluation in Section 4. We summarize our results in Section 6. 2 Related Work There are several previous works on extracting translations from comparable corpora ranging from (Rapp, 1999; Fung, 1998), and more recently (Haghighi et al., 2008; Laroche and Langlais, 2010), among others. Essentially, all these methods calculate the similarity of a query term’s context vector with each translation candidate’s context vector. The context vectors are extracted from the comparable corpora, and mapped to a common vector space with the help of an existing bilingual dictionary. The work in (D´ejean et al., 2002) uses crosslingually aligned classes in a multilingual thesaurus to improve the translation accuracy. Their method uses the probability that the query term and a translation candidate are assigned to the same class. In contrast, our method does not need cross-l</context>
<context position="7424" citStr="Laroche and Langlais (2010)" startWordPosition="1207" endWordPosition="1210">sume that synonyms are generated by the same probability distribution. Finally we use the mean vector of that distribution to represent the combined context vector. By using the assumption that each occurrence of a word corresponds to one sample of the probability distribution, our model places more weight on synonyms that are highly-frequent than synonyms that occur infrequently. This is motivated by the assumption that context vectors of synonyms that occur with high frequency in the corpus, are more reliable than the ones of low-frequency synonyms. When comparing context vectors, work like Laroche and Langlais (2010) observed that often the cosine similarity performs superior to other distance-measures, like, for example, the euclidean distance. This suggests that context vectors tend to lie in the spherical vector space, 656 and therefore the von Mises-Fisher distribution is a natural choice for our probabilistic model. The von Mises-Fisher distribution was also successfully used in the work of (Basu et al., 2004) to cluster text data. The von Mises-Fisher distribution with location parameter µ, and concentration parameter r. is defined as: Ax|µ, r.) = c(r.) · e where c(r.) is a normalization constant, a</context>
<context position="10273" citStr="Laroche and Langlais, 2010" startWordPosition="1683" endWordPosition="1686">rism (MLIT)3 and the USA National Highway Traffic Safety Administration (NHTSA)4, respectively. The Japanese corpus contains 24090 sentences that were POS tagged using MeCab (Kudo et al., 2004). The English corpus contains 47613 sentences, that were POS tagged using Stepp Tagger (Tsuruoka et al., 2005), and use the Lemmatizer (Okazaki et al., 2008) to extract and stem content words (nouns, verbs, adjectives, adverbs). For creating the context vectors, we calculate the association between two content words occurring in the same sentence, using the log-odds-ratio (Evert, 2004). It was shown in (Laroche and Langlais, 2010) that the log-odds-ratio in combination with the cosine-similarity performs superior to several other methods like PMIS and LLR6. For comparing two context vectors we use the cosine similarity. To transform the Japanese and English context vectors into the same vector space, we use a bilingual dictionary with around 1.6 million entries.7 To express all context vectors in the same vector space, we map the context vectors in English to context vectors in Japanese.8 First, for all the words which are listed in the bilingual dictionary we calculate word translation probabilities. These translation</context>
</contexts>
<marker>Laroche, Langlais, 2010</marker>
<rawString>A. Laroche and P. Langlais. 2010. Revisiting contextbased projection methods for term-translation spotting in comparable corpora. In Proceedings of the International Conference on Computational Linguistics, pages 617 – 625.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Okazaki</author>
<author>Y Tsuruoka</author>
<author>S Ananiadou</author>
<author>J Tsujii</author>
</authors>
<title>A discriminative candidate generator for string transformations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>447--456</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9996" citStr="Okazaki et al., 2008" startWordPosition="1641" endWordPosition="1644">encies of each word xi. 3. L2-normalize the weighted average. 4 Experiments As source and target language corpora we use a corpus extracted from a collection of complaints concerning automobiles compiled by the Japanese Ministry of Land, Infrastructure, Transport and Tourism (MLIT)3 and the USA National Highway Traffic Safety Administration (NHTSA)4, respectively. The Japanese corpus contains 24090 sentences that were POS tagged using MeCab (Kudo et al., 2004). The English corpus contains 47613 sentences, that were POS tagged using Stepp Tagger (Tsuruoka et al., 2005), and use the Lemmatizer (Okazaki et al., 2008) to extract and stem content words (nouns, verbs, adjectives, adverbs). For creating the context vectors, we calculate the association between two content words occurring in the same sentence, using the log-odds-ratio (Evert, 2004). It was shown in (Laroche and Langlais, 2010) that the log-odds-ratio in combination with the cosine-similarity performs superior to several other methods like PMIS and LLR6. For comparing two context vectors we use the cosine similarity. To transform the Japanese and English context vectors into the same vector space, we use a bilingual dictionary with around 1.6 m</context>
</contexts>
<marker>Okazaki, Tsuruoka, Ananiadou, Tsujii, 2008</marker>
<rawString>N. Okazaki, Y. Tsuruoka, S. Ananiadou, and J. Tsujii. 2008. A discriminative candidate generator for string transformations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 447–456. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Pekar</author>
<author>R Mitkov</author>
<author>D Blagoev</author>
<author>A Mulloni</author>
</authors>
<title>Finding translations for low-frequency words in comparable corpora.</title>
<date>2006</date>
<journal>Machine Translation,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>266</pages>
<contexts>
<context position="2733" citStr="Pekar et al., 2006" startWordPosition="431" endWordPosition="434"> the translations of the query term. Otherwise, if the synonym is not listed in the dictionary either, we use the synonym together with the original query term to find a translation. We claim that using a set of synonymous query terms to find a translation is better than using a single query term. The reason is that a single query term’s context vector is, in general, unreliable due to sparsity. For example, a low frequent query term tends to have many zero entries in its context vector. To mitigate this problem it has been proposed to smooth a query’s context vector by its nearest neighbors (Pekar et al., 2006). However, nearest neighbors, which context vectors are close the query’s context vector, can have different meanings and therefore might introduce noise. The contributions of this paper are two-fold. First, we confirm experimentally that smoothing a query’s context vector with its synonyms leads in deed to higher translation accuracy, compared to smoothing with nearest neighbors. Second, we propose a simple method to combine a set of context vectors that performs in this setting better than a method previously proposed by (Pekar et al., 2006). Our approach to combine a set of context vec&apos;Mono</context>
<context position="5456" citStr="Pekar et al., 2006" startWordPosition="866" endWordPosition="869">anandhar (2010) proposes a method that tries to improve a query’s context vector by using in-domain terms. In-domain terms are the terms that are highly associated to the query, as well as highly associated to one of the query’s highly associated terms. Their method makes it necessary that the query term has enough highly associated context terms.2 However, a low-frequent query term might not have enough highly associated terms. In general if a query term has a low-frequency in the corpus, then its context vector is sparse. In that case, the chance of finding a correct translation is reduced (Pekar et al., 2006). Therefore, Pekar et al. (2006) suggest to use distance-based averaging to smooth the context vector of a low-frequent query 2In their experiments, they require that a query word has at least 100 associated terms. term. Their smoothing strategy is dependent on the occurrence frequency of a query term and its close neighbors. Let us denote q the context vector of the query word, and K be the set of its close neighbors. The smoothed context vector q′ is then derived by using: � wx · x, (1) q′ = -y-q+(1--y)- xcK 7-q+(1-7)- xEK where wx is the weight of neighbor x, and all weights sum to one. The</context>
<context position="12972" citStr="Pekar et al., 2006" startWordPosition="2116" endWordPosition="2119">urn a ranked list of translation candidates, the accuracy is measured using the rank of the translation listed in the gold-standard. The inverse rank is the sum of the inverse ranks of each translation in the gold-standard. In Table 1, the first row shows the results when using no smoothing. Next, we smooth the query’s context vector by using Equation (1) and (2). The set of neighbors K is defined as the k-terms in the source language that are closest to the query word, with respect to the cosine similarity (sim). The weight wx for a neighbor x is set to wx = 100.13·sim(x,4) in accordance to (Pekar et al., 2006). For k we tried values between 1 and 100, and got the best inverse rank when using k=19. The resulting method (Topk Smoothing) performs consistently better than the method using no smoothing, see Table 1, second row. Next, instead of smoothing the query word with its nearest neighbors, we use as the set K the set of synonyms of the query word (Syn Smoothing). Table 1 shows a clear improvement over the method that uses nearest neighbor-smoothing. This confirms our claim that using synonyms for smoothing can lead to better translation accuracy than using nearest neighbors. In the last row of Ta</context>
</contexts>
<marker>Pekar, Mitkov, Blagoev, Mulloni, 2006</marker>
<rawString>V. Pekar, R. Mitkov, D. Blagoev, and A. Mulloni. 2006. Finding translations for low-frequency words in comparable corpora. Machine Translation, 20(4):247– 266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>519--526</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1496" citStr="Rapp, 1999" startWordPosition="216" endWordPosition="217">valuate our method, using the synsets from the cross-lingually aligned Japanese and English WordNet. The experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors. 1 Introduction Automatic translation acquisition is an important task for various applications. For example, finding term translations can be used to automatically update existing bilingual dictionaries, which are an indispensable resource for tasks such as cross-lingual information retrieval and text mining. Various previous research like (Rapp, 1999; Fung, 1998) has shown that it is possible to acquire word translations from comparable corpora. We suggest here an extension of this approach which uses several query terms instead of a single query term. A user who searches a translation for a query term that is not listed in an existing bilingual dictionary, might first try to find a synonym of that term. For example, the user might look up a synonym in a thesaurus) or might use methods for automatic synonym acquisition like described in (Grefenstette, 1994). If the synonym is listed in the bilingual dictionary, we can consider the synonym</context>
<context position="4115" citStr="Rapp, 1999" startWordPosition="647" endWordPosition="648">Association for Computational Linguistics tors is derived by learning the mean vector of a von Mises-Fisher distribution. The combined context vector is a weighted-average of the original contextvectors, where the weights are determined by the word occurrence frequencies. In the following section we briefly show the relation to other previous work. In Section 3, we explain our method in detail, followed by an empirical evaluation in Section 4. We summarize our results in Section 6. 2 Related Work There are several previous works on extracting translations from comparable corpora ranging from (Rapp, 1999; Fung, 1998), and more recently (Haghighi et al., 2008; Laroche and Langlais, 2010), among others. Essentially, all these methods calculate the similarity of a query term’s context vector with each translation candidate’s context vector. The context vectors are extracted from the comparable corpora, and mapped to a common vector space with the help of an existing bilingual dictionary. The work in (D´ejean et al., 2002) uses crosslingually aligned classes in a multilingual thesaurus to improve the translation accuracy. Their method uses the probability that the query term and a translation can</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>R. Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 519–526. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sato</author>
<author>T Ikeda</author>
<author>T Nakata</author>
<author>S Osada</author>
</authors>
<title>Introduction of a Japanese language processing middleware used for CRM.</title>
<date>2003</date>
<booktitle>In Annual Meeting of the Japanese Association for Natural Language Processing (in Japanese),</booktitle>
<pages>109--112</pages>
<contexts>
<context position="11314" citStr="Sato et al., 2003" startWordPosition="1828" endWordPosition="1831"> in English to context vectors in Japanese.8 First, for all the words which are listed in the bilingual dictionary we calculate word translation probabilities. These translation probabilities are calculated using the EM-algorithm described in (Koehn and Knight, 2000). We then create a translation matrix T which contains in each 3http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html 4http://www-odi.nhtsa.dot.gov/downloads/index.cfm 5point-wise mutual information 6log-likelihood ratio 7The bilingual dictionary was developed in the course of our Japanese language processing efforts described in (Sato et al., 2003). 8Alternatively, we could, for example, use canonical correlation analysis to match the vectors to a common latent vector space, like described in (Haghighi et al., 2008). κ·x·µT , 1 · µ = Z′ n i=1 Ei 657 column the translation probabilities for a word in English into any word in Japanese. Each context vector in English is then mapped into Japanese using the linear transformation described by the translation matrix T. For word x with context vector x in English, let x′ be its context vector after transformation into Japanese, i.e. x′ = T · x. The gold-standard was created by considering all n</context>
</contexts>
<marker>Sato, Ikeda, Nakata, Osada, 2003</marker>
<rawString>K. Sato, T. Ikeda, T. Nakata, and S. Osada. 2003. Introduction of a Japanese language processing middleware used for CRM. In Annual Meeting of the Japanese Association for Natural Language Processing (in Japanese), pages 109–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>Y Tateishi</author>
<author>J Kim</author>
<author>T Ohta</author>
<author>J McNaught</author>
<author>S Ananiadou</author>
<author>J Tsujii</author>
</authors>
<title>Developing a robust part-of-speech tagger for biomedical text.</title>
<date>2005</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>3746--382</pages>
<contexts>
<context position="9949" citStr="Tsuruoka et al., 2005" startWordPosition="1633" endWordPosition="1636"> xn, whereas the weights correspond to the frequencies of each word xi. 3. L2-normalize the weighted average. 4 Experiments As source and target language corpora we use a corpus extracted from a collection of complaints concerning automobiles compiled by the Japanese Ministry of Land, Infrastructure, Transport and Tourism (MLIT)3 and the USA National Highway Traffic Safety Administration (NHTSA)4, respectively. The Japanese corpus contains 24090 sentences that were POS tagged using MeCab (Kudo et al., 2004). The English corpus contains 47613 sentences, that were POS tagged using Stepp Tagger (Tsuruoka et al., 2005), and use the Lemmatizer (Okazaki et al., 2008) to extract and stem content words (nouns, verbs, adjectives, adverbs). For creating the context vectors, we calculate the association between two content words occurring in the same sentence, using the log-odds-ratio (Evert, 2004). It was shown in (Laroche and Langlais, 2010) that the log-odds-ratio in combination with the cosine-similarity performs superior to several other methods like PMIS and LLR6. For comparing two context vectors we use the cosine similarity. To transform the Japanese and English context vectors into the same vector space, </context>
</contexts>
<marker>Tsuruoka, Tateishi, Kim, Ohta, McNaught, Ananiadou, Tsujii, 2005</marker>
<rawString>Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught, S. Ananiadou, and J. Tsujii. 2005. Developing a robust part-of-speech tagger for biomedical text. Lecture Notes in Computer Science, 3746:382–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R R Wilcox</author>
</authors>
<title>Basic Statistics: Understanding Conventional Methods and Modern Insights.</title>
<date>2009</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="15786" citStr="Wilcox, 2009" startWordPosition="2586" endWordPosition="2587"> candidates suggested by the baseline methods and the proposed method is shown in Table 2. Using no smoothing, the baseline method outputs the correct translation for rI)L—X (cruise) and A )R (cruise) at rank 10 and 15, respectively. When combining both queries to form one context vector our proposed method (Syn Mises-Combination) retrieves the correct translation at rank 2. Note that we considered all nouns that occur three or more times as possible translation candidates. As can be seen in Table 2, this also includes spelling mistakes like ”sevice” and ”infromation”. 10We use the sign-test (Wilcox, 2009) to test the hypothesis that the proposed method ranks higher than the baseline. 11No normalization is performed before adding the context vectors. 658 Method Query Output Rank No Smoothing クルーズ ..., affinity, delco, cruise, sevice, sentrum,... 10 No Smoothing 巡航 ..., denali, attendant, cruise, abs, tactic,... 15 Top-k Smoothing クルーズ pillar, multi, cruise, star, affinity,... 3 Top-k Smoothing 巡航 ..., burnout, dipstick, cruise, infromation, speed,... 8 Syn Smoothing クルーズ smoothed with 巡航 ..., affinity, delco, cruise, sevice, sentrum,... 10 Syn Smoothing 巡航 smoothed with クルーズ ..., alldata, mode,</context>
</contexts>
<marker>Wilcox, 2009</marker>
<rawString>R.R. Wilcox. 2009. Basic Statistics: Understanding Conventional Methods and Modern Insights. Oxford University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>