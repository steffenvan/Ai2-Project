<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005350">
<title confidence="0.9852735">
EM Can Find Pretty Good HMM POS-Taggers
(When Given a Good Start)*
</title>
<author confidence="0.99458">
Yoav Goldberg and Meni Adler and Michael Elhadad
</author>
<affiliation confidence="0.999294">
Ben Gurion University of the Negev
Department of Computer Science
</affiliation>
<address confidence="0.886694">
POB 653 Be’er Sheva, 84105, Israel
</address>
<email confidence="0.998355">
{yoavg,adlerm,elhadad}@cs.bgu.ac.il
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994628">
We address the task of unsupervised POS tag-
ging. We demonstrate that good results can be
obtained using the robust EM-HMM learner
when provided with good initial conditions,
even with incomplete dictionaries. We present
a family of algorithms to compute effective
initial estimations p(t|w). We test the method
on the task of full morphological disambigua-
tion in Hebrew achieving an error reduction of
25% over a strong uniform distribution base-
line. We also test the same method on the stan-
dard WSJ unsupervised POS tagging task and
obtain results competitive with recent state-of-
the-art methods, while using simple and effi-
cient learning methods.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9816751">
The task of unsupervised (or semi-supervised) part-
of-speech (POS) tagging is the following: given a
dictionary mapping words in a language to their pos-
sible POS, and large quantities of unlabeled text
data, learn to predict the correct part of speech for
a given word in context. The only supervision given
to the learning process is the dictionary, which in
a realistic scenario, contains only part of the word
types observed in the corpus to be tagged.
Unsupervised POS tagging has been traditionally
approached with relative success (Merialdo, 1994;
Kupiec, 1992) by HMM-based generative mod-
els, employing EM parameters estimation using the
Baum-Welch algorithm. However, as recently noted
&apos;This work is supported in part by the Lynn and William
Frankel Center for Computer Science.
by Banko and Moore (2004), these works made use
of filtered dictionaries: dictionaries in which only
relatively probable analyses of a given word are pre-
served. This kind of filtering requires serious su-
pervision: in theory, an expert is needed to go over
the dictionary elements and filter out unlikely anal-
yses. In practice, counts from an annotated corpus
have been traditionally used to perform the filtering.
Furthermore, these methods require rather compre-
hensive dictionaries in order to perform well.
In recent work, researchers try to address these
deficiencies by using dictionaries with unfiltered
POS-tags, and testing the methods on “diluted dic-
tionaries” – in which many of the lexical entries are
missing (Smith and Eisner, 2005) (SE), (Goldwater
and Griffiths, 2007) (GG), (Toutanova and Johnson,
2008) (TJ).
All the work mentioned above focuses on unsu-
pervised English POS tagging. The dictionaries are
all derived from tagged English corpora (all recent
work uses the WSJ corpus). As such, the setting of
the research is artificial: there is no reason to per-
form unsupervised learning when an annotated cor-
pus is available. The problem is rather approached
as a workbench for exploring new learning methods.
The result is a series of creative algorithms, that have
steadily improved results on the same dataset: unsu-
pervised CRF training using contrastive estimation
(SE), a fully-bayesian HMM model that jointly per-
forms clustering and sequence learning (GG), and
a Bayesian LDA-based model using only observed
context features to predict tag words (TJ). These so-
phisticated learning algorithms all outperform the
traditional baseline of EM-HMM based methods,
</bodyText>
<page confidence="0.977411">
746
</page>
<note confidence="0.71465">
Proceedings of ACL-08: HLT, pages 746–754,
</note>
<page confidence="0.537767">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999843477272727">
while relying on similar knowledge: the lexical con-
text of the words to be tagged and their letter struc-
ture (e.g., presence of suffixes, capitalization and
hyphenation).1
Our motivation for tackling unsupervised POS
tagging is different: we are interested in develop-
ing a Hebrew POS tagger. We have access to a good
Hebrew lexicon (and a morphological analyzer), and
a fair amount of unlabeled training data, but hardly
any annotated corpora. We actually report results
on full morphological disambiguation for Hebrew, a
task similar but more challenging than POS tagging:
we deal with a tagset much larger than English (over
3,561 distinct tags) and an ambiguity level of about
2.7 per token as opposed to 1.4 for English. Instead
of inventing a new learning framework, we go back
to the traditional EM trained HMMs. We argue that
the key challenge to learning an effective model is
to define good enough initial conditions. Given suf-
ficiently good initial conditions, EM trained models
can yield highly competitive results. Such models
have other benefits as well: they are simple, robust,
and computationally more attractive.
In this paper, we concentrate on methods for de-
riving sufficiently good initial conditions for EM-
HMM learning. Our method for learning initial con-
ditions for the p(tjw) distributions relies on a mix-
ture of language specific models: a paradigmatic
model of similar words (where similar words are
words with similar inflection patterns), simple syn-
tagmatic constraints (e.g., the sequence V-V is ex-
tremely rare in English). These are complemented
by a linear lexical context model. Such models are
simple to build and test.
We present results for unsupervised PoS tagging
of Hebrew text and for the common WSJ English
test sets. We show that our method achieves state-of-
the-art results for the English setting, even with a rel-
atively small dictionary. Furthermore, while recent
work report results on a reduced English tagset of
17 PoS tags, we also present results for the complete
45 tags tagset of the WSJ corpus. This considerably
raises the bar of the EM-HMM baseline. We also
report state-of-the-art results for Hebrew full mor-
</bodyText>
<footnote confidence="0.9727115">
1Another notable work, though within a slightly differ-
ent framework, is the prototype-driven method proposed by
(Haghighi and Klein, 2006), in which the dictionary is replaced
with a very small seed of prototypical examples.
</footnote>
<bodyText confidence="0.997947">
phological disambiguation.
Our primary conclusion is that the problem of
learning effective stochastic classifiers remains pri-
marily a search task. Initial conditions play a domi-
nant role in solving this task and can rely on linguis-
tically motivated approximations. A robust learn-
ing method (EM-HMM) combined with good initial
conditions based on a robust feature set can go a
long way (as opposed to a more complex learning
method). It seems that computing initial conditions
is also the right place to capture complex linguistic
intuition without fear that over-generalization could
lead a learner to diverge.
</bodyText>
<sectionHeader confidence="0.995784" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.99979971875">
The tagging accuracy of supervised stochastic tag-
gers is around 96%–97% (Manning and Schutze,
1999). Merialdo (1994) reports an accuracy
of 86.6% for an unsupervised token-based EM-
estimated HMM, trained on a corpus of about 1M
words, over a tagset of 159 tags. Elworthy (1994), in
contrast, reports accuracy of 75.49%, 80.87%, and
79.12% for unsupervised word-based HMM trained
on parts of the LOB corpora, with a tagset of 134
tags. With (artificially created) good initial condi-
tions, such as a good approximation of the tag distri-
bution for each word, Elworthy reports an improve-
ment to 94.6%, 92.27%, and 94.51% on the same
data sets. Merialdo, on the other hand, reports an im-
provement to 92.6% and 94.4% for the case where
100 and 2,000 sentences of the training corpus are
manually tagged. Later, Banko and Moore (2004)
observed that earlier unsupervised HMM-EM re-
sults were artificially high due to use of Optimized
Lexicons, in which only frequent-enough analyses
of each word were kept. Brill (1995b) proposed
an unsupervised tagger based on transformation-
based learning (Brill, 1995a), achieving accuracies
of above 95%. This unsupervised tagger relied on
an initial step in which the most probable tag for
each word is chosen. Optimized lexicons and Brill’s
most-probable-tag Oracle are not available in realis-
tic unsupervised settings, yet, they show that good
initial conditions greatly facilitate learning.
Recent work on unsupervised POS tagging for
English has significantly improved the results on this
task: GG, SE and most recently TJ report the best re-
</bodyText>
<page confidence="0.99562">
747
</page>
<bodyText confidence="0.99985805882353">
sults so far on the task of unsupervised POS tagging
of the WSJ with diluted dictionaries. With dictionar-
ies as small as 1249 lexical entries the LDA-based
method with a strong ambiguity-class model reaches
POS accuracy as high as 89.7% on a reduced tagset
of 17 tags.
While these 3 methods rely on the same feature
set (lexical context, spelling features) for the learn-
ing stage, the LDA approach bases its predictions
entirely on observable features, and excludes the tra-
ditional hidden states sequence.
In Hebrew, Levinger et al. (1995) introduced the
similar-words algorithm for estimating p(t|w) from
unlabeled data, which we describe below. Our
method uses this algorithm as a first step, and refines
the approximation by introducing additional linguis-
tic constraints and an iterative refinement step.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="method">
3 Initial Conditions For EM-HMM
</sectionHeader>
<bodyText confidence="0.998573416666667">
The most common model for unsupervised learning
of stochastic processes is Hidden Markov Models
(HMM). For the case of tagging, the states corre-
spond to the tags ti, and words wi are emitted each
time a state is visited. The parameters of the model
can be estimated by applying the Baum-Welch EM
algorithm (Baum, 1972), on a large-scale corpus of
unlabeled text. The estimated parameters are then
used in conjunction with Viterbi search, to find the
most probable sequence of tags for a given sentence.
In this work, we follow Adler (2007) and use a vari-
ation of second-order HMM in which the probability
of a tag is conditioned by the tag that precedes it and
by the one that follows it, and the probability of an
emitted word is conditioned by its tag and the tag
that follows it2. In all experiments, we use the back-
off smoothing method of (Thede and Harper, 1999),
with additive smoothing (Chen, 1996) for the lexical
probabilities.
We investigate methods to approximate the initial
parameters of the p(t|w) distribution, from which
we obtain p(w|t) by marginalization and Bayesian
inversion. We also experiment with constraining the
p(t|t_1, t+1) distribution.
</bodyText>
<footnote confidence="0.56138875">
2Technically this is not Markov Model but a Dependency
Net. However, bidirectional conditioning seem more suitable
for language tasks, and in practice the learning and inference
methods are mostly unaffected. See (Toutanova et al., 2003).
</footnote>
<bodyText confidence="0.998261567567568">
General syntagmatic constraints We set linguis-
tically motivated constraints on the p(t|t_1, t+1)
distribution. In our setting, these are used to force
the probability of some events to 0 (e.g., “Hebrew
verbs can not be followed by the of preposition”).
Morphology-based p(t|w) approximation
Levinger et al. (1995) developed a context-free
method for acquiring morpho-lexical probabilities
(p(t|w)) from an untagged corpus. The method is
based on language-specific rules for constructing a
similar words (SW) set for each analysis of a word.
This set is composed of morphological variations
of the word under the given analysis. For example,
the Hebrew token דלי can be analyzed as either a
noun (boy) or a verb (gave birth). The noun SW set
for this token is composed of the definiteness and
number inflections םידליה,םידלי,דליה (the boy, boys,
the boys), while the verb SW set is composed
of gender and tense inflections ודלי,הדלי (she/they
gave birth). The approximated probability of each
analysis is based on the corpus frequency of its SW
set. For the complete details, refer to the original
paper. Cucerzan and Yarowsky (2000) proposed
a similar method for the unsupervised estimation
of p(t|w) in English, relying on simple spelling
features to characterize similar word classes.
Linear-Context-based p(t|w) approximation
The method of Levinger et al. makes use of Hebrew
inflection patterns in order to estimate context free
approximation of p(t|w) by relating a word to its
different inflections. However, the context in which
a word occurs can also be very informative with
respect to its POS-analysis (Sch¨utze, 1995). We
propose a novel algorithm for estimating p(t|w)
based on the contexts in which a word occurs.3
The algorithm starts with an initial p(t|w) esti-
mate, and iteratively re-estimates:
</bodyText>
<equation confidence="0.98658">
� wEW p(t|w)p(w|c)
�p(t|c) =
Z
EcERELC p(t|c)p(c|w)allow(t, w)
</equation>
<bodyText confidence="0.870992333333333">
3While we rely on the same intuition, our use of context
differs from earlier works on distributional POS-tagging like
(Sch¨utze, 1995), in which the purpose is to directly assign the
possible POS for an unknown word. In contrast, our algorithm
aims to improve the estimate for the whole distribution p(tIw),
to be further disambiguated by the EM-HMM learner.
</bodyText>
<equation confidence="0.994443">
Mt|w) = Z
</equation>
<page confidence="0.97325">
748
</page>
<bodyText confidence="0.9975467">
where Z is a normalization factor, W is the set of
all words in the corpus, C is the set of all contexts,
and RELc ⊆ C is a set of reliable contexts, defined
below. allow(t, w) is a binary function indicating
whether t is a valid tag for w. p(c|w) and p(w|c) are
estimated via raw corpus counts.
Intuitively, we estimate the probability of a tag
given a context as the average probability of a tag
given any of the words appearing in that context, and
similarly the probability of a tag given a word is the
averaged probability of that tag in all the (reliable)
contexts in which the word appears. At each round,
we define RELc, the set of reliable contexts, to be
the set of all contexts in which p(t|c) &gt; 0 for at most
X different ts.
The method is general, and can be applied to dif-
ferent languages. The parameters to specify for each
language are: the initial estimation p(t|w), the esti-
mation of the allow relation for known and OOV
words, and the types of contexts to consider.
</bodyText>
<sectionHeader confidence="0.938398" genericHeader="method">
4 Application to Hebrew
</sectionHeader>
<bodyText confidence="0.9999785">
In Hebrew, several words combine into a single to-
ken in both agglutinative and fusional ways. This
results in a potentially high number of tags for each
token. On average, in our corpus, the number of pos-
sible analyses per known word reached 2.7, with the
ambiguity level of the extended POS tagset in cor-
pus for English (1.41) (Dermatas and Kokkinakis,
1995).
In this work, we use the morphological analyzer
of MILA – Knowledge Center for Processing He-
brew (KC analyzer). In contrast to English tagsets,
the number of tags for Hebrew, based on all com-
binations of the morphological attributes, can grow
theoretically to about 300,000 tags. In practice, we
found ‘only’ about 3,560 tags in a corpus of 40M
tokens training corpus taken from Hebrew news ma-
terial and Knesset transcripts. For testing, we man-
ually tagged the text which is used in the Hebrew
Treebank (Sima’an et al., 2001) (about 90K tokens),
according to our tagging guidelines.
</bodyText>
<subsectionHeader confidence="0.867171">
4.1 Initial Conditions
</subsectionHeader>
<bodyText confidence="0.982576970588235">
General syntagmatic constraints We define 4
syntagmatic constraints over p(t|t_1, t+1): (1) a
construct state form cannot be followed by a verb,
preposition, punctuation, existential, modal, or cop-
ula; (2) a verb cannot be followed by the preposition
* ˇsel (of), (3) copula and existential cannot be fol-
lowed by a verb, and (4) a verb cannot be followed
by another verb, unless one of them has a prefix, or
the second verb is an infinitive, or the first verb is
imperative and the second verb is in future tense.4
Morphology-Based p(t|w) approximation We
extended the set of rules used in Levinger et al. , in
order to support the wider tagset used by the KC an-
alyzer: (1) The SW set for adjectives, copulas, exis-
tentials, personal pronouns, verbs and participles, is
composed of all gender-number inflections; (2) The
SW set for common nouns is composed of all num-
ber inflections, with definite article variation for ab-
solute noun; (3) Prefix variations for proper nouns;
(4) Gender variation for numerals; and (5) Gender-
number variation for all suffixes (possessive, nomi-
native and accusative).
Linear-Context-based p(t|w) approximation
For the initial p(t|w) we use either a uniform distri-
bution based on the tags allowed in the dictionary,
or the estimate obtained by using the modified
Levinger et al. algorithm. We use contexts of the
form LR=w_1, w+1 (the neighbouring words). We
estimate p(w|c) and p(c|w) via relative frequency
over all the events w1, w2, w3 occurring at least
10 times in the corpus. allow(t, w) follows the
dictionary. Because of the wide coverage of the
Hebrew lexicon, we take RELc to be C (all
available contexts).
</bodyText>
<subsectionHeader confidence="0.91929">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.998400818181818">
We run a series of experiments with 8 distinct ini-
tial conditions, as shown in Table 1: our baseline
(Uniform) is the uniform distribution over all tags
provided by the KC analyzer for each word. The
Syntagmatic initial conditions add the p(t|t_1, t+1)
constraints described above to the uniform base-
line. The Morphology-Based and Linear-Context
initial conditions are computed as described above,
while the Morph+Linear is the result of applying
the linear-context algorithm over initial values com-
puted by the Morphology-based method. We repeat
</bodyText>
<footnote confidence="0.984256">
4This rule was taken from Shacham and Wintner(2007).
</footnote>
<page confidence="0.971258">
749
</page>
<table confidence="0.999945571428571">
Initial Condition Dist Context-Free EM-HMM
Full Seg+Pos Full Seg+Pos
Uniform 60 63.8 71.9 85.5 89.8
Pair Constraints 60 / / 85.8 89.8
Syntagmatic Init-Trans 60 / / 87.9 91
Morph-Based 76.8 76.4 83.1 87.7 91.6
Morpho-Lexical Linear-Context 70.1 75.4 82.6 85.3 89.6
Morph+Linear 79.8 79.0 85.5 88 92
Morph-Based / / / 87.6 91.4
PairConst+Morph Linear-Context / / / 84.5 89.0
Morph+Linear / / / 87.1 91.5
Morph-Based / / / 89.2 92.3
InitTrans+Morph Linear-Context / / / 87.7 90.9
Morph+Linear / / / 89.4 92.4
</table>
<tableCaption confidence="0.974404">
Table 1: Accuracy (%) of Hebrew Morphological
Disambiguation and POS Tagging over various initial
conditions
</tableCaption>
<bodyText confidence="0.997999685185185">
these last 3 models with the addition of the syntag-
matic constraints (Synt+Morph).
For each of these, we first compare the computed
p(tjw) against a gold standard distribution, taken
from the test corpus (90K tokens), according to the
measure used by (Levinger et al., 1995) (Dist). On
this measure, we confirm that our improved morpho-
lexical approximation improves the results reported
by Levinger et al. from 74% to about 80% on a
richer tagset, and on a much larger test set (90K vs.
3,400 tokens).
We then report on the effectiveness of p(tjw) as
a context-free tagger that assigns to each word the
most likely tag, both for full morphological analy-
sis (3,561 tags) (Full) and for the simpler task of
token segmentation and POS tag selection (36 tags)
(Seg+Pos). The best results on this task are 80.8%
and 87.5% resp. achieved on the Morph+Linear ini-
tial conditions.
Finally, we test effectiveness of the initial con-
ditions with EM-HMM learning. We reach 88%
accuracy on full morphological and 92% accuracy
for POS tagging and word segmentation, for the
Morph+Linear initial conditions.
As expected, EM-HMM improves results (from
80% to 88%). Strikingly, EM-HMM improves the
uniform initial conditions from 64% to above 85%.
However, better initial conditions bring us much
over this particular local maximum – with an error
reduction of 20%. In all cases, the main improve-
ment over the uniform baseline is brought by the
morphology-based initial conditions. When applied
on its own, the linear context brings modest im-
provement. But the combination of the paradigmatic
morphology-based method with the linear context
improves all measures.
A most interesting observation is the detrimental
contribution of the syntagmatic constraints we in-
troduced. We found that 113,453 sentences of the
corpus (about 5%) contradict these basic and ap-
parently simple constraints. As an alternative to
these common-sense constraints, we tried to use a
small seed of randomly selected sentences (10K an-
notated tokens) in order to skew the initial uniform
distribution of the state transitions. We initialize the
p(tjt_1, t+1) distribution with smoothed ML esti-
mates based on tag trigram and bigram counts (ig-
noring the tag-word annotations). This small seed
initialization (InitTrans) has a great impact on ac-
curacy. Overall, we reach 89.4% accuracy on full
morphological and 92.4% accuracy for POS tagging
and word segmentation, for the Morph+Linear con-
ditions – an error reduction of more than 25% from
the uniform distribution baseline.
</bodyText>
<sectionHeader confidence="0.954961" genericHeader="method">
5 Application to English
</sectionHeader>
<bodyText confidence="0.999947294117647">
We now apply the same technique to English semi-
supervised POS tagging. Recent investigations of
this task use dictionaries derived from the Penn WSJ
corpus, with a reduced tag set of 17 tags5 instead of
the original 45-tags tagset. They experiment with
full dictionaries (containing complete POS informa-
tion for all the words in the text) as well as “diluted”
dictionaries, from which large portions of the vo-
cabulary are missing. These settings are very dif-
ferent from those used for Hebrew: the tagset is
much smaller (17 vs. 3,560) and the dictionaries
are either complete or extremely crippled. However,
for the sake of comparison, we have reproduced the
same experimental settings.
We derive dictionaries from the complete WSJ
corpus6, and the exact same diluted dictionaries used
in SE, TJ and GG.
</bodyText>
<footnote confidence="0.963636666666667">
5ADJ ADV CONJ DET ENDPUNC INPUNC LPUNC
RPUNC N POS PRT PREP PRT TO V VBG VBN WH
6The dictionary derived from the WSJ data is very noisy:
</footnote>
<bodyText confidence="0.894240375">
many of the stop words get wrong analyses stemming from tag-
ging mistakes (for instance, the word the has 6 possible analyses
in the data-derived dictionary, which we checked manually and
found all but DT erroneous). Such noise is not expected in a real
world dictionary, and our algorithm is not designed to accomo-
date it. We corrected the entries for the 20 most frequent words
in the corpus. This step could probably be done automatically,
but we consider it to be a non-issue in any realistic setting.
</bodyText>
<page confidence="0.991621">
750
</page>
<bodyText confidence="0.983476641304348">
Syntagmatic Constraints We indirectly incor-
porated syntagmatic constraints through a small
change to the tagset. The 17-tags English tagset
allows for V-V transitions. Such a construction is
generally unlikely in English. By separating modals
from the rest of the verbs, and creating an addi-
tional class for the 5 be verbs (am,is,are,was,were),
we made such transition much less probable. The
new 19-tags tagset reflects the “verb can not follow
a verb” constraint.
Morphology-Based p(t|w) approximation En-
glish morphology is much simpler compared to that
of Hebrew, making direct use of the Levinger con-
text free approximation impossible. However, some
morphological cues exist in English as well, in par-
ticular common suffixation patterns. We imple-
mented our morphology-based context-free p(t|w)
approximation for English as a special case of the
linear context-based algorithm described in Sect.3.
Instead of generating contexts based on neighboring
words, we generate them using the following 5 mor-
phological templates:
suff=S The word has suffix 5 (suff=ing).
L+suff=W,S The word appears just after word W,
with suffix 5 (L+suff=have,ed).
R+suff=S,W The word appears just before word W,
with suffix 5 (R+suff=ing,to)
wsuf=S1,S2 The word suffix is 51, the same stem is
seen with suffix 52 (wsuf=E,s).
suffs=SG The word stem appears with the 5G group
of suffixes (suffs=ed,ing,s).
We consider a word to have a suffix only if the
word stem appears with a different suffix somewhere
in the text. We implemented a primitive stemmer
for extracting the suffixes while preserving a us-
able stem by taking care of few English orthogra-
phy rules (handling, e.g., , bigger → big er, nicer
→ nice er, happily → happy ly, picnicking → pic-
nic ing). For the immediate context W in the tem-
plates L+suff,R+suff, we consider only the 20 most
frequent tokens in the corpus.
Linear-Context-based p(t|w) approximation
We expect the context based approximation to be
particularly useful in English. We use the following
3 context templates: LL=w_2,w_1, LR=w_1,w+1
and RR=w+1,w+2. We estimate p(w|c) and p(c|w)
by relative frequency over word triplets occurring at
least twice in the unannotated training corpus.
Combined p(t|w) approximation This approx-
imation combines the morphological and linear
context approximations by using all the above-
mentioned context templates together in the iterative
process.
For all three p(t|w) approximations, we take
RELC to be contexts containing at most 4 tags.
allow(t, w) follows the dictionary for known words,
and is the set of all open-class POS for unknown
words. We take the initial p(t|w) for each w to be
uniform over all the dictionary specified tags for w.
Accordingly, the initial p(t|w) = 0 for w not in the
dictionary. We run the process for 8 iterations.7
Diluted Dictionaries and Unknown Words
Some of the missing dictionary elements are as-
signed a set of possible POS-tags and corresponding
probabilities in the p(t|w) estimation process. Other
unknown tokens remain with no analysis at the
end of the initial process computation. For these
missing elements, we assign an ambiguity class by
a simple ambiguity-class guesser, and set p(t|w)
to be uniform over all the tags in the ambiguity
class. Our ambiguity-class guesser assigns for each
word the set of all open-class tags that appeared
with the word suffix in the dictionary. The word
suffix is the longest (up to 3 characters) suffix of the
word that also appears in the top-100 suffixes in the
dictionary.
Taggers We test the resulting p(t|w) approxima-
tion by training 2 taggers: CF-Tag, a context-free
tagger assigning for each word its most probable
POS according to p(t|w), with a fallback to the most
probable tag in case the word does not appear in
the dictionary or if ∀t, p(t|w) = 0. EM-HMM,
a second-order EM-HMM initialized with the esti-
mated p(t|w).
Baselines As baseline, we use two EM-trained
HMM taggers, initialized with a uniform p(t|w) for
every word, based on the allowed tags in the dic-
tionary. For words not in the dictionary, we take
the allowed tags to be either all the open-class POS
7This is the first value we tried, and it seems to work fine.
We haven’t experimented with other values. The same applies
for the choice of 4 as the RELC threshold.
</bodyText>
<page confidence="0.995951">
751
</page>
<bodyText confidence="0.99992025">
(uniform(oc)) or the allowed tags according to our
simple ambiguity-class guesser (uniform(suf)).
All the p(tjw) estimates and HMM models are
trained on the entire WSJ corpus. We use the same
24K word test-set as used in SE, TJ and GG, as well
as the same diluted dictionaries. We report the re-
sults on the same reduced tagsets for comparison,
but also include the results on the full 46 tags tagset.
</bodyText>
<subsectionHeader confidence="0.702277">
5.1 Results
</subsectionHeader>
<bodyText confidence="0.999926906976745">
Table 2 summarizes the results of our experiments.
Uniform initialization based on the simple suffix-
based ambiguity class guesser yields big improve-
ments over the uniform all-open-class initialization.
However, our refined initial conditions always im-
prove the results (by as much as 40% error re-
duction). As expected, the linear context is much
more effective than the morphological one, espe-
cially with richer dictionaries. This seem to indi-
cate that in English the linear context is better at re-
fining the estimations when the ambiguity classes
are known, while the morphological context is in
charge of adding possible tags when the ambigu-
ity classes are not known. Furthermore, the bene-
fit of the morphology-context is bigger for the com-
plete tagset setting, indicating that, while the coarse-
grained POS-tags are indicated by word distribu-
tion, the finer distinctions are indicated by inflec-
tions and orthography. The combination of linear
and morphology contexts is always beneficial. Syn-
tagmatic constraints (e.g., separating be verbs and
modals from the rest of the verbs) constantly im-
prove results by about 1%. Note that the context-free
tagger based on our p(tjw) estimates is quite accu-
rate. As with the EM trained models, combining lin-
ear and morphological contexts is always beneficial.
To put these numbers in context, Table 3 lists
current state-of-the art results for the same task.
CE+spl is the Contrastive-Estimation CRF method
of SE. BHMM is the completely Bayesian-HMM
of GG. PLSA+AC, LDA, LDA+AC are the mod-
els presented in TJ, LDA+AC is a Bayesian model
with a strong ambiguity class (AC) component, and
is the current state-of-the-art of this task. The other
models are variations excluding the Bayesian com-
ponents (PLSA+AC) or the ambiguity class.
While our models are trained on the unannotated
text of the entire WSJ Treebank, CE and BHMM use
much less training data (only the 24k words of the
test-set). However, as noted by TJ, there is no reason
one should limit the amount of unlabeled data used,
and in addition other results reported in GG,SE show
that accuracy does not seem to improve as more un-
labeled data are used with the models. We also re-
port results for training our EM-HMM tagger on the
smaller dataset (the p(tjw) estimation is still based
on the entire unlabeled WSJ).
All the abovementioned models follow the as-
sumption that all 17 tags are valid for the unknown
words. In contrast, we restrict the set of allowed
tags for an unknown word to open-class tags. Closed
class words are expected to be included in a dictio-
nary, even a small one. The practice of allowing only
open-class tags for unknown words goes back a long
way (Weischedel et al., 1993), and proved highly
beneficial also in our case.
Notice that even our simplest models, in which
the initial p(tjw) distribution for each w is uniform,
already outperform most of the other models, and,
in the case of the diluted dictionaries, by a wide
margin. Similarly, given the p(tjw) estimate, EM-
HMM training on the smaller dataset (24k) is still
very competitive (yet results improve with more un-
labeled data). When we use our refined p(tjw) dis-
tribution as the basis of EM-HMM training, we get
the best results for the complete dictionary case.
With the diluted dictionaries, we are outperformed
only by LDA+AC. As we outperform this model in
the complete dictionary case, it seems that the ad-
vantage of this model is due to its much stronger
ambiguity class model, and not its Bayesian com-
ponents. Also note that while we outperform this
model when using the 19-tags tagset, it is slightly
better in the original 17-tags setting. It could be that
the reliance of the LDA models on observed surface
features instead of hidden state features is beneficial
avoiding the misleading V-V transitions.
We also list the performance of our best mod-
els with a slightly more realistic dictionary setting:
we take our dictionary to include information for all
words occurring in section 0-18 of the WSJ corpus
(43208 words). We then train on the entire unanno-
tated corpus, and test on sections 22-24 – the stan-
dard train/test split for supervised English POS tag-
ging. We achieve accuracy of 92.85% for the 19-
tags set, and 91.3% for the complete 46-tags tagset.
</bodyText>
<page confidence="0.991148">
752
</page>
<table confidence="0.999862166666667">
Initial Conditions Full dict ≥ 2 dict ≥ 3 dict
(49206 words) (2141 words) (1249 words)
CF-Tag EM-HMM CF-Tag EM-HMM CF-Tag EM-HMM
Uniform(oc) 81.7 88.7 68.4 81.9 62.5 79.6
Uniform(suf) NA NA 76.8 83.4 76.9 81.6
17tags Morph-Cont 82.2 88.6 73.3 83.9 69.1 81.7
Linear-Cont 90.1 92.9 81.1 87.8 78.3 85.8
Combined-Cont 89.9 93.3 83.1 88.5 81.1 86.4
Uniform(oc) 79.9 91.0 66.6 83.4 60.7 84.7
Uniform(suf) NA NA 75.1 86.5 73.1 86.7
19tags Morph-Cont 80.5 89.2 71.5 86.5 67.5 87.1
Linear-Cont 88.4 93.7 78.9 89.0 76.3 86.9
Combined-Cont 88.0 93.8 81.1 89.4 79.2 87.4
Uniform(oc) 76.7 88.3 61.2 * 55.7 *
Uniform(suf) NA NA 64.2 81.9 60.3 79.8
46tags Morph-Cont 74.8 88.8 65.6 83.0 61.9 80.3
Linear-Cont 85.5 91.2 74.5 84.0 70.1 82.2
Combined-Cont 85.9 91.4 75.4 85.5 72.4 83.3
</table>
<tableCaption confidence="0.98042">
Table 2: Accuracy (%) of English POS Tagging over various initial conditions
</tableCaption>
<table confidence="0.9998545">
Dict InitEM-HMM (24k) LDA LDA+AC PLSA+AC CE+spl BHMM
Full 93.8 (91.1) 93.4 93.4 89.7 88.7 87.3
≥ 2 89.4 (87.9) 87.4 91.2 87.8 79.5 79.6
≥ 3 87.4 (85.9) 85 89.7 85.9 78.4 71
</table>
<tableCaption confidence="0.999949">
Table 3: Comparison of English Unsupervised POS Tagging Methods
</tableCaption>
<sectionHeader confidence="0.997456" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999989431372549">
We have demonstrated that unsupervised POS tag-
ging can reach good results using the robust EM-
HMM learner when provided with good initial con-
ditions, even with incomplete dictionaries. We pre-
sented a general family of algorithms to compute ef-
fective initial conditions: estimation of p(t|w) rely-
ing on an iterative process shifting probabilities be-
tween words and their contexts. The parameters of
this process (definition of the contexts and initial es-
timations of p(t|w) can safely encapsulate rich lin-
guistic intuitions.
While recent work, such as GG, aim to use the
Bayesian framework and incorporate “linguistically
motivated priors”, in practice such priors currently
only account for the fact that language related dis-
tributions are sparse - a very general kind of knowl-
edge. In contrast, our method allow the incorpora-
tion of much more fine-grained intuitions.
We tested the method on the challenging task
of full morphological disambiguation in Hebrew
(which was our original motivation) and on the stan-
dard WSJ unsupervised POS tagging task.
In Hebrew, our model includes an improved ver-
sion of the similar words algorithm of (Levinger et
al., 1995), a model of lexical context, and a small
set of tag ngrams. The combination of these knowl-
edge sources in the initial conditions brings an error
reduction of more than 25% over a strong uniform
distribution baseline. In English, our model is com-
petitive with recent state-of-the-art results, while us-
ing simple and efficient learning methods.
The comparison with other algorithms indicates
directions of potential improvement: (1) our initial-
conditions method might benefit the other, more so-
phisticated learning algorithms as well. (2) Our
models were designed under the assumption of a
relatively complete dictionary. As such, they are
not very good at assigning ambiguity-classes to
OOV tokens when starting with a very small dic-
tionary. While we demonstrate competitive results
using a simple suffix-based ambiguity-class guesser
which ignores capitalization and hyphenation infor-
mation, we believe there is much room for improve-
ment in this respect. In particular, (Haghighi and
Klein, 2006) presents very strong results using a
distributional-similarity module and achieve impres-
sive tagging accuracy while starting with a mere
116 prototypical words. Experimenting with com-
bining similar models (as well as TJ’s ambiguity
class model) with our p(t|w) distribution estimation
method is an interesting research direction.
</bodyText>
<page confidence="0.998402">
753
</page>
<sectionHeader confidence="0.995872" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999891905263158">
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of Coling
2004, pages 556–561, Geneva, Switzerland, Aug 23–
Aug 27. COLING.
Leonard E. Baum. 1972. An inequality and associ-
ated maximization technique in statistical estimation
for probabilistic functions of a Markov process. In-
equalities, 3:1–8.
Eric Brill. 1995a. Transformation-based error-driven
learning and natural languge processing: A case study
in part-of-speech tagging. Computational Linguistics,
21:543–565.
Eric Brill. 1995b. Unsupervised learning of disam-
biguation rules for part of speech tagging. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
1–13, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Stanley F. Chen. 1996. Building Probabilistic Models for
Natural Language. Ph.D. thesis, Harvard University,
Cambridge, MA.
Silviu Cucerzan and David Yarowsky. 2000. Language
independent, minimally supervised induction of lex-
ical probabilities. In ACL ’00: Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 270–277, Morristown, NJ,
USA. Association for Computational Linguistics.
Evangelos Dermatas and George Kokkinakis. 1995. Au-
tomatic stochastic tagging of natural language texts.
Computational Linguistics, 21(2):137–163.
David Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Proceeding of ANLP-94.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully bayesian approach to unsupervised part-of-
speech tagging. In Proceeding of ACL 2007, Prague,
Czech Republic.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 320–
327, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
J. Kupiec. 1992. Robust part-of-speech tagging using
hidden Markov model. Computer Speech and Lan-
guage, 6:225–242.
Moshe Levinger, Uzi Ornan, and Alon Itai. 1995. Learn-
ing morpholexical probabilities from an untagged cor-
pus with an application to Hebrew. Computational
Linguistics, 21:383–404.
Christopher D. Manning and Hinrich Schutze. 1999.
Foundation of Statistical Language Processing. MIT
Press.
Bernard Merialdo. 1994. Tagging English text
with probabilistic model. Computational Linguistics,
20:155–171.
Hinrich Sch¨utze. 1995. Distributional part-of-speech
tagging. In Proceedings of the seventh conference
on European chapter of the Association for Computa-
tional Linguistics, pages 141–148, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Danny Shacham and Shuly Wintner. 2007. Morpho-
logical disambiguation of hebrew: A case study in
classifier combination. In Proceeding of EMNLP-07,
Prague, Czech.
Khalil Sima’an, Alon Itai, Alon Altman Yoad Winter,
and Noa Nativ. 2001. Building a tree-bank of mod-
ern Hebrew text. Journal Traitement Automatique des
Langues (t.a.l.). Special Issue on NLP and Corpus
Linguistics.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
354–362, Ann Arbor, Michigan, June.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden Markov model for part-of-speech tag-
ging. In Proceeding of ACL-99.
Kristina Toutanova and Mark Johnson. 2008. A bayesian
lda-based model for semi-supervised part-of-speech
tagging. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20. MIT Press, Cambridge, MA.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In HLT-
NAACL.
R. Weischedel, R. Schwartz, J. Palmucci, M. Meteer, and
L. Ramshaw. 1993. Coping with ambiguity and un-
known words through probabilistic models. Computa-
tional Linguistics, 19:359–382.
</reference>
<page confidence="0.998881">
754
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.443102">
<title confidence="0.9007315">EM Can Find Pretty Good HMM POS-Taggers Given a Good</title>
<author confidence="0.815175">Goldberg Adler Elhadad Ben Gurion University of the Negev</author>
<affiliation confidence="0.998631">Department of Computer Science</affiliation>
<address confidence="0.961806">POB 653 Be’er Sheva, 84105, Israel</address>
<abstract confidence="0.999352125">We address the task of unsupervised POS tagging. We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries. We present a family of algorithms to compute effective estimations We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline. We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efficient learning methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Meni Adler</author>
</authors>
<title>Hebrew Morphological Disambiguation: An Unsupervised Stochastic Word-based Approach.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Ben-Gurion University of the Negev,</institution>
<location>Beer-Sheva, Israel.</location>
<contexts>
<context position="9489" citStr="Adler (2007)" startWordPosition="1503" endWordPosition="1504">raints and an iterative refinement step. 3 Initial Conditions For EM-HMM The most common model for unsupervised learning of stochastic processes is Hidden Markov Models (HMM). For the case of tagging, the states correspond to the tags ti, and words wi are emitted each time a state is visited. The parameters of the model can be estimated by applying the Baum-Welch EM algorithm (Baum, 1972), on a large-scale corpus of unlabeled text. The estimated parameters are then used in conjunction with Viterbi search, to find the most probable sequence of tags for a given sentence. In this work, we follow Adler (2007) and use a variation of second-order HMM in which the probability of a tag is conditioned by the tag that precedes it and by the one that follows it, and the probability of an emitted word is conditioned by its tag and the tag that follows it2. In all experiments, we use the backoff smoothing method of (Thede and Harper, 1999), with additive smoothing (Chen, 1996) for the lexical probabilities. We investigate methods to approximate the initial parameters of the p(t|w) distribution, from which we obtain p(w|t) by marginalization and Bayesian inversion. We also experiment with constraining the p</context>
</contexts>
<marker>Adler, 2007</marker>
<rawString>Meni Adler. 2007. Hebrew Morphological Disambiguation: An Unsupervised Stochastic Word-based Approach. Ph.D. thesis, Ben-Gurion University of the Negev, Beer-Sheva, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Robert C Moore</author>
</authors>
<title>Part-ofspeech tagging in context.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>556--561</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="1737" citStr="Banko and Moore (2004)" startWordPosition="268" endWordPosition="271">abeled text data, learn to predict the correct part of speech for a given word in context. The only supervision given to the learning process is the dictionary, which in a realistic scenario, contains only part of the word types observed in the corpus to be tagged. Unsupervised POS tagging has been traditionally approached with relative success (Merialdo, 1994; Kupiec, 1992) by HMM-based generative models, employing EM parameters estimation using the Baum-Welch algorithm. However, as recently noted &apos;This work is supported in part by the Lynn and William Frankel Center for Computer Science. by Banko and Moore (2004), these works made use of filtered dictionaries: dictionaries in which only relatively probable analyses of a given word are preserved. This kind of filtering requires serious supervision: in theory, an expert is needed to go over the dictionary elements and filter out unlikely analyses. In practice, counts from an annotated corpus have been traditionally used to perform the filtering. Furthermore, these methods require rather comprehensive dictionaries in order to perform well. In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and te</context>
<context position="7359" citStr="Banko and Moore (2004)" startWordPosition="1162" endWordPosition="1165">n a corpus of about 1M words, over a tagset of 159 tags. Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tagset of 134 tags. With (artificially created) good initial conditions, such as a good approximation of the tag distribution for each word, Elworthy reports an improvement to 94.6%, 92.27%, and 94.51% on the same data sets. Merialdo, on the other hand, reports an improvement to 92.6% and 94.4% for the case where 100 and 2,000 sentences of the training corpus are manually tagged. Later, Banko and Moore (2004) observed that earlier unsupervised HMM-EM results were artificially high due to use of Optimized Lexicons, in which only frequent-enough analyses of each word were kept. Brill (1995b) proposed an unsupervised tagger based on transformationbased learning (Brill, 1995a), achieving accuracies of above 95%. This unsupervised tagger relied on an initial step in which the most probable tag for each word is chosen. Optimized lexicons and Brill’s most-probable-tag Oracle are not available in realistic unsupervised settings, yet, they show that good initial conditions greatly facilitate learning. Rece</context>
</contexts>
<marker>Banko, Moore, 2004</marker>
<rawString>Michele Banko and Robert C. Moore. 2004. Part-ofspeech tagging in context. In Proceedings of Coling 2004, pages 556–561, Geneva, Switzerland, Aug 23– Aug 27. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation for probabilistic functions of a Markov process.</title>
<date>1972</date>
<journal>Inequalities,</journal>
<volume>3</volume>
<contexts>
<context position="9268" citStr="Baum, 1972" startWordPosition="1466" endWordPosition="1467">ced the similar-words algorithm for estimating p(t|w) from unlabeled data, which we describe below. Our method uses this algorithm as a first step, and refines the approximation by introducing additional linguistic constraints and an iterative refinement step. 3 Initial Conditions For EM-HMM The most common model for unsupervised learning of stochastic processes is Hidden Markov Models (HMM). For the case of tagging, the states correspond to the tags ti, and words wi are emitted each time a state is visited. The parameters of the model can be estimated by applying the Baum-Welch EM algorithm (Baum, 1972), on a large-scale corpus of unlabeled text. The estimated parameters are then used in conjunction with Viterbi search, to find the most probable sequence of tags for a given sentence. In this work, we follow Adler (2007) and use a variation of second-order HMM in which the probability of a tag is conditioned by the tag that precedes it and by the one that follows it, and the probability of an emitted word is conditioned by its tag and the tag that follows it2. In all experiments, we use the backoff smoothing method of (Thede and Harper, 1999), with additive smoothing (Chen, 1996) for the lexi</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>Leonard E. Baum. 1972. An inequality and associated maximization technique in statistical estimation for probabilistic functions of a Markov process. Inequalities, 3:1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural languge processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--543</pages>
<contexts>
<context position="7541" citStr="Brill (1995" startWordPosition="1192" endWordPosition="1193"> corpora, with a tagset of 134 tags. With (artificially created) good initial conditions, such as a good approximation of the tag distribution for each word, Elworthy reports an improvement to 94.6%, 92.27%, and 94.51% on the same data sets. Merialdo, on the other hand, reports an improvement to 92.6% and 94.4% for the case where 100 and 2,000 sentences of the training corpus are manually tagged. Later, Banko and Moore (2004) observed that earlier unsupervised HMM-EM results were artificially high due to use of Optimized Lexicons, in which only frequent-enough analyses of each word were kept. Brill (1995b) proposed an unsupervised tagger based on transformationbased learning (Brill, 1995a), achieving accuracies of above 95%. This unsupervised tagger relied on an initial step in which the most probable tag for each word is chosen. Optimized lexicons and Brill’s most-probable-tag Oracle are not available in realistic unsupervised settings, yet, they show that good initial conditions greatly facilitate learning. Recent work on unsupervised POS tagging for English has significantly improved the results on this task: GG, SE and most recently TJ report the best re747 sults so far on the task of uns</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995a. Transformation-based error-driven learning and natural languge processing: A case study in part-of-speech tagging. Computational Linguistics, 21:543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Unsupervised learning of disambiguation rules for part of speech tagging.</title>
<date>1995</date>
<booktitle>In David Yarovsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>1--13</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="7541" citStr="Brill (1995" startWordPosition="1192" endWordPosition="1193"> corpora, with a tagset of 134 tags. With (artificially created) good initial conditions, such as a good approximation of the tag distribution for each word, Elworthy reports an improvement to 94.6%, 92.27%, and 94.51% on the same data sets. Merialdo, on the other hand, reports an improvement to 92.6% and 94.4% for the case where 100 and 2,000 sentences of the training corpus are manually tagged. Later, Banko and Moore (2004) observed that earlier unsupervised HMM-EM results were artificially high due to use of Optimized Lexicons, in which only frequent-enough analyses of each word were kept. Brill (1995b) proposed an unsupervised tagger based on transformationbased learning (Brill, 1995a), achieving accuracies of above 95%. This unsupervised tagger relied on an initial step in which the most probable tag for each word is chosen. Optimized lexicons and Brill’s most-probable-tag Oracle are not available in realistic unsupervised settings, yet, they show that good initial conditions greatly facilitate learning. Recent work on unsupervised POS tagging for English has significantly improved the results on this task: GG, SE and most recently TJ report the best re747 sults so far on the task of uns</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995b. Unsupervised learning of disambiguation rules for part of speech tagging. In David Yarovsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora, pages 1–13, Somerset, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Building Probabilistic Models for Natural Language.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="9855" citStr="Chen, 1996" startWordPosition="1571" endWordPosition="1572">M algorithm (Baum, 1972), on a large-scale corpus of unlabeled text. The estimated parameters are then used in conjunction with Viterbi search, to find the most probable sequence of tags for a given sentence. In this work, we follow Adler (2007) and use a variation of second-order HMM in which the probability of a tag is conditioned by the tag that precedes it and by the one that follows it, and the probability of an emitted word is conditioned by its tag and the tag that follows it2. In all experiments, we use the backoff smoothing method of (Thede and Harper, 1999), with additive smoothing (Chen, 1996) for the lexical probabilities. We investigate methods to approximate the initial parameters of the p(t|w) distribution, from which we obtain p(w|t) by marginalization and Bayesian inversion. We also experiment with constraining the p(t|t_1, t+1) distribution. 2Technically this is not Markov Model but a Dependency Net. However, bidirectional conditioning seem more suitable for language tasks, and in practice the learning and inference methods are mostly unaffected. See (Toutanova et al., 2003). General syntagmatic constraints We set linguistically motivated constraints on the p(t|t_1, t+1) dis</context>
</contexts>
<marker>Chen, 1996</marker>
<rawString>Stanley F. Chen. 1996. Building Probabilistic Models for Natural Language. Ph.D. thesis, Harvard University, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>David Yarowsky</author>
</authors>
<title>Language independent, minimally supervised induction of lexical probabilities.</title>
<date>2000</date>
<booktitle>In ACL ’00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>270--277</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11487" citStr="Cucerzan and Yarowsky (2000)" startWordPosition="1818" endWordPosition="1821">SW) set for each analysis of a word. This set is composed of morphological variations of the word under the given analysis. For example, the Hebrew token דלי can be analyzed as either a noun (boy) or a verb (gave birth). The noun SW set for this token is composed of the definiteness and number inflections םידליה,םידלי,דליה (the boy, boys, the boys), while the verb SW set is composed of gender and tense inflections ודלי,הדלי (she/they gave birth). The approximated probability of each analysis is based on the corpus frequency of its SW set. For the complete details, refer to the original paper. Cucerzan and Yarowsky (2000) proposed a similar method for the unsupervised estimation of p(t|w) in English, relying on simple spelling features to characterize similar word classes. Linear-Context-based p(t|w) approximation The method of Levinger et al. makes use of Hebrew inflection patterns in order to estimate context free approximation of p(t|w) by relating a word to its different inflections. However, the context in which a word occurs can also be very informative with respect to its POS-analysis (Sch¨utze, 1995). We propose a novel algorithm for estimating p(t|w) based on the contexts in which a word occurs.3 The </context>
</contexts>
<marker>Cucerzan, Yarowsky, 2000</marker>
<rawString>Silviu Cucerzan and David Yarowsky. 2000. Language independent, minimally supervised induction of lexical probabilities. In ACL ’00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 270–277, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evangelos Dermatas</author>
<author>George Kokkinakis</author>
</authors>
<title>Automatic stochastic tagging of natural language texts.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="13971" citStr="Dermatas and Kokkinakis, 1995" startWordPosition="2244" endWordPosition="2247">The method is general, and can be applied to different languages. The parameters to specify for each language are: the initial estimation p(t|w), the estimation of the allow relation for known and OOV words, and the types of contexts to consider. 4 Application to Hebrew In Hebrew, several words combine into a single token in both agglutinative and fusional ways. This results in a potentially high number of tags for each token. On average, in our corpus, the number of possible analyses per known word reached 2.7, with the ambiguity level of the extended POS tagset in corpus for English (1.41) (Dermatas and Kokkinakis, 1995). In this work, we use the morphological analyzer of MILA – Knowledge Center for Processing Hebrew (KC analyzer). In contrast to English tagsets, the number of tags for Hebrew, based on all combinations of the morphological attributes, can grow theoretically to about 300,000 tags. In practice, we found ‘only’ about 3,560 tags in a corpus of 40M tokens training corpus taken from Hebrew news material and Knesset transcripts. For testing, we manually tagged the text which is used in the Hebrew Treebank (Sima’an et al., 2001) (about 90K tokens), according to our tagging guidelines. 4.1 Initial Con</context>
</contexts>
<marker>Dermatas, Kokkinakis, 1995</marker>
<rawString>Evangelos Dermatas and George Kokkinakis. 1995. Automatic stochastic tagging of natural language texts. Computational Linguistics, 21(2):137–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Elworthy</author>
</authors>
<title>Does Baum-Welch re-estimation help taggers?</title>
<date>1994</date>
<booktitle>In Proceeding of ANLP-94.</booktitle>
<contexts>
<context position="6809" citStr="Elworthy (1994)" startWordPosition="1070" endWordPosition="1071">d (EM-HMM) combined with good initial conditions based on a robust feature set can go a long way (as opposed to a more complex learning method). It seems that computing initial conditions is also the right place to capture complex linguistic intuition without fear that over-generalization could lead a learner to diverge. 2 Previous Work The tagging accuracy of supervised stochastic taggers is around 96%–97% (Manning and Schutze, 1999). Merialdo (1994) reports an accuracy of 86.6% for an unsupervised token-based EMestimated HMM, trained on a corpus of about 1M words, over a tagset of 159 tags. Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tagset of 134 tags. With (artificially created) good initial conditions, such as a good approximation of the tag distribution for each word, Elworthy reports an improvement to 94.6%, 92.27%, and 94.51% on the same data sets. Merialdo, on the other hand, reports an improvement to 92.6% and 94.4% for the case where 100 and 2,000 sentences of the training corpus are manually tagged. Later, Banko and Moore (2004) observed that earlier unsupervised HMM-EM results</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>David Elworthy. 1994. Does Baum-Welch re-estimation help taggers? In Proceeding of ANLP-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-ofspeech tagging.</title>
<date>2007</date>
<booktitle>In Proceeding of ACL 2007,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2494" citStr="Goldwater and Griffiths, 2007" startWordPosition="385" endWordPosition="388">eserved. This kind of filtering requires serious supervision: in theory, an expert is needed to go over the dictionary elements and filter out unlikely analyses. In practice, counts from an annotated corpus have been traditionally used to perform the filtering. Furthermore, these methods require rather comprehensive dictionaries in order to perform well. In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on “diluted dictionaries” – in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ). All the work mentioned above focuses on unsupervised English POS tagging. The dictionaries are all derived from tagged English corpora (all recent work uses the WSJ corpus). As such, the setting of the research is artificial: there is no reason to perform unsupervised learning when an annotated corpus is available. The problem is rather approached as a workbench for exploring new learning methods. The result is a series of creative algorithms, that have steadily improved results on the same dataset: unsupervised CRF training using contrastive estimati</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Thomas L. Griffiths. 2007. A fully bayesian approach to unsupervised part-ofspeech tagging. In Proceeding of ACL 2007, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>320--327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5818" citStr="Haghighi and Klein, 2006" startWordPosition="911" endWordPosition="914"> unsupervised PoS tagging of Hebrew text and for the common WSJ English test sets. We show that our method achieves state-ofthe-art results for the English setting, even with a relatively small dictionary. Furthermore, while recent work report results on a reduced English tagset of 17 PoS tags, we also present results for the complete 45 tags tagset of the WSJ corpus. This considerably raises the bar of the EM-HMM baseline. We also report state-of-the-art results for Hebrew full mor1Another notable work, though within a slightly different framework, is the prototype-driven method proposed by (Haghighi and Klein, 2006), in which the dictionary is replaced with a very small seed of prototypical examples. phological disambiguation. Our primary conclusion is that the problem of learning effective stochastic classifiers remains primarily a search task. Initial conditions play a dominant role in solving this task and can rely on linguistically motivated approximations. A robust learning method (EM-HMM) combined with good initial conditions based on a robust feature set can go a long way (as opposed to a more complex learning method). It seems that computing initial conditions is also the right place to capture c</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 320– 327, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using hidden Markov model.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<pages>6--225</pages>
<contexts>
<context position="1492" citStr="Kupiec, 1992" startWordPosition="232" endWordPosition="233">and efficient learning methods. 1 Introduction The task of unsupervised (or semi-supervised) partof-speech (POS) tagging is the following: given a dictionary mapping words in a language to their possible POS, and large quantities of unlabeled text data, learn to predict the correct part of speech for a given word in context. The only supervision given to the learning process is the dictionary, which in a realistic scenario, contains only part of the word types observed in the corpus to be tagged. Unsupervised POS tagging has been traditionally approached with relative success (Merialdo, 1994; Kupiec, 1992) by HMM-based generative models, employing EM parameters estimation using the Baum-Welch algorithm. However, as recently noted &apos;This work is supported in part by the Lynn and William Frankel Center for Computer Science. by Banko and Moore (2004), these works made use of filtered dictionaries: dictionaries in which only relatively probable analyses of a given word are preserved. This kind of filtering requires serious supervision: in theory, an expert is needed to go over the dictionary elements and filter out unlikely analyses. In practice, counts from an annotated corpus have been traditional</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>J. Kupiec. 1992. Robust part-of-speech tagging using hidden Markov model. Computer Speech and Language, 6:225–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Levinger</author>
<author>Uzi Ornan</author>
<author>Alon Itai</author>
</authors>
<title>Learning morpholexical probabilities from an untagged corpus with an application to Hebrew. Computational Linguistics,</title>
<date>1995</date>
<pages>21--383</pages>
<contexts>
<context position="8649" citStr="Levinger et al. (1995)" startWordPosition="1366" endWordPosition="1369">proved the results on this task: GG, SE and most recently TJ report the best re747 sults so far on the task of unsupervised POS tagging of the WSJ with diluted dictionaries. With dictionaries as small as 1249 lexical entries the LDA-based method with a strong ambiguity-class model reaches POS accuracy as high as 89.7% on a reduced tagset of 17 tags. While these 3 methods rely on the same feature set (lexical context, spelling features) for the learning stage, the LDA approach bases its predictions entirely on observable features, and excludes the traditional hidden states sequence. In Hebrew, Levinger et al. (1995) introduced the similar-words algorithm for estimating p(t|w) from unlabeled data, which we describe below. Our method uses this algorithm as a first step, and refines the approximation by introducing additional linguistic constraints and an iterative refinement step. 3 Initial Conditions For EM-HMM The most common model for unsupervised learning of stochastic processes is Hidden Markov Models (HMM). For the case of tagging, the states correspond to the tags ti, and words wi are emitted each time a state is visited. The parameters of the model can be estimated by applying the Baum-Welch EM alg</context>
<context position="10668" citStr="Levinger et al. (1995)" startWordPosition="1686" endWordPosition="1689"> We also experiment with constraining the p(t|t_1, t+1) distribution. 2Technically this is not Markov Model but a Dependency Net. However, bidirectional conditioning seem more suitable for language tasks, and in practice the learning and inference methods are mostly unaffected. See (Toutanova et al., 2003). General syntagmatic constraints We set linguistically motivated constraints on the p(t|t_1, t+1) distribution. In our setting, these are used to force the probability of some events to 0 (e.g., “Hebrew verbs can not be followed by the of preposition”). Morphology-based p(t|w) approximation Levinger et al. (1995) developed a context-free method for acquiring morpho-lexical probabilities (p(t|w)) from an untagged corpus. The method is based on language-specific rules for constructing a similar words (SW) set for each analysis of a word. This set is composed of morphological variations of the word under the given analysis. For example, the Hebrew token דלי can be analyzed as either a noun (boy) or a verb (gave birth). The noun SW set for this token is composed of the definiteness and number inflections םידליה,םידלי,דליה (the boy, boys, the boys), while the verb SW set is composed of gender and tense inf</context>
<context position="17734" citStr="Levinger et al., 1995" startWordPosition="2861" endWordPosition="2864">ph+Linear 79.8 79.0 85.5 88 92 Morph-Based / / / 87.6 91.4 PairConst+Morph Linear-Context / / / 84.5 89.0 Morph+Linear / / / 87.1 91.5 Morph-Based / / / 89.2 92.3 InitTrans+Morph Linear-Context / / / 87.7 90.9 Morph+Linear / / / 89.4 92.4 Table 1: Accuracy (%) of Hebrew Morphological Disambiguation and POS Tagging over various initial conditions these last 3 models with the addition of the syntagmatic constraints (Synt+Morph). For each of these, we first compare the computed p(tjw) against a gold standard distribution, taken from the test corpus (90K tokens), according to the measure used by (Levinger et al., 1995) (Dist). On this measure, we confirm that our improved morpholexical approximation improves the results reported by Levinger et al. from 74% to about 80% on a richer tagset, and on a much larger test set (90K vs. 3,400 tokens). We then report on the effectiveness of p(tjw) as a context-free tagger that assigns to each word the most likely tag, both for full morphological analysis (3,561 tags) (Full) and for the simpler task of token segmentation and POS tag selection (36 tags) (Seg+Pos). The best results on this task are 80.8% and 87.5% resp. achieved on the Morph+Linear initial conditions. Fi</context>
<context position="32657" citStr="Levinger et al., 1995" startWordPosition="5327" endWordPosition="5330">work, such as GG, aim to use the Bayesian framework and incorporate “linguistically motivated priors”, in practice such priors currently only account for the fact that language related distributions are sparse - a very general kind of knowledge. In contrast, our method allow the incorporation of much more fine-grained intuitions. We tested the method on the challenging task of full morphological disambiguation in Hebrew (which was our original motivation) and on the standard WSJ unsupervised POS tagging task. In Hebrew, our model includes an improved version of the similar words algorithm of (Levinger et al., 1995), a model of lexical context, and a small set of tag ngrams. The combination of these knowledge sources in the initial conditions brings an error reduction of more than 25% over a strong uniform distribution baseline. In English, our model is competitive with recent state-of-the-art results, while using simple and efficient learning methods. The comparison with other algorithms indicates directions of potential improvement: (1) our initialconditions method might benefit the other, more sophisticated learning algorithms as well. (2) Our models were designed under the assumption of a relatively </context>
</contexts>
<marker>Levinger, Ornan, Itai, 1995</marker>
<rawString>Moshe Levinger, Uzi Ornan, and Alon Itai. 1995. Learning morpholexical probabilities from an untagged corpus with an application to Hebrew. Computational Linguistics, 21:383–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Schutze</author>
</authors>
<title>Foundation of Statistical Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6632" citStr="Manning and Schutze, 1999" startWordPosition="1038" endWordPosition="1041">c classifiers remains primarily a search task. Initial conditions play a dominant role in solving this task and can rely on linguistically motivated approximations. A robust learning method (EM-HMM) combined with good initial conditions based on a robust feature set can go a long way (as opposed to a more complex learning method). It seems that computing initial conditions is also the right place to capture complex linguistic intuition without fear that over-generalization could lead a learner to diverge. 2 Previous Work The tagging accuracy of supervised stochastic taggers is around 96%–97% (Manning and Schutze, 1999). Merialdo (1994) reports an accuracy of 86.6% for an unsupervised token-based EMestimated HMM, trained on a corpus of about 1M words, over a tagset of 159 tags. Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tagset of 134 tags. With (artificially created) good initial conditions, such as a good approximation of the tag distribution for each word, Elworthy reports an improvement to 94.6%, 92.27%, and 94.51% on the same data sets. Merialdo, on the other hand, reports an improvement to 92.6%</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Schutze. 1999. Foundation of Statistical Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--155</pages>
<contexts>
<context position="1477" citStr="Merialdo, 1994" startWordPosition="230" endWordPosition="231">le using simple and efficient learning methods. 1 Introduction The task of unsupervised (or semi-supervised) partof-speech (POS) tagging is the following: given a dictionary mapping words in a language to their possible POS, and large quantities of unlabeled text data, learn to predict the correct part of speech for a given word in context. The only supervision given to the learning process is the dictionary, which in a realistic scenario, contains only part of the word types observed in the corpus to be tagged. Unsupervised POS tagging has been traditionally approached with relative success (Merialdo, 1994; Kupiec, 1992) by HMM-based generative models, employing EM parameters estimation using the Baum-Welch algorithm. However, as recently noted &apos;This work is supported in part by the Lynn and William Frankel Center for Computer Science. by Banko and Moore (2004), these works made use of filtered dictionaries: dictionaries in which only relatively probable analyses of a given word are preserved. This kind of filtering requires serious supervision: in theory, an expert is needed to go over the dictionary elements and filter out unlikely analyses. In practice, counts from an annotated corpus have b</context>
<context position="6649" citStr="Merialdo (1994)" startWordPosition="1042" endWordPosition="1043">ily a search task. Initial conditions play a dominant role in solving this task and can rely on linguistically motivated approximations. A robust learning method (EM-HMM) combined with good initial conditions based on a robust feature set can go a long way (as opposed to a more complex learning method). It seems that computing initial conditions is also the right place to capture complex linguistic intuition without fear that over-generalization could lead a learner to diverge. 2 Previous Work The tagging accuracy of supervised stochastic taggers is around 96%–97% (Manning and Schutze, 1999). Merialdo (1994) reports an accuracy of 86.6% for an unsupervised token-based EMestimated HMM, trained on a corpus of about 1M words, over a tagset of 159 tags. Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tagset of 134 tags. With (artificially created) good initial conditions, such as a good approximation of the tag distribution for each word, Elworthy reports an improvement to 94.6%, 92.27%, and 94.51% on the same data sets. Merialdo, on the other hand, reports an improvement to 92.6% and 94.4% for th</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with probabilistic model. Computational Linguistics, 20:155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of the seventh conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>141--148</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<marker>Sch¨utze, 1995</marker>
<rawString>Hinrich Sch¨utze. 1995. Distributional part-of-speech tagging. In Proceedings of the seventh conference on European chapter of the Association for Computational Linguistics, pages 141–148, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danny Shacham</author>
<author>Shuly Wintner</author>
</authors>
<title>Morphological disambiguation of hebrew: A case study in classifier combination.</title>
<date>2007</date>
<booktitle>In Proceeding of EMNLP-07,</booktitle>
<location>Prague, Czech.</location>
<marker>Shacham, Wintner, 2007</marker>
<rawString>Danny Shacham and Shuly Wintner. 2007. Morphological disambiguation of hebrew: A case study in classifier combination. In Proceeding of EMNLP-07, Prague, Czech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
<author>Alon Itai</author>
<author>Alon Altman Yoad Winter</author>
<author>Noa Nativ</author>
</authors>
<title>Building a tree-bank of modern Hebrew text.</title>
<date>2001</date>
<booktitle>Journal Traitement Automatique des Langues (t.a.l.). Special Issue on NLP and Corpus Linguistics.</booktitle>
<marker>Sima’an, Itai, Winter, Nativ, 2001</marker>
<rawString>Khalil Sima’an, Alon Itai, Alon Altman Yoad Winter, and Noa Nativ. 2001. Building a tree-bank of modern Hebrew text. Journal Traitement Automatique des Langues (t.a.l.). Special Issue on NLP and Corpus Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>354--362</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="2456" citStr="Smith and Eisner, 2005" startWordPosition="380" endWordPosition="383">analyses of a given word are preserved. This kind of filtering requires serious supervision: in theory, an expert is needed to go over the dictionary elements and filter out unlikely analyses. In practice, counts from an annotated corpus have been traditionally used to perform the filtering. Furthermore, these methods require rather comprehensive dictionaries in order to perform well. In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on “diluted dictionaries” – in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ). All the work mentioned above focuses on unsupervised English POS tagging. The dictionaries are all derived from tagged English corpora (all recent work uses the WSJ corpus). As such, the setting of the research is artificial: there is no reason to perform unsupervised learning when an annotated corpus is available. The problem is rather approached as a workbench for exploring new learning methods. The result is a series of creative algorithms, that have steadily improved results on the same dataset: unsupervised C</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 354–362, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott M Thede</author>
<author>Mary P Harper</author>
</authors>
<title>A secondorder hidden Markov model for part-of-speech tagging.</title>
<date>1999</date>
<booktitle>In Proceeding of ACL-99.</booktitle>
<contexts>
<context position="9817" citStr="Thede and Harper, 1999" startWordPosition="1564" endWordPosition="1567">odel can be estimated by applying the Baum-Welch EM algorithm (Baum, 1972), on a large-scale corpus of unlabeled text. The estimated parameters are then used in conjunction with Viterbi search, to find the most probable sequence of tags for a given sentence. In this work, we follow Adler (2007) and use a variation of second-order HMM in which the probability of a tag is conditioned by the tag that precedes it and by the one that follows it, and the probability of an emitted word is conditioned by its tag and the tag that follows it2. In all experiments, we use the backoff smoothing method of (Thede and Harper, 1999), with additive smoothing (Chen, 1996) for the lexical probabilities. We investigate methods to approximate the initial parameters of the p(t|w) distribution, from which we obtain p(w|t) by marginalization and Bayesian inversion. We also experiment with constraining the p(t|t_1, t+1) distribution. 2Technically this is not Markov Model but a Dependency Net. However, bidirectional conditioning seem more suitable for language tasks, and in practice the learning and inference methods are mostly unaffected. See (Toutanova et al., 2003). General syntagmatic constraints We set linguistically motivate</context>
</contexts>
<marker>Thede, Harper, 1999</marker>
<rawString>Scott M. Thede and Mary P. Harper. 1999. A secondorder hidden Markov model for part-of-speech tagging. In Proceeding of ACL-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A bayesian lda-based model for semi-supervised part-of-speech tagging.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20.</booktitle>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2530" citStr="Toutanova and Johnson, 2008" startWordPosition="390" endWordPosition="393">es serious supervision: in theory, an expert is needed to go over the dictionary elements and filter out unlikely analyses. In practice, counts from an annotated corpus have been traditionally used to perform the filtering. Furthermore, these methods require rather comprehensive dictionaries in order to perform well. In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on “diluted dictionaries” – in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ). All the work mentioned above focuses on unsupervised English POS tagging. The dictionaries are all derived from tagged English corpora (all recent work uses the WSJ corpus). As such, the setting of the research is artificial: there is no reason to perform unsupervised learning when an annotated corpus is available. The problem is rather approached as a workbench for exploring new learning methods. The result is a series of creative algorithms, that have steadily improved results on the same dataset: unsupervised CRF training using contrastive estimation (SE), a fully-bayesian HMM model </context>
</contexts>
<marker>Toutanova, Johnson, 2008</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2008. A bayesian lda-based model for semi-supervised part-of-speech tagging. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="10353" citStr="Toutanova et al., 2003" startWordPosition="1639" endWordPosition="1642">t2. In all experiments, we use the backoff smoothing method of (Thede and Harper, 1999), with additive smoothing (Chen, 1996) for the lexical probabilities. We investigate methods to approximate the initial parameters of the p(t|w) distribution, from which we obtain p(w|t) by marginalization and Bayesian inversion. We also experiment with constraining the p(t|t_1, t+1) distribution. 2Technically this is not Markov Model but a Dependency Net. However, bidirectional conditioning seem more suitable for language tasks, and in practice the learning and inference methods are mostly unaffected. See (Toutanova et al., 2003). General syntagmatic constraints We set linguistically motivated constraints on the p(t|t_1, t+1) distribution. In our setting, these are used to force the probability of some events to 0 (e.g., “Hebrew verbs can not be followed by the of preposition”). Morphology-based p(t|w) approximation Levinger et al. (1995) developed a context-free method for acquiring morpho-lexical probabilities (p(t|w)) from an untagged corpus. The method is based on language-specific rules for constructing a similar words (SW) set for each analysis of a word. This set is composed of morphological variations of the w</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>R Schwartz</author>
<author>J Palmucci</author>
<author>M Meteer</author>
<author>L Ramshaw</author>
</authors>
<title>Coping with ambiguity and unknown words through probabilistic models.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--359</pages>
<contexts>
<context position="28842" citStr="Weischedel et al., 1993" startWordPosition="4689" endWordPosition="4692">SE show that accuracy does not seem to improve as more unlabeled data are used with the models. We also report results for training our EM-HMM tagger on the smaller dataset (the p(tjw) estimation is still based on the entire unlabeled WSJ). All the abovementioned models follow the assumption that all 17 tags are valid for the unknown words. In contrast, we restrict the set of allowed tags for an unknown word to open-class tags. Closed class words are expected to be included in a dictionary, even a small one. The practice of allowing only open-class tags for unknown words goes back a long way (Weischedel et al., 1993), and proved highly beneficial also in our case. Notice that even our simplest models, in which the initial p(tjw) distribution for each w is uniform, already outperform most of the other models, and, in the case of the diluted dictionaries, by a wide margin. Similarly, given the p(tjw) estimate, EMHMM training on the smaller dataset (24k) is still very competitive (yet results improve with more unlabeled data). When we use our refined p(tjw) distribution as the basis of EM-HMM training, we get the best results for the complete dictionary case. With the diluted dictionaries, we are outperforme</context>
</contexts>
<marker>Weischedel, Schwartz, Palmucci, Meteer, Ramshaw, 1993</marker>
<rawString>R. Weischedel, R. Schwartz, J. Palmucci, M. Meteer, and L. Ramshaw. 1993. Coping with ambiguity and unknown words through probabilistic models. Computational Linguistics, 19:359–382.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>