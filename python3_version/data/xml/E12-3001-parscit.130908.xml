<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000898">
<title confidence="0.996757">
Improving Pronoun Translation for Statistical Machine Translation
</title>
<author confidence="0.994448">
Liane Guillou
</author>
<affiliation confidence="0.9978635">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.880756">
Edinburgh, UK, EH8 9AB
</address>
<email confidence="0.996917">
L.K.Guillou@sms.ed.ac.uk
</email>
<sectionHeader confidence="0.99373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99930304">
Machine Translation is a well–established
field, yet the majority of current systems
translate sentences in isolation, losing valu-
able contextual information from previ-
ously translated sentences in the discourse.
One important type of contextual informa-
tion concerns who or what a coreferring
pronoun corefers to (i.e., its antecedent).
Languages differ significantly in how they
achieve coreference, and awareness of an-
tecedents is important in choosing the cor-
rect pronoun. Disregarding a pronoun’s an-
tecedent in translation can lead to inappro-
priate coreferring forms in the target text,
seriously degrading a reader’s ability to un-
derstand it.
This work assesses the extent to which
source-language annotation of coreferring
pronouns can improve English–Czech Sta-
tistical Machine Translation (SMT). As
with previous attempts that use this method,
the results show little improvement. This
paper attempts to explain why and to pro-
vide insight into the factors affecting per-
formance.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994047384615384">
It is well-known that in many natural languages,
a pronoun that corefers must bear similar features
to its antecedent. These can include similar num-
ber, gender (morphological or referential), and/or
animacy. If a pronoun and its antecedent occur in
the same unit of translation (N-gram or syntactic
tree), these agreement features can influence the
translation. But this locality cannot be guaranteed
in either phrase-based or syntax-based Statistical
Machine Translation (SMT). If it is not within the
same unit, a coreferring pronoun will be trans-
lated without knowledge of its antecedent, mean-
ing that its translation will simply reflect local fre-
quency. Incorrectly translating a pronoun can re-
sult in readers/listeners identifying the wrong an-
tecedent, which can mislead or confuse them.
There have been two recent attempts to solve
this problem within the framework of phrase-
based SMT (Hardmeier &amp; Federico, 2010; Le
Nagard &amp; Koehn, 2010). Both involve anno-
tation projection, which in this context means
annotating coreferential pronouns in the source-
language with features derived from the transla-
tion of their aligned antecedents, and then build-
ing a translation model of the annotated forms.
When translating a coreferring pronoun in a new
source-language text, the antecedent is identified
and its translation used (differently in the two at-
tempts cited above) to annotate the pronoun prior
to translation.
The aim of this work was to better understand
why neither of the previous attempts achieved
more than a small improvement in translation
quality associated with coreferring pronouns.
Only by understanding this will it be possible to
ascertain whether the method of annotation pro-
jection is intrinsically flawed or the unexpectedly
small improvement is due to other factors.
Errors can arise when:
</bodyText>
<listItem confidence="0.988428142857143">
1. Deciding whether or not a third person pro-
noun corefers;
2. Identifying the pronoun antecedent;
3. Identifying the head of the antecedent, which
serves as the source of its features;
4. Aligning the source and target texts at the
phrase and word levels.
</listItem>
<page confidence="0.773171">
1
</page>
<note confidence="0.9819985">
Proceedings of the EACL 2012 Student Research Workshop, pages 1–10,
Avignon, France, 26 April 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999966333333334">
Factoring out the first two decisions would
show whether the lack of significant improvement
was simply due to imperfect coreference resolu-
tion. In order to control for these errors several
different manually annotated versions of the Penn
Wall Street Journal corpus were used, each pro-
viding different annotations over the same text.
The BBN Pronoun Coreference and Entity Type
corpus (Weischedel &amp; Brunstein, 2005) was used
to provide coreference information in the source-
language and exclude non-referential pronouns.
It also formed the source-language side of the
parallel training corpus. The PCEDT 2.0 cor-
pus (Hajiˇc et al., 2011), which contains a close
Czech translation of the Penn Wall Street Journal
corpus, provided reference translations for test-
ing and the target-language side of the parallel
corpus for training. To minimise (although not
completely eliminate) errors associated with an-
tecedent head identification (item 3 above), the
parse trees in the Penn Treebank 3.0 corpus (Mar-
cus et al., 1999) were used. The gold stan-
dard annotation provided by these corpora al-
lowed me to assume perfect identification of core-
ferring pronouns and coreference resolution and
near–perfect antecedent head noun identification.
These assumptions could not be made if state-of-
the-art methods had been used as they cannot yet
achieve sufficiently high levels of accuracy.
The remainder of the paper is structured as fol-
lows. The use of pronominal coreference in En-
glish and Czech and the problem of anaphora res-
olution are described in Section 2. The works
of Le Nagard &amp; Koehn (2010) and Hardmeier
&amp; Federico (2010) are discussed in Section 3,
and the source-language annotation projection
method is described in Section 4. The results are
presented and discussed in Section 5 and future
work is outlined in Section 6.
</bodyText>
<sectionHeader confidence="0.996852" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999297">
2.1 Anaphora Resolution
</subsectionHeader>
<bodyText confidence="0.9998654">
Anaphora resolution involves identifying the an-
tecedent of a referring expression, typically a pro-
noun or noun phrase that is used to refer to some-
thing previously mentioned in the discourse (its
antecedent). Where multiple referring expres-
sions refer to the same antecedent, they are said to
be coreferential. Anaphora resolution and the re-
lated task of coreference resolution have been the
subject of considerable research within Natural
Language Processing (NLP). Excellent surveys
are provided by Strube (2007) and Ng (2010).
Unresolved anaphora can add significant trans-
lation ambiguity, and their incorrect translation
can significantly decrease a reader’s ability to un-
derstand a text. Accurate coreference in trans-
lation is therefore necessary in order to produce
understandable and cohesive texts. This justifies
recent interest (Le Nagard &amp; Koehn, 2010; Hard-
meier &amp; Federico, 2010) and motivates the work
presented in this paper.
</bodyText>
<subsectionHeader confidence="0.999783">
2.2 Pronominal Coreference in English
</subsectionHeader>
<bodyText confidence="0.999980421052632">
Whilst English makes some use of case, it lacks
the grammatical gender found in other languages.
For monolingual speakers, the relatively few dif-
ferent pronoun forms in English make sentences
easy to generate: Pronoun choice depends on the
number and gender of the entity to which they re-
fer. For example, when talking about ownership
of a book, English uses the pronouns “his/her”
to refer to a book that belongs to a male/female
owner, and “their” to refer to one with multi-
ple owners (irrespective of their gender). One
source of difficulty is that the pronoun “it” has
both a coreferential and a pleonastic function. A
pleonastic pronoun is one that is not referential.
For example, in the sentence “It is raining”, “it”
does not corefer with anything. Coreference res-
olution algorithms must exclude such instances in
order to prevent the erroneous identification of an
antecedent when one does not exist.
</bodyText>
<subsectionHeader confidence="0.999616">
2.3 Pronominal Coreference in Czech
</subsectionHeader>
<bodyText confidence="0.999884625">
Czech, like other Slavic languages, is highly in-
flective. It is also a free word order language, in
which word order reflects the information struc-
ture of the sentence within the current discourse.
Czech has seven cases and four grammatical gen-
ders: masculine animate (for people and animals),
masculine inanimate (for inanimate objects), fem-
inine and neuter. (With feminine and neuter gen-
ders, animacy is not grammatically marked.) In
Czech, a pronoun must agree in number, gender
and animacy with its antecedent. The morpho-
logical form of possessive pronouns depends not
only on the possessor but also the object in pos-
session. Moreover, reflexive pronouns (both per-
sonal and possessive) are commonly used. In ad-
dition, Czech is a pro-drop language, whereby an
</bodyText>
<page confidence="0.988272">
2
</page>
<bodyText confidence="0.9989469">
explicit subject pronoun may be omitted if it is in-
ferable from other grammatical features such as
verb morphology. This is in contrast with En-
glish which exhibits relatively fixed Subject-Verb-
Object (SVO) order and only drops subject pro-
nouns in imperatives (e.g. “Stop babbling”) and
coordinated VPs.
Differences between the choice of coreferring
expressions used in English and Czech can be
seen in the following simple examples:
</bodyText>
<listItem confidence="0.9678645">
1. The dog has a ball. I can see it playing out-
side.
2. The cow is in the field. I can see it grazing.
3. The car is in the garage. I will take it to work.
</listItem>
<bodyText confidence="0.999724266666666">
In each example, the English pronoun “it”
refers to an entity that has a different gender in
Czech. Its correct translation requires identifying
the gender (and number) of its antecedent and en-
suring that the pronoun agrees. In 1 “it” refers to
the dog (“pes”, masculine, animate) and should
be translated as “ho”. In 2, “it” refers to the cow
(“kr´ava”, feminine) and should be translated as
“ji”. In 3, “it” refers to the car (“auto”, neuter)
and should be translated as “ho”.
In some cases, the same pronoun is used for
both animate and inanimate masculine genders,
but in general, different pronouns are used. For
example, with possessive reflexive pronouns in
the accusative case:
</bodyText>
<tableCaption confidence="0.329978">
English: I admired my (own) dog
Czech: Obdivoval jsme sv´eho psa
English: I admired my (own) castle
Czech: Obdivoval jsme sv˚uj hrad
</tableCaption>
<bodyText confidence="0.999374444444444">
Here “sv´eho” is used to refer to a dog (mascu-
line animate, singular) and “sv˚uj” to refer to a cas-
tle (masculine inanimate, singular), both of which
belong to the speaker.
Because a pronoun may take a large number
of morphological forms in Czech and because
case is not checked in annotation projection, the
method presented here for translating coreferring
pronouns does not guarantee their correct form.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999893934782609">
Early work on integrating anaphora resolution
with Machine Translation includes the rule-based
approaches of Mitkov et al. (1995) and Lappin &amp;
Leass (1994) and the transfer-based approach of
Saggion &amp; Carvalho (1994). Work in the 1990’s
culminated in the publication of a special issue
of Machine Translation on anaphora resolution
(Mitkov, 1999). Work then appears to have been
on hold until papers were published by Le Na-
gard &amp; Koehn (2010) and Hardmeier &amp; Federico
(2010). This resurgence of interest follows ad-
vances since the 1990’s which have made new ap-
proaches possible.
The work described in this paper resembles that
of Le Nagard &amp; Koehn (2010), with two main dif-
ferences. The first is the use of manually anno-
tated corpora to extract coreference information
and morphological properties of the target trans-
lations of the antecedents. The second lies in the
choice of language pair. They consider English-
French translation, focussing on gender-correct
translation of the third person pronouns “it” and
“they”. Coreference is more complex in Czech
with both number and gender influencing pronoun
selection. Annotating pronouns with both num-
ber and gender further exacerbates the problem of
data sparseness in the training data, but this can-
not be avoided if the aim is to improve their trans-
lation. This work also accommodates a wider
range of English pronouns.
In contrast, Hardmeier &amp; Federico (2010) focus
on English-German translation and model coref-
erence using a word dependency module inte-
grated within the log-linear SMT model as an ad-
ditional feature function.
Annotation projection has been used elsewhere
in SMT. Gimpel &amp; Smith (2008) use it to capture
long–distance phenomena within a single sen-
tence in the source-language text via the extrac-
tion of sentence-level contextual features, which
are used to augment SMT translation models and
better predict phrase translation. Projection tech-
niques have also been applied to multilingual
Word Sense Disambiguation whereby the sense
of a word may be determined in another language
(Diab, 2004; Khapra et al., 2009).
</bodyText>
<sectionHeader confidence="0.998038" genericHeader="method">
4 Methodology
</sectionHeader>
<subsectionHeader confidence="0.942674">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999900333333333">
I have followed Le Nagard &amp; Koehn (2010) in us-
ing a two-step approach to translation, with anno-
tation projection incorporated as a pre-processing
</bodyText>
<page confidence="0.987856">
3
</page>
<listItem confidence="0.542777">
(4) Annotation of English pronoun with
number and gender of Czech word
</listItem>
<figureCaption confidence="0.999767">
Figure 1: Overview of the Annotation Process
</figureCaption>
<figure confidence="0.909631315789474">
Input:
The castle is old. It stands on a hill.
Translate:
Translate:
The castle is old. Hrad je start&apos;.
It stands on a hill.
(2) Identification of
antecedent head
(1) Identification of
coreferential pronoun
The castle is old.
It stands on a hill.
(3) English – Czech mapping
of antecedent head
Hrad je start&apos;.
(4) Extraction of number
and gender of Czech word
Masculine inanimate, singular
It.mascin.sg stands on a hill.
</figure>
<bodyText confidence="0.999898328571429">
task. In the first step, pronouns are annotated in
the source-language text before the text is trans-
lated by a phrase-based SMT system in the second
step. This approach leaves the translation pro-
cess unaffected. In this work, the following pro-
nouns are annotated: third person personal pro-
nouns (except instances of “it” that are pleonastic
or that corefer with clauses or VPs), reflexive per-
sonal pronouns and possessive pronouns, includ-
ing reflexive possessives. Relative pronouns are
excluded as they are local dependencies in both
English and Czech and this work is concerned
with the longer range dependencies typically ex-
hibited by the previously listed pronoun types.
Annotation of the English source-language
text and its subsequent translation into Czech is
achieved using two phrase-based translation sys-
tems. The first, hereafter called the Baseline sys-
tem, is trained using English and Czech sentence–
aligned parallel training data with no annotation.
The second system, hereafter called the Annotated
system, is trained using the same target data, but
in the source-language text, each coreferring pro-
noun has been annotated with number, gender and
animacy features. These are obtained from the
existing (Czech reference) translation of the head
of its English antecedent. Word alignment of En-
glish and Czech is obtained from the PCEDT 2.0
alignment file which maps English words to their
corresponding t-Layer (deep syntactic, tectogram-
matical) node in the Czech translation. Starting
with this t-Layer node the annotation layers of the
PCEDT 2.0 corpus are traversed and the number
and gender of the Czech word are extracted from
the morphological layer (m-Layer).
The Baseline system serves a dual purpose. It
forms the first stage of the two-step translation
process, and as described in Section 5, it provides
a baseline against which Annotated system trans-
lations are compared.
The annotation process used here is shown
in Figure 1. It identifies coreferential pronouns
and their antecedents using the annotation in the
BBN Pronoun Coreference and Entity Type cor-
pus, and obtains the Czech translation of the En-
glish antecedent from the translation produced
by the Baseline system. Because many an-
tecedents come from previous sentences, these
sentences must be translated before translating the
current sentence. Here I follow Le Nagard &amp;
Koehn (2010) in translating the complete source-
language text using the Baseline system and then
extracting the (here, Czech) translations of the En-
glish antecedents from the output. This provides
a simple solution to the problem of obtaining the
Czech translation prior to annotation. In contrast
Hardmeier &amp; Federico (2010) translate sentence
by sentence using a process which was deemed
to be more complex than was necessary for this
project.
The English text is annotated such that all
coreferential pronouns whose antecedents have an
identifiable Czech translation are marked with the
number and gender of that Czech word. The out-
put of the annotation process is thus the same En-
glish text that was input to the Baseline system,
with the addition of annotation of the coreferen-
tial pronouns. This annotated English text is then
translated using the Annotated translation system,
the output of which is the final translation.
</bodyText>
<page confidence="0.977055">
4
</page>
<table confidence="0.9997125">
Training Dev. Final
Parallel Sentences 47,549 280 540
Czech Words 955,018 5,467 10,110
English Words 1,024,438 6,114 11,907
</table>
<tableCaption confidence="0.999563">
Table 1: Sizes of the training and testing datasets
</tableCaption>
<subsectionHeader confidence="0.871625">
4.2 Baseline and Annotated systems
</subsectionHeader>
<bodyText confidence="0.99997">
Both systems are phrase-based SMT models,
trained using the Moses toolkit (Hoang et al.,
2007). They share the same 3-gram language
model constructed from the target-side text of
the parallel training corpus and the Czech mono-
lingual 2010 and 2011 News Crawl corpora1.
The language model was constructed using the
SRILM toolkit (Stolcke, 2002) with interpolated
Kneser-Ney discounting (Kneser &amp; Ney, 1995).
In addition, both systems are forced to use the
same word alignments (constructed using Giza++
(Och &amp; Ney, 2003) in both language pair direc-
tions and using stemmed training data in which
words are limited to the first four characters) in
order to mitigate the effects of Czech word in-
flection on word alignment statistics. This helps
to ensure that the Czech translation of the head
of the antecedent remains constant in both steps
of the two-step process. If this were to change it
would defeat the purpose of pronoun annotation
as different Czech translations could result in dif-
ferent gender and/or number.
The Baseline system was trained using the
Penn Wall Street Journal corpus with no anno-
tation, while the Annotated system was trained
with an annotated version of the same text (see
Table 1), with the target-language text being the
same in both cases. The Penn Wall Street Journal
corpus was annotated using the process described
above, with the number and gender of the Czech
translation of the antecedent head obtained from
the PCEDT 2.0 alignment file.
</bodyText>
<subsectionHeader confidence="0.999531">
4.3 Processing test files
</subsectionHeader>
<bodyText confidence="0.999895714285714">
Two test files were used (see Table 1) – one called
‘Final’ and the other, ‘Development’ (Dev). A test
file is first translated using the Baseline system
with a trace added to the Moses decoder. Each
coreferential English pronoun is then identified
using the BBN Pronoun Coreference and Entity
Type corpus and the head of its antecedent is ex-
</bodyText>
<footnote confidence="0.4417885">
1Provided for the Sixth EMNLP Workshop on Statistical
Machine Translation (Callison-Burch et al., 2011)
</footnote>
<bodyText confidence="0.999732486486486">
tracted from the annotated NPs in the Penn Tree-
bank 3.0 corpus. The sentence number and word
position of the English pronoun and its antecedent
head noun(s) are extracted from the input English
text and used to identify the English/Czech phrase
pairs that contain the Czech translations of the En-
glish words. Using this information together with
the phrase alignments (output by the Moses de-
coder) and the phrase-internal word alignments
in the phrase translation table, a Czech transla-
tion is obtained from the Baseline system. Num-
ber, gender and animacy (if masculine) features
of the Czech word identified as the translation
of the head of the antecedent are extracted from
a pre-built morphological dictionary of Czech
words constructed from the PCEDT 2.0 corpus
for the purpose of this work. A copy of the
original English test file is then constructed, with
each coreferential pronoun annotated with the ex-
tracted Czech features.
The design of this process reflects two assump-
tions. First, the annotation of the Czech words
in the m-Layer of the PCEDT 2.0 corpus is both
accurate and consistent. Second, as the Base-
line and Annotated systems were trained using the
same word alignments, the Czech translation of
the head of the English antecedent should be the
same in the output of both. Judging by the very
small number of cases in which the antecedent
translations differed (3 out of 458 instances), this
assumption was proved to be reasonable. These
differences were due to the use of different phrase
tables for each system as a result of training on
different data (i.e. the annotation of English pro-
nouns or lack thereof). This would not be an is-
sue for single-step translation systems such as that
used by Hardmeier &amp; Federico (2010).
</bodyText>
<subsectionHeader confidence="0.971884">
4.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.999548083333333">
No standard method yet exists for evaluating pro-
noun translation in SMT. Early work focussed on
the development of techniques for anaphora reso-
lution and their integration within Machine Trans-
lation (Lappin &amp; Leass, 1994; Saggion &amp; Car-
valho, 1994; Mitkov et al., 1995), with little men-
tion of evaluation. In recent work, evaluation
has become much more important. Both Le Na-
gard &amp; Koehn (2010) and Hardmeier &amp; Federico
(2010) consider and reject BLEU (Papineni et al.,
2002) as ill-suited for evaluating pronoun transla-
tion. While Hardmeier &amp; Federico propose and
</bodyText>
<page confidence="0.965761">
5
</page>
<bodyText confidence="0.9999181">
use a strict recall and precision based metric for
English–German translation, I found it unsuitable
for English–Czech translation, given the highly
inflective nature of Czech.
Given the importance of evaluation to the goal
of assessing the effectiveness of annotation pro-
jection for improving the translation of corefer-
ring pronouns, I carried out two separate types
of evaluation — an automated evaluation which
could be applied to the entire test set, and an in-
depth manual assessment that might provide more
information, but could only be performed on a
subset of the test set. The automated evaluation
is based on the fact that a Czech pronoun must
agree in number and gender with its antecedent.
Thus one can count the number of pronouns in the
translation output for which this agreement holds,
rather than simply score the output against a sin-
gle reference translation. To obtain these figures,
the automated evaluation process counted:
</bodyText>
<listItem confidence="0.984354529411765">
1. Total pronouns in the input English test file.
2. Total English pronouns identified as corefer-
ential, as per the annotation of the BBN Pro-
noun Coreference and Entity Type corpus.
3. Total coreferential English pronouns that are
annotated by the annotation process.
4. Total coreferential English pronouns that are
aligned with any Czech translation.
5. Total coreferential English pronouns trans-
lated as any Czech pronoun.
6. Total coreferential English pronouns trans-
lated as a Czech pronoun corresponding to
a valid translation of the English pronoun.
7. Total coreferential English pronouns trans-
lated as a Czech pronoun (that is a valid
translation of the English pronoun) agreeing
in number and gender with the antecedent.
</listItem>
<bodyText confidence="0.999795090909091">
The representation of valid Czech translations
of English pronouns takes the form of a list pro-
vided by an expert in Czech NLP, which ignores
case and focusses solely on number and gender.
In contrast, the manual evaluation carried out
by that same expert, who is also a native speaker
of Czech, was used to determine whether devi-
ations from the single reference translation pro-
vided in the PCEDT 2.0 corpus were valid alter-
natives or simply poor translations. The following
judgements were provided:
</bodyText>
<listItem confidence="0.98856675">
1. Whether the pronoun had been translated
correctly, or in the case of a dropped pro-
noun, whether pro-drop was appropriate;
2. If the pronoun translation was incorrect,
whether a native Czech speaker would still
be able to derive the meaning;
3. For input to the Annotated system, whether
the pronoun had been correctly annotated
with respect to the Czech translation of its
identified antecedent;
4. Where an English pronoun was translated
differently by the Baseline and Annotated
</listItem>
<bodyText confidence="0.87327323076923">
systems, which was better. If both translated
an English pronoun to a valid Czech transla-
tion, equal correctness was assumed.
In order to ensure that the manual assessor
was directed to the Czech translations aligned
to the English pronouns, additional markup was
automatically inserted into the English and Czech
texts: (1) coreferential pronouns in both English
and Czech texts were marked with the head
noun of their antecedent (denoted by *), and
(2) coreferential pronouns in the English source
texts were marked with the Czech translation
of the antecedent head, and those in the Czech
target texts were marked with the original English
pronoun that they were aligned to:
English text input to the Baseline system: the u.s.
, claiming some success in its trade diplomacy, ...
Czech translation output by the Baseline system:
usa , tvrd´ı nˇekteˇr´ı jej´ı(its) obchodn´ı ´uspˇech v diplo-
macii , ...
English text input to the Annotated system: the
u.s.* , claiming some success in its(u.s.,usa).mascin.pl
trade diplomacy, ...
Czech translation output by the Annotated sys-
tem: usa ,* tvrd´ı nˇekteˇr´ı ´uspˇechu ve sv´e(its.mascin.pl)
obchodn´ı diplomacii , ...
</bodyText>
<sectionHeader confidence="0.999243" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.945667">
5.1 Automated Evaluation
</subsectionHeader>
<bodyText confidence="0.999968857142857">
Automated evaluation of both “Development”
and “Final” test sets (see Table 2) shows that even
factoring out the problems of accurate identifica-
tion of coreferring pronouns, coreference resolu-
tion and antecedent head–finding, does not im-
prove performance of the Annotated system much
above that of the Baseline.
</bodyText>
<page confidence="0.999459">
6
</page>
<table confidence="0.9987865">
Dev. Final
Baseline Annotated Baseline Annotated
Total pronouns in English file 156 156 350 350
Total pronouns identified as coreferential 141 141 331 331
Annotated coreferential English pronouns – 117 – 278
Coreferential English pronouns aligned with any Czech translation 141 141 317 317
Coreferential English pronouns translated as Czech pronouns 71 75 198 198
Czech pronouns that are valid translations of the English pronouns 63 71 182 182
Czech pronouns that are valid translations of the English pronouns 44 46 142 146
and that match their antecedent in number and gender
</table>
<tableCaption confidence="0.937803">
Table 2: Automated Evaluation Results for both test sets
</tableCaption>
<table confidence="0.995179333333333">
Criterion Baseline System Better Annotated System Better Systems Equal
Overall quality 9/31 (29.03%) 11/31 (35.48%) 11/31 (35.48%)
Quality when annotation is correct 3/18 (16.67%) 9/18 (50.00%) 6/18 (33.33%)
</table>
<tableCaption confidence="0.999621">
Table 3: Manual Evaluation Results: A direct comparison of pronoun translations that differ between systems
</tableCaption>
<bodyText confidence="0.9973596">
Taking the accuracy of pronoun translation to
be the proportion of coreferential English pro-
nouns having a valid Czech translation that agrees
in both number and gender with their antecedent,
yields the following on the two test sets:
</bodyText>
<equation confidence="0.934947">
Baseline system:
Development — 44/141 (31.21%)
Final — 142/331 (42.90%)
Annotated system:
Development — 46/141 (32.62%)
Final — 146/331 (44.10%)
</equation>
<bodyText confidence="0.9999566875">
There are, however, several reasons for not tak-
ing this evaluation as definitive. Firstly, it relies
on the accuracy of the word alignments output by
the decoder to identify the Czech translations of
the English pronoun and its antecedent. Secondly,
these results fail to capture variation between the
translations produced by the Baseline and Anno-
tated systems. Whilst there is a fairly high de-
gree of overlap, for approximately 1/3 of the “De-
velopment” set pronouns and 1/6 of the “Final”
set pronouns, the Czech translation is different.
Since the goal of this work was to understand
what is needed in order to improve the transla-
tion of coreferential pronouns, manual evaluation
was critical for understanding the potential capa-
bilities of source-side annotation.
</bodyText>
<subsectionHeader confidence="0.997829">
5.2 Manual Evaluation
</subsectionHeader>
<bodyText confidence="0.9996525">
The sample files provided for manual evaluation
contained 31 pronouns for which the translations
provided by the two systems differed (differences)
and 72 for which the translation provided by the
systems was the same (matches). Thus, the sam-
ple comprised 103 of the 472 coreferential pro-
nouns (about 22%) from across both test sets. Of
this sample, it is the differences that indicate the
relative performance of the two systems. Of the
31 pronouns in this set, 16 were 3rd-person pro-
nouns, 2 were reflexive personal pronouns and 13
were possessive pronouns.
The results corresponding to evaluation crite-
rion 4 in Section 4.4 provide a comparison of the
overall quality of pronoun translation for both sys-
tems. These results for the “Development” and
“Final” test sets (see Table 3) suggest that the per-
formance of the Annotated system is comparable
with, and even marginally better than, that of the
Baseline system, especially when the pronoun an-
notation is correct.
An example of where the Annotated system
produces a better translation than the Baseline
system is:
</bodyText>
<construct confidence="0.988933666666667">
Annotated English: he said mexico could be one of the
next countries to be removed from the priority list because of
its.neut.sg efforts to craft a new patent law .
Baseline translation: ˇrekl , ˇze mexiko by mohl b´yt jeden
z dalˇsich zemi, aby byl odvol´an z prioritou seznam , protoˇze
jejisnahy podpoˇrit nov´e patentov´y z´akon .
Annotated translation: ˇrekl , ˇze mexiko by mohl b´yt je-
den z dalˇsich zemi, aby byl odvol´an z prioritou seznam ,
protoˇze jeho snahy podpoˇrit nov´e patentov´y z´akon .
</construct>
<bodyText confidence="0.999970875">
In this example, the English pronoun “its”,
which refers to “mexico” is annotated as neuter
and singular (as extracted from the Baseline trans-
lation). Both systems translate “mexico” as
“mexiko” (neuter, singular) but differ in their
translation of the pronoun. The Baseline system
translates “its” incorrectly as “jeji” (feminine, sin-
gular), whereas the Annotated system produces
</bodyText>
<page confidence="0.998846">
7
</page>
<bodyText confidence="0.99994558974359">
the more correct translation: “jeho” (neuter, sin-
gular), which agrees with the antecedent in both
number and gender.
An analysis of the judgements on the remain-
ing three evaluation criteria (outlined in Section
4.4) for the 31 differences provides further infor-
mation. The Baseline system appears to be more
accurate, with 19 pronouns either correctly trans-
lated (in terms of number and gender) or appro-
priately dropped, compared with 17 for the An-
notated system. Of those pronouns, the meaning
could still be understood for 7/12 for the Baseline
system compared with 8/14 for the Annotated sys-
tem. On the surface this may seem strange but it
appears to be due to a small number of cases in
which the translations produced by both systems
were incorrect but those produced by the Anno-
tated system were deemed to be marginally better.
Due to the small sample size it is difficult to form
a complete picture of where one system may per-
form consistently better than the other. The anno-
tation of both number and gender was accurate for
18 pronouns. Whilst this accuracy is not particu-
larly high, the results (see Table 3) suggest that
translation is more accurate for those pronouns
that are correctly annotated.
Whilst pro-drop in Czech was not explicitly
handled in the annotation process, manual evalu-
ation revealed that both systems were able to suc-
cessfully ‘learn’ a few (local) scenarios in which
pro-drop is appropriate. This was unexpected but
found to be due to instances in which there are
short distances between the pronoun and verb in
English. For example, many of the occurrences
of “she” in English appear in the context of “she
said...” and are translated correctly with the verb
form “...ˇrekla...”.
An example of where the Annotated system
correctly drops a pronoun is:
</bodyText>
<construct confidence="0.994232923076923">
Annotated English: “ this is the worst shakeout ever in
the junk market, and it could take years before it.fem.sg ’
s over, ” says mark bachmann , a senior vice president at
standard &amp; poor ’ s corp . , a credit rating company.
Baseline translation: “ je to nejhorˇs´ı krize , kdy na trhu
s rizikov´ymi obligacemi , a to m˚uˇze trvat roky , neˇz je to pryˇc
, ” ˇr´ık´a mark bachmann , hlavn´ı viceprezident spoleˇcnosti
standard &amp; poor ’s corp . , ´uvˇerov´y rating spoleˇcnosti .
Annotated translation: “ je to nejhorˇs´ı krize , kdy na
trhu s rizikov´ymi obligacemi , a to m˚uˇze trvat roky , neˇz
je !! pryˇc , ” ˇr´ık´a mark bachmann , hlavn´ı viceprezident
spoleˇcnosti standard &amp; poor ’s corp . ,´uvˇerov´y rating
spoleˇcnosti .
</construct>
<bodyText confidence="0.9907865">
In this example, the Baseline system trans-
lates “it” incorrectly as the neuter singular pro-
noun “to”, whereas the Annotated system cor-
rectly drops the subject pronoun (indicated by !!)
— this is a less trivial example than “she said”. In
the case of the Baseline translation “to” could be
interpreted as referring to the whole event, which
would be correct, but poor from a stylistic point
of view.
An example of where the Annotated system
fails to drop a pronoun is:
Annotated English: taiwan has improved its.mascin.sg*
standing with the u.s. by initialing a bilateral copyright
agreement, amending its.mascin.sg** trademark law and
introducing legislation to protect foreign movie producers
from unauthorized showings of their.mascan.pl films .
</bodyText>
<construct confidence="0.9623915">
Annotated translation: tchaj-wan zlepˇsen´ı sv´e
postaven´ı s usa o initialing bilater´aln´ıch autorsk´ych pr´av na
jeho obchodn´ı dohody, ´uprava z´akona a zaveden´ı z´akona
na ochranu zahraniˇcn´ı filmov´e producenty z neopr´avnˇen´e
showings sv´ych film˚u .
Reference translation: tchaj-wan zlepˇsil svou reputaci
v usa , kdyˇz podepsal bilater´aln´ı smlouvu o autorsk´ych
pr´avech , pozmˇenil !! z´akon o ochrann´ych zn´amk´ach a
zavedl legislativu na ochranu zahraniˇcn´ıch filmov´ych produ-
cent˚u proti neautorizovan´emu prom´ıt´an´ıjejich film˚u .
</construct>
<bodyText confidence="0.999984333333333">
In this example, the English pronoun “its”,
which refers to “taiwan” is annotated as mascu-
line inanimate and singular. The first occurrence
of “its” is marked by * and the second occurrence
by ** in the annotated English text above. The
second occurrence should be translated either as
a reflexive pronoun (as the first occurrence is cor-
rectly translated) or it should be dropped as in the
reference translation (!! indicates the position of
the dropped pronoun).
In addition to the judgements, the manual as-
sessor also provided feedback on the evalua-
tion task. One of the major difficulties encoun-
tered concerned the translation of pronouns in
sentences which exhibit poor syntactic structure.
This is a criticism of Machine Translation as a
whole, but of the manual evaluation of pronoun
translation in particular, since the choice of core-
ferring form is sensitive to syntactic structure.
Also the effects of poor syntactic structure are
likely to introduce an additional element of sub-
jectivity if the assessor must first interpret the
structure of the sentences output by the transla-
tion systems.
</bodyText>
<subsectionHeader confidence="0.981286">
5.3 Potential Sources of Error
</subsectionHeader>
<bodyText confidence="0.987454">
Related errors that may have contributed to the
Annotated system not providing a significant im-
provement over the Baseline include: (1) incor-
</bodyText>
<page confidence="0.991818">
8
</page>
<bodyText confidence="0.9999585">
rect identification of the English antecedent head
noun, (2) incorrect identification of the Czech
translation of the antecedent head noun in the
Baseline output due to errors in the word align-
ments, and (3) errors in the PCEDT 2.0 align-
ment file (affecting training only). While “per-
fect” annotation of the BBN Pronoun Coreference
and Entity Type, the PCEDT 2.0 and the Penn
Treebank 3.0 corpora has been assumed, errors in
these corpora cannot be completely ruled out.
</bodyText>
<sectionHeader confidence="0.99663" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.996415921052632">
Despite factoring out three major sources of er-
ror — identifying coreferential pronouns, finding
their antecedents, and identifying the head of each
antecedent — through the use of manually anno-
tated corpora, the results of the Annotated system
show only a small improvement over the Baseline
system. Two possible reasons for this are that the
statistics in the phrase translation table have been
weakened in the Annotated system as a result of
including both number and gender in the anno-
tation and that the size of the training corpus is
relatively small.
However, more significant may be the avail-
ability of only a single reference translation. This
affects the development and application of au-
tomated evaluation metrics as a single reference
cannot capture the variety of possible valid trans-
lations. Coreference can be achieved without ex-
plicit pronouns. This is true of both English and
Czech, with sentences that contain pronouns hav-
ing common paraphrases that lack them. For ex-
ample,
the u.s. , claiming some success in its trade
diplomacy , ...
can be paraphrased as:
the u.s. , claiming some success in trade diplo-
macy , ...
A target-language translation of the former
might actually be a translation of the latter, and
hence lack the pronoun shown in bold. Given the
range of variability in whether pronouns are used
in conveying coreference, the availability of only
a single reference translation is a real problem.
Improving the accuracy of coreferential pro-
noun translation remains an open problem in Ma-
chine Translation and as such there is great scope
for future work in this area. The investigation re-
ported here suggests that it is not sufficient to fo-
cus solely on the source-side and further opera-
tions on the target side (besides post-translation
application of a target-language model) need also
be considered. Other target–side operations could
involve the extraction of features to score multi-
ple candidate translations in the selection of the
‘best’ option – for example, to ‘learn’ scenar-
ios in which pro-drop is appropriate and to select
translations that contain pronouns of the correct
morphological inflection. This requires identifica-
tion of features in the target side, their extraction
and incorporation in the translation process which
could be difficult to achieve within a purely sta-
tistical framework given that the antecedent of a
pronoun may be arbitrarily distant in the previous
discourse.
The aim of this work was to better understand
why previous attempts at using annotation projec-
tion in pronoun translation showed less than ex-
pected improvement. Thus it would be beneficial
to conduct an error analysis to show the frequency
of the errors described in Section 5.3 appear.
I will also be exploring other directions re-
lated to problems identified during the course of
the work completed to date. These include, but
are not limited to, handling pronoun dropping in
pro-drop languages, developing pronoun-specific
automated evaluation metrics and addressing the
problem of having only one reference translation
for use with such metrics. In this regard, I will be
considering the use of paraphrase techniques to
generate synthetic reference translations to aug-
ment an existing reference translation set. Ini-
tial efforts will focus on adapting the approach of
Kauchak &amp; Barzilay (2006) and back–translation
methods for extracting paraphrases (Bannard &amp;
Callison-Burch, 2005) to the more specific prob-
lem of pronoun variation.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998861090909091">
I would like to thank Bonnie Webber (Univer-
sity of Edinburgh) who supervised this project
and Mark´eta Lopatkov´a (Charles University) who
provided the much needed Czech language assis-
tance. I am very grateful to Ondˇrej Bojar (Charles
University) for his numerous helpful suggestions
and to the Institute of Formal and Applied Lin-
guistics (Charles University) for providing the
PCEDT 2.0 corpus. I would also like to thank
Wolodja Wentland and the three anonymous re-
viewers for their feedback.
</bodyText>
<page confidence="0.997388">
9
</page>
<sectionHeader confidence="0.995873" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99994689380531">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the ACL,
pages 597–604.
Chris Callison-Burch, Philipp Koehn, Christof Monz
and Omar Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22–64.
Mona Diab. 2004. An Unsupervised Approach for
Bootstrapping Arabic Sense Tagging. In Proceed-
ings of the Workshop on Computational Approaches
to Arabic Script-based Languages, pages 43–50.
Kevin Gimpel and Noah A. Smith. 2008. Rich
Source-Side Context for Statistical Machine Trans-
lation. In Proceedings of the Third Workshop on
Statistical Machine Translation, pages 9–17.
Barbara J. Grosz, Scott Weinstein and Aravind K.
Joshi. 1995. Centering: A Framework for Mod-
eling the Local Coherence Of Discourse. Computa-
tional Linguistics, 21(2):203–225.
Christian Hardmeier and Marcello Federico. 2010.
Modelling Pronominal Anaphora in Statistical Ma-
chine Translation. In Proceedings of the 7th In-
ternational Workshop on Spoken Language Trans-
lation, pages 283–290.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens. Chris Dyer, Ond&amp;quot;rej Bojar,
Alexandra Constantin and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 177–180.
Jerry R. Hobbs. 1978. Resolving Pronominal Refer-
ences. Lingua, 44:311–338.
Jan Haji&amp;quot;c, Eva Haji&amp;quot;cov´a, Jarmila Panevov´a, Petr Sgall,
Silvie Cinkov´a, Eva Fu&amp;quot;c´ıkov´a, Marie Mikulov´a,
Petr Pajas, Jan Popelka, Ji&amp;quot;r´ı Semeck´y, Jana
&amp;quot;Sindlerov´a, Jan &amp;quot;St&amp;quot;ep´anek, Josef Toman, Zde&amp;quot;nka
Ure&amp;quot;sov´a and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y. 2011. Prague
Czech-English Dependency Treebank 2.0. Institute
of Formal and Applied Linguistics. Prague, Czech
Republic.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing For Automatic Evaluation. In Proceedings
of the Main Conference on Human Language Tech-
nology Conference of the NAACL, June 5–7, New
York, USA, pages 455–462.
Mitesh M. Khapra, Sapan Shah, Piyush Kedia and
Pushpak Bhattacharyya. 2009. Projecting Param-
eters for Multilingual Word Sense Disambiguation.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, Au-
gust 6–7, Singapore, pages 459–467.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved Backing-Off for M-gram Language Model-
ing. IEEE International Conference on Acoustics,
Speech, and Signal Processing, May 9–12, Detroit,
USA, 1:181–184.
Shalom Lappin and Herbert J. Leass. 1994. An Algo-
rithm for Pronominal Anaphora Resolution. Com-
putational Linguistics, 20:535–561.
Ronan Le Nagard and Philipp Koehn. 2010. Aid-
ing Pronoun Translation with Co-reference Resolu-
tion. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 252–261.
Vincent Ng. 2010. Supervised Noun Phrase Corefer-
ence Research: The first 15 years. In Proceedings
of the 48th Meeting of the ACL, pages 1396–1411.
Mitchell P. Marcus, Beatrice Santorini, Mary A.
Marcinkiewicz and Ann Taylor. 1999. Penn Tree-
bank 3.0 LDC Calalog No.: LDC99T42. Linguistic
Data Consortium.
Ruslan Mitkov, Sung-Kwon Choi and Randall Sharp.
1995. Anaphora Resolution in Machine Transla-
tion. In Proceedings of the Sixth International Con-
ference on Theoretical and Methodological Issues
in Machine Translation, July 5-7, Leuven, Belgium,
pages 5–7.
Ruslan Mitkov. 1999. Introduction: Special Issue on
Anaphora Resolution in Machine Translation and
Multilingual NLP. Machine Translation, 14:159–
161.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311–
318.
Horacio Saggion and Ariadne Carvalho. 1994.
Anaphora Resolution in a Machine Translation Sys-
tem. In Proceedings of the International Con-
ference on Machine Translation: Ten Years On,
November, Cranfield, UK, 4.1-4.14.
Andreas Stolcke. 2002. SRILM — An Extensible
Language Modeling Toolkit. In Proceedings of In-
ternational Conference on Spoken Language Pro-
cessing, September 16-20, Denver, USA, 2:901–
904.
Michael Strube. 2007. Corpus-based and Ma-
chine Learning Approaches to Anaphora Resolu-
tion. Anaphors in Text: Cognitive, Formal and
Applied Approaches to Anaphoric Reference, John
Benjamins Pub Co.
Ralph Weischedel and Ada Brunstein. 2005. BBN
Pronoun Coreference and Entity Type Corpus LDC
Calalog No.: LDC2005T33. Linguistic Data Con-
sortium.
</reference>
<page confidence="0.997794">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.737570">
<title confidence="0.999515">Improving Pronoun Translation for Statistical Machine Translation</title>
<author confidence="0.963134">Liane</author>
<affiliation confidence="0.997822">School of University of</affiliation>
<address confidence="0.974258">Edinburgh, UK, EH8</address>
<email confidence="0.998862">L.K.Guillou@sms.ed.ac.uk</email>
<abstract confidence="0.991576653846154">Machine Translation is a well–established field, yet the majority of current systems translate sentences in isolation, losing valuable contextual information from previously translated sentences in the discourse. One important type of contextual information concerns who or what a coreferring corefers to (i.e., its Languages differ significantly in how they achieve coreference, and awareness of antecedents is important in choosing the correct pronoun. Disregarding a pronoun’s antecedent in translation can lead to inappropriate coreferring forms in the target text, seriously degrading a reader’s ability to understand it. This work assesses the extent to which annotation coreferring pronouns can improve English–Czech Statistical Machine Translation (SMT). As with previous attempts that use this method, the results show little improvement. This paper attempts to explain why and to provide insight into the factors affecting performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with Bilingual Parallel Corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>597--604</pages>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. In Proceedings of the 43rd Annual Meeting of the ACL, pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<date>2011</date>
<booktitle>Findings of the 2011 Workshop on Statistical Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<contexts>
<context position="18024" citStr="Callison-Burch et al., 2011" startWordPosition="2863" endWordPosition="2866">ed using the process described above, with the number and gender of the Czech translation of the antecedent head obtained from the PCEDT 2.0 alignment file. 4.3 Processing test files Two test files were used (see Table 1) – one called ‘Final’ and the other, ‘Development’ (Dev). A test file is first translated using the Baseline system with a trace added to the Moses decoder. Each coreferential English pronoun is then identified using the BBN Pronoun Coreference and Entity Type corpus and the head of its antecedent is ex1Provided for the Sixth EMNLP Workshop on Statistical Machine Translation (Callison-Burch et al., 2011) tracted from the annotated NPs in the Penn Treebank 3.0 corpus. The sentence number and word position of the English pronoun and its antecedent head noun(s) are extracted from the input English text and used to identify the English/Czech phrase pairs that contain the Czech translations of the English words. Using this information together with the phrase alignments (output by the Moses decoder) and the phrase-internal word alignments in the phrase translation table, a Czech translation is obtained from the Baseline system. Number, gender and animacy (if masculine) features of the Czech word i</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz and Omar Zaidan. 2011. Findings of the 2011 Workshop on Statistical Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
</authors>
<title>An Unsupervised Approach for Bootstrapping Arabic Sense Tagging.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages,</booktitle>
<pages>43--50</pages>
<contexts>
<context position="11844" citStr="Diab, 2004" startWordPosition="1874" endWordPosition="1875">d model coreference using a word dependency module integrated within the log-linear SMT model as an additional feature function. Annotation projection has been used elsewhere in SMT. Gimpel &amp; Smith (2008) use it to capture long–distance phenomena within a single sentence in the source-language text via the extraction of sentence-level contextual features, which are used to augment SMT translation models and better predict phrase translation. Projection techniques have also been applied to multilingual Word Sense Disambiguation whereby the sense of a word may be determined in another language (Diab, 2004; Khapra et al., 2009). 4 Methodology 4.1 Overview I have followed Le Nagard &amp; Koehn (2010) in using a two-step approach to translation, with annotation projection incorporated as a pre-processing 3 (4) Annotation of English pronoun with number and gender of Czech word Figure 1: Overview of the Annotation Process Input: The castle is old. It stands on a hill. Translate: Translate: The castle is old. Hrad je start&apos;. It stands on a hill. (2) Identification of antecedent head (1) Identification of coreferential pronoun The castle is old. It stands on a hill. (3) English – Czech mapping of anteced</context>
</contexts>
<marker>Diab, 2004</marker>
<rawString>Mona Diab. 2004. An Unsupervised Approach for Bootstrapping Arabic Sense Tagging. In Proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages, pages 43–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Rich Source-Side Context for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>9--17</pages>
<contexts>
<context position="11438" citStr="Gimpel &amp; Smith (2008)" startWordPosition="1810" endWordPosition="1813">s more complex in Czech with both number and gender influencing pronoun selection. Annotating pronouns with both number and gender further exacerbates the problem of data sparseness in the training data, but this cannot be avoided if the aim is to improve their translation. This work also accommodates a wider range of English pronouns. In contrast, Hardmeier &amp; Federico (2010) focus on English-German translation and model coreference using a word dependency module integrated within the log-linear SMT model as an additional feature function. Annotation projection has been used elsewhere in SMT. Gimpel &amp; Smith (2008) use it to capture long–distance phenomena within a single sentence in the source-language text via the extraction of sentence-level contextual features, which are used to augment SMT translation models and better predict phrase translation. Projection techniques have also been applied to multilingual Word Sense Disambiguation whereby the sense of a word may be determined in another language (Diab, 2004; Khapra et al., 2009). 4 Methodology 4.1 Overview I have followed Le Nagard &amp; Koehn (2010) in using a two-step approach to translation, with annotation projection incorporated as a pre-processi</context>
</contexts>
<marker>Gimpel, Smith, 2008</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2008. Rich Source-Side Context for Statistical Machine Translation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 9–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Scott Weinstein</author>
<author>Aravind K Joshi</author>
</authors>
<title>Centering: A Framework for Modeling the Local Coherence Of Discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<marker>Grosz, Weinstein, Joshi, 1995</marker>
<rawString>Barbara J. Grosz, Scott Weinstein and Aravind K. Joshi. 1995. Centering: A Framework for Modeling the Local Coherence Of Discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Marcello Federico</author>
</authors>
<title>Modelling Pronominal Anaphora in Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Workshop on Spoken Language Translation,</booktitle>
<pages>283--290</pages>
<contexts>
<context position="2102" citStr="Hardmeier &amp; Federico, 2010" startWordPosition="307" endWordPosition="310">tactic tree), these agreement features can influence the translation. But this locality cannot be guaranteed in either phrase-based or syntax-based Statistical Machine Translation (SMT). If it is not within the same unit, a coreferring pronoun will be translated without knowledge of its antecedent, meaning that its translation will simply reflect local frequency. Incorrectly translating a pronoun can result in readers/listeners identifying the wrong antecedent, which can mislead or confuse them. There have been two recent attempts to solve this problem within the framework of phrasebased SMT (Hardmeier &amp; Federico, 2010; Le Nagard &amp; Koehn, 2010). Both involve annotation projection, which in this context means annotating coreferential pronouns in the sourcelanguage with features derived from the translation of their aligned antecedents, and then building a translation model of the annotated forms. When translating a coreferring pronoun in a new source-language text, the antecedent is identified and its translation used (differently in the two attempts cited above) to annotate the pronoun prior to translation. The aim of this work was to better understand why neither of the previous attempts achieved more than</context>
<context position="5024" citStr="Hardmeier &amp; Federico (2010)" startWordPosition="767" endWordPosition="770">pus (Marcus et al., 1999) were used. The gold standard annotation provided by these corpora allowed me to assume perfect identification of coreferring pronouns and coreference resolution and near–perfect antecedent head noun identification. These assumptions could not be made if state-ofthe-art methods had been used as they cannot yet achieve sufficiently high levels of accuracy. The remainder of the paper is structured as follows. The use of pronominal coreference in English and Czech and the problem of anaphora resolution are described in Section 2. The works of Le Nagard &amp; Koehn (2010) and Hardmeier &amp; Federico (2010) are discussed in Section 3, and the source-language annotation projection method is described in Section 4. The results are presented and discussed in Section 5 and future work is outlined in Section 6. 2 Background 2.1 Anaphora Resolution Anaphora resolution involves identifying the antecedent of a referring expression, typically a pronoun or noun phrase that is used to refer to something previously mentioned in the discourse (its antecedent). Where multiple referring expressions refer to the same antecedent, they are said to be coreferential. Anaphora resolution and the related task of core</context>
<context position="10258" citStr="Hardmeier &amp; Federico (2010)" startWordPosition="1621" endWordPosition="1624">t checked in annotation projection, the method presented here for translating coreferring pronouns does not guarantee their correct form. 3 Related Work Early work on integrating anaphora resolution with Machine Translation includes the rule-based approaches of Mitkov et al. (1995) and Lappin &amp; Leass (1994) and the transfer-based approach of Saggion &amp; Carvalho (1994). Work in the 1990’s culminated in the publication of a special issue of Machine Translation on anaphora resolution (Mitkov, 1999). Work then appears to have been on hold until papers were published by Le Nagard &amp; Koehn (2010) and Hardmeier &amp; Federico (2010). This resurgence of interest follows advances since the 1990’s which have made new approaches possible. The work described in this paper resembles that of Le Nagard &amp; Koehn (2010), with two main differences. The first is the use of manually annotated corpora to extract coreference information and morphological properties of the target translations of the antecedents. The second lies in the choice of language pair. They consider EnglishFrench translation, focussing on gender-correct translation of the third person pronouns “it” and “they”. Coreference is more complex in Czech with both number </context>
<context position="15263" citStr="Hardmeier &amp; Federico (2010)" startWordPosition="2413" endWordPosition="2416">ference and Entity Type corpus, and obtains the Czech translation of the English antecedent from the translation produced by the Baseline system. Because many antecedents come from previous sentences, these sentences must be translated before translating the current sentence. Here I follow Le Nagard &amp; Koehn (2010) in translating the complete sourcelanguage text using the Baseline system and then extracting the (here, Czech) translations of the English antecedents from the output. This provides a simple solution to the problem of obtaining the Czech translation prior to annotation. In contrast Hardmeier &amp; Federico (2010) translate sentence by sentence using a process which was deemed to be more complex than was necessary for this project. The English text is annotated such that all coreferential pronouns whose antecedents have an identifiable Czech translation are marked with the number and gender of that Czech word. The output of the annotation process is thus the same English text that was input to the Baseline system, with the addition of annotation of the coreferential pronouns. This annotated English text is then translated using the Annotated translation system, the output of which is the final translat</context>
<context position="19772" citStr="Hardmeier &amp; Federico (2010)" startWordPosition="3156" endWordPosition="3159">eline and Annotated systems were trained using the same word alignments, the Czech translation of the head of the English antecedent should be the same in the output of both. Judging by the very small number of cases in which the antecedent translations differed (3 out of 458 instances), this assumption was proved to be reasonable. These differences were due to the use of different phrase tables for each system as a result of training on different data (i.e. the annotation of English pronouns or lack thereof). This would not be an issue for single-step translation systems such as that used by Hardmeier &amp; Federico (2010). 4.4 Evaluation No standard method yet exists for evaluating pronoun translation in SMT. Early work focussed on the development of techniques for anaphora resolution and their integration within Machine Translation (Lappin &amp; Leass, 1994; Saggion &amp; Carvalho, 1994; Mitkov et al., 1995), with little mention of evaluation. In recent work, evaluation has become much more important. Both Le Nagard &amp; Koehn (2010) and Hardmeier &amp; Federico (2010) consider and reject BLEU (Papineni et al., 2002) as ill-suited for evaluating pronoun translation. While Hardmeier &amp; Federico propose and 5 use a strict reca</context>
</contexts>
<marker>Hardmeier, Federico, 2010</marker>
<rawString>Christian Hardmeier and Marcello Federico. 2010. Modelling Pronominal Anaphora in Statistical Machine Translation. In Proceedings of the 7th International Workshop on Spoken Language Translation, pages 283–290.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ond&amp;quot;rej Bojar, Alexandra Constantin and Evan Herbst.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens. Chris Dyer, Ond&amp;quot;rej Bojar, Alexandra Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Resolving Pronominal References.</title>
<date>1978</date>
<tech>Lingua,</tech>
<pages>44--311</pages>
<marker>Hobbs, 1978</marker>
<rawString>Jerry R. Hobbs. 1978. Resolving Pronominal References. Lingua, 44:311–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
<author>Eva Hajicov´a</author>
<author>Jarmila Panevov´a</author>
<author>Petr Sgall</author>
<author>Silvie Cinkov´a</author>
<author>Eva Fuc´ıkov´a</author>
<author>Marie Mikulov´a</author>
<author>Petr Pajas</author>
<author>Jan Popelka</author>
</authors>
<title>Ji&amp;quot;r´ı Semeck´y, Jana &amp;quot;Sindlerov´a, Jan &amp;quot;St&amp;quot;ep´anek, Josef Toman, Zde&amp;quot;nka Ure&amp;quot;sov´a and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y.</title>
<date>2011</date>
<location>Prague, Czech Republic.</location>
<marker>Hajic, Hajicov´a, Panevov´a, Sgall, Cinkov´a, Fuc´ıkov´a, Mikulov´a, Pajas, Popelka, 2011</marker>
<rawString>Jan Haji&amp;quot;c, Eva Haji&amp;quot;cov´a, Jarmila Panevov´a, Petr Sgall, Silvie Cinkov´a, Eva Fu&amp;quot;c´ıkov´a, Marie Mikulov´a, Petr Pajas, Jan Popelka, Ji&amp;quot;r´ı Semeck´y, Jana &amp;quot;Sindlerov´a, Jan &amp;quot;St&amp;quot;ep´anek, Josef Toman, Zde&amp;quot;nka Ure&amp;quot;sov´a and Zden&amp;quot;ek &amp;quot;Zabokrtsk´y. 2011. Prague Czech-English Dependency Treebank 2.0. Institute of Formal and Applied Linguistics. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing For Automatic Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Main Conference on Human Language Technology Conference of the NAACL,</booktitle>
<pages>455--462</pages>
<location>New York, USA,</location>
<contexts>
<context position="37553" citStr="Kauchak &amp; Barzilay (2006)" startWordPosition="6009" endWordPosition="6012">tion 5.3 appear. I will also be exploring other directions related to problems identified during the course of the work completed to date. These include, but are not limited to, handling pronoun dropping in pro-drop languages, developing pronoun-specific automated evaluation metrics and addressing the problem of having only one reference translation for use with such metrics. In this regard, I will be considering the use of paraphrase techniques to generate synthetic reference translations to augment an existing reference translation set. Initial efforts will focus on adapting the approach of Kauchak &amp; Barzilay (2006) and back–translation methods for extracting paraphrases (Bannard &amp; Callison-Burch, 2005) to the more specific problem of pronoun variation. Acknowledgements I would like to thank Bonnie Webber (University of Edinburgh) who supervised this project and Mark´eta Lopatkov´a (Charles University) who provided the much needed Czech language assistance. I am very grateful to Ondˇrej Bojar (Charles University) for his numerous helpful suggestions and to the Institute of Formal and Applied Linguistics (Charles University) for providing the PCEDT 2.0 corpus. I would also like to thank Wolodja Wentland a</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing For Automatic Evaluation. In Proceedings of the Main Conference on Human Language Technology Conference of the NAACL, June 5–7, New York, USA, pages 455–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh M Khapra</author>
<author>Sapan Shah</author>
<author>Piyush Kedia</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Projecting Parameters for Multilingual Word Sense Disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>459--467</pages>
<contexts>
<context position="11866" citStr="Khapra et al., 2009" startWordPosition="1876" endWordPosition="1879">ference using a word dependency module integrated within the log-linear SMT model as an additional feature function. Annotation projection has been used elsewhere in SMT. Gimpel &amp; Smith (2008) use it to capture long–distance phenomena within a single sentence in the source-language text via the extraction of sentence-level contextual features, which are used to augment SMT translation models and better predict phrase translation. Projection techniques have also been applied to multilingual Word Sense Disambiguation whereby the sense of a word may be determined in another language (Diab, 2004; Khapra et al., 2009). 4 Methodology 4.1 Overview I have followed Le Nagard &amp; Koehn (2010) in using a two-step approach to translation, with annotation projection incorporated as a pre-processing 3 (4) Annotation of English pronoun with number and gender of Czech word Figure 1: Overview of the Annotation Process Input: The castle is old. It stands on a hill. Translate: Translate: The castle is old. Hrad je start&apos;. It stands on a hill. (2) Identification of antecedent head (1) Identification of coreferential pronoun The castle is old. It stands on a hill. (3) English – Czech mapping of antecedent head Hrad je start</context>
</contexts>
<marker>Khapra, Shah, Kedia, Bhattacharyya, 2009</marker>
<rawString>Mitesh M. Khapra, Sapan Shah, Piyush Kedia and Pushpak Bhattacharyya. 2009. Projecting Parameters for Multilingual Word Sense Disambiguation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, August 6–7, Singapore, pages 459–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Backing-Off for M-gram Language Modeling.</title>
<date>1995</date>
<booktitle>IEEE International Conference on Acoustics, Speech, and Signal Processing, May 9–12,</booktitle>
<pages>1--181</pages>
<location>Detroit, USA,</location>
<contexts>
<context position="16486" citStr="Kneser &amp; Ney, 1995" startWordPosition="2607" endWordPosition="2610"> 4 Training Dev. Final Parallel Sentences 47,549 280 540 Czech Words 955,018 5,467 10,110 English Words 1,024,438 6,114 11,907 Table 1: Sizes of the training and testing datasets 4.2 Baseline and Annotated systems Both systems are phrase-based SMT models, trained using the Moses toolkit (Hoang et al., 2007). They share the same 3-gram language model constructed from the target-side text of the parallel training corpus and the Czech monolingual 2010 and 2011 News Crawl corpora1. The language model was constructed using the SRILM toolkit (Stolcke, 2002) with interpolated Kneser-Ney discounting (Kneser &amp; Ney, 1995). In addition, both systems are forced to use the same word alignments (constructed using Giza++ (Och &amp; Ney, 2003) in both language pair directions and using stemmed training data in which words are limited to the first four characters) in order to mitigate the effects of Czech word inflection on word alignment statistics. This helps to ensure that the Czech translation of the head of the antecedent remains constant in both steps of the two-step process. If this were to change it would defeat the purpose of pronoun annotation as different Czech translations could result in different gender and</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved Backing-Off for M-gram Language Modeling. IEEE International Conference on Acoustics, Speech, and Signal Processing, May 9–12, Detroit, USA, 1:181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
<author>Herbert J Leass</author>
</authors>
<title>An Algorithm for Pronominal Anaphora Resolution. Computational Linguistics,</title>
<date>1994</date>
<pages>20--535</pages>
<contexts>
<context position="9939" citStr="Lappin &amp; Leass (1994)" startWordPosition="1568" endWordPosition="1571">wn) castle Czech: Obdivoval jsme sv˚uj hrad Here “sv´eho” is used to refer to a dog (masculine animate, singular) and “sv˚uj” to refer to a castle (masculine inanimate, singular), both of which belong to the speaker. Because a pronoun may take a large number of morphological forms in Czech and because case is not checked in annotation projection, the method presented here for translating coreferring pronouns does not guarantee their correct form. 3 Related Work Early work on integrating anaphora resolution with Machine Translation includes the rule-based approaches of Mitkov et al. (1995) and Lappin &amp; Leass (1994) and the transfer-based approach of Saggion &amp; Carvalho (1994). Work in the 1990’s culminated in the publication of a special issue of Machine Translation on anaphora resolution (Mitkov, 1999). Work then appears to have been on hold until papers were published by Le Nagard &amp; Koehn (2010) and Hardmeier &amp; Federico (2010). This resurgence of interest follows advances since the 1990’s which have made new approaches possible. The work described in this paper resembles that of Le Nagard &amp; Koehn (2010), with two main differences. The first is the use of manually annotated corpora to extract coreferenc</context>
<context position="20009" citStr="Lappin &amp; Leass, 1994" startWordPosition="3193" endWordPosition="3196">nslations differed (3 out of 458 instances), this assumption was proved to be reasonable. These differences were due to the use of different phrase tables for each system as a result of training on different data (i.e. the annotation of English pronouns or lack thereof). This would not be an issue for single-step translation systems such as that used by Hardmeier &amp; Federico (2010). 4.4 Evaluation No standard method yet exists for evaluating pronoun translation in SMT. Early work focussed on the development of techniques for anaphora resolution and their integration within Machine Translation (Lappin &amp; Leass, 1994; Saggion &amp; Carvalho, 1994; Mitkov et al., 1995), with little mention of evaluation. In recent work, evaluation has become much more important. Both Le Nagard &amp; Koehn (2010) and Hardmeier &amp; Federico (2010) consider and reject BLEU (Papineni et al., 2002) as ill-suited for evaluating pronoun translation. While Hardmeier &amp; Federico propose and 5 use a strict recall and precision based metric for English–German translation, I found it unsuitable for English–Czech translation, given the highly inflective nature of Czech. Given the importance of evaluation to the goal of assessing the effectiveness</context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>Shalom Lappin and Herbert J. Leass. 1994. An Algorithm for Pronominal Anaphora Resolution. Computational Linguistics, 20:535–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Le Nagard</author>
<author>Philipp Koehn</author>
</authors>
<title>Aiding Pronoun Translation with Co-reference Resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>252--261</pages>
<marker>Le Nagard, Koehn, 2010</marker>
<rawString>Ronan Le Nagard and Philipp Koehn. 2010. Aiding Pronoun Translation with Co-reference Resolution. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 252–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Supervised Noun Phrase Coreference Research: The first 15 years.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Meeting of the ACL,</booktitle>
<pages>1396--1411</pages>
<contexts>
<context position="5793" citStr="Ng (2010)" startWordPosition="888" endWordPosition="889">5 and future work is outlined in Section 6. 2 Background 2.1 Anaphora Resolution Anaphora resolution involves identifying the antecedent of a referring expression, typically a pronoun or noun phrase that is used to refer to something previously mentioned in the discourse (its antecedent). Where multiple referring expressions refer to the same antecedent, they are said to be coreferential. Anaphora resolution and the related task of coreference resolution have been the subject of considerable research within Natural Language Processing (NLP). Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader’s ability to understand a text. Accurate coreference in translation is therefore necessary in order to produce understandable and cohesive texts. This justifies recent interest (Le Nagard &amp; Koehn, 2010; Hardmeier &amp; Federico, 2010) and motivates the work presented in this paper. 2.2 Pronominal Coreference in English Whilst English makes some use of case, it lacks the grammatical gender found in other languages. For monolingual speakers, the relatively few differen</context>
</contexts>
<marker>Ng, 2010</marker>
<rawString>Vincent Ng. 2010. Supervised Noun Phrase Coreference Research: The first 15 years. In Proceedings of the 48th Meeting of the ACL, pages 1396–1411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary A Marcinkiewicz</author>
<author>Ann Taylor</author>
</authors>
<date>1999</date>
<booktitle>Penn Treebank 3.0 LDC Calalog No.: LDC99T42. Linguistic Data Consortium.</booktitle>
<contexts>
<context position="4422" citStr="Marcus et al., 1999" startWordPosition="667" endWordPosition="671">l &amp; Brunstein, 2005) was used to provide coreference information in the sourcelanguage and exclude non-referential pronouns. It also formed the source-language side of the parallel training corpus. The PCEDT 2.0 corpus (Hajiˇc et al., 2011), which contains a close Czech translation of the Penn Wall Street Journal corpus, provided reference translations for testing and the target-language side of the parallel corpus for training. To minimise (although not completely eliminate) errors associated with antecedent head identification (item 3 above), the parse trees in the Penn Treebank 3.0 corpus (Marcus et al., 1999) were used. The gold standard annotation provided by these corpora allowed me to assume perfect identification of coreferring pronouns and coreference resolution and near–perfect antecedent head noun identification. These assumptions could not be made if state-ofthe-art methods had been used as they cannot yet achieve sufficiently high levels of accuracy. The remainder of the paper is structured as follows. The use of pronominal coreference in English and Czech and the problem of anaphora resolution are described in Section 2. The works of Le Nagard &amp; Koehn (2010) and Hardmeier &amp; Federico (201</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, Taylor, 1999</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, Mary A. Marcinkiewicz and Ann Taylor. 1999. Penn Treebank 3.0 LDC Calalog No.: LDC99T42. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
<author>Sung-Kwon Choi</author>
<author>Randall Sharp</author>
</authors>
<title>Anaphora Resolution in Machine Translation.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>5--7</pages>
<location>Leuven, Belgium,</location>
<contexts>
<context position="9913" citStr="Mitkov et al. (1995)" startWordPosition="1563" endWordPosition="1566"> English: I admired my (own) castle Czech: Obdivoval jsme sv˚uj hrad Here “sv´eho” is used to refer to a dog (masculine animate, singular) and “sv˚uj” to refer to a castle (masculine inanimate, singular), both of which belong to the speaker. Because a pronoun may take a large number of morphological forms in Czech and because case is not checked in annotation projection, the method presented here for translating coreferring pronouns does not guarantee their correct form. 3 Related Work Early work on integrating anaphora resolution with Machine Translation includes the rule-based approaches of Mitkov et al. (1995) and Lappin &amp; Leass (1994) and the transfer-based approach of Saggion &amp; Carvalho (1994). Work in the 1990’s culminated in the publication of a special issue of Machine Translation on anaphora resolution (Mitkov, 1999). Work then appears to have been on hold until papers were published by Le Nagard &amp; Koehn (2010) and Hardmeier &amp; Federico (2010). This resurgence of interest follows advances since the 1990’s which have made new approaches possible. The work described in this paper resembles that of Le Nagard &amp; Koehn (2010), with two main differences. The first is the use of manually annotated cor</context>
<context position="20057" citStr="Mitkov et al., 1995" startWordPosition="3202" endWordPosition="3205">s assumption was proved to be reasonable. These differences were due to the use of different phrase tables for each system as a result of training on different data (i.e. the annotation of English pronouns or lack thereof). This would not be an issue for single-step translation systems such as that used by Hardmeier &amp; Federico (2010). 4.4 Evaluation No standard method yet exists for evaluating pronoun translation in SMT. Early work focussed on the development of techniques for anaphora resolution and their integration within Machine Translation (Lappin &amp; Leass, 1994; Saggion &amp; Carvalho, 1994; Mitkov et al., 1995), with little mention of evaluation. In recent work, evaluation has become much more important. Both Le Nagard &amp; Koehn (2010) and Hardmeier &amp; Federico (2010) consider and reject BLEU (Papineni et al., 2002) as ill-suited for evaluating pronoun translation. While Hardmeier &amp; Federico propose and 5 use a strict recall and precision based metric for English–German translation, I found it unsuitable for English–Czech translation, given the highly inflective nature of Czech. Given the importance of evaluation to the goal of assessing the effectiveness of annotation projection for improving the tran</context>
</contexts>
<marker>Mitkov, Choi, Sharp, 1995</marker>
<rawString>Ruslan Mitkov, Sung-Kwon Choi and Randall Sharp. 1995. Anaphora Resolution in Machine Translation. In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation, July 5-7, Leuven, Belgium, pages 5–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Introduction: Special Issue on Anaphora Resolution</title>
<date>1999</date>
<booktitle>in Machine Translation and Multilingual NLP. Machine Translation,</booktitle>
<pages>14--159</pages>
<contexts>
<context position="10130" citStr="Mitkov, 1999" startWordPosition="1599" endWordPosition="1600">g to the speaker. Because a pronoun may take a large number of morphological forms in Czech and because case is not checked in annotation projection, the method presented here for translating coreferring pronouns does not guarantee their correct form. 3 Related Work Early work on integrating anaphora resolution with Machine Translation includes the rule-based approaches of Mitkov et al. (1995) and Lappin &amp; Leass (1994) and the transfer-based approach of Saggion &amp; Carvalho (1994). Work in the 1990’s culminated in the publication of a special issue of Machine Translation on anaphora resolution (Mitkov, 1999). Work then appears to have been on hold until papers were published by Le Nagard &amp; Koehn (2010) and Hardmeier &amp; Federico (2010). This resurgence of interest follows advances since the 1990’s which have made new approaches possible. The work described in this paper resembles that of Le Nagard &amp; Koehn (2010), with two main differences. The first is the use of manually annotated corpora to extract coreference information and morphological properties of the target translations of the antecedents. The second lies in the choice of language pair. They consider EnglishFrench translation, focussing on</context>
</contexts>
<marker>Mitkov, 1999</marker>
<rawString>Ruslan Mitkov. 1999. Introduction: Special Issue on Anaphora Resolution in Machine Translation and Multilingual NLP. Machine Translation, 14:159– 161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="16600" citStr="Och &amp; Ney, 2003" startWordPosition="2626" endWordPosition="2629">14 11,907 Table 1: Sizes of the training and testing datasets 4.2 Baseline and Annotated systems Both systems are phrase-based SMT models, trained using the Moses toolkit (Hoang et al., 2007). They share the same 3-gram language model constructed from the target-side text of the parallel training corpus and the Czech monolingual 2010 and 2011 News Crawl corpora1. The language model was constructed using the SRILM toolkit (Stolcke, 2002) with interpolated Kneser-Ney discounting (Kneser &amp; Ney, 1995). In addition, both systems are forced to use the same word alignments (constructed using Giza++ (Och &amp; Ney, 2003) in both language pair directions and using stemmed training data in which words are limited to the first four characters) in order to mitigate the effects of Czech word inflection on word alignment statistics. This helps to ensure that the Czech translation of the head of the antecedent remains constant in both steps of the two-step process. If this were to change it would defeat the purpose of pronoun annotation as different Czech translations could result in different gender and/or number. The Baseline system was trained using the Penn Wall Street Journal corpus with no annotation, while th</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="20263" citStr="Papineni et al., 2002" startWordPosition="3237" endWordPosition="3240">s or lack thereof). This would not be an issue for single-step translation systems such as that used by Hardmeier &amp; Federico (2010). 4.4 Evaluation No standard method yet exists for evaluating pronoun translation in SMT. Early work focussed on the development of techniques for anaphora resolution and their integration within Machine Translation (Lappin &amp; Leass, 1994; Saggion &amp; Carvalho, 1994; Mitkov et al., 1995), with little mention of evaluation. In recent work, evaluation has become much more important. Both Le Nagard &amp; Koehn (2010) and Hardmeier &amp; Federico (2010) consider and reject BLEU (Papineni et al., 2002) as ill-suited for evaluating pronoun translation. While Hardmeier &amp; Federico propose and 5 use a strict recall and precision based metric for English–German translation, I found it unsuitable for English–Czech translation, given the highly inflective nature of Czech. Given the importance of evaluation to the goal of assessing the effectiveness of annotation projection for improving the translation of coreferring pronouns, I carried out two separate types of evaluation — an automated evaluation which could be applied to the entire test set, and an indepth manual assessment that might provide m</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the ACL, pages 311– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Saggion</author>
<author>Ariadne Carvalho</author>
</authors>
<title>Anaphora Resolution in a Machine Translation System.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Machine Translation: Ten Years On, November,</booktitle>
<pages>4--1</pages>
<location>Cranfield, UK,</location>
<contexts>
<context position="10000" citStr="Saggion &amp; Carvalho (1994)" startWordPosition="1577" endWordPosition="1580"> is used to refer to a dog (masculine animate, singular) and “sv˚uj” to refer to a castle (masculine inanimate, singular), both of which belong to the speaker. Because a pronoun may take a large number of morphological forms in Czech and because case is not checked in annotation projection, the method presented here for translating coreferring pronouns does not guarantee their correct form. 3 Related Work Early work on integrating anaphora resolution with Machine Translation includes the rule-based approaches of Mitkov et al. (1995) and Lappin &amp; Leass (1994) and the transfer-based approach of Saggion &amp; Carvalho (1994). Work in the 1990’s culminated in the publication of a special issue of Machine Translation on anaphora resolution (Mitkov, 1999). Work then appears to have been on hold until papers were published by Le Nagard &amp; Koehn (2010) and Hardmeier &amp; Federico (2010). This resurgence of interest follows advances since the 1990’s which have made new approaches possible. The work described in this paper resembles that of Le Nagard &amp; Koehn (2010), with two main differences. The first is the use of manually annotated corpora to extract coreference information and morphological properties of the target tran</context>
<context position="20035" citStr="Saggion &amp; Carvalho, 1994" startWordPosition="3197" endWordPosition="3201">out of 458 instances), this assumption was proved to be reasonable. These differences were due to the use of different phrase tables for each system as a result of training on different data (i.e. the annotation of English pronouns or lack thereof). This would not be an issue for single-step translation systems such as that used by Hardmeier &amp; Federico (2010). 4.4 Evaluation No standard method yet exists for evaluating pronoun translation in SMT. Early work focussed on the development of techniques for anaphora resolution and their integration within Machine Translation (Lappin &amp; Leass, 1994; Saggion &amp; Carvalho, 1994; Mitkov et al., 1995), with little mention of evaluation. In recent work, evaluation has become much more important. Both Le Nagard &amp; Koehn (2010) and Hardmeier &amp; Federico (2010) consider and reject BLEU (Papineni et al., 2002) as ill-suited for evaluating pronoun translation. While Hardmeier &amp; Federico propose and 5 use a strict recall and precision based metric for English–German translation, I found it unsuitable for English–Czech translation, given the highly inflective nature of Czech. Given the importance of evaluation to the goal of assessing the effectiveness of annotation projection </context>
</contexts>
<marker>Saggion, Carvalho, 1994</marker>
<rawString>Horacio Saggion and Ariadne Carvalho. 1994. Anaphora Resolution in a Machine Translation System. In Proceedings of the International Conference on Machine Translation: Ten Years On, November, Cranfield, UK, 4.1-4.14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM — An Extensible Language Modeling Toolkit. In</title>
<date>2002</date>
<booktitle>Proceedings of International Conference on Spoken Language Processing,</booktitle>
<pages>2--901</pages>
<location>Denver, USA,</location>
<contexts>
<context position="16424" citStr="Stolcke, 2002" startWordPosition="2601" endWordPosition="2602">ion system, the output of which is the final translation. 4 Training Dev. Final Parallel Sentences 47,549 280 540 Czech Words 955,018 5,467 10,110 English Words 1,024,438 6,114 11,907 Table 1: Sizes of the training and testing datasets 4.2 Baseline and Annotated systems Both systems are phrase-based SMT models, trained using the Moses toolkit (Hoang et al., 2007). They share the same 3-gram language model constructed from the target-side text of the parallel training corpus and the Czech monolingual 2010 and 2011 News Crawl corpora1. The language model was constructed using the SRILM toolkit (Stolcke, 2002) with interpolated Kneser-Ney discounting (Kneser &amp; Ney, 1995). In addition, both systems are forced to use the same word alignments (constructed using Giza++ (Och &amp; Ney, 2003) in both language pair directions and using stemmed training data in which words are limited to the first four characters) in order to mitigate the effects of Czech word inflection on word alignment statistics. This helps to ensure that the Czech translation of the head of the antecedent remains constant in both steps of the two-step process. If this were to change it would defeat the purpose of pronoun annotation as dif</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM — An Extensible Language Modeling Toolkit. In Proceedings of International Conference on Spoken Language Processing, September 16-20, Denver, USA, 2:901– 904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
</authors>
<title>Corpus-based and Machine Learning Approaches to Anaphora Resolution. Anaphors in Text: Cognitive, Formal and Applied Approaches to Anaphoric Reference, John Benjamins Pub Co.</title>
<date>2007</date>
<contexts>
<context position="5779" citStr="Strube (2007)" startWordPosition="885" endWordPosition="886">cussed in Section 5 and future work is outlined in Section 6. 2 Background 2.1 Anaphora Resolution Anaphora resolution involves identifying the antecedent of a referring expression, typically a pronoun or noun phrase that is used to refer to something previously mentioned in the discourse (its antecedent). Where multiple referring expressions refer to the same antecedent, they are said to be coreferential. Anaphora resolution and the related task of coreference resolution have been the subject of considerable research within Natural Language Processing (NLP). Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader’s ability to understand a text. Accurate coreference in translation is therefore necessary in order to produce understandable and cohesive texts. This justifies recent interest (Le Nagard &amp; Koehn, 2010; Hardmeier &amp; Federico, 2010) and motivates the work presented in this paper. 2.2 Pronominal Coreference in English Whilst English makes some use of case, it lacks the grammatical gender found in other languages. For monolingual speakers, the relativel</context>
</contexts>
<marker>Strube, 2007</marker>
<rawString>Michael Strube. 2007. Corpus-based and Machine Learning Approaches to Anaphora Resolution. Anaphors in Text: Cognitive, Formal and Applied Approaches to Anaphoric Reference, John Benjamins Pub Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Ada Brunstein</author>
</authors>
<date>2005</date>
<booktitle>BBN Pronoun Coreference and Entity Type Corpus LDC Calalog No.: LDC2005T33. Linguistic Data Consortium.</booktitle>
<contexts>
<context position="3822" citStr="Weischedel &amp; Brunstein, 2005" startWordPosition="575" endWordPosition="578">ng the source and target texts at the phrase and word levels. 1 Proceedings of the EACL 2012 Student Research Workshop, pages 1–10, Avignon, France, 26 April 2012. c�2012 Association for Computational Linguistics Factoring out the first two decisions would show whether the lack of significant improvement was simply due to imperfect coreference resolution. In order to control for these errors several different manually annotated versions of the Penn Wall Street Journal corpus were used, each providing different annotations over the same text. The BBN Pronoun Coreference and Entity Type corpus (Weischedel &amp; Brunstein, 2005) was used to provide coreference information in the sourcelanguage and exclude non-referential pronouns. It also formed the source-language side of the parallel training corpus. The PCEDT 2.0 corpus (Hajiˇc et al., 2011), which contains a close Czech translation of the Penn Wall Street Journal corpus, provided reference translations for testing and the target-language side of the parallel corpus for training. To minimise (although not completely eliminate) errors associated with antecedent head identification (item 3 above), the parse trees in the Penn Treebank 3.0 corpus (Marcus et al., 1999)</context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>Ralph Weischedel and Ada Brunstein. 2005. BBN Pronoun Coreference and Entity Type Corpus LDC Calalog No.: LDC2005T33. Linguistic Data Consortium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>