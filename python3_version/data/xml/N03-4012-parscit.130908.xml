<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.064119">
<note confidence="0.964258333333333">
Proceedings of HLT-NAACL 2003
Demonstrations , pp. 23-24
Edmonton, May-June 2003
</note>
<title confidence="0.98492">
Automatic Extraction of Semantic Networks from Text using Leximancer
</title>
<author confidence="0.995063">
Andrew E. Smith.
</author>
<affiliation confidence="0.921967333333333">
Key Centre for Human Factors and Applied Cognitive Psychology,
The University of Queensland,
Queensland, Australia, 4072.
</affiliation>
<email confidence="0.996678">
asmith@humanfactors.uq.edu.au
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999833428571429">
Leximancer is a software system for perform-
ing conceptual analysis of text data in a largely
language independent manner. The system is
modelled on Content Analysis and provides
unsupervised and supervised analysis using
seeded concept classifiers. Unsupervised on-
tology discovery is a key component.
</bodyText>
<sectionHeader confidence="0.924071" genericHeader="keywords">
1 Method
</sectionHeader>
<bodyText confidence="0.99908165">
The strategy used for conceptual mapping of text in-
volves abstracting families of words to thesaurus con-
cepts. These concepts are then used to classify text at
a resolution of several sentences. The resulting concept
tags are indexed to provide a document exploration en-
vironment for the user. A smaller number of simple
concepts can index many more complex relationships
by recording co-occurrences, and complex systems ap-
proaches can be applied to these systems of agents.
To achieve this, several novel algorithms were de-
veloped: a learning optimiser for automatically select-
ing, learning, and adapting a concept from the word us-
age within the text, and an asymmetric scaling process
for generating a cluster map of concepts based on co-
occurrence in the text.
Extensive evaluation has been performed on real doc-
ument collections in collaboration with domain experts.
The method adopted has been to perform parallel analy-
ses with these experts and compare the results.
An outline of the algorithms (Smith, 2000) follows:
</bodyText>
<listItem confidence="0.9807446">
1. Text preparation: Standard techniques are em-
ployed, including name and term preservation, to-
kenisation, and the application of a stop-list.
2. Unsupervised and supervised ontology discovery:
Concepts can be seeded by a domain expert to suit
</listItem>
<bodyText confidence="0.9956392">
user requirements, or they can be chosen automat-
ically using a ranking algorithm for finding seed
words which reflect the themes present in the data.
This process looks for words near the centre of local
maxima in the lexical co-occurrence network.
</bodyText>
<listItem confidence="0.958982909090909">
3. Filling the thesaurus: A machine learning algorithm
is used to find the relevant thesaurus words from the
text data. This iterative optimiser, derived from a
word disambiguation technique (Yarowsky, 1995),
finds the nearest local maximum in the lexical co-
occurrence network from each concept seed. Early
results show that this lexical network can be reduced
to a Scale-free and Small-world network1.
4. Classification: Text is tagged with multiple concepts
using the thesaurus, to a sentence resolution.
5. Mapping: The concepts and their relative co-
occurrence frequencies now form a semantic net-
work. This is scaled using an asymmetric scaling
algorithm, and made into a lattice by ranking con-
cepts by their connectedness, or centrality.
6. User interface: A browser is used for exploring the
classification system in depth. The semantic lat-
tice browser enables semantic characterisation of the
data and discovery of indirect association. Con-
cept co-occurrence spectra and themed text segment
browsing are also provided.
2 Analysis of the PNAS Data Set
</listItem>
<bodyText confidence="0.999976">
The data set presented here consisted of text and meta-
data from Proceedings of the National Academy of Sci-
ence, 1997 to 2002. These examples are extracted from
the abstract data. Firstly, Leximancer was configured to
map the document set in unsupervised mode. A screen
image of this interactive map is shown in figure 1. This
</bodyText>
<footnote confidence="0.65929">
1Following (Steyvers and Tenenbaum, 2003).
</footnote>
<bodyText confidence="0.7136385">
shows the semantic lattice (left), with the co-occurrence
links from the concept ‘brain’ highlighted (left and right).
</bodyText>
<figureCaption confidence="0.978487777777778">
Figure 1: Unsupervised map of PNAS abstracts.
Figure 2 shows the top of the thesaurus entry for the
concept ‘brain’. This concept was seeded with just the
word ‘brain’ and then the learning system found a larger
family of words and names which are strongly relevant
to ‘brain’ in the these abstracts. In the figure, terms in
square brackets are identified proper names, and numeri-
cal values are the relevancy weights.
Figure 2: Thesaurus entry for ‘brain’ (excerpt).
</figureCaption>
<bodyText confidence="0.99988">
It is also of interest to discover which concepts tend
to be unique to each year of the PNAS proceedings, and
so identify trends. This usually requires a different form
of analysis, since concepts which characterise the whole
data set may not be good for discriminating parts. By
placing the data for each year in a folder, Leximancer can
tag each text sentence with the relevant year, and place
each year as a prior concept on the map. The result-
ing map contains the prior concepts plus other concepts
which are relevant to at least one of the priors, and shows
trending from early years to later years (figure 3).
</bodyText>
<figureCaption confidence="0.98133">
Figure 3: Temporal map of PNAS abstracts.
</figureCaption>
<sectionHeader confidence="0.998186" genericHeader="conclusions">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.9996925">
The Leximancer system has demonstrated several major
strengths for text data analysis:
</bodyText>
<listItem confidence="0.98201375">
• Large amounts of text can be analysed rapidly in a
quantitative manner. Text is quickly re-classified us-
ing different ontologies when needs change.
• The unsupervised analysis generates concepts which
are well-defined — they have signifiers which com-
municate the meaning of each concept to the user.
• Machine Learning removes much of the need to re-
vise thesauri as the domain vocabulary evolves.
</listItem>
<sectionHeader confidence="0.998008" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9994736875">
Andrew E. Smith. 2000. Machine mapping of document
collections: the leximancer system. In Proceedings
of the Fifth Australasian Document Computing Sym-
posium, Sunshine Coast, Australia, December. DSTC.
http://www.leximancer.com/technology.html.
Mark Steyvers and Joshua B. Tenenbaum. 2003. The
large-scale structure of semantic networks: Statistical
analyses and a model of semantic growth. Submitted
to Cognitive Science. http://www-psych.stanford.edu/
˜msteyver.
David Yarowsky. 1995. Unsupervised word-sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics (ACL-95), pages 189–196,
Cambridge, MA. http://www.cs.jhu.edu/˜yarowsky/
pubs.html.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.032567">
<note confidence="0.984214666666667">Proceedings of HLT-NAACL 2003 Demonstrations , pp. 23-24 Edmonton, May-June 2003</note>
<title confidence="0.99827">Automatic Extraction of Semantic Networks from Text using Leximancer</title>
<author confidence="0.991484">E Andrew</author>
<affiliation confidence="0.7269855">Key Centre for Human Factors and Applied Cognitive The University of</affiliation>
<address confidence="0.709029">Queensland, Australia,</address>
<email confidence="0.998227">asmith@humanfactors.uq.edu.au</email>
<abstract confidence="0.953599786324787">Leximancer is a software system for performing conceptual analysis of text data in a largely language independent manner. The system is modelled on Content Analysis and provides unsupervised and supervised analysis using seeded concept classifiers. Unsupervised ontology discovery is a key component. 1 Method The strategy used for conceptual mapping of text involves abstracting families of words to thesaurus concepts. These concepts are then used to classify text at a resolution of several sentences. The resulting concept tags are indexed to provide a document exploration environment for the user. A smaller number of simple concepts can index many more complex relationships by recording co-occurrences, and complex systems approaches can be applied to these systems of agents. To achieve this, several novel algorithms were developed: a learning optimiser for automatically selecting, learning, and adapting a concept from the word usage within the text, and an asymmetric scaling process for generating a cluster map of concepts based on cooccurrence in the text. Extensive evaluation has been performed on real document collections in collaboration with domain experts. The method adopted has been to perform parallel analyses with these experts and compare the results. An outline of the algorithms (Smith, 2000) follows: 1. Text preparation: Standard techniques are employed, including name and term preservation, tokenisation, and the application of a stop-list. 2. Unsupervised and supervised ontology discovery: Concepts can be seeded by a domain expert to suit user requirements, or they can be chosen automatically using a ranking algorithm for finding seed words which reflect the themes present in the data. This process looks for words near the centre of local maxima in the lexical co-occurrence network. 3. Filling the thesaurus: A machine learning algorithm is used to find the relevant thesaurus words from the text data. This iterative optimiser, derived from a word disambiguation technique (Yarowsky, 1995), finds the nearest local maximum in the lexical cooccurrence network from each concept seed. Early results show that this lexical network can be reduced a Scale-free and Small-world 4. Classification: Text is tagged with multiple concepts using the thesaurus, to a sentence resolution. 5. Mapping: The concepts and their relative cooccurrence frequencies now form a semantic network. This is scaled using an asymmetric scaling algorithm, and made into a lattice by ranking concepts by their connectedness, or centrality. 6. User interface: A browser is used for exploring the classification system in depth. The semantic lattice browser enables semantic characterisation of the data and discovery of indirect association. Concept co-occurrence spectra and themed text segment browsing are also provided. 2 Analysis of the PNAS Data Set The data set presented here consisted of text and metadata from Proceedings of the National Academy of Science, 1997 to 2002. These examples are extracted from the abstract data. Firstly, Leximancer was configured to map the document set in unsupervised mode. A screen image of this interactive map is shown in figure 1. This (Steyvers and Tenenbaum, 2003). shows the semantic lattice (left), with the co-occurrence links from the concept ‘brain’ highlighted (left and right). Figure 1: Unsupervised map of PNAS abstracts. Figure 2 shows the top of the thesaurus entry for the concept ‘brain’. This concept was seeded with just the word ‘brain’ and then the learning system found a larger family of words and names which are strongly relevant to ‘brain’ in the these abstracts. In the figure, terms in square brackets are identified proper names, and numerical values are the relevancy weights. Figure 2: Thesaurus entry for ‘brain’ (excerpt). It is also of interest to discover which concepts tend to be unique to each year of the PNAS proceedings, and so identify trends. This usually requires a different form of analysis, since concepts which characterise the whole data set may not be good for discriminating parts. By placing the data for each year in a folder, Leximancer can tag each text sentence with the relevant year, and place year as a on the map. The resulting map contains the prior concepts plus other concepts which are relevant to at least one of the priors, and shows trending from early years to later years (figure 3). Figure 3: Temporal map of PNAS abstracts. 3 Conclusion The Leximancer system has demonstrated several major strengths for text data analysis: • Large amounts of text can be analysed rapidly in a quantitative manner. Text is quickly re-classified using different ontologies when needs change. • The unsupervised analysis generates concepts which are well-defined — they have signifiers which communicate the meaning of each concept to the user. • Machine Learning removes much of the need to revise thesauri as the domain vocabulary evolves. References Andrew E. Smith. 2000. Machine mapping of document the leximancer system. In of the Fifth Australasian Document Computing Sym- Sunshine Coast, Australia, December. DSTC. http://www.leximancer.com/technology.html. Mark Steyvers and Joshua B. Tenenbaum. 2003. The large-scale structure of semantic networks: Statistical and a model of semantic growth. Cognitive http://www-psych.stanford.edu/ David Yarowsky. 1995. Unsupervised word-sense disrivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Linguistics pages 189–196, MA.</abstract>
<intro confidence="0.384174">pubs.html.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrew E Smith</author>
</authors>
<title>Machine mapping of document collections: the leximancer system.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fifth Australasian Document Computing Symposium,</booktitle>
<location>Sunshine Coast, Australia,</location>
<note>DSTC. http://www.leximancer.com/technology.html.</note>
<contexts>
<context position="1651" citStr="Smith, 2000" startWordPosition="248" endWordPosition="249">and complex systems approaches can be applied to these systems of agents. To achieve this, several novel algorithms were developed: a learning optimiser for automatically selecting, learning, and adapting a concept from the word usage within the text, and an asymmetric scaling process for generating a cluster map of concepts based on cooccurrence in the text. Extensive evaluation has been performed on real document collections in collaboration with domain experts. The method adopted has been to perform parallel analyses with these experts and compare the results. An outline of the algorithms (Smith, 2000) follows: 1. Text preparation: Standard techniques are employed, including name and term preservation, tokenisation, and the application of a stop-list. 2. Unsupervised and supervised ontology discovery: Concepts can be seeded by a domain expert to suit user requirements, or they can be chosen automatically using a ranking algorithm for finding seed words which reflect the themes present in the data. This process looks for words near the centre of local maxima in the lexical co-occurrence network. 3. Filling the thesaurus: A machine learning algorithm is used to find the relevant thesaurus wor</context>
</contexts>
<marker>Smith, 2000</marker>
<rawString>Andrew E. Smith. 2000. Machine mapping of document collections: the leximancer system. In Proceedings of the Fifth Australasian Document Computing Symposium, Sunshine Coast, Australia, December. DSTC. http://www.leximancer.com/technology.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth. Submitted to Cognitive Science.</title>
<date>2003</date>
<note>http://www-psych.stanford.edu/ ˜msteyver.</note>
<contexts>
<context position="3578" citStr="Steyvers and Tenenbaum, 2003" startWordPosition="550" endWordPosition="553">e classification system in depth. The semantic lattice browser enables semantic characterisation of the data and discovery of indirect association. Concept co-occurrence spectra and themed text segment browsing are also provided. 2 Analysis of the PNAS Data Set The data set presented here consisted of text and metadata from Proceedings of the National Academy of Science, 1997 to 2002. These examples are extracted from the abstract data. Firstly, Leximancer was configured to map the document set in unsupervised mode. A screen image of this interactive map is shown in figure 1. This 1Following (Steyvers and Tenenbaum, 2003). shows the semantic lattice (left), with the co-occurrence links from the concept ‘brain’ highlighted (left and right). Figure 1: Unsupervised map of PNAS abstracts. Figure 2 shows the top of the thesaurus entry for the concept ‘brain’. This concept was seeded with just the word ‘brain’ and then the learning system found a larger family of words and names which are strongly relevant to ‘brain’ in the these abstracts. In the figure, terms in square brackets are identified proper names, and numerical values are the relevancy weights. Figure 2: Thesaurus entry for ‘brain’ (excerpt). It is also o</context>
</contexts>
<marker>Steyvers, Tenenbaum, 2003</marker>
<rawString>Mark Steyvers and Joshua B. Tenenbaum. 2003. The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth. Submitted to Cognitive Science. http://www-psych.stanford.edu/ ˜msteyver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word-sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL-95),</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA.</location>
<note>http://www.cs.jhu.edu/˜yarowsky/ pubs.html.</note>
<contexts>
<context position="2361" citStr="Yarowsky, 1995" startWordPosition="358" endWordPosition="359">vation, tokenisation, and the application of a stop-list. 2. Unsupervised and supervised ontology discovery: Concepts can be seeded by a domain expert to suit user requirements, or they can be chosen automatically using a ranking algorithm for finding seed words which reflect the themes present in the data. This process looks for words near the centre of local maxima in the lexical co-occurrence network. 3. Filling the thesaurus: A machine learning algorithm is used to find the relevant thesaurus words from the text data. This iterative optimiser, derived from a word disambiguation technique (Yarowsky, 1995), finds the nearest local maximum in the lexical cooccurrence network from each concept seed. Early results show that this lexical network can be reduced to a Scale-free and Small-world network1. 4. Classification: Text is tagged with multiple concepts using the thesaurus, to a sentence resolution. 5. Mapping: The concepts and their relative cooccurrence frequencies now form a semantic network. This is scaled using an asymmetric scaling algorithm, and made into a lattice by ranking concepts by their connectedness, or centrality. 6. User interface: A browser is used for exploring the classifica</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word-sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL-95), pages 189–196, Cambridge, MA. http://www.cs.jhu.edu/˜yarowsky/ pubs.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>