<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000039">
<title confidence="0.972297">
Large Scale Acquisition of Paraphrases for Learning Surface Patterns
</title>
<author confidence="0.968043">
Rahul Bhagat*
</author>
<affiliation confidence="0.9830705">
Information Sciences Institute
University of Southern California
</affiliation>
<address confidence="0.648092">
Marina del Rey, CA
</address>
<email confidence="0.999285">
rahul@isi.edu
</email>
<author confidence="0.871747">
Deepak Ravichandran
</author>
<affiliation confidence="0.858814">
Google Inc.
</affiliation>
<address confidence="0.7005945">
1600 Amphitheatre Parkway
Mountain View, CA
</address>
<email confidence="0.998866">
deepakr@google.com
</email>
<sectionHeader confidence="0.998453" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.985421882352941">
Paraphrases have proved to be useful in many
applications, including Machine Translation,
Question Answering, Summarization, and In-
formation Retrieval. Paraphrase acquisition
methods that use a single monolingual corpus
often produce only syntactic paraphrases. We
present a method for obtaining surface para-
phrases, using a 150GB (25 billion words)
monolingual corpus. Our method achieves an
accuracy of around 70% on the paraphrase ac-
quisition task. We further show that we can
use these paraphrases to generate surface pat-
terns for relation extraction. Our patterns are
much more precise than those obtained by us-
ing a state of the art baseline and can extract
relations with more than 80% precision for
each of the test relations.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99742185106383">
Paraphrases are textual expressions that convey the
same meaning using different surface words. For ex-
ample consider the following sentences:
Google acquired YouTube. (1)
Google completed the acquisition of YouTube. (2)
Since they convey the same meaning, sentences
(1) and (2) are sentence level paraphrases, and the
phrases “acquired” and “completed the acquisition
of” in (1) and (2) respectively are phrasal para-
phrases.
Paraphrases provide a way to capture the vari-
ability of language and hence play an important
*Work done during an internship at Google Inc.
role in many natural language processing (NLP) ap-
plications. For example, in question answering,
paraphrases have been used to find multiple pat-
terns that pinpoint the same answer (Ravichandran
and Hovy, 2002); in statistical machine transla-
tion, they have been used to find translations for
unseen source language phrases (Callison-Burch et
al., 2006); in multi-document summarization, they
have been used to identify phrases from different
sentences that express the same information (Barzi-
lay et al., 1999); in information retrieval they have
been used for query expansion (Anick and Tipirneni,
1999).
Learning paraphrases requires one to ensure iden-
tity of meaning. Since there are no adequate se-
mantic interpretation systems available today, para-
phrase acquisition techniques use some other mech-
anism as a kind of “pivot” to (help) ensure semantic
identity. Each pivot mechanism selects phrases with
similar meaning in a different characteristic way. A
popular method, the so-called distributional simi-
larity, is based on the dictum of Zelig Harris “you
shall know the words by the company they keep”:
given highly discriminating left and right contexts,
only words with very similar meaning will be found
to fit in between them. For paraphrasing, this has
been often used to find syntactic transformations in
parse trees that preserve (semantic) meaning. An-
other method is to use a bilingual dictionary or trans-
lation table as pivot mechanism: all source language
words or phrases that translate to a given foreign
word/phrase are deemed to be paraphrases of one
another. In this paper we call the paraphrases that
contain only words as surface paraphrases and those
</bodyText>
<page confidence="0.985784">
674
</page>
<note confidence="0.713555">
Proceedings of ACL-08: HLT, pages 674–682,
</note>
<page confidence="0.537401">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.960990962962963">
that contain paths in a syntax tree as syntactic para-
phrases.
We here, present a method to acquire surface
paraphrases from a single monolingual corpus. We
use a large corpus (about 150GB) to overcome the
data sparseness problem. To overcome the scalabil-
ity problem, we pre-process the text with a simple
parts-of-speech (POS) tagger and then apply locality
sensitive hashing (LSH) (Charikar, 2002; Ravichan-
dran et al., 2005) to speed up the remaining compu-
tation for paraphrase acquisition. Our experiments
show results to verify the following main claim:
Claim 1: Highly precise surface paraphrases can be
obtained from a very large monolingual corpus.
With this result, we further show that these para-
phrases can be used to obtain high precision surface
patterns that enable the discovery of relations in a
minimally supervised way. Surface patterns are tem-
plates for extracting information from text. For ex-
ample, if one wanted to extract a list of company ac-
quisitions, “(ACQUIRER) acquired (ACQUIREE)”
would be one surface pattern with “(ACQUIRER)”
and “(ACQUIREE)” as the slots to be extracted.
Thus we can claim:
Claim 2: These paraphrases can then be used for
generating high precision surface patterns for rela-
tion extraction.
</bodyText>
<sectionHeader confidence="0.999299" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.968069879310345">
Most recent work in paraphrase acquisition is based
on automatic acquisition. Barzilay and McKeown
(2001) used a monolingual parallel corpus to obtain
paraphrases. Bannard and Callison-Burch (2005)
and Zhou et al. (2006) both employed a bilingual
parallel corpus in which each foreign language word
or phrase was a pivot to obtain source language para-
phrases. Dolan et al. (2004) and Barzilay and Lee
(2003) used comparable news articles to obtain sen-
tence level paraphrases. All these approaches rely
on the presence of parallel or comparable corpora
and are thus limited by their availability and size.
Lin and Pantel (2001) and Szpektor et al. (2004)
proposed methods to obtain entailment templates by
using a single monolingual resource. While both dif-
fer in their approaches, they both end up finding syn-
tactic paraphrases. Their methods cannot be used if
we cannot parse the data (either because of scale or
data quality). Our approach on the other hand, finds
surface paraphrases; it is more scalable and robust
due to the use of simple POS tagging. Also, our use
of locality sensitive hashing makes finding similar
phrases in a large corpus feasible.
Another task related to our work is relation extrac-
tion. Its aim is to extract instances of a given rela-
tion. Hearst (1992) the pioneering paper in the field
used a small number of hand selected patterns to ex-
tract instances of hyponymy relation. Berland and
Charniak (1999) used a similar method for extract-
ing instances of meronymy relation. Ravichandran
and Hovy (2002) used seed instances of a relation
to automatically obtain surface patterns by querying
the web. But their method often finds patterns that
are too general (e.g., X and Y), resulting in low pre-
cision extractions. Rosenfeld and Feldman (2006)
present a somewhat similar web based method that
uses a combination of seed instances and seed pat-
terns to learn good quality surface patterns. Both
these methods differ from ours in that they learn
relation patterns on the fly (from the web). Our
method however, pre-computes paraphrases for a
large set of surface patterns using distributional sim-
ilarity over a large corpus and then obtains patterns
for a relation by simply finding paraphrases (offline)
for a few seed patterns. Using distributional simi-
larity avoids the problem of obtaining overly gen-
eral patterns and the pre-computation of paraphrases
means that we can obtain the set of patterns for any
relation instantaneously.
Romano et al. (2006) and Sekine (2006) used syn-
tactic paraphrases to obtain patterns for extracting
relations. While procedurally different, both meth-
ods depend heavily on the performance of the syntax
parser and require complex syntax tree matching to
extract the relation instances. Our method on the
other hand acquires surface patterns and thus avoids
the dependence on a parser and syntactic matching.
This also makes the extraction process scalable.
</bodyText>
<sectionHeader confidence="0.973379" genericHeader="method">
3 Acquiring Paraphrases
</sectionHeader>
<bodyText confidence="0.996588">
This section describes our model for acquiring para-
phrases from text.
</bodyText>
<page confidence="0.99836">
675
</page>
<subsectionHeader confidence="0.997396">
3.1 Distributional Similarity
</subsectionHeader>
<bodyText confidence="0.99989515">
Harris’s distributional hypothesis (Harris, 1954) has
played an important role in lexical semantics. It
states that words that appear in similar contexts tend
to have similar meanings. In this paper, we apply
the distributional hypothesis to phrases i.e. word n-
grams.
For example, consider the phrase “acquired” of
the form “X acquired Y ”. Considering the con-
text of this phrase, we might find {Google, eBay,
Yahoo,...} in position X and {YouTube, Skype,
Overture,...} in position Y . Now consider another
phrase “completed the acquisition of”, again of the
form “X completed the acquisition of Y ”. For this
phrase, we might find {Google, eBay, Hilton Hotel
corp.,...} in position X and {YouTube, Skype, Bally
Entertainment Corp.,...} in position Y . Since the
contexts of the two phrases are similar, our exten-
sion of the distributional hypothesis would assume
that “acquired” and “completed the acquisition of”
have similar meanings.
</bodyText>
<subsectionHeader confidence="0.999176">
3.2 Paraphrase Learning Model
</subsectionHeader>
<bodyText confidence="0.9999634">
Let p be a phrase (n-gram) of the form X p Y ,
where X and Y are the placeholders for words oc-
curring on either side of p. Our first task is to
find the set of phrases that are similar in meaning
top. Let P = {p1, p2, p3, ..., pl} be the set of all
phrases of the form X pi Y where pi E P. Let
5i,X be the set of words that occur in position X of
pi and 5i,Y be the set of words that occur in posi-
tion Y of pi. Let Vi be the vector representing pi
such that Vi = 5i,X U 5i,Y . Each word f E Vi
has an associated score that measures the strength
of the association of the word f with phrase pi; as
do many others, we employ pointwise mutual infor-
mation (Cover and Thomas, 1991) to measure this
strength of association.
</bodyText>
<equation confidence="0.998184">
pmi(pi; f) =log P(pi,f� (1)
P(pi�P (f�
</equation>
<bodyText confidence="0.999793916666667">
The probabilities in equation (1) are calculated by
using the maximum likelihood estimate over our
corpus.
Once we have the vectors for each phrase pi E P,
we can find the paraphrases for each pi by finding its
nearest neighbors. We use cosine similarity, which
is a commonly used measure for finding similarity
between two vectors.
If we have two phrases pi E P and pj E P with
the corresponding vectors Vi and Vj constructed
as described above, the similarity between the two
phrases is calculated as:
</bodyText>
<equation confidence="0.9805345">
sim(pi;pj) = Vi.Vj (2)
|Vi|*|Vj|
</equation>
<bodyText confidence="0.9984884">
Each word in Vi (and Vj) has with it an associated
flag which indicates weather the word came from
5i,X or 5i,Y . Hence for each phrase pi of the form
X pi Y , we have a corresponding phrase −pi that
has the form Y pi X. This is important to find cer-
tain kinds of paraphrases. The following example
will illustrate. Consider the sentences:
Google acquired YouTube. (3)
YouTube was bought by Google. (4)
From sentence (3), we obtain two phrases:
</bodyText>
<listItem confidence="0.954229909090909">
1. pi = acquired which has the form “X acquired Y ”
where “X = Google” and “Y = YouTube”
2. −pi = −acquired which has the form “Y acquired
X” where “X = YouTube” and “Y = Google”
Similarly, from sentence (4) we obtain two phrases:
1. pj = was bought by which has the form “X was
bought by Y ” where “X = YouTube” and “Y =
Google”
2. −pj = −was bought by which has the form “Y
was bought by X” where “X = Google” and “Y
= YouTube”
</listItem>
<bodyText confidence="0.999818333333333">
The switching of X and Y positions in (3) and (4)
ensures that “acquired” and “−was bought by” are
found to be paraphrases by the algorithm.
</bodyText>
<subsectionHeader confidence="0.999729">
3.3 Locality Sensitive Hashing
</subsectionHeader>
<bodyText confidence="0.999958454545455">
As described in Section 3.2, we find paraphrases of
a phrase pi by finding its nearest neighbors based
on cosine similarity between the feature vector of
pi and other phrases. To do this for all the phrases
in the corpus, we’ll have to compute the similarity
between all vector pairs. If n is the number of vec-
tors and d is the dimensionality of the vector space,
finding cosine similarity between each pair of vec-
tors has time complexity O(n2d). This computation
is infeasible for our corpus, since both n and d are
large.
</bodyText>
<page confidence="0.996102">
676
</page>
<bodyText confidence="0.9999575">
To solve this problem, we make use of Local-
ity Sensitive Hashing (LSH). The basic idea behind
LSH is that a LSH function creates a fingerprint
for each vector such that if two vectors are simi-
lar, they are likely to have similar fingerprints. The
LSH function we use here was proposed by Charikar
(2002). It represents a d dimensional vector by a
stream of b bits (b « d) and has the property of pre-
serving the cosine similarity between vectors, which
is exactly what we want. Ravichandran et al. (2005)
have shown that by using the LSH nearest neighbors
calculation can be done in O(nd) time.1.
</bodyText>
<sectionHeader confidence="0.978177" genericHeader="method">
4 Learning Surface Patterns
</sectionHeader>
<bodyText confidence="0.9996432">
Let r be a target relation. Our task is to find a set of
surface patterns S = {s1, s2, ..., sn} that express the
target relation. For example, consider the relation r
= “acquisition”. We want to find the set of patterns
S that express this relation:
</bodyText>
<equation confidence="0.824231">
S = {(ACQUIRER) acquired (ACQUIREE),
(ACQUIRER) bought (ACQUIREE), (ACQUIREE)
was bought by (ACQUIRER),...}.
</equation>
<bodyText confidence="0.999936">
The remainder of the section describes our model
for learning surface patterns for target relations.
</bodyText>
<subsectionHeader confidence="0.998776">
4.1 Model Assumption
</subsectionHeader>
<bodyText confidence="0.999899214285714">
Paraphrases express the same meaning using differ-
ent surface forms. So if one knew a pattern that ex-
presses a target relation, one could build more pat-
terns for that relation by finding paraphrases for the
surface phrase(s) in that pattern. This is the basic
assumption of our model.
For example, consider the seed pattern
“(ACQUIRER) acquired (ACQUIREE)” for
the target relation “acquisition”. The surface phrase
in the seed pattern is “acquired”. Our model then
assumes that we can obtain more surface patterns
for “acquisition” by replacing “acquired” in the
seed pattern with its paraphrases i.e. {bought, −was
bought by2,...}. The resulting surface patterns are:
</bodyText>
<footnote confidence="0.989042142857143">
1The details of the algorithm are omitted, but interested
readers are encouraged to read Charikar (2002) and Ravichan-
dran et al. (2005)
2The “−” in “−was bought by” indicates that the
(ACQUIRER) and (ACQUIREE) arguments of the input
phrase “acquired” need to be switched for the phrase “was
bought by”.
</footnote>
<listItem confidence="0.4035865">
{(ACQUIRER) bought (ACQUIREE), (ACQUIREE)
was bought by (ACQUIRER),...}
</listItem>
<subsectionHeader confidence="0.977662">
4.2 Surface Pattern Model
</subsectionHeader>
<bodyText confidence="0.999881">
Let r be a target relation. Let SEED = {seed1,
seed2,..., seedn} be the set of seed patterns that ex-
press the target relation. For each seedi E SEED,
we obtain the corresponding set of new patterns
PATi in two steps:
</bodyText>
<listItem confidence="0.768853230769231">
1. We find the surface phrase, pi, using a seed
and find the corresponding set of paraphrases,
Pi = {pi,1,pi,2,...,pi,m}. Each paraphrase,
pi,j E Pi, has with it an associated score which
is similarity between pi and pi,j.
2. In seed pattern, seedi, we replace the sur-
face phrase, pi, with its paraphrases and
obtain the set of new patterns PATi =
{pati,1, pati,2, ..., pati,m}. Each pattern has
with it an associated score, which is the same as
the score of the paraphrase from which it was
obtained3 . The patterns are ranked in the de-
creasing order of their scores.
</listItem>
<bodyText confidence="0.9997792">
After we obtain PATi for each seedi E SEED,
we obtain the complete set of patterns, PAT, for
the target relation r as the union of all the individual
pattern sets, i.e., PAT = PAT1 U PAT2 U ... U
PATn.
</bodyText>
<sectionHeader confidence="0.999241" genericHeader="method">
5 Experimental Methodology
</sectionHeader>
<bodyText confidence="0.9999604">
In this section, we describe experiments to validate
the main claims of the paper. We first describe para-
phrase acquisition, we then summarize our method
for learning surface patterns, and finally describe the
use of patterns for extracting relation instances.
</bodyText>
<subsectionHeader confidence="0.965024">
5.1 Paraphrases
</subsectionHeader>
<bodyText confidence="0.821152875">
Finding surface variations in text requires a large
corpus. The corpus needs to be orders of magnitude
larger than that required for learning syntactic varia-
tions, since surface phrases are sparser than syntac-
tic phrases.
For our experiments, we used a corpus of about
150GB (25 billion words) obtained from Google
News4 . It consists of few years worth of news data.
</bodyText>
<footnote confidence="0.998716">
3If a pattern is generated from more than one seed, we assign
it its average score.
4The corpus was cleaned to remove duplicate articles.
</footnote>
<page confidence="0.99505">
677
</page>
<bodyText confidence="0.999964666666667">
We POS tagged the corpus using Tnt tagger (Brants,
2000) and collected all phrases (n-grams) in the cor-
pus that contained at least one verb, and had a noun
or a noun-noun compound on either side. We re-
stricted the phrase length to at most five words.
We build a vector for each phrase as described in
Section 3. To mitigate the problem of sparseness and
co-reference to a certain extent, whenever we have a
noun-noun compound in the X or Y positions, we
treat it as bag of words. For example, in the sen-
tence “Google Inc. acquired YouTube”, “Google”
and “Inc.” will be treated as separate features in the
vector5.
Once we have constructed all the vectors, we find
the paraphrases for every phrase by finding its near-
est neighbors as described in Section 3. For our ex-
periments, we set the number of random bits in the
LSH function to 3000, and the similarity cut-off be-
tween vectors to 0.15. We eventually end up with
a resource containing over 2.5 million phrases such
that each phrase is connected to its paraphrases.
</bodyText>
<subsectionHeader confidence="0.999061">
5.2 Surface Patterns
</subsectionHeader>
<bodyText confidence="0.998916">
One claim of this paper is that we can find good sur-
face patterns for a target relation by starting with a
seed pattern. To verify this, we study two target re-
lations6:
</bodyText>
<listItem confidence="0.99158625">
1. Acquisition: We deÞne this as the relation be-
tween two companies such that one company
acquired the other.
2. Birthplace: We deÞne this as the relation be-
tween a person and his/her birthplace.
For “acquisition” relation, we start with the sur-
face patterns containing only the words buy and ac-
quire:
1. “(ACQUIRER) bought (ACQUIREE)” (and its
variants, i.e. buy, buys and buying)
2. “(ACQUIRER) acquired (ACQUIREE)” (and its
variants, i.e. acquire, acquires and acquiring)
</listItem>
<footnote confidence="0.9975054">
5This adds some noise in the vectors, but we found that this
results in better paraphrases.
6Since we have to do all the annotations for evaluations on
our own, we restricted our experiments to only two commonly
used relations.
</footnote>
<bodyText confidence="0.777504666666667">
This results in a total of eight seed patterns.
For “birthplace” relation, we start with two seed
patterns:
</bodyText>
<listItem confidence="0.9758285">
1. “(PERSON) was born in (LOCATION)”
2. “(PERSON) was born at (LOCATION)”.
</listItem>
<bodyText confidence="0.9705005">
We find other surface patterns for each of these
relations by replacing the surface words in the seed
patterns by their paraphrases, as described in Sec-
tion 4.
</bodyText>
<subsectionHeader confidence="0.997487">
5.3 Relation Extraction
</subsectionHeader>
<bodyText confidence="0.999947571428571">
The purpose of learning surface patterns for a rela-
tion is to extract instances of that relation. We use
the surface patterns obtained for the relations “ac-
quisition” and “birthplace” to extract instances of
these relations from the LDC North American News
Corpus. This helps us to extrinsically evaluate the
quality of the surface patterns.
</bodyText>
<sectionHeader confidence="0.997364" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.9995235">
In this section, we present the results of the experi-
ments and analyze them.
</bodyText>
<subsectionHeader confidence="0.997789">
6.1 Baselines
</subsectionHeader>
<bodyText confidence="0.999916944444444">
It is hard to construct a baseline for comparing the
quality of paraphrases, as there isn’t much work in
extracting surface level paraphrases using a mono-
lingual corpus. To overcome this, we show the effect
of reduction in corpus size on the quality of para-
phrases, and compare the results informally to the
other methods that produce syntactic paraphrases.
To compare the quality of the extraction patterns,
and relation instances, we use the method presented
by Ravichandran and Hovy (2002) as the baseline.
For each of the given relations, “acquisition” and
“birthplace”, we use 10 seed instances, download
the top 1000 results from the Google search engine
for each instance, extract the sentences that contain
the instances, and learn the set of baseline patterns
for each relation. We then apply these patterns to
the test corpus and extract the corresponding base-
line instances.
</bodyText>
<subsectionHeader confidence="0.997308">
6.2 Evaluation Criteria
</subsectionHeader>
<bodyText confidence="0.9989645">
Here we present the evaluation criteria we used to
evaluate the performance on the different tasks.
</bodyText>
<page confidence="0.997244">
678
</page>
<subsectionHeader confidence="0.548702">
Paraphrases
</subsectionHeader>
<bodyText confidence="0.999871">
We estimate the quality of paraphrases by annotating
a random sample as correct/incorrect and calculating
the accuracy. However, estimating the recall is diffi-
cult given that we do not have a complete set of para-
phrases for the input phrases. Following Szpektor et
al. (2004), instead of measuring recall, we calculate
the average number of correct paraphrases per input
phrase.
</bodyText>
<subsectionHeader confidence="0.963721">
Surface Patterns
</subsectionHeader>
<bodyText confidence="0.9837475">
We can calculate the precision (P) of learned pat-
terns for each relation by annotating the extracted
patterns as correct/incorrect. However calculating
the recall is a problem for the same reason as above.
But we can calculate the relative recall (RR) of the
system against the baseline and vice versa. The rela-
tive recall RRS|B of system 5 with respect to system
B can be calculated as:
</bodyText>
<equation confidence="0.991323">
= ° CBB
RRS|B
</equation>
<bodyText confidence="0.99983825">
where CS is the number of correct patterns found by
our system and CB is the number of correct patterns
found by the baseline. RRB|S can be found in a sim-
ilar way.
</bodyText>
<subsectionHeader confidence="0.596831">
Relation Extraction
</subsectionHeader>
<bodyText confidence="0.984520461538462">
We estimate the precision (P) of the extracted in-
stances by annotating a random sample of instances
as correct/incorrect. While calculating the true re-
call here is not possible, even calculating the true
relative recall of the system against the baseline is
not possible as we can annotate only a small sam-
ple. However, following Pantel et al. (2004), we as-
sume that the recall of the baseline is 1 and estimate
the relative recall RRS|B of the system 5 with re-
spect to the baseline B using their respective pre-
cision scores PS and PB and number of instances
extracted by them |5 |and |B |as:
= pB*�BI
</bodyText>
<sectionHeader confidence="0.951209" genericHeader="method">
RRS|B I
</sectionHeader>
<subsectionHeader confidence="0.999668">
6.3 Gold Standard
</subsectionHeader>
<bodyText confidence="0.9998995">
In this section, we describe the creation of gold stan-
dard for the different tasks.
</bodyText>
<subsectionHeader confidence="0.779693">
Paraphrases
</subsectionHeader>
<bodyText confidence="0.999952333333333">
We created the gold standard paraphrase test set by
randomly selecting 50 phrases and their correspond-
ing paraphrases from our collection of 2.5 million
phrases. For each test phrase, we asked two annota-
tors to annotate its paraphrases as correct/incorrect.
The annotators were instructed to look for strict
paraphrases i.e. equivalent phrases that can be sub-
stituted for each other.
To obtain the inter-annotator agreement, the two
annotators annotated the test set separately. The
kappa statistic (Siegal and Castellan Jr., 1988) was
n = 0.63. The interesting thing is that the anno-
tators got this respectable kappa score without any
prior training, which is hard to achieve when one
annotates for a similar task like textual entailment.
</bodyText>
<subsectionHeader confidence="0.9702">
Surface Patterns
</subsectionHeader>
<bodyText confidence="0.9999927">
For the target relations, we asked two annotators to
annotate the patterns for each relation as either “pre-
cise” or “vague”. The annotators annotated the sys-
tem as well as the baseline outputs. We consider the
“precise” patterns as correct and the “vague” as in-
correct. The intuition is that applying the vague pat-
terns for extracting target relation instances might
find some good instances, but will also find many
bad ones. For example, consider the following two
patterns for the “acquisition” relation:
</bodyText>
<equation confidence="0.9371065">
(ACQUIRER) acquired (ACQUIREE) (5)
(ACQUIRER) and (ACQUIREE) (6)
</equation>
<bodyText confidence="0.958153666666667">
Example (5) is a precise pattern as it clearly identi-
fies the “acquisition” relation while example (6) is
a vague pattern because it is too general and says
nothing about the “acquisition” relation. The kappa
statistic between the two annotators for this task was
n = 0.72.
</bodyText>
<subsectionHeader confidence="0.862082">
Relation Extraction
</subsectionHeader>
<bodyText confidence="0.9998422">
We randomly sampled 50 instances of the “acquisi-
tion” and “birthplace” relations from the system and
the baseline outputs. We asked two annotators to an-
notate the instances as correct/incorrect. The anno-
tators marked an instance as correct only if both the
entities and the relation between them were correct.
To make their task easier, the annotators were pro-
vided the context for each instance, and were free
to use any resources at their disposal (including a
web search engine), to verify the correctness of the
instances. The annotators found that the annotation
for this task was much easier than the previous two;
the few disagreements they had were due to ambigu-
ity of some of the instances. The kappa statistic for
this task was n = 0.91.
</bodyText>
<page confidence="0.9976">
679
</page>
<table confidence="0.999605">
Annotator Accuracy Average # correct
paraphrases
Annotator 1 67.31% 4.2
Annotator 2 74.27% 4.28
</table>
<tableCaption confidence="0.999701">
Table 1: Quality of paraphrases
</tableCaption>
<bodyText confidence="0.846317166666667">
are being distributed to approved a revision to the
have been distributed to unanimously approved a new
are being handed out to approved an annual
were distributed to will consider adopting a
−are handing out approved a revised
will be distributed to all approved a new
</bodyText>
<tableCaption confidence="0.99087">
Table 2: Example paraphrases
</tableCaption>
<subsectionHeader confidence="0.990023">
6.4 Result Summary
</subsectionHeader>
<bodyText confidence="0.9995245">
Table 1 shows the results of annotating the para-
phrases test set. We do not have a baseline
to compare against but we can analyze them in
light of numbers reported previously for syntac-
tic paraphrases. DIRT (Lin and Pantel, 2001) and
TEASE (Szpektor et al., 2004) report accuracies of
50.1% and 44.3% respectively compared to our av-
erage accuracy across two annotators of 70.79%.
The average number of paraphrases per phrase is
however 10.1 and 5.5 for DIRT and TEASE respec-
tively compared to our 4.2. One reason why this
number is lower is that our test set contains com-
pletely random phrases from our set (2.5 million
phrases): some of these phrases are rare and have
very few paraphrases. Table 2 shows some para-
phrases generated by our system for the phrases “are
being distributed to” and “approved a revision to
the”.
Table 3 shows the results on the quality of surface
patterns for the two relations. It can be observed
that our method outperforms the baseline by a wide
margin in both precision and relative recall. Table 4
shows some example patterns learned by our system.
Table 5 shows the results of the quality of ex-
tracted instances. Our system obtains very high pre-
cision scores but suffers in relative recall given that
the baseline with its very general patterns is likely
to Þnd a huge number of instances (though a very
small portion of them are correct). Table 6 shows
some example instances we extracted.
</bodyText>
<table confidence="0.959681625">
acquisition birthplace
X agreed to buy Y X , who was born in Y
X , which acquired Y X , was born in Y
X completed its acquisition X was raised in Y
of Y
X has acquired Y X was born in NNNNa in Y
X purchased Y X , born in Y
aEach “N” here is a placeholder for a number from 0 to 9.
</table>
<tableCaption confidence="0.984448">
Table 4: Example extraction templates
</tableCaption>
<table confidence="0.997869384615385">
acquisition birthplace
1. Huntington Bancshares 1. Cyril Andrew Ponnam-
Inc. agreed to acquire Re- peruma was born in Galle
liance Bank
2. Sony bought Columbia 2. Cook was born in NNNN
Pictures in Devonshire
3. Hanson Industries buys 3. Tansey was born in
Kidde Inc. Cincinnati
4. Casino America inc. 4. Tsoi was born in NNNN in
agreed to buy Grand Palais Uzbekistan
5. Tidewater inc. acquired 5. Mrs. Totenberg was born
Hornbeck Offshore Services in San Francisco
Inc.
</table>
<tableCaption confidence="0.997013">
Table 6: Example instances
</tableCaption>
<subsectionHeader confidence="0.907846">
6.5 Discussion and Error Analysis
</subsectionHeader>
<bodyText confidence="0.999980681818182">
We studied the effect of the decrease in size of the
available raw corpus on the quality of the acquired
paraphrases. We used about 10% of our original cor-
pus to learn the surface paraphrases and evaluated
them. The precision, and the average number of
correct paraphrases are calculated on the same test
set, as described in Section 6.2. The performance
drop on using 10% of the original corpus is signif-
icant (11.41% precision and on an average 1 cor-
rect paraphrase per phrase), which shows that we in-
deed need a large amount of data to learn good qual-
ity surface paraphrases. One reason for this drop
is also that when we use only 10% of the original
data, for some of the phrases from the test set, we do
not Þnd any paraphrases (thus resulting in 0% accu-
racy for them). This is not unexpected, as the larger
resource would have a much larger recall, which
again points at the advantage of using a large data
set. Another reason for this performance drop could
be the parameter settings: We found that the qual-
ity of learned paraphrases depended greatly on the
various cut-offs used. While we adjusted our model
</bodyText>
<page confidence="0.992163">
680
</page>
<table confidence="0.999915">
Relation Method # Patterns Annotator 1 Annotator 2
P RR P RR
Acquisition Baseline 160 55% 13.02% 60% 11.16%
Paraphrase Method 231 83.11% 28.40% 93.07% 25%
Birthplace Baseline 16 31.35% 15.38% 31.25% 15.38%
Paraphrase Method 16 81.25% 40% 81.25% 40%
</table>
<tableCaption confidence="0.991081">
Table 3: Quality of Extraction Patterns
</tableCaption>
<table confidence="0.999920333333333">
Relation Method # Patterns Annotator 1 Annotator 2
P RR P RR
Acquisition Baseline 1,261, 986 6% 100% 2% 100%
Paraphrase Method 3875 88% 4.5% 82% 12.59%
Birthplace Baseline 979, 607 4% 100% 2% 100%
Paraphrase Method 1811 98% 4.53% 98% 9.06%
</table>
<tableCaption confidence="0.999317">
Table 5: Quality of instances
</tableCaption>
<bodyText confidence="0.999870860465116">
parameters for working with smaller sized data, it is
conceivable that we did not Þnd the ideal setting for
them. So we consider these numbers to be a lower
bound. But even then, these numbers clearly indi-
cate the advantage of using more data.
We also manually inspected our paraphrases. We
found that the problem of “antonyms” was some-
what less pronounced due to our use of a large cor-
pus, but they still were the major source of error.
For example, our system Þnds the phrase “sell” as
a paraphrase for “buy”. We need to deal with this
problem separately in the future (may be as a post-
processing step using a list of antonyms).
Moving to the task of relation extraction, we see
from table 5 that our system has a much lower rel-
ative recall compared to the baseline. This was ex-
pected as the baseline method learns some very gen-
eral patterns, which are likely to extract some good
instances, even though they result in a huge hit to
its precision. However, our system was able to ob-
tain this performance using very few seeds. So an
increase in the number of input seeds, is likely to in-
crease the relative recall of the resource. The ques-
tion however remains as to what good seeds might
be. It is clear that it is much harder to come up with
good seed patterns (that our system needs), than seed
instances (that the baseline needs). But there are
some obvious ways to overcome this problem. One
way is to bootstrap. We can look at the paraphrases
of the seed patterns and use them to obtain more pat-
terns. Our initial experiments with this method using
handpicked seeds showed good promise. However,
we need to investigate automating this approach.
Another method is to use the good patterns from the
baseline system and use them as seeds for our sys-
tem. We plan to investigate this approach as well.
One reason, why we have seen good preliminary re-
sults using these approaches (for improving recall),
we believe, is that the precision of the paraphrases is
good. So either a seed doesn’t produce any new pat-
terns or it produces good patterns, thus keeping the
precision of the system high while increasing rela-
tive recall.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9998435">
Paraphrases are an important technique to handle
variations in language. Given their utility in many
NLP tasks, it is desirable that we come up with
methods that produce good quality paraphrases. We
believe that the paraphrase acquisition method pre-
sented here is a step towards this very goal. We have
shown that high precision surface paraphrases can be
obtained by using distributional similarity on a large
corpus. We made use of some recent advances in
theoretical computer science to make this task scal-
able. We have also shown that these paraphrases
can be used to obtain high precision extraction pat-
terns for information extraction. While we believe
that more work needs to be done to improve the sys-
tem recall (some of which we are investigating), this
seems to be a good Þrst step towards developing a
minimally supervised, easy to implement, and scal-
able relation extraction system.
</bodyText>
<page confidence="0.998329">
681
</page>
<sectionHeader confidence="0.998308" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999896360465117">
P. G. Anick and S. Tipirneni. 1999. The paraphrase
search assistant: terminological feedback for iterative
information seeking. In ACM SIGIR, pages 153–159.
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Association for
Computational Linguistics, pages 597–604.
R. Barzilay and L. Lee. 2003. Learning to paraphrase: an
unsupervised approach using multiple-sequence align-
ment. In In Proceedings North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 16–23.
R. Barzilay and K. R. McKeown. 2001. Extracting para-
phrases from a parallel corpus. In In Proceedings of
Association for Computational Linguistics, pages 50–
57.
R. Barzilay, K. R. McKeown, and M. Elhadad. 1999.
Information fusion in the context of multi-document
summarization. In Associationfor Computational Lin-
guistics, pages 550–557.
M. Berland and E. Charniak. 1999. Finding parts in very
large corpora. In In Proceedings of Association for
Computational Linguistics, pages 57–64.
T. Brants. 2000. Tnt – a statistical part-of-speech tag-
ger. In In Proceedings of the Applied NLP Conference
(ANLP).
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, pages 17–24.
M. S. Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In In Proceedings of the
thiry-fourth annual ACM symposium on Theory of
computing, pages 380–388.
T.M. Cover and J.A. Thomas. 1991. Elements of Infor-
mation Theory. John Wiley &amp; Sons.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: ex-
ploiting massively parallel news sources. In In Pro-
ceedings of the conference on Computational Linguis-
tics (COLING), pages 350–357.
Z. Harris. 1954. Distributional structure. Word, pages
10(23):146–162.
M. A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the confer-
ence on Computational linguistics, pages 539–545.
D. Lin and P. Pantel. 2001. Dirt: Discovery of infer-
ence rules from text. In ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 323–328.
P. Pantel, D. Ravichandran, and E.H. Hovy. 2004. To-
wards terascale knowledge acquisition. In In Proceed-
ings of the conference on Computational Linguistics
(COLING), pages 771–778.
D. Ravichandran and E.H. Hovy. 2002. Learning sur-
face text for a question answering system. In Associ-
ation for Computational Linguistics (ACL), Philadel-
phia, PA.
D. Ravichandran, P. Pantel, and E.H. Hovy. 2005. Ran-
domized algorithms and nlp: using locality sensitive
hash function for high speed noun clustering. In In
Proceedings of Association for Computational Lin-
guistics, pages 622–629.
L. Romano, M. Kouylekov, I. Szpektor, I. Dagan, and
A. Lavelli. 2006. Investigating a generic paraphrase-
based approach for relation extraction. In In Proceed-
ings of the European Chapter of the Association for
Computational Linguistics (EACL).
B. Rosenfeld and R. Feldman. 2006. Ures: an unsuper-
vised web relation extraction system. In Proceedings
of the COLING/ACL on Main conference poster ses-
sions, pages 667–674.
S. Sekine. 2006. On-demand information extraction. In
In Proceedings of COLING/ACL, pages 731–738.
S. Siegal and N.J. Castellan Jr. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling web-based acquisition of entailment relations.
In In Proceedings of Empirical Methods in Natural
Language Processing, pages 41–48.
L. Zhou, C.Y. Lin, D. Munteanu, and E.H. Hovy. 2006.
Paraeval: using paraphrases to evaluate summaries au-
tomatically. In In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, pages 447–454.
</reference>
<page confidence="0.997967">
682
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.722540">
<title confidence="0.959967">Large Scale Acquisition of Paraphrases for Learning Surface Patterns</title>
<affiliation confidence="0.994565">Information Sciences Institute University of Southern California</affiliation>
<author confidence="0.807634">Marina del Rey</author>
<author confidence="0.807634">CA</author>
<email confidence="0.999108">rahul@isi.edu</email>
<author confidence="0.969568">Deepak Ravichandran</author>
<affiliation confidence="0.999394">Google Inc.</affiliation>
<address confidence="0.9921605">1600 Amphitheatre Parkway Mountain View, CA</address>
<email confidence="0.999627">deepakr@google.com</email>
<abstract confidence="0.999241888888889">Paraphrases have proved to be useful in many applications, including Machine Translation, Question Answering, Summarization, and Information Retrieval. Paraphrase acquisition methods that use a single monolingual corpus often produce only syntactic paraphrases. We present a method for obtaining surface parausing a words) monolingual corpus. Our method achieves an of around the paraphrase acquisition task. We further show that we can use these paraphrases to generate surface patterns for relation extraction. Our patterns are much more precise than those obtained by using a state of the art baseline and can extract with more than for each of the test relations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P G Anick</author>
<author>S Tipirneni</author>
</authors>
<title>The paraphrase search assistant: terminological feedback for iterative information seeking.</title>
<date>1999</date>
<booktitle>In ACM SIGIR,</booktitle>
<pages>153--159</pages>
<contexts>
<context position="2203" citStr="Anick and Tipirneni, 1999" startWordPosition="324" endWordPosition="327">ernship at Google Inc. role in many natural language processing (NLP) applications. For example, in question answering, paraphrases have been used to find multiple patterns that pinpoint the same answer (Ravichandran and Hovy, 2002); in statistical machine translation, they have been used to find translations for unseen source language phrases (Callison-Burch et al., 2006); in multi-document summarization, they have been used to identify phrases from different sentences that express the same information (Barzilay et al., 1999); in information retrieval they have been used for query expansion (Anick and Tipirneni, 1999). Learning paraphrases requires one to ensure identity of meaning. Since there are no adequate semantic interpretation systems available today, paraphrase acquisition techniques use some other mechanism as a kind of “pivot” to (help) ensure semantic identity. Each pivot mechanism selects phrases with similar meaning in a different characteristic way. A popular method, the so-called distributional similarity, is based on the dictum of Zelig Harris “you shall know the words by the company they keep”: given highly discriminating left and right contexts, only words with very similar meaning will b</context>
</contexts>
<marker>Anick, Tipirneni, 1999</marker>
<rawString>P. G. Anick and S. Tipirneni. 1999. The paraphrase search assistant: terminological feedback for iterative information seeking. In ACM SIGIR, pages 153–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bannard</author>
<author>C Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Association for Computational Linguistics,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="4850" citStr="Bannard and Callison-Burch (2005)" startWordPosition="739" endWordPosition="742">lly supervised way. Surface patterns are templates for extracting information from text. For example, if one wanted to extract a list of company acquisitions, “(ACQUIRER) acquired (ACQUIREE)” would be one surface pattern with “(ACQUIRER)” and “(ACQUIREE)” as the slots to be extracted. Thus we can claim: Claim 2: These paraphrases can then be used for generating high precision surface patterns for relation extraction. 2 Related Work Most recent work in paraphrase acquisition is based on automatic acquisition. Barzilay and McKeown (2001) used a monolingual parallel corpus to obtain paraphrases. Bannard and Callison-Burch (2005) and Zhou et al. (2006) both employed a bilingual parallel corpus in which each foreign language word or phrase was a pivot to obtain source language paraphrases. Dolan et al. (2004) and Barzilay and Lee (2003) used comparable news articles to obtain sentence level paraphrases. All these approaches rely on the presence of parallel or comparable corpora and are thus limited by their availability and size. Lin and Pantel (2001) and Szpektor et al. (2004) proposed methods to obtain entailment templates by using a single monolingual resource. While both differ in their approaches, they both end up</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>C. Bannard and C. Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Association for Computational Linguistics, pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Learning to paraphrase: an unsupervised approach using multiple-sequence alignment. In</title>
<date>2003</date>
<booktitle>In Proceedings North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="5060" citStr="Barzilay and Lee (2003)" startWordPosition="776" endWordPosition="779"> with “(ACQUIRER)” and “(ACQUIREE)” as the slots to be extracted. Thus we can claim: Claim 2: These paraphrases can then be used for generating high precision surface patterns for relation extraction. 2 Related Work Most recent work in paraphrase acquisition is based on automatic acquisition. Barzilay and McKeown (2001) used a monolingual parallel corpus to obtain paraphrases. Bannard and Callison-Burch (2005) and Zhou et al. (2006) both employed a bilingual parallel corpus in which each foreign language word or phrase was a pivot to obtain source language paraphrases. Dolan et al. (2004) and Barzilay and Lee (2003) used comparable news articles to obtain sentence level paraphrases. All these approaches rely on the presence of parallel or comparable corpora and are thus limited by their availability and size. Lin and Pantel (2001) and Szpektor et al. (2004) proposed methods to obtain entailment templates by using a single monolingual resource. While both differ in their approaches, they both end up finding syntactic paraphrases. Their methods cannot be used if we cannot parse the data (either because of scale or data quality). Our approach on the other hand, finds surface paraphrases; it is more scalable</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>R. Barzilay and L. Lee. 2003. Learning to paraphrase: an unsupervised approach using multiple-sequence alignment. In In Proceedings North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus. In</title>
<date>2001</date>
<booktitle>In Proceedings of Association for Computational Linguistics,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="4758" citStr="Barzilay and McKeown (2001)" startWordPosition="727" endWordPosition="730">ain high precision surface patterns that enable the discovery of relations in a minimally supervised way. Surface patterns are templates for extracting information from text. For example, if one wanted to extract a list of company acquisitions, “(ACQUIRER) acquired (ACQUIREE)” would be one surface pattern with “(ACQUIRER)” and “(ACQUIREE)” as the slots to be extracted. Thus we can claim: Claim 2: These paraphrases can then be used for generating high precision surface patterns for relation extraction. 2 Related Work Most recent work in paraphrase acquisition is based on automatic acquisition. Barzilay and McKeown (2001) used a monolingual parallel corpus to obtain paraphrases. Bannard and Callison-Burch (2005) and Zhou et al. (2006) both employed a bilingual parallel corpus in which each foreign language word or phrase was a pivot to obtain source language paraphrases. Dolan et al. (2004) and Barzilay and Lee (2003) used comparable news articles to obtain sentence level paraphrases. All these approaches rely on the presence of parallel or comparable corpora and are thus limited by their availability and size. Lin and Pantel (2001) and Szpektor et al. (2004) proposed methods to obtain entailment templates by </context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>R. Barzilay and K. R. McKeown. 2001. Extracting paraphrases from a parallel corpus. In In Proceedings of Association for Computational Linguistics, pages 50– 57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K R McKeown</author>
<author>M Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>In Associationfor Computational Linguistics,</booktitle>
<pages>550--557</pages>
<contexts>
<context position="2109" citStr="Barzilay et al., 1999" startWordPosition="309" endWordPosition="313">o capture the variability of language and hence play an important *Work done during an internship at Google Inc. role in many natural language processing (NLP) applications. For example, in question answering, paraphrases have been used to find multiple patterns that pinpoint the same answer (Ravichandran and Hovy, 2002); in statistical machine translation, they have been used to find translations for unseen source language phrases (Callison-Burch et al., 2006); in multi-document summarization, they have been used to identify phrases from different sentences that express the same information (Barzilay et al., 1999); in information retrieval they have been used for query expansion (Anick and Tipirneni, 1999). Learning paraphrases requires one to ensure identity of meaning. Since there are no adequate semantic interpretation systems available today, paraphrase acquisition techniques use some other mechanism as a kind of “pivot” to (help) ensure semantic identity. Each pivot mechanism selects phrases with similar meaning in a different characteristic way. A popular method, the so-called distributional similarity, is based on the dictum of Zelig Harris “you shall know the words by the company they keep”: gi</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>R. Barzilay, K. R. McKeown, and M. Elhadad. 1999. Information fusion in the context of multi-document summarization. In Associationfor Computational Linguistics, pages 550–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Berland</author>
<author>E Charniak</author>
</authors>
<title>Finding parts in very large corpora. In</title>
<date>1999</date>
<booktitle>In Proceedings of Association for Computational Linguistics,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="6086" citStr="Berland and Charniak (1999)" startWordPosition="948" endWordPosition="951">syntactic paraphrases. Their methods cannot be used if we cannot parse the data (either because of scale or data quality). Our approach on the other hand, finds surface paraphrases; it is more scalable and robust due to the use of simple POS tagging. Also, our use of locality sensitive hashing makes finding similar phrases in a large corpus feasible. Another task related to our work is relation extraction. Its aim is to extract instances of a given relation. Hearst (1992) the pioneering paper in the field used a small number of hand selected patterns to extract instances of hyponymy relation. Berland and Charniak (1999) used a similar method for extracting instances of meronymy relation. Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web. But their method often finds patterns that are too general (e.g., X and Y), resulting in low precision extractions. Rosenfeld and Feldman (2006) present a somewhat similar web based method that uses a combination of seed instances and seed patterns to learn good quality surface patterns. Both these methods differ from ours in that they learn relation patterns on the fly (from the web). Our method howev</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>M. Berland and E. Charniak. 1999. Finding parts in very large corpora. In In Proceedings of Association for Computational Linguistics, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>Tnt – a statistical part-of-speech tagger. In</title>
<date>2000</date>
<booktitle>In Proceedings of the Applied NLP Conference (ANLP).</booktitle>
<contexts>
<context position="15568" citStr="Brants, 2000" startWordPosition="2597" endWordPosition="2598">extracting relation instances. 5.1 Paraphrases Finding surface variations in text requires a large corpus. The corpus needs to be orders of magnitude larger than that required for learning syntactic variations, since surface phrases are sparser than syntactic phrases. For our experiments, we used a corpus of about 150GB (25 billion words) obtained from Google News4 . It consists of few years worth of news data. 3If a pattern is generated from more than one seed, we assign it its average score. 4The corpus was cleaned to remove duplicate articles. 677 We POS tagged the corpus using Tnt tagger (Brants, 2000) and collected all phrases (n-grams) in the corpus that contained at least one verb, and had a noun or a noun-noun compound on either side. We restricted the phrase length to at most five words. We build a vector for each phrase as described in Section 3. To mitigate the problem of sparseness and co-reference to a certain extent, whenever we have a noun-noun compound in the X or Y positions, we treat it as bag of words. For example, in the sentence “Google Inc. acquired YouTube”, “Google” and “Inc.” will be treated as separate features in the vector5. Once we have constructed all the vectors, </context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. Tnt – a statistical part-of-speech tagger. In In Proceedings of the Applied NLP Conference (ANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>M Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="1952" citStr="Callison-Burch et al., 2006" startWordPosition="287" endWordPosition="290">e level paraphrases, and the phrases “acquired” and “completed the acquisition of” in (1) and (2) respectively are phrasal paraphrases. Paraphrases provide a way to capture the variability of language and hence play an important *Work done during an internship at Google Inc. role in many natural language processing (NLP) applications. For example, in question answering, paraphrases have been used to find multiple patterns that pinpoint the same answer (Ravichandran and Hovy, 2002); in statistical machine translation, they have been used to find translations for unseen source language phrases (Callison-Burch et al., 2006); in multi-document summarization, they have been used to identify phrases from different sentences that express the same information (Barzilay et al., 1999); in information retrieval they have been used for query expansion (Anick and Tipirneni, 1999). Learning paraphrases requires one to ensure identity of meaning. Since there are no adequate semantic interpretation systems available today, paraphrase acquisition techniques use some other mechanism as a kind of “pivot” to (help) ensure semantic identity. Each pivot mechanism selects phrases with similar meaning in a different characteristic w</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>C. Callison-Burch, P. Koehn, and M. Osborne. 2006. Improved statistical machine translation using paraphrases. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Charikar</author>
</authors>
<title>Similarity estimation techniques from rounding algorithms. In</title>
<date>2002</date>
<booktitle>In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,</booktitle>
<pages>380--388</pages>
<contexts>
<context position="3798" citStr="Charikar, 2002" startWordPosition="577" endWordPosition="578">s paper we call the paraphrases that contain only words as surface paraphrases and those 674 Proceedings of ACL-08: HLT, pages 674–682, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics that contain paths in a syntax tree as syntactic paraphrases. We here, present a method to acquire surface paraphrases from a single monolingual corpus. We use a large corpus (about 150GB) to overcome the data sparseness problem. To overcome the scalability problem, we pre-process the text with a simple parts-of-speech (POS) tagger and then apply locality sensitive hashing (LSH) (Charikar, 2002; Ravichandran et al., 2005) to speed up the remaining computation for paraphrase acquisition. Our experiments show results to verify the following main claim: Claim 1: Highly precise surface paraphrases can be obtained from a very large monolingual corpus. With this result, we further show that these paraphrases can be used to obtain high precision surface patterns that enable the discovery of relations in a minimally supervised way. Surface patterns are templates for extracting information from text. For example, if one wanted to extract a list of company acquisitions, “(ACQUIRER) acquired (</context>
<context position="11839" citStr="Charikar (2002)" startWordPosition="1968" endWordPosition="1969">rpus, we’ll have to compute the similarity between all vector pairs. If n is the number of vectors and d is the dimensionality of the vector space, finding cosine similarity between each pair of vectors has time complexity O(n2d). This computation is infeasible for our corpus, since both n and d are large. 676 To solve this problem, we make use of Locality Sensitive Hashing (LSH). The basic idea behind LSH is that a LSH function creates a fingerprint for each vector such that if two vectors are similar, they are likely to have similar fingerprints. The LSH function we use here was proposed by Charikar (2002). It represents a d dimensional vector by a stream of b bits (b « d) and has the property of preserving the cosine similarity between vectors, which is exactly what we want. Ravichandran et al. (2005) have shown that by using the LSH nearest neighbors calculation can be done in O(nd) time.1. 4 Learning Surface Patterns Let r be a target relation. Our task is to find a set of surface patterns S = {s1, s2, ..., sn} that express the target relation. For example, consider the relation r = “acquisition”. We want to find the set of patterns S that express this relation: S = {(ACQUIRER) acquired (ACQ</context>
<context position="13413" citStr="Charikar (2002)" startWordPosition="2225" endWordPosition="2226">for that relation by finding paraphrases for the surface phrase(s) in that pattern. This is the basic assumption of our model. For example, consider the seed pattern “(ACQUIRER) acquired (ACQUIREE)” for the target relation “acquisition”. The surface phrase in the seed pattern is “acquired”. Our model then assumes that we can obtain more surface patterns for “acquisition” by replacing “acquired” in the seed pattern with its paraphrases i.e. {bought, −was bought by2,...}. The resulting surface patterns are: 1The details of the algorithm are omitted, but interested readers are encouraged to read Charikar (2002) and Ravichandran et al. (2005) 2The “−” in “−was bought by” indicates that the (ACQUIRER) and (ACQUIREE) arguments of the input phrase “acquired” need to be switched for the phrase “was bought by”. {(ACQUIRER) bought (ACQUIREE), (ACQUIREE) was bought by (ACQUIRER),...} 4.2 Surface Pattern Model Let r be a target relation. Let SEED = {seed1, seed2,..., seedn} be the set of seed patterns that express the target relation. For each seedi E SEED, we obtain the corresponding set of new patterns PATi in two steps: 1. We find the surface phrase, pi, using a seed and find the corresponding set of para</context>
</contexts>
<marker>Charikar, 2002</marker>
<rawString>M. S. Charikar. 2002. Similarity estimation techniques from rounding algorithms. In In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 380–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="9343" citStr="Cover and Thomas, 1991" startWordPosition="1503" endWordPosition="1506">X and Y are the placeholders for words occurring on either side of p. Our first task is to find the set of phrases that are similar in meaning top. Let P = {p1, p2, p3, ..., pl} be the set of all phrases of the form X pi Y where pi E P. Let 5i,X be the set of words that occur in position X of pi and 5i,Y be the set of words that occur in position Y of pi. Let Vi be the vector representing pi such that Vi = 5i,X U 5i,Y . Each word f E Vi has an associated score that measures the strength of the association of the word f with phrase pi; as do many others, we employ pointwise mutual information (Cover and Thomas, 1991) to measure this strength of association. pmi(pi; f) =log P(pi,f� (1) P(pi�P (f� The probabilities in equation (1) are calculated by using the maximum likelihood estimate over our corpus. Once we have the vectors for each phrase pi E P, we can find the paraphrases for each pi by finding its nearest neighbors. We use cosine similarity, which is a commonly used measure for finding similarity between two vectors. If we have two phrases pi E P and pj E P with the corresponding vectors Vi and Vj constructed as described above, the similarity between the two phrases is calculated as: sim(pi;pj) = Vi</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>T.M. Cover and J.A. Thomas. 1991. Elements of Information Theory. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dolan</author>
<author>C Quirk</author>
<author>C Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In In Proceedings of the conference on Computational Linguistics (COLING),</booktitle>
<pages>350--357</pages>
<contexts>
<context position="5032" citStr="Dolan et al. (2004)" startWordPosition="771" endWordPosition="774">d be one surface pattern with “(ACQUIRER)” and “(ACQUIREE)” as the slots to be extracted. Thus we can claim: Claim 2: These paraphrases can then be used for generating high precision surface patterns for relation extraction. 2 Related Work Most recent work in paraphrase acquisition is based on automatic acquisition. Barzilay and McKeown (2001) used a monolingual parallel corpus to obtain paraphrases. Bannard and Callison-Burch (2005) and Zhou et al. (2006) both employed a bilingual parallel corpus in which each foreign language word or phrase was a pivot to obtain source language paraphrases. Dolan et al. (2004) and Barzilay and Lee (2003) used comparable news articles to obtain sentence level paraphrases. All these approaches rely on the presence of parallel or comparable corpora and are thus limited by their availability and size. Lin and Pantel (2001) and Szpektor et al. (2004) proposed methods to obtain entailment templates by using a single monolingual resource. While both differ in their approaches, they both end up finding syntactic paraphrases. Their methods cannot be used if we cannot parse the data (either because of scale or data quality). Our approach on the other hand, finds surface para</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>B. Dolan, C. Quirk, and C. Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In In Proceedings of the conference on Computational Linguistics (COLING), pages 350–357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1954</date>
<pages>10--23</pages>
<publisher>Word,</publisher>
<contexts>
<context position="7748" citStr="Harris, 1954" startWordPosition="1207" endWordPosition="1208"> and Sekine (2006) used syntactic paraphrases to obtain patterns for extracting relations. While procedurally different, both methods depend heavily on the performance of the syntax parser and require complex syntax tree matching to extract the relation instances. Our method on the other hand acquires surface patterns and thus avoids the dependence on a parser and syntactic matching. This also makes the extraction process scalable. 3 Acquiring Paraphrases This section describes our model for acquiring paraphrases from text. 675 3.1 Distributional Similarity Harris’s distributional hypothesis (Harris, 1954) has played an important role in lexical semantics. It states that words that appear in similar contexts tend to have similar meanings. In this paper, we apply the distributional hypothesis to phrases i.e. word ngrams. For example, consider the phrase “acquired” of the form “X acquired Y ”. Considering the context of this phrase, we might find {Google, eBay, Yahoo,...} in position X and {YouTube, Skype, Overture,...} in position Y . Now consider another phrase “completed the acquisition of”, again of the form “X completed the acquisition of Y ”. For this phrase, we might find {Google, eBay, Hi</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Z. Harris. 1954. Distributional structure. Word, pages 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the conference on Computational linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="5935" citStr="Hearst (1992)" startWordPosition="925" endWordPosition="926">s to obtain entailment templates by using a single monolingual resource. While both differ in their approaches, they both end up finding syntactic paraphrases. Their methods cannot be used if we cannot parse the data (either because of scale or data quality). Our approach on the other hand, finds surface paraphrases; it is more scalable and robust due to the use of simple POS tagging. Also, our use of locality sensitive hashing makes finding similar phrases in a large corpus feasible. Another task related to our work is relation extraction. Its aim is to extract instances of a given relation. Hearst (1992) the pioneering paper in the field used a small number of hand selected patterns to extract instances of hyponymy relation. Berland and Charniak (1999) used a similar method for extracting instances of meronymy relation. Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web. But their method often finds patterns that are too general (e.g., X and Y), resulting in low precision extractions. Rosenfeld and Feldman (2006) present a somewhat similar web based method that uses a combination of seed instances and seed patterns to le</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the conference on Computational linguistics, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Dirt: Discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="5279" citStr="Lin and Pantel (2001)" startWordPosition="811" endWordPosition="814">recent work in paraphrase acquisition is based on automatic acquisition. Barzilay and McKeown (2001) used a monolingual parallel corpus to obtain paraphrases. Bannard and Callison-Burch (2005) and Zhou et al. (2006) both employed a bilingual parallel corpus in which each foreign language word or phrase was a pivot to obtain source language paraphrases. Dolan et al. (2004) and Barzilay and Lee (2003) used comparable news articles to obtain sentence level paraphrases. All these approaches rely on the presence of parallel or comparable corpora and are thus limited by their availability and size. Lin and Pantel (2001) and Szpektor et al. (2004) proposed methods to obtain entailment templates by using a single monolingual resource. While both differ in their approaches, they both end up finding syntactic paraphrases. Their methods cannot be used if we cannot parse the data (either because of scale or data quality). Our approach on the other hand, finds surface paraphrases; it is more scalable and robust due to the use of simple POS tagging. Also, our use of locality sensitive hashing makes finding similar phrases in a large corpus feasible. Another task related to our work is relation extraction. Its aim is</context>
<context position="24031" citStr="Lin and Pantel, 2001" startWordPosition="4018" endWordPosition="4021">ator 1 67.31% 4.2 Annotator 2 74.27% 4.28 Table 1: Quality of paraphrases are being distributed to approved a revision to the have been distributed to unanimously approved a new are being handed out to approved an annual were distributed to will consider adopting a −are handing out approved a revised will be distributed to all approved a new Table 2: Example paraphrases 6.4 Result Summary Table 1 shows the results of annotating the paraphrases test set. We do not have a baseline to compare against but we can analyze them in light of numbers reported previously for syntactic paraphrases. DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al., 2004) report accuracies of 50.1% and 44.3% respectively compared to our average accuracy across two annotators of 70.79%. The average number of paraphrases per phrase is however 10.1 and 5.5 for DIRT and TEASE respectively compared to our 4.2. One reason why this number is lower is that our test set contains completely random phrases from our set (2.5 million phrases): some of these phrases are rare and have very few paraphrases. Table 2 shows some paraphrases generated by our system for the phrases “are being distributed to” and “approved a revision to the”. Table</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>D. Lin and P. Pantel. 2001. Dirt: Discovery of inference rules from text. In ACM SIGKDD international conference on Knowledge discovery and data mining, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Ravichandran</author>
<author>E H Hovy</author>
</authors>
<title>Towards terascale knowledge acquisition. In</title>
<date>2004</date>
<booktitle>In Proceedings of the conference on Computational Linguistics (COLING),</booktitle>
<pages>771--778</pages>
<contexts>
<context position="20606" citStr="Pantel et al. (2004)" startWordPosition="3446" endWordPosition="3449">recall RRS|B of system 5 with respect to system B can be calculated as: = ° CBB RRS|B where CS is the number of correct patterns found by our system and CB is the number of correct patterns found by the baseline. RRB|S can be found in a similar way. Relation Extraction We estimate the precision (P) of the extracted instances by annotating a random sample of instances as correct/incorrect. While calculating the true recall here is not possible, even calculating the true relative recall of the system against the baseline is not possible as we can annotate only a small sample. However, following Pantel et al. (2004), we assume that the recall of the baseline is 1 and estimate the relative recall RRS|B of the system 5 with respect to the baseline B using their respective precision scores PS and PB and number of instances extracted by them |5 |and |B |as: = pB*�BI RRS|B I 6.3 Gold Standard In this section, we describe the creation of gold standard for the different tasks. Paraphrases We created the gold standard paraphrase test set by randomly selecting 50 phrases and their corresponding paraphrases from our collection of 2.5 million phrases. For each test phrase, we asked two annotators to annotate its pa</context>
</contexts>
<marker>Pantel, Ravichandran, Hovy, 2004</marker>
<rawString>P. Pantel, D. Ravichandran, and E.H. Hovy. 2004. Towards terascale knowledge acquisition. In In Proceedings of the conference on Computational Linguistics (COLING), pages 771–778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E H Hovy</author>
</authors>
<title>Learning surface text for a question answering system.</title>
<date>2002</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1809" citStr="Ravichandran and Hovy, 2002" startWordPosition="266" endWordPosition="269">le acquired YouTube. (1) Google completed the acquisition of YouTube. (2) Since they convey the same meaning, sentences (1) and (2) are sentence level paraphrases, and the phrases “acquired” and “completed the acquisition of” in (1) and (2) respectively are phrasal paraphrases. Paraphrases provide a way to capture the variability of language and hence play an important *Work done during an internship at Google Inc. role in many natural language processing (NLP) applications. For example, in question answering, paraphrases have been used to find multiple patterns that pinpoint the same answer (Ravichandran and Hovy, 2002); in statistical machine translation, they have been used to find translations for unseen source language phrases (Callison-Burch et al., 2006); in multi-document summarization, they have been used to identify phrases from different sentences that express the same information (Barzilay et al., 1999); in information retrieval they have been used for query expansion (Anick and Tipirneni, 1999). Learning paraphrases requires one to ensure identity of meaning. Since there are no adequate semantic interpretation systems available today, paraphrase acquisition techniques use some other mechanism as </context>
<context position="6184" citStr="Ravichandran and Hovy (2002)" startWordPosition="963" endWordPosition="966">f scale or data quality). Our approach on the other hand, finds surface paraphrases; it is more scalable and robust due to the use of simple POS tagging. Also, our use of locality sensitive hashing makes finding similar phrases in a large corpus feasible. Another task related to our work is relation extraction. Its aim is to extract instances of a given relation. Hearst (1992) the pioneering paper in the field used a small number of hand selected patterns to extract instances of hyponymy relation. Berland and Charniak (1999) used a similar method for extracting instances of meronymy relation. Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web. But their method often finds patterns that are too general (e.g., X and Y), resulting in low precision extractions. Rosenfeld and Feldman (2006) present a somewhat similar web based method that uses a combination of seed instances and seed patterns to learn good quality surface patterns. Both these methods differ from ours in that they learn relation patterns on the fly (from the web). Our method however, pre-computes paraphrases for a large set of surface patterns using distributional similarity o</context>
<context position="18740" citStr="Ravichandran and Hovy (2002)" startWordPosition="3135" endWordPosition="3138">the surface patterns. 6 Experimental Results In this section, we present the results of the experiments and analyze them. 6.1 Baselines It is hard to construct a baseline for comparing the quality of paraphrases, as there isn’t much work in extracting surface level paraphrases using a monolingual corpus. To overcome this, we show the effect of reduction in corpus size on the quality of paraphrases, and compare the results informally to the other methods that produce syntactic paraphrases. To compare the quality of the extraction patterns, and relation instances, we use the method presented by Ravichandran and Hovy (2002) as the baseline. For each of the given relations, “acquisition” and “birthplace”, we use 10 seed instances, download the top 1000 results from the Google search engine for each instance, extract the sentences that contain the instances, and learn the set of baseline patterns for each relation. We then apply these patterns to the test corpus and extract the corresponding baseline instances. 6.2 Evaluation Criteria Here we present the evaluation criteria we used to evaluate the performance on the different tasks. 678 Paraphrases We estimate the quality of paraphrases by annotating a random samp</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>D. Ravichandran and E.H. Hovy. 2002. Learning surface text for a question answering system. In Association for Computational Linguistics (ACL), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>P Pantel</author>
<author>E H Hovy</author>
</authors>
<title>Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering. In</title>
<date>2005</date>
<booktitle>In Proceedings of Association for Computational Linguistics,</booktitle>
<pages>622--629</pages>
<contexts>
<context position="3826" citStr="Ravichandran et al., 2005" startWordPosition="579" endWordPosition="583">the paraphrases that contain only words as surface paraphrases and those 674 Proceedings of ACL-08: HLT, pages 674–682, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics that contain paths in a syntax tree as syntactic paraphrases. We here, present a method to acquire surface paraphrases from a single monolingual corpus. We use a large corpus (about 150GB) to overcome the data sparseness problem. To overcome the scalability problem, we pre-process the text with a simple parts-of-speech (POS) tagger and then apply locality sensitive hashing (LSH) (Charikar, 2002; Ravichandran et al., 2005) to speed up the remaining computation for paraphrase acquisition. Our experiments show results to verify the following main claim: Claim 1: Highly precise surface paraphrases can be obtained from a very large monolingual corpus. With this result, we further show that these paraphrases can be used to obtain high precision surface patterns that enable the discovery of relations in a minimally supervised way. Surface patterns are templates for extracting information from text. For example, if one wanted to extract a list of company acquisitions, “(ACQUIRER) acquired (ACQUIREE)” would be one surf</context>
<context position="12039" citStr="Ravichandran et al. (2005)" startWordPosition="2003" endWordPosition="2006">ir of vectors has time complexity O(n2d). This computation is infeasible for our corpus, since both n and d are large. 676 To solve this problem, we make use of Locality Sensitive Hashing (LSH). The basic idea behind LSH is that a LSH function creates a fingerprint for each vector such that if two vectors are similar, they are likely to have similar fingerprints. The LSH function we use here was proposed by Charikar (2002). It represents a d dimensional vector by a stream of b bits (b « d) and has the property of preserving the cosine similarity between vectors, which is exactly what we want. Ravichandran et al. (2005) have shown that by using the LSH nearest neighbors calculation can be done in O(nd) time.1. 4 Learning Surface Patterns Let r be a target relation. Our task is to find a set of surface patterns S = {s1, s2, ..., sn} that express the target relation. For example, consider the relation r = “acquisition”. We want to find the set of patterns S that express this relation: S = {(ACQUIRER) acquired (ACQUIREE), (ACQUIRER) bought (ACQUIREE), (ACQUIREE) was bought by (ACQUIRER),...}. The remainder of the section describes our model for learning surface patterns for target relations. 4.1 Model Assumptio</context>
<context position="13444" citStr="Ravichandran et al. (2005)" startWordPosition="2228" endWordPosition="2232"> finding paraphrases for the surface phrase(s) in that pattern. This is the basic assumption of our model. For example, consider the seed pattern “(ACQUIRER) acquired (ACQUIREE)” for the target relation “acquisition”. The surface phrase in the seed pattern is “acquired”. Our model then assumes that we can obtain more surface patterns for “acquisition” by replacing “acquired” in the seed pattern with its paraphrases i.e. {bought, −was bought by2,...}. The resulting surface patterns are: 1The details of the algorithm are omitted, but interested readers are encouraged to read Charikar (2002) and Ravichandran et al. (2005) 2The “−” in “−was bought by” indicates that the (ACQUIRER) and (ACQUIREE) arguments of the input phrase “acquired” need to be switched for the phrase “was bought by”. {(ACQUIRER) bought (ACQUIREE), (ACQUIREE) was bought by (ACQUIRER),...} 4.2 Surface Pattern Model Let r be a target relation. Let SEED = {seed1, seed2,..., seedn} be the set of seed patterns that express the target relation. For each seedi E SEED, we obtain the corresponding set of new patterns PATi in two steps: 1. We find the surface phrase, pi, using a seed and find the corresponding set of paraphrases, Pi = {pi,1,pi,2,...,pi</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>D. Ravichandran, P. Pantel, and E.H. Hovy. 2005. Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering. In In Proceedings of Association for Computational Linguistics, pages 622–629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Romano</author>
<author>M Kouylekov</author>
<author>I Szpektor</author>
<author>I Dagan</author>
<author>A Lavelli</author>
</authors>
<title>Investigating a generic paraphrasebased approach for relation extraction. In</title>
<date>2006</date>
<booktitle>In Proceedings of the European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="7135" citStr="Romano et al. (2006)" startWordPosition="1116" endWordPosition="1119"> seed patterns to learn good quality surface patterns. Both these methods differ from ours in that they learn relation patterns on the fly (from the web). Our method however, pre-computes paraphrases for a large set of surface patterns using distributional similarity over a large corpus and then obtains patterns for a relation by simply finding paraphrases (offline) for a few seed patterns. Using distributional similarity avoids the problem of obtaining overly general patterns and the pre-computation of paraphrases means that we can obtain the set of patterns for any relation instantaneously. Romano et al. (2006) and Sekine (2006) used syntactic paraphrases to obtain patterns for extracting relations. While procedurally different, both methods depend heavily on the performance of the syntax parser and require complex syntax tree matching to extract the relation instances. Our method on the other hand acquires surface patterns and thus avoids the dependence on a parser and syntactic matching. This also makes the extraction process scalable. 3 Acquiring Paraphrases This section describes our model for acquiring paraphrases from text. 675 3.1 Distributional Similarity Harris’s distributional hypothesis (</context>
</contexts>
<marker>Romano, Kouylekov, Szpektor, Dagan, Lavelli, 2006</marker>
<rawString>L. Romano, M. Kouylekov, I. Szpektor, I. Dagan, and A. Lavelli. 2006. Investigating a generic paraphrasebased approach for relation extraction. In In Proceedings of the European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rosenfeld</author>
<author>R Feldman</author>
</authors>
<title>Ures: an unsupervised web relation extraction system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>667--674</pages>
<contexts>
<context position="6425" citStr="Rosenfeld and Feldman (2006)" startWordPosition="1002" endWordPosition="1005">rpus feasible. Another task related to our work is relation extraction. Its aim is to extract instances of a given relation. Hearst (1992) the pioneering paper in the field used a small number of hand selected patterns to extract instances of hyponymy relation. Berland and Charniak (1999) used a similar method for extracting instances of meronymy relation. Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web. But their method often finds patterns that are too general (e.g., X and Y), resulting in low precision extractions. Rosenfeld and Feldman (2006) present a somewhat similar web based method that uses a combination of seed instances and seed patterns to learn good quality surface patterns. Both these methods differ from ours in that they learn relation patterns on the fly (from the web). Our method however, pre-computes paraphrases for a large set of surface patterns using distributional similarity over a large corpus and then obtains patterns for a relation by simply finding paraphrases (offline) for a few seed patterns. Using distributional similarity avoids the problem of obtaining overly general patterns and the pre-computation of p</context>
</contexts>
<marker>Rosenfeld, Feldman, 2006</marker>
<rawString>B. Rosenfeld and R. Feldman. 2006. Ures: an unsupervised web relation extraction system. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 667–674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
</authors>
<title>On-demand information extraction. In</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>731--738</pages>
<contexts>
<context position="7153" citStr="Sekine (2006)" startWordPosition="1121" endWordPosition="1122">ood quality surface patterns. Both these methods differ from ours in that they learn relation patterns on the fly (from the web). Our method however, pre-computes paraphrases for a large set of surface patterns using distributional similarity over a large corpus and then obtains patterns for a relation by simply finding paraphrases (offline) for a few seed patterns. Using distributional similarity avoids the problem of obtaining overly general patterns and the pre-computation of paraphrases means that we can obtain the set of patterns for any relation instantaneously. Romano et al. (2006) and Sekine (2006) used syntactic paraphrases to obtain patterns for extracting relations. While procedurally different, both methods depend heavily on the performance of the syntax parser and require complex syntax tree matching to extract the relation instances. Our method on the other hand acquires surface patterns and thus avoids the dependence on a parser and syntactic matching. This also makes the extraction process scalable. 3 Acquiring Paraphrases This section describes our model for acquiring paraphrases from text. 675 3.1 Distributional Similarity Harris’s distributional hypothesis (Harris, 1954) has </context>
</contexts>
<marker>Sekine, 2006</marker>
<rawString>S. Sekine. 2006. On-demand information extraction. In In Proceedings of COLING/ACL, pages 731–738.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Siegal</author>
<author>N J Castellan Jr</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<publisher>McGraw-Hill.</publisher>
<marker>Siegal, Jr, 1988</marker>
<rawString>S. Siegal and N.J. Castellan Jr. 1988. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>H Tanev</author>
<author>I Dagan</author>
<author>B Coppola</author>
</authors>
<title>Scaling web-based acquisition of entailment relations.</title>
<date>2004</date>
<booktitle>In In Proceedings of Empirical Methods in Natural Language Processing,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="5306" citStr="Szpektor et al. (2004)" startWordPosition="816" endWordPosition="819">acquisition is based on automatic acquisition. Barzilay and McKeown (2001) used a monolingual parallel corpus to obtain paraphrases. Bannard and Callison-Burch (2005) and Zhou et al. (2006) both employed a bilingual parallel corpus in which each foreign language word or phrase was a pivot to obtain source language paraphrases. Dolan et al. (2004) and Barzilay and Lee (2003) used comparable news articles to obtain sentence level paraphrases. All these approaches rely on the presence of parallel or comparable corpora and are thus limited by their availability and size. Lin and Pantel (2001) and Szpektor et al. (2004) proposed methods to obtain entailment templates by using a single monolingual resource. While both differ in their approaches, they both end up finding syntactic paraphrases. Their methods cannot be used if we cannot parse the data (either because of scale or data quality). Our approach on the other hand, finds surface paraphrases; it is more scalable and robust due to the use of simple POS tagging. Also, our use of locality sensitive hashing makes finding similar phrases in a large corpus feasible. Another task related to our work is relation extraction. Its aim is to extract instances of a </context>
<context position="19549" citStr="Szpektor et al. (2004)" startWordPosition="3264" endWordPosition="3267"> extract the sentences that contain the instances, and learn the set of baseline patterns for each relation. We then apply these patterns to the test corpus and extract the corresponding baseline instances. 6.2 Evaluation Criteria Here we present the evaluation criteria we used to evaluate the performance on the different tasks. 678 Paraphrases We estimate the quality of paraphrases by annotating a random sample as correct/incorrect and calculating the accuracy. However, estimating the recall is difficult given that we do not have a complete set of paraphrases for the input phrases. Following Szpektor et al. (2004), instead of measuring recall, we calculate the average number of correct paraphrases per input phrase. Surface Patterns We can calculate the precision (P) of learned patterns for each relation by annotating the extracted patterns as correct/incorrect. However calculating the recall is a problem for the same reason as above. But we can calculate the relative recall (RR) of the system against the baseline and vice versa. The relative recall RRS|B of system 5 with respect to system B can be calculated as: = ° CBB RRS|B where CS is the number of correct patterns found by our system and CB is the </context>
<context position="24065" citStr="Szpektor et al., 2004" startWordPosition="4024" endWordPosition="4027">27% 4.28 Table 1: Quality of paraphrases are being distributed to approved a revision to the have been distributed to unanimously approved a new are being handed out to approved an annual were distributed to will consider adopting a −are handing out approved a revised will be distributed to all approved a new Table 2: Example paraphrases 6.4 Result Summary Table 1 shows the results of annotating the paraphrases test set. We do not have a baseline to compare against but we can analyze them in light of numbers reported previously for syntactic paraphrases. DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al., 2004) report accuracies of 50.1% and 44.3% respectively compared to our average accuracy across two annotators of 70.79%. The average number of paraphrases per phrase is however 10.1 and 5.5 for DIRT and TEASE respectively compared to our 4.2. One reason why this number is lower is that our test set contains completely random phrases from our set (2.5 million phrases): some of these phrases are rare and have very few paraphrases. Table 2 shows some paraphrases generated by our system for the phrases “are being distributed to” and “approved a revision to the”. Table 3 shows the results on the qualit</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004. Scaling web-based acquisition of entailment relations. In In Proceedings of Empirical Methods in Natural Language Processing, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>C Y Lin</author>
<author>D Munteanu</author>
<author>E H Hovy</author>
</authors>
<title>Paraeval: using paraphrases to evaluate summaries automatically. In</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>447--454</pages>
<contexts>
<context position="4873" citStr="Zhou et al. (2006)" startWordPosition="744" endWordPosition="747">re templates for extracting information from text. For example, if one wanted to extract a list of company acquisitions, “(ACQUIRER) acquired (ACQUIREE)” would be one surface pattern with “(ACQUIRER)” and “(ACQUIREE)” as the slots to be extracted. Thus we can claim: Claim 2: These paraphrases can then be used for generating high precision surface patterns for relation extraction. 2 Related Work Most recent work in paraphrase acquisition is based on automatic acquisition. Barzilay and McKeown (2001) used a monolingual parallel corpus to obtain paraphrases. Bannard and Callison-Burch (2005) and Zhou et al. (2006) both employed a bilingual parallel corpus in which each foreign language word or phrase was a pivot to obtain source language paraphrases. Dolan et al. (2004) and Barzilay and Lee (2003) used comparable news articles to obtain sentence level paraphrases. All these approaches rely on the presence of parallel or comparable corpora and are thus limited by their availability and size. Lin and Pantel (2001) and Szpektor et al. (2004) proposed methods to obtain entailment templates by using a single monolingual resource. While both differ in their approaches, they both end up finding syntactic para</context>
</contexts>
<marker>Zhou, Lin, Munteanu, Hovy, 2006</marker>
<rawString>L. Zhou, C.Y. Lin, D. Munteanu, and E.H. Hovy. 2006. Paraeval: using paraphrases to evaluate summaries automatically. In In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 447–454.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>