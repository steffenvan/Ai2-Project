<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000827">
<title confidence="0.9987615">
A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and
Morphology Induction
</title>
<author confidence="0.986434">
Kairit Sirts
</author>
<affiliation confidence="0.9994595">
Institute of Cybernetics at
Tallinn University of Technology
</affiliation>
<email confidence="0.969119">
kairit.sirts@phon.ioc.ee
</email>
<author confidence="0.949781">
Tanel Alum¨ae
</author>
<affiliation confidence="0.999091">
Institute of Cybernetics at
Tallinn University of Technology
</affiliation>
<email confidence="0.986242">
tanel.alumae@phon.ioc.ee
</email>
<sectionHeader confidence="0.998615" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995046066666667">
In this paper we present a fully unsupervised
nonparametric Bayesian model that jointly in-
duces POS tags and morphological segmen-
tations. The model is essentially an infi-
nite HMM that infers the number of states
from data. Incorporating segmentation into
the same model provides the morphological
features to the system and eliminates the need
to find them during preprocessing step. We
show that learning both tasks jointly actually
leads to better results than learning either task
with gold standard data from the other task
provided. The evaluation on multilingual data
shows that the model produces state-of-the-art
results on POS induction.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99815104">
Nonparametric Bayesian modeling has recently be-
come very popular in natural language processing
(NLP), mostly because of its ability to provide pri-
ors that are especially suitable for tasks in NLP (Teh,
2006). Using nonparametric priors enables to treat
the size of the model as a random variable with its
value to be induced during inference which makes
its use very appealing in models that need to decide
upon the number of states.
The task of unsupervised parts-of-speech (POS)
tagging has been under research in numerous pa-
pers, for overview see (Christodoulopoulos et al.,
2010). Most of the POS induction models use the
structure of hidden Markov model (HMM) (Rabiner,
1989) that requires the knowledge about the num-
ber of hidden states (corresponding to the number
of tags) in advance. According to our consider-
ations, supplying this information is not desirable
for two opposing reasons: 1) it injects into the sys-
tem a piece of knowledge which in a truly unsu-
pervised setting would be unavailable; and 2) the
number of POS tags used is somewhat arbitrary any-
way because there is no common consensus of what
should be the true number of tags in each language
and therefore it seems unreasonable to constrain the
model with such a number instead of learning it from
the data.
Unsupervised morphology learning is another
popular task that has been extensively studied by
many authors. Here we are interested in learning
concatenative morphology of words, meaning the
substrings of the word corresponding to morphemes
that, when concatenated, will give the lexical repre-
sentation of the word type. For the rest of the paper
we will refer to this task as (morphological) segmen-
tation.
Several unsupervised POS induction systems
make use of morphological features (Blunsom and
Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et
al., 2010; Clark, 2003; Christodoulopoulos et al.,
2011) and this approach has been empirically proved
to be helpful (Christodoulopoulos et al., 2010). In a
similar fashion one could think that knowing POS
tags could be useful for learning morphological seg-
mentations and in this paper we will study this hy-
pothesis.
In this paper we will build a model that combines
POS induction and morphological segmentation into
one learning problem. We will show that the unsu-
pervised learning of both of these tasks in the same
</bodyText>
<page confidence="0.655328333333333">
407
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 407–416,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999961666666667">
model will lead to better results than learning both
tasks separately with the gold standard data of the
other task provided. We will also demonstrate that
our model produces state-of-the-art results on POS
tagging. As opposed to the compared methods, our
model also induces the number of tags from data.
In the following, section 2 gives the overview
of the Dirichlet Processes, section 3 describes the
model setup followed by the description of infer-
ence procedures in section 4, experimental results
are presented in section 5, section 6 summarizes the
previous work and last section concludes the paper.
</bodyText>
<sectionHeader confidence="0.99868" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.879692">
2.1 Dirichlet Process
</subsectionHeader>
<bodyText confidence="0.999956">
Let H be a distribution called base measure. Dirich-
let process (DP) (Ferguson, 1973) is a probability
distribution over distributions whose support is the
subset of the support of H:
</bodyText>
<equation confidence="0.965834">
G ∼ DP(α, H), (1)
</equation>
<bodyText confidence="0.999918">
where α is the concentration parameter that controls
the number of values instantiated by G.
DP has no analytic form and therefore other rep-
resentations must be developed for sampling. In the
next section we describe Chinese Restaurant Process
that enables to obtain samples from DP.
</bodyText>
<subsectionHeader confidence="0.997775">
2.2 Chinese Restaurant Process
</subsectionHeader>
<bodyText confidence="0.999669210526316">
Chinese Restaurant Process (CRP) (Aldous, 1985)
enables to calculate the marginal probabilities of the
elements conditioned on the values given to all pre-
viously seen items and integrating over possible DP
prior values.
Imagine an infinitely big Chinese restaurant with
infinitely many tables with each table having ca-
pacity for infinitely many customers. In the begin-
ning the restaurant is empty. Then customers, corre-
sponding to data points, start entering one after an-
other. The first customer chooses an empty table to
sit at. Next customers choose a new table with prob-
ability proportional to the concentration parameter
α or sit into one of the already occupied tables with
probability proportional to the number of customers
already sitting there. Whenever a customer chooses
an empty table, he will also pick a dish from H to
be served on that table. The predictive probability
distribution over dishes for the i-th customer is:
</bodyText>
<equation confidence="0.967753">
nφk + α
P(xi = Ok|x−i, α, H) = i 1+ α pH (Ok), (2)
—
</equation>
<bodyText confidence="0.99990975">
where x−i is the seating arrangement of customers
excluding the i-th customer and nφk is the number of
customers eating dish Ok and pH(·) is the probability
according to H.
</bodyText>
<subsectionHeader confidence="0.999558">
2.3 Hierarchical Dirichlet Process
</subsectionHeader>
<bodyText confidence="0.999385">
The notion of hierarchical Dirichlet Process (HDP)
(Teh et al., 2006) can be derived by letting the base
measure itself to be a draw from a DP:
</bodyText>
<equation confidence="0.9981135">
G0|α0, H ∼ DP(α0, H) (3)
Gj|α, G0 ∼ DP(α, G0) j = 1··· J (4)
</equation>
<bodyText confidence="0.996208555555556">
Under HDP, CRP becomes Chinese Restaurant
Franchise (Teh et al., 2006) with several restaurants
sharing the same franchise-wide menu G0. When a
customer sits at an empty table in one of the Gj-th
restaurants, the event of a new customer entering the
restaurant G0 will be triggered. Analogously, when
a table becomes empty in one of the Gj-th restau-
rants, it causes one of the customers leaving from
restaurant G0.
</bodyText>
<sectionHeader confidence="0.994966" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.961421833333334">
We consider the problem of unsupervised learning
of POS tags and morphological segmentations in a
joint model. Similarly to some recent successful at-
tempts (Lee et al., 2010; Christodoulopoulos et al.,
2011; Blunsom and Cohn, 2011), our model is type-
based, arranging word types into hard clusters. Un-
like many recent POS tagging models, our model
does not assume any prior information about the
number of POS tags. We will define the model as
a generative sequence model using the HMM struc-
ture. Graphical depiction of the model is given in
Figure 1.
</bodyText>
<subsectionHeader confidence="0.997651">
3.1 Generative story
</subsectionHeader>
<bodyText confidence="0.9998485">
We assume the presence of a fixed length vocabu-
lary W. The process starts with generating the lex-
icon that stores for each word type its POS tag and
morphological segmentation.
</bodyText>
<page confidence="0.995076">
408
</page>
<listItem confidence="0.9795357">
• Draw a unigram tag distribution from the re-
spective DP;
• Draw a segment distribution from the respec-
tive DP;
• For each tag, draw a tag-specific segment distri-
bution from HDP with the segment distribution
as base measure;
• For each word type, draw a tag from the uni-
gram tag distribution;
• For each word type, draw a segmentation from
the respective tag-specific segment distribution.
Next we proceed to generate the HMM parame-
ters:
• For each tag, draw a bigram distribution from
HDP with the unigram tag distribution as base
measure;
• For each tag bigram, draw a trigram distribu-
tion from HDP with the respective bigram dis-
tribution as base measure;
• For each tag, draw a Dirichlet concentration pa-
rameter from Gamma distribution and an emis-
sion distribution from the symmetric Dirichlet.
Finally the standard HMM procedure for generat-
ing the data sequence follows. At each time step:
• Generate the next tag conditioned on the last
two tags from the respective trigram HDP;
• Generate the word from the respective emission
distribution conditioned on the tag just drawn;
• Generate the segmentation of the word deter-
ministically by looking it up from the lexicon.
</listItem>
<subsectionHeader confidence="0.859123">
3.2 Model setup
</subsectionHeader>
<bodyText confidence="0.625135">
The trigram transition hierarchy is a HDP:
GU ∼ DP(αU, H)
GBj ∼ DP(αB, GU) j = 1 ··· ∞
GTjk ∼ DP(αT, GBj ) j, k = 1··· ∞,
where GU, GB and GT denote the unigram, bigram
and trigram context DP-s respectively, α-s are the
</bodyText>
<figureCaption confidence="0.996725125">
Figure 1: Plate diagram representation of the model. ti-
s, wi-s and si-s denote the tags, words and segmentations
respectively. G-s are various DP-s in the model, Ej-s and
Oj-s are the tag-specific emission distributions and their
respective Dirichlet prior parameters. H is Gamma base
distribution. S is the base distribution over segments.
Coupled DP concetrations parameters have been omitted
for clarity.
</figureCaption>
<bodyText confidence="0.9931254">
respective concentration parameters coupled for DP-
s of the same hierarchy level. Emission parame-
ters are drawn from multinomials with symmetric
Dirichlet priors:
JEj|�j, H ∼ Mult(B)Dir(Qj)dO j = 1 · · · ∞,
where each emission distribution has its own Dirich-
let concentration parameter Qj drawn from H.
Morphological segments are modelled with an-
other HDP where the groups are formed on the basis
of tags:
</bodyText>
<equation confidence="0.983853666666667">
GS ∼ DP(αS, S) (9)
GTS ∼ DP(αT S, GS) j = 1 ··· ∞, (10)
j
</equation>
<bodyText confidence="0.999355333333333">
where GTS jare the tag-specific segment DP-s and
GS is their common base distribution with S as base
measure over all possible strings. S consists of two
components: a geometric distribution over the seg-
ment lengths and collapsed Dirichlet-multinomial
over character unigrams.
</bodyText>
<sectionHeader confidence="0.99986" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.999810666666667">
We implemented Gibbs sampler to draw new val-
ues for tags and Metropolis-Hastings sampler for re-
sampling segmentations. We use a type-based col-
</bodyText>
<figure confidence="0.950059266666667">
W1 W2 W3
S1 S2 S3
Li Lp L3
���
���
���
��� �� ��
� �
��
�� GS S
E; ;
k=1...
������
������
H
</figure>
<page confidence="0.996271">
409
</page>
<bodyText confidence="0.9978166">
lapsed sampler that draws the tagging and segmen-
tation values for all tokens of a word type in one step
and integrates out the random DP measures by using
the CRP representation. The whole procedure alter-
nates between three sampling steps:
</bodyText>
<listItem confidence="0.999918666666667">
• Sampling new tag value for each word type;
• Resampling the segmentation for each type;
• Sampling new values for all parameters.
</listItem>
<subsectionHeader confidence="0.99863">
4.1 Tag sampling
</subsectionHeader>
<bodyText confidence="0.995526916666667">
The tags will be sampled from the posterior:
P(T|W, S, w, O), (11)
where W is the set of words in the vocabulary, T
and S are tags and segmentations assigned to each
word type, w is the actual word sequence, and O de-
notes the set of all parameters relevant for tag sam-
pling. For brevity, we will omit O notation in the
formulas below. For a single word type, this poste-
rior can be factored as follows:
The word type likelihood is calculated accord-
ing to the collapsed Dirichlet-multinomial likeli-
hood formula:
</bodyText>
<equation confidence="0.877087666666667">
ntWi + j + α
nt· + j + αN
(14)
</equation>
<bodyText confidence="0.999959125">
where ntWi is the number of times the word Wi has
been tagged with tag t so far, nt· is the number of
total word tokens tagged with the tag t and N is the
total number of words in the vocabulary.
The last factor is the word sequence likelihood
and covers the transition probabilities. Relevant tri-
grams are those three containing the current word,
and in all contexts where the word token appears in:
</bodyText>
<equation confidence="0.999832">
P(w|Ti = t, T−i, W) —
ri P(t|t(c−2), t(c−1))&apos;
c∈CWi (15)
P(t(c+1)|t(c−1), t)&apos;
P(t(c+2)|t, t(c+1))
P(Wi|Ti = t, T−i, W−i, w) = |Wi|−1
H
j=0
P(Ti = t|T−i, S, W, w) —
P(Si|Ti = t,T−i,S−i)x
P(Wi|Ti = t, T−i, W−i)x
P(w|Ti = t, T−i, W),
</equation>
<bodyText confidence="0.999945125">
where CWi denotes all the contexts where the word
(12) type Wi appears in, t(c) are the tags assigned to the
context words. All these terms can be calculated
with CRP formulas.
where —i in the subscript denotes the observations
with the i-th word type excluded.
The first term is the segmentation likelihood and
can be computed according to the CRP formula:
</bodyText>
<equation confidence="0.999502">
P(Si|Ti = t,T−i, S−i) =
α(ms Si + QP0(s))
(nt. Si + α) (m−Si + Q) ) ,
(13)
</equation>
<bodyText confidence="0.9999813">
where the outer product is over the word type count,
nts and ms denote the number of customers “eat-
ing” the segment s under tag t and the number of
tables “serving” the segment s across all restaurants
respectively, dot represents the marginal counts and
α and Q are the concentration parameters of the re-
spective DP-s. —Si in upper index means that the
segments belonging to the segmentation of the i-th
word type and not calculated into likelihood term yet
have been excluded.
</bodyText>
<subsectionHeader confidence="0.999095">
4.2 Segmentation sampling
</subsectionHeader>
<bodyText confidence="0.9987359">
We sample the whole segmentation of a word type
as a block with forward-filtering backward-sampling
scheme as described in (Mochihashi et al., 2009).
As we cannot sample from the exact marginal
conditional distribution due to the dependen-
cies between segments induced by the CRP, we
use the Metropolis-Hastings sampler that draws
a new proposal with forward-filtering backward-
sampling scheme and accepts it with probability
min(1, ((Ssrop) ), where Sprop is the proposed seg-
</bodyText>
<equation confidence="0.464986">
old)
</equation>
<bodyText confidence="0.99478925">
mentation and Sold is the current segmentation of a
word type. The acceptance rate during experiments
varied between 94-98%.
For each word type, we build a forward filter-
ing table where we maintain the forward variables
α[t][k] that present the probabilities of the last k
characters of a t-character string constituting a seg-
ment. Define:
</bodyText>
<equation confidence="0.990312692307692">
α[0][0] = 1 (16)
+
−Si
nts
H
s∈Si
n−Si
t· + α
|Wi|
H
j=1
410
α[t][0] = 0, t &gt; 0 (17)
</equation>
<bodyText confidence="0.838315">
Then the forward variables can be computed recur-
sively by using dynamic programming algorithm:
</bodyText>
<equation confidence="0.886538333333333">
α[t][k] = p(ctt−k) �t−k α[t − k][j], t = 1···L,
j=0
(18)
</equation>
<bodyText confidence="0.999982153846154">
where cm denotes the characters cm · · · c,,, of a string
c and L is the length of the word.
Sampling starts from the end of the word because
it is known for certain that the word end coincides
with the end of a segment. We sample the begin-
ning position k of the last segment from the forward
variables α[t][k], where t is the length of the word.
Then we set t = t − k and continue to sample the
start of the previous to the last segment. This pro-
cess continues until t = 0. The segment probabili-
ties, conditioned on the tag currently assigned to the
word type, will be calculated according to the seg-
mentation likelihood formula (13).
</bodyText>
<subsectionHeader confidence="0.999102">
4.3 Hyperparameter sampling
</subsectionHeader>
<bodyText confidence="0.999941666666667">
All DP and Dirichlet concentration parameters are
given vague Gamma(10, 0.1) priors and new values
are sampled by using the auxiliary variable sampling
scheme described in (Escobar and West, 1995) and
the extended version for HDP-s described in (Teh
et al., 2006). The segment length control parame-
ter is given uniform Beta prior and its new values
are sampled from the posterior which is also a Beta
distribution.
</bodyText>
<sectionHeader confidence="0.999994" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.994537">
5.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.998647555555556">
We test the POS induction part of the model on
all languages in the Multext-East corpora (Erjavec,
2010) as well as on the free corpora from CONLL-
X Shared Task1 for Dutch, Danish, Swedish and
Portuguese. The evaluation of morphological seg-
mentations is based on the Morpho Challenge gold
segmented wordlists for English, Finnish and Turk-
ish2. We gathered the sentences from Europarl cor-
pus3 for English and Finnish, and use the Turkish
</bodyText>
<footnote confidence="0.99992925">
1http://ilk.uvt.nl/conll/free_data.html
2http://research.ics.tkk.fi/events/
morphochallenge2010/datasets.shtml
3http://www.statmt.org/europarl/
</footnote>
<bodyText confidence="0.999080545454545">
text data from the Morpho Challenge 20094. Es-
tonian gold standard segmentations have been ob-
tained from the Estonian morphologically annotated
corpus5.
We report three accuracy measures for tagging:
greedy one-to-one mapping (1-1) (Haghighi and
Klein, 2006), many-to-one mapping (m-1) and V-
measure (V-m) (Rosenberg and Hirschberg, 2007).
Segmentation is evaluated on the basis of standard
F-score which is the harmonic mean of precision and
recall.
</bodyText>
<subsectionHeader confidence="0.997719">
5.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.999861387096774">
For each experiment, we made five runs with ran-
dom initializations and report the results of the me-
dian. The sampler was run 200 iterations for burnin,
after which we collected 5 samples, letting the sam-
pler to run for another 200 iterations between each
two sample. We start with 15 segmenting iterations
during each Gibbs iteration to enable the segmenta-
tion sampler to burnin to the current tagging state,
and gradually reduce this number to one. Segmenta-
tion likelihood term for tagging is calculated on the
basis of the last segment only because this setting
gave the best results in preliminary experiments and
it also makes the whole computation less expensive.
The first set of experiments was conducted to test
the model tagging accuracy on different languages
mentioned above. The results obtained were in gen-
eral slightly lower than the current state-of-the-art
and the number of tags learned was generally bigger
than the number of gold standard tags. We observed
that different components making up the corpus log-
arithmic probability have different magnitudes. In
particular, we found that the emission probability
component in log-scale is roughly four times smaller
than the transition probability. This observation mo-
tivated introducing the likelihood scaling heuristic
into the model to scale the emission probability up.
We tried a couple of different scaling factors on
Multext-East English corpus and then set its value
to 4 for all languages for the rest of the experi-
ments. This improved the tagging results consis-
tently across all languages.
</bodyText>
<footnote confidence="0.9975265">
4http://research.ics.tkk.fi/events/
morphochallenge2009/datasets.shtml
5http://www.cl.ut.ee/korpused/
morfkorpus/index.php?lang=eng
</footnote>
<page confidence="0.99486">
411
</page>
<bodyText confidence="0.999943416666667">
POS induction results are given in Table 1. When
comparing these results with the recently published
results on the same corpora (Christodoulopoulos et
al., 2011; Blunsom and Cohn, 2011; Lee et al.,
2010) we can see that our results compare favorably
with the state-of-the-art, resulting with the best pub-
lished results in many occasions. The number of tag
clusters learned by the model corresponds surpris-
ingly well to the number of true coarse-grained gold
standard tags across all languages. There are two
things to note here: 1) the tag distributions learned
are influenced by the likelihood scaling heuristic and
more experiments are needed in order to fully under-
stand the characteristics and influence of this heuris-
tic; 2) as the model is learning the coarse-grained
tagset consistently in all languages, it might as well
be that the POS tags are not as dependent on the mor-
phology as we assumed, especially in inflectional
languages with many derivational and inflectional
suffixes, because otherwise the model should have
learned a more fine-grained tagset.
Segmentation results are presented in Table 2.
For each language, we report the lexicon-based pre-
cision, recall and F-measure, the number of word
types in the corpus and and number of word types
with gold segmentation available. The reported stan-
dard deviations show that the segmentations ob-
tained are stable across different runs which is prob-
ably due to the blocked sampler. We give the seg-
mentation results both with and without likelihood
scaling heuristic and denote that while the emission
likelihood scaling improves the tagging accuracy, it
actually degrades the segmentation results.
It can also be seen that in general precision score
is better but for Estonian recall is higher. This can
be explained by the characteristics of the evalua-
tion data sets. For English, Finnish and Turkish we
use the Morpho Challenge wordlists where the gold
standard segmentations are fine-grained, separating
both inflectional and derivational morphemes. Espe-
cially derivational morphemes are hard to learn with
pure data-driven methods with no knowledge about
semantics and thus it can result in undersegmenta-
tion. On the other hand, Estonian corpus separates
only inflectional morphemes which thus leads to
higher recall. Some difference can also come from
the fact that the sets of gold-segmented word types
for other languages are much smaller than in Esto-
</bodyText>
<figure confidence="0.9492105">
0 100 200 300 400 500 600 700 800 900 1000
Iteration
</figure>
<figureCaption confidence="0.999504333333333">
Figure 2: Log-likelihood of samples plotted against iter-
ations. Dark lines show the average over five runs, grey
lines in the back show the real samples.
</figureCaption>
<bodyText confidence="0.9999723125">
nian and thus it would be interesting to see whether
and how the results would change if the evaluation
could be done on all word types in the corpus for
other languages as well. In general, undersegmen-
tation is more acceptable than oversegmentation, es-
pecially when the aim is to use the resulting segmen-
tations in some NLP application.
Next, we studied the convergence characteristics
of our model. For these experiments we made five
runs with random initializations on Estonian cor-
pus and let the sampler run up to 1100 iterations.
Samples were taken after each ten iterations. Fig-
ure 2 shows the log-likelihood of the samples plot-
ted against iteration number. Dark lines show the
averages over five runs and gray lines in the back-
ground are the likelihoods of real samples showing
also the variance. We first calculated the full like-
lihood of the samples (the solid line) that showed
a quick improvement during the first few iterations
and then stabilized by continuing with only slow im-
provements over time. We then divided the full like-
lihood into two factors in order to see the contribu-
tion of both tagging and segmentation parts sepa-
rately. The results are quite surprising. It turned
out that the random tagging initializations are very
good in terms of probability and as a matter of fact
much better than the data can support and thus the
tagging likelihood drops quite significantly after the
first iteration and then continues with very slow im-
provements. The matters are totally different with
segmentations where the initial random segmenta-
tions result in a low likelihood that improves heavily
</bodyText>
<figure confidence="0.967894071428571">
24
22
20
18
16
14
12
10
8
6
POS tagging
Segmentation
Joint
−105 x log(P)
</figure>
<page confidence="0.994353">
412
</page>
<table confidence="0.9997903125">
Types 1-1 m-1 V-m Induced True Best Pub.
Bulgarian 15103 50.3 (0.9) 71.9 (3.8) 54.9 (2.2) 13 (1.6) 12 - 66.5∗ 55.6∗
Czech 17607 46.0 (1.0) 60.7 (1.6) 46.2 (0.7) 12 (0.8) 12 - 64.2∗ 53.9∗
Danish 17157 53.2 (0.2) 69.5 (0.1) 52.7 (0.4) 14 (0.0) 25 43.2† 76.2* 59.0∗
Dutch 27313 60.5 (1.9) 74.0 (1.6) 59.1 (1.1) 22 (0.0) 13 55.1† 71.1∗ 54.7∗
English 9196 67.4 (0.1) 79.8 (0.1) 66.7 (0.1 13 (0.0) 12 - 73.3∗ 63.3∗
Estonian 16820 47.6 (0.9) 64.5 (1.9) 45.6 (1.4) 14 (0.5) 11 - 64.4∗ 53.3∗
Farsi 11319 54.9 (0.1) 65.3 (0.1) 52.1 (0.1) 13 (0.5) 12 - - -
Hungarian 19191 62.1 (0.7) 71.4 (0.3) 56.0 (0.6) 11 (0.9) 12 - 68.2∗ 54.8∗
Polish 19542 48.5 (1.8) 59.6 (1.9) 45.4 (1.0) 13 (0.8) 12 - - -
Portuguese 27250 45.4 (1.1) 71.3 (0.3) 55.4 (0.3) 21 (1.1) 16 56.5† 78.5* 63.9∗
Romanian 13822 44.3 (0.5) 60.5 (1.7) 46.7 (0.5) 14 (0.8) 14 - 61.1∗ 52.3∗
Serbian 16813 40.1 (0.2) 60.1 (0.2) 43.5 (0.2) 13 (0.0) 12 - 64.1∗ 51.1∗
Slovak 18793 44.1 (1.5) 56.2 (0.8) 41.2 (0.6) 14 (1.1) 12 - - -
Slovene 16420 51.6 (1.5) 66.8 (0.6) 51.6 (1.0) 12 (0.7) 12 - 67.9∗ 56.7∗
Swedish 18473 50.6 (0.1) 60.3 (0.1) 55.8 (0.1) 17 (0.0) 41 38.5† 68.7∗ 58.9∗
</table>
<tableCaption confidence="0.9517654">
Table 1: Tagging results for different languages. For each language we report median one-to-one (1-1), many-to-one
(m-1) and V-measure (V-m) together with standard deviation from five runs where median is taken over V-measure.
Types is the number of word types in each corpus, True is the number of gold tags and Induced reports the median
number of tags induced by the model together with standard deviation. Best Pub. lists the best published results so far
(also 1-1, m-1 and V-m) in (Christodoulopoulos et al., 2011)∗, (Blunsom and Cohn, 2011)* and (Lee et al., 2010)†.
</tableCaption>
<table confidence="0.999990111111111">
Precision Recall F1 Types Segmented
Estonian without LLS 43.5 (0.8) 59.4 (0.6) 50.3 (0.7) 16820 16820
with LLS 42.8 (1.1) 54.6 (0.7) 48.0 (0.9)
English without LLS 69.0 (1.3) 37.3 (1.5) 48.5 (1.1) 20628 399
with LLS 59.8 (1.8) 29.0 (1.0) 39.1 (1.3)
Finnish without LLS 56.2 (2.5) 29.5 (1.7) 38.7 (2.0) 25364 292
with LLS 56.0 (1.1) 28.0 (0.6) 37.4 (0.7)
Turkish without LLS 65.4 (1.8) 44.8 (1.8) 53.2 (1.7) 18459 293
with LLS 68.9 (0.8) 39.2 (1.0) 50.0 (0.6)
</table>
<tableCaption confidence="0.9703575">
Table 2: Segmentation results on different languages. Results are calculated based on word types. For each language
we report precision, recall and F1 measure, number of word types in the corpus and number of word types with gold
standard segmentation available. For each language we report the segmentation result without and with emission
likelihood scaling (without LLS and with LLS respectively).
</tableCaption>
<bodyText confidence="0.998844230769231">
with the first few iterations and then stabilizes but
still continues to improve over time. The explana-
tion for this kind of model behaviour needs further
studies and we leave it for future work.
Figure 3 plots the V-measure against the tagging
factor of the log-likelihood for all samples. It can
be seen that the lower V-measure values are more
spread out in terms of likelihood. These points cor-
respond to the early samples of the runs. The sam-
ples taken later during the runs are on the right in
the figure and the positive correlation between the
V-measure and likelihood values can be seen.
Next we studied whether the morphological seg-
</bodyText>
<figure confidence="0.969823785714286">
10.9
10.85
10.8
10.75
10.7
10.65
10.6
10.55
10.5
10.45
10.4
10.35
0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
V-meas��e
</figure>
<figureCaption confidence="0.9677635">
Figure 3: Tagging part of log-likelihood plotted against
V-measure
</figureCaption>
<figure confidence="0.683684">
−105 x log(p)
</figure>
<page confidence="0.980009">
413
</page>
<table confidence="0.998375142857143">
1-to-1 m-to-1 V-m
Fixed seg 40.5 (1.5) 53.4 (1.0) 37.5 (1.3)
Learned seg 47.6 (0.4) 64.5 (1.9) 45.6 (1.4)
Precision Recall F1
Fixed tag 36.7 (0.3) 56.4 (0.2) 44.5 (0.3)
Learned tag 42.8 (1.1) 54.6 (0.7) 48.0 (0.9)
Morfessor 51.29 52.59 51.94
</table>
<tableCaption confidence="0.991349142857143">
Table 3: Tagging and segmentation results on Estonian
Multext-East corpus (Learned seg and Learned tag) com-
pared to the semisupervised setting where segmentations
are fixed to gold standard (Fixed seg) and tags are fixed
to gold standard (Fixed tag). Finally the segmentatation
results from Morfessor system for comparison are pre-
sented.
</tableCaption>
<bodyText confidence="0.995160064516129">
mentations and POS tags help each other in the
learning process. For that we conducted two semisu-
pervised experiments on Estonian corpus. First we
provided gold standard segmentations to the model
and let it only learn the tags. Then, we gave the
model gold standard POS tags and only learned the
segmentations. The results are given in Table 3.
We also added the results from joint unusupervised
learning for easier comparison. Unfortunately we
cannot repeat this experiment on other languages
to see whether the results are stable across differ-
ent languages because to our knowledge there is no
other free corpus with both gold standard POS tags
and morphological segmentations available.
From the results it can be seen that the unsu-
pervised learning results for both tagging and seg-
mentation are better than the results obtained from
semisupervised learning. This is surprising because
one would assume that providing gold standard data
would lead to better results. On the other hand, these
results are encouraging, showing that learning two
dependent tasks in a joint model by unsupervised
manner can be as good or even better than learn-
ing the same tasks separately and providing the gold
standard data as features.
Finally, we learned the morphological segmen-
tations with the state-of-the-art morphology induc-
tion system Morfessor baseline6 (Creutz and Lagus,
2005) and report the best results in the last row of
Table 3. Apparently, our joint model cannot beat
Morfessor in morphological segmentation and when
</bodyText>
<footnote confidence="0.881461">
6http://www.cis.hut.fi/projects/morpho/
</footnote>
<bodyText confidence="0.999290625">
using the emission likelihood scaling that influences
the tagging results favorably, the segmentation re-
sults get even worse. Altough the semisupervised
experiments showed that there are dependencies be-
tween tags and segmentations, the conducted exper-
iments do not reveal of how to use these dependen-
cies for helping the POS tags to learn better morpho-
logical segmentations.
</bodyText>
<sectionHeader confidence="0.99992" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999969891891892">
We will review some of the recent works related
to Bayesian POS induction and morphological seg-
mentation.
One of the first Bayesian POS taggers is described
in (Goldwater and Griffiths, 2007). The model pre-
sented is a classical HMM with multinomial transi-
tion and emission distributions with Dirichlet priors.
Inference is done using a collapsed Gibbs sampler
and concentration parameter values are learned dur-
ing inference. The model is token-based, allowing
different words of the same type in different loca-
tions to have a different tag. This model can actu-
ally be classified as semi-supervised as it assumes
the presence of a tagging dictionary that contains
the list of possible POS tags for each word type -
an assumption that is clearly not realistic in an unsu-
pervised setting.
Models presented in (Christodoulopoulos et al.,
2011) and (Lee et al., 2010) are also built on
Dirichlet-multinomials and, rather than defining a
sequence model, present a clustering model based
on features. Both report good results on type basis
and use (among others) also morphological features,
with (Lee et al., 2010) making use of fixed length
suffixes and (Christodoulopoulos et al., 2011) using
the suffixes obtained from an unsupervised morphol-
ogy induction system.
Nonparametric Bayesian POS induction has been
studied in (Blunsom and Cohn, 2011) and (Gael et
al., 2009). The model in (Blunsom and Cohn, 2011)
uses Pitman-Yor Process (PYP) prior but the model
itself is finite in the sense that the size of the tagset is
fixed. Their model also captures morphological reg-
ularities by modeling the generation of words with
character n-grams. The model in (Gael et al., 2009)
uses infinite state space with Dirichlet Process prior.
The model structure is classical HMM consisting
</bodyText>
<page confidence="0.996702">
414
</page>
<bodyText confidence="0.9999476">
only of transitions and emissions and containing no
morphological features. Inference is done by us-
ing beam sampler introduced in (Gael et al., 2008)
which enables parallelized implementation.
One close model for morphology stems from
Bayesian word segmentation (Goldwater et al.,
2009) where the task is to induce word borders from
transcribed sentences. Our segmentation model is in
principle the same as the unigram word segmenta-
tion model and the main difference is that we are us-
ing blocked sampler while (Goldwater et al., 2009)
uses point-wise Gibbs sampler by drawing the pres-
ence or absence of the word border between every
two characters.
In (Goldwater et al., 2006) the morphology is
learned in the adaptor grammar framework (John-
son et al., 2006) by using a PYP adaptor. PYP adap-
tor caches the numbers of observed derivation trees
and forces the distribution over all possible trees to
take the shape of power law. In the PYP (and also
DP) case the adaptor grammar can be interpreted as
PYP (or DP) model with regular PCFG distribution
as base measure.
The model proposed in (Goldwater et al., 2006)
makes several assumptions that we do not: 1) seg-
mentations have a fixed structure of stem and suffix;
and 2) there is a fixed number of inflectional classes.
Inference is performed with Gibbs sampler by sam-
pling for each word its stem, suffix and inflectional
class.
</bodyText>
<sectionHeader confidence="0.999182" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999602029411765">
In this paper we presented a joint unsupervised
model for learning POS tags and morphological
segmentations with hierarchical Dirichlet Process
model. Our model induces the number of POS clus-
ters from data and does not contain any hand-tuned
parameters. We tested the model on many languages
and showed that by introcing a likelihood scaling
heuristic it produces state-of-the-art POS induction
results. We believe that the tagging results could
further be improved by adding additional features
concerning punctuation, capitalization etc. which
are heavily used in the other state-of-the-art POS in-
duction systems but these features were intentionally
left out in the current model for enabling to test the
concept of joint modelling of two dependent tasks.
We found some evidence that the tasks of POS
induction and morphological segmentation are de-
pendent by conducting semisupervised experiments
where we gave the model gold standard tags and seg-
mentations in turn and let it learn only segmentations
or tags respectively and found that the results in fully
unsupervised setting are better. Despite of that, the
model failed to learn as good segmentations as the
state-of-the-art morphological segmentation model
Morfessor. One way to improve the segmentation
results could be to use segment bigrams instead of
unigrams.
The model can serve as a basis for several further
extensions. For example, one possibility would be
to expand it into multilingual setting in a fashion of
(Naseem et al., 2009), or it could be extended to add
the joint learning of morphological paradigms of the
words given their tags and segmentations in a man-
ner described by (Dreyer and Eisner, 2011).
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998131714285714">
We would like to thank the anonymous reviewers
who helped to improve the quality of this paper.
This research was supported by the Estonian Min-
istry of Education and Research target-financed re-
search theme no. 0140007s12, and by European So-
cial Funds Doctoral Studies and Internationalisation
Programme DoRa.
</bodyText>
<sectionHeader confidence="0.999087" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998448222222222">
D. Aldous. 1985. Exchangeability and related topics.
In ´Ecole d’´et´e de Probabilit´es de Saint-Flour, XIII—
1983, pages 1–198. Springer.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless unsu-
pervised learning with features. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 582–590.
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
Pitman-Yor process HMM for unsupervised Part of
Speech induction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, pages
865–874.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
</reference>
<page confidence="0.987608">
415
</page>
<reference confidence="0.999397682692307">
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 575–584.
Christos Christodoulopoulos, Sharo Goldwater, and
Mark Steedman. 2011. A Bayesian mixture model
for PoS induction using multiple features. In Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 638–647, Edin-
burgh, Scotland, UK.
Alexander Clark. 2003. Combining distributional and
morphological information for Part of Speech induc-
tion. In Proceedings of the Tenth Conference on Eu-
ropean Chapter of the Association for Computational
Linguistics - Volume 1, pages 59–66.
Mathias Creutz and Krista Lagus. 2005. Inducing
the morphological lexicon of a natural language from
unannotated text. In In Proceedings of the Inter-
national and Interdisciplinary Conference on Adap-
tive Knowledge Representation and Reasoning, pages
106–113.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using a
Dirichlet Process mixture model. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 616–627.
Toma Erjavec. 2010. MULTEXT-East version 4: Mul-
tilingual morphosyntactic specifications, lexicons and
corpora. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation.
Michael D. Escobar and Mike West. 1995. Bayesian
density estimation and inference using mixtures. Jour-
nal of the American Statistical Association, 90(430).
Thomas S. Ferguson. 1973. A Bayesian analysis of
some nonparametric problems. The Annals of Statis-
tics, 1(2):209–230.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for the
infinite Hidden Markov Model. In Proceedings of the
25th International Conference on Machine Learning,
pages 1088–1095.
Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahra-
mani. 2009. The infinite HMM for unsupervised PoS
tagging. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 2 - Volume 2, pages 678–687.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised Part-of-Speech
tagging. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
744–751, Prague, Czech Republic.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens by
estimating power-law generators. In Advances in Neu-
ral Information Processing Systems 18, Cambridge,
MA.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112:21–54.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320–327, New York
City, USA.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2006. Adaptor grammars: A framework for speci-
fying compositional nonparametric Bayesian models.
In Advances in Neural Information Processing Sys-
tems 19, pages 641–648.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised POS tagging.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 853–
861.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP:
Volume 1 - Volume 1, pages 100–108.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. Journal ofAr-
tificial Intelligence Research, 36:1–45.
Lawrence R. Rabiner. 1989. A tutorial on Hidden
Markov Models and selected applications in speech
recognition. In Proceedings of the IEEE, pages 257–
286.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 410–420,
Prague, Czech Republic.
Yee Whye Teh, Michel I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceedings
of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 985–992.
</reference>
<page confidence="0.998598">
416
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.191919">
<title confidence="0.996727">A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction</title>
<author confidence="0.773836">Kairit</author>
<affiliation confidence="0.99645">Institute of Cybernetics Tallinn University of</affiliation>
<email confidence="0.797232">kairit.sirts@phon.ioc.ee</email>
<affiliation confidence="0.787164666666667">Tanel Institute of Cybernetics Tallinn University of</affiliation>
<email confidence="0.875632">tanel.alumae@phon.ioc.ee</email>
<abstract confidence="0.998547875">In this paper we present a fully unsupervised nonparametric Bayesian model that jointly induces POS tags and morphological segmentations. The model is essentially an infinite HMM that infers the number of states from data. Incorporating segmentation into the same model provides the morphological features to the system and eliminates the need to find them during preprocessing step. We show that learning both tasks jointly actually leads to better results than learning either task with gold standard data from the other task provided. The evaluation on multilingual data shows that the model produces state-of-the-art results on POS induction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Aldous</author>
</authors>
<title>Exchangeability and related topics.</title>
<date>1985</date>
<booktitle>In ´Ecole d’´et´e de Probabilit´es de Saint-Flour, XIII—</booktitle>
<pages>1--198</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4742" citStr="Aldous, 1985" startWordPosition="743" endWordPosition="744">des the paper. 2 Background 2.1 Dirichlet Process Let H be a distribution called base measure. Dirichlet process (DP) (Ferguson, 1973) is a probability distribution over distributions whose support is the subset of the support of H: G ∼ DP(α, H), (1) where α is the concentration parameter that controls the number of values instantiated by G. DP has no analytic form and therefore other representations must be developed for sampling. In the next section we describe Chinese Restaurant Process that enables to obtain samples from DP. 2.2 Chinese Restaurant Process Chinese Restaurant Process (CRP) (Aldous, 1985) enables to calculate the marginal probabilities of the elements conditioned on the values given to all previously seen items and integrating over possible DP prior values. Imagine an infinitely big Chinese restaurant with infinitely many tables with each table having capacity for infinitely many customers. In the beginning the restaurant is empty. Then customers, corresponding to data points, start entering one after another. The first customer chooses an empty table to sit at. Next customers choose a new table with probability proportional to the concentration parameter α or sit into one of </context>
</contexts>
<marker>Aldous, 1985</marker>
<rawString>D. Aldous. 1985. Exchangeability and related topics. In ´Ecole d’´et´e de Probabilit´es de Saint-Flour, XIII— 1983, pages 1–198. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>582--590</pages>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 582–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>A hierarchical Pitman-Yor process HMM for unsupervised Part of Speech induction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -</booktitle>
<volume>1</volume>
<pages>865--874</pages>
<contexts>
<context position="2750" citStr="Blunsom and Cohn, 2011" startWordPosition="426" endWordPosition="429">nd therefore it seems unreasonable to constrain the model with such a number instead of learning it from the data. Unsupervised morphology learning is another popular task that has been extensively studied by many authors. Here we are interested in learning concatenative morphology of words, meaning the substrings of the word corresponding to morphemes that, when concatenated, will give the lexical representation of the word type. For the rest of the paper we will refer to this task as (morphological) segmentation. Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) and this approach has been empirically proved to be helpful (Christodoulopoulos et al., 2010). In a similar fashion one could think that knowing POS tags could be useful for learning morphological segmentations and in this paper we will study this hypothesis. In this paper we will build a model that combines POS induction and morphological segmentation into one learning problem. We will show that the unsupervised learning of both of these tasks in the same 407 2012 Conference of the North American </context>
<context position="6752" citStr="Blunsom and Cohn, 2011" startWordPosition="1087" endWordPosition="1090">Restaurant Franchise (Teh et al., 2006) with several restaurants sharing the same franchise-wide menu G0. When a customer sits at an empty table in one of the Gj-th restaurants, the event of a new customer entering the restaurant G0 will be triggered. Analogously, when a table becomes empty in one of the Gj-th restaurants, it causes one of the customers leaving from restaurant G0. 3 Model We consider the problem of unsupervised learning of POS tags and morphological segmentations in a joint model. Similarly to some recent successful attempts (Lee et al., 2010; Christodoulopoulos et al., 2011; Blunsom and Cohn, 2011), our model is typebased, arranging word types into hard clusters. Unlike many recent POS tagging models, our model does not assume any prior information about the number of POS tags. We will define the model as a generative sequence model using the HMM structure. Graphical depiction of the model is given in Figure 1. 3.1 Generative story We assume the presence of a fixed length vocabulary W. The process starts with generating the lexicon that stores for each word type its POS tag and morphological segmentation. 408 • Draw a unigram tag distribution from the respective DP; • Draw a segment dis</context>
<context position="17682" citStr="Blunsom and Cohn, 2011" startWordPosition="2941" endWordPosition="2944">ood scaling heuristic into the model to scale the emission probability up. We tried a couple of different scaling factors on Multext-East English corpus and then set its value to 4 for all languages for the rest of the experiments. This improved the tagging results consistently across all languages. 4http://research.ics.tkk.fi/events/ morphochallenge2009/datasets.shtml 5http://www.cl.ut.ee/korpused/ morfkorpus/index.php?lang=eng 411 POS induction results are given in Table 1. When comparing these results with the recently published results on the same corpora (Christodoulopoulos et al., 2011; Blunsom and Cohn, 2011; Lee et al., 2010) we can see that our results compare favorably with the state-of-the-art, resulting with the best published results in many occasions. The number of tag clusters learned by the model corresponds surprisingly well to the number of true coarse-grained gold standard tags across all languages. There are two things to note here: 1) the tag distributions learned are influenced by the likelihood scaling heuristic and more experiments are needed in order to fully understand the characteristics and influence of this heuristic; 2) as the model is learning the coarse-grained tagset con</context>
<context position="23487" citStr="Blunsom and Cohn, 2011" startWordPosition="3930" endWordPosition="3933">wedish 18473 50.6 (0.1) 60.3 (0.1) 55.8 (0.1) 17 (0.0) 41 38.5† 68.7∗ 58.9∗ Table 1: Tagging results for different languages. For each language we report median one-to-one (1-1), many-to-one (m-1) and V-measure (V-m) together with standard deviation from five runs where median is taken over V-measure. Types is the number of word types in each corpus, True is the number of gold tags and Induced reports the median number of tags induced by the model together with standard deviation. Best Pub. lists the best published results so far (also 1-1, m-1 and V-m) in (Christodoulopoulos et al., 2011)∗, (Blunsom and Cohn, 2011)* and (Lee et al., 2010)†. Precision Recall F1 Types Segmented Estonian without LLS 43.5 (0.8) 59.4 (0.6) 50.3 (0.7) 16820 16820 with LLS 42.8 (1.1) 54.6 (0.7) 48.0 (0.9) English without LLS 69.0 (1.3) 37.3 (1.5) 48.5 (1.1) 20628 399 with LLS 59.8 (1.8) 29.0 (1.0) 39.1 (1.3) Finnish without LLS 56.2 (2.5) 29.5 (1.7) 38.7 (2.0) 25364 292 with LLS 56.0 (1.1) 28.0 (0.6) 37.4 (0.7) Turkish without LLS 65.4 (1.8) 44.8 (1.8) 53.2 (1.7) 18459 293 with LLS 68.9 (0.8) 39.2 (1.0) 50.0 (0.6) Table 2: Segmentation results on different languages. Results are calculated based on word types. For each languag</context>
<context position="29084" citStr="Blunsom and Cohn, 2011" startWordPosition="4833" endWordPosition="4836">sumption that is clearly not realistic in an unsupervised setting. Models presented in (Christodoulopoulos et al., 2011) and (Lee et al., 2010) are also built on Dirichlet-multinomials and, rather than defining a sequence model, present a clustering model based on features. Both report good results on type basis and use (among others) also morphological features, with (Lee et al., 2010) making use of fixed length suffixes and (Christodoulopoulos et al., 2011) using the suffixes obtained from an unsupervised morphology induction system. Nonparametric Bayesian POS induction has been studied in (Blunsom and Cohn, 2011) and (Gael et al., 2009). The model in (Blunsom and Cohn, 2011) uses Pitman-Yor Process (PYP) prior but the model itself is finite in the sense that the size of the tagset is fixed. Their model also captures morphological regularities by modeling the generation of words with character n-grams. The model in (Gael et al., 2009) uses infinite state space with Dirichlet Process prior. The model structure is classical HMM consisting 414 only of transitions and emissions and containing no morphological features. Inference is done by using beam sampler introduced in (Gael et al., 2008) which enables </context>
</contexts>
<marker>Blunsom, Cohn, 2011</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2011. A hierarchical Pitman-Yor process HMM for unsupervised Part of Speech induction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, pages 865–874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Two decades of unsupervised POS induction: How far have we come?</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>575--584</pages>
<contexts>
<context position="1543" citStr="Christodoulopoulos et al., 2010" startWordPosition="227" endWordPosition="230">he-art results on POS induction. 1 Introduction Nonparametric Bayesian modeling has recently become very popular in natural language processing (NLP), mostly because of its ability to provide priors that are especially suitable for tasks in NLP (Teh, 2006). Using nonparametric priors enables to treat the size of the model as a random variable with its value to be induced during inference which makes its use very appealing in models that need to decide upon the number of states. The task of unsupervised parts-of-speech (POS) tagging has been under research in numerous papers, for overview see (Christodoulopoulos et al., 2010). Most of the POS induction models use the structure of hidden Markov model (HMM) (Rabiner, 1989) that requires the knowledge about the number of hidden states (corresponding to the number of tags) in advance. According to our considerations, supplying this information is not desirable for two opposing reasons: 1) it injects into the system a piece of knowledge which in a truly unsupervised setting would be unavailable; and 2) the number of POS tags used is somewhat arbitrary anyway because there is no common consensus of what should be the true number of tags in each language and therefore it</context>
<context position="2940" citStr="Christodoulopoulos et al., 2010" startWordPosition="454" endWordPosition="457"> been extensively studied by many authors. Here we are interested in learning concatenative morphology of words, meaning the substrings of the word corresponding to morphemes that, when concatenated, will give the lexical representation of the word type. For the rest of the paper we will refer to this task as (morphological) segmentation. Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) and this approach has been empirically proved to be helpful (Christodoulopoulos et al., 2010). In a similar fashion one could think that knowing POS tags could be useful for learning morphological segmentations and in this paper we will study this hypothesis. In this paper we will build a model that combines POS induction and morphological segmentation into one learning problem. We will show that the unsupervised learning of both of these tasks in the same 407 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 407–416, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics mode</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2010</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2010. Two decades of unsupervised POS induction: How far have we come? In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 575–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharo Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>A Bayesian mixture model for PoS induction using multiple features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>638--647</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="2846" citStr="Christodoulopoulos et al., 2011" startWordPosition="440" endWordPosition="443">f learning it from the data. Unsupervised morphology learning is another popular task that has been extensively studied by many authors. Here we are interested in learning concatenative morphology of words, meaning the substrings of the word corresponding to morphemes that, when concatenated, will give the lexical representation of the word type. For the rest of the paper we will refer to this task as (morphological) segmentation. Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) and this approach has been empirically proved to be helpful (Christodoulopoulos et al., 2010). In a similar fashion one could think that knowing POS tags could be useful for learning morphological segmentations and in this paper we will study this hypothesis. In this paper we will build a model that combines POS induction and morphological segmentation into one learning problem. We will show that the unsupervised learning of both of these tasks in the same 407 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 407</context>
<context position="6727" citStr="Christodoulopoulos et al., 2011" startWordPosition="1083" endWordPosition="1086">) Under HDP, CRP becomes Chinese Restaurant Franchise (Teh et al., 2006) with several restaurants sharing the same franchise-wide menu G0. When a customer sits at an empty table in one of the Gj-th restaurants, the event of a new customer entering the restaurant G0 will be triggered. Analogously, when a table becomes empty in one of the Gj-th restaurants, it causes one of the customers leaving from restaurant G0. 3 Model We consider the problem of unsupervised learning of POS tags and morphological segmentations in a joint model. Similarly to some recent successful attempts (Lee et al., 2010; Christodoulopoulos et al., 2011; Blunsom and Cohn, 2011), our model is typebased, arranging word types into hard clusters. Unlike many recent POS tagging models, our model does not assume any prior information about the number of POS tags. We will define the model as a generative sequence model using the HMM structure. Graphical depiction of the model is given in Figure 1. 3.1 Generative story We assume the presence of a fixed length vocabulary W. The process starts with generating the lexicon that stores for each word type its POS tag and morphological segmentation. 408 • Draw a unigram tag distribution from the respective</context>
<context position="17658" citStr="Christodoulopoulos et al., 2011" startWordPosition="2937" endWordPosition="2940">motivated introducing the likelihood scaling heuristic into the model to scale the emission probability up. We tried a couple of different scaling factors on Multext-East English corpus and then set its value to 4 for all languages for the rest of the experiments. This improved the tagging results consistently across all languages. 4http://research.ics.tkk.fi/events/ morphochallenge2009/datasets.shtml 5http://www.cl.ut.ee/korpused/ morfkorpus/index.php?lang=eng 411 POS induction results are given in Table 1. When comparing these results with the recently published results on the same corpora (Christodoulopoulos et al., 2011; Blunsom and Cohn, 2011; Lee et al., 2010) we can see that our results compare favorably with the state-of-the-art, resulting with the best published results in many occasions. The number of tag clusters learned by the model corresponds surprisingly well to the number of true coarse-grained gold standard tags across all languages. There are two things to note here: 1) the tag distributions learned are influenced by the likelihood scaling heuristic and more experiments are needed in order to fully understand the characteristics and influence of this heuristic; 2) as the model is learning the c</context>
<context position="23460" citStr="Christodoulopoulos et al., 2011" startWordPosition="3926" endWordPosition="3929">.6 (1.0) 12 (0.7) 12 - 67.9∗ 56.7∗ Swedish 18473 50.6 (0.1) 60.3 (0.1) 55.8 (0.1) 17 (0.0) 41 38.5† 68.7∗ 58.9∗ Table 1: Tagging results for different languages. For each language we report median one-to-one (1-1), many-to-one (m-1) and V-measure (V-m) together with standard deviation from five runs where median is taken over V-measure. Types is the number of word types in each corpus, True is the number of gold tags and Induced reports the median number of tags induced by the model together with standard deviation. Best Pub. lists the best published results so far (also 1-1, m-1 and V-m) in (Christodoulopoulos et al., 2011)∗, (Blunsom and Cohn, 2011)* and (Lee et al., 2010)†. Precision Recall F1 Types Segmented Estonian without LLS 43.5 (0.8) 59.4 (0.6) 50.3 (0.7) 16820 16820 with LLS 42.8 (1.1) 54.6 (0.7) 48.0 (0.9) English without LLS 69.0 (1.3) 37.3 (1.5) 48.5 (1.1) 20628 399 with LLS 59.8 (1.8) 29.0 (1.0) 39.1 (1.3) Finnish without LLS 56.2 (2.5) 29.5 (1.7) 38.7 (2.0) 25364 292 with LLS 56.0 (1.1) 28.0 (0.6) 37.4 (0.7) Turkish without LLS 65.4 (1.8) 44.8 (1.8) 53.2 (1.7) 18459 293 with LLS 68.9 (0.8) 39.2 (1.0) 50.0 (0.6) Table 2: Segmentation results on different languages. Results are calculated based on w</context>
<context position="28581" citStr="Christodoulopoulos et al., 2011" startWordPosition="4756" endWordPosition="4759">ented is a classical HMM with multinomial transition and emission distributions with Dirichlet priors. Inference is done using a collapsed Gibbs sampler and concentration parameter values are learned during inference. The model is token-based, allowing different words of the same type in different locations to have a different tag. This model can actually be classified as semi-supervised as it assumes the presence of a tagging dictionary that contains the list of possible POS tags for each word type - an assumption that is clearly not realistic in an unsupervised setting. Models presented in (Christodoulopoulos et al., 2011) and (Lee et al., 2010) are also built on Dirichlet-multinomials and, rather than defining a sequence model, present a clustering model based on features. Both report good results on type basis and use (among others) also morphological features, with (Lee et al., 2010) making use of fixed length suffixes and (Christodoulopoulos et al., 2011) using the suffixes obtained from an unsupervised morphology induction system. Nonparametric Bayesian POS induction has been studied in (Blunsom and Cohn, 2011) and (Gael et al., 2009). The model in (Blunsom and Cohn, 2011) uses Pitman-Yor Process (PYP) pri</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2011</marker>
<rawString>Christos Christodoulopoulos, Sharo Goldwater, and Mark Steedman. 2011. A Bayesian mixture model for PoS induction using multiple features. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 638–647, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for Part of Speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics -</booktitle>
<volume>1</volume>
<pages>59--66</pages>
<contexts>
<context position="2812" citStr="Clark, 2003" startWordPosition="438" endWordPosition="439">ber instead of learning it from the data. Unsupervised morphology learning is another popular task that has been extensively studied by many authors. Here we are interested in learning concatenative morphology of words, meaning the substrings of the word corresponding to morphemes that, when concatenated, will give the lexical representation of the word type. For the rest of the paper we will refer to this task as (morphological) segmentation. Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) and this approach has been empirically proved to be helpful (Christodoulopoulos et al., 2010). In a similar fashion one could think that knowing POS tags could be useful for learning morphological segmentations and in this paper we will study this hypothesis. In this paper we will build a model that combines POS induction and morphological segmentation into one learning problem. We will show that the unsupervised learning of both of these tasks in the same 407 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Huma</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark. 2003. Combining distributional and morphological information for Part of Speech induction. In Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics - Volume 1, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Inducing the morphological lexicon of a natural language from unannotated text. In</title>
<date>2005</date>
<booktitle>In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning,</booktitle>
<pages>106--113</pages>
<contexts>
<context position="27166" citStr="Creutz and Lagus, 2005" startWordPosition="4535" endWordPosition="4538">ing results for both tagging and segmentation are better than the results obtained from semisupervised learning. This is surprising because one would assume that providing gold standard data would lead to better results. On the other hand, these results are encouraging, showing that learning two dependent tasks in a joint model by unsupervised manner can be as good or even better than learning the same tasks separately and providing the gold standard data as features. Finally, we learned the morphological segmentations with the state-of-the-art morphology induction system Morfessor baseline6 (Creutz and Lagus, 2005) and report the best results in the last row of Table 3. Apparently, our joint model cannot beat Morfessor in morphological segmentation and when 6http://www.cis.hut.fi/projects/morpho/ using the emission likelihood scaling that influences the tagging results favorably, the segmentation results get even worse. Altough the semisupervised experiments showed that there are dependencies between tags and segmentations, the conducted experiments do not reveal of how to use these dependencies for helping the POS tags to learn better morphological segmentations. 6 Related Work We will review some of t</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>Mathias Creutz and Krista Lagus. 2005. Inducing the morphological lexicon of a natural language from unannotated text. In In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning, pages 106–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Discovering morphological paradigms from plain text using a Dirichlet Process mixture model.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>616--627</pages>
<marker>Dreyer, Eisner, 2011</marker>
<rawString>Markus Dreyer and Jason Eisner. 2011. Discovering morphological paradigms from plain text using a Dirichlet Process mixture model. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 616–627.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toma Erjavec</author>
</authors>
<title>MULTEXT-East version 4: Multilingual morphosyntactic specifications, lexicons and corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="14846" citStr="Erjavec, 2010" startWordPosition="2530" endWordPosition="2531">the segmentation likelihood formula (13). 4.3 Hyperparameter sampling All DP and Dirichlet concentration parameters are given vague Gamma(10, 0.1) priors and new values are sampled by using the auxiliary variable sampling scheme described in (Escobar and West, 1995) and the extended version for HDP-s described in (Teh et al., 2006). The segment length control parameter is given uniform Beta prior and its new values are sampled from the posterior which is also a Beta distribution. 5 Results 5.1 Evaluation We test the POS induction part of the model on all languages in the Multext-East corpora (Erjavec, 2010) as well as on the free corpora from CONLLX Shared Task1 for Dutch, Danish, Swedish and Portuguese. The evaluation of morphological segmentations is based on the Morpho Challenge gold segmented wordlists for English, Finnish and Turkish2. We gathered the sentences from Europarl corpus3 for English and Finnish, and use the Turkish 1http://ilk.uvt.nl/conll/free_data.html 2http://research.ics.tkk.fi/events/ morphochallenge2010/datasets.shtml 3http://www.statmt.org/europarl/ text data from the Morpho Challenge 20094. Estonian gold standard segmentations have been obtained from the Estonian morphol</context>
</contexts>
<marker>Erjavec, 2010</marker>
<rawString>Toma Erjavec. 2010. MULTEXT-East version 4: Multilingual morphosyntactic specifications, lexicons and corpora. In Proceedings of the Seventh International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael D Escobar</author>
<author>Mike West</author>
</authors>
<title>Bayesian density estimation and inference using mixtures.</title>
<date>1995</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>90</volume>
<issue>430</issue>
<contexts>
<context position="14498" citStr="Escobar and West, 1995" startWordPosition="2468" endWordPosition="2471">ginning position k of the last segment from the forward variables α[t][k], where t is the length of the word. Then we set t = t − k and continue to sample the start of the previous to the last segment. This process continues until t = 0. The segment probabilities, conditioned on the tag currently assigned to the word type, will be calculated according to the segmentation likelihood formula (13). 4.3 Hyperparameter sampling All DP and Dirichlet concentration parameters are given vague Gamma(10, 0.1) priors and new values are sampled by using the auxiliary variable sampling scheme described in (Escobar and West, 1995) and the extended version for HDP-s described in (Teh et al., 2006). The segment length control parameter is given uniform Beta prior and its new values are sampled from the posterior which is also a Beta distribution. 5 Results 5.1 Evaluation We test the POS induction part of the model on all languages in the Multext-East corpora (Erjavec, 2010) as well as on the free corpora from CONLLX Shared Task1 for Dutch, Danish, Swedish and Portuguese. The evaluation of morphological segmentations is based on the Morpho Challenge gold segmented wordlists for English, Finnish and Turkish2. We gathered t</context>
</contexts>
<marker>Escobar, West, 1995</marker>
<rawString>Michael D. Escobar and Mike West. 1995. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association, 90(430).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Ferguson</author>
</authors>
<title>A Bayesian analysis of some nonparametric problems.</title>
<date>1973</date>
<journal>The Annals of Statistics,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="4263" citStr="Ferguson, 1973" startWordPosition="666" endWordPosition="667">ask provided. We will also demonstrate that our model produces state-of-the-art results on POS tagging. As opposed to the compared methods, our model also induces the number of tags from data. In the following, section 2 gives the overview of the Dirichlet Processes, section 3 describes the model setup followed by the description of inference procedures in section 4, experimental results are presented in section 5, section 6 summarizes the previous work and last section concludes the paper. 2 Background 2.1 Dirichlet Process Let H be a distribution called base measure. Dirichlet process (DP) (Ferguson, 1973) is a probability distribution over distributions whose support is the subset of the support of H: G ∼ DP(α, H), (1) where α is the concentration parameter that controls the number of values instantiated by G. DP has no analytic form and therefore other representations must be developed for sampling. In the next section we describe Chinese Restaurant Process that enables to obtain samples from DP. 2.2 Chinese Restaurant Process Chinese Restaurant Process (CRP) (Aldous, 1985) enables to calculate the marginal probabilities of the elements conditioned on the values given to all previously seen i</context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Thomas S. Ferguson. 1973. A Bayesian analysis of some nonparametric problems. The Annals of Statistics, 1(2):209–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Yunus Saatci</author>
<author>Yee Whye Teh</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Beam sampling for the infinite Hidden Markov Model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>1088--1095</pages>
<marker>Van Gael, Saatci, Teh, Ghahramani, 2008</marker>
<rawString>Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and Zoubin Ghahramani. 2008. Beam sampling for the infinite Hidden Markov Model. In Proceedings of the 25th International Conference on Machine Learning, pages 1088–1095.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Andreas Vlachos</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>The infinite HMM for unsupervised PoS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>678--687</pages>
<marker>Van Gael, Vlachos, Ghahramani, 2009</marker>
<rawString>Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahramani. 2009. The infinite HMM for unsupervised PoS tagging. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, pages 678–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised Part-of-Speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>744--751</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="27933" citStr="Goldwater and Griffiths, 2007" startWordPosition="4651" endWordPosition="4654">and when 6http://www.cis.hut.fi/projects/morpho/ using the emission likelihood scaling that influences the tagging results favorably, the segmentation results get even worse. Altough the semisupervised experiments showed that there are dependencies between tags and segmentations, the conducted experiments do not reveal of how to use these dependencies for helping the POS tags to learn better morphological segmentations. 6 Related Work We will review some of the recent works related to Bayesian POS induction and morphological segmentation. One of the first Bayesian POS taggers is described in (Goldwater and Griffiths, 2007). The model presented is a classical HMM with multinomial transition and emission distributions with Dirichlet priors. Inference is done using a collapsed Gibbs sampler and concentration parameter values are learned during inference. The model is token-based, allowing different words of the same type in different locations to have a different tag. This model can actually be classified as semi-supervised as it assumes the presence of a tagging dictionary that contains the list of possible POS tags for each word type - an assumption that is clearly not realistic in an unsupervised setting. Model</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully Bayesian approach to unsupervised Part-of-Speech tagging. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 18,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="30196" citStr="Goldwater et al., 2006" startWordPosition="5015" endWordPosition="5018">orphological features. Inference is done by using beam sampler introduced in (Gael et al., 2008) which enables parallelized implementation. One close model for morphology stems from Bayesian word segmentation (Goldwater et al., 2009) where the task is to induce word borders from transcribed sentences. Our segmentation model is in principle the same as the unigram word segmentation model and the main difference is that we are using blocked sampler while (Goldwater et al., 2009) uses point-wise Gibbs sampler by drawing the presence or absence of the word border between every two characters. In (Goldwater et al., 2006) the morphology is learned in the adaptor grammar framework (Johnson et al., 2006) by using a PYP adaptor. PYP adaptor caches the numbers of observed derivation trees and forces the distribution over all possible trees to take the shape of power law. In the PYP (and also DP) case the adaptor grammar can be interpreted as PYP (or DP) model with regular PCFG distribution as base measure. The model proposed in (Goldwater et al., 2006) makes several assumptions that we do not: 1) segmentations have a fixed structure of stem and suffix; and 2) there is a fixed number of inflectional classes. Infere</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. In Advances in Neural Information Processing Systems 18, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<pages>112--21</pages>
<contexts>
<context position="29806" citStr="Goldwater et al., 2009" startWordPosition="4948" endWordPosition="4951"> but the model itself is finite in the sense that the size of the tagset is fixed. Their model also captures morphological regularities by modeling the generation of words with character n-grams. The model in (Gael et al., 2009) uses infinite state space with Dirichlet Process prior. The model structure is classical HMM consisting 414 only of transitions and emissions and containing no morphological features. Inference is done by using beam sampler introduced in (Gael et al., 2008) which enables parallelized implementation. One close model for morphology stems from Bayesian word segmentation (Goldwater et al., 2009) where the task is to induce word borders from transcribed sentences. Our segmentation model is in principle the same as the unigram word segmentation model and the main difference is that we are using blocked sampler while (Goldwater et al., 2009) uses point-wise Gibbs sampler by drawing the presence or absence of the word border between every two characters. In (Goldwater et al., 2006) the morphology is learned in the adaptor grammar framework (Johnson et al., 2006) by using a PYP adaptor. PYP adaptor caches the numbers of observed derivation trees and forces the distribution over all possib</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112:21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>320--327</pages>
<location>New York City, USA.</location>
<contexts>
<context position="15579" citStr="Haghighi and Klein, 2006" startWordPosition="2624" endWordPosition="2627">uation of morphological segmentations is based on the Morpho Challenge gold segmented wordlists for English, Finnish and Turkish2. We gathered the sentences from Europarl corpus3 for English and Finnish, and use the Turkish 1http://ilk.uvt.nl/conll/free_data.html 2http://research.ics.tkk.fi/events/ morphochallenge2010/datasets.shtml 3http://www.statmt.org/europarl/ text data from the Morpho Challenge 20094. Estonian gold standard segmentations have been obtained from the Estonian morphologically annotated corpus5. We report three accuracy measures for tagging: greedy one-to-one mapping (1-1) (Haghighi and Klein, 2006), many-to-one mapping (m-1) and Vmeasure (V-m) (Rosenberg and Hirschberg, 2007). Segmentation is evaluated on the basis of standard F-score which is the harmonic mean of precision and recall. 5.2 Experimental results For each experiment, we made five runs with random initializations and report the results of the median. The sampler was run 200 iterations for burnin, after which we collected 5 samples, letting the sampler to run for another 200 iterations between each two sample. We start with 15 segmenting iterations during each Gibbs iteration to enable the segmentation sampler to burnin to t</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 320–327, New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 19,</booktitle>
<pages>641--648</pages>
<contexts>
<context position="30278" citStr="Johnson et al., 2006" startWordPosition="5028" endWordPosition="5032">t al., 2008) which enables parallelized implementation. One close model for morphology stems from Bayesian word segmentation (Goldwater et al., 2009) where the task is to induce word borders from transcribed sentences. Our segmentation model is in principle the same as the unigram word segmentation model and the main difference is that we are using blocked sampler while (Goldwater et al., 2009) uses point-wise Gibbs sampler by drawing the presence or absence of the word border between every two characters. In (Goldwater et al., 2006) the morphology is learned in the adaptor grammar framework (Johnson et al., 2006) by using a PYP adaptor. PYP adaptor caches the numbers of observed derivation trees and forces the distribution over all possible trees to take the shape of power law. In the PYP (and also DP) case the adaptor grammar can be interpreted as PYP (or DP) model with regular PCFG distribution as base measure. The model proposed in (Goldwater et al., 2006) makes several assumptions that we do not: 1) segmentations have a fixed structure of stem and suffix; and 2) there is a fixed number of inflectional classes. Inference is performed with Gibbs sampler by sampling for each word its stem, suffix and</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2006</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2006. Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models. In Advances in Neural Information Processing Systems 19, pages 641–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Simple type-level unsupervised POS tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>853--861</pages>
<contexts>
<context position="2768" citStr="Lee et al., 2010" startWordPosition="430" endWordPosition="433">reasonable to constrain the model with such a number instead of learning it from the data. Unsupervised morphology learning is another popular task that has been extensively studied by many authors. Here we are interested in learning concatenative morphology of words, meaning the substrings of the word corresponding to morphemes that, when concatenated, will give the lexical representation of the word type. For the rest of the paper we will refer to this task as (morphological) segmentation. Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011) and this approach has been empirically proved to be helpful (Christodoulopoulos et al., 2010). In a similar fashion one could think that knowing POS tags could be useful for learning morphological segmentations and in this paper we will study this hypothesis. In this paper we will build a model that combines POS induction and morphological segmentation into one learning problem. We will show that the unsupervised learning of both of these tasks in the same 407 2012 Conference of the North American Chapter of the Ass</context>
<context position="6694" citStr="Lee et al., 2010" startWordPosition="1079" endWordPosition="1082"> G0) j = 1··· J (4) Under HDP, CRP becomes Chinese Restaurant Franchise (Teh et al., 2006) with several restaurants sharing the same franchise-wide menu G0. When a customer sits at an empty table in one of the Gj-th restaurants, the event of a new customer entering the restaurant G0 will be triggered. Analogously, when a table becomes empty in one of the Gj-th restaurants, it causes one of the customers leaving from restaurant G0. 3 Model We consider the problem of unsupervised learning of POS tags and morphological segmentations in a joint model. Similarly to some recent successful attempts (Lee et al., 2010; Christodoulopoulos et al., 2011; Blunsom and Cohn, 2011), our model is typebased, arranging word types into hard clusters. Unlike many recent POS tagging models, our model does not assume any prior information about the number of POS tags. We will define the model as a generative sequence model using the HMM structure. Graphical depiction of the model is given in Figure 1. 3.1 Generative story We assume the presence of a fixed length vocabulary W. The process starts with generating the lexicon that stores for each word type its POS tag and morphological segmentation. 408 • Draw a unigram tag</context>
<context position="17701" citStr="Lee et al., 2010" startWordPosition="2945" endWordPosition="2948">to the model to scale the emission probability up. We tried a couple of different scaling factors on Multext-East English corpus and then set its value to 4 for all languages for the rest of the experiments. This improved the tagging results consistently across all languages. 4http://research.ics.tkk.fi/events/ morphochallenge2009/datasets.shtml 5http://www.cl.ut.ee/korpused/ morfkorpus/index.php?lang=eng 411 POS induction results are given in Table 1. When comparing these results with the recently published results on the same corpora (Christodoulopoulos et al., 2011; Blunsom and Cohn, 2011; Lee et al., 2010) we can see that our results compare favorably with the state-of-the-art, resulting with the best published results in many occasions. The number of tag clusters learned by the model corresponds surprisingly well to the number of true coarse-grained gold standard tags across all languages. There are two things to note here: 1) the tag distributions learned are influenced by the likelihood scaling heuristic and more experiments are needed in order to fully understand the characteristics and influence of this heuristic; 2) as the model is learning the coarse-grained tagset consistently in all la</context>
<context position="23511" citStr="Lee et al., 2010" startWordPosition="3935" endWordPosition="3938">0.1) 55.8 (0.1) 17 (0.0) 41 38.5† 68.7∗ 58.9∗ Table 1: Tagging results for different languages. For each language we report median one-to-one (1-1), many-to-one (m-1) and V-measure (V-m) together with standard deviation from five runs where median is taken over V-measure. Types is the number of word types in each corpus, True is the number of gold tags and Induced reports the median number of tags induced by the model together with standard deviation. Best Pub. lists the best published results so far (also 1-1, m-1 and V-m) in (Christodoulopoulos et al., 2011)∗, (Blunsom and Cohn, 2011)* and (Lee et al., 2010)†. Precision Recall F1 Types Segmented Estonian without LLS 43.5 (0.8) 59.4 (0.6) 50.3 (0.7) 16820 16820 with LLS 42.8 (1.1) 54.6 (0.7) 48.0 (0.9) English without LLS 69.0 (1.3) 37.3 (1.5) 48.5 (1.1) 20628 399 with LLS 59.8 (1.8) 29.0 (1.0) 39.1 (1.3) Finnish without LLS 56.2 (2.5) 29.5 (1.7) 38.7 (2.0) 25364 292 with LLS 56.0 (1.1) 28.0 (0.6) 37.4 (0.7) Turkish without LLS 65.4 (1.8) 44.8 (1.8) 53.2 (1.7) 18459 293 with LLS 68.9 (0.8) 39.2 (1.0) 50.0 (0.6) Table 2: Segmentation results on different languages. Results are calculated based on word types. For each language we report precision, r</context>
<context position="28604" citStr="Lee et al., 2010" startWordPosition="4761" endWordPosition="4764">ial transition and emission distributions with Dirichlet priors. Inference is done using a collapsed Gibbs sampler and concentration parameter values are learned during inference. The model is token-based, allowing different words of the same type in different locations to have a different tag. This model can actually be classified as semi-supervised as it assumes the presence of a tagging dictionary that contains the list of possible POS tags for each word type - an assumption that is clearly not realistic in an unsupervised setting. Models presented in (Christodoulopoulos et al., 2011) and (Lee et al., 2010) are also built on Dirichlet-multinomials and, rather than defining a sequence model, present a clustering model based on features. Both report good results on type basis and use (among others) also morphological features, with (Lee et al., 2010) making use of fixed length suffixes and (Christodoulopoulos et al., 2011) using the suffixes obtained from an unsupervised morphology induction system. Nonparametric Bayesian POS induction has been studied in (Blunsom and Cohn, 2011) and (Gael et al., 2009). The model in (Blunsom and Cohn, 2011) uses Pitman-Yor Process (PYP) prior but the model itself</context>
</contexts>
<marker>Lee, Haghighi, Barzilay, 2010</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2010. Simple type-level unsupervised POS tagging. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 853– 861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>1</volume>
<pages>100--108</pages>
<contexts>
<context position="12730" citStr="Mochihashi et al., 2009" startWordPosition="2154" endWordPosition="2157">er the word type count, nts and ms denote the number of customers “eating” the segment s under tag t and the number of tables “serving” the segment s across all restaurants respectively, dot represents the marginal counts and α and Q are the concentration parameters of the respective DP-s. —Si in upper index means that the segments belonging to the segmentation of the i-th word type and not calculated into likelihood term yet have been excluded. 4.2 Segmentation sampling We sample the whole segmentation of a word type as a block with forward-filtering backward-sampling scheme as described in (Mochihashi et al., 2009). As we cannot sample from the exact marginal conditional distribution due to the dependencies between segments induced by the CRP, we use the Metropolis-Hastings sampler that draws a new proposal with forward-filtering backwardsampling scheme and accepts it with probability min(1, ((Ssrop) ), where Sprop is the proposed segold) mentation and Sold is the current segmentation of a word type. The acceptance rate during experiments varied between 94-98%. For each word type, we build a forward filtering table where we maintain the forward variables α[t][k] that present the probabilities of the las</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, pages 100–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Benjamin Snyder</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Multilingual part-of-speech tagging: Two unsupervised approaches.</title>
<date>2009</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>36--1</pages>
<marker>Naseem, Snyder, Eisenstein, Barzilay, 2009</marker>
<rawString>Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and Regina Barzilay. 2009. Multilingual part-of-speech tagging: Two unsupervised approaches. Journal ofArtificial Intelligence Research, 36:1–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on Hidden Markov Models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="1640" citStr="Rabiner, 1989" startWordPosition="245" endWordPosition="246"> natural language processing (NLP), mostly because of its ability to provide priors that are especially suitable for tasks in NLP (Teh, 2006). Using nonparametric priors enables to treat the size of the model as a random variable with its value to be induced during inference which makes its use very appealing in models that need to decide upon the number of states. The task of unsupervised parts-of-speech (POS) tagging has been under research in numerous papers, for overview see (Christodoulopoulos et al., 2010). Most of the POS induction models use the structure of hidden Markov model (HMM) (Rabiner, 1989) that requires the knowledge about the number of hidden states (corresponding to the number of tags) in advance. According to our considerations, supplying this information is not desirable for two opposing reasons: 1) it injects into the system a piece of knowledge which in a truly unsupervised setting would be unavailable; and 2) the number of POS tags used is somewhat arbitrary anyway because there is no common consensus of what should be the true number of tags in each language and therefore it seems unreasonable to constrain the model with such a number instead of learning it from the dat</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on Hidden Markov Models and selected applications in speech recognition. In Proceedings of the IEEE, pages 257– 286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>410--420</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="15658" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="2635" endWordPosition="2638">ld segmented wordlists for English, Finnish and Turkish2. We gathered the sentences from Europarl corpus3 for English and Finnish, and use the Turkish 1http://ilk.uvt.nl/conll/free_data.html 2http://research.ics.tkk.fi/events/ morphochallenge2010/datasets.shtml 3http://www.statmt.org/europarl/ text data from the Morpho Challenge 20094. Estonian gold standard segmentations have been obtained from the Estonian morphologically annotated corpus5. We report three accuracy measures for tagging: greedy one-to-one mapping (1-1) (Haghighi and Klein, 2006), many-to-one mapping (m-1) and Vmeasure (V-m) (Rosenberg and Hirschberg, 2007). Segmentation is evaluated on the basis of standard F-score which is the harmonic mean of precision and recall. 5.2 Experimental results For each experiment, we made five runs with random initializations and report the results of the median. The sampler was run 200 iterations for burnin, after which we collected 5 samples, letting the sampler to run for another 200 iterations between each two sample. We start with 15 segmenting iterations during each Gibbs iteration to enable the segmentation sampler to burnin to the current tagging state, and gradually reduce this number to one. Segmentation</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 410–420, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michel I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="5962" citStr="Teh et al., 2006" startWordPosition="947" endWordPosition="950"> already occupied tables with probability proportional to the number of customers already sitting there. Whenever a customer chooses an empty table, he will also pick a dish from H to be served on that table. The predictive probability distribution over dishes for the i-th customer is: nφk + α P(xi = Ok|x−i, α, H) = i 1+ α pH (Ok), (2) — where x−i is the seating arrangement of customers excluding the i-th customer and nφk is the number of customers eating dish Ok and pH(·) is the probability according to H. 2.3 Hierarchical Dirichlet Process The notion of hierarchical Dirichlet Process (HDP) (Teh et al., 2006) can be derived by letting the base measure itself to be a draw from a DP: G0|α0, H ∼ DP(α0, H) (3) Gj|α, G0 ∼ DP(α, G0) j = 1··· J (4) Under HDP, CRP becomes Chinese Restaurant Franchise (Teh et al., 2006) with several restaurants sharing the same franchise-wide menu G0. When a customer sits at an empty table in one of the Gj-th restaurants, the event of a new customer entering the restaurant G0 will be triggered. Analogously, when a table becomes empty in one of the Gj-th restaurants, it causes one of the customers leaving from restaurant G0. 3 Model We consider the problem of unsupervised l</context>
<context position="14565" citStr="Teh et al., 2006" startWordPosition="2480" endWordPosition="2483">, where t is the length of the word. Then we set t = t − k and continue to sample the start of the previous to the last segment. This process continues until t = 0. The segment probabilities, conditioned on the tag currently assigned to the word type, will be calculated according to the segmentation likelihood formula (13). 4.3 Hyperparameter sampling All DP and Dirichlet concentration parameters are given vague Gamma(10, 0.1) priors and new values are sampled by using the auxiliary variable sampling scheme described in (Escobar and West, 1995) and the extended version for HDP-s described in (Teh et al., 2006). The segment length control parameter is given uniform Beta prior and its new values are sampled from the posterior which is also a Beta distribution. 5 Results 5.1 Evaluation We test the POS induction part of the model on all languages in the Multext-East corpora (Erjavec, 2010) as well as on the free corpora from CONLLX Shared Task1 for Dutch, Danish, Swedish and Portuguese. The evaluation of morphological segmentations is based on the Morpho Challenge gold segmented wordlists for English, Finnish and Turkish2. We gathered the sentences from Europarl corpus3 for English and Finnish, and use</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michel I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>985--992</pages>
<contexts>
<context position="1167" citStr="Teh, 2006" startWordPosition="167" endWordPosition="168">ame model provides the morphological features to the system and eliminates the need to find them during preprocessing step. We show that learning both tasks jointly actually leads to better results than learning either task with gold standard data from the other task provided. The evaluation on multilingual data shows that the model produces state-of-the-art results on POS induction. 1 Introduction Nonparametric Bayesian modeling has recently become very popular in natural language processing (NLP), mostly because of its ability to provide priors that are especially suitable for tasks in NLP (Teh, 2006). Using nonparametric priors enables to treat the size of the model as a random variable with its value to be induced during inference which makes its use very appealing in models that need to decide upon the number of states. The task of unsupervised parts-of-speech (POS) tagging has been under research in numerous papers, for overview see (Christodoulopoulos et al., 2010). Most of the POS induction models use the structure of hidden Markov model (HMM) (Rabiner, 1989) that requires the knowledge about the number of hidden states (corresponding to the number of tags) in advance. According to o</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 985–992.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>