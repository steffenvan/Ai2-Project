<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000123">
<title confidence="0.989438">
Learning from evolving data streams: online triage of bug reports
</title>
<author confidence="0.945661">
Grzegorz Chrupala
</author>
<affiliation confidence="0.9573735">
Spoken Language Systems
Saarland University
</affiliation>
<email confidence="0.994642">
gchrupala@lsv.uni-saarland.de
</email>
<sectionHeader confidence="0.995567" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999109461538461">
Open issue trackers are a type of social me-
dia that has received relatively little atten-
tion from the text-mining community. We
investigate the problems inherent in learn-
ing to triage bug reports from time-varying
data. We demonstrate that concept drift is
an important consideration. We show the
effectiveness of online learning algorithms
by evaluating them on several bug report
datasets collected from open issue trackers
associated with large open-source projects.
We make this collection of data publicly
available.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999818">
There has been relatively little research to date
on applying machine learning and Natural Lan-
guage Processing techniques to automate soft-
ware project workflows. In this paper we address
the problem of bug report triage.
</bodyText>
<subsectionHeader confidence="0.994943">
1.1 Issue tracking
</subsectionHeader>
<bodyText confidence="0.99962325">
Large software projects typically track defect re-
ports, feature requests and other issue reports us-
ing an issue tracker system. Open source projects
tend to use trackers which are open to both devel-
opers and users. If the product has many users its
tracker can receive an overwhelming number of
issue reports: Mozilla was receiving almost 300
reports per day in 2006 (Anvik et al. 2006). Some-
one has to monitor those reports and triage them,
that is decide which component they affect and
which developer or team of developers should be
responsible for analyzing them and fixing the re-
ported defects. An automated agent assisting the
staff responsible for such triage has the potential
to substantially reduce the time and cost of this
task.
</bodyText>
<subsectionHeader confidence="0.894539">
1.2 Issue trackers as social media
</subsectionHeader>
<bodyText confidence="0.999864939393939">
In a large software project with a loose, not
strictly hierarchical organization, standards and
practices are not exclusively imposed top-down
but also tend to spontaneously arise in a bottom-
up fashion, arrived at through interaction of in-
dividual developers, testers and users. The indi-
viduals involved may negotiate practices explic-
itly, but may also imitate and influence each other
via implicitly acquired reputation and status. This
process has a strong emergent component: an in-
formal taxonomy may arise and evolve in an is-
sue tracker via the use of free-form tags or labels.
Developers, testers and users can attach tags to
their issue reports in order to informally classify
them. The issue tracking software may give users
feedback by informing them which tags were fre-
quently used in the past, or suggest tags based
on the content of the report or other information.
Through this collaborative, feedback driven pro-
cess involving both human and machine partici-
pants, an evolving consensus on the label inven-
tory and semantics typically arises, without much
top-down control (Halpin et al. 2007).
This kind of emergent taxonomy is known as
a folksonomy or collaborative tagging and is
very common in the context of social web appli-
cations. Large software projects, especially those
with open policies and little hierarchical struc-
tures, tend to exhibit many of the same emergent
social properties as the more prototypical social
applications. While this is a useful phenomenon,
it presents a special challenge from the machine-
learning point of view.
</bodyText>
<page confidence="0.986609">
613
</page>
<note confidence="0.9886865">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 613–622,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.966522">
1.3 Concept drift
</subsectionHeader>
<bodyText confidence="0.99996887755102">
Many standard supervised approaches in
machine-learning assume a stationary distribution
from which training examples are independently
drawn. The set of training examples is processed
as a batch, and the resulting learned decision
function (such as a classifier) is then used on test
items, which are assumed to be drawn from the
same stationary distribution.
If we need an automated agent which uses hu-
man labels to learn to tag objects the batch learn-
ing approach is inadequate. Examples arrive one-
by-one in a stream, not as a batch. Even more
importantly, both the output (label) distribution
and the input distribution from which the exam-
ples come are emphatically not stationary. As a
software project progresses and matures, the type
of issues reported is going to change. As project
members and users come and go, the vocabulary
they use to describe the issues will vary. As the
consensus tag folksonomy emerges, the label and
training example distribution will evolve. This
phenomenon is sometimes referred to as concept
drift (Widmer and Kubat 1996, Tsymbal 2004).
Early research on learning to triage tended to
either not notice the problem (ˇCubrani´c and Mur-
phy 2004), or acknowledge but not address it (An-
vik et al. 2006): the evaluation these authors used
assigned bug reports randomly to training and
evaluation sets, discarding the temporal sequenc-
ing of the data stream.
Bhattacharya and Neamtiu (2010) explicitly
address the issue of online training and evalua-
tion. In their setup, the system predicts the out-
put for an item based only on items preceding it
in time. However, their approach to incremen-
tal learning is simplistic: they use a batch clas-
sifier, but retrain it from scratch after receiving
each training example. A fully retrained batch
classifier will adapt only slowly to changing data
stream, as more recent example have no more in-
fluence on the decision function that less recent
ones.
Tamrawi et al. (2011) propose an incremental
approach to bug triage: the classes are ranked
according to a fuzzy set membership function,
which is based on incrementally updated fea-
ture/class co-occurrence counts. The model is ef-
ficient in online classification, but also adapts only
slowly.
</bodyText>
<subsectionHeader confidence="0.987437">
1.4 Online learning
</subsectionHeader>
<bodyText confidence="0.999962714285714">
This paucity of research on online learning from
issue tracker streams is rather surprising, given
that truly incremental learners have been well-
known for many years. In fact one of the first
learning algorithms proposed was Rosenblatt’s
perceptron, a simple mistake-driven discrimina-
tive classification algorithm (Rosenblatt 1958). In
the current paper we address this situation and
show that by using simple, standard online learn-
ing methods we can improve on batch or pseudo-
online learning. We also show that when using
a sophisticated state-of-the-art stochastic gradient
descent technique the performance gains can be
quite large.
</bodyText>
<subsectionHeader confidence="0.987108">
1.5 Contributions
</subsectionHeader>
<bodyText confidence="0.999969266666667">
Our main contributions are the following: Firstly,
we explicitly show that concept-drift is pervasive
and serious in real bug report streams. We then
address this problem by leveraging state-of-the-
art online learning techniques which automati-
cally track the evolving data stream and incremen-
tally update the model after each data item. We
also adopt the continuous evaluation paradigm,
where the learner predicts the output for each ex-
ample before using it to update the model. Sec-
ondly, we address the important issue of repro-
ducibility in research in bug triage automation
by making available the data sets which we col-
lected and used, in both their raw and prepro-
cessed forms.
</bodyText>
<sectionHeader confidence="0.75173" genericHeader="method">
2 Open issue-tracker data
</sectionHeader>
<bodyText confidence="0.999729875">
Open source software repositories and their as-
sociated issue trackers are a naturally occurring
source of large amounts of (partially) labeled data.
There seems to be growing interest in exploiting
this rich resource as evidenced by existing publi-
cations as well as the appearance of a dedicated
workshop (Working Conference on Mining Soft-
ware Repositories).
In spite of the fact that the data is publicly avail-
able in open repositories, it is not possible to di-
rectly compare the results of the research con-
ducted on bug triage so far: authors use non-
trivial project-specific filtering, re-labeling and
pre-processing heuristics; these steps are usually
not specified in enough detail that they could be
easily reproduced.
</bodyText>
<page confidence="0.993308">
614
</page>
<figure confidence="0.961189733333333">
Field
Meaning
Issue ID
Short description of issue
of the closed statuses. We generated two data sets
from the Chromium issues:
Identifier
Title
Description
Author
CCS
Labels
Status
Assigned To
Published
</figure>
<bodyText confidence="0.939930866666666">
Content of issue report, which
may include steps to reproduce,
error messages, stack traces etc.
ID of report submitter
List of IDs of people CC’d on
the issue report
List of tags associated with is-
sue
Label describing the current sta-
tus of the issue (e.g. Invalid,
Fixed, Won’t Fix)
ID of person who has been as-
signed to deal with the issue
Date on which issue report was
submitted
</bodyText>
<tableCaption confidence="0.99408">
Table 1: Issue report record
</tableCaption>
<bodyText confidence="0.996010870967742">
To help remedy this situation we decided to col-
lect data from several open issue trackers, use the
minimal amount of simple preprocessing and fil-
ter heuristics to get useful input data, and publicly
share both the raw and preprocessed data.
We designed a simple record type which acts
as a common denominator for several tracker for-
mats. Thus we can use a common representation
for issue reports from various trackers. The fields
in our record are shown in Table 1.
Below we describe the issue trackers used
and the datasets we build from them. As dis-
cussed above (and in more detail in Section 4.1),
we use progressive validation rather than a split
into training and test set. However, in order
to avoid developing on the test data, we split
each data stream into two substreams, by assign-
ing odd-numbered examples to the test stream
and the even-numbered ones to the development
stream. We can use the development stream for
exploratory data analysis and feature and param-
eter tuning, and then use progressive validation to
evaluate on entirely unseen test data. Below we
specify the size and number of unique labels in
the development sets; the test sets are very similar
in size.
Chromium Chromium is the open source-
project behind Google’s Chrome browser
(http://code.google.com/p/
chromium/). We retrieved all the bugs
from the issue tracker, of which 66,704 have one
</bodyText>
<listItem confidence="0.7439115">
• Chromium SUBCOMPONENT. Chromium
uses special tags to help triage the bug re-
ports. Tags prefixed with Area- specify
which subcomponent of the project the bug
should be routed to. In some cases more
than one Area- tag is present. Since this
</listItem>
<bodyText confidence="0.95368525">
affects less than 1% of reports, for simplic-
ity we treat these as single, compound labels.
The development set contains 31,953 items,
and 75 unique output labels.
</bodyText>
<listItem confidence="0.98105">
• Chromium ASSIGNED. In this dataset the
output is the value of the assignedTo
</listItem>
<bodyText confidence="0.998531181818182">
field. We discarded issues where the
field was left empty, as well as the
ones which contained the placeholder value
all-bugs-test.chromium.org. The
development set contains 16,154 items and
591 unique output labels.
Android Android is a mobile operating sys-
tem project (http://code.google.com/
p/android/). We retrieved all the bugs reports,
of which 6,341 had a closed status. We generated
two datasets:
</bodyText>
<listItem confidence="0.829584">
• Android SUBCOMPONENT. The reports
which are labeled with tags prefixed with
Component-. The development set con-
tains 888 items and 12 unique output labels.
• Android ASSIGNED. The output label is the
value of the assignedTo field. We dis-
carded issues with the field left empty. The
development set contains 718 items and 72
unique output labels.
Firefox Firefox is the well-known web-browser
project (https://bugzilla.mozilla.
org).
</listItem>
<bodyText confidence="0.9377485">
We obtained a total of 81,987 issues with a
closed status.
</bodyText>
<listItem confidence="0.952858">
• Firefox ASSIGNED. We discarded issues
</listItem>
<bodyText confidence="0.938112428571429">
where the field was left empty, as well as
the ones which contained a placeholder value
(nobody). The development set contains
12,733 items and 503 unique output labels.
Launchpad Launchpad is an issue tracker
run by Canonical Ltd for mostly Ubuntu-related
projects (https://bugs.launchpad.
</bodyText>
<page confidence="0.99488">
615
</page>
<bodyText confidence="0.881074">
net/). We obtained a total of 99,380 issues with
a closed status.
</bodyText>
<listItem confidence="0.9568">
• Launchpad ASSIGNED. We discarded issues
</listItem>
<bodyText confidence="0.817629666666667">
where the field was left empty. The devel-
opment set contains 18,634 items and 1,970
unique output labels.
</bodyText>
<subsectionHeader confidence="0.656826">
3 Analysis of concept drift
</subsectionHeader>
<bodyText confidence="0.999980974358974">
In the introduction we have hypothesized that in
issue tracker streams concept drift would be an
especially acute problem. In this section we show
how class distributions evolve over time in the
data we collected.
A time-varying distribution is difficult to sum-
marize with a single number, but it is easy to ap-
preciate in a graph. Figures 1 and 2 show concept
drift for several of our data streams. The horizon-
tal axis indexes the position in the data stream.
The vertical axis shows the class proportions at
each position, averaged over a window containing
7% of all the examples in the stream, i.e. in each
thin vertical bar the proportion of colors used cor-
responds to the smoothed class distribution at a
particular position in the stream.
Consider the plot for Chromium SUBCOMPO-
NENT. We can see that a bit before the middle
point in the stream class proportions change quite
dramatically: The orange BROWSERUI and vio-
let MISC almost disappears, while blue INTER-
NALS, pink UI and dark red UNDEFINED take
over. This likely corresponds to an overhaul in the
label inventory and/or recommended best practice
for triage in this project. There are also more
gradual and smaller scale changes throughout the
data stream.
The Android SUBCOMPONENT stream con-
tains much less data so the plot is less smooth, but
there are clear transitions in this image also. We
see that light blue GOOGLE all but disappears after
about two thirds point and the proportion of vio-
let TOOLS and light-green DALVIK dramatically
increases.
In Figure 2 we see the evolution of class pro-
portions in the ASSIGNED datasets. Each plot’s
idiosyncratic shape illustrates that there is wide
variation in the amount and nature of concept drift
in different software project issue trackers.
</bodyText>
<figureCaption confidence="0.982346">
Figure 1: SUBCOMPONENT class distribution change
over time
</figureCaption>
<sectionHeader confidence="0.99662" genericHeader="method">
4 Experimental results
</sectionHeader>
<bodyText confidence="0.999695333333333">
In an online setting it is important to use an evalu-
ation regime which closely mimics the continuous
use of the system in a real-life situation.
</bodyText>
<subsectionHeader confidence="0.981007">
4.1 Progressive validation
</subsectionHeader>
<bodyText confidence="0.99982685">
When learning from data streams the standard
evaluation methodology where data is split into a
separate training and test set is not applicable. An
evaluation regime know as progressive validation
has been used to accurately measure the general-
ization performance of online algorithms (Blum
et al. 1999). Under progressive evaluation, an in-
put example from a temporally ordered sequence
is sent to the learner, which returns the prediction.
The error incurred on this example is recorded,
and the true output is only then sent to the learner
which may update its model based on it. The fi-
nal error is the mean of the per-example errors.
Thus even though there is no separate test set, the
prediction for each input is generated based on a
model trained on examples which do not include
it.
In previous work on bug report triage, Bhat-
tacharya and Neamtiu (2010) and Tamrawi et al.
(2011) used an evaluation scheme (close to) pro-
</bodyText>
<page confidence="0.996472">
616
</page>
<figureCaption confidence="0.998277">
Figure 2: ASSIGNED class distribution change over time
</figureCaption>
<bodyText confidence="0.425844">
gressive validation. They omit the initial 1 th of
</bodyText>
<page confidence="0.507876">
11
</page>
<bodyText confidence="0.682071">
the examples from the mean.
</bodyText>
<subsectionHeader confidence="0.957699">
4.2 Mean reciprocal rank
</subsectionHeader>
<bodyText confidence="0.992949416666666">
A bug report triaging agent is most likely to be
used in a semi-automatic workflow, where a hu-
man triager is presented with a ranked list of
possible outputs (component labels or developer
IDs). As such it is important to evaluate not only
accuracy of the top ranking suggesting, but rather
the quality of the whole ranked list.
Previous research (Bhattacharya and Neamtiu
2010, Tamrawi et al. 2011) made an attempt at
approximating this criterion by reporting scores
which indicate whether the true output is present
in the top n elements of the ranking, for several
values of n. Here we suggest borrowing the mean
reciprocal rank (MRR) metric from the informa-
tion retrieval domain (Voorhees 2000). It is de-
fined as the mean of the reciprocals of the rank at
which the true output is found:
rank(i)−1
where rank(i) indicates the rank of the ith true
output. MRR has the advantage of providing a
single number which summarizes the quality of
whole rankings for all the examples. MRR is also
a special case of Mean Average Precision when
there is only one true output per item.
</bodyText>
<subsectionHeader confidence="0.990071">
4.3 Input representation
</subsectionHeader>
<bodyText confidence="0.999898285714286">
Since in this paper we focus on the issues related
to concept drift and online learning, we kept the
feature set relatively simple. We preprocess the
text in the issue report title and description fields
by removing HTML markup, tokenizing, lower-
casing and removing most punctuation. We then
extracted the following feature types:
</bodyText>
<listItem confidence="0.9977704">
• Title unigram and bigram counts
• Description unigram and bigram counts
• Author ID (binary indicator feature)
• Year, month and day of submission (binary
indicator features)
</listItem>
<subsectionHeader confidence="0.989153">
4.4 Models
</subsectionHeader>
<bodyText confidence="0.997437285714286">
We tested a simple online baseline, a pseudo-
online algorithm which uses a batch model and
repeatedly retrains it, an online model used in pre-
vious research on bug triage and two generic on-
line learning algorithms.
Window Frequency Baseline This baseline
does not use any input features. It outputs the
</bodyText>
<equation confidence="0.931143">
1
MRR =
N
N
Z=1
</equation>
<page confidence="0.989804">
617
</page>
<bodyText confidence="0.999961260869565">
ranked list of labels for the current item based
on the relative frequencies of output labels in the
window of k previous items. We tested windows
of size 100 and 1000 and report the better result.
SVM Minibatch This model uses the mul-
ticlass linear Support Vector Machine model
(Crammer and Singer 2002) as implemented in
SVM Light (Joachims 1999). SVM is known
as a state-of-the-art batch model in classification
in general and in text categorization in particu-
lar. The output classes for an input example are
ranked according to the value of the discriminant
values returned by the SVM classifier. In order
to adapt the model to an online setting we retrain
it every n examples on the window of k previous
examples. The parameters n and k can have large
influence on the prediction, but it is not clear how
to set them when learning from streams. Here we
chose the values (100,1000) based on how feasi-
ble the run time was and on the performance dur-
ing exploratory experiments on Chromium SUB-
COMPONENT. Interestingly, keeping the window
parameter relatively small helps performance: a
window of 1,000 works better than a window of
5,000.
Perceptron We implemented a single-pass on-
line multiclass Perceptron with a constant learn-
ing rate. It maintains a weight vector for each
output seen so far: the prediction function ranks
outputs according to the inner product of the cur-
rent example with the corresponding weight vec-
tor. The update function takes the true output and
the predicted output. If they are not equal, the
current input is subtracted from the weight vector
corresponding to the predicted output and added
to the weight vector corresponding to the true out-
put (see Algorithm 1). We hash each feature to an
integer value and use it as the feature’s index in
the weight vectors in order to bound memory us-
age in an online setting (Weinberger et al. 2009).
The Perceptron is a simple but strong baseline for
online learning.
Bugzie This is the model described in Tamrawi
et al. (2011). The output classes are ranked ac-
cording to the fuzzy set membership function de-
fined as follows:
</bodyText>
<equation confidence="0.944443636363636">
� � n(y, x)
µ(y, X) = 1− 1 −
x∈X
n(y) + n(x) − n(y, x)
Algorithm 1 Multiclass online perceptron
function PREDICT(Y, W, x)
return {(y, WTy x)  |y ∈ Y }
procedure UPDATE(W, x, y, y)
if y� =6 y then
Wy ← Wy − x
Wy ← Wy + x
</equation>
<bodyText confidence="0.999886357142857">
where y is the output label, X the set of features
in the input issue report, n(y, x) the number of ex-
amples labeled as y which contain feature x, n(y)
number of examples labeled y and n(x) number
of examples containing feature x. The counts are
updated online. Tamrawi et al. (2011) also use
two so called caches: the label cache keeps the
j% most recent labels and the term cache the k
most significant features for each label. Since
in Tamrawi et al. (2011)’s experiments the label
cache did not affect the results significantly, here
we always set j to 100%. We select the optimal
k parameter from {100, 1000, 5000} based on the
development set.
</bodyText>
<sectionHeader confidence="0.818362" genericHeader="method">
Regression with Stochastic Gradient Descent
</sectionHeader>
<bodyText confidence="0.994510571428571">
This model performs online multiclass learning
by means of a reduction to regression. The re-
gressor is a linear model trained using Stochastic
Gradient Descent (Zhang 2004). SGD updates the
current parameter vector w(t) based on the gradi-
ent of the loss incurred by the regressor on the
current example (x(t), y(t)):
</bodyText>
<equation confidence="0.99485">
w(t+1) = w(t) − q(t)∇L(y(t),w(t)Tx(t))
</equation>
<bodyText confidence="0.881927666666667">
The parameter q(t) is the learning rate at time t,
and L is the loss function. We use the squared
loss:
</bodyText>
<equation confidence="0.96223">
L(y, y) = (y − 0)2
</equation>
<bodyText confidence="0.9998285">
We reduce multiclass learning to regression us-
ing a one-vs-all-type scheme, by effectively trans-
forming an example (x, y) ∈ X × Y into |Y |
(x0, y0) ∈ X0 × {0, 1} examples, where Y is the
set of labels seen so far. The transform T is de-
fined as follows:
</bodyText>
<equation confidence="0.991157">
T (x, y) = {(x0, I(y = y0))  |y0 ∈ Y, x0h(z,y&apos;) = xz}
</equation>
<bodyText confidence="0.99972975">
where h(i, y0) composes the index i with the label
y0 (by hashing).
For a new input x the ranking of the outputs
y ∈ Y is obtained according to the value of the
</bodyText>
<page confidence="0.996934">
618
</page>
<bodyText confidence="0.99996">
prediction of the base regressor on the binary ex-
ample corresponding to each class label.
As our basic regression learner we use the ef-
ficient implementation of regression via SGD,
Vowpal Wabbit (VW) (Langford et al. 2011). VW
implements setting adaptive individual learning
rates for each feature as proposed by Duchi et al.
(2010), McMahan and Streeter (2010).
This is appropriate when there are many sparse
features, and is especially useful in learning from
text from fast evolving data. The features such
as unigram and bigram counts that we rely on are
notoriously sparse, and this is exacerbated by the
change over time in bug report streams.
</bodyText>
<subsectionHeader confidence="0.554905">
4.5 Results
</subsectionHeader>
<bodyText confidence="0.999927">
Figures 3 and 4 show the progressive validation
results on all the development data streams. The
horizontal lines indicate the mean MRR scores for
the whole stream. The curves show a moving av-
erage of MRR in a window comprised of 7% of
the total number of items. In most of the plots it is
evident how the prediction performance depends
on the concept drift illustrated in the plots in Sec-
tion 3: for example on Chromium SUBCOMPO-
NENT the performance of all the models drops a
bit before the midpoint in the stream while the
learners adapt to the change in label distribution
that is happening at this time. This is especially
pronounced for Bugzie, since it is not able to learn
from mistakes and adapt rapidly, but simply accu-
mulates counts.
For five out of the six datasets, Regression SGD
gives the best overall performance. On Launch-
pad ASSIGNED, Bugzie scores higher – we inves-
tigate this anomaly below.
Another observation is that the window-based
frequency baseline can be quite hard to beat:
In three out of the six cases, the minibatch
SVM model is no better than the baseline.
Bugzie sometimes performs quite well, but for
Chromium SUBCOMPONENT and Firefox AS-
SIGNED it scores below the baseline.
Regarding the quality of the different datasets,
an interesting indicator is the relative error reduc-
tion by the best model over the baseline (see Ta-
ble 2). It is especially hard to extract meaning-
ful information about the labeling from the inputs
on the Firefox ASSIGNED dataset. One possible
cause of this can be that the assignment labeling
practices in this project are not consistent: this im-
</bodyText>
<table confidence="0.377085">
Dataset RER
</table>
<tableCaption confidence="0.9704395">
Table 2: Best model’s error relative to baseline on the
development set
</tableCaption>
<table confidence="0.999892454545455">
Task Model MRR Acc
Chromium Window 0.5747 0.3467
SVM 0.5766 0.4535
Perceptron 0.5793 0.4393
Bugzie 0.4971 0.2638
Regression 0.7271 0.5672
Android Window 0.5209 0.3080
SVM 0.5459 0.4255
Perceptron 0.5892 0.4390
Bugzie 0.6281 0.4614
Regression 0.7012 0.5610
</table>
<tableCaption confidence="0.979743">
Table 3: SUBCOMPONENT evaluation results on test
set.
</tableCaption>
<bodyText confidence="0.999880481481482">
pression seems to be born out by informal inspec-
tion.
On the other hand as the scores in Table 2
indicate, Chromium SUBCOMPONENT, Android
SUBCOMPOMENT and Launchpad ASSIGNED
contain enough high-quality signal for the best
model to substantially outperform the label fre-
quency baseline.
On Launchpad ASSIGNED Regression SGD
performs worse than Bugzie. The concept drift
plot for these data suggests one reason: there is
very little change in class distribution over time
as compared to the other datasets. In fact, even
though the issue reports in Launchpad range from
year 2005 to 2011, the more recent ones are heav-
ily overrepresented: 84% of the items in the de-
velopment data are from 2011. Thus fast adap-
tation is less important in this case and Bugzie is
able to perform well.
On the other hand, the reason for the less than
stellar score achieved with Regression SGD is due
to another special feature of this dataset: it has
by far the largest number of labels, almost 2,000.
This degrades the performance for the one-vs-all
scheme we use with SGD Regression. Prelim-
inary investigation indicates that the problem is
mostly caused by our application of the “hash-
</bodyText>
<table confidence="0.986518607142857">
Chromium SUB 0.36
Android SUB 0.38
Chromium AS 0.21
Android AS 0.19
Firefox AS 0.16
Launchpad AS 0.49
619
Task Model MRR Acc
Chromium Window 0.0999 0.0472
SVM 0.0908 0.0550
Perceptron 0.1817 0.1128
Bugzie 0.2063 0.0960
Regression 0.3074 0.2157
Android Window 0.3198 0.1684
SVM 0.2541 0.1684
Perceptron 0.3225 0.2057
Bugzie 0.3690 0.2086
Regression 0.4446 0.2951
Firefox Window 0.5695 0.4426
SVM 0.4604 0.4166
Perceptron 0.5191 0.4306
Bugzie 0.5402 0.4100
Regression 0.6367 0.5245
Launchpad Window 0.0725 0.0337
SVM 0.1006 0.0704
Perceptron 0.3323 0.2607
Bugzie 0.5271 0.4339
Regression 0.4702 0.3879
</table>
<tableCaption confidence="0.965252">
Table 4: ASSIGNED evaluation results on test set
</tableCaption>
<figureCaption confidence="0.979515">
Figure 3: SUBCOMPONENT evaluation results on the
development set
</figureCaption>
<bodyText confidence="0.999985090909091">
ing trick” to feature-label pairs (see section 4.4),
which leads to excessive collisions with very large
label sets. Our current implementation can use at
most 29 bit-sized hashes which is insufficient for
datasets like Launchpad ASSIGNED. We are cur-
rently removing this limitation and we expect it
will lead to substantial gains on massively multi-
class problems.
In Tables 3 and 4 we present the overall MRR
results on the test data streams. The picture is sim-
ilar to the development data discussed above.
</bodyText>
<sectionHeader confidence="0.992856" genericHeader="evaluation">
5 Discussion and related work
</sectionHeader>
<bodyText confidence="0.999984837209302">
Our results show that by choosing the appropri-
ate learner for the scenario of learning from data
streams, we can achieve much better results than
by attempting to twist batch algorithm to fit the
online learning setting. Even a simple and well-
know algorithm such as Perceptron can be effec-
tive, but by using recent advances in research on
SGD algorithms we can obtain substantial im-
provements on the best previously used approach.
Below we review the research on bug report triage
most relevant to our work.
ˇCubrani´c and Murphy (2004) seems to be the
first attempt to automate bug triage. The authors
cast bug triage as a text classification task and use
the data representation (bag of words) and learn-
ing algorithm (Naive Bayes) typical for text clas-
sification at the time. They collect over 15,000
bug reports from the Eclipse project. The max-
imum accuracy they report is 30% which was
achieved by using 90% of the data for training.
In Anvik et al. (2006) the authors experiment
with three learning algorithms: Naive Bayes,
SVM and Decision Tree: SVM performs best in
their experiments. They evaluate using precision
and recall rather than accuracy. They report re-
sults on the Eclipse and Firefox projects, with pre-
cision 57% and 64% respectively, but very low re-
call (7% and 2%).
Matter et al. (2009) adopt a different approach
to bug triage. In addition to the project’s issue
tracker data, they use also the source-code ver-
sion control data. They build an expertise model
for each developer which is a word count vec-
tor of the source code changes committed. They
also build a word count vector for each bug report,
and use the cosine between the report and the ex-
pertise model to rank developers. Using this ap-
proach (with a heuristic term weighting scheme)
they report 33.6% accuracy on Eclipse.
Bhattacharya and Neamtiu (2010) acknowl-
edge the evolving nature of bug report streams
and attempt to apply incremental learning meth-
ods to bug triage. They use a two-step approach:
</bodyText>
<page confidence="0.995211">
620
</page>
<figureCaption confidence="0.998684">
Figure 4: ASSIGNED evaluation results on the development set
</figureCaption>
<bodyText confidence="0.9999697">
first they predict the most likely developer to as-
sign to a bug using a classifier. In a second step
they rank candidate developers according to how
likely they were to take over a bug from the de-
veloper predicted in the first step. Their approach
to incremental learning simply involves fully re-
training a batch classifier after each item in the
data stream. They test their approach on fixed
bugs in Mozilla and Eclipse, reporting accuracies
of 27.5% and 38.2% respectively.
Tamrawi et al. (2011) propose the Bugzie
model where developers are ranked according to
the fuzzy set membership function as defined
in section 4.4. They also use the label (devel-
oper) cache and term cache to speed up pro-
cessing and make the model adapt better to the
evolving data stream. They evaluate Bugzie and
compare its performance to the models used in
Bhattacharya and Neamtiu (2010) on seven issue
trackers: Bugzie has superior performance on all
of them ranging from 29.9% to 45.7% for top-1
output. They do not use separate validation sets
for system development and parameter tuning.
In comparison to Bhattacharya and Neamtiu
(2010) and Tamrawi et al. (2011), here we focus
much more on the analysis of concept drift in data
streams and on the evaluation of learning under its
constraints. We also show that for evolving issue
tracker data, in a large majority of cases SGD Re-
gression handily outperforms Bugzie.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999993769230769">
We demonstrate that concept drift is a real, perva-
sive issue for learning from issue tracker streams.
We show how to adapt to it by leveraging recent
research in online learning algorithms. We also
make our dataset collection publicly available to
enable direct comparisons between different bug
triage systems.1
We have identified a good learning framework
for mining bug reports: in future we would like
to explore smarter ways of extracting useful sig-
nals from the data by using more linguistically
informed preprocessing and higher-level features
such as word classes.
</bodyText>
<sectionHeader confidence="0.998846" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99619925">
This work was carried out in the context of
the Software-Cluster project EMERGENT and was
partially funded by BMBF under grant number
01IC10S01O.
</bodyText>
<footnote confidence="0.99707">
1Available from http://goo.gl/ZquBe
</footnote>
<page confidence="0.997701">
621
</page>
<sectionHeader confidence="0.995842" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997922914634147">
Anvik, J., Hiew, L., and Murphy, G. (2006). Who
should fix this bug? In Proceedings of the 28th
international conference on Software engineer-
ing, pages 361–370. ACM.
Bhattacharya, P. and Neamtiu, I. (2010). Fine-
grained incremental learning and multi-feature
tossing graphs to improve bug triaging. In
International Conference on Software Mainte-
nance (ICSM), pages 1–10. IEEE.
Blum, A., Kalai, A., and Langford, J. (1999).
Beating the hold-out: Bounds for k-fold and
progressive cross-validation. In Proceedings
of the twelfth annual conference on Computa-
tional learning theory, pages 203–208. ACM.
Crammer, K. and Singer, Y. (2002). On the al-
gorithmic implementation of multiclass kernel-
based vector machines. The Journal of Ma-
chine Learning Research, 2:265–292.
Duchi, J., Hazan, E., and Singer, Y. (2010). Adap-
tive subgradient methods for online learning
and stochastic optimization. Journal of Ma-
chine Learning Research.
Halpin, H., Robu, V., and Shepherd, H. (2007).
The complex dynamics of collaborative tag-
ging. In Proceedings of the 16th international
conference on World Wide Web, pages 211–
220. ACM.
Joachims, T. (1999). Making large-scale svm
learning practical. In Sch¨olkopf, B., Burges,
C., and Smola, A., editors, Advances in Kernel
Methods-Support Vector Learning. MIT-Press.
Langford, J., Hsu, D., Karampatziakis, N.,
Chapelle, O., Mineiro, P., Hoffman, M.,
Hofman, J., Lamkhede, S., Chopra, S.,
Faigon, A., Li, L., Rios, G., and Strehl,
A. (2011). Vowpal wabbit. https:
//github.com/JohnLangford/
vowpal_wabbit/wiki.
Matter, D., Kuhn, A., and Nierstrasz, O. (2009).
Assigning bug reports using a vocabulary-
based expertise model of developers. In Sixth
IEEE Working Conference on Mining Software
Repositories.
McMahan, H. and Streeter, M. (2010). Adap-
tive bound optimization for online convex op-
timization. Arxiv preprint arXiv:1002.4908.
Rosenblatt, F. (1958). The perceptron: A prob-
abilistic model for information storage and or-
ganization in the brain. Psychological review,
65(6):386.
Tamrawi, A., Nguyen, T., Al-Kofahi, J., and
Nguyen, T. (2011). Fuzzy set and cache-based
approach for bug triaging. In Proceedings of
the 19th ACM SIGSOFT symposium and the
13th European conference on Foundations of
software engineering, pages 365–375. ACM.
Tsymbal, A. (2004). The problem of concept
drift: definitions and related work. Computer
Science Department, Trinity College Dublin.
Voorhees, E. (2000). The TREC-8 question an-
swering track report. NIST Special Publication,
pages 77–82.
Weinberger, K., Dasgupta, A., Langford, J.,
Smola, A., and Attenberg, J. (2009). Feature
hashing for large scale multitask learning. In
Proceedings of the 26th Annual International
Conference on Machine Learning, pages 1113–
1120. ACM.
Widmer, G. and Kubat, M. (1996). Learning in the
presence of concept drift and hidden contexts.
Machine learning, 23(1):69–101.
Zhang, T. (2004). Solving large scale linear
prediction problems using stochastic gradient
descent algorithms. In Proceedings of the
twenty-first international conference on Ma-
chine learning, page 116. ACM.
ˇCubrani´c, D. and Murphy, G. C. (2004). Auto-
matic bug triage using text categorization. In
In SEKE 2004: Proceedings of the Sixteenth In-
ternational Conference on Software Engineer-
ing &amp; Knowledge Engineering, pages 92–97.
KSI Press.
</reference>
<page confidence="0.997946">
622
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.404198">
<title confidence="0.817604">Learning from evolving data streams: online triage of bug reports Grzegorz Spoken Language</title>
<author confidence="0.452238">Saarland</author>
<email confidence="0.997512">gchrupala@lsv.uni-saarland.de</email>
<abstract confidence="0.994312285714286">Open issue trackers are a type of social media that has received relatively little attention from the text-mining community. We investigate the problems inherent in learning to triage bug reports from time-varying data. We demonstrate that concept drift is an important consideration. We show the effectiveness of online learning algorithms by evaluating them on several bug report datasets collected from open issue trackers associated with large open-source projects. We make this collection of data publicly available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Anvik</author>
<author>L Hiew</author>
<author>G Murphy</author>
</authors>
<title>Who should fix this bug?</title>
<date>2006</date>
<booktitle>In Proceedings of the 28th international conference on Software engineering,</booktitle>
<pages>361--370</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1330" citStr="Anvik et al. 2006" startWordPosition="200" endWordPosition="203">s been relatively little research to date on applying machine learning and Natural Language Processing techniques to automate software project workflows. In this paper we address the problem of bug report triage. 1.1 Issue tracking Large software projects typically track defect reports, feature requests and other issue reports using an issue tracker system. Open source projects tend to use trackers which are open to both developers and users. If the product has many users its tracker can receive an overwhelming number of issue reports: Mozilla was receiving almost 300 reports per day in 2006 (Anvik et al. 2006). Someone has to monitor those reports and triage them, that is decide which component they affect and which developer or team of developers should be responsible for analyzing them and fixing the reported defects. An automated agent assisting the staff responsible for such triage has the potential to substantially reduce the time and cost of this task. 1.2 Issue trackers as social media In a large software project with a loose, not strictly hierarchical organization, standards and practices are not exclusively imposed top-down but also tend to spontaneously arise in a bottomup fashion, arrive</context>
<context position="4746" citStr="Anvik et al. 2006" startWordPosition="748" endWordPosition="752">bution from which the examples come are emphatically not stationary. As a software project progresses and matures, the type of issues reported is going to change. As project members and users come and go, the vocabulary they use to describe the issues will vary. As the consensus tag folksonomy emerges, the label and training example distribution will evolve. This phenomenon is sometimes referred to as concept drift (Widmer and Kubat 1996, Tsymbal 2004). Early research on learning to triage tended to either not notice the problem (ˇCubrani´c and Murphy 2004), or acknowledge but not address it (Anvik et al. 2006): the evaluation these authors used assigned bug reports randomly to training and evaluation sets, discarding the temporal sequencing of the data stream. Bhattacharya and Neamtiu (2010) explicitly address the issue of online training and evaluation. In their setup, the system predicts the output for an item based only on items preceding it in time. However, their approach to incremental learning is simplistic: they use a batch classifier, but retrain it from scratch after receiving each training example. A fully retrained batch classifier will adapt only slowly to changing data stream, as more</context>
<context position="26825" citStr="Anvik et al. (2006)" startWordPosition="4462" endWordPosition="4465"> algorithms we can obtain substantial improvements on the best previously used approach. Below we review the research on bug report triage most relevant to our work. ˇCubrani´c and Murphy (2004) seems to be the first attempt to automate bug triage. The authors cast bug triage as a text classification task and use the data representation (bag of words) and learning algorithm (Naive Bayes) typical for text classification at the time. They collect over 15,000 bug reports from the Eclipse project. The maximum accuracy they report is 30% which was achieved by using 90% of the data for training. In Anvik et al. (2006) the authors experiment with three learning algorithms: Naive Bayes, SVM and Decision Tree: SVM performs best in their experiments. They evaluate using precision and recall rather than accuracy. They report results on the Eclipse and Firefox projects, with precision 57% and 64% respectively, but very low recall (7% and 2%). Matter et al. (2009) adopt a different approach to bug triage. In addition to the project’s issue tracker data, they use also the source-code version control data. They build an expertise model for each developer which is a word count vector of the source code changes commi</context>
</contexts>
<marker>Anvik, Hiew, Murphy, 2006</marker>
<rawString>Anvik, J., Hiew, L., and Murphy, G. (2006). Who should fix this bug? In Proceedings of the 28th international conference on Software engineering, pages 361–370. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bhattacharya</author>
<author>I Neamtiu</author>
</authors>
<title>Finegrained incremental learning and multi-feature tossing graphs to improve bug triaging.</title>
<date>2010</date>
<booktitle>In International Conference on Software Maintenance (ICSM),</booktitle>
<pages>1--10</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4931" citStr="Bhattacharya and Neamtiu (2010)" startWordPosition="776" endWordPosition="779">t members and users come and go, the vocabulary they use to describe the issues will vary. As the consensus tag folksonomy emerges, the label and training example distribution will evolve. This phenomenon is sometimes referred to as concept drift (Widmer and Kubat 1996, Tsymbal 2004). Early research on learning to triage tended to either not notice the problem (ˇCubrani´c and Murphy 2004), or acknowledge but not address it (Anvik et al. 2006): the evaluation these authors used assigned bug reports randomly to training and evaluation sets, discarding the temporal sequencing of the data stream. Bhattacharya and Neamtiu (2010) explicitly address the issue of online training and evaluation. In their setup, the system predicts the output for an item based only on items preceding it in time. However, their approach to incremental learning is simplistic: they use a batch classifier, but retrain it from scratch after receiving each training example. A fully retrained batch classifier will adapt only slowly to changing data stream, as more recent example have no more influence on the decision function that less recent ones. Tamrawi et al. (2011) propose an incremental approach to bug triage: the classes are ranked accord</context>
<context position="14652" citStr="Bhattacharya and Neamtiu (2010)" startWordPosition="2367" endWordPosition="2371">ure the generalization performance of online algorithms (Blum et al. 1999). Under progressive evaluation, an input example from a temporally ordered sequence is sent to the learner, which returns the prediction. The error incurred on this example is recorded, and the true output is only then sent to the learner which may update its model based on it. The final error is the mean of the per-example errors. Thus even though there is no separate test set, the prediction for each input is generated based on a model trained on examples which do not include it. In previous work on bug report triage, Bhattacharya and Neamtiu (2010) and Tamrawi et al. (2011) used an evaluation scheme (close to) pro616 Figure 2: ASSIGNED class distribution change over time gressive validation. They omit the initial 1 th of 11 the examples from the mean. 4.2 Mean reciprocal rank A bug report triaging agent is most likely to be used in a semi-automatic workflow, where a human triager is presented with a ranked list of possible outputs (component labels or developer IDs). As such it is important to evaluate not only accuracy of the top ranking suggesting, but rather the quality of the whole ranked list. Previous research (Bhattacharya and Ne</context>
<context position="27701" citStr="Bhattacharya and Neamtiu (2010)" startWordPosition="4610" endWordPosition="4613">jects, with precision 57% and 64% respectively, but very low recall (7% and 2%). Matter et al. (2009) adopt a different approach to bug triage. In addition to the project’s issue tracker data, they use also the source-code version control data. They build an expertise model for each developer which is a word count vector of the source code changes committed. They also build a word count vector for each bug report, and use the cosine between the report and the expertise model to rank developers. Using this approach (with a heuristic term weighting scheme) they report 33.6% accuracy on Eclipse. Bhattacharya and Neamtiu (2010) acknowledge the evolving nature of bug report streams and attempt to apply incremental learning methods to bug triage. They use a two-step approach: 620 Figure 4: ASSIGNED evaluation results on the development set first they predict the most likely developer to assign to a bug using a classifier. In a second step they rank candidate developers according to how likely they were to take over a bug from the developer predicted in the first step. Their approach to incremental learning simply involves fully retraining a batch classifier after each item in the data stream. They test their approach </context>
<context position="29038" citStr="Bhattacharya and Neamtiu (2010)" startWordPosition="4834" endWordPosition="4837"> (2011) propose the Bugzie model where developers are ranked according to the fuzzy set membership function as defined in section 4.4. They also use the label (developer) cache and term cache to speed up processing and make the model adapt better to the evolving data stream. They evaluate Bugzie and compare its performance to the models used in Bhattacharya and Neamtiu (2010) on seven issue trackers: Bugzie has superior performance on all of them ranging from 29.9% to 45.7% for top-1 output. They do not use separate validation sets for system development and parameter tuning. In comparison to Bhattacharya and Neamtiu (2010) and Tamrawi et al. (2011), here we focus much more on the analysis of concept drift in data streams and on the evaluation of learning under its constraints. We also show that for evolving issue tracker data, in a large majority of cases SGD Regression handily outperforms Bugzie. 6 Conclusion We demonstrate that concept drift is a real, pervasive issue for learning from issue tracker streams. We show how to adapt to it by leveraging recent research in online learning algorithms. We also make our dataset collection publicly available to enable direct comparisons between different bug triage sys</context>
</contexts>
<marker>Bhattacharya, Neamtiu, 2010</marker>
<rawString>Bhattacharya, P. and Neamtiu, I. (2010). Finegrained incremental learning and multi-feature tossing graphs to improve bug triaging. In International Conference on Software Maintenance (ICSM), pages 1–10. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>A Kalai</author>
<author>J Langford</author>
</authors>
<title>Beating the hold-out: Bounds for k-fold and progressive cross-validation.</title>
<date>1999</date>
<booktitle>In Proceedings of the twelfth annual conference on Computational learning theory,</booktitle>
<pages>203--208</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="14095" citStr="Blum et al. 1999" startWordPosition="2269" endWordPosition="2272">concept drift in different software project issue trackers. Figure 1: SUBCOMPONENT class distribution change over time 4 Experimental results In an online setting it is important to use an evaluation regime which closely mimics the continuous use of the system in a real-life situation. 4.1 Progressive validation When learning from data streams the standard evaluation methodology where data is split into a separate training and test set is not applicable. An evaluation regime know as progressive validation has been used to accurately measure the generalization performance of online algorithms (Blum et al. 1999). Under progressive evaluation, an input example from a temporally ordered sequence is sent to the learner, which returns the prediction. The error incurred on this example is recorded, and the true output is only then sent to the learner which may update its model based on it. The final error is the mean of the per-example errors. Thus even though there is no separate test set, the prediction for each input is generated based on a model trained on examples which do not include it. In previous work on bug report triage, Bhattacharya and Neamtiu (2010) and Tamrawi et al. (2011) used an evaluati</context>
</contexts>
<marker>Blum, Kalai, Langford, 1999</marker>
<rawString>Blum, A., Kalai, A., and Langford, J. (1999). Beating the hold-out: Bounds for k-fold and progressive cross-validation. In Proceedings of the twelfth annual conference on Computational learning theory, pages 203–208. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>On the algorithmic implementation of multiclass kernelbased vector machines.</title>
<date>2002</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>2--265</pages>
<contexts>
<context position="17132" citStr="Crammer and Singer 2002" startWordPosition="2792" endWordPosition="2795"> tested a simple online baseline, a pseudoonline algorithm which uses a batch model and repeatedly retrains it, an online model used in previous research on bug triage and two generic online learning algorithms. Window Frequency Baseline This baseline does not use any input features. It outputs the 1 MRR = N N Z=1 617 ranked list of labels for the current item based on the relative frequencies of output labels in the window of k previous items. We tested windows of size 100 and 1000 and report the better result. SVM Minibatch This model uses the multiclass linear Support Vector Machine model (Crammer and Singer 2002) as implemented in SVM Light (Joachims 1999). SVM is known as a state-of-the-art batch model in classification in general and in text categorization in particular. The output classes for an input example are ranked according to the value of the discriminant values returned by the SVM classifier. In order to adapt the model to an online setting we retrain it every n examples on the window of k previous examples. The parameters n and k can have large influence on the prediction, but it is not clear how to set them when learning from streams. Here we chose the values (100,1000) based on how feasi</context>
</contexts>
<marker>Crammer, Singer, 2002</marker>
<rawString>Crammer, K. and Singer, Y. (2002). On the algorithmic implementation of multiclass kernelbased vector machines. The Journal of Machine Learning Research, 2:265–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>E Hazan</author>
<author>Y Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="21120" citStr="Duchi et al. (2010)" startWordPosition="3516" endWordPosition="3519">he set of labels seen so far. The transform T is defined as follows: T (x, y) = {(x0, I(y = y0)) |y0 ∈ Y, x0h(z,y&apos;) = xz} where h(i, y0) composes the index i with the label y0 (by hashing). For a new input x the ranking of the outputs y ∈ Y is obtained according to the value of the 618 prediction of the base regressor on the binary example corresponding to each class label. As our basic regression learner we use the efficient implementation of regression via SGD, Vowpal Wabbit (VW) (Langford et al. 2011). VW implements setting adaptive individual learning rates for each feature as proposed by Duchi et al. (2010), McMahan and Streeter (2010). This is appropriate when there are many sparse features, and is especially useful in learning from text from fast evolving data. The features such as unigram and bigram counts that we rely on are notoriously sparse, and this is exacerbated by the change over time in bug report streams. 4.5 Results Figures 3 and 4 show the progressive validation results on all the development data streams. The horizontal lines indicate the mean MRR scores for the whole stream. The curves show a moving average of MRR in a window comprised of 7% of the total number of items. In most</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2010</marker>
<rawString>Duchi, J., Hazan, E., and Singer, Y. (2010). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Halpin</author>
<author>V Robu</author>
<author>H Shepherd</author>
</authors>
<title>The complex dynamics of collaborative tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>211--220</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2824" citStr="Halpin et al. 2007" startWordPosition="442" endWordPosition="445">formal taxonomy may arise and evolve in an issue tracker via the use of free-form tags or labels. Developers, testers and users can attach tags to their issue reports in order to informally classify them. The issue tracking software may give users feedback by informing them which tags were frequently used in the past, or suggest tags based on the content of the report or other information. Through this collaborative, feedback driven process involving both human and machine participants, an evolving consensus on the label inventory and semantics typically arises, without much top-down control (Halpin et al. 2007). This kind of emergent taxonomy is known as a folksonomy or collaborative tagging and is very common in the context of social web applications. Large software projects, especially those with open policies and little hierarchical structures, tend to exhibit many of the same emergent social properties as the more prototypical social applications. While this is a useful phenomenon, it presents a special challenge from the machinelearning point of view. 613 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 613–622, Avignon, France, </context>
</contexts>
<marker>Halpin, Robu, Shepherd, 2007</marker>
<rawString>Halpin, H., Robu, V., and Shepherd, H. (2007). The complex dynamics of collaborative tagging. In Proceedings of the 16th international conference on World Wide Web, pages 211– 220. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale svm learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods-Support Vector Learning. MIT-Press.</booktitle>
<editor>In Sch¨olkopf, B., Burges, C., and Smola, A., editors,</editor>
<contexts>
<context position="17176" citStr="Joachims 1999" startWordPosition="2801" endWordPosition="2802">ithm which uses a batch model and repeatedly retrains it, an online model used in previous research on bug triage and two generic online learning algorithms. Window Frequency Baseline This baseline does not use any input features. It outputs the 1 MRR = N N Z=1 617 ranked list of labels for the current item based on the relative frequencies of output labels in the window of k previous items. We tested windows of size 100 and 1000 and report the better result. SVM Minibatch This model uses the multiclass linear Support Vector Machine model (Crammer and Singer 2002) as implemented in SVM Light (Joachims 1999). SVM is known as a state-of-the-art batch model in classification in general and in text categorization in particular. The output classes for an input example are ranked according to the value of the discriminant values returned by the SVM classifier. In order to adapt the model to an online setting we retrain it every n examples on the window of k previous examples. The parameters n and k can have large influence on the prediction, but it is not clear how to set them when learning from streams. Here we chose the values (100,1000) based on how feasible the run time was and on the performance </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Joachims, T. (1999). Making large-scale svm learning practical. In Sch¨olkopf, B., Burges, C., and Smola, A., editors, Advances in Kernel Methods-Support Vector Learning. MIT-Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Langford</author>
<author>D Hsu</author>
<author>N Karampatziakis</author>
<author>O Chapelle</author>
<author>P Mineiro</author>
<author>M Hoffman</author>
<author>J Hofman</author>
<author>S Lamkhede</author>
<author>S Chopra</author>
<author>A Faigon</author>
<author>L Li</author>
<author>G Rios</author>
<author>A Strehl</author>
</authors>
<title>Vowpal wabbit. https: //github.com/JohnLangford/ vowpal_wabbit/wiki.</title>
<date>2011</date>
<contexts>
<context position="21010" citStr="Langford et al. 2011" startWordPosition="3499" endWordPosition="3502">e, by effectively transforming an example (x, y) ∈ X × Y into |Y | (x0, y0) ∈ X0 × {0, 1} examples, where Y is the set of labels seen so far. The transform T is defined as follows: T (x, y) = {(x0, I(y = y0)) |y0 ∈ Y, x0h(z,y&apos;) = xz} where h(i, y0) composes the index i with the label y0 (by hashing). For a new input x the ranking of the outputs y ∈ Y is obtained according to the value of the 618 prediction of the base regressor on the binary example corresponding to each class label. As our basic regression learner we use the efficient implementation of regression via SGD, Vowpal Wabbit (VW) (Langford et al. 2011). VW implements setting adaptive individual learning rates for each feature as proposed by Duchi et al. (2010), McMahan and Streeter (2010). This is appropriate when there are many sparse features, and is especially useful in learning from text from fast evolving data. The features such as unigram and bigram counts that we rely on are notoriously sparse, and this is exacerbated by the change over time in bug report streams. 4.5 Results Figures 3 and 4 show the progressive validation results on all the development data streams. The horizontal lines indicate the mean MRR scores for the whole str</context>
</contexts>
<marker>Langford, Hsu, Karampatziakis, Chapelle, Mineiro, Hoffman, Hofman, Lamkhede, Chopra, Faigon, Li, Rios, Strehl, 2011</marker>
<rawString>Langford, J., Hsu, D., Karampatziakis, N., Chapelle, O., Mineiro, P., Hoffman, M., Hofman, J., Lamkhede, S., Chopra, S., Faigon, A., Li, L., Rios, G., and Strehl, A. (2011). Vowpal wabbit. https: //github.com/JohnLangford/ vowpal_wabbit/wiki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Matter</author>
<author>A Kuhn</author>
<author>O Nierstrasz</author>
</authors>
<title>Assigning bug reports using a vocabularybased expertise model of developers.</title>
<date>2009</date>
<booktitle>In Sixth IEEE Working Conference on Mining Software Repositories.</booktitle>
<contexts>
<context position="27171" citStr="Matter et al. (2009)" startWordPosition="4519" endWordPosition="4522">f words) and learning algorithm (Naive Bayes) typical for text classification at the time. They collect over 15,000 bug reports from the Eclipse project. The maximum accuracy they report is 30% which was achieved by using 90% of the data for training. In Anvik et al. (2006) the authors experiment with three learning algorithms: Naive Bayes, SVM and Decision Tree: SVM performs best in their experiments. They evaluate using precision and recall rather than accuracy. They report results on the Eclipse and Firefox projects, with precision 57% and 64% respectively, but very low recall (7% and 2%). Matter et al. (2009) adopt a different approach to bug triage. In addition to the project’s issue tracker data, they use also the source-code version control data. They build an expertise model for each developer which is a word count vector of the source code changes committed. They also build a word count vector for each bug report, and use the cosine between the report and the expertise model to rank developers. Using this approach (with a heuristic term weighting scheme) they report 33.6% accuracy on Eclipse. Bhattacharya and Neamtiu (2010) acknowledge the evolving nature of bug report streams and attempt to </context>
</contexts>
<marker>Matter, Kuhn, Nierstrasz, 2009</marker>
<rawString>Matter, D., Kuhn, A., and Nierstrasz, O. (2009). Assigning bug reports using a vocabularybased expertise model of developers. In Sixth IEEE Working Conference on Mining Software Repositories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H McMahan</author>
<author>M Streeter</author>
</authors>
<title>Adaptive bound optimization for online convex optimization. Arxiv preprint arXiv:1002.4908.</title>
<date>2010</date>
<contexts>
<context position="21149" citStr="McMahan and Streeter (2010)" startWordPosition="3520" endWordPosition="3523"> so far. The transform T is defined as follows: T (x, y) = {(x0, I(y = y0)) |y0 ∈ Y, x0h(z,y&apos;) = xz} where h(i, y0) composes the index i with the label y0 (by hashing). For a new input x the ranking of the outputs y ∈ Y is obtained according to the value of the 618 prediction of the base regressor on the binary example corresponding to each class label. As our basic regression learner we use the efficient implementation of regression via SGD, Vowpal Wabbit (VW) (Langford et al. 2011). VW implements setting adaptive individual learning rates for each feature as proposed by Duchi et al. (2010), McMahan and Streeter (2010). This is appropriate when there are many sparse features, and is especially useful in learning from text from fast evolving data. The features such as unigram and bigram counts that we rely on are notoriously sparse, and this is exacerbated by the change over time in bug report streams. 4.5 Results Figures 3 and 4 show the progressive validation results on all the development data streams. The horizontal lines indicate the mean MRR scores for the whole stream. The curves show a moving average of MRR in a window comprised of 7% of the total number of items. In most of the plots it is evident h</context>
</contexts>
<marker>McMahan, Streeter, 2010</marker>
<rawString>McMahan, H. and Streeter, M. (2010). Adaptive bound optimization for online convex optimization. Arxiv preprint arXiv:1002.4908.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rosenblatt</author>
</authors>
<title>The perceptron: A probabilistic model for information storage and organization in the brain. Psychological review,</title>
<date>1958</date>
<pages>65--6</pages>
<contexts>
<context position="6076" citStr="Rosenblatt 1958" startWordPosition="958" endWordPosition="959">se an incremental approach to bug triage: the classes are ranked according to a fuzzy set membership function, which is based on incrementally updated feature/class co-occurrence counts. The model is efficient in online classification, but also adapts only slowly. 1.4 Online learning This paucity of research on online learning from issue tracker streams is rather surprising, given that truly incremental learners have been wellknown for many years. In fact one of the first learning algorithms proposed was Rosenblatt’s perceptron, a simple mistake-driven discriminative classification algorithm (Rosenblatt 1958). In the current paper we address this situation and show that by using simple, standard online learning methods we can improve on batch or pseudoonline learning. We also show that when using a sophisticated state-of-the-art stochastic gradient descent technique the performance gains can be quite large. 1.5 Contributions Our main contributions are the following: Firstly, we explicitly show that concept-drift is pervasive and serious in real bug report streams. We then address this problem by leveraging state-of-theart online learning techniques which automatically track the evolving data strea</context>
</contexts>
<marker>Rosenblatt, 1958</marker>
<rawString>Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tamrawi</author>
<author>T Nguyen</author>
<author>J Al-Kofahi</author>
<author>T Nguyen</author>
</authors>
<title>Fuzzy set and cache-based approach for bug triaging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering,</booktitle>
<pages>365--375</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5454" citStr="Tamrawi et al. (2011)" startWordPosition="865" endWordPosition="868">ation sets, discarding the temporal sequencing of the data stream. Bhattacharya and Neamtiu (2010) explicitly address the issue of online training and evaluation. In their setup, the system predicts the output for an item based only on items preceding it in time. However, their approach to incremental learning is simplistic: they use a batch classifier, but retrain it from scratch after receiving each training example. A fully retrained batch classifier will adapt only slowly to changing data stream, as more recent example have no more influence on the decision function that less recent ones. Tamrawi et al. (2011) propose an incremental approach to bug triage: the classes are ranked according to a fuzzy set membership function, which is based on incrementally updated feature/class co-occurrence counts. The model is efficient in online classification, but also adapts only slowly. 1.4 Online learning This paucity of research on online learning from issue tracker streams is rather surprising, given that truly incremental learners have been wellknown for many years. In fact one of the first learning algorithms proposed was Rosenblatt’s perceptron, a simple mistake-driven discriminative classification algor</context>
<context position="14678" citStr="Tamrawi et al. (2011)" startWordPosition="2373" endWordPosition="2376">f online algorithms (Blum et al. 1999). Under progressive evaluation, an input example from a temporally ordered sequence is sent to the learner, which returns the prediction. The error incurred on this example is recorded, and the true output is only then sent to the learner which may update its model based on it. The final error is the mean of the per-example errors. Thus even though there is no separate test set, the prediction for each input is generated based on a model trained on examples which do not include it. In previous work on bug report triage, Bhattacharya and Neamtiu (2010) and Tamrawi et al. (2011) used an evaluation scheme (close to) pro616 Figure 2: ASSIGNED class distribution change over time gressive validation. They omit the initial 1 th of 11 the examples from the mean. 4.2 Mean reciprocal rank A bug report triaging agent is most likely to be used in a semi-automatic workflow, where a human triager is presented with a ranked list of possible outputs (component labels or developer IDs). As such it is important to evaluate not only accuracy of the top ranking suggesting, but rather the quality of the whole ranked list. Previous research (Bhattacharya and Neamtiu 2010, Tamrawi et al.</context>
<context position="18825" citStr="Tamrawi et al. (2011)" startWordPosition="3083" endWordPosition="3086">rrent example with the corresponding weight vector. The update function takes the true output and the predicted output. If they are not equal, the current input is subtracted from the weight vector corresponding to the predicted output and added to the weight vector corresponding to the true output (see Algorithm 1). We hash each feature to an integer value and use it as the feature’s index in the weight vectors in order to bound memory usage in an online setting (Weinberger et al. 2009). The Perceptron is a simple but strong baseline for online learning. Bugzie This is the model described in Tamrawi et al. (2011). The output classes are ranked according to the fuzzy set membership function defined as follows: � � n(y, x) µ(y, X) = 1− 1 − x∈X n(y) + n(x) − n(y, x) Algorithm 1 Multiclass online perceptron function PREDICT(Y, W, x) return {(y, WTy x) |y ∈ Y } procedure UPDATE(W, x, y, y) if y� =6 y then Wy ← Wy − x Wy ← Wy + x where y is the output label, X the set of features in the input issue report, n(y, x) the number of examples labeled as y which contain feature x, n(y) number of examples labeled y and n(x) number of examples containing feature x. The counts are updated online. Tamrawi et al. (2011</context>
<context position="28414" citStr="Tamrawi et al. (2011)" startWordPosition="4731" endWordPosition="4734">ning methods to bug triage. They use a two-step approach: 620 Figure 4: ASSIGNED evaluation results on the development set first they predict the most likely developer to assign to a bug using a classifier. In a second step they rank candidate developers according to how likely they were to take over a bug from the developer predicted in the first step. Their approach to incremental learning simply involves fully retraining a batch classifier after each item in the data stream. They test their approach on fixed bugs in Mozilla and Eclipse, reporting accuracies of 27.5% and 38.2% respectively. Tamrawi et al. (2011) propose the Bugzie model where developers are ranked according to the fuzzy set membership function as defined in section 4.4. They also use the label (developer) cache and term cache to speed up processing and make the model adapt better to the evolving data stream. They evaluate Bugzie and compare its performance to the models used in Bhattacharya and Neamtiu (2010) on seven issue trackers: Bugzie has superior performance on all of them ranging from 29.9% to 45.7% for top-1 output. They do not use separate validation sets for system development and parameter tuning. In comparison to Bhattac</context>
</contexts>
<marker>Tamrawi, Nguyen, Al-Kofahi, Nguyen, 2011</marker>
<rawString>Tamrawi, A., Nguyen, T., Al-Kofahi, J., and Nguyen, T. (2011). Fuzzy set and cache-based approach for bug triaging. In Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering, pages 365–375. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tsymbal</author>
</authors>
<title>The problem of concept drift: definitions and related work.</title>
<date>2004</date>
<institution>Computer Science Department, Trinity College Dublin.</institution>
<contexts>
<context position="4584" citStr="Tsymbal 2004" startWordPosition="722" endWordPosition="723">proach is inadequate. Examples arrive oneby-one in a stream, not as a batch. Even more importantly, both the output (label) distribution and the input distribution from which the examples come are emphatically not stationary. As a software project progresses and matures, the type of issues reported is going to change. As project members and users come and go, the vocabulary they use to describe the issues will vary. As the consensus tag folksonomy emerges, the label and training example distribution will evolve. This phenomenon is sometimes referred to as concept drift (Widmer and Kubat 1996, Tsymbal 2004). Early research on learning to triage tended to either not notice the problem (ˇCubrani´c and Murphy 2004), or acknowledge but not address it (Anvik et al. 2006): the evaluation these authors used assigned bug reports randomly to training and evaluation sets, discarding the temporal sequencing of the data stream. Bhattacharya and Neamtiu (2010) explicitly address the issue of online training and evaluation. In their setup, the system predicts the output for an item based only on items preceding it in time. However, their approach to incremental learning is simplistic: they use a batch classif</context>
</contexts>
<marker>Tsymbal, 2004</marker>
<rawString>Tsymbal, A. (2004). The problem of concept drift: definitions and related work. Computer Science Department, Trinity College Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>The TREC-8 question answering track report.</title>
<date>2000</date>
<journal>NIST Special Publication,</journal>
<pages>77--82</pages>
<contexts>
<context position="15583" citStr="Voorhees 2000" startWordPosition="2527" endWordPosition="2528">where a human triager is presented with a ranked list of possible outputs (component labels or developer IDs). As such it is important to evaluate not only accuracy of the top ranking suggesting, but rather the quality of the whole ranked list. Previous research (Bhattacharya and Neamtiu 2010, Tamrawi et al. 2011) made an attempt at approximating this criterion by reporting scores which indicate whether the true output is present in the top n elements of the ranking, for several values of n. Here we suggest borrowing the mean reciprocal rank (MRR) metric from the information retrieval domain (Voorhees 2000). It is defined as the mean of the reciprocals of the rank at which the true output is found: rank(i)−1 where rank(i) indicates the rank of the ith true output. MRR has the advantage of providing a single number which summarizes the quality of whole rankings for all the examples. MRR is also a special case of Mean Average Precision when there is only one true output per item. 4.3 Input representation Since in this paper we focus on the issues related to concept drift and online learning, we kept the feature set relatively simple. We preprocess the text in the issue report title and description</context>
</contexts>
<marker>Voorhees, 2000</marker>
<rawString>Voorhees, E. (2000). The TREC-8 question answering track report. NIST Special Publication, pages 77–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Weinberger</author>
<author>A Dasgupta</author>
<author>J Langford</author>
<author>A Smola</author>
<author>J Attenberg</author>
</authors>
<title>Feature hashing for large scale multitask learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>1113--1120</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="18696" citStr="Weinberger et al. 2009" startWordPosition="3061" endWordPosition="3064">intains a weight vector for each output seen so far: the prediction function ranks outputs according to the inner product of the current example with the corresponding weight vector. The update function takes the true output and the predicted output. If they are not equal, the current input is subtracted from the weight vector corresponding to the predicted output and added to the weight vector corresponding to the true output (see Algorithm 1). We hash each feature to an integer value and use it as the feature’s index in the weight vectors in order to bound memory usage in an online setting (Weinberger et al. 2009). The Perceptron is a simple but strong baseline for online learning. Bugzie This is the model described in Tamrawi et al. (2011). The output classes are ranked according to the fuzzy set membership function defined as follows: � � n(y, x) µ(y, X) = 1− 1 − x∈X n(y) + n(x) − n(y, x) Algorithm 1 Multiclass online perceptron function PREDICT(Y, W, x) return {(y, WTy x) |y ∈ Y } procedure UPDATE(W, x, y, y) if y� =6 y then Wy ← Wy − x Wy ← Wy + x where y is the output label, X the set of features in the input issue report, n(y, x) the number of examples labeled as y which contain feature x, n(y) n</context>
</contexts>
<marker>Weinberger, Dasgupta, Langford, Smola, Attenberg, 2009</marker>
<rawString>Weinberger, K., Dasgupta, A., Langford, J., Smola, A., and Attenberg, J. (2009). Feature hashing for large scale multitask learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1113– 1120. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Widmer</author>
<author>M Kubat</author>
</authors>
<title>Learning in the presence of concept drift and hidden contexts.</title>
<date>1996</date>
<booktitle>Machine learning,</booktitle>
<pages>23--1</pages>
<contexts>
<context position="4569" citStr="Widmer and Kubat 1996" startWordPosition="718" endWordPosition="721">s the batch learning approach is inadequate. Examples arrive oneby-one in a stream, not as a batch. Even more importantly, both the output (label) distribution and the input distribution from which the examples come are emphatically not stationary. As a software project progresses and matures, the type of issues reported is going to change. As project members and users come and go, the vocabulary they use to describe the issues will vary. As the consensus tag folksonomy emerges, the label and training example distribution will evolve. This phenomenon is sometimes referred to as concept drift (Widmer and Kubat 1996, Tsymbal 2004). Early research on learning to triage tended to either not notice the problem (ˇCubrani´c and Murphy 2004), or acknowledge but not address it (Anvik et al. 2006): the evaluation these authors used assigned bug reports randomly to training and evaluation sets, discarding the temporal sequencing of the data stream. Bhattacharya and Neamtiu (2010) explicitly address the issue of online training and evaluation. In their setup, the system predicts the output for an item based only on items preceding it in time. However, their approach to incremental learning is simplistic: they use </context>
</contexts>
<marker>Widmer, Kubat, 1996</marker>
<rawString>Widmer, G. and Kubat, M. (1996). Learning in the presence of concept drift and hidden contexts. Machine learning, 23(1):69–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
</authors>
<title>Solving large scale linear prediction problems using stochastic gradient descent algorithms.</title>
<date>2004</date>
<booktitle>In Proceedings of the twenty-first international conference on Machine learning,</booktitle>
<pages>116</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="20009" citStr="Zhang 2004" startWordPosition="3306" endWordPosition="3307">nline. Tamrawi et al. (2011) also use two so called caches: the label cache keeps the j% most recent labels and the term cache the k most significant features for each label. Since in Tamrawi et al. (2011)’s experiments the label cache did not affect the results significantly, here we always set j to 100%. We select the optimal k parameter from {100, 1000, 5000} based on the development set. Regression with Stochastic Gradient Descent This model performs online multiclass learning by means of a reduction to regression. The regressor is a linear model trained using Stochastic Gradient Descent (Zhang 2004). SGD updates the current parameter vector w(t) based on the gradient of the loss incurred by the regressor on the current example (x(t), y(t)): w(t+1) = w(t) − q(t)∇L(y(t),w(t)Tx(t)) The parameter q(t) is the learning rate at time t, and L is the loss function. We use the squared loss: L(y, y) = (y − 0)2 We reduce multiclass learning to regression using a one-vs-all-type scheme, by effectively transforming an example (x, y) ∈ X × Y into |Y | (x0, y0) ∈ X0 × {0, 1} examples, where Y is the set of labels seen so far. The transform T is defined as follows: T (x, y) = {(x0, I(y = y0)) |y0 ∈ Y, x0</context>
</contexts>
<marker>Zhang, 2004</marker>
<rawString>Zhang, T. (2004). Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of the twenty-first international conference on Machine learning, page 116. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D ˇCubrani´c</author>
<author>G C Murphy</author>
</authors>
<title>Automatic bug triage using text categorization.</title>
<date>2004</date>
<booktitle>In In SEKE 2004: Proceedings of the Sixteenth International Conference on Software Engineering &amp; Knowledge Engineering,</booktitle>
<pages>92--97</pages>
<publisher>KSI Press.</publisher>
<marker>ˇCubrani´c, Murphy, 2004</marker>
<rawString>ˇCubrani´c, D. and Murphy, G. C. (2004). Automatic bug triage using text categorization. In In SEKE 2004: Proceedings of the Sixteenth International Conference on Software Engineering &amp; Knowledge Engineering, pages 92–97. KSI Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>