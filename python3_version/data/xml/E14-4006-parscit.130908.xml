<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.142006">
<title confidence="0.960698">
Projecting the Knowledge Graph to Syntactic Parsing
</title>
<author confidence="0.852136">
Andrea Gesmundo and Keith B. Hall
</author>
<affiliation confidence="0.759323">
Google, Inc.
</affiliation>
<email confidence="0.988688">
{agesmundo,kbhall}@google.com
</email>
<sectionHeader confidence="0.994573" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999804466666667">
We present a syntactic parser training
paradigm that learns from large scale
Knowledge Bases. By utilizing the
Knowledge Base context only during
training, the resulting parser has no
inference-time dependency on the Knowl-
edge Base, thus not decreasing the speed
during prediction. Knowledge Base infor-
mation is injected into the model using an
extension to the Augmented-loss training
framework. We present empirical results
that show this approach achieves a signif-
icant gain in accuracy for syntactic cat-
egories such as coordination and apposi-
tion.
</bodyText>
<sectionHeader confidence="0.998753" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999570048780488">
Natural Language Processing systems require
large amounts of world knowledge to achieve
state-of-the-art performance. Leveraging Knowl-
edge Bases (KB) provides allows us to inject hu-
man curated world-knowledge into our systems.
As these KBs have increased in size, we are now
able to leverage this information to improve upon
the state-of-the-art. Large scale KB have been de-
veloped rapidly in recent years, adding large num-
bers of entities and relations between the entities.
Such entities can be of any kind: an object, a per-
son, a place, a company, a book, etc. Entities
and relations are stored in association with rele-
vant data that describes the particular entity or re-
lation; for example, the name of a book, it’s author,
other books by the same author, etc.. Large scale
KB annotation efforts have focused on the collec-
tion of both current and historical entities, but are
biased towards the contemporary entities.
Of the many publicly available KBs, we focus
this study on the use of Freebase1: a large collab-
orative Knowledge Base composed and updated
by a member community. Currently it contains
roughly 40 million entities and 1.1 billion rela-
tions.
The aim of the presented work is to use the in-
formation provided by the KB to improve the ac-
curacy of the statistical dependency parsing task
(Kubler et al., 2009). In particular we focus on the
recognition of relations such as coordination and
apposition. This choice is motivated by the fact
that the KB stores information about real-world
entities while many of the errors associated with
coordination and apposition is the lack of knowl-
edge of these real-world entities.
We begin by defining the task (section 2). Fol-
lowing, we present the modified augmented-loss
training framework (section 3). In section 4, we
define how the Knowledge Base data is integrated
into the training process. Finally, we discuss the
empirical results (section 5).
</bodyText>
<sectionHeader confidence="0.985725" genericHeader="introduction">
2 Task
</sectionHeader>
<bodyText confidence="0.999846666666667">
Apposition is a relation between two adjacent
noun-phrases, where one noun-phrase specifies or
modifying the other. For example, in the sentence
“My friend Anna”, the nouns “friend” and “Anna”
are in apposition. Coordination between nouns
relates two or more elements of the same kind.
The coordination is often signaled by the appear-
ance of a coordinating conjunction. For example,
in the sentence “My friend and Anna”, the nouns
“friend” and “Anna” are in coordination. The se-
mantic difference between the two relations is that
the nouns in apposition refer to the same entity,
</bodyText>
<footnote confidence="0.995318">
1www.freebase.com
</footnote>
<page confidence="0.991292">
28
</page>
<note confidence="0.6911425">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 28–32,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.993702533333333">
while the nouns in coordination refer to distinct
entities of the same kind or sharing some proper-
ties.
Statistical parsers are inaccurate in classifying
relations involving proper nouns that appear rarely
in the training set. In the sentence:
“They invested in three companies, Google,
Microsoft, and Yahoo.”
“companies” is in apposition with the coordina-
tion “Google, Microsoft, and Yahoo”. By integrat-
ing the information provided by a large scale KB
into the syntactic parser, we attempt to increase
the ability to disambiguate the relations involving
these proper nouns, even if the parser has been
trained on a different domain.
</bodyText>
<sectionHeader confidence="0.995125" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999980263157895">
We present a Syntactic Parsing model that learns
from the KB. An important constraint that we im-
pose, is that the speed of the Syntactic Parser must
not decrease when this information is integrated.
As the queries to the KB would significantly slow
down the parser, we limit querying the KB to train-
ing. This constraint reduces the impact that the KB
can have on the accuracy, but allows us to design a
parser that can be substituted in any setting, even
in the absence of the KB.
We propose a solution based on the Augmented-
loss framework (Hall et al., 2011a). Augmented-
loss is a training framework for structured predic-
tion tasks such as parsing. It can be used to ex-
tend a standard objective function with additional
loss-functions and be integrated with the struc-
tured perceptron training algorithm. The input
is enriched with multiple datasets each associated
with a loss function. The algorithm iterates over
the datasets triggering parameter updates when-
ever the loss function is positive.
Loss functions return a positive value if the pre-
dicted output is “worse” than the gold standard.
Augmented-loss allows for the inclusion of mul-
tiple objective functions, either based on intrinsic
parsing quality or task-specific extrinsic measures
of quality. In the original formalization, both the
intrinsic and extrinsic losses require gold standard
information. Thus, each dataset must specify a
gold standard output for each input.
We extend the Augmented-loss framework to
apply it when the additional dataset gold-standard
is unknown. Without the gold standard, it is not
possible to trigger updates using a loss function.
Instead, we use a sampling function, S(·), that is
defined such that: if yˆ is a candidate parse tree,
then S(ˆy) returns a parse tree that is guaranteed to
be “not worse” than ˆy. In other words:
</bodyText>
<equation confidence="0.981707">
LS(ˆy, S(ˆy)) ≥ 0 (1)
</equation>
<bodyText confidence="0.999088142857143">
Where the LS(·) is the implicit loss function. This
formalization will allow us to avoid stating explic-
itly the loss function. Notice that S(ˆy) is not guar-
anteed to be the “best” parse tree. It can be any
parse tree in the search space that is “not worse”
than ˆy. S(ˆy) can represent an incremental im-
provement over ˆy.
</bodyText>
<listItem confidence="0.979599761904762">
Algorithm 1 Augmented-loss extension
1: {Input loss function: L(·)}
2: {Input sample function: S(·)}
3: {Input data sets}:
4: DL = {dLi = (xLi ,yLi )  |1 &lt; i &lt; NL}
5: DS = {dSi = (xSi )  |1 &lt; i &lt; NS}
6: θ = 0~
7: repeat
8: for i = 1... NL do
9: yˆ= Fθ(xLi )
10: if L(ˆy, yLi ) &gt; 0 then
11: θ = θ + b(yLi ) − b(ˆy)
12: end if
13: end for
14: for i = 1... NS do
15: yˆ = Fθ(xSi )
16: y∗ = S(ˆy)
17: θ = θ + b(y∗) − b(ˆy)
18: end for
19: until converged
20: {Return model θ}
</listItem>
<bodyText confidence="0.995733733333333">
Algorithm 1 summarizes the extension to the
Augmented-loss algorithm.
The algorithm takes as input: the loss func-
tion L(·); the sample function S(·); the loss func-
tion data samples DL; and the sample function
data samples DS. Notice that DL specifies the
gold standard parse yLi for each input sentence xLi .
While, DS specifies only the input sentence xSi .
The model parameter are initialized to the zero
vector (line 6). The main loop iterates until the
model reaches convergence (lines 7-19). After
which the model parameters are returned.
The first inner loop iterates over DL (lines 8-
13) executing the standard on-line training. The
candidate parse, ˆy, for the current input sentence,
</bodyText>
<page confidence="0.993617">
29
</page>
<bodyText confidence="0.9993105">
xLi , is predicted given the current model parame-
ters, 0 (line 9). In the structured perceptron setting
(Collins and Roark, 2004; Daum´e III et al., 2009),
we have that:
</bodyText>
<equation confidence="0.999927">
Fθ(x) = argmaxyEY 0 · 4b(y) (2)
</equation>
<bodyText confidence="0.962515823529412">
Where 4b(·) is the mapping from a parse tree y to
a high dimensional feature space. Then, the algo-
rithm tests if the current prediction is wrong (line
10). In which case the model is updated promot-
ing features that fire in the gold-standard 4b(yL i),
and penalizing features that fire in the predicted
output, 4b(ˆy) (line 11).
The second inner loop iterates over DS (lines
14-18). First, the candidate parse, ˆy, is predicted
(line 15). Then the sample parse, y*, is pro-
duced by the sample function (line 16). Finally,
the parameters are updated promoting the features
of y*. The updates are triggered without test-
ing if the loss is positive, since it is guaranteed
that LS(ˆy, y*) ≥ 0. Updating in cases where
LS(ˆy, y*) = 0 does not harm the model. To opti-
mize the algorithm, updates can be avoided when
yˆ= y*.
In order to simplify the algorithmic descrip-
tion, we define the algorithm with only one loss
function and one sample function, and we formal-
ized it for the specific task we are considering.
This definitions can be trivially generalized to in-
tegrate multiple loss/sample functions and to be
formalized for a generic structured prediction task.
This generalization can be achieved following the
guidelines of (Hall et al., 2011a). Furthermore, we
defined the algorithm such that it first iterates over
DL and then over DS. In practice, the algorithm
can switch between the data sets with a desired fre-
quency by using a scheduling policy as described
in (Hall et al., 2011a). For the experiments, we
trained on 8 samples of DL followed by 1 samples
of DS, looping over the training sets.
</bodyText>
<sectionHeader confidence="0.974158" genericHeader="method">
4 Sample Function
</sectionHeader>
<bodyText confidence="0.9990989">
We integrate the Knowledge Base data into the
training algorithm using a sampling function. The
idea is to correct errors in the candidate parse
by using the KB. The sample function corrects
only relations among entities described in the KB.
Thus, it returns a better or equal parse tree that
may still contain errors. This is sufficient to guar-
antee the constraint on the implicit loss function
(equation 1).
The sample function receives as input the can-
didate dependency parse and the input sentence
enriched with KB annotation. Then, it corrects
the labels of each arc in the dependency tree con-
necting two entities. The labels are corrected ac-
cording to the predictions produced by a classifier.
As classifier we use a standard multi-class percep-
tron (Crammer and Singer, 2003). The classifier is
trained in a preprocessing step on a parsed corpus
enriched with KB data. The features used by the
classifier are:
</bodyText>
<listItem confidence="0.998625875">
• Lexical features of the head and modifier.
• Sentence level features: words distance be-
tween head and modifier; arc direction (L/R);
neighboring words.
• Syntactic features: POS and syntactic label of
head and modifier and modifier’s left sibling.
• Knowledge Base features: types defined for
entities and for their direct relations.
</listItem>
<sectionHeader confidence="0.995788" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999979785714285">
The primary training corpus is composed of manu-
ally annotated sentences with syntactic tress which
are converted to dependency format using the
Stanford converter v1.6 (de Marneffe et al., 2006).
We run experiments using 10k sentences or 70k
sentences from this corpus. The test set contains
16k manually syntactically annotated sentences
crawled from the web. The test and train sets are
from different domains. This setting may degrade
the parser accuracy in labelling out-of-domain en-
tities, as we discussed in section 2. Thus, we use
web text as secondary training set to be used for
the Augmented-loss loss sample training. Web
text is available in any quantity, and we do not
need to provide gold-standard parses in order to
integrate it in the Augmented-loss sample train-
ing. The classifier is trained on 10k sentences ex-
tracted from news text which has been automati-
cally parsed. We chose to train the classifier on
news data as the quality of the automatic parses is
much higher than on general web text. We do this
despite the fact that we will apply the classifier to
a different domain (the web text).
As dependency parser, we use an implemen-
tation of the transition-based dependency parsing
framework (Nivre, 2008) with the arc-eager tran-
sition strategy. The part of Augmented-loss train-
ing based on the standard loss function, applies
</bodyText>
<page confidence="0.995647">
30
</page>
<table confidence="0.9987396">
Training set size Model appos F1 conj F1 LAS UAS
70k sentences Baseline 54.36 83.72 79.55 83.50
Augmented-loss 55.64 84.47 79.71 83.71
10k sentences Baseline 45.13 80.36 75.99 86.02
Augmented-loss 48.06 81.63 76.16 86.18
</table>
<tableCaption confidence="0.999839">
Table 1: Accuracy Comparison.
</tableCaption>
<bodyText confidence="0.999394181818182">
the perceptron algorithm as in (Zhang and Clark,
2008) with a beam size of 16. The baseline is the
same model but trained only the primary training
corpus without Augmented-loss.
Table 1 reports the results of the accuracy com-
parison. It reports the metrics for Labeled At-
tachment Score (LAS) and Unlabeled Attachment
Score (UAS) to measure the overall accuracy. The
syntactic classes that are affected the most are ap-
position (appos) and conjunction (conj). On the
development set we measured that the percentage
of arcs connecting 2 entities that are labeled as
conjunction is 36.11%. While those that are la-
belled as apposition is 25.06%. Each of the other
40 labels cover a small portion of the remaining
38.83%.
Training the models with the full primary train-
ing corpus (70k sentences), shows a significant
gain for the Augmented-loss model. Apposition
F1 gains 1.28, while conjunction gains 0.75. The
LAS gain is mainly due to the gain of the two men-
tioned classes. It is surprising to measure a simi-
lar gain also for the unlabeled accuracy. Since the
classifier can correct the label of an arc but never
change the structure of the parse. This implies
that just by penalizing a labeling action, the model
learns to construct better parse structures.
Training the model with 10k sentences shows a
significantly bigger gain on all the measures. This
results shows that, in cases where the set of la-
beled data is small, this approach can be applied
to integrate in unlimited amount of unlabeled data
to boost the learning.
</bodyText>
<sectionHeader confidence="0.999976" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9999773">
As we mentioned, Augmented-loss (Hall et al.,
2011a; Hall et al., 2011b) is perhaps the closest to
our framework. Another difference with its origi-
nal formalization is that it was primarily aimed to
cases where the additional weak signal is precisely
what we wish to optimize. Such as cases where
we wish to optimize parsing to be used as an input
to a downstream natural language processing tasks
and the accuracies to be optimized are those of the
downstream task and not directly the parsing ac-
curacy. While our work is focused on integrating
additional data in a semi-supervised fashion with
the aim of improving the primary task’s accuracy
and/or adapt it to a different domain.
Another similar idea is (Chang et al., 2007)
which presents a constraint driven learning. In this
study, they integrate a weak signal into the training
framework with the aim to improve the structured
prediction models on the intrinsic evaluation met-
rics.
</bodyText>
<sectionHeader confidence="0.998204" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999984533333333">
We extended the Augmented-loss framework
defining a method for integrating new types of sig-
nals that require neither gold standard data nor an
explicit loss function. At the same time, they al-
low the integration of additional information that
can inform training to learn for specific types of
phenomena.
This framework allows us to effectively inte-
grate large scale KB in the training of structured
prediction tasks. This approach integrates the data
at training time without affecting the prediction
time.
Experiments on syntactic parsing show that a
significant gain for categories that model relation
between entities defined in the KB.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994689083333333">
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint-driven
learning. In ACL ’07: Proceedings of the 45th Con-
ference of the Association for Computational Lin-
guistics.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In ACL ’04:
Proceedings of the 42rd Conference of the Associa-
tion for Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951–991.
</reference>
<page confidence="0.997761">
31
</page>
<reference confidence="0.9997875">
Hal Daum´e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Submit-
ted to Machine Learning Journal.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure trees. In
LREC.
Keith Hall, Ryan McDonald, Jason Katz-brown, and
Michael Ringgaard. 2011a. Training dependency
parsers by jointly optimizing multiple objectives. In
EMNLP ’11: Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing.
Keith Hall, Ryan McDonald, and Slav Petrov. 2011b.
Training structured prediction models with extrinsic
loss functions. In Domain Adaptation Workshop at
NIPS, October.
Sandra Kubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. In Synthesis Lectures
on Human Language Technologies. Morgan &amp; Clay-
pool Publishers.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. volume 34, pages
513–553.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing. In
EMNLP ’08: Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 562–571.
</reference>
<page confidence="0.999297">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.516067">
<title confidence="0.999238">Projecting the Knowledge Graph to Syntactic Parsing</title>
<author confidence="0.701842">B Gesmundo</author>
<affiliation confidence="0.876947">Google, Inc.</affiliation>
<abstract confidence="0.9853021875">We present a syntactic parser paradigm that learns from large scale Knowledge Bases. By utilizing Knowledge Base context only during training, the resulting parser has no inference-time dependency on the Knowledge Base, thus not decreasing the speed during prediction. Knowledge Base information is injected into the model using an extension to the Augmented-loss training framework. We present empirical results that show this approach achieves a significant gain in accuracy for syntactic categories such as coordination and apposition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Guiding semi-supervision with constraint-driven learning.</title>
<date>2007</date>
<booktitle>In ACL ’07: Proceedings of the 45th Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14368" citStr="Chang et al., 2007" startWordPosition="2408" endWordPosition="2411">ork. Another difference with its original formalization is that it was primarily aimed to cases where the additional weak signal is precisely what we wish to optimize. Such as cases where we wish to optimize parsing to be used as an input to a downstream natural language processing tasks and the accuracies to be optimized are those of the downstream task and not directly the parsing accuracy. While our work is focused on integrating additional data in a semi-supervised fashion with the aim of improving the primary task’s accuracy and/or adapt it to a different domain. Another similar idea is (Chang et al., 2007) which presents a constraint driven learning. In this study, they integrate a weak signal into the training framework with the aim to improve the structured prediction models on the intrinsic evaluation metrics. 7 Conclusion We extended the Augmented-loss framework defining a method for integrating new types of signals that require neither gold standard data nor an explicit loss function. At the same time, they allow the integration of additional information that can inform training to learn for specific types of phenomena. This framework allows us to effectively integrate large scale KB in th</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2007</marker>
<rawString>Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007. Guiding semi-supervision with constraint-driven learning. In ACL ’07: Proceedings of the 45th Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42rd Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7537" citStr="Collins and Roark, 2004" startWordPosition="1257" endWordPosition="1260">nction data samples DS. Notice that DL specifies the gold standard parse yLi for each input sentence xLi . While, DS specifies only the input sentence xSi . The model parameter are initialized to the zero vector (line 6). The main loop iterates until the model reaches convergence (lines 7-19). After which the model parameters are returned. The first inner loop iterates over DL (lines 8- 13) executing the standard on-line training. The candidate parse, ˆy, for the current input sentence, 29 xLi , is predicted given the current model parameters, 0 (line 9). In the structured perceptron setting (Collins and Roark, 2004; Daum´e III et al., 2009), we have that: Fθ(x) = argmaxyEY 0 · 4b(y) (2) Where 4b(·) is the mapping from a parse tree y to a high dimensional feature space. Then, the algorithm tests if the current prediction is wrong (line 10). In which case the model is updated promoting features that fire in the gold-standard 4b(yL i), and penalizing features that fire in the predicted output, 4b(ˆy) (line 11). The second inner loop iterates over DS (lines 14-18). First, the candidate parse, ˆy, is predicted (line 15). Then the sample parse, y*, is produced by the sample function (line 16). Finally, the pa</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In ACL ’04: Proceedings of the 42rd Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="10010" citStr="Crammer and Singer, 2003" startWordPosition="1683" endWordPosition="1686">e KB. The sample function corrects only relations among entities described in the KB. Thus, it returns a better or equal parse tree that may still contain errors. This is sufficient to guarantee the constraint on the implicit loss function (equation 1). The sample function receives as input the candidate dependency parse and the input sentence enriched with KB annotation. Then, it corrects the labels of each arc in the dependency tree connecting two entities. The labels are corrected according to the predictions produced by a classifier. As classifier we use a standard multi-class perceptron (Crammer and Singer, 2003). The classifier is trained in a preprocessing step on a parsed corpus enriched with KB data. The features used by the classifier are: • Lexical features of the head and modifier. • Sentence level features: words distance between head and modifier; arc direction (L/R); neighboring words. • Syntactic features: POS and syntactic label of head and modifier and modifier’s left sibling. • Knowledge Base features: types defined for entities and for their direct relations. 5 Experiments The primary training corpus is composed of manually annotated sentences with syntactic tress which are converted to</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e John Langford</author>
<author>Daniel Marcu</author>
</authors>
<date>2009</date>
<note>Search-based structured prediction. Submitted to Machine Learning Journal.</note>
<marker>Langford, Marcu, 2009</marker>
<rawString>Hal Daum´e III, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Submitted to Machine Learning Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure trees.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure trees. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Ryan McDonald</author>
<author>Jason Katz-brown</author>
<author>Michael Ringgaard</author>
</authors>
<title>Training dependency parsers by jointly optimizing multiple objectives.</title>
<date>2011</date>
<booktitle>In EMNLP ’11: Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4631" citStr="Hall et al., 2011" startWordPosition="740" endWordPosition="743">the parser has been trained on a different domain. 3 Model We present a Syntactic Parsing model that learns from the KB. An important constraint that we impose, is that the speed of the Syntactic Parser must not decrease when this information is integrated. As the queries to the KB would significantly slow down the parser, we limit querying the KB to training. This constraint reduces the impact that the KB can have on the accuracy, but allows us to design a parser that can be substituted in any setting, even in the absence of the KB. We propose a solution based on the Augmentedloss framework (Hall et al., 2011a). Augmentedloss is a training framework for structured prediction tasks such as parsing. It can be used to extend a standard objective function with additional loss-functions and be integrated with the structured perceptron training algorithm. The input is enriched with multiple datasets each associated with a loss function. The algorithm iterates over the datasets triggering parameter updates whenever the loss function is positive. Loss functions return a positive value if the predicted output is “worse” than the gold standard. Augmented-loss allows for the inclusion of multiple objective f</context>
<context position="8851" citStr="Hall et al., 2011" startWordPosition="1486" endWordPosition="1489">he loss is positive, since it is guaranteed that LS(ˆy, y*) ≥ 0. Updating in cases where LS(ˆy, y*) = 0 does not harm the model. To optimize the algorithm, updates can be avoided when yˆ= y*. In order to simplify the algorithmic description, we define the algorithm with only one loss function and one sample function, and we formalized it for the specific task we are considering. This definitions can be trivially generalized to integrate multiple loss/sample functions and to be formalized for a generic structured prediction task. This generalization can be achieved following the guidelines of (Hall et al., 2011a). Furthermore, we defined the algorithm such that it first iterates over DL and then over DS. In practice, the algorithm can switch between the data sets with a desired frequency by using a scheduling policy as described in (Hall et al., 2011a). For the experiments, we trained on 8 samples of DL followed by 1 samples of DS, looping over the training sets. 4 Sample Function We integrate the Knowledge Base data into the training algorithm using a sampling function. The idea is to correct errors in the candidate parse by using the KB. The sample function corrects only relations among entities d</context>
<context position="13690" citStr="Hall et al., 2011" startWordPosition="2292" endWordPosition="2295">is surprising to measure a similar gain also for the unlabeled accuracy. Since the classifier can correct the label of an arc but never change the structure of the parse. This implies that just by penalizing a labeling action, the model learns to construct better parse structures. Training the model with 10k sentences shows a significantly bigger gain on all the measures. This results shows that, in cases where the set of labeled data is small, this approach can be applied to integrate in unlimited amount of unlabeled data to boost the learning. 6 Related Work As we mentioned, Augmented-loss (Hall et al., 2011a; Hall et al., 2011b) is perhaps the closest to our framework. Another difference with its original formalization is that it was primarily aimed to cases where the additional weak signal is precisely what we wish to optimize. Such as cases where we wish to optimize parsing to be used as an input to a downstream natural language processing tasks and the accuracies to be optimized are those of the downstream task and not directly the parsing accuracy. While our work is focused on integrating additional data in a semi-supervised fashion with the aim of improving the primary task’s accuracy and/o</context>
</contexts>
<marker>Hall, McDonald, Katz-brown, Ringgaard, 2011</marker>
<rawString>Keith Hall, Ryan McDonald, Jason Katz-brown, and Michael Ringgaard. 2011a. Training dependency parsers by jointly optimizing multiple objectives. In EMNLP ’11: Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
</authors>
<title>Training structured prediction models with extrinsic loss functions.</title>
<date>2011</date>
<booktitle>In Domain Adaptation Workshop at NIPS,</booktitle>
<contexts>
<context position="4631" citStr="Hall et al., 2011" startWordPosition="740" endWordPosition="743">the parser has been trained on a different domain. 3 Model We present a Syntactic Parsing model that learns from the KB. An important constraint that we impose, is that the speed of the Syntactic Parser must not decrease when this information is integrated. As the queries to the KB would significantly slow down the parser, we limit querying the KB to training. This constraint reduces the impact that the KB can have on the accuracy, but allows us to design a parser that can be substituted in any setting, even in the absence of the KB. We propose a solution based on the Augmentedloss framework (Hall et al., 2011a). Augmentedloss is a training framework for structured prediction tasks such as parsing. It can be used to extend a standard objective function with additional loss-functions and be integrated with the structured perceptron training algorithm. The input is enriched with multiple datasets each associated with a loss function. The algorithm iterates over the datasets triggering parameter updates whenever the loss function is positive. Loss functions return a positive value if the predicted output is “worse” than the gold standard. Augmented-loss allows for the inclusion of multiple objective f</context>
<context position="8851" citStr="Hall et al., 2011" startWordPosition="1486" endWordPosition="1489">he loss is positive, since it is guaranteed that LS(ˆy, y*) ≥ 0. Updating in cases where LS(ˆy, y*) = 0 does not harm the model. To optimize the algorithm, updates can be avoided when yˆ= y*. In order to simplify the algorithmic description, we define the algorithm with only one loss function and one sample function, and we formalized it for the specific task we are considering. This definitions can be trivially generalized to integrate multiple loss/sample functions and to be formalized for a generic structured prediction task. This generalization can be achieved following the guidelines of (Hall et al., 2011a). Furthermore, we defined the algorithm such that it first iterates over DL and then over DS. In practice, the algorithm can switch between the data sets with a desired frequency by using a scheduling policy as described in (Hall et al., 2011a). For the experiments, we trained on 8 samples of DL followed by 1 samples of DS, looping over the training sets. 4 Sample Function We integrate the Knowledge Base data into the training algorithm using a sampling function. The idea is to correct errors in the candidate parse by using the KB. The sample function corrects only relations among entities d</context>
<context position="13690" citStr="Hall et al., 2011" startWordPosition="2292" endWordPosition="2295">is surprising to measure a similar gain also for the unlabeled accuracy. Since the classifier can correct the label of an arc but never change the structure of the parse. This implies that just by penalizing a labeling action, the model learns to construct better parse structures. Training the model with 10k sentences shows a significantly bigger gain on all the measures. This results shows that, in cases where the set of labeled data is small, this approach can be applied to integrate in unlimited amount of unlabeled data to boost the learning. 6 Related Work As we mentioned, Augmented-loss (Hall et al., 2011a; Hall et al., 2011b) is perhaps the closest to our framework. Another difference with its original formalization is that it was primarily aimed to cases where the additional weak signal is precisely what we wish to optimize. Such as cases where we wish to optimize parsing to be used as an input to a downstream natural language processing tasks and the accuracies to be optimized are those of the downstream task and not directly the parsing accuracy. While our work is focused on integrating additional data in a semi-supervised fashion with the aim of improving the primary task’s accuracy and/o</context>
</contexts>
<marker>Hall, McDonald, Petrov, 2011</marker>
<rawString>Keith Hall, Ryan McDonald, and Slav Petrov. 2011b. Training structured prediction models with extrinsic loss functions. In Domain Adaptation Workshop at NIPS, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Kubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency parsing.</title>
<date>2009</date>
<booktitle>In Synthesis Lectures on Human Language Technologies.</booktitle>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="2029" citStr="Kubler et al., 2009" startWordPosition="321" endWordPosition="324">of a book, it’s author, other books by the same author, etc.. Large scale KB annotation efforts have focused on the collection of both current and historical entities, but are biased towards the contemporary entities. Of the many publicly available KBs, we focus this study on the use of Freebase1: a large collaborative Knowledge Base composed and updated by a member community. Currently it contains roughly 40 million entities and 1.1 billion relations. The aim of the presented work is to use the information provided by the KB to improve the accuracy of the statistical dependency parsing task (Kubler et al., 2009). In particular we focus on the recognition of relations such as coordination and apposition. This choice is motivated by the fact that the KB stores information about real-world entities while many of the errors associated with coordination and apposition is the lack of knowledge of these real-world entities. We begin by defining the task (section 2). Following, we present the modified augmented-loss training framework (section 3). In section 4, we define how the Knowledge Base data is integrated into the training process. Finally, we discuss the empirical results (section 5). 2 Task Appositi</context>
</contexts>
<marker>Kubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra Kubler, Ryan McDonald, and Joakim Nivre. 2009. Dependency parsing. In Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<volume>34</volume>
<pages>513--553</pages>
<contexts>
<context position="11722" citStr="Nivre, 2008" startWordPosition="1966" endWordPosition="1967">ple training. Web text is available in any quantity, and we do not need to provide gold-standard parses in order to integrate it in the Augmented-loss sample training. The classifier is trained on 10k sentences extracted from news text which has been automatically parsed. We chose to train the classifier on news data as the quality of the automatic parses is much higher than on general web text. We do this despite the fact that we will apply the classifier to a different domain (the web text). As dependency parser, we use an implementation of the transition-based dependency parsing framework (Nivre, 2008) with the arc-eager transition strategy. The part of Augmented-loss training based on the standard loss function, applies 30 Training set size Model appos F1 conj F1 LAS UAS 70k sentences Baseline 54.36 83.72 79.55 83.50 Augmented-loss 55.64 84.47 79.71 83.71 10k sentences Baseline 45.13 80.36 75.99 86.02 Augmented-loss 48.06 81.63 76.16 86.18 Table 1: Accuracy Comparison. the perceptron algorithm as in (Zhang and Clark, 2008) with a beam size of 16. The baseline is the same model but trained only the primary training corpus without Augmented-loss. Table 1 reports the results of the accuracy c</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. volume 34, pages 513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In EMNLP ’08: Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>562--571</pages>
<contexts>
<context position="12152" citStr="Zhang and Clark, 2008" startWordPosition="2031" endWordPosition="2034">e fact that we will apply the classifier to a different domain (the web text). As dependency parser, we use an implementation of the transition-based dependency parsing framework (Nivre, 2008) with the arc-eager transition strategy. The part of Augmented-loss training based on the standard loss function, applies 30 Training set size Model appos F1 conj F1 LAS UAS 70k sentences Baseline 54.36 83.72 79.55 83.50 Augmented-loss 55.64 84.47 79.71 83.71 10k sentences Baseline 45.13 80.36 75.99 86.02 Augmented-loss 48.06 81.63 76.16 86.18 Table 1: Accuracy Comparison. the perceptron algorithm as in (Zhang and Clark, 2008) with a beam size of 16. The baseline is the same model but trained only the primary training corpus without Augmented-loss. Table 1 reports the results of the accuracy comparison. It reports the metrics for Labeled Attachment Score (LAS) and Unlabeled Attachment Score (UAS) to measure the overall accuracy. The syntactic classes that are affected the most are apposition (appos) and conjunction (conj). On the development set we measured that the percentage of arcs connecting 2 entities that are labeled as conjunction is 36.11%. While those that are labelled as apposition is 25.06%. Each of the </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing. In EMNLP ’08: Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 562–571.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>