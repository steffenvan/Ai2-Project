<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.166619">
<title confidence="0.98967">
Extracting Clinical Relationships from Patient Narratives
</title>
<author confidence="0.999547">
Angus Roberts, Robert Gaizauskas, Mark Hepple
</author>
<affiliation confidence="0.999673">
Department of Computer Science, University of Sheffield,
</affiliation>
<address confidence="0.742922">
Regent Court, 211 Portobello, Sheffield S1 4DP
</address>
<email confidence="0.99933">
{initial.surname}@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.997425" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998687071428571">
The Clinical E-Science Framework (CLEF)
project has built a system to extract clin-
ically significant information from the tex-
tual component of medical records, for clin-
ical research, evidence-based healthcare and
genotype-meets-phenotype informatics. One
part of this system is the identification of rela-
tionships between clinically important entities
in the text. Typical approaches to relationship
extraction in this domain have used full parses,
domain-specific grammars, and large knowl-
edge bases encoding domain knowledge. In
other areas of biomedical NLP, statistical ma-
chine learning approaches are now routinely
applied to relationship extraction. We report
on the novel application of these statistical
techniques to clinical relationships.
We describe a supervised machine learning
system, trained with a corpus of oncology nar-
ratives hand-annotated with clinically impor-
tant relationships. Various shallow features
are extracted from these texts, and used to
train statistical classifiers. We compare the
suitability of these features for clinical re-
lationship extraction, how extraction varies
between inter- and intra-sentential relation-
ships, and examine the amount of training data
needed to learn various relationships.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999136714285714">
The application of Natural Language Processing
(NLP) is widespread in biomedicine. Typically, it
is applied to improve access to the ever-burgeoning
research literature. Increasingly, biomedical re-
searchers need to relate this literature to pheno-
typic data: both to populations, and to individ-
ual clinical subjects. The computer applications
</bodyText>
<page confidence="0.968834">
10
</page>
<bodyText confidence="0.999694138888889">
used in biomedical research, including NLP appli-
cations, therefore need to support genotype-meets-
phenotype informatics and the move towards trans-
lational biology. Such support will undoubtedly in-
clude linkage to the information held in individual
medical records: both the structured portion, and the
unstructured textual portion.
The Clinical E-Science Framework (CLEF)
project (Rector et al., 2003) is building a frame-
work for the capture, integration and presentation of
this clinical information, for research and evidence-
based health care. The project’s data resource is a
repository of the full clinical records for over 20000
cancer patients from the Royal Marsden Hospital,
Europe’s largest oncology centre. These records
combine structured information, clinical narratives,
and free text investigation reports. CLEF uses infor-
mation extraction (IE) technology to make informa-
tion from the textual portion of the medical record
available for integration with the structured record,
and thus available for clinical care and research. The
CLEF IE system analyses the textual records to ex-
tract entities, events and the relationships between
them. These relationships give information that is
often not available in the structured record. Why
was a drug given? What were the results of a physi-
cal examination? What problems were not present?
We have previously reported entity extraction in the
CLEF IE system (Roberts et al., 2008b). This paper
examines relationship extraction.
Extraction of relationships from clinical text is
usually carried out as part of a full clinical IE sys-
tem. Several such systems have been described.
They generally use a syntactic parse with domain-
specific grammar rules. The Linguistic String
project (Sager et al., 1994) used a full syntactic and
</bodyText>
<note confidence="0.772542">
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 10–18,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999805866666667">
clinical sublanguage parse to fill template data struc-
tures corresponding to medical statements. These
were mapped to a database model incorporating
medical facts and the relationships between them.
MedLEE (Friedman et al., 1994), and more recently
BioMedLEE (Lussier et al., 2006) used a semantic
lexicon and grammar of domain-specific semantic
patterns. The patterns encode the possible relation-
ships between entities, allowing both entities and the
relationships between them to be directly matched
in the text. Other systems have incorporated large-
scale domain-specific knowledge bases. MEDSYN-
DIKATE (Hahn et al., 2002) employed a rich dis-
course model of entities and their relationships, built
using a dependency parse of texts and a descrip-
tion logic knowledge base re-engineered from exist-
ing terminologies. MENELAS (Zweigenbaum et al.,
1995) also used a full parse, a conceptual represen-
tation of the text, and a large scale knowledge base.
In other applications of biomedical NLP, a sec-
ond paradigm has become widespread: the appli-
cation of statistical machine learning techniques to
feature-based models of the text. Such approaches
have typically been applied to journal texts. They
have been used both for entity recognition and ex-
traction of various relations, such as protein-protein
interactions (see, for example, Grover et al (2007)).
This follows on from the success of these methods
in general NLP (see for example Zhou et al (2005)).
Statistical machine learning has also been applied to
clinical text, but its use has generally been limited
to entity recognition. The Mayo Clinic text analysis
system (Pakhomov et al., 2005), for example, uses a
combination of dictionary lookup and a Naive Bayes
classifier to identify entities for information retrieval
applications. To the best of our knowledge, statisti-
cal methods have not been previously applied to ex-
traction of clinical relationships from text.
This paper describes experiments in the statistical
machine learning of relationships from a novel text
type: oncology narratives. The set of relationships
extracted are considered to be of interest for clinical
and research applications down line of IE, such as
querying to support clinical research. We apply Sup-
port Vector Machine (SVM) classifiers to learn these
relationships. The classifiers are trained and eval-
uated using novel data: a gold standard corpus of
clinical text, hand-annotated with semantic entities
and relationships. In order to test the applicability
of this method to the clinical domain, we train clas-
sifiers using a number of comparatively simple text
features, and look at the contribution of these fea-
tures to system performance. Clinically interesting
relationships may span several sentences, and so we
compare classifiers trained for both intra- and inter-
sentential relationships (spanning one or more sen-
tence boundaries). We also examine the influence of
training corpus size on performance, as hand anno-
tation of training data is the major expense in super-
vised machine learning.
</bodyText>
<sectionHeader confidence="0.987383" genericHeader="introduction">
2 Relationship Schema
</sectionHeader>
<table confidence="0.999716307692308">
Relationship Argument 1 Argument 2
has target Investigation Locus
Intervention Locus
has finding Investigation Condition
Investigation Result
has indication Drug or device Condition
Intervention Condition
Investigation Condition
has location Condition Locus
negation modifies Negation modifier Condition
laterality modifies Laterality modifier Intervention
Laterality modifier Locus
sub-location modifies Sub-location modifier Locus
</table>
<tableCaption confidence="0.985517">
Table 1: Relationship types and their argument type con-
straints.
</tableCaption>
<bodyText confidence="0.99994425">
The CLEF application extracts entities, relation-
ships and modifiers from text. By entity, we mean
some real-world thing, event or state referred to in
the text: the drugs that are mentioned, the tests that
were carried out, etc. Modifiers are words that qual-
ify an entity in some way, referring e.g. to the lat-
erality of an anatomical locus, or the negation of a
condition (“no sign of inflammation”). Entities are
connected to each other and to modifiers by rela-
tionships: e.g. linking a drug entity to the condition
entity for which it is indicated, linking an investiga-
tion to its results, or linking a negating phrase to a
condition.
The entities, modifiers, and relationships are de-
scribed by both a formal XML schema, and by a
set of detailed definitions. These were developed by
a group of clinical experts through an iterative pro-
cess, until acceptable agreement was reached. Entity
types are mapped to types from the UMLS seman-
tic network (Lindberg et al., 1993), each CLEF en-
</bodyText>
<page confidence="0.998551">
11
</page>
<bodyText confidence="0.99913075">
tity type covering several UMLS types. Relationship
types are those that were felt necessary to capture the
essential clinical dependencies between entities re-
ferred to in patient documents, and to support CLEF
end user applications.
Each relationship type is constrained to exist be-
tween limited pairs of entity types. For example,
the has location relationship can only exist be-
tween a Condition entity and a Locus entity.
Some relationships can exist between multiple type
pairs. The full set of relationships and their argu-
ment type constraints are shown in Table 1. Ex-
amples of each relationship are given in Roberts et
al (2008a).
Some of the relationships considered important
by the clinical experts were not obvious without do-
main knowledge. For example,
He is suffering from nausea and severe
headaches. Dolasteron was prescribed.
Without domain knowledge, it is not clear that there
is a has indication relationship between the
“Dolasteron” Drug or device entity and the
“nausea” Condition entity. As in this example,
many of this type of relationship are intra-sentential.
A single real-world entity may be referred to sev-
eral times in the same text. Each of these co-
referring expressions is a mention of the entity. The
gold standard includes annotation of co-reference
between different textual mentions of the same en-
tity. For the work reported in this paper, however,
co-reference is not considered. Each entity is as-
sumed to have a single mention. Relationships be-
tween entities can be considered, by extension, as
relationships between the single mentions of those
entities. The implications of this are discussed fur-
ther below.
</bodyText>
<sectionHeader confidence="0.982872" genericHeader="method">
3 Gold Standard Corpus
</sectionHeader>
<bodyText confidence="0.999989606060606">
The schema and definitions were used to hand-
annotate the entities and relationships in 77 oncol-
ogy narratives, to provide a gold standard for sys-
tem training and evaluation. Corpora of this size
are typical in supervised machine learning, and re-
flect the expense of hand annotation. Narratives
were carefully selected and annotated according to
a best practice methodology, as described in Roberts
et al (2008a). Narratives were annotated by two in-
dependent, clinically trained, annotators, and a con-
sensus created by a third. We will refer to this corpus
as C77.
Annotators were asked to first mark the mentions
of entities and modifiers, and then to go through
each of these in turn, deciding if any had relation-
ships with mentions of other entities. Although the
annotators were marking co-reference between men-
tions of the same entity, they were asked to ignore
this with respect to relationship annotation. Both
the annotation tool that they were using and their
annotation guidelines, enforced the creation of rela-
tionships between mentions, and not between enti-
ties. The gold standard is thus analogous to the style
of relationship extraction reported here, in which
we extract relations between single mention entities,
and do not consider co-reference. Annotators were
further told that relationships could span multiple
sentences, and that it was acceptable to use clini-
cal domain knowledge to infer that a relationship
existed between two mentions. Counts of all rela-
tionships annotated in C77 are shown in Table 2,
sub-divided by the number of sentence boundaries
spanned by a relationship.
</bodyText>
<sectionHeader confidence="0.99687" genericHeader="method">
4 Relationship Extraction
</sectionHeader>
<bodyText confidence="0.999962277777778">
The system we have built uses the GATE NLP
toolkit (Cunningham et al., 2002) 1. The system is
shown in Figure 1, and is described below.
Narratives are first pre-processed using standard
GATE modules. Narratives were tokenised, sen-
tences found with a regular expression-based sen-
tence splitter, part-of-speech (POS) tagged, and
morphological roots found for tokens. Each to-
ken was also labelled with a generalised POS tag,
the first two characters of the full POS tag. This
takes advantage of the Penn Treebank tagset used
by GATE’s POS tagger, in which related POS tags
share the first two characters. For example, all six
verb POS tags start with the letters “VB”.
After pre-processing, mentions of entities within
the text are annotated. In the experiments reported,
we assume perfect entity recognition, as given by
the entities in the human annotated gold standard
</bodyText>
<footnote confidence="0.9993765">
1We used a development build of GATE 4.0, downloadable
from http://gate.ac.uk
</footnote>
<page confidence="0.993891">
12
</page>
<bodyText confidence="0.987509272727273">
Sentence boundaries between arguments Total
0 1 2 3 4 5 6 7 8 9 &gt;9
has finding 265 46 25 7 5 4 3 2 2 2 0 361
has indication 139 85 35 32 14 11 6 4 5 5 12 348
has location 360 4 1 1 1 1 1 0 0 0 4 373
has target 122 14 4 2 2 4 3 1 0 1 0 153
laterality modifies 128 0 0 0 0 0 0 0 0 0 0 128
negation modifies 100 1 0 0 0 0 0 0 0 0 0 101
sub location modifies 76 0 0 0 0 0 0 0 0 0 0 76
Total 1190 150 65 42 22 20 13 7 7 8 16 1540
Cumulative total 1190 1340 1405 1447 1469 1489 1502 1509 1516 1524 1540
</bodyText>
<tableCaption confidence="0.986853">
Table 2: Count of relationships in 77 gold standard documents.
</tableCaption>
<bodyText confidence="0.999402285714286">
described above. Our results are therefore higher
than would be expected in a system with automatic
entity recognition. It is useful and usual to fix en-
tity recognition in this way, to allow tuning specific
to relationship extraction, and to allow the isolation
of relation-specific problems. We accept, however,
that ultimately, relation extraction does depend on
the quality of entity recognition. The relation extrac-
tion described here is used as part of an operational
IE system in which clinical entity recognition is per-
formed by a combination of lexical lookup and su-
pervised machine learning. We have described our
entity extraction system elsewhere (Roberts et al.,
2008b).
</bodyText>
<subsectionHeader confidence="0.953472">
4.1 Classification
</subsectionHeader>
<bodyText confidence="0.999732803571429">
We treat clinical relationship extraction as a classi-
fication task, training classifiers to assign a relation-
ship type to an entity pair. An entity pair is a pairing
of entities that may or may not be the arguments of
a relation. For a given document, we create all pos-
sible entity pairs within two constraints. First, en-
tities that are paired must be within n sentences of
each other. For all of the work reported here, unless
stated, n &lt; 1 (crossing 0 or 1 sentence boundaries).
Second, we can constrain the entity pairs created
by argument type (Rindflesch and Fiszman, 2003).
For example, there is little point in creating an en-
tity pair between a Drug or device entity and
a Result entity, as no relationships, as specified
by the schema, exist between entities of these types.
Entity pairing is carried out by a GATE component
developed specifically for clinical relationship ex-
traction. In addition to pairing entities according to
the above constraints, this component also assigns
features to each pair that characterise its lexical and
syntactic qualities (described further in Section 4.2).
Entity pairs correspond to classifier training and
test instances. In classifier training, if an entity
pair corresponds to the arguments of a relationship
present in the gold standard, then it is assigned a
class of that relationship type. If it does not corre-
spond to such a relation, then it is assigned the class
null. The classifier builds a model of these entity
pair training instances, from their features. In classi-
fier application, entity pairs are created from unseen
text, under the above constraints. The classifier as-
signs one of our seven relationship types, or null,
to each entity pair.
We use Support Vector machines (SVMs) as train-
able classifiers, as these have proved to be robust and
efficient for a range of NLP tasks, including relation
extraction. We use an SVM implementation devel-
oped within our own group, and provided as part
of the GATE toolkit. This is a variant on the orig-
inal SVM algorithm, SVM with uneven margins, in
which classification may be biased towards positive
training examples. This is particularly suited to NLP
applications, in which positive training examples are
often rare. Full details of the classifier are given in
Li et al (2005). We used the implementation “out of
the box”, with default parameters as determined in
experiments with other data sets.
SVMs are binary classifiers: the multi-class prob-
lem of classifying entity pairs must therefore be
mapped to a number of binary classification prob-
lems. There are several ways in which a multi-
class problem can be recast as binary problems. The
commonest are one-against-one in which one classi-
fier is trained for every possible pair of classes, and
one-against-all in which a classifier is trained for
a binary decision between each class and all other
</bodyText>
<page confidence="0.995645">
13
</page>
<bodyText confidence="0.996275368421053">
classes, including null, combined. We have car-
ried out extensive experiments (not reported here),
with these two strategies, and have found little dif-
ference between them for our data. We have chosen
to use one-against-all, as it needs fewer classifiers
(for an n class problem, it needs n classifiers, as op-
posed to (n�1)�
2 for one-against-one).
The resultant class assignments by multiple bi-
nary classifiers must be post-processed to deal with
ambiguity. In application to unseen text, it is possi-
ble that several classifiers assign different classes to
an entity pair (test instance). To disambiguate these
cases, the output of each one-against-all classifier is
transformed into a probability, and the class with
the highest probability is assigned. Re-casting the
multi-class relation problem as a number of binary
problems, and post-processing to resolve ambigui-
ties, is handled by the GATE Learning API.
</bodyText>
<figureCaption confidence="0.999376">
Figure 1: The relationship extraction system.
</figureCaption>
<sectionHeader confidence="0.616211" genericHeader="method">
4.2 Features for Classification
</sectionHeader>
<bodyText confidence="0.999989243243243">
The SVM classification model is built from lexical
and syntactic features assigned to tokens and en-
tity pairs prior to classification. We use features
developed in part from those described in Zhou et
al (2005) and Wang et al (2006). These features are
split into 11 sets, as described in Table 3.
The tokN features are POS and surface string
taken from a window of N tokens on each side of
each paired entity’s mention. For N = 6, this
gives 48 features. The rationale behind these sim-
ple features is that there is useful information in the
words surrounding two mentions, that helps deter-
mine any relationship between them. The gentokN
features generalise tokN to use morphological root
and generalised POS. The str features are a set
of 14 surface string features, encoding the full sur-
face strings of both entity mentions, their heads,
their heads combined, the surface strings of the first,
last and other tokens between the mentions, and
of the two tokens immediately before and after the
leftmost and rightmost mentions respectively. The
pos, root, and genpos feature sets are similarly
constructed from the POS tags, roots, and gener-
alised POS tags of the entity mentions and their sur-
rounding tokens. These four feature sets differ from
tokN and gentokN, in that they provide more fine-
grained information about the position of features
relative to the paired entity mentions.
For the event feature set, the main entities
were divided into events (Investigation and
Intervention) and non-events (all others). Fea-
tures record whether the entity pair consists of two
events, two non-events, one of each, and whether
there are any intervening events and non-events.
This feature set gives similar information to atype
(semantic types of arguments) and inter (inter-
vening entities), but at a coarser level of typing.
</bodyText>
<sectionHeader confidence="0.999063" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.99995315">
We used a standard ten-fold cross validation
methodology and standard evaluation metrics. Met-
rics are defined in terms of true positive, false pos-
itive and false negative matches between relation-
ships in a system annotated response document and
a gold standard key document. A response relation-
ship is a true positive if a relationship of the same
type, and with the exact same arguments, exists in
the key. Corresponding definitions apply for false
positive and false negative. Counts of these matches
are used to calculate standard metrics of Recall (R),
Precision (P) and F1 measure.
The metrics do not say how hard relationship ex-
traction is. We therefore provide a comparison with
Inter Annotator Agreement (IAA) scores from the
gold standard. The IAA score gives the agreement
between the two independent double annotators. It
is equivalent to scoring one annotator against the
other using the F1 metric. IAA scores are not di-
rectly comparable here, as relationship annotation is
</bodyText>
<page confidence="0.99693">
14
</page>
<table confidence="0.999369714285714">
Feature set Size Description
tokN 8N Surface string and POS of tokens surrounding the arguments, windowed −N to +N, N = 6 by default
gentokN 8N Root and gerenalised POS of tokens surrounding the argument entities, windowed −N to +N, N = 6 by default
atype 1 Concatenated semantic type of arguments, in arg1-arg2 order
dir 1 Direction: linear text order of the arguments (is arg1 before arg2, or vice versa?)
dist 2 Distance: absolute number of sentence and paragraph boundaries between arguments
str 14 Surface string features based on Zhou et al (2005), see text for full description
pos 14 POS features, as above
root 14 Root features, as above
genpos 14 Generalised POS features, as above
inter 11 Intervening mentions: numbers and types of intervening entity mentions between arguments
event 5 Events: are any of the arguments, or intevening entities, events?
allgen 96 All features in root and generalised POS forms, i.e. gentok6+atype+dir+dist+root+genpos+inter+event
notok 48 All except tokN features, others in string and POS forms, i.e. atype+dir+dist+str+pos+inter+event
</table>
<tableCaption confidence="0.999789">
Table 3: Feature sets used for learning relationships. The size of a set is the number of features in that set.
</tableCaption>
<bodyText confidence="0.999730315789474">
a slightly different task for the human annotators.
The relationship extraction system is given entities,
and finds relationships between them. Human an-
notators must find both the entities and the relation-
ships. Therefore, were one human annotator to fail
to find a particular entity, they could never find rela-
tionships with that entity. The raw IAA score does
not take this into account: if an annotator fails to
find an entity, then they will also be penalised for
all relationships with that entity. We therefore give a
Corrected IAA, CIAA, in which annotators are only
compared on those relations for which they have
both found the entities involved. Both forms of IAA
are shown in Table 4. It is clear that it is hard for
annotators to reach agreement on relationships, and
that this is compounded massively by lack of perfect
agreement on entities. Note that the gold standard
used in training and evaluation reflects a further con-
sensus annotation, to correct this poor agreement.
</bodyText>
<sectionHeader confidence="0.999992" genericHeader="evaluation">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.999771">
6.1 Feature Selection
</subsectionHeader>
<bodyText confidence="0.999923565217391">
The first group of experiments reported looks at the
performance of relation extraction with various fea-
ture sets. We followed an additive strategy for fea-
ture selection. Starting with basic features, we added
further features one set at a time. We measured the
performance of the resulting classifier each time we
added a new feature set. Results are shown in Ta-
ble 4. The initial classifier used a tok6+atype
feature set. Addition of both dir and dist fea-
tures give significant improvements in all metrics, of
around 10% F1 overall, in each case. This suggests
that the linear text order of arguments, and whether
relations are intra- or inter-sentential is important to
classification. Addition of the str features also give
good improvement in most metrics, again 10% F1
overall. Addition of part-of-speech information, in
the form of pos features, however, leads to a drop
in some metrics, overall F1 dropping by 1%. Unex-
pectedly, POS seems to provide little extra informa-
tion above that in surface string. Errors in POS tag-
ging cannot be dismissed, and could be the cause of
this. The existence of intervening entities, as coded
in feature set inter, provides a small benefit. The
inclusion of information about events, in the event
feature set, is less clear-cut.
We were interested to see if generalising features
could improve performance, as this had benefited
our previous work in entity extraction. We replaced
all surface string features with their root form, and
POS features with their generalised POS form. This
gave the results shown in column allgen. Results
are not clear cut, in some cases better and in some
worse than the previous best. Overall, there is no
difference in F1. There is a slight increase in over-
all recall, and a corresponding drop in precision —
as might be expected.
Both the tokN, and the str and pos feature sets
provide surface string and POS information about
tokens surrounding and between relationship argu-
ments. The former gives features from a window
around each argument. The latter two give a greater
amount of positional information. Do these two pro-
vide enough information on their own, without the
windowed features? To test this, we removed the
tokN features from the full cumulative feature set,
from column +event. Results are given in column
</bodyText>
<page confidence="0.989197">
15
</page>
<table confidence="0.99997664">
Relation Metric tok6+atype +dir +dist +str +pos +inter +event allgen notok IAA CIAA
has finding P 44 49 58 63 62 64 65 63 63
R 39 63 78 80 80 81 81 82 82
F1 39 54 66 70 69 71 72 71 71 46 80
has indication P 37 23 38 42 40 41 42 37 44
R 14 14 46 44 44 47 47 45 47
F1 18 16 39 39 38 41 42 38 41 26 50
has location P 36 36 50 68 71 72 72 73 73
R 28 28 74 79 79 81 81 83 83
F1 30 30 58 72 74 76 75 77 76 55 80
has target P 9 9 32 63 57 60 62 60 59
R 11 11 51 68 67 67 66 68 68
F1 9 9 38 64 60 63 63 63 62 42 63
laterality modifies P 21 38 73 84 83 84 84 86 86
R 9 55 82 89 86 88 88 87 89
F1 12 44 76 85 83 84 84 84 85 73 94
negation modifies P 19 54 85 81 80 79 79 77 81
R 12 82 97 98 93 92 93 93 93
F1 13 63 89 88 85 84 85 83 85 66 93
sub location modifies P 2 2 55 88 86 86 88 88 87
R 1 1 62 94 92 95 95 95 95
F1 1 1 56 90 86 89 91 91 90 49 96
Overall P 33 38 50 63 62 64 65 64 64
R 22 36 70 74 73 75 75 76 76
F1 26 37 58 68 67 69 69 69 70 47 75
</table>
<tableCaption confidence="0.983064">
Table 4: Variation in performance by feature set. Features sets are abbreviated as in Table 3. For the first seven
columns, features were added cumulatively to each other. The next two columns, allgen and notok, are as de-
scribed in Table 3. The final two columns give inter annotator agreement and corrected inter annotator agreement, for
comparison.
</tableCaption>
<bodyText confidence="0.999863888888889">
notok. There is no clear change in performance,
some relationships improving, and some worsening.
Overall, there is a 1% improvement in F 1.
It appears that the bulk of performance is attained
through entity type and distance features, with some
contribution from positional surface string informa-
tion. Performance is between 1% and 9% lower than
CIAA for the same relationship, with a best overall
F1 of 70%, compared to a CIAA of 75%.
</bodyText>
<subsectionHeader confidence="0.999944">
6.2 Sentences Spanned
</subsectionHeader>
<bodyText confidence="0.999962793103448">
Table 2 shows that although most relationships are
intra-sentential, 23% are inter-sentential, 10% of all
relationships being between arguments in adjacent
sentences. If we consider a relationship to cross n
sentence boundaries, then the classifiers described in
the previous section were all trained on relationships
crossing n &lt; 1 sentence boundaries, i.e. with argu-
ments in the same or adjacent sentences. What effect
does including more distant relationships have on
performance? We trained classifiers on only intra-
sentential relationships, and on relationships span-
ning up to n sentence boundaries, for n E {1...5}.
We also trained a classifier on relationships with
1 &lt; n &lt; 5, comprising 85% of all inter-sentential
relationships. In each case, the cumulative feature
set +event from Table 4 was used. Results are
shown in Table 5. It is clear from the results that
the feature sets used do not perform well on inter-
sentential relationships. There is a 6% drop in over-
all F1 when including relationships with n = 1 to-
gether with n &lt; 1. Performance continues to drop as
more inter-sentential relationships are included, and
is very poor for just inter-sentential relationships.
A preliminary error analysis suggests that the
more distant relationship arguments are from each
other, the more likely clinical knowledge is required
to extract the relationship. This raises additional dif-
ficulties for extraction, which the simple features de-
scribed here are unable to address.
</bodyText>
<subsectionHeader confidence="0.999287">
6.3 Size of Training Corpus
</subsectionHeader>
<bodyText confidence="0.9999115">
The provision of sufficient training data for super-
vised learning algorithms is a limitation on their use.
We examined the effect of training corpus size on
relationship extraction. The C77 corpus, compris-
</bodyText>
<page confidence="0.990501">
16
</page>
<table confidence="0.999933222222222">
Number of sentence boundaries between arguments Corpus size
inter- intra- inter- and intra-sentential
Relation Metric 1 &lt; n &lt; 5 n &lt; 1 n &lt; 1 n &lt; 2 n &lt; 3 n &lt; 4 n &lt; 5 C25 C50 C77
has finding P 24 68 65 62 60 61 61 66 63 65
R 18 89 81 79 78 78 77 74 74 81
F1 18 76 72 69 67 68 67 67 67 72
has indication P 18 49 42 42 36 32 30 22 25 42
R 17 59 47 42 42 39 38 30 31 47
F1 16 51 42 39 37 34 33 23 25 42
has location P 0 74 72 73 72 72 72 72 71 72
R 0 83 81 81 81 82 82 76 80 81
F1 0 77 75 76 75 76 76 73 74 75
has target P 3 64 62 59 60 59 58 65 49 62
R 1 75 66 64 62 61 61 60 65 66
F1 2 68 63 61 60 60 59 59 54 63
laterality modifies P 0 86 84 86 86 86 87 77 78 84
R 0 89 88 88 88 87 88 69 68 88
F1 0 85 84 85 86 85 86 72 69 84
negation modifies P 0 80 79 79 80 80 80 78 79 79
R 0 94 93 91 93 93 93 80 93 93
F1 0 86 85 84 85 86 85 78 84 85
sub location modifies P 0 89 88 88 89 89 89 64 91 88
R 0 95 95 95 95 95 95 64 85 95
F1 0 91 91 91 91 91 91 64 86 91
Overall P 22 69 65 64 62 61 60 62 63 65
R 17 83 75 73 71 70 70 65 71 75
F1 19 75 69 68 66 65 65 63 66 69
</table>
<tableCaption confidence="0.999952">
Table 5: Variation in performance, by number of sentence boundaries (n), and by training corpus size.
</tableCaption>
<bodyText confidence="0.999767071428571">
ing 77 narratives and used in the previous experi-
ments, was subsetted to give corpora of 25 and 50
narratives, which will be referred to as C25 and C50
respectively. We trained two further classifiers on
these new corpora. Again, the cumulative feature
set +event from Table 4 was used. Results are
shown in Table 5. Overall, performance improves as
training corpus size increases (F1 rising from 63%
to 69%). We were struck however, by the fact that
increasing from 50 to 77 documents has little effect
onafewrelationships(negation modifies and
has location). It may well be that the amount
of training data required has plateaued for those re-
lationships.
</bodyText>
<sectionHeader confidence="0.999285" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99985184375">
We have shown that it is possible to extract clini-
cal relationships from text, using shallow features,
and supervised statistical machine learning. Judg-
ing from poor inter annotator agreement, the task
is hard. Our system achieves a reasonable perfor-
mance, with an overall F1 just 5% below a cor-
rected inter annotator agreement. This performance
is reached largely by using features of the text that
encode entity type, distance between arguments, and
some surface string information. Performance does,
however, vary with the number of sentences spanned
by the relationships. Learning inter-sentential rela-
tionships does not seem amenable to this approach,
and may require the use of domain knowledge.
A major concern when using supervised learning
algorithms is the expense and availability of training
data. We have shown that while this concern is jus-
tified in some cases, larger training corpora may not
improve performance for all relationships.
The technology used has proved scalable. The
full CLEF IE system, including automatic entity
recognition, is able to process a document in sub-
second time on a commodity workstation. We
have used the system to extract 6 million relations
from over half a million patient documents, for use
in downstream CLEF applications (Roberts et al.,
2008a). Our future work on relationship extrac-
tion in CLEF includes integration of a dependency
parse into the feature set, further analysis to deter-
mine what knowledge may be required to learn inter-
sentential relations, and integration of relationship
extraction with a co-reference algorithm.
</bodyText>
<page confidence="0.996744">
17
</page>
<bodyText confidence="0.999407285714286">
Availability All of the software described here
is open source and can be downloaded as part of
GATE, with the exception of the entity pairing com-
ponent, which will be released shortly. We are cur-
rently preparing a UK research ethics committee ap-
plication, requesting permission to release our anno-
tated corpus.
</bodyText>
<sectionHeader confidence="0.995724" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998446">
CLEF is funded by the UK Medical Research Coun-
cil. We would like to thank the Royal Marsden
Hospital for providing the corpus, and our clinical
partners in CLEF for assistance in developing the
schema, and for gold standard annotation.
</bodyText>
<sectionHeader confidence="0.99967" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999884918604651">
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphi-
cal development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics, pages 168–175, Philadelphia, PA, USA, July.
C. Friedman, P. Alderson, J. Austin, J. Cimino, and
S. Johnson. 1994. A general natural-language text
processor for clinical radiology. Journal of the Amer-
ican Medical Informatics Association, 1(2):161–174,
March.
C. Grover, B. Haddow, E. Klein, M. Matthews,
L. Nielsen, R. Tobin, and X. Wang. 2007. Adapting
a relation extraction pipeline for the BioCreAtIvE II
task. In Proceedings of the BioCreAtIvE II Workshop
2007, Madrid, Spain.
U. Hahn, M. Romacker, and S. Schulz. 2002. MEDSYN-
DIKATE — a natural language system for the ex-
traction of medical information from findings reports.
International Journal of Medical Informatics, 67(1–
3):63–74, December.
Y. Li, K. Bontcheva, and H. Cunningham. 2005.
SVM based learning system for information extrac-
tion. In Deterministic and statistical methods in ma-
chine learning: first international workshop, number
3635 in Lecture Notes in Computer Science, pages
319–339. Springer.
D. Lindberg, B. Humphreys, and A. McCray. 1993. The
Unified Medical Language System. Methods of Infor-
mation in Medicine, 32(4):281–291.
Y. Lussier, T. Borlawsky, D. Rappaport, Y. Liu, and
C. Friedman. 2006. PhenoGO: Assigning phenotypic
context to Gene Ontology annotations with natural lan-
guage processing. In Biocomputing 2006, Proceed-
ings of the Pacific Symposium, pages 64–75, Hawaii,
USA, January.
S. Pakhomov, J. Buntrock, and P. Duffy. 2005. High
throughput modularized NLP system for clinical text.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL’05),
interactive poster and demonstration sessions, pages
25–28, Ann Arbor, MI, USA, June.
A. Rector, J. Rogers, A. Taweel, D. Ingram, D. Kalra,
J. Milan, P. Singleton, R. Gaizauskas, M. Hepple,
D. Scott, and R. Power. 2003. CLEF — joining up
healthcare with clinical and post-genomic research. In
Proceedings of UK e-Science All Hands Meeting 2003,
pages 264–267, Nottingham, UK.
T. Rindflesch and M. Fiszman. 2003. The interaction of
domain knowledge and linguistic structure in natural
language processing: interpreting hypernymic propo-
sitions in biomedical text. Journal of Biomedical In-
formatics, 36(6):462–477.
A. Roberts, R. Gaizauskas, M. Hepple, G. Demetriou,
Y. Guo, A. Setzer, and I. Roberts. 2008a. Seman-
tic annotation of clinical text: The CLEF corpus. In
Proceedings of Building and evaluating resources for
biomedical text mining: workshop at LREC 2008,
Marrakech, Morocco, May. In press.
A. Roberts, R. Gaizauskas, M. Hepple, and Y. Guo.
2008b. Combining terminology resources and statis-
tical methods for entity recognition: an evaluation.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation, LREC 2008,
Marrakech, Morocco, May. In press.
N. Sager, M. Lyman, C. Bucknall, N. Nhan, and L. Tick.
1994. Natural language processing and the representa-
tion of clinical data. Journal of the American Medical
Informatics Association, 1(2):142–160, March-April.
T. Wang, Y. Li, K. Bontcheva, H. Cunningham, and
J. Wang. 2006. Automatic extraction of hierarchical
relations from text. In The Semantic Web: Research
and Applications. 3rd European Semantic Web Con-
ference, ESWC 2006, number 4011 in Lecture Notes
in Computer Science, pages 215–229. Springer.
G. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. Ex-
ploring Various Knowledge in Relation Extraction. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL’05), pages
427–434, Ann Arbor, MI, USA, June.
P. Zweigenbaum, B. Bachimont, J. Bouaud, J. Charlet,
and J-F. Boisvieux. 1995. A multi-lingual architec-
ture for building a normalised conceptual representa-
tion from medical language. In Proceedings of the An-
nual Symposium on Computer Applications in Medical
Care, pages 357–361, New York, NY, USA.
</reference>
<page confidence="0.999292">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.495102">
<title confidence="0.999919">Extracting Clinical Relationships from Patient Narratives</title>
<author confidence="0.999297">Angus Roberts</author>
<author confidence="0.999297">Robert Gaizauskas</author>
<author confidence="0.999297">Mark</author>
<affiliation confidence="0.971049">Department of Computer Science, University of</affiliation>
<address confidence="0.555968">Regent Court, 211 Portobello, Sheffield S1</address>
<abstract confidence="0.995991344827586">The Clinical E-Science Framework (CLEF) project has built a system to extract clinically significant information from the textual component of medical records, for clinical research, evidence-based healthcare and genotype-meets-phenotype informatics. One part of this system is the identification of relationships between clinically important entities in the text. Typical approaches to relationship extraction in this domain have used full parses, domain-specific grammars, and large knowledge bases encoding domain knowledge. In other areas of biomedical NLP, statistical machine learning approaches are now routinely applied to relationship extraction. We report on the novel application of these statistical techniques to clinical relationships. We describe a supervised machine learning system, trained with a corpus of oncology narratives hand-annotated with clinically important relationships. Various shallow features are extracted from these texts, and used to train statistical classifiers. We compare the suitability of these features for clinical relationship extraction, how extraction varies between interand intra-sentential relationships, and examine the amount of training data needed to learn various relationships.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>GATE: A framework and graphical development environment for robust NLP tools and applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics,</booktitle>
<pages>168--175</pages>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="11717" citStr="Cunningham et al., 2002" startWordPosition="1775" endWordPosition="1778">d standard is thus analogous to the style of relationship extraction reported here, in which we extract relations between single mention entities, and do not consider co-reference. Annotators were further told that relationships could span multiple sentences, and that it was acceptable to use clinical domain knowledge to infer that a relationship existed between two mentions. Counts of all relationships annotated in C77 are shown in Table 2, sub-divided by the number of sentence boundaries spanned by a relationship. 4 Relationship Extraction The system we have built uses the GATE NLP toolkit (Cunningham et al., 2002) 1. The system is shown in Figure 1, and is described below. Narratives are first pre-processed using standard GATE modules. Narratives were tokenised, sentences found with a regular expression-based sentence splitter, part-of-speech (POS) tagged, and morphological roots found for tokens. Each token was also labelled with a generalised POS tag, the first two characters of the full POS tag. This takes advantage of the Penn Treebank tagset used by GATE’s POS tagger, in which related POS tags share the first two characters. For example, all six verb POS tags start with the letters “VB”. After pre</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. 2002. GATE: A framework and graphical development environment for robust NLP tools and applications. In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics, pages 168–175, Philadelphia, PA, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Friedman</author>
<author>P Alderson</author>
<author>J Austin</author>
<author>J Cimino</author>
<author>S Johnson</author>
</authors>
<title>A general natural-language text processor for clinical radiology.</title>
<date>1994</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="4024" citStr="Friedman et al., 1994" startWordPosition="570" endWordPosition="573">s part of a full clinical IE system. Several such systems have been described. They generally use a syntactic parse with domainspecific grammar rules. The Linguistic String project (Sager et al., 1994) used a full syntactic and BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 10–18, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics clinical sublanguage parse to fill template data structures corresponding to medical statements. These were mapped to a database model incorporating medical facts and the relationships between them. MedLEE (Friedman et al., 1994), and more recently BioMedLEE (Lussier et al., 2006) used a semantic lexicon and grammar of domain-specific semantic patterns. The patterns encode the possible relationships between entities, allowing both entities and the relationships between them to be directly matched in the text. Other systems have incorporated largescale domain-specific knowledge bases. MEDSYNDIKATE (Hahn et al., 2002) employed a rich discourse model of entities and their relationships, built using a dependency parse of texts and a description logic knowledge base re-engineered from existing terminologies. MENELAS (Zweig</context>
</contexts>
<marker>Friedman, Alderson, Austin, Cimino, Johnson, 1994</marker>
<rawString>C. Friedman, P. Alderson, J. Austin, J. Cimino, and S. Johnson. 1994. A general natural-language text processor for clinical radiology. Journal of the American Medical Informatics Association, 1(2):161–174, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Grover</author>
<author>B Haddow</author>
<author>E Klein</author>
<author>M Matthews</author>
<author>L Nielsen</author>
<author>R Tobin</author>
<author>X Wang</author>
</authors>
<title>Adapting a relation extraction pipeline for the BioCreAtIvE II task.</title>
<date>2007</date>
<booktitle>In Proceedings of the BioCreAtIvE II Workshop</booktitle>
<location>Madrid,</location>
<contexts>
<context position="5144" citStr="Grover et al (2007)" startWordPosition="741" endWordPosition="744">ts and a description logic knowledge base re-engineered from existing terminologies. MENELAS (Zweigenbaum et al., 1995) also used a full parse, a conceptual representation of the text, and a large scale knowledge base. In other applications of biomedical NLP, a second paradigm has become widespread: the application of statistical machine learning techniques to feature-based models of the text. Such approaches have typically been applied to journal texts. They have been used both for entity recognition and extraction of various relations, such as protein-protein interactions (see, for example, Grover et al (2007)). This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)). Statistical machine learning has also been applied to clinical text, but its use has generally been limited to entity recognition. The Mayo Clinic text analysis system (Pakhomov et al., 2005), for example, uses a combination of dictionary lookup and a Naive Bayes classifier to identify entities for information retrieval applications. To the best of our knowledge, statistical methods have not been previously applied to extraction of clinical relationships from text. This paper describes expe</context>
</contexts>
<marker>Grover, Haddow, Klein, Matthews, Nielsen, Tobin, Wang, 2007</marker>
<rawString>C. Grover, B. Haddow, E. Klein, M. Matthews, L. Nielsen, R. Tobin, and X. Wang. 2007. Adapting a relation extraction pipeline for the BioCreAtIvE II task. In Proceedings of the BioCreAtIvE II Workshop 2007, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hahn</author>
<author>M Romacker</author>
<author>S Schulz</author>
</authors>
<title>MEDSYNDIKATE — a natural language system for the extraction of medical information from findings reports.</title>
<date>2002</date>
<journal>International Journal of Medical Informatics,</journal>
<volume>67</volume>
<issue>1</issue>
<pages>3--63</pages>
<contexts>
<context position="4418" citStr="Hahn et al., 2002" startWordPosition="627" endWordPosition="630">cal sublanguage parse to fill template data structures corresponding to medical statements. These were mapped to a database model incorporating medical facts and the relationships between them. MedLEE (Friedman et al., 1994), and more recently BioMedLEE (Lussier et al., 2006) used a semantic lexicon and grammar of domain-specific semantic patterns. The patterns encode the possible relationships between entities, allowing both entities and the relationships between them to be directly matched in the text. Other systems have incorporated largescale domain-specific knowledge bases. MEDSYNDIKATE (Hahn et al., 2002) employed a rich discourse model of entities and their relationships, built using a dependency parse of texts and a description logic knowledge base re-engineered from existing terminologies. MENELAS (Zweigenbaum et al., 1995) also used a full parse, a conceptual representation of the text, and a large scale knowledge base. In other applications of biomedical NLP, a second paradigm has become widespread: the application of statistical machine learning techniques to feature-based models of the text. Such approaches have typically been applied to journal texts. They have been used both for entit</context>
</contexts>
<marker>Hahn, Romacker, Schulz, 2002</marker>
<rawString>U. Hahn, M. Romacker, and S. Schulz. 2002. MEDSYNDIKATE — a natural language system for the extraction of medical information from findings reports. International Journal of Medical Informatics, 67(1– 3):63–74, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>K Bontcheva</author>
<author>H Cunningham</author>
</authors>
<title>SVM based learning system for information extraction.</title>
<date>2005</date>
<booktitle>In Deterministic and statistical methods in machine learning: first international workshop, number 3635 in Lecture Notes in Computer Science,</booktitle>
<pages>319--339</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="16140" citStr="Li et al (2005)" startWordPosition="2556" endWordPosition="2559">pes, or null, to each entity pair. We use Support Vector machines (SVMs) as trainable classifiers, as these have proved to be robust and efficient for a range of NLP tasks, including relation extraction. We use an SVM implementation developed within our own group, and provided as part of the GATE toolkit. This is a variant on the original SVM algorithm, SVM with uneven margins, in which classification may be biased towards positive training examples. This is particularly suited to NLP applications, in which positive training examples are often rare. Full details of the classifier are given in Li et al (2005). We used the implementation “out of the box”, with default parameters as determined in experiments with other data sets. SVMs are binary classifiers: the multi-class problem of classifying entity pairs must therefore be mapped to a number of binary classification problems. There are several ways in which a multiclass problem can be recast as binary problems. The commonest are one-against-one in which one classifier is trained for every possible pair of classes, and one-against-all in which a classifier is trained for a binary decision between each class and all other 13 classes, including nul</context>
</contexts>
<marker>Li, Bontcheva, Cunningham, 2005</marker>
<rawString>Y. Li, K. Bontcheva, and H. Cunningham. 2005. SVM based learning system for information extraction. In Deterministic and statistical methods in machine learning: first international workshop, number 3635 in Lecture Notes in Computer Science, pages 319–339. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lindberg</author>
<author>B Humphreys</author>
<author>A McCray</author>
</authors>
<title>The Unified Medical Language System.</title>
<date>1993</date>
<journal>Methods of Information in Medicine,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="8325" citStr="Lindberg et al., 1993" startWordPosition="1231" endWordPosition="1234">negation of a condition (“no sign of inflammation”). Entities are connected to each other and to modifiers by relationships: e.g. linking a drug entity to the condition entity for which it is indicated, linking an investigation to its results, or linking a negating phrase to a condition. The entities, modifiers, and relationships are described by both a formal XML schema, and by a set of detailed definitions. These were developed by a group of clinical experts through an iterative process, until acceptable agreement was reached. Entity types are mapped to types from the UMLS semantic network (Lindberg et al., 1993), each CLEF en11 tity type covering several UMLS types. Relationship types are those that were felt necessary to capture the essential clinical dependencies between entities referred to in patient documents, and to support CLEF end user applications. Each relationship type is constrained to exist between limited pairs of entity types. For example, the has location relationship can only exist between a Condition entity and a Locus entity. Some relationships can exist between multiple type pairs. The full set of relationships and their argument type constraints are shown in Table 1. Examples of </context>
</contexts>
<marker>Lindberg, Humphreys, McCray, 1993</marker>
<rawString>D. Lindberg, B. Humphreys, and A. McCray. 1993. The Unified Medical Language System. Methods of Information in Medicine, 32(4):281–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lussier</author>
<author>T Borlawsky</author>
<author>D Rappaport</author>
<author>Y Liu</author>
<author>C Friedman</author>
</authors>
<title>PhenoGO: Assigning phenotypic context to Gene Ontology annotations with natural language processing.</title>
<date>2006</date>
<booktitle>In Biocomputing 2006, Proceedings of the Pacific Symposium,</booktitle>
<pages>64--75</pages>
<location>Hawaii, USA,</location>
<contexts>
<context position="4076" citStr="Lussier et al., 2006" startWordPosition="578" endWordPosition="581">tems have been described. They generally use a syntactic parse with domainspecific grammar rules. The Linguistic String project (Sager et al., 1994) used a full syntactic and BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 10–18, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics clinical sublanguage parse to fill template data structures corresponding to medical statements. These were mapped to a database model incorporating medical facts and the relationships between them. MedLEE (Friedman et al., 1994), and more recently BioMedLEE (Lussier et al., 2006) used a semantic lexicon and grammar of domain-specific semantic patterns. The patterns encode the possible relationships between entities, allowing both entities and the relationships between them to be directly matched in the text. Other systems have incorporated largescale domain-specific knowledge bases. MEDSYNDIKATE (Hahn et al., 2002) employed a rich discourse model of entities and their relationships, built using a dependency parse of texts and a description logic knowledge base re-engineered from existing terminologies. MENELAS (Zweigenbaum et al., 1995) also used a full parse, a conce</context>
</contexts>
<marker>Lussier, Borlawsky, Rappaport, Liu, Friedman, 2006</marker>
<rawString>Y. Lussier, T. Borlawsky, D. Rappaport, Y. Liu, and C. Friedman. 2006. PhenoGO: Assigning phenotypic context to Gene Ontology annotations with natural language processing. In Biocomputing 2006, Proceedings of the Pacific Symposium, pages 64–75, Hawaii, USA, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pakhomov</author>
<author>J Buntrock</author>
<author>P Duffy</author>
</authors>
<title>High throughput modularized NLP system for clinical text.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), interactive poster and demonstration sessions,</booktitle>
<pages>25--28</pages>
<location>Ann Arbor, MI, USA,</location>
<contexts>
<context position="5440" citStr="Pakhomov et al., 2005" startWordPosition="790" endWordPosition="793">spread: the application of statistical machine learning techniques to feature-based models of the text. Such approaches have typically been applied to journal texts. They have been used both for entity recognition and extraction of various relations, such as protein-protein interactions (see, for example, Grover et al (2007)). This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)). Statistical machine learning has also been applied to clinical text, but its use has generally been limited to entity recognition. The Mayo Clinic text analysis system (Pakhomov et al., 2005), for example, uses a combination of dictionary lookup and a Naive Bayes classifier to identify entities for information retrieval applications. To the best of our knowledge, statistical methods have not been previously applied to extraction of clinical relationships from text. This paper describes experiments in the statistical machine learning of relationships from a novel text type: oncology narratives. The set of relationships extracted are considered to be of interest for clinical and research applications down line of IE, such as querying to support clinical research. We apply Support Ve</context>
</contexts>
<marker>Pakhomov, Buntrock, Duffy, 2005</marker>
<rawString>S. Pakhomov, J. Buntrock, and P. Duffy. 2005. High throughput modularized NLP system for clinical text. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), interactive poster and demonstration sessions, pages 25–28, Ann Arbor, MI, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rector</author>
<author>J Rogers</author>
<author>A Taweel</author>
<author>D Ingram</author>
<author>D Kalra</author>
<author>J Milan</author>
<author>P Singleton</author>
<author>R Gaizauskas</author>
<author>M Hepple</author>
<author>D Scott</author>
<author>R Power</author>
</authors>
<title>CLEF — joining up healthcare with clinical and post-genomic research.</title>
<date>2003</date>
<booktitle>In Proceedings of UK e-Science All Hands Meeting</booktitle>
<pages>264--267</pages>
<location>Nottingham, UK.</location>
<contexts>
<context position="2246" citStr="Rector et al., 2003" startWordPosition="303" endWordPosition="306"> to the ever-burgeoning research literature. Increasingly, biomedical researchers need to relate this literature to phenotypic data: both to populations, and to individual clinical subjects. The computer applications 10 used in biomedical research, including NLP applications, therefore need to support genotype-meetsphenotype informatics and the move towards translational biology. Such support will undoubtedly include linkage to the information held in individual medical records: both the structured portion, and the unstructured textual portion. The Clinical E-Science Framework (CLEF) project (Rector et al., 2003) is building a framework for the capture, integration and presentation of this clinical information, for research and evidencebased health care. The project’s data resource is a repository of the full clinical records for over 20000 cancer patients from the Royal Marsden Hospital, Europe’s largest oncology centre. These records combine structured information, clinical narratives, and free text investigation reports. CLEF uses information extraction (IE) technology to make information from the textual portion of the medical record available for integration with the structured record, and thus a</context>
</contexts>
<marker>Rector, Rogers, Taweel, Ingram, Kalra, Milan, Singleton, Gaizauskas, Hepple, Scott, Power, 2003</marker>
<rawString>A. Rector, J. Rogers, A. Taweel, D. Ingram, D. Kalra, J. Milan, P. Singleton, R. Gaizauskas, M. Hepple, D. Scott, and R. Power. 2003. CLEF — joining up healthcare with clinical and post-genomic research. In Proceedings of UK e-Science All Hands Meeting 2003, pages 264–267, Nottingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Rindflesch</author>
<author>M Fiszman</author>
</authors>
<title>The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text.</title>
<date>2003</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>36</volume>
<issue>6</issue>
<contexts>
<context position="14432" citStr="Rindflesch and Fiszman, 2003" startWordPosition="2276" endWordPosition="2279">ewhere (Roberts et al., 2008b). 4.1 Classification We treat clinical relationship extraction as a classification task, training classifiers to assign a relationship type to an entity pair. An entity pair is a pairing of entities that may or may not be the arguments of a relation. For a given document, we create all possible entity pairs within two constraints. First, entities that are paired must be within n sentences of each other. For all of the work reported here, unless stated, n &lt; 1 (crossing 0 or 1 sentence boundaries). Second, we can constrain the entity pairs created by argument type (Rindflesch and Fiszman, 2003). For example, there is little point in creating an entity pair between a Drug or device entity and a Result entity, as no relationships, as specified by the schema, exist between entities of these types. Entity pairing is carried out by a GATE component developed specifically for clinical relationship extraction. In addition to pairing entities according to the above constraints, this component also assigns features to each pair that characterise its lexical and syntactic qualities (described further in Section 4.2). Entity pairs correspond to classifier training and test instances. In classi</context>
</contexts>
<marker>Rindflesch, Fiszman, 2003</marker>
<rawString>T. Rindflesch and M. Fiszman. 2003. The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text. Journal of Biomedical Informatics, 36(6):462–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Roberts</author>
<author>R Gaizauskas</author>
<author>M Hepple</author>
<author>G Demetriou</author>
<author>Y Guo</author>
<author>A Setzer</author>
<author>I Roberts</author>
</authors>
<title>Semantic annotation of clinical text: The CLEF corpus.</title>
<date>2008</date>
<booktitle>In Proceedings of Building and</booktitle>
<location>Marrakech, Morocco,</location>
<note>In press.</note>
<contexts>
<context position="3282" citStr="Roberts et al., 2008" startWordPosition="462" endWordPosition="465">ts. CLEF uses information extraction (IE) technology to make information from the textual portion of the medical record available for integration with the structured record, and thus available for clinical care and research. The CLEF IE system analyses the textual records to extract entities, events and the relationships between them. These relationships give information that is often not available in the structured record. Why was a drug given? What were the results of a physical examination? What problems were not present? We have previously reported entity extraction in the CLEF IE system (Roberts et al., 2008b). This paper examines relationship extraction. Extraction of relationships from clinical text is usually carried out as part of a full clinical IE system. Several such systems have been described. They generally use a syntactic parse with domainspecific grammar rules. The Linguistic String project (Sager et al., 1994) used a full syntactic and BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 10–18, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics clinical sublanguage parse to fill template data structures corresponding to medical st</context>
<context position="8975" citStr="Roberts et al (2008" startWordPosition="1338" endWordPosition="1341">ring several UMLS types. Relationship types are those that were felt necessary to capture the essential clinical dependencies between entities referred to in patient documents, and to support CLEF end user applications. Each relationship type is constrained to exist between limited pairs of entity types. For example, the has location relationship can only exist between a Condition entity and a Locus entity. Some relationships can exist between multiple type pairs. The full set of relationships and their argument type constraints are shown in Table 1. Examples of each relationship are given in Roberts et al (2008a). Some of the relationships considered important by the clinical experts were not obvious without domain knowledge. For example, He is suffering from nausea and severe headaches. Dolasteron was prescribed. Without domain knowledge, it is not clear that there is a has indication relationship between the “Dolasteron” Drug or device entity and the “nausea” Condition entity. As in this example, many of this type of relationship are intra-sentential. A single real-world entity may be referred to several times in the same text. Each of these coreferring expressions is a mention of the entity. The </context>
<context position="10421" citStr="Roberts et al (2008" startWordPosition="1568" endWordPosition="1571">ion. Relationships between entities can be considered, by extension, as relationships between the single mentions of those entities. The implications of this are discussed further below. 3 Gold Standard Corpus The schema and definitions were used to handannotate the entities and relationships in 77 oncology narratives, to provide a gold standard for system training and evaluation. Corpora of this size are typical in supervised machine learning, and reflect the expense of hand annotation. Narratives were carefully selected and annotated according to a best practice methodology, as described in Roberts et al (2008a). Narratives were annotated by two independent, clinically trained, annotators, and a consensus created by a third. We will refer to this corpus as C77. Annotators were asked to first mark the mentions of entities and modifiers, and then to go through each of these in turn, deciding if any had relationships with mentions of other entities. Although the annotators were marking co-reference between mentions of the same entity, they were asked to ignore this with respect to relationship annotation. Both the annotation tool that they were using and their annotation guidelines, enforced the creat</context>
<context position="13831" citStr="Roberts et al., 2008" startWordPosition="2172" endWordPosition="2175">d be expected in a system with automatic entity recognition. It is useful and usual to fix entity recognition in this way, to allow tuning specific to relationship extraction, and to allow the isolation of relation-specific problems. We accept, however, that ultimately, relation extraction does depend on the quality of entity recognition. The relation extraction described here is used as part of an operational IE system in which clinical entity recognition is performed by a combination of lexical lookup and supervised machine learning. We have described our entity extraction system elsewhere (Roberts et al., 2008b). 4.1 Classification We treat clinical relationship extraction as a classification task, training classifiers to assign a relationship type to an entity pair. An entity pair is a pairing of entities that may or may not be the arguments of a relation. For a given document, we create all possible entity pairs within two constraints. First, entities that are paired must be within n sentences of each other. For all of the work reported here, unless stated, n &lt; 1 (crossing 0 or 1 sentence boundaries). Second, we can constrain the entity pairs created by argument type (Rindflesch and Fiszman, 2003</context>
<context position="31637" citStr="Roberts et al., 2008" startWordPosition="5350" endWordPosition="5353">the use of domain knowledge. A major concern when using supervised learning algorithms is the expense and availability of training data. We have shown that while this concern is justified in some cases, larger training corpora may not improve performance for all relationships. The technology used has proved scalable. The full CLEF IE system, including automatic entity recognition, is able to process a document in subsecond time on a commodity workstation. We have used the system to extract 6 million relations from over half a million patient documents, for use in downstream CLEF applications (Roberts et al., 2008a). Our future work on relationship extraction in CLEF includes integration of a dependency parse into the feature set, further analysis to determine what knowledge may be required to learn intersentential relations, and integration of relationship extraction with a co-reference algorithm. 17 Availability All of the software described here is open source and can be downloaded as part of GATE, with the exception of the entity pairing component, which will be released shortly. We are currently preparing a UK research ethics committee application, requesting permission to release our annotated co</context>
</contexts>
<marker>Roberts, Gaizauskas, Hepple, Demetriou, Guo, Setzer, Roberts, 2008</marker>
<rawString>A. Roberts, R. Gaizauskas, M. Hepple, G. Demetriou, Y. Guo, A. Setzer, and I. Roberts. 2008a. Semantic annotation of clinical text: The CLEF corpus. In Proceedings of Building and evaluating resources for biomedical text mining: workshop at LREC 2008, Marrakech, Morocco, May. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Roberts</author>
<author>R Gaizauskas</author>
<author>M Hepple</author>
<author>Y Guo</author>
</authors>
<title>Combining terminology resources and statistical methods for entity recognition: an evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation, LREC 2008,</booktitle>
<location>Marrakech, Morocco,</location>
<note>In press.</note>
<contexts>
<context position="3282" citStr="Roberts et al., 2008" startWordPosition="462" endWordPosition="465">ts. CLEF uses information extraction (IE) technology to make information from the textual portion of the medical record available for integration with the structured record, and thus available for clinical care and research. The CLEF IE system analyses the textual records to extract entities, events and the relationships between them. These relationships give information that is often not available in the structured record. Why was a drug given? What were the results of a physical examination? What problems were not present? We have previously reported entity extraction in the CLEF IE system (Roberts et al., 2008b). This paper examines relationship extraction. Extraction of relationships from clinical text is usually carried out as part of a full clinical IE system. Several such systems have been described. They generally use a syntactic parse with domainspecific grammar rules. The Linguistic String project (Sager et al., 1994) used a full syntactic and BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 10–18, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics clinical sublanguage parse to fill template data structures corresponding to medical st</context>
<context position="8975" citStr="Roberts et al (2008" startWordPosition="1338" endWordPosition="1341">ring several UMLS types. Relationship types are those that were felt necessary to capture the essential clinical dependencies between entities referred to in patient documents, and to support CLEF end user applications. Each relationship type is constrained to exist between limited pairs of entity types. For example, the has location relationship can only exist between a Condition entity and a Locus entity. Some relationships can exist between multiple type pairs. The full set of relationships and their argument type constraints are shown in Table 1. Examples of each relationship are given in Roberts et al (2008a). Some of the relationships considered important by the clinical experts were not obvious without domain knowledge. For example, He is suffering from nausea and severe headaches. Dolasteron was prescribed. Without domain knowledge, it is not clear that there is a has indication relationship between the “Dolasteron” Drug or device entity and the “nausea” Condition entity. As in this example, many of this type of relationship are intra-sentential. A single real-world entity may be referred to several times in the same text. Each of these coreferring expressions is a mention of the entity. The </context>
<context position="10421" citStr="Roberts et al (2008" startWordPosition="1568" endWordPosition="1571">ion. Relationships between entities can be considered, by extension, as relationships between the single mentions of those entities. The implications of this are discussed further below. 3 Gold Standard Corpus The schema and definitions were used to handannotate the entities and relationships in 77 oncology narratives, to provide a gold standard for system training and evaluation. Corpora of this size are typical in supervised machine learning, and reflect the expense of hand annotation. Narratives were carefully selected and annotated according to a best practice methodology, as described in Roberts et al (2008a). Narratives were annotated by two independent, clinically trained, annotators, and a consensus created by a third. We will refer to this corpus as C77. Annotators were asked to first mark the mentions of entities and modifiers, and then to go through each of these in turn, deciding if any had relationships with mentions of other entities. Although the annotators were marking co-reference between mentions of the same entity, they were asked to ignore this with respect to relationship annotation. Both the annotation tool that they were using and their annotation guidelines, enforced the creat</context>
<context position="13831" citStr="Roberts et al., 2008" startWordPosition="2172" endWordPosition="2175">d be expected in a system with automatic entity recognition. It is useful and usual to fix entity recognition in this way, to allow tuning specific to relationship extraction, and to allow the isolation of relation-specific problems. We accept, however, that ultimately, relation extraction does depend on the quality of entity recognition. The relation extraction described here is used as part of an operational IE system in which clinical entity recognition is performed by a combination of lexical lookup and supervised machine learning. We have described our entity extraction system elsewhere (Roberts et al., 2008b). 4.1 Classification We treat clinical relationship extraction as a classification task, training classifiers to assign a relationship type to an entity pair. An entity pair is a pairing of entities that may or may not be the arguments of a relation. For a given document, we create all possible entity pairs within two constraints. First, entities that are paired must be within n sentences of each other. For all of the work reported here, unless stated, n &lt; 1 (crossing 0 or 1 sentence boundaries). Second, we can constrain the entity pairs created by argument type (Rindflesch and Fiszman, 2003</context>
<context position="31637" citStr="Roberts et al., 2008" startWordPosition="5350" endWordPosition="5353">the use of domain knowledge. A major concern when using supervised learning algorithms is the expense and availability of training data. We have shown that while this concern is justified in some cases, larger training corpora may not improve performance for all relationships. The technology used has proved scalable. The full CLEF IE system, including automatic entity recognition, is able to process a document in subsecond time on a commodity workstation. We have used the system to extract 6 million relations from over half a million patient documents, for use in downstream CLEF applications (Roberts et al., 2008a). Our future work on relationship extraction in CLEF includes integration of a dependency parse into the feature set, further analysis to determine what knowledge may be required to learn intersentential relations, and integration of relationship extraction with a co-reference algorithm. 17 Availability All of the software described here is open source and can be downloaded as part of GATE, with the exception of the entity pairing component, which will be released shortly. We are currently preparing a UK research ethics committee application, requesting permission to release our annotated co</context>
</contexts>
<marker>Roberts, Gaizauskas, Hepple, Guo, 2008</marker>
<rawString>A. Roberts, R. Gaizauskas, M. Hepple, and Y. Guo. 2008b. Combining terminology resources and statistical methods for entity recognition: an evaluation. In Proceedings of the Sixth International Conference on Language Resources and Evaluation, LREC 2008, Marrakech, Morocco, May. In press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>N Sager</author>
<author>M Lyman</author>
<author>C Bucknall</author>
<author>N Nhan</author>
<author>L Tick</author>
</authors>
<title>Natural language processing and the representation of clinical data.</title>
<date>1994</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<booktitle>In The Semantic Web: Research and Applications. 3rd European Semantic Web Conference, ESWC 2006, number 4011 in Lecture Notes in Computer Science,</booktitle>
<volume>1</volume>
<issue>2</issue>
<pages>215--229</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3603" citStr="Sager et al., 1994" startWordPosition="511" endWordPosition="514"> between them. These relationships give information that is often not available in the structured record. Why was a drug given? What were the results of a physical examination? What problems were not present? We have previously reported entity extraction in the CLEF IE system (Roberts et al., 2008b). This paper examines relationship extraction. Extraction of relationships from clinical text is usually carried out as part of a full clinical IE system. Several such systems have been described. They generally use a syntactic parse with domainspecific grammar rules. The Linguistic String project (Sager et al., 1994) used a full syntactic and BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 10–18, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics clinical sublanguage parse to fill template data structures corresponding to medical statements. These were mapped to a database model incorporating medical facts and the relationships between them. MedLEE (Friedman et al., 1994), and more recently BioMedLEE (Lussier et al., 2006) used a semantic lexicon and grammar of domain-specific semantic patterns. The patterns encode the possible relationships betwe</context>
</contexts>
<marker>Sager, Lyman, Bucknall, Nhan, Tick, 1994</marker>
<rawString>N. Sager, M. Lyman, C. Bucknall, N. Nhan, and L. Tick. 1994. Natural language processing and the representation of clinical data. Journal of the American Medical Informatics Association, 1(2):142–160, March-April. T. Wang, Y. Li, K. Bontcheva, H. Cunningham, and J. Wang. 2006. Automatic extraction of hierarchical relations from text. In The Semantic Web: Research and Applications. 3rd European Semantic Web Conference, ESWC 2006, number 4011 in Lecture Notes in Computer Science, pages 215–229. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Su</author>
<author>J Zhang</author>
<author>M Zhang</author>
</authors>
<title>Exploring Various Knowledge in Relation Extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>427--434</pages>
<location>Ann Arbor, MI, USA,</location>
<contexts>
<context position="5246" citStr="Zhou et al (2005)" startWordPosition="760" endWordPosition="763">m et al., 1995) also used a full parse, a conceptual representation of the text, and a large scale knowledge base. In other applications of biomedical NLP, a second paradigm has become widespread: the application of statistical machine learning techniques to feature-based models of the text. Such approaches have typically been applied to journal texts. They have been used both for entity recognition and extraction of various relations, such as protein-protein interactions (see, for example, Grover et al (2007)). This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)). Statistical machine learning has also been applied to clinical text, but its use has generally been limited to entity recognition. The Mayo Clinic text analysis system (Pakhomov et al., 2005), for example, uses a combination of dictionary lookup and a Naive Bayes classifier to identify entities for information retrieval applications. To the best of our knowledge, statistical methods have not been previously applied to extraction of clinical relationships from text. This paper describes experiments in the statistical machine learning of relationships from a novel text type: oncology narrativ</context>
<context position="17918" citStr="Zhou et al (2005)" startWordPosition="2838" endWordPosition="2841">nce). To disambiguate these cases, the output of each one-against-all classifier is transformed into a probability, and the class with the highest probability is assigned. Re-casting the multi-class relation problem as a number of binary problems, and post-processing to resolve ambiguities, is handled by the GATE Learning API. Figure 1: The relationship extraction system. 4.2 Features for Classification The SVM classification model is built from lexical and syntactic features assigned to tokens and entity pairs prior to classification. We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006). These features are split into 11 sets, as described in Table 3. The tokN features are POS and surface string taken from a window of N tokens on each side of each paired entity’s mention. For N = 6, this gives 48 features. The rationale behind these simple features is that there is useful information in the words surrounding two mentions, that helps determine any relationship between them. The gentokN features generalise tokN to use morphological root and generalised POS. The str features are a set of 14 surface string features, encoding the full surface strings of both </context>
<context position="21084" citStr="Zhou et al (2005)" startWordPosition="3360" endWordPosition="3363"> not directly comparable here, as relationship annotation is 14 Feature set Size Description tokN 8N Surface string and POS of tokens surrounding the arguments, windowed −N to +N, N = 6 by default gentokN 8N Root and gerenalised POS of tokens surrounding the argument entities, windowed −N to +N, N = 6 by default atype 1 Concatenated semantic type of arguments, in arg1-arg2 order dir 1 Direction: linear text order of the arguments (is arg1 before arg2, or vice versa?) dist 2 Distance: absolute number of sentence and paragraph boundaries between arguments str 14 Surface string features based on Zhou et al (2005), see text for full description pos 14 POS features, as above root 14 Root features, as above genpos 14 Generalised POS features, as above inter 11 Intervening mentions: numbers and types of intervening entity mentions between arguments event 5 Events: are any of the arguments, or intevening entities, events? allgen 96 All features in root and generalised POS forms, i.e. gentok6+atype+dir+dist+root+genpos+inter+event notok 48 All except tokN features, others in string and POS forms, i.e. atype+dir+dist+str+pos+inter+event Table 3: Feature sets used for learning relationships. The size of a set</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>G. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. Exploring Various Knowledge in Relation Extraction. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 427–434, Ann Arbor, MI, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Zweigenbaum</author>
<author>B Bachimont</author>
<author>J Bouaud</author>
<author>J Charlet</author>
<author>J-F Boisvieux</author>
</authors>
<title>A multi-lingual architecture for building a normalised conceptual representation from medical language.</title>
<date>1995</date>
<booktitle>In Proceedings of the Annual Symposium on Computer Applications in Medical Care,</booktitle>
<pages>357--361</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="4644" citStr="Zweigenbaum et al., 1995" startWordPosition="662" endWordPosition="665">1994), and more recently BioMedLEE (Lussier et al., 2006) used a semantic lexicon and grammar of domain-specific semantic patterns. The patterns encode the possible relationships between entities, allowing both entities and the relationships between them to be directly matched in the text. Other systems have incorporated largescale domain-specific knowledge bases. MEDSYNDIKATE (Hahn et al., 2002) employed a rich discourse model of entities and their relationships, built using a dependency parse of texts and a description logic knowledge base re-engineered from existing terminologies. MENELAS (Zweigenbaum et al., 1995) also used a full parse, a conceptual representation of the text, and a large scale knowledge base. In other applications of biomedical NLP, a second paradigm has become widespread: the application of statistical machine learning techniques to feature-based models of the text. Such approaches have typically been applied to journal texts. They have been used both for entity recognition and extraction of various relations, such as protein-protein interactions (see, for example, Grover et al (2007)). This follows on from the success of these methods in general NLP (see for example Zhou et al (200</context>
</contexts>
<marker>Zweigenbaum, Bachimont, Bouaud, Charlet, Boisvieux, 1995</marker>
<rawString>P. Zweigenbaum, B. Bachimont, J. Bouaud, J. Charlet, and J-F. Boisvieux. 1995. A multi-lingual architecture for building a normalised conceptual representation from medical language. In Proceedings of the Annual Symposium on Computer Applications in Medical Care, pages 357–361, New York, NY, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>