<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013548">
<title confidence="0.984725">
BagPack: A general framework to represent semantic relations
</title>
<author confidence="0.998632">
Amaç Herda˘gdelen Marco Baroni
</author>
<affiliation confidence="0.959064">
CIMEC, University of Trento CIMEC, University of Trento
Rovereto, Italy Rovereto, Italy
</affiliation>
<email confidence="0.986112">
amac@herdagdelen.com marco.baroni@unitn.it
</email>
<sectionHeader confidence="0.993535" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998266">
We introduce a way to represent word pairs
instantiating arbitrary semantic relations that
keeps track of the contexts in which the words
in the pair occur both together and indepen-
dently. The resulting features are of sufficient
generality to allow us, with the help of a stan-
dard supervised machine learning algorithm,
to tackle a variety of unrelated semantic tasks
with good results and almost no task-specific
tailoring.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977692307693">
Co-occurrence statistics extracted from corpora lead
to good performance on a wide range of tasks that
involve the identification of the semantic relation be-
tween two words or concepts (Sahlgren, 2006; Turney,
2006). However, the difficulty of such tasks and the
fact that they are apparently unrelated has led to the
development of largely ad-hoc solutions, tuned to spe-
cific challenges. For many practical applications, this is
a drawback: Given the large number of semantic rela-
tions that might be relevant to one or the other task, we
need a multi-purpose approach that, given an appropri-
ate representation and training examples instantiating
an arbitrary target relation, can automatically mine new
pairs characterized by the same relation. Building on a
recent proposal in this direction by Turney (2008), we
propose a generic method of this sort, and we test it
on a set of unrelated tasks, reporting good performance
across the board with very little task-specific tweaking.
There has been much previous work on corpus-based
models to extract broad classes of related words. The
literature on word space models (Sahlgren, 2006) has
focused on taxonomic similarity (synonyms, antonyms,
co-hyponyms... ) and general association (e.g., find-
ing topically related words), exploiting the idea that
taxonomically or associated words will tend to occur
in similar contexts, and thus share a vector of co-
occurring words. The literature on relational similar-
ity, on the other hand, has focused on pairs of words,
devising various methods to compare how similar the
contexts in which target pairs appear are to the contexts
of other pairs that instantiate a relation of interest (Tur-
ney, 2006; Pantel and Pennacchiotti, 2006). Beyond
these domains, purely corpus-based methods play an
increasingly important role in modeling constraints on
composition of words, in particular verbal selectional
preferences – finding out that, say, children are more
likely to eat than apples, whereas the latter are more
likely to be eaten (Erk, 2007; Padó et al., 2007). Tasks
of this sort differ from relation extraction in that we
need to capture productive patterns: we want to find
out that shabu shabu (a Japanese meat dish) is eaten
whereas ink is not, even if in our corpus neither noun is
attested in proximity to forms of the verb to eat.
Turney (2008) is the first, to the best of our knowl-
edge, to raise the issue of a unified approach. In par-
ticular, he treats synonymy and association as special
cases of relational similarity: in the same way in which
we might be able to tell that hands and arms are in
a part-of relation by comparing the contexts in which
they co-occur to the contexts of known part-of pairs,
we can guess that cars and automobiles are synonyms
by comparing the contexts in which they co-occur to
the contexts linking known synonym pairs.
Here, we build on Turney’s work, adding two main
methodological innovations that allow us further gen-
eralization. First, merging classic approaches to taxo-
nomic and relational similarity, we represent concept
pairs by a vector that concatenates information about
the contexts in which the two words occur indepen-
dently, and the contexts in which they co-occur (Mirkin
et al. 2006 also integrate information from the lexi-
cal patterns in which two words co-occur and simi-
larity of the contexts in which each word occurs on
its own, to improve performance in lexical entailment
acquisition). Second, we represent contexts as bag of
words and bigrams, rather than strings of words (“pat-
terns”) of arbitrary length: we leave it to the machine
learning algorithm to zero in on the most interesting
words/bigrams.
Thanks to the concatenated vector, we can tackle
tasks in which the two words are not expected to
co-occur even in very large corpora (such as selec-
tional preference). Concatenation, together with un-
igram/bigram representation of context, allows us to
scale down the approach to smaller training corpora
(Turney used a corpus of more than 50 billion words),
since we do not need to see the words directly co-
occurring, and the unigram/bigram dimensions of the
</bodyText>
<note confidence="0.949531">
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 33–40,
Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.99878">
33
</page>
<bodyText confidence="0.999900297297297">
vectors are less sparse than dimensions based on longer
strings of words. We show that our method produces
reasonable results also on a corpus of 2 billion words,
with many unseen pairs. Moreover, our bigram and
unigram representation is general enough that we do
not need to extract separate statistics nor perform ad-
hoc feature selection for each task: we build the co-
occurrence matrix once, and use the same matrix in all
experiments. The bag-of-words assumption also makes
for faster and more compact model building, since the
number of features we extract from a context is linear
in the number of words in the context, whereas it is ex-
ponential for Turney. On the other hand, our method
is currently lagging behind Turney’s in terms of perfor-
mance, suggesting that at least some task-specific tun-
ing will be necessary.
Following Turney, we focus on devising a suitably
general featural representation, and we see the spe-
cific machine learning algorithm employed to perform
the various tasks as a parameter. Here, we use Sup-
port Vector Machines since they are a particularly ef-
fective general-purpose method. In terms of empirical
evaluation of the model, besides experimenting with
the “classic” SAT and TOEFL datasets, we show how
our algorithm can tackle the selectional preference task
proposed in Padó (2007) – a regression task – and we
introduce to the corpus-based semantics community a
challenge from the ConceptNet repository of common-
sense knowledge (extending such repository by auto-
mated means is the original motivation of our project).
In the next section, we will present our proposed
method along with the corpora and model parameter
choices used in the implementation. In Section 3, we
describe the tasks that we use to evaluate the model.
Results are reported in Section 4 and we conclude in
Section 5, with a brief overview of the contributions of
this paper.
</bodyText>
<sectionHeader confidence="0.993489" genericHeader="introduction">
2 Methodology
</sectionHeader>
<subsectionHeader confidence="0.975711">
2.1 Model
</subsectionHeader>
<bodyText confidence="0.999991275862069">
The central idea in BagPack (Bag-of-words represen-
tation of Paired concept knowledge) is to construct a
vector-based representation of a pair of words in such a
way that the vector represents both the contexts where
the two words co-occur and the contexts where the sin-
gle words occur on their own. A straightforward ap-
proach is to construct three different sub-vectors, one
for the first word, one for the second word, and one for
the co-occurring pair. The concatenation of these three
sub-vectors is the final vector that represents the pair.
This approach provides us a graceful fall back mech-
anism in case of data scarcity. Even if the two words are
not observed co-occurring in the corpus – no syntag-
maic information about the pair –, the corresponding
vector will still represent the individual contexts where
the words are observed on their own. Our hypothesis
(and hope) is that this information will be representa-
tive of the semantic relation between the pair, in the
sense that, given pairs characterized by same relation,
there should be paradigmatic similarity across the first,
resp. second elements of the pairs (e.g., if the relation
is between professionals and the typical tool of their
trade, it is reasonable to expect that that both profes-
sionals and tools will tend to share similar contexts).
Before going into further details, we need to describe
what a “co-occurrence” precisely means, define the no-
tion of context, and determine how to structure our vec-
tor. For a single word W, the following pseudo regular
expression identifies an observation of occurrence:
</bodyText>
<equation confidence="0.982051">
“C W D&amp;quot; (1)
</equation>
<bodyText confidence="0.9999056">
where C and D can be empty strings or concatena-
tions of up to 4 words separated by whitespace (i.e.
C1, ... , Ci and D1, ... , Dj where i, j &lt; 4). Each ob-
servation of this pattern constitutes a single context of
W. The pattern is matched with the longest possible
substring without crossing sentence boundaries.
Let (W1, W2) denote an ordered pair of words W1
and W2. We say the two words occur as a pair when-
ever one of the following pseudo regular expressions is
observed in the corpus:
</bodyText>
<equation confidence="0.999083">
“C W1 D W2 E&amp;quot; (2)
“C W2 D W1 E&amp;quot; (3)
</equation>
<bodyText confidence="0.999700580645161">
where C and E can be empty strings or concatena-
tions of up to 2 words and similarly, D can be ei-
ther an empty string or concatenation of up to 5 words
(i.e. C1, ... , Ci, D1, ... , Dj, and E1, ..., Ek where
i, j &lt; 2 and k &lt; 5). Together, patterns 2 and 3 con-
stitute the pair context for W1 and W2. The pattern is
matched with the longest possible substring while mak-
ing sure that D does not contain neither W1 nor W2.
The number of context words allowed before, after,
and between the targets are actually model parameters
but for the experiments reported in this study, we used
the aforementioned values with no attempt at tuning.
The vector representing (W1, W2) is a concatenation
v1v2v1,2, where, the sub-vectors v1 and v2 are con-
structed by using the single contexts of W1 and W2
correspondingly (i.e. by pattern 1) and the sub-vector
v1,2 is built by using the pair contexts identified by
the patterns 2 and 3. We refer to the components as
single-occurrence vectors and pair-occurrence vector
respectively.
The population of BagPack starts by identifying the
b most frequent unigrams and the b most frequent bi-
grams as basis terms. Let T denote a basis term. For
the construction of v1, we create two features for each
term T: tp,, corresponds to the number of observations
of T in the single contexts of W1 occurring before W1
and tp,,t corresponds to the number of observations of
T in the single occurrence of W1 where T occurs after
W1 (i.e. number of observations of the pattern 1 where
T E C and T E D correspondingly). The construc-
tion of v2 is identical except that this time the features
</bodyText>
<page confidence="0.995013">
34
</page>
<bodyText confidence="0.999945942307692">
correspond to the number of times the basis term is ob-
served before and after the target word W2 in single
contexts. The construction of the pair-occurrence sub-
vector v1,2 proceeds in a similar fashion but in addi-
tion, we incorporate also the order of W1 and W2 as
they co-occur in the pair context: The number of ob-
servations of the pair contexts where W1 occurs before
W2 and T precedes (follows) the pair, are represented
by feature t+pre (t+post). The number of cases where
the basis term is in between the target words is repre-
sented by t+betw. The number of cases where W2 oc-
curs before W1 and T precedes the pair is represented
by the feature t−pre. Similarly the number of cases
where T follows (is in between) the pair is represented
by the feature t−post (t−betw).
Assume that the words &amp;quot;only&amp;quot; and &amp;quot;that&amp;quot; are our ba-
sis terms and consider the following context for the
word pair (&amp;quot;cat&amp;quot;, &amp;quot;lion&amp;quot;): &amp;quot;Lion is the only cat that
lives in large social groups.&amp;quot; The observation of the ba-
sis terms should contribute to the pair-occurrence sub-
vector v1,2 and since the target words occur in reverse
order, this context results in the incrementation of the
features only−betw and that−post by one.
To sum up, we have 2b basis terms (b unigrams and
b bigrams). Each of the single-occurrence sub-vectors
v1 and v2 consists of 4b features: Each basis term
gives rise to 2 features incorporating the relative posi-
tion of basis term with respect to the single word. The
pair-occurrence sub-vector, v1,2, consists of 12b fea-
tures: Each basis term gives rise to 6 new features; x3
for possible relative positions of the basis term with re-
spect to the pair and x2 for the order of the words.
Importantly, the 2b basis terms are picked only once,
and the overall co-occurrence matrix is built once and
for all for all the tasks: unlike Turney, we do not need
to go back to the corpus to pick basis terms and collect
separate statistics for different tasks.
The specifics of the adaptation to each task will be
detailed in Section 3. For the moment, it should suffice
to note that the vectors v1 and v2 represent the con-
texts in which the two words occur on their own, thus
encode paradigmatic information. However, v1,2 rep-
resents the contexts in which the two words co-occur,
thus encode sytagmatic information.
The model training and evaluation is done in a 10-
fold cross-validation setting whenever applicable. The
reported performance measures are the averages over
all folds and the confidence intervals are calculated by
using the distribution of fold-specific results. The only
exception to this setting is the SAT analogy questions
task simply because we consider each question as a
separate mini dataset as described in Section 3.
</bodyText>
<subsectionHeader confidence="0.999135">
2.2 Source Corpora
</subsectionHeader>
<bodyText confidence="0.999982666666667">
We carried out our tests on two different corpora:
ukWaC, a Web-derived, POS-tagged and lemmatized
collection of about 2 billion tokens,1 and the Yahoo!
</bodyText>
<footnote confidence="0.817504">
1http://wacky.sslmit.unibo.it
</footnote>
<bodyText confidence="0.999953529411765">
database queried via the BOSS service.2 We will refer
to these corpora as ukWaC and Yahoo from now on.
In ukWaC, we limited the number of occurrence and
co-occurrence queries to the first 5000 observations
for computational efficiency. Since we collect cor-
pus statistics at the lemma level, we construct Yahoo!
queries using disjunctions of inflected forms that were
automatically generated with the NodeBox Linguistics
library.3 For example, the query to look for “lion” and
“cat” with 4 words in the middle is: “(lion OR lions) *
* * * (cat OR cats OR catting OR catted)”. Each pair
requires 14 Yahoo! queries (one for W1, one for W2,
6 for (W1, W2), in that order, with 0-to-5 intervening
words, 6 analogous queries for (W2, W1)). Yahoo! re-
turns maximally 1,000 snippets per query, and the latter
are lemmatized with the TreeTagger4 before feature ex-
traction.
</bodyText>
<subsectionHeader confidence="0.998011">
2.3 Model implementation
</subsectionHeader>
<bodyText confidence="0.99869325">
We did not carry out a search for “good” parameter val-
ues. Instead, the model parameters are generally picked
at convenience to ease memory requirements and com-
putational efficiency. For instance, in all experiments,
b is set to 1500 unless noted otherwise in order to fit
the vectors of all pairs at our hand into the computer
memory.
Once we construct the vectors for a set of word pairs,
we get a co-occurrence matrix with pairs on the rows
and the features on the columns. In all of our exper-
iments, the same normalization method and classifi-
cation algorithm is used with the default parameters:
First, a TF-IDF feature weighting is applied to the co-
occurrence matrix (Salton and Buckley, 1988). Then
following the suggestion of Hsu and Chang (2003),
each feature t’s [µt −2&amp;t, µt +2Qt] interval is scaled to
[0, 1], trimming the exceeding values from upper and
lower bounds (the symbols µt and Qt denote the av-
erage and standard deviation of the feature values re-
spectively). For the classification algorithm, we use the
C-SVM classifier and for regression the e-SVM regres-
sor, both implemented in the Matlab toolbox of Canu
et al. (2005). We employed a linear kernel. The cost
parameter C is set to 1 for all experiments; for the re-
gressor, e = 0.2. For other pattern recognition related
coding (e.g., cross validation, scaling, etc.) we made
use of the Matlab PRTools (Duin, 2001).
For each task that will be defined in the next section,
we evaluated our algorithm on the following represen-
tations: 1) Single-occurrence vectors (v1v2 condition)
2) Pair-occurrence vectors (v1,2 condition) 3) Entire
co-occurrence matrix (v1v2v1,2 condition).
</bodyText>
<footnote confidence="0.998494">
2http://developer.yahoo.com/search/
boss/
3http://nodebox.net/code/index.php/
Linguistics
4http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
</footnote>
<page confidence="0.998692">
35
</page>
<sectionHeader confidence="0.998751" genericHeader="method">
3 Tasks
</sectionHeader>
<subsectionHeader confidence="0.999115">
3.1 SAT Analogy Questions
</subsectionHeader>
<bodyText confidence="0.999991791666667">
The first task we evaluated our algorithm on is the
SAT analogy questions task introduced by Turney et al.
(2003). In this task, there are 374 multiple choice ques-
tions with a pair of related words like (lion,cat) as the
stem and 5 other pairs as the choices. The correct an-
swer is the choice pair which has the relationship most
similar to that in the stem pair.
We adopt a similar approach to the one used in Tur-
ney (2008) and consider each question as a separate bi-
nary classification problem with one positive training
instance and 5 unknown pairs. For a question, we pick
a pair at random from the stems of other questions as a
pseudo negative instance and train our classifier on this
two-instance training set. Then the trained classifier is
evaluated on the choice pairs and the pair with the high-
est posterior probability for the positive class is called
the winner. The procedure is repeated 10 times pick-
ing a different pseudo-negative instance each time and
the choice pair which is selected as the winner most of-
ten is taken as the answer to that question. The perfor-
mance measure on this task is defined as the percent-
age of correctly answered questions. The mean score
and confidence intervals are calculated over the perfor-
mance scores obtained for all folds.
</bodyText>
<subsectionHeader confidence="0.992695">
3.2 TOEFL Synonym Questions
</subsectionHeader>
<bodyText confidence="0.999944678571429">
This task, introduced by Landauer and Dumais (1997),
consists of 80 multiple choice questions in which a
word is given as the stem and the correct choice is the
word which has the closest meaning to that of the stem,
among 4 candidates. To fit the task into our frame-
work, we pair each choice with the stem word and ob-
tain 4 word pairs for each question. The word pair
constructed with the stem and the correct choice is la-
beled as positive and the other pairs are labeled as neg-
ative. We consider all 320 pairs constructed for all 80
questions as our dataset. Thus, the problem is turned
into a binary classification problem where the task is
to discriminate the synonymous word pairs (i.e. pos-
itive class) from the other pairs (i.e. negative class).
We made sure that the pairs constructed for the same
question were never split between training and test set,
so that no question-specific learning is performed. The
reason for this precaution is that the evaluation is done
on a per-question basis. The estimated posterior class
probabilities of the pairs constructed for the same ques-
tion are compared to each other and the pair with the
highest probability for the positive class is selected as
the answer for the question. By keeping the pairs of
a question in the same set we make sure their posteri-
ors are calculated by the same trained classifier. The
performance measure is the percentage of correctly an-
swered questions and we report the mean performance
over all 10 folds.
</bodyText>
<subsectionHeader confidence="0.998334">
3.3 Selectional Preference Judgments
</subsectionHeader>
<bodyText confidence="0.999996134615385">
Linguists have long been interested in the semantic
constraints that verbs impose on their arguments, a
broad area that has also attracted computational mod-
eling, with increasing interest in purely corpus-based
methods (Erk, 2007; Padó et al., 2007). This task is
of particular interest to us as an example of a broader
class of linguistic problems that involve productive
constraints on composition. As has been stressed at
least since Chomsky’s early work (Chomsky, 1957), no
matter how large a corpus is, if a phenomenon is pro-
ductive there will always be new well-formed instances
that are not in the corpus. In the domain of selectional
restrictions this is particularly obvious: we would not
say that an algorithm learned the constraints on the pos-
sible objects/patients of eating simply by producing the
list of all the attested objects of this verb in a very large
corpus; the interesting issue is whether the algorithm
can detect if an unseen object is or is not a plausible
“eatee”, like humans do without problems. Specifi-
cally, we test selectional preferences on the dataset con-
structed by Padó (2007), that collects average plausi-
bility judgments (from 20 speakers) for nouns as either
subjects or objects of verbs (211 noun-verb pairs).
We formulate this task as a regression problem. We
train the e-SVM regressor with 18-fold cross valida-
tion: Since the pair instances are not independent but
grouped according to the verbs, one fold is constructed
for each of the 18 verbs used in the dataset. In each
fold, all instances sharing the corresponding verb are
left out as the test set. The performance measure for
this task is the Spearman correlation between the hu-
man judgments and our algorithm’s estimates. There
are two possible ways to calculate this measure. One is
to get the overall correlation between the human judg-
ments and our estimates obtained by concatenating the
output of each cross-validation fold. That measure al-
lows us to compare our method with the previously re-
ported results. However, it cannot control for a possi-
ble verb-effect on the human judgment values: If the
average judgment values of the pairs associated with a
specific verb is significantly higher (or lower) than the
average of the pairs associated with another verb, then
any regressor which simply learns to assign the aver-
age value to all pairs associated with that verb (regard-
less of whether there is a patient or agent relation be-
tween the pairs) will still get a reasonably high correla-
tion because of the variation of judgment scores across
the verbs. To control for this effect, we also calculated
the correlation between the human judgments and our
estimates for each verb’s plausibility values separately,
and we report averages across these separate correla-
tions (the “mean” results reported below).
</bodyText>
<subsectionHeader confidence="0.987751">
3.4 Common-sense Relations from ConceptNet
</subsectionHeader>
<bodyText confidence="0.9859305">
Open Mind Common Sense5 is an ongoing project of
acquisition of common-sense knowledge from ordinary
</bodyText>
<footnote confidence="0.97579">
5http://commons.media.mit.edu/en/
</footnote>
<page confidence="0.996014">
36
</page>
<table confidence="0.99982825">
Relation Pairs Relation Pairs
IsA 316 PartOf 139
UsedFor 198 LocationOf 1379
CapableOf 228 Total 1943
</table>
<tableCaption confidence="0.999862">
Table 1: ConceptNet relations after filtering.
</tableCaption>
<bodyText confidence="0.994114090909091">
people by letting them carry out simple semantic and
linguistics tasks. An end result of the project is Con-
ceptNet 3, a large scale semantic network consisting of
relations between concept pairs (Havasi et al., 2007). It
is possible to view this network as a collection of se-
mantic assertions, each of which can be represented by
a triple involving two concepts and a relation between
them, e.g. UsedFor(piccolo, make music). One moti-
vation for this project is the fact that common-sense
knowledge is assumed to be known by both parties in
a communication setting and usually is not expressed
explicitly. Thus, corpus-based approaches may have
serious difficulties in capturing these relations (Havasi
et al., 2007), but there are reasons to believe that they
could still be useful: Eslick (2006) uses the assertions
of ConceptNet as seeds to parse Web search results and
augment ConceptNet by new candidate relations.
We use the ConceptNet snapshot released in June
2008, containing more than 200.000 assertions with
around 20 semantic relations like UsedFor, Desirious-
EffectOf, or SubEventOf. Each assertion has a confi-
dence rating based on the number of people who ex-
pressed or confirmed that assertion. For simplicity we
limited ourselves to single word concepts and the re-
lations between them. Furthermore, we eliminated the
assertions with a confidence score lower than 3 in an
attempt to increase the &amp;quot;quality&amp;quot; of the assertions and
focused on the most populated 5 relations of the re-
maining set, as given in Table 3.4. There may be more
than one relation between a pair of concepts, so the to-
tal number is less than the sum of the size of the indi-
vidual relation sets.
Percentage of correct answers
</bodyText>
<sectionHeader confidence="0.999909" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999956307692308">
For the multiple choice question tasks (i.e. SAT and
TOEFL), we say a question is complete when all of the
related pairs (stem and choice) are represented by vec-
tors with at least one non-zero component. If a ques-
tion has at least one pair represented by a zero-vector
(missing pairs), then we say that the question is partial.
For these tasks, we report the worst-case performance
scores where we assume that a random guessing per-
formance is obtained on the partial questions. This is
a strict lower bound because it discards all information
we have about a partial question even if it has only one
missing pair. We define coverage as the percentage of
complete questions.
</bodyText>
<subsectionHeader confidence="0.956401">
4.1 SAT
</subsectionHeader>
<bodyText confidence="0.998781411764706">
In Yahoo, the coverage is quite high. In the v1,2 only
condition, 4 questions had at least some choice/stem
pairs with all zero components. In all other cases, all of
the pairs were represented by vectors with at least one
non-zero component. The highest score is obtained for
the v1v2v1,2 condition with a 44.1% of correct ques-
tions, that is not significantly above the 42.5% perfor-
mance of v1,2 (paired t-test, α = 0.05). The v1v2 only
condition results in a poorer performance of 33.9% cor-
rect questions, statistically lower than the former two
conditions.
For ukWaC, the v1,2 only condition provides a rel-
atively low coverage. Only 238 questions out of 374
were complete. For the other conditions, we get a com-
plete coverage. The performances are statistically in-
distinguishable from each other and are 38.0%, 38.2%,
and 39.6% for v1,2, v1v2, and v1v2v1,2 respectively.
</bodyText>
<table confidence="0.9990015">
Condition Yahoo ukWaC
v1,2 42.5% 38.0%
v1v2 33.9% 38.2%
v1v2v1,2 44.1% 39.6%
</table>
<tableCaption confidence="0.96158">
Table 2: Percentage of correctly answered questions in
SAT analogy task, worst-case scenario.
</tableCaption>
<figureCaption confidence="0.823146375">
In Fig. 1, the best performances we get for Yahoo
and ukWaC are compared to previous studies with 95%
binomial confidence intervals plotted. The reported
values are taken from the ACL wiki page on the state of
the art for SAT analogy questions6. The algorithm pro-
posed by Turney (2008) is labeled as Turney-PairClass.
Figure 1: Comparison with previous algorithms on
SAT analogy questions.
</figureCaption>
<bodyText confidence="0.9996585">
Overall, the performance of BagPack is not at the
level of the state of the art but still provides a reasonable
level even in the v1v2 only condition for which we do
not utilize the contexts where the two words co-occur.
This aspect is most striking for ukWaC where the cov-
erage is low and by only utilizing the single-occurrence
sub-vectors we obtain a performance of 38.2% cor-
rect answers (the comparable “attributional” models re-
</bodyText>
<footnote confidence="0.995701">
6See http://aclweb.org/aclwiki/ for further
information and references
</footnote>
<page confidence="0.582368">
55
</page>
<figure confidence="0.852989666666667">
50
45
65
60
40
35
</figure>
<page confidence="0.996725">
37
</page>
<bodyText confidence="0.8654135">
ported in Turney, 2006, have an average performance of
31%).
</bodyText>
<subsectionHeader confidence="0.959326">
4.2 TOEFL
</subsectionHeader>
<bodyText confidence="0.999793882352941">
For the v1,2 sub-vector calculated for Yahoo, we have
two partial questions out of 80 and the system answers
80.0% of the questions correctly. The single occur-
rence case v1v2 instead provides a correct percentage
of 41.2% which is significantly above the random per-
formance of 25% but still very poor. The combined
case v1v2v1,2 provides a score of 75.0% with no sta-
tistically significant difference from the v1,2 case. The
reason of the low performance for v1v2 is an open
question.
For ukWaC, the coverage for the v1v2 case is pretty
low. Out of 320 pairs, 70 were represented by zero-
vectors, resulting in 34 partial questions out of 80.
The performance is at 33.8%. The v1v2 case on its
own does not lead to a performance better than random
guessing (27.5%) but the combined case v1v2v1,2
provides the highest ukWaC score of 42.5%.
</bodyText>
<table confidence="0.9731745">
Condition Yahoo ukWaC
v1,2 80.0% 33.8%
v1v2 41.2% 27.5%
v1v2v1,2 75.0% 42.5%
</table>
<tableCaption confidence="0.973442">
Table 3: Percentage of correctly answered questions in
TOEFL synonym task, worst-case scenario.
</tableCaption>
<bodyText confidence="0.993317409090909">
To our knowledge, the best performance with a
purely corpus-based approach is that of Rapp (2003)
who obtained a score of 92.5% with SVD. Fig. 2 re-
ports our results and a list of other corpus-based sys-
tems which achieve scores higher than 70%, along with
95% confidence interval values. The results are taken
from the ACL wiki page on the state of the art for
TOEFL synonym questions.
Figure 2: Comparison with previous algorithms on
TOEFL synonym questions with 95% confidence in-
tervals.
We note that our results obtained for Yahoo are com-
parable to the results of Turney but even the best re-
sults obtained for ukWaC and the Yahoo’s results for
v1v2 only condition are very poor. Whether this is
because of the inability of the sub-vectors to capture
synonymity or because the default parameter values of
SVM are not adequate is an open question. Notice that
our concatenated v1v2 vector does not exploit infor-
mation about the similarity of v1 to v2, that, presum-
ably, should be of great help in solving the synonym
task.
</bodyText>
<subsectionHeader confidence="0.999802">
4.3 Selectional Preference
</subsectionHeader>
<bodyText confidence="0.9623605">
The coverage for this dataset is quite high. All pairs
were represented by non-zero vectors for Yahoo while
only two pairs had zero-vectors for ukWaC. The two
pairs are discarded in our experiments. For Yahoo, the
best results are obtained for the v1,2 case. The single-
occurrence case, v1v2, provides an overall correlation
of 0.36 and mean correlation of 0.26. However low, in
case of rarely co-occurring word pairs this data could
be the only data we have in our hands and it is impor-
tant that it provides reasonable judgment estimates.
For the ukWaC corpus, the best results we get are
an overall correlation of 0.60 and a mean correlation of
0.52 for the combined case v1v2v1,2. The results for
v1,2 and v1v2v1,2 are statistically indistinguishable.
</bodyText>
<table confidence="0.999368">
Yahoo ukWaC
Condition Overall Mean Overall Mean
v1,2 0.60 0.45 0.58 0.48
v1v2 0.36 0.26 0.33 0.22
v1v2v1,2 0.55 0.42 0.60 0.52
</table>
<tableCaption confidence="0.987181">
Table 4: Spearman correlations between the targets and
estimations for selectional preference task.
</tableCaption>
<bodyText confidence="0.999053857142857">
In Fig. 3, we present a comparison of our results with
some previous studies reported in Padó et al. (2007).
The best result reported so far is a correlation of 0.52.
Our results for Yahoo and ukWaC are currently the
highest correlation values reported. Even the verb-
effect-controlled correlations achieve competitive per-
formance.
</bodyText>
<figure confidence="0.993807181818182">
Spearman correlation 0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
</figure>
<figureCaption confidence="0.9987335">
Figure 3: Comparison of algorithms on selectional
preference task.
</figureCaption>
<figure confidence="0.974033111111111">
90
Percentage of correct answers
80
70
60
50
100
40
30
</figure>
<page confidence="0.989583">
38
</page>
<subsectionHeader confidence="0.902348">
4.4 ConceptNet
</subsectionHeader>
<bodyText confidence="0.999962875">
Only for this task, (because of practical memory limita-
tions) we reduced the model parameter b to 500, which
means we used the 500 most frequent unigrams and
500 most frequent bigrams as our basis terms. For each
of the 5 relations at our hand, we trained a different
SVM classifier by labeling the pairs with the corre-
sponding relation as positive and the rest as negative.
To eliminate the issue of unbalanced number of nega-
tive and positive instances we randomly down-sampled
the positive or negative instances set (whichever is
larger). For the IsA, UsedFor, CapableOf, and PartOf
relations, the down-sampling procedure means keep-
ing some of the negative instances out of the training
and test sets while for the LocationOf relation it means
keeping a subset of the positive instances out. We per-
formed 5 iterations of the down-sampling procedure
and for each iteration we carried out a 10-fold cross-
validation to train and test our classifier. The results are
test set averages over all iterations and folds. The per-
formance measure we use is the area under the receiver
operating characteristic (AUC in short for area under
the curve). The AUC of a classifier is the area under the
curve defined by the corresponding true positive rate
and false positive rate values obtained for varying the
threshold of the classifier to accept an instance as posi-
tive. Intuitively, AUC is the probability that a randomly
picked positive instance’s estimated posterior probabil-
ity is higher than a randomly picked negative instance’s
estimated posterior probability (Fawcett, 2006).
The coverage is quite high for both corpora: Out of
1943 pairs,only 3 were represented by a zero-vector in
Yahoo while in ukWaC this number is 68. For sim-
plicity, we discarded missing pairs from our analysis.
We report only the results obtained for the entire co-
occurrence matrix. The results are virtually identi-
cal for the other conditions too: Both for Yahoo and
ukWaC, almost all of the AUC values obtained for all
relations and for all conditions are above 95%. Only
the PartOf relation has AUC values above 90% (which
is still a very good result).
</bodyText>
<table confidence="0.997446166666667">
Relation Yahoo ukWaC
IsA 99.0% 98.0%
UsedFor 98.2% 98.5%
CapableOf 98.9% 99.1%
PartOf 97.6% 95.0%
LocationOf 99.0% 98.8%
</table>
<tableCaption confidence="0.9412295">
Table 5: AUC scores for 5 relations of ConceptNet,
classifier trained for v1v2v1,2 condition.
</tableCaption>
<bodyText confidence="0.999578227272727">
The very high performance we observe for the Con-
ceptNet task is surprising when compared to the mod-
erate performance we observe for other tasks. Our ex-
tensive filtering of the assertions could have resulted
in a biased dataset which might have made the job of
the classifier easy while reducing its generalization ca-
pacity. To investigate this, we decided to use the pairs
coming from the SAT task as a validation set.
Again, we trained an SVM classifier on the Concept-
Net data for each of the 5 relations like we did previ-
ously, but this time without cross-validation (i.e. after
the down-sampling, we used the entire set as the train-
ing dataset in each iteration). Then we evaluated the
classifiers on the 2224 pairs of the SAT analogy task
(removing pairs that were in the training data) and av-
eraged the posterior probability reported by each SVM
over each down-sampling iteration. The 5 pairs which
are assigned the highest posterior probability for each
relation are reported in Table 6. We have not yet quan-
tified the performance of BagPack in this task but the
preliminary results in this table are, qualitatively, ex-
ceptionally good.
</bodyText>
<sectionHeader confidence="0.999718" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999808">
We presented a general way to build a vector-based
space to represent the semantic relations between word
pairs and showed how that representation can be used
to solve various tasks involving semantic similarity.
For SAT and TOEFL, we obtained reasonable perfor-
mances comparable to the state of the art. For the es-
timation of selective preference judgments about verb-
noun pairs, we achieved state of the art performance.
Perhaps more importantly, our representation format
allows us to provide meaningful estimates even when
the verb and noun are not observed co-occurring in the
corpus – which is an obvious advantage over the mod-
els which rely on sytagmatic contexts alone and cannot
provide estimates for word pairs that are not seen di-
rectly co-occurring. We also obtained very promising
results for the automated augmentation of ConceptNet.
The generality of the proposed method is also re-
flected in the fact that we built a single feature space
based on frequent basis terms and used the same fea-
tures for all pairs coming from different tasks. The
use of the same feature set for all pairs makes it pos-
sible to build a single database of word-pair vectors.
For example, we were able to re-use the vectors con-
structed for SAT pairs as a validation set in the Con-
ceptNet task. Furthermore, the results reported here are
obtained for the same machine learning model (SVM)
without any parameter tweaking, which renders them
very strict lower bounds.
Another contribution is that the proposed method
provides a way to represent the relations between
words even if they are not observed co-occurring in the
corpus. Employing a larger corpus can be an alternative
solution for some cases but this is not always possible
and some tasks, like estimating selectional preference
judgments, inherently call for a method that does not
exclusively depends on paired co-occurrence observa-
tions.
Finally, we introduced ConceptNet, a common-sense
semantic network, to the corpus-based semantics com-
munity, both as a new challenge and as a repository we
</bodyText>
<page confidence="0.998999">
39
</page>
<table confidence="0.9946845">
Rank IsA UsedFor PartOf CapableOf LocationOf
1 watch,timepiece pencil,draw vehicle,wheel motorist,drive spectator,arena
2 emerald,gem blueprint,build spider,leg volatile,vaporize water,riverbed
3 cherry,fruit detergent,clean keyboard,finger concrete,harden bovine,pasture
4 dinosaur,reptile guard,protect train,caboose parasite,contribute benediction,church
5 ostrich,bird buttress,support hub,wheel immature,develop byline,newspaper
</table>
<tableCaption confidence="0.9979">
Table 6: Top 5 SAT pairs classified as positive for ConceptNet relations, classifier trained for v1v2v1,2 condition.
</tableCaption>
<bodyText confidence="0.996323181818182">
can benefit from.
In future work, one of the most pressing issue we
want to explore is how to better exploit the informa-
tion in the single occurrence vectors: currently, we do
not make any use of the overlap between v1 and v2.
In this way, we are missing the classic intuition that
taxonomically similar words tend to occur in similar
contexts, and it is thus not surprising that v1v2 flunks
the TOEFL. We are currently looking at ways to aug-
ment our concatenated vector with “meta-information”
about vector overlap.
</bodyText>
<sectionHeader confidence="0.999416" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997436954545455">
S. Canu, Y. Grandvalet, V. Guigue and A. Rakotoma-
monjy. 2005. SVM and Kernel Methods Matlab
Toolbox, Perception Systèmes et Information, INSA
de Rouen, Rouen, France
N. Chomsky. 1957. Syntactic structures. Mouton, The
Hague.
R. P. W. Duin. 2001. PRTOOLD (Version 3.1.7),
A Matlab toolbox for pattern recognition. Pattern
Recognition Group. Delft University of Technology.
K. Erk. 2007. A simple, similarity-based model for
selectional preferences. Proceedings of ACL 2007.
K. Erk and S. Padó. 2008. A structured vector space
model for word meaning in context. Proceedings of
EMNLP 2008.
I. Eslick. 2006. Searching for commonsense. Master’s
thesis, Massachusetts Institute of Technology.
T. Fawcett. 2006. An introduction to roc analysis.
Pattern Recogn. Lett., 27(8):861–874.
C. Havasi, R. Speer and J. Alonso. 2007. Concept-
net 3: a flexible, multilingual semantic network for
common sense knowledge. In Recent Advances in
Natural Language Processing, Borovets, Bulgaria,
September.
C.-W. Hsu, C.-C Chang. 2003. A practical guide
to support vector classification. Technical report,
Department of Computer Science, National Taiwan
University.
T.K. Landauer and S.T. Dumais. 1997. A solution
to Plato’s problem: The Latent Semantic Analysis
theory of acquisition, induction and representation
of knowledge. Psychological Review, 104(2): 211–
240.
H. Liu and P. Singh. 2004. ConceptNet — A practi-
cal commonsense reasoning tool-kit. BT Technology
Journal, 22(4) 211–226.
S. Mirkin, I. Dagan and M. Geffet. 2006. Integrat-
ing pattern-based and distributional similarity meth-
ods for lexical entailment acquisition. Proceedings
of COLING/ACL 2006, 579–586.
S. Padó, S. Padó and K. Erk. 2007. Flexible, corpus-
based modelling of human plausibility judgements.
Proceedings EMNLP 2007, 400–409.
U. Padó. 2007. The Integration of Syntax and Semantic
Plausibility in a Wide-Coverage Model of Sentence
Processing. Ph.D. thesis, Saarland University.
P. Pantel and M. Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically har-
vesting semantic relations. Proceedings of COL-
ING/ACL 2006, 113–120.
R. Rapp. 2003. Word sense discovery based on sense
descriptor dissimilarity. Proceedings of MT Summit
IX: 315–322.
M. Sahlgren. 2006. The Word-space model. Ph.D. dis-
sertation, Stockholm University, Stockholm.
G. Salton and C. Buckley. 1988. Term-weighting
approaches in automatic text retrieval. Information
Processing and Management, 24(5): 513–523.
R. Speer, C. Havasi and H. Lieberman. 2008. Anal-
ogyspace: Reducing the dimensionality of common
sense knowledge. In Dieter Fox and Carla P. Gomes,
editors, AAAI, pages 548–553. AAAI Press.
P. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3): 379–416.
P. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms and associations. Proceedings
of COLING 2008, 905–912.
</reference>
<page confidence="0.998605">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.883649">
<title confidence="0.999814">BagPack: A general framework to represent semantic relations</title>
<author confidence="0.999977">Amaç Herda˘gdelen Marco Baroni</author>
<affiliation confidence="0.999952">CIMEC, University of Trento CIMEC, University of Trento</affiliation>
<address confidence="0.965467">Rovereto, Italy Rovereto, Italy</address>
<email confidence="0.994854">amac@herdagdelen.commarco.baroni@unitn.it</email>
<abstract confidence="0.992655727272727">We introduce a way to represent word pairs instantiating arbitrary semantic relations that keeps track of the contexts in which the words in the pair occur both together and independently. The resulting features are of sufficient generality to allow us, with the help of a standard supervised machine learning algorithm, to tackle a variety of unrelated semantic tasks with good results and almost no task-specific tailoring.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Canu</author>
<author>Y Grandvalet</author>
<author>V Guigue</author>
<author>A Rakotomamonjy</author>
</authors>
<date>2005</date>
<booktitle>SVM and Kernel Methods Matlab Toolbox, Perception Systèmes et Information, INSA de</booktitle>
<location>Rouen, Rouen, France</location>
<contexts>
<context position="15587" citStr="Canu et al. (2005)" startWordPosition="2629" endWordPosition="2632">n method and classification algorithm is used with the default parameters: First, a TF-IDF feature weighting is applied to the cooccurrence matrix (Salton and Buckley, 1988). Then following the suggestion of Hsu and Chang (2003), each feature t’s [µt −2&amp;t, µt +2Qt] interval is scaled to [0, 1], trimming the exceeding values from upper and lower bounds (the symbols µt and Qt denote the average and standard deviation of the feature values respectively). For the classification algorithm, we use the C-SVM classifier and for regression the e-SVM regressor, both implemented in the Matlab toolbox of Canu et al. (2005). We employed a linear kernel. The cost parameter C is set to 1 for all experiments; for the regressor, e = 0.2. For other pattern recognition related coding (e.g., cross validation, scaling, etc.) we made use of the Matlab PRTools (Duin, 2001). For each task that will be defined in the next section, we evaluated our algorithm on the following representations: 1) Single-occurrence vectors (v1v2 condition) 2) Pair-occurrence vectors (v1,2 condition) 3) Entire co-occurrence matrix (v1v2v1,2 condition). 2http://developer.yahoo.com/search/ boss/ 3http://nodebox.net/code/index.php/ Linguistics 4htt</context>
</contexts>
<marker>Canu, Grandvalet, Guigue, Rakotomamonjy, 2005</marker>
<rawString>S. Canu, Y. Grandvalet, V. Guigue and A. Rakotomamonjy. 2005. SVM and Kernel Methods Matlab Toolbox, Perception Systèmes et Information, INSA de Rouen, Rouen, France</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Syntactic structures.</title>
<date>1957</date>
<location>Mouton, The Hague.</location>
<contexts>
<context position="19581" citStr="Chomsky, 1957" startWordPosition="3292" endWordPosition="3293">is the percentage of correctly answered questions and we report the mean performance over all 10 folds. 3.3 Selectional Preference Judgments Linguists have long been interested in the semantic constraints that verbs impose on their arguments, a broad area that has also attracted computational modeling, with increasing interest in purely corpus-based methods (Erk, 2007; Padó et al., 2007). This task is of particular interest to us as an example of a broader class of linguistic problems that involve productive constraints on composition. As has been stressed at least since Chomsky’s early work (Chomsky, 1957), no matter how large a corpus is, if a phenomenon is productive there will always be new well-formed instances that are not in the corpus. In the domain of selectional restrictions this is particularly obvious: we would not say that an algorithm learned the constraints on the possible objects/patients of eating simply by producing the list of all the attested objects of this verb in a very large corpus; the interesting issue is whether the algorithm can detect if an unseen object is or is not a plausible “eatee”, like humans do without problems. Specifically, we test selectional preferences o</context>
</contexts>
<marker>Chomsky, 1957</marker>
<rawString>N. Chomsky. 1957. Syntactic structures. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R P W Duin</author>
</authors>
<title>PRTOOLD (Version 3.1.7), A Matlab toolbox for pattern recognition.</title>
<date>2001</date>
<institution>Pattern Recognition Group. Delft University of Technology.</institution>
<contexts>
<context position="15831" citStr="Duin, 2001" startWordPosition="2674" endWordPosition="2675">−2&amp;t, µt +2Qt] interval is scaled to [0, 1], trimming the exceeding values from upper and lower bounds (the symbols µt and Qt denote the average and standard deviation of the feature values respectively). For the classification algorithm, we use the C-SVM classifier and for regression the e-SVM regressor, both implemented in the Matlab toolbox of Canu et al. (2005). We employed a linear kernel. The cost parameter C is set to 1 for all experiments; for the regressor, e = 0.2. For other pattern recognition related coding (e.g., cross validation, scaling, etc.) we made use of the Matlab PRTools (Duin, 2001). For each task that will be defined in the next section, we evaluated our algorithm on the following representations: 1) Single-occurrence vectors (v1v2 condition) 2) Pair-occurrence vectors (v1,2 condition) 3) Entire co-occurrence matrix (v1v2v1,2 condition). 2http://developer.yahoo.com/search/ boss/ 3http://nodebox.net/code/index.php/ Linguistics 4http://www.ims.uni-stuttgart.de/ projekte/corplex/TreeTagger/ 35 3 Tasks 3.1 SAT Analogy Questions The first task we evaluated our algorithm on is the SAT analogy questions task introduced by Turney et al. (2003). In this task, there are 374 multi</context>
</contexts>
<marker>Duin, 2001</marker>
<rawString>R. P. W. Duin. 2001. PRTOOLD (Version 3.1.7), A Matlab toolbox for pattern recognition. Pattern Recognition Group. Delft University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>Proceedings of ACL</booktitle>
<contexts>
<context position="2704" citStr="Erk, 2007" startWordPosition="414" endWordPosition="415">e literature on relational similarity, on the other hand, has focused on pairs of words, devising various methods to compare how similar the contexts in which target pairs appear are to the contexts of other pairs that instantiate a relation of interest (Turney, 2006; Pantel and Pennacchiotti, 2006). Beyond these domains, purely corpus-based methods play an increasingly important role in modeling constraints on composition of words, in particular verbal selectional preferences – finding out that, say, children are more likely to eat than apples, whereas the latter are more likely to be eaten (Erk, 2007; Padó et al., 2007). Tasks of this sort differ from relation extraction in that we need to capture productive patterns: we want to find out that shabu shabu (a Japanese meat dish) is eaten whereas ink is not, even if in our corpus neither noun is attested in proximity to forms of the verb to eat. Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. In particular, he treats synonymy and association as special cases of relational similarity: in the same way in which we might be able to tell that hands and arms are in a part-of relation by comparing</context>
<context position="19337" citStr="Erk, 2007" startWordPosition="3252" endWordPosition="3253">highest probability for the positive class is selected as the answer for the question. By keeping the pairs of a question in the same set we make sure their posteriors are calculated by the same trained classifier. The performance measure is the percentage of correctly answered questions and we report the mean performance over all 10 folds. 3.3 Selectional Preference Judgments Linguists have long been interested in the semantic constraints that verbs impose on their arguments, a broad area that has also attracted computational modeling, with increasing interest in purely corpus-based methods (Erk, 2007; Padó et al., 2007). This task is of particular interest to us as an example of a broader class of linguistic problems that involve productive constraints on composition. As has been stressed at least since Chomsky’s early work (Chomsky, 1957), no matter how large a corpus is, if a phenomenon is productive there will always be new well-formed instances that are not in the corpus. In the domain of selectional restrictions this is particularly obvious: we would not say that an algorithm learned the constraints on the possible objects/patients of eating simply by producing the list of all the at</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>K. Erk. 2007. A simple, similarity-based model for selectional preferences. Proceedings of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
<author>S Padó</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>Proceedings of EMNLP</booktitle>
<marker>Erk, Padó, 2008</marker>
<rawString>K. Erk and S. Padó. 2008. A structured vector space model for word meaning in context. Proceedings of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Eslick</author>
</authors>
<title>Searching for commonsense. Master’s thesis,</title>
<date>2006</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="23047" citStr="Eslick (2006)" startWordPosition="3856" endWordPosition="3857">ncept pairs (Havasi et al., 2007). It is possible to view this network as a collection of semantic assertions, each of which can be represented by a triple involving two concepts and a relation between them, e.g. UsedFor(piccolo, make music). One motivation for this project is the fact that common-sense knowledge is assumed to be known by both parties in a communication setting and usually is not expressed explicitly. Thus, corpus-based approaches may have serious difficulties in capturing these relations (Havasi et al., 2007), but there are reasons to believe that they could still be useful: Eslick (2006) uses the assertions of ConceptNet as seeds to parse Web search results and augment ConceptNet by new candidate relations. We use the ConceptNet snapshot released in June 2008, containing more than 200.000 assertions with around 20 semantic relations like UsedFor, DesiriousEffectOf, or SubEventOf. Each assertion has a confidence rating based on the number of people who expressed or confirmed that assertion. For simplicity we limited ourselves to single word concepts and the relations between them. Furthermore, we eliminated the assertions with a confidence score lower than 3 in an attempt to i</context>
</contexts>
<marker>Eslick, 2006</marker>
<rawString>I. Eslick. 2006. Searching for commonsense. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fawcett</author>
</authors>
<title>An introduction to roc analysis.</title>
<date>2006</date>
<journal>Pattern Recogn. Lett.,</journal>
<volume>27</volume>
<issue>8</issue>
<contexts>
<context position="31836" citStr="Fawcett, 2006" startWordPosition="5330" endWordPosition="5331">ts are test set averages over all iterations and folds. The performance measure we use is the area under the receiver operating characteristic (AUC in short for area under the curve). The AUC of a classifier is the area under the curve defined by the corresponding true positive rate and false positive rate values obtained for varying the threshold of the classifier to accept an instance as positive. Intuitively, AUC is the probability that a randomly picked positive instance’s estimated posterior probability is higher than a randomly picked negative instance’s estimated posterior probability (Fawcett, 2006). The coverage is quite high for both corpora: Out of 1943 pairs,only 3 were represented by a zero-vector in Yahoo while in ukWaC this number is 68. For simplicity, we discarded missing pairs from our analysis. We report only the results obtained for the entire cooccurrence matrix. The results are virtually identical for the other conditions too: Both for Yahoo and ukWaC, almost all of the AUC values obtained for all relations and for all conditions are above 95%. Only the PartOf relation has AUC values above 90% (which is still a very good result). Relation Yahoo ukWaC IsA 99.0% 98.0% UsedFor</context>
</contexts>
<marker>Fawcett, 2006</marker>
<rawString>T. Fawcett. 2006. An introduction to roc analysis. Pattern Recogn. Lett., 27(8):861–874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Havasi</author>
<author>R Speer</author>
<author>J Alonso</author>
</authors>
<title>Conceptnet 3: a flexible, multilingual semantic network for common sense knowledge.</title>
<date>2007</date>
<booktitle>In Recent Advances in Natural Language Processing,</booktitle>
<location>Borovets, Bulgaria,</location>
<contexts>
<context position="22467" citStr="Havasi et al., 2007" startWordPosition="3760" endWordPosition="3763">s across these separate correlations (the “mean” results reported below). 3.4 Common-sense Relations from ConceptNet Open Mind Common Sense5 is an ongoing project of acquisition of common-sense knowledge from ordinary 5http://commons.media.mit.edu/en/ 36 Relation Pairs Relation Pairs IsA 316 PartOf 139 UsedFor 198 LocationOf 1379 CapableOf 228 Total 1943 Table 1: ConceptNet relations after filtering. people by letting them carry out simple semantic and linguistics tasks. An end result of the project is ConceptNet 3, a large scale semantic network consisting of relations between concept pairs (Havasi et al., 2007). It is possible to view this network as a collection of semantic assertions, each of which can be represented by a triple involving two concepts and a relation between them, e.g. UsedFor(piccolo, make music). One motivation for this project is the fact that common-sense knowledge is assumed to be known by both parties in a communication setting and usually is not expressed explicitly. Thus, corpus-based approaches may have serious difficulties in capturing these relations (Havasi et al., 2007), but there are reasons to believe that they could still be useful: Eslick (2006) uses the assertions</context>
</contexts>
<marker>Havasi, Speer, Alonso, 2007</marker>
<rawString>C. Havasi, R. Speer and J. Alonso. 2007. Conceptnet 3: a flexible, multilingual semantic network for common sense knowledge. In Recent Advances in Natural Language Processing, Borovets, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-W Hsu</author>
<author>C-C Chang</author>
</authors>
<title>A practical guide to support vector classification.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science, National Taiwan University.</institution>
<contexts>
<context position="15197" citStr="Hsu and Chang (2003)" startWordPosition="2562" endWordPosition="2565">memory requirements and computational efficiency. For instance, in all experiments, b is set to 1500 unless noted otherwise in order to fit the vectors of all pairs at our hand into the computer memory. Once we construct the vectors for a set of word pairs, we get a co-occurrence matrix with pairs on the rows and the features on the columns. In all of our experiments, the same normalization method and classification algorithm is used with the default parameters: First, a TF-IDF feature weighting is applied to the cooccurrence matrix (Salton and Buckley, 1988). Then following the suggestion of Hsu and Chang (2003), each feature t’s [µt −2&amp;t, µt +2Qt] interval is scaled to [0, 1], trimming the exceeding values from upper and lower bounds (the symbols µt and Qt denote the average and standard deviation of the feature values respectively). For the classification algorithm, we use the C-SVM classifier and for regression the e-SVM regressor, both implemented in the Matlab toolbox of Canu et al. (2005). We employed a linear kernel. The cost parameter C is set to 1 for all experiments; for the regressor, e = 0.2. For other pattern recognition related coding (e.g., cross validation, scaling, etc.) we made use </context>
</contexts>
<marker>Hsu, Chang, 2003</marker>
<rawString>C.-W. Hsu, C.-C Chang. 2003. A practical guide to support vector classification. Technical report, Department of Computer Science, National Taiwan University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<pages>211--240</pages>
<contexts>
<context position="17638" citStr="Landauer and Dumais (1997)" startWordPosition="2959" endWordPosition="2962">ained classifier is evaluated on the choice pairs and the pair with the highest posterior probability for the positive class is called the winner. The procedure is repeated 10 times picking a different pseudo-negative instance each time and the choice pair which is selected as the winner most often is taken as the answer to that question. The performance measure on this task is defined as the percentage of correctly answered questions. The mean score and confidence intervals are calculated over the performance scores obtained for all folds. 3.2 TOEFL Synonym Questions This task, introduced by Landauer and Dumais (1997), consists of 80 multiple choice questions in which a word is given as the stem and the correct choice is the word which has the closest meaning to that of the stem, among 4 candidates. To fit the task into our framework, we pair each choice with the stem word and obtain 4 word pairs for each question. The word pair constructed with the stem and the correct choice is labeled as positive and the other pairs are labeled as negative. We consider all 320 pairs constructed for all 80 questions as our dataset. Thus, the problem is turned into a binary classification problem where the task is to disc</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T.K. Landauer and S.T. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2): 211– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Liu</author>
<author>P Singh</author>
</authors>
<title>ConceptNet — A practical commonsense reasoning tool-kit.</title>
<date>2004</date>
<journal>BT Technology Journal,</journal>
<volume>22</volume>
<issue>4</issue>
<pages>211--226</pages>
<marker>Liu, Singh, 2004</marker>
<rawString>H. Liu and P. Singh. 2004. ConceptNet — A practical commonsense reasoning tool-kit. BT Technology Journal, 22(4) 211–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mirkin</author>
<author>I Dagan</author>
<author>M Geffet</author>
</authors>
<title>Integrating pattern-based and distributional similarity methods for lexical entailment acquisition.</title>
<date>2006</date>
<booktitle>Proceedings of COLING/ACL 2006,</booktitle>
<pages>579--586</pages>
<contexts>
<context position="3906" citStr="Mirkin et al. 2006" startWordPosition="621" endWordPosition="624">ion by comparing the contexts in which they co-occur to the contexts of known part-of pairs, we can guess that cars and automobiles are synonyms by comparing the contexts in which they co-occur to the contexts linking known synonym pairs. Here, we build on Turney’s work, adding two main methodological innovations that allow us further generalization. First, merging classic approaches to taxonomic and relational similarity, we represent concept pairs by a vector that concatenates information about the contexts in which the two words occur independently, and the contexts in which they co-occur (Mirkin et al. 2006 also integrate information from the lexical patterns in which two words co-occur and similarity of the contexts in which each word occurs on its own, to improve performance in lexical entailment acquisition). Second, we represent contexts as bag of words and bigrams, rather than strings of words (“patterns”) of arbitrary length: we leave it to the machine learning algorithm to zero in on the most interesting words/bigrams. Thanks to the concatenated vector, we can tackle tasks in which the two words are not expected to co-occur even in very large corpora (such as selectional preference). Conc</context>
</contexts>
<marker>Mirkin, Dagan, Geffet, 2006</marker>
<rawString>S. Mirkin, I. Dagan and M. Geffet. 2006. Integrating pattern-based and distributional similarity methods for lexical entailment acquisition. Proceedings of COLING/ACL 2006, 579–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Padó</author>
<author>S Padó</author>
<author>K Erk</author>
</authors>
<title>Flexible, corpusbased modelling of human plausibility judgements.</title>
<date>2007</date>
<booktitle>Proceedings EMNLP 2007,</booktitle>
<pages>400--409</pages>
<contexts>
<context position="2724" citStr="Padó et al., 2007" startWordPosition="416" endWordPosition="419">e on relational similarity, on the other hand, has focused on pairs of words, devising various methods to compare how similar the contexts in which target pairs appear are to the contexts of other pairs that instantiate a relation of interest (Turney, 2006; Pantel and Pennacchiotti, 2006). Beyond these domains, purely corpus-based methods play an increasingly important role in modeling constraints on composition of words, in particular verbal selectional preferences – finding out that, say, children are more likely to eat than apples, whereas the latter are more likely to be eaten (Erk, 2007; Padó et al., 2007). Tasks of this sort differ from relation extraction in that we need to capture productive patterns: we want to find out that shabu shabu (a Japanese meat dish) is eaten whereas ink is not, even if in our corpus neither noun is attested in proximity to forms of the verb to eat. Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. In particular, he treats synonymy and association as special cases of relational similarity: in the same way in which we might be able to tell that hands and arms are in a part-of relation by comparing the contexts in whi</context>
<context position="19357" citStr="Padó et al., 2007" startWordPosition="3254" endWordPosition="3257">bability for the positive class is selected as the answer for the question. By keeping the pairs of a question in the same set we make sure their posteriors are calculated by the same trained classifier. The performance measure is the percentage of correctly answered questions and we report the mean performance over all 10 folds. 3.3 Selectional Preference Judgments Linguists have long been interested in the semantic constraints that verbs impose on their arguments, a broad area that has also attracted computational modeling, with increasing interest in purely corpus-based methods (Erk, 2007; Padó et al., 2007). This task is of particular interest to us as an example of a broader class of linguistic problems that involve productive constraints on composition. As has been stressed at least since Chomsky’s early work (Chomsky, 1957), no matter how large a corpus is, if a phenomenon is productive there will always be new well-formed instances that are not in the corpus. In the domain of selectional restrictions this is particularly obvious: we would not say that an algorithm learned the constraints on the possible objects/patients of eating simply by producing the list of all the attested objects of th</context>
<context position="29829" citStr="Padó et al. (2007)" startWordPosition="5000" endWordPosition="5003"> and it is important that it provides reasonable judgment estimates. For the ukWaC corpus, the best results we get are an overall correlation of 0.60 and a mean correlation of 0.52 for the combined case v1v2v1,2. The results for v1,2 and v1v2v1,2 are statistically indistinguishable. Yahoo ukWaC Condition Overall Mean Overall Mean v1,2 0.60 0.45 0.58 0.48 v1v2 0.36 0.26 0.33 0.22 v1v2v1,2 0.55 0.42 0.60 0.52 Table 4: Spearman correlations between the targets and estimations for selectional preference task. In Fig. 3, we present a comparison of our results with some previous studies reported in Padó et al. (2007). The best result reported so far is a correlation of 0.52. Our results for Yahoo and ukWaC are currently the highest correlation values reported. Even the verbeffect-controlled correlations achieve competitive performance. Spearman correlation 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 Figure 3: Comparison of algorithms on selectional preference task. 90 Percentage of correct answers 80 70 60 50 100 40 30 38 4.4 ConceptNet Only for this task, (because of practical memory limitations) we reduced the model parameter b to 500, which means we used the 500 most frequent unigrams and 500 mos</context>
</contexts>
<marker>Padó, Padó, Erk, 2007</marker>
<rawString>S. Padó, S. Padó and K. Erk. 2007. Flexible, corpusbased modelling of human plausibility judgements. Proceedings EMNLP 2007, 400–409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Padó</author>
</authors>
<title>The Integration of Syntax and Semantic Plausibility in a Wide-Coverage Model of Sentence Processing.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Saarland University.</institution>
<contexts>
<context position="6303" citStr="Padó (2007)" startWordPosition="1013" endWordPosition="1014">agging behind Turney’s in terms of performance, suggesting that at least some task-specific tuning will be necessary. Following Turney, we focus on devising a suitably general featural representation, and we see the specific machine learning algorithm employed to perform the various tasks as a parameter. Here, we use Support Vector Machines since they are a particularly effective general-purpose method. In terms of empirical evaluation of the model, besides experimenting with the “classic” SAT and TOEFL datasets, we show how our algorithm can tackle the selectional preference task proposed in Padó (2007) – a regression task – and we introduce to the corpus-based semantics community a challenge from the ConceptNet repository of commonsense knowledge (extending such repository by automated means is the original motivation of our project). In the next section, we will present our proposed method along with the corpora and model parameter choices used in the implementation. In Section 3, we describe the tasks that we use to evaluate the model. Results are reported in Section 4 and we conclude in Section 5, with a brief overview of the contributions of this paper. 2 Methodology 2.1 Model The centr</context>
<context position="20221" citStr="Padó (2007)" startWordPosition="3402" endWordPosition="3403">s is, if a phenomenon is productive there will always be new well-formed instances that are not in the corpus. In the domain of selectional restrictions this is particularly obvious: we would not say that an algorithm learned the constraints on the possible objects/patients of eating simply by producing the list of all the attested objects of this verb in a very large corpus; the interesting issue is whether the algorithm can detect if an unseen object is or is not a plausible “eatee”, like humans do without problems. Specifically, we test selectional preferences on the dataset constructed by Padó (2007), that collects average plausibility judgments (from 20 speakers) for nouns as either subjects or objects of verbs (211 noun-verb pairs). We formulate this task as a regression problem. We train the e-SVM regressor with 18-fold cross validation: Since the pair instances are not independent but grouped according to the verbs, one fold is constructed for each of the 18 verbs used in the dataset. In each fold, all instances sharing the corresponding verb are left out as the test set. The performance measure for this task is the Spearman correlation between the human judgments and our algorithm’s </context>
</contexts>
<marker>Padó, 2007</marker>
<rawString>U. Padó. 2007. The Integration of Syntax and Semantic Plausibility in a Wide-Coverage Model of Sentence Processing. Ph.D. thesis, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>M Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>Proceedings of COLING/ACL 2006,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="2395" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="365" endWordPosition="368">rature on word space models (Sahlgren, 2006) has focused on taxonomic similarity (synonyms, antonyms, co-hyponyms... ) and general association (e.g., finding topically related words), exploiting the idea that taxonomically or associated words will tend to occur in similar contexts, and thus share a vector of cooccurring words. The literature on relational similarity, on the other hand, has focused on pairs of words, devising various methods to compare how similar the contexts in which target pairs appear are to the contexts of other pairs that instantiate a relation of interest (Turney, 2006; Pantel and Pennacchiotti, 2006). Beyond these domains, purely corpus-based methods play an increasingly important role in modeling constraints on composition of words, in particular verbal selectional preferences – finding out that, say, children are more likely to eat than apples, whereas the latter are more likely to be eaten (Erk, 2007; Padó et al., 2007). Tasks of this sort differ from relation extraction in that we need to capture productive patterns: we want to find out that shabu shabu (a Japanese meat dish) is eaten whereas ink is not, even if in our corpus neither noun is attested in proximity to forms of the verb </context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. Proceedings of COLING/ACL 2006, 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Word sense discovery based on sense descriptor dissimilarity.</title>
<date>2003</date>
<booktitle>Proceedings of MT Summit IX:</booktitle>
<pages>315--322</pages>
<contexts>
<context position="27788" citStr="Rapp (2003)" startWordPosition="4654" endWordPosition="4655"> the coverage for the v1v2 case is pretty low. Out of 320 pairs, 70 were represented by zerovectors, resulting in 34 partial questions out of 80. The performance is at 33.8%. The v1v2 case on its own does not lead to a performance better than random guessing (27.5%) but the combined case v1v2v1,2 provides the highest ukWaC score of 42.5%. Condition Yahoo ukWaC v1,2 80.0% 33.8% v1v2 41.2% 27.5% v1v2v1,2 75.0% 42.5% Table 3: Percentage of correctly answered questions in TOEFL synonym task, worst-case scenario. To our knowledge, the best performance with a purely corpus-based approach is that of Rapp (2003) who obtained a score of 92.5% with SVD. Fig. 2 reports our results and a list of other corpus-based systems which achieve scores higher than 70%, along with 95% confidence interval values. The results are taken from the ACL wiki page on the state of the art for TOEFL synonym questions. Figure 2: Comparison with previous algorithms on TOEFL synonym questions with 95% confidence intervals. We note that our results obtained for Yahoo are comparable to the results of Turney but even the best results obtained for ukWaC and the Yahoo’s results for v1v2 only condition are very poor. Whether this is </context>
</contexts>
<marker>Rapp, 2003</marker>
<rawString>R. Rapp. 2003. Word sense discovery based on sense descriptor dissimilarity. Proceedings of MT Summit IX: 315–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
</authors>
<title>The Word-space model.</title>
<date>2006</date>
<institution>Stockholm University,</institution>
<location>Stockholm.</location>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="873" citStr="Sahlgren, 2006" startWordPosition="126" endWordPosition="127">sent word pairs instantiating arbitrary semantic relations that keeps track of the contexts in which the words in the pair occur both together and independently. The resulting features are of sufficient generality to allow us, with the help of a standard supervised machine learning algorithm, to tackle a variety of unrelated semantic tasks with good results and almost no task-specific tailoring. 1 Introduction Co-occurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts (Sahlgren, 2006; Turney, 2006). However, the difficulty of such tasks and the fact that they are apparently unrelated has led to the development of largely ad-hoc solutions, tuned to specific challenges. For many practical applications, this is a drawback: Given the large number of semantic relations that might be relevant to one or the other task, we need a multi-purpose approach that, given an appropriate representation and training examples instantiating an arbitrary target relation, can automatically mine new pairs characterized by the same relation. Building on a recent proposal in this direction by Tur</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>M. Sahlgren. 2006. The Word-space model. Ph.D. dissertation, Stockholm University, Stockholm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term-weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>24</volume>
<issue>5</issue>
<pages>513--523</pages>
<contexts>
<context position="15142" citStr="Salton and Buckley, 1988" startWordPosition="2553" endWordPosition="2556">odel parameters are generally picked at convenience to ease memory requirements and computational efficiency. For instance, in all experiments, b is set to 1500 unless noted otherwise in order to fit the vectors of all pairs at our hand into the computer memory. Once we construct the vectors for a set of word pairs, we get a co-occurrence matrix with pairs on the rows and the features on the columns. In all of our experiments, the same normalization method and classification algorithm is used with the default parameters: First, a TF-IDF feature weighting is applied to the cooccurrence matrix (Salton and Buckley, 1988). Then following the suggestion of Hsu and Chang (2003), each feature t’s [µt −2&amp;t, µt +2Qt] interval is scaled to [0, 1], trimming the exceeding values from upper and lower bounds (the symbols µt and Qt denote the average and standard deviation of the feature values respectively). For the classification algorithm, we use the C-SVM classifier and for regression the e-SVM regressor, both implemented in the Matlab toolbox of Canu et al. (2005). We employed a linear kernel. The cost parameter C is set to 1 for all experiments; for the regressor, e = 0.2. For other pattern recognition related codi</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>G. Salton and C. Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information Processing and Management, 24(5): 513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Speer</author>
<author>C Havasi</author>
<author>H Lieberman</author>
</authors>
<title>Analogyspace: Reducing the dimensionality of common sense knowledge.</title>
<date>2008</date>
<pages>548--553</pages>
<editor>In Dieter Fox and Carla P. Gomes, editors, AAAI,</editor>
<publisher>AAAI Press.</publisher>
<marker>Speer, Havasi, Lieberman, 2008</marker>
<rawString>R. Speer, C. Havasi and H. Lieberman. 2008. Analogyspace: Reducing the dimensionality of common sense knowledge. In Dieter Fox and Carla P. Gomes, editors, AAAI, pages 548–553. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<pages>379--416</pages>
<contexts>
<context position="888" citStr="Turney, 2006" startWordPosition="128" endWordPosition="129">instantiating arbitrary semantic relations that keeps track of the contexts in which the words in the pair occur both together and independently. The resulting features are of sufficient generality to allow us, with the help of a standard supervised machine learning algorithm, to tackle a variety of unrelated semantic tasks with good results and almost no task-specific tailoring. 1 Introduction Co-occurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts (Sahlgren, 2006; Turney, 2006). However, the difficulty of such tasks and the fact that they are apparently unrelated has led to the development of largely ad-hoc solutions, tuned to specific challenges. For many practical applications, this is a drawback: Given the large number of semantic relations that might be relevant to one or the other task, we need a multi-purpose approach that, given an appropriate representation and training examples instantiating an arbitrary target relation, can automatically mine new pairs characterized by the same relation. Building on a recent proposal in this direction by Turney (2008), we </context>
<context position="2362" citStr="Turney, 2006" startWordPosition="362" endWordPosition="364">ords. The literature on word space models (Sahlgren, 2006) has focused on taxonomic similarity (synonyms, antonyms, co-hyponyms... ) and general association (e.g., finding topically related words), exploiting the idea that taxonomically or associated words will tend to occur in similar contexts, and thus share a vector of cooccurring words. The literature on relational similarity, on the other hand, has focused on pairs of words, devising various methods to compare how similar the contexts in which target pairs appear are to the contexts of other pairs that instantiate a relation of interest (Turney, 2006; Pantel and Pennacchiotti, 2006). Beyond these domains, purely corpus-based methods play an increasingly important role in modeling constraints on composition of words, in particular verbal selectional preferences – finding out that, say, children are more likely to eat than apples, whereas the latter are more likely to be eaten (Erk, 2007; Padó et al., 2007). Tasks of this sort differ from relation extraction in that we need to capture productive patterns: we want to find out that shabu shabu (a Japanese meat dish) is eaten whereas ink is not, even if in our corpus neither noun is attested i</context>
<context position="26634" citStr="Turney, 2006" startWordPosition="4460" endWordPosition="4461"> Comparison with previous algorithms on SAT analogy questions. Overall, the performance of BagPack is not at the level of the state of the art but still provides a reasonable level even in the v1v2 only condition for which we do not utilize the contexts where the two words co-occur. This aspect is most striking for ukWaC where the coverage is low and by only utilizing the single-occurrence sub-vectors we obtain a performance of 38.2% correct answers (the comparable “attributional” models re6See http://aclweb.org/aclwiki/ for further information and references 55 50 45 65 60 40 35 37 ported in Turney, 2006, have an average performance of 31%). 4.2 TOEFL For the v1,2 sub-vector calculated for Yahoo, we have two partial questions out of 80 and the system answers 80.0% of the questions correctly. The single occurrence case v1v2 instead provides a correct percentage of 41.2% which is significantly above the random performance of 25% but still very poor. The combined case v1v2v1,2 provides a score of 75.0% with no statistically significant difference from the v1,2 case. The reason of the low performance for v1v2 is an open question. For ukWaC, the coverage for the v1v2 case is pretty low. Out of 320</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>P. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3): 379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms and associations.</title>
<date>2008</date>
<booktitle>Proceedings of COLING 2008,</booktitle>
<pages>905--912</pages>
<contexts>
<context position="1483" citStr="Turney (2008)" startWordPosition="222" endWordPosition="223">006; Turney, 2006). However, the difficulty of such tasks and the fact that they are apparently unrelated has led to the development of largely ad-hoc solutions, tuned to specific challenges. For many practical applications, this is a drawback: Given the large number of semantic relations that might be relevant to one or the other task, we need a multi-purpose approach that, given an appropriate representation and training examples instantiating an arbitrary target relation, can automatically mine new pairs characterized by the same relation. Building on a recent proposal in this direction by Turney (2008), we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking. There has been much previous work on corpus-based models to extract broad classes of related words. The literature on word space models (Sahlgren, 2006) has focused on taxonomic similarity (synonyms, antonyms, co-hyponyms... ) and general association (e.g., finding topically related words), exploiting the idea that taxonomically or associated words will tend to occur in similar contexts, and thus share a vector of cooccurri</context>
<context position="3016" citStr="Turney (2008)" startWordPosition="472" endWordPosition="473"> these domains, purely corpus-based methods play an increasingly important role in modeling constraints on composition of words, in particular verbal selectional preferences – finding out that, say, children are more likely to eat than apples, whereas the latter are more likely to be eaten (Erk, 2007; Padó et al., 2007). Tasks of this sort differ from relation extraction in that we need to capture productive patterns: we want to find out that shabu shabu (a Japanese meat dish) is eaten whereas ink is not, even if in our corpus neither noun is attested in proximity to forms of the verb to eat. Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. In particular, he treats synonymy and association as special cases of relational similarity: in the same way in which we might be able to tell that hands and arms are in a part-of relation by comparing the contexts in which they co-occur to the contexts of known part-of pairs, we can guess that cars and automobiles are synonyms by comparing the contexts in which they co-occur to the contexts linking known synonym pairs. Here, we build on Turney’s work, adding two main methodological innovations that allow us</context>
<context position="16707" citStr="Turney (2008)" startWordPosition="2803" endWordPosition="2805">http://developer.yahoo.com/search/ boss/ 3http://nodebox.net/code/index.php/ Linguistics 4http://www.ims.uni-stuttgart.de/ projekte/corplex/TreeTagger/ 35 3 Tasks 3.1 SAT Analogy Questions The first task we evaluated our algorithm on is the SAT analogy questions task introduced by Turney et al. (2003). In this task, there are 374 multiple choice questions with a pair of related words like (lion,cat) as the stem and 5 other pairs as the choices. The correct answer is the choice pair which has the relationship most similar to that in the stem pair. We adopt a similar approach to the one used in Turney (2008) and consider each question as a separate binary classification problem with one positive training instance and 5 unknown pairs. For a question, we pick a pair at random from the stems of other questions as a pseudo negative instance and train our classifier on this two-instance training set. Then the trained classifier is evaluated on the choice pairs and the pair with the highest posterior probability for the positive class is called the winner. The procedure is repeated 10 times picking a different pseudo-negative instance each time and the choice pair which is selected as the winner most o</context>
<context position="25980" citStr="Turney (2008)" startWordPosition="4352" endWordPosition="4353">te coverage. The performances are statistically indistinguishable from each other and are 38.0%, 38.2%, and 39.6% for v1,2, v1v2, and v1v2v1,2 respectively. Condition Yahoo ukWaC v1,2 42.5% 38.0% v1v2 33.9% 38.2% v1v2v1,2 44.1% 39.6% Table 2: Percentage of correctly answered questions in SAT analogy task, worst-case scenario. In Fig. 1, the best performances we get for Yahoo and ukWaC are compared to previous studies with 95% binomial confidence intervals plotted. The reported values are taken from the ACL wiki page on the state of the art for SAT analogy questions6. The algorithm proposed by Turney (2008) is labeled as Turney-PairClass. Figure 1: Comparison with previous algorithms on SAT analogy questions. Overall, the performance of BagPack is not at the level of the state of the art but still provides a reasonable level even in the v1v2 only condition for which we do not utilize the contexts where the two words co-occur. This aspect is most striking for ukWaC where the coverage is low and by only utilizing the single-occurrence sub-vectors we obtain a performance of 38.2% correct answers (the comparable “attributional” models re6See http://aclweb.org/aclwiki/ for further information and ref</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>P. Turney. 2008. A uniform approach to analogies, synonyms, antonyms and associations. Proceedings of COLING 2008, 905–912.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>