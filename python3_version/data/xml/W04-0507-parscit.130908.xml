<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.991444">
A Practical QA System in Restricted Domains
</title>
<author confidence="0.833870333333333">
Soo-Hong Kim
Hoojung Chung, Young-In Song, Kyoung-Soo Han,
Do-Sang Yoon, Joo-Young Lee, Hae-Chang Rim
</author>
<affiliation confidence="0.998674">
Dept. of Comp. Science and Engineering
Korea University
</affiliation>
<address confidence="0.84014">
Seoul 136-701 Korea
</address>
<email confidence="0.987358">
{hjchung,sprabbit,kshan,yds5004,jylee,rim}@nlp.korea.ac.kr
</email>
<note confidence="0.568051">
Dept. of Comp. Software Engineering
Sangmyung University
Chonan 330-720 Korea
</note>
<email confidence="0.984975">
soohkim@smuc.ac.kr
</email>
<sectionHeader confidence="0.997213" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999702066666667">
This paper describes an on-going research for a
practical question answering system for a home
agent robot. Because the main concern of the QA
system for the home robot is the precision, rather
than coverage (No answer is better than wrong an-
swers), our approach is try to achieve high accuracy
in QA. We restrict the question domains and extract
answers from the pre-selected, semi-structured doc-
uments on the Internet. A named entity tagger and a
dependency parser are used to analyze the question
accurately. User profiling and inference rules are
used to infer hidden information that is required for
finding a precise answer. Testing with a small set of
queries on weather domain, the QA system showed
90.9% of precision and 75.0% of recall.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972571428571">
During the last decade, automatic question-
answering has become an interesting research field
and resulted in a significant improvement in its
performance, which has been largely driven by
the TREC (Text REtrieval Conference) QA Track
(Voorhees, 2004). The best of the systems in the QA
Track is able to answer questions correctly 70% of
the time (Light et al., 2003). The 70% of accuracy
is, of course, high enough to surprise the researchers
of this field, but, on the other hand, the accuracy is
not enough to satisfy the normal users in the real
world, who expect more precise answers.
The difficulty of constructing open-domain
knowledge base is one reason for the difficulties of
open-domain question answering. Since question
answering requires understanding of natural lan-
guage text, the QA system requires much linguis-
tic and common knowledge for answering correctly.
The simplest approach to improve the accuracy of a
question answering system might be restricting the
domain it covers. By restricting the question do-
main, the size of knowledge base to build becomes
smaller.
This paper describes our restricted domain ques-
tion answering system for an agent robot in home
environment. One of the roles of the home agent
robot is to be able to answer the practical ques-
tions such as weather information, stock quote, TV
broadcasting schedule, traffic information etc. via a
speech interface. The agent should provide high-
precision answers, otherwise the users will not trust
the entire functions of the home agent robot, which
includes not only the ability of question answering
but also the speech interface for controlling home
appliances. That means no answer is preferred to
a wrong answer and the primary concern in our re-
search is improving the precision of the question an-
swering system.
In this paper, we present a question answering
system which is restricted to answer only to the
questions on weather forecasts 1, and provide some
experimental results of the restricted QA system.
To achieve the high accuracy, the QA system pro-
cesses the semi-structured text data on the Inter-
net and store it in the form of relational database.
The domain specific hand-coded ontology contain-
ing weather terms and cities is manually built for the
question analysis and the inference process.
The remainder of the paper is organized as fol-
lows. Section 2 describes the overall architecture
of the QA system. Section 3 describes the prac-
tical QA system. Section 4 evaluates the system
and reports the limitation of the QA system. Sec-
tion 5 compares our system with other QA systems.
Section 6 concludes with some directions for future
work.
</bodyText>
<sectionHeader confidence="0.964064" genericHeader="introduction">
2 Overall Architecture
</sectionHeader>
<bodyText confidence="0.97372075">
The overall framework of the QA system is pre-
sented in Figure 1. The QA system consists of two
major parts; the IE (Information Extractor) engine
and the QA engine.
</bodyText>
<footnote confidence="0.616233">
1We’ve developed the QA system for a TV broadcast sched-
ule domain as well, which is more complex to process than the
weather forecasts QA, but have not evaluated it yet. So, in this
paper, we present the system for weather forecasts only.
</footnote>
<figureCaption confidence="0.999594">
Figure 1: Overall architecture of the Question Answering System
</figureCaption>
<figure confidence="0.997563636363636">
Web Browser
DBMS
QA
Engine
IE
Engine
Internet
(WWW)
QA SYSTEM
Speech Processor
Web Interface
</figure>
<bodyText confidence="0.999788041666667">
The IE engine consists of two parts; a web
crawler and a wrapper. The web crawler down-
loads the selected webpages from the website of the
Korea Meteorological Administration (KMA) every
hour. The website provides current weather con-
ditions and 7 day-forecasts for dozens of Korean
cities. The wrapper, which is a set of extraction
rules, is used to extract weather information from
the webpages . The extracted information is stored
in the database.
The QA engine performs three-phase processing:
First, it analyzes natural language questions and
translates the questions into Structured Query Lan-
guage (SQL) statements. Second, the SQL queries
are directed to a DBMS to retrieve the answers in
the database. Finally, the result from the DBMS
is converted to natural language sentences for out-
put. Figure 2 depicts overall processes for the QA
engine. A DBMS (Currently, Oracle Database) is
used for managing extracted data. A speech proces-
sor can be merged with the system when it is used
in the home agent robot, which provides speech in-
terface. A web interface is used for providing web-
based QA service with the QA system.
</bodyText>
<sectionHeader confidence="0.993153" genericHeader="method">
3 A Practical QA System
</sectionHeader>
<bodyText confidence="0.999744625">
The question answering starts from extracting
weather information from the web site. The user
request is analyzed with the question analyzer and
the appropriate query frame is selected, and then the
request is translated into the SQL expression. The
SQL query is used to retrieve the correct answer
from the database, which stores weather informa-
tion from the webpages. Finally, natural language
</bodyText>
<figure confidence="0.581268">
Natural Language
Question
Natural Language
Answer
</figure>
<figureCaption confidence="0.82093">
Figure 2: The QA Engine
answer is generated based on the every result ex-
tracted from the DBMS.
</figureCaption>
<figure confidence="0.99776275">
Temporal Data
Normalizer
QA Engine
Named Entity
Tagger
Inference
Rules
Query Frame
Classifier
NL Answer
Generator
Keywords
SQL
Generator
Question
Analyzer
Query
Frame
Query
Result
SQL
Query
User Profile
DBMS
</figure>
<subsectionHeader confidence="0.99072">
3.1 Information Extraction
</subsectionHeader>
<bodyText confidence="0.99989385">
The weather information in the webpages is semi-
structured. Semi-structured resources generally do
not employ unrestricted natural language text, but
rather exhibit a fair degree of structure (Kushmer-
ick, 1997). Therefore, information can be accu-
rately and easily extracted from the webpage, com-
pared to IE from unstructured data.
On the other hand, semi-structured resources
are usually formatted for use by people, and con-
tain irrelevant elements that must be ignored, such
as images, advertisements, and HTML formatting
tags (Figure 3). Thus information extraction from
the semi-structured documents is not entirely triv-
ial. Currently, the QA system is using hand-coded
wrappers. However, we are developing an auto-
matic process of constructing wrappers (wrapper in-
duction) for semi-structured resources and that can
detect the modification of the web page design and
adapt the wrapper according to the modification, au-
tomatically, like (Sigletos et al., 2003).
</bodyText>
<listItem confidence="0.84478">
Presently, the IE engine extracts following infor-
mation :
• Current observation: weather summary, visi-
bility, temperature, wind, relative humidity
• 7 days-forecasts : weather summary, forecast
temperature (highest/lowest).
</listItem>
<subsectionHeader confidence="0.99986">
3.2 Question Analysis
</subsectionHeader>
<bodyText confidence="0.989172925925926">
First, user’s request is analyzed with the query an-
alyzer as represented in Figure 2. The analyzer ex-
tracts several keywords that describing the question,
such as event word, date, time, and location, by us-
ing a dependency parser, and the user question is
represented only by these extracted keywords.
The named entity tagger is used to identify tem-
poral expressions, place names, and weather events.
The tagger consults the domain-dependent ontology
for recognizing weather events, and the domain-
independent ontology for place names. The ontol-
ogy for the weather events consists of event con-
cepts, which are similar to Synset in WORDNET
(Fellbaum, 1998). For example, rain and umbrella
are in same event concept in the domain ontology
for weather events, because the questions about us-
ing umbrella are usually asking about raining (e.g.
Will I need to bring umbrella tomorrow? and Will it
be raining tomorrow?)
The temporal data normalizer converts temporal
expressions such as today, this weekend and now
into absolute values that can be used in querying to
the database.
Seoul, March. 11., wide spread dust, (-/-)
Seoul, March. 12., cloudy, (0/11)
Seoul, March, 13., Sunny, (1/11)
...
</bodyText>
<figureCaption confidence="0.9675055">
Figure 3: Wrappers extracts weather information
from the semi-structured documents
</figureCaption>
<bodyText confidence="0.997002666666667">
If the information on date, time, or location is
not expressed in the user’s request, the question an-
alyzer infers the missing information. The infer-
ence rules, which are built based on our observation
on various user questions, are domain-independent,
because the omission of temporal or spatial infor-
mation is common not only in weather information
question, but also in questions for other domains.
The user profile is used for the inference in
query analysis. We observed many people omit the
place name in the weather-domain question. Unlike
the temporal information, it is impossible to guess
the current location without any user information.
Thus, we store some user-related information in the
user profile. Portfolio of stocks or favorite TV pro-
grams can be stored in the user profile if the QA sys-
tem processes queries on stock quote or TV sched-
ule domain.
Let’s take an example of the query analysis. The
following keywords are extracted from the question
”Is it raining?”
</bodyText>
<sectionHeader confidence="0.44285725" genericHeader="method">
EVENT : rain
DATE : 03/12/04
TIME : 02:20
CITY : Seoul
</sectionHeader>
<bodyText confidence="0.99929575">
Even though the time, date, and city is not explic-
itly mentioned in the question, the question analyzer
infers the information with the user profile and the
inference rules.
</bodyText>
<subsectionHeader confidence="0.99769">
3.3 Query Frame Decision
</subsectionHeader>
<bodyText confidence="0.999749555555556">
Restricting the question domain and information re-
source, we could restrict the scope of user request.
That is, there is a finite number of expected ques-
tion topics. Each expected question topic is defined
as a single query frame. The following are query
frame examples. They are used for processing the
query for the precipitation forecast for the next day,
diurnal range of today, current wind direction, and
current temperature, respectively.
</bodyText>
<figure confidence="0.894014375">
[PRECIPITATION_TOMORROW]
[DIURNALRANGE_TODAY]
[WINDDIRECTION_CURRENT]
[TEMPERATURE_CURRENT]
Each frame has a rule for SQL generation. PRE-
CIPITATION TOMORROW has the following
SQL generation rule.
[PRECIPITATION_TOMORROW]
</figure>
<bodyText confidence="0.972937392857143">
SELECT date, amprecpr, pmprecpr FROM
forecast tbl WHERE $date $city
date, amprecpr and pmprecpr are field names in the
database table forecast tbl, which mean date, the
precipitation probability of morning and afternoon
of the day. The rule generates the SQL statement
that means: retrieve the precipitation probability of
tomorrow morning and afternoon from the DB table
which stores forecast information.
Here is another example, which is the SQL gen-
eration rule for [DIURNALRANGE TODAY].
[DIURNALRANGE_TODAY]
SELECT city, max(temp)-main(temp) FROM
current tbl WHERE $date $city group by city
Analyzing a question means selecting a query
frame in this system. Thus, it is important to se-
lect the appropriate query frame for the user request.
The selection process is a great influence on the pre-
cision of the system, while there is not much likeli-
hood of errors in other processes, such as generating
SQL query from the selected query frame, retriev-
ing DB records, and generating an answer.
As represented in Figure 2, the extracted event,
temporal and spatial keywords are used for selecting
an appropriate query frame. Currently, we are us-
ing a hand-coded decision tree-like classifier for se-
lecting an appropriate query frame for the extracted
keywords. If a question isn’t proper for the handling
</bodyText>
<figure confidence="0.842850636363636">
Is it raining?
1
EVENT : rain
DATE : 03/12/04
TIME : 02:20
CITY : Seoul
1
The frame [RAIN_CURRENT] is selected.
1
SELECT time, city, weather FROM current tbl
WHERE time=‘03/12/04/0200’, city=‘Seoul’
</figure>
<figureCaption confidence="0.9951395">
Figure 4: Interpreting the natural language question
to the SQL query
</figureCaption>
<figure confidence="0.808166333333333">
SELECT time, city, weather FROM current tbl
WHERE time=‘03/12/04/0200’, city=‘Seoul’
1
DBMS returns “03/12/04/0200 Seoul Sunny”
1
On 2:00 p.m., Seoul is sunny.
</figure>
<figureCaption confidence="0.97223">
Figure 5: Answer generation from the result of
query
</figureCaption>
<bodyText confidence="0.913283">
domain, the classifier rejects it. Machine learned
classifier is being developed in order to substitute
for the hand-coded classifier.
</bodyText>
<subsectionHeader confidence="0.966498">
3.4 SQL Generation
</subsectionHeader>
<bodyText confidence="0.999992166666667">
If a query frame is selected for a question, an SQL
query statement is generated from the SQL pro-
duction rule of the frame. The query is sent to
the DBMS to acquire the records that match to the
query. Figure 4 depicts the conversion from a natu-
ral language question to its SQL expression.
</bodyText>
<subsectionHeader confidence="0.972576">
3.5 Answer Generation
</subsectionHeader>
<bodyText confidence="0.986898733333333">
Based on the result of the DBMS, a natural lan-
guage answer is generated. We use a rule based
answer generation method. Each query frame has
an answer generation pattern for the frame. For
example, DIURNALRANGE TODAY has the
following generation pattern.
[DIURNALRANGE_TODAY]
The diurnal temperature range of $date($1) is $2◦C
$1 and $2 are the the first and second field value of
the queried result. $date() is the function that con-
verts a normalized date expression to a natural lan-
guage expression. Figure 5 shows the answer gener-
ated from the SQL query shown in Figure 4 (More
sample outputs from the QA System are presented
on the Appendix) .
</bodyText>
<sectionHeader confidence="0.985259" genericHeader="method">
4 Evaluation and Limitation
</sectionHeader>
<bodyText confidence="0.999774290322581">
We have evaluated our domain restricted QA sys-
tem based on precision and recall, and investigated
the limitation of the our approach to the restricted-
domain QA system.
For evaluation, we’ve collected 50 weather ques-
tions from 10 graduate students. Precision and re-
call rates are 90.9 % and 75.0% respectively.
The low recall rate is due to some questions re-
lated to invalid date and topic. The system provides
weather forecasts for 7 days from the querying day.
But some of queries are asking for a future weather
outlook which is out of range ( e.g. Will it be very
hot summer this year? or Will it be snow on this
Christmas?). Some questions asked the information
that the database doesn’t contain, such as UVI (ul-
traviolet index).
The primary reason for the wrong answer is the
failure of invalid topic rejection. It is due to the
insufficient of weather-domain ontology data. For
example, from the question What is the discom-
fort index calculated from the today’s weather?,
the keyword discomfort index was not extracted
while weather was extracted, because the former
was not in the ontology. So the query frame
WEATHER TODAY was misselected and the sys-
tem generated the wrong answer Seoul will be sunny
on March 9th 2004.
An error was caused by the flaw of our keyword-
based query frame decision approach. For the ques-
tion Can the flight for Jeju Island take off today?,
the extracted keywords are
</bodyText>
<sectionHeader confidence="0.587809333333333" genericHeader="method">
EVENT flight take_off
DATE 03/12/04
CITY Jeju
</sectionHeader>
<bodyText confidence="0.999923">
In order to know whether the flight can take off
or not, the weather information of the departure city
instead of the destination city (i.e. Jeju) should be
returned, but our keyword based approach failed to
make an appropriate query. To solve this problem,
more sophisticated semantic representation, rather
than the sequence of keywords, is required for the
question.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="method">
5 Related Works
</sectionHeader>
<bodyText confidence="0.999387404761905">
In this section, we compare our system with other
QA-related approaches and briefly describe the dis-
tinctive characteristics of our system. Open-domain
QA systems in QA track mostly extract answers
from unstructrued documents. In the contrast, our
system extracts answers from semi-structured web
pages, which are pre-selected by us, because our
system aims to achieve high precision with the sac-
rifice of the coverage of questions.
Natural language front ends for databases
(Copestake and Jones, 1990) and our system handle
user questions similarly. However, our system has
information extraction part that makes the database
be updated regularly and automatically. Moreover,
our system returns natural language responses to
users.
The START system (Katz, 1997) is a web-based
QA system. It uses World Wide Web as knowledge
resource. Unstructured natural language sentences
are indexed in the form of ternary expressions and
stored in RDB. The START system covers much
wider domain of questions than ours, however, it
seems that the system returns more wrong answers
than ours, because we extract the answer only from
semi-structured documents.
The Jupiter system (Zue et al., 2000) is a con-
versational system that provides weather informa-
tion over the phone. Based on the Galaxy architec-
ture (Goddeau et al., 1994), Jupiter recognizes user
question over the phone, parses the question with
the TINA language understanding system (Seneff,
1992) and generates SQL and natural language an-
swer with the GENESIS system (Baptist and Sen-
eff, 2000). The generated answer is synthesized
with the ENVOICE system. Even the Jupiter system
deals with the same domain, ours can process a bit
wider-range of weather topics. Our QA system can
cover the question which requires inferences such
as When is the best day for washing my car in this
week? Moreover, our system has an ability of infer-
ring missing information from the user profile and
the inferring algorithm.
</bodyText>
<sectionHeader confidence="0.99566" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999884294117647">
This paper describes the practical QA system for re-
stricted domains. To be practically used, our sys-
tem tries to achieve high precision at the sacrifice of
question coverage.
To achieve high accuracy, we pre-designate semi-
structured information resource webpages and ex-
tracted domain-specific information from them. We
also prepare a domain-specific ontology and query
frames for the question analysis. The user’s request
in natural language is converted into SQL expres-
sion to generate an answer for the question. Testing
with a small set of queries on weather domain, the
QA system showed 90.9% of precision and 75.0%
of recall. By restricting the coverage of questions,
our system could achieve relatively high precision.
However, the figures are not enough for a real prac-
tical system.
</bodyText>
<table confidence="0.3333505">
QA Engine QA Engine QA Engine
for Domain 1 for Domain 2 for Domain n
</table>
<figureCaption confidence="0.9945945">
Figure 6: A domain classifier selects a proper re-
stricted domain QA engine
</figureCaption>
<bodyText confidence="0.99986548">
Much work is left for our future work. First,
we are expanding the domain for the system. A
domain classifier will be added to the QA sys-
tem to process multiple-domain questions, as rep-
resented in Figure 6. We will separate domain de-
pendent resources (query frames, ontology contain-
ing domain-dependent information, and etc.) and
domain independent resources (linguistic resources,
and ontology for domain-independent information)
to allow easier domain expansion.
Second, the information extractor has to be up-
graded. Currently, the QA system is using hand-
coded wrappers, and the wrappers cannot extract
necessary information robustly when the webpages
are modified. We are developing an information ex-
tractor that can recognize the modification of the
webpages and modify the wrappers automatically.
The upgraded information extractor will improve
the robustness of our system.
Finally, we will increase the size of ontology to
cover more question types. From the experimenta-
tion, we realize that a larger ontology for weather
terms is necessary to classify a question correctly.
It seems more query frames are necessary for more
proper answers to the users’ requests.
</bodyText>
<sectionHeader confidence="0.993953" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989527947368421">
L. Baptist and S. Seneff. 2000. Genesis-II: A ver-
stile system for language generation in conversa-
tional system applications. In Proceedings of In-
ternational Conference on Spoken Language Pro-
cessing, October.
A. Copestake and K. Sparck Jones. 1990. Natural
language interfaces to databases. The Knowledge
Engineering Review, 5(4):225–249.
C. Fellbaum. 1998. WordNet: an Electronic Lexi-
cal Database. The MIT Press.
D. Goddeau, E. Brill, J. Glass., C. Pao, M. Phillips,
J. Polifrini, S. Seneff, and V. Zue. 1994. Galaxy:
A human language interface to on-line travel in-
formation. In Proceedings of International Con-
ference on Spoken Language Processing, pages
707–710.
B. Katz. 1997. Annotating the World Wide Web
using natural language. In Proceedings of the 5th
RIAO conference on Computer Assisted Informa-
tion Searching on the Internet.
N. Kushmerick. 1997. Wrapper Inductin for In-
formation Extraction. Ph.D. thesis, University of
Washington.
M. Light, A. Ittycheriah, A. Latto, and N. Mac-
Cracken. 2003. Reuse in question answering: A
preliminary study. In New Directions in Question
Answering: Papers from the 2003 AAAI Sympo-
sium, pages 78–86.
S. Seneff. 1992. Tina: A natural language system
for spoken language applications. Computational
Linguistics, 18(1):pp. 61–86.
G. Sigletos, G. Paliouras, C. D. Spyropoulos, and
M. Hatzopoulos. 2003. Mining web sites us-
ing wrapper induction, named entities and post-
processing. In 1st European Web Mining Forum
Workshop at ECML/PKDD-2003.
Ellen .M. Voorhees. 2004. Overview of the TREC
2003 question answering track. In Proceedings
</reference>
<bodyText confidence="0.580609">
of the 12th Text Retrieval Conference.
V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao,
T. J. Hazen, and L. Hetherington. 2000. Jupiter:
A telephone-based conversational interface for
weather information. IEEE Transactions on
Speech and Audio Processing, pages 100–112.
</bodyText>
<sectionHeader confidence="0.756246" genericHeader="references">
Appendix: Example of QA
</sectionHeader>
<bodyText confidence="0.829616333333333">
Q: Is it cold outside?
A: The temperature of Seoul at 1 p.m. is 11◦ C.
Q: Will it be cold today?
</bodyText>
<figure confidence="0.994329783783784">
SQL Generator
for Domain 1
Question
Analyzer
for Domain 1
Query Frame
Classifier for
Domain 1
.
.
.
Natural Language
Question
SQL Generator
for Domain 2
Question
Analyzer
for Domain 2
Query Frame
Classifier for
Domain 2
Domain
ClassiÞer
.
.
.
SQL Generator
for Domain n
Question
Analyzer
for Domain n
Query Frame
Classifier for
Domain n
.
.
.
</figure>
<bodyText confidence="0.996222428571429">
A: The highest temperature of Seoul on March 9th
2004, will be 12° C.
Q: What’s the diurnal temperature range of today?
A: The diurnal temperature range of Seoul is 1°C.
Q: I am planing a picnic to Yongin on these week-
ends, and which day would be better for the
picnic?
A: Yongin will be sunny on March 3rd 2004. Yon-
gin will be partly cloudy on March 4th 2004.
Q: What is the ultraviolet index?
A: The system cannot answer to that topic.
Q: Can I have car wash today?
A: The precipitation probability of Seoul this after-
noon is 10%.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.140053">
<title confidence="0.999461">A Practical QA System in Restricted Domains</title>
<author confidence="0.889480666666667">Soo-Hong Kim Hoojung Chung</author>
<author confidence="0.889480666666667">Young-In Song</author>
<author confidence="0.889480666666667">Kyoung-Soo Do-Sang Yoon</author>
<author confidence="0.889480666666667">Joo-Young Lee</author>
<author confidence="0.889480666666667">Hae-Chang Rim</author>
<affiliation confidence="0.7327025">Dept. of Comp. Science and Korea</affiliation>
<address confidence="0.589467">Seoul 136-701</address>
<affiliation confidence="0.70567">Dept. of Comp. Software Sangmyung</affiliation>
<address confidence="0.863215">Chonan 330-720</address>
<email confidence="0.974742">soohkim@smuc.ac.kr</email>
<abstract confidence="0.9978165625">This paper describes an on-going research for a practical question answering system for a home agent robot. Because the main concern of the QA system for the home robot is the precision, rather than coverage (No answer is better than wrong answers), our approach is try to achieve high accuracy in QA. We restrict the question domains and extract answers from the pre-selected, semi-structured documents on the Internet. A named entity tagger and a dependency parser are used to analyze the question accurately. User profiling and inference rules are used to infer hidden information that is required for finding a precise answer. Testing with a small set of queries on weather domain, the QA system showed 90.9% of precision and 75.0% of recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Baptist</author>
<author>S Seneff</author>
</authors>
<title>Genesis-II: A verstile system for language generation in conversational system applications.</title>
<date>2000</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing,</booktitle>
<contexts>
<context position="16990" citStr="Baptist and Seneff, 2000" startWordPosition="2720" endWordPosition="2724">ions and stored in RDB. The START system covers much wider domain of questions than ours, however, it seems that the system returns more wrong answers than ours, because we extract the answer only from semi-structured documents. The Jupiter system (Zue et al., 2000) is a conversational system that provides weather information over the phone. Based on the Galaxy architecture (Goddeau et al., 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff, 1992) and generates SQL and natural language answer with the GENESIS system (Baptist and Seneff, 2000). The generated answer is synthesized with the ENVOICE system. Even the Jupiter system deals with the same domain, ours can process a bit wider-range of weather topics. Our QA system can cover the question which requires inferences such as When is the best day for washing my car in this week? Moreover, our system has an ability of inferring missing information from the user profile and the inferring algorithm. 6 Conclusion This paper describes the practical QA system for restricted domains. To be practically used, our system tries to achieve high precision at the sacrifice of question coverage</context>
</contexts>
<marker>Baptist, Seneff, 2000</marker>
<rawString>L. Baptist and S. Seneff. 2000. Genesis-II: A verstile system for language generation in conversational system applications. In Proceedings of International Conference on Spoken Language Processing, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>K Sparck Jones</author>
</authors>
<title>Natural language interfaces to databases.</title>
<date>1990</date>
<journal>The Knowledge Engineering Review,</journal>
<volume>5</volume>
<issue>4</issue>
<contexts>
<context position="15950" citStr="Copestake and Jones, 1990" startWordPosition="2558" endWordPosition="2561"> sophisticated semantic representation, rather than the sequence of keywords, is required for the question. 5 Related Works In this section, we compare our system with other QA-related approaches and briefly describe the distinctive characteristics of our system. Open-domain QA systems in QA track mostly extract answers from unstructrued documents. In the contrast, our system extracts answers from semi-structured web pages, which are pre-selected by us, because our system aims to achieve high precision with the sacrifice of the coverage of questions. Natural language front ends for databases (Copestake and Jones, 1990) and our system handle user questions similarly. However, our system has information extraction part that makes the database be updated regularly and automatically. Moreover, our system returns natural language responses to users. The START system (Katz, 1997) is a web-based QA system. It uses World Wide Web as knowledge resource. Unstructured natural language sentences are indexed in the form of ternary expressions and stored in RDB. The START system covers much wider domain of questions than ours, however, it seems that the system returns more wrong answers than ours, because we extract the </context>
</contexts>
<marker>Copestake, Jones, 1990</marker>
<rawString>A. Copestake and K. Sparck Jones. 1990. Natural language interfaces to databases. The Knowledge Engineering Review, 5(4):225–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: an Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="8165" citStr="Fellbaum, 1998" startWordPosition="1296" endWordPosition="1297">the query analyzer as represented in Figure 2. The analyzer extracts several keywords that describing the question, such as event word, date, time, and location, by using a dependency parser, and the user question is represented only by these extracted keywords. The named entity tagger is used to identify temporal expressions, place names, and weather events. The tagger consults the domain-dependent ontology for recognizing weather events, and the domainindependent ontology for place names. The ontology for the weather events consists of event concepts, which are similar to Synset in WORDNET (Fellbaum, 1998). For example, rain and umbrella are in same event concept in the domain ontology for weather events, because the questions about using umbrella are usually asking about raining (e.g. Will I need to bring umbrella tomorrow? and Will it be raining tomorrow?) The temporal data normalizer converts temporal expressions such as today, this weekend and now into absolute values that can be used in querying to the database. Seoul, March. 11., wide spread dust, (-/-) Seoul, March. 12., cloudy, (0/11) Seoul, March, 13., Sunny, (1/11) ... Figure 3: Wrappers extracts weather information from the semi-stru</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: an Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goddeau</author>
<author>E Brill</author>
<author>J Glass</author>
<author>C Pao</author>
<author>M Phillips</author>
<author>J Polifrini</author>
<author>S Seneff</author>
<author>V Zue</author>
</authors>
<title>Galaxy: A human language interface to on-line travel information.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing,</booktitle>
<pages>707--710</pages>
<contexts>
<context position="16764" citStr="Goddeau et al., 1994" startWordPosition="2686" endWordPosition="2689"> natural language responses to users. The START system (Katz, 1997) is a web-based QA system. It uses World Wide Web as knowledge resource. Unstructured natural language sentences are indexed in the form of ternary expressions and stored in RDB. The START system covers much wider domain of questions than ours, however, it seems that the system returns more wrong answers than ours, because we extract the answer only from semi-structured documents. The Jupiter system (Zue et al., 2000) is a conversational system that provides weather information over the phone. Based on the Galaxy architecture (Goddeau et al., 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff, 1992) and generates SQL and natural language answer with the GENESIS system (Baptist and Seneff, 2000). The generated answer is synthesized with the ENVOICE system. Even the Jupiter system deals with the same domain, ours can process a bit wider-range of weather topics. Our QA system can cover the question which requires inferences such as When is the best day for washing my car in this week? Moreover, our system has an ability of inferring missing information from the us</context>
</contexts>
<marker>Goddeau, Brill, Glass, Pao, Phillips, Polifrini, Seneff, Zue, 1994</marker>
<rawString>D. Goddeau, E. Brill, J. Glass., C. Pao, M. Phillips, J. Polifrini, S. Seneff, and V. Zue. 1994. Galaxy: A human language interface to on-line travel information. In Proceedings of International Conference on Spoken Language Processing, pages 707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Katz</author>
</authors>
<title>Annotating the World Wide Web using natural language.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th RIAO conference on Computer Assisted Information Searching on the Internet.</booktitle>
<contexts>
<context position="16210" citStr="Katz, 1997" startWordPosition="2597" endWordPosition="2598"> QA systems in QA track mostly extract answers from unstructrued documents. In the contrast, our system extracts answers from semi-structured web pages, which are pre-selected by us, because our system aims to achieve high precision with the sacrifice of the coverage of questions. Natural language front ends for databases (Copestake and Jones, 1990) and our system handle user questions similarly. However, our system has information extraction part that makes the database be updated regularly and automatically. Moreover, our system returns natural language responses to users. The START system (Katz, 1997) is a web-based QA system. It uses World Wide Web as knowledge resource. Unstructured natural language sentences are indexed in the form of ternary expressions and stored in RDB. The START system covers much wider domain of questions than ours, however, it seems that the system returns more wrong answers than ours, because we extract the answer only from semi-structured documents. The Jupiter system (Zue et al., 2000) is a conversational system that provides weather information over the phone. Based on the Galaxy architecture (Goddeau et al., 1994), Jupiter recognizes user question over the ph</context>
</contexts>
<marker>Katz, 1997</marker>
<rawString>B. Katz. 1997. Annotating the World Wide Web using natural language. In Proceedings of the 5th RIAO conference on Computer Assisted Information Searching on the Internet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kushmerick</author>
</authors>
<title>Wrapper Inductin for Information Extraction.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Washington.</institution>
<contexts>
<context position="6511" citStr="Kushmerick, 1997" startWordPosition="1044" endWordPosition="1046">. Finally, natural language Natural Language Question Natural Language Answer Figure 2: The QA Engine answer is generated based on the every result extracted from the DBMS. Temporal Data Normalizer QA Engine Named Entity Tagger Inference Rules Query Frame Classifier NL Answer Generator Keywords SQL Generator Question Analyzer Query Frame Query Result SQL Query User Profile DBMS 3.1 Information Extraction The weather information in the webpages is semistructured. Semi-structured resources generally do not employ unrestricted natural language text, but rather exhibit a fair degree of structure (Kushmerick, 1997). Therefore, information can be accurately and easily extracted from the webpage, compared to IE from unstructured data. On the other hand, semi-structured resources are usually formatted for use by people, and contain irrelevant elements that must be ignored, such as images, advertisements, and HTML formatting tags (Figure 3). Thus information extraction from the semi-structured documents is not entirely trivial. Currently, the QA system is using hand-coded wrappers. However, we are developing an automatic process of constructing wrappers (wrapper induction) for semi-structured resources and </context>
</contexts>
<marker>Kushmerick, 1997</marker>
<rawString>N. Kushmerick. 1997. Wrapper Inductin for Information Extraction. Ph.D. thesis, University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Light</author>
<author>A Ittycheriah</author>
<author>A Latto</author>
<author>N MacCracken</author>
</authors>
<title>Reuse in question answering: A preliminary study.</title>
<date>2003</date>
<booktitle>In New Directions in Question Answering: Papers from the 2003 AAAI Symposium,</booktitle>
<pages>78--86</pages>
<contexts>
<context position="1515" citStr="Light et al., 2003" startWordPosition="228" endWordPosition="231">y. User profiling and inference rules are used to infer hidden information that is required for finding a precise answer. Testing with a small set of queries on weather domain, the QA system showed 90.9% of precision and 75.0% of recall. 1 Introduction During the last decade, automatic questionanswering has become an interesting research field and resulted in a significant improvement in its performance, which has been largely driven by the TREC (Text REtrieval Conference) QA Track (Voorhees, 2004). The best of the systems in the QA Track is able to answer questions correctly 70% of the time (Light et al., 2003). The 70% of accuracy is, of course, high enough to surprise the researchers of this field, but, on the other hand, the accuracy is not enough to satisfy the normal users in the real world, who expect more precise answers. The difficulty of constructing open-domain knowledge base is one reason for the difficulties of open-domain question answering. Since question answering requires understanding of natural language text, the QA system requires much linguistic and common knowledge for answering correctly. The simplest approach to improve the accuracy of a question answering system might be rest</context>
</contexts>
<marker>Light, Ittycheriah, Latto, MacCracken, 2003</marker>
<rawString>M. Light, A. Ittycheriah, A. Latto, and N. MacCracken. 2003. Reuse in question answering: A preliminary study. In New Directions in Question Answering: Papers from the 2003 AAAI Symposium, pages 78–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
</authors>
<title>Tina: A natural language system for spoken language applications.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>1</issue>
<pages>61--86</pages>
<contexts>
<context position="16893" citStr="Seneff, 1992" startWordPosition="2706" endWordPosition="2707">e. Unstructured natural language sentences are indexed in the form of ternary expressions and stored in RDB. The START system covers much wider domain of questions than ours, however, it seems that the system returns more wrong answers than ours, because we extract the answer only from semi-structured documents. The Jupiter system (Zue et al., 2000) is a conversational system that provides weather information over the phone. Based on the Galaxy architecture (Goddeau et al., 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff, 1992) and generates SQL and natural language answer with the GENESIS system (Baptist and Seneff, 2000). The generated answer is synthesized with the ENVOICE system. Even the Jupiter system deals with the same domain, ours can process a bit wider-range of weather topics. Our QA system can cover the question which requires inferences such as When is the best day for washing my car in this week? Moreover, our system has an ability of inferring missing information from the user profile and the inferring algorithm. 6 Conclusion This paper describes the practical QA system for restricted domains. To be p</context>
</contexts>
<marker>Seneff, 1992</marker>
<rawString>S. Seneff. 1992. Tina: A natural language system for spoken language applications. Computational Linguistics, 18(1):pp. 61–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sigletos</author>
<author>G Paliouras</author>
<author>C D Spyropoulos</author>
<author>M Hatzopoulos</author>
</authors>
<title>Mining web sites using wrapper induction, named entities and postprocessing.</title>
<date>2003</date>
<booktitle>In 1st European Web Mining Forum Workshop at ECML/PKDD-2003.</booktitle>
<contexts>
<context position="7263" citStr="Sigletos et al., 2003" startWordPosition="1157" endWordPosition="1160">her hand, semi-structured resources are usually formatted for use by people, and contain irrelevant elements that must be ignored, such as images, advertisements, and HTML formatting tags (Figure 3). Thus information extraction from the semi-structured documents is not entirely trivial. Currently, the QA system is using hand-coded wrappers. However, we are developing an automatic process of constructing wrappers (wrapper induction) for semi-structured resources and that can detect the modification of the web page design and adapt the wrapper according to the modification, automatically, like (Sigletos et al., 2003). Presently, the IE engine extracts following information : • Current observation: weather summary, visibility, temperature, wind, relative humidity • 7 days-forecasts : weather summary, forecast temperature (highest/lowest). 3.2 Question Analysis First, user’s request is analyzed with the query analyzer as represented in Figure 2. The analyzer extracts several keywords that describing the question, such as event word, date, time, and location, by using a dependency parser, and the user question is represented only by these extracted keywords. The named entity tagger is used to identify tempor</context>
</contexts>
<marker>Sigletos, Paliouras, Spyropoulos, Hatzopoulos, 2003</marker>
<rawString>G. Sigletos, G. Paliouras, C. D. Spyropoulos, and M. Hatzopoulos. 2003. Mining web sites using wrapper induction, named entities and postprocessing. In 1st European Web Mining Forum Workshop at ECML/PKDD-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>question answering track.</title>
<date>2004</date>
<journal>Overview of the TREC</journal>
<booktitle>In Proceedings</booktitle>
<contexts>
<context position="1399" citStr="Voorhees, 2004" startWordPosition="207" endWordPosition="208">uments on the Internet. A named entity tagger and a dependency parser are used to analyze the question accurately. User profiling and inference rules are used to infer hidden information that is required for finding a precise answer. Testing with a small set of queries on weather domain, the QA system showed 90.9% of precision and 75.0% of recall. 1 Introduction During the last decade, automatic questionanswering has become an interesting research field and resulted in a significant improvement in its performance, which has been largely driven by the TREC (Text REtrieval Conference) QA Track (Voorhees, 2004). The best of the systems in the QA Track is able to answer questions correctly 70% of the time (Light et al., 2003). The 70% of accuracy is, of course, high enough to surprise the researchers of this field, but, on the other hand, the accuracy is not enough to satisfy the normal users in the real world, who expect more precise answers. The difficulty of constructing open-domain knowledge base is one reason for the difficulties of open-domain question answering. Since question answering requires understanding of natural language text, the QA system requires much linguistic and common knowledge</context>
</contexts>
<marker>Voorhees, 2004</marker>
<rawString>Ellen .M. Voorhees. 2004. Overview of the TREC 2003 question answering track. In Proceedings</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>