<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000576">
<title confidence="0.9978535">
Exploring Supervised LDA Models for Assigning Attributes to
Adjective-Noun Phrases
</title>
<author confidence="0.984064">
Matthias Hartung and Anette Frank
</author>
<affiliation confidence="0.9780575">
Computational Linguistics Department
Heidelberg University
</affiliation>
<email confidence="0.998015">
{hartung,frank}@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.998593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946318181818">
This paper introduces an attribute selection
task as a way to characterize the inherent mea-
ning of property-denoting adjectives in adjec-
tive-noun phrases, such as e.g. hot in hot sum-
mer denoting the attribute TEMPERATURE,
rather than TASTE. We formulate this task
in a vector space model that represents adjec-
tives and nouns as vectors in a semantic space
defined over possible attributes. The vectors
incorporate latent semantic information ob-
tained from two variants of LDA topic mod-
els. Our LDA models outperform previous ap-
proaches on a small set of 10 attributes with
considerable gains on sparse representations,
which highlights the strong smoothing power
of LDA models. For the first time, we extend
the attribute selection task to a new data set
with more than 200 classes. We observe that
large-scale attribute selection is a hard prob-
lem, but a subset of attributes performs ro-
bustly on the large scale as well. Again, the
LDA models outperform the VSM baseline.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999905375">
Corpus-based statistical modeling of semantics is
gaining increased attention in computational linguis-
tics. This field of research includes distributional
vector space models (VSMs), i.e., models that rep-
resent the semantics of words or phrases as vectors
over high-dimensional cooccurrence data (Turney
and Pantel, 2010; Baroni and Lenci, 2010, i.a.), as
well as latent variable models (LVMs) which aggre-
gate distributional observations in ’hidden’, or latent
variables, thereby reducing the dimensionality of the
data. An example of the latter are topic models (Blei
et al., 2003), which have recently been applied to
modeling selectional preferences of verbs (Ritter et
al., 2010; O´ S´eaghdha, 2010), or word sense disam-
biguation (Li et al., 2010).
A topic that is increasingly studied in distribu-
tional semantics is the semantics of adjectives, both
in isolation (Almuhareb, 2006) and in compositional
adjective-noun phrases (Hartung and Frank, 2010;
Guevara, 2010; Baroni and Zamparelli, 2010).
In this paper, we propose a new approach to a
problem we denote as attribute selection: The task is
to predict the hidden attribute meaning expressed by
a property-denoting adjective in composition with
a noun. The adjective hot, e.g., may denote at-
tributes such as TEMPERATURE, TASTE or EMO-
TIONALITY. These adjective meanings can be com-
bined with nouns such as tea, soup or debate, which
can be characterized in terms of attributes as well.
The goal of the task is to determine the hidden at-
tribute meaning predicated over the noun in a given
adjective-noun phrase, as illustrated in (1).
</bodyText>
<listItem confidence="0.976857">
(1) a. a hotvalue summerconcept
b. TEMPERATURE(summer) = hot
</listItem>
<bodyText confidence="0.999985111111111">
It is by way of the composition of adjective and
noun that specific attributes are selected from the ad-
jective’s space of possible attribute meanings, and
typically lead to a disambiguation of the adjective
and possibly the noun. Hartung and Frank (2010)
were the first to model this insight in a VSM by rep-
resenting the meaning of adjectives and nouns in se-
mantic vectors defined over attributes. The meaning
of adjective-noun phrases is computed by means of
</bodyText>
<page confidence="0.949979">
540
</page>
<note confidence="0.9815925">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 540–551,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<figureCaption confidence="0.99948">
Figure 1: Vectors for enormous (~e) and ball (~b)
</figureCaption>
<bodyText confidence="0.999927228571429">
vector composition, such that the ‘hidden’ attribute
meaning of the phrase can be ‘selected’ as a promi-
nent component from the composed vector. This is
illustrated in Fig. 1 for the adjective enormous (~e)
in combination with the noun ball (~b), with alter-
native composition operations: vector multiplication
(×) and addition (+).1 Both yield SIZE as the most
prominent component in the composed vector.
In the present paper we offer a new approach to
this formalization of the compositional meaning of
adjectives and nouns that owes to both distributional
VSMs and LVMs. Through this combination, we
attempt to improve on earlier work in Almuhareb
(2006) and Hartung and Frank (2010), which are
both embedded in a purely distributional setting.
Specifically, we use Latent Dirichlet Allocation
(LDA; Blei et al. (2003)) to train an attribute model
that captures semantic information encoded in ad-
jectives and nouns independently of one another.
Following Hartung and Frank (2010), this model is
embedded into a VSM that employs vector com-
position to combine the meaning of adjectives and
nouns. We present two variants of LDA that differ
in the way attributes are associated with the induced
LDA topics: Controled LDA (C-LDA) and Labeled
LDA (L-LDA; Ramage et al. (2009)). Both will be
presented in detail in Section 3.
Our aims in this paper are two-fold: (i) We inves-
tigate LDA as a modeling framework in the attribute
selection task, as its use of topics as latent variables
may alleviate inherent sparsity problems faced by
prior work using pattern-based (Almuhareb, 2006)
or vector space models (Hartung and Frank, 2010).
(ii) While these prior approaches were restricted to
a confined set of 10 attributes, we will we apply our
</bodyText>
<footnote confidence="0.949780333333333">
1The figure is adopted from the distributional setting of Har-
tung and Frank (2010), with component values defined by pat-
tern frequency counts for the chosen attribute nouns.
</footnote>
<bodyText confidence="0.998835866666667">
models on a much larger space of attributes, to probe
their capacity on a more realistic data set.
The remainder of this paper is divided as fol-
lows. Section 2 reviews related work on distribu-
tional models of adjective semantics, and introduces
the two frameworks in which we ground our ap-
proach: LVMs and VSMs. In Section 3 we introduce
two LDA models for attribute selection: C-LDA and
L-LDA. Section 4 describes the settings for two ex-
periments: In the first experiment, we perform at-
tribute selection confined to a space of 10 attributes
to compare against prior work. In the second setting
we perform attribute selection on a large scale, using
206 attributes. Section 5 presents and discusses the
results. Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.999942" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.961364533333333">
Distributional models of adjective semantics.
Almuhareb (2006) aims at capturing the relationship
between adjectives and attributes based on lexico-
syntactic patterns, such as the ATTR of the * is ADJ.
Apart from inherent sparsity issues, his approach
does not account for the compositional nature of the
problem, as the contextual information contributed
by a noun is neglected: For instance, his model is
unable to predict that hot is unlikely to denote TASTE
in the context of summer, other than in hot meal.
Compositionality of adjective-noun phrases and
how it can be adequately modeled in VSMs is
the main concern in Baroni and Zamparelli (2010)
and Guevara (2010), who are in search of the
best composition operator for combining adjective
with noun meanings. While these works adhere
to a purely latent representation of meaning, Har-
tung and Frank (2010) include attributes as sym-
bolic ‘hidden’ meanings of adjectives, nouns and
adjective-noun phrases in a distributional VSM.
Finally, a large body of work dealing with com-
positionality in distributional frameworks is not con-
fined to the special case of adjective-noun composi-
tion (Mitchell and Lapata (2008), Rudolph and Gies-
brecht (2010), i.a.). All these approaches regard
composition as a process combining vectors (or ma-
trices, resp.) to yield a new, contextualized vector
representation within the same semantic space.
Latent Dirichlet Allocation, aka. Topic Models
(TMs). LDA is a generative probabilistic model
</bodyText>
<figure confidence="0.944964405405405">
e
21
1 1 0 1 45 0 4 0 0
2
0
0
0
0
0
0
0
2
0
0
0
b
e×
e+b
20
420
41
20
20
21
45
180
49
38
38
39
14
14
15
26
1170
71
COLOR DIRECTION DURATION SHAPE SIZE SMELL SPEED TASTE TEMPERATURE WEIGHT
</figure>
<page confidence="0.99554">
541
</page>
<bodyText confidence="0.999949935483871">
for document collections. Each document is repre-
sented as a mixture over latent topics, where each
topic is a probability distribution over words (Blei et
al., 2003). These topics can be used as dense fea-
tures for, e.g., document clustering. Depending on
the number of topics, which has to be pre-specified,
the dimensionality of the document representation
can be considerably reduced in comparison to sim-
ple bag-of-words models. The remainder of this pa-
per will assume some familiarity with LDA and the
LDA terminology as introduced in Blei et al. (2003).
Recent work investigates ways of accommodating
supervision with LDA, e.g. supervised topic models
(Blei and McAuliffe, 2007), Labeled LDA (L-LDA)
(Ramage et al., 2009) or DiscLDA (Lacoste-Julien
et al., 2008). We will discuss L-LDA in Section 3.
Distributional VSMs and TMs. The idea to inte-
grate topic models and VSMs goes back to Mitchell
and Lapata (2009) who build a distributional model
with dimensions set to topics over bag-of-words fea-
tures. In their setting, LDA merely serves the pur-
pose of dimensionality reduction, whereas our par-
ticular motivation is to use topics as probabilistic
indicators for the prediction of attributes as seman-
tic target categories in adjective-noun composition.
Mitchell and Lapata (2010) compare VSMs defined
over bags of context words vs. latent topics in a sim-
ilarity judgement task. Their results indicate that a
multiplicative setting works best for vector compo-
sition in word-based models, while vector addition
is better suited for topic vectors.
</bodyText>
<sectionHeader confidence="0.993878" genericHeader="method">
3 Topic Models for Attribute Selection
</sectionHeader>
<subsectionHeader confidence="0.999778">
3.1 Using LDA for modeling lexical semantics
</subsectionHeader>
<bodyText confidence="0.999970379310345">
Recently, LDA has been used for problems in lexical
semantics, where the primary goal is not document
modeling but the induction of semantic knowledge
from high-dimensional co-occurrence data. Ritter et
al. (2010) and O´ S´eaghdha (2010) model selectional
restrictions of verbs by inducing topic distributions
that characterize ’mixtures of topics’ observed in
verb argument positions. As a basis for LDA mod-
eling, they collect pseudo-documents, i.e. bags of
words that co-occur in syntactic argument positions.
We apply a similar idea to the attribute selection
problem: we collect pseudo-documents that char-
acterize attributes by adjectives and nouns that co-
occur with the attribute nouns in local contextual re-
lations. The topic distributions obtained from fitting
an LDA model to the collection of these pseudo-
documents can then be injected into semantic vector
representations for adjectives and nouns.
In its original statement, LDA is a fully unsuper-
vised process (apart from the desired number of top-
ics which has to be specified in advance) that es-
timates topic distributions over documents Bd and
topic-word distributions Ot with topics represented
as latent variables. Estimating these parameters on a
document collection yields topic proportions P(t|d)
and topic distributions P(w|t) that can be used to
compute a smooth distribution P(w|d) as in (2),
where t denotes a latent topic, w a word and d a
document in the corpus.
</bodyText>
<equation confidence="0.999211">
P(w|d) = � P(w|t)P(t|d) (2)
t
</equation>
<bodyText confidence="0.999965862068966">
Being designed for exploratory rather than dis-
criminative analysis, LDA does not intend condi-
tioning of words or topics on external categories.
That is, the resulting topics cannot be related to pre-
viously defined target categories. For attribute se-
lection, the LDA-inferred topics need to be linked
to semantic attributes. Therefore, we apply two ex-
tensions of standard LDA that are capable of taking
supervised category information into account, either
implicitly or directly, by including an additional ob-
servable variable into the generative process.
In general, LVMs can be expected to overcome
sparsity issues that are frequently encountered in
distributional models. This positive smoothing ef-
fect is achieved by marginalization over the latent
variables (cf. Prescher et al. (2000)). For instance, it
is unlikely to observe a dependency path linking the
adjective mature to the attribute MATURITY. Such
a relation is more likely for young, for example. If
young co-occurs with mature in a different pseudo-
document (AGE might be a candidate), this results in
a situation where (i) young and mature share one or
more latent topics and (ii) the topic proportions for
the attributes MATURITY and AGE will become sim-
ilar to the extent of common words in their pseudo-
documents. Consequently, the final attribute model
is expected to assign a (small) positive probability to
the relation between mature and MATURITY without
observing it in the training data.
</bodyText>
<page confidence="0.981712">
542
</page>
<subsectionHeader confidence="0.997226">
3.2 Controled LDA
</subsectionHeader>
<bodyText confidence="0.9999773125">
The generative story behind C-LDA is equivalent to
standard LDA. However, the collection of pseudo-
documents used as input to C-LDA is structured in
a controled way such that each document conveys
semantic information that specifically characterizes
the individual categories of interest (attributes, in
our case). In line with the distributional hypothesis
(Harris, 1968), we consider the pseudo-documents
constructed in this way as distributional fingerprints
of the meaning of the corresponding attribute.
The contents of the pseudo-documents are se-
lected along syntactic dependency paths linking
each attribute noun to meaningful context words (ad-
jectives and nouns).2 A corpus consisting of the two
sentences in (3), e.g., yields a pseudo-document for
the attribute noun SPEED containing car and fast.
</bodyText>
<listItem confidence="0.5813335">
(3) What is the speed of this car? The machine
runs at a very fast speed.
</listItem>
<bodyText confidence="0.999913695652174">
Though we are ultimately interested in triples of
attributes, adjectives and nouns that define the com-
positional semantics of adjective-noun phrases (cf.
(1)), C-LDA is only exposed to binary tuples be-
tween attributes and adjectives or nouns, respec-
tively. This is in line with Hartung and Frank
(2010), who obtained substantial performance im-
provements by splitting the ternary relation into two
binary relations.
Presenting LDA with pseudo-documents that cha-
racterize individual target attributes imports super-
vision into the LDA process in two respects: the
estimated topic proportions P(t|d) will be highly
attribute-specific, and similarly so for the topic dis-
tributions P(w|t). This makes the model more ex-
pressive for the ultimate labeling task. Moreover,
since C-LDA collects pseudo-documents focused on
individual target attributes, we are able to link exter-
nal categories to the generative process by heuristi-
cally labeling pseudo-documents with their respec-
tive attribute as target category. Thus, we approx-
imate P(w|a), the probability of a word given an
attribute, by P(w|d) as obtained from LDA:
</bodyText>
<footnote confidence="0.618602333333333">
2The dependency paths, together with the set of attribute
nouns of interest, have to be manually specified. See the sup-
plementary material for the full list of dependency paths used.
</footnote>
<figure confidence="0.946229666666667">
1 For each topic k ∈ {1, ... ,K}:
2 Generate βk = (βk,1, ... , βk,V )T ∼ Dir(·  |η)
3 For each document d:
4 For each topic k ∈ {1, ... ,K}
5 Generate Λ(d)
k ∈ {0, 1} ∼ Bernoulli(·  |Φk)
6 Generate α(d) = L(d) × α
7 Generate θ(d) = (θl1,...,θlMd )T ∼ Dir(·  |α(d))
8 For each i in {1, ... ,Nd}:
9 Generate zi ∈ {λ(d)
1 , ... ,λ(d)Md} ∼ Mult(·  |θ(d))
10 Generate wi ∈ {1, ... , V } ∼ Mult(·  |βzi)
</figure>
<figureCaption confidence="0.993123">
Figure 2: L-LDA generative process (Ramage et al. 2009)
</figureCaption>
<equation confidence="0.997159">
P(w|a) Pz P(w|d) = � P(w|t)P(t|d) (4)
t
</equation>
<subsectionHeader confidence="0.991368">
3.3 Labeled LDA
</subsectionHeader>
<bodyText confidence="0.9972476875">
L-LDA (Ramage et al., 2009) extends standard LDA
to include supervision for specific target categories,
yet in a different way: (i) The generative process
includes a second observed variable, i.e. each doc-
ument is explicitly labeled with a target category.
A document may be labeled with an arbitrary num-
ber of categories; unlabeled documents are also pos-
sible. However, L-LDA permits only binary as-
signments of categories to documents; probabilistic
weights over categories are not intended. (ii) Con-
trary to LDA, where the number of topics has to be
specified in advance, L-LDA sets this parameter to
the number of unique target categories. Moreover,
the model is constrained such that documents may
be assigned only those topics that correspond to their
observable category label(s). That is, latent topics
t in the standard formulation of LDA (2) are con-
strained to correspond to explicit labels a.
More specifically, L-LDA extends the generative
process of LDA by constraining the topic distribu-
tions over documents B(d) to only those topics that
correspond to the document’s set of labels A(d). This
is done by projecting the parameter vector of the
Dirichlet topic prior α to a lower-dimensional vec-
tor α(d) whose topic dimensions correspond to the
document labels.
This extension is integrated in steps 5 and 6 of
Fig. 2: First, in step 5, the document’s labels A(d)
are generated for each topic k. The resulting vector
of document’s labels A(d) = {k  |A(d)
k = 11 is used
to define a document-specific label projection matrix
</bodyText>
<page confidence="0.916539">
543
</page>
<equation confidence="0.960421666666667">
L(d)
|λ(d)|×K, such that L(d)
ij = 1 if λ(d)
</equation>
<bodyText confidence="0.9962965">
i = j, and 0 oth-
erwise. This matrix is used in step 6 to project the
Dirichlet topic prior α to a lower-dimensional vec-
tor α(d), whose topic dimensions correspond to the
document labels. Topic proportions are then, in step
7, generated for this reduced parameter space.
In our instantiation of L-LDA, we collect pseudo-
documents for attributes exactly as for C-LDA. Doc-
uments are labeled with exactly one category, the at-
tribute noun. Note that, even though the relationship
between documents and topics is fixed, the one be-
tween topics and words is not. Any word occurring
in more than one document will be assigned a non-
zero probability for each corresponding topic.
Thus, with regard to attribute modeling, C-LDA
and L-LDA build an interesting pair of opposites:
The L-LDA model assumes that attributes are se-
mantically primitive in the sense that they cannot
be decomposed into smaller topical units, whereas
words may be associated with several attributes at
the same time. C-LDA, at the other end of the spec-
trum, licenses semantic variability on both the at-
tribute and the word level. Particularly, a word might
be associated with some of the topics underlying an
attribute, but not with all of them, and an attribute
can be characterized by multiple topics.
</bodyText>
<subsectionHeader confidence="0.965205">
3.4 Vector Space Framework
</subsectionHeader>
<bodyText confidence="0.999709303030303">
For integrating the information obtained from C-
LDA or L-LDA into a distributional VSM, we fol-
low Hartung and Frank (2010): Adjectives and
nouns are modeled as independent semantic vectors
along their relationship to attributes; the most promi-
nent attribute(s) that represent the hidden meaning
of adjective-noun phrases are selected from their
composition (cf. Fig. 1).
The dimensions of the VSM are set to the pre-
selected attributes. Semantic vectors are computed
for all adjectives and nouns occurring at least five
times in the pseudo-documents. Vector component
values vow,a) are derived from the C-LDA and L-
LDA models in different ways: with C-LDA we
obtain P (w|a) by approximation from P (w|d) (cf.
equation (4)), while in L-LDA we obtain P(w|a) di-
rectly from the induced topic-word distribution Ot,
through labeled topics t = a (cf. equation (2)).
Vector composition is defined as vector multipli-
cation (x) or vector addition (+).
For attribute selection on the composed vector, we
use two methods we found to perform best in Har-
tung and Frank (2010): Entropy Selection (ESel)
and Most Prominent Component (MPC). ESel mea-
sures entropy over the vector components to identify
components that encode a high amount of informa-
tion. It selects all attributes that lead to an increase of
entropy when suppressed from the vector represen-
tation. If no informative components can be detected
in a vector due to a very broad, flat distribution of
the probability mass (cf. b in Fig. 1), ESel yields an
empty list. MPC always chooses exactly one vector
component, i.e. the one with the highest value.
</bodyText>
<sectionHeader confidence="0.999445" genericHeader="method">
4 Experimental Settings
</sectionHeader>
<bodyText confidence="0.994749451612903">
Attribute selection over small and large semantic
spaces. We evaluate the performance of the VSMs
based on C-LDA and L-LDA in two experimental
settings, contrasting the problem of attribute selec-
tion on semantic spaces of radically different dimen-
sionality, using sets of 10 vs. 206 attributes.
Evaluation measures. We evaluate against two
gold standards consisting of adjective-noun phrases
(or adjective-noun pairs) and their associated at-
tribute meanings. We report precision, recall and
f1-score. Where appropriate, we test differences in
the performance of various model configurations for
statistical significance in a randomized permutation
test (Yeh, 2000), using the sigf tool (Pad´o, 2006).
Baselines. We compare our models against two
baselines, PATTVSM and DEPVSM. PATTSVM is
reconstructed from Hartung and Frank (2010). It is
grounded in a selection of lexical patterns that iden-
tify the target elements (adjectives and nouns) for
the vector basis elements (i.e., the attribute nouns)
in a local context window. The component values
are defined using raw frequency counts over the ex-
tracted patterns. DEPVSM is similar to PATTVSM;
however, it relies on dependency paths that connect
the target elements and attributes in local contexts.
The paths are identical to the ones used for con-
structing pseudo-documents in C-LDA and L-LDA.
As in PATTVSM, the vector components are set to
raw frequencies over extracted paths.
Implementations. To implement our models, we
rely on MALLET (McCallum, 2002) for C-LDA and
</bodyText>
<page confidence="0.990694">
544
</page>
<bodyText confidence="0.999450707317073">
the Stanford Topic Modeling Toolbox3 for L-LDA.
In both cases, we run 1000 iterations of Gibbs sam-
pling, using default values for all hyperparameters.
Data set for attribute selection over 10 attributes.
The first experiment is conducted on the data set
used in Hartung and Frank (2010). It consists of
100 adjective-noun pairs manually annotated for
ten attributes: COLOR, DIRECTION, DURATION,
SHAPE, SIZE, SMELL, SPEED, TASTE, TEMPER-
ATURE, WEIGHT. To enable comparison, the di-
mensions of our models are set to exactly these at-
tributes.
Data set for attribute selection over a large se-
mantic space (206 attributes). In the second ex-
periment, we max out the attribute selection task
to a much larger set of attributes in order to an-
alyze the difficulty of the task on more represen-
tative data. We automatically construct a data set
of adjective-noun phrases labeled with appropriate
attributes from WordNet 3.0 (Fellbaum, 1998), re-
lying on the assumption that examples given in
glosses correspond to the respective word sense of
the adjective. We first extract all adjectives that
are linked to at least one attribute synset by the
attribute relation. Next, we run the glosses of
these adjectives (3592 in number) through TreeTag-
ger (Schmid, 1994) to find examples of adjectives
modifying nouns in attributive constructions. The
resulting adjective-noun phrases are labeled with the
attribute label linked to the given adjective sense.
This method yields 7901 labeled adjective-noun
phrases. They are divided into development and test
data according to a sampling procedure that respects
the following criteria: (i) Both sets must contain
all attributes with an equal number of phrases for
each attribute; (ii) phrases with both elements con-
tained in CoreWordNet4 are preferred, while others
are only considered if necessary to satisfy the first
criterion. This procedure yields 496/345 phrases
in the development/test set, distributed over 206 at-
tributes5.
</bodyText>
<footnote confidence="0.982074857142857">
3http://nlp.stanford.edu/software/tmt/.
4A subset of WordNet restricted to the 5000 most fre-
quently used word senses. Available from: http://
wordnetcode.princeton.edu/standoff-files/
core-wordnet.txt
5If an attribute provides only one example, this was added
to the development set. Therefore, the test set only comprises
</footnote>
<bodyText confidence="0.998813666666667">
Training data. The pseudo-documents are collec-
ted from dependency paths obtained from section 2
of the parsed pukWaC corpus (Baroni et al., 2009).
</bodyText>
<sectionHeader confidence="0.975718" genericHeader="method">
5 Discussion of Results
</sectionHeader>
<subsectionHeader confidence="0.897345">
5.1 Experiment 1
</subsectionHeader>
<bodyText confidence="0.999944571428572">
In Experiment 1, we evaluate the performance
of C-LDA and L-LDA on the attribute selection
task over 10 attributes against the pattern-based
and dependency-based models PATTVSM and DE-
PVSM as competitive baselines. Besides a com-
parison to standard VSMs, we are especially in-
terested in the relative performance of the LDA
models. Given that C-LDA and L-LDA estimate
attribute-specific topic distributions in the structured
pseudo-documents under different assumptions re-
garding the correspondence of attributes and topics
(cf. Sec. 3.2 and 3.3), we expect the two LDA vari-
ants to differ in their capability to capture the topic
distributions in the labeled pseudo-documents.
</bodyText>
<subsubsectionHeader confidence="0.902693">
5.1.1 Attribute Selection for 10 Attributes
</subsubsectionHeader>
<bodyText confidence="0.991677769230769">
Tables 1 and 2 summarize the results for at-
tribute selection over 10 attributes against the la-
beled adjective-noun pairs in the test set, using ESel
and MPC as selection functions on vectors com-
posed by multiplication (Table 1) and addition (Ta-
ble 2). The results reported for C-LDA correspond
to the best performing model (with number of top-
ics set to 42, as this setting yields the best and most
constant results over both composition operators).
C-LDA shows highest f-scores and recall over all
settings, and highest precision with vector addition.6
In line with Mitchell and Lapata (2010) (cf. Sec. 2),
we obtain the best overall results with vector addi-
tion (ESel: P: 0.55, R: 0.66, F: 0.61; MPC: P: 0.59,
R: 0.71, F: 0.64). The difference between C-LDA
and L-LDA is small but significant for vector mul-
tiplication; for vector addition, it is not significant.
Compared to the LDA models, the VSM baselines
206 attributes, while all models were trained on 262 attributes
obtained from WordNet in the first extraction step.
6In Tables 1 and 2, statistical significance of the differences
between the models is marked by the superscripts L, D and P,
denoting a significant difference over L-LDA, DepVSM and
PattVSM, respectively. All differences reported are significant
at p &lt; 0.05, except for the difference between C-LDA and L-
LDA in Table 3 (p &lt; 0.1).
</bodyText>
<page confidence="0.996657">
545
</page>
<table confidence="0.980398285714286">
F������
P ESel P MPC
R F R F
C-LDA 0.58 0.65 0.61L,P 0.57 0.64 0.60
L-LDA 0.68 0.54 0.60D 0.55 0.61 0.58D
DepVSM 0.48 0.58 0.53P 0.57 0.60 0.58
PattVSM 0.63 0.46 0.54 0.60 0.58 0.59
</table>
<tableCaption confidence="0.993815">
Table 1: Attribute selection over 10 attributes (×)
</tableCaption>
<table confidence="0.999962833333333">
P ESel P MPC
R F R F
C-LDA 0.55 0.66 0.61D,P 0.59 0.71 0.64
L-LDA 0.53 0.57 0.55D,P 0.50 0.45 0.47D,P
DepVSM 0.38 0.65 0.48P 0.57 0.60 0.58
PattVSM 0.71 0.35 0.47 0.47 0.56 0.51
</table>
<tableCaption confidence="0.999511">
Table 2: Attribute selection over 10 attributes (+)
</tableCaption>
<bodyText confidence="0.999388575757576">
are competitive, but tend to perform lower. This ef-
fect is statistically significant for ESel with vector
multiplication: each of the LDA models statistically
significantly outperforms one of the VSM models,
DEPVSM and PATTVSM. With ESel and vector
addition, both LDA models outperform both VSM
models statistically significantly. The LDAESel,+
models outperform the PATTVSMESel,+ model of
Hartung and Frank (2010) by a high margin in
f-score: +0.14 for C-LDA; +0.08 for L-LDA.
Compared to the stronger multiplicative settings
PATTVSMESel,x and PATTVSMMPC,x this still
represents a plus of +0.07 and +0.02 in f-score, re-
spectively. We further observe a clear improvement
of the LDA models over the VSM models in terms of
recall (+0.20, C-LDAESel,+ vs. PATTVSMESel,x),
at the expense of some loss in precision (-0.08, C-
LDAESel,+ vs. PATTVSMESel,x). This clearly con-
firms a stronger generalization power of LDA com-
pared to VSM models.
With regard to selection functions, we observe
that MPC tends to perform better for the VSM mod-
els, while ESel is more suitable in the LDA models.
Figures 3 and 4 display the overall performance
curve ranging over different topic numbers for C-
LDAESel,+ and C-LDAESel,x – compared to the
remaining models that are not dependent on topic
size. For topic numbers smaller than the attribute set
size, C-LDA underperforms, for obvious reasons.
Increasing ranges of topic numbers to 60 does not
show a linear effect on performance. Parameter set-
tings with performance drops below the VSM base-
lines are rare, which holds particularly for vector ad-
</bodyText>
<figure confidence="0.969442">
0 10 20 30 40 50 60
Num. Topics
</figure>
<figureCaption confidence="0.9999015">
Figure 4: Performance of C-LDAESel,+ for different
topic numbers, compared against all other models
</figureCaption>
<bodyText confidence="0.999631066666667">
dition at topic ranges larger than 10. With vector
addition, C-LDA outperforms L-LDA in almost all
configurations, yet at an overall lower performance
level of L-LDA (0.55 with addition vs. 0.6 with mul-
tiplication). Note that in the multiplicative setting,
C-LDA reaches the performance of L-LDA only in
its best configurations, while with vector addition it
obtains high performance that exceeds L-LDA’s top
f-score of 0.6 for topic ranges between 10 and 20.
Based on these observations, vector addition
seems to offer the more robust setting for C-LDA,
the model that is less strict with regard to topic-
attribute correspondences. Vector multiplication, on
the other hand, is more suitable for L-LDA and its
stricter association of topics with class labels.
</bodyText>
<subsectionHeader confidence="0.599538">
5.1.2 Smoothing Power of LDA Models
</subsectionHeader>
<bodyText confidence="0.9789805">
Our hypothesis was that LDA models should be
better suited for dealing with sparse data, compared
</bodyText>
<figure confidence="0.999098357142857">
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
C-LDA
L-LDA
DepVSM
PattVSM
</figure>
<figureCaption confidence="0.9979805">
Figure 3: Performance of C-LDAESel,× for different
topic numbers, compared against all other models
</figureCaption>
<figure confidence="0.9996228125">
0.65
0.6
0.55
0.5
F������ 0.45
0.4
0.35
0.3
0.25
0.2
0 10 20 30 40 50 60
Num. Topics
C-LDA
L-LDA
DepVSM
PattVSM
</figure>
<page confidence="0.993772">
546
</page>
<table confidence="0.999753">
P ESel P MPC
R F R F
C-LDA 0.39 0.31 0.35 0.37 0.27 0.32
L-LDA 0.30 0.18 0.23 0.20 0.18 0.19
DepVSM 0.20 0.10 0.13 0.37 0.26 0.30
PattVSM 0.00 0.00 0.00 0.00 0.00 0.00
</table>
<tableCaption confidence="0.993976">
Table 3: Performance figures on sparse vectors (×)
</tableCaption>
<table confidence="0.999962">
P ESel P MPC
R F R F
C-LDA 0.43 0.33 0.38 0.44 0.28 0.34
L-LDA 0.34 0.16 0.22 0.37 0.18 0.24
DepVSM 0.16 0.17 0.17 0.36 0.21 0.27
PattVSM 0.13 0.04 0.06 0.17 0.25 0.20
</table>
<tableCaption confidence="0.999904">
Table 4: Performance figures on sparse vectors (+)
</tableCaption>
<bodyText confidence="0.999957111111111">
to pattern-based or purely distributional approaches.
While this is broadly confirmed in the above results
by global gains in recall, we conduct a special evalu-
ation focused on those pairs in the test set that suffer
from sparse data. We selected all adjective and noun
vectors that did not yield any positive component
values in the PATTVSM model. The 22 adjective-
noun pairs in the test set affected by these ’zero vec-
tors’ were evaluated using the remaining models.
The results in Tables 3 and 4 yield a very clear pic-
ture: C-LDA obtains highest precision, recall and
f-score across all settings, followed by L-LDA and
DEPVSMESel, while their ranks are reversed when
using MPC. Again, MPC works better for the VSM
models, ESel for the LDA models. Vector addition
performs best for C-LDA with f-scores of 0.38 and
0.34 – outperforming the pattern-based results on
sparse vectors by orders of magnitude.
</bodyText>
<subsectionHeader confidence="0.990872">
5.2 Experiment 2
</subsectionHeader>
<bodyText confidence="0.999859857142857">
Experiment 2 is designed to max out the space of
attributes to be modeled, to assess the capacity of
both LDA models and the DEPVSM baseline model
in the attribute selection task on a large attribute
space.7 In contrast to Experiment 1, with its con-
fined semantic space of 10 target attributes, this rep-
resents a huge undertaking.
</bodyText>
<subsectionHeader confidence="0.683529">
5.2.1 Large-scale Attribute Selection
</subsectionHeader>
<bodyText confidence="0.5034155">
Table 5 (column all) displays the performance of
all models on attribute selection over a range of 206
</bodyText>
<footnote confidence="0.5137555">
7We did not apply PATTVSM to this large-scale experiment,
as only poor performance can be expected.
</footnote>
<table confidence="0.996687">
all property
× + × +
C-LDA 0.04 0.02 0.18L,D 0.10D
L-LDA 0.03 0.04 0.15 0.15
DepVSM 0.02 0.02 0.12 0.07
</table>
<tableCaption confidence="0.974673">
Table 5: Performance figures (in f-score) of C-LDAESel
on 206 (all) and 73 property attributes (property)
</tableCaption>
<table confidence="0.999466181818182">
all property
P R F P R F
WIDTH 0.67 1.00 0.80 1.00 0.50 0.67
WEIGHT 0.80 0.57 0.67 0.50 0.57 0.53
MAGNETISM 0.50 1.00 0.67
SPEED 0.50 0.50 0.50 1.00 0.50 0.67
TEXTURE 0.33 1.00 0.50 0.33 1.00 0.50
DURATION 0.50 0.50 0.50 1.00 1.00 1.00
TEMPERATURE 0.30 0.75 0.43 0.43 0.75 0.55
AGE 0.33 0.50 0.40
THICKNESS 1.00 0.25 0.40 0.50 0.13 0.20
DEGREE 1.00 0.20 0.33
LENGTH 0.17 1.00 0.29 0.50 1.00 0.67
DEPTH 1.00 0.14 0.25 1.00 0.86 0.92
ACTION 0.17 0.50 0.25
LIGHT 0.33 0.17 0.22 0.20 0.17 0.18
POSITION 0.14 0.25 0.18 0.20 0.25 0.22
SHARPNESS 1.00 1.00 1.00
SERIOUSNESS 0.50 1.00 0.67
COLOR 0.13 0.25 0.17 0.29 0.50 0.36
LOYALTY 1.00 1.00 1.00
average 0.49 0.54 0.51 0.63 0.63 0.63
</table>
<tableCaption confidence="0.991238">
Table 6: Attribute selection on 206 attributes (all) and 73
property attributes (property); performance figures of C-
LDAESel,× for best attributes (F&gt;0)
</tableCaption>
<bodyText confidence="0.999970714285714">
dimensions, contrasting vector addition and multi-
plication. The number of topics was set to 400. As
the overall performance is close to 0 for both com-
position methods, no parameter setting can be iden-
tified as particularly suited for this large-scale at-
tribute selection task. The differences between the
three models are very small and not significant8.
</bodyText>
<subsectionHeader confidence="0.780246">
5.2.2 Focused Evaluation and Data Analysis
</subsectionHeader>
<bodyText confidence="0.989068666666667">
To gain a deeper insight into the modeling capac-
ity of the LDA models for this large-scale selection
task, Table 6 (column all) presents a partial evalua-
tion of attributes that could be assigned to adjective-
noun pairs with an f-score &gt;0 by C-LDAESel,×.
Despite the disappointing overall performance of
8Again, statistically significant differences are marked by
superscripts (cf. footnote 6). All differences reported are sig-
nificant at α &lt; 0.05.
</bodyText>
<page confidence="0.998315">
547
</page>
<tableCaption confidence="0.9944535">
Table 7: Sample of correct and false predictions of C-
LDAESel,× in Experiment 2
</tableCaption>
<bodyText confidence="0.988582333333333">
the LDA models on this large attribute space, it is
remarkable that C-LDA is able to induce distinctive
topic distributions for a number of attributes with up
to 0.51 f-score with balanced precision and recall,
a moderate drop of only -0.10 relative to the corre-
sponding model induced over 10 attributes.
Raising the attribute selection task from 10 to 206
attributes poses a true challenge to our models, by
the sheer size and diversity of the semantic space
considered. Table 7 gives an insight into the nature
of the data and the difficulty of the task, by listing
correct and false preditions of C-LDA for a small
sample of adjective-noun pairs. Possible explana-
tions for false predictions are manifold, among them
near misses (e.g. serious book, weakpresident, short
flight, rough bark), idiomatic expressions (e.g. faint
heart, blue line) or questionable labels provided by
WordNet (e.g. serious book).
As seen above, C-LDA achieves relatively high
performance figures on selected attributes (cf. Table
6, col. all). In order to identify what makes these
attributes different from others that resist success-
ful modeling, we investigated three factors: (i) the
amount of training data available for each attribute,
(ii) the ambiguity rate per attribute, and (iii) their
ontological subtype.
(i) Measuring the dependence between training
data size and f-score per attribute shows that a large
amount of training data is generally helpful, but not
the decisive factor (Pearson’s r = 0.19, p &lt; 0.01).
(ii) The ambiguity rate ARattr per attribute attr
is computed by averaging over all test pairs TPattr
labeled with attr, counting the total number of at-
</bodyText>
<equation confidence="0.9961815">
ARattr =
|T Pattr|
</equation>
<bodyText confidence="0.963096813953488">
Correlating this figure with the performance per at-
tribute in terms of f-score yields only a small pos-
itive correlation (Pearson’s r = 0.23, p &lt; 0.01).
In fact, the qualitative analysis in Table 7 shows that
C-LDA is capable of assigning meaningful attributes
to adjective-noun phrases not only in easy, but also
ambiguous cases (cf. shallow water, where DEPTH
is the only attribute provided for shallow in Word-
Net vs. short holiday, short hair or short flight).
(iii) Although the 206 attributes used in Exp. 2 are
rather diverse, including concepts such as HEIGHT,
KINDNESS or INDIVIDUALITY, we observe a high
number of attributes from Exp. 1 that are success-
fully modeled in Exp. 2 (5 out of 10, cf. column
all in Table 6). Given that they are categorized into
the property class in WordNet9, we presume that the
varying performance across attributes might be in-
fluenced by their ontological subtype. This hypoth-
esis is validated in a replication of Exp. 2, with train-
ing data limited to the 73 attributes pertaining to the
property subtype in WordNet. The test set was re-
stricted accordingly, resulting in 112 pairs that are
linked to a property attribute.
The overall performance of the models in this ex-
periment is shown in Table 5 (column property):
With vector multiplication, the best-performing op-
eration across all models, all models benefit consid-
erably (+0.10 or more). C-LDA shows the largest
improvement, significantly outperforming both L-
LDA and DEPVSM. With vector addition, the per-
formance gains are slightly lower in general. In
this setting, L-LDA shows higher f-score than C-
LDA, though this difference is not statistically sig-
nificant. Still, C-LDA significantly outranges DE-
PVSM. Note that we can not show a significant dif-
ference between C-LDAESel,x and L-LDAESel,+,
so the comparison between these models remains in-
conclusive here. Note further that the affinity of C-
LDA with vector addition and L-LDA with vector
multiplication, respectively, is inverted in the large-
scale experiment (cf. Table 5).
9WordNet separates attributes into properties, qualities and
states, among several others.
</bodyText>
<figure confidence="0.999148477272727">
prediction correct
thin layer
heavyload
shallow water
short holiday
attractive force
short hair
serious book
blue line
weak president
fluid society
short flight
rough bark
faint heart
THICKNESS
WEIGHT
DEPTH
DURATION
MAGNETISM
LENGTH
DIFFICULTY
COLOR
POSITION
REPUTE
DISTANCE
TEXTURE
CONSTANCY
THICKNESS
WEIGHT
DEPTH
DURATION
MAGNETISM
LENGTH
MIND
UNION
POWER
CHANGEABLENESS
DURATION
EVENNESS
COWARDICE
tributes attr′ that are associated with each adjective
in pairs (adj, n) E TPattr in WordNet:
F- F- (adj,n)ETPattr |(adj, attr′)WN|
attr′
</figure>
<page confidence="0.985665">
548
</page>
<bodyText confidence="0.99998192">
While these overall results are far from satisfac-
tory, they still clearly indicate that the LDA models
work effectively for at least a subset of attributes,
and outperform the VSM baseline.
Again, a more detailed analysis is given in Ta-
ble 6 (column property), showing the performance
of the best individual property attributes (F&gt;0) in
the restricted experiment. Average performance of
the best property attributes with F&gt;0, individually,
amounts to F=0.6310. In comparison to the unres-
tricted setting (cf. column all), nearly all property
attributes benefit from model training on selective
data. Exceptions are WIDTH, WEIGHT, THICKNESS,
AGE, DEGREE and LIGHT. Thus, apparently, some
of the adjectives associated with non-property at-
tributes in the full set provide some discriminative
power that is helpful to distinguish property types.
In a qualitative analysis of the 133 non-property
attributes filtered out in this experiment, we find that
the WordNet-SUMO mapping (Niles, 2003) does
not provide differentiating definitions for about 60%
of these attributes, linking them instead to a single
subjective assessment attribute. This suggests that
in many cases the distinctions drawn by WordNet
are too subtle even for humans to reproduce.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.991967820895523">
This paper explored the use of LDA topic models
in a semantic labeling task that predicts attributes
as ’hidden’ meanings in the compositional seman-
tics of adjective-noun phrases. LDA topic models
are expected to alleviate sparsity problems of dis-
tributional VSMs as encountered in prior work, by
incorporating latent semantic information about at-
tribute nouns. We investigated two variants of LDA
that employ different degrees of supervision for as-
sociating topics with attributes.
Our contributions are as follows. We proposed
two LDA models for the attribute selection task that
import supervision for a target category parameter
in different ways: L-LDA (Ramage et al., 2009)
embeds the target categories into the LDA process,
by defining a 1:1 correspondence of topics and tar-
get categories. C-LDA, by contrast, does not af-
fect the LDA generative process. Here, we heuris-
10In comparison, L-LDAESel,× yields an average f-score of
0.47 for attributes with F&gt;0 in the property setting.
tically equate pseudo-documents with target cate-
gories, to approximate category-specific word-topic
distributions. By adhering to standard LDA, C-LDA
accommodates a greater variety in the distributions
of topics to attribute-specific documents and words,
as compared to L-LDA. Combining standard LDA
topic modeling with a means of interpreting the in-
duced topics relative to a set of external categories,
C-LDA offers greater flexibility and expressiveness.
Our experimental results show that modeling at-
tributes as latent or explicit topics with C-LDA and
L-LDA, respectively, outperforms the purely distri-
butional baseline model DEPVSM and PATTVSM
of prior work. Targeted evaluation on sparse data
points confirms that LDA models help to overcome
inherent sparsity effects of VSMs. C-LDA and L-
LDA are close in performance in Experiment 1. C-
LDA outperforms L-LDA only with optimal topic
parameter settings.
Finally, we probed the modeling capacity of LDA
and VSM models on a vast space of 206 attributes.
This task proved to be extremely difficult. However,
we obtain respectable results on a subset of attributes
denoting properties, where C-LDA performs best in
quantitative performance measures. It yields high-
est f-scores in full and partial evaluation – both with
the full-size attribute model, and when training and
testing is restricted to property attributes. The differ-
ences are small, but statistically significant between
the LDA models and the VSM baseline in a setting
restricted to property attributes.
Data analysis indicates that our models perform
more robustly on concrete attributes in contrast to
abstract attribute types that lack clear categorization.
This suggests that our approach to attribute selec-
tion is most appropriate for detecting attributes that
reflect clear ontological distinctions.
However, there is ample space for improvement.
In Hartung and Frank (2011), we show that the
quality of the noun vectors lags behind the adjec-
tive vectors. This clearly affects the performance
of our models in cases where the semantic contri-
bution of the noun is decisive for disambiguation.
Future work will focus on ways to enhance the noun
vector representations through additional contextual
features, to make them denser and more articulated
in structure.
</bodyText>
<page confidence="0.997838">
549
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999600074766355">
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Ph.D. Dissertation, Department of Com-
puter Science, University of Essex.
Marco Baroni and Alessandro Lenci. 2010. Distri-
butional Memory. A General Framework for Corpus-
based Semantics. Computational Linguistics, 36:673–
721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, East
Stroudsburg, PA, pages 1183–1193.
M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.
2009. The WaCky Wide Web: A Collection of Very
Large Linguistically Processed Web-Crawled Corpora.
Language Resources and Evaluation, 43:209–226.
D. Blei and J. McAuliffe. 2007. Supervised topic mod-
els. Neural Information Processing Systems, 21.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet allocation. JMLR, 3:993–1022.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Semantics,
Stroudsburg, PA. Association for Computational Lin-
guistics.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley.
Matthias Hartung and Anette Frank. 2010. A Structured
Vector Space Model for Hidden Attribute Meaning in
Adjective-Noun Phrases. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING), Beijing, China, August.
Matthias Hartung and Anette Frank. 2011. Assessing in-
terpretable, attribute-related meaning representations
for adjective-noun phrases in a similarity prediction
task. In Proceedings of GEometrical Models of Nat-
ural Language Semantics (GEMS-2011), Edinburgh,
UK.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.
2008. DiscLDA: Discriminative Learning for Dimen-
sionality Reduction and Classification. In NIPS, vol-
ume 22.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
token-based idiom detection. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ’10, pages 1138–1147, Upp-
sala, Sweden.
Andrew Kachites McCallum. 2002. MAL-
LET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
Models of Semantic Composition. In Proceedings of
ACL-08: HLT, pages 236–244, Columbus, Ohio, June.
Jeff Mitchell and Mirella Lapata. 2009. Language Mod-
els Based on Semantic Composition. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, Singapore, August 2009,
pages 430–439, Singapore, August.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34:1388–1429.
Ian Niles. 2003. Mapping WordNet to the SUMO Ontol-
ogy. In Proceedings of the IEEE International Knowl-
edge Engineering conference, pages 23–26, June.
Diarmuid O´ S´eaghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 435–444, Uppsala, Sweden, July.
Association for Computational Linguistics.
Sebastian Pad´o, 2006. User’s guide to sigf: Signifi-
cance testing by approximate randomisation.
D. Prescher, S. Riezler, and M. Rooth. 2000. Using a
probabilistic class-based lexicon for lexical ambiguity
resolution. In Proceedings of the 18th COLING, pages
649–655.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Singapore, August 2009, pages 248–256.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation Method for Selectional Pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 424–434, Uppsala, Sweden, July. Association
for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010. Com-
positional matrix-space models of language. In Pro-
ceedings ofthe 48th Annual Meeting ofthe Association
for Computational Linguistics, pages 907–916. Asso-
ciation for Computational Linguistics, July.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing (NeMLaP). Manchester, U.K., 14–
16 September 1994.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141–
188.
</reference>
<page confidence="0.968307">
550
</page>
<reference confidence="0.990828166666667">
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the Fourth Conference on Computational Lan-
guage Learning (CoNLL-2000) and the Second Learn-
ing Language in Logic Workshop, Lisbon, Portugal,
pages 947–953.
</reference>
<page confidence="0.998035">
551
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.533703">
<title confidence="0.983659">Exploring Supervised LDA Models for Assigning Attributes Adjective-Noun Phrases</title>
<author confidence="0.707928">Hartung</author>
<affiliation confidence="0.820284">Computational Linguistics</affiliation>
<address confidence="0.70134">Heidelberg</address>
<abstract confidence="0.997497826086957">This paper introduces an attribute selection task as a way to characterize the inherent meaning of property-denoting adjectives in adjecphrases, such as e.g. sumthe attribute than We formulate this task in a vector space model that represents adjectives and nouns as vectors in a semantic space defined over possible attributes. The vectors incorporate latent semantic information obtained from two variants of LDA topic models. Our LDA models outperform previous approaches on a small set of 10 attributes with considerable gains on sparse representations, which highlights the strong smoothing power of LDA models. For the first time, we extend the attribute selection task to a new data set with more than 200 classes. We observe that large-scale attribute selection is a hard problem, but a subset of attributes performs robustly on the large scale as well. Again, the LDA models outperform the VSM baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abdulrahman Almuhareb</author>
</authors>
<title>Attributes in Lexical Acquisition.</title>
<date>2006</date>
<tech>Ph.D. Dissertation,</tech>
<institution>Department of Computer Science, University of Essex.</institution>
<contexts>
<context position="2096" citStr="Almuhareb, 2006" startWordPosition="315" endWordPosition="316">ensional cooccurrence data (Turney and Pantel, 2010; Baroni and Lenci, 2010, i.a.), as well as latent variable models (LVMs) which aggregate distributional observations in ’hidden’, or latent variables, thereby reducing the dimensionality of the data. An example of the latter are topic models (Blei et al., 2003), which have recently been applied to modeling selectional preferences of verbs (Ritter et al., 2010; O´ S´eaghdha, 2010), or word sense disambiguation (Li et al., 2010). A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both in isolation (Almuhareb, 2006) and in compositional adjective-noun phrases (Hartung and Frank, 2010; Guevara, 2010; Baroni and Zamparelli, 2010). In this paper, we propose a new approach to a problem we denote as attribute selection: The task is to predict the hidden attribute meaning expressed by a property-denoting adjective in composition with a noun. The adjective hot, e.g., may denote attributes such as TEMPERATURE, TASTE or EMOTIONALITY. These adjective meanings can be combined with nouns such as tea, soup or debate, which can be characterized in terms of attributes as well. The goal of the task is to determine the h</context>
<context position="4227" citStr="Almuhareb (2006)" startWordPosition="658" endWordPosition="659">en’ attribute meaning of the phrase can be ‘selected’ as a prominent component from the composed vector. This is illustrated in Fig. 1 for the adjective enormous (~e) in combination with the noun ball (~b), with alternative composition operations: vector multiplication (×) and addition (+).1 Both yield SIZE as the most prominent component in the composed vector. In the present paper we offer a new approach to this formalization of the compositional meaning of adjectives and nouns that owes to both distributional VSMs and LVMs. Through this combination, we attempt to improve on earlier work in Almuhareb (2006) and Hartung and Frank (2010), which are both embedded in a purely distributional setting. Specifically, we use Latent Dirichlet Allocation (LDA; Blei et al. (2003)) to train an attribute model that captures semantic information encoded in adjectives and nouns independently of one another. Following Hartung and Frank (2010), this model is embedded into a VSM that employs vector composition to combine the meaning of adjectives and nouns. We present two variants of LDA that differ in the way attributes are associated with the induced LDA topics: Controled LDA (C-LDA) and Labeled LDA (L-LDA; Rama</context>
<context position="6291" citStr="Almuhareb (2006)" startWordPosition="997" endWordPosition="998">onal models of adjective semantics, and introduces the two frameworks in which we ground our approach: LVMs and VSMs. In Section 3 we introduce two LDA models for attribute selection: C-LDA and L-LDA. Section 4 describes the settings for two experiments: In the first experiment, we perform attribute selection confined to a space of 10 attributes to compare against prior work. In the second setting we perform attribute selection on a large scale, using 206 attributes. Section 5 presents and discusses the results. Section 6 concludes. 2 Related Work Distributional models of adjective semantics. Almuhareb (2006) aims at capturing the relationship between adjectives and attributes based on lexicosyntactic patterns, such as the ATTR of the * is ADJ. Apart from inherent sparsity issues, his approach does not account for the compositional nature of the problem, as the contextual information contributed by a noun is neglected: For instance, his model is unable to predict that hot is unlikely to denote TASTE in the context of summer, other than in hot meal. Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara</context>
</contexts>
<marker>Almuhareb, 2006</marker>
<rawString>Abdulrahman Almuhareb. 2006. Attributes in Lexical Acquisition. Ph.D. Dissertation, Department of Computer Science, University of Essex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional Memory. A General Framework for Corpusbased Semantics. Computational Linguistics,</title>
<date>2010</date>
<pages>36--673</pages>
<contexts>
<context position="1555" citStr="Baroni and Lenci, 2010" startWordPosition="229" endWordPosition="232"> attribute selection task to a new data set with more than 200 classes. We observe that large-scale attribute selection is a hard problem, but a subset of attributes performs robustly on the large scale as well. Again, the LDA models outperform the VSM baseline. 1 Introduction Corpus-based statistical modeling of semantics is gaining increased attention in computational linguistics. This field of research includes distributional vector space models (VSMs), i.e., models that represent the semantics of words or phrases as vectors over high-dimensional cooccurrence data (Turney and Pantel, 2010; Baroni and Lenci, 2010, i.a.), as well as latent variable models (LVMs) which aggregate distributional observations in ’hidden’, or latent variables, thereby reducing the dimensionality of the data. An example of the latter are topic models (Blei et al., 2003), which have recently been applied to modeling selectional preferences of verbs (Ritter et al., 2010; O´ S´eaghdha, 2010), or word sense disambiguation (Li et al., 2010). A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both in isolation (Almuhareb, 2006) and in compositional adjective-noun phrases (Hartung and F</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional Memory. A General Framework for Corpusbased Semantics. Computational Linguistics, 36:673– 721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<location>East Stroudsburg, PA,</location>
<contexts>
<context position="2210" citStr="Baroni and Zamparelli, 2010" startWordPosition="328" endWordPosition="331">variable models (LVMs) which aggregate distributional observations in ’hidden’, or latent variables, thereby reducing the dimensionality of the data. An example of the latter are topic models (Blei et al., 2003), which have recently been applied to modeling selectional preferences of verbs (Ritter et al., 2010; O´ S´eaghdha, 2010), or word sense disambiguation (Li et al., 2010). A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both in isolation (Almuhareb, 2006) and in compositional adjective-noun phrases (Hartung and Frank, 2010; Guevara, 2010; Baroni and Zamparelli, 2010). In this paper, we propose a new approach to a problem we denote as attribute selection: The task is to predict the hidden attribute meaning expressed by a property-denoting adjective in composition with a noun. The adjective hot, e.g., may denote attributes such as TEMPERATURE, TASTE or EMOTIONALITY. These adjective meanings can be combined with nouns such as tea, soup or debate, which can be characterized in terms of attributes as well. The goal of the task is to determine the hidden attribute meaning predicated over the noun in a given adjective-noun phrase, as illustrated in (1). (1) a. a</context>
<context position="6879" citStr="Baroni and Zamparelli (2010)" startWordPosition="1092" endWordPosition="1095">of adjective semantics. Almuhareb (2006) aims at capturing the relationship between adjectives and attributes based on lexicosyntactic patterns, such as the ATTR of the * is ADJ. Apart from inherent sparsity issues, his approach does not account for the compositional nature of the problem, as the contextual information contributed by a noun is neglected: For instance, his model is unable to predict that hot is unlikely to denote TASTE in the context of summer, other than in hot meal. Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. While these works adhere to a purely latent representation of meaning, Hartung and Frank (2010) include attributes as symbolic ‘hidden’ meanings of adjectives, nouns and adjective-noun phrases in a distributional VSM. Finally, a large body of work dealing with compositionality in distributional frameworks is not confined to the special case of adjective-noun composition (Mitchell and Lapata (2008), Rudolph and Giesbrecht (2010), i.a.). All these approaches regard composition as a</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, East Stroudsburg, PA, pages 1183–1193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>S Bernardini</author>
<author>A Ferraresi</author>
<author>E Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--209</pages>
<contexts>
<context position="23607" citStr="Baroni et al., 2009" startWordPosition="3787" endWordPosition="3790">dered if necessary to satisfy the first criterion. This procedure yields 496/345 phrases in the development/test set, distributed over 206 attributes5. 3http://nlp.stanford.edu/software/tmt/. 4A subset of WordNet restricted to the 5000 most frequently used word senses. Available from: http:// wordnetcode.princeton.edu/standoff-files/ core-wordnet.txt 5If an attribute provides only one example, this was added to the development set. Therefore, the test set only comprises Training data. The pseudo-documents are collected from dependency paths obtained from section 2 of the parsed pukWaC corpus (Baroni et al., 2009). 5 Discussion of Results 5.1 Experiment 1 In Experiment 1, we evaluate the performance of C-LDA and L-LDA on the attribute selection task over 10 attributes against the pattern-based and dependency-based models PATTVSM and DEPVSM as competitive baselines. Besides a comparison to standard VSMs, we are especially interested in the relative performance of the LDA models. Given that C-LDA and L-LDA estimate attribute-specific topic distributions in the structured pseudo-documents under different assumptions regarding the correspondence of attributes and topics (cf. Sec. 3.2 and 3.3), we expect th</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta. 2009. The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora. Language Resources and Evaluation, 43:209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>J McAuliffe</author>
</authors>
<title>Supervised topic models.</title>
<date>2007</date>
<booktitle>Neural Information Processing Systems,</booktitle>
<pages>21</pages>
<contexts>
<context position="8580" citStr="Blei and McAuliffe, 2007" startWordPosition="1380" endWordPosition="1383"> a mixture over latent topics, where each topic is a probability distribution over words (Blei et al., 2003). These topics can be used as dense features for, e.g., document clustering. Depending on the number of topics, which has to be pre-specified, the dimensionality of the document representation can be considerably reduced in comparison to simple bag-of-words models. The remainder of this paper will assume some familiarity with LDA and the LDA terminology as introduced in Blei et al. (2003). Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al., 2009) or DiscLDA (Lacoste-Julien et al., 2008). We will discuss L-LDA in Section 3. Distributional VSMs and TMs. The idea to integrate topic models and VSMs goes back to Mitchell and Lapata (2009) who build a distributional model with dimensions set to topics over bag-of-words features. In their setting, LDA merely serves the purpose of dimensionality reduction, whereas our particular motivation is to use topics as probabilistic indicators for the prediction of attributes as semantic target categories in adjective-noun composition. Mitchell and Lapata (201</context>
</contexts>
<marker>Blei, McAuliffe, 2007</marker>
<rawString>D. Blei and J. McAuliffe. 2007. Supervised topic models. Neural Information Processing Systems, 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<date>2003</date>
<booktitle>Latent dirichlet allocation. JMLR,</booktitle>
<pages>3--993</pages>
<contexts>
<context position="1793" citStr="Blei et al., 2003" startWordPosition="266" endWordPosition="269">rm the VSM baseline. 1 Introduction Corpus-based statistical modeling of semantics is gaining increased attention in computational linguistics. This field of research includes distributional vector space models (VSMs), i.e., models that represent the semantics of words or phrases as vectors over high-dimensional cooccurrence data (Turney and Pantel, 2010; Baroni and Lenci, 2010, i.a.), as well as latent variable models (LVMs) which aggregate distributional observations in ’hidden’, or latent variables, thereby reducing the dimensionality of the data. An example of the latter are topic models (Blei et al., 2003), which have recently been applied to modeling selectional preferences of verbs (Ritter et al., 2010; O´ S´eaghdha, 2010), or word sense disambiguation (Li et al., 2010). A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both in isolation (Almuhareb, 2006) and in compositional adjective-noun phrases (Hartung and Frank, 2010; Guevara, 2010; Baroni and Zamparelli, 2010). In this paper, we propose a new approach to a problem we denote as attribute selection: The task is to predict the hidden attribute meaning expressed by a property-denoting adjectiv</context>
<context position="4391" citStr="Blei et al. (2003)" startWordPosition="681" endWordPosition="684">(~e) in combination with the noun ball (~b), with alternative composition operations: vector multiplication (×) and addition (+).1 Both yield SIZE as the most prominent component in the composed vector. In the present paper we offer a new approach to this formalization of the compositional meaning of adjectives and nouns that owes to both distributional VSMs and LVMs. Through this combination, we attempt to improve on earlier work in Almuhareb (2006) and Hartung and Frank (2010), which are both embedded in a purely distributional setting. Specifically, we use Latent Dirichlet Allocation (LDA; Blei et al. (2003)) to train an attribute model that captures semantic information encoded in adjectives and nouns independently of one another. Following Hartung and Frank (2010), this model is embedded into a VSM that employs vector composition to combine the meaning of adjectives and nouns. We present two variants of LDA that differ in the way attributes are associated with the induced LDA topics: Controled LDA (C-LDA) and Labeled LDA (L-LDA; Ramage et al. (2009)). Both will be presented in detail in Section 3. Our aims in this paper are two-fold: (i) We investigate LDA as a modeling framework in the attribu</context>
<context position="8063" citStr="Blei et al., 2003" startWordPosition="1299" endWordPosition="1302">approaches regard composition as a process combining vectors (or matrices, resp.) to yield a new, contextualized vector representation within the same semantic space. Latent Dirichlet Allocation, aka. Topic Models (TMs). LDA is a generative probabilistic model e 21 1 1 0 1 45 0 4 0 0 2 0 0 0 0 0 0 0 2 0 0 0 b e× e+b 20 420 41 20 20 21 45 180 49 38 38 39 14 14 15 26 1170 71 COLOR DIRECTION DURATION SHAPE SIZE SMELL SPEED TASTE TEMPERATURE WEIGHT 541 for document collections. Each document is represented as a mixture over latent topics, where each topic is a probability distribution over words (Blei et al., 2003). These topics can be used as dense features for, e.g., document clustering. Depending on the number of topics, which has to be pre-specified, the dimensionality of the document representation can be considerably reduced in comparison to simple bag-of-words models. The remainder of this paper will assume some familiarity with LDA and the LDA terminology as introduced in Blei et al. (2003). Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al., 2009) or DiscLDA (Lacoste-Julien et al., 2008</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent dirichlet allocation. JMLR, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="2180" citStr="Guevara, 2010" startWordPosition="326" endWordPosition="327">well as latent variable models (LVMs) which aggregate distributional observations in ’hidden’, or latent variables, thereby reducing the dimensionality of the data. An example of the latter are topic models (Blei et al., 2003), which have recently been applied to modeling selectional preferences of verbs (Ritter et al., 2010; O´ S´eaghdha, 2010), or word sense disambiguation (Li et al., 2010). A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both in isolation (Almuhareb, 2006) and in compositional adjective-noun phrases (Hartung and Frank, 2010; Guevara, 2010; Baroni and Zamparelli, 2010). In this paper, we propose a new approach to a problem we denote as attribute selection: The task is to predict the hidden attribute meaning expressed by a property-denoting adjective in composition with a noun. The adjective hot, e.g., may denote attributes such as TEMPERATURE, TASTE or EMOTIONALITY. These adjective meanings can be combined with nouns such as tea, soup or debate, which can be characterized in terms of attributes as well. The goal of the task is to determine the hidden attribute meaning predicated over the noun in a given adjective-noun phrase, a</context>
<context position="6898" citStr="Guevara (2010)" startWordPosition="1097" endWordPosition="1098"> (2006) aims at capturing the relationship between adjectives and attributes based on lexicosyntactic patterns, such as the ATTR of the * is ADJ. Apart from inherent sparsity issues, his approach does not account for the compositional nature of the problem, as the contextual information contributed by a noun is neglected: For instance, his model is unable to predict that hot is unlikely to denote TASTE in the context of summer, other than in hot meal. Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. While these works adhere to a purely latent representation of meaning, Hartung and Frank (2010) include attributes as symbolic ‘hidden’ meanings of adjectives, nouns and adjective-noun phrases in a distributional VSM. Finally, a large body of work dealing with compositionality in distributional frameworks is not confined to the special case of adjective-noun composition (Mitchell and Lapata (2008), Rudolph and Giesbrecht (2010), i.a.). All these approaches regard composition as a process combining </context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, Stroudsburg, PA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley.</publisher>
<contexts>
<context position="12852" citStr="Harris, 1968" startWordPosition="2050" endWordPosition="2051"> common words in their pseudodocuments. Consequently, the final attribute model is expected to assign a (small) positive probability to the relation between mature and MATURITY without observing it in the training data. 542 3.2 Controled LDA The generative story behind C-LDA is equivalent to standard LDA. However, the collection of pseudodocuments used as input to C-LDA is structured in a controled way such that each document conveys semantic information that specifically characterizes the individual categories of interest (attributes, in our case). In line with the distributional hypothesis (Harris, 1968), we consider the pseudo-documents constructed in this way as distributional fingerprints of the meaning of the corresponding attribute. The contents of the pseudo-documents are selected along syntactic dependency paths linking each attribute noun to meaningful context words (adjectives and nouns).2 A corpus consisting of the two sentences in (3), e.g., yields a pseudo-document for the attribute noun SPEED containing car and fast. (3) What is the speed of this car? The machine runs at a very fast speed. Though we are ultimately interested in triples of attributes, adjectives and nouns that def</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Zellig Harris. 1968. Mathematical Structures of Language. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Hartung</author>
<author>Anette Frank</author>
</authors>
<title>A Structured Vector Space Model for Hidden Attribute Meaning in Adjective-Noun Phrases.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<location>Beijing, China,</location>
<contexts>
<context position="2165" citStr="Hartung and Frank, 2010" startWordPosition="322" endWordPosition="325">d Lenci, 2010, i.a.), as well as latent variable models (LVMs) which aggregate distributional observations in ’hidden’, or latent variables, thereby reducing the dimensionality of the data. An example of the latter are topic models (Blei et al., 2003), which have recently been applied to modeling selectional preferences of verbs (Ritter et al., 2010; O´ S´eaghdha, 2010), or word sense disambiguation (Li et al., 2010). A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both in isolation (Almuhareb, 2006) and in compositional adjective-noun phrases (Hartung and Frank, 2010; Guevara, 2010; Baroni and Zamparelli, 2010). In this paper, we propose a new approach to a problem we denote as attribute selection: The task is to predict the hidden attribute meaning expressed by a property-denoting adjective in composition with a noun. The adjective hot, e.g., may denote attributes such as TEMPERATURE, TASTE or EMOTIONALITY. These adjective meanings can be combined with nouns such as tea, soup or debate, which can be characterized in terms of attributes as well. The goal of the task is to determine the hidden attribute meaning predicated over the noun in a given adjective</context>
<context position="4256" citStr="Hartung and Frank (2010)" startWordPosition="661" endWordPosition="664"> of the phrase can be ‘selected’ as a prominent component from the composed vector. This is illustrated in Fig. 1 for the adjective enormous (~e) in combination with the noun ball (~b), with alternative composition operations: vector multiplication (×) and addition (+).1 Both yield SIZE as the most prominent component in the composed vector. In the present paper we offer a new approach to this formalization of the compositional meaning of adjectives and nouns that owes to both distributional VSMs and LVMs. Through this combination, we attempt to improve on earlier work in Almuhareb (2006) and Hartung and Frank (2010), which are both embedded in a purely distributional setting. Specifically, we use Latent Dirichlet Allocation (LDA; Blei et al. (2003)) to train an attribute model that captures semantic information encoded in adjectives and nouns independently of one another. Following Hartung and Frank (2010), this model is embedded into a VSM that employs vector composition to combine the meaning of adjectives and nouns. We present two variants of LDA that differ in the way attributes are associated with the induced LDA topics: Controled LDA (C-LDA) and Labeled LDA (L-LDA; Ramage et al. (2009)). Both will </context>
<context position="7090" citStr="Hartung and Frank (2010)" startWordPosition="1125" endWordPosition="1129"> his approach does not account for the compositional nature of the problem, as the contextual information contributed by a noun is neglected: For instance, his model is unable to predict that hot is unlikely to denote TASTE in the context of summer, other than in hot meal. Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. While these works adhere to a purely latent representation of meaning, Hartung and Frank (2010) include attributes as symbolic ‘hidden’ meanings of adjectives, nouns and adjective-noun phrases in a distributional VSM. Finally, a large body of work dealing with compositionality in distributional frameworks is not confined to the special case of adjective-noun composition (Mitchell and Lapata (2008), Rudolph and Giesbrecht (2010), i.a.). All these approaches regard composition as a process combining vectors (or matrices, resp.) to yield a new, contextualized vector representation within the same semantic space. Latent Dirichlet Allocation, aka. Topic Models (TMs). LDA is a generative prob</context>
<context position="13663" citStr="Hartung and Frank (2010)" startWordPosition="2176" endWordPosition="2179">along syntactic dependency paths linking each attribute noun to meaningful context words (adjectives and nouns).2 A corpus consisting of the two sentences in (3), e.g., yields a pseudo-document for the attribute noun SPEED containing car and fast. (3) What is the speed of this car? The machine runs at a very fast speed. Though we are ultimately interested in triples of attributes, adjectives and nouns that define the compositional semantics of adjective-noun phrases (cf. (1)), C-LDA is only exposed to binary tuples between attributes and adjectives or nouns, respectively. This is in line with Hartung and Frank (2010), who obtained substantial performance improvements by splitting the ternary relation into two binary relations. Presenting LDA with pseudo-documents that characterize individual target attributes imports supervision into the LDA process in two respects: the estimated topic proportions P(t|d) will be highly attribute-specific, and similarly so for the topic distributions P(w|t). This makes the model more expressive for the ultimate labeling task. Moreover, since C-LDA collects pseudo-documents focused on individual target attributes, we are able to link external categories to the generative pr</context>
<context position="18155" citStr="Hartung and Frank (2010)" startWordPosition="2940" endWordPosition="2943">that attributes are semantically primitive in the sense that they cannot be decomposed into smaller topical units, whereas words may be associated with several attributes at the same time. C-LDA, at the other end of the spectrum, licenses semantic variability on both the attribute and the word level. Particularly, a word might be associated with some of the topics underlying an attribute, but not with all of them, and an attribute can be characterized by multiple topics. 3.4 Vector Space Framework For integrating the information obtained from CLDA or L-LDA into a distributional VSM, we follow Hartung and Frank (2010): Adjectives and nouns are modeled as independent semantic vectors along their relationship to attributes; the most prominent attribute(s) that represent the hidden meaning of adjective-noun phrases are selected from their composition (cf. Fig. 1). The dimensions of the VSM are set to the preselected attributes. Semantic vectors are computed for all adjectives and nouns occurring at least five times in the pseudo-documents. Vector component values vow,a) are derived from the C-LDA and LLDA models in different ways: with C-LDA we obtain P (w|a) by approximation from P (w|d) (cf. equation (4)), </context>
<context position="20486" citStr="Hartung and Frank (2010)" startWordPosition="3305" endWordPosition="3308">ntic spaces of radically different dimensionality, using sets of 10 vs. 206 attributes. Evaluation measures. We evaluate against two gold standards consisting of adjective-noun phrases (or adjective-noun pairs) and their associated attribute meanings. We report precision, recall and f1-score. Where appropriate, we test differences in the performance of various model configurations for statistical significance in a randomized permutation test (Yeh, 2000), using the sigf tool (Pad´o, 2006). Baselines. We compare our models against two baselines, PATTVSM and DEPVSM. PATTSVM is reconstructed from Hartung and Frank (2010). It is grounded in a selection of lexical patterns that identify the target elements (adjectives and nouns) for the vector basis elements (i.e., the attribute nouns) in a local context window. The component values are defined using raw frequency counts over the extracted patterns. DEPVSM is similar to PATTVSM; however, it relies on dependency paths that connect the target elements and attributes in local contexts. The paths are identical to the ones used for constructing pseudo-documents in C-LDA and L-LDA. As in PATTVSM, the vector components are set to raw frequencies over extracted paths. </context>
<context position="26607" citStr="Hartung and Frank (2010)" startWordPosition="4281" endWordPosition="4284"> C-LDA 0.55 0.66 0.61D,P 0.59 0.71 0.64 L-LDA 0.53 0.57 0.55D,P 0.50 0.45 0.47D,P DepVSM 0.38 0.65 0.48P 0.57 0.60 0.58 PattVSM 0.71 0.35 0.47 0.47 0.56 0.51 Table 2: Attribute selection over 10 attributes (+) are competitive, but tend to perform lower. This effect is statistically significant for ESel with vector multiplication: each of the LDA models statistically significantly outperforms one of the VSM models, DEPVSM and PATTVSM. With ESel and vector addition, both LDA models outperform both VSM models statistically significantly. The LDAESel,+ models outperform the PATTVSMESel,+ model of Hartung and Frank (2010) by a high margin in f-score: +0.14 for C-LDA; +0.08 for L-LDA. Compared to the stronger multiplicative settings PATTVSMESel,x and PATTVSMMPC,x this still represents a plus of +0.07 and +0.02 in f-score, respectively. We further observe a clear improvement of the LDA models over the VSM models in terms of recall (+0.20, C-LDAESel,+ vs. PATTVSMESel,x), at the expense of some loss in precision (-0.08, CLDAESel,+ vs. PATTVSMESel,x). This clearly confirms a stronger generalization power of LDA compared to VSM models. With regard to selection functions, we observe that MPC tends to perform better f</context>
</contexts>
<marker>Hartung, Frank, 2010</marker>
<rawString>Matthias Hartung and Anette Frank. 2010. A Structured Vector Space Model for Hidden Attribute Meaning in Adjective-Noun Phrases. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Hartung</author>
<author>Anette Frank</author>
</authors>
<title>Assessing interpretable, attribute-related meaning representations for adjective-noun phrases in a similarity prediction task.</title>
<date>2011</date>
<booktitle>In Proceedings of GEometrical Models of Natural Language Semantics (GEMS-2011),</booktitle>
<location>Edinburgh, UK.</location>
<marker>Hartung, Frank, 2011</marker>
<rawString>Matthias Hartung and Anette Frank. 2011. Assessing interpretable, attribute-related meaning representations for adjective-noun phrases in a similarity prediction task. In Proceedings of GEometrical Models of Natural Language Semantics (GEMS-2011), Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Lacoste-Julien</author>
<author>Fei Sha</author>
<author>Michael I Jordan</author>
</authors>
<title>DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification.</title>
<date>2008</date>
<booktitle>In NIPS,</booktitle>
<volume>22</volume>
<contexts>
<context position="8664" citStr="Lacoste-Julien et al., 2008" startWordPosition="1393" endWordPosition="1396">r words (Blei et al., 2003). These topics can be used as dense features for, e.g., document clustering. Depending on the number of topics, which has to be pre-specified, the dimensionality of the document representation can be considerably reduced in comparison to simple bag-of-words models. The remainder of this paper will assume some familiarity with LDA and the LDA terminology as introduced in Blei et al. (2003). Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al., 2009) or DiscLDA (Lacoste-Julien et al., 2008). We will discuss L-LDA in Section 3. Distributional VSMs and TMs. The idea to integrate topic models and VSMs goes back to Mitchell and Lapata (2009) who build a distributional model with dimensions set to topics over bag-of-words features. In their setting, LDA merely serves the purpose of dimensionality reduction, whereas our particular motivation is to use topics as probabilistic indicators for the prediction of attributes as semantic target categories in adjective-noun composition. Mitchell and Lapata (2010) compare VSMs defined over bags of context words vs. latent topics in a similarity</context>
</contexts>
<marker>Lacoste-Julien, Sha, Jordan, 2008</marker>
<rawString>Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. 2008. DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification. In NIPS, volume 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linlin Li</author>
<author>Benjamin Roth</author>
<author>Caroline Sporleder</author>
</authors>
<title>Topic models for word sense disambiguation and token-based idiom detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1138--1147</pages>
<location>Uppsala,</location>
<contexts>
<context position="1962" citStr="Li et al., 2010" startWordPosition="293" endWordPosition="296">udes distributional vector space models (VSMs), i.e., models that represent the semantics of words or phrases as vectors over high-dimensional cooccurrence data (Turney and Pantel, 2010; Baroni and Lenci, 2010, i.a.), as well as latent variable models (LVMs) which aggregate distributional observations in ’hidden’, or latent variables, thereby reducing the dimensionality of the data. An example of the latter are topic models (Blei et al., 2003), which have recently been applied to modeling selectional preferences of verbs (Ritter et al., 2010; O´ S´eaghdha, 2010), or word sense disambiguation (Li et al., 2010). A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both in isolation (Almuhareb, 2006) and in compositional adjective-noun phrases (Hartung and Frank, 2010; Guevara, 2010; Baroni and Zamparelli, 2010). In this paper, we propose a new approach to a problem we denote as attribute selection: The task is to predict the hidden attribute meaning expressed by a property-denoting adjective in composition with a noun. The adjective hot, e.g., may denote attributes such as TEMPERATURE, TASTE or EMOTIONALITY. These adjective meanings can be combined with no</context>
</contexts>
<marker>Li, Roth, Sporleder, 2010</marker>
<rawString>Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010. Topic models for word sense disambiguation and token-based idiom detection. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1138–1147, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="21162" citStr="McCallum, 2002" startWordPosition="3415" endWordPosition="3416">tify the target elements (adjectives and nouns) for the vector basis elements (i.e., the attribute nouns) in a local context window. The component values are defined using raw frequency counts over the extracted patterns. DEPVSM is similar to PATTVSM; however, it relies on dependency paths that connect the target elements and attributes in local contexts. The paths are identical to the ones used for constructing pseudo-documents in C-LDA and L-LDA. As in PATTVSM, the vector components are set to raw frequencies over extracted paths. Implementations. To implement our models, we rely on MALLET (McCallum, 2002) for C-LDA and 544 the Stanford Topic Modeling Toolbox3 for L-LDA. In both cases, we run 1000 iterations of Gibbs sampling, using default values for all hyperparameters. Data set for attribute selection over 10 attributes. The first experiment is conducted on the data set used in Hartung and Frank (2010). It consists of 100 adjective-noun pairs manually annotated for ten attributes: COLOR, DIRECTION, DURATION, SHAPE, SIZE, SMELL, SPEED, TASTE, TEMPERATURE, WEIGHT. To enable comparison, the dimensions of our models are set to exactly these attributes. Data set for attribute selection over a lar</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based Models of Semantic Composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7395" citStr="Mitchell and Lapata (2008)" startWordPosition="1172" endWordPosition="1175">-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. While these works adhere to a purely latent representation of meaning, Hartung and Frank (2010) include attributes as symbolic ‘hidden’ meanings of adjectives, nouns and adjective-noun phrases in a distributional VSM. Finally, a large body of work dealing with compositionality in distributional frameworks is not confined to the special case of adjective-noun composition (Mitchell and Lapata (2008), Rudolph and Giesbrecht (2010), i.a.). All these approaches regard composition as a process combining vectors (or matrices, resp.) to yield a new, contextualized vector representation within the same semantic space. Latent Dirichlet Allocation, aka. Topic Models (TMs). LDA is a generative probabilistic model e 21 1 1 0 1 45 0 4 0 0 2 0 0 0 0 0 0 0 2 0 0 0 b e× e+b 20 420 41 20 20 21 45 180 49 38 38 39 14 14 15 26 1170 71 COLOR DIRECTION DURATION SHAPE SIZE SMELL SPEED TASTE TEMPERATURE WEIGHT 541 for document collections. Each document is represented as a mixture over latent topics, where eac</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based Models of Semantic Composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Language Models Based on Semantic Composition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>430--439</pages>
<location>Singapore,</location>
<contexts>
<context position="8814" citStr="Mitchell and Lapata (2009)" startWordPosition="1420" endWordPosition="1423">be pre-specified, the dimensionality of the document representation can be considerably reduced in comparison to simple bag-of-words models. The remainder of this paper will assume some familiarity with LDA and the LDA terminology as introduced in Blei et al. (2003). Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al., 2009) or DiscLDA (Lacoste-Julien et al., 2008). We will discuss L-LDA in Section 3. Distributional VSMs and TMs. The idea to integrate topic models and VSMs goes back to Mitchell and Lapata (2009) who build a distributional model with dimensions set to topics over bag-of-words features. In their setting, LDA merely serves the purpose of dimensionality reduction, whereas our particular motivation is to use topics as probabilistic indicators for the prediction of attributes as semantic target categories in adjective-noun composition. Mitchell and Lapata (2010) compare VSMs defined over bags of context words vs. latent topics in a similarity judgement task. Their results indicate that a multiplicative setting works best for vector composition in word-based models, while vector addition is</context>
</contexts>
<marker>Mitchell, Lapata, 2009</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2009. Language Models Based on Semantic Composition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Singapore, August 2009, pages 430–439, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<pages>34--1388</pages>
<contexts>
<context position="9182" citStr="Mitchell and Lapata (2010)" startWordPosition="1475" endWordPosition="1478">lei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al., 2009) or DiscLDA (Lacoste-Julien et al., 2008). We will discuss L-LDA in Section 3. Distributional VSMs and TMs. The idea to integrate topic models and VSMs goes back to Mitchell and Lapata (2009) who build a distributional model with dimensions set to topics over bag-of-words features. In their setting, LDA merely serves the purpose of dimensionality reduction, whereas our particular motivation is to use topics as probabilistic indicators for the prediction of attributes as semantic target categories in adjective-noun composition. Mitchell and Lapata (2010) compare VSMs defined over bags of context words vs. latent topics in a similarity judgement task. Their results indicate that a multiplicative setting works best for vector composition in word-based models, while vector addition is better suited for topic vectors. 3 Topic Models for Attribute Selection 3.1 Using LDA for modeling lexical semantics Recently, LDA has been used for problems in lexical semantics, where the primary goal is not document modeling but the induction of semantic knowledge from high-dimensional co-occurrence data. Ritter et al. (2010) and O´ S´eaghdha (2010) model select</context>
<context position="24960" citStr="Mitchell and Lapata (2010)" startWordPosition="4001" endWordPosition="4004">ibute Selection for 10 Attributes Tables 1 and 2 summarize the results for attribute selection over 10 attributes against the labeled adjective-noun pairs in the test set, using ESel and MPC as selection functions on vectors composed by multiplication (Table 1) and addition (Table 2). The results reported for C-LDA correspond to the best performing model (with number of topics set to 42, as this setting yields the best and most constant results over both composition operators). C-LDA shows highest f-scores and recall over all settings, and highest precision with vector addition.6 In line with Mitchell and Lapata (2010) (cf. Sec. 2), we obtain the best overall results with vector addition (ESel: P: 0.55, R: 0.66, F: 0.61; MPC: P: 0.59, R: 0.71, F: 0.64). The difference between C-LDA and L-LDA is small but significant for vector multiplication; for vector addition, it is not significant. Compared to the LDA models, the VSM baselines 206 attributes, while all models were trained on 262 attributes obtained from WordNet in the first extraction step. 6In Tables 1 and 2, statistical significance of the differences between the models is marked by the superscripts L, D and P, denoting a significant difference over L</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34:1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Niles</author>
</authors>
<title>Mapping WordNet to the SUMO Ontology.</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE International Knowledge Engineering conference,</booktitle>
<pages>23--26</pages>
<contexts>
<context position="38297" citStr="Niles, 2003" startWordPosition="6204" endWordPosition="6205">rformance of the best property attributes with F&gt;0, individually, amounts to F=0.6310. In comparison to the unrestricted setting (cf. column all), nearly all property attributes benefit from model training on selective data. Exceptions are WIDTH, WEIGHT, THICKNESS, AGE, DEGREE and LIGHT. Thus, apparently, some of the adjectives associated with non-property attributes in the full set provide some discriminative power that is helpful to distinguish property types. In a qualitative analysis of the 133 non-property attributes filtered out in this experiment, we find that the WordNet-SUMO mapping (Niles, 2003) does not provide differentiating definitions for about 60% of these attributes, linking them instead to a single subjective assessment attribute. This suggests that in many cases the distinctions drawn by WordNet are too subtle even for humans to reproduce. 6 Conclusion This paper explored the use of LDA topic models in a semantic labeling task that predicts attributes as ’hidden’ meanings in the compositional semantics of adjective-noun phrases. LDA topic models are expected to alleviate sparsity problems of distributional VSMs as encountered in prior work, by incorporating latent semantic i</context>
</contexts>
<marker>Niles, 2003</marker>
<rawString>Ian Niles. 2003. Mapping WordNet to the SUMO Ontology. In Proceedings of the IEEE International Knowledge Engineering conference, pages 23–26, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>435--444</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O´ S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 435–444, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
</authors>
<title>User’s guide to sigf: Significance testing by approximate randomisation.</title>
<date>2006</date>
<marker>Pad´o, 2006</marker>
<rawString>Sebastian Pad´o, 2006. User’s guide to sigf: Significance testing by approximate randomisation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Prescher</author>
<author>S Riezler</author>
<author>M Rooth</author>
</authors>
<title>Using a probabilistic class-based lexicon for lexical ambiguity resolution.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th COLING,</booktitle>
<pages>649--655</pages>
<contexts>
<context position="11788" citStr="Prescher et al. (2000)" startWordPosition="1879" endWordPosition="1882">topics cannot be related to previously defined target categories. For attribute selection, the LDA-inferred topics need to be linked to semantic attributes. Therefore, we apply two extensions of standard LDA that are capable of taking supervised category information into account, either implicitly or directly, by including an additional observable variable into the generative process. In general, LVMs can be expected to overcome sparsity issues that are frequently encountered in distributional models. This positive smoothing effect is achieved by marginalization over the latent variables (cf. Prescher et al. (2000)). For instance, it is unlikely to observe a dependency path linking the adjective mature to the attribute MATURITY. Such a relation is more likely for young, for example. If young co-occurs with mature in a different pseudodocument (AGE might be a candidate), this results in a situation where (i) young and mature share one or more latent topics and (ii) the topic proportions for the attributes MATURITY and AGE will become similar to the extent of common words in their pseudodocuments. Consequently, the final attribute model is expected to assign a (small) positive probability to the relation </context>
</contexts>
<marker>Prescher, Riezler, Rooth, 2000</marker>
<rawString>D. Prescher, S. Riezler, and M. Rooth. 2000. Using a probabilistic class-based lexicon for lexical ambiguity resolution. In Proceedings of the 18th COLING, pages 649–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Labeled LDA: A supervised topic model for credit attribution in multilabeled corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>248--256</pages>
<location>Singapore,</location>
<contexts>
<context position="4843" citStr="Ramage et al. (2009)" startWordPosition="755" endWordPosition="758">006) and Hartung and Frank (2010), which are both embedded in a purely distributional setting. Specifically, we use Latent Dirichlet Allocation (LDA; Blei et al. (2003)) to train an attribute model that captures semantic information encoded in adjectives and nouns independently of one another. Following Hartung and Frank (2010), this model is embedded into a VSM that employs vector composition to combine the meaning of adjectives and nouns. We present two variants of LDA that differ in the way attributes are associated with the induced LDA topics: Controled LDA (C-LDA) and Labeled LDA (L-LDA; Ramage et al. (2009)). Both will be presented in detail in Section 3. Our aims in this paper are two-fold: (i) We investigate LDA as a modeling framework in the attribute selection task, as its use of topics as latent variables may alleviate inherent sparsity problems faced by prior work using pattern-based (Almuhareb, 2006) or vector space models (Hartung and Frank, 2010). (ii) While these prior approaches were restricted to a confined set of 10 attributes, we will we apply our 1The figure is adopted from the distributional setting of Hartung and Frank (2010), with component values defined by pattern frequency c</context>
<context position="8623" citStr="Ramage et al., 2009" startWordPosition="1387" endWordPosition="1390">is a probability distribution over words (Blei et al., 2003). These topics can be used as dense features for, e.g., document clustering. Depending on the number of topics, which has to be pre-specified, the dimensionality of the document representation can be considerably reduced in comparison to simple bag-of-words models. The remainder of this paper will assume some familiarity with LDA and the LDA terminology as introduced in Blei et al. (2003). Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al., 2009) or DiscLDA (Lacoste-Julien et al., 2008). We will discuss L-LDA in Section 3. Distributional VSMs and TMs. The idea to integrate topic models and VSMs goes back to Mitchell and Lapata (2009) who build a distributional model with dimensions set to topics over bag-of-words features. In their setting, LDA merely serves the purpose of dimensionality reduction, whereas our particular motivation is to use topics as probabilistic indicators for the prediction of attributes as semantic target categories in adjective-noun composition. Mitchell and Lapata (2010) compare VSMs defined over bags of contex</context>
<context position="15102" citStr="Ramage et al. 2009" startWordPosition="2427" endWordPosition="2430">ncy paths, together with the set of attribute nouns of interest, have to be manually specified. See the supplementary material for the full list of dependency paths used. 1 For each topic k ∈ {1, ... ,K}: 2 Generate βk = (βk,1, ... , βk,V )T ∼ Dir(· |η) 3 For each document d: 4 For each topic k ∈ {1, ... ,K} 5 Generate Λ(d) k ∈ {0, 1} ∼ Bernoulli(· |Φk) 6 Generate α(d) = L(d) × α 7 Generate θ(d) = (θl1,...,θlMd )T ∼ Dir(· |α(d)) 8 For each i in {1, ... ,Nd}: 9 Generate zi ∈ {λ(d) 1 , ... ,λ(d)Md} ∼ Mult(· |θ(d)) 10 Generate wi ∈ {1, ... , V } ∼ Mult(· |βzi) Figure 2: L-LDA generative process (Ramage et al. 2009) P(w|a) Pz P(w|d) = � P(w|t)P(t|d) (4) t 3.3 Labeled LDA L-LDA (Ramage et al., 2009) extends standard LDA to include supervision for specific target categories, yet in a different way: (i) The generative process includes a second observed variable, i.e. each document is explicitly labeled with a target category. A document may be labeled with an arbitrary number of categories; unlabeled documents are also possible. However, L-LDA permits only binary assignments of categories to documents; probabilistic weights over categories are not intended. (ii) Contrary to LDA, where the number of topics h</context>
<context position="39248" citStr="Ramage et al., 2009" startWordPosition="6347" endWordPosition="6350">ic labeling task that predicts attributes as ’hidden’ meanings in the compositional semantics of adjective-noun phrases. LDA topic models are expected to alleviate sparsity problems of distributional VSMs as encountered in prior work, by incorporating latent semantic information about attribute nouns. We investigated two variants of LDA that employ different degrees of supervision for associating topics with attributes. Our contributions are as follows. We proposed two LDA models for the attribute selection task that import supervision for a target category parameter in different ways: L-LDA (Ramage et al., 2009) embeds the target categories into the LDA process, by defining a 1:1 correspondence of topics and target categories. C-LDA, by contrast, does not affect the LDA generative process. Here, we heuris10In comparison, L-LDAESel,× yields an average f-score of 0.47 for attributes with F&gt;0 in the property setting. tically equate pseudo-documents with target categories, to approximate category-specific word-topic distributions. By adhering to standard LDA, C-LDA accommodates a greater variety in the distributions of topics to attribute-specific documents and words, as compared to L-LDA. Combining stan</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D. Manning. 2009. Labeled LDA: A supervised topic model for credit attribution in multilabeled corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Singapore, August 2009, pages 248–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A Latent Dirichlet Allocation Method for Selectional Preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>424--434</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1893" citStr="Ritter et al., 2010" startWordPosition="281" endWordPosition="284">ased attention in computational linguistics. This field of research includes distributional vector space models (VSMs), i.e., models that represent the semantics of words or phrases as vectors over high-dimensional cooccurrence data (Turney and Pantel, 2010; Baroni and Lenci, 2010, i.a.), as well as latent variable models (LVMs) which aggregate distributional observations in ’hidden’, or latent variables, thereby reducing the dimensionality of the data. An example of the latter are topic models (Blei et al., 2003), which have recently been applied to modeling selectional preferences of verbs (Ritter et al., 2010; O´ S´eaghdha, 2010), or word sense disambiguation (Li et al., 2010). A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both in isolation (Almuhareb, 2006) and in compositional adjective-noun phrases (Hartung and Frank, 2010; Guevara, 2010; Baroni and Zamparelli, 2010). In this paper, we propose a new approach to a problem we denote as attribute selection: The task is to predict the hidden attribute meaning expressed by a property-denoting adjective in composition with a noun. The adjective hot, e.g., may denote attributes such as TEMPERATURE, TA</context>
<context position="9745" citStr="Ritter et al. (2010)" startWordPosition="1562" endWordPosition="1565">in adjective-noun composition. Mitchell and Lapata (2010) compare VSMs defined over bags of context words vs. latent topics in a similarity judgement task. Their results indicate that a multiplicative setting works best for vector composition in word-based models, while vector addition is better suited for topic vectors. 3 Topic Models for Attribute Selection 3.1 Using LDA for modeling lexical semantics Recently, LDA has been used for problems in lexical semantics, where the primary goal is not document modeling but the induction of semantic knowledge from high-dimensional co-occurrence data. Ritter et al. (2010) and O´ S´eaghdha (2010) model selectional restrictions of verbs by inducing topic distributions that characterize ’mixtures of topics’ observed in verb argument positions. As a basis for LDA modeling, they collect pseudo-documents, i.e. bags of words that co-occur in syntactic argument positions. We apply a similar idea to the attribute selection problem: we collect pseudo-documents that characterize attributes by adjectives and nouns that cooccur with the attribute nouns in local contextual relations. The topic distributions obtained from fitting an LDA model to the collection of these pseud</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A Latent Dirichlet Allocation Method for Selectional Preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424–434, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Rudolph</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Compositional matrix-space models of language.</title>
<date>2010</date>
<booktitle>In Proceedings ofthe 48th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<pages>907--916</pages>
<contexts>
<context position="7426" citStr="Rudolph and Giesbrecht (2010)" startWordPosition="1176" endWordPosition="1180"> be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. While these works adhere to a purely latent representation of meaning, Hartung and Frank (2010) include attributes as symbolic ‘hidden’ meanings of adjectives, nouns and adjective-noun phrases in a distributional VSM. Finally, a large body of work dealing with compositionality in distributional frameworks is not confined to the special case of adjective-noun composition (Mitchell and Lapata (2008), Rudolph and Giesbrecht (2010), i.a.). All these approaches regard composition as a process combining vectors (or matrices, resp.) to yield a new, contextualized vector representation within the same semantic space. Latent Dirichlet Allocation, aka. Topic Models (TMs). LDA is a generative probabilistic model e 21 1 1 0 1 45 0 4 0 0 2 0 0 0 0 0 0 0 2 0 0 0 b e× e+b 20 420 41 20 20 21 45 180 49 38 38 39 14 14 15 26 1170 71 COLOR DIRECTION DURATION SHAPE SIZE SMELL SPEED TASTE TEMPERATURE WEIGHT 541 for document collections. Each document is represented as a mixture over latent topics, where each topic is a probability distri</context>
</contexts>
<marker>Rudolph, Giesbrecht, 2010</marker>
<rawString>Sebastian Rudolph and Eugenie Giesbrecht. 2010. Compositional matrix-space models of language. In Proceedings ofthe 48th Annual Meeting ofthe Association for Computational Linguistics, pages 907–916. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing (NeMLaP).</booktitle>
<volume>14</volume>
<pages>16</pages>
<location>Manchester, U.K.,</location>
<contexts>
<context position="22428" citStr="Schmid, 1994" startWordPosition="3622" endWordPosition="3623">eriment, we max out the attribute selection task to a much larger set of attributes in order to analyze the difficulty of the task on more representative data. We automatically construct a data set of adjective-noun phrases labeled with appropriate attributes from WordNet 3.0 (Fellbaum, 1998), relying on the assumption that examples given in glosses correspond to the respective word sense of the adjective. We first extract all adjectives that are linked to at least one attribute synset by the attribute relation. Next, we run the glosses of these adjectives (3592 in number) through TreeTagger (Schmid, 1994) to find examples of adjectives modifying nouns in attributive constructions. The resulting adjective-noun phrases are labeled with the attribute label linked to the given adjective sense. This method yields 7901 labeled adjective-noun phrases. They are divided into development and test data according to a sampling procedure that respects the following criteria: (i) Both sets must contain all attributes with an equal number of phrases for each attribute; (ii) phrases with both elements contained in CoreWordNet4 are preferred, while others are only considered if necessary to satisfy the first c</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing (NeMLaP). Manchester, U.K., 14– 16 September 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<pages>188</pages>
<contexts>
<context position="1531" citStr="Turney and Pantel, 2010" startWordPosition="225" endWordPosition="228">first time, we extend the attribute selection task to a new data set with more than 200 classes. We observe that large-scale attribute selection is a hard problem, but a subset of attributes performs robustly on the large scale as well. Again, the LDA models outperform the VSM baseline. 1 Introduction Corpus-based statistical modeling of semantics is gaining increased attention in computational linguistics. This field of research includes distributional vector space models (VSMs), i.e., models that represent the semantics of words or phrases as vectors over high-dimensional cooccurrence data (Turney and Pantel, 2010; Baroni and Lenci, 2010, i.a.), as well as latent variable models (LVMs) which aggregate distributional observations in ’hidden’, or latent variables, thereby reducing the dimensionality of the data. An example of the latter are topic models (Blei et al., 2003), which have recently been applied to modeling selectional preferences of verbs (Ritter et al., 2010; O´ S´eaghdha, 2010), or word sense disambiguation (Li et al., 2010). A topic that is increasingly studied in distributional semantics is the semantics of adjectives, both in isolation (Almuhareb, 2006) and in compositional adjective-nou</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141– 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fourth Conference on Computational Language Learning (CoNLL-2000) and the Second Learning Language in Logic Workshop,</booktitle>
<pages>947--953</pages>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="20319" citStr="Yeh, 2000" startWordPosition="3282" endWordPosition="3283">es. We evaluate the performance of the VSMs based on C-LDA and L-LDA in two experimental settings, contrasting the problem of attribute selection on semantic spaces of radically different dimensionality, using sets of 10 vs. 206 attributes. Evaluation measures. We evaluate against two gold standards consisting of adjective-noun phrases (or adjective-noun pairs) and their associated attribute meanings. We report precision, recall and f1-score. Where appropriate, we test differences in the performance of various model configurations for statistical significance in a randomized permutation test (Yeh, 2000), using the sigf tool (Pad´o, 2006). Baselines. We compare our models against two baselines, PATTVSM and DEPVSM. PATTSVM is reconstructed from Hartung and Frank (2010). It is grounded in a selection of lexical patterns that identify the target elements (adjectives and nouns) for the vector basis elements (i.e., the attribute nouns) in a local context window. The component values are defined using raw frequency counts over the extracted patterns. DEPVSM is similar to PATTVSM; however, it relies on dependency paths that connect the target elements and attributes in local contexts. The paths are </context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the Fourth Conference on Computational Language Learning (CoNLL-2000) and the Second Learning Language in Logic Workshop, Lisbon, Portugal, pages 947–953.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>