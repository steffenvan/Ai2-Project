<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9460815">
Cheap and Fast — But is it Good?
Evaluating Non-Expert Annotations for Natural Language Tasks
</title>
<author confidence="0.996347">
Rion Snow† Brendan O’Connor$ Daniel Jurafsky§ Andrew Y. Ng†
</author>
<affiliation confidence="0.9615315">
†Computer Science Dept. $Dolores Labs, Inc. §Linguistics Dept.
Stanford University 832 Capp St. Stanford University
</affiliation>
<address confidence="0.95071">
Stanford, CA 94305 San Francisco, CA 94110 Stanford, CA 94305
</address>
<email confidence="0.999676">
{rion,ang}@cs.stanford.edu brendano@doloreslabs.com jurafsky@stanford.edu
</email>
<sectionHeader confidence="0.997399" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954083333333">
Human linguistic annotation is crucial for
many natural language processing tasks but
can be expensive and time-consuming. We ex-
plore the use of Amazon’s Mechanical Turk
system, a significantly cheaper and faster
method for collecting annotations from a
broad base of paid non-expert contributors
over the Web. We investigate five tasks: af-
fect recognition, word similarity, recognizing
textual entailment, event temporal ordering,
and word sense disambiguation. For all five,
we show high agreement between Mechani-
cal Turk non-expert annotations and existing
gold standard labels provided by expert label-
ers. For the task of affect recognition, we also
show that using non-expert labels for training
machine learning algorithms can be as effec-
tive as using gold standard annotations from
experts. We propose a technique for bias
correction that significantly improves annota-
tion quality on two tasks. We conclude that
many large labeling tasks can be effectively
designed and carried out in this method at a
fraction of the usual expense.
</bodyText>
<sectionHeader confidence="0.999481" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9980312">
Large scale annotation projects such as TreeBank
(Marcus et al., 1993), PropBank (Palmer et
al., 2005), TimeBank (Pustejovsky et al., 2003),
FrameNet (Baker et al., 1998), SemCor (Miller et
al., 1993), and others play an important role in
natural language processing research, encouraging
the development of novel ideas, tasks, and algo-
rithms. The construction of these datasets, how-
ever, is extremely expensive in both annotator-hours
and financial cost. Since the performance of many
natural language processing tasks is limited by the
amount and quality of data available to them (Banko
and Brill, 2001), one promising alternative for some
tasks is the collection of non-expert annotations.
In this work we explore the use of Amazon Me-
chanical Turk1 (AMT) to determine whether non-
expert labelers can provide reliable natural language
annotations. We chose five natural language under-
standing tasks that we felt would be sufficiently nat-
ural and learnable for non-experts, and for which
we had gold standard labels from expert labelers,
as well as (in some cases) expert labeler agree-
ment information. The tasks are: affect recogni-
tion, word similarity, recognizing textual entailment,
event temporal ordering, and word sense disam-
biguation. For each task, we used AMT to annotate
data and measured the quality of the annotations by
comparing them with the gold standard (expert) la-
bels on the same data. Further, we compare machine
learning classifiers trained on expert annotations vs.
non-expert annotations.
In the next sections of the paper we introduce
the five tasks and the evaluation metrics, and offer
methodological insights, including a technique for
bias correction that improves annotation quality.2
</bodyText>
<footnote confidence="0.98672675">
1http://mturk.com
2 Please see http://blog.doloreslabs.com/?p=109
for a condensed version of this paper, follow-ups, and on-
going public discussion. We encourage comments to be di-
rected here in addition to email when appropriate. Dolores
Labs Blog, “AMT is fast, cheap, and good for machine learning
data,” Brendan O’Connor, Sept. 9, 2008. More related work at
http://blog.doloreslabs.com/topics/wisdom/.
</footnote>
<page confidence="0.933348">
254
</page>
<note confidence="0.9744565">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 254–263,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.999123" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999933446428572">
The idea of collecting annotations from volunteer
contributors has been used for a variety of tasks.
Luis von Ahn pioneered the collection of data via
online annotation tasks in the form of games, includ-
ing the ESPGame for labeling images (von Ahn and
Dabbish, 2004) and Verbosity for annotating word
relations (von Ahn et al., 2006). The Open Mind
Initiative (Stork, 1999) has taken a similar approach,
attempting to make such tasks as annotating word
sense (Chklovski and Mihalcea, 2002) and common-
sense word relations (Singh, 2002) sufficiently “easy
and fun” to entice users into freely labeling data.
There have been an increasing number of experi-
ments using Mechanical Turk for annotation. In (Su
et al., 2007) workers provided annotations for the
tasks of hotel name entity resolution and attribute
extraction of age, product brand, and product model,
and were found to have high accuracy compared
to gold-standard labels. Kittur et al. (2008) com-
pared AMT evaluations of Wikipedia article qual-
ity against experts, finding validation tests were im-
portant to ensure good results. Zaenen (Submitted)
studied the agreement of annotators on the problem
of recognizing textual entailment (a similar task and
dataset is explained in more detail in Section 4).
At least several studies have already used AMT
without external gold standard comparisons. In
(Nakov, 2008) workers generated paraphrases of
250 noun-noun compounds which were then used
as the gold standard dataset for evaluating an au-
tomatic method of noun compound paraphrasing.
Kaisser and Lowe (2008) use AMT to help build a
dataset for question answering, annotating the an-
swers to 8107 questions with the sentence contain-
ing the answer. Kaisser et al. (2008) examines the
task of customizing the summary length of QA out-
put; non-experts from AMT chose a summary length
that suited their information needs for varying query
types. Dakka and Ipeirotis (2008) evaluate a docu-
ment facet generation system against AMT-supplied
facets, and also use workers for user studies of the
system. Sorokin and Forsyth (2008) collect data for
machine vision tasks and report speed and costs sim-
ilar to our findings; their summaries of worker be-
havior also corroborate with what we have found.
In general, volunteer-supplied or AMT-supplied
data is more plentiful but noisier than expert data.
It is powerful because independent annotations can
be aggregated to achieve high reliability. Sheng et
al. (2008) explore several methods for using many
noisy labels to create labeled data, how to choose
which examples should get more labels, and how to
include labels’ uncertainty information when train-
ing classifiers. Since we focus on empirically val-
idating AMT as a data source, we tend to stick to
simple aggregation methods.
</bodyText>
<sectionHeader confidence="0.992116" genericHeader="method">
3 Task Design
</sectionHeader>
<bodyText confidence="0.9996155">
In this section we describe Amazon Mechanical
Turk and the general design of our experiments.
</bodyText>
<subsectionHeader confidence="0.998653">
3.1 Amazon Mechanical Turk
</subsectionHeader>
<bodyText confidence="0.999579677419355">
We employ the Amazon Mechanical Turk system
in order to elicit annotations from non-expert label-
ers. AMT is an online labor market where workers
are paid small amounts of money to complete small
tasks. The design of the system is as follows: one is
required to have an Amazon account to either sub-
mit tasks for annotations or to annotate submitted
tasks. These Amazon accounts are anonymous, but
are referenced by a unique Amazon ID. A Requester
can create a group of Human Intelligence Tasks (or
HITs), each of which is a form composed of an arbi-
trary number of questions. The user requesting an-
notations for the group of HITs can specify the num-
ber of unique annotations per HIT they are willing
to pay for, as well as the reward payment for each
individual HIT. While this does not guarantee that
unique people will annotate the task (since a single
person could conceivably annotate tasks using mul-
tiple accounts, in violation of the user agreement),
this does guarantee that annotations will be collected
from unique accounts. AMT also allows a requester
to restrict which workers are allowed to annotate a
task by requiring that all workers have a particular
set of qualifications, such as sufficient accuracy on
a small test set or a minimum percentage of previ-
ously accepted submissions. Annotators (variously
referred to as Workers or Turkers) may then annotate
the tasks of their choosing. Finally, after each HIT
has been annotated, the Requester has the option of
approving the work and optionally giving a bonus
to individual workers. There is a two-way commu-
</bodyText>
<page confidence="0.99638">
255
</page>
<bodyText confidence="0.998897">
nication channel between the task designer and the
workers mediated by Amazon, and Amazon handles
all financial transactions.
</bodyText>
<subsectionHeader confidence="0.999145">
3.2 Task Design
</subsectionHeader>
<bodyText confidence="0.999949">
In general we follow a few simple design principles:
we attempt to keep our task descriptions as succinct
as possible, and we attempt to give demonstrative
examples for each class wherever possible. We have
published the full experimental design and the data
we have collected for each task online3. We have
restricted our study to tasks where we require only
a multiple-choice response or numeric input within
a fixed range. For every task we collect ten inde-
pendent annotations for each unique item; this re-
dundancy allows us to perform an in-depth study of
how data quality improves with the number of inde-
pendent annotations.
</bodyText>
<sectionHeader confidence="0.9713" genericHeader="method">
4 Annotation Tasks
</sectionHeader>
<bodyText confidence="0.999994875">
We analyze the quality of non-expert annotations on
five tasks: affect recognition, word similarity, rec-
ognizing textual entailment, temporal event recogni-
tion, and word sense disambiguation. In this section
we define each annotation task and the parameters
of the annotations we request using AMT. Addition-
ally we give an initial analysis of the task results,
and summarize the cost of the experiments.
</bodyText>
<subsectionHeader confidence="0.997">
4.1 Affective Text Analysis
</subsectionHeader>
<bodyText confidence="0.9999804">
This experiment is based on the affective text an-
notation task proposed in Strapparava and Mihalcea
(2007), wherein each annotator is presented with a
list of short headlines, and is asked to give numeric
judgments in the interval [0,100] rating the headline
for six emotions: anger, disgust, fear, joy, sadness,
and surprise, and a single numeric rating in the inter-
val [-100,100] to denote the overall positive or nega-
tive valence of the emotional content of the headline,
as in this sample headline-annotation pair:
</bodyText>
<equation confidence="0.615740666666667">
Outcry at N Korea ‘nuclear test’
(Anger, 30), (Disgust,30), (Fear,30), (Joy,0),
(Sadness,20), (Surprise,40), (Valence,-50).
</equation>
<footnote confidence="0.9600635">
3All tasks and collected data are available at
http://ai.stanford.edu/˜rion/annotations/.
</footnote>
<bodyText confidence="0.999875928571429">
For our experiment we select a 100-headline sample
from the original SemEval test set, and collect 10
affect annotations for each of the seven label types,
for a total of 7000 affect labels.
We then performed two comparisons to evaluate
the quality of the AMT annotations. First, we asked
how well the non-experts agreed with the experts.
We did this by comparing the interannotator agree-
ment (ITA) of individual expert annotations to that
of single non-expert and averaged non-expert anno-
tations. In the original experiment ITA is measured
by calculating the Pearson correlation of one anno-
tator’s labels with the average of the labels of the
other five annotators. For each expert labeler, we
computed this ITA score of the expert against the
other five; we then average these ITA scores across
all expert annotators to compute the average expert
ITA (reported in Table 1 as “E vs. E”. We then do the
same for individual non-expert annotations, averag-
ing Pearson correlation across all sets of the five ex-
pert labelers (“NE vs. E”). We then calculate the ITA
for each expert vs. the averaged labels from all other
experts and non-experts (marked as “E vs. All”) and
for each non-expert vs. the pool of other non-experts
and all experts (“NE vs. All”). We compute these
ITA scores for each emotion task separately, aver-
aging the six emotion tasks as “Avg. Emo” and the
average of all tasks as “Avg. All”.
</bodyText>
<table confidence="0.9944668">
Emotion E vs. E E vs. All NE vs. E NE vs. All
Anger 0.459 0.503 0.444 0.573
Disgust 0.583 0.594 0.537 0.647
Fear 0.711 0.683 0.418 0.498
Joy 0.596 0.585 0.340 0.421
Sadness 0.645 0.650 0.563 0.651
Surprise 0.464 0.463 0.201 0.225
Valence 0.759 0.767 0.530 0.554
Avg. Emo 0.576 0.603 0.417 0.503
Avg. All 0.580 0.607 0.433 0.510
</table>
<tableCaption confidence="0.999958">
Table 1: Average expert and non-expert ITA on test-set
</tableCaption>
<bodyText confidence="0.99993975">
The results in Table 1 conform to the expectation
that experts are better labelers: experts agree with
experts more than non-experts agree with experts,
although the ITAs are in many cases quite close. But
we also found that adding non-experts to the gold
standard (“E vs. All”) improves agreement, suggest-
ing that non-expert annotations are good enough to
increase the overall quality of the gold labels. Our
</bodyText>
<page confidence="0.987364">
256
</page>
<bodyText confidence="0.999831714285714">
first comparison showed that individual experts were
better than individual non-experts. In our next com-
parison we ask how many averaged non-experts it
would take to rival the performance of a single ex-
pert. We did this by averaging the labels of each pos-
sible subset of n non-expert annotations, for value
of n in {1, 2, ... ,10}. We then treat this average as
though it is the output of a single ‘meta-labeler’, and
compute the ITA with respect to each subset of five
of the six expert annotators. We then average the
results of these studies across each subset size; the
results of this experiment are given in Table 2 and in
Figure 1. In addition to the single meta-labeler, we
ask: what is the minimum number of non-expert an-
notations k from which we can create a meta-labeler
that has equal or better ITA than an expert annotator?
In Table 2 we give the minimum k for each emotion,
and the averaged ITA for that meta-labeler consist-
ing of k non-experts (marked “k-NE”). In Figure 1
we plot the expert ITA correlation as the horizontal
dashed line.
</bodyText>
<table confidence="0.732119">
Emotion 1-Expert 10-NE k k-NE
Anger 0.459 0.675 2 0.536
Disgust 0.583 0.746 2 0.627
Fear 0.711 0.689 – –
Joy 0.596 0.632 7 0.600
Sadness 0.645 0.776 2 0.656
Surprise 0.464 0.496 9 0.481
Valence 0.759 0.844 5 0.803
Avg. Emo. 0.576 0.669 4 0.589
Avg. All 0.603 0.694 4 0.613
</table>
<tableCaption confidence="0.992629">
Table 2: Average expert and averaged correlation over
10 non-experts on test-set. k is the minimum number of
non-experts needed to beat an average expert.
</tableCaption>
<bodyText confidence="0.999982636363636">
These results show that for all tasks except “Fear”
we are able to achieve expert-level ITA with the
held-out set of experts within 9 labelers, and fre-
quently within only 2 labelers. Pooling judgments
across all 7 tasks we find that on average it re-
quires only 4 non-expert annotations per example to
achieve the equivalent ITA as a single expert anno-
tator. Given that we paid US$2.00 in order to collect
the 7000 non-expert annotations, we may interpret
our rate of 3500 non-expert labels per USD as at
least 875 expert-equivalent labels per USD.
</bodyText>
<subsectionHeader confidence="0.995629">
4.2 Word Similarity
</subsectionHeader>
<bodyText confidence="0.9992545">
This task replicates the word similarity task used in
(Miller and Charles, 1991), following a previous
</bodyText>
<figureCaption confidence="0.998535">
Figure 1: Non-expert correlation for affect recognition
</figureCaption>
<bodyText confidence="0.972314625">
task initially proposed by (Rubenstein and Good-
enough, 1965). Specifically, we ask for numeric
judgments of word similarity for 30 word pairs on
a scale of [0,10], allowing fractional responses4.
These word pairs range from highly similar (e.g.,
{boy, lad}), to unrelated (e.g., {noon, string}). Nu-
merous expert and non-expert studies have shown
that this task typically yields very high interannota-
tor agreement as measured by Pearson correlation;
(Miller and Charles, 1991) found a 0.97 correla-
tion of the annotations of 38 subjects with the an-
notations given by 51 subjects in (Rubenstein and
Goodenough, 1965), and a following study (Resnik,
1999) with 10 subjects found a 0.958 correlation
with (Miller and Charles, 1991).
In our experiment we ask for 10 annotations each
of the full 30 word pairs, at an offered price of $0.02
for each set of 30 annotations (or, equivalently, at
the rate of 1500 annotations per USD). The most
surprising aspect of this study was the speed with
which it was completed; the task of 300 annotations
was completed by 10 annotators in less than 11 min-
4(Miller and Charles, 1991) and others originally used a
numerical score of [0,4].
</bodyText>
<figure confidence="0.995229692307692">
disgust
anger
0.55 0.65 0.75
0.45 0.55 0.65
correlation
correlation
2 4 6 8 10
2 4 6 8 10
fear
joy
0.35 0.45 0.55 0.65
0.40 0.50 0.60 0.70
correlation
correlation
2 4 6 8 10
2 4 6 8 10
sadness
2 4 6 8 10
annotators
surprise
2 4 6 8 10
annotators
0.20 0.30 0.40 0.50
correlation
0.55 0.65 0.75
correlation
</figure>
<page confidence="0.975854">
257
</page>
<bodyText confidence="0.999629">
utes from the time of submission of our task to AMT,
at the rate of 1724 annotations / hour.
As in the previous task we evaluate our non-
expert annotations by averaging the numeric re-
sponses from each possible subset of n annotators
and computing the interannotator agreement with
respect to the gold scores reported in (Miller and
Charles, 1991). Our results are displayed in Figure
2, with Resnik’s 0.958 correlation plotted as the hor-
izontal line; we find that at 10 annotators we achieve
a correlation of 0.952, well within the range of other
studies of expert and non-expert annotations.
</bodyText>
<figure confidence="0.393698666666667">
Word Similarity ITA
2 4 6 8 10
annotations
</figure>
<figureCaption confidence="0.993786">
Figure 2: ITA for word similarity experiment
</figureCaption>
<subsectionHeader confidence="0.999185">
4.3 Recognizing Textual Entailment
</subsectionHeader>
<bodyText confidence="0.999823625">
This task replicates the recognizing textual entail-
ment task originally proposed in the PASCAL Rec-
ognizing Textual Entailment task (Dagan et al.,
2006); here for each question the annotator is pre-
sented with two sentences and given a binary choice
of whether the second hypothesis sentence can be
inferred from the first. For example, the hypothesis
sentence “Oil prices drop” would constitute a true
entailment from the text “Crude Oil Prices Slump”,
but a false entailment from “The government an-
nounced last week that it plans to raise oil prices”.
We gather 10 annotations each for all 800 sen-
tence pairs in the PASCAL RTE-1 dataset. For this
dataset expert interannotator agreement studies have
been reported as achieving 91% and 96% agreement
over various subsections of the corpus. When con-
sidering multiple non-expert annotations for a sen-
tence pair we use simple majority voting, breaking
ties randomly and averaging performance over all
possible ways to break ties. We collect 10 annota-
tions for each of 100 RTE sentence pairs; as dis-
played in Figure 3, we achieve a maximum accu-
racy of 89.7%, averaging over the annotations of 10
workers5.
</bodyText>
<sectionHeader confidence="0.842656" genericHeader="method">
RTE ITA
</sectionHeader>
<bodyText confidence="0.665847">
2 4 6 8 10
annotations
</bodyText>
<figureCaption confidence="0.995216">
Figure 3: Inter-annotator agreement for RTE experiment
</figureCaption>
<subsectionHeader confidence="0.990798">
4.4 Event Annotation
</subsectionHeader>
<bodyText confidence="0.99975425">
This task is inspired by the TimeBank corpus (Puste-
jovsky et al., 2003), which includes among its anno-
tations a label for event-pairs that represents the tem-
poral relation between them, from a set of fourteen
relations (before, after, during, includes, etc.). We
implement temporal ordering as a simplified version
of the TimeBank event temporal annotation task:
rather than annotating all fourteen event types, we
restrict our consideration to the two simplest labels:
“strictly before” and “strictly after”. Furthermore,
rather than marking both nouns and verbs in the text
as possible events, we only consider possible verb
events. We extract the 462 verb event pairs labeled
as “strictly before” or “strictly after” in the Time-
Bank corpus, and we present these pairs to annota-
tors with a forced binary choice on whether the event
described by the first verb occurs before or after the
second. For example, in a dialogue about a plane
explosion, we have the utterance: “It just blew up in
the air, and then we saw two fireballs go down to the,
</bodyText>
<footnote confidence="0.6721466">
5It might seem pointless to consider an even number of an-
notations in this circumstance, since the majority voting mech-
anism and tie-breaking yields identical performance for 2n + 1
and 2n + 2 annotators; however, in Section 5 we will consider
methods that can make use of the even annotations.
</footnote>
<figure confidence="0.75745075">
0.84 0.90 0.96
correlation
0.70 0.80 0.90
accuracy
</figure>
<page confidence="0.984482">
258
</page>
<bodyText confidence="0.999551538461539">
to the water, and there was a big small, ah, smoke,
from ah, coming up from that”. Here for each anno-
tation we highlight the specific verb pair of interest
(e.g., go/coming, or blew/saw) and ask which event
occurs first (here, go and blew, respectively).
The results of this task are presented in Figure 4.
We achieve high agreement for this task, at a rate
of 0.94 with simple voting over 10 annotators (4620
total annotations). While an expert ITA of 0.77 was
reported for the more general task involving all four-
teen labels on both noun and verb events, no expert
ITA numbers have been reported for this simplified
temporal ordering task.
</bodyText>
<figure confidence="0.386955333333333">
Temp. Ordering ITA
2 4 6 8 10
annotators
</figure>
<figureCaption confidence="0.996914">
Figure 4: ITA for temporal ordering experiment
</figureCaption>
<subsectionHeader confidence="0.916902">
4.5 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.999842727272727">
In this task we consider a simple problem on which
machine learning algorithms have been shown to
produce extremely good results; here we annotate
part of the SemEval Word Sense Disambiguation
Lexical Sample task (Pradhan et al., 2007); specif-
ically, we present the labeler with a paragraph of
text containing the word “president” (e.g., a para-
graph containing “Robert E. Lyons III...was ap-
pointed president and chief operating officer...”) and
ask the labeler which one of the following three
sense labels is most appropriate:
</bodyText>
<listItem confidence="0.899133333333333">
1) executive officer of a firm, corporation, or university
2) head of a country (other than the U.S.)
3) head of the U.S., President of the United States
</listItem>
<bodyText confidence="0.999893952380952">
We collect 10 annotations for each of 177 examples
of the noun “president” for the three senses given in
SemEval. As shown in Figure 5, performing simple
majority voting (with random tie-breaking) over an-
notators results in a rapid accuracy plateau at a very
high rate of 0.994 accuracy. In fact, further analy-
sis reveals that there was only a single disagreement
between the averaged non-expert vote and the gold
standard; on inspection it was observed that the an-
notators voted strongly against the original gold la-
bel (9-to-1 against), and that it was in fact found to
be an error in the original gold standard annotation.6
After correcting this error, the non-expert accuracy
rate is 100% on the 177 examples in this task. This
is a specific example where non-expert annotations
can be used to correct expert annotations.
Since expert ITA was not reported per word on
this dataset, we compare instead to the performance
of the best automatic system performance for dis-
ambiguating “president” in SemEval Task 17 (Cai et
al., 2007), with an accuracy of 0.98.
</bodyText>
<sectionHeader confidence="0.821052" genericHeader="method">
WSD ITA
</sectionHeader>
<bodyText confidence="0.840021">
2 4 6 8 10
annotators
</bodyText>
<figureCaption confidence="0.994071">
Figure 5: Inter-annotator agreement for WSD experiment
</figureCaption>
<subsectionHeader confidence="0.49532">
4.6 Summary
</subsectionHeader>
<table confidence="0.998262375">
Task Labels Cost Time Labels Labels
(USD) (hrs) per USD per hr
Affect 7000 $2.00 5.93 3500 1180.4
WSim 300 $0.20 0.174 1500 1724.1
RTE 8000 $8.00 89.3 1000 89.59
Event 4620 $13.86 39.9 333.3 115.85
WSD 1770 $1.76 8.59 1005.7 206.1
Total 21690 25.82 143.9 840.0 150.7
</table>
<tableCaption confidence="0.999847">
Table 3: Summary of costs for non-expert labels
</tableCaption>
<footnote confidence="0.85445875">
6The example sentence began “The Egyptian president said
he would visit Libya today...” and was mistakenly marked as
the “head of a company” sense in the gold annotation (example
id 24:0@24@wsj/23/wsj 2381@wsj@en@on).
</footnote>
<figure confidence="0.955597375">
0.70 0.80 0.90
accuracy
0.980 0.990 1.000
accuracy
259
accuracy 0.4 0.6 0.8 1.0
0 200 400 600 800
number of annotations
</figure>
<figureCaption confidence="0.9746685">
Figure 6: Worker accuracies on the RTE task. Each point
is one worker. Vertical jitter has been added to points on
the left to show the large number of workers who did the
minimum amount of work (20 examples).
</figureCaption>
<bodyText confidence="0.999973333333333">
In Table 3 we give a summary of the costs asso-
ciated with obtaining the non-expert annotations for
each of our 5 tasks. Here Time is given as the to-
tal amount of time in hours elapsed from submitting
the group of HITs to AMT until the last assignment
is submitted by the last worker.
</bodyText>
<sectionHeader confidence="0.9854365" genericHeader="method">
5 Bias correction for non-expert
annotators
</sectionHeader>
<bodyText confidence="0.999985772727273">
The reliability of individual workers varies. Some
are very accurate, while others are more careless and
make mistakes; and a small few give very noisy re-
sponses. Furthermore, for most AMT data collec-
tion experiments, a relatively small number of work-
ers do a large portion of the task, since workers may
do as much or as little as they please. Figure 6 shows
accuracy rates for individual workers on one task.
Both the overall variability, as well as the prospect
of identifying high-volume but low-quality workers,
suggest that controlling for individual worker qual-
ity could yield higher quality overall judgments.
In general, there are at least three ways to enhance
quality in the face of worker error. More work-
ers can be used, as described in previous sections.
Another method is to use Amazon’s compensation
mechanisms to give monetary bonuses to highly-
performing workers and deny payments to unreli-
able ones; this is useful, but beyond the scope of
this paper. In this section we explore a third alterna-
tive, to model the reliability and biases of individual
workers and correct for them.
A wide number of methods have been explored to
correct for the bias of annotators. Dawid and Skene
(1979) are the first to consider the case of having
multiple annotators per example but unknown true
labels. They introduce an EM algorithm to simul-
taneously estimate annotator biases and latent label
classes. Wiebe et al. (1999) analyze linguistic anno-
tator agreement statistics to find bias, and use a sim-
ilar model to correct labels. A large literature in bio-
statistics addresses this same problem for medical
diagnosis. Albert and Dodd (2004) review several
related models, but argue they have various short-
comings and emphasize instead the importance of
having a gold standard.
Here we take an approach based on gold standard
labels, using a small amount of expert-labeled train-
ing data in order to correct for the individual biases
of different non-expert annotators. The idea is to re-
calibrate worker’s responses to more closely match
expert behavior. We focus on categorical examples,
though a similar method can be used with numeric
data.
</bodyText>
<subsectionHeader confidence="0.99859">
5.1 Bias correction in categorical data
</subsectionHeader>
<bodyText confidence="0.999971777777778">
Following Dawid and Skene, we model labels and
workers with a multinomial model similar to Naive
Bayes. Every example i has a true label xi. For sim-
plicity, assume two labels {Y, N}. Several differ-
ent workers give labels yi1, yi2, ... yiW. A worker’s
conditional probability of response is modeled as
multinomial, and we model each worker’s judgment
as conditionally independent of other workers given
the true label xi, i.e.:
</bodyText>
<equation confidence="0.945918666666667">
P(yi1, ... , yiW, xi) = (ri
w
or log-odds:
</equation>
<bodyText confidence="0.999972666666667">
To infer the posterior probability of the true label
for a new example, worker judgments are integrated
via Bayes rule, yielding the posteri
</bodyText>
<equation confidence="0.949822571428571">
P(xi = Y |yi1 ... yiW)
log
P(xi = N|yi1 ... yiW)
P (yiw|xi = Y ) P(xi = Y )
log P(yiw|xi = N) + log P(xi = N)
�P(yiw|xi) p(xi)
�=w
</equation>
<page confidence="0.978146">
260
</page>
<bodyText confidence="0.974829578947369">
The worker response likelihoods P(ywJx = Y )
and P(ywJx = N) can be directly estimated from
frequencies of worker performance on gold standard
examples. (If we used maximum likelihood esti-
mation with no Laplace smoothing, then each ywJx
is just the worker’s empirical confusion matrix.)
For MAP label estimation, the above equation de-
scribes a weighted voting rule: each worker’s vote is
weighted by their log likelihood ratio for their given
response. Intuitively, workers who are more than
50% accurate have positive votes; workers whose
judgments are pure noise have zero votes; and an-
ticorrelated workers have negative votes. (A simpler
form of the model only considers accuracy rates,
thus weighting worker votes by log 1 accw . But we
accw
use the full unconstrained multinomial model here.)
5.1.1 Example tasks: RTE-1 and event
annotation
We used this model to improve accuracy on the
RTE-1 and event annotation tasks. (The other cate-
gorical task, word sense disambiguation, could not
be improved because it already had maximum accu-
racy.) First we took a sample of annotations giving
k responses per example. Within this sample, we
trained and tested via 20-fold cross-validation across
examples. Worker models were fit using Laplace
smoothing of 1 pseudocount; label priors were uni-
form, which was reasonably similar to the empirical
distribution for both tasks.
curacy increase, averaged across 2 through 10 anno-
tators. We find a +3.4% gain on event annotation.
Finally, we experimented with a similar calibration
method for numeric data, using a Gaussian noise
model for each worker: ywJx — N(x + pw, aw).
On the affect task, this yielded a small but consis-
tent increases in Pearson correlation at all numbers
of annotators, averaging a +0.6% gain.
</bodyText>
<sectionHeader confidence="0.8183645" genericHeader="method">
6 Training a system with non-expert
annotations
</sectionHeader>
<bodyText confidence="0.999508">
In this section we train a supervised affect recogni-
tion system with expert vs. non-expert annotations.
</bodyText>
<subsectionHeader confidence="0.986508">
6.1 Experimental Design
</subsectionHeader>
<bodyText confidence="0.9978745">
For the purpose of this experiment we create a sim-
ple bag-of-words unigram model for predicting af-
fect and valence, similar to the SWAT system (Katz
et al., 2007), one of the top-performing systems on
the SemEval Affective Text task.7 For each token
t in our training set, we assign t a weight for each
emotion e equal to the average emotion score ob-
served in each headline H that t participates in. i.e.,
if Ht is the set of headlines containing the token t,
then:
</bodyText>
<figure confidence="0.937175888888889">
EHEHt Score(e, H)
Score(e,t) = JHtJ
RTE
before/after
Gold calibrated
Naive voting
accuracy
0.7 0.8 0.9
0.7 0.8 0.9
</figure>
<bodyText confidence="0.818568">
With these weights of the individual tokens we
may then compute the score for an emotion e of a
new headline H as the average score over the set of
tokens t E H that we’ve observed in the training set
(ignoring those tokens not in the training set), i.e.:
annotators annotators
</bodyText>
<figureCaption confidence="0.952552666666667">
Figure 7: Gold-calibrated labels versus raw labels �Score(e, H) = Score(e, t)
tEH JHJ
Figure 7 shows improved accuracy at different
</figureCaption>
<bodyText confidence="0.9608015">
numbers of annotators. The lowest line is for the
naive 50% majority voting rule. (This is equivalent
to the model under uniform priors and equal accu-
racies across workers and labels.) Each point is the
data set’s accuracy against the gold labels, averaged
across resamplings each of which obtains k annota-
tions per example. RTE has an average +4.0% ac-
Where JHJ is simply the number of tokens in
headline H, ignoring tokens not observed in the
training set.
</bodyText>
<footnote confidence="0.9848315">
7 Unlike the SWAT system we perform no lemmatization,
synonym expansion, or any other preprocessing of the tokens;
we simply use whitespace-separated tokens within each head-
line.
</footnote>
<page confidence="0.991579">
261
</page>
<subsectionHeader confidence="0.866086">
6.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999165857142857">
We use 100 headlines as a training set (examples
500-599 from the test set of SemEval Task 14), and
we use the remaining 900 headlines as our test set.
Since we are fortunate to have the six separate ex-
pert annotations in this task, we can perform an ex-
tended systematic comparison of the performance of
the classifier trained with expert vs. non-expert data.
</bodyText>
<table confidence="0.9998731">
Emotion 1-Expert 10-NE k k-NE
Anger 0.084 0.233 1 0.172
Disgust 0.130 0.231 1 0.185
Fear 0.159 0.247 1 0.176
Joy 0.130 0.125 – –
Sadness 0.127 0.174 1 0.141
Surprise 0.060 0.101 1 0.061
Valence 0.159 0.229 2 0.146
Avg. Emo 0.116 0.185 1 0.135
Avg. All 0.122 0.191 1 0.137
</table>
<tableCaption confidence="0.975209333333333">
Table 4: Performance of expert-trained and non-expert-
trained classifiers on test-set. k is the minimum number
of non-experts needed to beat an average expert.
</tableCaption>
<bodyText confidence="0.999934333333334">
For this evaluation we compare the performance
of systems trained on expert and non-expert annota-
tions. For each expert annotator we train a system
using only the judgments provided by that annota-
tor, and then create a gold standard test set using the
average of the responses of the remaining five label-
ers on that set. In this way we create six indepen-
dent expert-trained systems and compute the aver-
age across their performance, calculated as Pearson
correlation to the gold standard; this is reported in
the “1-Expert” column of Table 4.
Next we train systems using non-expert labels;
for each possible subset of n annotators, for n E
11, 2, ... ,10} we train a system, and evaluate by
calculating Pearson correlation with the same set of
gold standard datasets used in the expert-trained sys-
tem evaluation. Averaging the results of these stud-
ies yields the results in Table 4.
As in Table 2 we calculate the minimum number
of non-expert annotations per example k required on
average to achieve similar performance to the ex-
pert annotations; surprisingly we find that for five
of the seven tasks, the average system trained with a
single set of non-expert annotations outperforms the
average system trained with the labels from a sin-
gle expert. One possible hypothesis for the cause
of this non-intuitive result is that individual labelers
(including experts) tend to have a strong bias, and
since multiple non-expert labelers may contribute to
a single set of non-expert annotations, the annotator
diversity within the single set of labels may have the
effect of reducing annotator bias and thus increasing
system performance.
</bodyText>
<sectionHeader confidence="0.998865" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999951769230769">
We demonstrate the effectiveness of using Amazon
Mechanical Turk for a variety of natural language
annotation tasks. Our evaluation of non-expert la-
beler data vs. expert annotations for five tasks found
that for many tasks only a small number of non-
expert annotations per item are necessary to equal
the performance of an expert annotator. In a detailed
study of expert and non-expert agreement for an af-
fect recognition task we find that we require an av-
erage of 4 non-expert labels per item in order to em-
ulate expert-level label quality. Finally, we demon-
strate significant improvement by controlling for la-
beler bias.
</bodyText>
<sectionHeader confidence="0.997315" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99834925">
Thanks to Nathanael Chambers, Annie Zaenen,
Rada Mihalcea, Qi Su, Panos Ipeirotis, Bob Car-
penter, David Vickrey, William Morgan, and Lukas
Biewald for useful discussions, and for the gener-
ous support of Dolores Labs. This work was sup-
ported in part by the Disruptive Technology Office
(DTO)’s Advanced Question Answering for Intelli-
gence (AQUAINT) Phase III Program.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997444538461538">
Paul S. Albert and Lori E. Dodd. 2004. A Cautionary
Note on the Robustness of Latent Class Models for
Estimating Diagnostic Error without a Gold Standard.
Biometrics, Vol. 60 (2004), pp. 427-435.
Collin F. Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc. of
COLING-ACL 1998.
Michele Banko and Eric Brill. 2001. Scaling to Very
Very Large Corpora for Natural Language Disam-
biguation. In Proc. ofACL-2001.
Junfu Cai, Wee Sun Lee and Yee Whye Teh. 2007. Im-
proving Word Sense Disambiguation Using Topic Fea-
tures. In Proc. of EMNLP-2007 .
</reference>
<page confidence="0.961052">
262
</page>
<reference confidence="0.999456952380952">
Timothy Chklovski and Rada Mihalcea. 2002. Building
a sense tagged corpus with Open Mind Word Expert.
In Proc. of the Workshop on ”Word Sense Disam-
biguation: Recent Successes and Future Directions”,
ACL 2002.
Timothy Chklovski and Yolanda Gil. 2005. Towards
Managing Knowledge Collection from Volunteer Con-
tributors. Proceedings of AAAI Spring Symposium
on Knowledge Collection from Volunteer Contributors
(KCVC05).
Ido Dagan, Oren Glickman and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
Challenge. Machine Learning Challenges. Lecture
Notes in Computer Science, Vol. 3944, pp. 177-190,
Springer, 2006.
Wisam Dakka and Panagiotis G. Ipeirotis. 2008. Au-
tomatic Extraction of Useful Facet Terms from Text
Documents. In Proc. ofICDE-2008.
A. P. Dawid and A. M. Skene. 1979. Maximum Like-
lihood Estimation of Observer Error-Rates Using the
EM Algorithm. Applied Statistics, Vol. 28, No. 1
(1979), pp. 20-28.
Michael Kaisser and John B. Lowe. 2008. A Re-
search Collection of QuestionAnswer Sentence Pairs.
In Proc. ofLREC-2008.
Michael Kaisser, Marti Hearst, and John B. Lowe.
2008. Evidence for Varying Search Results Summary
Lengths. In Proc. ofACL-2008.
Phil Katz, Matthew Singleton, Richard Wicentowski.
2007. SWAT-MP: The SemEval-2007 Systems for
Task 5 and Task 14. In Proc. of SemEval-2007.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with Mechanical Turk. In
Proc. of CHI-2008.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics 19:2, June 1993.
George A. Miller and William G. Charles. 1991. Con-
textual Correlates of Semantic Similarity. Language
and Cognitive Processes, vol. 6, no. 1, pp. 1-28, 1991.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunke. 1993. A semantic concordance. In
Proc. ofHLT-1993.
Preslav Nakov. 2008. Paraphrasing Verbs for Noun
Compound Interpretation. In Proc. of the Workshop
on Multiword Expressions, LREC-2008.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: A Corpus Annotated with Se-
mantic Roles. Computational Linguistics, 31:1.
Sameer Pradhan, Edward Loper, Dmitriy Dligach and
Martha Palmer. 2007. SemEval-2007 Task-17: En-
glish Lexical Sample, SRL and All Words. In Proc.
of SemEval-2007 .
James Pustejovsky, Patrick Hanks, Roser Saur, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus. In
Proc. of Corpus Linguistics 2003, 647-656.
Philip Resnik. 1999. Semantic Similarity in a Taxon-
omy: An Information-Based Measure and its Applica-
tion to Problems of Ambiguity in Natural Language.
JAIR, Volume 11, pages 95-130.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Communications
of the ACM, 8(10):627–633.
Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeiro-
tis. 2008. Get Another Label? Improving Data Qual-
ity and Data Mining Using Multiple, Noisy Labelers.
In Proc. ofKDD-2008.
Push Singh. 2002. The public acquisition of common-
sense knowledge. In Proc. of AAAI Spring Sympo-
sium on Acquiring (and Using) Linguistic (and World)
Knowledge for Information Access, 2002.
Alexander Sorokin and David Forsyth. 2008. Util-
ity data annotation with Amazon Mechanical Turk.
To appear in Proc. of First IEEE Workshop on
Internet Vision at CVPR, 2008. See also:
http://vision.cs.uiuc.edu/annotation/
David G. Stork. 1999. The Open Mind Initiative.
IEEE Expert Systems and Their Applications pp. 16-
20, May/June 1999.
Carlo Strapparava and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text In Proc. of SemEval-
2007.
Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and Wendell C.
Baker. 2007. Internet-Scale Collection of Human-
Reviewed Data. In Proc. of WWW-2007.
Luis von Ahn and Laura Dabbish. 2004. Labeling Im-
ages with a Computer Game. In ACM Conference on
Human Factors in Computing Systems, CHI 2004.
Luis von Ahn, Mihir Kedia and Manuel Blum. 2006.
Verbosity: A Game for Collecting Common-Sense
Knowledge. In ACM Conference on Human Factors
in Computing Systems, CHI Notes 2006.
Ellen Voorhees and Hoa Trang Dang. 2006. Overview of
the TREC 2005 question answering track. In Proc. of
TREC-2005.
Janyce M. Wiebe, Rebecca F. Bruce and Thomas P.
O’Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In
Proc. ofACL-1999.
Annie Zaenen. Submitted. Do give a penny for their
thoughts. International Journal of Natural Language
Engineering (submitted).
</reference>
<page confidence="0.998905">
263
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.787849">
<title confidence="0.998943">Cheap and Fast — But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks</title>
<author confidence="0.800092">Y</author>
<affiliation confidence="0.996889">Science Dept. Labs, Inc. Dept. Stanford University 832 Capp St. Stanford University</affiliation>
<address confidence="0.999841">Stanford, CA 94305 San Francisco, CA 94110 Stanford, CA 94305</address>
<email confidence="0.999675">brendano@doloreslabs.comjurafsky@stanford.edu</email>
<abstract confidence="0.9996104">Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon’s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paul S Albert</author>
<author>Lori E Dodd</author>
</authors>
<title>A Cautionary Note on the Robustness of Latent Class Models for Estimating Diagnostic Error without a Gold Standard.</title>
<date>2004</date>
<journal>Biometrics,</journal>
<volume>60</volume>
<pages>427--435</pages>
<contexts>
<context position="24922" citStr="Albert and Dodd (2004)" startWordPosition="4104" endWordPosition="4107">model the reliability and biases of individual workers and correct for them. A wide number of methods have been explored to correct for the bias of annotators. Dawid and Skene (1979) are the first to consider the case of having multiple annotators per example but unknown true labels. They introduce an EM algorithm to simultaneously estimate annotator biases and latent label classes. Wiebe et al. (1999) analyze linguistic annotator agreement statistics to find bias, and use a similar model to correct labels. A large literature in biostatistics addresses this same problem for medical diagnosis. Albert and Dodd (2004) review several related models, but argue they have various shortcomings and emphasize instead the importance of having a gold standard. Here we take an approach based on gold standard labels, using a small amount of expert-labeled training data in order to correct for the individual biases of different non-expert annotators. The idea is to recalibrate worker’s responses to more closely match expert behavior. We focus on categorical examples, though a similar method can be used with numeric data. 5.1 Bias correction in categorical data Following Dawid and Skene, we model labels and workers wit</context>
</contexts>
<marker>Albert, Dodd, 2004</marker>
<rawString>Paul S. Albert and Lori E. Dodd. 2004. A Cautionary Note on the Robustness of Latent Class Models for Estimating Diagnostic Error without a Gold Standard. Biometrics, Vol. 60 (2004), pp. 427-435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL</booktitle>
<contexts>
<context position="1640" citStr="Baker et al., 1998" startWordPosition="236" endWordPosition="239">rs. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense. 1 Introduction Large scale annotation projects such as TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), TimeBank (Pustejovsky et al., 2003), FrameNet (Baker et al., 1998), SemCor (Miller et al., 1993), and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms. The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost. Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them (Banko and Brill, 2001), one promising alternative for some tasks is the collection of non-expert annotations. In this work we explore the use of Amazon Mechanical Turk1 (AMT) to determin</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore and John B. Lowe. 1998. The Berkeley FrameNet project. In Proc. of COLING-ACL 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to Very Very Large Corpora for Natural Language Disambiguation. In</title>
<date>2001</date>
<booktitle>Proc. ofACL-2001.</booktitle>
<contexts>
<context position="2076" citStr="Banko and Brill, 2001" startWordPosition="303" endWordPosition="306"> 1 Introduction Large scale annotation projects such as TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), TimeBank (Pustejovsky et al., 2003), FrameNet (Baker et al., 1998), SemCor (Miller et al., 1993), and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms. The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost. Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them (Banko and Brill, 2001), one promising alternative for some tasks is the collection of non-expert annotations. In this work we explore the use of Amazon Mechanical Turk1 (AMT) to determine whether nonexpert labelers can provide reliable natural language annotations. We chose five natural language understanding tasks that we felt would be sufficiently natural and learnable for non-experts, and for which we had gold standard labels from expert labelers, as well as (in some cases) expert labeler agreement information. The tasks are: affect recognition, word similarity, recognizing textual entailment, event temporal ord</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to Very Very Large Corpora for Natural Language Disambiguation. In Proc. ofACL-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junfu Cai</author>
</authors>
<title>Wee Sun Lee and Yee Whye Teh.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-2007 .</booktitle>
<marker>Cai, 2007</marker>
<rawString>Junfu Cai, Wee Sun Lee and Yee Whye Teh. 2007. Improving Word Sense Disambiguation Using Topic Features. In Proc. of EMNLP-2007 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Rada Mihalcea</author>
</authors>
<title>Building a sense tagged corpus with Open Mind Word Expert.</title>
<date>2002</date>
<booktitle>In Proc. of the Workshop on ”Word Sense Disambiguation: Recent Successes and Future Directions”, ACL</booktitle>
<contexts>
<context position="4273" citStr="Chklovski and Mihalcea, 2002" startWordPosition="634" endWordPosition="637">thods in Natural Language Processing, pages 254–263, Honolulu, October 2008.c�2008 Association for Computational Linguistics 2 Related Work The idea of collecting annotations from volunteer contributors has been used for a variety of tasks. Luis von Ahn pioneered the collection of data via online annotation tasks in the form of games, including the ESPGame for labeling images (von Ahn and Dabbish, 2004) and Verbosity for annotating word relations (von Ahn et al., 2006). The Open Mind Initiative (Stork, 1999) has taken a similar approach, attempting to make such tasks as annotating word sense (Chklovski and Mihalcea, 2002) and commonsense word relations (Singh, 2002) sufficiently “easy and fun” to entice users into freely labeling data. There have been an increasing number of experiments using Mechanical Turk for annotation. In (Su et al., 2007) workers provided annotations for the tasks of hotel name entity resolution and attribute extraction of age, product brand, and product model, and were found to have high accuracy compared to gold-standard labels. Kittur et al. (2008) compared AMT evaluations of Wikipedia article quality against experts, finding validation tests were important to ensure good results. Zae</context>
</contexts>
<marker>Chklovski, Mihalcea, 2002</marker>
<rawString>Timothy Chklovski and Rada Mihalcea. 2002. Building a sense tagged corpus with Open Mind Word Expert. In Proc. of the Workshop on ”Word Sense Disambiguation: Recent Successes and Future Directions”, ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Yolanda Gil</author>
</authors>
<title>Towards Managing Knowledge Collection from Volunteer Contributors.</title>
<date>2005</date>
<booktitle>Proceedings of AAAI Spring Symposium on Knowledge Collection from Volunteer Contributors (KCVC05).</booktitle>
<marker>Chklovski, Gil, 2005</marker>
<rawString>Timothy Chklovski and Yolanda Gil. 2005. Towards Managing Knowledge Collection from Volunteer Contributors. Proceedings of AAAI Spring Symposium on Knowledge Collection from Volunteer Contributors (KCVC05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge. Machine Learning Challenges.</title>
<date>2006</date>
<journal>Lecture Notes in Computer Science,</journal>
<volume>3944</volume>
<pages>177--190</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="16984" citStr="Dagan et al., 2006" startWordPosition="2759" endWordPosition="2762">he interannotator agreement with respect to the gold scores reported in (Miller and Charles, 1991). Our results are displayed in Figure 2, with Resnik’s 0.958 correlation plotted as the horizontal line; we find that at 10 annotators we achieve a correlation of 0.952, well within the range of other studies of expert and non-expert annotations. Word Similarity ITA 2 4 6 8 10 annotations Figure 2: ITA for word similarity experiment 4.3 Recognizing Textual Entailment This task replicates the recognizing textual entailment task originally proposed in the PASCAL Recognizing Textual Entailment task (Dagan et al., 2006); here for each question the annotator is presented with two sentences and given a binary choice of whether the second hypothesis sentence can be inferred from the first. For example, the hypothesis sentence “Oil prices drop” would constitute a true entailment from the text “Crude Oil Prices Slump”, but a false entailment from “The government announced last week that it plans to raise oil prices”. We gather 10 annotations each for all 800 sentence pairs in the PASCAL RTE-1 dataset. For this dataset expert interannotator agreement studies have been reported as achieving 91% and 96% agreement ov</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman and Bernardo Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. Machine Learning Challenges. Lecture Notes in Computer Science, Vol. 3944, pp. 177-190, Springer, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wisam Dakka</author>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>Automatic Extraction of Useful Facet Terms from Text Documents. In</title>
<date>2008</date>
<booktitle>Proc. ofICDE-2008.</booktitle>
<contexts>
<context position="5707" citStr="Dakka and Ipeirotis (2008)" startWordPosition="861" endWordPosition="864">y used AMT without external gold standard comparisons. In (Nakov, 2008) workers generated paraphrases of 250 noun-noun compounds which were then used as the gold standard dataset for evaluating an automatic method of noun compound paraphrasing. Kaisser and Lowe (2008) use AMT to help build a dataset for question answering, annotating the answers to 8107 questions with the sentence containing the answer. Kaisser et al. (2008) examines the task of customizing the summary length of QA output; non-experts from AMT chose a summary length that suited their information needs for varying query types. Dakka and Ipeirotis (2008) evaluate a document facet generation system against AMT-supplied facets, and also use workers for user studies of the system. Sorokin and Forsyth (2008) collect data for machine vision tasks and report speed and costs similar to our findings; their summaries of worker behavior also corroborate with what we have found. In general, volunteer-supplied or AMT-supplied data is more plentiful but noisier than expert data. It is powerful because independent annotations can be aggregated to achieve high reliability. Sheng et al. (2008) explore several methods for using many noisy labels to create lab</context>
</contexts>
<marker>Dakka, Ipeirotis, 2008</marker>
<rawString>Wisam Dakka and Panagiotis G. Ipeirotis. 2008. Automatic Extraction of Useful Facet Terms from Text Documents. In Proc. ofICDE-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dawid</author>
<author>A M Skene</author>
</authors>
<title>Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm.</title>
<date>1979</date>
<journal>Applied Statistics,</journal>
<volume>28</volume>
<pages>20--28</pages>
<contexts>
<context position="24482" citStr="Dawid and Skene (1979)" startWordPosition="4033" endWordPosition="4036">ld higher quality overall judgments. In general, there are at least three ways to enhance quality in the face of worker error. More workers can be used, as described in previous sections. Another method is to use Amazon’s compensation mechanisms to give monetary bonuses to highlyperforming workers and deny payments to unreliable ones; this is useful, but beyond the scope of this paper. In this section we explore a third alternative, to model the reliability and biases of individual workers and correct for them. A wide number of methods have been explored to correct for the bias of annotators. Dawid and Skene (1979) are the first to consider the case of having multiple annotators per example but unknown true labels. They introduce an EM algorithm to simultaneously estimate annotator biases and latent label classes. Wiebe et al. (1999) analyze linguistic annotator agreement statistics to find bias, and use a similar model to correct labels. A large literature in biostatistics addresses this same problem for medical diagnosis. Albert and Dodd (2004) review several related models, but argue they have various shortcomings and emphasize instead the importance of having a gold standard. Here we take an approac</context>
</contexts>
<marker>Dawid, Skene, 1979</marker>
<rawString>A. P. Dawid and A. M. Skene. 1979. Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm. Applied Statistics, Vol. 28, No. 1 (1979), pp. 20-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kaisser</author>
<author>John B Lowe</author>
</authors>
<title>A Research Collection of QuestionAnswer Sentence Pairs. In</title>
<date>2008</date>
<booktitle>Proc. ofLREC-2008.</booktitle>
<contexts>
<context position="5349" citStr="Kaisser and Lowe (2008)" startWordPosition="801" endWordPosition="804">. (2008) compared AMT evaluations of Wikipedia article quality against experts, finding validation tests were important to ensure good results. Zaenen (Submitted) studied the agreement of annotators on the problem of recognizing textual entailment (a similar task and dataset is explained in more detail in Section 4). At least several studies have already used AMT without external gold standard comparisons. In (Nakov, 2008) workers generated paraphrases of 250 noun-noun compounds which were then used as the gold standard dataset for evaluating an automatic method of noun compound paraphrasing. Kaisser and Lowe (2008) use AMT to help build a dataset for question answering, annotating the answers to 8107 questions with the sentence containing the answer. Kaisser et al. (2008) examines the task of customizing the summary length of QA output; non-experts from AMT chose a summary length that suited their information needs for varying query types. Dakka and Ipeirotis (2008) evaluate a document facet generation system against AMT-supplied facets, and also use workers for user studies of the system. Sorokin and Forsyth (2008) collect data for machine vision tasks and report speed and costs similar to our findings</context>
</contexts>
<marker>Kaisser, Lowe, 2008</marker>
<rawString>Michael Kaisser and John B. Lowe. 2008. A Research Collection of QuestionAnswer Sentence Pairs. In Proc. ofLREC-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kaisser</author>
<author>Marti Hearst</author>
<author>John B Lowe</author>
</authors>
<title>Evidence for Varying Search Results Summary Lengths. In</title>
<date>2008</date>
<booktitle>Proc. ofACL-2008.</booktitle>
<contexts>
<context position="5509" citStr="Kaisser et al. (2008)" startWordPosition="829" endWordPosition="832"> studied the agreement of annotators on the problem of recognizing textual entailment (a similar task and dataset is explained in more detail in Section 4). At least several studies have already used AMT without external gold standard comparisons. In (Nakov, 2008) workers generated paraphrases of 250 noun-noun compounds which were then used as the gold standard dataset for evaluating an automatic method of noun compound paraphrasing. Kaisser and Lowe (2008) use AMT to help build a dataset for question answering, annotating the answers to 8107 questions with the sentence containing the answer. Kaisser et al. (2008) examines the task of customizing the summary length of QA output; non-experts from AMT chose a summary length that suited their information needs for varying query types. Dakka and Ipeirotis (2008) evaluate a document facet generation system against AMT-supplied facets, and also use workers for user studies of the system. Sorokin and Forsyth (2008) collect data for machine vision tasks and report speed and costs similar to our findings; their summaries of worker behavior also corroborate with what we have found. In general, volunteer-supplied or AMT-supplied data is more plentiful but noisier</context>
</contexts>
<marker>Kaisser, Hearst, Lowe, 2008</marker>
<rawString>Michael Kaisser, Marti Hearst, and John B. Lowe. 2008. Evidence for Varying Search Results Summary Lengths. In Proc. ofACL-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Katz</author>
<author>Matthew Singleton</author>
<author>Richard Wicentowski</author>
</authors>
<date>2007</date>
<booktitle>SWAT-MP: The SemEval-2007 Systems for Task 5 and Task 14. In Proc. of SemEval-2007.</booktitle>
<contexts>
<context position="28307" citStr="Katz et al., 2007" startWordPosition="4669" endWordPosition="4672"> we experimented with a similar calibration method for numeric data, using a Gaussian noise model for each worker: ywJx — N(x + pw, aw). On the affect task, this yielded a small but consistent increases in Pearson correlation at all numbers of annotators, averaging a +0.6% gain. 6 Training a system with non-expert annotations In this section we train a supervised affect recognition system with expert vs. non-expert annotations. 6.1 Experimental Design For the purpose of this experiment we create a simple bag-of-words unigram model for predicting affect and valence, similar to the SWAT system (Katz et al., 2007), one of the top-performing systems on the SemEval Affective Text task.7 For each token t in our training set, we assign t a weight for each emotion e equal to the average emotion score observed in each headline H that t participates in. i.e., if Ht is the set of headlines containing the token t, then: EHEHt Score(e, H) Score(e,t) = JHtJ RTE before/after Gold calibrated Naive voting accuracy 0.7 0.8 0.9 0.7 0.8 0.9 With these weights of the individual tokens we may then compute the score for an emotion e of a new headline H as the average score over the set of tokens t E H that we’ve observed </context>
</contexts>
<marker>Katz, Singleton, Wicentowski, 2007</marker>
<rawString>Phil Katz, Matthew Singleton, Richard Wicentowski. 2007. SWAT-MP: The SemEval-2007 Systems for Task 5 and Task 14. In Proc. of SemEval-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniket Kittur</author>
<author>Ed H Chi</author>
<author>Bongwon Suh</author>
</authors>
<title>Crowdsourcing user studies with Mechanical Turk.</title>
<date>2008</date>
<booktitle>In Proc. of CHI-2008.</booktitle>
<contexts>
<context position="4734" citStr="Kittur et al. (2008)" startWordPosition="707" endWordPosition="710">06). The Open Mind Initiative (Stork, 1999) has taken a similar approach, attempting to make such tasks as annotating word sense (Chklovski and Mihalcea, 2002) and commonsense word relations (Singh, 2002) sufficiently “easy and fun” to entice users into freely labeling data. There have been an increasing number of experiments using Mechanical Turk for annotation. In (Su et al., 2007) workers provided annotations for the tasks of hotel name entity resolution and attribute extraction of age, product brand, and product model, and were found to have high accuracy compared to gold-standard labels. Kittur et al. (2008) compared AMT evaluations of Wikipedia article quality against experts, finding validation tests were important to ensure good results. Zaenen (Submitted) studied the agreement of annotators on the problem of recognizing textual entailment (a similar task and dataset is explained in more detail in Section 4). At least several studies have already used AMT without external gold standard comparisons. In (Nakov, 2008) workers generated paraphrases of 250 noun-noun compounds which were then used as the gold standard dataset for evaluating an automatic method of noun compound paraphrasing. Kaisser </context>
</contexts>
<marker>Kittur, Chi, Suh, 2008</marker>
<rawString>Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008. Crowdsourcing user studies with Mechanical Turk. In Proc. of CHI-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics 19:2,</title>
<date>1993</date>
<contexts>
<context position="1540" citStr="Marcus et al., 1993" startWordPosition="221" endWordPosition="224">en Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense. 1 Introduction Large scale annotation projects such as TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), TimeBank (Pustejovsky et al., 2003), FrameNet (Baker et al., 1998), SemCor (Miller et al., 1993), and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms. The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost. Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them (Banko and Brill, 2001), one promising alternative for some tasks is the collection of </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics 19:2, June 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>William G Charles</author>
</authors>
<date>1991</date>
<booktitle>Contextual Correlates of Semantic Similarity. Language and Cognitive Processes,</booktitle>
<volume>6</volume>
<pages>1--28</pages>
<contexts>
<context position="14560" citStr="Miller and Charles, 1991" startWordPosition="2345" endWordPosition="2348"> for all tasks except “Fear” we are able to achieve expert-level ITA with the held-out set of experts within 9 labelers, and frequently within only 2 labelers. Pooling judgments across all 7 tasks we find that on average it requires only 4 non-expert annotations per example to achieve the equivalent ITA as a single expert annotator. Given that we paid US$2.00 in order to collect the 7000 non-expert annotations, we may interpret our rate of 3500 non-expert labels per USD as at least 875 expert-equivalent labels per USD. 4.2 Word Similarity This task replicates the word similarity task used in (Miller and Charles, 1991), following a previous Figure 1: Non-expert correlation for affect recognition task initially proposed by (Rubenstein and Goodenough, 1965). Specifically, we ask for numeric judgments of word similarity for 30 word pairs on a scale of [0,10], allowing fractional responses4. These word pairs range from highly similar (e.g., {boy, lad}), to unrelated (e.g., {noon, string}). Numerous expert and non-expert studies have shown that this task typically yields very high interannotator agreement as measured by Pearson correlation; (Miller and Charles, 1991) found a 0.97 correlation of the annotations o</context>
<context position="16463" citStr="Miller and Charles, 1991" startWordPosition="2675" endWordPosition="2678">0.45 0.55 0.65 correlation correlation 2 4 6 8 10 2 4 6 8 10 fear joy 0.35 0.45 0.55 0.65 0.40 0.50 0.60 0.70 correlation correlation 2 4 6 8 10 2 4 6 8 10 sadness 2 4 6 8 10 annotators surprise 2 4 6 8 10 annotators 0.20 0.30 0.40 0.50 correlation 0.55 0.65 0.75 correlation 257 utes from the time of submission of our task to AMT, at the rate of 1724 annotations / hour. As in the previous task we evaluate our nonexpert annotations by averaging the numeric responses from each possible subset of n annotators and computing the interannotator agreement with respect to the gold scores reported in (Miller and Charles, 1991). Our results are displayed in Figure 2, with Resnik’s 0.958 correlation plotted as the horizontal line; we find that at 10 annotators we achieve a correlation of 0.952, well within the range of other studies of expert and non-expert annotations. Word Similarity ITA 2 4 6 8 10 annotations Figure 2: ITA for word similarity experiment 4.3 Recognizing Textual Entailment This task replicates the recognizing textual entailment task originally proposed in the PASCAL Recognizing Textual Entailment task (Dagan et al., 2006); here for each question the annotator is presented with two sentences and give</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and William G. Charles. 1991. Contextual Correlates of Semantic Similarity. Language and Cognitive Processes, vol. 6, no. 1, pp. 1-28, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunke</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proc. ofHLT-1993.</booktitle>
<contexts>
<context position="1670" citStr="Miller et al., 1993" startWordPosition="241" endWordPosition="244">cognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense. 1 Introduction Large scale annotation projects such as TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), TimeBank (Pustejovsky et al., 2003), FrameNet (Baker et al., 1998), SemCor (Miller et al., 1993), and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms. The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost. Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them (Banko and Brill, 2001), one promising alternative for some tasks is the collection of non-expert annotations. In this work we explore the use of Amazon Mechanical Turk1 (AMT) to determine whether nonexpert labelers c</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunke, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunke. 1993. A semantic concordance. In Proc. ofHLT-1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Paraphrasing Verbs for Noun Compound Interpretation.</title>
<date>2008</date>
<booktitle>In Proc. of the Workshop on Multiword Expressions, LREC-2008.</booktitle>
<contexts>
<context position="5152" citStr="Nakov, 2008" startWordPosition="773" endWordPosition="774">sks of hotel name entity resolution and attribute extraction of age, product brand, and product model, and were found to have high accuracy compared to gold-standard labels. Kittur et al. (2008) compared AMT evaluations of Wikipedia article quality against experts, finding validation tests were important to ensure good results. Zaenen (Submitted) studied the agreement of annotators on the problem of recognizing textual entailment (a similar task and dataset is explained in more detail in Section 4). At least several studies have already used AMT without external gold standard comparisons. In (Nakov, 2008) workers generated paraphrases of 250 noun-noun compounds which were then used as the gold standard dataset for evaluating an automatic method of noun compound paraphrasing. Kaisser and Lowe (2008) use AMT to help build a dataset for question answering, annotating the answers to 8107 questions with the sentence containing the answer. Kaisser et al. (2008) examines the task of customizing the summary length of QA output; non-experts from AMT chose a summary length that suited their information needs for varying query types. Dakka and Ipeirotis (2008) evaluate a document facet generation system </context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Preslav Nakov. 2008. Paraphrasing Verbs for Noun Compound Interpretation. In Proc. of the Workshop on Multiword Expressions, LREC-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: A Corpus Annotated with Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--1</pages>
<contexts>
<context position="1572" citStr="Palmer et al., 2005" startWordPosition="226" endWordPosition="229">notations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense. 1 Introduction Large scale annotation projects such as TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), TimeBank (Pustejovsky et al., 2003), FrameNet (Baker et al., 1998), SemCor (Miller et al., 1993), and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms. The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost. Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them (Banko and Brill, 2001), one promising alternative for some tasks is the collection of non-expert annotations. In this </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The Proposition Bank: A Corpus Annotated with Semantic Roles. Computational Linguistics, 31:1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<date>2007</date>
<booktitle>SemEval-2007 Task-17: English Lexical Sample, SRL and All Words. In Proc. of SemEval-2007 .</booktitle>
<contexts>
<context position="20483" citStr="Pradhan et al., 2007" startWordPosition="3350" endWordPosition="3353">ple voting over 10 annotators (4620 total annotations). While an expert ITA of 0.77 was reported for the more general task involving all fourteen labels on both noun and verb events, no expert ITA numbers have been reported for this simplified temporal ordering task. Temp. Ordering ITA 2 4 6 8 10 annotators Figure 4: ITA for temporal ordering experiment 4.5 Word Sense Disambiguation In this task we consider a simple problem on which machine learning algorithms have been shown to produce extremely good results; here we annotate part of the SemEval Word Sense Disambiguation Lexical Sample task (Pradhan et al., 2007); specifically, we present the labeler with a paragraph of text containing the word “president” (e.g., a paragraph containing “Robert E. Lyons III...was appointed president and chief operating officer...”) and ask the labeler which one of the following three sense labels is most appropriate: 1) executive officer of a firm, corporation, or university 2) head of a country (other than the U.S.) 3) head of the U.S., President of the United States We collect 10 annotations for each of 177 examples of the noun “president” for the three senses given in SemEval. As shown in Figure 5, performing simple</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer Pradhan, Edward Loper, Dmitriy Dligach and Martha Palmer. 2007. SemEval-2007 Task-17: English Lexical Sample, SRL and All Words. In Proc. of SemEval-2007 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Saur</author>
<author>Andrew See</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
</authors>
<title>The TIMEBANK Corpus.</title>
<date>2003</date>
<booktitle>In Proc. of Corpus Linguistics</booktitle>
<pages>647--656</pages>
<institution>Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro and</institution>
<contexts>
<context position="1609" citStr="Pustejovsky et al., 2003" startWordPosition="231" endWordPosition="234">dard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense. 1 Introduction Large scale annotation projects such as TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), TimeBank (Pustejovsky et al., 2003), FrameNet (Baker et al., 1998), SemCor (Miller et al., 1993), and others play an important role in natural language processing research, encouraging the development of novel ideas, tasks, and algorithms. The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost. Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them (Banko and Brill, 2001), one promising alternative for some tasks is the collection of non-expert annotations. In this work we explore the use of Amazon Mec</context>
<context position="18162" citStr="Pustejovsky et al., 2003" startWordPosition="2956" endWordPosition="2960"> reported as achieving 91% and 96% agreement over various subsections of the corpus. When considering multiple non-expert annotations for a sentence pair we use simple majority voting, breaking ties randomly and averaging performance over all possible ways to break ties. We collect 10 annotations for each of 100 RTE sentence pairs; as displayed in Figure 3, we achieve a maximum accuracy of 89.7%, averaging over the annotations of 10 workers5. RTE ITA 2 4 6 8 10 annotations Figure 3: Inter-annotator agreement for RTE experiment 4.4 Event Annotation This task is inspired by the TimeBank corpus (Pustejovsky et al., 2003), which includes among its annotations a label for event-pairs that represents the temporal relation between them, from a set of fourteen relations (before, after, during, includes, etc.). We implement temporal ordering as a simplified version of the TimeBank event temporal annotation task: rather than annotating all fourteen event types, we restrict our consideration to the two simplest labels: “strictly before” and “strictly after”. Furthermore, rather than marking both nouns and verbs in the text as possible events, we only consider possible verb events. We extract the 462 verb event pairs </context>
</contexts>
<marker>Pustejovsky, Hanks, Saur, See, Gaizauskas, Setzer, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Saur, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro and Marcia Lazo. 2003. The TIMEBANK Corpus. In Proc. of Corpus Linguistics 2003, 647-656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity</title>
<date>1999</date>
<booktitle>in Natural Language. JAIR,</booktitle>
<volume>11</volume>
<pages>95--130</pages>
<contexts>
<context position="15290" citStr="Resnik, 1999" startWordPosition="2459" endWordPosition="2460"> and Goodenough, 1965). Specifically, we ask for numeric judgments of word similarity for 30 word pairs on a scale of [0,10], allowing fractional responses4. These word pairs range from highly similar (e.g., {boy, lad}), to unrelated (e.g., {noon, string}). Numerous expert and non-expert studies have shown that this task typically yields very high interannotator agreement as measured by Pearson correlation; (Miller and Charles, 1991) found a 0.97 correlation of the annotations of 38 subjects with the annotations given by 51 subjects in (Rubenstein and Goodenough, 1965), and a following study (Resnik, 1999) with 10 subjects found a 0.958 correlation with (Miller and Charles, 1991). In our experiment we ask for 10 annotations each of the full 30 word pairs, at an offered price of $0.02 for each set of 30 annotations (or, equivalently, at the rate of 1500 annotations per USD). The most surprising aspect of this study was the speed with which it was completed; the task of 300 annotations was completed by 10 annotators in less than 11 min4(Miller and Charles, 1991) and others originally used a numerical score of [0,4]. disgust anger 0.55 0.65 0.75 0.45 0.55 0.65 correlation correlation 2 4 6 8 10 2 </context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language. JAIR, Volume 11, pages 95-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual Correlates of Synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="14699" citStr="Rubenstein and Goodenough, 1965" startWordPosition="2363" endWordPosition="2367">tly within only 2 labelers. Pooling judgments across all 7 tasks we find that on average it requires only 4 non-expert annotations per example to achieve the equivalent ITA as a single expert annotator. Given that we paid US$2.00 in order to collect the 7000 non-expert annotations, we may interpret our rate of 3500 non-expert labels per USD as at least 875 expert-equivalent labels per USD. 4.2 Word Similarity This task replicates the word similarity task used in (Miller and Charles, 1991), following a previous Figure 1: Non-expert correlation for affect recognition task initially proposed by (Rubenstein and Goodenough, 1965). Specifically, we ask for numeric judgments of word similarity for 30 word pairs on a scale of [0,10], allowing fractional responses4. These word pairs range from highly similar (e.g., {boy, lad}), to unrelated (e.g., {noon, string}). Numerous expert and non-expert studies have shown that this task typically yields very high interannotator agreement as measured by Pearson correlation; (Miller and Charles, 1991) found a 0.97 correlation of the annotations of 38 subjects with the annotations given by 51 subjects in (Rubenstein and Goodenough, 1965), and a following study (Resnik, 1999) with 10 </context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual Correlates of Synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor S Sheng</author>
<author>Foster Provost</author>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>Get Another Label? Improving Data Quality and Data Mining Using Multiple, Noisy Labelers. In</title>
<date>2008</date>
<booktitle>Proc. ofKDD-2008.</booktitle>
<contexts>
<context position="6241" citStr="Sheng et al. (2008)" startWordPosition="945" endWordPosition="948">at suited their information needs for varying query types. Dakka and Ipeirotis (2008) evaluate a document facet generation system against AMT-supplied facets, and also use workers for user studies of the system. Sorokin and Forsyth (2008) collect data for machine vision tasks and report speed and costs similar to our findings; their summaries of worker behavior also corroborate with what we have found. In general, volunteer-supplied or AMT-supplied data is more plentiful but noisier than expert data. It is powerful because independent annotations can be aggregated to achieve high reliability. Sheng et al. (2008) explore several methods for using many noisy labels to create labeled data, how to choose which examples should get more labels, and how to include labels’ uncertainty information when training classifiers. Since we focus on empirically validating AMT as a data source, we tend to stick to simple aggregation methods. 3 Task Design In this section we describe Amazon Mechanical Turk and the general design of our experiments. 3.1 Amazon Mechanical Turk We employ the Amazon Mechanical Turk system in order to elicit annotations from non-expert labelers. AMT is an online labor market where workers a</context>
</contexts>
<marker>Sheng, Provost, Ipeirotis, 2008</marker>
<rawString>Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeirotis. 2008. Get Another Label? Improving Data Quality and Data Mining Using Multiple, Noisy Labelers. In Proc. ofKDD-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Push Singh</author>
</authors>
<title>The public acquisition of commonsense knowledge.</title>
<date>2002</date>
<booktitle>In Proc. of AAAI Spring Symposium on Acquiring (and Using) Linguistic (and World) Knowledge for Information Access,</booktitle>
<contexts>
<context position="4318" citStr="Singh, 2002" startWordPosition="643" endWordPosition="644"> October 2008.c�2008 Association for Computational Linguistics 2 Related Work The idea of collecting annotations from volunteer contributors has been used for a variety of tasks. Luis von Ahn pioneered the collection of data via online annotation tasks in the form of games, including the ESPGame for labeling images (von Ahn and Dabbish, 2004) and Verbosity for annotating word relations (von Ahn et al., 2006). The Open Mind Initiative (Stork, 1999) has taken a similar approach, attempting to make such tasks as annotating word sense (Chklovski and Mihalcea, 2002) and commonsense word relations (Singh, 2002) sufficiently “easy and fun” to entice users into freely labeling data. There have been an increasing number of experiments using Mechanical Turk for annotation. In (Su et al., 2007) workers provided annotations for the tasks of hotel name entity resolution and attribute extraction of age, product brand, and product model, and were found to have high accuracy compared to gold-standard labels. Kittur et al. (2008) compared AMT evaluations of Wikipedia article quality against experts, finding validation tests were important to ensure good results. Zaenen (Submitted) studied the agreement of anno</context>
</contexts>
<marker>Singh, 2002</marker>
<rawString>Push Singh. 2002. The public acquisition of commonsense knowledge. In Proc. of AAAI Spring Symposium on Acquiring (and Using) Linguistic (and World) Knowledge for Information Access, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Sorokin</author>
<author>David Forsyth</author>
</authors>
<title>Utility data annotation with Amazon Mechanical Turk. To appear in</title>
<date>2008</date>
<booktitle>Proc. of First IEEE Workshop on Internet Vision at CVPR,</booktitle>
<note>See also: http://vision.cs.uiuc.edu/annotation/</note>
<contexts>
<context position="5860" citStr="Sorokin and Forsyth (2008)" startWordPosition="885" endWordPosition="888">the gold standard dataset for evaluating an automatic method of noun compound paraphrasing. Kaisser and Lowe (2008) use AMT to help build a dataset for question answering, annotating the answers to 8107 questions with the sentence containing the answer. Kaisser et al. (2008) examines the task of customizing the summary length of QA output; non-experts from AMT chose a summary length that suited their information needs for varying query types. Dakka and Ipeirotis (2008) evaluate a document facet generation system against AMT-supplied facets, and also use workers for user studies of the system. Sorokin and Forsyth (2008) collect data for machine vision tasks and report speed and costs similar to our findings; their summaries of worker behavior also corroborate with what we have found. In general, volunteer-supplied or AMT-supplied data is more plentiful but noisier than expert data. It is powerful because independent annotations can be aggregated to achieve high reliability. Sheng et al. (2008) explore several methods for using many noisy labels to create labeled data, how to choose which examples should get more labels, and how to include labels’ uncertainty information when training classifiers. Since we fo</context>
</contexts>
<marker>Sorokin, Forsyth, 2008</marker>
<rawString>Alexander Sorokin and David Forsyth. 2008. Utility data annotation with Amazon Mechanical Turk. To appear in Proc. of First IEEE Workshop on Internet Vision at CVPR, 2008. See also: http://vision.cs.uiuc.edu/annotation/</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Stork</author>
</authors>
<title>The Open Mind Initiative.</title>
<date>1999</date>
<booktitle>IEEE Expert Systems and Their Applications</booktitle>
<pages>16--20</pages>
<contexts>
<context position="4157" citStr="Stork, 1999" startWordPosition="618" endWordPosition="619"> http://blog.doloreslabs.com/topics/wisdom/. 254 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 254–263, Honolulu, October 2008.c�2008 Association for Computational Linguistics 2 Related Work The idea of collecting annotations from volunteer contributors has been used for a variety of tasks. Luis von Ahn pioneered the collection of data via online annotation tasks in the form of games, including the ESPGame for labeling images (von Ahn and Dabbish, 2004) and Verbosity for annotating word relations (von Ahn et al., 2006). The Open Mind Initiative (Stork, 1999) has taken a similar approach, attempting to make such tasks as annotating word sense (Chklovski and Mihalcea, 2002) and commonsense word relations (Singh, 2002) sufficiently “easy and fun” to entice users into freely labeling data. There have been an increasing number of experiments using Mechanical Turk for annotation. In (Su et al., 2007) workers provided annotations for the tasks of hotel name entity resolution and attribute extraction of age, product brand, and product model, and were found to have high accuracy compared to gold-standard labels. Kittur et al. (2008) compared AMT evaluatio</context>
</contexts>
<marker>Stork, 1999</marker>
<rawString>David G. Stork. 1999. The Open Mind Initiative. IEEE Expert Systems and Their Applications pp. 16-20, May/June 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Rada Mihalcea</author>
</authors>
<date>2007</date>
<booktitle>SemEval2007 Task 14: Affective Text In Proc. of SemEval2007.</booktitle>
<contexts>
<context position="9600" citStr="Strapparava and Mihalcea (2007)" startWordPosition="1499" endWordPosition="1502"> of how data quality improves with the number of independent annotations. 4 Annotation Tasks We analyze the quality of non-expert annotations on five tasks: affect recognition, word similarity, recognizing textual entailment, temporal event recognition, and word sense disambiguation. In this section we define each annotation task and the parameters of the annotations we request using AMT. Additionally we give an initial analysis of the task results, and summarize the cost of the experiments. 4.1 Affective Text Analysis This experiment is based on the affective text annotation task proposed in Strapparava and Mihalcea (2007), wherein each annotator is presented with a list of short headlines, and is asked to give numeric judgments in the interval [0,100] rating the headline for six emotions: anger, disgust, fear, joy, sadness, and surprise, and a single numeric rating in the interval [-100,100] to denote the overall positive or negative valence of the emotional content of the headline, as in this sample headline-annotation pair: Outcry at N Korea ‘nuclear test’ (Anger, 30), (Disgust,30), (Fear,30), (Joy,0), (Sadness,20), (Surprise,40), (Valence,-50). 3All tasks and collected data are available at http://ai.stanfo</context>
</contexts>
<marker>Strapparava, Mihalcea, 2007</marker>
<rawString>Carlo Strapparava and Rada Mihalcea. 2007. SemEval2007 Task 14: Affective Text In Proc. of SemEval2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Su</author>
<author>Dmitry Pavlov</author>
<author>Jyh-Herng Chow</author>
<author>Wendell C Baker</author>
</authors>
<title>Internet-Scale Collection of HumanReviewed Data.</title>
<date>2007</date>
<booktitle>In Proc. of WWW-2007.</booktitle>
<contexts>
<context position="4500" citStr="Su et al., 2007" startWordPosition="671" endWordPosition="674"> Luis von Ahn pioneered the collection of data via online annotation tasks in the form of games, including the ESPGame for labeling images (von Ahn and Dabbish, 2004) and Verbosity for annotating word relations (von Ahn et al., 2006). The Open Mind Initiative (Stork, 1999) has taken a similar approach, attempting to make such tasks as annotating word sense (Chklovski and Mihalcea, 2002) and commonsense word relations (Singh, 2002) sufficiently “easy and fun” to entice users into freely labeling data. There have been an increasing number of experiments using Mechanical Turk for annotation. In (Su et al., 2007) workers provided annotations for the tasks of hotel name entity resolution and attribute extraction of age, product brand, and product model, and were found to have high accuracy compared to gold-standard labels. Kittur et al. (2008) compared AMT evaluations of Wikipedia article quality against experts, finding validation tests were important to ensure good results. Zaenen (Submitted) studied the agreement of annotators on the problem of recognizing textual entailment (a similar task and dataset is explained in more detail in Section 4). At least several studies have already used AMT without </context>
</contexts>
<marker>Su, Pavlov, Chow, Baker, 2007</marker>
<rawString>Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and Wendell C. Baker. 2007. Internet-Scale Collection of HumanReviewed Data. In Proc. of WWW-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling Images with a Computer Game.</title>
<date>2004</date>
<booktitle>In ACM Conference on Human Factors in Computing Systems,</booktitle>
<location>CHI</location>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2004. Labeling Images with a Computer Game. In ACM Conference on Human Factors in Computing Systems, CHI 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Mihir Kedia</author>
<author>Manuel Blum</author>
</authors>
<title>Verbosity: A Game for Collecting Common-Sense Knowledge.</title>
<date>2006</date>
<booktitle>In ACM Conference on Human Factors in Computing Systems, CHI Notes</booktitle>
<marker>von Ahn, Kedia, Blum, 2006</marker>
<rawString>Luis von Ahn, Mihir Kedia and Manuel Blum. 2006. Verbosity: A Game for Collecting Common-Sense Knowledge. In ACM Conference on Human Factors in Computing Systems, CHI Notes 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
<author>Hoa Trang Dang</author>
</authors>
<title>question answering track.</title>
<date>2006</date>
<journal>Overview of the TREC</journal>
<booktitle>In Proc. of TREC-2005.</booktitle>
<marker>Voorhees, Dang, 2006</marker>
<rawString>Ellen Voorhees and Hoa Trang Dang. 2006. Overview of the TREC 2005 question answering track. In Proc. of TREC-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
<author>Rebecca F Bruce</author>
<author>Thomas P O’Hara</author>
</authors>
<title>Development and use of a goldstandard data set for subjectivity classifications. In</title>
<date>1999</date>
<booktitle>Proc. ofACL-1999.</booktitle>
<marker>Wiebe, Bruce, O’Hara, 1999</marker>
<rawString>Janyce M. Wiebe, Rebecca F. Bruce and Thomas P. O’Hara. 1999. Development and use of a goldstandard data set for subjectivity classifications. In Proc. ofACL-1999.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Submitted</author>
</authors>
<title>Do give a penny for their thoughts.</title>
<journal>International Journal of Natural Language Engineering</journal>
<marker>Submitted, </marker>
<rawString>Annie Zaenen. Submitted. Do give a penny for their thoughts. International Journal of Natural Language Engineering (submitted).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>