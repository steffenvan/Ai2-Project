<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001258">
<title confidence="0.967463">
Head-Driven PCFGs with Latent-Head Statistics
</title>
<author confidence="0.982381">
Detlef Prescher
</author>
<affiliation confidence="0.997842">
Institute for Logic, Language and Computation
University of Amsterdam
</affiliation>
<email confidence="0.986478">
prescher@science.uva.nl
</email>
<sectionHeader confidence="0.997282" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998834">
Although state-of-the-art parsers for nat-
ural language are lexicalized, it was re-
cently shown that an accurate unlexical-
ized parser for the Penn tree-bank can be
simply read off a manually refined tree-
bank. While lexicalized parsers often suf-
fer from sparse data, manual mark-up is
costly and largely based on individual lin-
guistic intuition. Thus, across domains,
languages, and tree-bank annotations, a
fundamental question arises: Is it possible
to automatically induce an accurate parser
from a tree-bank without resorting to full
lexicalization? In this paper, we show how
to induce head-driven probabilistic parsers
with latent heads from a tree-bank. Our
automatically trained parser has a perfor-
mance of 85.7% (LP/LR Fl), which is al-
ready better than that of early lexicalized
ones.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978">
State-of-the-art statistical parsers for natural lan-
guage are based on probabilistic grammars acquired
from transformed tree-banks. The method of trans-
forming the tree-bank is of major influence on the
accuracy and coverage of the statistical parser. The
most important tree-bank transformation in the lit-
erature is lexicalization: Each node in a tree is la-
beled with its head word, the most important word of
the constituent under the node (Magerman (1995),
Collins (1996), Charniak (1997), Collins (1997),
Carroll and Rooth (1998), etc.). It turns out, how-
ever, that lexicalization is not unproblematic: First,
there is evidence that full lexicalization does not
carry over across different tree-banks for other lan-
guages, annotations or domains (Dubey and Keller,
2003). Second, full lexicalization leads to a serious
sparse-data problem, which can only be solved by
sophisticated smoothing and pruning techniques.
Recently, Klein and Manning (2003) showed that
a carefully performed linguistic mark-up of the tree-
bank leads to almost the same performance results as
lexicalization. This result is attractive since unlexi-
calized grammars are easy to estimate, easy to parse
with, and time- and space-efficient: Klein and Man-
ning (2003) do not smooth grammar-rule probabil-
ities, except unknown-word probabilities, and they
do not prune since they are able to determine the
most probable parse of each full parse forest. Both
facts are noteworthy in the context of statistical pars-
ing with a tree-bank grammar. A drawback of their
method is, however, that manual linguistic mark-up
is not based on abstract rules but rather on individual
linguistic intuition, which makes it difficult to repeat
their experiment and to generalize their findings to
languages other than English.
Is it possible to automatically acquire a more re-
fined probabilistic grammar from a given tree-bank
without resorting to full lexicalization? We present
a novel method that is able to induce a parser that
is located between two extremes: a fully-lexicalized
parser on one side versus an accurate unlexicalized
parser based on a manually refined tree-bank on the
other side.
In short, our method is based on the same lin-
guistic principles of headedness as other methods:
We do believe that lexical information represents
an important knowledge source. To circumvent
data sparseness resulting from full lexicalization
</bodyText>
<page confidence="0.98617">
115
</page>
<note confidence="0.682592">
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 115–124,
Vancouver, October 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.99994379245283">
with words, we simply follow the suggestion of
various advanced linguistic theories, e.g. Lexical-
Functional Grammar (Bresnan and Kaplan, 1982),
where more complex categories based on feature
combinations represent the lexical effect. We com-
plement this by a learning paradigm: lexical entries
carry latent information to be used as head informa-
tion, and this head information is induced from the
tree-bank.
In this paper, we study two different latent-head
models, as well as two different estimation meth-
ods: The first model is built around completely hid-
den heads, whereas the second one uses relatively
fine-grained combinations of Part-Of-Speech (POS)
tags with hidden extra-information; The first esti-
mation method selects a head-driven probabilistic
context-free grammar (PCFG) by exploiting latent-
head distributions for each node in the tree-bank,
whereas the second one is more traditional, reading
off the grammar from the tree-bank annotated with
the most probable latent heads only. In other words,
both models and estimation methods differ in the de-
gree of information incorporated into them as prior
knowledge. In general, it can be expected that the
better (sharper or richer, or more accurate) the in-
formation is, the better the induced grammar will
be. Our empirical results, however, are surprising:
First, estimation with latent-head distributions out-
performs estimation with most-probable-head anno-
tation. Second, modeling with completely hidden
heads is almost as good as modeling with latent
heads based on POS tags, and moreover, results in
much smaller grammars.
We emphasize that our task is to automatically in-
duce a more refined grammar based on a few linguis-
tic principles. With automatic refinement it is harder
to guarantee improved performance than with man-
ual refinements (Klein and Manning, 2003) or with
refinements based on direct lexicalization (Mager-
man (1995), Collins (1996), Charniak (1997), etc.).
If, however, our refinement provides improved per-
formance then it has a clear advantage: it is automat-
ically induced, which suggests that it is applicable
across different domains, languages and tree-bank
annotations.
Applying our method to the benchmark Penn tree-
bank Wall-Street Journal, we obtain a refined proba-
bilistic grammar that significantly improves over the
original tree-bank grammar and that shows perfor-
mance that is on par with early work on lexicalized
probabilistic grammars. This is a promising result
given the hard task of automatic induction of im-
proved probabilistic grammars.
</bodyText>
<sectionHeader confidence="0.940826" genericHeader="method">
2 Head Lexicalization
</sectionHeader>
<bodyText confidence="0.998294775">
As previously shown (Charniak (1997), Collins
(1997), Carroll and Rooth (1998), etc.), Context-
Free Grammars (CFGs) can be transformed to lexi-
calized CFGs, provided that a head-marking scheme
for rules is given. The basic idea is that the head
marking on rules is used to project lexical items up
a chain of nodes. Figure 1 displays an example.
In this Section, we focus on the approaches of
Charniak (1997) and Carroll and Rooth (1998).
These approaches are especially attractive for us for
two reasons: First, both approaches make use of an
explicit linguistic grammar. By contrast, alternative
approaches, like Collins (1997), apply an additional
transformation to each tree in the tree-bank, splitting
each rule into small parts, which finally results in a
new grammar covering many more sentences than
the explicit one. Second, Charniak (1997) and Car-
roll and Rooth (1998) rely on almost the same lex-
icalization technique. In fact, the significant differ-
ence between them is that, in one case, a lexicalized
version of the tree-bank grammar is learned from
a corpus of trees (supervised learning), whereas, in
the other case, a lexicalized version of a manually
written CFG is learned from a a text corpus (un-
supervised learning). As we will see in Section 3,
our approach is a blend of these approaches in that
it aims at unsupervised learning of a (latent-head-)
lexicalized version of the tree-bank grammar.
Starting with Charniak (1997), Figure 2 displays
an internal rule as it is used in the parse in Figure1,
and its probability as defined by Charniak. Here, H
is the head-child of the rule, which inherits the head
h from its parent C. The children D1:d1, ..., Dm:dm
and Dm+1:dm+1, ..., Dm+n:dm+n are left and right
modifiers of H. Either n or m may be zero, and
n = m = 0 for unary rules. Because the probabil-
ities occurring in Charniak’s definition are already
so specific that there is no real chance of obtaining
the data empirically, they are smoothed by deleted
interpolation:
</bodyText>
<page confidence="0.994927">
116
</page>
<figure confidence="0.9993013">
S:rose
ADJ:Corporate
Corporate
N:profits
profits
VP:rose
V:rose
rose
PUNC:.
.
NP:profits
Internal Rules:
S:rose NP:profits VP:rose PUNC:.
NP:profits ADJ:Corporate N:profits
VP:rose V:rose
Lexical Rules:
ADJ:Corporate Corporate
N:profits profits
V:rose rose
PUNC:. .
</figure>
<figureCaption confidence="0.996286">
Figure 1: Parse tree, and a list of the rules it contains (Charniak, 1997)
</figureCaption>
<figure confidence="0.662102833333333">
C:h
D1:d1 ··· Dm:dm H:h Dm+1:dm+1 ··· Dm+n:dm+n
pCHARNIAK97( this local tree) = p( r  |C, h, Cp ) x �n+m
i=1 p( di  |Di, C, h )
(r is the unlexicalized rule,
Cp is C’s parent category)
</figure>
<figureCaption confidence="0.944071">
Figure 2: Internal rule, and its probability (Charniak, 1997)
</figureCaption>
<equation confidence="0.995400555555556">
p( r  |C, h, Cp ) = A1 · �p( r  |C, h, Cp )
+ A2 · P( r  |C, h )
+ A3 · p( r  |C, class(h) )
+ A4 · p( r  |C, Cp )
+ A5 · p( r  |C )
p( d  |D, C, h ) = A1 · p( d  |D, C, h )
+ A2 · p( d  |D, C, class(h) )
+ A3 · p(d  |D, C )
+ A4 · p( d  |D )
</equation>
<bodyText confidence="0.999987032258065">
Here, class(h) denotes a class for the head word
h. Charniak takes these word classes from an ex-
ternal distributional clustering model, but does not
describe this model in detail.
An at a first glance different lexicalization tech-
nique is described in Carroll and Rooth (1998). In
their approach, a grammar transformation is used
to lexicalize a manually written grammar. The key
step for understanding their model is to imagine that
the rule in Figure 2 is transformed to a sub-tree, the
one displayed in Figure 3. After this transformation,
the sub-tree probability is simply calculated with the
PCFG’s standard model; The result is also displayed
in the figure. Comparing this probability with the
probability that Charniak assigns to the rule itself,
we see that the subtree probability equals the rule
probability1. In other words, both probability mod-
els are based on the same idea for lexicalization, but
the type of the corpora they are estimated from differ
(trees versus sentences).
In more detail, Table 1 displays all four grammar-
rule types resulting from the grammar transforma-
tion of Carroll and Rooth (1998). The underlying
entities from the original CFG are: The starting sym-
bol S (also the starting symbol of the transform),
the internal rule C −→ D1 ...Dm H Dm+1 ...Dm+n,
and the lexical rule C −→ w. From these, the
context-free transforms are generated as displayed
in the table (for all possible head words h and d, and
for all non-head children D=D1, ..., Dm+n). Fig-
ure 4 displays an example parse on the basis of the
</bodyText>
<footnote confidence="0.97249925">
1at least, if we ignore Charniak’s conditioning on C’s parent
category C, for the moment; Note that C’s parent category is
available in the tree-bank, but may not occur in the left-hand
sides of the rules of a manually written CFG
</footnote>
<page confidence="0.984992">
117
</page>
<bodyText confidence="0.290136">
C:h
</bodyText>
<equation confidence="0.8448401">
D1:C:h ··· Dm:C:h H:h Dm+1:C:h · · · Dm+n:C:h
D1:d1 Dm:dm Dm+1:dm+1 Dm+n:dm+n
pSTANDARD-PCFG( this sub-tree )
� p( D1:C:h ... Dm:C:h H:h Dm+1:C:h ... Dm+n:C: h  |C:h ) × �m+n
i=1 p( Di:di  |Di:C:h )
� p( D1 ... Dm H Dm+1 ... Dm+n  |C, h ) × �m+n
i=1 p( di  |Di, C, h )
� p( r  |C, h ) × �m+n
i=1 p( di  |Di, C, h )
(r is the unlexicalized rule)
</equation>
<figureCaption confidence="0.998633">
Figure 3: Transformed internal rule, and its standard-PCFG probability (Carroll and Rooth, 1998)
</figureCaption>
<figure confidence="0.856844">
S S:h (Starting Rules)
C:h D1:C:h ... D,,t:C:h H:h D,,t+1:C:h ... D,,t+,,,:C:h (Lexicalized Rules)
D:C:h D:d (Dependencies)
C:w w (Lexical Rules)
</figure>
<tableCaption confidence="0.974281">
Table 1: Context-free rule types in the transform (Carroll and Rooth, 1998)
</tableCaption>
<figure confidence="0.990544666666667">
S
S:rose
ADJ:NP:profits
ADJ:Corporate
Corporate
N:profits
profits
VP:rose
V:rose
rose
PUNC:S:rose
PUNC:.
.
NP:S:rose
NP:profits
</figure>
<table confidence="0.9786084">
Starting Rule:
S −→ S:rose
Lexicalized Rules:
S:rose −→ NP:S:rose VP:rose PUNC:S:rose
NP:profits −→ ADJ:NP:profits N:profits
VP:rose −→ V:rose
Dependencies:
NP:S:rose −→ NP:profits
PUNC:S:rose −→ PUNC:.
ADJ:NP:profits −→ ADJ:Corporate
Lexical Rules:
ADJ:Corporate −→ Corporate
N:profits −→ profits
V:rose −→ rose
PUNC:. −→ .
</table>
<figureCaption confidence="0.990851">
Figure 4: Transformed parse tree, and a list of the rules it contains (Carroll and Rooth, 1998)
</figureCaption>
<page confidence="0.991194">
118
</page>
<bodyText confidence="0.99834508">
transformed grammar. It is noteworthy that although
Carroll and Rooth (1998) learn from a text corpus
of about 50 million words, it is still necessary to
smooth the rule probabilities of the transform. Un-
like Charniak (1997), however, they do not use word
classes in their back-off scheme.
To summarize, the major problem of full-
lexicalization techniques is that they lead to serious
sparse-data problems. For both models presented in
this section, a large number |T |of full word forms
makes it difficult to reliably estimate the probability
weights of the O(|T|2) dependencies and the O(|T|)
lexicalized rules.
A linguistically naive approach to this problem
is to use POS tags as heads to decrease the num-
ber of heads. From a computational perspective,
the sparse data problem would then be completely
solved since the number |POS |of POS tags is tiny
compared to the number |T |of full-word forms.
Although we will demonstrate that parsing results
benefit already from this naive lexicalization rou-
tine, we expect that (computationally and linguisti-
cally) optimal head-lexicalized models are arranged
around a number |HEADS |of head elements such
that |POS |&lt; |HEADS |&lt;&lt; |T  |.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="method">
3 Latent-Head Models
</sectionHeader>
<bodyText confidence="0.999985444444444">
This section defines two probability models over the
trees licensed by a head-lexicalized CFG with latent
head-information, thereby exploiting three simple
linguistic principles: (i) all rules have head mark-
ers, (ii) information is projected up a chain of cat-
egories marked as heads, (iii) lexical entries carry
latent head values which can be learned. Moreover,
two estimation methods for the latent-head models
are described.
</bodyText>
<subsectionHeader confidence="0.893902">
Head-Lexicalized CFGs with Latent Heads
</subsectionHeader>
<bodyText confidence="0.999877458333334">
Principles (i) and (ii) are satisfied by all head lexical-
ized models we know of, and clearly, they are also
satisfied by the model of Carroll and Rooth (1998).
Principle (iii), however, deals with latent informa-
tion for lexical entries, which is beyond the capabil-
ity of this model. To see this, remember that lex-
ical rules C ) w are unambiguously transformed
to C:w ) w. Because this transformation is unam-
biguous, latent information does not play a role in it.
It is surprisingly simple, however, to satisfy princi-
ple (iii) with slightly modified versions of Carroll
and Rooth’s transformation of lexical rules. In the
following, we present two of them:
Lexical-Rule Transformation (Model 1): Trans-
form each lexical rule C ) w to a set of rules, hav-
ing the form C:h ) w, where h E {1, ... , L}, and
L is a free parameter.
Lexical-Rule Transformation (Model 2): Trans-
form each lexical rule C ) w to a set of rules,
having the form C:h ) w, where h E {C} x
{1, ... , L}, and L is a free parameter.
Both models introduce latent heads for lexical en-
tries. The difference is that Model 1 introduces com-
pletely latent heads h, whereas Model 2 introduces
heads h on the basis of the POS tag C of the word
w: each such head is a combination of C with an ab-
stract extra-information. Figure 5 gives an example.
Because we still apply Carroll and Rooth’s gram-
mar transformation scheme to the non-lexical rules,
latent heads are percolated up a path of categories
marked as heads.
Although our modifications are small, their ef-
fect is remarkable. In contrast to Carroll and Rooth
(1998), where an unlexicalized tree is unambigu-
ously mapped to a single transform, our models map
an unlexicalized tree to multiple transforms (for free
parameters &gt; 2). Note also that although latent in-
formation is freely introduced at the lexical level, it
is not freely distributed over the nodes of the tree.
Rather, the space of latent heads for a tree is con-
strained according the linguistic principle of head-
edness. Finally, for the case L = 1, our models per-
form unambiguous transformations: in Model 1 the
transformation makes no relevant changes, whereas
Model 2 performs unambiguous lexicalization with
POS tags. In the rest of the paper, we show how to
learn models with hidden, richer, and more accurate
head-information from a tree-bank, if L &gt; 2.
</bodyText>
<subsectionHeader confidence="0.8841575">
Unsupervised Estimation of Head-Lexicalized
CFGs with Latent Heads
</subsectionHeader>
<bodyText confidence="0.982221">
In the following, we define two methods for es-
timating latent-head models. The main difficulty
here is that the rules of a head-lexicalized CFG
</bodyText>
<page confidence="0.993532">
119
</page>
<figure confidence="0.955108129032258">
S
S:hV
NP:S:hV VP:hV PUNC:S:hV
NP:hN V:hV PUNC:hPUNC
ADJ:NP:hN N:hN rose .
ADJ:hADJ profits
Corporate
Starting Rule:
S −→ S:hV
Lexicalized Rules:
S:hV −→ NP:S:hV VP:hV PUNC:S:hV
NP:hN −→ ADJ:NP:hN N:hN
VP:hV −→ V:hV
Dependencies:
NP:S:hV −→ NP:hN
PUNC:S:hV −→ PUNC:hPUNC
ADJ:NP:hN −→ ADJ:hADJ
Lexical Rules:
ADJ:hADJ −→ Corporate
N:hN −→ profits
V:hV −→ rose
PUNC:hPUNC −→ .
Model 1 (Completely Latent Heads): Model 2 (Latent Heads Based on POS Tags):
hADJ, hN, hV, and hPUNC ∈ {1, ... , L} hADJ ∈ {ADJ} × {1, ... , L}
hN ∈ {N} × {1, . . . , L}
hV ∈ {V} × {1, . . . , L}
hPUNC ∈ {PUNC} × {1, ... , L}
�
L for Model 1
Number of Latent-Head Types = (L is a free parameter)
POS |× L for Model 2
</figure>
<figureCaption confidence="0.999085">
Figure 5: Parse tree with latent heads, and a list of the rules it contains.
</figureCaption>
<page confidence="0.957543">
120
</page>
<bodyText confidence="0.772041">
Initialization: Generate a randomly initialized distribution po for the rules of GLEX (a head-
lexicalized CFG with latent heads as previously defined).
Iterations:
</bodyText>
<listItem confidence="0.940453363636364">
(1) for each i = 1, 2, 3, ..., number of iterations do
(2) set p = pi_1
(3) E step: Generate a lexicalized tree-bank TLEX, by
- running over all unlexicalized trees t of the original tree-bank
- generating the finite set GLEX(t) of the lexicalized transforms of t
- allocating the frequency c(t&apos;) = c(t) · p( t&apos;  |t ) to the lexicalized trees t&apos; E GLEX(t)
[ Here, c(t) is the frequency of t in the original tree-bank ]
(4) M step: Read the tree-bank grammar off TLEX, by
- calculating relative frequencies p� for all rules of GLEX as occurring in TLEX
(5) set pi = p�
(6) end
</listItem>
<figureCaption confidence="0.99677">
Figure 6: Grammar induction algorithm (EM algorithm)
</figureCaption>
<bodyText confidence="0.999746627906977">
with latent heads cannot be directly estimated from
the tree-bank (by counting rules) since the latent
heads are not annotated in the trees. Faced with this
incomplete-data problem, we apply the Expectation-
Maximization (EM) algorithm developed for these
type of problems (Dempster et al., 1977). For details
of the EM algorithm, we refer to the numerous tuto-
rials on EM (e.g. Prescher (2003)). Here, it suffices
to know that it is a sort of meta algorithm, result-
ing for each incomplete-data problem in an iterative
estimation method that aims at maximum-likelihood
estimation on the data. Disregarding the fact that we
implement a dynamic-programming version for our
experiments (running in linear time in the size of the
trees in the tree-bank (Prescher, 2005)), the EM al-
gorithm is here as displayed in Figure 6. Beside this
pure form of the EM algorithm, we also use a variant
where the original tree-bank is annotated with most
probable heads only. Here is a characterization of
both estimation methods:
Estimation from latent-head distributions: The
key steps of the EM algorithm produce a lexicalized
tree-bank TLEX, consisting of all lexicalized versions
of the original trees (E-step), and calculate the prob-
abilities for the rules of GLEX on the basis of TLEX
(M-step). Clearly, all lexicalized trees in GLEX(t)
differ only in the heads of their nodes. Thus, EM
estimation uses the original tree-bank, where each
node can be thought of as annotated with a latent-
head distribution.
Estimation from most probable heads: By con-
trast, a quite different scheme is applied in Klein and
Manning (2003): extensive manual annotation en-
riches the tree-bank with information, but no trees
are added to the tree-bank. We borrow from this
scheme in that we take the best EM model to cal-
culate the most probable head-lexicalized versions
of the trees in the original tree-bank. After collect-
ing this Viterbi-style lexicalized tree-bank, the ordi-
nary tree-bank estimation yields another estimate of
GLEX. Clearly, this estimation method uses the orig-
inal tree-bank, where each node can be thought of
annotated with the most probable latent head.
</bodyText>
<sectionHeader confidence="0.999175" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9971145">
This section presents empirical results across our
models and estimation methods.
</bodyText>
<subsectionHeader confidence="0.921427">
Data and Parameters
</subsectionHeader>
<bodyText confidence="0.999992">
To facilitate comparison with previous work, we
trained our models on sections 2-21 of the WSJ sec-
tion of the Penn tree-bank (Marcus et al., 1993). All
trees were modified such that: The empty top node
got the category TOP, node labels consisted solely
of syntactic category information, empty nodes (i.e.
nodes dominating the empty string) were deleted,
and words in rules occurring less than 3 times in
the tree-bank were replaced by (word-suffix based)
</bodyText>
<page confidence="0.995307">
121
</page>
<table confidence="0.985672625">
Estimation from most probable heads Estimation from head distributions
Model 1 Model 2 Model 1 Model 2
(completely latent) (POS+latent) (completely latent) (POS+latent)
baseline (15 400) 73.5 (25 000) 78.9 (15 400) 73.5 (25 000) 78.9
L=2 (17 900) 76.3 (32 300) 81.1 (25 900) 76.9 (49 500) 81.6
L=5 (22 800) 80.7 (46 200) 83.3 (49 200) 82.0 (116 300) 84.9
L=10 (28 100) 83.3 (58 900) 82.6 (79 200) 84.6 (224 300) 85.7
A=9.8 A=4.4 A=11.1 A=6.8
</table>
<tableCaption confidence="0.999473">
Table 2: Parsing results in LP/LR F1 (the baseline is L = 1)
</tableCaption>
<bodyText confidence="0.998472227272727">
unknown-word symbols. No other changes were
made.
On this tree-bank, we trained several head-
lexicalized CFGs with latent-heads as described in
Section 3, but smoothed the grammar rules using
deleted interpolation; We also performed some pre-
liminary experiments without smoothing, but after
observing that about 3000 trees of our training cor-
pus were allocated a zero-probability (resulting from
the fact that too many grammar rules got a zero-
probability), we decided to smooth all rule proba-
bilities.
We tried to find optimal starting parameters by re-
peating the whole training process multiple times,
but we observed that starting parameters affect fi-
nal results only up to 0.5%. We also tried to find
optimal iteration numbers by evaluating our models
after each iteration step on a held-out corpus, and
observed that the best results were obtained with 70
to 130 iterations. Within a wide range from 50 to
200 iteration, however, iteration numbers affect fi-
nal results only up to 0.5%
</bodyText>
<subsectionHeader confidence="0.614943">
Empirical Results
</subsectionHeader>
<bodyText confidence="0.99997724">
We evaluated on a parsing task performed on Sec-
tion 22 of the WSJ section of the Penn tree-bank. For
parsing, we mapped all unknown words to unknown
word symbols, and applied the Viterbi algorithm as
implemented in Schmid (2004), exploiting its abil-
ity to deal with highly-ambiguous grammars. That
is, we did not use any pruning or smoothing routines
for parsing sentences. We then de-transformed the
resulting maximum-probability parses to the format
described in the previous sub-section. That is, we
deleted the heads, the dependencies, and the start-
ing rules. All grammars were able to exhaustively
parse the evaluation corpus. Table 2 displays our re-
sults in terms of LP/LR F1 (Black and al., 1991).
The largest number per column is printed in italics.
The absolutely largest number is printed in boldface.
The numbers in brackets are the number of gram-
mar rules (without counting lexical rules). The gain
in LP/LR F1 per estimation method and per model
is also displayed (A). Finally, the average training
time per iteration ranges from 2 to 4 hours (depend-
ing on both L and the type of the model). The aver-
age parsing time is 10 seconds per sentence, which
is comparable to what is reported in Klein and Man-
ning (2003).
</bodyText>
<sectionHeader confidence="0.998926" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999958578947369">
First of all, all model instances outperform the base-
line, i.e., the original grammar (F1=73.5), and the
head-lexicalized grammar with POS tags as heads
(F1=78.9). The only plausible explanation for these
significant improvements is that useful head classes
have been learned by our method. Moreover, in-
creasing L consistently increases F1 (except for
Model 2 estimated from most probable heads; L =
10 is out of the row). We thus argue that the granu-
larity of the current head classes is not fine enough;
Further refinement may lead to even better latent-
head statistics.
Second, estimation from head distributions con-
sistently outperforms estimation from most probable
heads (for both models). Although coarse-grained
models clearly benefit from POS information in the
heads (L = 1, 2, 5), it is surprising that the best
models with completely latent heads are on a par
with or almost as good as the best ones using POS
</bodyText>
<page confidence="0.988499">
122
</page>
<table confidence="0.998893444444444">
LP LR Fi Exact CB
Model 1 (this paper) 84.8 84.4 84.6 26.4 1.37
Magerman (1995) 84.9 84.6 1.26
Model 2 (this paper) 85.7 85.7 85.7 29.3 1.29
Collins (1996) 86.3 85.8 1.14
Matsuzaki etal. (2005) 86.6 86.7 1.19
Klein and Manning (2003) 86.9 85.7 86.3 30.9 1.10
Charniak (1997) 87.4 87.5 1.00
Collins (1997) 88.6 88.1 0.91
</table>
<tableCaption confidence="0.999993">
Table 3: Comparison with other parsers (sentences of length ≤ 40)
</tableCaption>
<bodyText confidence="0.998345985714286">
as head information.
Finally, our absolutely best model (Fi=85.7) com-
bines POS tags with latent extra-information (L =
10) and is estimated from latent-head distributions.
Although it also has the largest number of gram-
mar rules (about 224 300), it is still much smaller
than fully-lexicalized models. The best model with
completely latent heads, however, leads to almost
the same performance (Fi=84.6), and has the further
advantage of having significantly fewer rules (only
about 79 200). Moreover, it is the model which
leads to the largest gain compared to the baseline
(A = 11.1).
In the rest of the section, we compare our method
to related methods. To start with performance val-
ues, Table 3 displays previous results on parsing
Section 23 of the WSJ section of the Penn tree-bank.
Comparison indicates that our best model is already
better than the early lexicalized model of Mager-
man (1995). It is a bit worse than the unlexical-
ized PCFGs of Klein and Manning (2003) and Mat-
suzaki et al. (2005), and of course, it is also worse
than state-of-the-art lexicalized parsers (experience
shows that evaluation results on sections 22 and 23
do not differ much).
Beyond performance values, we believe our for-
malism and methodology have the following attrac-
tive features: first, our models incorporate con-
text and lexical information collected from the
whole tree-bank. Information is bundled into ab-
stract heads of higher-order information, which re-
sults in a drastically reduced parameter space. In
terms of Section 2, our approach does not aim at
improving the approximation of rule probabilities
p(r|C, h) and dependency probabilities p(d|D, C, h)
by smoothing. Rather, our approach induces head
classes for the words h and d from the tree-bank
and aims at a exact calculation of rule proba-
bilities p(r|C, class(h)) and dependency probabil-
ities p(class(d)|D, C, class(h)). This is in sharp
contrast to the smoothed fixed-word statistics in
most lexicalized parsing models derived from sparse
data (Magerman (1995), Collins (1996), Char-
niak (1997), etc.). Particularly, class-based depen-
dency probabilities p(class(d)|D, C, class(h)) in-
duced from the tree-bank are not exploited by most
of these parsers.
Second, our method results in an automatic lin-
guistic mark-up of tree-bank grammars. In contrast,
manual linguistic mark-up of the tree-bank like in
Klein and Manning (2003) is based on individual
linguistic intuition and might be cost and time in-
tensive.
Third, our method can be thought of as a new lex-
icalization scheme of CFG based on the notion of
latent head-information, or as a successful attempt
to incorporate lexical classes into parsers, combined
with a new word clustering method based on the
context represented by tree structure. It thus com-
plements and extends the approach of Chiang and
Bikel (2002), who aim at discovering latent head
markers in tree-banks to improve manually written
head-percolation rules.
Finally, the method can also be viewed as an ex-
tension offactorial HMMs (Ghahramani and Jordan,
1995) to PCFGs: the node labels on trees are en-
riched with a latent variable and the latent variables
are learned by EM. Matsuzaki et al. (2005) inde-
pendently introduce a similar approach and present
empirical results that rival ours. In contrast to us,
</bodyText>
<page confidence="0.995873">
123
</page>
<bodyText confidence="0.999914863636364">
they do not use an explicit linguistic grammar, and
they do not attempt to constrain the space of la-
tent variables by linguistic principles. As a conse-
quence, our best models are three orders of mag-
nitude more space efficient than theirs (with about
30 000 000 parameters). Therefore, parsing with
their models requires sophisticated smoothing and
pruning, whereas parsing with ours does not. More-
over, we calculate the most probable latent-head-
decorated parse and delete the latent heads in a post-
processing step. This is comparable to what they call
’Viterbi complete tree’ parsing. Under this regime,
our parser is on a par with theirs (Fi=85.5). This
suggests that both models have learned a compara-
ble degree of information, which is surprising, be-
cause we learn latent heads only, whereas they aim
at learning general features. Crucially, a final 1%
improvement comes from selecting most-probable
parses by bagging all complete parses with the same
incomplete skeleton beforehand; Clearly, a solu-
tion to this NP-Complete problem (Sima’an, 2002)
can/should be also incorporated into our parser.
</bodyText>
<sectionHeader confidence="0.999668" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999961">
We introduced a method for inducing a head-driven
PCFG with latent-head statistics from a tree-bank.
The automatically trained parser is time and space
efficient and achieves a performance already better
than early lexicalized ones. This result suggests that
our grammar-induction method can be successfully
applied across domains, languages, and tree-bank
annotations.
</bodyText>
<sectionHeader confidence="0.968998" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9979505">
This work was supported by the Netherlands Orga-
nization for Scientific Research, NWO project no.
612.000.312, ’Learning Stochastic Tree-Grammars
from Tree-banks’. I also would like to thank Yoav
Seginer and Jelle Zuidema and the anonymous re-
viewers. A special thanks goes to Khalil Sima’an.
</bodyText>
<sectionHeader confidence="0.999592" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999895901960785">
Ezra Black and al. 1991. A procedure for quantitatively
comparing the syntactic coverage of English gram-
mars. In Proc. ofDARPA-91.
Joan Bresnan and Ronald M. Kaplan. 1982. Lexical
functional grammar: A formal system for grammati-
cal representation. In The Mental Representation of
Grammatical Relations. MIT Press.
Glenn Carroll and Mats Rooth. 1998. Valence induction
with a head-lexicalized PCFG. In Proc. of EMNLP-3.
Eugene Charniak. 1997. Parsing with a context-free
grammar and word statistics. In Proc. ofAAAI-97.
David Chiang and D. Bikel. 2002. Recovering latent
information in treebanks. In Proc. of COLING-02.
Michael Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proc. ofACL-96.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proc. ofACL-97.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. J. Royal Statist. Soc., 39(B).
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In
Proc. ofACL-03.
Zoubin Ghahramani and Michael Jordan. 1995. Factorial
Hidden Markov Models. Technical report, MIT.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proc. ofACL-03.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proc. ofACL-95.
Mitch Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computational
Linguistics, 19(2).
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. ofACL-05.
Detlef Prescher. 2003. A Tutorial on the Expectation-
Maximization Algorithm Including Maximum-
Likelihood Estimation and EM Training of Proba-
bilistic Context-Free Grammars. Presented at the 15th
European Summer School in Logic, Language and
Information (ESSLLI).
Detlef Prescher. 2005. Inducing Head-Driven PCFGs
with Latent Heads: Refining a Tree-bank Grammar for
Parsing. In Proc. of the 16th European Conference on
Machine Learning.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
Proc. of COLING-04.
Khalil Sima’an. 2002. Computational complexity of
probabilistic disambiguation. Grammars, 5(2).
</reference>
<page confidence="0.998315">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.453237">
<title confidence="0.965998">Head-Driven PCFGs with Latent-Head Statistics</title>
<author confidence="0.528031">Detlef</author>
<affiliation confidence="0.99263">Institute for Logic, Language and University of</affiliation>
<email confidence="0.936184">prescher@science.uva.nl</email>
<abstract confidence="0.996094238095238">for natural language are lexicalized, it was reshown that an unlexicalized parser for the Penn tree-bank can be simply read off a manually refined tree- While often suffrom sparse data, mark-up costly and largely based on individual linguistic intuition. Thus, across domains, languages, and tree-bank annotations, a fundamental question arises: Is it possible an from a tree-bank without resorting to full lexicalization? In this paper, we show how to induce head-driven probabilistic parsers with latent heads from a tree-bank. Our automatically trained parser has a perforof 85.7% (LP/LR which is albetter than that of early ones.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>al</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proc. ofDARPA-91.</booktitle>
<marker>Black, al, 1991</marker>
<rawString>Ezra Black and al. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proc. ofDARPA-91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>Ronald M Kaplan</author>
</authors>
<title>Lexical functional grammar: A formal system for grammatical representation. In The Mental Representation of Grammatical Relations.</title>
<date>1982</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3681" citStr="Bresnan and Kaplan, 1982" startWordPosition="548" endWordPosition="551">zed parser based on a manually refined tree-bank on the other side. In short, our method is based on the same linguistic principles of headedness as other methods: We do believe that lexical information represents an important knowledge source. To circumvent data sparseness resulting from full lexicalization 115 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 115–124, Vancouver, October 2005. c�2005 Association for Computational Linguistics with words, we simply follow the suggestion of various advanced linguistic theories, e.g. LexicalFunctional Grammar (Bresnan and Kaplan, 1982), where more complex categories based on feature combinations represent the lexical effect. We complement this by a learning paradigm: lexical entries carry latent information to be used as head information, and this head information is induced from the tree-bank. In this paper, we study two different latent-head models, as well as two different estimation methods: The first model is built around completely hidden heads, whereas the second one uses relatively fine-grained combinations of Part-Of-Speech (POS) tags with hidden extra-information; The first estimation method selects a head-driven </context>
</contexts>
<marker>Bresnan, Kaplan, 1982</marker>
<rawString>Joan Bresnan and Ronald M. Kaplan. 1982. Lexical functional grammar: A formal system for grammatical representation. In The Mental Representation of Grammatical Relations. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Mats Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized PCFG.</title>
<date>1998</date>
<booktitle>In Proc. of EMNLP-3.</booktitle>
<contexts>
<context position="1503" citStr="Carroll and Rooth (1998)" startWordPosition="220" endWordPosition="223">.7% (LP/LR Fl), which is already better than that of early lexicalized ones. 1 Introduction State-of-the-art statistical parsers for natural language are based on probabilistic grammars acquired from transformed tree-banks. The method of transforming the tree-bank is of major influence on the accuracy and coverage of the statistical parser. The most important tree-bank transformation in the literature is lexicalization: Each node in a tree is labeled with its head word, the most important word of the constituent under the node (Magerman (1995), Collins (1996), Charniak (1997), Collins (1997), Carroll and Rooth (1998), etc.). It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003). Second, full lexicalization leads to a serious sparse-data problem, which can only be solved by sophisticated smoothing and pruning techniques. Recently, Klein and Manning (2003) showed that a carefully performed linguistic mark-up of the treebank leads to almost the same performance results as lexicalization. This result is attractive since unlexicalized </context>
<context position="6176" citStr="Carroll and Rooth (1998)" startWordPosition="927" endWordPosition="930">ar advantage: it is automatically induced, which suggests that it is applicable across different domains, languages and tree-bank annotations. Applying our method to the benchmark Penn treebank Wall-Street Journal, we obtain a refined probabilistic grammar that significantly improves over the original tree-bank grammar and that shows performance that is on par with early work on lexicalized probabilistic grammars. This is a promising result given the hard task of automatic induction of improved probabilistic grammars. 2 Head Lexicalization As previously shown (Charniak (1997), Collins (1997), Carroll and Rooth (1998), etc.), ContextFree Grammars (CFGs) can be transformed to lexicalized CFGs, provided that a head-marking scheme for rules is given. The basic idea is that the head marking on rules is used to project lexical items up a chain of nodes. Figure 1 displays an example. In this Section, we focus on the approaches of Charniak (1997) and Carroll and Rooth (1998). These approaches are especially attractive for us for two reasons: First, both approaches make use of an explicit linguistic grammar. By contrast, alternative approaches, like Collins (1997), apply an additional transformation to each tree i</context>
<context position="9191" citStr="Carroll and Rooth (1998)" startWordPosition="1476" endWordPosition="1479">is the unlexicalized rule, Cp is C’s parent category) Figure 2: Internal rule, and its probability (Charniak, 1997) p( r |C, h, Cp ) = A1 · �p( r |C, h, Cp ) + A2 · P( r |C, h ) + A3 · p( r |C, class(h) ) + A4 · p( r |C, Cp ) + A5 · p( r |C ) p( d |D, C, h ) = A1 · p( d |D, C, h ) + A2 · p( d |D, C, class(h) ) + A3 · p(d |D, C ) + A4 · p( d |D ) Here, class(h) denotes a class for the head word h. Charniak takes these word classes from an external distributional clustering model, but does not describe this model in detail. An at a first glance different lexicalization technique is described in Carroll and Rooth (1998). In their approach, a grammar transformation is used to lexicalize a manually written grammar. The key step for understanding their model is to imagine that the rule in Figure 2 is transformed to a sub-tree, the one displayed in Figure 3. After this transformation, the sub-tree probability is simply calculated with the PCFG’s standard model; The result is also displayed in the figure. Comparing this probability with the probability that Charniak assigns to the rule itself, we see that the subtree probability equals the rule probability1. In other words, both probability models are based on th</context>
<context position="11129" citStr="Carroll and Rooth, 1998" startWordPosition="1823" endWordPosition="1826"> C’s parent category C, for the moment; Note that C’s parent category is available in the tree-bank, but may not occur in the left-hand sides of the rules of a manually written CFG 117 C:h D1:C:h ··· Dm:C:h H:h Dm+1:C:h · · · Dm+n:C:h D1:d1 Dm:dm Dm+1:dm+1 Dm+n:dm+n pSTANDARD-PCFG( this sub-tree ) � p( D1:C:h ... Dm:C:h H:h Dm+1:C:h ... Dm+n:C: h |C:h ) × �m+n i=1 p( Di:di |Di:C:h ) � p( D1 ... Dm H Dm+1 ... Dm+n |C, h ) × �m+n i=1 p( di |Di, C, h ) � p( r |C, h ) × �m+n i=1 p( di |Di, C, h ) (r is the unlexicalized rule) Figure 3: Transformed internal rule, and its standard-PCFG probability (Carroll and Rooth, 1998) S S:h (Starting Rules) C:h D1:C:h ... D,,t:C:h H:h D,,t+1:C:h ... D,,t+,,,:C:h (Lexicalized Rules) D:C:h D:d (Dependencies) C:w w (Lexical Rules) Table 1: Context-free rule types in the transform (Carroll and Rooth, 1998) S S:rose ADJ:NP:profits ADJ:Corporate Corporate N:profits profits VP:rose V:rose rose PUNC:S:rose PUNC:. . NP:S:rose NP:profits Starting Rule: S −→ S:rose Lexicalized Rules: S:rose −→ NP:S:rose VP:rose PUNC:S:rose NP:profits −→ ADJ:NP:profits N:profits VP:rose −→ V:rose Dependencies: NP:S:rose −→ NP:profits PUNC:S:rose −→ PUNC:. ADJ:NP:profits −→ ADJ:Corporate Lexical Rules:</context>
<context position="13733" citStr="Carroll and Rooth (1998)" startWordPosition="2224" endWordPosition="2227">is section defines two probability models over the trees licensed by a head-lexicalized CFG with latent head-information, thereby exploiting three simple linguistic principles: (i) all rules have head markers, (ii) information is projected up a chain of categories marked as heads, (iii) lexical entries carry latent head values which can be learned. Moreover, two estimation methods for the latent-head models are described. Head-Lexicalized CFGs with Latent Heads Principles (i) and (ii) are satisfied by all head lexicalized models we know of, and clearly, they are also satisfied by the model of Carroll and Rooth (1998). Principle (iii), however, deals with latent information for lexical entries, which is beyond the capability of this model. To see this, remember that lexical rules C ) w are unambiguously transformed to C:w ) w. Because this transformation is unambiguous, latent information does not play a role in it. It is surprisingly simple, however, to satisfy principle (iii) with slightly modified versions of Carroll and Rooth’s transformation of lexical rules. In the following, we present two of them: Lexical-Rule Transformation (Model 1): Transform each lexical rule C ) w to a set of rules, having the</context>
<context position="15151" citStr="Carroll and Rooth (1998)" startWordPosition="2483" endWordPosition="2486">C} x {1, ... , L}, and L is a free parameter. Both models introduce latent heads for lexical entries. The difference is that Model 1 introduces completely latent heads h, whereas Model 2 introduces heads h on the basis of the POS tag C of the word w: each such head is a combination of C with an abstract extra-information. Figure 5 gives an example. Because we still apply Carroll and Rooth’s grammar transformation scheme to the non-lexical rules, latent heads are percolated up a path of categories marked as heads. Although our modifications are small, their effect is remarkable. In contrast to Carroll and Rooth (1998), where an unlexicalized tree is unambiguously mapped to a single transform, our models map an unlexicalized tree to multiple transforms (for free parameters &gt; 2). Note also that although latent information is freely introduced at the lexical level, it is not freely distributed over the nodes of the tree. Rather, the space of latent heads for a tree is constrained according the linguistic principle of headedness. Finally, for the case L = 1, our models perform unambiguous transformations: in Model 1 the transformation makes no relevant changes, whereas Model 2 performs unambiguous lexicalizati</context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Glenn Carroll and Mats Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Proc. of EMNLP-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proc. ofAAAI-97.</booktitle>
<contexts>
<context position="1461" citStr="Charniak (1997)" startWordPosition="216" endWordPosition="217">ed parser has a performance of 85.7% (LP/LR Fl), which is already better than that of early lexicalized ones. 1 Introduction State-of-the-art statistical parsers for natural language are based on probabilistic grammars acquired from transformed tree-banks. The method of transforming the tree-bank is of major influence on the accuracy and coverage of the statistical parser. The most important tree-bank transformation in the literature is lexicalization: Each node in a tree is labeled with its head word, the most important word of the constituent under the node (Magerman (1995), Collins (1996), Charniak (1997), Collins (1997), Carroll and Rooth (1998), etc.). It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003). Second, full lexicalization leads to a serious sparse-data problem, which can only be solved by sophisticated smoothing and pruning techniques. Recently, Klein and Manning (2003) showed that a carefully performed linguistic mark-up of the treebank leads to almost the same performance results as lexicalization. This</context>
<context position="5468" citStr="Charniak (1997)" startWordPosition="823" endWordPosition="824">g: First, estimation with latent-head distributions outperforms estimation with most-probable-head annotation. Second, modeling with completely hidden heads is almost as good as modeling with latent heads based on POS tags, and moreover, results in much smaller grammars. We emphasize that our task is to automatically induce a more refined grammar based on a few linguistic principles. With automatic refinement it is harder to guarantee improved performance than with manual refinements (Klein and Manning, 2003) or with refinements based on direct lexicalization (Magerman (1995), Collins (1996), Charniak (1997), etc.). If, however, our refinement provides improved performance then it has a clear advantage: it is automatically induced, which suggests that it is applicable across different domains, languages and tree-bank annotations. Applying our method to the benchmark Penn treebank Wall-Street Journal, we obtain a refined probabilistic grammar that significantly improves over the original tree-bank grammar and that shows performance that is on par with early work on lexicalized probabilistic grammars. This is a promising result given the hard task of automatic induction of improved probabilistic gr</context>
<context position="6945" citStr="Charniak (1997)" startWordPosition="1053" endWordPosition="1054">t the head marking on rules is used to project lexical items up a chain of nodes. Figure 1 displays an example. In this Section, we focus on the approaches of Charniak (1997) and Carroll and Rooth (1998). These approaches are especially attractive for us for two reasons: First, both approaches make use of an explicit linguistic grammar. By contrast, alternative approaches, like Collins (1997), apply an additional transformation to each tree in the tree-bank, splitting each rule into small parts, which finally results in a new grammar covering many more sentences than the explicit one. Second, Charniak (1997) and Carroll and Rooth (1998) rely on almost the same lexicalization technique. In fact, the significant difference between them is that, in one case, a lexicalized version of the tree-bank grammar is learned from a corpus of trees (supervised learning), whereas, in the other case, a lexicalized version of a manually written CFG is learned from a a text corpus (unsupervised learning). As we will see in Section 3, our approach is a blend of these approaches in that it aims at unsupervised learning of a (latent-head-) lexicalized version of the tree-bank grammar. Starting with Charniak (1997), F</context>
<context position="8437" citStr="Charniak, 1997" startWordPosition="1301" endWordPosition="1302">ght modifiers of H. Either n or m may be zero, and n = m = 0 for unary rules. Because the probabilities occurring in Charniak’s definition are already so specific that there is no real chance of obtaining the data empirically, they are smoothed by deleted interpolation: 116 S:rose ADJ:Corporate Corporate N:profits profits VP:rose V:rose rose PUNC:. . NP:profits Internal Rules: S:rose NP:profits VP:rose PUNC:. NP:profits ADJ:Corporate N:profits VP:rose V:rose Lexical Rules: ADJ:Corporate Corporate N:profits profits V:rose rose PUNC:. . Figure 1: Parse tree, and a list of the rules it contains (Charniak, 1997) C:h D1:d1 ··· Dm:dm H:h Dm+1:dm+1 ··· Dm+n:dm+n pCHARNIAK97( this local tree) = p( r |C, h, Cp ) x �n+m i=1 p( di |Di, C, h ) (r is the unlexicalized rule, Cp is C’s parent category) Figure 2: Internal rule, and its probability (Charniak, 1997) p( r |C, h, Cp ) = A1 · �p( r |C, h, Cp ) + A2 · P( r |C, h ) + A3 · p( r |C, class(h) ) + A4 · p( r |C, Cp ) + A5 · p( r |C ) p( d |D, C, h ) = A1 · p( d |D, C, h ) + A2 · p( d |D, C, class(h) ) + A3 · p(d |D, C ) + A4 · p( d |D ) Here, class(h) denotes a class for the head word h. Charniak takes these word classes from an external distributional clus</context>
<context position="12129" citStr="Charniak (1997)" startWordPosition="1971" endWordPosition="1972">ized Rules: S:rose −→ NP:S:rose VP:rose PUNC:S:rose NP:profits −→ ADJ:NP:profits N:profits VP:rose −→ V:rose Dependencies: NP:S:rose −→ NP:profits PUNC:S:rose −→ PUNC:. ADJ:NP:profits −→ ADJ:Corporate Lexical Rules: ADJ:Corporate −→ Corporate N:profits −→ profits V:rose −→ rose PUNC:. −→ . Figure 4: Transformed parse tree, and a list of the rules it contains (Carroll and Rooth, 1998) 118 transformed grammar. It is noteworthy that although Carroll and Rooth (1998) learn from a text corpus of about 50 million words, it is still necessary to smooth the rule probabilities of the transform. Unlike Charniak (1997), however, they do not use word classes in their back-off scheme. To summarize, the major problem of fulllexicalization techniques is that they lead to serious sparse-data problems. For both models presented in this section, a large number |T |of full word forms makes it difficult to reliably estimate the probability weights of the O(|T|2) dependencies and the O(|T|) lexicalized rules. A linguistically naive approach to this problem is to use POS tags as heads to decrease the number of heads. From a computational perspective, the sparse data problem would then be completely solved since the nu</context>
<context position="24351" citStr="Charniak (1997)" startWordPosition="4049" endWordPosition="4050">mation from head distributions consistently outperforms estimation from most probable heads (for both models). Although coarse-grained models clearly benefit from POS information in the heads (L = 1, 2, 5), it is surprising that the best models with completely latent heads are on a par with or almost as good as the best ones using POS 122 LP LR Fi Exact CB Model 1 (this paper) 84.8 84.4 84.6 26.4 1.37 Magerman (1995) 84.9 84.6 1.26 Model 2 (this paper) 85.7 85.7 85.7 29.3 1.29 Collins (1996) 86.3 85.8 1.14 Matsuzaki etal. (2005) 86.6 86.7 1.19 Klein and Manning (2003) 86.9 85.7 86.3 30.9 1.10 Charniak (1997) 87.4 87.5 1.00 Collins (1997) 88.6 88.1 0.91 Table 3: Comparison with other parsers (sentences of length ≤ 40) as head information. Finally, our absolutely best model (Fi=85.7) combines POS tags with latent extra-information (L = 10) and is estimated from latent-head distributions. Although it also has the largest number of grammar rules (about 224 300), it is still much smaller than fully-lexicalized models. The best model with completely latent heads, however, leads to almost the same performance (Fi=84.6), and has the further advantage of having significantly fewer rules (only about 79 200</context>
<context position="26512" citStr="Charniak (1997)" startWordPosition="4397" endWordPosition="4399">ation, which results in a drastically reduced parameter space. In terms of Section 2, our approach does not aim at improving the approximation of rule probabilities p(r|C, h) and dependency probabilities p(d|D, C, h) by smoothing. Rather, our approach induces head classes for the words h and d from the tree-bank and aims at a exact calculation of rule probabilities p(r|C, class(h)) and dependency probabilities p(class(d)|D, C, class(h)). This is in sharp contrast to the smoothed fixed-word statistics in most lexicalized parsing models derived from sparse data (Magerman (1995), Collins (1996), Charniak (1997), etc.). Particularly, class-based dependency probabilities p(class(d)|D, C, class(h)) induced from the tree-bank are not exploited by most of these parsers. Second, our method results in an automatic linguistic mark-up of tree-bank grammars. In contrast, manual linguistic mark-up of the tree-bank like in Klein and Manning (2003) is based on individual linguistic intuition and might be cost and time intensive. Third, our method can be thought of as a new lexicalization scheme of CFG based on the notion of latent head-information, or as a successful attempt to incorporate lexical classes into p</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Parsing with a context-free grammar and word statistics. In Proc. ofAAAI-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>D Bikel</author>
</authors>
<title>Recovering latent information in treebanks.</title>
<date>2002</date>
<booktitle>In Proc. of COLING-02.</booktitle>
<contexts>
<context position="27286" citStr="Chiang and Bikel (2002)" startWordPosition="4520" endWordPosition="4523">rsers. Second, our method results in an automatic linguistic mark-up of tree-bank grammars. In contrast, manual linguistic mark-up of the tree-bank like in Klein and Manning (2003) is based on individual linguistic intuition and might be cost and time intensive. Third, our method can be thought of as a new lexicalization scheme of CFG based on the notion of latent head-information, or as a successful attempt to incorporate lexical classes into parsers, combined with a new word clustering method based on the context represented by tree structure. It thus complements and extends the approach of Chiang and Bikel (2002), who aim at discovering latent head markers in tree-banks to improve manually written head-percolation rules. Finally, the method can also be viewed as an extension offactorial HMMs (Ghahramani and Jordan, 1995) to PCFGs: the node labels on trees are enriched with a latent variable and the latent variables are learned by EM. Matsuzaki et al. (2005) independently introduce a similar approach and present empirical results that rival ours. In contrast to us, 123 they do not use an explicit linguistic grammar, and they do not attempt to constrain the space of latent variables by linguistic princi</context>
</contexts>
<marker>Chiang, Bikel, 2002</marker>
<rawString>David Chiang and D. Bikel. 2002. Recovering latent information in treebanks. In Proc. of COLING-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proc. ofACL-96.</booktitle>
<contexts>
<context position="1444" citStr="Collins (1996)" startWordPosition="214" endWordPosition="215">omatically trained parser has a performance of 85.7% (LP/LR Fl), which is already better than that of early lexicalized ones. 1 Introduction State-of-the-art statistical parsers for natural language are based on probabilistic grammars acquired from transformed tree-banks. The method of transforming the tree-bank is of major influence on the accuracy and coverage of the statistical parser. The most important tree-bank transformation in the literature is lexicalization: Each node in a tree is labeled with its head word, the most important word of the constituent under the node (Magerman (1995), Collins (1996), Charniak (1997), Collins (1997), Carroll and Rooth (1998), etc.). It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003). Second, full lexicalization leads to a serious sparse-data problem, which can only be solved by sophisticated smoothing and pruning techniques. Recently, Klein and Manning (2003) showed that a carefully performed linguistic mark-up of the treebank leads to almost the same performance results as lex</context>
<context position="5451" citStr="Collins (1996)" startWordPosition="821" endWordPosition="822">r, are surprising: First, estimation with latent-head distributions outperforms estimation with most-probable-head annotation. Second, modeling with completely hidden heads is almost as good as modeling with latent heads based on POS tags, and moreover, results in much smaller grammars. We emphasize that our task is to automatically induce a more refined grammar based on a few linguistic principles. With automatic refinement it is harder to guarantee improved performance than with manual refinements (Klein and Manning, 2003) or with refinements based on direct lexicalization (Magerman (1995), Collins (1996), Charniak (1997), etc.). If, however, our refinement provides improved performance then it has a clear advantage: it is automatically induced, which suggests that it is applicable across different domains, languages and tree-bank annotations. Applying our method to the benchmark Penn treebank Wall-Street Journal, we obtain a refined probabilistic grammar that significantly improves over the original tree-bank grammar and that shows performance that is on par with early work on lexicalized probabilistic grammars. This is a promising result given the hard task of automatic induction of improved</context>
<context position="24232" citStr="Collins (1996)" startWordPosition="4029" endWordPosition="4030">urrent head classes is not fine enough; Further refinement may lead to even better latenthead statistics. Second, estimation from head distributions consistently outperforms estimation from most probable heads (for both models). Although coarse-grained models clearly benefit from POS information in the heads (L = 1, 2, 5), it is surprising that the best models with completely latent heads are on a par with or almost as good as the best ones using POS 122 LP LR Fi Exact CB Model 1 (this paper) 84.8 84.4 84.6 26.4 1.37 Magerman (1995) 84.9 84.6 1.26 Model 2 (this paper) 85.7 85.7 85.7 29.3 1.29 Collins (1996) 86.3 85.8 1.14 Matsuzaki etal. (2005) 86.6 86.7 1.19 Klein and Manning (2003) 86.9 85.7 86.3 30.9 1.10 Charniak (1997) 87.4 87.5 1.00 Collins (1997) 88.6 88.1 0.91 Table 3: Comparison with other parsers (sentences of length ≤ 40) as head information. Finally, our absolutely best model (Fi=85.7) combines POS tags with latent extra-information (L = 10) and is estimated from latent-head distributions. Although it also has the largest number of grammar rules (about 224 300), it is still much smaller than fully-lexicalized models. The best model with completely latent heads, however, leads to almo</context>
<context position="26495" citStr="Collins (1996)" startWordPosition="4395" endWordPosition="4396">her-order information, which results in a drastically reduced parameter space. In terms of Section 2, our approach does not aim at improving the approximation of rule probabilities p(r|C, h) and dependency probabilities p(d|D, C, h) by smoothing. Rather, our approach induces head classes for the words h and d from the tree-bank and aims at a exact calculation of rule probabilities p(r|C, class(h)) and dependency probabilities p(class(d)|D, C, class(h)). This is in sharp contrast to the smoothed fixed-word statistics in most lexicalized parsing models derived from sparse data (Magerman (1995), Collins (1996), Charniak (1997), etc.). Particularly, class-based dependency probabilities p(class(d)|D, C, class(h)) induced from the tree-bank are not exploited by most of these parsers. Second, our method results in an automatic linguistic mark-up of tree-bank grammars. In contrast, manual linguistic mark-up of the tree-bank like in Klein and Manning (2003) is based on individual linguistic intuition and might be cost and time intensive. Third, our method can be thought of as a new lexicalization scheme of CFG based on the notion of latent head-information, or as a successful attempt to incorporate lexic</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proc. ofACL-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. ofACL-97.</booktitle>
<contexts>
<context position="1477" citStr="Collins (1997)" startWordPosition="218" endWordPosition="219">erformance of 85.7% (LP/LR Fl), which is already better than that of early lexicalized ones. 1 Introduction State-of-the-art statistical parsers for natural language are based on probabilistic grammars acquired from transformed tree-banks. The method of transforming the tree-bank is of major influence on the accuracy and coverage of the statistical parser. The most important tree-bank transformation in the literature is lexicalization: Each node in a tree is labeled with its head word, the most important word of the constituent under the node (Magerman (1995), Collins (1996), Charniak (1997), Collins (1997), Carroll and Rooth (1998), etc.). It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003). Second, full lexicalization leads to a serious sparse-data problem, which can only be solved by sophisticated smoothing and pruning techniques. Recently, Klein and Manning (2003) showed that a carefully performed linguistic mark-up of the treebank leads to almost the same performance results as lexicalization. This result is attra</context>
<context position="6150" citStr="Collins (1997)" startWordPosition="925" endWordPosition="926">hen it has a clear advantage: it is automatically induced, which suggests that it is applicable across different domains, languages and tree-bank annotations. Applying our method to the benchmark Penn treebank Wall-Street Journal, we obtain a refined probabilistic grammar that significantly improves over the original tree-bank grammar and that shows performance that is on par with early work on lexicalized probabilistic grammars. This is a promising result given the hard task of automatic induction of improved probabilistic grammars. 2 Head Lexicalization As previously shown (Charniak (1997), Collins (1997), Carroll and Rooth (1998), etc.), ContextFree Grammars (CFGs) can be transformed to lexicalized CFGs, provided that a head-marking scheme for rules is given. The basic idea is that the head marking on rules is used to project lexical items up a chain of nodes. Figure 1 displays an example. In this Section, we focus on the approaches of Charniak (1997) and Carroll and Rooth (1998). These approaches are especially attractive for us for two reasons: First, both approaches make use of an explicit linguistic grammar. By contrast, alternative approaches, like Collins (1997), apply an additional tra</context>
<context position="24381" citStr="Collins (1997)" startWordPosition="4054" endWordPosition="4055">consistently outperforms estimation from most probable heads (for both models). Although coarse-grained models clearly benefit from POS information in the heads (L = 1, 2, 5), it is surprising that the best models with completely latent heads are on a par with or almost as good as the best ones using POS 122 LP LR Fi Exact CB Model 1 (this paper) 84.8 84.4 84.6 26.4 1.37 Magerman (1995) 84.9 84.6 1.26 Model 2 (this paper) 85.7 85.7 85.7 29.3 1.29 Collins (1996) 86.3 85.8 1.14 Matsuzaki etal. (2005) 86.6 86.7 1.19 Klein and Manning (2003) 86.9 85.7 86.3 30.9 1.10 Charniak (1997) 87.4 87.5 1.00 Collins (1997) 88.6 88.1 0.91 Table 3: Comparison with other parsers (sentences of length ≤ 40) as head information. Finally, our absolutely best model (Fi=85.7) combines POS tags with latent extra-information (L = 10) and is estimated from latent-head distributions. Although it also has the largest number of grammar rules (about 224 300), it is still much smaller than fully-lexicalized models. The best model with completely latent heads, however, leads to almost the same performance (Fi=84.6), and has the further advantage of having significantly fewer rules (only about 79 200). Moreover, it is the model w</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proc. ofACL-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>J. Royal Statist. Soc.,</journal>
<volume>39</volume>
<contexts>
<context position="17980" citStr="Dempster et al., 1977" startWordPosition="2985" endWordPosition="2988">(t) · p( t&apos; |t ) to the lexicalized trees t&apos; E GLEX(t) [ Here, c(t) is the frequency of t in the original tree-bank ] (4) M step: Read the tree-bank grammar off TLEX, by - calculating relative frequencies p� for all rules of GLEX as occurring in TLEX (5) set pi = p� (6) end Figure 6: Grammar induction algorithm (EM algorithm) with latent heads cannot be directly estimated from the tree-bank (by counting rules) since the latent heads are not annotated in the trees. Faced with this incomplete-data problem, we apply the ExpectationMaximization (EM) algorithm developed for these type of problems (Dempster et al., 1977). For details of the EM algorithm, we refer to the numerous tutorials on EM (e.g. Prescher (2003)). Here, it suffices to know that it is a sort of meta algorithm, resulting for each incomplete-data problem in an iterative estimation method that aims at maximum-likelihood estimation on the data. Disregarding the fact that we implement a dynamic-programming version for our experiments (running in linear time in the size of the trees in the tree-bank (Prescher, 2005)), the EM algorithm is here as displayed in Figure 6. Beside this pure form of the EM algorithm, we also use a variant where the ori</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. J. Royal Statist. Soc., 39(B).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Dubey</author>
<author>Frank Keller</author>
</authors>
<title>Probabilistic parsing for German using sister-head dependencies.</title>
<date>2003</date>
<booktitle>In Proc. ofACL-03.</booktitle>
<contexts>
<context position="1743" citStr="Dubey and Keller, 2003" startWordPosition="256" endWordPosition="259">nsforming the tree-bank is of major influence on the accuracy and coverage of the statistical parser. The most important tree-bank transformation in the literature is lexicalization: Each node in a tree is labeled with its head word, the most important word of the constituent under the node (Magerman (1995), Collins (1996), Charniak (1997), Collins (1997), Carroll and Rooth (1998), etc.). It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003). Second, full lexicalization leads to a serious sparse-data problem, which can only be solved by sophisticated smoothing and pruning techniques. Recently, Klein and Manning (2003) showed that a carefully performed linguistic mark-up of the treebank leads to almost the same performance results as lexicalization. This result is attractive since unlexicalized grammars are easy to estimate, easy to parse with, and time- and space-efficient: Klein and Manning (2003) do not smooth grammar-rule probabilities, except unknown-word probabilities, and they do not prune since they are able to determine t</context>
</contexts>
<marker>Dubey, Keller, 2003</marker>
<rawString>Amit Dubey and Frank Keller. 2003. Probabilistic parsing for German using sister-head dependencies. In Proc. ofACL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Michael Jordan</author>
</authors>
<title>Factorial Hidden Markov Models.</title>
<date>1995</date>
<tech>Technical report, MIT.</tech>
<contexts>
<context position="27498" citStr="Ghahramani and Jordan, 1995" startWordPosition="4552" endWordPosition="4555">guistic intuition and might be cost and time intensive. Third, our method can be thought of as a new lexicalization scheme of CFG based on the notion of latent head-information, or as a successful attempt to incorporate lexical classes into parsers, combined with a new word clustering method based on the context represented by tree structure. It thus complements and extends the approach of Chiang and Bikel (2002), who aim at discovering latent head markers in tree-banks to improve manually written head-percolation rules. Finally, the method can also be viewed as an extension offactorial HMMs (Ghahramani and Jordan, 1995) to PCFGs: the node labels on trees are enriched with a latent variable and the latent variables are learned by EM. Matsuzaki et al. (2005) independently introduce a similar approach and present empirical results that rival ours. In contrast to us, 123 they do not use an explicit linguistic grammar, and they do not attempt to constrain the space of latent variables by linguistic principles. As a consequence, our best models are three orders of magnitude more space efficient than theirs (with about 30 000 000 parameters). Therefore, parsing with their models requires sophisticated smoothing and</context>
</contexts>
<marker>Ghahramani, Jordan, 1995</marker>
<rawString>Zoubin Ghahramani and Michael Jordan. 1995. Factorial Hidden Markov Models. Technical report, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. ofACL-03.</booktitle>
<contexts>
<context position="1923" citStr="Klein and Manning (2003)" startWordPosition="281" endWordPosition="284">on: Each node in a tree is labeled with its head word, the most important word of the constituent under the node (Magerman (1995), Collins (1996), Charniak (1997), Collins (1997), Carroll and Rooth (1998), etc.). It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003). Second, full lexicalization leads to a serious sparse-data problem, which can only be solved by sophisticated smoothing and pruning techniques. Recently, Klein and Manning (2003) showed that a carefully performed linguistic mark-up of the treebank leads to almost the same performance results as lexicalization. This result is attractive since unlexicalized grammars are easy to estimate, easy to parse with, and time- and space-efficient: Klein and Manning (2003) do not smooth grammar-rule probabilities, except unknown-word probabilities, and they do not prune since they are able to determine the most probable parse of each full parse forest. Both facts are noteworthy in the context of statistical parsing with a tree-bank grammar. A drawback of their method is, however, </context>
<context position="5367" citStr="Klein and Manning, 2003" startWordPosition="807" endWordPosition="810">ate) the information is, the better the induced grammar will be. Our empirical results, however, are surprising: First, estimation with latent-head distributions outperforms estimation with most-probable-head annotation. Second, modeling with completely hidden heads is almost as good as modeling with latent heads based on POS tags, and moreover, results in much smaller grammars. We emphasize that our task is to automatically induce a more refined grammar based on a few linguistic principles. With automatic refinement it is harder to guarantee improved performance than with manual refinements (Klein and Manning, 2003) or with refinements based on direct lexicalization (Magerman (1995), Collins (1996), Charniak (1997), etc.). If, however, our refinement provides improved performance then it has a clear advantage: it is automatically induced, which suggests that it is applicable across different domains, languages and tree-bank annotations. Applying our method to the benchmark Penn treebank Wall-Street Journal, we obtain a refined probabilistic grammar that significantly improves over the original tree-bank grammar and that shows performance that is on par with early work on lexicalized probabilistic grammar</context>
<context position="19290" citStr="Klein and Manning (2003)" startWordPosition="3201" endWordPosition="3204"> of both estimation methods: Estimation from latent-head distributions: The key steps of the EM algorithm produce a lexicalized tree-bank TLEX, consisting of all lexicalized versions of the original trees (E-step), and calculate the probabilities for the rules of GLEX on the basis of TLEX (M-step). Clearly, all lexicalized trees in GLEX(t) differ only in the heads of their nodes. Thus, EM estimation uses the original tree-bank, where each node can be thought of as annotated with a latenthead distribution. Estimation from most probable heads: By contrast, a quite different scheme is applied in Klein and Manning (2003): extensive manual annotation enriches the tree-bank with information, but no trees are added to the tree-bank. We borrow from this scheme in that we take the best EM model to calculate the most probable head-lexicalized versions of the trees in the original tree-bank. After collecting this Viterbi-style lexicalized tree-bank, the ordinary tree-bank estimation yields another estimate of GLEX. Clearly, this estimation method uses the original tree-bank, where each node can be thought of annotated with the most probable latent head. 4 Experiments This section presents empirical results across ou</context>
<context position="23137" citStr="Klein and Manning (2003)" startWordPosition="3841" endWordPosition="3845">y parse the evaluation corpus. Table 2 displays our results in terms of LP/LR F1 (Black and al., 1991). The largest number per column is printed in italics. The absolutely largest number is printed in boldface. The numbers in brackets are the number of grammar rules (without counting lexical rules). The gain in LP/LR F1 per estimation method and per model is also displayed (A). Finally, the average training time per iteration ranges from 2 to 4 hours (depending on both L and the type of the model). The average parsing time is 10 seconds per sentence, which is comparable to what is reported in Klein and Manning (2003). 5 Discussion First of all, all model instances outperform the baseline, i.e., the original grammar (F1=73.5), and the head-lexicalized grammar with POS tags as heads (F1=78.9). The only plausible explanation for these significant improvements is that useful head classes have been learned by our method. Moreover, increasing L consistently increases F1 (except for Model 2 estimated from most probable heads; L = 10 is out of the row). We thus argue that the granularity of the current head classes is not fine enough; Further refinement may lead to even better latenthead statistics. Second, estim</context>
<context position="25437" citStr="Klein and Manning (2003)" startWordPosition="4229" endWordPosition="4232">ever, leads to almost the same performance (Fi=84.6), and has the further advantage of having significantly fewer rules (only about 79 200). Moreover, it is the model which leads to the largest gain compared to the baseline (A = 11.1). In the rest of the section, we compare our method to related methods. To start with performance values, Table 3 displays previous results on parsing Section 23 of the WSJ section of the Penn tree-bank. Comparison indicates that our best model is already better than the early lexicalized model of Magerman (1995). It is a bit worse than the unlexicalized PCFGs of Klein and Manning (2003) and Matsuzaki et al. (2005), and of course, it is also worse than state-of-the-art lexicalized parsers (experience shows that evaluation results on sections 22 and 23 do not differ much). Beyond performance values, we believe our formalism and methodology have the following attractive features: first, our models incorporate context and lexical information collected from the whole tree-bank. Information is bundled into abstract heads of higher-order information, which results in a drastically reduced parameter space. In terms of Section 2, our approach does not aim at improving the approximati</context>
<context position="26843" citStr="Klein and Manning (2003)" startWordPosition="4445" endWordPosition="4448"> at a exact calculation of rule probabilities p(r|C, class(h)) and dependency probabilities p(class(d)|D, C, class(h)). This is in sharp contrast to the smoothed fixed-word statistics in most lexicalized parsing models derived from sparse data (Magerman (1995), Collins (1996), Charniak (1997), etc.). Particularly, class-based dependency probabilities p(class(d)|D, C, class(h)) induced from the tree-bank are not exploited by most of these parsers. Second, our method results in an automatic linguistic mark-up of tree-bank grammars. In contrast, manual linguistic mark-up of the tree-bank like in Klein and Manning (2003) is based on individual linguistic intuition and might be cost and time intensive. Third, our method can be thought of as a new lexicalization scheme of CFG based on the notion of latent head-information, or as a successful attempt to incorporate lexical classes into parsers, combined with a new word clustering method based on the context represented by tree structure. It thus complements and extends the approach of Chiang and Bikel (2002), who aim at discovering latent head markers in tree-banks to improve manually written head-percolation rules. Finally, the method can also be viewed as an e</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proc. ofACL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proc. ofACL-95.</booktitle>
<contexts>
<context position="1428" citStr="Magerman (1995)" startWordPosition="212" endWordPosition="213">ree-bank. Our automatically trained parser has a performance of 85.7% (LP/LR Fl), which is already better than that of early lexicalized ones. 1 Introduction State-of-the-art statistical parsers for natural language are based on probabilistic grammars acquired from transformed tree-banks. The method of transforming the tree-bank is of major influence on the accuracy and coverage of the statistical parser. The most important tree-bank transformation in the literature is lexicalization: Each node in a tree is labeled with its head word, the most important word of the constituent under the node (Magerman (1995), Collins (1996), Charniak (1997), Collins (1997), Carroll and Rooth (1998), etc.). It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003). Second, full lexicalization leads to a serious sparse-data problem, which can only be solved by sophisticated smoothing and pruning techniques. Recently, Klein and Manning (2003) showed that a carefully performed linguistic mark-up of the treebank leads to almost the same performanc</context>
<context position="5435" citStr="Magerman (1995)" startWordPosition="818" endWordPosition="820">l results, however, are surprising: First, estimation with latent-head distributions outperforms estimation with most-probable-head annotation. Second, modeling with completely hidden heads is almost as good as modeling with latent heads based on POS tags, and moreover, results in much smaller grammars. We emphasize that our task is to automatically induce a more refined grammar based on a few linguistic principles. With automatic refinement it is harder to guarantee improved performance than with manual refinements (Klein and Manning, 2003) or with refinements based on direct lexicalization (Magerman (1995), Collins (1996), Charniak (1997), etc.). If, however, our refinement provides improved performance then it has a clear advantage: it is automatically induced, which suggests that it is applicable across different domains, languages and tree-bank annotations. Applying our method to the benchmark Penn treebank Wall-Street Journal, we obtain a refined probabilistic grammar that significantly improves over the original tree-bank grammar and that shows performance that is on par with early work on lexicalized probabilistic grammars. This is a promising result given the hard task of automatic induc</context>
<context position="24156" citStr="Magerman (1995)" startWordPosition="4015" endWordPosition="4016">heads; L = 10 is out of the row). We thus argue that the granularity of the current head classes is not fine enough; Further refinement may lead to even better latenthead statistics. Second, estimation from head distributions consistently outperforms estimation from most probable heads (for both models). Although coarse-grained models clearly benefit from POS information in the heads (L = 1, 2, 5), it is surprising that the best models with completely latent heads are on a par with or almost as good as the best ones using POS 122 LP LR Fi Exact CB Model 1 (this paper) 84.8 84.4 84.6 26.4 1.37 Magerman (1995) 84.9 84.6 1.26 Model 2 (this paper) 85.7 85.7 85.7 29.3 1.29 Collins (1996) 86.3 85.8 1.14 Matsuzaki etal. (2005) 86.6 86.7 1.19 Klein and Manning (2003) 86.9 85.7 86.3 30.9 1.10 Charniak (1997) 87.4 87.5 1.00 Collins (1997) 88.6 88.1 0.91 Table 3: Comparison with other parsers (sentences of length ≤ 40) as head information. Finally, our absolutely best model (Fi=85.7) combines POS tags with latent extra-information (L = 10) and is estimated from latent-head distributions. Although it also has the largest number of grammar rules (about 224 300), it is still much smaller than fully-lexicalized</context>
<context position="26479" citStr="Magerman (1995)" startWordPosition="4393" endWordPosition="4394">ract heads of higher-order information, which results in a drastically reduced parameter space. In terms of Section 2, our approach does not aim at improving the approximation of rule probabilities p(r|C, h) and dependency probabilities p(d|D, C, h) by smoothing. Rather, our approach induces head classes for the words h and d from the tree-bank and aims at a exact calculation of rule probabilities p(r|C, class(h)) and dependency probabilities p(class(d)|D, C, class(h)). This is in sharp contrast to the smoothed fixed-word statistics in most lexicalized parsing models derived from sparse data (Magerman (1995), Collins (1996), Charniak (1997), etc.). Particularly, class-based dependency probabilities p(class(d)|D, C, class(h)) induced from the tree-bank are not exploited by most of these parsers. Second, our method results in an automatic linguistic mark-up of tree-bank grammars. In contrast, manual linguistic mark-up of the tree-bank like in Klein and Manning (2003) is based on individual linguistic intuition and might be cost and time intensive. Third, our method can be thought of as a new lexicalization scheme of CFG based on the notion of latent head-information, or as a successful attempt to i</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical decision-tree models for parsing. In Proc. ofACL-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitch Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="20089" citStr="Marcus et al., 1993" startWordPosition="3328" endWordPosition="3331">culate the most probable head-lexicalized versions of the trees in the original tree-bank. After collecting this Viterbi-style lexicalized tree-bank, the ordinary tree-bank estimation yields another estimate of GLEX. Clearly, this estimation method uses the original tree-bank, where each node can be thought of annotated with the most probable latent head. 4 Experiments This section presents empirical results across our models and estimation methods. Data and Parameters To facilitate comparison with previous work, we trained our models on sections 2-21 of the WSJ section of the Penn tree-bank (Marcus et al., 1993). All trees were modified such that: The empty top node got the category TOP, node labels consisted solely of syntactic category information, empty nodes (i.e. nodes dominating the empty string) were deleted, and words in rules occurring less than 3 times in the tree-bank were replaced by (word-suffix based) 121 Estimation from most probable heads Estimation from head distributions Model 1 Model 2 Model 1 Model 2 (completely latent) (POS+latent) (completely latent) (POS+latent) baseline (15 400) 73.5 (25 000) 78.9 (15 400) 73.5 (25 000) 78.9 L=2 (17 900) 76.3 (32 300) 81.1 (25 900) 76.9 (49 50</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitch Marcus, Beatrice Santorini, and Mary Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proc. ofACL-05.</booktitle>
<contexts>
<context position="25465" citStr="Matsuzaki et al. (2005)" startWordPosition="4234" endWordPosition="4238">e performance (Fi=84.6), and has the further advantage of having significantly fewer rules (only about 79 200). Moreover, it is the model which leads to the largest gain compared to the baseline (A = 11.1). In the rest of the section, we compare our method to related methods. To start with performance values, Table 3 displays previous results on parsing Section 23 of the WSJ section of the Penn tree-bank. Comparison indicates that our best model is already better than the early lexicalized model of Magerman (1995). It is a bit worse than the unlexicalized PCFGs of Klein and Manning (2003) and Matsuzaki et al. (2005), and of course, it is also worse than state-of-the-art lexicalized parsers (experience shows that evaluation results on sections 22 and 23 do not differ much). Beyond performance values, we believe our formalism and methodology have the following attractive features: first, our models incorporate context and lexical information collected from the whole tree-bank. Information is bundled into abstract heads of higher-order information, which results in a drastically reduced parameter space. In terms of Section 2, our approach does not aim at improving the approximation of rule probabilities p(r</context>
<context position="27637" citStr="Matsuzaki et al. (2005)" startWordPosition="4578" endWordPosition="4581">otion of latent head-information, or as a successful attempt to incorporate lexical classes into parsers, combined with a new word clustering method based on the context represented by tree structure. It thus complements and extends the approach of Chiang and Bikel (2002), who aim at discovering latent head markers in tree-banks to improve manually written head-percolation rules. Finally, the method can also be viewed as an extension offactorial HMMs (Ghahramani and Jordan, 1995) to PCFGs: the node labels on trees are enriched with a latent variable and the latent variables are learned by EM. Matsuzaki et al. (2005) independently introduce a similar approach and present empirical results that rival ours. In contrast to us, 123 they do not use an explicit linguistic grammar, and they do not attempt to constrain the space of latent variables by linguistic principles. As a consequence, our best models are three orders of magnitude more space efficient than theirs (with about 30 000 000 parameters). Therefore, parsing with their models requires sophisticated smoothing and pruning, whereas parsing with ours does not. Moreover, we calculate the most probable latent-headdecorated parse and delete the latent hea</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proc. ofACL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detlef Prescher</author>
</authors>
<title>A Tutorial on the ExpectationMaximization Algorithm Including MaximumLikelihood Estimation and EM Training of Probabilistic Context-Free Grammars. Presented at the 15th European Summer School in Logic, Language and Information (ESSLLI).</title>
<date>2003</date>
<contexts>
<context position="18077" citStr="Prescher (2003)" startWordPosition="3005" endWordPosition="3006">l tree-bank ] (4) M step: Read the tree-bank grammar off TLEX, by - calculating relative frequencies p� for all rules of GLEX as occurring in TLEX (5) set pi = p� (6) end Figure 6: Grammar induction algorithm (EM algorithm) with latent heads cannot be directly estimated from the tree-bank (by counting rules) since the latent heads are not annotated in the trees. Faced with this incomplete-data problem, we apply the ExpectationMaximization (EM) algorithm developed for these type of problems (Dempster et al., 1977). For details of the EM algorithm, we refer to the numerous tutorials on EM (e.g. Prescher (2003)). Here, it suffices to know that it is a sort of meta algorithm, resulting for each incomplete-data problem in an iterative estimation method that aims at maximum-likelihood estimation on the data. Disregarding the fact that we implement a dynamic-programming version for our experiments (running in linear time in the size of the trees in the tree-bank (Prescher, 2005)), the EM algorithm is here as displayed in Figure 6. Beside this pure form of the EM algorithm, we also use a variant where the original tree-bank is annotated with most probable heads only. Here is a characterization of both es</context>
</contexts>
<marker>Prescher, 2003</marker>
<rawString>Detlef Prescher. 2003. A Tutorial on the ExpectationMaximization Algorithm Including MaximumLikelihood Estimation and EM Training of Probabilistic Context-Free Grammars. Presented at the 15th European Summer School in Logic, Language and Information (ESSLLI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detlef Prescher</author>
</authors>
<title>Inducing Head-Driven PCFGs with Latent Heads: Refining a Tree-bank Grammar for Parsing.</title>
<date>2005</date>
<booktitle>In Proc. of the 16th European Conference on Machine Learning.</booktitle>
<contexts>
<context position="18448" citStr="Prescher, 2005" startWordPosition="3064" endWordPosition="3065">with this incomplete-data problem, we apply the ExpectationMaximization (EM) algorithm developed for these type of problems (Dempster et al., 1977). For details of the EM algorithm, we refer to the numerous tutorials on EM (e.g. Prescher (2003)). Here, it suffices to know that it is a sort of meta algorithm, resulting for each incomplete-data problem in an iterative estimation method that aims at maximum-likelihood estimation on the data. Disregarding the fact that we implement a dynamic-programming version for our experiments (running in linear time in the size of the trees in the tree-bank (Prescher, 2005)), the EM algorithm is here as displayed in Figure 6. Beside this pure form of the EM algorithm, we also use a variant where the original tree-bank is annotated with most probable heads only. Here is a characterization of both estimation methods: Estimation from latent-head distributions: The key steps of the EM algorithm produce a lexicalized tree-bank TLEX, consisting of all lexicalized versions of the original trees (E-step), and calculate the probabilities for the rules of GLEX on the basis of TLEX (M-step). Clearly, all lexicalized trees in GLEX(t) differ only in the heads of their nodes.</context>
</contexts>
<marker>Prescher, 2005</marker>
<rawString>Detlef Prescher. 2005. Inducing Head-Driven PCFGs with Latent Heads: Refining a Tree-bank Grammar for Parsing. In Proc. of the 16th European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient parsing of highly ambiguous context-free grammars with bit vectors.</title>
<date>2004</date>
<booktitle>In Proc. of COLING-04.</booktitle>
<contexts>
<context position="22140" citStr="Schmid (2004)" startWordPosition="3674" endWordPosition="3675">t starting parameters affect final results only up to 0.5%. We also tried to find optimal iteration numbers by evaluating our models after each iteration step on a held-out corpus, and observed that the best results were obtained with 70 to 130 iterations. Within a wide range from 50 to 200 iteration, however, iteration numbers affect final results only up to 0.5% Empirical Results We evaluated on a parsing task performed on Section 22 of the WSJ section of the Penn tree-bank. For parsing, we mapped all unknown words to unknown word symbols, and applied the Viterbi algorithm as implemented in Schmid (2004), exploiting its ability to deal with highly-ambiguous grammars. That is, we did not use any pruning or smoothing routines for parsing sentences. We then de-transformed the resulting maximum-probability parses to the format described in the previous sub-section. That is, we deleted the heads, the dependencies, and the starting rules. All grammars were able to exhaustively parse the evaluation corpus. Table 2 displays our results in terms of LP/LR F1 (Black and al., 1991). The largest number per column is printed in italics. The absolutely largest number is printed in boldface. The numbers in b</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient parsing of highly ambiguous context-free grammars with bit vectors. In Proc. of COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
</authors>
<title>Computational complexity of probabilistic disambiguation.</title>
<date>2002</date>
<journal>Grammars,</journal>
<volume>5</volume>
<issue>2</issue>
<marker>Sima’an, 2002</marker>
<rawString>Khalil Sima’an. 2002. Computational complexity of probabilistic disambiguation. Grammars, 5(2).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>