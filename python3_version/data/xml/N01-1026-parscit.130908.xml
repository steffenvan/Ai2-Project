<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001452">
<figure confidence="0.994862583333334">
DT 77 NN IN 77 NN
a significant producer ] for [ crude oil
18 19 20 21 22 23
10 11 12 13 14 15
[un producteur important] de [petrole brut ]
DT NN 77 NN 77
77 NNS VBG IN NNP NNP
[ National laws applying in Hong Kong
] [ ]
0 1
2 3 4 5
10
[][ ]
IN NNP NNP VBG VBG 77 77 77 NNS NNS
1
0
8 9
6 7
4 5
2 3
In Hong
Kong
national law(s)
implementing of
</figure>
<bodyText confidence="0.985298565217391">
French sides.
There are two central limitations to this paradigm,
however. The first is the often very poor accuracy
of word alignments, due both to the current limi-
tations of word-alignment algorithms, and also to
the often weak or incomplete inherent match be-
tween the two sides of a bilingual corpus. The paper
will address and handle this problem through ro-
bust, noise-tolerant learning algorithms capable of
being trained effectively on incomplete and highly
inaccurate alignments. The second limitation is the
potential mismatch in the annotation needs of two
languages; not all distinctions that may be desir-
able for one language (such as grammatical gender in
French) are compatible or even present in a parallel
language such as English. The paper will discuss so-
lutions to these language-level mismatches, and will
illustrate that at the level of noun-phrase structure
and core part-of-speech tags, essential annotations
can be projected with remarkable effectiveness and
coverage in many cases.
Finally, the paper will empirically evaluate two
major questions for each of the tasks:
</bodyText>
<listItem confidence="0.944847555555556">
• The accuracy of the direct projections of
BaseNP structure and POS tags across lan-
guages when (a) word alignments are derived
fully automatically (with heavy noise), and (b)
word alignments are hand-corrected and as op-
timal as possible. The latter offers an upper
bound for direct transfer accuracy.
• The algorithms&apos; ability to generalize from the
noisy training data and tag a held-out monolin-
gual corpus, (c) when standard algorithms are
applied directly to the noisy data without mod-
ification, and (d) when the robust algorithms
described below are employed. The high ac-
curacy of the latter, significantly outperform-
ing direct transfer on cleanly aligned data, indi-
cates the importance of the induction algorithm
beyond simple projection, even under ideal cir-
cumstances.
</listItem>
<sectionHeader confidence="0.903505" genericHeader="abstract">
2 Background
</sectionHeader>
<bodyText confidence="0.99994354">
The approach and general algorithms investigated
in this paper were initiated in conjunction with the
EGYPT project of the 1999 Johns Hopkins sum-
mer machine translation workshop (Al-Onaizan et
al., 1999). Previously, tools for automatic word-
alignment of bilingual corpora were not widely avail-
able outside IBM, the research group pioneering sta-
tistical machine translation with the Candide system
(Brown et al, 1990). The researchers who developed
independent word-alignment tools (e.g. Dagan et al,
1993; Fung and Church, 1994; Wu, 1994; Melamed,
1999; Och and Ney, 2000) tended to focus on trans-
lation model applications for their word-alignments
rather than the induction of stand-alone monolin-
gual analyzers via cross-language projection. For
example, Kupiec (1993) began with existing Xerox
monolingual bracketers to improve translation align-
ments, rather than the converse.
The primary exception has been in the area of par-
allel bilingual parsing. Wu (1995, 1997) proposed
a framework for inversion transduction grammars,
where parallel corpora in languages such as English
and Chinese are parsed concurrently, with cross-
language order differences captured via mobile-like
CFG production reordering. Structural relation-
ships in one language help constrain structural re-
lationships in the second language. Evaluation on
noun-phrase bracketing showed 78% precision for
Chinese, and 80% precision for English. Thus, while
remarkably effective for learning without human-
annotated training data, the algorithm does assume
the existence of a parallel second-language mirror for
all sentences to be parsed. Also, Wu observed signifi-
cant performance degradation when either the word
alignment or translation faithfulness in these pairs
are weak. This further motivates the noise-robust
training and stand-alone application of our current
work.
In a related framework, Jones and Havrilla (1998)
investigated the use of twisted-pair grammars for
syntactic transfer. Given an existing Hindi/Urdu
sentence parse, English output was generated by ro-
tating subtrees using the constraints and preferences
of the transduction grammar. The ability to gener-
ate candidate target-language orderings in this man-
ner offers great potential to productively constrain
search in a statistical MT system. Yet the assump-
tion of existing syntactic analyses for each source
language further motivates the need to induce such
analyses.
</bodyText>
<sectionHeader confidence="0.987301" genericHeader="categories and subject descriptors">
3 Data Resources
</sectionHeader>
<bodyText confidence="0.983572555555556">
The data used in our experiments are the English-
French Canadian Hansards and English-Chinese
Hong Kong Hansards, parallel records of parlia-
mentary proceedings and publications. Both cor-
pora were word-aligned by the now publicly avail-
able EGYPT system (Al-Onaizan et al., 1999) and
based on IBM&apos;s Model 3 statistical MT formalism
(Brown et al., 1990). The data sets used for our
projection studies both contained approximately 2
million words in each language. Their alignment
was based on strictly word-based model variants for
English and character-based model variants for Chi-
nese, with no use of morphological analysis or stem-
ming, POS-tagging, bracketing, outside dictionar-
ies or any other external data source or annotation
tool.&apos; Thus the experiments were carefully designed
1-The two exceptions are end-of-sentence detection and to-
kenization. For the French Hansards, before alignment only
</bodyText>
<figure confidence="0.985634484848485">
4 simple transformations were performed: au—a le, auxa
Tagger Output
English
NNS
NNS
NNS
NNS
Laws ...
... veterans ...
O Laws ...
... potatoes ...
French
Induced Tag
Correct Tag
NNS NNS
a
Les lois ...
(DT) (NNS)
b O NNS NNS a NNS NNS
b c NNS a NNS b
Les lois ...
(DT) (NNS)
... pommes de terre ...
(NNS)
(IN) (NN)
DT NNS VBG NN
Tagger Output
English The laws ... ... living room ...
... anciens combattants ...
(JJ) (NNS)
French
Les lois ... O ... salon ...
Induced Tags DT NNS NN
</figure>
<bodyText confidence="0.9998982">
even when the high-error automatic alignments have
been manually corrected, yielding 69% and 78%
direct projection accuracy respectively (at English
tagset granularity). Traditional supervised learning
algorithms tend to perform poorly at this level of
noise, and a standard bigram tagger trained on the
automatically aligned (uncorrected) data achieves
only 82% when evaluated on a held-out test set.
More highly lexicalized learning algorithms exhibit
even greater potential for overmodeling the specific
projection errors of this data.
Thus our research has focused on noise-robust
techniques for distilling a conservative but effective
tagger from this challenging raw projection data. To
do so, we (a) downweight or exclude training data
segments identified as poorly aligned or likely noise
(b) use a conservative bigram learning algorithm,
and (c) train the lexical prior and tag-sequence mod-
els separately using aggressive generalization tech-
niques.
</bodyText>
<subsubsectionHeader confidence="0.492904">
4.2.1 Lexical Prior Estimation
</subsubsectionHeader>
<bodyText confidence="0.87582275">
In a standard bigram tagging model, one selects a
tag sequence T for a word sequence W by:
argmax P(TIW) = P(WIT)P(T)
where
</bodyText>
<equation confidence="0.8835404">
P(T) = P(t1)P(t2It1)-P(tnItn-1)
and
71
p(wIT)=p(wl...wnitl...tr),Hp(wilti)
i=1
</equation>
<bodyText confidence="0.999905">
using standard independence assumptions. Sec-
tion 4.2.2 will discuss the estimation of P(tilti-i)•
The following section describes the estimation of
P(tiiwi), which using Bayes rule and direct (rel-
atively noise-free) measurement of P(w) from the
French data, can be used to calculate P(wiiti) as:
</bodyText>
<equation confidence="0.996975333333333">
P(tiiwi)P(wi)
P(wilti) =
E, P(tiln)P(w3)
</equation>
<bodyText confidence="0.999291428571428">
Inspection of the raw projected tag data shows the
need for an improved estimation of P(t1w). Tem-
porarily excluding the case of compound alignments
(e.g. NNS(,), Table 1 shows the observed frequency
distributions of English tags projected onto four
French words from 1-to-1 alignments, for the core
N/V/J/R/I POS tags. Note that the total proba-
bility mass assigned to potentially correct tags (in
bold) is relatively low, with fairly broad misassign-
ment to incorrect tags for the given word.
At the core tag level in particular, we observe em-
pirically that words in French have a strong ten-
dency to have only 1 possible core POS tag, and
very rarely have more than 2. Even in English, with
</bodyText>
<table confidence="0.9995975">
Directly Projected Tag Tag
Word JN V R I Error
achat 0 62 48 0 1 0.44
cadre 2 35 7 1 1 0.27
cadres 1 5 0 0 0 0.17
prevu 1 11 48 0 0 0.20
</table>
<tableCaption confidence="0.999931">
Table 1: Raw projected tag distributions.
</tableCaption>
<bodyText confidence="0.9999335">
relatively high P(POSIw) ambiguity, only 0.37% of
the tokens in the Brown Corpus are not covered by
a word type&apos;s two most frequent core tags, and in
French the percentage drops to 0.03%. Thus we em-
ploy an aggressive re-estimation in favor of this bias,
where for t(i) = the ith most frequent tag for w:
</bodyText>
<equation confidence="0.999959333333333">
f&amp;quot;(t(2)1w) = /3 (t(2)1w) where A1 &lt; 1.0
P(t(i )1w) = 1 - P(t(2)1w)
P(t(c)1w) = 0 for all c&gt; 2
</equation>
<bodyText confidence="0.948145">
giving the large majority of the new probability mass
to the single highest frequency core tag.
</bodyText>
<table confidence="0.999368">
Smoothed P(t1w)
Word N V NN NNS VBN VBG
achat .76 .24 .73 .03 .03 .21
cadre .90 .10 .86 .04 .03 .00
cadres .94 .00 .04 .90 .00 .00
prevu .09 .91 .08 .01 .86 .00
</table>
<tableCaption confidence="0.997124">
Table 2: Smoothed P(t1w) tag probabilities
</tableCaption>
<bodyText confidence="0.997281071428571">
Applying this model recursively, the finer grained
subtag probabilities (e.g. NN, NNS) are assigned by
selecting the two highest frequency subtags for each
of the two remaining core tags, and reallocating the
core tag probability mass between these two as in
the equations above, as illustrated in Table 2.
Finally, the issue arises of what to do with the
1-to-n phrasal alignment cases shown in Figure
2 (e.g. potatoes/NNS pommesINNS„ de/NNSb
terre/NNS, and Laws/NNS Les/NNS„ /ois/NNSb).
The potential seems to be great for function
words to inherit substantial spurious probability
mass via such data. However, the relatively fre-
quent occurrence of correct 1-to-1 alignments (e.g.
TheluN4Les and ofliN-xcle), the diffuse nature of
the noise, and the aggressive smoothing towards a
single POS tag, prevent these cases from adversely
affecting final function word assignments. Given the
lower frequency of most content words, the potential
risks of using these 1-to-n alignments are greater,
but so are the benefits given that the 1-to-1 align-
ments tend to be both sparse and somewhat biased.
Several options are under investigation for combin-
ing these two P(t1w) estimators, but the simplest,
and currently most effective, is to perform basic in-
terpolation between the tag distributions estimated
from 1-to-1 alignments only and from the entire set
of 1-to-n alignments (including 1-to-1) as follows:
</bodyText>
<equation confidence="0.998308">
P(t1w) = A2P1-to- 1 + (1 — A2)P1-to-010
</equation>
<bodyText confidence="0.9999825">
While this does indeed introduce substantial spuri-
ous tag probabilities initially, the aggressive smooth-
ing towards the majority tag(s) described above
tends to eliminate most of this noise.
</bodyText>
<subsubsectionHeader confidence="0.820665">
4.2.2 Tag Sequence Model Estimation
</subsubsectionHeader>
<bodyText confidence="0.930020326086957">
The major reason for estimating the lexical priors
and tag sequence model separately is that a tag se-
quence bigram (or even trigram) model has far fewer
parameters than the lexical prior model and thus
can be estimated on a very conservatively chosen set
of filtered, high confidence alignment data. In con-
trast, the lexical prior models already suffer from
sparse data problems and are negatively affected by
an order-of-magnitude data reduction, even if the
data is of higher quality.
The proposed model for identifying high-quality
tag sequence data for training considers two different
information sources for sentence filtering/weighting.
The first is the final Model-3 alignment score for
the sentence, indicating a multi-source measure of
overall alignment confidence. The second mea-
sure more directly targets confidence in the tag se-
quences themselves. After the lexical prior mod-
els have been trained (as above), sentences are also
tested to identify those where the directly projected
tag sequence (from the automatic alignments) is
closely compatible with the estimated lexical prior
probabilities for each word. A pseudo-divergence
weighting is computed for a sentence of length k
by I Ejk_i log P (projected-tag, lw,), penalizing words
whose projected tag doesn&apos;t match the majority lexi-
cal prior.2 Sorting and filtering/weighting by the cu-
mulative normalized score yields a subset of training
data where multiple sources essentially concur on the
correct tag sequence. While the potential exists that
this higher confidence data subset may be biased in
the sequence phenomena it contains, the substantial
noise reduction in preliminary investigations appears
to be a worthwhile tradeoff. Future work will focus
on differential confidence weighting of sentence frag-
ments, and iterative (E-M) re-estimation.
2The exception is for function words (i.e. the major-
ity lexical prior is not a Noun, Verb, Adjective or Adverb)
located in a 1-to-n alignment sequence. Given the very
high probability of these raw projections being incorrect, and
their prevalence, it is expedient to attempt to correct (rather
than weight/filter) these tag instances prior to the first tag-
sequence-model training, by replacing their raw projection
tag with the majority lexical prior for the word from 4.2.1.
Doing so salvages very large quantities of otherwise accurate
tag sequence data with very little introduced noise.
</bodyText>
<subsectionHeader confidence="0.977419">
4.3 Evaluation of Induced Taggers
</subsectionHeader>
<bodyText confidence="0.996926038461538">
Evaluation of the tagger projection and induction al-
gorithms is conducted on two granularities of tagset.
The first tagset is at the level of core part-of-speech
tags such as Verb (V), Noun (N), Pronoun (P), Ad-
jective (J), Adverb (R), Preposition (I), Determiner
(D), etc., for which English and French share re-
markable compatibility.&apos; The second is at the level
of granularity captured in the English Penn Tree-
bank tagset, where for example singular and plural
nouns (NN and NNS) are distinguished. As previ-
ously noted, the goal of this work is not to induce
potential French tagset features such as grammatical
gender, mood or subtle tense distinctions that do not
appear in English, but to focus on the algorithm&apos;s
effectiveness at accurately transferring tagging capa-
bilities at the granularity that is present in English
(or whichever projection source language used).
For independent evaluation data, a 120K-word
hand-tagged French dataset generously provided by
Universite de Montreal was used. However, because
both this text stream and tagset had no overlap with
parallel data used to train the algorithm, a sim-
ple mapping table between the tagsets was defined
so that output could be compared on a compati-
ble common denominator. An abbreviated version
is shown in Table 3:4
</bodyText>
<table confidence="0.99956119047619">
Original French Tagset English Core
Equiv Consensus
Tagset
NomC-sing-* NN N
NomC-plur-* NNS N
AdjQ-* JJ J
Adve RB R
Prep IN I
Num CD #
ConcC CC C
Pron-* PRP P
Dete-* DT D
Verb-ParPas-* VBN v
Verb-ParPre VBG v
Verb-IndImp-* VBD v
Verb-SubImp-* VBD v
Verb-IndPas-* VBD v
Verb-IndPre-* VBP v
Verb-SubPre-* VBP v
Verb-ConPre-* VB v
Verb-InfPre-* VB v
</table>
<tableCaption confidence="0.699641571428571">
Table 3: French-English consensus tagset map
3Indeed, Comrie (1990) indicates that these core POS tag
distinctions tend to be almost language universal. Although
some individual lexical concepts may be realized by different
parts of speech in different languages, the general functional
class of &amp;quot;noun&amp;quot; (for example) tends to exist in nearly all lan-
guages, and concepts which are considered to be nouns in one
</tableCaption>
<footnote confidence="0.85165325">
language also strongly tend to be realized as nouns in other
languages.
4For compatibility with the consensus tagset, the English
output tags were condensed somewhat as well, downmapping
English distinctions not made in the French tagset such as
comparative and superlative adjectives (JJR—}JJ and
the special status for 3PsingPres verbal forms (yEtz—m3P),
and a separate category for modal verbs (A/D—m3).
</footnote>
<figure confidence="0.810594352941176">
J’ ai vu la maison infestee par termites .
[{ } { }]
I saw the termite - infested house .
[ ]
]
[ J N ] VBD N N IN N
[ 2 ] [
1 1 2 3
O
[DT N J VBD [ N de N DT N
] ] [ [
(1) (1) (1) (2) (2) (3)
]
[DT 1 J 1 N 1 ] VBD N N
[ 2 2 ]
O
[DT( 1 ) N (1) ] VBD [ N (2)} { J ( 1 ) de N(2)]
</figure>
<table confidence="0.98437">
Method Exact Match Acceptable Match
Pr Pr
Chinese:
Direct (auto) .26 .58 .36 .48 .58 .51
Direct (hand) .47 .61 .53 .86 .86 .86
French:
Direct (auto) .43 .48 .45 .60 .58 .59
Direct (hand) .56 .51 .53 .74 .70 .72
FTBL (auto) .82 .81 .81 .91 .91 .91
</table>
<tableCaption confidence="0.929374">
Table 5: Performance of BaseNP induction models
with precision (Pr), recall (R) and F-measure.
</tableCaption>
<bodyText confidence="0.9997095">
The large majority of these compatible diver-
gences in bracketing convention are due to the pro-
jection algorithm&apos;s tendency to bracket possessive
compounds as single NP&apos;s (e.g. [DT N de N]), and its
tendency to bracket simple conjunctive compounds
(e.g. [DT N et ND also as single NPs, following
the Ramshaw and Marcus convention which differed
from the French and Chinese goldstandard annota-
tor&apos;s intuitions.
Overall, these translingual projection results are
quite encouraging. For Chinese, they are similar to
Wu&apos;s 78% precision result, and especially promising
given that no word segmentation (only raw charac-
ters) were used. For French, the increase from 59%
F-measure on direct projection to 91% F-measure
for the stand-alone induced bracketer shows that the
training algorithm is able to generalize successfully
from the very noisy raw projection data, distilling
a reasonably accurate (and transferable) model of
BaseNP structure from this high degree of noise.
</bodyText>
<sectionHeader confidence="0.998523" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999948918918919">
This paper has shown that automatically word-
aligned bilingual corpora can be used to induce both
successful part-of-speech taggers and noun-phrase
bracketers. It has further illustrated that simple di-
rect projection of POS and NP annotations across
languages is very noisy, even when the word align-
ments have been manually corrected. Noise-robust
data filtering and modeling procedures are shown to
train effectively on this low-quality data. The result-
ing stand-alone part-of-speech taggers and BaseNP
bracketers significantly outperform the raw direct
projections on which they were trained. This indi-
cates that they have successfully distilled and mod-
eled the signal present in the very noisy projection
data, and are able to perform as respectable stand-
alone monolingual tools with absolutely no human-
supervised training data in the target language.
These results also show considerable potential for
further improvement by co-training with monolin-
gually induced morphological analyzers. The stand-
alone monolingual POS taggers and bracketers in-
duced from word-aligned data also show potential
for improving their initial alignments. NP bracket-
ings for both the source and target language can im-
prove the IBM MT distortion model, by boosting the
probabilities of word alignments consistent with co-
hesive NP structure, and penalizing alignments that
break NP cohesion. A stand-alone POS tagger ap-
plicable to new data can be used to improve statisti-
cal MT translation models, both by supporting finer
translation model granularity (e.g. wind/NN mod-
eled distinctly from wind/VB), and by serving as a
source of backoff alignment probabilities for previ-
ously unseen words. Thus tagging models induced
from bilingual alignments can be used to improve
these very alignments, and hence improve their own
training source.
</bodyText>
<sectionHeader confidence="0.998489" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833367346939">
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty, D.
Melamed, FJ Och, D. Purdy, N. Smith and D. Yarowsky.
1999. Statistical Machine Translation (tech report). Johns
Hopkins University.
E. Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part of
speech tagging. Computational Linguistics, 24(1):543-565.
P. Brown, J. Cocke, S. DellaPietra, V. DellaPietra, F. Jelinek,
J. Lafferty, R. Mercer, and P. Rossin. 1990. A statistical
approach to machine translation. Computational Linguis-
tics, 16(2):29-85.
B. Comrie. 1990. All the World&apos;s Major Languages. Oxford:
Oxford University Press.
S. Cucerzan and D. Yarowsky, 2000. Language independent
minimally supervised induction of lexical probabilities. In
Proceedings of ACL-2000, Hong Kong, pp. 270-277.
I. Dagan, K. Church, and W. Gale. 1993. Robust bilingual
word alignment for machine aided translation. In Procs. of
the Workshop on Very Large Corpora, pp. 1-8.
P. Fung and K. Church. 1994. K-vec: a new approach for
aligning parallel texts. In COLING-94, pp. 1096-1102.
D. Jones, and R. Havrilla. 1998 Twisted pair grammar: sup-
port for rapid development of machine translation for low
density languages In Procs. of AMTA&apos;98, pp. 318-332.
J. Kupiec. 1993. An algorithm for finding noun phrase cor-
respondences in bilingual corpora. In Proceedings of ACL-
93, pp. 17-22.
D. Melamed. 1999. Bitext maps and alignment via pattern
recognition. Computational Linguistics, 25(1):107-130.
G. Ngai and R. Florian. 2001. Transformation-based learning
in the fast lane. In Proceedings of NAACL-2001.
F. Och and H. Ney. 2000. Improved statistical alignment
models. In Proceedings of ACL-2000, pp. 440-447.
L. Ramshaw and M. Marcus, 1999. Text chunking using
transformation-based learning. In Natural Language Pro-
cessing Using Very Large Corpora. Kluwer. pp. 157-176.
D. Wu. 1994. Aligning a parallel English-Chinese corpus sta-
tistically with lexical criteria. In Proc. ACL-94, pp. 80-87.
D. Wu. 1995. An algorithm for simultaneously bracketing
parallel texts. In Proc. of ACL-95, pp. 244-251.
D. Wu. 1997. Statistical inversion transduction grammars an
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377-404.
D. Yarowsky and R. Wicentowski. 2000. Minimally super-
vised morphological analysis by multimodal alignment. In
Proceedings of ACL-2000, pp. 207-216.
D. Yarowsky, G. Ngai and R. Wicentowski. 2001. Inducing
multilingual text analysis tools via robust projection across
aligned corpora. In Proceedings of HLT-2001, pp. 109-116.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.5684435">DT 77 NN IN 77 NN significant producer for[ oil</note>
<phone confidence="0.4976665">18 19 20 21 22 23 10 11 12 13 14 15</phone>
<abstract confidence="0.90586385915493">producteur brut DT NN 77 NN 77 77 NNS VBG IN NNP NNP laws applying in Hong Kong ] [ ] 0 1 2 3 4 5 10 [][ ] IN NNP NNP VBG VBG 77 77 77 NNS NNS 1 0 8 9 6 7 4 5 2 3 In Hong Kong national law(s) implementing of French sides. There are two central limitations to this paradigm, however. The first is the often very poor accuracy of word alignments, due both to the current limitations of word-alignment algorithms, and also to the often weak or incomplete inherent match between the two sides of a bilingual corpus. The paper will address and handle this problem through robust, noise-tolerant learning algorithms capable of being trained effectively on incomplete and highly inaccurate alignments. The second limitation is the potential mismatch in the annotation needs of two languages; not all distinctions that may be desirable for one language (such as grammatical gender in French) are compatible or even present in a parallel language such as English. The paper will discuss solutions to these language-level mismatches, and will illustrate that at the level of noun-phrase structure and core part-of-speech tags, essential annotations can be projected with remarkable effectiveness and coverage in many cases. Finally, the paper will empirically evaluate two major questions for each of the tasks: • The accuracy of the direct projections of BaseNP structure and POS tags across languages when (a) word alignments are derived fully automatically (with heavy noise), and (b) word alignments are hand-corrected and as optimal as possible. The latter offers an upper bound for direct transfer accuracy. • The algorithms&apos; ability to generalize from the noisy training data and tag a held-out monolingual corpus, (c) when standard algorithms are applied directly to the noisy data without modification, and (d) when the robust algorithms described below are employed. The high accuracy of the latter, significantly outperforming direct transfer on cleanly aligned data, indicates the importance of the induction algorithm beyond simple projection, even under ideal circumstances. 2 Background The approach and general algorithms investigated in this paper were initiated in conjunction with the of the 1999 Johns Hopkins summer machine translation workshop (Al-Onaizan et al., 1999). Previously, tools for automatic wordalignment of bilingual corpora were not widely available outside IBM, the research group pioneering statistical machine translation with the Candide system (Brown et al, 1990). The researchers who developed independent word-alignment tools (e.g. Dagan et al, 1993; Fung and Church, 1994; Wu, 1994; Melamed, 1999; Och and Ney, 2000) tended to focus on translation model applications for their word-alignments than the induction of stand-alone monolinvia cross-language projection. For example, Kupiec (1993) began with existing Xerox monolingual bracketers to improve translation alignments, rather than the converse. The primary exception has been in the area of parallel bilingual parsing. Wu (1995, 1997) proposed a framework for inversion transduction grammars, where parallel corpora in languages such as English and Chinese are parsed concurrently, with crosslanguage order differences captured via mobile-like CFG production reordering. Structural relationships in one language help constrain structural relationships in the second language. Evaluation on noun-phrase bracketing showed 78% precision for Chinese, and 80% precision for English. Thus, while remarkably effective for learning without humanannotated training data, the algorithm does assume the existence of a parallel second-language mirror for all sentences to be parsed. Also, Wu observed significant performance degradation when either the word alignment or translation faithfulness in these pairs are weak. This further motivates the noise-robust training and stand-alone application of our current work. In a related framework, Jones and Havrilla (1998) investigated the use of twisted-pair grammars for syntactic transfer. Given an existing Hindi/Urdu sentence parse, English output was generated by rotating subtrees using the constraints and preferences of the transduction grammar. The ability to generate candidate target-language orderings in this manner offers great potential to productively constrain search in a statistical MT system. Yet the assumption of existing syntactic analyses for each source language further motivates the need to induce such analyses. 3 Data Resources The data used in our experiments are the English- French Canadian Hansards and English-Chinese Hong Kong Hansards, parallel records of parliamentary proceedings and publications. Both corpora were word-aligned by the now publicly avail- (Al-Onaizan et al., 1999) and based on IBM&apos;s Model 3 statistical MT formalism (Brown et al., 1990). The data sets used for our projection studies both contained approximately 2 million words in each language. Their alignment was based on strictly word-based model variants for English and character-based model variants for Chiwith of morphological analysis or stemming, POS-tagging, bracketing, outside dictionaries or any other external data source or annotation tool.&apos; Thus the experiments were carefully designed two exceptions are end-of-sentence detection and tokenization. For the French Hansards, before alignment only 4 simple transformations were performed: au—a le, auxa Tagger Output English NNS NNS NNS NNS Laws ... ... veterans ... O Laws ... ... potatoes ...</abstract>
<title confidence="0.600656">French Induced Tag Correct Tag NNS NNS</title>
<abstract confidence="0.987484712121212">a Les lois ... (DT) (NNS) b O NNS NNS a NNS NNS b c NNS a NNS b Les lois ... (DT) (NNS) ... pommes de terre ... (NNS) (IN) (NN) DT NNS VBG NN Tagger Output laws ... ... living room ... ... anciens combattants ... (JJ) (NNS) French Les lois ... O ... salon ... Induced Tags DT NNS NN even when the high-error automatic alignments have been manually corrected, yielding 69% and 78% direct projection accuracy respectively (at English tagset granularity). Traditional supervised learning algorithms tend to perform poorly at this level of noise, and a standard bigram tagger trained on the automatically aligned (uncorrected) data achieves only 82% when evaluated on a held-out test set. More highly lexicalized learning algorithms exhibit even greater potential for overmodeling the specific projection errors of this data. Thus our research has focused on noise-robust techniques for distilling a conservative but effective tagger from this challenging raw projection data. To do so, we (a) downweight or exclude training data segments identified as poorly aligned or likely noise (b) use a conservative bigram learning algorithm, and (c) train the lexical prior and tag-sequence models separately using aggressive generalization techniques. 4.2.1 Lexical Prior Estimation In a standard bigram tagging model, one selects a sequence a word sequence W by: where P(t1)P(t2It1)-P(tnItn-1) and 71 p(wIT)=p(wl...wnitl...tr),Hp(wilti) using standard independence assumptions. Section 4.2.2 will discuss the estimation of P(tilti-i)• The following section describes the estimation of using Bayes rule and direct (relnoise-free) measurement of the data, can be used to calculate as: P(wilti) = Inspection of the raw projected tag data shows the for an improved estimation of Temporarily excluding the case of compound alignments Table 1 shows the observed frequency distributions of English tags projected onto four French words from 1-to-1 alignments, for the core N/V/J/R/I POS tags. Note that the total probability mass assigned to potentially correct tags (in bold) is relatively low, with fairly broad misassignment to incorrect tags for the given word. At the core tag level in particular, we observe empirically that words in French have a strong tendency to have only 1 possible core POS tag, and very rarely have more than 2. Even in English, with Directly Projected Tag Tag Word JN V R I Error achat 0 62 48 0 1 0.44 cadre 2 35 7 1 1 0.27 cadres 1 5 0 0 0 0.17 prevu 1 11 48 0 0 0.20 Table 1: Raw projected tag distributions. high only 0.37% of the tokens in the Brown Corpus are not covered by a word type&apos;s two most frequent core tags, and in French the percentage drops to 0.03%. Thus we employ an aggressive re-estimation in favor of this bias, for = the most frequent tag for w: = (t(2)1w) where A1 &lt; 1.0 )1w) = 1- P(t(2)1w) 0for all giving the large majority of the new probability mass to the single highest frequency core tag. Word N V NN NNS VBN VBG achat .76 .24 .73 .03 .03 .21 cadre .90 .10 .86 .04 .03 .00 cadres .94 .00 .04 .90 .00 .00 prevu .09 .91 .08 .01 .86 .00 2: Smoothed probabilities Applying this model recursively, the finer grained subtag probabilities (e.g. NN, NNS) are assigned by selecting the two highest frequency subtags for each of the two remaining core tags, and reallocating the core tag probability mass between these two as in the equations above, as illustrated in Table 2. Finally, the issue arises of what to do with the 1-to-n phrasal alignment cases shown in Figure (e.g. pommesINNS„ and Laws/NNS Les/NNS„ The potential seems to be great for function words to inherit substantial spurious probability mass via such data. However, the relatively frequent occurrence of correct 1-to-1 alignments (e.g. diffuse nature of the noise, and the aggressive smoothing towards a single POS tag, prevent these cases from adversely affecting final function word assignments. Given the lower frequency of most content words, the potential risks of using these 1-to-n alignments are greater, but so are the benefits given that the 1-to-1 alignments tend to be both sparse and somewhat biased. Several options are under investigation for combinthese two but the simplest, and currently most effective, is to perform basic interpolation between the tag distributions estimated from 1-to-1 alignments only and from the entire set of 1-to-n alignments (including 1-to-1) as follows: + — While this does indeed introduce substantial spurious tag probabilities initially, the aggressive smoothing towards the majority tag(s) described above tends to eliminate most of this noise. 4.2.2 Tag Sequence Model Estimation The major reason for estimating the lexical priors and tag sequence model separately is that a tag sequence bigram (or even trigram) model has far fewer parameters than the lexical prior model and thus can be estimated on a very conservatively chosen set of filtered, high confidence alignment data. In contrast, the lexical prior models already suffer from sparse data problems and are negatively affected by an order-of-magnitude data reduction, even if the data is of higher quality. The proposed model for identifying high-quality tag sequence data for training considers two different information sources for sentence filtering/weighting. The first is the final Model-3 alignment score for the sentence, indicating a multi-source measure of overall alignment confidence. The second measure more directly targets confidence in the tag sequences themselves. After the lexical prior models have been trained (as above), sentences are also tested to identify those where the directly projected tag sequence (from the automatic alignments) is closely compatible with the estimated lexical prior probabilities for each word. A pseudo-divergence is computed for a sentence of length I log P penalizing words whose projected tag doesn&apos;t match the majority lexi- Sorting and filtering/weighting by the cumulative normalized score yields a subset of training data where multiple sources essentially concur on the correct tag sequence. While the potential exists that this higher confidence data subset may be biased in the sequence phenomena it contains, the substantial noise reduction in preliminary investigations appears to be a worthwhile tradeoff. Future work will focus on differential confidence weighting of sentence fragments, and iterative (E-M) re-estimation. exception is for function words (i.e. the majority lexical prior is not a Noun, Verb, Adjective or Adverb) located in a 1-to-n alignment sequence. Given the very high probability of these raw projections being incorrect, and their prevalence, it is expedient to attempt to correct (rather than weight/filter) these tag instances prior to the first tagsequence-model training, by replacing their raw projection tag with the majority lexical prior for the word from 4.2.1. Doing so salvages very large quantities of otherwise accurate tag sequence data with very little introduced noise. 4.3 Evaluation of Induced Taggers Evaluation of the tagger projection and induction algorithms is conducted on two granularities of tagset. The first tagset is at the level of core part-of-speech tags such as Verb (V), Noun (N), Pronoun (P), Adjective (J), Adverb (R), Preposition (I), Determiner (D), etc., for which English and French share remarkable compatibility.&apos; The second is at the level of granularity captured in the English Penn Treebank tagset, where for example singular and plural nouns (NN and NNS) are distinguished. As previously noted, the goal of this work is not to induce potential French tagset features such as grammatical gender, mood or subtle tense distinctions that do not appear in English, but to focus on the algorithm&apos;s effectiveness at accurately transferring tagging capabilities at the granularity that is present in English (or whichever projection source language used). For independent evaluation data, a 120K-word hand-tagged French dataset generously provided by Universite de Montreal was used. However, because both this text stream and tagset had no overlap with parallel data used to train the algorithm, a simple mapping table between the tagsets was defined so that output could be compared on a compatible common denominator. An abbreviated version shown in Table</abstract>
<title confidence="0.7088315">Original French Tagset English Equiv Tagset NomC-sing-* NomC-plur-* NNS N AdjQ-* JJ J Adve Prep Num ConcC Pron-* Dete-* RB IN CD CC PRP DT I P D</title>
<abstract confidence="0.76066228125">Verb-ParPas-* Verb-ParPre Verb-IndImp-* Verb-SubImp-* Verb-IndPas-* Verb-IndPre-* Verb-SubPre-* Verb-ConPre-* Verb-InfPre-* VBN VBG VBD VBD VBD VBP VBP VB VB v v v v v v v v v Table 3: French-English consensus tagset map Comrie (1990) indicates that these core POS tag distinctions tend to be almost language universal. Although some individual lexical concepts may be realized by different parts of speech in different languages, the general functional class of &amp;quot;noun&amp;quot; (for example) tends to exist in nearly all languages, and concepts which are considered to be nouns in one language also strongly tend to be realized as nouns in other languages. compatibility with the consensus tagset, the English output tags were condensed somewhat as well, downmapping English distinctions not made in the French tagset such as and superlative adjectives the special status for 3PsingPres verbal forms (yEtz—m3P), and a separate category for modal verbs (A/D—m3). J’ ai vu la maison infestee par termites . [{ } { }] I saw the termite infested house . [ ] ] [ N N N IN N [ 1 1 2 3 O N J VBD de N DT N ] ] [ [ (1) (1) (1) (2) (2) (3) ] 1J N N 2 O</abstract>
<note confidence="0.882760222222222">1 ) (1)] { 1 ) Method Exact Match Acceptable Match Pr Pr Chinese: Direct .26 .58 .36 .48 .58 .51 Direct (hand) .47 .61 .53 .86 .86 .86 French: Direct (auto) .43 .48 .45 .60 .58 .59 Direct (hand) .56 .51 .53 .74 .70 .72</note>
<abstract confidence="0.989869803278689">FTBL (auto) .82 .81 .81 .91 .91 .91 Table 5: Performance of BaseNP induction models with precision (Pr), recall (R) and F-measure. The large majority of these compatible divergences in bracketing convention are due to the projection algorithm&apos;s tendency to bracket possessive as single NP&apos;s (e.g. [DT and its tendency to bracket simple conjunctive compounds [DT as single NPs, following the Ramshaw and Marcus convention which differed from the French and Chinese goldstandard annotator&apos;s intuitions. Overall, these translingual projection results are quite encouraging. For Chinese, they are similar to Wu&apos;s 78% precision result, and especially promising given that no word segmentation (only raw characters) were used. For French, the increase from 59% F-measure on direct projection to 91% F-measure for the stand-alone induced bracketer shows that the training algorithm is able to generalize successfully from the very noisy raw projection data, distilling a reasonably accurate (and transferable) model of BaseNP structure from this high degree of noise. 6 Conclusion This paper has shown that automatically wordaligned bilingual corpora can be used to induce both successful part-of-speech taggers and noun-phrase bracketers. It has further illustrated that simple direct projection of POS and NP annotations across languages is very noisy, even when the word alignments have been manually corrected. Noise-robust data filtering and modeling procedures are shown to train effectively on this low-quality data. The resulting stand-alone part-of-speech taggers and BaseNP bracketers significantly outperform the raw direct projections on which they were trained. This indicates that they have successfully distilled and modeled the signal present in the very noisy projection data, and are able to perform as respectable standmonolingual tools with absolutely humansupervised training data in the target language. These results also show considerable potential for further improvement by co-training with monolingually induced morphological analyzers. The standalone monolingual POS taggers and bracketers induced from word-aligned data also show potential for improving their initial alignments. NP bracketings for both the source and target language can improve the IBM MT distortion model, by boosting the probabilities of word alignments consistent with cohesive NP structure, and penalizing alignments that break NP cohesion. A stand-alone POS tagger applicable to new data can be used to improve statistical MT translation models, both by supporting finer translation model granularity (e.g. wind/NN modeled distinctly from wind/VB), and by serving as a source of backoff alignment probabilities for previously unseen words. Thus tagging models induced from bilingual alignments can be used to improve these very alignments, and hence improve their own training source.</abstract>
<note confidence="0.5116765">References Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty, D. Melamed, FJ Och, D. Purdy, N. Smith and D. Yarowsky. Machine Translation report). Johns</note>
<affiliation confidence="0.925463">Hopkins University.</affiliation>
<abstract confidence="0.722316">E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of tagging. Linguistics,</abstract>
<author confidence="0.7525415">A statistical</author>
<affiliation confidence="0.508125">to machine translation. Linguis-</affiliation>
<address confidence="0.600695">Comrie. 1990. the World&apos;s Major Languages.</address>
<affiliation confidence="0.78531">Oxford University Press.</affiliation>
<note confidence="0.857736371428572">S. Cucerzan and D. Yarowsky, 2000. Language independent minimally supervised induction of lexical probabilities. In of ACL-2000, Kong, pp. 270-277. I. Dagan, K. Church, and W. Gale. 1993. Robust bilingual alignment for machine aided translation. In of Workshop on Very Large Corpora, 1-8. P. Fung and K. Church. 1994. K-vec: a new approach for parallel texts. In 1096-1102. D. Jones, and R. Havrilla. 1998 Twisted pair grammar: support for rapid development of machine translation for low languages In of AMTA&apos;98, 318-332. J. Kupiec. 1993. An algorithm for finding noun phrase corin bilingual corpora. In of ACL- 17-22. D. Melamed. 1999. Bitext maps and alignment via pattern Linguistics, G. Ngai and R. Florian. 2001. Transformation-based learning the fast lane. In of NAACL-2001. F. Och and H. Ney. 2000. Improved statistical alignment In of ACL-2000, 440-447. L. Ramshaw and M. Marcus, 1999. Text chunking using learning. In Language Pro- Using Very Large Corpora. pp. 157-176. D. Wu. 1994. Aligning a parallel English-Chinese corpus stawith lexical criteria. In ACL-94, 80-87. D. Wu. 1995. An algorithm for simultaneously bracketing texts. In of ACL-95, 244-251. D. Wu. 1997. Statistical inversion transduction grammars an parsing of parallel corpora. Lin- D. Yarowsky and R. Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In of ACL-2000, 207-216. D. Yarowsky, G. Ngai and R. Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across corpora. In of HLT-2001, 109-116.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>J Curin</author>
<author>M Jahr</author>
<author>K Knight</author>
<author>J Lafferty</author>
<author>D Melamed</author>
<author>FJ Och</author>
<author>D Purdy</author>
<author>N Smith</author>
<author>D Yarowsky</author>
</authors>
<title>Statistical Machine Translation (tech report).</title>
<date>1999</date>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="2430" citStr="Al-Onaizan et al., 1999" startWordPosition="412" endWordPosition="415">a and tag a held-out monolingual corpus, (c) when standard algorithms are applied directly to the noisy data without modification, and (d) when the robust algorithms described below are employed. The high accuracy of the latter, significantly outperforming direct transfer on cleanly aligned data, indicates the importance of the induction algorithm beyond simple projection, even under ideal circumstances. 2 Background The approach and general algorithms investigated in this paper were initiated in conjunction with the EGYPT project of the 1999 Johns Hopkins summer machine translation workshop (Al-Onaizan et al., 1999). Previously, tools for automatic wordalignment of bilingual corpora were not widely available outside IBM, the research group pioneering statistical machine translation with the Candide system (Brown et al, 1990). The researchers who developed independent word-alignment tools (e.g. Dagan et al, 1993; Fung and Church, 1994; Wu, 1994; Melamed, 1999; Och and Ney, 2000) tended to focus on translation model applications for their word-alignments rather than the induction of stand-alone monolingual analyzers via cross-language projection. For example, Kupiec (1993) began with existing Xerox monolin</context>
<context position="4930" citStr="Al-Onaizan et al., 1999" startWordPosition="776" endWordPosition="779">and preferences of the transduction grammar. The ability to generate candidate target-language orderings in this manner offers great potential to productively constrain search in a statistical MT system. Yet the assumption of existing syntactic analyses for each source language further motivates the need to induce such analyses. 3 Data Resources The data used in our experiments are the EnglishFrench Canadian Hansards and English-Chinese Hong Kong Hansards, parallel records of parliamentary proceedings and publications. Both corpora were word-aligned by the now publicly available EGYPT system (Al-Onaizan et al., 1999) and based on IBM&apos;s Model 3 statistical MT formalism (Brown et al., 1990). The data sets used for our projection studies both contained approximately 2 million words in each language. Their alignment was based on strictly word-based model variants for English and character-based model variants for Chinese, with no use of morphological analysis or stemming, POS-tagging, bracketing, outside dictionaries or any other external data source or annotation tool.&apos; Thus the experiments were carefully designed 1-The two exceptions are end-of-sentence detection and tokenization. For the French Hansards, b</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty, D. Melamed, FJ Och, D. Purdy, N. Smith and D. Yarowsky. 1999. Statistical Machine Translation (tech report). Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>24--1</pages>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 24(1):543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>S DellaPietra</author>
<author>V DellaPietra</author>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>R Mercer</author>
<author>P Rossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--2</pages>
<contexts>
<context position="2643" citStr="Brown et al, 1990" startWordPosition="444" endWordPosition="447">e latter, significantly outperforming direct transfer on cleanly aligned data, indicates the importance of the induction algorithm beyond simple projection, even under ideal circumstances. 2 Background The approach and general algorithms investigated in this paper were initiated in conjunction with the EGYPT project of the 1999 Johns Hopkins summer machine translation workshop (Al-Onaizan et al., 1999). Previously, tools for automatic wordalignment of bilingual corpora were not widely available outside IBM, the research group pioneering statistical machine translation with the Candide system (Brown et al, 1990). The researchers who developed independent word-alignment tools (e.g. Dagan et al, 1993; Fung and Church, 1994; Wu, 1994; Melamed, 1999; Och and Ney, 2000) tended to focus on translation model applications for their word-alignments rather than the induction of stand-alone monolingual analyzers via cross-language projection. For example, Kupiec (1993) began with existing Xerox monolingual bracketers to improve translation alignments, rather than the converse. The primary exception has been in the area of parallel bilingual parsing. Wu (1995, 1997) proposed a framework for inversion transductio</context>
<context position="5003" citStr="Brown et al., 1990" startWordPosition="789" endWordPosition="792"> target-language orderings in this manner offers great potential to productively constrain search in a statistical MT system. Yet the assumption of existing syntactic analyses for each source language further motivates the need to induce such analyses. 3 Data Resources The data used in our experiments are the EnglishFrench Canadian Hansards and English-Chinese Hong Kong Hansards, parallel records of parliamentary proceedings and publications. Both corpora were word-aligned by the now publicly available EGYPT system (Al-Onaizan et al., 1999) and based on IBM&apos;s Model 3 statistical MT formalism (Brown et al., 1990). The data sets used for our projection studies both contained approximately 2 million words in each language. Their alignment was based on strictly word-based model variants for English and character-based model variants for Chinese, with no use of morphological analysis or stemming, POS-tagging, bracketing, outside dictionaries or any other external data source or annotation tool.&apos; Thus the experiments were carefully designed 1-The two exceptions are end-of-sentence detection and tokenization. For the French Hansards, before alignment only 4 simple transformations were performed: au—a le, au</context>
</contexts>
<marker>Brown, Cocke, DellaPietra, DellaPietra, Jelinek, Lafferty, Mercer, Rossin, 1990</marker>
<rawString>P. Brown, J. Cocke, S. DellaPietra, V. DellaPietra, F. Jelinek, J. Lafferty, R. Mercer, and P. Rossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):29-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Comrie</author>
</authors>
<title>All the World&apos;s Major Languages.</title>
<date>1990</date>
<publisher>Oxford University Press.</publisher>
<location>Oxford:</location>
<contexts>
<context position="14940" citStr="Comrie (1990)" startWordPosition="2395" endWordPosition="2396"> used to train the algorithm, a simple mapping table between the tagsets was defined so that output could be compared on a compatible common denominator. An abbreviated version is shown in Table 3:4 Original French Tagset English Core Equiv Consensus Tagset NomC-sing-* NN N NomC-plur-* NNS N AdjQ-* JJ J Adve RB R Prep IN I Num CD # ConcC CC C Pron-* PRP P Dete-* DT D Verb-ParPas-* VBN v Verb-ParPre VBG v Verb-IndImp-* VBD v Verb-SubImp-* VBD v Verb-IndPas-* VBD v Verb-IndPre-* VBP v Verb-SubPre-* VBP v Verb-ConPre-* VB v Verb-InfPre-* VB v Table 3: French-English consensus tagset map 3Indeed, Comrie (1990) indicates that these core POS tag distinctions tend to be almost language universal. Although some individual lexical concepts may be realized by different parts of speech in different languages, the general functional class of &amp;quot;noun&amp;quot; (for example) tends to exist in nearly all languages, and concepts which are considered to be nouns in one language also strongly tend to be realized as nouns in other languages. 4For compatibility with the consensus tagset, the English output tags were condensed somewhat as well, downmapping English distinctions not made in the French tagset such as comparative</context>
</contexts>
<marker>Comrie, 1990</marker>
<rawString>B. Comrie. 1990. All the World&apos;s Major Languages. Oxford: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Language independent minimally supervised induction of lexical probabilities.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL-2000, Hong Kong,</booktitle>
<pages>270--277</pages>
<marker>Cucerzan, Yarowsky, 2000</marker>
<rawString>S. Cucerzan and D. Yarowsky, 2000. Language independent minimally supervised induction of lexical probabilities. In Proceedings of ACL-2000, Hong Kong, pp. 270-277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>K Church</author>
<author>W Gale</author>
</authors>
<title>Robust bilingual word alignment for machine aided translation.</title>
<date>1993</date>
<booktitle>In Procs. of the Workshop on Very Large Corpora,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2731" citStr="Dagan et al, 1993" startWordPosition="456" endWordPosition="459">the importance of the induction algorithm beyond simple projection, even under ideal circumstances. 2 Background The approach and general algorithms investigated in this paper were initiated in conjunction with the EGYPT project of the 1999 Johns Hopkins summer machine translation workshop (Al-Onaizan et al., 1999). Previously, tools for automatic wordalignment of bilingual corpora were not widely available outside IBM, the research group pioneering statistical machine translation with the Candide system (Brown et al, 1990). The researchers who developed independent word-alignment tools (e.g. Dagan et al, 1993; Fung and Church, 1994; Wu, 1994; Melamed, 1999; Och and Ney, 2000) tended to focus on translation model applications for their word-alignments rather than the induction of stand-alone monolingual analyzers via cross-language projection. For example, Kupiec (1993) began with existing Xerox monolingual bracketers to improve translation alignments, rather than the converse. The primary exception has been in the area of parallel bilingual parsing. Wu (1995, 1997) proposed a framework for inversion transduction grammars, where parallel corpora in languages such as English and Chinese are parsed c</context>
</contexts>
<marker>Dagan, Church, Gale, 1993</marker>
<rawString>I. Dagan, K. Church, and W. Gale. 1993. Robust bilingual word alignment for machine aided translation. In Procs. of the Workshop on Very Large Corpora, pp. 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>K Church</author>
</authors>
<title>K-vec: a new approach for aligning parallel texts.</title>
<date>1994</date>
<booktitle>In COLING-94,</booktitle>
<pages>1096--1102</pages>
<contexts>
<context position="2754" citStr="Fung and Church, 1994" startWordPosition="460" endWordPosition="463">he induction algorithm beyond simple projection, even under ideal circumstances. 2 Background The approach and general algorithms investigated in this paper were initiated in conjunction with the EGYPT project of the 1999 Johns Hopkins summer machine translation workshop (Al-Onaizan et al., 1999). Previously, tools for automatic wordalignment of bilingual corpora were not widely available outside IBM, the research group pioneering statistical machine translation with the Candide system (Brown et al, 1990). The researchers who developed independent word-alignment tools (e.g. Dagan et al, 1993; Fung and Church, 1994; Wu, 1994; Melamed, 1999; Och and Ney, 2000) tended to focus on translation model applications for their word-alignments rather than the induction of stand-alone monolingual analyzers via cross-language projection. For example, Kupiec (1993) began with existing Xerox monolingual bracketers to improve translation alignments, rather than the converse. The primary exception has been in the area of parallel bilingual parsing. Wu (1995, 1997) proposed a framework for inversion transduction grammars, where parallel corpora in languages such as English and Chinese are parsed concurrently, with cross</context>
</contexts>
<marker>Fung, Church, 1994</marker>
<rawString>P. Fung and K. Church. 1994. K-vec: a new approach for aligning parallel texts. In COLING-94, pp. 1096-1102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jones</author>
<author>R Havrilla</author>
</authors>
<title>Twisted pair grammar: support for rapid development of machine translation for low density languages</title>
<date>1998</date>
<booktitle>In Procs. of AMTA&apos;98,</booktitle>
<pages>318--332</pages>
<contexts>
<context position="4118" citStr="Jones and Havrilla (1998)" startWordPosition="655" endWordPosition="658">ctural relationships in the second language. Evaluation on noun-phrase bracketing showed 78% precision for Chinese, and 80% precision for English. Thus, while remarkably effective for learning without humanannotated training data, the algorithm does assume the existence of a parallel second-language mirror for all sentences to be parsed. Also, Wu observed significant performance degradation when either the word alignment or translation faithfulness in these pairs are weak. This further motivates the noise-robust training and stand-alone application of our current work. In a related framework, Jones and Havrilla (1998) investigated the use of twisted-pair grammars for syntactic transfer. Given an existing Hindi/Urdu sentence parse, English output was generated by rotating subtrees using the constraints and preferences of the transduction grammar. The ability to generate candidate target-language orderings in this manner offers great potential to productively constrain search in a statistical MT system. Yet the assumption of existing syntactic analyses for each source language further motivates the need to induce such analyses. 3 Data Resources The data used in our experiments are the EnglishFrench Canadian </context>
</contexts>
<marker>Jones, Havrilla, 1998</marker>
<rawString>D. Jones, and R. Havrilla. 1998 Twisted pair grammar: support for rapid development of machine translation for low density languages In Procs. of AMTA&apos;98, pp. 318-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>An algorithm for finding noun phrase correspondences in bilingual corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL93,</booktitle>
<pages>17--22</pages>
<contexts>
<context position="2996" citStr="Kupiec (1993)" startWordPosition="497" endWordPosition="498"> translation workshop (Al-Onaizan et al., 1999). Previously, tools for automatic wordalignment of bilingual corpora were not widely available outside IBM, the research group pioneering statistical machine translation with the Candide system (Brown et al, 1990). The researchers who developed independent word-alignment tools (e.g. Dagan et al, 1993; Fung and Church, 1994; Wu, 1994; Melamed, 1999; Och and Ney, 2000) tended to focus on translation model applications for their word-alignments rather than the induction of stand-alone monolingual analyzers via cross-language projection. For example, Kupiec (1993) began with existing Xerox monolingual bracketers to improve translation alignments, rather than the converse. The primary exception has been in the area of parallel bilingual parsing. Wu (1995, 1997) proposed a framework for inversion transduction grammars, where parallel corpora in languages such as English and Chinese are parsed concurrently, with crosslanguage order differences captured via mobile-like CFG production reordering. Structural relationships in one language help constrain structural relationships in the second language. Evaluation on noun-phrase bracketing showed 78% precision </context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>J. Kupiec. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proceedings of ACL93, pp. 17-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Melamed</author>
</authors>
<title>Bitext maps and alignment via pattern recognition.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--1</pages>
<contexts>
<context position="2779" citStr="Melamed, 1999" startWordPosition="466" endWordPosition="467">ple projection, even under ideal circumstances. 2 Background The approach and general algorithms investigated in this paper were initiated in conjunction with the EGYPT project of the 1999 Johns Hopkins summer machine translation workshop (Al-Onaizan et al., 1999). Previously, tools for automatic wordalignment of bilingual corpora were not widely available outside IBM, the research group pioneering statistical machine translation with the Candide system (Brown et al, 1990). The researchers who developed independent word-alignment tools (e.g. Dagan et al, 1993; Fung and Church, 1994; Wu, 1994; Melamed, 1999; Och and Ney, 2000) tended to focus on translation model applications for their word-alignments rather than the induction of stand-alone monolingual analyzers via cross-language projection. For example, Kupiec (1993) began with existing Xerox monolingual bracketers to improve translation alignments, rather than the converse. The primary exception has been in the area of parallel bilingual parsing. Wu (1995, 1997) proposed a framework for inversion transduction grammars, where parallel corpora in languages such as English and Chinese are parsed concurrently, with crosslanguage order difference</context>
</contexts>
<marker>Melamed, 1999</marker>
<rawString>D. Melamed. 1999. Bitext maps and alignment via pattern recognition. Computational Linguistics, 25(1):107-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ngai</author>
<author>R Florian</author>
</authors>
<title>Transformation-based learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL-2001.</booktitle>
<marker>Ngai, Florian, 2001</marker>
<rawString>G. Ngai and R. Florian. 2001. Transformation-based learning in the fast lane. In Proceedings of NAACL-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL-2000,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="2799" citStr="Och and Ney, 2000" startWordPosition="468" endWordPosition="471"> even under ideal circumstances. 2 Background The approach and general algorithms investigated in this paper were initiated in conjunction with the EGYPT project of the 1999 Johns Hopkins summer machine translation workshop (Al-Onaizan et al., 1999). Previously, tools for automatic wordalignment of bilingual corpora were not widely available outside IBM, the research group pioneering statistical machine translation with the Candide system (Brown et al, 1990). The researchers who developed independent word-alignment tools (e.g. Dagan et al, 1993; Fung and Church, 1994; Wu, 1994; Melamed, 1999; Och and Ney, 2000) tended to focus on translation model applications for their word-alignments rather than the induction of stand-alone monolingual analyzers via cross-language projection. For example, Kupiec (1993) began with existing Xerox monolingual bracketers to improve translation alignments, rather than the converse. The primary exception has been in the area of parallel bilingual parsing. Wu (1995, 1997) proposed a framework for inversion transduction grammars, where parallel corpora in languages such as English and Chinese are parsed concurrently, with crosslanguage order differences captured via mobil</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. Och and H. Ney. 2000. Improved statistical alignment models. In Proceedings of ACL-2000, pp. 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1999</date>
<booktitle>In Natural Language Processing Using Very Large Corpora. Kluwer.</booktitle>
<pages>157--176</pages>
<marker>Ramshaw, Marcus, 1999</marker>
<rawString>L. Ramshaw and M. Marcus, 1999. Text chunking using transformation-based learning. In Natural Language Processing Using Very Large Corpora. Kluwer. pp. 157-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Aligning a parallel English-Chinese corpus statistically with lexical criteria.</title>
<date>1994</date>
<booktitle>In Proc. ACL-94,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="2764" citStr="Wu, 1994" startWordPosition="464" endWordPosition="465">beyond simple projection, even under ideal circumstances. 2 Background The approach and general algorithms investigated in this paper were initiated in conjunction with the EGYPT project of the 1999 Johns Hopkins summer machine translation workshop (Al-Onaizan et al., 1999). Previously, tools for automatic wordalignment of bilingual corpora were not widely available outside IBM, the research group pioneering statistical machine translation with the Candide system (Brown et al, 1990). The researchers who developed independent word-alignment tools (e.g. Dagan et al, 1993; Fung and Church, 1994; Wu, 1994; Melamed, 1999; Och and Ney, 2000) tended to focus on translation model applications for their word-alignments rather than the induction of stand-alone monolingual analyzers via cross-language projection. For example, Kupiec (1993) began with existing Xerox monolingual bracketers to improve translation alignments, rather than the converse. The primary exception has been in the area of parallel bilingual parsing. Wu (1995, 1997) proposed a framework for inversion transduction grammars, where parallel corpora in languages such as English and Chinese are parsed concurrently, with crosslanguage o</context>
</contexts>
<marker>Wu, 1994</marker>
<rawString>D. Wu. 1994. Aligning a parallel English-Chinese corpus statistically with lexical criteria. In Proc. ACL-94, pp. 80-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>An algorithm for simultaneously bracketing parallel texts.</title>
<date>1995</date>
<booktitle>In Proc. of ACL-95,</booktitle>
<pages>244--251</pages>
<contexts>
<context position="3189" citStr="Wu (1995" startWordPosition="527" endWordPosition="528">achine translation with the Candide system (Brown et al, 1990). The researchers who developed independent word-alignment tools (e.g. Dagan et al, 1993; Fung and Church, 1994; Wu, 1994; Melamed, 1999; Och and Ney, 2000) tended to focus on translation model applications for their word-alignments rather than the induction of stand-alone monolingual analyzers via cross-language projection. For example, Kupiec (1993) began with existing Xerox monolingual bracketers to improve translation alignments, rather than the converse. The primary exception has been in the area of parallel bilingual parsing. Wu (1995, 1997) proposed a framework for inversion transduction grammars, where parallel corpora in languages such as English and Chinese are parsed concurrently, with crosslanguage order differences captured via mobile-like CFG production reordering. Structural relationships in one language help constrain structural relationships in the second language. Evaluation on noun-phrase bracketing showed 78% precision for Chinese, and 80% precision for English. Thus, while remarkably effective for learning without humanannotated training data, the algorithm does assume the existence of a parallel second-lang</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>D. Wu. 1995. An algorithm for simultaneously bracketing parallel texts. In Proc. of ACL-95, pp. 244-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Statistical inversion transduction grammars an bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Statistical inversion transduction grammars an bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>R Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL-2000,</booktitle>
<pages>207--216</pages>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>D. Yarowsky and R. Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proceedings of ACL-2000, pp. 207-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>G Ngai</author>
<author>R Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of HLT-2001,</booktitle>
<pages>109--116</pages>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>D. Yarowsky, G. Ngai and R. Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of HLT-2001, pp. 109-116.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>