<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009429">
<title confidence="0.993516">
Summarizing multiple spoken documents: finding evidence from
untranscribed audio
</title>
<author confidence="0.999331">
Xiaodan Zhu, Gerald Penn and Frank Rudzicz
</author>
<affiliation confidence="0.999518">
University of Toronto
</affiliation>
<address confidence="0.995368">
10 King’s College Rd.,
Toronto, M5S 3G4, ON, Canada
</address>
<email confidence="0.99985">
{xzhu,gpenn,frank}@cs.toronto.edu
</email>
<sectionHeader confidence="0.997403" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999950692307692">
This paper presents a model for summa-
rizing multiple untranscribed spoken doc-
uments. Without assuming the availabil-
ity of transcripts, the model modifies a
recently proposed unsupervised algorithm
to detect re-occurring acoustic patterns in
speech and uses them to estimate similari-
ties between utterances, which are in turn
used to identify salient utterances and re-
move redundancies. This model is of in-
terest due to its independence from spo-
ken language transcription, an error-prone
and resource-intensive process, its abil-
ity to integrate multiple sources of infor-
mation on the same topic, and its novel
use of acoustic patterns that extends pre-
vious work on low-level prosodic feature
detection. We compare the performance of
this model with that achieved using man-
ual and automatic transcripts, and find that
this new approach is roughly equivalent
to having access to ASR transcripts with
word error rates in the 33–37% range with-
out actually having to do the ASR, plus
it better handles utterances with out-of-
vocabulary words.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953923076923">
Summarizing spoken documents has been exten-
sively studied over the past several years (Penn
and Zhu, 2008; Maskey and Hirschberg, 2005;
Murray et al., 2005; Christensen et al., 2004;
Zechner, 2001). Conventionally called speech
summarization, although speech connotes more
than spoken documents themselves, it is motivated
by the demand for better ways to navigate spoken
content and the natural difficulty in doing so —
speech is inherently more linear or sequential than
text in its traditional delivery.
Previous research on speech summarization has
addressed several important problems in this field
(see Section 2.1). All of this work, however,
has focused on single-document summarization
and the integration of fairly simplistic acoustic
features, inspired by work in descriptive linguis-
tics. The issues of navigating speech content are
magnified when dealing with larger collections —
multiple spoken documents on the same topic. For
example, when one is browsing news broadcasts
covering the same events or call-centre record-
ings related to the same type of customer ques-
tions, content redundancy is a prominent issue.
Multi-document summarization on written docu-
ments has been studied for more than a decade
(see Section 2.2). Unfortunately, no such effort
has been made on audio documents yet.
An obvious way to summarize multiple spo-
ken documents is to adopt the transcribe-and-
summarize approach, in which automatic speech
recognition (ASR) is first employed to acquire
written transcripts. Speech summarization is ac-
cordingly reduced to a text summarization task
conducted on error-prone transcripts.
Such an approach, however, encounters several
problems. First, assuming the availability of ASR
is not always valid for many languages other than
English that one may want to summarize. Even
when it is, transcription quality is often an issue—
training ASR models requires collecting and an-
notating corpora on specific languages, dialects,
or even different domains. Although recognition
errors do not significantly impair extractive sum-
marizers (Christensen et al., 2004; Zhu and Penn,
2006), error-laden transcripts are not necessarily
browseable if recognition errors are higher than
certain thresholds (Munteanu et al., 2006). In
such situations, audio summaries are an alterna-
tive when salient content can be identified directly
from untranscribed audio. Third, the underlying
paradigm of most ASR models aims to solve a
</bodyText>
<page confidence="0.979596">
549
</page>
<note confidence="0.9996115">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 549–557,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999922524590164">
classification problem, in which speech is seg-
mented and classified into pre-existing categories
(words). Words not in the predefined dictionary
are certain to be misrecognized without excep-
tion. This out-of-vocabulary (OOV) problem is
unavoidable in the regular ASR framework, al-
though it is more likely to happen on salient words
such as named entities or domain-specific terms.
Our approach uses acoustic evidence from the
untranscribed audio stream. Consider text sum-
marization first: many well-known models such
as MMR (Carbonell and Goldstein, 1998) and
MEAD (Radev et al., 2004) rely on the reoccur-
rence statistics of words. That is, if we switch
any word w1 with another word w2 across an
entire corpus, the ranking of extracts (often sen-
tences) will be unaffected, because no word-
specific knowledge is involved. These mod-
els have achieved state-of-the-art performance in
transcript-based speech summarization (Zechner,
2001; Penn and Zhu, 2008). For spoken docu-
ments, such reoccurrence statistics are available
directly from the speech signal. In recent years, a
variant of dynamic time warping (DTW) has been
proposed to find reoccurring patterns in the speech
signal (Park and Glass, 2008). This method has
been successfully applied to tasks such as word
detection (Park and Glass, 2006) and topic bound-
ary detection (Malioutov et al., 2007).
Motivated by the work above, this paper ex-
plores the approach to summarizing multiple spo-
ken documents directly over an untranscribed au-
dio stream. Such a model is of interest because of
its independence from ASR. It is directly applica-
ble to audio recordings in languages or domains
when ASR is not possible or transcription quality
is low. In principle, this approach is free from the
OOV problem inherent to ASR. The premise of
this approach, however, is to reliably find reoccur-
ing acoustic patterns in audio, which is challeng-
ing because of noise and pronunciation variance
existing in the speech signal, as well as the dif-
ficulty of finding alignments with proper lengths
corresponding to words well. Therefore, our pri-
mary goal in this paper is to empirically determine
the extent to which acoustic information alone can
effectively replace conventional speech recogni-
tion with or without simple prosodic feature de-
tection within the multi-document speech summa-
rization task. As shown below, a modification of
the Park-Glass approach amounts to the efficacy
of a 33-37% WER ASR engine in the domain
of multiple spoken document summarization, and
also has better treatment of OOV items. Park-
Glass similarity scores by themselves can attribute
a high score to distorted paths that, in our context,
ultimately leads to too many false-alarm align-
ments, even after applying the distortion thresh-
old. We introduce additional distortion penalty
and subpath length constraints on their scoring to
discourage this possibility.
</bodyText>
<sectionHeader confidence="0.999955" genericHeader="introduction">
2 Related work
</sectionHeader>
<subsectionHeader confidence="0.999622">
2.1 Speech summarization
</subsectionHeader>
<bodyText confidence="0.999952605263158">
Although abstractive summarization is more de-
sirable, the state-of-the-art research on speech
summarization has been less ambitious, focus-
ing primarily on extractive summarization, which
presents the most important N% of words,
phrases, utterances, or speaker turns of a spo-
ken document. The presentation can be in tran-
scripts (Zechner, 2001), edited speech data (Fu-
rui et al., 2003), or a combination of these (He
et al., 2000). Audio data amenable to summa-
rization include meeting recordings (Murray et al.,
2005), telephone conversations (Zhu and Penn,
2006; Zechner, 2001), news broadcasts (Maskey
and Hirschberg, 2005; Christensen et al., 2004),
presentations (He et al., 2000; Zhang et al., 2007;
Penn and Zhu, 2008), etc.
Although extractive summarization is not as
ideal as abstractive summarization, it outperforms
several comparable alternatives. Tucker and Whit-
taker (2008) have shown that extractive summa-
rization is generally preferable to time compres-
sion, which speeds up the playback of audio doc-
uments with either fixed or variable rates. He et
al. (2000) have shown that either playing back im-
portant audio-video segments or just highlighting
the corresponding transcripts is significantly bet-
ter than providing users with full transcripts, elec-
tronic slides, or both for browsing presentation
recordings.
Given the limitations associated with ASR, it is
no surprise that previous work (He et al., 1999;
Maskey and Hirschberg, 2005; Murray et al.,
2005; Zhu and Penn, 2006) has studied features
available in audio. The focus, however, is pri-
marily limited to prosody. The assumption is that
prosodic effects such as stress can indicate salient
information. Since a direct modeling of compli-
cated compound prosodic effects like stress is dif-
</bodyText>
<page confidence="0.991788">
550
</page>
<bodyText confidence="0.9992756875">
ficult, they have used basic features of prosody in-
stead, such as pitch, energy, duration, and pauses.
The usefulness of prosody was found to be very
limited by itself, if the effect of utterance length is
not considered (Penn and Zhu, 2008). In multiple-
spoken-document summarization, it is unlikely
that prosody will be more useful in predicating
salience than in single document summarization.
Furthermore, prosody is also unlikely to be appli-
cable to detecting or handling redundancy, which
is prominent in the multiple-document setting.
All of the work above has been conducted on
single-document summarization. In this paper
we are interested in summarizing multiple spo-
ken documents by using reoccurrence statistics of
acoustic patterns.
</bodyText>
<subsectionHeader confidence="0.996633">
2.2 Multiple-document summarization
</subsectionHeader>
<bodyText confidence="0.99990278125">
Multi-document summarization on written text
has been studied for over a decade. Compared
with the single-document task, it needs to remove
more content, cope with prominent redundancy,
and organize content from different sources prop-
erly. This field has been pioneered by early work
such as the SUMMONS architecture (Mckeown
and Radev, 1995; Radev and McKeown, 1998).
Several well-known models have been proposed,
i.e., MMR (Carbonell and Goldstein, 1998), multi-
Gen (Barzilay et al., 1999), and MEAD (Radev
et al., 2004). Multi-document summarization has
received intensive study at DUC. 1 Unfortunately,
no such efforts have been extended to summarize
multiple spoken documents yet.
Abstractive approaches have been studied since
the beginning. A famous effort in this direction
is the information fusion approach proposed in
Barzilay et al. (1999). However, for error-prone
transcripts of spoken documents, an abstractive
method still seems to be too ambitious for the time
being. As in single-spoken-document summariza-
tion, this paper focuses on the extractive approach.
Among the extractive models, MMR (Carbonell
and Goldstein, 1998) and MEAD (Radev et al.,
2004), are possibly the most widely known. Both
of them are linear models that balance salience and
redundancy. Although in principle, these mod-
els allow for any estimates of salience and re-
dundancy, they themselves calculate these scores
with word reoccurrence statistics, e.g., tf.idf,
and yield state-of-the-art performance. MMR it-
</bodyText>
<footnote confidence="0.905868">
1http://duc.nist.gov/
</footnote>
<bodyText confidence="0.999862666666667">
eratively selects sentences that are similar to the
entire documents, but dissimilar to the previously
selected sentences to avoid redundancy. Its de-
tails will be revisited below. MEAD uses a redun-
dancy removal mechanism similar to MMR, but
to decide the salience of a sentence to the whole
topic, MEAD uses not only its similarity score
but also sentence position, e.g., the first sentence
of each new story is considered important. Our
work adopts the general framework of MMR and
MEAD to study the effectiveness of the acoustic
pattern evidence found in untranscribed audio.
</bodyText>
<sectionHeader confidence="0.762591" genericHeader="method">
3 An acoustics-based approach
</sectionHeader>
<bodyText confidence="0.999970533333333">
The acoustics-based summarization technique
proposed in this paper consists of three consecu-
tive components. First, we detect acoustic patterns
that recur between pairs of utterances in a set of
documents that discuss a common topic. The as-
sumption here is that lemmata, words, or phrases
that are shared between utterances are more likely
to be acoustically similar. The next step is to com-
pute a relatedness score between each pair of ut-
terances, given the matching patterns found in the
first step. This yields a symmetric relatedness ma-
trix for the entire document set. Finally, the relat-
edness matrix is incorporated into a general sum-
marization model, where it is used for utterance
selection.
</bodyText>
<subsectionHeader confidence="0.999361">
3.1 Finding common acoustic patterns
</subsectionHeader>
<bodyText confidence="0.999976333333333">
Our goal is to identify subsequences within acous-
tic sequences that appear highly similar to regions
within other sequences, where each sequence con-
sists of a progression of overlapping 20ms vec-
tors (frames). In order to find those shared pat-
terns, we apply a modification of the segmen-
tal dynamic time warping (SDTW) algorithm to
pairs of audio sequences. This method is similar
to standard DTW, except that it computes multi-
ple constrained alignments, each within predeter-
mined bands of the similarity matrix (Park and
Glass, 2008).2 SDTW has been successfully ap-
plied to problems such as topic boundary detec-
tion (Malioutov et al., 2007) and word detection
(Park and Glass, 2006). An example application
of SDTW is shown in Figure 1, which shows the
results of two utterances from the TDT-4 English
dataset:
</bodyText>
<footnote confidence="0.894924333333333">
2Park and Glass (2008) used Euclidean distance. We used
cosine distance instead, which was found to be better on our
held-out dataset.
</footnote>
<page confidence="0.995685">
551
</page>
<bodyText confidence="0.994976923076923">
I: the explosion in aden harbor killed seven-
teen u.s. sailors and injured other thirty
nine last month.
II: seventeen sailors were killed.
These two utterances share three words: killed,
seventeen, and sailors, though in different orders.
The upper panel of Figure 1 shows a matrix of
frame-level similarity scores between these two
utterances where lighter grey represents higher
similarity. The lower panel shows the four most
similar shared subpaths, three of which corre-
spond to the common words, as determined by the
approach detailed below.
</bodyText>
<figureCaption confidence="0.846138666666667">
Figure 1: Using segmental dynamic time warping
to find matching acoustic patterns between two ut-
terances.
</figureCaption>
<subsectionHeader confidence="0.67559">
Calculating MFCC
</subsectionHeader>
<bodyText confidence="0.999801227272727">
The first step of SDTW is to represent each utter-
ance as sequences of Mel-frequency cepstral coef-
ficient (MFCC) vectors, a commonly used repre-
sentation of the spectral characteristics of speech
acoustics. First, conventional short-time Fourier
transforms are applied to overlapping 20ms Ham-
ming windows of the speech amplitude signal.
The resulting spectral energy is then weighted
by filters on the Mel-scale and converted to 39-
dimensional feature vectors, each consisting of 12
MFCCs, one normalized log-energy term, as well
as the first and second derivatives of these 13 com-
ponents over time. The MFCC features used in
the acoustics-based approach are the same as those
used below in the ASR systems.
As in (Park and Glass, 2008), an additional
whitening step is taken to normalize the variances
on each of these 39 dimensions. The similarities
between frames are then estimated using cosine
distance. All similarity scores are then normalized
to the range of [0, 1], which yields similarity ma-
trices exemplified in the upper panel of Figure 1.
</bodyText>
<subsectionHeader confidence="0.804917">
Finding optimal paths
</subsectionHeader>
<bodyText confidence="0.999542783783784">
For each similarity matrix obtained above, local
alignments of matching patterns need to be found,
as shown in the lower panel of Figure 1. A sin-
gle global DTW alignment is not adequate, since
words or phrases held in common between utter-
ances may occur in any order. For example, in Fig-
ure 1 killed occurs before all other shared words in
one document and after all of these in the other, so
a single alignment path that monotonically seeks
the lower right-hand corner of the similarity ma-
trix could not possibly match all common words.
Instead, multiple DTWs are applied, each starting
from different points on the left or top edges of the
similarity matrix, and ending at different points on
the bottom or right edges, respectively. The width
of this diagonal band is proportional to the esti-
mated number of words per sequence.
Given an M-by-N matrix of frame-level simi-
larity scores, the top-left corner is considered the
origin, and the bottom-right corner represents an
alignment of the last frames in each sequence. For
each of the multiple starting points p0 = (x0, y0)
where either x0 = 0 or y0 = 0, but not neces-
sarily both, we apply DTW to find paths P =
p0, p1, ...,pK that maximize E0≤ i≤ K sim(pi),
where sim(pi) is the cosine similarity score of
point pi = (xi, yi) in the matrix. Each point on the
path, pi, is subject to the constraint |xi − yi |&lt; T,
where T limits the distortion of the path, as we
determine experimentally. The ending points are
pK = (xK, yK) with either xK = N or yK =
M. For considerations of efficiency, the multi-
ple DTW processes do not start from every point
on the left or top edges. Instead, they skip every
T such starting points, which still guarantees that
there will be no blind-spot in the matrices that are
inaccessible to all DTW search paths.
</bodyText>
<subsectionHeader confidence="0.760069">
Finding optimal subpaths
</subsectionHeader>
<bodyText confidence="0.999024857142857">
After the multiple DTW paths are calculated, the
optimal subpath on each is then detected in or-
der to find the local alignments where the simi-
larity is maximal, which is where we expect ac-
tual matched phrases to occur. For a given path
P = p0, p2, ..., pK, the optimal subpath is defined
to be a continuous subpath, P∗ = pm, pm+1..., pn
</bodyText>
<page confidence="0.936975">
552
</page>
<equation confidence="0.702844">
that maximizes �&amp;quot;```-z`-n aim(pz) 0 &lt; n &lt; m &lt; k
n−m+1 ,— — —
</equation>
<bodyText confidence="0.999966486486486">
and m − n + 1 &gt; L. That is, the subpath is at
least as long as L and has the maximal average
similarity. L is used to avoid short alignments that
correspond to subword segments or short function
words. The value of L is determined on a devel-
opment set.
The version of SDTW employed by (Malioutov
et al., 2007) and Park and Glass (2008) employed
an algorithm of complexity O(Klog(L)) from
(Lin et al., 2002) to find subpaths. Lin et al. (2002)
have also proven that the length of the optimal sub-
path is between L and 2L − 1, inclusively. There-
fore, our version uses a very simple algorithm—
just search and find the maximum of average simi-
larities among all possible subpaths with lengths
between L and 2L − 1. Although the theoreti-
cal upper bound for this algorithm is O(KL), in
practice we have found no significant increase in
computation time compared with the O(Klog(L))
algorithm—L is actually a constant for both Park
and Glass (2008) and us, it is much smaller than
K, and the O(Klog(L)) algorithm has (constant)
overhead of calculating right-skew partitions.
In our implementation, since most of the time is
spent on calculating the average similarity scores
on candidate subpaths, all average scores are
therefore pre-calculated incrementally and saved.
We have also parallelized the computation of sim-
ilarities by topics over several computer clusters.
A detailed comparison of different parallelization
techniques has been conducted by Gajjar et al.
(2008). In addition, comparing time efficiency
between the acoustics-based approach and ASR-
based summarizers is interesting but not straight-
forward since a great deal of comparable program-
ming optimization needs to be additionally consid-
ered in the present approach.
</bodyText>
<subsectionHeader confidence="0.999548">
3.2 Estimating utterance-level similarity
</subsectionHeader>
<bodyText confidence="0.999975625">
In the previous stage, we calculated frame-level
similarities between utterance pairs and used these
to find potential matching patterns between the
utterances. With this information, we estimate
utterance-level similarities by estimating the num-
bers of true subpath alignments between two utter-
ances, which are in turn determined by combining
the following features associated with subpaths:
</bodyText>
<subsectionHeader confidence="0.567932">
Similarity of subpath
</subsectionHeader>
<bodyText confidence="0.99992175">
We compute similarity features on each subpath.
We have obtained the average similarity score of
each subpath as discussed in Section 3.1. Based
on this, we calculate relative similarity scores,
which are computed by dividing the original sim-
ilarity of a given subpath by the average similar-
ity of its surrounding background. The motivation
for capturing the relative similarity is to punish
subpaths that cannot distinguish themselves from
their background, e.g., those found in a block of
high-similarity regions caused by certain acoustic
noise.
</bodyText>
<subsectionHeader confidence="0.498436">
Distortion score
</subsectionHeader>
<bodyText confidence="0.999948">
Warped subpaths are less likely to correspond to
valid matching patterns than straighter ones. In
addition to removing very distorted subpaths by
applying a distortion threshold as in (Park and
Glass, 2008), we also quantitatively measured the
remaining ones. We fit each of them with least-
square linear regression and estimate the residue
scores. As discussed above, each point on a sub-
path satisfies |xi − yi |&lt; T, so the residue cannot
be bigger than T. We used this to normalize the
distortion scores to the range of [0,1].
</bodyText>
<subsectionHeader confidence="0.739653">
Subpath length
</subsectionHeader>
<bodyText confidence="0.99992792">
Given two subpaths with nearly identical average
similarity scores, we suggest that the longer of the
two is more likely to refer to content of interest
that is shared between two speech utterances, e.g.,
named entities. Longer subpaths may in this sense
therefore be more useful in identifying similarities
and redundancies within a speech summarization
system. As discussed above, since the length of a
subpath len(P′) has been proven to fall between
L and 2L − 1, i.e., L &lt; len(P′) &lt; 2L − 1,
given a parameter L, we normalize the path length
to (len(P′) − L)/L, corresponding to the range
[0,1).
The similarity scores of subpaths can vary widely
over different spoken documents. We do not use
the raw similarity score of a subpath, but rather
its rank. For example, given an utterance pair, the
top-1 subpath is more likely to be a true alignment
than the rest, even if its distortion score may be
higher. The similarity ranks are combined with
distortion scores and subpath lengths simply as
follows. We divide subpaths into the top 1, 3, 5,
and 10 by their raw similarity scores. For sub-
paths in each group, we check whether their dis-
tortion scores are below and lengths are above
</bodyText>
<page confidence="0.997259">
553
</page>
<bodyText confidence="0.999917714285714">
some thresholds. If they are, in any group, then
the corresponding subpaths are selected as “true”
alignments for the purposes of building utterance-
level similarity matrix. The numbers of true align-
ments are used to measure the similarity between
two utterances. We therefore have 8 threshold pa-
rameters to estimate, and subpaths with similarity
scores outside the top 10 are ignored. The rank
groups are checked one after another in a decision
list. Powell’s algorithm (Press et al., 2007) is used
to find the optimal parameters that directly mini-
mize summarization errors made by the acoustics-
based model relative to utterances selected from
manual transcripts.
</bodyText>
<subsectionHeader confidence="0.993277">
3.3 Extractive summarization
</subsectionHeader>
<bodyText confidence="0.999925166666667">
Once the similarity matrix between sentences in a
topic is acquired, we can conduct extractive sum-
marization by using the matrix to estimate both
similarity and redundancy. As discussed above,
we take the general framework of MMR and
MEAD, i.e., a linear model combining salience
and redundancy. In practice, we used MMR in our
experiments, since the original MEAD considers
also sentence positions 3 , which can always been
added later as in (Penn and Zhu, 2008).
To facilitate our discussion below, we briefly re-
visit MMR here. MMR (Carbonell and Goldstein,
1998) iteratively augments the summary with ut-
terances that are most similar to the document
set under consideration, but most dissimilar to the
previously selected utterances in that summary, as
shown in the equation below. Here, the sim1 term
represents the similarity between a sentence and
the document set it belongs to. The assumption is
that a sentence having a higher sim1 would better
represent the content of the documents. The sim2
term represents the similarity between a candidate
sentence and sentences already in the summary. It
is used to control redundancy. For the transcript-
based systems, the sim1 and sim2 scores in this
paper are measured by the number of words shared
between a sentence and a sentence/document set
mentioned above, weighted by the idf scores of
these words, which is similar to the calculation of
sentence centroid values by Radev et al. (2004).
</bodyText>
<figureCaption confidence="0.959163428571429">
3The usefulness of position varies significantly in differ-
ent genres (Penn and Zhu, 2008). Even in the news domain,
the style of broadcast news differs from written news, for
example, the first sentence often serves to attract audiences
(Christensen et al., 2004) and is hence less important as in
written news. Without consideration of position, MEAD is
more similar to MMR.
</figureCaption>
<bodyText confidence="0.998707">
Note that the acoustics-based approach estimates
this by using the method discussed above in Sec-
tion 3.2.
</bodyText>
<equation confidence="0.997561333333333">
Nextsent = argmax (A sim1(doc, tnr,j)
tnr,j
−(1 − A)maxtr,ksim2(tnr,j, tr,k))
</equation>
<sectionHeader confidence="0.998229" genericHeader="method">
4 Experimental setup
</sectionHeader>
<bodyText confidence="0.999333744186047">
We use the TDT-4 dataset for our evaluation,
which consists of annotated news broadcasts
grouped into common topics. Since our aim in this
paper is to study the achievable performance of the
audio-based model, we grouped together news sto-
ries by their news anchors for each topic. Then we
selected the largest 20 groups for our experiments.
Each of these contained between 5 and 20 articles.
We compare our acoustics-only approach
against transcripts produced automatically from
two ASR systems. The first set of transcripts
was obtained directly from the TDT-4 database.
These transcripts contain a word error rate of
12.6%, which is comparable to the best accura-
cies obtained in the literature on this data set.
We also run a custom ASR system designed to
produce transcripts at various degrees of accu-
racy in order to simulate the type of performance
one might expect given languages with sparser
training corpora. These custom acoustic mod-
els consist of context-dependent tri-phone units
trained on HUB-4 broadcast news data by se-
quential Viterbi forced alignment. During each
round of forced alignment, the maximum likeli-
hood linear regression (MLLR) transform is used
on gender-dependent models to improve the align-
ment quality. Language models are also trained on
HUB-4 data.
Our aim in this paper is to study the achievable
performance of the audio-based model. Instead
of evaluating the result against human generated
summaries, we directly compare the performance
against the summaries obtained by using manual
transcripts, which we take as an upper bound to
the audio-based system’s performance. This ob-
viously does not preclude using the audio-based
system together with other features such as utter-
ance position, length, speaker’s roles, and most
others used in the literature (Penn and Zhu, 2008).
Here, we do not want our results to be affected by
them with the hope of observing the difference ac-
curately. As such, we quantify success based on
ROUGE (Lin, 2004) scores. Our goal is to evalu-
</bodyText>
<page confidence="0.996513">
554
</page>
<bodyText confidence="0.999769333333333">
ate whether the relatedness of spoken documents
can reasonably be gleaned solely from the surface
acoustic information.
</bodyText>
<sectionHeader confidence="0.987877" genericHeader="evaluation">
5 Experimental results
</sectionHeader>
<bodyText confidence="0.997498708333333">
We aim to empirically determine the extent to
which acoustic information alone can effectively
replace conventional speech recognition within the
multi-document speech summarization task. Since
ASR performance can vary greatly as we dis-
cussed above, we compare our system against
automatic transcripts having word error rates of
12.6%, 20.9%, 29.2%, and 35.5% on the same
speech source. We changed our language mod-
els by restricting the training data so as to obtain
the worst WER and then interpolated the corre-
sponding transcripts with the TDT-4 original au-
tomatic transcripts to obtain the rest. Figure 2
shows ROUGE scores for our acoustics-only sys-
tem, as depicted by horizontal lines, as well as
those for the extractive summaries given automatic
transcripts having different WERs, as depicted
by points. Dotted lines represent the 95% con-
fidence intervals of the transcript-based models.
Figure 2 reveals that, typically, as the WERs of au-
tomatic transcripts increase to around 33%-37%,
the difference between the transcript-based and the
acoustics-based models is no longer significant.
These observations are consistent across sum-
maries with different fixed lengths, namely 10%,
20%, and 30% of the lengths of the source docu-
ments for the top, middle, and bottom rows of Fig-
ure 2, respectively. The consistency of this trend is
shown across both ROUGE-2 and ROUGE-SU4,
which are the official measures used in the DUC
evaluation. We also varied the MMR parameter A
within a typical range of 0.4–1, which yielded the
same observation.
Since the acoustics-based approach can be ap-
plied to any data domain and to any language
in principle, this would be of special interest
when those situations yield relatively high WER
with conventional ASR. Figure 2 also shows the
ROUGE scores achievable by selecting utterances
uniformly at random for extractive summarization,
which are significantly lower than all other pre-
sented methods and corroborate the usefulness of
acoustic information.
Although our acoustics-based method performs
similarly to automatic transcripts with 33-37%
WER, the errors observed are not the same, which
Figure 2: ROUGE scores and 95% confidence in-
tervals for the MMR-based extractive summaries
produced from our acoustics-only approach (hori-
zontal lines), and from ASR-generated transcripts
having varying WER (points). The top, middle,
and bottom rows of subfigures correspond to sum-
maries whose lengths are fixed at 10%, 20%, and
30% the sizes of the source text, respectively. A in
MMR takes 1, 0.7, and 0.4 in these rows, respec-
tively.
we attribute to fundamental differences between
these two methods. Table 1 presents the number
of different utterances correctly selected by the
acoustics-based and ASR-based methods across
three categories, namely those sentences that are
correctly selected by both methods, those ap-
pearing only in the acoustics-based summaries,
and those appearing only in the ASR-based sum-
maries. These are shown for summaries having
different proportional lengths relative to the source
documents and at different WERs. Again, correct-
ness here means that the utterance is also selected
when using a manual transcript, since that is our
defined topline.
A manual analysis of the corpus shows that
utterances correctly included in summaries by
</bodyText>
<figure confidence="0.99877880952381">
Len=10%, Rand=0.176
0.75
0.7
0 0.1 0.2 0.3 0.4 0.5
Word error rate
1
0.95 Len=10% Rand=0.197
0.9
0.85
0.8
0.75
0.7
0 0.1 0.2 0.3 0.4 0.5
Word error rate
ROUGE−SU4
1
0.95
ROUGE−2
0.9
0.85
0.8
0.75
0.7
0 0.1 0.2 0.3 0.4 0.5
Word error rate
1
Len=30%, Rand=0.389
0.95
0.9
0.85
0.8
0.75
0.70 0.1 0.2 0.3 0.4 0.5
Word error rate
0.7
0 0.1 0.2 0.3 0.4 0.5
Word error rate
1
0.95
0.9
0.85
0.8
0.75
0.70 0.1 0.2 0.3 0.4 0.5
Word error rate
ROUGE−2
ROUGE−SU4
Len=30%, Rand=0.402
ROUGE−2
0.95
0.85
0.9
0.8
1
0.95
Len=20%, Rand=0.340
0.9
0.85
0.8
ROUGE−SU4
Len=20%, Rand=0.324
1
0.75
</figure>
<page confidence="0.990256">
555
</page>
<table confidence="0.999457285714286">
Summ. Both ASR Aco.-
length only only
10% 85 37 8
WER=12.6% 20% 185 62 12
30% 297 87 20
10% 83 36 10
WER=20.9% 20% 178 65 19
30% 293 79 24
10% 77 34 16
WER=29.2% 20% 172 58 25
30% 286 64 31
10% 75 33 18
WER=35.5% 20% 164 54 33
30% 272 67 45
</table>
<tableCaption confidence="0.99884">
Table 1: Utterances correctly selected by both
</tableCaption>
<bodyText confidence="0.998612903225806">
the ASR-based models and acoustics-based ap-
proach, or by either of them, under different
WERs (12.6%, 20.9%, 29.2%, and 35.5%) and
summary lengths (10%, 20%, and 30% utterances
of the original documents)
the acoustics-based method often contain out-of-
vocabulary errors in the corresponding ASR tran-
scripts. For example, given the news topic of the
bombing of the U.S. destroyer ship Cole in Yemen,
the ASR-based method always mistook the word
Cole, which was not in the vocabulary, for cold,
khol, and called. Although named entities and
domain-specific terms are often highly relevant
to the documents in which they are referenced,
these types of words are often not included in
ASR vocabularies, due to their relative global rar-
ity. Importantly, an unsupervised acoustics-based
approach such as ours does not suffer from this
fundamental discord. At the very least, these find-
ings suggest that ASR-based summarization sys-
tems augmented with our type of approach might
be more robust against out-of-vocabulary errors.
It is, however, very encouraging that an acoustics-
based approach can perform to within a typical
WER range within non-broadcast-news domains,
although those domains can likewise be more
challenging for the acoustics-based approach. Fur-
ther experimentation is necessary. It is also of sci-
entific interest to be able to quantify this WER as
an acoustics-only baseline for further research on
ASR-based spoken document summarizers.
</bodyText>
<sectionHeader confidence="0.995406" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999989027777778">
In text summarization, statistics based on word
counts have traditionally served as the foundation
of state-of-the-art models. In this paper, the simi-
larity of utterances is estimated directly from re-
curring acoustic patterns in untranscribed audio
sequences. These relatedness scores are then in-
tegrated into a maximum marginal relevance lin-
ear model to estimate the salience and redundancy
of those utterance for extractive summarization.
Our empirical results show that the summarization
performance given acoustic information alone is
statistically indistinguishable from that of modern
ASR on broadcast news in cases where the WER
of the latter approaches 33%-37%. This is an en-
couraging result in cases where summarization is
required, but ASR is not available or speech recog-
nition performance is degraded. Additional anal-
ysis suggests that the acoustics-based approach
is useful in overcoming situations where out-of-
vocabulary error may be more prevalent, and we
suggest that a hybrid approach of traditional ASR
with acoustics-based pattern matching may be the
most desirable future direction of research.
One limitation of the current analysis is that
summaries are extracted only for collections of
spoken documents from among similar speakers.
Namely, none of the topics under analysis consists
of a mix of male and female speakers. We are cur-
rently investigating supervised methods to learn
joint probabilistic models relating the acoustics of
groups of speakers in order to normalize acoustic
similarity matrices (Toda et al., 2001). We sug-
gest that if a stochastic transfer function between
male and female voices can be estimated, then the
somewhat disparate acoustics of these groups of
speakers may be more easily compared.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99943225">
R. Barzilay, K. McKeown, and M. Elhadad. 1999. In-
formation fusion in the context of multi-document
summarization. In Proc. of the 37th Association for
Computational Linguistics, pages 550–557.
J. G. Carbonell and J. Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of the 21st annual international ACM SIGIR con-
ference on research and development in information
retrieval, pages 335–336.
H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.
2004. From text summarisation to style-specific
</reference>
<page confidence="0.983689">
556
</page>
<reference confidence="0.999665411111111">
summarisation for broadcast news. In Proceedings
of the 26th European Conference on Information Re-
trieval (ECIR-2004), pages 223–237.
S. Furui, T. Kikuichi, Y. Shinnaka, and C. Hori. 2003.
Speech-to-speech and speech to text summarization.
In First International workshop on Language Un-
derstanding and Agents for Real World Interaction.
M. Gajjar, R. Govindarajan, and T. V. Sreenivas. 2008.
Online unsupervised pattern discovery in speech us-
ing parallelization. In Proc. Interspeech, pages
2458–2461.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999.
Auto-summarization of audio-video presentations.
In Proceedings of the seventh ACM international
conference on Multimedia, pages 489–498.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 2000.
Comparing presentation summaries: Slides vs. read-
ing vs. listening. In Proceedings ofACM CHI, pages
177–184.
Y. Lin, T. Jiang, and Chao. K. 2002. Efficient al-
gorithms for locating the length-constrained heavi-
est segments with applications to biomolecular se-
quence analysis. J. Computer and System Science,
63(3):570–586.
C. Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings of the
42st Annual Meeting of the Association for Com-
putational Linguistics (ACL), Text Summarization
Branches Out Workshop, pages 74–81.
I Malioutov, A. Park, B. Barzilay, and J. Glass. 2007.
Making sense of sound: Unsupervised topic seg-
mentation over acoustic input. In Proc. ACL, pages
504–511.
S. Maskey and J. Hirschberg. 2005. Comparing lexial,
acoustic/prosodic, discourse and structural features
for speech summarization. In Proceedings of the
9th European Conference on Speech Communica-
tion and Technology (Eurospeech), pages 621–624.
K. Mckeown and D.R. Radev. 1995. Generating sum-
maries of multiple news articles. In Proc. of SIGIR,
pages 72–82.
C. Munteanu, R. Baecker, G Penn, E. Toms, and
E. James. 2006. Effect of speech recognition ac-
curacy rates on the usefulness and usability of we-
bcast archives. In Proceedings of SIGCHI, pages
493–502.
G. Murray, S. Renals, and J. Carletta. 2005.
Extractive summarization of meeting recordings.
In Proceedings of the 9th European Conference
on Speech Communication and Technology (Eu-
rospeech), pages 593–596.
A. Park and J. Glass. 2006. Unsupervised word ac-
quisition from speech using pattern discovery. Proc.
ICASSP, pages 409–412.
A. Park and J. Glass. 2008. Unsupervised pattern dis-
covery in speech. IEEE Trans. ASLP, 16(1):186–
197.
G. Penn and X. Zhu. 2008. A critical reassessment of
evaluation baselines for speech summarization. In
Proc. ofthe 46th Associationfor Computational Lin-
guistics, pages 407–478.
W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P.
Flannery. 2007. Numerical recipes: The art of sci-
ence computing.
D. Radev and K. McKeown. 1998. Generating natural
language summaries from multiple on-line sources.
In Computational Linguistics, pages 469–500.
D. Radev, H. Jing, M. Stys, and D. Tam. 2004.
Centroid-based summarization of multiple docu-
ments. Information Processing and Management,
40:919–938.
T. Toda, H. Saruwatari, and K. Shikano. 2001. Voice
conversion algorithm based on gaussian mixture
model with dynamic frequency warping of straight
spectrum. In Proc. ICASPP, pages 841–844.
S. Tucker and S. Whittaker. 2008. Temporal compres-
sion of speech: an evaluation. IEEE Transactions
on Audio, Speech and Language Processing, pages
790–796.
K. Zechner. 2001. Automatic Summarization of Spo-
ken Dialogues in Unrestricted Domains. Ph.D. the-
sis, Carnegie Mellon University.
J. Zhang, H. Chan, P. Fung, and L Cao. 2007. Compar-
ative study on speech summarization of broadcast
news and lecture speech. In Proc. of Interspeech,
pages 2781–2784.
X. Zhu and G. Penn. 2006. Summarization of spon-
taneous conversations. In Proceedings of the 9th
International Conference on Spoken Language Pro-
cessing, pages 1531–1534.
</reference>
<page confidence="0.99739">
557
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.848299">
<title confidence="0.9380365">Summarizing multiple spoken documents: finding evidence from untranscribed audio</title>
<author confidence="0.999126">Xiaodan Zhu</author>
<author confidence="0.999126">Gerald Penn</author>
<author confidence="0.999126">Frank Rudzicz</author>
<affiliation confidence="0.999956">University of Toronto</affiliation>
<address confidence="0.9986755">10 King’s College Rd., Toronto, M5S 3G4, ON, Canada</address>
<abstract confidence="0.997285814814815">This paper presents a model for summarizing multiple untranscribed spoken documents. Without assuming the availability of transcripts, the model modifies a recently proposed unsupervised algorithm to detect re-occurring acoustic patterns in speech and uses them to estimate similarities between utterances, which are in turn used to identify salient utterances and remove redundancies. This model is of interest due to its independence from spoken language transcription, an error-prone and resource-intensive process, its ability to integrate multiple sources of information on the same topic, and its novel use of acoustic patterns that extends previous work on low-level prosodic feature detection. We compare the performance of this model with that achieved using manual and automatic transcripts, and find that this new approach is roughly equivalent to having access to ASR transcripts with word error rates in the 33–37% range without actually having to do the ASR, plus it better handles utterances with out-ofvocabulary words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K McKeown</author>
<author>M Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>In Proc. of the 37th Association for Computational Linguistics,</booktitle>
<pages>550--557</pages>
<contexts>
<context position="9842" citStr="Barzilay et al., 1999" startWordPosition="1506" endWordPosition="1509">in summarizing multiple spoken documents by using reoccurrence statistics of acoustic patterns. 2.2 Multiple-document summarization Multi-document summarization on written text has been studied for over a decade. Compared with the single-document task, it needs to remove more content, cope with prominent redundancy, and organize content from different sources properly. This field has been pioneered by early work such as the SUMMONS architecture (Mckeown and Radev, 1995; Radev and McKeown, 1998). Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multiGen (Barzilay et al., 1999), and MEAD (Radev et al., 2004). Multi-document summarization has received intensive study at DUC. 1 Unfortunately, no such efforts have been extended to summarize multiple spoken documents yet. Abstractive approaches have been studied since the beginning. A famous effort in this direction is the information fusion approach proposed in Barzilay et al. (1999). However, for error-prone transcripts of spoken documents, an abstractive method still seems to be too ambitious for the time being. As in single-spoken-document summarization, this paper focuses on the extractive approach. Among the extra</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>R. Barzilay, K. McKeown, and M. Elhadad. 1999. Information fusion in the context of multi-document summarization. In Proc. of the 37th Association for Computational Linguistics, pages 550–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Carbonell</author>
<author>J Goldstein</author>
</authors>
<title>The use of mmr, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st annual international ACM SIGIR conference on</booktitle>
<pages>335--336</pages>
<contexts>
<context position="4446" citStr="Carbonell and Goldstein, 1998" startWordPosition="667" endWordPosition="670">P, pages 549–557, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP classification problem, in which speech is segmented and classified into pre-existing categories (words). Words not in the predefined dictionary are certain to be misrecognized without exception. This out-of-vocabulary (OOV) problem is unavoidable in the regular ASR framework, although it is more likely to happen on salient words such as named entities or domain-specific terms. Our approach uses acoustic evidence from the untranscribed audio stream. Consider text summarization first: many well-known models such as MMR (Carbonell and Goldstein, 1998) and MEAD (Radev et al., 2004) rely on the reoccurrence statistics of words. That is, if we switch any word w1 with another word w2 across an entire corpus, the ranking of extracts (often sentences) will be unaffected, because no wordspecific knowledge is involved. These models have achieved state-of-the-art performance in transcript-based speech summarization (Zechner, 2001; Penn and Zhu, 2008). For spoken documents, such reoccurrence statistics are available directly from the speech signal. In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring pattern</context>
<context position="9808" citStr="Carbonell and Goldstein, 1998" startWordPosition="1500" endWordPosition="1503">rization. In this paper we are interested in summarizing multiple spoken documents by using reoccurrence statistics of acoustic patterns. 2.2 Multiple-document summarization Multi-document summarization on written text has been studied for over a decade. Compared with the single-document task, it needs to remove more content, cope with prominent redundancy, and organize content from different sources properly. This field has been pioneered by early work such as the SUMMONS architecture (Mckeown and Radev, 1995; Radev and McKeown, 1998). Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multiGen (Barzilay et al., 1999), and MEAD (Radev et al., 2004). Multi-document summarization has received intensive study at DUC. 1 Unfortunately, no such efforts have been extended to summarize multiple spoken documents yet. Abstractive approaches have been studied since the beginning. A famous effort in this direction is the information fusion approach proposed in Barzilay et al. (1999). However, for error-prone transcripts of spoken documents, an abstractive method still seems to be too ambitious for the time being. As in single-spoken-document summarization, this paper focuses on the ex</context>
<context position="22873" citStr="Carbonell and Goldstein, 1998" startWordPosition="3653" endWordPosition="3656">s selected from manual transcripts. 3.3 Extractive summarization Once the similarity matrix between sentences in a topic is acquired, we can conduct extractive summarization by using the matrix to estimate both similarity and redundancy. As discussed above, we take the general framework of MMR and MEAD, i.e., a linear model combining salience and redundancy. In practice, we used MMR in our experiments, since the original MEAD considers also sentence positions 3 , which can always been added later as in (Penn and Zhu, 2008). To facilitate our discussion below, we briefly revisit MMR here. MMR (Carbonell and Goldstein, 1998) iteratively augments the summary with utterances that are most similar to the document set under consideration, but most dissimilar to the previously selected utterances in that summary, as shown in the equation below. Here, the sim1 term represents the similarity between a sentence and the document set it belongs to. The assumption is that a sentence having a higher sim1 would better represent the content of the documents. The sim2 term represents the similarity between a candidate sentence and sentences already in the summary. It is used to control redundancy. For the transcriptbased system</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>J. G. Carbonell and J. Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval, pages 335–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Christensen</author>
<author>B Kolluru</author>
<author>Y Gotoh</author>
<author>S Renals</author>
</authors>
<title>From text summarisation to style-specific summarisation for broadcast news.</title>
<date>2004</date>
<booktitle>In Proceedings of the 26th European Conference on Information Retrieval (ECIR-2004),</booktitle>
<pages>223--237</pages>
<contexts>
<context position="1473" citStr="Christensen et al., 2004" startWordPosition="221" endWordPosition="224"> use of acoustic patterns that extends previous work on low-level prosodic feature detection. We compare the performance of this model with that achieved using manual and automatic transcripts, and find that this new approach is roughly equivalent to having access to ASR transcripts with word error rates in the 33–37% range without actually having to do the ASR, plus it better handles utterances with out-ofvocabulary words. 1 Introduction Summarizing spoken documents has been extensively studied over the past several years (Penn and Zhu, 2008; Maskey and Hirschberg, 2005; Murray et al., 2005; Christensen et al., 2004; Zechner, 2001). Conventionally called speech summarization, although speech connotes more than spoken documents themselves, it is motivated by the demand for better ways to navigate spoken content and the natural difficulty in doing so — speech is inherently more linear or sequential than text in its traditional delivery. Previous research on speech summarization has addressed several important problems in this field (see Section 2.1). All of this work, however, has focused on single-document summarization and the integration of fairly simplistic acoustic features, inspired by work in descri</context>
<context position="3377" citStr="Christensen et al., 2004" startWordPosition="504" endWordPosition="507"> employed to acquire written transcripts. Speech summarization is accordingly reduced to a text summarization task conducted on error-prone transcripts. Such an approach, however, encounters several problems. First, assuming the availability of ASR is not always valid for many languages other than English that one may want to summarize. Even when it is, transcription quality is often an issue— training ASR models requires collecting and annotating corpora on specific languages, dialects, or even different domains. Although recognition errors do not significantly impair extractive summarizers (Christensen et al., 2004; Zhu and Penn, 2006), error-laden transcripts are not necessarily browseable if recognition errors are higher than certain thresholds (Munteanu et al., 2006). In such situations, audio summaries are an alternative when salient content can be identified directly from untranscribed audio. Third, the underlying paradigm of most ASR models aims to solve a 549 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 549–557, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP classification problem, in which speech is segmented and classified into pre-existing </context>
<context position="7459" citStr="Christensen et al., 2004" startWordPosition="1142" endWordPosition="1145">marization is more desirable, the state-of-the-art research on speech summarization has been less ambitious, focusing primarily on extractive summarization, which presents the most important N% of words, phrases, utterances, or speaker turns of a spoken document. The presentation can be in transcripts (Zechner, 2001), edited speech data (Furui et al., 2003), or a combination of these (He et al., 2000). Audio data amenable to summarization include meeting recordings (Murray et al., 2005), telephone conversations (Zhu and Penn, 2006; Zechner, 2001), news broadcasts (Maskey and Hirschberg, 2005; Christensen et al., 2004), presentations (He et al., 2000; Zhang et al., 2007; Penn and Zhu, 2008), etc. Although extractive summarization is not as ideal as abstractive summarization, it outperforms several comparable alternatives. Tucker and Whittaker (2008) have shown that extractive summarization is generally preferable to time compression, which speeds up the playback of audio documents with either fixed or variable rates. He et al. (2000) have shown that either playing back important audio-video segments or just highlighting the corresponding transcripts is significantly better than providing users with full tra</context>
<context position="24016" citStr="Christensen et al., 2004" startWordPosition="3839" endWordPosition="3842">y in the summary. It is used to control redundancy. For the transcriptbased systems, the sim1 and sim2 scores in this paper are measured by the number of words shared between a sentence and a sentence/document set mentioned above, weighted by the idf scores of these words, which is similar to the calculation of sentence centroid values by Radev et al. (2004). 3The usefulness of position varies significantly in different genres (Penn and Zhu, 2008). Even in the news domain, the style of broadcast news differs from written news, for example, the first sentence often serves to attract audiences (Christensen et al., 2004) and is hence less important as in written news. Without consideration of position, MEAD is more similar to MMR. Note that the acoustics-based approach estimates this by using the method discussed above in Section 3.2. Nextsent = argmax (A sim1(doc, tnr,j) tnr,j −(1 − A)maxtr,ksim2(tnr,j, tr,k)) 4 Experimental setup We use the TDT-4 dataset for our evaluation, which consists of annotated news broadcasts grouped into common topics. Since our aim in this paper is to study the achievable performance of the audio-based model, we grouped together news stories by their news anchors for each topic. T</context>
</contexts>
<marker>Christensen, Kolluru, Gotoh, Renals, 2004</marker>
<rawString>H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals. 2004. From text summarisation to style-specific summarisation for broadcast news. In Proceedings of the 26th European Conference on Information Retrieval (ECIR-2004), pages 223–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Furui</author>
<author>T Kikuichi</author>
<author>Y Shinnaka</author>
<author>C Hori</author>
</authors>
<title>Speech-to-speech and speech to text summarization.</title>
<date>2003</date>
<booktitle>In First International workshop on Language</booktitle>
<contexts>
<context position="7193" citStr="Furui et al., 2003" startWordPosition="1101" endWordPosition="1105">ny false-alarm alignments, even after applying the distortion threshold. We introduce additional distortion penalty and subpath length constraints on their scoring to discourage this possibility. 2 Related work 2.1 Speech summarization Although abstractive summarization is more desirable, the state-of-the-art research on speech summarization has been less ambitious, focusing primarily on extractive summarization, which presents the most important N% of words, phrases, utterances, or speaker turns of a spoken document. The presentation can be in transcripts (Zechner, 2001), edited speech data (Furui et al., 2003), or a combination of these (He et al., 2000). Audio data amenable to summarization include meeting recordings (Murray et al., 2005), telephone conversations (Zhu and Penn, 2006; Zechner, 2001), news broadcasts (Maskey and Hirschberg, 2005; Christensen et al., 2004), presentations (He et al., 2000; Zhang et al., 2007; Penn and Zhu, 2008), etc. Although extractive summarization is not as ideal as abstractive summarization, it outperforms several comparable alternatives. Tucker and Whittaker (2008) have shown that extractive summarization is generally preferable to time compression, which speeds</context>
</contexts>
<marker>Furui, Kikuichi, Shinnaka, Hori, 2003</marker>
<rawString>S. Furui, T. Kikuichi, Y. Shinnaka, and C. Hori. 2003. Speech-to-speech and speech to text summarization. In First International workshop on Language Understanding and Agents for Real World Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gajjar</author>
<author>R Govindarajan</author>
<author>T V Sreenivas</author>
</authors>
<title>Online unsupervised pattern discovery in speech using parallelization.</title>
<date>2008</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>2458--2461</pages>
<contexts>
<context position="18598" citStr="Gajjar et al. (2008)" startWordPosition="2966" endWordPosition="2969">ompared with the O(Klog(L)) algorithm—L is actually a constant for both Park and Glass (2008) and us, it is much smaller than K, and the O(Klog(L)) algorithm has (constant) overhead of calculating right-skew partitions. In our implementation, since most of the time is spent on calculating the average similarity scores on candidate subpaths, all average scores are therefore pre-calculated incrementally and saved. We have also parallelized the computation of similarities by topics over several computer clusters. A detailed comparison of different parallelization techniques has been conducted by Gajjar et al. (2008). In addition, comparing time efficiency between the acoustics-based approach and ASRbased summarizers is interesting but not straightforward since a great deal of comparable programming optimization needs to be additionally considered in the present approach. 3.2 Estimating utterance-level similarity In the previous stage, we calculated frame-level similarities between utterance pairs and used these to find potential matching patterns between the utterances. With this information, we estimate utterance-level similarities by estimating the numbers of true subpath alignments between two utteran</context>
</contexts>
<marker>Gajjar, Govindarajan, Sreenivas, 2008</marker>
<rawString>M. Gajjar, R. Govindarajan, and T. V. Sreenivas. 2008. Online unsupervised pattern discovery in speech using parallelization. In Proc. Interspeech, pages 2458–2461.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L He</author>
<author>E Sanocki</author>
<author>A Gupta</author>
<author>J Grudin</author>
</authors>
<title>Auto-summarization of audio-video presentations.</title>
<date>1999</date>
<booktitle>In Proceedings of the seventh ACM international conference on Multimedia,</booktitle>
<pages>489--498</pages>
<contexts>
<context position="8230" citStr="He et al., 1999" startWordPosition="1260" endWordPosition="1263">on, it outperforms several comparable alternatives. Tucker and Whittaker (2008) have shown that extractive summarization is generally preferable to time compression, which speeds up the playback of audio documents with either fixed or variable rates. He et al. (2000) have shown that either playing back important audio-video segments or just highlighting the corresponding transcripts is significantly better than providing users with full transcripts, electronic slides, or both for browsing presentation recordings. Given the limitations associated with ASR, it is no surprise that previous work (He et al., 1999; Maskey and Hirschberg, 2005; Murray et al., 2005; Zhu and Penn, 2006) has studied features available in audio. The focus, however, is primarily limited to prosody. The assumption is that prosodic effects such as stress can indicate salient information. Since a direct modeling of complicated compound prosodic effects like stress is dif550 ficult, they have used basic features of prosody instead, such as pitch, energy, duration, and pauses. The usefulness of prosody was found to be very limited by itself, if the effect of utterance length is not considered (Penn and Zhu, 2008). In multiplespok</context>
</contexts>
<marker>He, Sanocki, Gupta, Grudin, 1999</marker>
<rawString>L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999. Auto-summarization of audio-video presentations. In Proceedings of the seventh ACM international conference on Multimedia, pages 489–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L He</author>
<author>E Sanocki</author>
<author>A Gupta</author>
<author>J Grudin</author>
</authors>
<title>Comparing presentation summaries: Slides vs. reading vs. listening.</title>
<date>2000</date>
<booktitle>In Proceedings ofACM CHI,</booktitle>
<pages>177--184</pages>
<contexts>
<context position="7238" citStr="He et al., 2000" startWordPosition="1111" endWordPosition="1114">he distortion threshold. We introduce additional distortion penalty and subpath length constraints on their scoring to discourage this possibility. 2 Related work 2.1 Speech summarization Although abstractive summarization is more desirable, the state-of-the-art research on speech summarization has been less ambitious, focusing primarily on extractive summarization, which presents the most important N% of words, phrases, utterances, or speaker turns of a spoken document. The presentation can be in transcripts (Zechner, 2001), edited speech data (Furui et al., 2003), or a combination of these (He et al., 2000). Audio data amenable to summarization include meeting recordings (Murray et al., 2005), telephone conversations (Zhu and Penn, 2006; Zechner, 2001), news broadcasts (Maskey and Hirschberg, 2005; Christensen et al., 2004), presentations (He et al., 2000; Zhang et al., 2007; Penn and Zhu, 2008), etc. Although extractive summarization is not as ideal as abstractive summarization, it outperforms several comparable alternatives. Tucker and Whittaker (2008) have shown that extractive summarization is generally preferable to time compression, which speeds up the playback of audio documents with eith</context>
</contexts>
<marker>He, Sanocki, Gupta, Grudin, 2000</marker>
<rawString>L. He, E. Sanocki, A. Gupta, and J. Grudin. 2000. Comparing presentation summaries: Slides vs. reading vs. listening. In Proceedings ofACM CHI, pages 177–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K</author>
</authors>
<title>Efficient algorithms for locating the length-constrained heaviest segments with applications to biomolecular sequence analysis.</title>
<date>2002</date>
<journal>J. Computer and System Science,</journal>
<volume>63</volume>
<issue>3</issue>
<marker>K, 2002</marker>
<rawString>Y. Lin, T. Jiang, and Chao. K. 2002. Efficient algorithms for locating the length-constrained heaviest segments with applications to biomolecular sequence analysis. J. Computer and System Science, 63(3):570–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
</authors>
<title>Rouge: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42st Annual Meeting of the Association for Computational Linguistics (ACL), Text Summarization Branches Out Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="26311" citStr="Lin, 2004" startWordPosition="4209" endWordPosition="4210">l. Instead of evaluating the result against human generated summaries, we directly compare the performance against the summaries obtained by using manual transcripts, which we take as an upper bound to the audio-based system’s performance. This obviously does not preclude using the audio-based system together with other features such as utterance position, length, speaker’s roles, and most others used in the literature (Penn and Zhu, 2008). Here, we do not want our results to be affected by them with the hope of observing the difference accurately. As such, we quantify success based on ROUGE (Lin, 2004) scores. Our goal is to evalu554 ate whether the relatedness of spoken documents can reasonably be gleaned solely from the surface acoustic information. 5 Experimental results We aim to empirically determine the extent to which acoustic information alone can effectively replace conventional speech recognition within the multi-document speech summarization task. Since ASR performance can vary greatly as we discussed above, we compare our system against automatic transcripts having word error rates of 12.6%, 20.9%, 29.2%, and 35.5% on the same speech source. We changed our language models by res</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C. Lin. 2004. Rouge: a package for automatic evaluation of summaries. In Proceedings of the 42st Annual Meeting of the Association for Computational Linguistics (ACL), Text Summarization Branches Out Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Malioutov</author>
<author>A Park</author>
<author>B Barzilay</author>
<author>J Glass</author>
</authors>
<title>Making sense of sound: Unsupervised topic segmentation over acoustic input. In</title>
<date>2007</date>
<booktitle>Proc. ACL,</booktitle>
<pages>504--511</pages>
<contexts>
<context position="5243" citStr="Malioutov et al., 2007" startWordPosition="795" endWordPosition="798">tracts (often sentences) will be unaffected, because no wordspecific knowledge is involved. These models have achieved state-of-the-art performance in transcript-based speech summarization (Zechner, 2001; Penn and Zhu, 2008). For spoken documents, such reoccurrence statistics are available directly from the speech signal. In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech signal (Park and Glass, 2008). This method has been successfully applied to tasks such as word detection (Park and Glass, 2006) and topic boundary detection (Malioutov et al., 2007). Motivated by the work above, this paper explores the approach to summarizing multiple spoken documents directly over an untranscribed audio stream. Such a model is of interest because of its independence from ASR. It is directly applicable to audio recordings in languages or domains when ASR is not possible or transcription quality is low. In principle, this approach is free from the OOV problem inherent to ASR. The premise of this approach, however, is to reliably find reoccuring acoustic patterns in audio, which is challenging because of noise and pronunciation variance existing in the spe</context>
<context position="12857" citStr="Malioutov et al., 2007" startWordPosition="1978" endWordPosition="1981">entify subsequences within acoustic sequences that appear highly similar to regions within other sequences, where each sequence consists of a progression of overlapping 20ms vectors (frames). In order to find those shared patterns, we apply a modification of the segmental dynamic time warping (SDTW) algorithm to pairs of audio sequences. This method is similar to standard DTW, except that it computes multiple constrained alignments, each within predetermined bands of the similarity matrix (Park and Glass, 2008).2 SDTW has been successfully applied to problems such as topic boundary detection (Malioutov et al., 2007) and word detection (Park and Glass, 2006). An example application of SDTW is shown in Figure 1, which shows the results of two utterances from the TDT-4 English dataset: 2Park and Glass (2008) used Euclidean distance. We used cosine distance instead, which was found to be better on our held-out dataset. 551 I: the explosion in aden harbor killed seventeen u.s. sailors and injured other thirty nine last month. II: seventeen sailors were killed. These two utterances share three words: killed, seventeen, and sailors, though in different orders. The upper panel of Figure 1 shows a matrix of frame</context>
<context position="17440" citStr="Malioutov et al., 2007" startWordPosition="2779" endWordPosition="2782">d in order to find the local alignments where the similarity is maximal, which is where we expect actual matched phrases to occur. For a given path P = p0, p2, ..., pK, the optimal subpath is defined to be a continuous subpath, P∗ = pm, pm+1..., pn 552 that maximizes �&amp;quot;```-z`-n aim(pz) 0 &lt; n &lt; m &lt; k n−m+1 ,— — — and m − n + 1 &gt; L. That is, the subpath is at least as long as L and has the maximal average similarity. L is used to avoid short alignments that correspond to subword segments or short function words. The value of L is determined on a development set. The version of SDTW employed by (Malioutov et al., 2007) and Park and Glass (2008) employed an algorithm of complexity O(Klog(L)) from (Lin et al., 2002) to find subpaths. Lin et al. (2002) have also proven that the length of the optimal subpath is between L and 2L − 1, inclusively. Therefore, our version uses a very simple algorithm— just search and find the maximum of average similarities among all possible subpaths with lengths between L and 2L − 1. Although the theoretical upper bound for this algorithm is O(KL), in practice we have found no significant increase in computation time compared with the O(Klog(L)) algorithm—L is actually a constant</context>
</contexts>
<marker>Malioutov, Park, Barzilay, Glass, 2007</marker>
<rawString>I Malioutov, A. Park, B. Barzilay, and J. Glass. 2007. Making sense of sound: Unsupervised topic segmentation over acoustic input. In Proc. ACL, pages 504–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Maskey</author>
<author>J Hirschberg</author>
</authors>
<title>Comparing lexial, acoustic/prosodic, discourse and structural features for speech summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th European Conference on Speech Communication and Technology (Eurospeech),</booktitle>
<pages>621--624</pages>
<contexts>
<context position="1426" citStr="Maskey and Hirschberg, 2005" startWordPosition="213" endWordPosition="216">es of information on the same topic, and its novel use of acoustic patterns that extends previous work on low-level prosodic feature detection. We compare the performance of this model with that achieved using manual and automatic transcripts, and find that this new approach is roughly equivalent to having access to ASR transcripts with word error rates in the 33–37% range without actually having to do the ASR, plus it better handles utterances with out-ofvocabulary words. 1 Introduction Summarizing spoken documents has been extensively studied over the past several years (Penn and Zhu, 2008; Maskey and Hirschberg, 2005; Murray et al., 2005; Christensen et al., 2004; Zechner, 2001). Conventionally called speech summarization, although speech connotes more than spoken documents themselves, it is motivated by the demand for better ways to navigate spoken content and the natural difficulty in doing so — speech is inherently more linear or sequential than text in its traditional delivery. Previous research on speech summarization has addressed several important problems in this field (see Section 2.1). All of this work, however, has focused on single-document summarization and the integration of fairly simplisti</context>
<context position="7432" citStr="Maskey and Hirschberg, 2005" startWordPosition="1138" endWordPosition="1141">tion Although abstractive summarization is more desirable, the state-of-the-art research on speech summarization has been less ambitious, focusing primarily on extractive summarization, which presents the most important N% of words, phrases, utterances, or speaker turns of a spoken document. The presentation can be in transcripts (Zechner, 2001), edited speech data (Furui et al., 2003), or a combination of these (He et al., 2000). Audio data amenable to summarization include meeting recordings (Murray et al., 2005), telephone conversations (Zhu and Penn, 2006; Zechner, 2001), news broadcasts (Maskey and Hirschberg, 2005; Christensen et al., 2004), presentations (He et al., 2000; Zhang et al., 2007; Penn and Zhu, 2008), etc. Although extractive summarization is not as ideal as abstractive summarization, it outperforms several comparable alternatives. Tucker and Whittaker (2008) have shown that extractive summarization is generally preferable to time compression, which speeds up the playback of audio documents with either fixed or variable rates. He et al. (2000) have shown that either playing back important audio-video segments or just highlighting the corresponding transcripts is significantly better than pr</context>
</contexts>
<marker>Maskey, Hirschberg, 2005</marker>
<rawString>S. Maskey and J. Hirschberg. 2005. Comparing lexial, acoustic/prosodic, discourse and structural features for speech summarization. In Proceedings of the 9th European Conference on Speech Communication and Technology (Eurospeech), pages 621–624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Mckeown</author>
<author>D R Radev</author>
</authors>
<title>Generating summaries of multiple news articles.</title>
<date>1995</date>
<booktitle>In Proc. of SIGIR,</booktitle>
<pages>72--82</pages>
<contexts>
<context position="9693" citStr="Mckeown and Radev, 1995" startWordPosition="1484" endWordPosition="1487">rominent in the multiple-document setting. All of the work above has been conducted on single-document summarization. In this paper we are interested in summarizing multiple spoken documents by using reoccurrence statistics of acoustic patterns. 2.2 Multiple-document summarization Multi-document summarization on written text has been studied for over a decade. Compared with the single-document task, it needs to remove more content, cope with prominent redundancy, and organize content from different sources properly. This field has been pioneered by early work such as the SUMMONS architecture (Mckeown and Radev, 1995; Radev and McKeown, 1998). Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multiGen (Barzilay et al., 1999), and MEAD (Radev et al., 2004). Multi-document summarization has received intensive study at DUC. 1 Unfortunately, no such efforts have been extended to summarize multiple spoken documents yet. Abstractive approaches have been studied since the beginning. A famous effort in this direction is the information fusion approach proposed in Barzilay et al. (1999). However, for error-prone transcripts of spoken documents, an abstractive method still see</context>
</contexts>
<marker>Mckeown, Radev, 1995</marker>
<rawString>K. Mckeown and D.R. Radev. 1995. Generating summaries of multiple news articles. In Proc. of SIGIR, pages 72–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Munteanu</author>
<author>R Baecker</author>
<author>G Penn</author>
<author>E Toms</author>
<author>E James</author>
</authors>
<title>Effect of speech recognition accuracy rates on the usefulness and usability of webcast archives.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGCHI,</booktitle>
<pages>493--502</pages>
<contexts>
<context position="3535" citStr="Munteanu et al., 2006" startWordPosition="526" endWordPosition="529"> approach, however, encounters several problems. First, assuming the availability of ASR is not always valid for many languages other than English that one may want to summarize. Even when it is, transcription quality is often an issue— training ASR models requires collecting and annotating corpora on specific languages, dialects, or even different domains. Although recognition errors do not significantly impair extractive summarizers (Christensen et al., 2004; Zhu and Penn, 2006), error-laden transcripts are not necessarily browseable if recognition errors are higher than certain thresholds (Munteanu et al., 2006). In such situations, audio summaries are an alternative when salient content can be identified directly from untranscribed audio. Third, the underlying paradigm of most ASR models aims to solve a 549 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 549–557, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP classification problem, in which speech is segmented and classified into pre-existing categories (words). Words not in the predefined dictionary are certain to be misrecognized without exception. This out-of-vocabulary (OOV) problem is unavoida</context>
</contexts>
<marker>Munteanu, Baecker, Penn, Toms, James, 2006</marker>
<rawString>C. Munteanu, R. Baecker, G Penn, E. Toms, and E. James. 2006. Effect of speech recognition accuracy rates on the usefulness and usability of webcast archives. In Proceedings of SIGCHI, pages 493–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Carletta</author>
</authors>
<title>Extractive summarization of meeting recordings.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th European Conference on Speech Communication and Technology (Eurospeech),</booktitle>
<pages>593--596</pages>
<contexts>
<context position="1447" citStr="Murray et al., 2005" startWordPosition="217" endWordPosition="220"> topic, and its novel use of acoustic patterns that extends previous work on low-level prosodic feature detection. We compare the performance of this model with that achieved using manual and automatic transcripts, and find that this new approach is roughly equivalent to having access to ASR transcripts with word error rates in the 33–37% range without actually having to do the ASR, plus it better handles utterances with out-ofvocabulary words. 1 Introduction Summarizing spoken documents has been extensively studied over the past several years (Penn and Zhu, 2008; Maskey and Hirschberg, 2005; Murray et al., 2005; Christensen et al., 2004; Zechner, 2001). Conventionally called speech summarization, although speech connotes more than spoken documents themselves, it is motivated by the demand for better ways to navigate spoken content and the natural difficulty in doing so — speech is inherently more linear or sequential than text in its traditional delivery. Previous research on speech summarization has addressed several important problems in this field (see Section 2.1). All of this work, however, has focused on single-document summarization and the integration of fairly simplistic acoustic features, </context>
<context position="7325" citStr="Murray et al., 2005" startWordPosition="1124" endWordPosition="1127">gth constraints on their scoring to discourage this possibility. 2 Related work 2.1 Speech summarization Although abstractive summarization is more desirable, the state-of-the-art research on speech summarization has been less ambitious, focusing primarily on extractive summarization, which presents the most important N% of words, phrases, utterances, or speaker turns of a spoken document. The presentation can be in transcripts (Zechner, 2001), edited speech data (Furui et al., 2003), or a combination of these (He et al., 2000). Audio data amenable to summarization include meeting recordings (Murray et al., 2005), telephone conversations (Zhu and Penn, 2006; Zechner, 2001), news broadcasts (Maskey and Hirschberg, 2005; Christensen et al., 2004), presentations (He et al., 2000; Zhang et al., 2007; Penn and Zhu, 2008), etc. Although extractive summarization is not as ideal as abstractive summarization, it outperforms several comparable alternatives. Tucker and Whittaker (2008) have shown that extractive summarization is generally preferable to time compression, which speeds up the playback of audio documents with either fixed or variable rates. He et al. (2000) have shown that either playing back import</context>
</contexts>
<marker>Murray, Renals, Carletta, 2005</marker>
<rawString>G. Murray, S. Renals, and J. Carletta. 2005. Extractive summarization of meeting recordings. In Proceedings of the 9th European Conference on Speech Communication and Technology (Eurospeech), pages 593–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Park</author>
<author>J Glass</author>
</authors>
<title>Unsupervised word acquisition from speech using pattern discovery.</title>
<date>2006</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>409--412</pages>
<contexts>
<context position="5189" citStr="Park and Glass, 2006" startWordPosition="786" endWordPosition="789">r word w2 across an entire corpus, the ranking of extracts (often sentences) will be unaffected, because no wordspecific knowledge is involved. These models have achieved state-of-the-art performance in transcript-based speech summarization (Zechner, 2001; Penn and Zhu, 2008). For spoken documents, such reoccurrence statistics are available directly from the speech signal. In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech signal (Park and Glass, 2008). This method has been successfully applied to tasks such as word detection (Park and Glass, 2006) and topic boundary detection (Malioutov et al., 2007). Motivated by the work above, this paper explores the approach to summarizing multiple spoken documents directly over an untranscribed audio stream. Such a model is of interest because of its independence from ASR. It is directly applicable to audio recordings in languages or domains when ASR is not possible or transcription quality is low. In principle, this approach is free from the OOV problem inherent to ASR. The premise of this approach, however, is to reliably find reoccuring acoustic patterns in audio, which is challenging because o</context>
<context position="12899" citStr="Park and Glass, 2006" startWordPosition="1985" endWordPosition="1988">s that appear highly similar to regions within other sequences, where each sequence consists of a progression of overlapping 20ms vectors (frames). In order to find those shared patterns, we apply a modification of the segmental dynamic time warping (SDTW) algorithm to pairs of audio sequences. This method is similar to standard DTW, except that it computes multiple constrained alignments, each within predetermined bands of the similarity matrix (Park and Glass, 2008).2 SDTW has been successfully applied to problems such as topic boundary detection (Malioutov et al., 2007) and word detection (Park and Glass, 2006). An example application of SDTW is shown in Figure 1, which shows the results of two utterances from the TDT-4 English dataset: 2Park and Glass (2008) used Euclidean distance. We used cosine distance instead, which was found to be better on our held-out dataset. 551 I: the explosion in aden harbor killed seventeen u.s. sailors and injured other thirty nine last month. II: seventeen sailors were killed. These two utterances share three words: killed, seventeen, and sailors, though in different orders. The upper panel of Figure 1 shows a matrix of frame-level similarity scores between these two</context>
</contexts>
<marker>Park, Glass, 2006</marker>
<rawString>A. Park and J. Glass. 2006. Unsupervised word acquisition from speech using pattern discovery. Proc. ICASSP, pages 409–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Park</author>
<author>J Glass</author>
</authors>
<title>Unsupervised pattern discovery in speech.</title>
<date>2008</date>
<journal>IEEE Trans. ASLP,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>197</pages>
<contexts>
<context position="5091" citStr="Park and Glass, 2008" startWordPosition="770" endWordPosition="773"> 2004) rely on the reoccurrence statistics of words. That is, if we switch any word w1 with another word w2 across an entire corpus, the ranking of extracts (often sentences) will be unaffected, because no wordspecific knowledge is involved. These models have achieved state-of-the-art performance in transcript-based speech summarization (Zechner, 2001; Penn and Zhu, 2008). For spoken documents, such reoccurrence statistics are available directly from the speech signal. In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech signal (Park and Glass, 2008). This method has been successfully applied to tasks such as word detection (Park and Glass, 2006) and topic boundary detection (Malioutov et al., 2007). Motivated by the work above, this paper explores the approach to summarizing multiple spoken documents directly over an untranscribed audio stream. Such a model is of interest because of its independence from ASR. It is directly applicable to audio recordings in languages or domains when ASR is not possible or transcription quality is low. In principle, this approach is free from the OOV problem inherent to ASR. The premise of this approach, </context>
<context position="12750" citStr="Park and Glass, 2008" startWordPosition="1960" endWordPosition="1963">n model, where it is used for utterance selection. 3.1 Finding common acoustic patterns Our goal is to identify subsequences within acoustic sequences that appear highly similar to regions within other sequences, where each sequence consists of a progression of overlapping 20ms vectors (frames). In order to find those shared patterns, we apply a modification of the segmental dynamic time warping (SDTW) algorithm to pairs of audio sequences. This method is similar to standard DTW, except that it computes multiple constrained alignments, each within predetermined bands of the similarity matrix (Park and Glass, 2008).2 SDTW has been successfully applied to problems such as topic boundary detection (Malioutov et al., 2007) and word detection (Park and Glass, 2006). An example application of SDTW is shown in Figure 1, which shows the results of two utterances from the TDT-4 English dataset: 2Park and Glass (2008) used Euclidean distance. We used cosine distance instead, which was found to be better on our held-out dataset. 551 I: the explosion in aden harbor killed seventeen u.s. sailors and injured other thirty nine last month. II: seventeen sailors were killed. These two utterances share three words: kill</context>
<context position="14572" citStr="Park and Glass, 2008" startWordPosition="2252" endWordPosition="2255">ctors, a commonly used representation of the spectral characteristics of speech acoustics. First, conventional short-time Fourier transforms are applied to overlapping 20ms Hamming windows of the speech amplitude signal. The resulting spectral energy is then weighted by filters on the Mel-scale and converted to 39- dimensional feature vectors, each consisting of 12 MFCCs, one normalized log-energy term, as well as the first and second derivatives of these 13 components over time. The MFCC features used in the acoustics-based approach are the same as those used below in the ASR systems. As in (Park and Glass, 2008), an additional whitening step is taken to normalize the variances on each of these 39 dimensions. The similarities between frames are then estimated using cosine distance. All similarity scores are then normalized to the range of [0, 1], which yields similarity matrices exemplified in the upper panel of Figure 1. Finding optimal paths For each similarity matrix obtained above, local alignments of matching patterns need to be found, as shown in the lower panel of Figure 1. A single global DTW alignment is not adequate, since words or phrases held in common between utterances may occur in any o</context>
<context position="17466" citStr="Park and Glass (2008)" startWordPosition="2784" endWordPosition="2787"> alignments where the similarity is maximal, which is where we expect actual matched phrases to occur. For a given path P = p0, p2, ..., pK, the optimal subpath is defined to be a continuous subpath, P∗ = pm, pm+1..., pn 552 that maximizes �&amp;quot;```-z`-n aim(pz) 0 &lt; n &lt; m &lt; k n−m+1 ,— — — and m − n + 1 &gt; L. That is, the subpath is at least as long as L and has the maximal average similarity. L is used to avoid short alignments that correspond to subword segments or short function words. The value of L is determined on a development set. The version of SDTW employed by (Malioutov et al., 2007) and Park and Glass (2008) employed an algorithm of complexity O(Klog(L)) from (Lin et al., 2002) to find subpaths. Lin et al. (2002) have also proven that the length of the optimal subpath is between L and 2L − 1, inclusively. Therefore, our version uses a very simple algorithm— just search and find the maximum of average similarities among all possible subpaths with lengths between L and 2L − 1. Although the theoretical upper bound for this algorithm is O(KL), in practice we have found no significant increase in computation time compared with the O(Klog(L)) algorithm—L is actually a constant for both Park and Glass (</context>
<context position="20088" citStr="Park and Glass, 2008" startWordPosition="3184" endWordPosition="3187">s, we calculate relative similarity scores, which are computed by dividing the original similarity of a given subpath by the average similarity of its surrounding background. The motivation for capturing the relative similarity is to punish subpaths that cannot distinguish themselves from their background, e.g., those found in a block of high-similarity regions caused by certain acoustic noise. Distortion score Warped subpaths are less likely to correspond to valid matching patterns than straighter ones. In addition to removing very distorted subpaths by applying a distortion threshold as in (Park and Glass, 2008), we also quantitatively measured the remaining ones. We fit each of them with leastsquare linear regression and estimate the residue scores. As discussed above, each point on a subpath satisfies |xi − yi |&lt; T, so the residue cannot be bigger than T. We used this to normalize the distortion scores to the range of [0,1]. Subpath length Given two subpaths with nearly identical average similarity scores, we suggest that the longer of the two is more likely to refer to content of interest that is shared between two speech utterances, e.g., named entities. Longer subpaths may in this sense therefor</context>
</contexts>
<marker>Park, Glass, 2008</marker>
<rawString>A. Park and J. Glass. 2008. Unsupervised pattern discovery in speech. IEEE Trans. ASLP, 16(1):186– 197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Penn</author>
<author>X Zhu</author>
</authors>
<title>A critical reassessment of evaluation baselines for speech summarization.</title>
<date>2008</date>
<booktitle>In Proc. ofthe 46th Associationfor Computational Linguistics,</booktitle>
<pages>407--478</pages>
<contexts>
<context position="1397" citStr="Penn and Zhu, 2008" startWordPosition="209" endWordPosition="212">grate multiple sources of information on the same topic, and its novel use of acoustic patterns that extends previous work on low-level prosodic feature detection. We compare the performance of this model with that achieved using manual and automatic transcripts, and find that this new approach is roughly equivalent to having access to ASR transcripts with word error rates in the 33–37% range without actually having to do the ASR, plus it better handles utterances with out-ofvocabulary words. 1 Introduction Summarizing spoken documents has been extensively studied over the past several years (Penn and Zhu, 2008; Maskey and Hirschberg, 2005; Murray et al., 2005; Christensen et al., 2004; Zechner, 2001). Conventionally called speech summarization, although speech connotes more than spoken documents themselves, it is motivated by the demand for better ways to navigate spoken content and the natural difficulty in doing so — speech is inherently more linear or sequential than text in its traditional delivery. Previous research on speech summarization has addressed several important problems in this field (see Section 2.1). All of this work, however, has focused on single-document summarization and the in</context>
<context position="4844" citStr="Penn and Zhu, 2008" startWordPosition="731" endWordPosition="734">ch as named entities or domain-specific terms. Our approach uses acoustic evidence from the untranscribed audio stream. Consider text summarization first: many well-known models such as MMR (Carbonell and Goldstein, 1998) and MEAD (Radev et al., 2004) rely on the reoccurrence statistics of words. That is, if we switch any word w1 with another word w2 across an entire corpus, the ranking of extracts (often sentences) will be unaffected, because no wordspecific knowledge is involved. These models have achieved state-of-the-art performance in transcript-based speech summarization (Zechner, 2001; Penn and Zhu, 2008). For spoken documents, such reoccurrence statistics are available directly from the speech signal. In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech signal (Park and Glass, 2008). This method has been successfully applied to tasks such as word detection (Park and Glass, 2006) and topic boundary detection (Malioutov et al., 2007). Motivated by the work above, this paper explores the approach to summarizing multiple spoken documents directly over an untranscribed audio stream. Such a model is of interest because of its independ</context>
<context position="7532" citStr="Penn and Zhu, 2008" startWordPosition="1155" endWordPosition="1158">tion has been less ambitious, focusing primarily on extractive summarization, which presents the most important N% of words, phrases, utterances, or speaker turns of a spoken document. The presentation can be in transcripts (Zechner, 2001), edited speech data (Furui et al., 2003), or a combination of these (He et al., 2000). Audio data amenable to summarization include meeting recordings (Murray et al., 2005), telephone conversations (Zhu and Penn, 2006; Zechner, 2001), news broadcasts (Maskey and Hirschberg, 2005; Christensen et al., 2004), presentations (He et al., 2000; Zhang et al., 2007; Penn and Zhu, 2008), etc. Although extractive summarization is not as ideal as abstractive summarization, it outperforms several comparable alternatives. Tucker and Whittaker (2008) have shown that extractive summarization is generally preferable to time compression, which speeds up the playback of audio documents with either fixed or variable rates. He et al. (2000) have shown that either playing back important audio-video segments or just highlighting the corresponding transcripts is significantly better than providing users with full transcripts, electronic slides, or both for browsing presentation recordings</context>
<context position="8813" citStr="Penn and Zhu, 2008" startWordPosition="1357" endWordPosition="1360"> that previous work (He et al., 1999; Maskey and Hirschberg, 2005; Murray et al., 2005; Zhu and Penn, 2006) has studied features available in audio. The focus, however, is primarily limited to prosody. The assumption is that prosodic effects such as stress can indicate salient information. Since a direct modeling of complicated compound prosodic effects like stress is dif550 ficult, they have used basic features of prosody instead, such as pitch, energy, duration, and pauses. The usefulness of prosody was found to be very limited by itself, if the effect of utterance length is not considered (Penn and Zhu, 2008). In multiplespoken-document summarization, it is unlikely that prosody will be more useful in predicating salience than in single document summarization. Furthermore, prosody is also unlikely to be applicable to detecting or handling redundancy, which is prominent in the multiple-document setting. All of the work above has been conducted on single-document summarization. In this paper we are interested in summarizing multiple spoken documents by using reoccurrence statistics of acoustic patterns. 2.2 Multiple-document summarization Multi-document summarization on written text has been studied</context>
<context position="22771" citStr="Penn and Zhu, 2008" startWordPosition="3637" endWordPosition="3640">rectly minimize summarization errors made by the acousticsbased model relative to utterances selected from manual transcripts. 3.3 Extractive summarization Once the similarity matrix between sentences in a topic is acquired, we can conduct extractive summarization by using the matrix to estimate both similarity and redundancy. As discussed above, we take the general framework of MMR and MEAD, i.e., a linear model combining salience and redundancy. In practice, we used MMR in our experiments, since the original MEAD considers also sentence positions 3 , which can always been added later as in (Penn and Zhu, 2008). To facilitate our discussion below, we briefly revisit MMR here. MMR (Carbonell and Goldstein, 1998) iteratively augments the summary with utterances that are most similar to the document set under consideration, but most dissimilar to the previously selected utterances in that summary, as shown in the equation below. Here, the sim1 term represents the similarity between a sentence and the document set it belongs to. The assumption is that a sentence having a higher sim1 would better represent the content of the documents. The sim2 term represents the similarity between a candidate sentence </context>
<context position="26144" citStr="Penn and Zhu, 2008" startWordPosition="4176" endWordPosition="4179">nt models to improve the alignment quality. Language models are also trained on HUB-4 data. Our aim in this paper is to study the achievable performance of the audio-based model. Instead of evaluating the result against human generated summaries, we directly compare the performance against the summaries obtained by using manual transcripts, which we take as an upper bound to the audio-based system’s performance. This obviously does not preclude using the audio-based system together with other features such as utterance position, length, speaker’s roles, and most others used in the literature (Penn and Zhu, 2008). Here, we do not want our results to be affected by them with the hope of observing the difference accurately. As such, we quantify success based on ROUGE (Lin, 2004) scores. Our goal is to evalu554 ate whether the relatedness of spoken documents can reasonably be gleaned solely from the surface acoustic information. 5 Experimental results We aim to empirically determine the extent to which acoustic information alone can effectively replace conventional speech recognition within the multi-document speech summarization task. Since ASR performance can vary greatly as we discussed above, we comp</context>
</contexts>
<marker>Penn, Zhu, 2008</marker>
<rawString>G. Penn and X. Zhu. 2008. A critical reassessment of evaluation baselines for speech summarization. In Proc. ofthe 46th Associationfor Computational Linguistics, pages 407–478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Press</author>
<author>S A Teukolsky</author>
<author>W T Vetterling</author>
<author>B P Flannery</author>
</authors>
<title>Numerical recipes: The art of science computing.</title>
<date>2007</date>
<contexts>
<context position="22105" citStr="Press et al., 2007" startWordPosition="3531" endWordPosition="3534"> raw similarity scores. For subpaths in each group, we check whether their distortion scores are below and lengths are above 553 some thresholds. If they are, in any group, then the corresponding subpaths are selected as “true” alignments for the purposes of building utterancelevel similarity matrix. The numbers of true alignments are used to measure the similarity between two utterances. We therefore have 8 threshold parameters to estimate, and subpaths with similarity scores outside the top 10 are ignored. The rank groups are checked one after another in a decision list. Powell’s algorithm (Press et al., 2007) is used to find the optimal parameters that directly minimize summarization errors made by the acousticsbased model relative to utterances selected from manual transcripts. 3.3 Extractive summarization Once the similarity matrix between sentences in a topic is acquired, we can conduct extractive summarization by using the matrix to estimate both similarity and redundancy. As discussed above, we take the general framework of MMR and MEAD, i.e., a linear model combining salience and redundancy. In practice, we used MMR in our experiments, since the original MEAD considers also sentence position</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 2007</marker>
<rawString>W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery. 2007. Numerical recipes: The art of science computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>K McKeown</author>
</authors>
<title>Generating natural language summaries from multiple on-line sources.</title>
<date>1998</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>469--500</pages>
<contexts>
<context position="9719" citStr="Radev and McKeown, 1998" startWordPosition="1488" endWordPosition="1491">document setting. All of the work above has been conducted on single-document summarization. In this paper we are interested in summarizing multiple spoken documents by using reoccurrence statistics of acoustic patterns. 2.2 Multiple-document summarization Multi-document summarization on written text has been studied for over a decade. Compared with the single-document task, it needs to remove more content, cope with prominent redundancy, and organize content from different sources properly. This field has been pioneered by early work such as the SUMMONS architecture (Mckeown and Radev, 1995; Radev and McKeown, 1998). Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multiGen (Barzilay et al., 1999), and MEAD (Radev et al., 2004). Multi-document summarization has received intensive study at DUC. 1 Unfortunately, no such efforts have been extended to summarize multiple spoken documents yet. Abstractive approaches have been studied since the beginning. A famous effort in this direction is the information fusion approach proposed in Barzilay et al. (1999). However, for error-prone transcripts of spoken documents, an abstractive method still seems to be too ambitious for</context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>D. Radev and K. McKeown. 1998. Generating natural language summaries from multiple on-line sources. In Computational Linguistics, pages 469–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>H Jing</author>
<author>M Stys</author>
<author>D Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>40--919</pages>
<contexts>
<context position="4476" citStr="Radev et al., 2004" startWordPosition="673" endWordPosition="676">August 2009. c�2009 ACL and AFNLP classification problem, in which speech is segmented and classified into pre-existing categories (words). Words not in the predefined dictionary are certain to be misrecognized without exception. This out-of-vocabulary (OOV) problem is unavoidable in the regular ASR framework, although it is more likely to happen on salient words such as named entities or domain-specific terms. Our approach uses acoustic evidence from the untranscribed audio stream. Consider text summarization first: many well-known models such as MMR (Carbonell and Goldstein, 1998) and MEAD (Radev et al., 2004) rely on the reoccurrence statistics of words. That is, if we switch any word w1 with another word w2 across an entire corpus, the ranking of extracts (often sentences) will be unaffected, because no wordspecific knowledge is involved. These models have achieved state-of-the-art performance in transcript-based speech summarization (Zechner, 2001; Penn and Zhu, 2008). For spoken documents, such reoccurrence statistics are available directly from the speech signal. In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech signal (Park a</context>
<context position="9873" citStr="Radev et al., 2004" startWordPosition="1512" endWordPosition="1515">uments by using reoccurrence statistics of acoustic patterns. 2.2 Multiple-document summarization Multi-document summarization on written text has been studied for over a decade. Compared with the single-document task, it needs to remove more content, cope with prominent redundancy, and organize content from different sources properly. This field has been pioneered by early work such as the SUMMONS architecture (Mckeown and Radev, 1995; Radev and McKeown, 1998). Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multiGen (Barzilay et al., 1999), and MEAD (Radev et al., 2004). Multi-document summarization has received intensive study at DUC. 1 Unfortunately, no such efforts have been extended to summarize multiple spoken documents yet. Abstractive approaches have been studied since the beginning. A famous effort in this direction is the information fusion approach proposed in Barzilay et al. (1999). However, for error-prone transcripts of spoken documents, an abstractive method still seems to be too ambitious for the time being. As in single-spoken-document summarization, this paper focuses on the extractive approach. Among the extractive models, MMR (Carbonell an</context>
<context position="23751" citStr="Radev et al. (2004)" startWordPosition="3797" endWordPosition="3800">larity between a sentence and the document set it belongs to. The assumption is that a sentence having a higher sim1 would better represent the content of the documents. The sim2 term represents the similarity between a candidate sentence and sentences already in the summary. It is used to control redundancy. For the transcriptbased systems, the sim1 and sim2 scores in this paper are measured by the number of words shared between a sentence and a sentence/document set mentioned above, weighted by the idf scores of these words, which is similar to the calculation of sentence centroid values by Radev et al. (2004). 3The usefulness of position varies significantly in different genres (Penn and Zhu, 2008). Even in the news domain, the style of broadcast news differs from written news, for example, the first sentence often serves to attract audiences (Christensen et al., 2004) and is hence less important as in written news. Without consideration of position, MEAD is more similar to MMR. Note that the acoustics-based approach estimates this by using the method discussed above in Section 3.2. Nextsent = argmax (A sim1(doc, tnr,j) tnr,j −(1 − A)maxtr,ksim2(tnr,j, tr,k)) 4 Experimental setup We use the TDT-4 </context>
</contexts>
<marker>Radev, Jing, Stys, Tam, 2004</marker>
<rawString>D. Radev, H. Jing, M. Stys, and D. Tam. 2004. Centroid-based summarization of multiple documents. Information Processing and Management, 40:919–938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Toda</author>
<author>H Saruwatari</author>
<author>K Shikano</author>
</authors>
<title>Voice conversion algorithm based on gaussian mixture model with dynamic frequency warping of straight spectrum.</title>
<date>2001</date>
<booktitle>In Proc. ICASPP,</booktitle>
<pages>841--844</pages>
<marker>Toda, Saruwatari, Shikano, 2001</marker>
<rawString>T. Toda, H. Saruwatari, and K. Shikano. 2001. Voice conversion algorithm based on gaussian mixture model with dynamic frequency warping of straight spectrum. In Proc. ICASPP, pages 841–844.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tucker</author>
<author>S Whittaker</author>
</authors>
<title>Temporal compression of speech: an evaluation.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<pages>790--796</pages>
<contexts>
<context position="7694" citStr="Tucker and Whittaker (2008)" startWordPosition="1175" endWordPosition="1179">er turns of a spoken document. The presentation can be in transcripts (Zechner, 2001), edited speech data (Furui et al., 2003), or a combination of these (He et al., 2000). Audio data amenable to summarization include meeting recordings (Murray et al., 2005), telephone conversations (Zhu and Penn, 2006; Zechner, 2001), news broadcasts (Maskey and Hirschberg, 2005; Christensen et al., 2004), presentations (He et al., 2000; Zhang et al., 2007; Penn and Zhu, 2008), etc. Although extractive summarization is not as ideal as abstractive summarization, it outperforms several comparable alternatives. Tucker and Whittaker (2008) have shown that extractive summarization is generally preferable to time compression, which speeds up the playback of audio documents with either fixed or variable rates. He et al. (2000) have shown that either playing back important audio-video segments or just highlighting the corresponding transcripts is significantly better than providing users with full transcripts, electronic slides, or both for browsing presentation recordings. Given the limitations associated with ASR, it is no surprise that previous work (He et al., 1999; Maskey and Hirschberg, 2005; Murray et al., 2005; Zhu and Penn</context>
</contexts>
<marker>Tucker, Whittaker, 2008</marker>
<rawString>S. Tucker and S. Whittaker. 2008. Temporal compression of speech: an evaluation. IEEE Transactions on Audio, Speech and Language Processing, pages 790–796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
</authors>
<title>Automatic Summarization of Spoken Dialogues in Unrestricted Domains.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="1489" citStr="Zechner, 2001" startWordPosition="225" endWordPosition="226">that extends previous work on low-level prosodic feature detection. We compare the performance of this model with that achieved using manual and automatic transcripts, and find that this new approach is roughly equivalent to having access to ASR transcripts with word error rates in the 33–37% range without actually having to do the ASR, plus it better handles utterances with out-ofvocabulary words. 1 Introduction Summarizing spoken documents has been extensively studied over the past several years (Penn and Zhu, 2008; Maskey and Hirschberg, 2005; Murray et al., 2005; Christensen et al., 2004; Zechner, 2001). Conventionally called speech summarization, although speech connotes more than spoken documents themselves, it is motivated by the demand for better ways to navigate spoken content and the natural difficulty in doing so — speech is inherently more linear or sequential than text in its traditional delivery. Previous research on speech summarization has addressed several important problems in this field (see Section 2.1). All of this work, however, has focused on single-document summarization and the integration of fairly simplistic acoustic features, inspired by work in descriptive linguistic</context>
<context position="4823" citStr="Zechner, 2001" startWordPosition="729" endWordPosition="730">alient words such as named entities or domain-specific terms. Our approach uses acoustic evidence from the untranscribed audio stream. Consider text summarization first: many well-known models such as MMR (Carbonell and Goldstein, 1998) and MEAD (Radev et al., 2004) rely on the reoccurrence statistics of words. That is, if we switch any word w1 with another word w2 across an entire corpus, the ranking of extracts (often sentences) will be unaffected, because no wordspecific knowledge is involved. These models have achieved state-of-the-art performance in transcript-based speech summarization (Zechner, 2001; Penn and Zhu, 2008). For spoken documents, such reoccurrence statistics are available directly from the speech signal. In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech signal (Park and Glass, 2008). This method has been successfully applied to tasks such as word detection (Park and Glass, 2006) and topic boundary detection (Malioutov et al., 2007). Motivated by the work above, this paper explores the approach to summarizing multiple spoken documents directly over an untranscribed audio stream. Such a model is of interest be</context>
<context position="7152" citStr="Zechner, 2001" startWordPosition="1096" endWordPosition="1097"> context, ultimately leads to too many false-alarm alignments, even after applying the distortion threshold. We introduce additional distortion penalty and subpath length constraints on their scoring to discourage this possibility. 2 Related work 2.1 Speech summarization Although abstractive summarization is more desirable, the state-of-the-art research on speech summarization has been less ambitious, focusing primarily on extractive summarization, which presents the most important N% of words, phrases, utterances, or speaker turns of a spoken document. The presentation can be in transcripts (Zechner, 2001), edited speech data (Furui et al., 2003), or a combination of these (He et al., 2000). Audio data amenable to summarization include meeting recordings (Murray et al., 2005), telephone conversations (Zhu and Penn, 2006; Zechner, 2001), news broadcasts (Maskey and Hirschberg, 2005; Christensen et al., 2004), presentations (He et al., 2000; Zhang et al., 2007; Penn and Zhu, 2008), etc. Although extractive summarization is not as ideal as abstractive summarization, it outperforms several comparable alternatives. Tucker and Whittaker (2008) have shown that extractive summarization is generally pre</context>
</contexts>
<marker>Zechner, 2001</marker>
<rawString>K. Zechner. 2001. Automatic Summarization of Spoken Dialogues in Unrestricted Domains. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhang</author>
<author>H Chan</author>
<author>P Fung</author>
<author>L Cao</author>
</authors>
<title>Comparative study on speech summarization of broadcast news and lecture speech.</title>
<date>2007</date>
<booktitle>In Proc. of Interspeech,</booktitle>
<pages>2781--2784</pages>
<contexts>
<context position="7511" citStr="Zhang et al., 2007" startWordPosition="1151" endWordPosition="1154"> on speech summarization has been less ambitious, focusing primarily on extractive summarization, which presents the most important N% of words, phrases, utterances, or speaker turns of a spoken document. The presentation can be in transcripts (Zechner, 2001), edited speech data (Furui et al., 2003), or a combination of these (He et al., 2000). Audio data amenable to summarization include meeting recordings (Murray et al., 2005), telephone conversations (Zhu and Penn, 2006; Zechner, 2001), news broadcasts (Maskey and Hirschberg, 2005; Christensen et al., 2004), presentations (He et al., 2000; Zhang et al., 2007; Penn and Zhu, 2008), etc. Although extractive summarization is not as ideal as abstractive summarization, it outperforms several comparable alternatives. Tucker and Whittaker (2008) have shown that extractive summarization is generally preferable to time compression, which speeds up the playback of audio documents with either fixed or variable rates. He et al. (2000) have shown that either playing back important audio-video segments or just highlighting the corresponding transcripts is significantly better than providing users with full transcripts, electronic slides, or both for browsing pr</context>
</contexts>
<marker>Zhang, Chan, Fung, Cao, 2007</marker>
<rawString>J. Zhang, H. Chan, P. Fung, and L Cao. 2007. Comparative study on speech summarization of broadcast news and lecture speech. In Proc. of Interspeech, pages 2781–2784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>G Penn</author>
</authors>
<title>Summarization of spontaneous conversations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 9th International Conference on Spoken Language Processing,</booktitle>
<pages>1531--1534</pages>
<contexts>
<context position="3398" citStr="Zhu and Penn, 2006" startWordPosition="508" endWordPosition="511">en transcripts. Speech summarization is accordingly reduced to a text summarization task conducted on error-prone transcripts. Such an approach, however, encounters several problems. First, assuming the availability of ASR is not always valid for many languages other than English that one may want to summarize. Even when it is, transcription quality is often an issue— training ASR models requires collecting and annotating corpora on specific languages, dialects, or even different domains. Although recognition errors do not significantly impair extractive summarizers (Christensen et al., 2004; Zhu and Penn, 2006), error-laden transcripts are not necessarily browseable if recognition errors are higher than certain thresholds (Munteanu et al., 2006). In such situations, audio summaries are an alternative when salient content can be identified directly from untranscribed audio. Third, the underlying paradigm of most ASR models aims to solve a 549 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 549–557, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP classification problem, in which speech is segmented and classified into pre-existing categories (words). W</context>
<context position="7370" citStr="Zhu and Penn, 2006" startWordPosition="1130" endWordPosition="1133">this possibility. 2 Related work 2.1 Speech summarization Although abstractive summarization is more desirable, the state-of-the-art research on speech summarization has been less ambitious, focusing primarily on extractive summarization, which presents the most important N% of words, phrases, utterances, or speaker turns of a spoken document. The presentation can be in transcripts (Zechner, 2001), edited speech data (Furui et al., 2003), or a combination of these (He et al., 2000). Audio data amenable to summarization include meeting recordings (Murray et al., 2005), telephone conversations (Zhu and Penn, 2006; Zechner, 2001), news broadcasts (Maskey and Hirschberg, 2005; Christensen et al., 2004), presentations (He et al., 2000; Zhang et al., 2007; Penn and Zhu, 2008), etc. Although extractive summarization is not as ideal as abstractive summarization, it outperforms several comparable alternatives. Tucker and Whittaker (2008) have shown that extractive summarization is generally preferable to time compression, which speeds up the playback of audio documents with either fixed or variable rates. He et al. (2000) have shown that either playing back important audio-video segments or just highlighting</context>
</contexts>
<marker>Zhu, Penn, 2006</marker>
<rawString>X. Zhu and G. Penn. 2006. Summarization of spontaneous conversations. In Proceedings of the 9th International Conference on Spoken Language Processing, pages 1531–1534.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>