<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.7839445">
Graph-based Analysis of Semantic Drift in Espresso-like
Bootstrapping Algorithms
</title>
<author confidence="0.920227">
Mamoru Komachi Taku Kudo Masashi Shimbo Yuji Matsumoto
</author>
<affiliation confidence="0.927858">
NAIST, Japan Google Inc. NAIST, Japan NAIST, Japan
</affiliation>
<email confidence="0.994282">
mamoru-k@is.naist.jp taku@google.com shimbo@is.naist.jp matsu@is.naist.jp
</email>
<sectionHeader confidence="0.996599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999250235294117">
Bootstrapping has a tendency, called seman-
tic drift, to select instances unrelated to the
seed instances as the iteration proceeds. We
demonstrate the semantic drift of bootstrap-
ping has the same root as the topic drift of
Kleinberg’s HITS, using a simplified graph-
based reformulation of bootstrapping. We
confirm that two graph-based algorithms, the
von Neumann kernels and the regularized
Laplacian, can reduce semantic drift in the
task of word sense disambiguation (WSD)
on Senseval-3 English Lexical Sample Task.
Proposed algorithms achieve superior perfor-
mance to Espresso and previous graph-based
WSD methods, even though the proposed al-
gorithms have less parameters and are easy to
calibrate.
</bodyText>
<sectionHeader confidence="0.998887" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999532875">
In recent years machine learning techniques be-
come widely used in natural language processing
(NLP). These techniques offer various ways to ex-
ploit large corpora and are known to perform well
in many tasks. However, these techniques often re-
quire tagged corpora, which are not readily available
to many languages. So far, reducing the cost of hu-
man annotation is one of the important problems for
building NLP systems.
To mitigate the problem of hand-tagging re-
sources, semi(or minimally)-supervised and unsu-
pervised techniques have been actively studied.
Hearst (1992) first presented a bootstrapping method
which requires only a small amount of instances
(seed instances) to start with, but can easily mul-
tiply the number of tagged instances with mini-
mal human annotation cost, by iteratively apply-
ing the following phases: pattern induction, pattern
ranking/selection, and instance extraction. Boot-
strapping has been widely adopted in NLP applica-
tions such as word sense disambiguation (Yarowsky,
1995), named entity recognition (Collins and Singer,
1999) and relation extraction (Riloff and Jones,
1999; Pantel and Pennacchiotti, 2006).
However, it is known that bootstrapping often ac-
quires instances not related to seed instances. For
example, consider the task of collecting the names
of common tourist sites from web corpora. Given
words like “Geneva” and “Bali” as seed instances,
bootstrapping would eventually learn generic pat-
terns such as “pictures” and “photos,” which also
co-occur with many other unrelated instances. The
subsequent iterations would likely acquire frequent
words that co-occur with these generic patterns,
such as “Britney Spears.” This phenomenon is
called semantic drift (Curran et al., 2007).
A straightforward approach to avoid semantic
drift is to terminate iterations before hitting generic
patterns, but the optimal number of iterations is task
dependent and is hard to come by. The recently pro-
posed Espresso (Pantel and Pennacchiotti, 2006) al-
gorithm incorporates sophisticated scoring functions
to cope with generic patterns, but as Komachi and
Suzuki (2008) pointed out, Espresso still shows se-
mantic drift unless iterations are terminated appro-
priately.
Another deficiency in bootstrapping is its sensi-
tivity to many parameters such as the number of
</bodyText>
<page confidence="0.953341">
1011
</page>
<note confidence="0.880857">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1011–1020,
Honolulu, October 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999807277777778">
seed instances, the stopping criterion of iteration, the
number of instances and patterns selected on each it-
eration, and so forth. These parameters also need to
be calibrated for each task.
In this paper, we present a graph-theoretic anal-
ysis of Espresso-like bootstrapping algorithms. We
argue that semantic drift is inherent in these algo-
rithms, and propose to use two graph-based algo-
rithms that are theoretically less prone to semantic
drift, as an alternative to bootstrapping.
After a brief review of related work in Section 2,
we analyze in Section 3 a bootstrapping algorithm
(Simplified Espresso) which can be thought of as a
degenerate version of Espresso. Simplified Espresso
is simple enough to allow an algebraic treatment,
and its equivalence to Kleinberg’s HITS algorithm
(Kleinberg, 1999) is shown. An implication of this
equivalence is that semantic drift in this bootstrap-
ping algorithm is essentially the same phenomenon
as topic drift observed in link analysis. Another im-
plication is that semantic drift is inevitable in Sim-
plified Espresso as it converges to the same score
vector regardless of seed instances.
The original Espresso also suffers from the same
problem as its simplified version does. It incorpo-
rates heuristics not present in Simplified Espresso to
reduce semantic drift, but these heuristics have lim-
ited effect as we demonstrate in Section 3.3.
In Section 4, we propose two graph-based algo-
rithms to reduce semantic drift. These algorithms
are used in link analysis community to reduce the
effect of topic drift. In Section 5 we apply them to
the task of word sense disambiguation on Senseval-3
Lexical Sample Task and verify that they indeed re-
duce semantic drift. Finally, we conclude our work
in Section 6.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999196">
2.1 Overview of Bootstrapping
</subsectionHeader>
<bodyText confidence="0.999679487804878">
Bootstrapping (or self-training) is a general frame-
work for reducing the requirement of manual an-
notation. Hearst (1992) described a bootstrapping
procedure for extracting words in hyponym (is-a)
relation, starting with three manually given lexico-
syntactic patterns.
The idea of learning with a bootstrapping method
was adopted for many tasks. Yarowsky (1995) pre-
sented an unsupervised WSD system which rivals
supervised techniques. Abney (2004) presented a
thorough discussion on the Yarowsky algorithm. He
extended the original Yarowsky algorithm to a new
family of bootstrapping algorithms that are mathe-
matically well understood.
Li and Li (2004) proposed a method called Bilin-
gual Bootstrapping. It makes use of a translation
dictionary and a comparable corpus to help disam-
biguate word senses in the source language, by ex-
ploiting the asymmetric many-to-many sense map-
ping relationship between words in two languages.
Curran et al. (2007) presented an algorithm called
Mutual Exclusion Bootstrapping, which minimizes
semantic drift using mutual exclusion between se-
mantic classes of learned instances. They prepared
a list of so-called stop classes similar to a stop word
list used in information retrieval to help bound the
semantic classes. Stop classes are sets of terms
known to cause semantic drift in particular seman-
tic classes. However, stop classes vary from task to
task and domain to domain, and human intervention
is essential to create an effective list of stop classes.
A major drawback of bootstrapping is the lack
of principled method for selecting optimal param-
eter values (Ng and Cardie, 2003; Banko and Brill,
2001). Also, there is an issue of generic patterns
which deteriorates the quality of acquired instances.
Previously proposed bootstrapping algorithms differ
in how they deal with the problem of semantic drift.
We will take recently proposed Espresso algorithm
as the example to explain common configuration for
bootstrapping in detail.
</bodyText>
<subsectionHeader confidence="0.999716">
2.2 The Espresso Algorithm
</subsectionHeader>
<bodyText confidence="0.9999091">
Pantel and Pennachiotti (2006) proposed a boot-
strapping algorithm called Espresso to learn binary
semantic relations such as is-a and part-of from
a corpus. What distinguishes Espresso from other
bootstrapping algorithms is that it benefits from
generic patterns by using a principled measure of
instance and pattern reliability. The key idea of
Espresso is recursive definition of pattern-instance
scoring metrics. The reliability scores of pattern p
and instance i, denoted respectively as r,(p) and
</bodyText>
<page confidence="0.93254">
1012
</page>
<bodyText confidence="0.705089">
rι(i), are given as follows:
</bodyText>
<equation confidence="0.9992985">
pmi(i,p)
�iEI max pmirι(i) (1)
|I|
pmi(i,p) r
EPEP max pmi �rl p) rι(i) =(2)
|P|
</equation>
<bodyText confidence="0.576863">
where
</bodyText>
<equation confidence="0.999109">
pmi(i,p) = lo92 |i |i,∗||*I pI (3)
</equation>
<bodyText confidence="0.897636054054054">
is pointwise mutual information between i and p, P
and I are sets of patterns and instances, and |P |and
|I |are the numbers of patterns and instances, respec-
tively. |i, ∗ |and |∗, p |are the frequencies of pattern
p and instance i in a given corpus, respectively, and
|i, p |is the frequency of pattern p which co-occurs
with instance i. max pmi is a maximum value of
the pointwise mutual information over all instances
and patterns. The intuition behind these definitions
is that a reliable pattern co-occurs with many reli-
able instances, and a reliable instance co-occurs with
many reliable patterns.
Espresso and other bootstrapping methods iterate
the following three phases: pattern induction, pat-
tern ranking/selection, and instance extraction.
We describe these phases below, along with the
parameters that controls each phase.
Phase 1. Pattern Induction Induce patterns from
a corpus given seed instances. Patterns may be sur-
face text patterns, lexico-syntactic patterns, and/or
just features.
Phase 2. Pattern Ranking/Selection Create a
pattern ranker from a corpus using instances as fea-
tures and select patterns which co-occur with seed
instances for the next instance extraction phase. The
main issue here is to avoid ranking generic patterns
high and to choose patterns with high relatedness to
the seed instances. Parameters and configurations:
(a) a pattern scoring metrics and (b) the number of
patterns to use for extraction of instances.
Phase 3. Instance Extraction Select high-
confidence instances to the seed instance set. It is
desirable to keep only high-confidence instances at
this phase, as they are used as seed instances for the
input:
seed vector i0
pattern-instance co-occurrence matrix M
</bodyText>
<listItem confidence="0.910443583333333">
output:
instance and pattern score vectors i and p
1: i = i0
2: loop
3: p ← Mi
4: Normalize p
5: i ← MTp
6: Normalize i
7: if i and p have both converged then
8: return i and p
9: end if
10: end loop
</listItem>
<figureCaption confidence="0.997107">
Figure 1: A simple bootstrapping algorithm
</figureCaption>
<bodyText confidence="0.998539">
next iteration. Optionally, instances can be cumula-
tively obtained on each iteration to retain highly rel-
evant instances learned in early iterations. Parame-
ters and configurations: (c) instance scoring metrics,
(d) whether to retain extracted instances on each it-
eration or not, and (e) the number of instances to
pass to the next iteration.
Bootstrapping iterates the above three phases sev-
eral times until stopping criteria are met. Acquired
instances tend to become noisy as the iteration pro-
ceeds, so it is important to terminate before semantic
drift occurs. Thus, we have another configuration:
(f) stopping criterion.
Espresso uses Equations (1) for (a) and (2) for (c)
respectively, whereas other parameters rely on the
tasks and need calibration. Even though Espresso
greatly improves recall while keeping high precision
by using these pattern and instance scoring metrics,
Komachi and Suzuki (2008) observed that extracted
instances matched against generic patterns may be-
come erroneous after tens of iterations, showing the
difficulty of applying bootstrapping methods to dif-
ferent domains.
</bodyText>
<subsectionHeader confidence="0.850984333333333">
3 Analysis of an Espresso-like
Bootstrapping Algorithm
3.1 Simplified Espresso
</subsectionHeader>
<bodyText confidence="0.9999385">
Let us consider a simple bootstrapping algorithm
illustrated in Figure 1, in order to elucidate the cause
</bodyText>
<equation confidence="0.976648">
rπ(p) =
</equation>
<page confidence="0.733948">
1013
</page>
<bodyText confidence="0.999257333333334">
of semantic drift.
As before, let |I |and |P |be the numbers of
instances and patterns, respectively. The algo-
rithm takes a seed vector io, and a pattern-instance
co-occurrence matrix M as input. io is a |I|-
dimensional vector with 1 at the position of seed in-
stances, and 0 elsewhere. M is a |P |x |I|-matrix
whose (p, i)-element [M]pi holds the (possibly re-
weighted) number of co-occurrence of pattern p and
instance i in the corpus. If both i and p have con-
verged, the algorithm returns the pair of i and p as
output.
This algorithm, though simple, can encode
Espresso’s update formulae (1) and (2) as Steps 3
through 6 if we pose
</bodyText>
<equation confidence="0.9971335">
[M]pi = pmi(i, p)
max pmi, (4)
</equation>
<bodyText confidence="0.993880545454546">
and normalize p and i in Steps 4 and 6 by
p +— p/|I |and i +— i/|P|, (5)
respectively.
This specific instance of the algorithm of Fig-
ure 1, obtained by specialization through Equations
(4) and (5), will be henceforth referred to as Simpli-
fied Espresso. Indeed, it is an instance of the origi-
nal Espresso in which the iteration is not terminated
until convergence, all instances are carried over to
the next iteration, and instances are not cumulatively
learned.
</bodyText>
<subsectionHeader confidence="0.998992">
3.2 Simplified Espresso as Link Analysis
</subsectionHeader>
<bodyText confidence="0.99873725">
Let n denote the number of times Steps 2–10 are
iterated. Plugging (4) and (5) into Steps 3–6, we
see that the score vector of instances after the nth
iteration is
</bodyText>
<equation confidence="0.982719">
in = Ani0 (6)
</equation>
<bodyText confidence="0.851609">
where
</bodyText>
<equation confidence="0.948843">
A = |I||P|MTM.
1
(7)
</equation>
<bodyText confidence="0.930850321428571">
Suppose matrix A is irreducible; i.e., the graph
induced by taking A as the adjacency matrix is con-
nected. If n is increased and in is normalized on
each iteration, in tends to the principal eigenvec-
tor of A. This implies that no matter what seed in-
stances are input, the algorithm will end up with the
same ranking of instances, if it is run until conver-
gence. Because A = MT M
I P , the principal eigen-
vector of A is identical to the authority vector of
HITS (Kleinberg, 1999) algorithm run on the graph
induced by M. 1 This similarity of Equations (1),
(2) and HITS is not discussed in (Pantel and Pen-
nacchiotti, 2006).
As a consequence of the above discussion, se-
mantic drift in simplified Espresso seems to be in-
evitable as the iteration proceeds, since the principal
eigenvector of A need not resemble seed vector io.
A similar phenomenon is reported for HITS and is
known as topic drift, in which pages of the dominant
topic are ranked high regardless of the given query.
(Bharat and Henzinger, 1998)
Unlike HITS and Simplified Espresso, how-
ever, Espresso and other bootstrapping algo-
rithms (Yarowsky, 1995; Riloff and Jones, 1999),
incorporate heuristics so that only patterns and in-
stances with high confidence score are carried over
to the next iteration.
</bodyText>
<subsectionHeader confidence="0.997455">
3.3 Convergence Process of Espresso
</subsectionHeader>
<bodyText confidence="0.988329416666667">
To investigate the effect of semantic drift on
Espresso with and without the heuristics of selecting
the most confident instances on each iteration (i.e.,
the original Espresso and Simplified Espresso of
Section 3.2), we apply them to the task of word sense
disambiguation of word “bank” in the Senseval-3
Lexical Sample (S3LS) Task data.2 There are 394
instances of word “bank” and their occurring con-
text in this dataset, and each of them is annotated
with its true sense. Of the ten senses of bank, the
most frequent is the bank as in “bank of the river.”
We use the standard training-test split provided with
the data set.
We henceforth denote Espresso with the follow-
ing filtering strategy as Filtered Espresso to stress
the distinction from Simplified Espresso. For Fil-
tered Espresso, we cleared all but the 100 top-
scoring instances in the instance vector on each iter-
ation, and the number of non-zeroed instance scores
&apos;As long as the relative magnitude of the components of vec-
tor i. is preserved, the vector can be normalized in any way on
each iteration. Hence HITS and Simplified Espresso use differ-
ent normalization but both converge to the principal eigenvector
of A.
</bodyText>
<footnote confidence="0.542066">
zhttp://www.senseval.org/senseval3/data.html
</footnote>
<page confidence="0.988163">
1014
</page>
<bodyText confidence="0.982996933333333">
grows by 100 on each iteration. On the other hand,
we cleared all but the 20 top-scoring patterns in the
pattern vector on each iteration, and the number of
non-zeroed pattern scores grows by 1 on each iter-
ation following (Pantel and Pennacchiotti, 2006).3
The values of other parameters (b), (d), (e) and (f)
remains the same as those for simplified Espresso in
Section 3.1.
The task of WSD is to correctly predict the senses
of test instances whose true sense is hidden from the
system, using training data and their true senses. To
predict the sense of a given instance i, we apply k-
nearest neighbor algorithm.
Given a test instance i, its sense is predicted with
the following procedure:
</bodyText>
<listItem confidence="0.9978628">
1. Compute the instance-pattern matrix M from
the entire set of instances. We defer the details
of this step to Section 5.2.
2. Run Simplified- and Filtered Espresso using
the given instance i as the only seed instance.
3. After the termination of the algorithm, select k
training instances with the highest scores in the
score vector i output by the algorithm.
4. Since the selected k instances are training
instances, their true senses are accessible.
Choose the majority sense s from these k in-
stances, and output s as the prediction for the
given instance i. When there is a tie, output the
sense of the instance with the highest score in
i. Note that only Step 4 uses sense information.
</listItem>
<bodyText confidence="0.7545762">
Figure 2 shows the convergence process of
Simplified- and Filtered Espresso. X-axis indicates
the number of bootstrapping iterations and Y-axis
indicates the recall, which in this case equals pre-
cision, as the coverage is 100% in all cases.
3We conducted preliminary experiment to find these param-
eters to maximize the performance of Filtered Espresso. (These
numbers are different from the original Espresso (Pantel and
Pennacchiotti, 2006).) The number of initial patterns is rel-
atively large because of a data sparseness problem in WSD,
unlike relation extraction and named entity recognition. Also,
WSD basically uses more features than relation extraction and
thus it is hard to determine the stopping criterion based on the
number and scores of patterns, as (Pantel and Pennacchiotti,
2006) does.
</bodyText>
<figure confidence="0.997489111111111">
1
0.9
0.8
0.7
0.6
0.5
0.4
5 10 15 20 25 30
iteration
</figure>
<figureCaption confidence="0.999951">
Figure 2: Recall of Simplified- and Filtered Espresso
</figureCaption>
<bodyText confidence="0.992840516129032">
Simplified Espresso tends to select the most fre-
quent sense as the iteration proceeds, and after nine
iterations it selects the most frequent sense (“the
bank of the river”) regardless of the seed instances.
As expected from the discussion in Section 3.2,
generic patterns gradually got more weight and se-
mantic drift occurred in later iterations. Indeed, the
ranking of the instances after convergence was iden-
tical to the HITS authority ranking computed from
instance-pattern matrix M (i.e., the ranking induced
by the dominant eigenvector of MTM).
On the other hand, Filtered Espresso suffers less
from semantic drift. The final recall achieved
was 0.773 after convergence on the 20th iteration,
outperforming the most-frequent sense baseline by
0.10. However, a closer look reveals that the filter-
ing heuristics is limited in effectiveness.
Figure 3 plots the learning curve of Filtered
Espresso on the set of test instances. We show re-
call ( Icorrect instances ) of each sense to see how
|total true instances|
Filtered Espresso tends to select the most frequent
sense. If semantic drift takes place, the number
of instances predicted as the most frequent sense
should increase as the iteration proceeds, resulting
in increased recall on the most frequent sense and
decreased recall on other senses. Figure 3 exactly
exhibit this trend, meaning that Filtered Espresso is
not completely free from semantic drift. Figure 2
also shows that the recall of Filtered Espresso starts
to decay after the seventh iteration.
</bodyText>
<figure confidence="0.967599294117647">
recall of &amp;quot;bank&amp;quot;
Simplified Espresso
Filtered Espresso
most frequent sense (baseline)
1015
most frequent sense
other senses
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
5 10 15 20 25 30
iteration
</figure>
<figureCaption confidence="0.995408">
Figure 3: Recall of Filtered Espresso on the instances
having “bank of the river” and other senses
</figureCaption>
<sectionHeader confidence="0.978687" genericHeader="method">
4 Two Graph-based Algorithms for
Exploiting Generic Patterns
</sectionHeader>
<bodyText confidence="0.999949166666667">
We explore two graph-based methods which have
the advantage of Espresso to harness the property of
generic patterns by the mutual recursive definition
of instance and pattern scores. They also have less
parameters than bootstrapping, and are less prone to
semantic drift.
</bodyText>
<subsectionHeader confidence="0.992648">
4.1 Von Neumann Kernel
</subsectionHeader>
<bodyText confidence="0.932861727272727">
Kandola et al. (2002) proposed the von Neumann
kernels for measuring similarity of documents us-
ing words. If we apply the von Neumann kernels to
the pattern-instance co-occurrence matrix instead of
the document-word matrix, the relative importance
of an instance to seed instances can be estimated.
Let A = MT M be the instance similarity matrix
obtained from pattern-instance matrix M, and A be
the principal eigenvalue of A. The von Neumann
kernel matrix KO with diffusion factor Q (0 &lt; Q &lt;
A−1) is defined as follows:
</bodyText>
<equation confidence="0.957331666666667">
00
KO = A E QnAn = A(I − QA)−1_ (8)
n=0
</equation>
<bodyText confidence="0.999373135135135">
The similarity between two instances i, j is given by
the (i, j) element of KO. Hence, the i-th column
vector can be used as the score vector for seed in-
stance i.
Ito et al. (2005) showed that the von Neumann
kernels represent a mixture of the co-citation re-
latedness and Kleinberg’s HITS importance. They
compute the weighted sum of all paths between two
nodes in the co-citation graph induced by A =
MT M. The (MT M)n term of smaller n corre-
sponds to the relatedness to the seed instances, and
the (MT M)n term of larger n corresponds to HITS
importance. The von Neumann kernels calculate the
weighted sum of (MT M)n from n = 1 to oc, and
therefore smaller diffusion factor Q results in rank-
ing by relatedness, and larger Q returns ranking by
HITS importance.
In NLP literature, Sch¨utze (1998) introduced the
notion of first- and second-order co-occurrence.
First-order co-occurrence is a context which directly
co-occurs with a word, whereas second-order co-
occurrence is a context which occurs with the (con-
textual) words that co-occur with a word. Higher-
order co-occurrence information is less sparse and
more robust than lower-order co-occurrence, and
thus is useful for a proximity measure.
Given these definitions, we see that the (MT M)n
term of smaller n corresponds to lower-order co-
occurrence, which is accurate but sparse, and the
(MTM)n term of larger n corresponds to higher-
order co-occurrence, which is dense but possibly
giving too much weight on unrelated instances ex-
tracted by generic patterns.
As a result, it is expected that setting diffusion
factor Q to a small value prevents semantic drift and
also takes higher order pattern vectors into account.
We verify this claim in Section 5.3.
</bodyText>
<subsectionHeader confidence="0.96327">
4.2 Regularized Laplacian Kernel
</subsectionHeader>
<bodyText confidence="0.999908">
The von Neumann kernels can be regarded as a mix-
ture of relatedness and importance, and diffusion
factor Q controls the trade-off between relatedness
and importance. In practice, however, setting the
right parameter value becomes an issue. We solve
this problem by the regularized Laplacian (Smola
and Kondor, 2003; Chebotarev and Shamis, 1998),
which are stable across diffusion factors and can
safely benefit from generic patterns.
Let G be a weighted undirected graph whose adja-
cency (weight) matrix is a symmetric matrix A. The
(combinatorial) graph Laplacian L of a graph G is
defined as follows:
</bodyText>
<equation confidence="0.841788666666667">
L = D − A (9)
where D is a diagonal matrix, and the ith diagonal
recall
</equation>
<page confidence="0.993466">
1016
</page>
<tableCaption confidence="0.997838">
Table 1: Recall of predicted labels of bank
</tableCaption>
<equation confidence="0.83050825">
algorithm MFS others
element [D]ii is given by
�[D]ii = [A]ij.
j
</equation>
<bodyText confidence="0.997875">
Here, [A]ij stands for the (i, j) element of A. By re-
placing A with −L in Equation (8) and deleting the
first A, we obtain a regularized Laplacian kernel 4.
</bodyText>
<equation confidence="0.92831675">
00
RO = E Qn(−L)n = (j + QL)−&apos; (11)
n=0
Again, Q(&gt; 0) is called the diffusion factor.
</equation>
<bodyText confidence="0.999949733333333">
Both the regularized Laplacian and the von Neu-
mann kernels compute all the possible paths in a
graph, and consequently they can calculate influence
between nodes in a long distance in the graph. Also,
Equations (9) and (10) show that the negative Lapla-
cian −L can be regarded as a modification to the
graph G with the weight of self-loops re-weighted
to negative values. In this modified graph, if an in-
stance co-occurs with a pattern which also co-occurs
with a large number of other instances, a self-loop
of a node in the instance similarity graph induced
by MTM will receive a higher negative weight.
In other words, instances co-occurring with generic
patterns will get less weight in the regularized Lapla-
cian than in the von Neumann kernels.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.996791">
5.1 Experiment 1: Reducing Semantic Drift
</subsectionHeader>
<bodyText confidence="0.997689333333333">
We test the von Neumann kernels and the regular-
ized Laplacian on the same task as we used in Sec-
tion 3.3; i.e., word sense disambiguation of word
</bodyText>
<footnote confidence="0.596571333333333">
4It has been reported that normalization of A improves per-
formance in application (Johnson and Zhang, 2007), so we nor-
malize L by L = I − D−
</footnote>
<bodyText confidence="0.999859633333333">
“bank.” During the training phase, a pattern-instance
matrix M was constructed using the training and
testing data from Senseval-3 Lexical Sample (S3LS)
Task. The (i, j) element of M of both kernels is set
to pointwise mutual information of a pattern i and
an instance j, just the same as in Espresso. Recall is
used in evaluation.5 The diffusion parameter Q is set
to 10−5 and 10−2 for the von Neumann kernels and
the regularized Laplacian, respectively.
Table 1 illustrates how well the proposed meth-
ods reduce semantic drift, just the same as the ex-
periment of Figure 3 in Section 3.3. We evalu-
ate the recall on predicting the most frequent sense
(MFS) and the recall on predicting other less fre-
quent senses (others). For Filtered Espresso, two
results are shown: the result on the seventh iter-
ation, which maximizes the performance (Filtered
Espresso (optimal stopping)), and the one after con-
vergence. As in Section 3.3, if semantic drift oc-
curs, recall of prediction on the most frequent sense
increases while recall of prediction on other senses
declines. Even Filtered Espresso was affected by se-
mantic drift, which is again a consequence of the
inherent graphical nature of Espresso-like bootstrap-
ping algorithms. On the other hand, both proposed
methods succeeded to balance the most frequent
sense and other senses. Filtered Espresso at the op-
timal number of iterations achieved the best perfor-
mance. Nevertheless, the number of iterations has to
be estimated separately.
</bodyText>
<subsectionHeader confidence="0.998085">
5.2 Experiment 2: WSD Benchmark Data
</subsectionHeader>
<bodyText confidence="0.998010666666667">
We conducted experiments on the task of word sense
disambiguation of S3LS data, this time not just on
the word “bank” but on all target nouns in the data,
following (Agirre et al., 2006). We used two types
of patterns.
Unordered single words (bag-of-words) We
used all single words (unigrams) in the provided
context from S3LS data sets. Each word in the con-
text constructs one pattern. The pattern correspond-
ing to a word w is set to 1 if it appears in the con-
text of instance i. Words were lowercased and pre-
processed with the Porter Stemmer6.
</bodyText>
<footnote confidence="0.953493666666667">
5Again, recall equals precision in this case as the coverage
is 100% in all cases.
6http://tartarus.org/˜martin/PorterStemmer/def.txt
</footnote>
<figure confidence="0.920159272727273">
Simplified Espresso
Filtered Espresso
Filtered Espresso (optimal stopping)
von Neumann kernels
regularized Laplacian
100.0 0.0
100.0 30.2
94.4 67.4
92.1 65.1
92.1 62.8
(10)
</figure>
<page confidence="0.875979666666667">
11
2 AD− 2 .
1017
</page>
<tableCaption confidence="0.999748">
Table 2: Comparison of WSD algorithms
</tableCaption>
<table confidence="0.956313">
algorithm Recall
most frequent sense 54.5
HyperLex (V´eronis, 2004) 64.6
PageRank (Agirre et al., 2006) 64.5
Simplified Espresso 44.1
Filtered Espresso 46.9
Filtered Espresso (optimal stopping) 66.5
von Neumann kernels (0 = 10−5) 67.2
regularized Laplacian (0 = 10−2) 67.1
Local collocations A local collocation refers to
</table>
<bodyText confidence="0.998570818181818">
the ordered sequence of tokens in the local, narrow
context of the target word. We allowed a pattern to
have wildcard expressions like “sale of * interest in
* *” for the target word interest. We set the window
size to f3 by a preliminary experiment.
We report the results of Filtered Espresso both af-
ter convergence, and with its optimal number of iter-
ations to show the upper bound of its performance.
Table 2 compares proposed methods with
Espresso with various configurations. The proposed
methods outperform by a large margin the most fre-
quent sense baseline and both Simplified- and Fil-
tered Espresso. This means that the proposed meth-
ods effectively prevent semantic drift.
Also, Filtered Espresso without early stopping
shows more or less identical performance to Sim-
plified Espresso. It is implied that the heuristics of
filtering and early stopping is a crucial step not to
select generic patterns in Espresso, and the result is
consistent with the experiment of convergence pro-
cess of Espresso in Section 3.3.
Filtered Espresso halted after the seventh itera-
tion (Filtered Espresso (optimal stopping)) is com-
parable to the proposed methods. However, in boot-
strapping, not only the number of iterations but also
a large number of parameters must be adjusted for
each task and domain. This shortcoming makes it
hard to adapt bootstrapping in practical cases. One
of the main advantages of the proposed methods is
that they have only one parameter β and are much
easier to tune.
It is suggested in Sections 3.3 and 4.1 that
Espresso and the von Neumann kernel with large β
</bodyText>
<figureCaption confidence="0.91528">
Figure 4: Recall of the von Neumann kernels with a dif-
ferent diffusion factor 0 on S3LS WSD task
</figureCaption>
<bodyText confidence="0.9999418">
converge to the principal eigenvector of A, though
the result does not seem to support this claim (both
Simplified- and Filtered Espresso are 10 points
lower than the most frequent sense baseline). The
reason seems to be because Espresso and the von
Neumann kernels use pointwise mutual information
as a weighting factor so that the principal eigenvec-
tor of A may not always represent the most frequent
sense.7
We also show the results of previous graph-based
methods (Agirre et al., 2006), based on Hyper-
Lex (V´eronis, 2004) and PageRank (Brin and Page,
1998). The experimental set-up is the same as ours
in that they do not use the sense tags of training cor-
pus to construct a co-occurrence graph, and they use
the sense tags of all the S3LS training corpus for
mapping senses to clusters. However, these meth-
ods have seven parameters to tune in order to achieve
the best performance, and hence are difficult to opti-
mize.
</bodyText>
<sectionHeader confidence="0.6177905" genericHeader="method">
5.3 Experiment 3: Sensitivity to a Different
Diffusion Factor
</sectionHeader>
<bodyText confidence="0.9997348">
Figure 4 shows the performance of the von Neu-
mann kernels with a diffusion factor β. As ex-
pected, smaller β leads to relatedness to seed in-
stances, and larger β asymptotically converges to the
HITS authority ranking (or equivalently, Simplified
</bodyText>
<footnote confidence="0.905428333333333">
7A similar but more extreme case is described in (Ito et al.,
2005) in which the use of a normalized weight matrix M results
in an unintuitive principal eigenvector.
</footnote>
<figure confidence="0.9570494">
1e-07 1e-06 1e-05 0.0001 0.001
diffusion factor
recall
45
40
75
70
65
60
55
50
von Neumann kernel
Simplified Espresso
most frequent sense
1018
</figure>
<figureCaption confidence="0.946954">
Figure 5: Recall of the regularized Laplacian with a dif-
ferent diffusion factor 0 on S3LS WSD task
</figureCaption>
<bodyText confidence="0.998820142857143">
Espresso).
One of the disadvantages of the von Neumann
kernels over the regularized Laplacian is their sen-
sitivity to parameter Q. Figure 5 illustrates the per-
formance of the regularized Laplacian with a diffu-
sion factor Q. The regularized Laplacian is stable for
various values of Q, while the von Neumann kernels
change their behavior drastically depending on the
value of Q. However, Q in the von Neumann kernels
is upper-bounded by the reciprocal 1/A of the prin-
cipal eigenvalue of A, and the derivatives of kernel
matrices with respect to Q can be used to guide sys-
tematic calibration of Q (see (Ito et al., 2005) for
detail).
</bodyText>
<sectionHeader confidence="0.996123" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999988605263158">
This paper gives a graph-based analysis of seman-
tic drift in Espresso-like bootstrapping algorithms.
We indicate that semantic drift in bootstrapping is a
parallel to topic drift in HITS. We confirm that the
von Neumann kernels and the regularized Laplacian
reduce semantic drift in the Senseval-3 Lexical Sam-
ple task. Our proposed methods have only one pa-
rameters and are easy to calibrate.
Beside the regularized Laplacian, many other ker-
nels based on the eigenvalue regularization of the
Laplacian matrix have been proposed in machine
learning community (Kondor and Lafferty, 2002;
Nadler et al., 2006; Saerens et al., 2004). One such
kernel is the commute-time kernel (Saerens et al.,
2004) defined as the pseudo-inverse of Laplacian.
Despite having no parameters at all, it has been re-
ported to perform well in many collaborative filter-
ing tasks (Fouss et al., 2007). We plan to test these
kernels in our task as well.
Another research topic is to investigate other
semi-supervised learning techniques such as co-
training (Blum and Mitchell, 1998). As we have
described in this paper, self-training can be thought
of a graph-based algorithm. It is also interesting to
analyze how co-training is related to the proposed
algorithm.
Bootstrapping algorithms have been used in many
NLP applications. Two major tasks of bootstrap-
ping are word sense disambiguation and named en-
tity recognition. In named entity recognition task,
instances are usually retained on each iteration and
added to seed instance set. This seems to be be-
cause named entity recognition suffers from seman-
tic drift more severely than word sense disambigua-
tion. Even though this problem setting is different
from ours, it needs to be verified that the graph-
based approaches presented in this paper are also ef-
fective in named entity recognition.
</bodyText>
<sectionHeader confidence="0.994588" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999208">
We thank anonymous reviewers for helpful com-
ments and for making us aware of Abney’s work.
The first author is partially supported by the Japan
Society for Promotion of Science (JSPS), Grant-in-
Aid for JSPS Fellows.
</bodyText>
<sectionHeader confidence="0.997932" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.975544875">
Steven Abney. 2004. Understanding the Yarowsky Al-
gorithm. Computational Linguistics, 30(3):365–395.
Eneko Agirre, David Martinez, Oier L´opez de Lacalle,
and Aitor Soroa. 2006. Two graph-based algorithms
for state-of-the-art WSD. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 585–593.
Michele Banko and Eric Brill. 2001. Scaling to Very
Very Large Corpora for Natural Language Disam-
biguation. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, pages
26–33.
Krishna Bharat and Monika R. Henzinger. 1998. Im-
proved algorithms for topic distillation in a hyper-
linked environment. In Proceedings of the 21st ACM
SIGIR Conference.
</reference>
<figure confidence="0.995396153846154">
0.001 0.01 0.1 1 10 100 1000
diffusion factor
45
40
75
70
65
60
55
50
regularized Laplacian
most frequent sense
recall
</figure>
<page confidence="0.977128">
1019
</page>
<reference confidence="0.999499625">
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In Pro-
ceedings of the Workshop on Computational Learning
Theory (COLT), pages 92–100. Morgan Kaufmann.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1–7):107–117.
Pavel Yu Chebotarev and Elena V. Shamis. 1998. On
proximity measures for graph vertices. Automation
and Remote Control, 59(10):1443–1459.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. In Pro-
ceedings of the Joint SIGDAT Conference on Empiri-
cal Methods in Natural Language Processing and Very
Large Corpora, pages 100–110.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with Mutual Exclu-
sion Bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 172–180.
Franc¸ois Fouss, Luh Yen, Pierr Dupont, and Marco
Saerens. 2007. Random-walk computation of simi-
larities between nodes of a graph with application to
collaborative recommendation. IEEE Transactions on
Knowledge and Data Engineering, 19(3):355–369.
Marti Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics, pages 539–545.
Takahiko Ito, Masashi Shimbo, Taku Kudo, and Yuji
Matsumoto. 2005. Application of Kernels to
Link Analysis. In Proceedings of the Eleventh
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 586–592.
Rie Johnson and Tong Zhang. 2007. On the Effec-
tiveness of Laplacian Normalization for Graph Semi-
supervised Learning. Journal of Machine Learning
Research, 8:1489–1517.
Jaz Kandola, John Shawe-Taylor, and Nello Cristianini.
2002. Learning Semantic Similarity. In Advances
in Neural Information Processing Systems 15, pages
657–664.
Jon Kleinberg. 1999. Authoritative Sources in a Hyper-
linked Environment. Journal of the ACM, 46(5):604–
632.
Mamoru Komachi and Hisami Suzuki. 2008. Mini-
mally Supervised Learning of Semantic Knowledge
from Query Logs. In Proceedings of the 3rd Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 358–365.
Risi Imre Kondor and John Lafferty. 2002. Diffusion
kernels on graphs and other discrete input spaces. In
Proceedings of the 19th International Conference on
Machine Learning (ICML-2002).
Hang Li and Cong Li. 2004. Word Translation Disam-
biguation Using Bilingual Bootstrapping. Computa-
tional Linguistics, 30(1):1–22.
Boaz Nadler, Stephane Lafon, Ronald Coifman, and
Ioannis Kevrekidis. 2006. Diffusion maps, spectral
clustering and eigenfunctions of fokker-planck opera-
tors. Advances in Neural Information Processing Sys-
tems 18, pages 955–962.
Vincent Ng and Claire Cardie. 2003. Weakly Su-
pervised Natural Language Learning Without Redun-
dant Views. In Proceedings of the HLT-NAACL 2003,
pages 94–101.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging Generic Patterns for Automatically Har-
vesting Semantic Relations. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the ACL, pages
113–120.
Ellen Riloff and Rosie Jones. 1999. Learning Dic-
tionaries for Information Extraction by Multi-Level
Bootstrapping. In Proceedings of the Sixteenth Na-
tional Conference on Artificial Intellligence (AAAI-
99), pages 474–479.
Marco Saerens, Franc¸ois Fouss, Luh Yen, and Pierre
Dupont. 2004. The principal component analysis
of a graph, and its relationship to spectral clustering.
In Proceedings of European Conference on Machine
Learning (ECML 2004), pages 371–383. Springer.
Heinrich Sch¨utze. 1998. Automatic Word Sense Dis-
crimination. Computational Linguistics, 24(1):97–
123.
Alex J. Smola and Risi Imre Kondor. 2003. Kernels and
Regularization of Graphs. In Proceedings of the 16th
Annual Conference on Learning Theory, pages 144–
158.
Jean V´eronis. 2004. HyperLex: Lexical Cartography for
Information Retrieval. Computer Speech &amp; Language,
18(3):223–252.
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 189–196.
</reference>
<page confidence="0.990701">
1020
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.332655">
<title confidence="0.986978">Graph-based Analysis of Semantic Drift in Bootstrapping Algorithms</title>
<author confidence="0.991259">Mamoru Komachi Taku Kudo Masashi Shimbo Yuji Matsumoto</author>
<affiliation confidence="0.595077">NAIST, Japan Google Inc. NAIST, Japan NAIST, Japan</affiliation>
<email confidence="0.632423">mamoru-k@is.naist.jptaku@google.comshimbo@is.naist.jpmatsu@is.naist.jp</email>
<abstract confidence="0.993687111111111">has a tendency, called semanto select instances unrelated to the seed instances as the iteration proceeds. We demonstrate the semantic drift of bootstrapping has the same root as the topic drift of Kleinberg’s HITS, using a simplified graphbased reformulation of bootstrapping. We confirm that two graph-based algorithms, the von Neumann kernels and the regularized Laplacian, can reduce semantic drift in the task of word sense disambiguation (WSD) on Senseval-3 English Lexical Sample Task. Proposed algorithms achieve superior perforto previous graph-based WSD methods, even though the proposed algorithms have less parameters and are easy to calibrate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Understanding the Yarowsky Algorithm.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="5705" citStr="Abney (2004)" startWordPosition="858" endWordPosition="859">eval-3 Lexical Sample Task and verify that they indeed reduce semantic drift. Finally, we conclude our work in Section 6. 2 Related Work 2.1 Overview of Bootstrapping Bootstrapping (or self-training) is a general framework for reducing the requirement of manual annotation. Hearst (1992) described a bootstrapping procedure for extracting words in hyponym (is-a) relation, starting with three manually given lexicosyntactic patterns. The idea of learning with a bootstrapping method was adopted for many tasks. Yarowsky (1995) presented an unsupervised WSD system which rivals supervised techniques. Abney (2004) presented a thorough discussion on the Yarowsky algorithm. He extended the original Yarowsky algorithm to a new family of bootstrapping algorithms that are mathematically well understood. Li and Li (2004) proposed a method called Bilingual Bootstrapping. It makes use of a translation dictionary and a comparable corpus to help disambiguate word senses in the source language, by exploiting the asymmetric many-to-many sense mapping relationship between words in two languages. Curran et al. (2007) presented an algorithm called Mutual Exclusion Bootstrapping, which minimizes semantic drift using m</context>
</contexts>
<marker>Abney, 2004</marker>
<rawString>Steven Abney. 2004. Understanding the Yarowsky Algorithm. Computational Linguistics, 30(3):365–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
<author>Oier L´opez de Lacalle</author>
<author>Aitor Soroa</author>
</authors>
<title>Two graph-based algorithms for state-of-the-art WSD.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>585--593</pages>
<marker>Agirre, Martinez, de Lacalle, Soroa, 2006</marker>
<rawString>Eneko Agirre, David Martinez, Oier L´opez de Lacalle, and Aitor Soroa. 2006. Two graph-based algorithms for state-of-the-art WSD. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 585–593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to Very Very Large Corpora for Natural Language Disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="6900" citStr="Banko and Brill, 2001" startWordPosition="1045" endWordPosition="1048">izes semantic drift using mutual exclusion between semantic classes of learned instances. They prepared a list of so-called stop classes similar to a stop word list used in information retrieval to help bound the semantic classes. Stop classes are sets of terms known to cause semantic drift in particular semantic classes. However, stop classes vary from task to task and domain to domain, and human intervention is essential to create an effective list of stop classes. A major drawback of bootstrapping is the lack of principled method for selecting optimal parameter values (Ng and Cardie, 2003; Banko and Brill, 2001). Also, there is an issue of generic patterns which deteriorates the quality of acquired instances. Previously proposed bootstrapping algorithms differ in how they deal with the problem of semantic drift. We will take recently proposed Espresso algorithm as the example to explain common configuration for bootstrapping in detail. 2.2 The Espresso Algorithm Pantel and Pennachiotti (2006) proposed a bootstrapping algorithm called Espresso to learn binary semantic relations such as is-a and part-of from a corpus. What distinguishes Espresso from other bootstrapping algorithms is that it benefits f</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to Very Very Large Corpora for Natural Language Disambiguation. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krishna Bharat</author>
<author>Monika R Henzinger</author>
</authors>
<title>Improved algorithms for topic distillation in a hyperlinked environment.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st ACM SIGIR Conference.</booktitle>
<contexts>
<context position="13559" citStr="Bharat and Henzinger, 1998" startWordPosition="2161" endWordPosition="2164"> P , the principal eigenvector of A is identical to the authority vector of HITS (Kleinberg, 1999) algorithm run on the graph induced by M. 1 This similarity of Equations (1), (2) and HITS is not discussed in (Pantel and Pennacchiotti, 2006). As a consequence of the above discussion, semantic drift in simplified Espresso seems to be inevitable as the iteration proceeds, since the principal eigenvector of A need not resemble seed vector io. A similar phenomenon is reported for HITS and is known as topic drift, in which pages of the dominant topic are ranked high regardless of the given query. (Bharat and Henzinger, 1998) Unlike HITS and Simplified Espresso, however, Espresso and other bootstrapping algorithms (Yarowsky, 1995; Riloff and Jones, 1999), incorporate heuristics so that only patterns and instances with high confidence score are carried over to the next iteration. 3.3 Convergence Process of Espresso To investigate the effect of semantic drift on Espresso with and without the heuristics of selecting the most confident instances on each iteration (i.e., the original Espresso and Simplified Espresso of Section 3.2), we apply them to the task of word sense disambiguation of word “bank” in the Senseval-3</context>
</contexts>
<marker>Bharat, Henzinger, 1998</marker>
<rawString>Krishna Bharat and Monika R. Henzinger. 1998. Improved algorithms for topic distillation in a hyperlinked environment. In Proceedings of the 21st ACM SIGIR Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Computational Learning Theory (COLT),</booktitle>
<pages>92--100</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="31755" citStr="Blum and Mitchell, 1998" startWordPosition="5210" endWordPosition="5213">any other kernels based on the eigenvalue regularization of the Laplacian matrix have been proposed in machine learning community (Kondor and Lafferty, 2002; Nadler et al., 2006; Saerens et al., 2004). One such kernel is the commute-time kernel (Saerens et al., 2004) defined as the pseudo-inverse of Laplacian. Despite having no parameters at all, it has been reported to perform well in many collaborative filtering tasks (Fouss et al., 2007). We plan to test these kernels in our task as well. Another research topic is to investigate other semi-supervised learning techniques such as cotraining (Blum and Mitchell, 1998). As we have described in this paper, self-training can be thought of a graph-based algorithm. It is also interesting to analyze how co-training is related to the proposed algorithm. Bootstrapping algorithms have been used in many NLP applications. Two major tasks of bootstrapping are word sense disambiguation and named entity recognition. In named entity recognition task, instances are usually retained on each iteration and added to seed instance set. This seems to be because named entity recognition suffers from semantic drift more severely than word sense disambiguation. Even though this pr</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining Labeled and Unlabeled Data with Co-Training. In Proceedings of the Workshop on Computational Learning Theory (COLT), pages 92–100. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual Web search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<pages>30--1</pages>
<contexts>
<context position="28964" citStr="Brin and Page, 1998" startWordPosition="4739" endWordPosition="4742">ernels with a different diffusion factor 0 on S3LS WSD task converge to the principal eigenvector of A, though the result does not seem to support this claim (both Simplified- and Filtered Espresso are 10 points lower than the most frequent sense baseline). The reason seems to be because Espresso and the von Neumann kernels use pointwise mutual information as a weighting factor so that the principal eigenvector of A may not always represent the most frequent sense.7 We also show the results of previous graph-based methods (Agirre et al., 2006), based on HyperLex (V´eronis, 2004) and PageRank (Brin and Page, 1998). The experimental set-up is the same as ours in that they do not use the sense tags of training corpus to construct a co-occurrence graph, and they use the sense tags of all the S3LS training corpus for mapping senses to clusters. However, these methods have seven parameters to tune in order to achieve the best performance, and hence are difficult to optimize. 5.3 Experiment 3: Sensitivity to a Different Diffusion Factor Figure 4 shows the performance of the von Neumann kernels with a diffusion factor β. As expected, smaller β leads to relatedness to seed instances, and larger β asymptoticall</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems, 30(1–7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Yu Chebotarev</author>
<author>Elena V Shamis</author>
</authors>
<title>On proximity measures for graph vertices. Automation and Remote Control,</title>
<date>1998</date>
<pages>59--10</pages>
<contexts>
<context position="22183" citStr="Chebotarev and Shamis, 1998" startWordPosition="3588" endWordPosition="3591"> on unrelated instances extracted by generic patterns. As a result, it is expected that setting diffusion factor Q to a small value prevents semantic drift and also takes higher order pattern vectors into account. We verify this claim in Section 5.3. 4.2 Regularized Laplacian Kernel The von Neumann kernels can be regarded as a mixture of relatedness and importance, and diffusion factor Q controls the trade-off between relatedness and importance. In practice, however, setting the right parameter value becomes an issue. We solve this problem by the regularized Laplacian (Smola and Kondor, 2003; Chebotarev and Shamis, 1998), which are stable across diffusion factors and can safely benefit from generic patterns. Let G be a weighted undirected graph whose adjacency (weight) matrix is a symmetric matrix A. The (combinatorial) graph Laplacian L of a graph G is defined as follows: L = D − A (9) where D is a diagonal matrix, and the ith diagonal recall 1016 Table 1: Recall of predicted labels of bank algorithm MFS others element [D]ii is given by �[D]ii = [A]ij. j Here, [A]ij stands for the (i, j) element of A. By replacing A with −L in Equation (8) and deleting the first A, we obtain a regularized Laplacian kernel 4.</context>
</contexts>
<marker>Chebotarev, Shamis, 1998</marker>
<rawString>Pavel Yu Chebotarev and Elena V. Shamis. 1998. On proximity measures for graph vertices. Automation and Remote Control, 59(10):1443–1459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>100--110</pages>
<contexts>
<context position="2044" citStr="Collins and Singer, 1999" startWordPosition="295" endWordPosition="298">e problem of hand-tagging resources, semi(or minimally)-supervised and unsupervised techniques have been actively studied. Hearst (1992) first presented a bootstrapping method which requires only a small amount of instances (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction. Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999) and relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006). However, it is known that bootstrapping often acquires instances not related to seed instances. For example, consider the task of collecting the names of common tourist sites from web corpora. Given words like “Geneva” and “Bali” as seed instances, bootstrapping would eventually learn generic patterns such as “pictures” and “photos,” which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-occur with these generic patterns, such as “Britney Sp</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised Models for Named Entity Classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 100–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Tara Murphy</author>
<author>Bernhard Scholz</author>
</authors>
<title>Minimising semantic drift with Mutual Exclusion Bootstrapping.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics,</booktitle>
<pages>172--180</pages>
<contexts>
<context position="2713" citStr="Curran et al., 2007" startWordPosition="394" endWordPosition="397"> Pantel and Pennacchiotti, 2006). However, it is known that bootstrapping often acquires instances not related to seed instances. For example, consider the task of collecting the names of common tourist sites from web corpora. Given words like “Geneva” and “Bali” as seed instances, bootstrapping would eventually learn generic patterns such as “pictures” and “photos,” which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-occur with these generic patterns, such as “Britney Spears.” This phenomenon is called semantic drift (Curran et al., 2007). A straightforward approach to avoid semantic drift is to terminate iterations before hitting generic patterns, but the optimal number of iterations is task dependent and is hard to come by. The recently proposed Espresso (Pantel and Pennacchiotti, 2006) algorithm incorporates sophisticated scoring functions to cope with generic patterns, but as Komachi and Suzuki (2008) pointed out, Espresso still shows semantic drift unless iterations are terminated appropriately. Another deficiency in bootstrapping is its sensitivity to many parameters such as the number of 1011 Proceedings of the 2008 Con</context>
<context position="6204" citStr="Curran et al. (2007)" startWordPosition="934" endWordPosition="937">ted for many tasks. Yarowsky (1995) presented an unsupervised WSD system which rivals supervised techniques. Abney (2004) presented a thorough discussion on the Yarowsky algorithm. He extended the original Yarowsky algorithm to a new family of bootstrapping algorithms that are mathematically well understood. Li and Li (2004) proposed a method called Bilingual Bootstrapping. It makes use of a translation dictionary and a comparable corpus to help disambiguate word senses in the source language, by exploiting the asymmetric many-to-many sense mapping relationship between words in two languages. Curran et al. (2007) presented an algorithm called Mutual Exclusion Bootstrapping, which minimizes semantic drift using mutual exclusion between semantic classes of learned instances. They prepared a list of so-called stop classes similar to a stop word list used in information retrieval to help bound the semantic classes. Stop classes are sets of terms known to cause semantic drift in particular semantic classes. However, stop classes vary from task to task and domain to domain, and human intervention is essential to create an effective list of stop classes. A major drawback of bootstrapping is the lack of princ</context>
</contexts>
<marker>Curran, Murphy, Scholz, 2007</marker>
<rawString>James R. Curran, Tara Murphy, and Bernhard Scholz. 2007. Minimising semantic drift with Mutual Exclusion Bootstrapping. In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics, pages 172–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Fouss</author>
<author>Luh Yen</author>
<author>Pierr Dupont</author>
<author>Marco Saerens</author>
</authors>
<title>Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation.</title>
<date>2007</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="31575" citStr="Fouss et al., 2007" startWordPosition="5181" endWordPosition="5184">acian reduce semantic drift in the Senseval-3 Lexical Sample task. Our proposed methods have only one parameters and are easy to calibrate. Beside the regularized Laplacian, many other kernels based on the eigenvalue regularization of the Laplacian matrix have been proposed in machine learning community (Kondor and Lafferty, 2002; Nadler et al., 2006; Saerens et al., 2004). One such kernel is the commute-time kernel (Saerens et al., 2004) defined as the pseudo-inverse of Laplacian. Despite having no parameters at all, it has been reported to perform well in many collaborative filtering tasks (Fouss et al., 2007). We plan to test these kernels in our task as well. Another research topic is to investigate other semi-supervised learning techniques such as cotraining (Blum and Mitchell, 1998). As we have described in this paper, self-training can be thought of a graph-based algorithm. It is also interesting to analyze how co-training is related to the proposed algorithm. Bootstrapping algorithms have been used in many NLP applications. Two major tasks of bootstrapping are word sense disambiguation and named entity recognition. In named entity recognition task, instances are usually retained on each itera</context>
</contexts>
<marker>Fouss, Yen, Dupont, Saerens, 2007</marker>
<rawString>Franc¸ois Fouss, Luh Yen, Pierr Dupont, and Marco Saerens. 2007. Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. IEEE Transactions on Knowledge and Data Engineering, 19(3):355–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="1555" citStr="Hearst (1992)" startWordPosition="225" endWordPosition="226">s and are easy to calibrate. 1 Introduction In recent years machine learning techniques become widely used in natural language processing (NLP). These techniques offer various ways to exploit large corpora and are known to perform well in many tasks. However, these techniques often require tagged corpora, which are not readily available to many languages. So far, reducing the cost of human annotation is one of the important problems for building NLP systems. To mitigate the problem of hand-tagging resources, semi(or minimally)-supervised and unsupervised techniques have been actively studied. Hearst (1992) first presented a bootstrapping method which requires only a small amount of instances (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction. Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999) and relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006). However, it is known that bo</context>
<context position="5380" citStr="Hearst (1992)" startWordPosition="812" endWordPosition="813"> drift, but these heuristics have limited effect as we demonstrate in Section 3.3. In Section 4, we propose two graph-based algorithms to reduce semantic drift. These algorithms are used in link analysis community to reduce the effect of topic drift. In Section 5 we apply them to the task of word sense disambiguation on Senseval-3 Lexical Sample Task and verify that they indeed reduce semantic drift. Finally, we conclude our work in Section 6. 2 Related Work 2.1 Overview of Bootstrapping Bootstrapping (or self-training) is a general framework for reducing the requirement of manual annotation. Hearst (1992) described a bootstrapping procedure for extracting words in hyponym (is-a) relation, starting with three manually given lexicosyntactic patterns. The idea of learning with a bootstrapping method was adopted for many tasks. Yarowsky (1995) presented an unsupervised WSD system which rivals supervised techniques. Abney (2004) presented a thorough discussion on the Yarowsky algorithm. He extended the original Yarowsky algorithm to a new family of bootstrapping algorithms that are mathematically well understood. Li and Li (2004) proposed a method called Bilingual Bootstrapping. It makes use of a t</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In Proceedings of the Fourteenth International Conference on Computational Linguistics, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takahiko Ito</author>
<author>Masashi Shimbo</author>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Application of Kernels to Link Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>586--592</pages>
<contexts>
<context position="20275" citStr="Ito et al. (2005)" startWordPosition="3277" endWordPosition="3280">n Neumann kernels to the pattern-instance co-occurrence matrix instead of the document-word matrix, the relative importance of an instance to seed instances can be estimated. Let A = MT M be the instance similarity matrix obtained from pattern-instance matrix M, and A be the principal eigenvalue of A. The von Neumann kernel matrix KO with diffusion factor Q (0 &lt; Q &lt; A−1) is defined as follows: 00 KO = A E QnAn = A(I − QA)−1_ (8) n=0 The similarity between two instances i, j is given by the (i, j) element of KO. Hence, the i-th column vector can be used as the score vector for seed instance i. Ito et al. (2005) showed that the von Neumann kernels represent a mixture of the co-citation relatedness and Kleinberg’s HITS importance. They compute the weighted sum of all paths between two nodes in the co-citation graph induced by A = MT M. The (MT M)n term of smaller n corresponds to the relatedness to the seed instances, and the (MT M)n term of larger n corresponds to HITS importance. The von Neumann kernels calculate the weighted sum of (MT M)n from n = 1 to oc, and therefore smaller diffusion factor Q results in ranking by relatedness, and larger Q returns ranking by HITS importance. In NLP literature,</context>
<context position="29702" citStr="Ito et al., 2005" startWordPosition="4868" endWordPosition="4871">-occurrence graph, and they use the sense tags of all the S3LS training corpus for mapping senses to clusters. However, these methods have seven parameters to tune in order to achieve the best performance, and hence are difficult to optimize. 5.3 Experiment 3: Sensitivity to a Different Diffusion Factor Figure 4 shows the performance of the von Neumann kernels with a diffusion factor β. As expected, smaller β leads to relatedness to seed instances, and larger β asymptotically converges to the HITS authority ranking (or equivalently, Simplified 7A similar but more extreme case is described in (Ito et al., 2005) in which the use of a normalized weight matrix M results in an unintuitive principal eigenvector. 1e-07 1e-06 1e-05 0.0001 0.001 diffusion factor recall 45 40 75 70 65 60 55 50 von Neumann kernel Simplified Espresso most frequent sense 1018 Figure 5: Recall of the regularized Laplacian with a different diffusion factor 0 on S3LS WSD task Espresso). One of the disadvantages of the von Neumann kernels over the regularized Laplacian is their sensitivity to parameter Q. Figure 5 illustrates the performance of the regularized Laplacian with a diffusion factor Q. The regularized Laplacian is stable</context>
</contexts>
<marker>Ito, Shimbo, Kudo, Matsumoto, 2005</marker>
<rawString>Takahiko Ito, Masashi Shimbo, Taku Kudo, and Yuji Matsumoto. 2005. Application of Kernels to Link Analysis. In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 586–592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Johnson</author>
<author>Tong Zhang</author>
</authors>
<title>On the Effectiveness of Laplacian Normalization for Graph Semisupervised Learning.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>8--1489</pages>
<contexts>
<context position="23940" citStr="Johnson and Zhang, 2007" startWordPosition="3902" endWordPosition="3905">co-occurs with a large number of other instances, a self-loop of a node in the instance similarity graph induced by MTM will receive a higher negative weight. In other words, instances co-occurring with generic patterns will get less weight in the regularized Laplacian than in the von Neumann kernels. 5 Experiments and Results 5.1 Experiment 1: Reducing Semantic Drift We test the von Neumann kernels and the regularized Laplacian on the same task as we used in Section 3.3; i.e., word sense disambiguation of word 4It has been reported that normalization of A improves performance in application (Johnson and Zhang, 2007), so we normalize L by L = I − D− “bank.” During the training phase, a pattern-instance matrix M was constructed using the training and testing data from Senseval-3 Lexical Sample (S3LS) Task. The (i, j) element of M of both kernels is set to pointwise mutual information of a pattern i and an instance j, just the same as in Espresso. Recall is used in evaluation.5 The diffusion parameter Q is set to 10−5 and 10−2 for the von Neumann kernels and the regularized Laplacian, respectively. Table 1 illustrates how well the proposed methods reduce semantic drift, just the same as the experiment of Fi</context>
</contexts>
<marker>Johnson, Zhang, 2007</marker>
<rawString>Rie Johnson and Tong Zhang. 2007. On the Effectiveness of Laplacian Normalization for Graph Semisupervised Learning. Journal of Machine Learning Research, 8:1489–1517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaz Kandola</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Learning Semantic Similarity.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems 15,</booktitle>
<pages>657--664</pages>
<contexts>
<context position="19555" citStr="Kandola et al. (2002)" startWordPosition="3144" endWordPosition="3147"> Filtered Espresso most frequent sense (baseline) 1015 most frequent sense other senses 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 5 10 15 20 25 30 iteration Figure 3: Recall of Filtered Espresso on the instances having “bank of the river” and other senses 4 Two Graph-based Algorithms for Exploiting Generic Patterns We explore two graph-based methods which have the advantage of Espresso to harness the property of generic patterns by the mutual recursive definition of instance and pattern scores. They also have less parameters than bootstrapping, and are less prone to semantic drift. 4.1 Von Neumann Kernel Kandola et al. (2002) proposed the von Neumann kernels for measuring similarity of documents using words. If we apply the von Neumann kernels to the pattern-instance co-occurrence matrix instead of the document-word matrix, the relative importance of an instance to seed instances can be estimated. Let A = MT M be the instance similarity matrix obtained from pattern-instance matrix M, and A be the principal eigenvalue of A. The von Neumann kernel matrix KO with diffusion factor Q (0 &lt; Q &lt; A−1) is defined as follows: 00 KO = A E QnAn = A(I − QA)−1_ (8) n=0 The similarity between two instances i, j is given by the (i</context>
</contexts>
<marker>Kandola, Shawe-Taylor, Cristianini, 2002</marker>
<rawString>Jaz Kandola, John Shawe-Taylor, and Nello Cristianini. 2002. Learning Semantic Similarity. In Advances in Neural Information Processing Systems 15, pages 657–664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Kleinberg</author>
</authors>
<title>Authoritative Sources in a Hyperlinked Environment.</title>
<date>1999</date>
<journal>Journal of the ACM,</journal>
<volume>46</volume>
<issue>5</issue>
<pages>632</pages>
<contexts>
<context position="4269" citStr="Kleinberg, 1999" startWordPosition="631" endWordPosition="632">. In this paper, we present a graph-theoretic analysis of Espresso-like bootstrapping algorithms. We argue that semantic drift is inherent in these algorithms, and propose to use two graph-based algorithms that are theoretically less prone to semantic drift, as an alternative to bootstrapping. After a brief review of related work in Section 2, we analyze in Section 3 a bootstrapping algorithm (Simplified Espresso) which can be thought of as a degenerate version of Espresso. Simplified Espresso is simple enough to allow an algebraic treatment, and its equivalence to Kleinberg’s HITS algorithm (Kleinberg, 1999) is shown. An implication of this equivalence is that semantic drift in this bootstrapping algorithm is essentially the same phenomenon as topic drift observed in link analysis. Another implication is that semantic drift is inevitable in Simplified Espresso as it converges to the same score vector regardless of seed instances. The original Espresso also suffers from the same problem as its simplified version does. It incorporates heuristics not present in Simplified Espresso to reduce semantic drift, but these heuristics have limited effect as we demonstrate in Section 3.3. In Section 4, we pr</context>
<context position="13030" citStr="Kleinberg, 1999" startWordPosition="2071" endWordPosition="2072">(4) and (5) into Steps 3–6, we see that the score vector of instances after the nth iteration is in = Ani0 (6) where A = |I||P|MTM. 1 (7) Suppose matrix A is irreducible; i.e., the graph induced by taking A as the adjacency matrix is connected. If n is increased and in is normalized on each iteration, in tends to the principal eigenvector of A. This implies that no matter what seed instances are input, the algorithm will end up with the same ranking of instances, if it is run until convergence. Because A = MT M I P , the principal eigenvector of A is identical to the authority vector of HITS (Kleinberg, 1999) algorithm run on the graph induced by M. 1 This similarity of Equations (1), (2) and HITS is not discussed in (Pantel and Pennacchiotti, 2006). As a consequence of the above discussion, semantic drift in simplified Espresso seems to be inevitable as the iteration proceeds, since the principal eigenvector of A need not resemble seed vector io. A similar phenomenon is reported for HITS and is known as topic drift, in which pages of the dominant topic are ranked high regardless of the given query. (Bharat and Henzinger, 1998) Unlike HITS and Simplified Espresso, however, Espresso and other boots</context>
</contexts>
<marker>Kleinberg, 1999</marker>
<rawString>Jon Kleinberg. 1999. Authoritative Sources in a Hyperlinked Environment. Journal of the ACM, 46(5):604– 632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mamoru Komachi</author>
<author>Hisami Suzuki</author>
</authors>
<title>Minimally Supervised Learning of Semantic Knowledge from Query Logs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing,</booktitle>
<pages>358--365</pages>
<contexts>
<context position="3087" citStr="Komachi and Suzuki (2008)" startWordPosition="450" endWordPosition="453"> which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-occur with these generic patterns, such as “Britney Spears.” This phenomenon is called semantic drift (Curran et al., 2007). A straightforward approach to avoid semantic drift is to terminate iterations before hitting generic patterns, but the optimal number of iterations is task dependent and is hard to come by. The recently proposed Espresso (Pantel and Pennacchiotti, 2006) algorithm incorporates sophisticated scoring functions to cope with generic patterns, but as Komachi and Suzuki (2008) pointed out, Espresso still shows semantic drift unless iterations are terminated appropriately. Another deficiency in bootstrapping is its sensitivity to many parameters such as the number of 1011 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1011–1020, Honolulu, October 2008. c�2008 Association for Computational Linguistics seed instances, the stopping criterion of iteration, the number of instances and patterns selected on each iteration, and so forth. These parameters also need to be calibrated for each task. In this paper, we present a grap</context>
<context position="10786" citStr="Komachi and Suzuki (2008)" startWordPosition="1668" endWordPosition="1671">(e) the number of instances to pass to the next iteration. Bootstrapping iterates the above three phases several times until stopping criteria are met. Acquired instances tend to become noisy as the iteration proceeds, so it is important to terminate before semantic drift occurs. Thus, we have another configuration: (f) stopping criterion. Espresso uses Equations (1) for (a) and (2) for (c) respectively, whereas other parameters rely on the tasks and need calibration. Even though Espresso greatly improves recall while keeping high precision by using these pattern and instance scoring metrics, Komachi and Suzuki (2008) observed that extracted instances matched against generic patterns may become erroneous after tens of iterations, showing the difficulty of applying bootstrapping methods to different domains. 3 Analysis of an Espresso-like Bootstrapping Algorithm 3.1 Simplified Espresso Let us consider a simple bootstrapping algorithm illustrated in Figure 1, in order to elucidate the cause rπ(p) = 1013 of semantic drift. As before, let |I |and |P |be the numbers of instances and patterns, respectively. The algorithm takes a seed vector io, and a pattern-instance co-occurrence matrix M as input. io is a |I|-</context>
</contexts>
<marker>Komachi, Suzuki, 2008</marker>
<rawString>Mamoru Komachi and Hisami Suzuki. 2008. Minimally Supervised Learning of Semantic Knowledge from Query Logs. In Proceedings of the 3rd International Joint Conference on Natural Language Processing, pages 358–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Risi Imre Kondor</author>
<author>John Lafferty</author>
</authors>
<title>Diffusion kernels on graphs and other discrete input spaces.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Machine Learning (ICML-2002).</booktitle>
<contexts>
<context position="31287" citStr="Kondor and Lafferty, 2002" startWordPosition="5132" endWordPosition="5135">for detail). 6 Conclusion and Future Work This paper gives a graph-based analysis of semantic drift in Espresso-like bootstrapping algorithms. We indicate that semantic drift in bootstrapping is a parallel to topic drift in HITS. We confirm that the von Neumann kernels and the regularized Laplacian reduce semantic drift in the Senseval-3 Lexical Sample task. Our proposed methods have only one parameters and are easy to calibrate. Beside the regularized Laplacian, many other kernels based on the eigenvalue regularization of the Laplacian matrix have been proposed in machine learning community (Kondor and Lafferty, 2002; Nadler et al., 2006; Saerens et al., 2004). One such kernel is the commute-time kernel (Saerens et al., 2004) defined as the pseudo-inverse of Laplacian. Despite having no parameters at all, it has been reported to perform well in many collaborative filtering tasks (Fouss et al., 2007). We plan to test these kernels in our task as well. Another research topic is to investigate other semi-supervised learning techniques such as cotraining (Blum and Mitchell, 1998). As we have described in this paper, self-training can be thought of a graph-based algorithm. It is also interesting to analyze how</context>
</contexts>
<marker>Kondor, Lafferty, 2002</marker>
<rawString>Risi Imre Kondor and John Lafferty. 2002. Diffusion kernels on graphs and other discrete input spaces. In Proceedings of the 19th International Conference on Machine Learning (ICML-2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Cong Li</author>
</authors>
<title>Word Translation Disambiguation Using Bilingual Bootstrapping.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="5910" citStr="Li and Li (2004)" startWordPosition="887" endWordPosition="890">s a general framework for reducing the requirement of manual annotation. Hearst (1992) described a bootstrapping procedure for extracting words in hyponym (is-a) relation, starting with three manually given lexicosyntactic patterns. The idea of learning with a bootstrapping method was adopted for many tasks. Yarowsky (1995) presented an unsupervised WSD system which rivals supervised techniques. Abney (2004) presented a thorough discussion on the Yarowsky algorithm. He extended the original Yarowsky algorithm to a new family of bootstrapping algorithms that are mathematically well understood. Li and Li (2004) proposed a method called Bilingual Bootstrapping. It makes use of a translation dictionary and a comparable corpus to help disambiguate word senses in the source language, by exploiting the asymmetric many-to-many sense mapping relationship between words in two languages. Curran et al. (2007) presented an algorithm called Mutual Exclusion Bootstrapping, which minimizes semantic drift using mutual exclusion between semantic classes of learned instances. They prepared a list of so-called stop classes similar to a stop word list used in information retrieval to help bound the semantic classes. S</context>
</contexts>
<marker>Li, Li, 2004</marker>
<rawString>Hang Li and Cong Li. 2004. Word Translation Disambiguation Using Bilingual Bootstrapping. Computational Linguistics, 30(1):1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boaz Nadler</author>
<author>Stephane Lafon</author>
<author>Ronald Coifman</author>
<author>Ioannis Kevrekidis</author>
</authors>
<title>Diffusion maps, spectral clustering and eigenfunctions of fokker-planck operators.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 18,</booktitle>
<pages>955--962</pages>
<contexts>
<context position="31308" citStr="Nadler et al., 2006" startWordPosition="5136" endWordPosition="5139">nd Future Work This paper gives a graph-based analysis of semantic drift in Espresso-like bootstrapping algorithms. We indicate that semantic drift in bootstrapping is a parallel to topic drift in HITS. We confirm that the von Neumann kernels and the regularized Laplacian reduce semantic drift in the Senseval-3 Lexical Sample task. Our proposed methods have only one parameters and are easy to calibrate. Beside the regularized Laplacian, many other kernels based on the eigenvalue regularization of the Laplacian matrix have been proposed in machine learning community (Kondor and Lafferty, 2002; Nadler et al., 2006; Saerens et al., 2004). One such kernel is the commute-time kernel (Saerens et al., 2004) defined as the pseudo-inverse of Laplacian. Despite having no parameters at all, it has been reported to perform well in many collaborative filtering tasks (Fouss et al., 2007). We plan to test these kernels in our task as well. Another research topic is to investigate other semi-supervised learning techniques such as cotraining (Blum and Mitchell, 1998). As we have described in this paper, self-training can be thought of a graph-based algorithm. It is also interesting to analyze how co-training is relat</context>
</contexts>
<marker>Nadler, Lafon, Coifman, Kevrekidis, 2006</marker>
<rawString>Boaz Nadler, Stephane Lafon, Ronald Coifman, and Ioannis Kevrekidis. 2006. Diffusion maps, spectral clustering and eigenfunctions of fokker-planck operators. Advances in Neural Information Processing Systems 18, pages 955–962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Weakly Supervised Natural Language Learning Without Redundant Views.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL</booktitle>
<pages>94--101</pages>
<contexts>
<context position="6876" citStr="Ng and Cardie, 2003" startWordPosition="1041" endWordPosition="1044">trapping, which minimizes semantic drift using mutual exclusion between semantic classes of learned instances. They prepared a list of so-called stop classes similar to a stop word list used in information retrieval to help bound the semantic classes. Stop classes are sets of terms known to cause semantic drift in particular semantic classes. However, stop classes vary from task to task and domain to domain, and human intervention is essential to create an effective list of stop classes. A major drawback of bootstrapping is the lack of principled method for selecting optimal parameter values (Ng and Cardie, 2003; Banko and Brill, 2001). Also, there is an issue of generic patterns which deteriorates the quality of acquired instances. Previously proposed bootstrapping algorithms differ in how they deal with the problem of semantic drift. We will take recently proposed Espresso algorithm as the example to explain common configuration for bootstrapping in detail. 2.2 The Espresso Algorithm Pantel and Pennachiotti (2006) proposed a bootstrapping algorithm called Espresso to learn binary semantic relations such as is-a and part-of from a corpus. What distinguishes Espresso from other bootstrapping algorith</context>
</contexts>
<marker>Ng, Cardie, 2003</marker>
<rawString>Vincent Ng and Claire Cardie. 2003. Weakly Supervised Natural Language Learning Without Redundant Views. In Proceedings of the HLT-NAACL 2003, pages 94–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="2125" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="306" endWordPosition="309">pervised techniques have been actively studied. Hearst (1992) first presented a bootstrapping method which requires only a small amount of instances (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction. Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999) and relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006). However, it is known that bootstrapping often acquires instances not related to seed instances. For example, consider the task of collecting the names of common tourist sites from web corpora. Given words like “Geneva” and “Bali” as seed instances, bootstrapping would eventually learn generic patterns such as “pictures” and “photos,” which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-occur with these generic patterns, such as “Britney Spears.” This phenomenon is called semantic drift (Curran et al., 2007). A straight</context>
<context position="13173" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="2094" endWordPosition="2098">TM. 1 (7) Suppose matrix A is irreducible; i.e., the graph induced by taking A as the adjacency matrix is connected. If n is increased and in is normalized on each iteration, in tends to the principal eigenvector of A. This implies that no matter what seed instances are input, the algorithm will end up with the same ranking of instances, if it is run until convergence. Because A = MT M I P , the principal eigenvector of A is identical to the authority vector of HITS (Kleinberg, 1999) algorithm run on the graph induced by M. 1 This similarity of Equations (1), (2) and HITS is not discussed in (Pantel and Pennacchiotti, 2006). As a consequence of the above discussion, semantic drift in simplified Espresso seems to be inevitable as the iteration proceeds, since the principal eigenvector of A need not resemble seed vector io. A similar phenomenon is reported for HITS and is known as topic drift, in which pages of the dominant topic are ranked high regardless of the given query. (Bharat and Henzinger, 1998) Unlike HITS and Simplified Espresso, however, Espresso and other bootstrapping algorithms (Yarowsky, 1995; Riloff and Jones, 1999), incorporate heuristics so that only patterns and instances with high confidence s</context>
<context position="15339" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="2451" endWordPosition="2454"> vector on each iteration, and the number of non-zeroed instance scores &apos;As long as the relative magnitude of the components of vector i. is preserved, the vector can be normalized in any way on each iteration. Hence HITS and Simplified Espresso use different normalization but both converge to the principal eigenvector of A. zhttp://www.senseval.org/senseval3/data.html 1014 grows by 100 on each iteration. On the other hand, we cleared all but the 20 top-scoring patterns in the pattern vector on each iteration, and the number of non-zeroed pattern scores grows by 1 on each iteration following (Pantel and Pennacchiotti, 2006).3 The values of other parameters (b), (d), (e) and (f) remains the same as those for simplified Espresso in Section 3.1. The task of WSD is to correctly predict the senses of test instances whose true sense is hidden from the system, using training data and their true senses. To predict the sense of a given instance i, we apply knearest neighbor algorithm. Given a test instance i, its sense is predicted with the following procedure: 1. Compute the instance-pattern matrix M from the entire set of instances. We defer the details of this step to Section 5.2. 2. Run Simplified- and Filtered Espre</context>
<context position="16910" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="2716" endWordPosition="2719">es, and output s as the prediction for the given instance i. When there is a tie, output the sense of the instance with the highest score in i. Note that only Step 4 uses sense information. Figure 2 shows the convergence process of Simplified- and Filtered Espresso. X-axis indicates the number of bootstrapping iterations and Y-axis indicates the recall, which in this case equals precision, as the coverage is 100% in all cases. 3We conducted preliminary experiment to find these parameters to maximize the performance of Filtered Espresso. (These numbers are different from the original Espresso (Pantel and Pennacchiotti, 2006).) The number of initial patterns is relatively large because of a data sparseness problem in WSD, unlike relation extraction and named entity recognition. Also, WSD basically uses more features than relation extraction and thus it is hard to determine the stopping criterion based on the number and scores of patterns, as (Pantel and Pennacchiotti, 2006) does. 1 0.9 0.8 0.7 0.6 0.5 0.4 5 10 15 20 25 30 iteration Figure 2: Recall of Simplified- and Filtered Espresso Simplified Espresso tends to select the most frequent sense as the iteration proceeds, and after nine iterations it selects the mos</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intellligence (AAAI99),</booktitle>
<pages>474--479</pages>
<contexts>
<context position="2092" citStr="Riloff and Jones, 1999" startWordPosition="302" endWordPosition="305">lly)-supervised and unsupervised techniques have been actively studied. Hearst (1992) first presented a bootstrapping method which requires only a small amount of instances (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction. Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999) and relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006). However, it is known that bootstrapping often acquires instances not related to seed instances. For example, consider the task of collecting the names of common tourist sites from web corpora. Given words like “Geneva” and “Bali” as seed instances, bootstrapping would eventually learn generic patterns such as “pictures” and “photos,” which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-occur with these generic patterns, such as “Britney Spears.” This phenomenon is called semantic drift </context>
<context position="13690" citStr="Riloff and Jones, 1999" startWordPosition="2180" endWordPosition="2183">y M. 1 This similarity of Equations (1), (2) and HITS is not discussed in (Pantel and Pennacchiotti, 2006). As a consequence of the above discussion, semantic drift in simplified Espresso seems to be inevitable as the iteration proceeds, since the principal eigenvector of A need not resemble seed vector io. A similar phenomenon is reported for HITS and is known as topic drift, in which pages of the dominant topic are ranked high regardless of the given query. (Bharat and Henzinger, 1998) Unlike HITS and Simplified Espresso, however, Espresso and other bootstrapping algorithms (Yarowsky, 1995; Riloff and Jones, 1999), incorporate heuristics so that only patterns and instances with high confidence score are carried over to the next iteration. 3.3 Convergence Process of Espresso To investigate the effect of semantic drift on Espresso with and without the heuristics of selecting the most confident instances on each iteration (i.e., the original Espresso and Simplified Espresso of Section 3.2), we apply them to the task of word sense disambiguation of word “bank” in the Senseval-3 Lexical Sample (S3LS) Task data.2 There are 394 instances of word “bank” and their occurring context in this dataset, and each of </context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intellligence (AAAI99), pages 474–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Saerens</author>
<author>Franc¸ois Fouss</author>
<author>Luh Yen</author>
<author>Pierre Dupont</author>
</authors>
<title>The principal component analysis of a graph, and its relationship to spectral clustering.</title>
<date>2004</date>
<booktitle>In Proceedings of European Conference on Machine Learning (ECML 2004),</booktitle>
<pages>371--383</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="31331" citStr="Saerens et al., 2004" startWordPosition="5140" endWordPosition="5143">aper gives a graph-based analysis of semantic drift in Espresso-like bootstrapping algorithms. We indicate that semantic drift in bootstrapping is a parallel to topic drift in HITS. We confirm that the von Neumann kernels and the regularized Laplacian reduce semantic drift in the Senseval-3 Lexical Sample task. Our proposed methods have only one parameters and are easy to calibrate. Beside the regularized Laplacian, many other kernels based on the eigenvalue regularization of the Laplacian matrix have been proposed in machine learning community (Kondor and Lafferty, 2002; Nadler et al., 2006; Saerens et al., 2004). One such kernel is the commute-time kernel (Saerens et al., 2004) defined as the pseudo-inverse of Laplacian. Despite having no parameters at all, it has been reported to perform well in many collaborative filtering tasks (Fouss et al., 2007). We plan to test these kernels in our task as well. Another research topic is to investigate other semi-supervised learning techniques such as cotraining (Blum and Mitchell, 1998). As we have described in this paper, self-training can be thought of a graph-based algorithm. It is also interesting to analyze how co-training is related to the proposed algo</context>
</contexts>
<marker>Saerens, Fouss, Yen, Dupont, 2004</marker>
<rawString>Marco Saerens, Franc¸ois Fouss, Luh Yen, and Pierre Dupont. 2004. The principal component analysis of a graph, and its relationship to spectral clustering. In Proceedings of European Conference on Machine Learning (ECML 2004), pages 371–383. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heinrich Sch¨utze</author>
</authors>
<title>Automatic Word Sense Discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Heinrich Sch¨utze. 1998. Automatic Word Sense Discrimination. Computational Linguistics, 24(1):97– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex J Smola</author>
<author>Risi Imre Kondor</author>
</authors>
<title>Kernels and Regularization of Graphs.</title>
<date>2003</date>
<booktitle>In Proceedings of the 16th Annual Conference on Learning Theory,</booktitle>
<pages>144--158</pages>
<contexts>
<context position="22153" citStr="Smola and Kondor, 2003" startWordPosition="3584" endWordPosition="3587">y giving too much weight on unrelated instances extracted by generic patterns. As a result, it is expected that setting diffusion factor Q to a small value prevents semantic drift and also takes higher order pattern vectors into account. We verify this claim in Section 5.3. 4.2 Regularized Laplacian Kernel The von Neumann kernels can be regarded as a mixture of relatedness and importance, and diffusion factor Q controls the trade-off between relatedness and importance. In practice, however, setting the right parameter value becomes an issue. We solve this problem by the regularized Laplacian (Smola and Kondor, 2003; Chebotarev and Shamis, 1998), which are stable across diffusion factors and can safely benefit from generic patterns. Let G be a weighted undirected graph whose adjacency (weight) matrix is a symmetric matrix A. The (combinatorial) graph Laplacian L of a graph G is defined as follows: L = D − A (9) where D is a diagonal matrix, and the ith diagonal recall 1016 Table 1: Recall of predicted labels of bank algorithm MFS others element [D]ii is given by �[D]ii = [A]ij. j Here, [A]ij stands for the (i, j) element of A. By replacing A with −L in Equation (8) and deleting the first A, we obtain a r</context>
</contexts>
<marker>Smola, Kondor, 2003</marker>
<rawString>Alex J. Smola and Risi Imre Kondor. 2003. Kernels and Regularization of Graphs. In Proceedings of the 16th Annual Conference on Learning Theory, pages 144– 158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
</authors>
<title>HyperLex: Lexical Cartography for Information Retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>V´eronis, 2004</marker>
<rawString>Jean V´eronis. 2004. HyperLex: Lexical Cartography for Information Retrieval. Computer Speech &amp; Language, 18(3):223–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="1991" citStr="Yarowsky, 1995" startWordPosition="290" endWordPosition="291">ms for building NLP systems. To mitigate the problem of hand-tagging resources, semi(or minimally)-supervised and unsupervised techniques have been actively studied. Hearst (1992) first presented a bootstrapping method which requires only a small amount of instances (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction. Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999) and relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006). However, it is known that bootstrapping often acquires instances not related to seed instances. For example, consider the task of collecting the names of common tourist sites from web corpora. Given words like “Geneva” and “Bali” as seed instances, bootstrapping would eventually learn generic patterns such as “pictures” and “photos,” which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-o</context>
<context position="5619" citStr="Yarowsky (1995)" startWordPosition="846" endWordPosition="847"> topic drift. In Section 5 we apply them to the task of word sense disambiguation on Senseval-3 Lexical Sample Task and verify that they indeed reduce semantic drift. Finally, we conclude our work in Section 6. 2 Related Work 2.1 Overview of Bootstrapping Bootstrapping (or self-training) is a general framework for reducing the requirement of manual annotation. Hearst (1992) described a bootstrapping procedure for extracting words in hyponym (is-a) relation, starting with three manually given lexicosyntactic patterns. The idea of learning with a bootstrapping method was adopted for many tasks. Yarowsky (1995) presented an unsupervised WSD system which rivals supervised techniques. Abney (2004) presented a thorough discussion on the Yarowsky algorithm. He extended the original Yarowsky algorithm to a new family of bootstrapping algorithms that are mathematically well understood. Li and Li (2004) proposed a method called Bilingual Bootstrapping. It makes use of a translation dictionary and a comparable corpus to help disambiguate word senses in the source language, by exploiting the asymmetric many-to-many sense mapping relationship between words in two languages. Curran et al. (2007) presented an a</context>
<context position="13665" citStr="Yarowsky, 1995" startWordPosition="2178" endWordPosition="2179"> graph induced by M. 1 This similarity of Equations (1), (2) and HITS is not discussed in (Pantel and Pennacchiotti, 2006). As a consequence of the above discussion, semantic drift in simplified Espresso seems to be inevitable as the iteration proceeds, since the principal eigenvector of A need not resemble seed vector io. A similar phenomenon is reported for HITS and is known as topic drift, in which pages of the dominant topic are ranked high regardless of the given query. (Bharat and Henzinger, 1998) Unlike HITS and Simplified Espresso, however, Espresso and other bootstrapping algorithms (Yarowsky, 1995; Riloff and Jones, 1999), incorporate heuristics so that only patterns and instances with high confidence score are carried over to the next iteration. 3.3 Convergence Process of Espresso To investigate the effect of semantic drift on Espresso with and without the heuristics of selecting the most confident instances on each iteration (i.e., the original Espresso and Simplified Espresso of Section 3.2), we apply them to the task of word sense disambiguation of word “bank” in the Senseval-3 Lexical Sample (S3LS) Task data.2 There are 394 instances of word “bank” and their occurring context in t</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>