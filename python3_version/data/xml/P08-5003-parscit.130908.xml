<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.324173">
<title confidence="0.996141">
Semi-supervised Learning for Natural Language Processing
</title>
<author confidence="0.989232">
John Blitzer
</author>
<affiliation confidence="0.848459666666667">
Natural Language Computing Group
Microsoft Research Asia
Beijing, China
</affiliation>
<email confidence="0.997733">
blitzer@cis.upenn.edu
</email>
<author confidence="0.984093">
Xiaojin Jerry Zhu
</author>
<affiliation confidence="0.9975">
Department of Computer Science
University of Wisconsin, Madison
</affiliation>
<address confidence="0.86387">
Madison, WI, USA
</address>
<email confidence="0.999505">
jerryzhu@cs.wisc.edu
</email>
<sectionHeader confidence="0.999747" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996517">
The amount of unlabeled linguistic data available
to us is much larger and growing much faster than
the amount of labeled data. Semi-supervised learn-
ing algorithms combine unlabeled data with a small
labeled training set to train better models. This
tutorial emphasizes practical applications of semi-
supervised learning; we treat semi-supervised learn-
ing methods as tools for building effective models
from limited training data. An attendee will leave
our tutorial with
</bodyText>
<listItem confidence="0.998557428571429">
1. A basic knowledge of the most common classes
of semi-supervised learning algorithms and where
they have been used in NLP before.
2. The ability to decide which class will be useful
in her research.
3. Suggestions against potential pitfalls in semi-
supervised learning.
</listItem>
<sectionHeader confidence="0.796437" genericHeader="categories and subject descriptors">
2 Content Overview
</sectionHeader>
<bodyText confidence="0.999672615384615">
Self-training methods Self-training methods use
the labeled data to train an initial model and then
use that model to label the unlabeled data and re-
train a new model. We will examine in detail the co-
training method of Blum and Mitchell [2], includ-
ing the assumptions it makes, and two applications
of co-training to NLP data. Another popular self-
training method treats the labels of the unlabeled
data as hidden and estimates a single model from
labeled and unlabeled data. We explore new meth-
ods in this framework that make use of declarative
linguistic side information to constrain the solutions
found using unlabeled data [3].
</bodyText>
<page confidence="0.96279">
3
</page>
<bodyText confidence="0.999439818181818">
Graph regularization methods Graph regulariza-
tion methods build models based on a graph on in-
stances, where edges in the graph indicate similarity.
The regularization constraint is one of smoothness
along this graph. We wish to find models that per-
form well on the training data, but we also regularize
so that unlabeled nodes which are similar according
to the graph have similar labels. For this section, we
focus in detail on the Gaussian fields method of Zhu
et al. [4].
Structural learning Structural learning [1] uses un-
labeled data to find a new, reduced-complexity hy-
pothesis space by exploiting regularities in feature
space via unlabeled data. If this new hypothesis
space still contains good hypotheses for our super-
vised learning problem, we may achieve high accu-
racy with much less training data. The regularities
we use come in the form of lexical features that func-
tion similarly for prediction. This section will fo-
cus on the assumptions behind structural learning, as
well as applications to tagging and sentiment analy-
sis.
</bodyText>
<sectionHeader confidence="0.998628" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99276225">
[1] Rie Ando and Tong Zhang. A Framework for Learn-
ing Predictive Structures from Multiple Tasks and Unla-
beled Data. JMLR 2005.
[2] Avrim Blum and Tom Mitchell. Combining Labeled
and Unlabeled Data with Co-training. COLT 1998.
[3] Aria Haghighi and Dan Klein. Prototype-driven
Learning for Sequence Models. HLT/NAACL 2006.
[4] Xiaojin Zhu, Zoubin Ghahramani, and John Laf-
ferty. Semi-supervised Learning using Gaussian Fields
and Harmonic Functions. ICML 2003.
Tutorial Abstracts of ACL-08: HLT, page 3,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957551">
<title confidence="0.999862">Semi-supervised Learning for Natural Language Processing</title>
<author confidence="0.999511">John Blitzer</author>
<affiliation confidence="0.9994445">Natural Language Computing Group Microsoft Research Asia</affiliation>
<address confidence="0.964446">Beijing, China</address>
<email confidence="0.999884">blitzer@cis.upenn.edu</email>
<author confidence="0.99967">Xiaojin Jerry Zhu</author>
<affiliation confidence="0.999695">Department of Computer Science University of Wisconsin, Madison</affiliation>
<address confidence="0.999588">Madison, WI, USA</address>
<email confidence="0.995999">jerryzhu@cs.wisc.edu</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data. JMLR</title>
<date>2005</date>
<marker>[1]</marker>
<rawString>Rie Ando and Tong Zhang. A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data. JMLR 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-training.</title>
<date>1998</date>
<location>COLT</location>
<contexts>
<context position="1300" citStr="[2]" startWordPosition="195" endWordPosition="195">els from limited training data. An attendee will leave our tutorial with 1. A basic knowledge of the most common classes of semi-supervised learning algorithms and where they have been used in NLP before. 2. The ability to decide which class will be useful in her research. 3. Suggestions against potential pitfalls in semisupervised learning. 2 Content Overview Self-training methods Self-training methods use the labeled data to train an initial model and then use that model to label the unlabeled data and retrain a new model. We will examine in detail the cotraining method of Blum and Mitchell [2], including the assumptions it makes, and two applications of co-training to NLP data. Another popular selftraining method treats the labels of the unlabeled data as hidden and estimates a single model from labeled and unlabeled data. We explore new methods in this framework that make use of declarative linguistic side information to constrain the solutions found using unlabeled data [3]. 3 Graph regularization methods Graph regularization methods build models based on a graph on instances, where edges in the graph indicate similarity. The regularization constraint is one of smoothness along t</context>
</contexts>
<marker>[2]</marker>
<rawString>Avrim Blum and Tom Mitchell. Combining Labeled and Unlabeled Data with Co-training. COLT 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven Learning for Sequence Models. HLT/NAACL</title>
<date>2006</date>
<contexts>
<context position="1690" citStr="[3]" startWordPosition="258" endWordPosition="258">training methods use the labeled data to train an initial model and then use that model to label the unlabeled data and retrain a new model. We will examine in detail the cotraining method of Blum and Mitchell [2], including the assumptions it makes, and two applications of co-training to NLP data. Another popular selftraining method treats the labels of the unlabeled data as hidden and estimates a single model from labeled and unlabeled data. We explore new methods in this framework that make use of declarative linguistic side information to constrain the solutions found using unlabeled data [3]. 3 Graph regularization methods Graph regularization methods build models based on a graph on instances, where edges in the graph indicate similarity. The regularization constraint is one of smoothness along this graph. We wish to find models that perform well on the training data, but we also regularize so that unlabeled nodes which are similar according to the graph have similar labels. For this section, we focus in detail on the Gaussian fields method of Zhu et al. [4]. Structural learning Structural learning [1] uses unlabeled data to find a new, reduced-complexity hypothesis space by exp</context>
</contexts>
<marker>[3]</marker>
<rawString>Aria Haghighi and Dan Klein. Prototype-driven Learning for Sequence Models. HLT/NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised Learning using Gaussian Fields and Harmonic Functions.</title>
<date>2008</date>
<booktitle>ICML 2003. Tutorial Abstracts of ACL-08: HLT,</booktitle>
<pages>3</pages>
<location>Columbus, Ohio, USA,</location>
<marker>[4]</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised Learning using Gaussian Fields and Harmonic Functions. ICML 2003. Tutorial Abstracts of ACL-08: HLT, page 3, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>