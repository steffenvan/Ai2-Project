<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.034972">
<title confidence="0.98539">
Correcting ESL Errors Using Phrasal SMT Techniques
</title>
<author confidence="0.993028">
Chris Brockett, William B. Dolan, and Michael Gamon
</author>
<affiliation confidence="0.929001">
Natural Language Processing Group
Microsoft Research
One Microsoft Way, Redmond, WA 98005, USA
</affiliation>
<email confidence="0.998869">
{chrisbkt,billdol,mgamon}@microsoft.com
</email>
<sectionHeader confidence="0.99739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999995434782609">
This paper presents a pilot study of the
use of phrasal Statistical Machine Trans-
lation (SMT) techniques to identify and
correct writing errors made by learners of
English as a Second Language (ESL).
Using examples of mass noun errors
found in the Chinese Learner Error Cor-
pus (CLEC) to guide creation of an engi-
neered training set, we show that applica-
tion of the SMT paradigm can capture er-
rors not well addressed by widely-used
proofing tools designed for native speak-
ers. Our system was able to correct
61.81% of mistakes in a set of naturally-
occurring examples of mass noun errors
found on the World Wide Web, suggest-
ing that efforts to collect alignable cor-
pora of pre- and post-editing ESL writing
samples offer can enable the develop-
ment of SMT-based writing assistance
tools capable of repairing many of the
complex syntactic and lexical problems
found in the writing of ESL learners.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953188679246">
Every day, in schools, universities and busi-
nesses around the world, in email and on blogs
and websites, people create texts in languages
that are not their own, most notably English. Yet,
for writers of English as a Second Language
(ESL), useful editorial assistance geared to their
needs is surprisingly hard to come by. Grammar
checkers such as that provided in Microsoft
Word have been designed primarily with native
speakers in mind. Moreover, despite growing
demand for ESL proofing tools, there has been
remarkably little progress in this area over the
last decade. Research into computer feedback for
ESL writers remains largely focused on small-
scale pedagogical systems implemented within
the framework of CALL (Computer Aided Lan-
guage Learning) (Reuer 2003; Vanderventer
Faltin, 2003), while commercial ESL grammar
checkers remain brittle and difficult to customize
to meet the needs of ESL writers of different
first-language (L1) backgrounds and skill levels.
Some researchers have begun to apply statis-
tical techniques to identify learner errors in the
context of essay evaluation (Chodorow &amp; Lea-
cock, 2000; Lonsdale &amp; Strong-Krause, 2003), to
detect non-native text (Tomokiyo &amp; Jones, 2001),
and to support lexical selection by ESL learners
through first-language translation (Liu et al.,
2000). However, none of this work appears to
directly address the more general problem of
how to robustly provide feedback to ESL writ-
ers—and for that matter non-native writers in
any second language—in a way that is easily tai-
lored to different L1 backgrounds and second-
language (L2) skill levels.
In this paper, we show that a noisy channel
model instantiated within the paradigm of Statis-
tical Machine Translation (SMT) (Brown et al.,
1993) can successfully provide editorial assis-
tance for non-native writers. In particular, the
SMT approach provides a natural mechanism for
suggesting a correction, rather than simply
stranding the user with a flag indicating that the
text contains an error. Section 2 further motivates
the approach and briefly describes our SMT sys-
tem. Section 3 discusses the data used in our ex-
periment, which is aimed at repairing a common
type of ESL error that is not well-handled by cur-
rent grammar checking technology: mass/count
noun confusions. Section 4 presents experimental
results, along with an analysis of errors produced
by the system. Finally we present discussion and
some future directions for investigation.
</bodyText>
<page confidence="0.982273">
249
</page>
<note confidence="0.8396195">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 249–256,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.92408" genericHeader="method">
2 Error Correction as SMT
</sectionHeader>
<subsectionHeader confidence="0.971803">
2.1 Beyond Grammar Checking
</subsectionHeader>
<bodyText confidence="0.999550296296296">
A major difficulty for ESL proofing is that errors
of grammar, lexical choice, idiomaticity, and
style rarely occur in isolation. Instead, any given
sentence produced by an ESL learner may in-
volve a complex combination of all these error
types. It is difficult enough to design a proofing
tool that can reliably correct individual errors;
the simultaneous combination of multiple errors
is beyond the capabilities of current proofing
tools designed for native speakers. Consider the
following example, written by a Korean speaker
and found on the World Wide Web, which in-
volves the misapplication of countability to a
mass noun:
And I knew many informations
about Christmas while I was
preparing this article.
The grammar and spelling checkers in Microsoft
Word 2003 correctly suggest many 4 much
and informations 4 information.
Accepting these proposed changes, however,
does not render the sentence entirely native-like.
Substituting the word much for many leaves
the sentence stilted in a way that is probably un-
detectable to an inexperienced non-native
speaker, while the use of the word knew repre-
sents a lexical selection error that falls well out-
side the scope of conventional proofing tools. A
better rewrite might be:
And I learned a lot of in-
formation about Christmas
while I was preparing this
article.
or, even more colloquially:
And I learned a lot about
Christmas while I was pre-
paring this article
Repairing the error in the original sentence,
then, is not a simple matter of fixing an agree-
ment marker or substituting one determiner for
another. Instead, wholesale replacement of the
phrase knew many informations with
the phrase learned a lot is needed to pro-
duce idiomatic-sounding output. Seen in these
terms, the process of mapping from a raw, ESL-
authored string to its colloquial equivalent looks
remarkably like translation. Our goal is to show
that providing editorial assistance for writers
should be viewed as a special case of translation.
Rather than learning how strings in one language
map to strings in another, however, “translation”
now involves learning how systematic patterns of
errors in ESL learners’ English map to corre-
sponding patterns in native English
</bodyText>
<subsectionHeader confidence="0.999191">
2.2 A Noisy Channel Model of ESL Errors
</subsectionHeader>
<bodyText confidence="0.999975076923077">
If ESL error correction is seen as a translation
task, the task can be treated as an SMT problem
using the noisy channel model of (Brown et al.,
1993): here the L2 sentence produced by the
learner can be regarded as having been corrupted
by noise in the form of interference from his or
her L1 model and incomplete language models
internalized during language learning. The task,
then, is to reconstruct a corresponding valid sen-
tence of L2 (target). Accordingly, we can seek to
probabilistically identify the optimal correct tar-
get sentence(s) T* of an ESL input sentence S by
applying the familiar SMT formula:
</bodyText>
<equation confidence="0.9917984">
arg max{P(T|S)}
T
arg max { P(  |) P( )}
S T T
T
</equation>
<bodyText confidence="0.999979142857143">
In the context of this model, editorial assis-
tance becomes a matter of identifying those seg-
ments of the optimal target sentence or sentences
that differ from the writer’s original input and
displaying them to the user. In practice, the pat-
terns of errors produced by ESL writers of spe-
cific L1 backgrounds can be captured in the
channel model as an emergent property of train-
ing data consisting ESL sentences aligned with
their corrected edited counterparts. The highest
frequency errors and infelicities should emerge
as targets for replacement, while lesser frequency
or idiosyncratic problems will in general not sur-
face as false flags.
</bodyText>
<subsectionHeader confidence="0.996877">
2.3 Implementation
</subsectionHeader>
<bodyText confidence="0.9999541">
In this paper, we explore the use of a large-scale
production statistical machine translation system
to correct a class of ESL errors. A detailed de-
scription of the system can be found in (Menezes
&amp; Quirk 2005) and (Quirk et al., 2005). In keep-
ing with current best practices in SMT, our sys-
tem is a phrasal machine translation system that
attempts to learn mappings between “phrases”
(which may not correspond to linguistic units)
rather than individual words. What distinguishes
</bodyText>
<equation confidence="0.964581">
T* =
</equation>
<page confidence="0.929584">
250
</page>
<bodyText confidence="0.999881333333333">
this system from other phrasal SMT systems is
that rather than aligning simple sequences of
words, it maps small phrasal “treelets” generated
by a dependency parse to corresponding strings
in the target. This “Tree-To-String” model holds
promise in that it allows us to potentially benefit
from being able to access a certain amount of
structural information during translation, without
necessarily being completely tied to the need for
a fully-well-formed linguistic analysis of the in-
put—an important consideration when it is
sought to handle ungrammatical or otherwise ill-
formed ESL input, but also simultaneously to
capture relationships not involving contiguous
strings, for example determiner-noun relations.
In our pilot study, this system was em-
ployed without modification to the system archi-
tecture. The sole adjustment made was to have
both Source (erroneous) and Target (correct) sen-
tences tokenized using an English language to-
kenizer. N-best results for phrasal alignment and
ordering models in the decoder were optimized
by lambda training via Maximum Bleu, along the
lines described in (Och, 2003).
</bodyText>
<sectionHeader confidence="0.998872" genericHeader="method">
3 Data Development
</sectionHeader>
<subsectionHeader confidence="0.999634">
3.1 Identifying Mass Nouns
</subsectionHeader>
<bodyText confidence="0.998829545454545">
In this paper, we focus on countability errors as-
sociated with mass nouns. This class of errors
(involving nouns that cannot be counted, such as
information, pollution, and home-
work) is characteristically encountered in ESL
writing by native speakers of several East Asian
languages (Dalgish, 1983; Hua &amp; Lee, 2004).1
We began by identifying a list of English nouns
that are frequently involved in mass/count errors
in by writing by Chinese ESL learners, by taking
the intersection of words which:
</bodyText>
<listItem confidence="0.9530789">
• occurred in either the Longman Dictionary
of Contemporary English or the American
Heritage Dictionary with a mass sense
• were involved in n ≥ 2 mass/count errors in
the Chinese Learner English Corpus
CLEC (Gui and Yang, 2003), either tagged
as a mass noun error or else with an adja-
cent tag indicating an article error.2
1 These constructions are also problematic for hand-
crafted MT systems (Bond et al., 1994).
</listItem>
<bodyText confidence="0.979076111111111">
2 CLEC tagging is not comprehensive; some common
mass noun errors (e.g., make a good progress)
are not tagged in this corpus.
This procedure yielded a list of 14 words:
knowledge, food, homework, fruit,
news, color, nutrition, equipment,
paper, advice, haste, information,
lunch, and tea. 3 Countability errors in-
volving these words are scattered across 46 sen-
tences in the CLEC corpus.
For a baseline representing the level of writing
assistance currently available to the average ESL
writer, we submitted these sentences to the
proofing tools in Microsoft Word 2003. The
spelling and grammar checkers correctly identi-
fied 21 of the 46 relevant errors, proposed one
incorrect substitution (a few advice 4 a few
advices), and failed to flag the remaining 25
errors. With one exception, the proofing tools
successfully detected as spelling errors incorrect
plurals on lexical items that permit only mass
noun interpretations (e.g., informations),
but ignored plural forms like fruits and pa-
pers even when contextually inappropriate. The
proofing tools in Word 2003 also detected singu-
lar determiner mismatches with obligatory plural
forms (e.g. a news).
</bodyText>
<subsectionHeader confidence="0.999841">
3.2 Training Data
</subsectionHeader>
<bodyText confidence="0.999958941176471">
The errors identified in these sentences provided
an informal template for engineering the data in
our training set, which was created by manipulat-
ing well-formed, edited English sentences. Raw
data came from a corpus of ~484.6 million words
of Reuters Limited newswire articles, released
between 1995 and 1998, combined with a
~7,175,000-word collection of articles from mul-
tiple news sources from 2004-2005. The result-
ing dataset was large enough to ensure that all
targeted forms occurred with some frequency.
From this dataset we culled about 346,000
sentences containing examples of the 14 targeted
words. We then used hand-constructed regular
expressions to convert these sentences into
mostly-ungrammatical strings that exhibited
characteristics of the CLEC data, for example:
</bodyText>
<listItem confidence="0.991625">
• much 4 many: much advice 4
many advice
• some 4 a/an: some advice 4
an advice
• conversions to plurals: much good
advice 4 many good advices
</listItem>
<footnote confidence="0.88822">
3 Terms that also had a function word sense, such as
will, were eliminated for this experiment.
</footnote>
<page confidence="0.949123">
251
</page>
<table confidence="0.999934333333333">
Data Size Whole Partial Correctly Left New Error Missed Word Order
Error
45K 55.28 0.81 8.13 12.20 21.14 1.63
30K 36.59 4.07 7.32 16.26 32.52 3.25
15K 47.15 2.44 5.69 11.38 29.27 4.07
cf. Word 29.27 0.81 10.57 1.63 57.72 N/A
</table>
<tableCaption confidence="0.99975">
Table 1. Replacement percentages (per sentence basis) using different training data sets
</tableCaption>
<listItem confidence="0.999740333333333">
• deletion of counters: piece(s)/
item(s)/sheet(s) of)
• insertion of determiners
</listItem>
<bodyText confidence="0.99576775">
These were produced in multiple combinations
for broad coverage, for example:
I&apos;m not trying to give you
legal advice. 4
</bodyText>
<listItem confidence="0.999590833333333">
• I&apos;m not trying to give you a
legal advice.
• I&apos;m not trying to give you
the legal advice.
• I&apos;m not trying to give you
the legal advices.
</listItem>
<bodyText confidence="0.999957105263158">
A total of 24128 sentences from the news data
were “lesioned” in this manner to create a set of
65826 sentence pairs. To create a balanced train-
ing set that would not introduce too many arti-
facts of the substitution (e.g., many should not
always be recast as much just because that is the
only mapping observed in the training data), we
randomly created an equivalent number of iden-
tity-mapped pairs from the 346,000 examples,
with each sentence mapping to itself.
Training sets of various sizes up to 45,000
pairs were then randomly extracted from the le-
sioned and non-lesioned pairs so that data from
both sets occurred in roughly equal proportions.
Thus the 45K data set contains approximately
22,500 lesioned examples. An additional 1,000
randomly selected lesioned sentences were set
aside for lambda training the SMT system’s or-
dering and replacement models.
</bodyText>
<sectionHeader confidence="0.99941" genericHeader="method">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.998636">
4.1 Test Data
</subsectionHeader>
<bodyText confidence="0.999990916666667">
The amount of tagged data in CLEC is too small
to yield both development and test sets from the
same data. In order to create a test set, we had a
third party collect 150 examples of the 14 words
from English websites in China. After minor
cleanup to eliminate sentences irrelevant to the
task,4 we ended up with 123 example sentences
to use as test set. The test examples vary widely
in style, from the highly casual to more formal
public announcements. Thirteen examples were
determined to contain no errors relevant to our
experiment, but were retained in the data.5
</bodyText>
<sectionHeader confidence="0.85357" genericHeader="method">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999824259259259">
Table 1 shows per-sentence results of translating
the test set on systems built with training data
sets of various sizes (given in thousands of sen-
tence pairs). Numbers for the proofing tools in
Word 2003 are presented by way of comparison,
with the caveat that these tools have been inten-
tionally implemented conservatively so as not to
potentially irritate native users with false flags.
For our purposes, a replacement string is viewed
as correct if, in the view of a native speaker who
might be helping an ESL writer, the replacement
would appear more natural and hence potentially
useful as a suggestion in the context of that sen-
tence taken in isolation. Number disagreement
on subject and verb were ignored for the pur-
poses of this evaluation, since these errors were
not modeled when we introduced lesions into the
data. A correction counted as Whole if the sys-
tem produced a contextually plausible substitu-
tion meeting two criteria: 1) number and 2) de-
terminer/quantifier selection (e.g., many in-
formations 4 much information).
Transformations involving bare singular targets
(e.g., the fruits 4 fruit) also counted
as Whole. Partial corrections are those where
only one of the two criteria was met and part of
the desired correction was missing (e.g., an
</bodyText>
<footnote confidence="0.87964125">
4 In addition to eliminating cases that only involved
subject-verb number agreement, we excluded a small
amount of spam-like word salad, several instances of
the word homework being misused to mean “work
done out of the home”, and one misidentified quota-
tion from Scott’s Ivanhoe.
5 This test set may be downloaded at
http://research.microsoft.com/research/downloads
</footnote>
<page confidence="0.990301">
252
</page>
<figure confidence="0.990360590909091">
Shanghai residents can buy the fruits for a cheaper price
Input
than before.
Shanghai residents can buy fruit for a cheaper price than
Replacement
before .
Input Thank u for giving me so many advice.
Replacement thank u for giving me so much advice .
Acquiring the knowledge of information warfare is key to
Input
winning wars
Replacement acquiring knowledge of information warfare is key to win-
ning wars
Input Many knowledge about Li Bai can be gain through it.
Replacement much knowledge about Li Bai can be gain through it .
Input I especially like drinking the tea.
Replacement i especially like drinking tea .
Input and were pasted on boards on Taiwan.
icons printed on paper have been brought from Europe , and
Replacement
were pasted on boards on Taiwan .
Icons printed on a paper have been brought from Europe,
</figure>
<tableCaption confidence="0.970122">
Table 2. Sample corrections, using 45K engineered training data
</tableCaption>
<bodyText confidence="0.999341194444444">
equipments 4 an equipment versus the
targeted bare noun equipment). Incorrect sub-
stitutions and newly injected erroneous material
anywhere in the sentence counted as New Errors,
even if the proposed replacement were otherwise
correct. However, changes in upper and lower
case and punctuation were ignored.
The 55.28% per-sentence score for Whole
matches in the system trained on the 45K data set
means that it correctly proposed full corrections
in 61.8% of locations where corrections needed
to be made. The percentage of Missed errors, i.e.,
targeted errors that were ignored by the system,
is correspondingly low. On the 45K training data
set, the system performs nearly on a par with
Word in terms of not inducing corrections on
forms that did not require replacement, as shown
in the Correctly Left column. The dip in accu-
racy in the 30K sentence pair training set is an
artifact of our extraction methodology: the rela-
tively small lexical set that we are addressing
here appears to be oversensitive to random varia-
tion in the engineered training data. This makes
it difficult to set a meaningful lower bound on
the amount of training data that might be needed
for adequate coverage. Nonetheless, it is evident
from the table, that given sufficient data, SMT
techniques can successfully offer corrections for
a significant percentage of cases of the phenom-
ena in question.
Table 2 shows some sample inputs together
with successful corrections made by the system.
Table 3 illustrates a case where two valid correc-
tions are found in the 5-best ranked translations;
intervening candidates were identical with the
top-ranked candidate.
</bodyText>
<subsectionHeader confidence="0.980805">
4.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999829590909091">
Table 1 also indicates that errors associated with
the SMT system itself are encouragingly few. A
small number of errors in word order were found,
one of which resulted in a severely garbled sen-
tence in the 45K data set. In general, the percent-
age of this type of error declines consistently
with growth of the training data size. Linearity of
the training data may play a role, since the sen-
tence pairs differ by only a few words. On the
whole, however, we expect the system’s order
model to benefit from more training data.
The most frequent single class of newly intro-
duced error relates to sporadic substitution of the
word their for determiners a/the. This is
associated with three words, lunch, tea, and
haste, and is the principal contributor to the
lower percentages in the Correctly Left bin, as
compared with Word. This overgeneralization
error reflects our attempt to engineer the discon-
tinuous mapping the X of them 4 their
X, motivated by examples like the following,
encountered in the CLEC dataset:
</bodyText>
<page confidence="0.998112">
253
</page>
<table confidence="0.578691">
Input: And we can learn many knowledge or new information from TV
Candidate 1: And we can learn much knowledge or new information from TV
And we can learn a lot of knowledge or new information from
Candidate 5: TV
</table>
<tableCaption confidence="0.998496">
Table 3. Multiple replacement candidates generated by 45K training set
</tableCaption>
<bodyText confidence="0.998942">
In this equal world, lots of
people are still concerned
on the colors of them ...
The inability of our translation system to handle
such discontinuities in a unitary manner reflects
the limited ability of current SMT modeling
techniques to capture long-distance effects. Simi-
lar alternations are rife in bilingual data, e.g.,
ne...pas in French (Fox, 2002) and separable
prefixes in German (Collins et al. 2005). As
SMT models become more adept at modeling
long-distance effects in a principled manner,
monolingual proofing will benefit as well.
The Missed category is heterogeneous. The
SMT system has an inherent bias against deletion,
with the result that unwanted determiners tended
not to be deleted, especially in the smaller train-
ing sets.
Other errors related to coverage in the devel-
opment data set. Several occurrences of green-
grocer’s apostrophes (tea’s, equipment’s)
caused correction failures: these were not antici-
pated when engineering the training data. Like-
wise, the test data presented several malformed
quantifiers and quantifier-like phrases (plenty
tea 4 plenty of tea, a lot infor-
mation 4 a lot of information,
few information 4 too little in-
formation) that had been unattested in the
development set. Examples such as these high-
light the difficulty in obtaining complete cover-
age when using handcrafted techniques, whether
to engineer errors, as in our case, or to handcraft
targeted correction solutions.
The system performed poorly on words that
commonly present both mass and count noun
senses in ways that are apt to confuse L2 writers.
One problematic case was paper. The follow-
ing sentences, for example, remained uncor-
rected:
He published many paper in
provincial and national pub-
lication.
He has published thirty-two
pieces of papers.
Large amounts of additional training data
would doubtless be helpful in providing contex-
tual resolutions to the problems. Improved
alignment models may also play a role here in
capturing complex structures of the kind repre-
sented by constructions involving counters.
</bodyText>
<sectionHeader confidence="0.999316" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9990414">
The artificially-engineered training data that we
relied on for our experiments proved surprisingly
useful in modeling real errors made by non-
native speakers. However, this is obviously a less
than ideal data source, since the errors introduced
by regular expressions are homogenously dis-
tributed in a way that naturally-occurring errors
are not, creating artifacts that undoubtedly impair
our SMT models.
Artificial data of this sort may be useful as
proof of concept, but hand engineering such data
plainly does not present a viable path to develop-
ing real world applications. In order to be able to
handle the rich panoply of errors and error inter-
actions encountered in the text of second lan-
guage learners large quantities of naturally-
occurring “before” and “after” texts will need to
be collected. By way of illustration, Table 4
shows the output of results of “translating” our
test data into more natural English by hand and
dumping the pre- and post-editing pairs to the
45K training set.6 Although we were unable to
exactly recover the target sentences, inspection
showed that 25 sentences had improved, some
significantly, as Table 4 shows. Under the right
conditions, the SMT system can capture contex-
tual morphological alternations (nutri-
tion/nutritious), together with complex
mappings represented by the dependencies
learn F knowledge F many (ESL) and
</bodyText>
<footnote confidence="0.99817925">
6 Since a single example of each pair was insufficient
to override the system’s inherent bias towards uni-
gram mappings, 5 copies of each pair were appended
to the training data.
</footnote>
<page confidence="0.997111">
254
</page>
<bodyText confidence="0.9529756875">
And we can learn many knowledge or new information from
Input sentence
TV.
and we can learn much knowledge or new information from
45K system output TV .
45K + translation sys- we can gain a lot of knowledge or new information from
tem output TV .
Input sentence The following is one of the homework for last week.
45K system output the following is one of their homework for last week .
45K + translation sys- the following is one of the homework assignments for
tem output last week .
Input sentence i like mushroom,its very nutrition
45K system output i like mushroom , its very nutrition
45K + translation sys-
i like mushroom , its very nutritious
tem output
</bodyText>
<tableCaption confidence="0.941548">
Table 4. Contextual corrections before and after adding “translations” to 45K training data
</tableCaption>
<bodyText confidence="0.990514">
gain  knowledge  a lot of (Eng-
lish). In a rule-based correction system, an im-
mense amount of hand-coding would be required
to handle even a small subset of the potential
range of such mismatches between learner and
native-like English. This knowledge, we believe,
is best acquired from data.
</bodyText>
<subsectionHeader confidence="0.986173">
5.1 The Need for Data Collection
</subsectionHeader>
<bodyText confidence="0.999995208333333">
Given a sufficiently large corpus of aligned sen-
tences containing error patterns produced by ESL
writers of the same L1 background and their cor-
rected counterparts we expect eventually to be
able to capture the rich complexity of non-native
error within a noisy-channel based SMT model.
As a practical matter, however, parallel data of
the kind needed is far from easy to come by. This
does not mean, however, that such data does not
exist. The void left by commercial grammar
checkers is filled, largely unobserved, by a num-
ber of services that provide editorial assistance,
ranging from foreign language teachers, to lan-
guage helpdesks in multinational corporations, to
mentoring services for conferences. Translation
bureaus frequently offer editing services for non-
native speakers. Yet, unlike translation, the “be-
fore” and “after” texts are rarely recycled in a
form that can be used to build translation models.
Although collecting this data will involve a large
investment in time, effort, and infrastructure, a
serious effort along these lines is likely to prove
fruitful in terms of making it possible to apply
the SMT paradigm to ESL error correction.
</bodyText>
<subsectionHeader confidence="0.992815">
5.2 Feedback to SMT
</subsectionHeader>
<bodyText confidence="0.999971">
One challenge faced by the SMT model is the
extremely high quality that will need to be at-
tained before a system might be usable. Since it
is highly undesirable that learners should be pre-
sented with inaccurate feedback that they may
not have the experience or knowledge to assess,
the quality bar imposed on error correction is far
higher than is that tolerated in machine transla-
tion. Exploration of error correction and writing
assistance using SMT models may thus prove an
important venue for testing new SMT models.
</bodyText>
<subsectionHeader confidence="0.993907">
5.3 Advantages of the SMT Approach
</subsectionHeader>
<bodyText confidence="0.999988263157895">
Statistical Machine Translation has provided a
hugely successful research paradigm within the
field of natural language processing over the last
decade. One of the major advantages of using
SMT in ESL writing assistance is that it can be
expected to benefit automatically from any pro-
gress made in SMT itself. In fact, the approach
presented here benefits from all the advantages
of statistical machine translation. Since the archi-
tecture is not dependent on hard-to-maintain
rules or regular expressions, little or no linguistic
expertise will be required in developing and
maintain applications. As with SMT, this exper-
tise is pushed into the data component, to be
handled by instructors and editors, who do not
need programming or scripting skills.
We expect it to be possible, moreover, once
parallel data becomes available, to quickly ramp
up new systems to accommodate the needs of
</bodyText>
<page confidence="0.992449">
255
</page>
<bodyText confidence="0.999942666666667">
learners with different first-language back-
grounds and different skill levels and to writing
assistance for learners of L2s other than English.
It is also likely that this architecture may have
applications in pedagogical environments and as
a tool to assist editors and instructors who deal
regularly with ESL texts, much in the manner of
either Human Assisted Machine Translation or
Machine Assisted Human Translation. We also
believe that this same architecture could be ex-
tended naturally to provide grammar and style
tools for native writers.
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="conclusions">
6 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.999982952380952">
In this pilot study we have shown that SMT tech-
niques have potential to provide error correction
and stylistic writing assistance to L2 learners.
The next step will be to obtain a large dataset of
pre- and post-editing ESL text with which to
train a model that does not rely on engineered
data. A major purpose of the present study has
been to determine whether our hypothesis is ro-
bust enough to warrant the cost and effort of a
collection or data creation effort.
Although we anticipate that it will take a sig-
nificant lead time to assemble the necessary
aligned data, once a sufficiently large corpus is
in hand, we expect to begin exploring ways to
improve our SMT system by tailoring it more
specifically to the demands of editorial assistance.
In particular, we expect to be looking into alter-
native word alignment models and possibly en-
hancing our system’s decoder using some of the
richer, more structured language models that are
beginning to emerge.
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999748">
The authors have benefited extensively from dis-
cussions with Casey Whitelaw when he interned
at Microsoft Research during the summer of
2005. We also thank the Butler Hill Group for
collecting the examples in our test set.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998936491228071">
Bond, Francis, Kentaro Ogura and Satoru Ikehara.
1994. Countability and Number in Japanese-to-
English Machine Translation. COLING-94.
Peter E Brown, Stephen A. Della Pietra, Robert L.
Mercer, and Vincent J. Della Pietra. 1993. The
Mathematics of Statistical Machine Translation.
Computational Linguistics, Vol. 19(2): 263-311.
Martin Chodorow and Claudia Leacock. 2000. An
Unsupervised Method for Detecting Grammatical
Errors. NAACL 2000.
Michael Collins, Philipp Koehn and Ivona Kučerová.
2005. Clause Restructuring for Statistical machine
Translation. ACL 2005, 531-540.
Gerard M. Dalgish. 1984. Computer-Assisted ESL
Research. CALICO Journal. 2(2): 32-33
Heidi J. Fox. 2002. Phrasal Cohesion and Statistical
Machine Translation. EMNLP 2002.
Shicun Gui and Huizhong Yang (eds). 2003 Zhong-
guo Xuexizhe Yingyu Yuliaohu. (Chinese Learner
English Corpus). Shanghai: Shanghai Waiyu
Jiaoyu Chubanshe. (In Chinese).
Hua Dongfan and Thomas Hun-Tak Lee. 2004. Chi-
nese ESL Learners&apos; Understanding of the English
Count-Mass Distinction. In Proceedings of the 7th
Generative Approaches to Second Language Ac-
quisition Conference (GASLA 2004).
Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun,
and Changning Huang. 2000. PENS: A Machine-
aided English Writing System for Chinese Users.
ACL 2000.
Deryle Lonsdale and Diane Strong-Krause. 2003.
Automated Rating of ESL Essays. In Proceedings
of the HLT/NAACL Workshop: Building Educa-
tional Applications Using Natural Language Proc-
essing.
Arul Menezes, and Chris Quirk. 2005. Microsoft Re-
search Treelet Translation System: IWSLT Evalua-
tion. Proceedings of the International Workshop on
Spoken Language Translation.
Franz Josef Och, 2003. Minimum error rate training
in statistical machine translation. ACL 2003.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. ACL 2000.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Tree Translation: Syntactically In-
formed Phrasal SMT. ACL 2005.
Veit Reuer. 2003. Error Recognition and Feedback
with Lexical Functional Grammar. CALICO Jour-
nal, 20(3): 497-512.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You’re not from round here, are you? Naive Bayes
Detection of Non-Native Utterance Text. NAACL
2001.
Anne Vandeventer Faltin. 2003. Natural language
processing tools for computer assisted language
learning. Linguistik online 17, 5/03 (http://
www.linguistik-online.de/17_03/vandeventer.html)
</reference>
<page confidence="0.998296">
256
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.946357">
<title confidence="0.999662">Correcting ESL Errors Using Phrasal SMT Techniques</title>
<author confidence="0.999973">Chris Brockett</author>
<author confidence="0.999973">William B Dolan</author>
<author confidence="0.999973">Michael Gamon</author>
<affiliation confidence="0.999629">Natural Language Processing Group Microsoft Research</affiliation>
<address confidence="0.999391">One Microsoft Way, Redmond, WA 98005, USA</address>
<email confidence="0.999859">chrisbkt@microsoft.com</email>
<email confidence="0.999859">billdol@microsoft.com</email>
<email confidence="0.999859">mgamon@microsoft.com</email>
<abstract confidence="0.997758875">This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL). Using examples of mass noun errors in the Learner Error Cor- (CLEC) guide creation of an engineered training set, we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers. Our system was able to correct 61.81% of mistakes in a set of naturallyoccurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect alignable corpora of preand post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis Bond</author>
</authors>
<title>Kentaro Ogura and Satoru Ikehara.</title>
<date>1994</date>
<booktitle>Countability and Number in Japanese-toEnglish Machine Translation. COLING-94.</booktitle>
<marker>Bond, 1994</marker>
<rawString>Bond, Francis, Kentaro Ogura and Satoru Ikehara. 1994. Countability and Number in Japanese-toEnglish Machine Translation. COLING-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter E Brown</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
</authors>
<date>1993</date>
<journal>The Mathematics of Statistical Machine Translation. Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="2902" citStr="Brown et al., 1993" startWordPosition="455" endWordPosition="458">le &amp; Strong-Krause, 2003), to detect non-native text (Tomokiyo &amp; Jones, 2001), and to support lexical selection by ESL learners through first-language translation (Liu et al., 2000). However, none of this work appears to directly address the more general problem of how to robustly provide feedback to ESL writers—and for that matter non-native writers in any second language—in a way that is easily tailored to different L1 backgrounds and secondlanguage (L2) skill levels. In this paper, we show that a noisy channel model instantiated within the paradigm of Statistical Machine Translation (SMT) (Brown et al., 1993) can successfully provide editorial assistance for non-native writers. In particular, the SMT approach provides a natural mechanism for suggesting a correction, rather than simply stranding the user with a flag indicating that the text contains an error. Section 2 further motivates the approach and briefly describes our SMT system. Section 3 discusses the data used in our experiment, which is aimed at repairing a common type of ESL error that is not well-handled by current grammar checking technology: mass/count noun confusions. Section 4 presents experimental results, along with an analysis o</context>
<context position="6250" citStr="Brown et al., 1993" startWordPosition="991" endWordPosition="994">SLauthored string to its colloquial equivalent looks remarkably like translation. Our goal is to show that providing editorial assistance for writers should be viewed as a special case of translation. Rather than learning how strings in one language map to strings in another, however, “translation” now involves learning how systematic patterns of errors in ESL learners’ English map to corresponding patterns in native English 2.2 A Noisy Channel Model of ESL Errors If ESL error correction is seen as a translation task, the task can be treated as an SMT problem using the noisy channel model of (Brown et al., 1993): here the L2 sentence produced by the learner can be regarded as having been corrupted by noise in the form of interference from his or her L1 model and incomplete language models internalized during language learning. The task, then, is to reconstruct a corresponding valid sentence of L2 (target). Accordingly, we can seek to probabilistically identify the optimal correct target sentence(s) T* of an ESL input sentence S by applying the familiar SMT formula: arg max{P(T|S)} T arg max { P( |) P( )} S T T T In the context of this model, editorial assistance becomes a matter of identifying those </context>
</contexts>
<marker>Brown, Pietra, Mercer, Pietra, 1993</marker>
<rawString>Peter E Brown, Stephen A. Della Pietra, Robert L. Mercer, and Vincent J. Della Pietra. 1993. The Mathematics of Statistical Machine Translation. Computational Linguistics, Vol. 19(2): 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>An Unsupervised Method for Detecting Grammatical Errors. NAACL</title>
<date>2000</date>
<contexts>
<context position="2275" citStr="Chodorow &amp; Leacock, 2000" startWordPosition="354" endWordPosition="358"> there has been remarkably little progress in this area over the last decade. Research into computer feedback for ESL writers remains largely focused on smallscale pedagogical systems implemented within the framework of CALL (Computer Aided Language Learning) (Reuer 2003; Vanderventer Faltin, 2003), while commercial ESL grammar checkers remain brittle and difficult to customize to meet the needs of ESL writers of different first-language (L1) backgrounds and skill levels. Some researchers have begun to apply statistical techniques to identify learner errors in the context of essay evaluation (Chodorow &amp; Leacock, 2000; Lonsdale &amp; Strong-Krause, 2003), to detect non-native text (Tomokiyo &amp; Jones, 2001), and to support lexical selection by ESL learners through first-language translation (Liu et al., 2000). However, none of this work appears to directly address the more general problem of how to robustly provide feedback to ESL writers—and for that matter non-native writers in any second language—in a way that is easily tailored to different L1 backgrounds and secondlanguage (L2) skill levels. In this paper, we show that a noisy channel model instantiated within the paradigm of Statistical Machine Translation</context>
</contexts>
<marker>Chodorow, Leacock, 2000</marker>
<rawString>Martin Chodorow and Claudia Leacock. 2000. An Unsupervised Method for Detecting Grammatical Errors. NAACL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kučerová</author>
</authors>
<title>Clause Restructuring for Statistical machine Translation. ACL</title>
<date>2005</date>
<pages>531--540</pages>
<contexts>
<context position="20202" citStr="Collins et al. 2005" startWordPosition="3281" endWordPosition="3284">1: And we can learn much knowledge or new information from TV And we can learn a lot of knowledge or new information from Candidate 5: TV Table 3. Multiple replacement candidates generated by 45K training set In this equal world, lots of people are still concerned on the colors of them ... The inability of our translation system to handle such discontinuities in a unitary manner reflects the limited ability of current SMT modeling techniques to capture long-distance effects. Similar alternations are rife in bilingual data, e.g., ne...pas in French (Fox, 2002) and separable prefixes in German (Collins et al. 2005). As SMT models become more adept at modeling long-distance effects in a principled manner, monolingual proofing will benefit as well. The Missed category is heterogeneous. The SMT system has an inherent bias against deletion, with the result that unwanted determiners tended not to be deleted, especially in the smaller training sets. Other errors related to coverage in the development data set. Several occurrences of greengrocer’s apostrophes (tea’s, equipment’s) caused correction failures: these were not anticipated when engineering the training data. Likewise, the test data presented several</context>
</contexts>
<marker>Collins, Koehn, Kučerová, 2005</marker>
<rawString>Michael Collins, Philipp Koehn and Ivona Kučerová. 2005. Clause Restructuring for Statistical machine Translation. ACL 2005, 531-540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard M Dalgish</author>
</authors>
<date>1984</date>
<journal>Computer-Assisted ESL Research. CALICO Journal.</journal>
<volume>2</volume>
<issue>2</issue>
<pages>32--33</pages>
<marker>Dalgish, 1984</marker>
<rawString>Gerard M. Dalgish. 1984. Computer-Assisted ESL Research. CALICO Journal. 2(2): 32-33</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<date>2002</date>
<booktitle>Phrasal Cohesion and Statistical Machine Translation. EMNLP</booktitle>
<contexts>
<context position="20147" citStr="Fox, 2002" startWordPosition="3274" endWordPosition="3275">owledge or new information from TV Candidate 1: And we can learn much knowledge or new information from TV And we can learn a lot of knowledge or new information from Candidate 5: TV Table 3. Multiple replacement candidates generated by 45K training set In this equal world, lots of people are still concerned on the colors of them ... The inability of our translation system to handle such discontinuities in a unitary manner reflects the limited ability of current SMT modeling techniques to capture long-distance effects. Similar alternations are rife in bilingual data, e.g., ne...pas in French (Fox, 2002) and separable prefixes in German (Collins et al. 2005). As SMT models become more adept at modeling long-distance effects in a principled manner, monolingual proofing will benefit as well. The Missed category is heterogeneous. The SMT system has an inherent bias against deletion, with the result that unwanted determiners tended not to be deleted, especially in the smaller training sets. Other errors related to coverage in the development data set. Several occurrences of greengrocer’s apostrophes (tea’s, equipment’s) caused correction failures: these were not anticipated when engineering the t</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal Cohesion and Statistical Machine Translation. EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shicun Gui</author>
<author>Huizhong Yang</author>
</authors>
<title>Zhongguo Xuexizhe Yingyu Yuliaohu. (Chinese Learner English Corpus). Shanghai: Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).</title>
<date>2003</date>
<contexts>
<context position="9797" citStr="Gui and Yang, 2003" startWordPosition="1564" endWordPosition="1567">nouns that cannot be counted, such as information, pollution, and homework) is characteristically encountered in ESL writing by native speakers of several East Asian languages (Dalgish, 1983; Hua &amp; Lee, 2004).1 We began by identifying a list of English nouns that are frequently involved in mass/count errors in by writing by Chinese ESL learners, by taking the intersection of words which: • occurred in either the Longman Dictionary of Contemporary English or the American Heritage Dictionary with a mass sense • were involved in n ≥ 2 mass/count errors in the Chinese Learner English Corpus CLEC (Gui and Yang, 2003), either tagged as a mass noun error or else with an adjacent tag indicating an article error.2 1 These constructions are also problematic for handcrafted MT systems (Bond et al., 1994). 2 CLEC tagging is not comprehensive; some common mass noun errors (e.g., make a good progress) are not tagged in this corpus. This procedure yielded a list of 14 words: knowledge, food, homework, fruit, news, color, nutrition, equipment, paper, advice, haste, information, lunch, and tea. 3 Countability errors involving these words are scattered across 46 sentences in the CLEC corpus. For a baseline representin</context>
</contexts>
<marker>Gui, Yang, 2003</marker>
<rawString>Shicun Gui and Huizhong Yang (eds). 2003 Zhongguo Xuexizhe Yingyu Yuliaohu. (Chinese Learner English Corpus). Shanghai: Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Dongfan</author>
<author>Thomas Hun-Tak Lee</author>
</authors>
<title>Chinese ESL Learners&apos; Understanding of the English Count-Mass Distinction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 7th Generative Approaches to Second Language Acquisition Conference (GASLA</booktitle>
<marker>Dongfan, Lee, 2004</marker>
<rawString>Hua Dongfan and Thomas Hun-Tak Lee. 2004. Chinese ESL Learners&apos; Understanding of the English Count-Mass Distinction. In Proceedings of the 7th Generative Approaches to Second Language Acquisition Conference (GASLA 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu</author>
</authors>
<title>Ming Zhou, Jianfeng Gao, Endong Xun, and Changning Huang.</title>
<date>2000</date>
<publisher>ACL</publisher>
<marker>Liu, 2000</marker>
<rawString>Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun, and Changning Huang. 2000. PENS: A Machineaided English Writing System for Chinese Users. ACL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deryle Lonsdale</author>
<author>Diane Strong-Krause</author>
</authors>
<title>Automated Rating of ESL Essays.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT/NAACL Workshop: Building Educational Applications Using Natural Language Processing.</booktitle>
<contexts>
<context position="2308" citStr="Lonsdale &amp; Strong-Krause, 2003" startWordPosition="359" endWordPosition="362"> little progress in this area over the last decade. Research into computer feedback for ESL writers remains largely focused on smallscale pedagogical systems implemented within the framework of CALL (Computer Aided Language Learning) (Reuer 2003; Vanderventer Faltin, 2003), while commercial ESL grammar checkers remain brittle and difficult to customize to meet the needs of ESL writers of different first-language (L1) backgrounds and skill levels. Some researchers have begun to apply statistical techniques to identify learner errors in the context of essay evaluation (Chodorow &amp; Leacock, 2000; Lonsdale &amp; Strong-Krause, 2003), to detect non-native text (Tomokiyo &amp; Jones, 2001), and to support lexical selection by ESL learners through first-language translation (Liu et al., 2000). However, none of this work appears to directly address the more general problem of how to robustly provide feedback to ESL writers—and for that matter non-native writers in any second language—in a way that is easily tailored to different L1 backgrounds and secondlanguage (L2) skill levels. In this paper, we show that a noisy channel model instantiated within the paradigm of Statistical Machine Translation (SMT) (Brown et al., 1993) can s</context>
</contexts>
<marker>Lonsdale, Strong-Krause, 2003</marker>
<rawString>Deryle Lonsdale and Diane Strong-Krause. 2003. Automated Rating of ESL Essays. In Proceedings of the HLT/NAACL Workshop: Building Educational Applications Using Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arul Menezes</author>
<author>Chris Quirk</author>
</authors>
<title>Microsoft Research Treelet Translation System: IWSLT Evaluation.</title>
<date>2005</date>
<booktitle>Proceedings of the International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="7631" citStr="Menezes &amp; Quirk 2005" startWordPosition="1224" endWordPosition="1227">errors produced by ESL writers of specific L1 backgrounds can be captured in the channel model as an emergent property of training data consisting ESL sentences aligned with their corrected edited counterparts. The highest frequency errors and infelicities should emerge as targets for replacement, while lesser frequency or idiosyncratic problems will in general not surface as false flags. 2.3 Implementation In this paper, we explore the use of a large-scale production statistical machine translation system to correct a class of ESL errors. A detailed description of the system can be found in (Menezes &amp; Quirk 2005) and (Quirk et al., 2005). In keeping with current best practices in SMT, our system is a phrasal machine translation system that attempts to learn mappings between “phrases” (which may not correspond to linguistic units) rather than individual words. What distinguishes T* = 250 this system from other phrasal SMT systems is that rather than aligning simple sequences of words, it maps small phrasal “treelets” generated by a dependency parse to corresponding strings in the target. This “Tree-To-String” model holds promise in that it allows us to potentially benefit from being able to access a ce</context>
</contexts>
<marker>Menezes, Quirk, 2005</marker>
<rawString>Arul Menezes, and Chris Quirk. 2005. Microsoft Research Treelet Translation System: IWSLT Evaluation. Proceedings of the International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation. ACL</title>
<date>2003</date>
<contexts>
<context position="9023" citStr="Och, 2003" startWordPosition="1440" endWordPosition="1441"> consideration when it is sought to handle ungrammatical or otherwise illformed ESL input, but also simultaneously to capture relationships not involving contiguous strings, for example determiner-noun relations. In our pilot study, this system was employed without modification to the system architecture. The sole adjustment made was to have both Source (erroneous) and Target (correct) sentences tokenized using an English language tokenizer. N-best results for phrasal alignment and ordering models in the decoder were optimized by lambda training via Maximum Bleu, along the lines described in (Och, 2003). 3 Data Development 3.1 Identifying Mass Nouns In this paper, we focus on countability errors associated with mass nouns. This class of errors (involving nouns that cannot be counted, such as information, pollution, and homework) is characteristically encountered in ESL writing by native speakers of several East Asian languages (Dalgish, 1983; Hua &amp; Lee, 2004).1 We began by identifying a list of English nouns that are frequently involved in mass/count errors in by writing by Chinese ESL learners, by taking the intersection of words which: • occurred in either the Longman Dictionary of Contemp</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och, 2003. Minimum error rate training in statistical machine translation. ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<publisher>ACL</publisher>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. ACL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency Tree Translation: Syntactically Informed Phrasal SMT.</title>
<date>2005</date>
<publisher>ACL</publisher>
<contexts>
<context position="7656" citStr="Quirk et al., 2005" startWordPosition="1229" endWordPosition="1232">ers of specific L1 backgrounds can be captured in the channel model as an emergent property of training data consisting ESL sentences aligned with their corrected edited counterparts. The highest frequency errors and infelicities should emerge as targets for replacement, while lesser frequency or idiosyncratic problems will in general not surface as false flags. 2.3 Implementation In this paper, we explore the use of a large-scale production statistical machine translation system to correct a class of ESL errors. A detailed description of the system can be found in (Menezes &amp; Quirk 2005) and (Quirk et al., 2005). In keeping with current best practices in SMT, our system is a phrasal machine translation system that attempts to learn mappings between “phrases” (which may not correspond to linguistic units) rather than individual words. What distinguishes T* = 250 this system from other phrasal SMT systems is that rather than aligning simple sequences of words, it maps small phrasal “treelets” generated by a dependency parse to corresponding strings in the target. This “Tree-To-String” model holds promise in that it allows us to potentially benefit from being able to access a certain amount of structura</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Tree Translation: Syntactically Informed Phrasal SMT. ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veit Reuer</author>
</authors>
<title>Error Recognition and Feedback with Lexical Functional Grammar.</title>
<date>2003</date>
<journal>CALICO Journal,</journal>
<volume>20</volume>
<issue>3</issue>
<pages>497--512</pages>
<contexts>
<context position="1922" citStr="Reuer 2003" startWordPosition="304" endWordPosition="305"> own, most notably English. Yet, for writers of English as a Second Language (ESL), useful editorial assistance geared to their needs is surprisingly hard to come by. Grammar checkers such as that provided in Microsoft Word have been designed primarily with native speakers in mind. Moreover, despite growing demand for ESL proofing tools, there has been remarkably little progress in this area over the last decade. Research into computer feedback for ESL writers remains largely focused on smallscale pedagogical systems implemented within the framework of CALL (Computer Aided Language Learning) (Reuer 2003; Vanderventer Faltin, 2003), while commercial ESL grammar checkers remain brittle and difficult to customize to meet the needs of ESL writers of different first-language (L1) backgrounds and skill levels. Some researchers have begun to apply statistical techniques to identify learner errors in the context of essay evaluation (Chodorow &amp; Leacock, 2000; Lonsdale &amp; Strong-Krause, 2003), to detect non-native text (Tomokiyo &amp; Jones, 2001), and to support lexical selection by ESL learners through first-language translation (Liu et al., 2000). However, none of this work appears to directly address t</context>
</contexts>
<marker>Reuer, 2003</marker>
<rawString>Veit Reuer. 2003. Error Recognition and Feedback with Lexical Functional Grammar. CALICO Journal, 20(3): 497-512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Mayfield Tomokiyo</author>
<author>Rosie Jones</author>
</authors>
<title>You’re not from round here, are you? Naive Bayes Detection of Non-Native Utterance Text. NAACL</title>
<date>2001</date>
<contexts>
<context position="2360" citStr="Tomokiyo &amp; Jones, 2001" startWordPosition="367" endWordPosition="370"> into computer feedback for ESL writers remains largely focused on smallscale pedagogical systems implemented within the framework of CALL (Computer Aided Language Learning) (Reuer 2003; Vanderventer Faltin, 2003), while commercial ESL grammar checkers remain brittle and difficult to customize to meet the needs of ESL writers of different first-language (L1) backgrounds and skill levels. Some researchers have begun to apply statistical techniques to identify learner errors in the context of essay evaluation (Chodorow &amp; Leacock, 2000; Lonsdale &amp; Strong-Krause, 2003), to detect non-native text (Tomokiyo &amp; Jones, 2001), and to support lexical selection by ESL learners through first-language translation (Liu et al., 2000). However, none of this work appears to directly address the more general problem of how to robustly provide feedback to ESL writers—and for that matter non-native writers in any second language—in a way that is easily tailored to different L1 backgrounds and secondlanguage (L2) skill levels. In this paper, we show that a noisy channel model instantiated within the paradigm of Statistical Machine Translation (SMT) (Brown et al., 1993) can successfully provide editorial assistance for non-nat</context>
</contexts>
<marker>Tomokiyo, Jones, 2001</marker>
<rawString>Laura Mayfield Tomokiyo and Rosie Jones. 2001. You’re not from round here, are you? Naive Bayes Detection of Non-Native Utterance Text. NAACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Vandeventer Faltin</author>
</authors>
<title>Natural language processing tools for computer assisted language learning.</title>
<date>2003</date>
<note>Linguistik online 17, 5/03 (http:// www.linguistik-online.de/17_03/vandeventer.html)</note>
<contexts>
<context position="1950" citStr="Faltin, 2003" startWordPosition="307" endWordPosition="308">h. Yet, for writers of English as a Second Language (ESL), useful editorial assistance geared to their needs is surprisingly hard to come by. Grammar checkers such as that provided in Microsoft Word have been designed primarily with native speakers in mind. Moreover, despite growing demand for ESL proofing tools, there has been remarkably little progress in this area over the last decade. Research into computer feedback for ESL writers remains largely focused on smallscale pedagogical systems implemented within the framework of CALL (Computer Aided Language Learning) (Reuer 2003; Vanderventer Faltin, 2003), while commercial ESL grammar checkers remain brittle and difficult to customize to meet the needs of ESL writers of different first-language (L1) backgrounds and skill levels. Some researchers have begun to apply statistical techniques to identify learner errors in the context of essay evaluation (Chodorow &amp; Leacock, 2000; Lonsdale &amp; Strong-Krause, 2003), to detect non-native text (Tomokiyo &amp; Jones, 2001), and to support lexical selection by ESL learners through first-language translation (Liu et al., 2000). However, none of this work appears to directly address the more general problem of h</context>
</contexts>
<marker>Faltin, 2003</marker>
<rawString>Anne Vandeventer Faltin. 2003. Natural language processing tools for computer assisted language learning. Linguistik online 17, 5/03 (http:// www.linguistik-online.de/17_03/vandeventer.html)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>