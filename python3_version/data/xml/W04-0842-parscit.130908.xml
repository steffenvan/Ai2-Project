<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000259">
<note confidence="0.65930725">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
WSD system based on Specialized Hidden Markov Model (upv-shmm-eaw)
</note>
<author confidence="0.621681">
Antonio Molina, Ferran Pla and Encarna Segarra
</author>
<affiliation confidence="0.561755">
Departament de Sistemes Inform`atics i Computaci´o
</affiliation>
<address confidence="0.406092">
Universitat Polit`ecnica de Val`encia
Camide Vera s/n Val`encia (Spain)
</address>
<email confidence="0.99668">
{amolina,fpla,esegarra}@dsic.upv.es
</email>
<sectionHeader confidence="0.995596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999334142857143">
We present a supervised approach to Word Sense
Disambiguation (WSD) based on Specialized Hid-
den Markov Models. We used as training data the
Semcor corpus and the test data set provided by
Senseval 2 competition and as dictionary the Word-
net 1.6. We evaluated our system on the English
all-word task of the Senseval-3 competition.
</bodyText>
<sectionHeader confidence="0.429235" genericHeader="keywords">
1 Description of the WSD System
</sectionHeader>
<bodyText confidence="0.9999697">
We consider WSD to be a tagging problem (Molina
et al., 2002a). The tagging process can be formu-
lated as a maximization problem using the Hidden
Markov Model (HMM) formalism. Let O be the
set of output tags considered, and I, the input vo-
cabulary of the application. Given an input sen-
tence, I = i1, ... , iT, where ij  I, the tag-
ging process consists of finding the sequence of tags
(O = o1, ... , oT, where oj  O) of maximum
probability on the model, that is:
</bodyText>
<equation confidence="0.91861825">
�
O = arg max
O
= arg mo I P(O) (I) P(I|O) I ; O  OT (1)
</equation>
<bodyText confidence="0.999958142857143">
Due to the fact that the probability P(I) is a con-
stant that can be ignored in the maximization pro-
cess, the problem is reduced to maximize the nu-
merator of equation 1. To solve this equation, the
Markov assumptions should be made in order to
simplify the problem. For a first-order HMM, the
problem is reduced to solve the following equation:
</bodyText>
<equation confidence="0.961865333333333">
(H P(oj |oj−1) · P(ij |oj) (2)
j:1 ...
T
</equation>
<bodyText confidence="0.6926174">
The parameters of equation 2 can be represented
as a first-order HMM where each state corresponds
to an output tag
represent the transi-
tion probabilities between states and
</bodyText>
<equation confidence="0.6281195">
oj,P(oj|oj−1)
P(ij|oj) rep-
</equation>
<bodyText confidence="0.96776996">
resent the probability of emission of input symbols,
ij, in every state,
The parameters of this model
are estimated by maximum likelihood from seman-
tic annotated corpora using an appropriate smooth-
ing method (linear interpolation in our work).
Different kinds of available linguistic information
can be useful to solve WSD. The training corpus we
used provides as input features: words (W), lemmas
and the corresponding POS tags (P); and it also
provides as output tags the WordNet senses.
WordNet senses can be represented by a sense key
which has the form
sense. The high
number of different sense keys and the scarce an-
notated training data make difficult the estimation
of the models. In order to alleviate this sparness
problem we considered the
sense field (S) of the
sense key associated to each lemma as the semantic
tag. This assumption reduces the size of the output
tag set and it does not lead to any loss of information
because we can obtain the sense key by concatenat-
ing the lemma to the output tag.
Therefore, in our system the input vocabulary is
</bodyText>
<equation confidence="0.749603">
I = W
</equation>
<bodyText confidence="0.978324909090909">
P, and the output vocabulary is
= S. In order to incorporate this kind of in-
formation to the model we used Specialized HMM
(SHMM) (Molina et al., 2002b). This technique
has been successfully applied to other disambigua-
tion tasks such as part-of-speech tagging (Pla and
Molina, 2004) and shallow parsing (Molina and Pla,
2002).
Other HMM-based approaches have also been
applied to WSD. In (Segond et al., 1997), they esti-
mated a bigram model of ambiguity classes from the
SemCor corpus for the task of disambiguating the
semantic categories corresponding to the lexicogra-
pher level. These semantic categories are codified
into the
sense field. Asecond-order HMM was
used in (Loupy et al., 1998) in a two-step strategy.
First, they determined the semantic category associ-
ated to a word. Then, they assigned the most prob-
able sense according to the word and the semantic
category.
A SHMM consists of chan
</bodyText>
<equation confidence="0.560180142857143">
oj.
(L)
lemma%lex
lex
×L×
O
lex
</equation>
<bodyText confidence="0.644535">
ging the topology of
</bodyText>
<equation confidence="0.79969975">
the HMM in order to get a more accurate model
P(O|I)
arg max
O
</equation>
<bodyText confidence="0.9999925">
which includes more information. This is done by
means of an initial step previous to the learning pro-
cess. It consists of the redefinition of the input vo-
cabulary and the output tags. This redefinition is
done by means of two processes which transform
the training set: the selection process, which is ap-
plied to the input vocabulary, and the specialization
process, which redefines the output tags.
</bodyText>
<subsectionHeader confidence="0.99967">
1.1 Selection process
</subsectionHeader>
<bodyText confidence="0.999926333333333">
The aim of the selection process is to choose which
input features are relevant to the task. This pro-
cess applies a determined selection criterion to I
that produces a new input vocabulary (I). This new
vocabulary consists of the concatenation of the rel-
evant input features selected.
Taking into account the input vocabulary I =
W x G x P, some selection criteria could be as fol-
lows: to consider only the word (wi), to consider
only the lemma (li), to consider the concatenation
of the word and its POS&apos; (wi · pi), and to consider
the concatenation of the lemma and its POS (li · pi).
Moreover, different criteria can be applied depend-
ing on the kind of word (e.g. distinguishing content
and non-content words).
For example, for the input word interest, which
has an entry in WordNet and whose lemma and POS
are interest and NN (common noun) respectively,
the input considered could be interest·1. For a non-
content word, such as the article a, we could con-
sider only its lemma a as input.
</bodyText>
<subsectionHeader confidence="0.97058">
1.2 Specialization process
</subsectionHeader>
<bodyText confidence="0.999402966666667">
The specialization process allows for the codifica-
tion of certain information into the context (that is,
into the states of the model). It consists of redefin-
ing the output tag set by adding information from
the input. This redefinition produces some changes
in the model topology, in order to allow the model
to better capture some contextual restrictions and to
get a more accurate model.
The application of a specialization criterion to O
produces a new output tag set (6), whose elements
are the result of the concatenation of some relevant
input features to the original output tags.
Taking into account that the POS input feature is
already codified in the lex sense field, only words
or lemmas can be considered in the specialization
process (wi· lex sensei or li· lex sensei).
This specialization can be total or partial depend-
ing on whether we specialize the model with all the
elements of a feature or only with a subset of them.
&apos;We mapped the POS tags to the following tags: 1 for
nouns, 2 for verbs, 3 for adjectives and 4 for adverbs.
For instance, the input token interest·1 is tagged
with the semantic tag 1:09:00:: in the training data
set. If we estimate that the lemma interest should
specialize the model, then the semantic tag is rede-
fined as interest·1:09:00::. Non-content words, that
share the same output tag (the symbol notag in our
system), could be also considered to specialize the
model. For example, for the word a, the specialized
output tag associated could be a·notag.
</bodyText>
<subsectionHeader confidence="0.995424">
1.3 System scheme
</subsectionHeader>
<bodyText confidence="0.999974277777778">
The disambiguation process is presented in (Figure
1). First, the original input sentence (I) is processed
in order to select its relevant features, providing the
input sentence (I). Then, the semantic tagging is
carried out through the Viterbi algorithm using the
estimated SHMM. WordNet is used to know all the
possible semantic tags associated to an input word.
If the input word is unknown for the model (i.e., the
word has not been seen in the training data set) the
system takes the first sense provided by WordNet.
The learning process of a SHMM is similar to the
learning of a basic HMM. The only difference is that
SHMM are based on an appropriate definition of the
input information to the learning process. This in-
formation consists of the input features (words, lem-
mas and POS tags) and the output tag set (senses)
provided by the training corpus. A SHMM is built
according to the following steps (see Figure 2):
</bodyText>
<listItem confidence="0.936671428571428">
1. To define which available input information is
relevant to the task (selection criterion).
2. To define which input features are relevant to
redefine or specialize the output tag set (spe-
cialization criterion).
3. To apply the chosen criteria to the original
training data set to produce a new one.
4. To learn a model from the new training data
set.
5. To disambiguate a development data set using
that model.
6. To evaluate the output of the WSD system in
order to compare the behavior of the selected
criteria on the development set.
</listItem>
<bodyText confidence="0.998688333333333">
These steps are done using different combina-
tions of input features in order to determine the best
selection criterion and the best total specialization
criterion. Once these criteria are determined, some
partial specializations are tested in order to improve
the performance of the model.
</bodyText>
<figureCaption confidence="0.9999615">
Figure 1: System Description
Figure 2: Learning Phase Description
</figureCaption>
<figure confidence="0.999185555555556">
HMM
WORDNET
Selection
criterion
Original Input sentence Input sentence
Selection
of Relevant
Features
Disambiguated sentence
WSD
I
~
I
TRAINING
SET
DEVELOPMENT
SET
DEVELOPMENT
REFERENCE SET
Input
sentence
Selection
criterion (1)
Specialization
criterion (2)
Specialization
Output Tags
Selection
of Relevant
Features
3
of
New
Training set
4 6
Learning
the
Model
Evaluation
Disambiguated
sentence
5
WSD
HMM
WORDNET
</figure>
<sectionHeader confidence="0.996741" genericHeader="introduction">
2 Experimental Work
</sectionHeader>
<bodyText confidence="0.999993444444444">
We used as training data the part of the SemCor cor-
pus which is semantically annotated and supervised
for nouns, verbs, adjectives and adverbs (that is, the
files contained in the Brown1 and the Brown2 fold-
ers of SemCor corpus), and the test data set provided
by Senseval-2. We used 10% of the training corpus
as a development data set in order to determine the
best selection and specialization criteria.
In the experiments, we used WordNet 1.6 as a
dictionary which supplies all the possible semantic
senses for a given word. Our system disambiguated
all the polysemic lemmas, that is, the coverage of
our system was 100% (therefore, precision and re-
call were the same). For unknown words (words
that did not appear in the training data set), we as-
signed the first sense in WordNet.
The best selection criterion determined from the
experimental work on the development set is as fol-
lows: if a word wi has a sense in WordNet we con-
catenate the lemma (li) and the POS (pi) associ-
ated to the word (wi) as input vocabulary. For non-
content words, we only consider their lemma (li) as
input.
The best specialization criterion consisted of se-
lecting the lemmas whose frequency in the training
data set was higher than a certain threshold (other
specialization criteria could have been chosen, but
frequency criterion usually worked well in other
tasks as we reported in (Molina and Pla, 2002)). In
order to determine which threshold maximized the
performance of the model, we conducted a tuning
experiment on the development set. The best per-
formance was obtained using the lemmas whose fre-
quency was higher than 20 (about 1,600 lemmas).
The performance of our system on the Senseval 3
data test set was 60.9% of precision and recall.
</bodyText>
<sectionHeader confidence="0.993012" genericHeader="method">
3 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999981647058824">
In our WSD system, the choice of the best special-
ization criterion is based on the results of the system
on the development set. The tuning experiments in-
cluded totally specialized models, which is equiva-
lent to consider the sense keys as the output vocab-
ulary, non-specialized models, which is equivalent
to consider the lex senses as the output vocabulary,
and partially specialized models using different sets
of lemmas.
For the best specialization criterion, we have not
studied the linguistic characteristics of the different
groups of synsets associated to the same lex sense
for non-specialized output tags. We think that we
could improve our WSD system through a more ad-
equate definition of the selection and specialization
criteria. This definition could be done using seman-
tic knowledge about the domain of the task.
</bodyText>
<sectionHeader confidence="0.999778" genericHeader="method">
4 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999805">
This work has been supported by the Spanish
research projects CICYT TIC2003-07158-C04-03
and TIC2003-08681-C02-02.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999817066666667">
C. Loupy, M. El-Beze, and P. F. Marteau. 1998.
Word Sense Disambiguation using HMM Tag-
ger. In Proceedings of the 1st International Con-
ference on Language Resources and Evaluation,
LREC, pages 1255–1258, Granada, Spain, May.
Antonio Molina and Ferran Pla. 2002. Shallow
Parsing using Specialized HMMs. Journal of
Machine Learning Research, 2:595–613.
Antonio Molina, Ferran Pla, and Encarna Segarra.
2002a. A Hidden Markov Model Approach to
Word Sense Disambiguation. In Proceedings
of the VIII Conferencia Iberoamericana de In-
teligencia Artificial, IBERAMIA2002, Sevilla,
Spain.
Antonio Molina, Ferran Pla, and Encarna Segarra.
2002b. Una formulaci´on unificada para resolver
distinto problemas de ambig¨uedad en PLN. Re-
vista para el Procesamiento del Lenguaje Natu-
ral, (SEPLN’02), Septiembre.
Ferran Pla and Antonio Molina. 2004. Improv-
ing Part-of-Speech Tagging using Lexicalized
HMMs. Natural Language Engineering, 10. In
press.
F. Segond, A. Schiller, G. Grefenstette, and J-P.
Chanod. 1997. An Experiment in Semantic Tag-
ging using Hidden Markov Model Tagging. In
Proceedings of the Joint ACL/EACL Workshop
on Automatic Information Extraction and Build-
ing of Lexical Semantic Resources, pages 78–81,
Madrid, Spain.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000616">
<note confidence="0.6448195">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics WSD system based on Specialized Hidden Markov Model (upv-shmm-eaw)</note>
<author confidence="0.983126">Ferran Pla Molina</author>
<affiliation confidence="0.985064">Departament de Sistemes Inform`atics i Universitat Polit`ecnica de</affiliation>
<address confidence="0.338943">Camide Vera s/n Val`encia</address>
<abstract confidence="0.997314840206185">We present a supervised approach to Word Sense Disambiguation (WSD) based on Specialized Hidden Markov Models. We used as training data the Semcor corpus and the test data set provided by Senseval 2 competition and as dictionary the Wordnet 1.6. We evaluated our system on the English all-word task of the Senseval-3 competition. 1 Description of the WSD System We consider WSD to be a tagging problem (Molina et al., 2002a). The tagging process can be formulated as a maximization problem using the Hidden Model (HMM) formalism. Let the of output tags considered, and the input vocabulary of the application. Given an input sen- ... , where the tagging process consists of finding the sequence of tags ... , where of maximum probability on the model, that is: � arg max O arg mo I (1) to the fact that the probability a constant that can be ignored in the maximization process, the problem is reduced to maximize the numerator of equation 1. To solve this equation, the Markov assumptions should be made in order to simplify the problem. For a first-order HMM, the problem is reduced to solve the following equation: T The parameters of equation 2 can be represented as a first-order HMM where each state corresponds to an output tag represent the transition probabilities between states and oj,P(oj|oj−1) P(ij|oj) represent the probability of emission of input symbols, ij, in every state, The parameters of this model are estimated by maximum likelihood from semantic annotated corpora using an appropriate smoothing method (linear interpolation in our work). Different kinds of available linguistic information can be useful to solve WSD. The training corpus we used provides as input features: words (W), lemmas and the corresponding POS tags (P); and it also provides as output tags the WordNet senses. WordNet senses can be represented by a sense key which has the form sense. The high number of different sense keys and the scarce annotated training data make difficult the estimation of the models. In order to alleviate this sparness problem we considered the sense field (S) of the sense key associated to each lemma as the semantic tag. This assumption reduces the size of the output tag set and it does not lead to any loss of information because we can obtain the sense key by concatenating the lemma to the output tag. Therefore, in our system the input vocabulary is I = W P, and the output vocabulary is = S. In order to incorporate this kind of information to the model we used Specialized HMM (SHMM) (Molina et al., 2002b). This technique has been successfully applied to other disambiguation tasks such as part-of-speech tagging (Pla and Molina, 2004) and shallow parsing (Molina and Pla, 2002). Other HMM-based approaches have also been applied to WSD. In (Segond et al., 1997), they estimated a bigram model of ambiguity classes from the SemCor corpus for the task of disambiguating the semantic categories corresponding to the lexicographer level. These semantic categories are codified into the sense field. Asecond-order HMM was used in (Loupy et al., 1998) in a two-step strategy. First, they determined the semantic category associated to a word. Then, they assigned the most probable sense according to the word and the semantic category. A SHMM consists of chan oj. (L) lemma%lex lex ×L× lex ging the topology of the HMM in order to get a more accurate model arg max O which includes more information. This is done by means of an initial step previous to the learning process. It consists of the redefinition of the input vocabulary and the output tags. This redefinition is done by means of two processes which transform training set: the which is apto the input vocabulary, and the process, which redefines the output tags. 1.1 Selection process aim of the is to choose which input features are relevant to the task. This proapplies a determined criterion produces a new input vocabulary This new vocabulary consists of the concatenation of the relevant input features selected. into account the input vocabulary x G x some selection criteria could be as folto consider only the word to consider the lemma to consider the concatenation the word and its and to consider concatenation of the lemma and its POS Moreover, different criteria can be applied depending on the kind of word (e.g. distinguishing content and non-content words). example, for the input word which an entry in whose lemma and POS noun) respectively, input considered could be For a nonword, such as the article we could cononly its lemma input. 1.2 Specialization process process for the codification of certain information into the context (that is, into the states of the model). It consists of redefining the output tag set by adding information from the input. This redefinition produces some changes in the model topology, in order to allow the model to better capture some contextual restrictions and to get a more accurate model. application of a criterion a new output tag set whose elements are the result of the concatenation of some relevant input features to the original output tags. Taking into account that the POS input feature is codified in the sense only words or lemmas can be considered in the specialization specialization can be depending on whether we specialize the model with all the elements of a feature or only with a subset of them. mapped the POS tags to the following tags: 1 for nouns, 2 for verbs, 3 for adjectives and 4 for adverbs. instance, the input token tagged the semantic tag the training data If we estimate that the lemma specialize the model, then the semantic tag is redeas Non-content words, that the same output tag (the symbol our system), could be also considered to specialize the For example, for the word the specialized tag associated could be 1.3 System scheme The disambiguation process is presented in (Figure First, the original input sentence is processed in order to select its relevant features, providing the sentence Then, the semantic tagging is carried out through the Viterbi algorithm using the SHMM. used to know all the possible semantic tags associated to an input word. If the input word is unknown for the model (i.e., the word has not been seen in the training data set) the takes the first sense provided by The learning process of a SHMM is similar to the learning of a basic HMM. The only difference is that SHMM are based on an appropriate definition of the input information to the learning process. This information consists of the input features (words, lemmas and POS tags) and the output tag set (senses) provided by the training corpus. A SHMM is built according to the following steps (see Figure 2): 1. To define which available input information is to the task 2. To define which input features are relevant to or output tag set 3. To apply the chosen criteria to the original training data set to produce a new one. 4. To learn a model from the new training data set. 5. To disambiguate a development data set using that model. 6. To evaluate the output of the WSD system in order to compare the behavior of the selected criteria on the development set. These steps are done using different combinations of input features in order to determine the best and the best total criterion. Once these criteria are determined, some partial specializations are tested in order to improve the performance of the model.</abstract>
<title confidence="0.91580375">Figure 1: System Description Figure 2: Learning Phase Description HMM WORDNET Selection criterion Original Input sentence Input sentence Selection of Relevant Features Disambiguated sentence WSD I ~ I SET DEVELOPMENT SET DEVELOPMENT REFERENCE SET</title>
<abstract confidence="0.9528774875">Input sentence Selection criterion (1) Specialization criterion (2) Specialization Output Tags of Features 3 of New Training set 4 6 Learning the Model Evaluation Disambiguated sentence 5 WSD HMM WORDNET 2 Experimental Work used as training data the part of the corpus which is semantically annotated and supervised for nouns, verbs, adjectives and adverbs (that is, the files contained in the Brown1 and the Brown2 foldof and the test data set provided We used 10% of the training corpus as a development data set in order to determine the the experiments, we used as a dictionary which supplies all the possible semantic senses for a given word. Our system disambiguated all the polysemic lemmas, that is, the coverage of our system was 100% (therefore, precision and recall were the same). For unknown words (words that did not appear in the training data set), we asthe first sense in best criterion from the experimental work on the development set is as folif a word a sense in conthe lemma and the POS associto the word as input vocabulary. For nonwords, we only consider their lemma as input. best criterion of selecting the lemmas whose frequency in the training data set was higher than a certain threshold (other specialization criteria could have been chosen, but frequency criterion usually worked well in other tasks as we reported in (Molina and Pla, 2002)). In order to determine which threshold maximized the performance of the model, we conducted a tuning experiment on the development set. The best performance was obtained using the lemmas whose frequency was higher than 20 (about 1,600 lemmas). The performance of our system on the Senseval 3 data test set was 60.9% of precision and recall. 3 Concluding remarks our WSD system, the choice of the best specialcriterion based on the results of the system on the development set. The tuning experiments included totally specialized models, which is equivato consider the keys the output vocabulary, non-specialized models, which is equivalent consider the senses the output vocabulary, and partially specialized models using different sets of lemmas. the best we have not studied the linguistic characteristics of the different of to the same sense for non-specialized output tags. We think that we could improve our WSD system through a more addefinition of the criteria. This definition could be done using semantic knowledge about the domain of the task. 4 Acknowledgments</abstract>
<note confidence="0.865413242424242">This work has been supported by the Spanish research projects CICYT TIC2003-07158-C04-03 and TIC2003-08681-C02-02. References C. Loupy, M. El-Beze, and P. F. Marteau. 1998. Word Sense Disambiguation using HMM Tag- In of the 1st International Conference on Language Resources and Evaluation, pages 1255–1258, Granada, Spain, May. Antonio Molina and Ferran Pla. 2002. Shallow using Specialized HMMs. of Learning 2:595–613. Antonio Molina, Ferran Pla, and Encarna Segarra. 2002a. A Hidden Markov Model Approach to Sense Disambiguation. In of the VIII Conferencia Iberoamericana de In- Artificial, Sevilla, Spain. Antonio Molina, Ferran Pla, and Encarna Segarra. 2002b. Una formulaci´on unificada para resolver problemas de ambig¨uedad en PLN. Revista para el Procesamiento del Lenguaje Natu- Septiembre. Ferran Pla and Antonio Molina. 2004. Improving Part-of-Speech Tagging using Lexicalized Language 10. In press. F. Segond, A. Schiller, G. Grefenstette, and J-P. Chanod. 1997. An Experiment in Semantic Tagging using Hidden Markov Model Tagging. In Proceedings of the Joint ACL/EACL Workshop on Automatic Information Extraction and Buildof Lexical Semantic pages 78–81,</note>
<address confidence="0.96294">Madrid, Spain.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Loupy</author>
<author>M El-Beze</author>
<author>P F Marteau</author>
</authors>
<title>Word Sense Disambiguation using HMM Tagger.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference on Language Resources and Evaluation, LREC,</booktitle>
<pages>1255--1258</pages>
<location>Granada, Spain,</location>
<contexts>
<context position="3688" citStr="Loupy et al., 1998" startWordPosition="626" endWordPosition="629">information to the model we used Specialized HMM (SHMM) (Molina et al., 2002b). This technique has been successfully applied to other disambiguation tasks such as part-of-speech tagging (Pla and Molina, 2004) and shallow parsing (Molina and Pla, 2002). Other HMM-based approaches have also been applied to WSD. In (Segond et al., 1997), they estimated a bigram model of ambiguity classes from the SemCor corpus for the task of disambiguating the semantic categories corresponding to the lexicographer level. These semantic categories are codified into the sense field. Asecond-order HMM was used in (Loupy et al., 1998) in a two-step strategy. First, they determined the semantic category associated to a word. Then, they assigned the most probable sense according to the word and the semantic category. A SHMM consists of chan oj. (L) lemma%lex lex ×L× O lex ging the topology of the HMM in order to get a more accurate model P(O|I) arg max O which includes more information. This is done by means of an initial step previous to the learning process. It consists of the redefinition of the input vocabulary and the output tags. This redefinition is done by means of two processes which transform the training set: the </context>
</contexts>
<marker>Loupy, El-Beze, Marteau, 1998</marker>
<rawString>C. Loupy, M. El-Beze, and P. F. Marteau. 1998. Word Sense Disambiguation using HMM Tagger. In Proceedings of the 1st International Conference on Language Resources and Evaluation, LREC, pages 1255–1258, Granada, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Molina</author>
<author>Ferran Pla</author>
</authors>
<title>Shallow Parsing using Specialized HMMs.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--595</pages>
<contexts>
<context position="3320" citStr="Molina and Pla, 2002" startWordPosition="567" endWordPosition="570">sense key associated to each lemma as the semantic tag. This assumption reduces the size of the output tag set and it does not lead to any loss of information because we can obtain the sense key by concatenating the lemma to the output tag. Therefore, in our system the input vocabulary is I = W P, and the output vocabulary is = S. In order to incorporate this kind of information to the model we used Specialized HMM (SHMM) (Molina et al., 2002b). This technique has been successfully applied to other disambiguation tasks such as part-of-speech tagging (Pla and Molina, 2004) and shallow parsing (Molina and Pla, 2002). Other HMM-based approaches have also been applied to WSD. In (Segond et al., 1997), they estimated a bigram model of ambiguity classes from the SemCor corpus for the task of disambiguating the semantic categories corresponding to the lexicographer level. These semantic categories are codified into the sense field. Asecond-order HMM was used in (Loupy et al., 1998) in a two-step strategy. First, they determined the semantic category associated to a word. Then, they assigned the most probable sense according to the word and the semantic category. A SHMM consists of chan oj. (L) lemma%lex lex ×</context>
<context position="10616" citStr="Molina and Pla, 2002" startWordPosition="1806" endWordPosition="1809"> sense in WordNet. The best selection criterion determined from the experimental work on the development set is as follows: if a word wi has a sense in WordNet we concatenate the lemma (li) and the POS (pi) associated to the word (wi) as input vocabulary. For noncontent words, we only consider their lemma (li) as input. The best specialization criterion consisted of selecting the lemmas whose frequency in the training data set was higher than a certain threshold (other specialization criteria could have been chosen, but frequency criterion usually worked well in other tasks as we reported in (Molina and Pla, 2002)). In order to determine which threshold maximized the performance of the model, we conducted a tuning experiment on the development set. The best performance was obtained using the lemmas whose frequency was higher than 20 (about 1,600 lemmas). The performance of our system on the Senseval 3 data test set was 60.9% of precision and recall. 3 Concluding remarks In our WSD system, the choice of the best specialization criterion is based on the results of the system on the development set. The tuning experiments included totally specialized models, which is equivalent to consider the sense keys </context>
</contexts>
<marker>Molina, Pla, 2002</marker>
<rawString>Antonio Molina and Ferran Pla. 2002. Shallow Parsing using Specialized HMMs. Journal of Machine Learning Research, 2:595–613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Molina</author>
<author>Ferran Pla</author>
<author>Encarna Segarra</author>
</authors>
<title>A Hidden Markov Model Approach to Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the VIII Conferencia Iberoamericana de Inteligencia Artificial, IBERAMIA2002,</booktitle>
<location>Sevilla,</location>
<contexts>
<context position="879" citStr="Molina et al., 2002" startWordPosition="125" endWordPosition="128">rran Pla and Encarna Segarra Departament de Sistemes Inform`atics i Computaci´o Universitat Polit`ecnica de Val`encia Camide Vera s/n Val`encia (Spain) {amolina,fpla,esegarra}@dsic.upv.es Abstract We present a supervised approach to Word Sense Disambiguation (WSD) based on Specialized Hidden Markov Models. We used as training data the Semcor corpus and the test data set provided by Senseval 2 competition and as dictionary the Wordnet 1.6. We evaluated our system on the English all-word task of the Senseval-3 competition. 1 Description of the WSD System We consider WSD to be a tagging problem (Molina et al., 2002a). The tagging process can be formulated as a maximization problem using the Hidden Markov Model (HMM) formalism. Let O be the set of output tags considered, and I, the input vocabulary of the application. Given an input sentence, I = i1, ... , iT, where ij  I, the tagging process consists of finding the sequence of tags (O = o1, ... , oT, where oj  O) of maximum probability on the model, that is: � O = arg max O = arg mo I P(O) (I) P(I|O) I ; O  OT (1) Due to the fact that the probability P(I) is a constant that can be ignored in the maximization process, the problem is reduced to maximiz</context>
<context position="3145" citStr="Molina et al., 2002" startWordPosition="541" endWordPosition="544">eys and the scarce annotated training data make difficult the estimation of the models. In order to alleviate this sparness problem we considered the sense field (S) of the sense key associated to each lemma as the semantic tag. This assumption reduces the size of the output tag set and it does not lead to any loss of information because we can obtain the sense key by concatenating the lemma to the output tag. Therefore, in our system the input vocabulary is I = W P, and the output vocabulary is = S. In order to incorporate this kind of information to the model we used Specialized HMM (SHMM) (Molina et al., 2002b). This technique has been successfully applied to other disambiguation tasks such as part-of-speech tagging (Pla and Molina, 2004) and shallow parsing (Molina and Pla, 2002). Other HMM-based approaches have also been applied to WSD. In (Segond et al., 1997), they estimated a bigram model of ambiguity classes from the SemCor corpus for the task of disambiguating the semantic categories corresponding to the lexicographer level. These semantic categories are codified into the sense field. Asecond-order HMM was used in (Loupy et al., 1998) in a two-step strategy. First, they determined the seman</context>
</contexts>
<marker>Molina, Pla, Segarra, 2002</marker>
<rawString>Antonio Molina, Ferran Pla, and Encarna Segarra. 2002a. A Hidden Markov Model Approach to Word Sense Disambiguation. In Proceedings of the VIII Conferencia Iberoamericana de Inteligencia Artificial, IBERAMIA2002, Sevilla, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Molina</author>
<author>Ferran Pla</author>
<author>Encarna Segarra</author>
</authors>
<title>Una formulaci´on unificada para resolver distinto problemas de ambig¨uedad en PLN. Revista para el Procesamiento del Lenguaje Natural,</title>
<date>2002</date>
<location>(SEPLN’02), Septiembre.</location>
<contexts>
<context position="879" citStr="Molina et al., 2002" startWordPosition="125" endWordPosition="128">rran Pla and Encarna Segarra Departament de Sistemes Inform`atics i Computaci´o Universitat Polit`ecnica de Val`encia Camide Vera s/n Val`encia (Spain) {amolina,fpla,esegarra}@dsic.upv.es Abstract We present a supervised approach to Word Sense Disambiguation (WSD) based on Specialized Hidden Markov Models. We used as training data the Semcor corpus and the test data set provided by Senseval 2 competition and as dictionary the Wordnet 1.6. We evaluated our system on the English all-word task of the Senseval-3 competition. 1 Description of the WSD System We consider WSD to be a tagging problem (Molina et al., 2002a). The tagging process can be formulated as a maximization problem using the Hidden Markov Model (HMM) formalism. Let O be the set of output tags considered, and I, the input vocabulary of the application. Given an input sentence, I = i1, ... , iT, where ij  I, the tagging process consists of finding the sequence of tags (O = o1, ... , oT, where oj  O) of maximum probability on the model, that is: � O = arg max O = arg mo I P(O) (I) P(I|O) I ; O  OT (1) Due to the fact that the probability P(I) is a constant that can be ignored in the maximization process, the problem is reduced to maximiz</context>
<context position="3145" citStr="Molina et al., 2002" startWordPosition="541" endWordPosition="544">eys and the scarce annotated training data make difficult the estimation of the models. In order to alleviate this sparness problem we considered the sense field (S) of the sense key associated to each lemma as the semantic tag. This assumption reduces the size of the output tag set and it does not lead to any loss of information because we can obtain the sense key by concatenating the lemma to the output tag. Therefore, in our system the input vocabulary is I = W P, and the output vocabulary is = S. In order to incorporate this kind of information to the model we used Specialized HMM (SHMM) (Molina et al., 2002b). This technique has been successfully applied to other disambiguation tasks such as part-of-speech tagging (Pla and Molina, 2004) and shallow parsing (Molina and Pla, 2002). Other HMM-based approaches have also been applied to WSD. In (Segond et al., 1997), they estimated a bigram model of ambiguity classes from the SemCor corpus for the task of disambiguating the semantic categories corresponding to the lexicographer level. These semantic categories are codified into the sense field. Asecond-order HMM was used in (Loupy et al., 1998) in a two-step strategy. First, they determined the seman</context>
</contexts>
<marker>Molina, Pla, Segarra, 2002</marker>
<rawString>Antonio Molina, Ferran Pla, and Encarna Segarra. 2002b. Una formulaci´on unificada para resolver distinto problemas de ambig¨uedad en PLN. Revista para el Procesamiento del Lenguaje Natural, (SEPLN’02), Septiembre.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferran Pla</author>
<author>Antonio Molina</author>
</authors>
<title>Improving Part-of-Speech Tagging using Lexicalized HMMs.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<volume>10</volume>
<note>In press.</note>
<contexts>
<context position="3277" citStr="Pla and Molina, 2004" startWordPosition="560" endWordPosition="563">m we considered the sense field (S) of the sense key associated to each lemma as the semantic tag. This assumption reduces the size of the output tag set and it does not lead to any loss of information because we can obtain the sense key by concatenating the lemma to the output tag. Therefore, in our system the input vocabulary is I = W P, and the output vocabulary is = S. In order to incorporate this kind of information to the model we used Specialized HMM (SHMM) (Molina et al., 2002b). This technique has been successfully applied to other disambiguation tasks such as part-of-speech tagging (Pla and Molina, 2004) and shallow parsing (Molina and Pla, 2002). Other HMM-based approaches have also been applied to WSD. In (Segond et al., 1997), they estimated a bigram model of ambiguity classes from the SemCor corpus for the task of disambiguating the semantic categories corresponding to the lexicographer level. These semantic categories are codified into the sense field. Asecond-order HMM was used in (Loupy et al., 1998) in a two-step strategy. First, they determined the semantic category associated to a word. Then, they assigned the most probable sense according to the word and the semantic category. A SH</context>
</contexts>
<marker>Pla, Molina, 2004</marker>
<rawString>Ferran Pla and Antonio Molina. 2004. Improving Part-of-Speech Tagging using Lexicalized HMMs. Natural Language Engineering, 10. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Segond</author>
<author>A Schiller</author>
<author>G Grefenstette</author>
<author>J-P Chanod</author>
</authors>
<title>An Experiment in Semantic Tagging using Hidden Markov Model Tagging.</title>
<date>1997</date>
<booktitle>In Proceedings of the Joint ACL/EACL Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources,</booktitle>
<pages>78--81</pages>
<location>Madrid,</location>
<contexts>
<context position="3404" citStr="Segond et al., 1997" startWordPosition="581" endWordPosition="584">ize of the output tag set and it does not lead to any loss of information because we can obtain the sense key by concatenating the lemma to the output tag. Therefore, in our system the input vocabulary is I = W P, and the output vocabulary is = S. In order to incorporate this kind of information to the model we used Specialized HMM (SHMM) (Molina et al., 2002b). This technique has been successfully applied to other disambiguation tasks such as part-of-speech tagging (Pla and Molina, 2004) and shallow parsing (Molina and Pla, 2002). Other HMM-based approaches have also been applied to WSD. In (Segond et al., 1997), they estimated a bigram model of ambiguity classes from the SemCor corpus for the task of disambiguating the semantic categories corresponding to the lexicographer level. These semantic categories are codified into the sense field. Asecond-order HMM was used in (Loupy et al., 1998) in a two-step strategy. First, they determined the semantic category associated to a word. Then, they assigned the most probable sense according to the word and the semantic category. A SHMM consists of chan oj. (L) lemma%lex lex ×L× O lex ging the topology of the HMM in order to get a more accurate model P(O|I) a</context>
</contexts>
<marker>Segond, Schiller, Grefenstette, Chanod, 1997</marker>
<rawString>F. Segond, A. Schiller, G. Grefenstette, and J-P. Chanod. 1997. An Experiment in Semantic Tagging using Hidden Markov Model Tagging. In Proceedings of the Joint ACL/EACL Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources, pages 78–81, Madrid, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>