<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.126856">
<title confidence="0.992024">
Analysis of Summarization Evaluation Experiments
</title>
<author confidence="0.984791">
Marie-Josée Goulet
</author>
<affiliation confidence="0.977943">
CIRAL, Department of Linguistics
Laval University, Quebec City
</affiliation>
<address confidence="0.877042">
G1K 7P4, Canada
</address>
<email confidence="0.996154">
marie-josee.goulet.1@ulaval.ca
</email>
<sectionHeader confidence="0.995592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999815533333333">
The goals of my dissertation are: 1) to pro-
pose a French terminology for the presen-
tation of evaluation results of automatic
summaries, 2) to identify and describe
experimental variables in evaluations of
automatic summaries, 3) to highlight the
most common tendencies, inconsistencies
and methodological problems in summa-
rization evaluation experiments, and 4)
to make recommendations for the presen-
tation of evaluation results of automatic
summaries. In this paper, I focus on the
second objective, i.e. identifying and de-
scribing variables in summarization eval-
uation experiments.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999551642857143">
The general subject of my dissertation is summa-
rization evaluation. As stated in my thesis proposal,
my work aims at four goals: 1) proposing a French
terminology for the presentation of evaluation re-
sults of automatic summaries, 2) identifying and de-
scribing experimental variables in evaluations of au-
tomatic summaries, 3) highlighting the most com-
mon tendencies, inconsistencies and methodological
problems in summarization evaluations, and 4) mak-
ing recommendations for the presentation of evalua-
tion results of automatic summaries. In this paper, I
will focus on the second objective.
My ultimate goal is to provide the francophone
scientific community with guidelines for the evalua-
</bodyText>
<page confidence="0.985608">
17
</page>
<bodyText confidence="0.999839">
tion of automatic summaries of French texts. Evalu-
ation campaigns for NLP applications already exist
in France, the EVALDA project1. However, no cam-
paign has yet been launched for French automatic
summaries, like Document Understanding Confer-
ences for English texts or Text Summarization Chal-
lenge for Japanese texts. I hope that such a campaign
will begin in the near future and that my thesis work
may then serve as a guide for its design.
</bodyText>
<sectionHeader confidence="0.997283" genericHeader="introduction">
2 Completed Work
</sectionHeader>
<bodyText confidence="0.999961785714286">
I collected 22 scientific papers about summarization
evaluation, published between 1961 and 2005. Each
paper has been the subject of an in-depth analysis,
where every detail regarding the evaluation has been
carefully noted, yielding a quasi-monstrous amount
of experimental variables. These variables have
been classified into four categories: 1) information
about source texts, 2) information about automatic
summaries being evaluated, 3) information about
other summaries used in the evaluation process, and
4) information about evaluation methods and crite-
ria. At the current stage of my research work, the
first three types of variables have been analyzed and
will be presented here.
</bodyText>
<subsectionHeader confidence="0.994954">
2.1 Variables about source texts
</subsectionHeader>
<bodyText confidence="0.9981276">
Four types of information about source texts
emerged from the analysis: 1) the number of source
texts, 2) the length, 3) the type of text, and 4) the lan-
guage. First, the number of source texts is an indica-
tor of the significance of the evaluation. In my study,
</bodyText>
<footnote confidence="0.973629">
1http://www.elda.org/rubrique25.html
</footnote>
<subsubsectionHeader confidence="0.249119">
Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 17–20,
</subsubsectionHeader>
<bodyText confidence="0.989598290322581">
Rochester, April 2007. c�2007 Association for Computational Linguistics
all the evaluations used less than 100 source texts,
except for Mani and Bloedorn (1999) (300 source
texts), Brandow et al. (1995) (250 source texts), Ku-
piec et al. (1995) (188 source texts) and Teufel and
Moens (1999) (123 source texts).
Secondly, regarding source text length, it is ex-
pressed in different ways from one evaluation to an-
other. For example, Edmundson (1969) gives the
number of words, Klavans et al. (1998) give the
number of sentences and Minel et al. (1997) give
the number of pages. In some papers, the length
of the shortest and of the longest text is provided
(Marcu, 1999) while in others it is the average num-
ber of words, sentences or pages that is given (Teufel
and Moens, 1999). Obviously, it would be wise to
standardize the way source texts length is given in
evaluation experiments.
In my corpora, there are three main types of
source texts: 1) scientific papers, 2) technical re-
ports, and 3) newspapers. Also, Minel et al. (1997)
used book extracts and memos, and Farzindar and
Lapalme (2005) used judgments of the Canadian
federal court. All evaluations used only one type of
source texts, except for Kupiec et al. (1995) and for
Minel et al. (1997).
Finally, the majority of the evaluations used Eng-
lish texts. Some authors used French texts (Minel et
al., 1997; Châar et al., 2004), Korean texts (Myaeng
and Jang, 1999) or Japanese texts (Nanba and Oku-
mura, 2000).
</bodyText>
<subsectionHeader confidence="0.9426375">
2.2 Variables about automatic summaries
being evaluated
</subsectionHeader>
<bodyText confidence="0.999976">
In this section, I describe variables about automatic
summaries being evaluated. The variables have been
classified into six categories: 1) the total number
of automatic summaries evaluated, 2) the number
of automatic summaries produced per source text,
3) if they are multiple document summaries, 4) the
length, 5) if they are extracts or abstracts, and 6)
their purpose.
First, concerning the total number of automatic
summaries, Brandow et al. (1995), Mani and Bloe-
dorn (1999), Kupiec et al. (1995), Salton et al.
(1997) and Teufel and Moens (1999) evaluated re-
spectively 750, 300, 188, 150 and 123 automatic
summaries. All the other studies for which this in-
formation is given evaluated less than 100 automatic
summaries. It may appear redundant to give the
number of source texts and the number of automatic
summaries in an evaluation, but sometimes more
than one automatic summary per source text may
have been produced. This is the case in Brandow
et al. (1995) and Barzilay and Elhadad (1999) where
automatic summaries of different lengths have been
evaluated.
Automatic summaries can either be produced
from one text or more than one text. In my cor-
pora, only Mani and Bloedorn (1999) and Châar et
al. (2004) evaluated multiple document summaries.
As for source texts, automatic summary length is
expressed in different ways from one evaluation to
another. Moreover, it is not always expressed in the
same way than source text length, which is inconsis-
tent.
On a different note, most experiments evaluated
extracts, except for Maybury (1999) and Saggion
and Lapalme (2002) who evaluated abstracts, re-
flecting the predominance of systems producing ex-
tracts in the domain of summarization. Extracts are
summaries produced by extracting the most impor-
tant segments from texts while abstracts are the re-
sult of a comprehension process and text generation.
Most extracts evaluated are composed of sentences,
except for Salton et al. (1997) and Châar et al. (2004)
where they are respectively composed of paragraphs
and passages. The type of automatic summaries is
crucial information because it normally influences
the choice of the evaluation method and criteria. In-
deed, we do not evaluate extracts and abstracts in the
same way since they are not produced in the same
way. Also, their purposes generally differ, which can
also influence the choice of the evaluation method
and criteria.
Last, some papers contain the specific purpose of
automatic summaries, not only if they are indica-
tive or informative, which is interesting because it
can sometimes explain the choice of the evaluation
method. Only 9 experiments out of 22 give this in-
formation in my corpora.
</bodyText>
<subsectionHeader confidence="0.907837">
2.3 Variables about other summaries used in
the evaluation process
</subsectionHeader>
<bodyText confidence="0.999904333333333">
One of the most common evaluation methods con-
sists of comparing automatic summaries with other
summaries. During my analysis, I identified seven
</bodyText>
<page confidence="0.996017">
18
</page>
<bodyText confidence="0.998030162162162">
types of information about these other summaries:
1) the total number of other summaries, 2) the type
of summaries, 3) the length, 4) the total number of
human summarizers, 5) the number of human sum-
marizers per source text, 6) the instructions given to
the human summarizers, and 7) the human summa-
rizers’ profile.
The number of other summaries does not neces-
sarily correspond to the number of automatic sum-
maries evaluated, depending on many factors: the
use of other summaries of different types or different
lengths, the number of persons producing the other
summaries, the number of other systems producing
the other summaries, and so on.
There are two general types of summaries used
for comparison with the automatic summaries be-
ing evaluated. First, gold standard summaries (or
target summaries) can be author summaries, pro-
fessional summaries or summaries produced specif-
ically for the evaluation. Second, baseline sum-
maries are generally produced by extracting random
sentences from source texts or produced by another
system.
In my corpora, gold standard summaries are of-
ten produced specifically for the evaluation. In most
cases, they are produced by manually extracting the
most important passages, sentences or paragraphs,
allowing automatic comparison between automatic
summaries and gold standard summaries.
On the other hand, many evaluations used base-
line summaries. For example, Barzilay and Elhadad
(1999) used summaries produced by Word AutoSum-
marize, Hovy and Lin (1999) used summaries pro-
duced by automatically extracting random sentences
from source texts. In Brandow et al. (1995), Kupiec
et al. (1995) and Teufel and Moens (1999), baseline
summaries were produced by automatically extract-
ing sentences at the beginning of the texts, and in
Myaeng and Jang (1999) by extracting the first five
sentences of the conclusion.
Logically, the length of the summaries used for
the comparison should be equivalent to the length of
the automatic summaries being evaluated. If auto-
matic summaries of different lengths are evaluated,
there should be corresponding baselines and/or gold
standard summaries for each length, unless the goal
of the evaluation is to determine if the length plays a
role in the quality of automatic summaries.
Many of the evaluations analyzed do not indicate
the number of human summarizers participating in
the production of gold standard summaries. A few of
them specify the total number of persons involved,
but not the number for each source text. This is an
important variable because summarizing, either by
extracting or abstracting, is a subjective task. The
more people involved in the summarization of one
text, the more we can consider the final summary
to be reliable. From the pieces of information I was
able to gather, the number of summarizers per source
text ranges from 1 to 13 in my corpora.
In analyzing the evaluations of my corpora, I re-
alized that some authors gave clear instructions to
the human summarizers, for example Edmundson
(1969). In other cases, authors asked the summariz-
ers to extract the most “important” sentences. The
term “important” includes other terms like represen-
tative, informative, relevant, and eligible. It is rarely
mentioned however if those words were explained to
the summarizers.
I also noticed that some evaluations used people
coming from different backgrounds, for example in
Salton et al. (1997), while others used more homo-
geneous groups, for example in Barzilay and El-
hadad (1999) and Kupiec et al. (1995).
</bodyText>
<sectionHeader confidence="0.993921" genericHeader="method">
3 Future Directions
</sectionHeader>
<bodyText confidence="0.99997475">
In the next couple of months, I plan to analyze evalu-
ation methods identified in my corpora, for example
comparing automatic summaries with gold standard
or baseline summaries, and asking judges to give
their opinion on the quality of automatic summaries.
I will also describe evaluation criteria used to as-
sess the quality of the automatic summaries, for ex-
ample informativeness and readability. Next, I will
make recommendations for the presentation of sum-
marization evaluation results, based on the knowl-
edge acquired from my analysis of 22 scientific pa-
pers, and from previous evaluation campaigns.
</bodyText>
<sectionHeader confidence="0.997176" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999889">
In this paper, I described variables about source
texts, about automatic summaries being evaluated
and about other summaries used in summarization
evaluation experiments. These variables provide
important information for the understanding of the
</bodyText>
<page confidence="0.998172">
19
</page>
<bodyText confidence="0.9999090625">
evaluation results presented in a scientific paper. My
analysis is based on 22 scientific papers on summa-
rization evaluation, which is to my knowledge the
largest study on the variables found in evaluation ex-
periments. This constitutes a notable contribution in
the domain of summarization. In another paper (in
French) to appear, I propose a French terminology
for the presentation of evaluation results in the do-
main of summarization, which is also a major con-
tribution.
To conclude, the analysis presented in this pa-
per gave an overview of summarization evaluation
habits since 1961. Also, it showed that there is
no common agreement as to how evaluation results
should be presented in a scientific paper about auto-
matic summaries.
</bodyText>
<sectionHeader confidence="0.996281" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9994928">
I would like to thank the SSHRC and the FQRSC
for granting me doctoral scholarships. I would also
like to thank Joël Bourgeoys, Neil Cruickshank,
Lorraine Couture and the anonymous reviewer for
their useful comments.
</bodyText>
<sectionHeader confidence="0.998009" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999501492957747">
R. Barzilay and M. Elhadad. 1999. Using lexical chains
for text summarization. In I. Mani and M. T. May-
bury, editors, Advances in Automatic Text Summariza-
tion, pages 111–121, Cambridge, Massachusetts. MIT
Press.
R. Brandow, K. Mitze, and L. Rau. 1995. Auto-
matic condensation of electronic publications by sen-
tence selection. Information Processing Management,
31(5):675–685.
S. L. Châar, O. Ferret, and C. Fluhr. 2004. Filtrage pour
la construction de résumés multidocuments guidée
par un profil. Traitement automatique des langues,
45(1):65–93.
H. P. Edmundson. 1969. New methods in automatic ab-
stracting. Journal of the Association for Computing
Machinery, 16(2):264–285.
A. Farzindar and G. Lapalme. 2005. Production automa-
tique de résumé de textes juridiques : évaluation de
qualité et d’acceptabilité. In TALN, pages 183–192,
Dourdan.
E. Hovy and C.-Y. Lin. 1999. Automated text sum-
marization in SUMMARIST. In I. Mani and M. T.
Maybury, editors, Advances in Automatic Text Sum-
marization, pages 81–94, Cambridge, Massachusetts.
MIT Press.
J. L. Klavans, K. R. McKeown, M.-Y. Kan, and S. Lee.
1998. Resources for the evaluation of summarization
techniques. In Antonio Zampolli, editor, LREC, pages
899–902, Granada, Spain.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable
document summarizer. In SIGIR, pages 68–73, Seat-
tle.
I. Mani and E. Bloedorn. 1999. Summarizing simi-
larities and differences among related documents. In
I. Mani and M. T. Maybury, editors, Advances in Au-
tomatic Text Summarization, pages 357–379, Cam-
bridge, Massachusetts. MIT Press.
D. Marcu. 1999. Discourse trees are good indicators
of importance in text. In I. Mani and M. T. May-
bury, editors, Advances in Automatic Text Summariza-
tion, pages 123–136, Cambridge, Massachusetts. MIT
Press.
M. Maybury. 1999. Generating summaries from event
data. In I. Mani and M. T. Maybury, editors, Ad-
vances in Automatic Text Summarization, pages 265–
281, Cambridge, Massachusetts. MIT Press.
J.-L. Minel, S. Nugier, and G. Piat. 1997. How to ap-
preciate the quality of automatic text summarization?
Examples of FAN and MLUCE protocols and their re-
sults on SERAPHIN. In EACL, pages 25–31, Madrid.
S. H. Myaeng and D.-H. Jang. 1999. Development
and evaluation of a statistically-based document sum-
marization system. In I. Mani and M. T. Maybury,
editors, Advances in Automatic Text Summarization,
pages 61–70, Cambridge, Massachusetts. MIT Press.
H. Nanba and M. Okumura. 2000. Producing more
readable extracts by revising them. In 18th Inter-
national Conference on Computational Linguistics,
pages 1071–1075, Saarbrucker.
H. Saggion and G. Lapalme. 2002. Generat-
ing indicative-informative summaries with SumUM.
Computational Linguistics, 28(4):497–526.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 33(2):193–207.
S. Teufel and M. Moens. 1999. Argumentative clas-
sification of extracted sentences as a first step to-
wards flexible abstracting. In I. Mani and M. T. May-
bury, editors, Advances in Automatic Text Summariza-
tion, pages 155–171, Cambridge, Massachusetts. MIT
Press.
</reference>
<page confidence="0.994713">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.503894">
<title confidence="0.982999">Analysis of Summarization Evaluation Experiments</title>
<author confidence="0.900891">Marie-Josée</author>
<affiliation confidence="0.989234">CIRAL, Department of Laval University, Quebec</affiliation>
<address confidence="0.60903">G1K 7P4,</address>
<email confidence="0.895543">marie-josee.goulet.1@ulaval.ca</email>
<abstract confidence="0.997498625">The goals of my dissertation are: 1) to propose a French terminology for the presentation of evaluation results of automatic summaries, 2) to identify and describe experimental variables in evaluations of automatic summaries, 3) to highlight the most common tendencies, inconsistencies and methodological problems in summarization evaluation experiments, and 4) to make recommendations for the presentation of evaluation results of automatic summaries. In this paper, I focus on the second objective, i.e. identifying and describing variables in summarization evaluation experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization,</booktitle>
<pages>111--121</pages>
<editor>In I. Mani and M. T. Maybury, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="5552" citStr="Barzilay and Elhadad (1999)" startWordPosition="873" endWordPosition="876">First, concerning the total number of automatic summaries, Brandow et al. (1995), Mani and Bloedorn (1999), Kupiec et al. (1995), Salton et al. (1997) and Teufel and Moens (1999) evaluated respectively 750, 300, 188, 150 and 123 automatic summaries. All the other studies for which this information is given evaluated less than 100 automatic summaries. It may appear redundant to give the number of source texts and the number of automatic summaries in an evaluation, but sometimes more than one automatic summary per source text may have been produced. This is the case in Brandow et al. (1995) and Barzilay and Elhadad (1999) where automatic summaries of different lengths have been evaluated. Automatic summaries can either be produced from one text or more than one text. In my corpora, only Mani and Bloedorn (1999) and Châar et al. (2004) evaluated multiple document summaries. As for source texts, automatic summary length is expressed in different ways from one evaluation to another. Moreover, it is not always expressed in the same way than source text length, which is inconsistent. On a different note, most experiments evaluated extracts, except for Maybury (1999) and Saggion and Lapalme (2002) who evaluated abst</context>
<context position="8863" citStr="Barzilay and Elhadad (1999)" startWordPosition="1395" endWordPosition="1398">ies) can be author summaries, professional summaries or summaries produced specifically for the evaluation. Second, baseline summaries are generally produced by extracting random sentences from source texts or produced by another system. In my corpora, gold standard summaries are often produced specifically for the evaluation. In most cases, they are produced by manually extracting the most important passages, sentences or paragraphs, allowing automatic comparison between automatic summaries and gold standard summaries. On the other hand, many evaluations used baseline summaries. For example, Barzilay and Elhadad (1999) used summaries produced by Word AutoSummarize, Hovy and Lin (1999) used summaries produced by automatically extracting random sentences from source texts. In Brandow et al. (1995), Kupiec et al. (1995) and Teufel and Moens (1999), baseline summaries were produced by automatically extracting sentences at the beginning of the texts, and in Myaeng and Jang (1999) by extracting the first five sentences of the conclusion. Logically, the length of the summaries used for the comparison should be equivalent to the length of the automatic summaries being evaluated. If automatic summaries of different </context>
<context position="10919" citStr="Barzilay and Elhadad (1999)" startWordPosition="1725" endWordPosition="1729"> analyzing the evaluations of my corpora, I realized that some authors gave clear instructions to the human summarizers, for example Edmundson (1969). In other cases, authors asked the summarizers to extract the most “important” sentences. The term “important” includes other terms like representative, informative, relevant, and eligible. It is rarely mentioned however if those words were explained to the summarizers. I also noticed that some evaluations used people coming from different backgrounds, for example in Salton et al. (1997), while others used more homogeneous groups, for example in Barzilay and Elhadad (1999) and Kupiec et al. (1995). 3 Future Directions In the next couple of months, I plan to analyze evaluation methods identified in my corpora, for example comparing automatic summaries with gold standard or baseline summaries, and asking judges to give their opinion on the quality of automatic summaries. I will also describe evaluation criteria used to assess the quality of the automatic summaries, for example informativeness and readability. Next, I will make recommendations for the presentation of summarization evaluation results, based on the knowledge acquired from my analysis of 22 scientifi</context>
</contexts>
<marker>Barzilay, Elhadad, 1999</marker>
<rawString>R. Barzilay and M. Elhadad. 1999. Using lexical chains for text summarization. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization, pages 111–121, Cambridge, Massachusetts. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brandow</author>
<author>K Mitze</author>
<author>L Rau</author>
</authors>
<title>Automatic condensation of electronic publications by sentence selection.</title>
<date>1995</date>
<journal>Information Processing Management,</journal>
<volume>31</volume>
<issue>5</issue>
<contexts>
<context position="3227" citStr="Brandow et al. (1995)" startWordPosition="481" endWordPosition="484">be presented here. 2.1 Variables about source texts Four types of information about source texts emerged from the analysis: 1) the number of source texts, 2) the length, 3) the type of text, and 4) the language. First, the number of source texts is an indicator of the significance of the evaluation. In my study, 1http://www.elda.org/rubrique25.html Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 17–20, Rochester, April 2007. c�2007 Association for Computational Linguistics all the evaluations used less than 100 source texts, except for Mani and Bloedorn (1999) (300 source texts), Brandow et al. (1995) (250 source texts), Kupiec et al. (1995) (188 source texts) and Teufel and Moens (1999) (123 source texts). Secondly, regarding source text length, it is expressed in different ways from one evaluation to another. For example, Edmundson (1969) gives the number of words, Klavans et al. (1998) give the number of sentences and Minel et al. (1997) give the number of pages. In some papers, the length of the shortest and of the longest text is provided (Marcu, 1999) while in others it is the average number of words, sentences or pages that is given (Teufel and Moens, 1999). Obviously, it would be w</context>
<context position="5005" citStr="Brandow et al. (1995)" startWordPosition="779" endWordPosition="782">., 1997; Châar et al., 2004), Korean texts (Myaeng and Jang, 1999) or Japanese texts (Nanba and Okumura, 2000). 2.2 Variables about automatic summaries being evaluated In this section, I describe variables about automatic summaries being evaluated. The variables have been classified into six categories: 1) the total number of automatic summaries evaluated, 2) the number of automatic summaries produced per source text, 3) if they are multiple document summaries, 4) the length, 5) if they are extracts or abstracts, and 6) their purpose. First, concerning the total number of automatic summaries, Brandow et al. (1995), Mani and Bloedorn (1999), Kupiec et al. (1995), Salton et al. (1997) and Teufel and Moens (1999) evaluated respectively 750, 300, 188, 150 and 123 automatic summaries. All the other studies for which this information is given evaluated less than 100 automatic summaries. It may appear redundant to give the number of source texts and the number of automatic summaries in an evaluation, but sometimes more than one automatic summary per source text may have been produced. This is the case in Brandow et al. (1995) and Barzilay and Elhadad (1999) where automatic summaries of different lengths have </context>
<context position="9043" citStr="Brandow et al. (1995)" startWordPosition="1423" endWordPosition="1426">es from source texts or produced by another system. In my corpora, gold standard summaries are often produced specifically for the evaluation. In most cases, they are produced by manually extracting the most important passages, sentences or paragraphs, allowing automatic comparison between automatic summaries and gold standard summaries. On the other hand, many evaluations used baseline summaries. For example, Barzilay and Elhadad (1999) used summaries produced by Word AutoSummarize, Hovy and Lin (1999) used summaries produced by automatically extracting random sentences from source texts. In Brandow et al. (1995), Kupiec et al. (1995) and Teufel and Moens (1999), baseline summaries were produced by automatically extracting sentences at the beginning of the texts, and in Myaeng and Jang (1999) by extracting the first five sentences of the conclusion. Logically, the length of the summaries used for the comparison should be equivalent to the length of the automatic summaries being evaluated. If automatic summaries of different lengths are evaluated, there should be corresponding baselines and/or gold standard summaries for each length, unless the goal of the evaluation is to determine if the length plays</context>
</contexts>
<marker>Brandow, Mitze, Rau, 1995</marker>
<rawString>R. Brandow, K. Mitze, and L. Rau. 1995. Automatic condensation of electronic publications by sentence selection. Information Processing Management, 31(5):675–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Châar</author>
<author>O Ferret</author>
<author>C Fluhr</author>
</authors>
<title>Filtrage pour la construction de résumés multidocuments guidée par un profil. Traitement automatique des langues,</title>
<date>2004</date>
<pages>45--1</pages>
<contexts>
<context position="4412" citStr="Châar et al., 2004" startWordPosition="688" endWordPosition="691">s, 1999). Obviously, it would be wise to standardize the way source texts length is given in evaluation experiments. In my corpora, there are three main types of source texts: 1) scientific papers, 2) technical reports, and 3) newspapers. Also, Minel et al. (1997) used book extracts and memos, and Farzindar and Lapalme (2005) used judgments of the Canadian federal court. All evaluations used only one type of source texts, except for Kupiec et al. (1995) and for Minel et al. (1997). Finally, the majority of the evaluations used English texts. Some authors used French texts (Minel et al., 1997; Châar et al., 2004), Korean texts (Myaeng and Jang, 1999) or Japanese texts (Nanba and Okumura, 2000). 2.2 Variables about automatic summaries being evaluated In this section, I describe variables about automatic summaries being evaluated. The variables have been classified into six categories: 1) the total number of automatic summaries evaluated, 2) the number of automatic summaries produced per source text, 3) if they are multiple document summaries, 4) the length, 5) if they are extracts or abstracts, and 6) their purpose. First, concerning the total number of automatic summaries, Brandow et al. (1995), Mani </context>
<context position="5769" citStr="Châar et al. (2004)" startWordPosition="910" endWordPosition="913">123 automatic summaries. All the other studies for which this information is given evaluated less than 100 automatic summaries. It may appear redundant to give the number of source texts and the number of automatic summaries in an evaluation, but sometimes more than one automatic summary per source text may have been produced. This is the case in Brandow et al. (1995) and Barzilay and Elhadad (1999) where automatic summaries of different lengths have been evaluated. Automatic summaries can either be produced from one text or more than one text. In my corpora, only Mani and Bloedorn (1999) and Châar et al. (2004) evaluated multiple document summaries. As for source texts, automatic summary length is expressed in different ways from one evaluation to another. Moreover, it is not always expressed in the same way than source text length, which is inconsistent. On a different note, most experiments evaluated extracts, except for Maybury (1999) and Saggion and Lapalme (2002) who evaluated abstracts, reflecting the predominance of systems producing extracts in the domain of summarization. Extracts are summaries produced by extracting the most important segments from texts while abstracts are the result of a</context>
</contexts>
<marker>Châar, Ferret, Fluhr, 2004</marker>
<rawString>S. L. Châar, O. Ferret, and C. Fluhr. 2004. Filtrage pour la construction de résumés multidocuments guidée par un profil. Traitement automatique des langues, 45(1):65–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic abstracting.</title>
<date>1969</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="3471" citStr="Edmundson (1969)" startWordPosition="524" endWordPosition="525">an indicator of the significance of the evaluation. In my study, 1http://www.elda.org/rubrique25.html Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 17–20, Rochester, April 2007. c�2007 Association for Computational Linguistics all the evaluations used less than 100 source texts, except for Mani and Bloedorn (1999) (300 source texts), Brandow et al. (1995) (250 source texts), Kupiec et al. (1995) (188 source texts) and Teufel and Moens (1999) (123 source texts). Secondly, regarding source text length, it is expressed in different ways from one evaluation to another. For example, Edmundson (1969) gives the number of words, Klavans et al. (1998) give the number of sentences and Minel et al. (1997) give the number of pages. In some papers, the length of the shortest and of the longest text is provided (Marcu, 1999) while in others it is the average number of words, sentences or pages that is given (Teufel and Moens, 1999). Obviously, it would be wise to standardize the way source texts length is given in evaluation experiments. In my corpora, there are three main types of source texts: 1) scientific papers, 2) technical reports, and 3) newspapers. Also, Minel et al. (1997) used book ext</context>
<context position="10441" citStr="Edmundson (1969)" startWordPosition="1654" endWordPosition="1655">ries. A few of them specify the total number of persons involved, but not the number for each source text. This is an important variable because summarizing, either by extracting or abstracting, is a subjective task. The more people involved in the summarization of one text, the more we can consider the final summary to be reliable. From the pieces of information I was able to gather, the number of summarizers per source text ranges from 1 to 13 in my corpora. In analyzing the evaluations of my corpora, I realized that some authors gave clear instructions to the human summarizers, for example Edmundson (1969). In other cases, authors asked the summarizers to extract the most “important” sentences. The term “important” includes other terms like representative, informative, relevant, and eligible. It is rarely mentioned however if those words were explained to the summarizers. I also noticed that some evaluations used people coming from different backgrounds, for example in Salton et al. (1997), while others used more homogeneous groups, for example in Barzilay and Elhadad (1999) and Kupiec et al. (1995). 3 Future Directions In the next couple of months, I plan to analyze evaluation methods identifi</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>H. P. Edmundson. 1969. New methods in automatic abstracting. Journal of the Association for Computing Machinery, 16(2):264–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Farzindar</author>
<author>G Lapalme</author>
</authors>
<title>Production automatique de résumé de textes juridiques : évaluation de qualité et d’acceptabilité.</title>
<date>2005</date>
<booktitle>In TALN,</booktitle>
<pages>183--192</pages>
<location>Dourdan.</location>
<contexts>
<context position="4120" citStr="Farzindar and Lapalme (2005)" startWordPosition="637" endWordPosition="640">words, Klavans et al. (1998) give the number of sentences and Minel et al. (1997) give the number of pages. In some papers, the length of the shortest and of the longest text is provided (Marcu, 1999) while in others it is the average number of words, sentences or pages that is given (Teufel and Moens, 1999). Obviously, it would be wise to standardize the way source texts length is given in evaluation experiments. In my corpora, there are three main types of source texts: 1) scientific papers, 2) technical reports, and 3) newspapers. Also, Minel et al. (1997) used book extracts and memos, and Farzindar and Lapalme (2005) used judgments of the Canadian federal court. All evaluations used only one type of source texts, except for Kupiec et al. (1995) and for Minel et al. (1997). Finally, the majority of the evaluations used English texts. Some authors used French texts (Minel et al., 1997; Châar et al., 2004), Korean texts (Myaeng and Jang, 1999) or Japanese texts (Nanba and Okumura, 2000). 2.2 Variables about automatic summaries being evaluated In this section, I describe variables about automatic summaries being evaluated. The variables have been classified into six categories: 1) the total number of automati</context>
</contexts>
<marker>Farzindar, Lapalme, 2005</marker>
<rawString>A. Farzindar and G. Lapalme. 2005. Production automatique de résumé de textes juridiques : évaluation de qualité et d’acceptabilité. In TALN, pages 183–192, Dourdan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C-Y Lin</author>
</authors>
<title>Automated text summarization</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization,</booktitle>
<pages>81--94</pages>
<editor>in SUMMARIST. In I. Mani and M. T. Maybury, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="8930" citStr="Hovy and Lin (1999)" startWordPosition="1406" endWordPosition="1409">specifically for the evaluation. Second, baseline summaries are generally produced by extracting random sentences from source texts or produced by another system. In my corpora, gold standard summaries are often produced specifically for the evaluation. In most cases, they are produced by manually extracting the most important passages, sentences or paragraphs, allowing automatic comparison between automatic summaries and gold standard summaries. On the other hand, many evaluations used baseline summaries. For example, Barzilay and Elhadad (1999) used summaries produced by Word AutoSummarize, Hovy and Lin (1999) used summaries produced by automatically extracting random sentences from source texts. In Brandow et al. (1995), Kupiec et al. (1995) and Teufel and Moens (1999), baseline summaries were produced by automatically extracting sentences at the beginning of the texts, and in Myaeng and Jang (1999) by extracting the first five sentences of the conclusion. Logically, the length of the summaries used for the comparison should be equivalent to the length of the automatic summaries being evaluated. If automatic summaries of different lengths are evaluated, there should be corresponding baselines and/</context>
</contexts>
<marker>Hovy, Lin, 1999</marker>
<rawString>E. Hovy and C.-Y. Lin. 1999. Automated text summarization in SUMMARIST. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization, pages 81–94, Cambridge, Massachusetts. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Klavans</author>
<author>K R McKeown</author>
<author>M-Y Kan</author>
<author>S Lee</author>
</authors>
<title>Resources for the evaluation of summarization techniques.</title>
<date>1998</date>
<pages>899--902</pages>
<editor>In Antonio Zampolli, editor, LREC,</editor>
<location>Granada, Spain.</location>
<contexts>
<context position="3520" citStr="Klavans et al. (1998)" startWordPosition="531" endWordPosition="534">uation. In my study, 1http://www.elda.org/rubrique25.html Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 17–20, Rochester, April 2007. c�2007 Association for Computational Linguistics all the evaluations used less than 100 source texts, except for Mani and Bloedorn (1999) (300 source texts), Brandow et al. (1995) (250 source texts), Kupiec et al. (1995) (188 source texts) and Teufel and Moens (1999) (123 source texts). Secondly, regarding source text length, it is expressed in different ways from one evaluation to another. For example, Edmundson (1969) gives the number of words, Klavans et al. (1998) give the number of sentences and Minel et al. (1997) give the number of pages. In some papers, the length of the shortest and of the longest text is provided (Marcu, 1999) while in others it is the average number of words, sentences or pages that is given (Teufel and Moens, 1999). Obviously, it would be wise to standardize the way source texts length is given in evaluation experiments. In my corpora, there are three main types of source texts: 1) scientific papers, 2) technical reports, and 3) newspapers. Also, Minel et al. (1997) used book extracts and memos, and Farzindar and Lapalme (2005)</context>
</contexts>
<marker>Klavans, McKeown, Kan, Lee, 1998</marker>
<rawString>J. L. Klavans, K. R. McKeown, M.-Y. Kan, and S. Lee. 1998. Resources for the evaluation of summarization techniques. In Antonio Zampolli, editor, LREC, pages 899–902, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>F Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In SIGIR,</booktitle>
<pages>68--73</pages>
<location>Seattle.</location>
<contexts>
<context position="3268" citStr="Kupiec et al. (1995)" startWordPosition="488" endWordPosition="492">rce texts Four types of information about source texts emerged from the analysis: 1) the number of source texts, 2) the length, 3) the type of text, and 4) the language. First, the number of source texts is an indicator of the significance of the evaluation. In my study, 1http://www.elda.org/rubrique25.html Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 17–20, Rochester, April 2007. c�2007 Association for Computational Linguistics all the evaluations used less than 100 source texts, except for Mani and Bloedorn (1999) (300 source texts), Brandow et al. (1995) (250 source texts), Kupiec et al. (1995) (188 source texts) and Teufel and Moens (1999) (123 source texts). Secondly, regarding source text length, it is expressed in different ways from one evaluation to another. For example, Edmundson (1969) gives the number of words, Klavans et al. (1998) give the number of sentences and Minel et al. (1997) give the number of pages. In some papers, the length of the shortest and of the longest text is provided (Marcu, 1999) while in others it is the average number of words, sentences or pages that is given (Teufel and Moens, 1999). Obviously, it would be wise to standardize the way source texts l</context>
<context position="5053" citStr="Kupiec et al. (1995)" startWordPosition="788" endWordPosition="791">g and Jang, 1999) or Japanese texts (Nanba and Okumura, 2000). 2.2 Variables about automatic summaries being evaluated In this section, I describe variables about automatic summaries being evaluated. The variables have been classified into six categories: 1) the total number of automatic summaries evaluated, 2) the number of automatic summaries produced per source text, 3) if they are multiple document summaries, 4) the length, 5) if they are extracts or abstracts, and 6) their purpose. First, concerning the total number of automatic summaries, Brandow et al. (1995), Mani and Bloedorn (1999), Kupiec et al. (1995), Salton et al. (1997) and Teufel and Moens (1999) evaluated respectively 750, 300, 188, 150 and 123 automatic summaries. All the other studies for which this information is given evaluated less than 100 automatic summaries. It may appear redundant to give the number of source texts and the number of automatic summaries in an evaluation, but sometimes more than one automatic summary per source text may have been produced. This is the case in Brandow et al. (1995) and Barzilay and Elhadad (1999) where automatic summaries of different lengths have been evaluated. Automatic summaries can either b</context>
<context position="9065" citStr="Kupiec et al. (1995)" startWordPosition="1427" endWordPosition="1430"> produced by another system. In my corpora, gold standard summaries are often produced specifically for the evaluation. In most cases, they are produced by manually extracting the most important passages, sentences or paragraphs, allowing automatic comparison between automatic summaries and gold standard summaries. On the other hand, many evaluations used baseline summaries. For example, Barzilay and Elhadad (1999) used summaries produced by Word AutoSummarize, Hovy and Lin (1999) used summaries produced by automatically extracting random sentences from source texts. In Brandow et al. (1995), Kupiec et al. (1995) and Teufel and Moens (1999), baseline summaries were produced by automatically extracting sentences at the beginning of the texts, and in Myaeng and Jang (1999) by extracting the first five sentences of the conclusion. Logically, the length of the summaries used for the comparison should be equivalent to the length of the automatic summaries being evaluated. If automatic summaries of different lengths are evaluated, there should be corresponding baselines and/or gold standard summaries for each length, unless the goal of the evaluation is to determine if the length plays a role in the quality</context>
<context position="10944" citStr="Kupiec et al. (1995)" startWordPosition="1731" endWordPosition="1734"> corpora, I realized that some authors gave clear instructions to the human summarizers, for example Edmundson (1969). In other cases, authors asked the summarizers to extract the most “important” sentences. The term “important” includes other terms like representative, informative, relevant, and eligible. It is rarely mentioned however if those words were explained to the summarizers. I also noticed that some evaluations used people coming from different backgrounds, for example in Salton et al. (1997), while others used more homogeneous groups, for example in Barzilay and Elhadad (1999) and Kupiec et al. (1995). 3 Future Directions In the next couple of months, I plan to analyze evaluation methods identified in my corpora, for example comparing automatic summaries with gold standard or baseline summaries, and asking judges to give their opinion on the quality of automatic summaries. I will also describe evaluation criteria used to assess the quality of the automatic summaries, for example informativeness and readability. Next, I will make recommendations for the presentation of summarization evaluation results, based on the knowledge acquired from my analysis of 22 scientific papers, and from previo</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable document summarizer. In SIGIR, pages 68–73, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>E Bloedorn</author>
</authors>
<title>Summarizing similarities and differences among related documents.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization,</booktitle>
<pages>357--379</pages>
<editor>In I. Mani and M. T. Maybury, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="3185" citStr="Mani and Bloedorn (1999)" startWordPosition="474" endWordPosition="477">pes of variables have been analyzed and will be presented here. 2.1 Variables about source texts Four types of information about source texts emerged from the analysis: 1) the number of source texts, 2) the length, 3) the type of text, and 4) the language. First, the number of source texts is an indicator of the significance of the evaluation. In my study, 1http://www.elda.org/rubrique25.html Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 17–20, Rochester, April 2007. c�2007 Association for Computational Linguistics all the evaluations used less than 100 source texts, except for Mani and Bloedorn (1999) (300 source texts), Brandow et al. (1995) (250 source texts), Kupiec et al. (1995) (188 source texts) and Teufel and Moens (1999) (123 source texts). Secondly, regarding source text length, it is expressed in different ways from one evaluation to another. For example, Edmundson (1969) gives the number of words, Klavans et al. (1998) give the number of sentences and Minel et al. (1997) give the number of pages. In some papers, the length of the shortest and of the longest text is provided (Marcu, 1999) while in others it is the average number of words, sentences or pages that is given (Teufel </context>
<context position="5031" citStr="Mani and Bloedorn (1999)" startWordPosition="783" endWordPosition="787">2004), Korean texts (Myaeng and Jang, 1999) or Japanese texts (Nanba and Okumura, 2000). 2.2 Variables about automatic summaries being evaluated In this section, I describe variables about automatic summaries being evaluated. The variables have been classified into six categories: 1) the total number of automatic summaries evaluated, 2) the number of automatic summaries produced per source text, 3) if they are multiple document summaries, 4) the length, 5) if they are extracts or abstracts, and 6) their purpose. First, concerning the total number of automatic summaries, Brandow et al. (1995), Mani and Bloedorn (1999), Kupiec et al. (1995), Salton et al. (1997) and Teufel and Moens (1999) evaluated respectively 750, 300, 188, 150 and 123 automatic summaries. All the other studies for which this information is given evaluated less than 100 automatic summaries. It may appear redundant to give the number of source texts and the number of automatic summaries in an evaluation, but sometimes more than one automatic summary per source text may have been produced. This is the case in Brandow et al. (1995) and Barzilay and Elhadad (1999) where automatic summaries of different lengths have been evaluated. Automatic </context>
</contexts>
<marker>Mani, Bloedorn, 1999</marker>
<rawString>I. Mani and E. Bloedorn. 1999. Summarizing similarities and differences among related documents. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization, pages 357–379, Cambridge, Massachusetts. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>Discourse trees are good indicators of importance in text.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization,</booktitle>
<pages>123--136</pages>
<editor>In I. Mani and M. T. Maybury, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="3692" citStr="Marcu, 1999" startWordPosition="565" endWordPosition="566">onal Linguistics all the evaluations used less than 100 source texts, except for Mani and Bloedorn (1999) (300 source texts), Brandow et al. (1995) (250 source texts), Kupiec et al. (1995) (188 source texts) and Teufel and Moens (1999) (123 source texts). Secondly, regarding source text length, it is expressed in different ways from one evaluation to another. For example, Edmundson (1969) gives the number of words, Klavans et al. (1998) give the number of sentences and Minel et al. (1997) give the number of pages. In some papers, the length of the shortest and of the longest text is provided (Marcu, 1999) while in others it is the average number of words, sentences or pages that is given (Teufel and Moens, 1999). Obviously, it would be wise to standardize the way source texts length is given in evaluation experiments. In my corpora, there are three main types of source texts: 1) scientific papers, 2) technical reports, and 3) newspapers. Also, Minel et al. (1997) used book extracts and memos, and Farzindar and Lapalme (2005) used judgments of the Canadian federal court. All evaluations used only one type of source texts, except for Kupiec et al. (1995) and for Minel et al. (1997). Finally, the</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>D. Marcu. 1999. Discourse trees are good indicators of importance in text. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization, pages 123–136, Cambridge, Massachusetts. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maybury</author>
</authors>
<title>Generating summaries from event data. In</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization,</booktitle>
<pages>265--281</pages>
<editor>I. Mani and M. T. Maybury, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="6102" citStr="Maybury (1999)" startWordPosition="963" endWordPosition="964">he case in Brandow et al. (1995) and Barzilay and Elhadad (1999) where automatic summaries of different lengths have been evaluated. Automatic summaries can either be produced from one text or more than one text. In my corpora, only Mani and Bloedorn (1999) and Châar et al. (2004) evaluated multiple document summaries. As for source texts, automatic summary length is expressed in different ways from one evaluation to another. Moreover, it is not always expressed in the same way than source text length, which is inconsistent. On a different note, most experiments evaluated extracts, except for Maybury (1999) and Saggion and Lapalme (2002) who evaluated abstracts, reflecting the predominance of systems producing extracts in the domain of summarization. Extracts are summaries produced by extracting the most important segments from texts while abstracts are the result of a comprehension process and text generation. Most extracts evaluated are composed of sentences, except for Salton et al. (1997) and Châar et al. (2004) where they are respectively composed of paragraphs and passages. The type of automatic summaries is crucial information because it normally influences the choice of the evaluation me</context>
</contexts>
<marker>Maybury, 1999</marker>
<rawString>M. Maybury. 1999. Generating summaries from event data. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization, pages 265– 281, Cambridge, Massachusetts. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-L Minel</author>
<author>S Nugier</author>
<author>G Piat</author>
</authors>
<title>How to appreciate the quality of automatic text summarization? Examples of FAN and MLUCE protocols and their results on SERAPHIN.</title>
<date>1997</date>
<booktitle>In EACL,</booktitle>
<pages>25--31</pages>
<location>Madrid.</location>
<contexts>
<context position="3573" citStr="Minel et al. (1997)" startWordPosition="541" endWordPosition="544">ml Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 17–20, Rochester, April 2007. c�2007 Association for Computational Linguistics all the evaluations used less than 100 source texts, except for Mani and Bloedorn (1999) (300 source texts), Brandow et al. (1995) (250 source texts), Kupiec et al. (1995) (188 source texts) and Teufel and Moens (1999) (123 source texts). Secondly, regarding source text length, it is expressed in different ways from one evaluation to another. For example, Edmundson (1969) gives the number of words, Klavans et al. (1998) give the number of sentences and Minel et al. (1997) give the number of pages. In some papers, the length of the shortest and of the longest text is provided (Marcu, 1999) while in others it is the average number of words, sentences or pages that is given (Teufel and Moens, 1999). Obviously, it would be wise to standardize the way source texts length is given in evaluation experiments. In my corpora, there are three main types of source texts: 1) scientific papers, 2) technical reports, and 3) newspapers. Also, Minel et al. (1997) used book extracts and memos, and Farzindar and Lapalme (2005) used judgments of the Canadian federal court. All ev</context>
</contexts>
<marker>Minel, Nugier, Piat, 1997</marker>
<rawString>J.-L. Minel, S. Nugier, and G. Piat. 1997. How to appreciate the quality of automatic text summarization? Examples of FAN and MLUCE protocols and their results on SERAPHIN. In EACL, pages 25–31, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S H Myaeng</author>
<author>D-H Jang</author>
</authors>
<title>Development and evaluation of a statistically-based document summarization system.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization,</booktitle>
<pages>61--70</pages>
<editor>In I. Mani and M. T. Maybury, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="4450" citStr="Myaeng and Jang, 1999" startWordPosition="694" endWordPosition="697">se to standardize the way source texts length is given in evaluation experiments. In my corpora, there are three main types of source texts: 1) scientific papers, 2) technical reports, and 3) newspapers. Also, Minel et al. (1997) used book extracts and memos, and Farzindar and Lapalme (2005) used judgments of the Canadian federal court. All evaluations used only one type of source texts, except for Kupiec et al. (1995) and for Minel et al. (1997). Finally, the majority of the evaluations used English texts. Some authors used French texts (Minel et al., 1997; Châar et al., 2004), Korean texts (Myaeng and Jang, 1999) or Japanese texts (Nanba and Okumura, 2000). 2.2 Variables about automatic summaries being evaluated In this section, I describe variables about automatic summaries being evaluated. The variables have been classified into six categories: 1) the total number of automatic summaries evaluated, 2) the number of automatic summaries produced per source text, 3) if they are multiple document summaries, 4) the length, 5) if they are extracts or abstracts, and 6) their purpose. First, concerning the total number of automatic summaries, Brandow et al. (1995), Mani and Bloedorn (1999), Kupiec et al. (19</context>
<context position="9226" citStr="Myaeng and Jang (1999)" startWordPosition="1453" endWordPosition="1456">ually extracting the most important passages, sentences or paragraphs, allowing automatic comparison between automatic summaries and gold standard summaries. On the other hand, many evaluations used baseline summaries. For example, Barzilay and Elhadad (1999) used summaries produced by Word AutoSummarize, Hovy and Lin (1999) used summaries produced by automatically extracting random sentences from source texts. In Brandow et al. (1995), Kupiec et al. (1995) and Teufel and Moens (1999), baseline summaries were produced by automatically extracting sentences at the beginning of the texts, and in Myaeng and Jang (1999) by extracting the first five sentences of the conclusion. Logically, the length of the summaries used for the comparison should be equivalent to the length of the automatic summaries being evaluated. If automatic summaries of different lengths are evaluated, there should be corresponding baselines and/or gold standard summaries for each length, unless the goal of the evaluation is to determine if the length plays a role in the quality of automatic summaries. Many of the evaluations analyzed do not indicate the number of human summarizers participating in the production of gold standard summar</context>
</contexts>
<marker>Myaeng, Jang, 1999</marker>
<rawString>S. H. Myaeng and D.-H. Jang. 1999. Development and evaluation of a statistically-based document summarization system. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization, pages 61–70, Cambridge, Massachusetts. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nanba</author>
<author>M Okumura</author>
</authors>
<title>Producing more readable extracts by revising them.</title>
<date>2000</date>
<booktitle>In 18th International Conference on Computational Linguistics,</booktitle>
<pages>1071--1075</pages>
<contexts>
<context position="4494" citStr="Nanba and Okumura, 2000" startWordPosition="701" endWordPosition="705">gth is given in evaluation experiments. In my corpora, there are three main types of source texts: 1) scientific papers, 2) technical reports, and 3) newspapers. Also, Minel et al. (1997) used book extracts and memos, and Farzindar and Lapalme (2005) used judgments of the Canadian federal court. All evaluations used only one type of source texts, except for Kupiec et al. (1995) and for Minel et al. (1997). Finally, the majority of the evaluations used English texts. Some authors used French texts (Minel et al., 1997; Châar et al., 2004), Korean texts (Myaeng and Jang, 1999) or Japanese texts (Nanba and Okumura, 2000). 2.2 Variables about automatic summaries being evaluated In this section, I describe variables about automatic summaries being evaluated. The variables have been classified into six categories: 1) the total number of automatic summaries evaluated, 2) the number of automatic summaries produced per source text, 3) if they are multiple document summaries, 4) the length, 5) if they are extracts or abstracts, and 6) their purpose. First, concerning the total number of automatic summaries, Brandow et al. (1995), Mani and Bloedorn (1999), Kupiec et al. (1995), Salton et al. (1997) and Teufel and Moe</context>
</contexts>
<marker>Nanba, Okumura, 2000</marker>
<rawString>H. Nanba and M. Okumura. 2000. Producing more readable extracts by revising them. In 18th International Conference on Computational Linguistics, pages 1071–1075, Saarbrucker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>G Lapalme</author>
</authors>
<title>Generating indicative-informative summaries with SumUM.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="6133" citStr="Saggion and Lapalme (2002)" startWordPosition="966" endWordPosition="969">et al. (1995) and Barzilay and Elhadad (1999) where automatic summaries of different lengths have been evaluated. Automatic summaries can either be produced from one text or more than one text. In my corpora, only Mani and Bloedorn (1999) and Châar et al. (2004) evaluated multiple document summaries. As for source texts, automatic summary length is expressed in different ways from one evaluation to another. Moreover, it is not always expressed in the same way than source text length, which is inconsistent. On a different note, most experiments evaluated extracts, except for Maybury (1999) and Saggion and Lapalme (2002) who evaluated abstracts, reflecting the predominance of systems producing extracts in the domain of summarization. Extracts are summaries produced by extracting the most important segments from texts while abstracts are the result of a comprehension process and text generation. Most extracts evaluated are composed of sentences, except for Salton et al. (1997) and Châar et al. (2004) where they are respectively composed of paragraphs and passages. The type of automatic summaries is crucial information because it normally influences the choice of the evaluation method and criteria. Indeed, we d</context>
</contexts>
<marker>Saggion, Lapalme, 2002</marker>
<rawString>H. Saggion and G. Lapalme. 2002. Generating indicative-informative summaries with SumUM. Computational Linguistics, 28(4):497–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Singhal</author>
<author>M Mitra</author>
<author>C Buckley</author>
</authors>
<title>Automatic text structuring and summarization.</title>
<date>1997</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="5075" citStr="Salton et al. (1997)" startWordPosition="792" endWordPosition="795">apanese texts (Nanba and Okumura, 2000). 2.2 Variables about automatic summaries being evaluated In this section, I describe variables about automatic summaries being evaluated. The variables have been classified into six categories: 1) the total number of automatic summaries evaluated, 2) the number of automatic summaries produced per source text, 3) if they are multiple document summaries, 4) the length, 5) if they are extracts or abstracts, and 6) their purpose. First, concerning the total number of automatic summaries, Brandow et al. (1995), Mani and Bloedorn (1999), Kupiec et al. (1995), Salton et al. (1997) and Teufel and Moens (1999) evaluated respectively 750, 300, 188, 150 and 123 automatic summaries. All the other studies for which this information is given evaluated less than 100 automatic summaries. It may appear redundant to give the number of source texts and the number of automatic summaries in an evaluation, but sometimes more than one automatic summary per source text may have been produced. This is the case in Brandow et al. (1995) and Barzilay and Elhadad (1999) where automatic summaries of different lengths have been evaluated. Automatic summaries can either be produced from one te</context>
<context position="6495" citStr="Salton et al. (1997)" startWordPosition="1022" endWordPosition="1025">erent ways from one evaluation to another. Moreover, it is not always expressed in the same way than source text length, which is inconsistent. On a different note, most experiments evaluated extracts, except for Maybury (1999) and Saggion and Lapalme (2002) who evaluated abstracts, reflecting the predominance of systems producing extracts in the domain of summarization. Extracts are summaries produced by extracting the most important segments from texts while abstracts are the result of a comprehension process and text generation. Most extracts evaluated are composed of sentences, except for Salton et al. (1997) and Châar et al. (2004) where they are respectively composed of paragraphs and passages. The type of automatic summaries is crucial information because it normally influences the choice of the evaluation method and criteria. Indeed, we do not evaluate extracts and abstracts in the same way since they are not produced in the same way. Also, their purposes generally differ, which can also influence the choice of the evaluation method and criteria. Last, some papers contain the specific purpose of automatic summaries, not only if they are indicative or informative, which is interesting because i</context>
<context position="10832" citStr="Salton et al. (1997)" startWordPosition="1711" endWordPosition="1714"> the number of summarizers per source text ranges from 1 to 13 in my corpora. In analyzing the evaluations of my corpora, I realized that some authors gave clear instructions to the human summarizers, for example Edmundson (1969). In other cases, authors asked the summarizers to extract the most “important” sentences. The term “important” includes other terms like representative, informative, relevant, and eligible. It is rarely mentioned however if those words were explained to the summarizers. I also noticed that some evaluations used people coming from different backgrounds, for example in Salton et al. (1997), while others used more homogeneous groups, for example in Barzilay and Elhadad (1999) and Kupiec et al. (1995). 3 Future Directions In the next couple of months, I plan to analyze evaluation methods identified in my corpora, for example comparing automatic summaries with gold standard or baseline summaries, and asking judges to give their opinion on the quality of automatic summaries. I will also describe evaluation criteria used to assess the quality of the automatic summaries, for example informativeness and readability. Next, I will make recommendations for the presentation of summarizati</context>
</contexts>
<marker>Salton, Singhal, Mitra, Buckley, 1997</marker>
<rawString>G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997. Automatic text structuring and summarization. Information Processing and Management, 33(2):193–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Argumentative classification of extracted sentences as a first step towards flexible abstracting.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization,</booktitle>
<pages>155--171</pages>
<editor>In I. Mani and M. T. Maybury, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="3315" citStr="Teufel and Moens (1999)" startWordPosition="497" endWordPosition="500">urce texts emerged from the analysis: 1) the number of source texts, 2) the length, 3) the type of text, and 4) the language. First, the number of source texts is an indicator of the significance of the evaluation. In my study, 1http://www.elda.org/rubrique25.html Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 17–20, Rochester, April 2007. c�2007 Association for Computational Linguistics all the evaluations used less than 100 source texts, except for Mani and Bloedorn (1999) (300 source texts), Brandow et al. (1995) (250 source texts), Kupiec et al. (1995) (188 source texts) and Teufel and Moens (1999) (123 source texts). Secondly, regarding source text length, it is expressed in different ways from one evaluation to another. For example, Edmundson (1969) gives the number of words, Klavans et al. (1998) give the number of sentences and Minel et al. (1997) give the number of pages. In some papers, the length of the shortest and of the longest text is provided (Marcu, 1999) while in others it is the average number of words, sentences or pages that is given (Teufel and Moens, 1999). Obviously, it would be wise to standardize the way source texts length is given in evaluation experiments. In my</context>
<context position="5103" citStr="Teufel and Moens (1999)" startWordPosition="797" endWordPosition="800">Okumura, 2000). 2.2 Variables about automatic summaries being evaluated In this section, I describe variables about automatic summaries being evaluated. The variables have been classified into six categories: 1) the total number of automatic summaries evaluated, 2) the number of automatic summaries produced per source text, 3) if they are multiple document summaries, 4) the length, 5) if they are extracts or abstracts, and 6) their purpose. First, concerning the total number of automatic summaries, Brandow et al. (1995), Mani and Bloedorn (1999), Kupiec et al. (1995), Salton et al. (1997) and Teufel and Moens (1999) evaluated respectively 750, 300, 188, 150 and 123 automatic summaries. All the other studies for which this information is given evaluated less than 100 automatic summaries. It may appear redundant to give the number of source texts and the number of automatic summaries in an evaluation, but sometimes more than one automatic summary per source text may have been produced. This is the case in Brandow et al. (1995) and Barzilay and Elhadad (1999) where automatic summaries of different lengths have been evaluated. Automatic summaries can either be produced from one text or more than one text. In</context>
<context position="9093" citStr="Teufel and Moens (1999)" startWordPosition="1432" endWordPosition="1435">em. In my corpora, gold standard summaries are often produced specifically for the evaluation. In most cases, they are produced by manually extracting the most important passages, sentences or paragraphs, allowing automatic comparison between automatic summaries and gold standard summaries. On the other hand, many evaluations used baseline summaries. For example, Barzilay and Elhadad (1999) used summaries produced by Word AutoSummarize, Hovy and Lin (1999) used summaries produced by automatically extracting random sentences from source texts. In Brandow et al. (1995), Kupiec et al. (1995) and Teufel and Moens (1999), baseline summaries were produced by automatically extracting sentences at the beginning of the texts, and in Myaeng and Jang (1999) by extracting the first five sentences of the conclusion. Logically, the length of the summaries used for the comparison should be equivalent to the length of the automatic summaries being evaluated. If automatic summaries of different lengths are evaluated, there should be corresponding baselines and/or gold standard summaries for each length, unless the goal of the evaluation is to determine if the length plays a role in the quality of automatic summaries. Man</context>
</contexts>
<marker>Teufel, Moens, 1999</marker>
<rawString>S. Teufel and M. Moens. 1999. Argumentative classification of extracted sentences as a first step towards flexible abstracting. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization, pages 155–171, Cambridge, Massachusetts. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>