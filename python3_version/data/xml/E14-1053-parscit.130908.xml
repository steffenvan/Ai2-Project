<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.100539">
<title confidence="0.716991">
Topical PageRank: A Model of Scientific Expertise for Bibliographic Search
</title>
<author confidence="0.909256">
James Jardine Simone Teufel
</author>
<affiliation confidence="0.8826595">
Natural Language and Information Processing Group
Computer Laboratory
</affiliation>
<address confidence="0.768969">
Cambridge University, CB3 0FD, UK
</address>
<email confidence="0.997924">
{jgj29,sht25}@cam.ac.uk
</email>
<sectionHeader confidence="0.99477" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980727272727">
We model scientific expertise as a mixture
of topics and authority. Authority is calcu-
lated based on the network properties of each
topic network. ThemedPageRank, our combi-
nation of LDA-derived topics with PageRank
differs from previous models in that topics in-
fluence both the bias and transition probabili-
ties of PageRank. It also incorporates the age
of documents. Our model is general in that
it can be applied to all tasks which require an
estimate of document–document, document–
query, document–topic and topic–query sim-
ilarities. We present two evaluations, one
on the task of restoring the reference lists of
10,000 articles, the other on the task of au-
tomatically creating reading lists that mimic
reading lists created by experts. In both eval-
uations, our system beats state-of-the-art, as
well as Google Scholar and Google Search in-
dexed againt the corpus. Our experiments also
allow us to quantify the beneficial effect of our
two proposed modifications to PageRank.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999723125">
For search, the presence of links in a document
collection adds valuable information over that con-
tained in the text of the documents alone. Each act
of linking can be interpreted as a latent judgement of
authority or trust which is bestowed onto the linked
documents (Kleinberg, 1998). This makes author-
ity an objective measure of how important that pa-
per is to a community who confer that authority.
The citation count is the simplest of these, which
has been used successfully for decades for biblio-
metrics (Garfield, 1972) and for mapping out scien-
tific fields via bibliometric coupling (Kessler, 1963)
and co-citations (Small, 1978). More recently, cita-
tion counts have been shown to improve effective-
ness of ad-hoc retrieval (Meij and De Rijke, 2007;
Fujii, 2007).
In science, the peer review process ensures that
the right to cite is hard-earned, but on the web, hy-
perlinking is infinitely cheap. This means that that
the authority of webpages cannot simply be approx-
imated as the number of incoming links. Algorith-
mically more complex authority such as the random-
surfer model PageRank (Brin and Page, 1998) or the
authorities/hub based algorithm HITS (Kleinberg,
1998)) have spectacularly improved search results in
comparison to standard IR models relying on simi-
larity calculations based on the words in the text and
other text-internal informatioh.
Much recent work in bibliographic search has
been driven by the intuition that what works for the
web should also work for science, even though ci-
tations are more comparable to each other in weight
than hyperlinks. Case studies comparing PageRank-
based authority measures against citation counts
alone report some cases where PageRank is supe-
rior (Chen et al., 2007; Ma et al., 2008), but exper-
imental proof of standard PageRank outperforming
citation counts in a large-scale bibliographic search
experiment is still outstanding. In at least one such
experiment, PageRank performed worse than cita-
tion count (Bethard and Jurafsky, 2010).
Straightforward PageRank calculations, when ap-
plied to the scientific literature, are hampered by two
factors: on the one hand, the progression of time im-
poses a directional structure on the citation network.
Therefore, PageRank values of older papers are sys-
tematically inflated as PageRank can only ever flow
from newer to older papers (Walker et al., 2007).
</bodyText>
<page confidence="0.955219">
501
</page>
<note confidence="0.9930885">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 501–510,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999981928571429">
Secondly, and more interestingly, researchers earn
their expertise in particular, well-defined scientific
fields. We propose that this requires a more fine-
grained notion of specific – not global – expertise.
Our solution is to use LDA-derived topics (Blei
et al., 2003) as approximations for scientific fields,
and to model the importance of a paper as a mixture
of its relative expertise in each of the topics it cov-
ers. The second aspect of our solution, somewhat
more mundane but still necessary to adapt PageR-
ank successfully to model scientific expertise, is to
age-taper the resultant estimation.
In this paper, we present ThemedPageRank
(TPR), our model of topic-specific scientific exper-
tise, which incorporates the two modifications, and
provide evidence that both are necessary for the ad-
equate application of PageRank-style authority cal-
culations to the scientific literature. In two evalua-
tions, our model beats standard PageRank and cita-
tion counts by a large margin. Previous models exist
which combine the idea of personalising PageRank
by topics, but our manipulation of both PageRank’s
bias and transition probabilities differs from these.
Our experiments also support the claim of our sys-
tem’s superiority over these models.
We use two tasks to evaluate the system’s per-
formance. The first is the reintroduction of an ar-
ticle’s reference items that have been artificially re-
moved. The assumption here is that a good model
of document–document similarity should be able to
guess which articles any given paper would have
cited. The second task is the automatic creation of
reading lists, of the kind that an expert might pre-
pare for their students. We asked experts to create a
gold standard of such reading lists, and compare our
system against the current de facto state-of-the-art in
such tasks, Google Scholar, and again find that our
system beats it comfortably.
This article is structured as follows: the next sec-
tion describes our model, which section 3 contrasts
to related work. The evaluations are described in
sections 4 and 5. Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.995822" genericHeader="introduction">
2 Authority Model
</sectionHeader>
<bodyText confidence="0.99980525">
Our model first determines an LDA space (Blei et
al., 2003) representing the entire document collec-
tion, which results in a set of topics describing the
entirety of the field. It then calculates an author-
</bodyText>
<figureCaption confidence="0.999495">
Figure 1: A High-level view of LDA.
</figureCaption>
<bodyText confidence="0.999785162162162">
ity model for each topic based on a modification
of Personalised PageRank (Page et al., 1998). De-
pending on the search need, the input (one or more
keyword(s) or paper(s)) is converted into a topic dis-
tribution, which we then use to linearly combine the
multiple topic-specific expertise scores of our model
into a unique authority score representing the fit be-
tween search need and document.
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) is a Bayesian generative probabilistic model
for collections of discrete data, which has become
popular for the modelling of scientific text corpora
(Wei and Croft, 2006; He et al., 2009; Blei and
Lafferty, 2006). In LDA, a document in the cor-
pus is modelled and explicitly represented as a fi-
nite mixture over an underlying set of topics, while
each topic is modelled as an infinite mixture over
the underlying set of words in the corpus. We use
LDA predominantly to produce the latent topics that
form a foundation for the relationships between pa-
pers and technical terms in a corpus.
Technical terms act as the terms in our model
(rather than words), because technical terms are im-
portant artefacts for formulating knowledge from
scientific texts (Ananiadou, 1994; Justeson and
Katz, 1995), because descriptions of topics are bet-
ter understandable using technical terms rather than
words (Wallach, 2006; Wang et al., 2007); and to
make our model more scalable to large corpora. The
method we use to find technical terms is light-weight
and requires little infrastructure, but does not repre-
sent state-of-the-art in terminology detection (Lopez
and Romary, 2010; Wang et al., 2007). We collect
all n-grams of words which appear in 2 or more titles
of all documents in the corpus, filter out all unigrams
appearing in the Scrabble TWL98 word list, then all
n-grams starting or ending in stop words. To de-
</bodyText>
<page confidence="0.995508">
502
</page>
<bodyText confidence="0.999956368421053">
cide whether a subsumed term should be removed
if the subsuming term exists (“statistical machine
translation” subsumes both “statistical machine” and
“machine translation”), we remove those n-grams
whose frequency is lower than 25% of their subsum-
ing terms. Finally, only the most frequent 25% of the
remaining unigrams and bigrams are retained.
We then build a D × V matrix Ω, which con-
tains the counts of V technical-terms (the columns)
in each of the D documents (the rows) in Fig. 1. Our
own implementation of LDA (with LDA parameters
α = β = 0.01) is used to collapse matrix Ω into two
denser, smaller matrices © (containing the distribu-
tion of documents over topics), and Φ (containing
the distribution of topics over technical-terms).
To model topic-specific expertise in science, we
modify the original PageRank calculation of Page at
al. (1998) by adding a topic dimension to the score
of both the bias and transition probabilities:
</bodyText>
<equation confidence="0.997654666666667">
TPR(t, d, k + 1) = αB(t, d)
− α) E T(t, d, d′)TPR(t, d′, k)
d′Eli(d)
</equation>
<bodyText confidence="0.999976444444444">
where TPR(t, d, k) is the topic-specific PageR-
ank of topic t for paper d at iteration k; B(t, d) is
the probability that paper d is chosen at random from
the corpus, given topic t, and T(t, d, d′) is the tran-
sition probability of reaching page d from page d′,
given topic t. In our formula, the transition proba-
bility T(t, d, d′) takes into account the probabilities
of topic t not only in documents d and d′, but also in
the other documents d″referenced by document d′:
</bodyText>
<equation confidence="0.999535125">
P (t|d)
B(t, d) =
E
d∗ED P(t|d*)
T *(t, d, d′)
T (t, d, d′) =
E
d∗Eli(d) T*(t, d, d*)
</equation>
<bodyText confidence="0.997191411764706">
Here d is a document whose TPR is being calcu-
lated, d′ is a document that refers to document d and
whose TPR score is being distributed during this it-
eration of the algorithm, and d″is a document that
is referred to by document d′. The first term in the
transition function ensures that TPR scores are prop-
agated only from citing documents that are highly
relevant to topic t. The second term ensures that a
larger proportion of a documents TPR score is prop-
agated to cited documents that are highly relevant to
topic t. The value P(t|d) can be read directly from
matrix © in Fig. 1.
In a final step, we age-taper TPR by dividing
TPR values by the age of the citation concerned in
years. Experimentally, this achieved the best model
in comparison to more complex dampening methods
(e.g., exponential).
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999633214285714">
Others before us have observed that time effects bias
PageRank if applied unmodified to the scientific lit-
erature (Walker et al., 2007). Walker et al.’s Cit-
eRank algorithm modifies the bias probabilities of
PageRank exponentially with age, favouring more
recent publications.
We are also not the first to have combined a notion
of topic-specification with Personalised PageRank.
The idea goes back to the original PageRank paper
by Page et al. (1998), who discuss the personaliza-
tion of PageRank by introducing a bias towards only
a set of trusted web sites W. Page et al. alter only
the bias probability B, while leaving the transition
probabilities T unchanged from global PageRank:
</bodyText>
<equation confidence="0.9389576">
� 1
B(t, d) if d ∈ W
= 0 if d ∈/ W
T(t,d,d′) =
|lo(d′)|
</equation>
<bodyText confidence="0.999034923076923">
Richardson and Domingos (2002) first used
PageRank personalisation for specialisation at
search time. For query q with corresponding topic
t = q, they use the relevance of document d to query
q as a bias. Haveliwalla (2003) calculates a Person-
alised PageRank for each of a set of 16 manually
created topics t comprised of several documents by
altering only the Bias term B, using Page et al.’s for-
mula above. This solution avoids the computational
scalability problem with Richardson and Domingos’
approach, but is limited in applicability by requiring
predefined topics. Several researchers followed Brin
and Page and Haveliwala in altering only the bias
</bodyText>
<equation confidence="0.972280666666667">
T*(t,d,d′) = �
P(t|d′) P(t|d)
E E
d∗ED P(t|d*) d′′Elo(d′) P(t|d″)
+(1
1
</equation>
<page confidence="0.987107">
503
</page>
<bodyText confidence="0.999716411764706">
probabilities, including Wu et al. (2006) and Gori
and Pucci (2006).
In contrast, Narayan et al. (2003) and Pal and
Narayan (2005) propose a model of personalisation
that alters the transition probabilities instead of the
bias probabilities. Under their model, the transition
probability T (t, d) is proportional to the number of
words in document d that are strongly present in the
documents contained in topic t. Nie et al. (2006)
produce a more computationally scalable version of
the ideas presented in Pal and Narayan (2005) by as-
sociating a context vector with each document, with
a fixed set of topics (12 in their case), for which they
learn these context vectors using a naive Bayes clas-
sifier. They then provide the possibility to alter both
the bias and transition probabilities of each webpage
as follows:
</bodyText>
<equation confidence="0.989199">
1
B(t, d) = D Ct(d)
T(t,d,d′) = -y 1′ + (1 − -y)E Ct′(d′)
|lo(d)  |lo(d′)
t′6=t
</equation>
<bodyText confidence="0.999821583333333">
where Ct(d) is the context vector score for topic
t associated with document d; the first term in
T(t, d, d′) corresponds to the probability of arriving
at page d from other pages in the same topic con-
text; the second term is the probability of arriving at
page d from other pages in a different context; and
-y is a factor that weights the influence of same-topic
jumps over other-topic jumps. Their results suggest
that -y should be close to 1, indicating that distribut-
ing PageRank within topics generates better Person-
alised PageRank scores.
Other than the fact that they treat bias and transi-
tion probabilities differently to how we treat them,
all personalisation methods discussed up to now
have the disadvantage that they rely on a fixed list
of manually selected topics, whereas our method of-
fers adaptive specialisation to corpus or domain.
The previous work closest to ours is Yang et al.
(2009), who were the first to use LDA to automat-
ically discover abstract topic distributions in a cor-
pus of scientific articles, and to combine them with
Pagerank by – in principle – altering both the bias
and transition probabilities according to the follow-
ing model:
</bodyText>
<equation confidence="0.987007833333333">
1
B(t,d) = DP(t|d)
T (t, d, d′) = -yTs t(t, d, d′) + (1 − -y)To t(t, d, d′)
1
Ts t(t,d,d′) = P(d|d′,t) ,.&apos;=
|lo(d′)|
</equation>
<bodyText confidence="0.999923772727273">
where T is the number of LDA topics, P(t|d) is a
probability of topic t given document d, which can
be read directly from the generated LDA probabili-
ties, Ts t is the probability of arriving at page d from
other pages in the same topic context, whereas To t
treats the case of arriving at a different topic. Like
Nie et al., they achieve best results with -y = 1, so
they ultimately only use bias probabilities, like the
models discussed above. Crucially, their decision
that P(d|d′, t) does not to involve any of the LDA
topic distributions is surprising. Under their model,
as in ours, when the reader randomly jumps to a new
paper, they will tend to favour papers that are closely
associated with the topic. However, when they fol-
low a citation in Yang et. al’s model, one is picked
with equal probability. In contrast, our model imple-
ments the obvious intuition that if one follows cita-
tions, one should also favour those that are closely
associated with the topic.
Let us now turn to the task of reference list rein-
troduction (RLR), i.e., the prediction of which pa-
pers a target papers originally cited, given only some
information about the paper which stands in as a
search need – either its abstract, author names and
other bibliometric information, and/or the full text of
a paper (with citation information redacted). Evalu-
ation of a search model by RLR is cheap because of
the readily available gold standard, and it thus allows
for experiments with large data sets.
State-of-the-art solutions to RLR combine lexical
similarity (often via topic models), measures of au-
thority over a citation graph, and information about
social constructs and historic patterns of citation be-
haviour. Strohman et al. (2007) perform RLR with
the paper text as a query to their recommendation
system, using text similarity, citation counts, cita-
tion coupling, author information, and the citation
graph. Their model achieves a mean-average pre-
cision of 0.102 against a corpus from the Rexa10
database. Bethard and Jurafsky (2010) improve on
Strohman et al. by the use of a SVM with 19 fea-
tures from 6 broad categories: similar terms; cited
by others; recency; cited using similar terms; simi-
lar topics; and social habits. They achieve a MAP of
</bodyText>
<page confidence="0.990813">
504
</page>
<bodyText confidence="0.999799265822785">
0.279 against the ACL Anthology Reference Corpus
(Bird et al., 2008), with the following features per-
forming best: publication age, citation counts, the
terms in citation sentences, and the LDA topics of
the citing documents. They also use (unchanged)
PageRank authority counts as one of the features,
but find that it provides little discriminative power
to the SVM. A drawback of their method is the large
amount of information that has to be provided to
create their SVM features, and the expensive train-
ing routine, which is based on pairwise paper–paper
comparisons in the corpus.
Variations of the RLR tasks exist, which addi-
tionally determine the position in the text of a pa-
per where each recommended citation should occur
(Tang and Zhang, 2009; He et al., 2011; Lu, 2011), a
task which is typically solved by comparing a mov-
ing window in the query paper against millions of
previously located citation contexts with. The draw-
back of this technique in contrast to ours is the fact
that new papers, which have not collected sufficient
contexts in the literature, are severely disadvantaged
and will never be recommended.
We first create topics and then apply PageRank
to find expertise within topical networks. It is how-
ever also possible to simultaneously model citations
and terms (Cohn and Hofmann, 2001; Mann et al.,
2006). Such models are not normally directly com-
parable to ours; for instance Bharat and Henzinger’s
(1998) model, a modified version of HITS (Klein-
berg, 1998), is query-specific.
There are numerous extensions to LDA that incor-
porate external information in addition to the lex-
ical information inside the documents in a corpus,
via author-topic models and models of publication
venues (Steyvers and Griffiths, 2007; Rosen-Zvi et
al., 2010; Tang et al., 2008). Erosheva et al. (2004)
model a corpus using a multinomial distribution si-
multaneously over the citations and terms in each
document. Topics (which they call aspects) are as-
sociated with a list of the most likely words (inter-
pretable as topics) and citations (interpretable as au-
thorities) in that aspect. Extensions of the model ex-
ist (Nallapati and Cohen, 2008; Gruber et al., 2007;
Chang and Blei, 2010; Kataria et al., 2010; Dietz et
al., 2007).
We avoid the tight coupling of topic discovery and
citation modeling that the above-mentioned works
follow for several reasons. Firstly, such models only
work for papers and citations that were present dur-
ing the learning stage, and there is no mechanism
for predicting influential citations for topics in gen-
eral, or for combinations of topics. The tight cou-
pling might also result in overlooking some author-
ities, namely those that are authoritative across sev-
eral topics, which will be penalised via low joint
distribution probabilities in combined methods be-
cause of the division of the probabilities across sev-
eral topics. Secondly, and more disturbingly, such
models will not locate topics that lack an authority
because the authority component of the joint distri-
bution will be near-zero. This rules out niches in
a corpus where papers are equally relevant to each
other, or where the niches are so young that they do
not yet have an established citation network. There
is also a scalability issue with joint models of top-
ics and citations. The evaluation data used in cou-
pled models is generally small, with the number of
papers ranging under around 2,000, the number of
citations ranging under 10,000, and the number of
topics in their models ranging from eight to twenty.
But LDA has been shown to scale to corpora of mil-
lions of terms (Newman et al., 2006), and PageRank
to billions (Page et al., 1998) of documents. Our
model, which advocates a pipelined approach, ben-
efits from the fact that separate topic modelling is
computationally tractable using LDA, and the fact
that citation graph modelling is cheap using Person-
alised PageRank.
</bodyText>
<sectionHeader confidence="0.977614" genericHeader="method">
4 Evaluation 1: RLR
</sectionHeader>
<bodyText confidence="0.999986733333333">
We evaluate our authority-based search model us-
ing the 2010 ACL Anthology Network (Radev et al.,
2009). We removed from it corrupted documents,
i.e., those of less than 100 characters or contain-
ing only control characters. The ACL Anthology
Network provides external meta-data about the ar-
ticles, which was manually curated. We do not use
this meta-data because we wanted to build as system
that can be applied to any large collection of arti-
cles, where external meta-data would not normally
exist. We therefore build an approximate citation
graph from the paper text itself, as a one-off task
when constructing the LDA space. We extract titles,
dates and full-text from every article and perform a
search of each articles title in the full-text of all other
</bodyText>
<page confidence="0.982586">
505
</page>
<table confidence="0.999648">
Model MAP
800 test papers, as in B&amp;J (2010)
B&amp;J; best model 0.287
TPR-NoDB 0.264
TPR-NoAge 0.267
TPR 0.302
10,000 test papers
A: NFIDF Cosine 0.062
B: NFIDF + citation count 0.092
C: NFIDF + global PageRank 0.099
D: NFIDF LDA (KL divergence) 0.115
E: TPR-NoDB 0.233
F: TPR-NoAge 0.242
G: TPR 0.268
</table>
<figureCaption confidence="0.672730666666667">
Figure 2: RLR results
articles (i.e., under the assumption that the reference
list is the (only) place where we will find such titles).
</figureCaption>
<bodyText confidence="0.999558033333333">
Our system generates the RLR output (the recom-
mended articles) for an article d by extracting tech-
nical terms as described in section 2, examining the
topic distribution for that article Bd,t (i.e. a Bi in
Fig. 1). We use the topic distribution of article d in
place to generate the unique age-adjusted TPR tai-
lored to the article, TPR(d, d′). The 100 articles
d′ with the highest ThemedPageRanks are recom-
mend as citations for article d. Results are reported
as mean average precision (MAP) of these 100 doc-
uments against the actual citations in the article.
We first compare our model to the state-of-the-
art (Bethard and Jurafsky, 2010). We emulate their
experimental setup by including only the pre-2004
articles in the corpus and testing only on the roughly
800 2005/6 articles with more than 5 intra-corpus
citations in their reference list, for which we have
per-paper average precision scores. The top part of
Fig. 2 shows that our model (MAP=0.302) outper-
forms their best model (MAP=0.287; difference at
5% confidence with Wilcoxon Ranked Squares test),
despite our model being a general, light-weight IR
system, which relies on LDA and PageRank alone,
and theirs is a specialised state-of-the art system,
which relies on heavy-weight machine learning and
on additional sociological features.
The lower part of Fig. 2 compares the influence
of citation count, global PageRank, topic similar-
ity, and combinations of topic similarity with ci-
tation counts or global PageRank, and our model
(TPR). For these tests, we use the entire corpus of
10,000 papers with more than 5 citations. Over the
baseline (A), n-gram-frequency-inverse-document-
frequency (NFIDF), both citation counts (B) and
global PageRank (C) make a small improvement.
Global LDA similarity scores (D) fare little better.
As the performance of the full model (G;
MAP=0.268) shows, the inclusion of topic models
lead to a large improvement over any of the above.
This is, as far as we are aware, the first time that a
large-scale evaluation that finds significant improve-
ments of a PageRank implementation over citation
counts in scientific search.
We next consider our two modifications, age-
adjusting (E) and double-biasing (F), in isolation.
We use two versions of our system where we
switched off age-tapering and double-biasing (ie.,
we only work with a change in the bias probabili-
ties, as do Nie etal. (2006), Havaliwala (2003) (al-
though their models do not include automatically
generated topics) and Yang et al. (2009)). Our
model comfortably outperforms TPR-NoDB in both
the 800 and 10,000 paper experiment. Similarly,
the effect of age-tapering alone can be seen from
the performance of TPR-NoAge (our model with-
out age-adjusting), in the difference between 0.267
and 0.302 and that between 0.242 and 0.268 (signif-
icant at 99%). This confirms our claim that a topic-
specific age-tapered PageRank is superior to global
PageRank in scientific citation networks.
</bodyText>
<sectionHeader confidence="0.971404" genericHeader="evaluation">
5 Evaluation 2: Reading Lists
</sectionHeader>
<bodyText confidence="0.999956">
The aim of the second experiment is to test our
model against a much cleaner, albeit smaller gold
standard: on the task of reconstructing the mate-
rial of expert-created reading lists. We compare our
system’s performance to three standard, commonly
used search engines: Lucene TFIDF, the Google-
indexed ACL Anthology, and Google Scholar. We
chose Google-index and Google Scholar because
they represent commonly used state-of-the-art com-
mercial search engines, and the Google-index is
what is currently offered as the standard ACL An-
thology search tool. In contrast, Lucene TFIDF
was chosen to represent an easy-to-interpret, repro-
ducible, out-of-the-box baseline implementing the
simplest kind of lexical similarity search without
any notion of authority. Of the three search engines,
</bodyText>
<page confidence="0.995937">
506
</page>
<bodyText confidence="0.999960521276596">
we would predict Google Scholar to be the tough-
est competitor to TPR, because it uses citation in-
formation directly and it is reasonable to expect that
the Google Scholar algorithm employs some domain
adaptation to the scientific domain.
We created gold standard expert-written reading
lists using the following protocol. Eight experts
were recruited from the computational linguistics
groups of two universities (3 from one, 5 from the
other). All experts had a PhD in computational lin-
guistics and several years of research experience.
They were asked to choose a subject for an (imag-
inary or existing) reading list for an MPhil student,
concerning an area in which they know the litera-
ture well. We purposefully did not give them guid-
ance as to the size of the reading list as we wanted
to observe how experts create reading lists. During
the interview, the experimenter documented the final
list chosen by the expert and made sure all papers
chosen were present in the 2010 version of the ACL
Anthology Network.
This procedure resulted in reading lists of the fol-
lowing topics and sizes: statistical parsing (22 pa-
pers); parser evaluation (4); distributional semantics
(14); domain adaptation for parsing (11); informa-
tion extraction (9); lexical semantics (14); statistical
machine translation models (5); and concept-to-text
generation (16).
In our retrieval model, which topic distribution is
chosen for a query depends on whether the query is
an exact match to one of the technical terms found
by our model. If it is, then the topic distribution
of the technical term is used directly as the query
topic distribution Bq, t (i.e. a transposed renormal-
ized ψ in Fig. 1). If not, we perform a keyword-
based search (using Lucene TFIDF), and use the av-
erage topic distribution of the top 20 documents re-
turned as the query topic distribution (i.e. several Bi
in Fig. 1). The query topic distribution is then used
to linearly combine the topic-specific TPRs into a
unique TPR tailored to the query. The 20 documents
with the highest TPR are recommended.
The three baselines are used as follows in the
experiment: The experiment is performed by issu-
ing the topic of the reading list (exactly as given
to us by the experts) as a key-word based query to
each system and recording the top 20 resulting pa-
pers answers. For Lucene TFIDF, we downloaded
Lucene.NET v2.9.2 and indexed our 2010 snapshot
of the ACL Anthology using standard Lucene pa-
rameters for the TFIDF model. For the Google-
indexed ACL Anthology (AAN), we use the in-
terface provided on the ACL Anthology website.
In order to provide an identical search ground, we
automatically exclude from the return lists papers
added after the creation of the AAN snapshot. For
Google Scholar (GS), we use the interface provided
at scholar.google.com, and parse returns to ex-
clude non-AAN material semi-automatically. In
the case of Google Scholar, we restrict the search
ground to the ACL Anthology by filtering the top
200 return sets (which may lead to fewer than 20
papers returned).
We report FCSC, RCSC and F-score for each al-
gorithm. FCSC and RCSC are new metrics which
address the problem that F-score, being binary, does
not support the notion of a “close hit”, combined
with the fact that we require a fine-grained compari-
son of the quality of different systems retrieved lists
despite the small size of our gold standard. Cita-
tion Substitution Coefficient (FCSC), a new metric
for RLR, gives higher scores to papers closely re-
lated to the target papers by citation distance. The
FCSC of each expert paper is the inverse of the num-
ber of nodes in the minimal citation graph connect-
ing each expert paper to any system-retrieved pa-
per (thus ranging between 0 and 1; non-connected
expert papers receive a zero score). We also in-
troduce Reverse Citation Substitution Coefficient
(RCSC), which measures the inverse of the num-
ber of nodes in the minimal citation graph connect-
ing each system-retrieved paper to any expert pa-
per. RCSC makes sure that systems cannot simply
increase their FCSC values by returning many ir-
relevant papers. RCSC thus corresponds to preci-
sion, while FCSC corresponds to recall. The sys-
tem RCSC and FCSC scores we report are the av-
erage scores of all the system-retrieved and expert
papers, respectively. Reporting both scores gives a
good overall picture of system performance, partic-
ularly when read together with the F-score.
Fig. 3 shows that our model comfortably beats the
competitor systems according to all metrics. In par-
ticular, our model &gt; GS/AAN &gt; Lucene TFIDF1.
</bodyText>
<footnote confidence="0.995107">
1For FCSC, the differences are statistically significant at
</footnote>
<page confidence="0.978295">
507
</page>
<table confidence="0.9988512">
FCSC RCSC F-score
AAN/Google 0.527 0.317 0.117
GS 0.519 0.364 0.112
Lucene TFIDF 0.412 0.330 0.040
TPR 0.563 0.456 0.128
</table>
<figureCaption confidence="0.98088">
Figure 3: Reading List Creation: Results.
</figureCaption>
<bodyText confidence="0.999820125">
Concerning simpler methods of estimating author-
ity, Fig. 4 shows that a multiplication of TFIDF
by citation count (as Fujii (2007) does) results in a
FCSC/RCSC of 0.419/0.359 (reported as TF-CC),
and age-tapering of citation-count by dividing the
citation count by the age of the paper in years
(reported as TF-CC-A) results in FCSC/RCSC of
0.491/0.442. We again compare different versions
of PageRank. Global PageRank can be built into
the system by simple multiplication of PR scores
as above, with and without age-tapering (reported
as TF-PR and TF-PR-A, respectively). We observe
a similar effect to the one reported by Bethard and
Jurafsky and seen in experiment 1, namely that
global PageRank only performs similar to citation
counts (0.450/0.360 vs 0.419/0.359). With respect
to double-biasing and age-tapering we see the same
effect as in experiment 22. In fact, we can see from
these results that global PageRank barely improves
over standard TFIDF, while age-tapering even with-
out topics already brings quite some improvement.
Overall, these results confirms our claim of the su-
periority of a topic-specific PageRank over global
PageRank in scientific citation networks.
</bodyText>
<sectionHeader confidence="0.99649" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.970714470588235">
We present here the first experiments that pinpoint
which modifications to PageRank are necessary to
99% confidence via a two-tailed Wilcoxon Signed Ranks test,
except that between GS and AAN (for which the confidence in-
terval is only 96%) and that between Lucene and AAN, where
it is 98%. Non-parametric paired tests such as the Wilcoxon
Signed Ranks test can be used on FCSC, but not on RCSC,
as there are different sets of underlying system-retrieved pa-
pers in each case. For RCSC, differences between our model
and all others at 99% confidence interval, between GS and
AAN/Lucene TFIDF at the 95% interval. F-score is reported
for completeness.
2Wilcoxon Signed Rank test found all differences significant
at the 99% level, except that between TF-PR and Lucene TFIDF
(significant only at the 90% level), and the following equiva-
lences: Lucene TFIDF = TF-CC; TF-PR = TF-CC; TF-CC-A =
TF-PR-A; TF-CC-A = TF-PR.
</bodyText>
<table confidence="0.996110857142857">
FCSC RCSC
TF-CC 0.419 0.359
TF-CC-A 0.491 0.442
TF-PR 0.450 0.360
TF-PR-A 0.512 0.407
TPR-NoDB 0.541 0.440
TPR-NoAge 0.526 0.436
</table>
<figureCaption confidence="0.996621">
Figure 4: Citation counts and PageRank variants.
</figureCaption>
<bodyText confidence="0.999856236842105">
adequately cater for the highly specialised situation
we encounter in science. The modification we sug-
gest are to use LDA-derived topics (Blei et al., 2003)
as approximations for scientific fields, to calculate
authority in a topic-specific way, and to age-taper
the authority scores. We present formulae where
topics personalise both the bias and the transition
probabilities. This results in a general IR model
for science incorporating a robust notion of author-
ity. Our implementation requires only minimal re-
sources and relies only on LDA and PageRank cal-
culation, which means that it is efficient during train-
ing, retraining and at search time.
We perform two evaluations. In both, our
model significantly outperforms not only state-
of-the-art, but also standard PageRank, non-age-
tapered (but topical) PageRank, and non-topical (but
age-tapered) PageRank. Our model achieves its
competitive performance by using only the raw text
and citation links. It requires no external informa-
tion, neither explicit sociological information such
as past collaborations between authors, nor the ex-
pertise and cooperation of like-minded readers, as
collaborative models do. While successful applica-
tions of collaborative filtering to bibliometric search
are rife (Goldberg et al., 2001; Agarwal et al., 2005;
McNee et al., 2006; Torres et al., 2004), including
to reading list generation (Ekstrand et al., 2010), we
wanted an entirely independent authority-based IR
model similarity. CF also suffers from a cold-start
phenomenon, where recommendations are generally
poor where data is sparse, and has to wait for papers
to be rated by a large number of authors (rather than
cited) before it can rank them.
Should the reader wish to evaluate the perfor-
mance of TPR on their own PDF papers, it has been
incorporated into the Qiqqa reference management
software 3.
</bodyText>
<footnote confidence="0.997828">
3Available at http://www.qiqqa.com
</footnote>
<page confidence="0.992887">
508
</page>
<sectionHeader confidence="0.982094" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999522701923077">
N. Agarwal, E. Haque, H. Liu, and L. Parsons. 2005. Re-
search paper recommender systems: A subspace clus-
tering approach. Advances in Web-Age Information
Management.
S. Ananiadou. 1994. A methodology for automatic term
recognition. In Proceedings of COLING.
S.K. Pal B. Narayan, C. Murthy. 2003. Topic continu-
ity for web document categorization and ranking. In
IEEE/WIC International Conference on Web Intelli-
gence.
B.D. Davison B. Wu, V. Goel. 2006. Topical trustrank:
Using topicality to combat web spam. In Proceedings
of the 15th international conference on World Wide
Web.
S. Bethard and D. Jurafsky. 2010. Who should i cite:
learning literature search models from citation behav-
ior. In Proceedings of the 19th ACM International
Conference on Information and Knowledge Manage-
ment.
K. Bharat and M.R. Henzinger. 1998. Improved algo-
rithms for topic distillation in a hyperlinked environ-
ment. In Proceedings of SIGIR.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph, M.Y.
Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan.
2008. The ACL anthology reference corpus: A ref-
erence dataset for bibliographic research in computa-
tional linguistics. In Proc. ofLREC08.
D.M. Blei and J.D. Lafferty. 2006. Correlated Topic
Models. In Advances in Neural Information Process-
ing Systems 18: Proceedings of the 2005 Conference,
page 147. Citeseer.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet allocation. The Journal ofMachine Learning
Research, 3:993–1022.
J. Boyd-Graber, D. Blei, and X. Zhu. 2007. A topic
model for word sense disambiguation. In Proceedings
ofEMNLP-CoNLL, pages 1024–1033.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. In Proceedings of the
7th International World Wide Web Conference.
J. Chang and D.M. Blei. 2010. Hierarchical relational
models for document networks. The Annals ofApplied
Statistics, 4(1):124–150.
P. Chen, H. Xie, S. Maslov, and S. Redner. 2007. Find-
ing scientific gems with google’s pagerank algorithm.
Journal ofInfometrics, 1(1):8–15.
D. Cohn and T. Hofmann. 2001. The missing link-a
probabilistic model of document content and hypertext
connectivity. Advances in neural information process-
ing systems, pages 430–436.
L. Dietz, S. Bickel, and T. Scheffer. 2007. Unsuper-
vised prediction of citation influences. In Proceedings
ofthe 24th international conference on Machine learn-
ing, page 240. ACM.
M.D. Ekstrand, P. Kannan, J.A. Stemper, J.T. Butler,
J.A. Konstan, and J.T. Riedl. 2010. Automatically
building research reading lists. In Proceedings of
the fourth ACM conference on Recommender systems,
pages 159–166. ACM.
E. Erosheva, S. Fienberg, and J. Lafferty. 2004. Mixed-
membership models of scientific publications. Pro-
ceedings of the National Academy of Sciences of the
United States ofAmerica, 101(Suppl 1):5220.
A. Fujii. 2007. Enhancing patent retrieval by citation
analysis. In Proceedings of SIGIR.
E. Garfield. 1972. Citation analysis as a tool in jour-
nal evaluation. American Association for the Advance-
ment of Science.
K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. 2001.
Eigentaste: A constant time collaborative filtering al-
gorithm. Information Retrieval, 4(2):133–151.
M. Gori and A. Pucci. 2006. Research paper rec-
ommender systems: A random-walk based approach.
IEEE Computer Society.
A. Gruber, M. Rosen-Zvi, and Y. Weiss. 2007. Hidden
topic markov models. In Proceedings ofAISTATS.
T.H. Haveliwala. 2003. Topic-sensitive pagerank: A
context-sensitive ranking algorithm for web search.
IEEE transactions on knowledge and data engineer-
ing, pages 784–796.
Q. He, B. Chen, J. Pei, B. Qiu, P. Mitra, and L. Giles.
2009. Detecting topic evolution in scientific literature:
how can citations help? In Proceeding of the 18th
ACM conference on Information and knowledge man-
agement.
Q. He, D. Kifer, J. Pei, P. Mitra, and C.L. Giles. 2011.
Citation recommendation without author supervision.
In Proceedings of the fourth ACM international con-
ference on Web search and data mining.
J.S. Justeson and S.M. Katz. 1995. Technical terminol-
ogy: some linguistic properties and an algorithm for
identification in text. Natural Language Engineering,
1(01):9–27.
S. Kataria, P. Mitra, and S. Bhatia. 2010. Utilizing Con-
text in Generative Bayesian Models for Linked Cor-
pus. In Proceedings ofAAAI.
M.M. Kessler. 1963. Bibliographic coupling be-
tween scientific papers. American Documentation,
14(1):10–25.
J. Kleinberg. 1998. Authoritative sources in a hy-
perlinked environment. In Proceedings of the 9th
ACM-SIAM Symposium on Discrete Algorithms. Also
available from http://www.cs.cornell.edu/
home/kleinber/.
</reference>
<page confidence="0.982739">
509
</page>
<reference confidence="0.999857336956521">
P. Lopez and L. Romary. 2010. HUMB: Automatic Key
Term Extraction from Scientific Articles in GROBID.
In SemEval 2010 Workshop.
Y. et al. Lu. 2011. Recommending citations with transla-
tion model. In Proceedings of the 20th ACM interna-
tional conference on Information and knowledge man-
agement.
N. Ma, J. Guan, and Y. Zhao. 2008. Bringing pagerank
to the citation analysis. Information Processing and
Management, 44(2):800–810.
G.S. Mann, D. Mimno, and A. McCallum. 2006. Biblio-
metric impact measures leveraging topic analysis. In
Proceedings of the 6th ACM/IEEE-CSjoint conference
on Digital libraries.
S.M. McNee, J. Riedl, and J.A. Konstan. 2006. Mak-
ing recommendations better: an analytic model for
human-recommender interaction. In CHI’06 extended
abstracts on Human factors in computing systems.
E. Meij and M. De Rijke. 2007. Using prior information
derived from citations in literature search. In Large
Scale Semantic Access to Content (Text, Image, Video,
and Sound).
R. Nallapati and W. Cohen. 2008. Link-plsa-lda: A new
unsupervised model for topics and influence of blogs.
In International Conference for Weblogs and Social
Media.
D. Newman, P. Smyth, and M. Steyvers. 2006. Scalable
Parallel Topic Models. Journal of Intelligence Com-
munity Research and Development.
L. Nie, B.D. Davison, and X. Qi. 2006. Topical link
analysis for web search. In Proceedings of SIGIR.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the
web. Stanford Digital Library Technologies Project.
D.R. Radev, P. Muthukrishnan, and V. Qazvinian. 2009.
The ACL Anthology Network Corpus. In Proceed-
ings, ACL Workshop on NLP and IR for Digital Li-
braries, Singapore.
M. Richardson and P Domingos. 2002. The intelligent
surfer: Probabilistic combination of link and content
information in pagerank. Advances in neural informa-
tion processing systems, 14:1441–1448.
M. Rosen-Zvi, C. Chemudugunta, T. Griffiths, P. Smyth,
and M. Steyvers. 2010. Learning author-topic models
from text corpora. ACM Transactions on Information
Systems (TOIS), 28(1):1–38.
B. Narayan S.K. Pal. 2005. A web surfer model incorpo-
rating topic continuity. IEEE Transactions on Knowl-
edge and Data Engineering, 17:726729.
H.G. Small. 1978. Cited documents as concept symbols.
Social Studies of Science, 8:327–340.
M. Steyvers and T. Griffiths. 2007. Probabilistic topic
models. In T. Landauer, D. S. McNamara, S. Dennis,
and W. Kintsch, editors, Handbook of latent semantic
analysis, page 427. Erlbaum, Hillsdale, NJ.
T. Strohman, W.B. Croft, and D. Jensen. 2007. Recom-
mending citations for academic papers. In Proceed-
ings ofSIGIR.
J. Tang and J. Zhang. 2009. A discriminative approach
to Topic-Based citation recommendation. Advances in
Knowledge Discovery and Data Mining.
J. Tang, R. Jin, and J. Zhang. 2008. A topic model-
ing approach and its integration into the random walk
framework for academic search. In Eighth IEEE Inter-
national Conference on Data Mining.
R. Torres, S.M. McNee, M. Abel, J.A. Konstan, and
J. Riedl. 2004. Enhancing digital libraries with Tech-
Lens+. In Proceedings of the 4th ACM/IEEE-CS joint
conference on Digital libraries.
D. Walker, H. Xie, K.K. Yan, and S. Maslov. 2007.
Ranking scientific publications using a model of net-
work traffic. Journal of Statistical Mechanics: Theory
and Experiment, 2007:P06010.
H.M. Wallach. 2006. Topic modeling: beyond bag-of-
words (powerpoint). In Proceedings of the 23rd inter-
national conference on Machine learning.
X. Wang, A. McCallum, and X. Wei. 2007. Topical n-
grams: Phrase and topic discovery, with an applica-
tion to information retrieval. In Proceedings of the 7th
IEEE international conference on data mining.
X. Wei and W.B. Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proceedings of SIGIR.
W. Wong, W. Liu, and M. Bennamoun. 2009. A proba-
bilistic framework for automatic term recognition. In-
telligent Data Analysis, 13(4):499–539.
Z. Yang, J. Tang, J. Zhang, J. Li, and B. Gao. 2009.
Topic-level random walk through probabilistic model.
Advances in Data and Web Management.
D. Zhou, S. Zhu, K. Yu, X. Song, B.L. Tseng, H. Zha, and
C.L. Giles. 2008. Learning multiple graphs for doc-
ument recommendations. In Proceeding of the 17th
international conference on World Wide Web.
</reference>
<page confidence="0.996651">
510
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.636469">
<title confidence="0.995576">Topical PageRank: A Model of Scientific Expertise for Bibliographic Search</title>
<author confidence="0.998438">James Jardine Simone Teufel</author>
<affiliation confidence="0.829973333333333">Natural Language and Information Processing Computer Cambridge University, CB3 0FD,</affiliation>
<abstract confidence="0.998747782608696">We model scientific expertise as a mixture of topics and authority. Authority is calculated based on the network properties of each topic network. ThemedPageRank, our combination of LDA-derived topics with PageRank differs from previous models in that topics influence both the bias and transition probabilities of PageRank. It also incorporates the age of documents. Our model is general in that it can be applied to all tasks which require an estimate of document–document, document– query, document–topic and topic–query similarities. We present two evaluations, one on the task of restoring the reference lists of 10,000 articles, the other on the task of automatically creating reading lists that mimic reading lists created by experts. In both evaluations, our system beats state-of-the-art, as well as Google Scholar and Google Search indexed againt the corpus. Our experiments also allow us to quantify the beneficial effect of our two proposed modifications to PageRank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Agarwal</author>
<author>E Haque</author>
<author>H Liu</author>
<author>L Parsons</author>
</authors>
<title>Research paper recommender systems: A subspace clustering approach. Advances in Web-Age Information Management.</title>
<date>2005</date>
<contexts>
<context position="33504" citStr="Agarwal et al., 2005" startWordPosition="5520" endWordPosition="5523"> In both, our model significantly outperforms not only stateof-the-art, but also standard PageRank, non-agetapered (but topical) PageRank, and non-topical (but age-tapered) PageRank. Our model achieves its competitive performance by using only the raw text and citation links. It requires no external information, neither explicit sociological information such as past collaborations between authors, nor the expertise and cooperation of like-minded readers, as collaborative models do. While successful applications of collaborative filtering to bibliometric search are rife (Goldberg et al., 2001; Agarwal et al., 2005; McNee et al., 2006; Torres et al., 2004), including to reading list generation (Ekstrand et al., 2010), we wanted an entirely independent authority-based IR model similarity. CF also suffers from a cold-start phenomenon, where recommendations are generally poor where data is sparse, and has to wait for papers to be rated by a large number of authors (rather than cited) before it can rank them. Should the reader wish to evaluate the performance of TPR on their own PDF papers, it has been incorporated into the Qiqqa reference management software 3. 3Available at http://www.qiqqa.com 508 Refere</context>
</contexts>
<marker>Agarwal, Haque, Liu, Parsons, 2005</marker>
<rawString>N. Agarwal, E. Haque, H. Liu, and L. Parsons. 2005. Research paper recommender systems: A subspace clustering approach. Advances in Web-Age Information Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ananiadou</author>
</authors>
<title>A methodology for automatic term recognition.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="7356" citStr="Ananiadou, 1994" startWordPosition="1170" endWordPosition="1171">ei and Croft, 2006; He et al., 2009; Blei and Lafferty, 2006). In LDA, a document in the corpus is modelled and explicitly represented as a finite mixture over an underlying set of topics, while each topic is modelled as an infinite mixture over the underlying set of words in the corpus. We use LDA predominantly to produce the latent topics that form a foundation for the relationships between papers and technical terms in a corpus. Technical terms act as the terms in our model (rather than words), because technical terms are important artefacts for formulating knowledge from scientific texts (Ananiadou, 1994; Justeson and Katz, 1995), because descriptions of topics are better understandable using technical terms rather than words (Wallach, 2006; Wang et al., 2007); and to make our model more scalable to large corpora. The method we use to find technical terms is light-weight and requires little infrastructure, but does not represent state-of-the-art in terminology detection (Lopez and Romary, 2010; Wang et al., 2007). We collect all n-grams of words which appear in 2 or more titles of all documents in the corpus, filter out all unigrams appearing in the Scrabble TWL98 word list, then all n-grams </context>
</contexts>
<marker>Ananiadou, 1994</marker>
<rawString>S. Ananiadou. 1994. A methodology for automatic term recognition. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K Pal B Narayan</author>
<author>C Murthy</author>
</authors>
<title>Topic continuity for web document categorization and ranking.</title>
<date>2003</date>
<booktitle>In IEEE/WIC International Conference on Web Intelligence.</booktitle>
<marker>Narayan, Murthy, 2003</marker>
<rawString>S.K. Pal B. Narayan, C. Murthy. 2003. Topic continuity for web document categorization and ranking. In IEEE/WIC International Conference on Web Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B D Davison B Wu</author>
<author>V Goel</author>
</authors>
<title>Topical trustrank: Using topicality to combat web spam.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web.</booktitle>
<marker>Wu, Goel, 2006</marker>
<rawString>B.D. Davison B. Wu, V. Goel. 2006. Topical trustrank: Using topicality to combat web spam. In Proceedings of the 15th international conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bethard</author>
<author>D Jurafsky</author>
</authors>
<title>Who should i cite: learning literature search models from citation behavior.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM International Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="3230" citStr="Bethard and Jurafsky, 2010" startWordPosition="503" endWordPosition="506">ent work in bibliographic search has been driven by the intuition that what works for the web should also work for science, even though citations are more comparable to each other in weight than hyperlinks. Case studies comparing PageRankbased authority measures against citation counts alone report some cases where PageRank is superior (Chen et al., 2007; Ma et al., 2008), but experimental proof of standard PageRank outperforming citation counts in a large-scale bibliographic search experiment is still outstanding. In at least one such experiment, PageRank performed worse than citation count (Bethard and Jurafsky, 2010). Straightforward PageRank calculations, when applied to the scientific literature, are hampered by two factors: on the one hand, the progression of time imposes a directional structure on the citation network. Therefore, PageRank values of older papers are systematically inflated as PageRank can only ever flow from newer to older papers (Walker et al., 2007). 501 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 501–510, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Secondly, and more int</context>
<context position="16062" citStr="Bethard and Jurafsky (2010)" startWordPosition="2678" endWordPosition="2681">y available gold standard, and it thus allows for experiments with large data sets. State-of-the-art solutions to RLR combine lexical similarity (often via topic models), measures of authority over a citation graph, and information about social constructs and historic patterns of citation behaviour. Strohman et al. (2007) perform RLR with the paper text as a query to their recommendation system, using text similarity, citation counts, citation coupling, author information, and the citation graph. Their model achieves a mean-average precision of 0.102 against a corpus from the Rexa10 database. Bethard and Jurafsky (2010) improve on Strohman et al. by the use of a SVM with 19 features from 6 broad categories: similar terms; cited by others; recency; cited using similar terms; similar topics; and social habits. They achieve a MAP of 504 0.279 against the ACL Anthology Reference Corpus (Bird et al., 2008), with the following features performing best: publication age, citation counts, the terms in citation sentences, and the LDA topics of the citing documents. They also use (unchanged) PageRank authority counts as one of the features, but find that it provides little discriminative power to the SVM. A drawback of</context>
<context position="22024" citStr="Bethard and Jurafsky, 2010" startWordPosition="3678" endWordPosition="3681">generates the RLR output (the recommended articles) for an article d by extracting technical terms as described in section 2, examining the topic distribution for that article Bd,t (i.e. a Bi in Fig. 1). We use the topic distribution of article d in place to generate the unique age-adjusted TPR tailored to the article, TPR(d, d′). The 100 articles d′ with the highest ThemedPageRanks are recommend as citations for article d. Results are reported as mean average precision (MAP) of these 100 documents against the actual citations in the article. We first compare our model to the state-of-theart (Bethard and Jurafsky, 2010). We emulate their experimental setup by including only the pre-2004 articles in the corpus and testing only on the roughly 800 2005/6 articles with more than 5 intra-corpus citations in their reference list, for which we have per-paper average precision scores. The top part of Fig. 2 shows that our model (MAP=0.302) outperforms their best model (MAP=0.287; difference at 5% confidence with Wilcoxon Ranked Squares test), despite our model being a general, light-weight IR system, which relies on LDA and PageRank alone, and theirs is a specialised state-of-the art system, which relies on heavy-we</context>
</contexts>
<marker>Bethard, Jurafsky, 2010</marker>
<rawString>S. Bethard and D. Jurafsky. 2010. Who should i cite: learning literature search models from citation behavior. In Proceedings of the 19th ACM International Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bharat</author>
<author>M R Henzinger</author>
</authors>
<title>Improved algorithms for topic distillation in a hyperlinked environment.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<marker>Bharat, Henzinger, 1998</marker>
<rawString>K. Bharat and M.R. Henzinger. 1998. Improved algorithms for topic distillation in a hyperlinked environment. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bird</author>
<author>R Dale</author>
<author>B J Dorr</author>
<author>B Gibson</author>
<author>M T Joseph</author>
<author>M Y Kan</author>
<author>D Lee</author>
<author>B Powley</author>
<author>D R Radev</author>
<author>Y F Tan</author>
</authors>
<title>The ACL anthology reference corpus: A reference dataset for bibliographic research in computational linguistics.</title>
<date>2008</date>
<booktitle>In Proc. ofLREC08.</booktitle>
<contexts>
<context position="16349" citStr="Bird et al., 2008" startWordPosition="2730" endWordPosition="2733">our. Strohman et al. (2007) perform RLR with the paper text as a query to their recommendation system, using text similarity, citation counts, citation coupling, author information, and the citation graph. Their model achieves a mean-average precision of 0.102 against a corpus from the Rexa10 database. Bethard and Jurafsky (2010) improve on Strohman et al. by the use of a SVM with 19 features from 6 broad categories: similar terms; cited by others; recency; cited using similar terms; similar topics; and social habits. They achieve a MAP of 504 0.279 against the ACL Anthology Reference Corpus (Bird et al., 2008), with the following features performing best: publication age, citation counts, the terms in citation sentences, and the LDA topics of the citing documents. They also use (unchanged) PageRank authority counts as one of the features, but find that it provides little discriminative power to the SVM. A drawback of their method is the large amount of information that has to be provided to create their SVM features, and the expensive training routine, which is based on pairwise paper–paper comparisons in the corpus. Variations of the RLR tasks exist, which additionally determine the position in th</context>
</contexts>
<marker>Bird, Dale, Dorr, Gibson, Joseph, Kan, Lee, Powley, Radev, Tan, 2008</marker>
<rawString>S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph, M.Y. Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan. 2008. The ACL anthology reference corpus: A reference dataset for bibliographic research in computational linguistics. In Proc. ofLREC08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J D Lafferty</author>
</authors>
<title>Correlated Topic Models.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 18: Proceedings of the 2005 Conference,</booktitle>
<pages>147</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="6802" citStr="Blei and Lafferty, 2006" startWordPosition="1074" endWordPosition="1077"> modification of Personalised PageRank (Page et al., 1998). Depending on the search need, the input (one or more keyword(s) or paper(s)) is converted into a topic distribution, which we then use to linearly combine the multiple topic-specific expertise scores of our model into a unique authority score representing the fit between search need and document. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a Bayesian generative probabilistic model for collections of discrete data, which has become popular for the modelling of scientific text corpora (Wei and Croft, 2006; He et al., 2009; Blei and Lafferty, 2006). In LDA, a document in the corpus is modelled and explicitly represented as a finite mixture over an underlying set of topics, while each topic is modelled as an infinite mixture over the underlying set of words in the corpus. We use LDA predominantly to produce the latent topics that form a foundation for the relationships between papers and technical terms in a corpus. Technical terms act as the terms in our model (rather than words), because technical terms are important artefacts for formulating knowledge from scientific texts (Ananiadou, 1994; Justeson and Katz, 1995), because descriptio</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>D.M. Blei and J.D. Lafferty. 2006. Correlated Topic Models. In Advances in Neural Information Processing Systems 18: Proceedings of the 2005 Conference, page 147. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>The Journal ofMachine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="4077" citStr="Blei et al., 2003" startWordPosition="629" endWordPosition="632">k values of older papers are systematically inflated as PageRank can only ever flow from newer to older papers (Walker et al., 2007). 501 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 501–510, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Secondly, and more interestingly, researchers earn their expertise in particular, well-defined scientific fields. We propose that this requires a more finegrained notion of specific – not global – expertise. Our solution is to use LDA-derived topics (Blei et al., 2003) as approximations for scientific fields, and to model the importance of a paper as a mixture of its relative expertise in each of the topics it covers. The second aspect of our solution, somewhat more mundane but still necessary to adapt PageRank successfully to model scientific expertise, is to age-taper the resultant estimation. In this paper, we present ThemedPageRank (TPR), our model of topic-specific scientific expertise, which incorporates the two modifications, and provide evidence that both are necessary for the adequate application of PageRank-style authority calculations to the scie</context>
<context position="5962" citStr="Blei et al., 2003" startWordPosition="935" endWordPosition="938">e cited. The second task is the automatic creation of reading lists, of the kind that an expert might prepare for their students. We asked experts to create a gold standard of such reading lists, and compare our system against the current de facto state-of-the-art in such tasks, Google Scholar, and again find that our system beats it comfortably. This article is structured as follows: the next section describes our model, which section 3 contrasts to related work. The evaluations are described in sections 4 and 5. Section 6 concludes. 2 Authority Model Our model first determines an LDA space (Blei et al., 2003) representing the entire document collection, which results in a set of topics describing the entirety of the field. It then calculates an authorFigure 1: A High-level view of LDA. ity model for each topic based on a modification of Personalised PageRank (Page et al., 1998). Depending on the search need, the input (one or more keyword(s) or paper(s)) is converted into a topic distribution, which we then use to linearly combine the multiple topic-specific expertise scores of our model into a unique authority score representing the fit between search need and document. Latent Dirichlet Allocatio</context>
<context position="32363" citStr="Blei et al., 2003" startWordPosition="5351" endWordPosition="5354">d for completeness. 2Wilcoxon Signed Rank test found all differences significant at the 99% level, except that between TF-PR and Lucene TFIDF (significant only at the 90% level), and the following equivalences: Lucene TFIDF = TF-CC; TF-PR = TF-CC; TF-CC-A = TF-PR-A; TF-CC-A = TF-PR. FCSC RCSC TF-CC 0.419 0.359 TF-CC-A 0.491 0.442 TF-PR 0.450 0.360 TF-PR-A 0.512 0.407 TPR-NoDB 0.541 0.440 TPR-NoAge 0.526 0.436 Figure 4: Citation counts and PageRank variants. adequately cater for the highly specialised situation we encounter in science. The modification we suggest are to use LDA-derived topics (Blei et al., 2003) as approximations for scientific fields, to calculate authority in a topic-specific way, and to age-taper the authority scores. We present formulae where topics personalise both the bias and the transition probabilities. This results in a general IR model for science incorporating a robust notion of authority. Our implementation requires only minimal resources and relies only on LDA and PageRank calculation, which means that it is efficient during training, retraining and at search time. We perform two evaluations. In both, our model significantly outperforms not only stateof-the-art, but als</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent dirichlet allocation. The Journal ofMachine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boyd-Graber</author>
<author>D Blei</author>
<author>X Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings ofEMNLP-CoNLL,</booktitle>
<pages>1024--1033</pages>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>J. Boyd-Graber, D. Blei, and X. Zhu. 2007. A topic model for word sense disambiguation. In Proceedings ofEMNLP-CoNLL, pages 1024–1033.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<booktitle>In Proceedings of the 7th International World Wide Web Conference.</booktitle>
<contexts>
<context position="2349" citStr="Brin and Page, 1998" startWordPosition="368" endWordPosition="371">r bibliometrics (Garfield, 1972) and for mapping out scientific fields via bibliometric coupling (Kessler, 1963) and co-citations (Small, 1978). More recently, citation counts have been shown to improve effectiveness of ad-hoc retrieval (Meij and De Rijke, 2007; Fujii, 2007). In science, the peer review process ensures that the right to cite is hard-earned, but on the web, hyperlinking is infinitely cheap. This means that that the authority of webpages cannot simply be approximated as the number of incoming links. Algorithmically more complex authority such as the randomsurfer model PageRank (Brin and Page, 1998) or the authorities/hub based algorithm HITS (Kleinberg, 1998)) have spectacularly improved search results in comparison to standard IR models relying on similarity calculations based on the words in the text and other text-internal informatioh. Much recent work in bibliographic search has been driven by the intuition that what works for the web should also work for science, even though citations are more comparable to each other in weight than hyperlinks. Case studies comparing PageRankbased authority measures against citation counts alone report some cases where PageRank is superior (Chen et</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual web search engine. In Proceedings of the 7th International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chang</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical relational models for document networks.</title>
<date>2010</date>
<journal>The Annals ofApplied Statistics,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="18473" citStr="Chang and Blei, 2010" startWordPosition="3079" endWordPosition="3082">nformation in addition to the lexical information inside the documents in a corpus, via author-topic models and models of publication venues (Steyvers and Griffiths, 2007; Rosen-Zvi et al., 2010; Tang et al., 2008). Erosheva et al. (2004) model a corpus using a multinomial distribution simultaneously over the citations and terms in each document. Topics (which they call aspects) are associated with a list of the most likely words (interpretable as topics) and citations (interpretable as authorities) in that aspect. Extensions of the model exist (Nallapati and Cohen, 2008; Gruber et al., 2007; Chang and Blei, 2010; Kataria et al., 2010; Dietz et al., 2007). We avoid the tight coupling of topic discovery and citation modeling that the above-mentioned works follow for several reasons. Firstly, such models only work for papers and citations that were present during the learning stage, and there is no mechanism for predicting influential citations for topics in general, or for combinations of topics. The tight coupling might also result in overlooking some authorities, namely those that are authoritative across several topics, which will be penalised via low joint distribution probabilities in combined met</context>
</contexts>
<marker>Chang, Blei, 2010</marker>
<rawString>J. Chang and D.M. Blei. 2010. Hierarchical relational models for document networks. The Annals ofApplied Statistics, 4(1):124–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Chen</author>
<author>H Xie</author>
<author>S Maslov</author>
<author>S Redner</author>
</authors>
<title>Finding scientific gems with google’s pagerank algorithm.</title>
<date>2007</date>
<journal>Journal ofInfometrics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="2959" citStr="Chen et al., 2007" startWordPosition="463" endWordPosition="466">, 1998) or the authorities/hub based algorithm HITS (Kleinberg, 1998)) have spectacularly improved search results in comparison to standard IR models relying on similarity calculations based on the words in the text and other text-internal informatioh. Much recent work in bibliographic search has been driven by the intuition that what works for the web should also work for science, even though citations are more comparable to each other in weight than hyperlinks. Case studies comparing PageRankbased authority measures against citation counts alone report some cases where PageRank is superior (Chen et al., 2007; Ma et al., 2008), but experimental proof of standard PageRank outperforming citation counts in a large-scale bibliographic search experiment is still outstanding. In at least one such experiment, PageRank performed worse than citation count (Bethard and Jurafsky, 2010). Straightforward PageRank calculations, when applied to the scientific literature, are hampered by two factors: on the one hand, the progression of time imposes a directional structure on the citation network. Therefore, PageRank values of older papers are systematically inflated as PageRank can only ever flow from newer to ol</context>
</contexts>
<marker>Chen, Xie, Maslov, Redner, 2007</marker>
<rawString>P. Chen, H. Xie, S. Maslov, and S. Redner. 2007. Finding scientific gems with google’s pagerank algorithm. Journal ofInfometrics, 1(1):8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cohn</author>
<author>T Hofmann</author>
</authors>
<title>The missing link-a probabilistic model of document content and hypertext connectivity. Advances in neural information processing systems,</title>
<date>2001</date>
<pages>430--436</pages>
<contexts>
<context position="17594" citStr="Cohn and Hofmann, 2001" startWordPosition="2936" endWordPosition="2939"> each recommended citation should occur (Tang and Zhang, 2009; He et al., 2011; Lu, 2011), a task which is typically solved by comparing a moving window in the query paper against millions of previously located citation contexts with. The drawback of this technique in contrast to ours is the fact that new papers, which have not collected sufficient contexts in the literature, are severely disadvantaged and will never be recommended. We first create topics and then apply PageRank to find expertise within topical networks. It is however also possible to simultaneously model citations and terms (Cohn and Hofmann, 2001; Mann et al., 2006). Such models are not normally directly comparable to ours; for instance Bharat and Henzinger’s (1998) model, a modified version of HITS (Kleinberg, 1998), is query-specific. There are numerous extensions to LDA that incorporate external information in addition to the lexical information inside the documents in a corpus, via author-topic models and models of publication venues (Steyvers and Griffiths, 2007; Rosen-Zvi et al., 2010; Tang et al., 2008). Erosheva et al. (2004) model a corpus using a multinomial distribution simultaneously over the citations and terms in each do</context>
</contexts>
<marker>Cohn, Hofmann, 2001</marker>
<rawString>D. Cohn and T. Hofmann. 2001. The missing link-a probabilistic model of document content and hypertext connectivity. Advances in neural information processing systems, pages 430–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Dietz</author>
<author>S Bickel</author>
<author>T Scheffer</author>
</authors>
<title>Unsupervised prediction of citation influences.</title>
<date>2007</date>
<booktitle>In Proceedings ofthe 24th international conference on Machine learning,</booktitle>
<pages>240</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="18516" citStr="Dietz et al., 2007" startWordPosition="3087" endWordPosition="3090">ation inside the documents in a corpus, via author-topic models and models of publication venues (Steyvers and Griffiths, 2007; Rosen-Zvi et al., 2010; Tang et al., 2008). Erosheva et al. (2004) model a corpus using a multinomial distribution simultaneously over the citations and terms in each document. Topics (which they call aspects) are associated with a list of the most likely words (interpretable as topics) and citations (interpretable as authorities) in that aspect. Extensions of the model exist (Nallapati and Cohen, 2008; Gruber et al., 2007; Chang and Blei, 2010; Kataria et al., 2010; Dietz et al., 2007). We avoid the tight coupling of topic discovery and citation modeling that the above-mentioned works follow for several reasons. Firstly, such models only work for papers and citations that were present during the learning stage, and there is no mechanism for predicting influential citations for topics in general, or for combinations of topics. The tight coupling might also result in overlooking some authorities, namely those that are authoritative across several topics, which will be penalised via low joint distribution probabilities in combined methods because of the division of the probabi</context>
</contexts>
<marker>Dietz, Bickel, Scheffer, 2007</marker>
<rawString>L. Dietz, S. Bickel, and T. Scheffer. 2007. Unsupervised prediction of citation influences. In Proceedings ofthe 24th international conference on Machine learning, page 240. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Ekstrand</author>
<author>P Kannan</author>
<author>J A Stemper</author>
<author>J T Butler</author>
<author>J A Konstan</author>
<author>J T Riedl</author>
</authors>
<title>Automatically building research reading lists.</title>
<date>2010</date>
<booktitle>In Proceedings of the fourth ACM conference on Recommender systems,</booktitle>
<pages>159--166</pages>
<publisher>ACM.</publisher>
<marker>Ekstrand, Kannan, Stemper, Butler, Konstan, Riedl, 2010</marker>
<rawString>M.D. Ekstrand, P. Kannan, J.A. Stemper, J.T. Butler, J.A. Konstan, and J.T. Riedl. 2010. Automatically building research reading lists. In Proceedings of the fourth ACM conference on Recommender systems, pages 159–166. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Erosheva</author>
<author>S Fienberg</author>
<author>J Lafferty</author>
</authors>
<title>Mixedmembership models of scientific publications.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States ofAmerica, 101(Suppl</booktitle>
<pages>1--5220</pages>
<contexts>
<context position="18091" citStr="Erosheva et al. (2004)" startWordPosition="3015" endWordPosition="3018">tise within topical networks. It is however also possible to simultaneously model citations and terms (Cohn and Hofmann, 2001; Mann et al., 2006). Such models are not normally directly comparable to ours; for instance Bharat and Henzinger’s (1998) model, a modified version of HITS (Kleinberg, 1998), is query-specific. There are numerous extensions to LDA that incorporate external information in addition to the lexical information inside the documents in a corpus, via author-topic models and models of publication venues (Steyvers and Griffiths, 2007; Rosen-Zvi et al., 2010; Tang et al., 2008). Erosheva et al. (2004) model a corpus using a multinomial distribution simultaneously over the citations and terms in each document. Topics (which they call aspects) are associated with a list of the most likely words (interpretable as topics) and citations (interpretable as authorities) in that aspect. Extensions of the model exist (Nallapati and Cohen, 2008; Gruber et al., 2007; Chang and Blei, 2010; Kataria et al., 2010; Dietz et al., 2007). We avoid the tight coupling of topic discovery and citation modeling that the above-mentioned works follow for several reasons. Firstly, such models only work for papers and</context>
</contexts>
<marker>Erosheva, Fienberg, Lafferty, 2004</marker>
<rawString>E. Erosheva, S. Fienberg, and J. Lafferty. 2004. Mixedmembership models of scientific publications. Proceedings of the National Academy of Sciences of the United States ofAmerica, 101(Suppl 1):5220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fujii</author>
</authors>
<title>Enhancing patent retrieval by citation analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="2004" citStr="Fujii, 2007" startWordPosition="312" endWordPosition="313">ing can be interpreted as a latent judgement of authority or trust which is bestowed onto the linked documents (Kleinberg, 1998). This makes authority an objective measure of how important that paper is to a community who confer that authority. The citation count is the simplest of these, which has been used successfully for decades for bibliometrics (Garfield, 1972) and for mapping out scientific fields via bibliometric coupling (Kessler, 1963) and co-citations (Small, 1978). More recently, citation counts have been shown to improve effectiveness of ad-hoc retrieval (Meij and De Rijke, 2007; Fujii, 2007). In science, the peer review process ensures that the right to cite is hard-earned, but on the web, hyperlinking is infinitely cheap. This means that that the authority of webpages cannot simply be approximated as the number of incoming links. Algorithmically more complex authority such as the randomsurfer model PageRank (Brin and Page, 1998) or the authorities/hub based algorithm HITS (Kleinberg, 1998)) have spectacularly improved search results in comparison to standard IR models relying on similarity calculations based on the words in the text and other text-internal informatioh. Much rece</context>
<context position="30051" citStr="Fujii (2007)" startWordPosition="4986" endWordPosition="4987">scores gives a good overall picture of system performance, particularly when read together with the F-score. Fig. 3 shows that our model comfortably beats the competitor systems according to all metrics. In particular, our model &gt; GS/AAN &gt; Lucene TFIDF1. 1For FCSC, the differences are statistically significant at 507 FCSC RCSC F-score AAN/Google 0.527 0.317 0.117 GS 0.519 0.364 0.112 Lucene TFIDF 0.412 0.330 0.040 TPR 0.563 0.456 0.128 Figure 3: Reading List Creation: Results. Concerning simpler methods of estimating authority, Fig. 4 shows that a multiplication of TFIDF by citation count (as Fujii (2007) does) results in a FCSC/RCSC of 0.419/0.359 (reported as TF-CC), and age-tapering of citation-count by dividing the citation count by the age of the paper in years (reported as TF-CC-A) results in FCSC/RCSC of 0.491/0.442. We again compare different versions of PageRank. Global PageRank can be built into the system by simple multiplication of PR scores as above, with and without age-tapering (reported as TF-PR and TF-PR-A, respectively). We observe a similar effect to the one reported by Bethard and Jurafsky and seen in experiment 1, namely that global PageRank only performs similar to citati</context>
</contexts>
<marker>Fujii, 2007</marker>
<rawString>A. Fujii. 2007. Enhancing patent retrieval by citation analysis. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Garfield</author>
</authors>
<title>Citation analysis as a tool in journal evaluation. American Association for the Advancement of Science.</title>
<date>1972</date>
<contexts>
<context position="1761" citStr="Garfield, 1972" startWordPosition="274" endWordPosition="275">tify the beneficial effect of our two proposed modifications to PageRank. 1 Introduction For search, the presence of links in a document collection adds valuable information over that contained in the text of the documents alone. Each act of linking can be interpreted as a latent judgement of authority or trust which is bestowed onto the linked documents (Kleinberg, 1998). This makes authority an objective measure of how important that paper is to a community who confer that authority. The citation count is the simplest of these, which has been used successfully for decades for bibliometrics (Garfield, 1972) and for mapping out scientific fields via bibliometric coupling (Kessler, 1963) and co-citations (Small, 1978). More recently, citation counts have been shown to improve effectiveness of ad-hoc retrieval (Meij and De Rijke, 2007; Fujii, 2007). In science, the peer review process ensures that the right to cite is hard-earned, but on the web, hyperlinking is infinitely cheap. This means that that the authority of webpages cannot simply be approximated as the number of incoming links. Algorithmically more complex authority such as the randomsurfer model PageRank (Brin and Page, 1998) or the auth</context>
</contexts>
<marker>Garfield, 1972</marker>
<rawString>E. Garfield. 1972. Citation analysis as a tool in journal evaluation. American Association for the Advancement of Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Goldberg</author>
<author>T Roeder</author>
<author>D Gupta</author>
<author>C Perkins</author>
</authors>
<title>Eigentaste: A constant time collaborative filtering algorithm.</title>
<date>2001</date>
<journal>Information Retrieval,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="33482" citStr="Goldberg et al., 2001" startWordPosition="5516" endWordPosition="5519">erform two evaluations. In both, our model significantly outperforms not only stateof-the-art, but also standard PageRank, non-agetapered (but topical) PageRank, and non-topical (but age-tapered) PageRank. Our model achieves its competitive performance by using only the raw text and citation links. It requires no external information, neither explicit sociological information such as past collaborations between authors, nor the expertise and cooperation of like-minded readers, as collaborative models do. While successful applications of collaborative filtering to bibliometric search are rife (Goldberg et al., 2001; Agarwal et al., 2005; McNee et al., 2006; Torres et al., 2004), including to reading list generation (Ekstrand et al., 2010), we wanted an entirely independent authority-based IR model similarity. CF also suffers from a cold-start phenomenon, where recommendations are generally poor where data is sparse, and has to wait for papers to be rated by a large number of authors (rather than cited) before it can rank them. Should the reader wish to evaluate the performance of TPR on their own PDF papers, it has been incorporated into the Qiqqa reference management software 3. 3Available at http://ww</context>
</contexts>
<marker>Goldberg, Roeder, Gupta, Perkins, 2001</marker>
<rawString>K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. 2001. Eigentaste: A constant time collaborative filtering algorithm. Information Retrieval, 4(2):133–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gori</author>
<author>A Pucci</author>
</authors>
<title>Research paper recommender systems: A random-walk based approach.</title>
<date>2006</date>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="11925" citStr="Gori and Pucci (2006)" startWordPosition="1964" endWordPosition="1967">nt d to query q as a bias. Haveliwalla (2003) calculates a Personalised PageRank for each of a set of 16 manually created topics t comprised of several documents by altering only the Bias term B, using Page et al.’s formula above. This solution avoids the computational scalability problem with Richardson and Domingos’ approach, but is limited in applicability by requiring predefined topics. Several researchers followed Brin and Page and Haveliwala in altering only the bias T*(t,d,d′) = � P(t|d′) P(t|d) E E d∗ED P(t|d*) d′′Elo(d′) P(t|d″) +(1 1 503 probabilities, including Wu et al. (2006) and Gori and Pucci (2006). In contrast, Narayan et al. (2003) and Pal and Narayan (2005) propose a model of personalisation that alters the transition probabilities instead of the bias probabilities. Under their model, the transition probability T (t, d) is proportional to the number of words in document d that are strongly present in the documents contained in topic t. Nie et al. (2006) produce a more computationally scalable version of the ideas presented in Pal and Narayan (2005) by associating a context vector with each document, with a fixed set of topics (12 in their case), for which they learn these context vec</context>
</contexts>
<marker>Gori, Pucci, 2006</marker>
<rawString>M. Gori and A. Pucci. 2006. Research paper recommender systems: A random-walk based approach. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gruber</author>
<author>M Rosen-Zvi</author>
<author>Y Weiss</author>
</authors>
<title>Hidden topic markov models.</title>
<date>2007</date>
<booktitle>In Proceedings ofAISTATS.</booktitle>
<contexts>
<context position="18451" citStr="Gruber et al., 2007" startWordPosition="3075" endWordPosition="3078">ncorporate external information in addition to the lexical information inside the documents in a corpus, via author-topic models and models of publication venues (Steyvers and Griffiths, 2007; Rosen-Zvi et al., 2010; Tang et al., 2008). Erosheva et al. (2004) model a corpus using a multinomial distribution simultaneously over the citations and terms in each document. Topics (which they call aspects) are associated with a list of the most likely words (interpretable as topics) and citations (interpretable as authorities) in that aspect. Extensions of the model exist (Nallapati and Cohen, 2008; Gruber et al., 2007; Chang and Blei, 2010; Kataria et al., 2010; Dietz et al., 2007). We avoid the tight coupling of topic discovery and citation modeling that the above-mentioned works follow for several reasons. Firstly, such models only work for papers and citations that were present during the learning stage, and there is no mechanism for predicting influential citations for topics in general, or for combinations of topics. The tight coupling might also result in overlooking some authorities, namely those that are authoritative across several topics, which will be penalised via low joint distribution probabi</context>
</contexts>
<marker>Gruber, Rosen-Zvi, Weiss, 2007</marker>
<rawString>A. Gruber, M. Rosen-Zvi, and Y. Weiss. 2007. Hidden topic markov models. In Proceedings ofAISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Haveliwala</author>
</authors>
<title>Topic-sensitive pagerank: A context-sensitive ranking algorithm for web search. IEEE transactions on knowledge and data engineering,</title>
<date>2003</date>
<pages>784--796</pages>
<marker>Haveliwala, 2003</marker>
<rawString>T.H. Haveliwala. 2003. Topic-sensitive pagerank: A context-sensitive ranking algorithm for web search. IEEE transactions on knowledge and data engineering, pages 784–796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q He</author>
<author>B Chen</author>
<author>J Pei</author>
<author>B Qiu</author>
<author>P Mitra</author>
<author>L Giles</author>
</authors>
<title>Detecting topic evolution in scientific literature: how can citations help?</title>
<date>2009</date>
<booktitle>In Proceeding of the 18th ACM conference on Information and knowledge management.</booktitle>
<contexts>
<context position="6776" citStr="He et al., 2009" startWordPosition="1070" endWordPosition="1073"> topic based on a modification of Personalised PageRank (Page et al., 1998). Depending on the search need, the input (one or more keyword(s) or paper(s)) is converted into a topic distribution, which we then use to linearly combine the multiple topic-specific expertise scores of our model into a unique authority score representing the fit between search need and document. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a Bayesian generative probabilistic model for collections of discrete data, which has become popular for the modelling of scientific text corpora (Wei and Croft, 2006; He et al., 2009; Blei and Lafferty, 2006). In LDA, a document in the corpus is modelled and explicitly represented as a finite mixture over an underlying set of topics, while each topic is modelled as an infinite mixture over the underlying set of words in the corpus. We use LDA predominantly to produce the latent topics that form a foundation for the relationships between papers and technical terms in a corpus. Technical terms act as the terms in our model (rather than words), because technical terms are important artefacts for formulating knowledge from scientific texts (Ananiadou, 1994; Justeson and Katz,</context>
</contexts>
<marker>He, Chen, Pei, Qiu, Mitra, Giles, 2009</marker>
<rawString>Q. He, B. Chen, J. Pei, B. Qiu, P. Mitra, and L. Giles. 2009. Detecting topic evolution in scientific literature: how can citations help? In Proceeding of the 18th ACM conference on Information and knowledge management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q He</author>
<author>D Kifer</author>
<author>J Pei</author>
<author>P Mitra</author>
<author>C L Giles</author>
</authors>
<title>Citation recommendation without author supervision.</title>
<date>2011</date>
<booktitle>In Proceedings of the fourth ACM international conference on Web search and data mining.</booktitle>
<contexts>
<context position="17050" citStr="He et al., 2011" startWordPosition="2847" endWordPosition="2850">erms in citation sentences, and the LDA topics of the citing documents. They also use (unchanged) PageRank authority counts as one of the features, but find that it provides little discriminative power to the SVM. A drawback of their method is the large amount of information that has to be provided to create their SVM features, and the expensive training routine, which is based on pairwise paper–paper comparisons in the corpus. Variations of the RLR tasks exist, which additionally determine the position in the text of a paper where each recommended citation should occur (Tang and Zhang, 2009; He et al., 2011; Lu, 2011), a task which is typically solved by comparing a moving window in the query paper against millions of previously located citation contexts with. The drawback of this technique in contrast to ours is the fact that new papers, which have not collected sufficient contexts in the literature, are severely disadvantaged and will never be recommended. We first create topics and then apply PageRank to find expertise within topical networks. It is however also possible to simultaneously model citations and terms (Cohn and Hofmann, 2001; Mann et al., 2006). Such models are not normally direc</context>
</contexts>
<marker>He, Kifer, Pei, Mitra, Giles, 2011</marker>
<rawString>Q. He, D. Kifer, J. Pei, P. Mitra, and C.L. Giles. 2011. Citation recommendation without author supervision. In Proceedings of the fourth ACM international conference on Web search and data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Justeson</author>
<author>S M Katz</author>
</authors>
<title>Technical terminology: some linguistic properties and an algorithm for identification in text.</title>
<date>1995</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>01</issue>
<contexts>
<context position="7382" citStr="Justeson and Katz, 1995" startWordPosition="1172" endWordPosition="1175">6; He et al., 2009; Blei and Lafferty, 2006). In LDA, a document in the corpus is modelled and explicitly represented as a finite mixture over an underlying set of topics, while each topic is modelled as an infinite mixture over the underlying set of words in the corpus. We use LDA predominantly to produce the latent topics that form a foundation for the relationships between papers and technical terms in a corpus. Technical terms act as the terms in our model (rather than words), because technical terms are important artefacts for formulating knowledge from scientific texts (Ananiadou, 1994; Justeson and Katz, 1995), because descriptions of topics are better understandable using technical terms rather than words (Wallach, 2006; Wang et al., 2007); and to make our model more scalable to large corpora. The method we use to find technical terms is light-weight and requires little infrastructure, but does not represent state-of-the-art in terminology detection (Lopez and Romary, 2010; Wang et al., 2007). We collect all n-grams of words which appear in 2 or more titles of all documents in the corpus, filter out all unigrams appearing in the Scrabble TWL98 word list, then all n-grams starting or ending in stop</context>
</contexts>
<marker>Justeson, Katz, 1995</marker>
<rawString>J.S. Justeson and S.M. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for identification in text. Natural Language Engineering, 1(01):9–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kataria</author>
<author>P Mitra</author>
<author>S Bhatia</author>
</authors>
<title>Utilizing Context in Generative Bayesian Models for Linked Corpus.</title>
<date>2010</date>
<booktitle>In Proceedings ofAAAI.</booktitle>
<contexts>
<context position="18495" citStr="Kataria et al., 2010" startWordPosition="3083" endWordPosition="3086"> to the lexical information inside the documents in a corpus, via author-topic models and models of publication venues (Steyvers and Griffiths, 2007; Rosen-Zvi et al., 2010; Tang et al., 2008). Erosheva et al. (2004) model a corpus using a multinomial distribution simultaneously over the citations and terms in each document. Topics (which they call aspects) are associated with a list of the most likely words (interpretable as topics) and citations (interpretable as authorities) in that aspect. Extensions of the model exist (Nallapati and Cohen, 2008; Gruber et al., 2007; Chang and Blei, 2010; Kataria et al., 2010; Dietz et al., 2007). We avoid the tight coupling of topic discovery and citation modeling that the above-mentioned works follow for several reasons. Firstly, such models only work for papers and citations that were present during the learning stage, and there is no mechanism for predicting influential citations for topics in general, or for combinations of topics. The tight coupling might also result in overlooking some authorities, namely those that are authoritative across several topics, which will be penalised via low joint distribution probabilities in combined methods because of the di</context>
</contexts>
<marker>Kataria, Mitra, Bhatia, 2010</marker>
<rawString>S. Kataria, P. Mitra, and S. Bhatia. 2010. Utilizing Context in Generative Bayesian Models for Linked Corpus. In Proceedings ofAAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Kessler</author>
</authors>
<title>Bibliographic coupling between scientific papers.</title>
<date>1963</date>
<journal>American Documentation,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="1841" citStr="Kessler, 1963" startWordPosition="286" endWordPosition="287">duction For search, the presence of links in a document collection adds valuable information over that contained in the text of the documents alone. Each act of linking can be interpreted as a latent judgement of authority or trust which is bestowed onto the linked documents (Kleinberg, 1998). This makes authority an objective measure of how important that paper is to a community who confer that authority. The citation count is the simplest of these, which has been used successfully for decades for bibliometrics (Garfield, 1972) and for mapping out scientific fields via bibliometric coupling (Kessler, 1963) and co-citations (Small, 1978). More recently, citation counts have been shown to improve effectiveness of ad-hoc retrieval (Meij and De Rijke, 2007; Fujii, 2007). In science, the peer review process ensures that the right to cite is hard-earned, but on the web, hyperlinking is infinitely cheap. This means that that the authority of webpages cannot simply be approximated as the number of incoming links. Algorithmically more complex authority such as the randomsurfer model PageRank (Brin and Page, 1998) or the authorities/hub based algorithm HITS (Kleinberg, 1998)) have spectacularly improved </context>
</contexts>
<marker>Kessler, 1963</marker>
<rawString>M.M. Kessler. 1963. Bibliographic coupling between scientific papers. American Documentation, 14(1):10–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1998</date>
<booktitle>In Proceedings of the 9th ACM-SIAM Symposium on Discrete Algorithms. Also</booktitle>
<note>available from http://www.cs.cornell.edu/ home/kleinber/.</note>
<contexts>
<context position="1520" citStr="Kleinberg, 1998" startWordPosition="233" endWordPosition="234">matically creating reading lists that mimic reading lists created by experts. In both evaluations, our system beats state-of-the-art, as well as Google Scholar and Google Search indexed againt the corpus. Our experiments also allow us to quantify the beneficial effect of our two proposed modifications to PageRank. 1 Introduction For search, the presence of links in a document collection adds valuable information over that contained in the text of the documents alone. Each act of linking can be interpreted as a latent judgement of authority or trust which is bestowed onto the linked documents (Kleinberg, 1998). This makes authority an objective measure of how important that paper is to a community who confer that authority. The citation count is the simplest of these, which has been used successfully for decades for bibliometrics (Garfield, 1972) and for mapping out scientific fields via bibliometric coupling (Kessler, 1963) and co-citations (Small, 1978). More recently, citation counts have been shown to improve effectiveness of ad-hoc retrieval (Meij and De Rijke, 2007; Fujii, 2007). In science, the peer review process ensures that the right to cite is hard-earned, but on the web, hyperlinking is</context>
<context position="17768" citStr="Kleinberg, 1998" startWordPosition="2966" endWordPosition="2968">millions of previously located citation contexts with. The drawback of this technique in contrast to ours is the fact that new papers, which have not collected sufficient contexts in the literature, are severely disadvantaged and will never be recommended. We first create topics and then apply PageRank to find expertise within topical networks. It is however also possible to simultaneously model citations and terms (Cohn and Hofmann, 2001; Mann et al., 2006). Such models are not normally directly comparable to ours; for instance Bharat and Henzinger’s (1998) model, a modified version of HITS (Kleinberg, 1998), is query-specific. There are numerous extensions to LDA that incorporate external information in addition to the lexical information inside the documents in a corpus, via author-topic models and models of publication venues (Steyvers and Griffiths, 2007; Rosen-Zvi et al., 2010; Tang et al., 2008). Erosheva et al. (2004) model a corpus using a multinomial distribution simultaneously over the citations and terms in each document. Topics (which they call aspects) are associated with a list of the most likely words (interpretable as topics) and citations (interpretable as authorities) in that as</context>
</contexts>
<marker>Kleinberg, 1998</marker>
<rawString>J. Kleinberg. 1998. Authoritative sources in a hyperlinked environment. In Proceedings of the 9th ACM-SIAM Symposium on Discrete Algorithms. Also available from http://www.cs.cornell.edu/ home/kleinber/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lopez</author>
<author>L Romary</author>
</authors>
<title>HUMB: Automatic Key Term Extraction from Scientific Articles in GROBID.</title>
<date>2010</date>
<booktitle>In SemEval 2010 Workshop.</booktitle>
<contexts>
<context position="7753" citStr="Lopez and Romary, 2010" startWordPosition="1229" endWordPosition="1232">tween papers and technical terms in a corpus. Technical terms act as the terms in our model (rather than words), because technical terms are important artefacts for formulating knowledge from scientific texts (Ananiadou, 1994; Justeson and Katz, 1995), because descriptions of topics are better understandable using technical terms rather than words (Wallach, 2006; Wang et al., 2007); and to make our model more scalable to large corpora. The method we use to find technical terms is light-weight and requires little infrastructure, but does not represent state-of-the-art in terminology detection (Lopez and Romary, 2010; Wang et al., 2007). We collect all n-grams of words which appear in 2 or more titles of all documents in the corpus, filter out all unigrams appearing in the Scrabble TWL98 word list, then all n-grams starting or ending in stop words. To de502 cide whether a subsumed term should be removed if the subsuming term exists (“statistical machine translation” subsumes both “statistical machine” and “machine translation”), we remove those n-grams whose frequency is lower than 25% of their subsuming terms. Finally, only the most frequent 25% of the remaining unigrams and bigrams are retained. We then</context>
</contexts>
<marker>Lopez, Romary, 2010</marker>
<rawString>P. Lopez and L. Romary. 2010. HUMB: Automatic Key Term Extraction from Scientific Articles in GROBID. In SemEval 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y</author>
</authors>
<title>Recommending citations with translation model.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM international conference on Information and knowledge management.</booktitle>
<marker>Y, 2011</marker>
<rawString>Y. et al. Lu. 2011. Recommending citations with translation model. In Proceedings of the 20th ACM international conference on Information and knowledge management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ma</author>
<author>J Guan</author>
<author>Y Zhao</author>
</authors>
<title>Bringing pagerank to the citation analysis.</title>
<date>2008</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>44--2</pages>
<contexts>
<context position="2977" citStr="Ma et al., 2008" startWordPosition="467" endWordPosition="470">orities/hub based algorithm HITS (Kleinberg, 1998)) have spectacularly improved search results in comparison to standard IR models relying on similarity calculations based on the words in the text and other text-internal informatioh. Much recent work in bibliographic search has been driven by the intuition that what works for the web should also work for science, even though citations are more comparable to each other in weight than hyperlinks. Case studies comparing PageRankbased authority measures against citation counts alone report some cases where PageRank is superior (Chen et al., 2007; Ma et al., 2008), but experimental proof of standard PageRank outperforming citation counts in a large-scale bibliographic search experiment is still outstanding. In at least one such experiment, PageRank performed worse than citation count (Bethard and Jurafsky, 2010). Straightforward PageRank calculations, when applied to the scientific literature, are hampered by two factors: on the one hand, the progression of time imposes a directional structure on the citation network. Therefore, PageRank values of older papers are systematically inflated as PageRank can only ever flow from newer to older papers (Walker</context>
</contexts>
<marker>Ma, Guan, Zhao, 2008</marker>
<rawString>N. Ma, J. Guan, and Y. Zhao. 2008. Bringing pagerank to the citation analysis. Information Processing and Management, 44(2):800–810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>D Mimno</author>
<author>A McCallum</author>
</authors>
<title>Bibliometric impact measures leveraging topic analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the 6th ACM/IEEE-CSjoint conference on Digital libraries.</booktitle>
<contexts>
<context position="17614" citStr="Mann et al., 2006" startWordPosition="2940" endWordPosition="2943">on should occur (Tang and Zhang, 2009; He et al., 2011; Lu, 2011), a task which is typically solved by comparing a moving window in the query paper against millions of previously located citation contexts with. The drawback of this technique in contrast to ours is the fact that new papers, which have not collected sufficient contexts in the literature, are severely disadvantaged and will never be recommended. We first create topics and then apply PageRank to find expertise within topical networks. It is however also possible to simultaneously model citations and terms (Cohn and Hofmann, 2001; Mann et al., 2006). Such models are not normally directly comparable to ours; for instance Bharat and Henzinger’s (1998) model, a modified version of HITS (Kleinberg, 1998), is query-specific. There are numerous extensions to LDA that incorporate external information in addition to the lexical information inside the documents in a corpus, via author-topic models and models of publication venues (Steyvers and Griffiths, 2007; Rosen-Zvi et al., 2010; Tang et al., 2008). Erosheva et al. (2004) model a corpus using a multinomial distribution simultaneously over the citations and terms in each document. Topics (whic</context>
</contexts>
<marker>Mann, Mimno, McCallum, 2006</marker>
<rawString>G.S. Mann, D. Mimno, and A. McCallum. 2006. Bibliometric impact measures leveraging topic analysis. In Proceedings of the 6th ACM/IEEE-CSjoint conference on Digital libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M McNee</author>
<author>J Riedl</author>
<author>J A Konstan</author>
</authors>
<title>Making recommendations better: an analytic model for human-recommender interaction. In CHI’06 extended abstracts on Human factors in computing systems.</title>
<date>2006</date>
<marker>McNee, Riedl, Konstan, 2006</marker>
<rawString>S.M. McNee, J. Riedl, and J.A. Konstan. 2006. Making recommendations better: an analytic model for human-recommender interaction. In CHI’06 extended abstracts on Human factors in computing systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Meij</author>
<author>M De Rijke</author>
</authors>
<title>Using prior information derived from citations in literature search.</title>
<date>2007</date>
<booktitle>In Large Scale Semantic Access to Content (Text, Image, Video, and Sound).</booktitle>
<marker>Meij, De Rijke, 2007</marker>
<rawString>E. Meij and M. De Rijke. 2007. Using prior information derived from citations in literature search. In Large Scale Semantic Access to Content (Text, Image, Video, and Sound).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nallapati</author>
<author>W Cohen</author>
</authors>
<title>Link-plsa-lda: A new unsupervised model for topics and influence of blogs.</title>
<date>2008</date>
<booktitle>In International Conference for Weblogs and Social Media.</booktitle>
<contexts>
<context position="18430" citStr="Nallapati and Cohen, 2008" startWordPosition="3071" endWordPosition="3074">us extensions to LDA that incorporate external information in addition to the lexical information inside the documents in a corpus, via author-topic models and models of publication venues (Steyvers and Griffiths, 2007; Rosen-Zvi et al., 2010; Tang et al., 2008). Erosheva et al. (2004) model a corpus using a multinomial distribution simultaneously over the citations and terms in each document. Topics (which they call aspects) are associated with a list of the most likely words (interpretable as topics) and citations (interpretable as authorities) in that aspect. Extensions of the model exist (Nallapati and Cohen, 2008; Gruber et al., 2007; Chang and Blei, 2010; Kataria et al., 2010; Dietz et al., 2007). We avoid the tight coupling of topic discovery and citation modeling that the above-mentioned works follow for several reasons. Firstly, such models only work for papers and citations that were present during the learning stage, and there is no mechanism for predicting influential citations for topics in general, or for combinations of topics. The tight coupling might also result in overlooking some authorities, namely those that are authoritative across several topics, which will be penalised via low joint</context>
</contexts>
<marker>Nallapati, Cohen, 2008</marker>
<rawString>R. Nallapati and W. Cohen. 2008. Link-plsa-lda: A new unsupervised model for topics and influence of blogs. In International Conference for Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>P Smyth</author>
<author>M Steyvers</author>
</authors>
<title>Scalable Parallel Topic Models.</title>
<date>2006</date>
<journal>Journal of Intelligence Community Research and Development.</journal>
<contexts>
<context position="19885" citStr="Newman et al., 2006" startWordPosition="3317" endWordPosition="3320">ent of the joint distribution will be near-zero. This rules out niches in a corpus where papers are equally relevant to each other, or where the niches are so young that they do not yet have an established citation network. There is also a scalability issue with joint models of topics and citations. The evaluation data used in coupled models is generally small, with the number of papers ranging under around 2,000, the number of citations ranging under 10,000, and the number of topics in their models ranging from eight to twenty. But LDA has been shown to scale to corpora of millions of terms (Newman et al., 2006), and PageRank to billions (Page et al., 1998) of documents. Our model, which advocates a pipelined approach, benefits from the fact that separate topic modelling is computationally tractable using LDA, and the fact that citation graph modelling is cheap using Personalised PageRank. 4 Evaluation 1: RLR We evaluate our authority-based search model using the 2010 ACL Anthology Network (Radev et al., 2009). We removed from it corrupted documents, i.e., those of less than 100 characters or containing only control characters. The ACL Anthology Network provides external meta-data about the articles,</context>
</contexts>
<marker>Newman, Smyth, Steyvers, 2006</marker>
<rawString>D. Newman, P. Smyth, and M. Steyvers. 2006. Scalable Parallel Topic Models. Journal of Intelligence Community Research and Development.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Nie</author>
<author>B D Davison</author>
<author>X Qi</author>
</authors>
<title>Topical link analysis for web search.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="12290" citStr="Nie et al. (2006)" startWordPosition="2024" endWordPosition="2027">iring predefined topics. Several researchers followed Brin and Page and Haveliwala in altering only the bias T*(t,d,d′) = � P(t|d′) P(t|d) E E d∗ED P(t|d*) d′′Elo(d′) P(t|d″) +(1 1 503 probabilities, including Wu et al. (2006) and Gori and Pucci (2006). In contrast, Narayan et al. (2003) and Pal and Narayan (2005) propose a model of personalisation that alters the transition probabilities instead of the bias probabilities. Under their model, the transition probability T (t, d) is proportional to the number of words in document d that are strongly present in the documents contained in topic t. Nie et al. (2006) produce a more computationally scalable version of the ideas presented in Pal and Narayan (2005) by associating a context vector with each document, with a fixed set of topics (12 in their case), for which they learn these context vectors using a naive Bayes classifier. They then provide the possibility to alter both the bias and transition probabilities of each webpage as follows: 1 B(t, d) = D Ct(d) T(t,d,d′) = -y 1′ + (1 − -y)E Ct′(d′) |lo(d) |lo(d′) t′6=t where Ct(d) is the context vector score for topic t associated with document d; the first term in T(t, d, d′) corresponds to the probab</context>
</contexts>
<marker>Nie, Davison, Qi, 2006</marker>
<rawString>L. Nie, B.D. Davison, and X. Qi. 2006. Topical link analysis for web search. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Page</author>
<author>S Brin</author>
<author>R Motwani</author>
<author>T Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web. Stanford Digital Library Technologies Project.</title>
<date>1998</date>
<contexts>
<context position="6236" citStr="Page et al., 1998" startWordPosition="982" endWordPosition="985">asks, Google Scholar, and again find that our system beats it comfortably. This article is structured as follows: the next section describes our model, which section 3 contrasts to related work. The evaluations are described in sections 4 and 5. Section 6 concludes. 2 Authority Model Our model first determines an LDA space (Blei et al., 2003) representing the entire document collection, which results in a set of topics describing the entirety of the field. It then calculates an authorFigure 1: A High-level view of LDA. ity model for each topic based on a modification of Personalised PageRank (Page et al., 1998). Depending on the search need, the input (one or more keyword(s) or paper(s)) is converted into a topic distribution, which we then use to linearly combine the multiple topic-specific expertise scores of our model into a unique authority score representing the fit between search need and document. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a Bayesian generative probabilistic model for collections of discrete data, which has become popular for the modelling of scientific text corpora (Wei and Croft, 2006; He et al., 2009; Blei and Lafferty, 2006). In LDA, a document in the corpus</context>
<context position="10835" citStr="Page et al. (1998)" startWordPosition="1778" endWordPosition="1781">ge of the citation concerned in years. Experimentally, this achieved the best model in comparison to more complex dampening methods (e.g., exponential). 3 Related Work Others before us have observed that time effects bias PageRank if applied unmodified to the scientific literature (Walker et al., 2007). Walker et al.’s CiteRank algorithm modifies the bias probabilities of PageRank exponentially with age, favouring more recent publications. We are also not the first to have combined a notion of topic-specification with Personalised PageRank. The idea goes back to the original PageRank paper by Page et al. (1998), who discuss the personalization of PageRank by introducing a bias towards only a set of trusted web sites W. Page et al. alter only the bias probability B, while leaving the transition probabilities T unchanged from global PageRank: � 1 B(t, d) if d ∈ W = 0 if d ∈/ W T(t,d,d′) = |lo(d′)| Richardson and Domingos (2002) first used PageRank personalisation for specialisation at search time. For query q with corresponding topic t = q, they use the relevance of document d to query q as a bias. Haveliwalla (2003) calculates a Personalised PageRank for each of a set of 16 manually created topics t </context>
<context position="19931" citStr="Page et al., 1998" startWordPosition="3325" endWordPosition="3328"> This rules out niches in a corpus where papers are equally relevant to each other, or where the niches are so young that they do not yet have an established citation network. There is also a scalability issue with joint models of topics and citations. The evaluation data used in coupled models is generally small, with the number of papers ranging under around 2,000, the number of citations ranging under 10,000, and the number of topics in their models ranging from eight to twenty. But LDA has been shown to scale to corpora of millions of terms (Newman et al., 2006), and PageRank to billions (Page et al., 1998) of documents. Our model, which advocates a pipelined approach, benefits from the fact that separate topic modelling is computationally tractable using LDA, and the fact that citation graph modelling is cheap using Personalised PageRank. 4 Evaluation 1: RLR We evaluate our authority-based search model using the 2010 ACL Anthology Network (Radev et al., 2009). We removed from it corrupted documents, i.e., those of less than 100 characters or containing only control characters. The ACL Anthology Network provides external meta-data about the articles, which was manually curated. We do not use thi</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>L. Page, S. Brin, R. Motwani, and T. Winograd. 1998. The pagerank citation ranking: Bringing order to the web. Stanford Digital Library Technologies Project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>P Muthukrishnan</author>
<author>V Qazvinian</author>
</authors>
<title>The ACL Anthology Network Corpus.</title>
<date>2009</date>
<booktitle>In Proceedings, ACL Workshop on NLP and IR for Digital Libraries,</booktitle>
<location>Singapore.</location>
<contexts>
<context position="20291" citStr="Radev et al., 2009" startWordPosition="3382" endWordPosition="3385">und 2,000, the number of citations ranging under 10,000, and the number of topics in their models ranging from eight to twenty. But LDA has been shown to scale to corpora of millions of terms (Newman et al., 2006), and PageRank to billions (Page et al., 1998) of documents. Our model, which advocates a pipelined approach, benefits from the fact that separate topic modelling is computationally tractable using LDA, and the fact that citation graph modelling is cheap using Personalised PageRank. 4 Evaluation 1: RLR We evaluate our authority-based search model using the 2010 ACL Anthology Network (Radev et al., 2009). We removed from it corrupted documents, i.e., those of less than 100 characters or containing only control characters. The ACL Anthology Network provides external meta-data about the articles, which was manually curated. We do not use this meta-data because we wanted to build as system that can be applied to any large collection of articles, where external meta-data would not normally exist. We therefore build an approximate citation graph from the paper text itself, as a one-off task when constructing the LDA space. We extract titles, dates and full-text from every article and perform a sea</context>
</contexts>
<marker>Radev, Muthukrishnan, Qazvinian, 2009</marker>
<rawString>D.R. Radev, P. Muthukrishnan, and V. Qazvinian. 2009. The ACL Anthology Network Corpus. In Proceedings, ACL Workshop on NLP and IR for Digital Libraries, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>The intelligent surfer: Probabilistic combination of link and content information in pagerank. Advances in neural information processing systems,</title>
<date>2002</date>
<pages>14--1441</pages>
<contexts>
<context position="11156" citStr="Richardson and Domingos (2002)" startWordPosition="1838" endWordPosition="1841">er et al.’s CiteRank algorithm modifies the bias probabilities of PageRank exponentially with age, favouring more recent publications. We are also not the first to have combined a notion of topic-specification with Personalised PageRank. The idea goes back to the original PageRank paper by Page et al. (1998), who discuss the personalization of PageRank by introducing a bias towards only a set of trusted web sites W. Page et al. alter only the bias probability B, while leaving the transition probabilities T unchanged from global PageRank: � 1 B(t, d) if d ∈ W = 0 if d ∈/ W T(t,d,d′) = |lo(d′)| Richardson and Domingos (2002) first used PageRank personalisation for specialisation at search time. For query q with corresponding topic t = q, they use the relevance of document d to query q as a bias. Haveliwalla (2003) calculates a Personalised PageRank for each of a set of 16 manually created topics t comprised of several documents by altering only the Bias term B, using Page et al.’s formula above. This solution avoids the computational scalability problem with Richardson and Domingos’ approach, but is limited in applicability by requiring predefined topics. Several researchers followed Brin and Page and Haveliwala </context>
</contexts>
<marker>Richardson, Domingos, 2002</marker>
<rawString>M. Richardson and P Domingos. 2002. The intelligent surfer: Probabilistic combination of link and content information in pagerank. Advances in neural information processing systems, 14:1441–1448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rosen-Zvi</author>
<author>C Chemudugunta</author>
<author>T Griffiths</author>
<author>P Smyth</author>
<author>M Steyvers</author>
</authors>
<title>Learning author-topic models from text corpora.</title>
<date>2010</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="18047" citStr="Rosen-Zvi et al., 2010" startWordPosition="3007" endWordPosition="3010">topics and then apply PageRank to find expertise within topical networks. It is however also possible to simultaneously model citations and terms (Cohn and Hofmann, 2001; Mann et al., 2006). Such models are not normally directly comparable to ours; for instance Bharat and Henzinger’s (1998) model, a modified version of HITS (Kleinberg, 1998), is query-specific. There are numerous extensions to LDA that incorporate external information in addition to the lexical information inside the documents in a corpus, via author-topic models and models of publication venues (Steyvers and Griffiths, 2007; Rosen-Zvi et al., 2010; Tang et al., 2008). Erosheva et al. (2004) model a corpus using a multinomial distribution simultaneously over the citations and terms in each document. Topics (which they call aspects) are associated with a list of the most likely words (interpretable as topics) and citations (interpretable as authorities) in that aspect. Extensions of the model exist (Nallapati and Cohen, 2008; Gruber et al., 2007; Chang and Blei, 2010; Kataria et al., 2010; Dietz et al., 2007). We avoid the tight coupling of topic discovery and citation modeling that the above-mentioned works follow for several reasons. F</context>
</contexts>
<marker>Rosen-Zvi, Chemudugunta, Griffiths, Smyth, Steyvers, 2010</marker>
<rawString>M. Rosen-Zvi, C. Chemudugunta, T. Griffiths, P. Smyth, and M. Steyvers. 2010. Learning author-topic models from text corpora. ACM Transactions on Information Systems (TOIS), 28(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Narayan S K Pal</author>
</authors>
<title>A web surfer model incorporating topic continuity.</title>
<date>2005</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<pages>17--726729</pages>
<marker>Pal, 2005</marker>
<rawString>B. Narayan S.K. Pal. 2005. A web surfer model incorporating topic continuity. IEEE Transactions on Knowledge and Data Engineering, 17:726729.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H G Small</author>
</authors>
<title>Cited documents as concept symbols.</title>
<date>1978</date>
<journal>Social Studies of Science,</journal>
<pages>8--327</pages>
<contexts>
<context position="1872" citStr="Small, 1978" startWordPosition="290" endWordPosition="291">of links in a document collection adds valuable information over that contained in the text of the documents alone. Each act of linking can be interpreted as a latent judgement of authority or trust which is bestowed onto the linked documents (Kleinberg, 1998). This makes authority an objective measure of how important that paper is to a community who confer that authority. The citation count is the simplest of these, which has been used successfully for decades for bibliometrics (Garfield, 1972) and for mapping out scientific fields via bibliometric coupling (Kessler, 1963) and co-citations (Small, 1978). More recently, citation counts have been shown to improve effectiveness of ad-hoc retrieval (Meij and De Rijke, 2007; Fujii, 2007). In science, the peer review process ensures that the right to cite is hard-earned, but on the web, hyperlinking is infinitely cheap. This means that that the authority of webpages cannot simply be approximated as the number of incoming links. Algorithmically more complex authority such as the randomsurfer model PageRank (Brin and Page, 1998) or the authorities/hub based algorithm HITS (Kleinberg, 1998)) have spectacularly improved search results in comparison to</context>
</contexts>
<marker>Small, 1978</marker>
<rawString>H.G. Small. 1978. Cited documents as concept symbols. Social Studies of Science, 8:327–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steyvers</author>
<author>T Griffiths</author>
</authors>
<title>Probabilistic topic models.</title>
<date>2007</date>
<booktitle>Handbook of latent semantic analysis,</booktitle>
<pages>427</pages>
<editor>In T. Landauer, D. S. McNamara, S. Dennis, and W. Kintsch, editors,</editor>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="18023" citStr="Steyvers and Griffiths, 2007" startWordPosition="3003" endWordPosition="3006"> recommended. We first create topics and then apply PageRank to find expertise within topical networks. It is however also possible to simultaneously model citations and terms (Cohn and Hofmann, 2001; Mann et al., 2006). Such models are not normally directly comparable to ours; for instance Bharat and Henzinger’s (1998) model, a modified version of HITS (Kleinberg, 1998), is query-specific. There are numerous extensions to LDA that incorporate external information in addition to the lexical information inside the documents in a corpus, via author-topic models and models of publication venues (Steyvers and Griffiths, 2007; Rosen-Zvi et al., 2010; Tang et al., 2008). Erosheva et al. (2004) model a corpus using a multinomial distribution simultaneously over the citations and terms in each document. Topics (which they call aspects) are associated with a list of the most likely words (interpretable as topics) and citations (interpretable as authorities) in that aspect. Extensions of the model exist (Nallapati and Cohen, 2008; Gruber et al., 2007; Chang and Blei, 2010; Kataria et al., 2010; Dietz et al., 2007). We avoid the tight coupling of topic discovery and citation modeling that the above-mentioned works follo</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>M. Steyvers and T. Griffiths. 2007. Probabilistic topic models. In T. Landauer, D. S. McNamara, S. Dennis, and W. Kintsch, editors, Handbook of latent semantic analysis, page 427. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strohman</author>
<author>W B Croft</author>
<author>D Jensen</author>
</authors>
<title>Recommending citations for academic papers.</title>
<date>2007</date>
<booktitle>In Proceedings ofSIGIR.</booktitle>
<contexts>
<context position="15758" citStr="Strohman et al. (2007)" startWordPosition="2631" endWordPosition="2634">inally cited, given only some information about the paper which stands in as a search need – either its abstract, author names and other bibliometric information, and/or the full text of a paper (with citation information redacted). Evaluation of a search model by RLR is cheap because of the readily available gold standard, and it thus allows for experiments with large data sets. State-of-the-art solutions to RLR combine lexical similarity (often via topic models), measures of authority over a citation graph, and information about social constructs and historic patterns of citation behaviour. Strohman et al. (2007) perform RLR with the paper text as a query to their recommendation system, using text similarity, citation counts, citation coupling, author information, and the citation graph. Their model achieves a mean-average precision of 0.102 against a corpus from the Rexa10 database. Bethard and Jurafsky (2010) improve on Strohman et al. by the use of a SVM with 19 features from 6 broad categories: similar terms; cited by others; recency; cited using similar terms; similar topics; and social habits. They achieve a MAP of 504 0.279 against the ACL Anthology Reference Corpus (Bird et al., 2008), with th</context>
</contexts>
<marker>Strohman, Croft, Jensen, 2007</marker>
<rawString>T. Strohman, W.B. Croft, and D. Jensen. 2007. Recommending citations for academic papers. In Proceedings ofSIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tang</author>
<author>J Zhang</author>
</authors>
<title>A discriminative approach to Topic-Based citation recommendation.</title>
<date>2009</date>
<booktitle>Advances in Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="17033" citStr="Tang and Zhang, 2009" startWordPosition="2843" endWordPosition="2846">citation counts, the terms in citation sentences, and the LDA topics of the citing documents. They also use (unchanged) PageRank authority counts as one of the features, but find that it provides little discriminative power to the SVM. A drawback of their method is the large amount of information that has to be provided to create their SVM features, and the expensive training routine, which is based on pairwise paper–paper comparisons in the corpus. Variations of the RLR tasks exist, which additionally determine the position in the text of a paper where each recommended citation should occur (Tang and Zhang, 2009; He et al., 2011; Lu, 2011), a task which is typically solved by comparing a moving window in the query paper against millions of previously located citation contexts with. The drawback of this technique in contrast to ours is the fact that new papers, which have not collected sufficient contexts in the literature, are severely disadvantaged and will never be recommended. We first create topics and then apply PageRank to find expertise within topical networks. It is however also possible to simultaneously model citations and terms (Cohn and Hofmann, 2001; Mann et al., 2006). Such models are n</context>
</contexts>
<marker>Tang, Zhang, 2009</marker>
<rawString>J. Tang and J. Zhang. 2009. A discriminative approach to Topic-Based citation recommendation. Advances in Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tang</author>
<author>R Jin</author>
<author>J Zhang</author>
</authors>
<title>A topic modeling approach and its integration into the random walk framework for academic search.</title>
<date>2008</date>
<booktitle>In Eighth IEEE International Conference on Data Mining.</booktitle>
<contexts>
<context position="18067" citStr="Tang et al., 2008" startWordPosition="3011" endWordPosition="3014">geRank to find expertise within topical networks. It is however also possible to simultaneously model citations and terms (Cohn and Hofmann, 2001; Mann et al., 2006). Such models are not normally directly comparable to ours; for instance Bharat and Henzinger’s (1998) model, a modified version of HITS (Kleinberg, 1998), is query-specific. There are numerous extensions to LDA that incorporate external information in addition to the lexical information inside the documents in a corpus, via author-topic models and models of publication venues (Steyvers and Griffiths, 2007; Rosen-Zvi et al., 2010; Tang et al., 2008). Erosheva et al. (2004) model a corpus using a multinomial distribution simultaneously over the citations and terms in each document. Topics (which they call aspects) are associated with a list of the most likely words (interpretable as topics) and citations (interpretable as authorities) in that aspect. Extensions of the model exist (Nallapati and Cohen, 2008; Gruber et al., 2007; Chang and Blei, 2010; Kataria et al., 2010; Dietz et al., 2007). We avoid the tight coupling of topic discovery and citation modeling that the above-mentioned works follow for several reasons. Firstly, such models </context>
</contexts>
<marker>Tang, Jin, Zhang, 2008</marker>
<rawString>J. Tang, R. Jin, and J. Zhang. 2008. A topic modeling approach and its integration into the random walk framework for academic search. In Eighth IEEE International Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Torres</author>
<author>S M McNee</author>
<author>M Abel</author>
<author>J A Konstan</author>
<author>J Riedl</author>
</authors>
<title>Enhancing digital libraries with TechLens+.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th ACM/IEEE-CS joint conference on Digital libraries.</booktitle>
<marker>Torres, McNee, Abel, Konstan, Riedl, 2004</marker>
<rawString>R. Torres, S.M. McNee, M. Abel, J.A. Konstan, and J. Riedl. 2004. Enhancing digital libraries with TechLens+. In Proceedings of the 4th ACM/IEEE-CS joint conference on Digital libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Walker</author>
<author>H Xie</author>
<author>K K Yan</author>
<author>S Maslov</author>
</authors>
<title>Ranking scientific publications using a model of network traffic.</title>
<date>2007</date>
<journal>Journal of Statistical Mechanics: Theory and Experiment,</journal>
<contexts>
<context position="3591" citStr="Walker et al., 2007" startWordPosition="560" endWordPosition="563"> 2008), but experimental proof of standard PageRank outperforming citation counts in a large-scale bibliographic search experiment is still outstanding. In at least one such experiment, PageRank performed worse than citation count (Bethard and Jurafsky, 2010). Straightforward PageRank calculations, when applied to the scientific literature, are hampered by two factors: on the one hand, the progression of time imposes a directional structure on the citation network. Therefore, PageRank values of older papers are systematically inflated as PageRank can only ever flow from newer to older papers (Walker et al., 2007). 501 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 501–510, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Secondly, and more interestingly, researchers earn their expertise in particular, well-defined scientific fields. We propose that this requires a more finegrained notion of specific – not global – expertise. Our solution is to use LDA-derived topics (Blei et al., 2003) as approximations for scientific fields, and to model the importance of a paper as a mixture of its relative expe</context>
<context position="10520" citStr="Walker et al., 2007" startWordPosition="1729" endWordPosition="1732">cuments that are highly relevant to topic t. The second term ensures that a larger proportion of a documents TPR score is propagated to cited documents that are highly relevant to topic t. The value P(t|d) can be read directly from matrix © in Fig. 1. In a final step, we age-taper TPR by dividing TPR values by the age of the citation concerned in years. Experimentally, this achieved the best model in comparison to more complex dampening methods (e.g., exponential). 3 Related Work Others before us have observed that time effects bias PageRank if applied unmodified to the scientific literature (Walker et al., 2007). Walker et al.’s CiteRank algorithm modifies the bias probabilities of PageRank exponentially with age, favouring more recent publications. We are also not the first to have combined a notion of topic-specification with Personalised PageRank. The idea goes back to the original PageRank paper by Page et al. (1998), who discuss the personalization of PageRank by introducing a bias towards only a set of trusted web sites W. Page et al. alter only the bias probability B, while leaving the transition probabilities T unchanged from global PageRank: � 1 B(t, d) if d ∈ W = 0 if d ∈/ W T(t,d,d′) = |lo</context>
</contexts>
<marker>Walker, Xie, Yan, Maslov, 2007</marker>
<rawString>D. Walker, H. Xie, K.K. Yan, and S. Maslov. 2007. Ranking scientific publications using a model of network traffic. Journal of Statistical Mechanics: Theory and Experiment, 2007:P06010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Wallach</author>
</authors>
<title>Topic modeling: beyond bag-ofwords (powerpoint).</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd international conference on Machine learning.</booktitle>
<contexts>
<context position="7495" citStr="Wallach, 2006" startWordPosition="1190" endWordPosition="1191">inite mixture over an underlying set of topics, while each topic is modelled as an infinite mixture over the underlying set of words in the corpus. We use LDA predominantly to produce the latent topics that form a foundation for the relationships between papers and technical terms in a corpus. Technical terms act as the terms in our model (rather than words), because technical terms are important artefacts for formulating knowledge from scientific texts (Ananiadou, 1994; Justeson and Katz, 1995), because descriptions of topics are better understandable using technical terms rather than words (Wallach, 2006; Wang et al., 2007); and to make our model more scalable to large corpora. The method we use to find technical terms is light-weight and requires little infrastructure, but does not represent state-of-the-art in terminology detection (Lopez and Romary, 2010; Wang et al., 2007). We collect all n-grams of words which appear in 2 or more titles of all documents in the corpus, filter out all unigrams appearing in the Scrabble TWL98 word list, then all n-grams starting or ending in stop words. To de502 cide whether a subsumed term should be removed if the subsuming term exists (“statistical machin</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>H.M. Wallach. 2006. Topic modeling: beyond bag-ofwords (powerpoint). In Proceedings of the 23rd international conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wang</author>
<author>A McCallum</author>
<author>X Wei</author>
</authors>
<title>Topical ngrams: Phrase and topic discovery, with an application to information retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th IEEE international conference on data mining.</booktitle>
<contexts>
<context position="7515" citStr="Wang et al., 2007" startWordPosition="1192" endWordPosition="1195">ver an underlying set of topics, while each topic is modelled as an infinite mixture over the underlying set of words in the corpus. We use LDA predominantly to produce the latent topics that form a foundation for the relationships between papers and technical terms in a corpus. Technical terms act as the terms in our model (rather than words), because technical terms are important artefacts for formulating knowledge from scientific texts (Ananiadou, 1994; Justeson and Katz, 1995), because descriptions of topics are better understandable using technical terms rather than words (Wallach, 2006; Wang et al., 2007); and to make our model more scalable to large corpora. The method we use to find technical terms is light-weight and requires little infrastructure, but does not represent state-of-the-art in terminology detection (Lopez and Romary, 2010; Wang et al., 2007). We collect all n-grams of words which appear in 2 or more titles of all documents in the corpus, filter out all unigrams appearing in the Scrabble TWL98 word list, then all n-grams starting or ending in stop words. To de502 cide whether a subsumed term should be removed if the subsuming term exists (“statistical machine translation” subsu</context>
</contexts>
<marker>Wang, McCallum, Wei, 2007</marker>
<rawString>X. Wang, A. McCallum, and X. Wei. 2007. Topical ngrams: Phrase and topic discovery, with an application to information retrieval. In Proceedings of the 7th IEEE international conference on data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wei</author>
<author>W B Croft</author>
</authors>
<title>LDA-based document models for ad-hoc retrieval.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="6759" citStr="Wei and Croft, 2006" startWordPosition="1066" endWordPosition="1069">A. ity model for each topic based on a modification of Personalised PageRank (Page et al., 1998). Depending on the search need, the input (one or more keyword(s) or paper(s)) is converted into a topic distribution, which we then use to linearly combine the multiple topic-specific expertise scores of our model into a unique authority score representing the fit between search need and document. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a Bayesian generative probabilistic model for collections of discrete data, which has become popular for the modelling of scientific text corpora (Wei and Croft, 2006; He et al., 2009; Blei and Lafferty, 2006). In LDA, a document in the corpus is modelled and explicitly represented as a finite mixture over an underlying set of topics, while each topic is modelled as an infinite mixture over the underlying set of words in the corpus. We use LDA predominantly to produce the latent topics that form a foundation for the relationships between papers and technical terms in a corpus. Technical terms act as the terms in our model (rather than words), because technical terms are important artefacts for formulating knowledge from scientific texts (Ananiadou, 1994; J</context>
</contexts>
<marker>Wei, Croft, 2006</marker>
<rawString>X. Wei and W.B. Croft. 2006. LDA-based document models for ad-hoc retrieval. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wong</author>
<author>W Liu</author>
<author>M Bennamoun</author>
</authors>
<title>A probabilistic framework for automatic term recognition.</title>
<date>2009</date>
<journal>Intelligent Data Analysis,</journal>
<volume>13</volume>
<issue>4</issue>
<marker>Wong, Liu, Bennamoun, 2009</marker>
<rawString>W. Wong, W. Liu, and M. Bennamoun. 2009. A probabilistic framework for automatic term recognition. Intelligent Data Analysis, 13(4):499–539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Yang</author>
<author>J Tang</author>
<author>J Zhang</author>
<author>J Li</author>
<author>B Gao</author>
</authors>
<title>Topic-level random walk through probabilistic model.</title>
<date>2009</date>
<booktitle>Advances in Data</booktitle>
<contexts>
<context position="13661" citStr="Yang et al. (2009)" startWordPosition="2263" endWordPosition="2266">fferent context; and -y is a factor that weights the influence of same-topic jumps over other-topic jumps. Their results suggest that -y should be close to 1, indicating that distributing PageRank within topics generates better Personalised PageRank scores. Other than the fact that they treat bias and transition probabilities differently to how we treat them, all personalisation methods discussed up to now have the disadvantage that they rely on a fixed list of manually selected topics, whereas our method offers adaptive specialisation to corpus or domain. The previous work closest to ours is Yang et al. (2009), who were the first to use LDA to automatically discover abstract topic distributions in a corpus of scientific articles, and to combine them with Pagerank by – in principle – altering both the bias and transition probabilities according to the following model: 1 B(t,d) = DP(t|d) T (t, d, d′) = -yTs t(t, d, d′) + (1 − -y)To t(t, d, d′) 1 Ts t(t,d,d′) = P(d|d′,t) ,.&apos;= |lo(d′)| where T is the number of LDA topics, P(t|d) is a probability of topic t given document d, which can be read directly from the generated LDA probabilities, Ts t is the probability of arriving at page d from other pages in</context>
<context position="23881" citStr="Yang et al. (2009)" startWordPosition="3973" endWordPosition="3976">els lead to a large improvement over any of the above. This is, as far as we are aware, the first time that a large-scale evaluation that finds significant improvements of a PageRank implementation over citation counts in scientific search. We next consider our two modifications, ageadjusting (E) and double-biasing (F), in isolation. We use two versions of our system where we switched off age-tapering and double-biasing (ie., we only work with a change in the bias probabilities, as do Nie etal. (2006), Havaliwala (2003) (although their models do not include automatically generated topics) and Yang et al. (2009)). Our model comfortably outperforms TPR-NoDB in both the 800 and 10,000 paper experiment. Similarly, the effect of age-tapering alone can be seen from the performance of TPR-NoAge (our model without age-adjusting), in the difference between 0.267 and 0.302 and that between 0.242 and 0.268 (significant at 99%). This confirms our claim that a topicspecific age-tapered PageRank is superior to global PageRank in scientific citation networks. 5 Evaluation 2: Reading Lists The aim of the second experiment is to test our model against a much cleaner, albeit smaller gold standard: on the task of reco</context>
</contexts>
<marker>Yang, Tang, Zhang, Li, Gao, 2009</marker>
<rawString>Z. Yang, J. Tang, J. Zhang, J. Li, and B. Gao. 2009. Topic-level random walk through probabilistic model. Advances in Data and Web Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhou</author>
<author>S Zhu</author>
<author>K Yu</author>
<author>X Song</author>
<author>B L Tseng</author>
<author>H Zha</author>
<author>C L Giles</author>
</authors>
<title>Learning multiple graphs for document recommendations.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th international conference on World Wide Web.</booktitle>
<marker>Zhou, Zhu, Yu, Song, Tseng, Zha, Giles, 2008</marker>
<rawString>D. Zhou, S. Zhu, K. Yu, X. Song, B.L. Tseng, H. Zha, and C.L. Giles. 2008. Learning multiple graphs for document recommendations. In Proceeding of the 17th international conference on World Wide Web.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>