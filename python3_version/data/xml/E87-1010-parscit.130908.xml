<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015623">
<title confidence="0.983340571428571">
PATTERN RECOGNITION APPLIED TO
THE ACQUISITION OF A GRAMMATICAL CLASSIFICATION SYSTEM
FROM UNRESTRICTED ENGLISH TEXT
Eric Steven Atwell and Nicos Frixou Drakos
Artificial Intelligence Group
Department of Computer Studies
Leeds University, Leeds LS2 9JT, U.K.
</title>
<email confidence="0.935859">
(EARN/BITNET: eric%leeds.ai@ac.uk)
</email>
<sectionHeader confidence="0.985094" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.993812962962963">
Within computational linguistics, the use of statistical
pattern matching is generally restricted to speech processing.
We have attempted to apply statistical techniques to discover
a grammatical classification system from a Corpus of &apos;raw&apos;
English text. A discovery procedure is simpler for a simpler
language model; we assume a first-order Markov model,
which (surprisingly) is shown elsewhere to be sufficient for
practical applications. The extraction of the parameters of a
standard Markov model is theoretically straightforward;
however, the huge size of the standard model for a Natural
Language renders it incomputable in reasonable time. We
have explored various constrained models to reduce
computation, which have yielded results of varying success.
Pattern recognition and NLP
In the area of language-related computational research,
there is a perceived dichotomy between, on the one hand,
&amp;quot;Natural Language&amp;quot; research dealing principally with
syntactic and other analysis of typed text, and on the other
hand, &amp;quot;Speech Processing&amp;quot; research dealing with synthesis,
recognition, and understanding of speech signals. This
distinction is not based merely on a difference of input
and/or output media, but seems also to correlate to noticeable
differences in assumptions and techniques used in research.
One example is in the use of statistical pattern recognition
techniques: these are used in a wide variety of computer-
based research areas, and many speech researchers take it for
granted that such methods are part of their stock in trade. In
contrast, statistical pattern recognition is hardly ever even
considered as a technique to be used in &amp;quot;Natural Language&amp;quot;
text analysis. One reason for this is that speech researchers
deal with &amp;quot;real&amp;quot;, &amp;quot;unrestricted&amp;quot; data (speech samples),
whereas much NLP research deals with highly restricted
language data, such as examples intuited by theoreticians, or
simplified English as allowed by a dialogue system, such as
a Natural Language Database Query system.
Chomsky (57) did much to discredit the use of
representative text samples or Corpora in syntactic research;
he dismissed both statistics and semantics as being of no use
to syntacticians: &amp;quot;Despite the undeniable interest and
importance of semantic and statistical studies of language,
they appear to have no direct relevance to the problem of
determining or characterizing the set of grammatical
utterances&amp;quot; (Chomsky 57 p.17). Subsequent research in
Computational Linguistics has shown that Semantics is far
more relevant and important than Chomsky gave credit for.
Phenomenal advances in computer power and capabilities
mean that we can now try statistical pattern recognition
techniques which would have been incomputable in
Chomsky&apos;s early days. Therefore, we felt that the case for
Corpus-based statistical Pattern Recognition techniques
should be reopened. Specifically, we have investigated the
possibility of using Pattern Recognition techniques for the
acquisition of a grammatical classification system from
Unrestricted English text.
</bodyText>
<subsectionHeader confidence="0.843948">
Corpus Linguistics
</subsectionHeader>
<bodyText confidence="0.999872058823529">
A Corpus of English text samples can constitute a
definitive source of data in the description of linguistic
constructs or structures. Computational linguists may use
their intuitions about the English language to devise a
grammar of English (or of some part of the English
language), and then cite example sentences from the Corpus
as evidence for their grammar (or counter-evidence against
someone else&apos;s grammar). Going one stage further,
computational linguists may use data from a Corpus as a
source of inspiration at the earlier stage of devising the rules
of the grammar, relying as little as possible on intuitions
about English grammatical structures (see, for example,
(Leech, Garside &amp; Atwell 83a)). With appropriate software
tools to extract relevant sentences from the computerised
Corpus, the process of providing evidence for (or against) a
particular grammar might in theory be largely mechanised.
Another way to use data from a Corpus for inspiration is to
manually draw parse-trees on top of example sentences taken
from the Corpus, without explicitly formulating a
corresponding Context-Free or other rewrite-rule grammar.
These trees could then be used as a set of examples for a
grammar-rule extraction program, since every subtree of
mother and immediate daughters corresponds to a phrase-
structure rewrite rule; such an experiment is described by
Atwell (forthcoming b).
However, the linguists must still use their expertise in
theoretical linguistics to devise the rules for the grammar and
the grammatical categories used in these rules. To
completely automate the process of devising a grammar for
English (or some other language), the computer system
would have to &amp;quot;know&amp;quot; about theories of grammar, how to
choose an appropriate model (e.g. context-free rules,
Generalized Phrase Structure Grammar, transition network,
or Markov process), and how to go about devising a set of
rules in the chosen formalism which actually produces the
set of sentences in the Corpus (and doesn&apos;t produce (too
many) other sentences).
Chomsky (1957), in discussing the goals of linguistic
theory, considered the possibility of a discovery procedure
for grammars, that is, a mechanical method for constructing
a grammar, given a corpus of utterances. His conclusion
was: &amp;quot;I think it is very questionable that this goal is
attainable in any interesting way&amp;quot;. Since then, linguists have
proposed various different grammatical formalisms or models
for the description of natural languages, and there has been
no general consensus amongst expert linguists as to the
&apos;best&apos; model. If even human experts can&apos;t agree on this
issue, Chomsky was probably right in thinking it
unreasonable to expect a machine, even an &apos;intelligent&apos;
expert system, to be able to choose which theory or model to
start from.
</bodyText>
<subsectionHeader confidence="0.51838">
Constrained discovery procedures
</subsectionHeader>
<bodyText confidence="0.999985966666667">
However, it may still be possible to devise a discovery
procedure if we constrain the computer system to a specific
grammatical model. The problem is simplified further if we
constrain the input to the discovery procedure, to carefully
chosen example sentences (and possibly counter-example
non-sentences). This is the approach used, for example, by
Berwick (85); his system extracted grammar rules in a
formalism based on that of Marcus&apos;s PARSIFAL (Marcus
80) from fairly simple example sentences, and managed to
acquire &amp;quot;approximately 70% of the parsing rules originally
hand-written for [Marcus&apos;s] parser&amp;quot;. Unfortunately, it is not
at all clear that such a system could be generalised to deal
with Unrestricted English text, including deviant, idiomatic
and even ill-formed sentences found in a Corpus of &apos;real&apos;
language data. This is the kind of problem best suited to
statistical pattern matching methods.
The plausibility of a truly general discovery procedure,
capable of working with unrestricted input, increases if we
can use a very simple model to describe the language in
question. Chomsky believed that English could only be
described by a phrase structure grammar augmented with
transformations, and clearly a discovery procedure for
devising Transformational Generative grammars from a
Corpus would have to be extremely complex and &apos;clever&apos;.
More recently, (Gazdar et al 85) and others have argued that
a less powerful mechanism such as a variant of phrase
structure grammar is sufficient to describe English syntax. A
discovery procedure for phrase structure grammars would be
simpler than one for TG grammars because phrase structure
grammars are simpler (more constrained) than TG granunars.
</bodyText>
<sectionHeader confidence="0.893486" genericHeader="keywords">
CLAWS
</sectionHeader>
<bodyText confidence="0.997838830188679">
For the more limited task of assigning pan-of-speech
labels to words, (Leech, Garside &amp; Atwell 83b), (Atwell 83)
and (Atwell, Leech &amp; Garside 84) showed that an even
simpler model, a first-order Markov model, will suffice.
This model was used by CLAWS, the Constituent-
Likelihood Automatic Word-tagging System, to assign
grammatical wordclass (part-of-speech) markers to words in
the LOB Corpus. The LOB Corpus is a collection of 500
British English text samples, each of just over 2000 words,
totalling over a million words in all; it is available in several
formats (with or without word-tags associated with each
word) from the Norwegian Computing Centre for the
Humanities, Bergen University (see (Johansson et al 78),
(Johansson et al 86)). The Markovian CLAWS was able to
assign the correct tag to c96% of words in the LOB Corpus,
leaving only a small residual of problematic constructs to be
analysed manually (see (Atwell 81, 82)). Although CLAWS
does not yield a full grammatical parse of input sentences,
this level of analysis is still useful for some applications; for
example, Atwell (83, 86c) showed that the first-order
Markov model could be used in detecting grammatical errors
in ill-formed input English text. The main components of
the first order Markov model or grammar used by CLAWS
were:
i) a set of 133 grammatical class labels or TAGS, e.g.
NN (singular common noun) or JJR (comparative adjective)
ii) a l33 133 tag-pair matrix, giving the frequency of
cooccurrence of every possible pair of tags (the rowsums or
colunmsums giving frequencies of individual tags)
iii) a wordlist associating each word with a list of
possible tags (with some indication of relative frequency of
each tag where a word has more than one), supplemented by
a suffuclist, prefuclist, and other default routines to deal with
input words not found in the wordlist
iv) a set of formulae to use in calculating likelihood-in-
context, to disambiguate word-tags in tagging new text.
The last item, the formulae underlying the CLAWS
system (see (Atwell 83)), constitutes the Markovian
mathematical model, and it is too much to ask of any expert
system to devise or extract this from data. At least in
theory, the first three components could be automatically
extracted from sample text WHICH HAS ALREADY BEEN
TAGGED, providing there is enough of it (in particular,
there should be many examples of each word in the wordlist,
to ensure relative tag likelihoods are accurate). However, this
is effectively &amp;quot;learning by example&amp;quot;: the tagged texts
constitute examples of correct analyses, and the program
extracting word-tag and tag-pair frequencies could be said to
be &amp;quot;learning&amp;quot; the parameters of a Markov model compatible
with the example data. Such a learning system is not a truly
generalised discovery procedure. Ideally, we would like to be
able to extract the parameters of a compatible Markov model
from RAW, untagged text.
</bodyText>
<sectionHeader confidence="0.936583" genericHeader="introduction">
RUNNEWTAGSET
</sectionHeader>
<bodyText confidence="0.999956031746032">
Statistical pattern recognition techniques have been used
in many fields of scientific computing for data classification
and pattern detection. In a typical application, there will be
a large number of data records, each of which will have a
fairly complex internal structure; the task is to somehow
group together sets of data records with &apos;similar&apos; internal
structures, and/or to note types of internal structures which
occur frequently in data records. For example, a speech
pattern recognition system is &apos;trained&apos; with repeated
examples of each word in its vocabulary to recognise the
stereotypical structure of the given speech signal, and then
when given a &apos;new&apos; sound it must classify it in terms of the
&apos;known&apos; patterns. In attempting to devise a grammatical
classification system for words in text, a record consists of
the word itself, and its grammatical context. A reasonably
large sample of text such as the million-word LOB Corpus
corresponds to a huge amount of data if the &apos;grammatical
context&apos; considered with each word is very large. The
simplest model is to assume that only the single word
immediately to the left and/or right of each TARGET word
is important in the context; and even this oversimplification
of context entails vast amounts of processing.
If we assume that each word can belong to one and only
one word-class, then whenever two words tend to occur in
the same set of immediate (lexical) contexts, they will
probably belong to the same word-class. This idea was
tested using a suite of programs called RUNNEWTAGSET
to group words in a c200,000-wonl subsection of the LOB
Corpus into word-classes. The system only attempted to
classify wordforms which occurred a hundred times or more,
the minimum sample size for lexical collocation analysis
suggested by Sinclair et al (70). All possible pairings of one
wordfonn with another wordfonn (wl,w2) were compared: if
the immediate lexical contexts in which wl occurred were
significantly similar to the immediate contexts of w2, the two
were deemed to belong to the same word-class, and the two
context-sets were merged. A threshold was used to test
&amp;quot;significant similarity&amp;quot;; initially, only words which occurred
very frequently in the same contexts were classified together,
but then the threshold was lowered in stages, allowing less
and less similar context-sets to be merged at each stage.
Unfortunately, the 200,000-word sample turned out to be
far too small for conclusive results: even in a sample of this
size, only 175 words occur 100 times or more. However,
this program run took several weeks, so it was impractical to
try a much larger text sample. There were some promising
trends; for example, at the initial threshold level, &lt;will
should could must may might&gt;, &lt;in for on by at during&gt;, &lt;is
was&gt;, &lt;had has&gt;, &lt;it he there&gt;, &lt;they we&gt;, &lt;but if when
while&gt;, &lt;make take&gt;, &lt;end use point question&gt;, and &lt;sense
number&gt; were grouped into word-classes on the basis of
their immediate lexical contexts, and in subsequent
reductions of the threshold these classes were enlarged and
new classes were added. However, even if the mammoth
computing requirements could be met, this approach to
automatic generation of a tagset or word-classification system
is unlikely to be wholely successful because it tries to assign
every word to one and only one word-class, whereas
intuitively many words can have more than one possible tag.
For example, this technique will tend to form three separate
classes for nouns, verbs, and words which can function in
both ways. For further details of the RUNNEWTAGSET
experiment, see (Atwell 86a, 86b).
</bodyText>
<subsectionHeader confidence="0.435835">
Baker&apos;s algorithm
</subsectionHeader>
<bodyText confidence="0.999903355555556">
Baker (75, 79) gives a technique which might in theory
solve this problem. Baker showed that if we assume that a
language is generated by a Markov process, then it is
theoretically possible, given a sufficiently large sample of
data, to automatically calculate the parameters of a Markov
model compatible with the data. Baker&apos;s method was
proposed as a technique for automatic training of the
parameters of a model of an acoustic processor, but it could
in theory be applied to the syntactic description of text. In
Baker&apos;s technique, the principle parameters of the Markov
model were two matrices, a(i,j) and b(i,j,k). For the word-
tagging application, i and j correspond to tags, while k
corresponds to a word; a(i,j) is the probability of tag i being
followed by tag j, and b(i,j,k) is the probability of a word
with tag i being followed by the word k with tag j. a(i,j) is
the direct equivalent of the tag-pair matrix in the CLAWS
model above. b(i,j,k) is analogous to the wordlist, except
that the information associated with each word is more
detailed: instead of just a relative frequency for each tag that
can appear with the word, there is a frequency for every
possible pair of &lt;previous tag - this tag&gt;. Baker&apos;s model is
mathematically equivalent to the one used in CLAWS; and it
has the advantage that if the tore matrices a(i,j) and b(i,j,k)
are not known, then they can be calculated by analysing raw
text. We start with initial estimates for each value, and then
use an iterative procedure to repeatedly improve on these
estimates of a(i,j) and b(i,j,k).
Unfortunately, although this grammar discovery procedure
might work in theory, the amount of computation in practice
turns out to be vast. We must iteratively estimate a
likelihood for every &lt;tag-tag&gt; pair for a(i,j), and for every
possible &lt;tag-tag-word&gt; triple for b(i,j,k). Work on tagging
the LOB Corpus has shown that a tag-set of the order of 133
tags is reasonable for English (if we include separate tags for
different inflections, since different inflexions can appear in
distinguishable syntactic contexts). Furthermore, the LOB
Corpus has roughly 50,000 word-forms in it (counting, for
example, &amp;quot;man&amp;quot;, &amp;quot;men&amp;quot;, &amp;quot;mans&amp;quot;, &amp;quot;manned&amp;quot;, &amp;quot;manning&amp;quot;, etc
as separate wordforms). Working from the &apos;raw&apos; LOB
Corpus, we would have to estimate c18,000 values for a(i,j),
and 900,000,000 values for b(i,j,k). As the process of
estimating each a(i,j) and b(i,j,k) value is in itself
computationally expensive, it is impractical to use Baker&apos;s
formulae unmodified to automatically extract word-classes
from the LOB Corpus.
</bodyText>
<subsectionHeader confidence="0.692554">
Grouping by suffix
</subsectionHeader>
<bodyText confidence="0.997957910714286">
To cut down the number of variables, we tried the
simplifying assumption that the last five letters of a word
determine which grammatical class(es) it belongs to. In
other words, we assumed words ending in the same suffix
shared the same wordclass; a not unreasonable assumption,
at least for English. CLAWS was able to assign
grammatical classes to almost any given word using a
wordlist of only c7000 words supplemented by a suffixlist,
so the assumption seemed intuitively reasonable for most
words. To further reduce the computation, we used tag-pair
probabilities from the tagged LOB Corpus to initialise a(i,j):
by using &apos;sensible&apos; starting values rather than completely
arbitrary ones, convergence should have been much more
rapid. Unfortunately, there were still far too many
interdependent variables for computation in a reasonable
time: we estimated that even with a single LOB text instead
of the complete Corpus, the first iteration alone in Baker&apos;s
scheme would take c66 hours!
algorithm and introduce other constraints into the First Order
Markov model. Another intuitively acceptable constraint
was to allow each word to belong to only a small number of
possible word classes (Baker&apos;s algorithm allowed words to
belong to many different classes, up to the total number of
classes in the system). This allowed us to try entirely
different algorithms suggested by (Wolff 76) and (Wolff 78),
based on the assumption that the class(es) a word belongs to
are determined by the immediate contexts that word appears
in in the example texts. Unfortunately, these still involved
prohibitive computing times. Wolff&apos;s second model was the
more successful of the two, coming up with putative classes
such as &lt;and at for in of to&gt;, &lt;had was&gt;, &lt;a an it one the&gt;,
&lt;at by in not on to with&gt; and &lt;but he i it one there&gt;; yet
our implementation took 5 hours CPU time to extract these
classes from an 11,000 word sample.
Heuristic constraints
We are beginning to investigate alternative strategies; for
instance, Artificial Intelligence techniques such as heuristics
to reduce the &apos;search space&apos; would seem appropriate.
However, any heuristics must not be tied too closely to our
intuitive knowledge of the English language, or else the
resultant grammar discovery procedure will effectively have
some of the grammar &amp;quot;built in&amp;quot; to it. For example, one
might try constraining the number of tags allowed for each
specific word (e.g &amp;quot;the&amp;quot;, &amp;quot;of&apos;, &amp;quot;sexy&amp;quot; can have only one tag;
&amp;quot;to&amp;quot;, &amp;quot;he?&apos;, &amp;quot;book&amp;quot; have two possible tags; &amp;quot;cold&amp;quot;, &amp;quot;base&amp;quot;,
&amp;quot;about&amp;quot; have three tags; &amp;quot;back&amp;quot;, &amp;quot;bid&amp;quot;, &amp;quot;according&amp;quot; have four
tags; &amp;quot;bound&amp;quot;, &amp;quot;beat&amp;quot;, &amp;quot;round&amp;quot; have five tags; and so on); but
this is clearly against the spirit of a truly automatic discovery
procedure in the Chomskyan sense. A more &apos;acceptable&apos;
constraint would be a general limit of, say, up to five tags
per word. A discovery procedure would start by assuming
that the context-set of every word could be partitioned into
five subsets, and then it would attempt a Prolog-style
&apos;unification&apos; of pairs of similar context-subsets, using belief
revision techniques from Artificial Intelligence (see, for
example, (Drakos 86)).
</bodyText>
<sectionHeader confidence="0.571872" genericHeader="method">
Applications
</sectionHeader>
<bodyText confidence="0.999172415384615">
Overall, we concede that the case for statistical pattern-
matching for syntactic classification is not proven. However,
there have been some promising results, which deserve
further investigation, since there would be useful applications
for any successful pattern recognition technique for the
acquisition of a grammatical classification system from
Unrestricted English text.
Note that variables in formulae mentioned above such as i
and j are not tag names (NN, VB, etc), but just integers
denoting positions in a tag-pair matrix. In a Markov model,
Alternative constraints
An alternative approach was to abandon Baker&apos;s
a tag is defined entirely by its cooccurrence likelihoods with
other tags, and with words: labels like NN, VB will not be
generated by a pattern recognition technique. However, if we
assumed initially that there are 133 tags, e.g. if we initialised
a(i,j) to a 133*133 matrix, then hopefully there should be
some correlation between distributions of tags in the LOB
tagset and the automatically generated tagset If there is
poor correlation for some tags (e.g. if the automatically-
derived tagset includes some tags whose collocational
distributions are unlike those of any of the tags used in the
LOB Corpus), then this constitutes empirical, objective
evidence that the LOB tagset could be improved upon.
In general, any alternative wordclass system could be
empirically assessed in an analogous way. The Longman
Dictionary of Contemporary English (LDOCE: Procter 78)
and the Oxford Advanced Learner&apos;s Dictionary of Current
English (OALD; Homby 74) give detailed grammatical
codes with each entry, but the two classification systems are
quite different; if samples of text tagged according to the
LDOCE and OALD tagsets were available, a pattern
recognition technique might give us an empirical, objective
way to compare and assess the classification systems, and
suggest particular areas for improvement in forthcoming
revised editions of LDOCE and OALD. This would be
particularly useful for Machine Readable versions of such
dictionaries, for use in Natural Language Processing systems
(see, for example, (Alckennan et al 85), (Alshawi et al 85),
(Atwell forthcoming a)); these could be tailored to a given
application domain (semi-)automatically.
Even though the experiments mentioned achieved only
limited success in discovering a complete grammatical
classification system, a more restricted (and hence more
achievable) aim is to concentrate on specific word classes
which are traditionally recognised as difficult to define. For
example, the techniques were particularly successful at
finding groups of words corresponding to invariant function
word classes, such as particles; Atwell (forthcoming c)
explores this further.
A bottleneck in commercial exploitation of current
research ideas in NLP is the problem of tailoring systems to
specialised linguistic registers, that is, application-specific
variations in lexicon and grammar. This research, we hope,
points the way to (semi-)automating the solution for a wide
range of applications (such as described, for example, by
Atwell (86d)). Particularly appropriate to the approach
outlined in this paper are applications systems based on
statistical models of grammar, such as (Atwell 86c). If
grammar discovery can be made to work not just for variant
registers of English, but for completely different languages
as well, then it may be possible to automate (or at least
greatly simplify) the transfer of systems such as that
described by Atwell (86c) to a wide variety of natural
languages.
</bodyText>
<sectionHeader confidence="0.786423" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999906142857143">
Automatic grammar discovery procedures are a tantalising
possibility, but the techniques we have tried so far are far
from perfect. It is worth continuing the search because of
the enormous potential benefits: a discovery procedure would
provide a solution to a major bottleneck in commercial
exploitation of NLP technology. We are keen to find
collaborators and sponsors for further research.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.970249888888889">
AkIcerman, Erik, Pieter Masereeuw, and Willem Meijs 1985
Designing a computerized lexicon for linguistic purposes
Rodopi, Amsterdam
Alshawi, Hiyan, Branimir Boguraev, and Ted Briscoe 1985,
&amp;quot;Towards a lexicon support environment for real time
parsing&amp;quot; in Proceedings of the Second Conference of the
European Chapter of the Association for Computational
Linguistics, Geneva
Atwell, Eric Steven 1981 LOB Corpus Tagging Project:
Manual Pre-edit Handbook. Departments of Computer
Studies and Linguistics, University of Lancaster
Atwell, Eric Steven 1982 LOB Corpus Tagging Project:
Manual Postedit Handbook (A mini-grammar of LOB
Corpus English, examining the types of error commonly
made during automatic (computational) analysis of ordinary
written English.) Departments of Computer Studies and
Linguistics, University of Lancaster
Atwell, Eric Steven 1983 &amp;quot;Constituent-Likelihood Grammar&amp;quot;
in Newsletter of the International Computer Archive of
Modern English (ICAME NEWS) 7: 34-67, Norwegian
Computing Centre for the Humanities, Bergen University
Atwell, Eric Steven 1986a Extracting a Natural Language
grammar from raw text Department of Computer Studies
Research Report no.208, University of Leeds
Atwell, Eric Steven I986b, &amp;quot;A parsing expert system which
learns from corpus analysis&amp;quot; in Willem Meijs (ed) Corpus
Linguistics and Beyond: Proceedings of the Seventh
International Conference on English Language Research on
Computerised Corpora, Amsterdam, Netherlands Rodopi,
Amsterdam
Atwell, Eric Steven 1986c, &amp;quot;How to detect grammatical
errors in a text without parsing it&amp;quot; Department of Computer
Studies Research Report no.212, University of Leeds; to
appear in Proceedings of the Association for Computational
Linguistics Third European Chapter Conference,
Copenhagen, Denmark (elsewhere in this book).
Atwell, Eric Steven 19$6d &amp;quot;Beyond the micro: advanced
software for research and teaching from computer science
and artificial intelligence&amp;quot; in Leech, Geoffrey and Candlin,
Christopher (eds.) Computers in English language teaching
and research: selected papers from the British Council
Symposium on computers in English language education and
research, Lancaster, England 167-183, Longman
Atwell, Eric Steven (forthcoming a) &amp;quot;A lexical database for
English learners and users: the Oxford Advanced Learner&apos;s
Dictionary&amp;quot; to appear in Proceedings of ICDBHSS87, the
1987 International Conference on DataBases in the
Humanities and Social Sciences, Montgomery, Alabama,
USA
Atwell, Eric Steven (forthcoming b) &amp;quot;Transforming a Parsed
Corpus into a Corpus Parsee&apos;, to appear in Proceedings of
the 1987 ICAME 8th International Conference on English
Language Research on Computerised Corpora, Helsinki,
Finland
Atwell, Eric Steven (forthcoming c) &amp;quot;An Expert System for
the Automatic Discovery of Particles&amp;quot; to appear in
Proceedings of the 1987 International Conference on the
Study of Particles, Berlin, East Germany
Atwell, Eric Steven, Geoffrey Leech and Roger Garside
1984, &amp;quot;Analysis of the LOB Corpus: progress and
prospects&amp;quot;, in Jan Aarts and Willem Meijs (ed), Corpus
Linguistics; Proceedings of the !CAME Conference on the
use of computer corpora in English Language Research,
Nijmegen, Netherlands Rodopi.
Baker, J K 1975 &amp;quot;Stochastic modeling for automatic speech
understanding&amp;quot; in D R Reddy (ed) Speech recognition
Academic Press
Baker, J K 1979 &amp;quot;Trainable grammars for speech
recognition&amp;quot; in Klatt, D H and Wolf J J (eds.) Speech
communication papers for the 97th meeting of the acoustical
society of America: 547-550
Berwick, R 1985 The acquisition of syntactic knowledge
MIT Press, Cambridge (MA) and London
Chomsky, Noam 1957 Syntactic Structures Mouton, The
Hague
Drakos, Nicos Frixou 1986 Electrical circuit analysis using
algebraic manipulation and belief revision Department of
Computer Studies, Leeds University
Leech, Geoffrey, Roger Garside, and Eric Steven Atwell
1983a, &amp;quot;Recent developments in the use of computer corpora
in English language research&amp;quot; in Transactions of the
Philological Society 1983: 23-40.
Leech, Geoffrey, Garside, Roger and Atwell, Eric Steven
1983b &amp;quot;The Automatic Grammatical Tagging of the LOB
Corpus&amp;quot; in Newsletter of the International Computer Archive
of Modern English (ICAME NEWS) 7: 13-33, Norwegian
Computing Centre for the Humanities, Bergen University
Gazdar, Gerald, Ewan Klein, Geoffrey Pullum, and Ivan Sag
1985 Generalized Phrase Structure Grammar Blackwell,
Oxford
Hornby, A S, with Cowie, A P (eds.) 1974 Oxford Advanced
Learner&apos;s Dictionary of Current English (third edition)
Oxford University Press
Johansson, Stig, Geoffrey Leech and Helen Goodluck 1978
Manual of information to accompany the Lancaster-
Oslo/Bergen Corpus of British English, for use with digital
computers Department of English, Oslo University
Johansson, Stig, Eric Atwell, Roger Garside, and Geoffrey
Leech 1986 The Tagged LOB Corpus Norwegian Computing
</reference>
<page confidence="0.984754">
61
</page>
<reference confidence="0.991596076923077">
Centre for the Humanities, University of Bergen, Norway.
Marcus, M P 1980 A Theory of Syntactic Recognition for
Natural Language MIT Press, Cambridge, MA
Procter, Paul (editor-in-chief) 1978 Longman Dictionary of
Contemporary English Longman
Sinclair, J, Jones, S, and Daley, R 1970 English lexical
studies, Report to OSTI on pmject OLP/08; Dept of English,
Birmingham University
Wolff, J G 1976 &amp;quot;Frequency, Conceptual Structure and
Pattern Recognition&amp;quot; in British Journal of Psychology
67:377-390
Wolff, J G 1978 &amp;quot;The Discovery of Syntagmatic and
Paradigmatic Classes&amp;quot; in ALLC Bulletin 6(1):141
</reference>
<page confidence="0.999188">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.986283666666667">PATTERN RECOGNITION APPLIED TO THE ACQUISITION OF A GRAMMATICAL CLASSIFICATION SYSTEM FROM UNRESTRICTED ENGLISH TEXT</title>
<author confidence="0.998866">Eric Steven Atwell</author>
<author confidence="0.998866">Nicos Frixou Drakos</author>
<affiliation confidence="0.9999155">Artificial Intelligence Group Department of Computer Studies</affiliation>
<address confidence="0.72244">Leeds University, Leeds LS2 9JT, U.K.</address>
<email confidence="0.91329">(EARN/BITNET:eric%leeds.ai@ac.uk)</email>
<abstract confidence="0.997939572748269">computational linguistics, the statistical pattern matching is generally restricted to speech processing. We have attempted to apply statistical techniques to discover a grammatical classification system from a Corpus of &apos;raw&apos; English text. A discovery procedure is simpler for a simpler language model; we assume a first-order Markov model, which (surprisingly) is shown elsewhere to be sufficient for practical applications. The extraction of the parameters of a standard Markov model is theoretically straightforward; however, the huge size of the standard model for a Natural Language renders it incomputable in reasonable time. We have explored various constrained models to reduce computation, which have yielded results of varying success. Pattern recognition and NLP In the area of language-related computational research, there is a perceived dichotomy between, on the one hand, &amp;quot;Natural Language&amp;quot; research dealing principally with syntactic and other analysis of typed text, and on the other hand, &amp;quot;Speech Processing&amp;quot; research dealing with synthesis, recognition, and understanding of speech signals. This distinction is not based merely on a difference of input and/or output media, but seems also to correlate to noticeable differences in assumptions and techniques used in research. One example is in the use of statistical pattern recognition techniques: these are used in a wide variety of computerbased research areas, and many speech researchers take it for granted that such methods are part of their stock in trade. In contrast, statistical pattern recognition is hardly ever even considered as a technique to be used in &amp;quot;Natural Language&amp;quot; text analysis. One reason for this is that speech researchers deal with &amp;quot;real&amp;quot;, &amp;quot;unrestricted&amp;quot; data (speech samples), whereas much NLP research deals with highly restricted language data, such as examples intuited by theoreticians, or simplified English as allowed by a dialogue system, such as a Natural Language Database Query system. Chomsky (57) did much to discredit the use of representative text samples or Corpora in syntactic research; dismissed both statistics and semantics as being of no to syntacticians: &amp;quot;Despite the undeniable interest and importance of semantic and statistical studies of language, they appear to have no direct relevance to the problem of determining or characterizing the set of grammatical utterances&amp;quot; (Chomsky 57 p.17). Subsequent research in Computational Linguistics has shown that Semantics is far more relevant and important than Chomsky gave credit for. Phenomenal advances in computer power and capabilities mean that we can now try statistical pattern recognition techniques which would have been incomputable in Chomsky&apos;s early days. Therefore, we felt that the case for Corpus-based statistical Pattern Recognition techniques should be reopened. Specifically, we have investigated the possibility of using Pattern Recognition techniques for the acquisition of a grammatical classification system from Unrestricted English text. Corpus Linguistics A Corpus of English text samples can constitute a definitive source of data in the description of linguistic constructs or structures. Computational linguists may use their intuitions about the English language to devise a grammar of English (or of some part of the English language), and then cite example sentences from the Corpus as evidence for their grammar (or counter-evidence against someone else&apos;s grammar). Going one stage further, computational linguists may use data from a Corpus as a source of inspiration at the earlier stage of devising the rules of the grammar, relying as little as possible on intuitions about English grammatical structures (see, for example, (Leech, Garside &amp; Atwell 83a)). With appropriate software tools to extract relevant sentences from the computerised Corpus, the process of providing evidence for (or against) a particular grammar might in theory be largely mechanised. Another way to use data from a Corpus for inspiration is to manually draw parse-trees on top of example sentences taken from the Corpus, without explicitly formulating a corresponding Context-Free or other rewrite-rule grammar. These trees could then be used as a set of examples for a grammar-rule extraction program, since every subtree of mother and immediate daughters corresponds to a phrasestructure rewrite rule; such an experiment is described by Atwell (forthcoming b). However, the linguists must still use their expertise in theoretical linguistics to devise the rules for the grammar and the grammatical categories used in these rules. To completely automate the process of devising a grammar for English (or some other language), the computer system would have to &amp;quot;know&amp;quot; about theories of grammar, how to choose an appropriate model (e.g. context-free rules, Generalized Phrase Structure Grammar, transition network, or Markov process), and how to go about devising a set of rules in the chosen formalism which actually produces the set of sentences in the Corpus (and doesn&apos;t produce (too many) other sentences). Chomsky (1957), in discussing the goals of linguistic considered the possibility of a procedure grammars, that a mechanical method for constructing a grammar, given a corpus of utterances. His conclusion was: &amp;quot;I think it is very questionable that this goal is attainable in any interesting way&amp;quot;. Since then, linguists have proposed various different grammatical formalisms or models for the description of natural languages, and there has been no general consensus amongst expert linguists as to the &apos;best&apos; model. If even human experts can&apos;t agree on this issue, Chomsky was probably right in thinking it unreasonable to expect a machine, even an &apos;intelligent&apos; expert system, to be able to choose which theory or model to start from. Constrained discovery procedures However, it may still be possible to devise a discovery procedure if we constrain the computer system to a specific grammatical model. The problem is simplified further if we constrain the input to the discovery procedure, to carefully chosen example sentences (and possibly counter-example non-sentences). This is the approach used, for example, by Berwick (85); his system extracted grammar rules in a formalism based on that of Marcus&apos;s PARSIFAL (Marcus 80) from fairly simple example sentences, and managed to acquire &amp;quot;approximately 70% of the parsing rules originally hand-written for [Marcus&apos;s] parser&amp;quot;. Unfortunately, it is not at all clear that such a system could be generalised to deal with Unrestricted English text, including deviant, idiomatic and even ill-formed sentences found in a Corpus of &apos;real&apos; language data. This is the kind of problem best suited to statistical pattern matching methods. The plausibility of a truly general discovery procedure, capable of working with unrestricted input, increases if we can use a very simple model to describe the language in question. Chomsky believed that English could only be described by a phrase structure grammar augmented with transformations, and clearly a discovery procedure for devising Transformational Generative grammars from a Corpus would have to be extremely complex and &apos;clever&apos;. More recently, (Gazdar et al 85) and others have argued that a less powerful mechanism such as a variant of phrase structure grammar is sufficient to describe English syntax. A discovery procedure for phrase structure grammars would be simpler than one for TG grammars because phrase structure grammars are simpler (more constrained) than TG granunars. CLAWS For the more limited task of assigning pan-of-speech labels to words, (Leech, Garside &amp; Atwell 83b), (Atwell 83) and (Atwell, Leech &amp; Garside 84) showed that an even simpler model, a first-order Markov model, will suffice. This model was used by CLAWS, the Constituent- Likelihood Automatic Word-tagging System, to assign grammatical wordclass (part-of-speech) markers to words in the LOB Corpus. The LOB Corpus is a collection of 500 British English text samples, each of just over 2000 words, totalling over a million words in all; it is available in several (with or without word-tags associated word) from the Norwegian Computing Centre for the Humanities, Bergen University (see (Johansson et al 78), (Johansson et al 86)). The Markovian CLAWS was able to assign the correct tag to c96% of words in the LOB Corpus, leaving only a small residual of problematic constructs to be analysed manually (see (Atwell 81, 82)). Although CLAWS does not yield a full grammatical parse of input sentences, this level of analysis is still useful for some applications; for example, Atwell (83, 86c) showed that the first-order Markov model could be used in detecting grammatical errors in ill-formed input English text. The main components of the first order Markov model or grammar used by CLAWS were: i) a set of 133 grammatical class labels or TAGS, e.g. NN (singular common noun) or JJR (comparative adjective) ii) a l33 133 tag-pair matrix, giving the frequency of cooccurrence of every possible pair of tags (the rowsums or colunmsums giving frequencies of individual tags) iii) a wordlist associating each word with a list of possible tags (with some indication of relative frequency of each tag where a word has more than one), supplemented by a suffuclist, prefuclist, and other default routines to deal with input words not found in the wordlist iv) a set of formulae to use in calculating likelihood-incontext, to disambiguate word-tags in tagging new text. The last item, the formulae underlying the CLAWS system (see (Atwell 83)), constitutes the Markovian mathematical model, and it is too much to ask of any expert system to devise or extract this from data. At least in theory, the first three components could be automatically extracted from sample text WHICH HAS ALREADY BEEN TAGGED, providing there is enough of it (in particular, there should be many examples of each word in the wordlist, to ensure relative tag likelihoods are accurate). However, this is effectively &amp;quot;learning by example&amp;quot;: the tagged texts constitute examples of correct analyses, and the program extracting word-tag and tag-pair frequencies could be said to be &amp;quot;learning&amp;quot; the parameters of a Markov model compatible with the example data. Such a learning system is not a truly generalised discovery procedure. Ideally, we would like to be able to extract the parameters of a compatible Markov model from RAW, untagged text. RUNNEWTAGSET Statistical pattern recognition techniques have been used in many fields of scientific computing for data classification and pattern detection. In a typical application, there will be a large number of data records, each of which will have a fairly complex internal structure; the task is to somehow group together sets of data records with &apos;similar&apos; internal structures, and/or to note types of internal structures which occur frequently in data records. For example, a speech pattern recognition system is &apos;trained&apos; with repeated examples of each word in its vocabulary to recognise the stereotypical structure of the given speech signal, and then when given a &apos;new&apos; sound it must classify it in terms of the &apos;known&apos; patterns. In attempting to devise a grammatical classification system for words in text, a record consists of the word itself, and its grammatical context. A reasonably large sample of text such as the million-word LOB Corpus corresponds to a huge amount of data if the &apos;grammatical context&apos; considered with each word is very large. The simplest model is to assume that only the single word immediately to the left and/or right of each TARGET word is important in the context; and even this oversimplification of context entails vast amounts of processing. If we assume that each word can belong to one and only one word-class, then whenever two words tend to occur in the same set of immediate (lexical) contexts, they will probably belong to the same word-class. This idea was tested using a suite of programs called RUNNEWTAGSET to group words in a c200,000-wonl subsection of the LOB Corpus into word-classes. The system only attempted to classify wordforms which occurred a hundred times or more, the minimum sample size for lexical collocation analysis suggested by Sinclair et al (70). All possible pairings of one wordfonn with another wordfonn (wl,w2) were compared: if the immediate lexical contexts in which wl occurred were significantly similar to the immediate contexts of w2, the two were deemed to belong to the same word-class, and the two context-sets were merged. A threshold was used to test &amp;quot;significant similarity&amp;quot;; initially, only words which occurred very frequently in the same contexts were classified together, but then the threshold was lowered in stages, allowing less and less similar context-sets to be merged at each stage. Unfortunately, the 200,000-word sample turned out to be far too small for conclusive results: even in a sample of this size, only 175 words occur 100 times or more. However, this program run took several weeks, so it was impractical to try a much larger text sample. There were some promising trends; for example, at the initial threshold level, &lt;will should could must may might&gt;, &lt;in for on by at during&gt;, &lt;is was&gt;, &lt;had has&gt;, &lt;it he there&gt;, &lt;they we&gt;, &lt;but if when while&gt;, &lt;make take&gt;, &lt;end use point question&gt;, and &lt;sense number&gt; were grouped into word-classes on the basis of their immediate lexical contexts, and in subsequent reductions of the threshold these classes were enlarged and new classes were added. However, even if the mammoth computing requirements could be met, this approach to automatic generation of a tagset or word-classification system is unlikely to be wholely successful because it tries to assign every word to one and only one word-class, whereas intuitively many words can have more than one possible tag. For example, this technique will tend to form three separate classes for nouns, verbs, and words which can function in both ways. For further details of the RUNNEWTAGSET experiment, see (Atwell 86a, 86b). Baker&apos;s algorithm Baker (75, 79) gives a technique which might in theory solve this problem. Baker showed that if we assume that a language is generated by a Markov process, then it is theoretically possible, given a sufficiently large sample of data, to automatically calculate the parameters of a Markov model compatible with the data. Baker&apos;s method was proposed as a technique for automatic training of the parameters of a model of an acoustic processor, but it could in theory be applied to the syntactic description of text. In Baker&apos;s technique, the principle parameters of the Markov model were two matrices, a(i,j) and b(i,j,k). For the wordtagging application, i and j correspond to tags, while k corresponds to a word; a(i,j) is the probability of tag i being followed by tag j, and b(i,j,k) is the probability of a word with tag i being followed by the word k with tag j. a(i,j) is the direct equivalent of the tag-pair matrix in the CLAWS model above. b(i,j,k) is analogous to the wordlist, except that the information associated with each word is more detailed: instead of just a relative frequency for each tag that can appear with the word, there is a frequency for every possible pair of &lt;previous tag this tag&gt;. Baker&apos;s model is mathematically equivalent to the one used in CLAWS; and it has the advantage that if the tore matrices a(i,j) and b(i,j,k) are not known, then they can be calculated by analysing raw text. We start with initial estimates for each value, and then use an iterative procedure to repeatedly improve on these estimates of a(i,j) and b(i,j,k). Unfortunately, although this grammar discovery procedure might work in theory, the amount of computation in practice turns out to be vast. We must iteratively estimate a likelihood for every &lt;tag-tag&gt; pair for a(i,j), and for every possible &lt;tag-tag-word&gt; triple for b(i,j,k). Work on tagging the LOB Corpus has shown that a tag-set of the order of 133 tags is reasonable for English (if we include separate tags for different inflections, since different inflexions can appear in distinguishable syntactic contexts). Furthermore, the LOB Corpus has roughly 50,000 word-forms in it (counting, for example, &amp;quot;man&amp;quot;, &amp;quot;men&amp;quot;, &amp;quot;mans&amp;quot;, &amp;quot;manned&amp;quot;, &amp;quot;manning&amp;quot;, etc as separate wordforms). Working from the &apos;raw&apos; LOB Corpus, we would have to estimate c18,000 values for a(i,j), and 900,000,000 values for b(i,j,k). As the process of estimating each a(i,j) and b(i,j,k) value is in itself computationally expensive, it is impractical to use Baker&apos;s formulae unmodified to automatically extract word-classes from the LOB Corpus. Grouping by suffix To cut down the number of variables, we tried the simplifying assumption that the last five letters of a word determine which grammatical class(es) it belongs to. In other words, we assumed words ending in the same suffix shared the same wordclass; a not unreasonable assumption, at least for English. CLAWS was able to assign grammatical classes to almost any given word using a wordlist of only c7000 words supplemented by a suffixlist, so the assumption seemed intuitively reasonable for most words. To further reduce the computation, we used tag-pair probabilities from the tagged LOB Corpus to initialise a(i,j): by using &apos;sensible&apos; starting values rather than completely arbitrary ones, convergence should have been much more rapid. Unfortunately, there were still far too many interdependent variables for computation in a reasonable time: we estimated that even with a single LOB text instead of the complete Corpus, the first iteration alone in Baker&apos;s scheme would take c66 hours! algorithm and introduce other constraints into the First Order Markov model. Another intuitively acceptable constraint was to allow each word to belong to only a small number of possible word classes (Baker&apos;s algorithm allowed words to belong to many different classes, up to the total number of classes in the system). This allowed us to try entirely different algorithms suggested by (Wolff 76) and (Wolff 78), based on the assumption that the class(es) a word belongs to are determined by the immediate contexts that word appears in in the example texts. Unfortunately, these still involved prohibitive computing times. Wolff&apos;s second model was the more successful of the two, coming up with putative classes such as &lt;and at for in of to&gt;, &lt;had was&gt;, &lt;a an it one the&gt;, &lt;at by in not on to with&gt; and &lt;but he i it one there&gt;; yet our implementation took 5 hours CPU time to extract these classes from an 11,000 word sample. Heuristic constraints We are beginning to investigate alternative strategies; for instance, Artificial Intelligence techniques such as heuristics to reduce the &apos;search space&apos; would seem appropriate. However, any heuristics must not be tied too closely to our intuitive knowledge of the English language, or else the resultant grammar discovery procedure will effectively have some of the grammar &amp;quot;built in&amp;quot; to it. For example, one might try constraining the number of tags allowed for each specific word (e.g &amp;quot;the&amp;quot;, &amp;quot;of&apos;, &amp;quot;sexy&amp;quot; can have only one tag; &amp;quot;to&amp;quot;, &amp;quot;he?&apos;, &amp;quot;book&amp;quot; have two possible tags; &amp;quot;cold&amp;quot;, &amp;quot;base&amp;quot;, &amp;quot;about&amp;quot; have three tags; &amp;quot;back&amp;quot;, &amp;quot;bid&amp;quot;, &amp;quot;according&amp;quot; have four tags; &amp;quot;bound&amp;quot;, &amp;quot;beat&amp;quot;, &amp;quot;round&amp;quot; have five tags; and so on); but this is clearly against the spirit of a truly automatic discovery procedure in the Chomskyan sense. A more &apos;acceptable&apos; constraint would be a general limit of, say, up to five tags per word. A discovery procedure would start by assuming that the context-set of every word could be partitioned into five subsets, and then it would attempt a Prolog-style &apos;unification&apos; of pairs of similar context-subsets, using belief revision techniques from Artificial Intelligence (see, for example, (Drakos 86)). Applications Overall, we concede that the case for statistical patternmatching for syntactic classification is not proven. However, there have been some promising results, which deserve further investigation, since there would be useful applications for any successful pattern recognition technique for the acquisition of a grammatical classification system from Unrestricted English text. Note that variables in formulae mentioned above such as i and j are not tag names (NN, VB, etc), but just integers denoting positions in a tag-pair matrix. In a Markov model, Alternative constraints An alternative approach was to abandon Baker&apos;s a tag is defined entirely by its cooccurrence likelihoods with other tags, and with words: labels like NN, VB will not be generated by a pattern recognition technique. However, if we assumed initially that there are 133 tags, e.g. if we initialised a(i,j) to a 133*133 matrix, then hopefully there should be some correlation between distributions of tags in the LOB tagset and the automatically generated tagset If there is poor correlation for some tags (e.g. if the automaticallyderived tagset includes some tags whose collocational distributions are unlike those of any of the tags used in the LOB Corpus), then this constitutes empirical, objective evidence that the LOB tagset could be improved upon. In general, any alternative wordclass system could be empirically assessed in an analogous way. The Longman Dictionary of Contemporary English (LDOCE: Procter 78) and the Oxford Advanced Learner&apos;s Dictionary of Current English (OALD; Homby 74) give detailed grammatical codes with each entry, but the two classification systems are quite different; if samples of text tagged according to the LDOCE and OALD tagsets were available, a pattern recognition technique might give us an empirical, objective way to compare and assess the classification systems, and suggest particular areas for improvement in forthcoming revised editions of LDOCE and OALD. This would be particularly useful for Machine Readable versions of such dictionaries, for use in Natural Language Processing systems (see, for example, (Alckennan et al 85), (Alshawi et al 85), (Atwell forthcoming a)); these could be tailored to a given application domain (semi-)automatically. Even though the experiments mentioned achieved only limited success in discovering a complete grammatical classification system, a more restricted (and hence more achievable) aim is to concentrate on specific word classes which are traditionally recognised as difficult to define. For example, the techniques were particularly successful at finding groups of words corresponding to invariant function word classes, such as particles; Atwell (forthcoming c) explores this further. A bottleneck in commercial exploitation of current research ideas in NLP is the problem of tailoring systems to specialised linguistic registers, that is, application-specific variations in lexicon and grammar. This research, we hope, points the way to (semi-)automating the solution for a wide range of applications (such as described, for example, by Atwell (86d)). Particularly appropriate to the approach in paper are applications systems based on statistical models of grammar, such as (Atwell 86c). If discovery be made to work not just for variant registers of English, but for completely different languages as well, then it may be possible to automate (or at least greatly simplify) the transfer of systems such as that described by Atwell (86c) to a wide variety of natural languages. Conclusion Automatic grammar discovery procedures are a tantalising possibility, but the techniques we have tried so far are far from perfect. It is worth continuing the search because of the enormous potential benefits: a discovery procedure would provide a solution to a major bottleneck in commercial exploitation of NLP technology. We are keen to find collaborators and sponsors for further research.</abstract>
<note confidence="0.704016">REFERENCES AkIcerman, Erik, Pieter Masereeuw, and Willem Meijs 1985</note>
<title confidence="0.850839">Designing a computerized lexicon for linguistic purposes</title>
<address confidence="0.570986">Rodopi, Amsterdam Alshawi, Hiyan, Branimir Boguraev, and Ted Briscoe 1985,</address>
<degree confidence="0.463432727272727">amp;quot;Towards a lexicon support environment for real time parsing&amp;quot; in Proceedings of the Second Conference of the European Chapter of the Association for Computational Linguistics, Geneva Eric Steven 1981 Corpus Tagging Project: Pre-edit Handbook. of Computer Studies and Linguistics, University of Lancaster Eric Steven 1982 Corpus Tagging Project: Manual Postedit Handbook (A mini-grammar of LOB Corpus English, examining the types of error commonly made during automatic (computational) analysis of ordinary</degree>
<affiliation confidence="0.906162">of Computer Studies and Linguistics, University of Lancaster</affiliation>
<address confidence="0.855663">Atwell, Eric Steven 1983 &amp;quot;Constituent-Likelihood Grammar&amp;quot;</address>
<note confidence="0.5034415">of the International Computer Archive of English (ICAME NEWS) 34-67, Norwegian</note>
<affiliation confidence="0.89096">Computing Centre for the Humanities, Bergen University</affiliation>
<author confidence="0.889568">Eric Steven a a Natural Language</author>
<note confidence="0.752797538461538">from raw Department of Computer Studies Research Report no.208, University of Leeds Atwell, Eric Steven I986b, &amp;quot;A parsing expert system which from corpus analysis&amp;quot; in Willem Meijs Corpus Linguistics and Beyond: Proceedings of the Seventh International Conference on English Language Research on Corpora, Amsterdam, Netherlands Amsterdam Atwell, Eric Steven 1986c, &amp;quot;How to detect grammatical errors in a text without parsing it&amp;quot; Department of Computer Studies Research Report no.212, University of Leeds; to in of the Association for Computational Linguistics Third European Chapter Conference, Denmark in this book). Atwell, Eric Steven 19$6d &amp;quot;Beyond the micro: advanced software for research and teaching from computer science and artificial intelligence&amp;quot; in Leech, Geoffrey and Candlin, (eds.) in English language teaching and research: selected papers from the British Council Symposium on computers in English language education and Lancaster, England Longman Atwell, Eric Steven (forthcoming a) &amp;quot;A lexical database for English learners and users: the Oxford Advanced Learner&apos;s to appear Proceedings of ICDBHSS87, the 1987 International Conference on DataBases in the Humanities and Social Sciences, Montgomery, Alabama,</note>
<address confidence="0.944075">USA</address>
<abstract confidence="0.759130666666667">Atwell, Eric Steven (forthcoming b) &amp;quot;Transforming a Parsed into a Corpus Parsee&apos;, to appear in of the 1987 ICAME 8th International Conference on English</abstract>
<affiliation confidence="0.875162">Language Research on Computerised Corpora, Helsinki,</affiliation>
<address confidence="0.70702">Finland</address>
<note confidence="0.695536">Atwell, Eric Steven (forthcoming c) &amp;quot;An Expert System for the Automatic Discovery of Particles&amp;quot; to appear in Proceedings of the 1987 International Conference on the Study of Particles, Berlin, East Germany</note>
<author confidence="0.80541">Eric Steven Atwell</author>
<author confidence="0.80541">Geoffrey Leech</author>
<author confidence="0.80541">Roger Garside</author>
<abstract confidence="0.503517333333333">1984, &amp;quot;Analysis of the LOB Corpus: progress and in Jan Aarts and Willem Meijs (ed), Linguistics; Proceedings of the !CAME Conference on the use of computer corpora in English Language Research, Netherlands Baker, J K 1975 &amp;quot;Stochastic modeling for automatic speech in D R Reddy (ed) recognition Academic Press Baker, J K 1979 &amp;quot;Trainable grammars for speech in Klatt, D H and Wolf J J (eds.) communication papers for the 97th meeting of the acoustical of America: R 1985 acquisition of syntactic knowledge MIT Press, Cambridge (MA) and London Noam 1957 Structures The Hague Nicos Frixou 1986 circuit analysis using manipulation and belief revision Department</abstract>
<affiliation confidence="0.997591">Computer Studies, Leeds University</affiliation>
<address confidence="0.466284">Leech, Geoffrey, Roger Garside, and Eric Steven Atwell 1983a, &amp;quot;Recent developments in the use of computer corpora</address>
<note confidence="0.6453255">English language research&amp;quot; Transactions of the Society 23-40. Leech, Geoffrey, Garside, Roger and Atwell, Eric Steven 1983b &amp;quot;The Automatic Grammatical Tagging of the LOB</note>
<title confidence="0.426421">in of the International Computer Archive</title>
<author confidence="0.52297">Modern English</author>
<affiliation confidence="0.98927">Computing Centre for the Humanities, Bergen University</affiliation>
<author confidence="0.455313">Gerald Gazdar</author>
<author confidence="0.455313">Ewan Klein</author>
<author confidence="0.455313">Geoffrey Pullum</author>
<author confidence="0.455313">Ivan Sag</author>
<affiliation confidence="0.716604444444444">Phrase Structure Grammar Oxford A S, with Cowie, A P (eds.) 1974 Advanced Learner&apos;s Dictionary of Current English (third edition) Oxford University Press Johansson, Stig, Geoffrey Leech and Helen Goodluck 1978 Manual of information to accompany the Lancaster- Oslo/Bergen Corpus of British English, for use with digital of English, Oslo University</affiliation>
<address confidence="0.697079">Johansson, Stig, Eric Atwell, Roger Garside, and Geoffrey 1986 Tagged LOB Corpus Computing</address>
<note confidence="0.94190175">61 Centre for the Humanities, University of Bergen, Norway. M P 1980 Theory of Syntactic Recognition for Language Press, Cambridge, MA Paul (editor-in-chief) 1978 Dictionary of English J, Jones, S, and Daley, R 1970 lexical to OSTI on pmject OLP/08; Dept of English,</note>
<affiliation confidence="0.99478">Birmingham University</affiliation>
<address confidence="0.919055">Wolff, J G 1976 &amp;quot;Frequency, Conceptual Structure and</address>
<note confidence="0.907827">Recognition&amp;quot; in Journal of Psychology 67:377-390 Wolff, J G 1978 &amp;quot;The Discovery of Syntagmatic and Classes&amp;quot; in Bulletin 62</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Erik AkIcerman</author>
<author>Pieter Masereeuw</author>
<author>Willem Meijs</author>
</authors>
<title>Designing a computerized lexicon for linguistic purposes Rodopi,</title>
<date>1985</date>
<location>Amsterdam</location>
<marker>AkIcerman, Masereeuw, Meijs, 1985</marker>
<rawString>AkIcerman, Erik, Pieter Masereeuw, and Willem Meijs 1985 Designing a computerized lexicon for linguistic purposes Rodopi, Amsterdam</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Branimir Boguraev</author>
<author>Ted Briscoe</author>
</authors>
<title>Towards a lexicon support environment for real time parsing&amp;quot;</title>
<date>1985</date>
<booktitle>in Proceedings of the Second Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Geneva</location>
<marker>Alshawi, Boguraev, Briscoe, 1985</marker>
<rawString>Alshawi, Hiyan, Branimir Boguraev, and Ted Briscoe 1985, &amp;quot;Towards a lexicon support environment for real time parsing&amp;quot; in Proceedings of the Second Conference of the European Chapter of the Association for Computational Linguistics, Geneva</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atwell</author>
</authors>
<title>Eric Steven</title>
<date>1981</date>
<institution>Departments of Computer Studies and Linguistics, University of Lancaster</institution>
<marker>Atwell, 1981</marker>
<rawString>Atwell, Eric Steven 1981 LOB Corpus Tagging Project: Manual Pre-edit Handbook. Departments of Computer Studies and Linguistics, University of Lancaster</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atwell</author>
</authors>
<title>Eric Steven</title>
<date>1982</date>
<institution>English.) Departments of Computer Studies and Linguistics, University of Lancaster</institution>
<marker>Atwell, 1982</marker>
<rawString>Atwell, Eric Steven 1982 LOB Corpus Tagging Project: Manual Postedit Handbook (A mini-grammar of LOB Corpus English, examining the types of error commonly made during automatic (computational) analysis of ordinary written English.) Departments of Computer Studies and Linguistics, University of Lancaster</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atwell</author>
</authors>
<title>Eric Steven</title>
<date>1983</date>
<journal>in Newsletter of the International Computer Archive of Modern English (ICAME NEWS)</journal>
<volume>7</volume>
<pages>34--67</pages>
<location>Bergen University</location>
<marker>Atwell, 1983</marker>
<rawString>Atwell, Eric Steven 1983 &amp;quot;Constituent-Likelihood Grammar&amp;quot; in Newsletter of the International Computer Archive of Modern English (ICAME NEWS) 7: 34-67, Norwegian Computing Centre for the Humanities, Bergen University</rawString>
</citation>
<citation valid="false">
<authors>
<author>Atwell</author>
</authors>
<title>Eric Steven 1986a Extracting a Natural Language grammar from raw text</title>
<institution>Department of Computer Studies</institution>
<note>Research Report no.208,</note>
<marker>Atwell, </marker>
<rawString>Atwell, Eric Steven 1986a Extracting a Natural Language grammar from raw text Department of Computer Studies Research Report no.208, University of Leeds</rawString>
</citation>
<citation valid="false">
<authors>
<author>Atwell</author>
</authors>
<title>Eric Steven I986b, &amp;quot;A parsing expert system which learns from corpus analysis&amp;quot;</title>
<booktitle>in Willem Meijs (ed) Corpus Linguistics and Beyond: Proceedings of the Seventh International Conference on English Language Research on Computerised Corpora,</booktitle>
<location>Amsterdam, Netherlands Rodopi, Amsterdam</location>
<marker>Atwell, </marker>
<rawString>Atwell, Eric Steven I986b, &amp;quot;A parsing expert system which learns from corpus analysis&amp;quot; in Willem Meijs (ed) Corpus Linguistics and Beyond: Proceedings of the Seventh International Conference on English Language Research on Computerised Corpora, Amsterdam, Netherlands Rodopi, Amsterdam</rawString>
</citation>
<citation valid="false">
<authors>
<author>Atwell</author>
</authors>
<title>Eric Steven 1986c, &amp;quot;How to detect grammatical errors in a text without parsing it&amp;quot;</title>
<booktitle>Proceedings of the Association for Computational Linguistics Third European Chapter Conference,</booktitle>
<institution>Department of Computer Studies</institution>
<location>Copenhagen, Denmark</location>
<note>Research Report no.212,</note>
<marker>Atwell, </marker>
<rawString>Atwell, Eric Steven 1986c, &amp;quot;How to detect grammatical errors in a text without parsing it&amp;quot; Department of Computer Studies Research Report no.212, University of Leeds; to appear in Proceedings of the Association for Computational Linguistics Third European Chapter Conference, Copenhagen, Denmark (elsewhere in this book).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Atwell</author>
</authors>
<title>Eric Steven 19$6d &amp;quot;Beyond the micro: advanced software for research and teaching from computer science and artificial intelligence&amp;quot;</title>
<booktitle>Computers in English language teaching and research: selected papers from the British Council Symposium on computers in English language education and research,</booktitle>
<pages>167--183</pages>
<editor>in Leech, Geoffrey and Candlin, Christopher (eds.)</editor>
<location>Lancaster, England</location>
<marker>Atwell, </marker>
<rawString>Atwell, Eric Steven 19$6d &amp;quot;Beyond the micro: advanced software for research and teaching from computer science and artificial intelligence&amp;quot; in Leech, Geoffrey and Candlin, Christopher (eds.) Computers in English language teaching and research: selected papers from the British Council Symposium on computers in English language education and research, Lancaster, England 167-183, Longman</rawString>
</citation>
<citation valid="false">
<authors>
<author>Atwell</author>
</authors>
<title>Eric Steven (forthcoming a) &amp;quot;A lexical database for English learners and users: the Oxford Advanced Learner&apos;s Dictionary&amp;quot; to appear in</title>
<booktitle>Proceedings of ICDBHSS87, the 1987 International Conference on DataBases in the Humanities and Social Sciences,</booktitle>
<location>Montgomery, Alabama, USA</location>
<marker>Atwell, </marker>
<rawString>Atwell, Eric Steven (forthcoming a) &amp;quot;A lexical database for English learners and users: the Oxford Advanced Learner&apos;s Dictionary&amp;quot; to appear in Proceedings of ICDBHSS87, the 1987 International Conference on DataBases in the Humanities and Social Sciences, Montgomery, Alabama, USA</rawString>
</citation>
<citation valid="false">
<authors>
<author>Atwell</author>
</authors>
<title>Eric Steven (forthcoming b) &amp;quot;Transforming a Parsed Corpus into a Corpus Parsee&apos;, to appear in</title>
<booktitle>Proceedings of the 1987 ICAME 8th International Conference on English Language Research on Computerised Corpora,</booktitle>
<location>Helsinki, Finland</location>
<marker>Atwell, </marker>
<rawString>Atwell, Eric Steven (forthcoming b) &amp;quot;Transforming a Parsed Corpus into a Corpus Parsee&apos;, to appear in Proceedings of the 1987 ICAME 8th International Conference on English Language Research on Computerised Corpora, Helsinki, Finland</rawString>
</citation>
<citation valid="false">
<authors>
<author>Atwell</author>
</authors>
<title>Eric Steven (forthcoming c) &amp;quot;An Expert System for the Automatic Discovery of Particles&amp;quot; to appear in</title>
<booktitle>Proceedings of the 1987 International Conference on the Study of Particles,</booktitle>
<location>Berlin, East Germany</location>
<marker>Atwell, </marker>
<rawString>Atwell, Eric Steven (forthcoming c) &amp;quot;An Expert System for the Automatic Discovery of Particles&amp;quot; to appear in Proceedings of the 1987 International Conference on the Study of Particles, Berlin, East Germany</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Steven Atwell</author>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
</authors>
<title>Analysis of the LOB Corpus: progress and prospects&amp;quot;,</title>
<date>1984</date>
<booktitle>in Jan Aarts and Willem Meijs (ed), Corpus Linguistics; Proceedings of the !CAME Conference on the use of computer corpora in English Language Research,</booktitle>
<location>Nijmegen, Netherlands Rodopi.</location>
<marker>Atwell, Leech, Garside, 1984</marker>
<rawString>Atwell, Eric Steven, Geoffrey Leech and Roger Garside 1984, &amp;quot;Analysis of the LOB Corpus: progress and prospects&amp;quot;, in Jan Aarts and Willem Meijs (ed), Corpus Linguistics; Proceedings of the !CAME Conference on the use of computer corpora in English Language Research, Nijmegen, Netherlands Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Stochastic modeling for automatic speech understanding&amp;quot; in D R Reddy (ed) Speech recognition</title>
<date>1975</date>
<publisher>Academic Press</publisher>
<marker>Baker, 1975</marker>
<rawString>Baker, J K 1975 &amp;quot;Stochastic modeling for automatic speech understanding&amp;quot; in D R Reddy (ed) Speech recognition Academic Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition&amp;quot;</title>
<date>1979</date>
<booktitle>Speech communication papers for the 97th meeting of the acoustical society of America:</booktitle>
<pages>547--550</pages>
<editor>in Klatt, D H and Wolf J J (eds.)</editor>
<marker>Baker, 1979</marker>
<rawString>Baker, J K 1979 &amp;quot;Trainable grammars for speech recognition&amp;quot; in Klatt, D H and Wolf J J (eds.) Speech communication papers for the 97th meeting of the acoustical society of America: 547-550</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Berwick</author>
</authors>
<title>The acquisition of syntactic knowledge</title>
<date>1985</date>
<publisher>MIT Press,</publisher>
<location>Cambridge (MA) and London</location>
<marker>Berwick, 1985</marker>
<rawString>Berwick, R 1985 The acquisition of syntactic knowledge MIT Press, Cambridge (MA) and London</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<date>1957</date>
<journal>Syntactic Structures Mouton, The Hague</journal>
<contexts>
<context position="5431" citStr="Chomsky (1957)" startWordPosition="803" endWordPosition="804">rtise in theoretical linguistics to devise the rules for the grammar and the grammatical categories used in these rules. To completely automate the process of devising a grammar for English (or some other language), the computer system would have to &amp;quot;know&amp;quot; about theories of grammar, how to choose an appropriate model (e.g. context-free rules, Generalized Phrase Structure Grammar, transition network, or Markov process), and how to go about devising a set of rules in the chosen formalism which actually produces the set of sentences in the Corpus (and doesn&apos;t produce (too many) other sentences). Chomsky (1957), in discussing the goals of linguistic theory, considered the possibility of a discovery procedure for grammars, that is, a mechanical method for constructing a grammar, given a corpus of utterances. His conclusion was: &amp;quot;I think it is very questionable that this goal is attainable in any interesting way&amp;quot;. Since then, linguists have proposed various different grammatical formalisms or models for the description of natural languages, and there has been no general consensus amongst expert linguists as to the &apos;best&apos; model. If even human experts can&apos;t agree on this issue, Chomsky was probably righ</context>
</contexts>
<marker>Chomsky, 1957</marker>
<rawString>Chomsky, Noam 1957 Syntactic Structures Mouton, The Hague</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drakos</author>
</authors>
<title>Nicos Frixou</title>
<date>1986</date>
<institution>Department of Computer Studies, Leeds University</institution>
<marker>Drakos, 1986</marker>
<rawString>Drakos, Nicos Frixou 1986 Electrical circuit analysis using algebraic manipulation and belief revision Department of Computer Studies, Leeds University</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
</authors>
<title>and Eric Steven Atwell 1983a, &amp;quot;Recent developments in the use of computer corpora in English language research&amp;quot;</title>
<date>1983</date>
<journal>in Transactions of the Philological Society</journal>
<pages>23--40</pages>
<marker>Leech, Garside, 1983</marker>
<rawString>Leech, Geoffrey, Roger Garside, and Eric Steven Atwell 1983a, &amp;quot;Recent developments in the use of computer corpora in English language research&amp;quot; in Transactions of the Philological Society 1983: 23-40.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
<author>Atwell</author>
</authors>
<title>Eric Steven 1983b &amp;quot;The Automatic Grammatical Tagging of the LOB Corpus&amp;quot;</title>
<journal>in Newsletter of the International Computer Archive of Modern English (ICAME NEWS)</journal>
<volume>7</volume>
<pages>13--33</pages>
<location>Bergen University</location>
<marker>Leech, Garside, Atwell, </marker>
<rawString>Leech, Geoffrey, Garside, Roger and Atwell, Eric Steven 1983b &amp;quot;The Automatic Grammatical Tagging of the LOB Corpus&amp;quot; in Newsletter of the International Computer Archive of Modern English (ICAME NEWS) 7: 13-33, Norwegian Computing Centre for the Humanities, Bergen University</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey Pullum</author>
<author>Ivan Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar Blackwell,</title>
<date>1985</date>
<location>Oxford</location>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, Gerald, Ewan Klein, Geoffrey Pullum, and Ivan Sag 1985 Generalized Phrase Structure Grammar Blackwell, Oxford</rawString>
</citation>
<citation valid="false">
<booktitle>1974 Oxford Advanced Learner&apos;s Dictionary of Current English (third edition)</booktitle>
<editor>Hornby, A S, with Cowie, A P (eds.)</editor>
<publisher>Oxford University Press</publisher>
<marker></marker>
<rawString>Hornby, A S, with Cowie, A P (eds.) 1974 Oxford Advanced Learner&apos;s Dictionary of Current English (third edition) Oxford University Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stig Johansson</author>
</authors>
<title>Geoffrey Leech and Helen Goodluck</title>
<date>1978</date>
<institution>Department of English, Oslo University</institution>
<marker>Johansson, 1978</marker>
<rawString>Johansson, Stig, Geoffrey Leech and Helen Goodluck 1978 Manual of information to accompany the LancasterOslo/Bergen Corpus of British English, for use with digital computers Department of English, Oslo University</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stig Johansson</author>
<author>Eric Atwell</author>
<author>Roger Garside</author>
<author>Geoffrey Leech</author>
</authors>
<title>The Tagged LOB Corpus Norwegian Computing Centre for the Humanities,</title>
<date>1986</date>
<institution>University of Bergen,</institution>
<marker>Johansson, Atwell, Garside, Leech, 1986</marker>
<rawString>Johansson, Stig, Eric Atwell, Roger Garside, and Geoffrey Leech 1986 The Tagged LOB Corpus Norwegian Computing Centre for the Humanities, University of Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA</location>
<marker>Marcus, 1980</marker>
<rawString>Marcus, M P 1980 A Theory of Syntactic Recognition for Natural Language MIT Press, Cambridge, MA</rawString>
</citation>
<citation valid="true">
<date>1978</date>
<journal>Longman Dictionary of Contemporary English Longman</journal>
<editor>Procter, Paul (editor-in-chief)</editor>
<marker>1978</marker>
<rawString>Procter, Paul (editor-in-chief) 1978 Longman Dictionary of Contemporary English Longman</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sinclair</author>
<author>S Jones</author>
<author>R Daley</author>
</authors>
<title>English lexical studies, Report to OSTI on pmject OLP/08; Dept of English,</title>
<date>1970</date>
<location>Birmingham University</location>
<marker>Sinclair, Jones, Daley, 1970</marker>
<rawString>Sinclair, J, Jones, S, and Daley, R 1970 English lexical studies, Report to OSTI on pmject OLP/08; Dept of English, Birmingham University</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Wolff</author>
</authors>
<title>Frequency, Conceptual Structure and Pattern Recognition&amp;quot;</title>
<date>1976</date>
<journal>in British Journal of Psychology</journal>
<pages>67--377</pages>
<marker>Wolff, 1976</marker>
<rawString>Wolff, J G 1976 &amp;quot;Frequency, Conceptual Structure and Pattern Recognition&amp;quot; in British Journal of Psychology 67:377-390</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Wolff</author>
</authors>
<title>The Discovery of Syntagmatic and Paradigmatic Classes&amp;quot;</title>
<date>1978</date>
<journal>in ALLC Bulletin</journal>
<volume>6</volume>
<issue>1</issue>
<marker>Wolff, 1978</marker>
<rawString>Wolff, J G 1978 &amp;quot;The Discovery of Syntagmatic and Paradigmatic Classes&amp;quot; in ALLC Bulletin 6(1):141</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>