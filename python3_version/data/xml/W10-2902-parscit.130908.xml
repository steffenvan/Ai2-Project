<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.886123">
Viterbi Training Improves Unsupervised Dependency Parsing
</title>
<author confidence="0.990234">
Valentin I. Spitkovsky Hiyan Alshawi
</author>
<affiliation confidence="0.993207">
Computer Science Department Google Inc.
Stanford University and Google Inc. Mountain View, CA, 94043, USA
</affiliation>
<email confidence="0.988252">
valentin@cs.stanford.edu hiyan@google.com
</email>
<author confidence="0.996349">
Daniel Jurafsky and Christopher D. Manning
</author>
<affiliation confidence="0.9068695">
Departments of Linguistics and Computer Science
Stanford University, Stanford, CA, 94305, USA
</affiliation>
<email confidence="0.995863">
jurafsky@stanford.edu and manning@cs.stanford.edu
</email>
<sectionHeader confidence="0.994602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999254">
We show that Viterbi (or “hard”) EM is
well-suited to unsupervised grammar in-
duction. It is more accurate than standard
inside-outside re-estimation (classic EM),
significantly faster, and simpler. Our ex-
periments with Klein and Manning’s De-
pendency Model with Valence (DMV) at-
tain state-of-the-art performance — 44.8%
accuracy on Section 23 (all sentences) of
the Wall Street Journal corpus — without
clever initialization; with a good initial-
izer, Viterbi training improves to 47.9%.
This generalizes to the Brown corpus,
our held-out set, where accuracy reaches
50.8% — a 7.5% gain over previous best
results. We find that classic EM learns bet-
ter from short sentences but cannot cope
with longer ones, where Viterbi thrives.
However, we explain that both algorithms
optimize the wrong objectives and prove
that there are fundamental disconnects be-
tween the likelihoods of sentences, best
parses, and true parses, beyond the well-
established discrepancies between likeli-
hood, accuracy and extrinsic performance.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997570212766">
Unsupervised learning is hard, often involving dif-
ficult objective functions. A typical approach is
to attempt maximizing the likelihood of unlabeled
data, in accordance with a probabilistic model.
Sadly, such functions are riddled with local op-
tima (Charniak, 1993, Ch. 7, inter alia), since their
number of peaks grows exponentially with in-
stances of hidden variables. Furthermore, a higher
likelihood does not always translate into superior
task-specific accuracy (Elworthy, 1994; Merialdo,
1994). Both complications are real, but we will
discuss perhaps more significant shortcomings.
We prove that learning can be error-prone even
in cases when likelihood is an appropriate mea-
sure of extrinsic performance and where global
optimization is feasible. This is because a key
challenge in unsupervised learning is that the de-
sired likelihood is unknown. Its absence renders
tasks like structure discovery inherently under-
constrained. Search-based algorithms adopt sur-
rogate metrics, gambling on convergence to the
“right” regularities in data. Their wrong objec-
tives create cases in which both efficiency and per-
formance improve when expensive exact learning
techniques are replaced by cheap approximations.
We propose using Viterbi training (Brown
et al., 1993), instead of inside-outside re-
estimation (Baker, 1979), to induce hierarchical
syntactic structure from natural language text. Our
experiments with Klein and Manning’s (2004) De-
pendency Model with Valence (DMV), a popular
state-of-the-art model (Headden et al., 2009; Co-
hen and Smith, 2009; Spitkovsky et al., 2009),
beat previous benchmark accuracies by 3.8% (on
Section 23 of WSJ) and 7.5% (on parsed Brown).
Since objective functions used in unsupervised
grammar induction are provably wrong, advan-
tages of exact inference may not apply. It makes
sense to try the Viterbi approximation — it is also
wrong, only simpler and cheaper than classic EM.
As it turns out, Viterbi EM is not only faster but
also more accurate, consistent with hypotheses of
de Marcken (1995) and Spitkovsky et al. (2009).
We begin by reviewing the model, standard data
sets and metrics, and our experimental results. Af-
ter relating our contributions to prior work, we
delve into proofs by construction, using the DMV.
</bodyText>
<page confidence="0.970847">
9
</page>
<note confidence="0.969462">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 9–17,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.987507565217391">
WSJ� 5 10 15 20 25 30 35 40 45
45
40
35
30
25
20
15
10
5
Thousands
of Sentences
Thousands
of Tokens
900
800
600
500
400
300
200
700
100
</figure>
<table confidence="0.970136615384615">
Corpus Sentences POS Tokens Corpus Sentences POS Tokens
WSJ1 159 159 WSJ13 12,270 110,760
WSJ2 499 839 WSJ14 14,095 136,310
WSJ3 876 1,970 WSJ15 15,922 163,715
WSJ4 1,394 4,042 WSJ20 25,523 336,555
WSJ5 2,008 7,112 WSJ25 34,431 540,895
WSJ6 2,745 11,534 WSJ30 41,227 730,099
WSJ7 3,623 17,680 WSJ35 45,191 860,053
WSJ8 4,730 26,536 WSJ40 47,385 942,801
WSJ9 5,938 37,408 WSJ45 48,418 986,830
WSJ10 7,422 52,248 WSJ100 49,206 1,028,054
WSJ11 8,856 68,022 Section 23 2,353 48,201
WSJ12 10,500 87,750 Brown100 24,208 391,796
</table>
<figureCaption confidence="0.931925">
Figure 1: Sizes of WSJ11, ... , 45,100}, Section 23 of WSJ&apos; and Brown100 (Spitkovsky et al., 2009).
</figureCaption>
<equation confidence="0.892202692307692">
NNS VBD IN NN Q
Payrolls fell in September .
0
z } |{
P = (1 − PSTOP(0, L, T)) X PATTACH(0, L, VBD)
X (1 − PSTOP(VBD, L, T)) X PATTACH(VBD, L, NNS)
X (1 − PSTOP(VBD, R, T)) X PATTACH(VBD, R, IN)
X (1 − PSTOP(IN, R, T)) X PATTACH(IN, R, NN)
X PSTOP(VBD, L, F) X PSTOP(VBD, R, F)
X PSTOP(NNS, L, T) X PSTOP(NNS, R, T)
X PSTOP(IN, L, T) X PSTOP(IN, R, F)
X PSTOP(NN, L, T) X PSTOP(NN, R, T)
X PSTOP(0, L, F) X PSTOP(0, R, T) �
</equation>
<bodyText confidence="0.999872444444444">
reference parse trees is straight-forward, since
maximum-likelihood estimation reduces to count-
ing: lATTACH(ch, dir, ca) is the fraction of children —
those of class ca — attached on the dir side of a
head of class ch; �PSTOP(ch, dir, adj = T), the frac-
tion of words of class ch with no children on the
dir side; and INTOP(ch, dir, adj = F), the ratio1 of the
number of words of class ch having a child on the
dir side to their total number of such children.
</bodyText>
<figure confidence="0.687678666666667">
3 Standard Data Sets and Evaluation
 |{z }  |{z }
1 1
</figure>
<figureCaption confidence="0.994871">
Figure 2: A dependency structure for a short sen-
</figureCaption>
<bodyText confidence="0.524536">
tence and its probability, as factored by the DMV,
after summing out PORDER (Spitkovsky et al., 2009).
</bodyText>
<sectionHeader confidence="0.984705" genericHeader="introduction">
2 Dependency Model with Valence
</sectionHeader>
<bodyText confidence="0.99839480952381">
The DMV (Klein and Manning, 2004) is a single-
state head automata model (Alshawi, 1996) over
lexical word classes {cw} — POS tags. Its gener-
ative story for a sub-tree rooted at a head (of class
ch) rests on three types of independent decisions:
(i) initial direction dir E {L, R} in which to attach
children, via probability PORDER(ch); (ii) whether to
seal dir, stopping with probability PSTOP(ch, dir, adj),
conditioned on adj E {T, F} (true iff considering
dir’s first, i.e., adjacent, child); and (iii) attach-
ments (of class ca), according to PATTACH(ch, dir, ca).
This produces only projective trees. A root token
Q generates the head of a sentence as its left (and
only) child. Figure 2 displays a simple example.
The DMV lends itself to unsupervised learn-
ing via inside-outside re-estimation (Baker, 1979).
Viterbi training (Brown et al., 1993) re-estimates
each next model as if supervised by the previous
best parse trees. And supervised learning from
The DMV is traditionally trained and tested on
customized subsets of Penn English Treebank’s
Wall Street Journal portion (Marcus et al., 1993).
Following Klein and Manning (2004), we be-
gin with reference constituent parses and com-
pare against deterministically derived dependen-
cies: after pruning out all empty sub-trees, punc-
tuation and terminals (tagged # and $) not pro-
nounced where they appear, we drop all sentences
with more than a prescribed number of tokens
remaining and use automatic “head-percolation”
rules (Collins, 1999) to convert the rest, as is stan-
dard practice. We experiment with WSJk (sen-
tences with at most k tokens), for 1 G k G 45, and
Section 23 of WSJ&apos; (all sentence lengths). We
also evaluate on Brown100, similarly derived from
the parsed portion of the Brown corpus (Francis
and Kucera, 1979), as our held-out set. Figure 1
shows these corpora’s sentence and token counts.
Proposed parse trees are judged on accuracy: a
directed score is simply the overall fraction of cor-
rectly guessed dependencies. Let S be a set of
sentences, with |s |the number of terminals (to-
</bodyText>
<footnote confidence="0.666264">
1The expected number of trials needed to get one
Bernoulli(p) success is n - Geometric(p), with n E Z+,
</footnote>
<equation confidence="0.9874105">
P(n) = (1 − p)&amp;quot;−&apos;p and E(n) = p−�; MoM and MLE
agree, p� = (# of successes)/(# of trials).
</equation>
<page confidence="0.988388">
10
</page>
<figure confidence="0.998497958333333">
70
60
50
40
30
20
10
5 10 15 20 25 30 35 40 WSJk
(training on all WSJ sentences up to k tokens in length)
(a) %-Accuracy for Inside-Outside (Soft EM)
Uninformed
Ad-Hoc∗
Oracle
5 10 15 20 25 30 35 40 WSJk
400
Oracle
Ad-Hoc∗
Uninformed
(c) Iterations for Inside-Outside (Soft EM)
5 10 15 20 25 30 35 40 WSJk
70
60
50
40
30
20
10
200
150
100
50
Directed Dependency Accuracy on WSJ40
350
Iterations to Convergence
5 10 15 20 25 30 35 40 WSJk
(d) Iterations for Viterbi (Hard EM)
Ad-Hoc∗
Uninformed
Oracle
Iterations to Convergence
200
150
100
50
Directed Dependency Accuracy on WSJ40
(b) %-Accuracy for Viterbi (Hard EM)
Oracle
Ad-Hoc∗ Uninformed
</figure>
<figureCaption confidence="0.999589">
Figure 3: Directed dependency accuracies attained by the DMV, when trained on WSJk, smoothed, then
</figureCaption>
<bodyText confidence="0.9466025">
tested against a fixed evaluation set, WSJ40, for three different initialization strategies (Spitkovsky et al.,
2009). Red, green and blue graphs represent the supervised (maximum-likelihood oracle) initialization,
a linguistically-biased initializer (Ad-Hoc*) and the uninformed (uniform) prior. Panel (b) shows results
obtained with Viterbi training instead of classic EM — Panel (a), but is otherwise identical (in both, each
of the 45 vertical slices captures five new experimental results and arrows connect starting performance
with final accuracy, emphasizing the impact of learning). Panels (c) and (d) show the corresponding
numbers of iterations until EM’s convergence.
kens) for each s E S. Denote by T (s) the set
of all dependency parse trees of s, and let ti(s)
stand for the parent of token i, 1 G i G |s|, in
t(s) E T (s). Call the gold reference t*(s) E T (s).
For a given model of grammar, parameterized by
B, let ˆtθ(s) E T(s) be a (not necessarily unique)
likeliest (also known as Viterbi) parse of s:
</bodyText>
<equation confidence="0.981052833333333">
�ˆtθ(s) E arg max Pθ(t) ;
tET (s)
then B’s directed accuracy on a reference set R is
s
EsER Ei=1 1{£e(s)=tz (s)}
100% �
</equation>
<sectionHeader confidence="0.97569" genericHeader="method">
4 Experimental Setup and Results
</sectionHeader>
<bodyText confidence="0.955968357142857">
Following Spitkovsky et al. (2009), we trained the
DMV on data sets WSJ{1, ... , 451 using three ini-
tialization strategies: (i) the uninformed uniform
prior; (ii) a linguistically-biased initializer, Ad-
Hoc*;2 and (iii) an oracle — the supervised MLE
solution. Standard training is without smoothing,
iterating each run until successive changes in over-
all per-token cross-entropy drop below 2−20 bits.
We re-trained all models using Viterbi EM
instead of inside-outside re-estimation, explored
Laplace (add-one) smoothing during training, and
experimented with hybrid initialization strategies.
2Ad-Hoc∗ is Spitkovsky et al.’s (2009) variation on Klein
and Manning’s (2004) “ad-hoc harmonic” completion.
</bodyText>
<equation confidence="0.526178">
E.
sER |s|
</equation>
<page confidence="0.907434">
11
</page>
<figure confidence="0.999756346153846">
5 10 15 20 25 30 35 40 WSJ�
5 10 15 20 25 30 35 40 WSJ�
(a) %-Accuracy for Inside-Outside (Soft EM)
Uninformed
Ad-Hoc*
Baby Steps
Oracle
(b) %-Accuracy for Viterbi (Hard EM)
Oracle
Ad-Hoc* Uninformed
Baby Steps
70
60
50
40
30
20
10
Directed Dependency Accuracy on WSJ40 70
60
50
40
30
20
10
Directed Dependency Accuracy on WSJ40
</figure>
<figureCaption confidence="0.9925255">
Figure 4: Superimposes directed accuracies attained by DMV models trained with Laplace smoothing
(brightly-colored curves) over Figure 3(a,b); violet curves represent Baby Steps (Spitkovsky et al., 2009).
</figureCaption>
<subsectionHeader confidence="0.984136">
4.1 Result #1: Viterbi-Trained Models
</subsectionHeader>
<bodyText confidence="0.999983607142857">
The results of Spitkovsky et al. (2009), tested
against WSJ40, are re-printed in Figure 3(a); our
corresponding Viterbi runs appear in Figure 3(b).
We observe crucial differences between the two
training modes for each of the three initialization
strategies. Both algorithms walk away from the
supervised maximum-likelihood solution; how-
ever, Viterbi EM loses at most a few points of
accuracy (3.7% at WSJ40), whereas classic EM
drops nearly twenty points (19.1% at WSJ45). In
both cases, the single best unsupervised result is
with good initialization, although Viterbi peaks
earlier (45.9% at WSJ8) and in a narrower range
(WSJ8-9) than classic EM (44.3% at WSJ15;
WSJ13-20). The uniform prior never quite gets off
the ground with classic EM but manages quite well
under Viterbi training,3 given sufficient data — it
even beats the “clever” initializer everywhere past
WSJ10. The “sweet spot” at WSJ15 — a neigh-
borhood where both Ad-Hoc* and the oracle ex-
cel under classic EM — disappears with Viterbi.
Furthermore, Viterbi does not degrade with more
(complex) data, except with a biased initializer.
More than a simple efficiency hack, Viterbi EM
actually improves performance. And its benefits to
running times are also non-trivial: it not only skips
computing the outside charts in every iteration but
also converges (sometimes an order of magnitude)
</bodyText>
<footnote confidence="0.849015">
3In a concurrently published related work, Cohen and
Smith (2010) prove that the uniform-at-random initializer is a
competitive starting M-step for Viterbi EM; our uninformed
prior consists of uniform multinomials, seeding the E-step.
</footnote>
<bodyText confidence="0.605698">
faster than classic EM (see Figure 3(c,d)).4
</bodyText>
<subsectionHeader confidence="0.657935">
4.2 Result #2: Smoothed Models
</subsectionHeader>
<bodyText confidence="0.999957928571429">
Smoothing rarely helps classic EM and hurts in
the case of oracle training (see Figure 4(a)). With
Viterbi, supervised initialization suffers much less,
the biased initializer is a wash, and the uninformed
uniform prior generally gains a few points of ac-
curacy, e.g., up 2.9% (from 42.4% to 45.2%, eval-
uated against WSJ40) at WSJ15 (see Figure 4(b)).
Baby Steps (Spitkovsky et al., 2009) — iterative
re-training with increasingly more complex data
sets, WSJ1, ... ,WSJ45 — using smoothed Viterbi
training fails miserably (see Figure 4(b)), due to
Viterbi’s poor initial performance at short sen-
tences (possibly because of data sparsity and sen-
sitivity to non-sentences — see examples in §7.3).
</bodyText>
<subsectionHeader confidence="0.994291">
4.3 Result #3: State-of-the-Art Models
</subsectionHeader>
<bodyText confidence="0.999954583333333">
Simply training up smoothed Viterbi at WSJ15,
using the uninformed uniform prior, yields 44.8%
accuracy on Section 23 of WSJ&apos;, already beating
previous state-of-the-art by 0.7% (see Table 1(A)).
Since both classic EM and Ad-Hoc* initializers
work well with short sentences (see Figure 3(a)),
it makes sense to use their pre-trained models to
initialize Viterbi training, mixing the two strate-
gies. We judged all Ad-Hoc* initializers against
WSJ15 and found that the one for WSJ8 mini-
mizes sentence-level cross-entropy (see Figure 5).
This approach does not involve reference parse
</bodyText>
<footnote confidence="0.998691">
4For classic EM, the number of iterations to convergence
appears sometimes inversely related to performance, giving
credence to the notion of early termination as a regularizer.
</footnote>
<page confidence="0.979547">
12
</page>
<table confidence="0.999930625">
Model Incarnation WSJ10 WSJ20 WSJ°
DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100
Less is More (Ad-Hoc* @15) (Spitkovsky et al., 2009) 56.2 48.2 44.1 43.3
A. Smoothed Viterbi Training (@15), Initialized with the Uniform Prior 59.9 50.0 44.8 48.1
B. A Good Initializer (Ad-Hoc*’s @8), Classically Pre-Trained (@15) 63.8 52.3 46.2 49.3
C. Smoothed Viterbi Training (@15), Initialized with B 64.4 53.5 47.8 50.5
D. Smoothed Viterbi Training (@45), Initialized with C 65.3 53.8 47.9 50.8
EVG Smoothed (skip-head), Lexicalized (Headden et al., 2009) 68.8
</table>
<tableCaption confidence="0.99482">
Table 1: Accuracies on Section 23 of WSJ{10, 20,&apos; } and Brown100 for three recent state-of-the-art
systems, our initializer, and smoothed Viterbi-trained runs that employ different initialization strategies.
</tableCaption>
<figure confidence="0.6773435">
4.5
WSJ� 5 10 15 20 25 30 35 40 45
</figure>
<figureCaption confidence="0.9965885">
Figure 5: Sentence-level cross-entropy on WSJ15
for Ad-Hoc* initializers of WSJ{1, ... , 45}.
</figureCaption>
<bodyText confidence="0.990265142857143">
trees and is therefore still unsupervised. Using the
Ad-Hoc* initializer based on WSJ8 to seed classic
training at WSJ15 yields a further 1.4% gain in ac-
curacy, scoring 46.2% on WSJ&apos; (see Table 1(B)).
This good initializer boosts accuracy attained
by smoothed Viterbi at WSJ15 to 47.8% (see Ta-
ble 1(C)). Using its solution to re-initialize train-
ing at WSJ45 gives a tiny further improvement
(0.1%) on Section 23 of WSJ&apos; but bigger gains
on WSJ10 (0.9%) and WSJ20 (see Table 1(D)).
Our results generalize. Gains due to smoothed
Viterbi training and favorable initialization carry
over to Brown100 — accuracy improves by 7.5%
over previous published numbers (see Table 1).5
</bodyText>
<sectionHeader confidence="0.990252" genericHeader="method">
5 Discussion of Experimental Results
</sectionHeader>
<bodyText confidence="0.999710461538462">
The DMV has no parameters to capture syntactic
relationships beyond local trees, e.g., agreement.
Spitkovsky et al. (2009) suggest that classic EM
breaks down as sentences get longer precisely be-
cause the model makes unwarranted independence
assumptions. They hypothesize that the DMV re-
serves too much probability mass for what should
be unlikely productions. Since EM faithfully al-
locates such re-distributions across the possible
parse trees, once sentences grow sufficiently long,
this process begins to deplete what began as like-
lier structures. But medium lengths avoid a flood
of exponentially-confusing longer sentences (and
</bodyText>
<footnote confidence="0.97329025">
5In a sister paper, Spitkovsky et al. (2010) improve perfor-
mance by incorporating parsing constraints harvested from
the web into Viterbi training; nevertheless, results presented
in this paper remain the best of models trained purely on WSJ.
</footnote>
<bodyText confidence="0.998074214285714">
the sparseness of unrepresentative shorter ones).6
Our experiments corroborate this hypothesis.
First of all, Viterbi manages to hang on to su-
pervised solutions much better than classic EM.
Second, Viterbi does not universally degrade with
more (complex) training sets, except with a biased
initializer. And third, Viterbi learns poorly from
small data sets of short sentences (WSJk, k &lt; 5).
Viterbi may be better suited to unsupervised
grammar induction compared with classic EM, but
neither is sufficient, by itself. Both algorithms
abandon good solutions and make no guarantees
with respect to extrinsic performance. Unfortu-
nately, these two approaches share a deep flaw.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="method">
6 Related Work on Improper Objectives
</sectionHeader>
<bodyText confidence="0.999943272727273">
It is well-known that maximizing likelihood may,
in fact, degrade accuracy (Pereira and Schabes,
1992; Elworthy, 1994; Merialdo, 1994). de Mar-
cken (1995) showed that classic EM suffers from
a fatal attraction towards deterministic grammars
and suggested a Viterbi training scheme as a rem-
edy. Liang and Klein’s (2008) analysis of errors
in unsupervised learning began with the inappro-
priateness of the likelihood objective (approxima-
tion), explored problems of data sparsity (estima-
tion) and focused on EM-specific issues related to
non-convexity (identifiability and optimization).
Previous literature primarily relied on experi-
mental evidence. de Marcken’s analytical result is
an exception but pertains only to EM-specific lo-
cal attractors. Our analysis confirms his intuitions
and moreover shows that there can be global pref-
erences for deterministic grammars — problems
that would persist with tractable optimization. We
prove that there is a fundamental disconnect be-
tween objective functions even when likelihood is
a reasonable metric and training data are infinite.
</bodyText>
<footnote confidence="0.823313">
6Klein and Manning (2004) originally trained the DMV
on WSJ10 and Gillenwater et al. (2009) found it useful to dis-
card data from WSJ3, which is mostly incomplete sentences.
</footnote>
<figure confidence="0.9520398">
x-Entropy h (in bits per token) on WSJ15
lowest cross-entropy (4.32bpt) attained at WSJ8
bpt
5.5
5.0
</figure>
<page confidence="0.988463">
13
</page>
<sectionHeader confidence="0.91506" genericHeader="method">
7 Proofs (by Construction)
</sectionHeader>
<bodyText confidence="0.999990444444444">
There is a subtle distinction between three differ-
ent probability distributions that arise in parsing,
each of which can be legitimately termed “likeli-
hood” — the mass that a particular model assigns
to (i) highest-scoring (Viterbi) parse trees; (ii) the
correct (gold) reference trees; and (iii) the sen-
tence strings (sums over all derivations). A classic
unsupervised parser trains to optimize the third,
makes actual parsing decisions according to the
first, and is evaluated against the second. There
are several potential disconnects here. First of all,
the true generative model θ* may not yield the
largest margin separations for discriminating be-
tween gold parse trees and next best alternatives;
and second, θ* may assign sub-optimal mass to
string probabilities. There is no reason why an op-
timal estimate θˆ should make the best parser or
coincide with a peak of an unsupervised objective.
</bodyText>
<subsectionHeader confidence="0.993739">
7.1 The Three Likelihood Objectives
</subsectionHeader>
<bodyText confidence="0.99990375">
A supervised parser finds the “best” parameters
θˆ by maximizing the likelihood of all reference
structures t*(s) — the product, over all sentences,
of the probabilities that it assigns to each such tree:
</bodyText>
<equation confidence="0.982044">
ˆθSUP = arg max L(θ) = arg max
θ θ
</equation>
<bodyText confidence="0.999948333333333">
For the DMV, this objective function is convex —
its unique peak is easy to find and should match
the true distribution θ* given enough data, barring
practical problems caused by numerical instability
and inappropriate independence assumptions. It is
often easier to work in log-probability space:
</bodyText>
<equation confidence="0.867555125">
ˆθSUP = arg maxθ log L(θ)
K
= arg maxθ s log Pθ(t*(s)).
Cross-entropy, measured in bits per token (bpt),
offers an interpretable proxy for a model’s quality:
__ E3 lg Pθ (t*(s))
h(θ) Ks |s |.
Clearly, arg maxθ L(θ) = ˆθSUP = arg minθ h(θ).
</equation>
<bodyText confidence="0.9998895">
Unsupervised parsers cannot rely on references
and attempt to jointly maximize the probability of
each sentence instead, summing over the probabil-
ities of all possible trees, according to a model θ:
This objective function is not convex and in gen-
eral does not have a unique peak, so in practice one
usually settles for ˜θUNS — a fixed point. There is no
reason why ˆθSUP should agree with ˆθUNS, which is
in turn (often badly) approximated by ˜θUNS, in our
case using EM. A logical alternative to maximiz-
ing the probability of sentences is to maximize the
probability of the most likely parse trees instead:7
</bodyText>
<equation confidence="0.991297333333333">
ˆθVIT = arg max
θ �
s log Pθ(ˆtθ(s)).
</equation>
<bodyText confidence="0.9994896">
This 1-best approximation similarly arrives at ˜θVIT,
with no claims of optimality. Each next model is
re-estimated as if supervised by reference parses.
7.2 A Warm-Up Case: Accuracy vs. ˆθSUP =� θ*
A simple way to derail accuracy is to maximize
the likelihood of an incorrect model, e.g., one that
makes false independence assumptions. Consider
fitting the DMV to a contrived distribution — two
equiprobable structures over identical three-token
sentences from a unary vocabulary { a�}:
</bodyText>
<equation confidence="0.890011666666667">
(i) x x
�a �a a�; (ii) y y
�a �a a� .
</equation>
<bodyText confidence="0.9348634">
There are six tokens and only two have children
on any given side, so adjacent stopping MLEs are:
ˆPSTOP( a�, L, T) = STOP (®, R, T) = 1 − 6 3.
2= 2
The rest of the estimated model is deterministic:
ˆPATTACH(�, L, a�) = ˆPATTACH( a�, *, a�) = 1
and ˆPSTOP( a�, *, F) = 1,
since all dependents are ® and every one is an
only child. But the DMV generates left- and right-
attachments independently, allowing a third parse:
(iii) x y
�a �a a�.
It also cannot capture the fact that all structures are
local (or that all dependency arcs point in the same
direction), admitting two additional parse trees:
</bodyText>
<equation confidence="0.8624835">
(iv) ® �a a�; (v) y
x �a �a a�.
</equation>
<bodyText confidence="0.949327">
Each possible structure must make four (out of six)
adjacent stops, incurring identical probabilities:
</bodyText>
<equation confidence="0.926726923076923">
ri Pθ(t*(s)).
s
�
ˆθUNS = arg max
θ
s
�
log
tET(s)
ˆPSTOP( a�, *, T)4 X (1 − ˆPSTOP( a�, *, T))2 = 24
36.
.
Pθ(t)
</equation>
<footnote confidence="0.693682">
� v �
Pe(s)
7It is also possible to use k-best Viterbi, with k &gt; 1.
</footnote>
<page confidence="0.996982">
14
</page>
<bodyText confidence="0.9967085">
Thus, the MLE model does not break symmetry
and rates each of the five parse trees as equally
likely. Therefore, its expected per-token accuracy
is 40%. Average overlaps between structures (i-v)
and answers (i,ii) are (i) 100% or 0; (ii) 0 or 100%;
and (iii,iv,v) 33.3%: (3+3)/(5×3) = 2/5 = 0.4.
A decoy model without left- or right-branching,
i.e., ˜PSTOP( a, L, T) = 1 or ˜PSTOP( a, R, T) = 1,
would assign zero probability to some of the train-
ing data. It would be forced to parse every instance
of @@@ either as (i) or as (ii), deterministically.
Nevertheless, it would attain a higher per-token ac-
curacy of 50%. (Judged on exact matches, at the
granularity of whole trees, the decoy’s guaranteed
50% accuracy clobbers the MLE’s expected 20%.)
Our toy data set could be replicated n-fold with-
out changing the analysis. This confirms that, even
in the absence of estimation errors or data sparsity,
there can be a fundamental disconnect between
likelihood and accuracy, if the model is wrong.8
This kernel is tiny, but, as before, our analysis is
invariant to n-fold replication: the problem cannot
be explained away by a small training size — it
persists even in infinitely large data sets. And so,
we consider three reference parse trees for two-
token sentences over a binary vocabulary { a�, z�}:
</bodyText>
<equation confidence="0.489658">
(i) ®�a�; (ii) ® z�; (iii) ®�a� .
</equation>
<bodyText confidence="0.999446666666667">
One third of the time, Q is the head; only ® can
be a child; and only ® has right-dependents. Trees
(i)-(iii) are the only two-terminal parses generated
by the model and are equiprobable. Thus, these
sentences are representative of a length-two re-
striction of everything generated by the true θ∗:
</bodyText>
<equation confidence="0.9572665">
2 4
PATTACH(♦, L, a) = 3 and PSTOP( a, ∗, T) = 5,
</equation>
<bodyText confidence="0.976852666666667">
since ® is the head two out of three times, and
since only one out of five a�’s attaches a child on
either side. Elsewhere, the model is deterministic:
</bodyText>
<subsectionHeader confidence="0.994874">
7.3 A Subtler Case: θ∗ = ˆθSUP vs. ˆθUNS vs. ˆθVIT PSTOP( z�, L, T) = 0;
</subsectionHeader>
<bodyText confidence="0.999976526315789">
We now prove that, even with the right model,
mismatches between the different objective like-
lihoods can also handicap the truth. Our calcula-
tions are again exact, so there are no issues with
numerical stability. We work with a set of param-
eters θ∗ already factored by the DMV, so that its
problems could not be blamed on invalid indepen-
dence assumptions. Yet we are able to find another
impostor distribution θ˜ that outshines ˆθSUP = θ∗ on
both unsupervised metrics, which proves that the
true models ˆθSUP and θ∗ are not globally optimal,
as judged by the two surrogate objective functions.
This next example is organic. We began with
WSJ10 and confirmed that classic EM abandons
the supervised solution. We then iteratively dis-
carded large portions of the data set, so long as
the remainder maintained the (un)desired effect —
EM walking away from its ˆθSUP. This procedure
isolated such behavior, arriving at a minimal set:
</bodyText>
<equation confidence="0.944169">
NP : NNP NNP Q
— Marvin Alisky.
PSTOP(∗, ∗, F) = PSTOP( z�, R, T) = 1;
PATTACH( a�, ∗, a�) = PATTACH( z�, L, a�) = 1.
</equation>
<bodyText confidence="0.935797125">
Contrast the optimal estimate ˆθSUP = θ∗ with the
decoy fixed point9 θ˜ that is identical to θ∗, except
˜PSTOP( a, L, T) = 35 and ˜PSTOP( a, R, T) = 1.
The probability of stopping is now 3/5 on the left
and 1 on the right, instead of 4/5 on both sides —
θ˜ disallows a�’s right-dependents but preserves its
overall fertility. The probabilities of leaves ® (no
children), under the models ˆθSUP and ˜θ, are:
</bodyText>
<table confidence="0.658102857142857">
ˆP( a) = ˆPSTOP( a, L, T)× (4 )2
ˆPSTOP( a, R, T) = 5
and ˜P( a) = ˜PSTOP( a, L,T)× 3
PSTOP ( ®, R, T) =
5.
�
And the probabilities of, e.g., structure a z, are:
</table>
<equation confidence="0.899012625">
S : NNP VBD Q
(Braniff declined).
NP-LOC : NNP NNP Q
Victoria, Texas
ˆPATTACH(♦, L, z) ×
ˆPSTOP( z, R, T)
× (1 − ˆPSTOP( z, L, T)) × ˆPSTOP( z, L, F)
× ˆPATTACH( z�, L, a) × ˆP( a�)
</equation>
<footnote confidence="0.99981">
8And as George Box quipped, “Essentially, all models are
wrong, but some are useful” (Box and Draper, 1987, p. 424).
9The model estimated from the parse trees induced by B
over the three sentences is again B, for both soft and hard EM.
</footnote>
<page confidence="0.989432">
15
</page>
<table confidence="0.897849333333333">
= ˆIATTACH(♦, L, z) × ˆI( a�) =1 16
3 25
and ˜IATTACH(♦, L, z) × ˜I( a�) =3 1 · 35.
</table>
<bodyText confidence="0.899674666666667">
Similarly, the probabilities of all four possible
parse trees for the two distinct sentences, ®® and
®z�, under the two models, ˆθSUP = θ* and ˜θ, are:
</bodyText>
<figure confidence="0.95850525">
ˆBSUP = B*
B˜
` 16 ´ =
1
3 25
75 = 0.213
16
0
3 `5´ `1 5´ ` 16
25´
3 `5´
5 = 0.2
1
0
2 `1 � 3 ´ `3 ´ =
3 5 5
128
1875
= 0.06826
= 0.16
4
25
9@
V@
9@
V@
0
0.06826
</figure>
<bodyText confidence="0.348685">
To the three true parses, ˆθSUP assigns probability
</bodyText>
<equation confidence="0.3781275">
�16 � � 128 ~2 ≈ 0.0009942 — about 1.66bpt; θ˜
75 1875
</equation>
<bodyText confidence="0.985904127272727">
leaves zero mass for (iii), corresponding to a larger
(infinite) cross-entropy, consistent with theory.
So far so good, but if asked for best (Viterbi)
parses, ˆθSUP could still produce the actual trees,
whereas θ˜ would happily parse sentences of (iii)
and (i) the same, perceiving a joint probability of
(0.2)(0.16)2 = 0.00512 — just 1.27bpt, appear-
ing to outperform ˆθSUP = θ*! Asked for sentence
probabilities, θ˜ would remain unchanged (it parses
each sentence unambiguously), but ˆθSUP would ag-
) (2 1875 128 ) 2 0.003977, improv-
gregate to ( 75 ing to 1.33bpt, but still noticeably “worse” than ˜θ.
Despite leaving zero probability to the truth, θ˜
beats θ* on both surrogate metrics, globally. This
seems like an egregious error. Judged by (extrin-
sic) accuracy, θ˜ still holds its own: it gets four
directed edges from predicting parse trees (i) and
(ii) completely right, but none of (iii) — a solid
66.7%. Subject to tie-breaking, θ* is equally likely
to get (i) and/or (iii) entirely right or totally wrong
(they are indistinguishable): it could earn a perfect
16 One reason why Viterbi EM may work well is
that its score is used in selecting actual output
parse trees. Wainwright (2006) provided strong
theoretical and empirical arguments for using the
same approximate inference method in training
as in performing predictions for a learned model.
He showed that if inference involves an approxi-
mation, then using the same approximate method
to train the model gives even better performance
guarantees than exact training methods. If our task
were not parsing but language modeling, where
the relevant score is the sum of the probabilities
over individual derivations, perhaps classic EM
Viterbi training is not only faster and more accu-
recursion con-
straints. It therefore invites more flexible model-
ing techniques, including discriminative, feature-
essentially via (unsupervised) self-training (Clark
et al., 2003; Ng and Cardie, 2003; McClosky et
al., 2006a; McClosky et al., 2006b, inter alia).
Such
by
approaches may be
quisition, as children frequently find themselves
forced to interpret a sentence in order to inter-
act with the world. Since most models of human
sky, 1996; Chater et al., 1998; Lewis and Vasishth,
2005, inter alia), the serial nature of Viterbi EM
than
inside-outside’s
“learning
doing”
the fully-integrated inside-outside solution.
rate but also free of
</bodyText>
<sectionHeader confidence="0.994169" genericHeader="method">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.866573333333333">
100%, tie ˜θ, or score a low 33.3%, at 1:2:1 odds,
spectively —same as ˜θ’s deterministic 66.7%
ce.
</bodyText>
<sectionHeader confidence="0.986315" genericHeader="discussions">
8 Discussion of Theoretical Results
</sectionHeader>
<bodyText confidence="0.995154433962265">
et al. (2009) questioned the benefits of us-
ing exact models in approximate inference. In our
case, the model already makes strong simplifying
assumptions and the objective is also incorrect. It
makes sense that Viterbi EM sometimes works,
since an approximate wrong
could, by
chance, be better than
Daum´e
“solution”
one that is exactly wrong.
would not be doing as badly, compared to Viterbi.
rich approaches that target conditional likelihoods,
relevant to understanding human language ac-
probabilistic parsing are massively pruned (Juraf-
— or the very limited parallelism of k-best Viterbi
— may be more appropriate in modeling this task
re
accuracy, in expectation, but with higher varian
Without a known objective, as in unsupervised
learning, correct exact optimization becomes im-
possible. In such cases, approximations, although
liable to pass over a true optimum, may achieve
faster convergence and still improve performance.
We showed that this is the case with Viterbi
training, a cheap alternative to inside-outside re-
estimation, for unsupervised dependency parsing.
We explained why Viterbi EM may be partic-
ularly well-suited to learning from longer sen-
tences, in addition to any general benefits to syn-
chronizing approximation methods across learn-
ing and inference. Our best algorithm is sim-
pler and an order of magnitude faster than clas-
sic EM. It achieves
performance:
3.8% higher accuracy than pre
state-of-the-art
vious published best
results on Section 23 (all sentences) of the Wall
Street Journal corpus. This improvement general-
izes to the Brown corpus, our held-out evaluation
set, where the same model registers a 7.5% gain.
Unfortunately, approximations alone do not
bridge the real gap between objective functions.
This deeper issue could be addressed by drawing
parsing constraints (Pereira and Schabes, 1992)
from specific applications. One example of such
an approach, tied to machine translation, is syn-
chronous grammars (Alshawi and Douglas, 2000).
An alternative — observing constraints induced by
hyper-text mark-up, harvested from the web — is
explored in a sister paper (Spitkovsky et al., 2010),
published concurrently.
</bodyText>
<sectionHeader confidence="0.999073" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.943098166666667">
Partially funded by NSF award IIS-0811974 and by the Air
Force Research Laboratory (AFRL), under prime contract
no. FA8750-09-C-0181; first author supported by the Fan-
nie &amp; John Hertz Foundation Fellowship. We thank An-
gel X. Chang, Mengqiu Wang and the anonymous reviewers
for many helpful comments on draft versions of this paper.
</bodyText>
<sectionHeader confidence="0.994564" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999867404761905">
H. Alshawi and S. Douglas. 2000. Learning dependency
transduction models from unannotated examples. In
Royal Society of London Philosophical Transactions Se-
ries A, volume 358.
H. Alshawi. 1996. Head automata for speech translation. In
Proc. ofICSLP.
J. K. Baker. 1979. Trainable grammars for speech recogni-
tion. In Speech Communication Papers for the 97th Meet-
ing of the Acoustical Society ofAmerica.
G. E. P. Box and N. R. Draper. 1987. Empirical Model-
Building and Response Surfaces. John Wiley.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19.
E. Charniak. 1993. Statistical Language Learning. MIT
Press.
N. Chater, M. J. Crocker, and M. J. Pickering. 1998. The
rational analysis of inquiry: The case of parsing. In
M. Oaksford and N. Chater, editors, Rational Models of
Cognition. Oxford University Press.
S. Clark, J. Curran, and M. Osborne. 2003. Bootstrapping
POS-taggers using unlabelled data. In Proc. of CoNLL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsupervised
grammar induction. In Proc. ofNAACL-HLT.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for
PCFGs: Hardness results and competitiveness of uniform
initialization. In Proc. ofACL.
M. Collins. 1999. Head-Driven Statistical Models for Nat-
ural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
H. Daum´e, III, J. Langford, and D. Marcu. 2009. Search-
based structured prediction. Machine Learning, 75(3).
C. de Marcken. 1995. Lexical heads, phrase structure and
the induction of grammar. In WVLC.
D. Elworthy. 1994. Does Baum-Welch re-estimation help
taggers? In Proc. ofANLP.
W. N. Francis and H. Kucera, 1979. Manual of Information
to Accompany a Standard Corpus of Present-Day Edited
American English, for use with Digital Computers. De-
partment of Linguistic, Brown University.
J. Gillenwater, K. Ganchev, J. Grac¸a, B. Taskar, and
F. Pereira. 2009. Sparsity in grammar induction. In
NIPS: Grammar Induction, Representation of Language
and Language Learning.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with richer
contexts and smoothing. In Proc. ofNAACL-HLT.
D. Jurafsky. 1996. A probabilistic model of lexical and syn-
tactic access and disambiguation. Cognitive Science, 20.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In Proc. ofACL.
R. L. Lewis and S. Vasishth. 2005. An activation-based
model of sentence processing as skilled memory retrieval.
Cognitive Science, 29.
P. Liang and D. Klein. 2008. Analyzing the errors of unsu-
pervised learning. In Proc. of HLT-ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2).
D. McClosky, E. Charniak, and M. Johnson. 2006a. Effec-
tive self-training for parsing. In Proc. ofNAACL-HLT.
D. McClosky, E. Charniak, and M. Johnson. 2006b. Rerank-
ing and self-training for parser adaptation. In Proc. of
COLING-ACL.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2).
V. Ng and C. Cardie. 2003. Weakly supervised natural lan-
guage learning without redundant views. In Proc. ofHLT-
NAACL.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In Proc. ofACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby
Steps: How “Less is More” in unsupervised dependency
parsing. In NIPS: Grammar Induction, Representation of
Language and Language Learning.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010. Profit-
ing from mark-up: Hyper-text annotations for guided pars-
ing. In Proc. ofACL.
M. J. Wainwright. 2006. Estimating the “wrong” graphical
model: Benefits in the computation-limited setting. Jour-
nal ofMachine Learning Research, 7.
</reference>
<page confidence="0.999409">
17
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.459342">
<title confidence="0.999844">Viterbi Training Improves Unsupervised Dependency Parsing</title>
<author confidence="0.999483">Valentin I Spitkovsky Hiyan Alshawi</author>
<affiliation confidence="0.99997">Computer Science Department Google Inc.</affiliation>
<address confidence="0.88688">Stanford University and Google Inc. Mountain View, CA, 94043, USA</address>
<email confidence="0.996244">valentin@cs.stanford.eduhiyan@google.com</email>
<author confidence="0.800906">D Jurafsky</author>
<affiliation confidence="0.979426">Departments of Linguistics and Computer</affiliation>
<address confidence="0.693115">Stanford University, Stanford, CA, 94305,</address>
<abstract confidence="0.997059807692308">We show that Viterbi (or “hard”) EM is well-suited to unsupervised grammar in- It is than standard inside-outside re-estimation (classic EM), significantly faster, and simpler. Our experiments with Klein and Manning’s Dependency Model with Valence (DMV) attain state-of-the-art performance — 44.8% accuracy on Section 23 (all sentences) of the Wall Street Journal corpus — without clever initialization; with a good initializer, Viterbi training improves to 47.9%. This generalizes to the Brown corpus, our held-out set, where accuracy reaches 50.8% — a 7.5% gain over previous best results. We find that classic EM learns better from short sentences but cannot cope with longer ones, where Viterbi thrives. However, we explain that both algorithms optimize the wrong objectives and prove that there are fundamental disconnects between the likelihoods of sentences, best parses, and true parses, beyond the wellestablished discrepancies between likelihood, accuracy and extrinsic performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Douglas</author>
</authors>
<title>Learning dependency transduction models from unannotated examples.</title>
<date>2000</date>
<journal>In Royal Society of London Philosophical Transactions Series A,</journal>
<volume>358</volume>
<marker>Alshawi, Douglas, 2000</marker>
<rawString>H. Alshawi and S. Douglas. 2000. Learning dependency transduction models from unannotated examples. In Royal Society of London Philosophical Transactions Series A, volume 358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head automata for speech translation.</title>
<date>1996</date>
<booktitle>In Proc. ofICSLP.</booktitle>
<contexts>
<context position="5871" citStr="Alshawi, 1996" startWordPosition="945" endWordPosition="946">ca — attached on the dir side of a head of class ch; �PSTOP(ch, dir, adj = T), the fraction of words of class ch with no children on the dir side; and INTOP(ch, dir, adj = F), the ratio1 of the number of words of class ch having a child on the dir side to their total number of such children. 3 Standard Data Sets and Evaluation |{z } |{z } 1 1 Figure 2: A dependency structure for a short sentence and its probability, as factored by the DMV, after summing out PORDER (Spitkovsky et al., 2009). 2 Dependency Model with Valence The DMV (Klein and Manning, 2004) is a singlestate head automata model (Alshawi, 1996) over lexical word classes {cw} — POS tags. Its generative story for a sub-tree rooted at a head (of class ch) rests on three types of independent decisions: (i) initial direction dir E {L, R} in which to attach children, via probability PORDER(ch); (ii) whether to seal dir, stopping with probability PSTOP(ch, dir, adj), conditioned on adj E {T, F} (true iff considering dir’s first, i.e., adjacent, child); and (iii) attachments (of class ca), according to PATTACH(ch, dir, ca). This produces only projective trees. A root token Q generates the head of a sentence as its left (and only) child. Fig</context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996. Head automata for speech translation. In Proc. ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Speech Communication Papers for the 97th Meeting of the Acoustical Society ofAmerica.</booktitle>
<contexts>
<context position="2785" citStr="Baker, 1979" startWordPosition="396" endWordPosition="397">rmance and where global optimization is feasible. This is because a key challenge in unsupervised learning is that the desired likelihood is unknown. Its absence renders tasks like structure discovery inherently underconstrained. Search-based algorithms adopt surrogate metrics, gambling on convergence to the “right” regularities in data. Their wrong objectives create cases in which both efficiency and performance improve when expensive exact learning techniques are replaced by cheap approximations. We propose using Viterbi training (Brown et al., 1993), instead of inside-outside reestimation (Baker, 1979), to induce hierarchical syntactic structure from natural language text. Our experiments with Klein and Manning’s (2004) Dependency Model with Valence (DMV), a popular state-of-the-art model (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2009), beat previous benchmark accuracies by 3.8% (on Section 23 of WSJ) and 7.5% (on parsed Brown). Since objective functions used in unsupervised grammar induction are provably wrong, advantages of exact inference may not apply. It makes sense to try the Viterbi approximation — it is also wrong, only simpler and cheaper than classic EM. As </context>
<context position="6596" citStr="Baker, 1979" startWordPosition="1066" endWordPosition="1067">sts on three types of independent decisions: (i) initial direction dir E {L, R} in which to attach children, via probability PORDER(ch); (ii) whether to seal dir, stopping with probability PSTOP(ch, dir, adj), conditioned on adj E {T, F} (true iff considering dir’s first, i.e., adjacent, child); and (iii) attachments (of class ca), according to PATTACH(ch, dir, ca). This produces only projective trees. A root token Q generates the head of a sentence as its left (and only) child. Figure 2 displays a simple example. The DMV lends itself to unsupervised learning via inside-outside re-estimation (Baker, 1979). Viterbi training (Brown et al., 1993) re-estimates each next model as if supervised by the previous best parse trees. And supervised learning from The DMV is traditionally trained and tested on customized subsets of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Following Klein and Manning (2004), we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty sub-trees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed nu</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J. K. Baker. 1979. Trainable grammars for speech recognition. In Speech Communication Papers for the 97th Meeting of the Acoustical Society ofAmerica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E P Box</author>
<author>N R Draper</author>
</authors>
<title>Empirical ModelBuilding and Response Surfaces.</title>
<date>1987</date>
<publisher>John Wiley.</publisher>
<contexts>
<context position="26632" citStr="Box and Draper, 1987" startWordPosition="4418" endWordPosition="4421"> on both sides — θ˜ disallows a�’s right-dependents but preserves its overall fertility. The probabilities of leaves ® (no children), under the models ˆθSUP and ˜θ, are: ˆP( a) = ˆPSTOP( a, L, T)× (4 )2 ˆPSTOP( a, R, T) = 5 and ˜P( a) = ˜PSTOP( a, L,T)× 3 PSTOP ( ®, R, T) = 5. � And the probabilities of, e.g., structure a z, are: S : NNP VBD Q (Braniff declined). NP-LOC : NNP NNP Q Victoria, Texas ˆPATTACH(♦, L, z) × ˆPSTOP( z, R, T) × (1 − ˆPSTOP( z, L, T)) × ˆPSTOP( z, L, F) × ˆPATTACH( z�, L, a) × ˆP( a�) 8And as George Box quipped, “Essentially, all models are wrong, but some are useful” (Box and Draper, 1987, p. 424). 9The model estimated from the parse trees induced by B over the three sentences is again B, for both soft and hard EM. 15 = ˆIATTACH(♦, L, z) × ˆI( a�) =1 16 3 25 and ˜IATTACH(♦, L, z) × ˜I( a�) =3 1 · 35. Similarly, the probabilities of all four possible parse trees for the two distinct sentences, ®® and ®z�, under the two models, ˆθSUP = θ* and ˜θ, are: ˆBSUP = B* B˜ ` 16 ´ = 1 3 25 75 = 0.213 16 0 3 `5´ `1 5´ ` 16 25´ 3 `5´ 5 = 0.2 1 0 2 `1 � 3 ´ `3 ´ = 3 5 5 128 1875 = 0.06826 = 0.16 4 25 9@ V@ 9@ V@ 0 0.06826 To the three true parses, ˆθSUP assigns probability �16 � � 128 ~2 ≈ </context>
</contexts>
<marker>Box, Draper, 1987</marker>
<rawString>G. E. P. Box and N. R. Draper. 1987. Empirical ModelBuilding and Response Surfaces. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="2731" citStr="Brown et al., 1993" startWordPosition="387" endWordPosition="390"> when likelihood is an appropriate measure of extrinsic performance and where global optimization is feasible. This is because a key challenge in unsupervised learning is that the desired likelihood is unknown. Its absence renders tasks like structure discovery inherently underconstrained. Search-based algorithms adopt surrogate metrics, gambling on convergence to the “right” regularities in data. Their wrong objectives create cases in which both efficiency and performance improve when expensive exact learning techniques are replaced by cheap approximations. We propose using Viterbi training (Brown et al., 1993), instead of inside-outside reestimation (Baker, 1979), to induce hierarchical syntactic structure from natural language text. Our experiments with Klein and Manning’s (2004) Dependency Model with Valence (DMV), a popular state-of-the-art model (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2009), beat previous benchmark accuracies by 3.8% (on Section 23 of WSJ) and 7.5% (on parsed Brown). Since objective functions used in unsupervised grammar induction are provably wrong, advantages of exact inference may not apply. It makes sense to try the Viterbi approximation — it is als</context>
<context position="6635" citStr="Brown et al., 1993" startWordPosition="1070" endWordPosition="1073">t decisions: (i) initial direction dir E {L, R} in which to attach children, via probability PORDER(ch); (ii) whether to seal dir, stopping with probability PSTOP(ch, dir, adj), conditioned on adj E {T, F} (true iff considering dir’s first, i.e., adjacent, child); and (iii) attachments (of class ca), according to PATTACH(ch, dir, ca). This produces only projective trees. A root token Q generates the head of a sentence as its left (and only) child. Figure 2 displays a simple example. The DMV lends itself to unsupervised learning via inside-outside re-estimation (Baker, 1979). Viterbi training (Brown et al., 1993) re-estimates each next model as if supervised by the previous best parse trees. And supervised learning from The DMV is traditionally trained and tested on customized subsets of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Following Klein and Manning (2004), we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty sub-trees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automa</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1733" citStr="Charniak, 1993" startWordPosition="243" endWordPosition="244">ot cope with longer ones, where Viterbi thrives. However, we explain that both algorithms optimize the wrong objectives and prove that there are fundamental disconnects between the likelihoods of sentences, best parses, and true parses, beyond the wellestablished discrepancies between likelihood, accuracy and extrinsic performance. 1 Introduction Unsupervised learning is hard, often involving difficult objective functions. A typical approach is to attempt maximizing the likelihood of unlabeled data, in accordance with a probabilistic model. Sadly, such functions are riddled with local optima (Charniak, 1993, Ch. 7, inter alia), since their number of peaks grows exponentially with instances of hidden variables. Furthermore, a higher likelihood does not always translate into superior task-specific accuracy (Elworthy, 1994; Merialdo, 1994). Both complications are real, but we will discuss perhaps more significant shortcomings. We prove that learning can be error-prone even in cases when likelihood is an appropriate measure of extrinsic performance and where global optimization is feasible. This is because a key challenge in unsupervised learning is that the desired likelihood is unknown. Its absenc</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>E. Charniak. 1993. Statistical Language Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chater</author>
<author>M J Crocker</author>
<author>M J Pickering</author>
</authors>
<title>The rational analysis of inquiry: The case of parsing.</title>
<date>1998</date>
<editor>In M. Oaksford and N. Chater, editors,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="29479" citStr="Chater et al., 1998" startWordPosition="4926" endWordPosition="4929">ng, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM Viterbi training is not only faster and more accurecursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featureessentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such by approaches may be quisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human sky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM than inside-outside’s “learning doing” the fully-integrated inside-outside solution. rate but also free of 9 Conclusion 100%, tie ˜θ, or score a low 33.3%, at 1:2:1 odds, spectively —same as ˜θ’s deterministic 66.7% ce. 8 Discussion of Theoretical Results et al. (2009) questioned the benefits of using exact models in approximate inference. In our case, the model already makes strong simplifying assumptions and the objective is also incorrect. It makes sense that Viterbi EM sometimes works, since an approximate wrong could</context>
</contexts>
<marker>Chater, Crocker, Pickering, 1998</marker>
<rawString>N. Chater, M. J. Crocker, and M. J. Pickering. 1998. The rational analysis of inquiry: The case of parsing. In M. Oaksford and N. Chater, editors, Rational Models of Cognition. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Curran</author>
<author>M Osborne</author>
</authors>
<title>Bootstrapping POS-taggers using unlabelled data.</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="29194" citStr="Clark et al., 2003" startWordPosition="4877" endWordPosition="4880">ng as in performing predictions for a learned model. He showed that if inference involves an approximation, then using the same approximate method to train the model gives even better performance guarantees than exact training methods. If our task were not parsing but language modeling, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM Viterbi training is not only faster and more accurecursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featureessentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such by approaches may be quisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human sky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM than inside-outside’s “learning doing” the fully-integrated inside-outside solution. rate but also free of 9 Conclusion 100%, tie ˜θ, or score a low 33.3%, at 1:2:1 odds, spectively —same as ˜θ’s deterministic 66.7% ce. 8 Discussion of Theore</context>
</contexts>
<marker>Clark, Curran, Osborne, 2003</marker>
<rawString>S. Clark, J. Curran, and M. Osborne. 2003. Bootstrapping POS-taggers using unlabelled data. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. ofNAACL-HLT.</booktitle>
<contexts>
<context position="3020" citStr="Cohen and Smith, 2009" startWordPosition="428" endWordPosition="432">ed. Search-based algorithms adopt surrogate metrics, gambling on convergence to the “right” regularities in data. Their wrong objectives create cases in which both efficiency and performance improve when expensive exact learning techniques are replaced by cheap approximations. We propose using Viterbi training (Brown et al., 1993), instead of inside-outside reestimation (Baker, 1979), to induce hierarchical syntactic structure from natural language text. Our experiments with Klein and Manning’s (2004) Dependency Model with Valence (DMV), a popular state-of-the-art model (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2009), beat previous benchmark accuracies by 3.8% (on Section 23 of WSJ) and 7.5% (on parsed Brown). Since objective functions used in unsupervised grammar induction are provably wrong, advantages of exact inference may not apply. It makes sense to try the Viterbi approximation — it is also wrong, only simpler and cheaper than classic EM. As it turns out, Viterbi EM is not only faster but also more accurate, consistent with hypotheses of de Marcken (1995) and Spitkovsky et al. (2009). We begin by reviewing the model, standard data sets and metrics, and our experimental res</context>
<context position="14506" citStr="Cohen and Smith, 2009" startWordPosition="2347" endWordPosition="2350">s work well with short sentences (see Figure 3(a)), it makes sense to use their pre-trained models to initialize Viterbi training, mixing the two strategies. We judged all Ad-Hoc* initializers against WSJ15 and found that the one for WSJ8 minimizes sentence-level cross-entropy (see Figure 5). This approach does not involve reference parse 4For classic EM, the number of iterations to convergence appears sometimes inversely related to performance, giving credence to the notion of early termination as a regularizer. 12 Model Incarnation WSJ10 WSJ20 WSJ° DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100 Less is More (Ad-Hoc* @15) (Spitkovsky et al., 2009) 56.2 48.2 44.1 43.3 A. Smoothed Viterbi Training (@15), Initialized with the Uniform Prior 59.9 50.0 44.8 48.1 B. A Good Initializer (Ad-Hoc*’s @8), Classically Pre-Trained (@15) 63.8 52.3 46.2 49.3 C. Smoothed Viterbi Training (@15), Initialized with B 64.4 53.5 47.8 50.5 D. Smoothed Viterbi Training (@45), Initialized with C 65.3 53.8 47.9 50.8 EVG Smoothed (skip-head), Lexicalized (Headden et al., 2009) 68.8 Table 1: Accuracies on Section 23 of WSJ{10, 20,&apos; } and Brown100 for three recent state-of-the-art systems,</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proc. ofNAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization.</title>
<date>2010</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="12665" citStr="Cohen and Smith (2010)" startWordPosition="2067" endWordPosition="2070">ufficient data — it even beats the “clever” initializer everywhere past WSJ10. The “sweet spot” at WSJ15 — a neighborhood where both Ad-Hoc* and the oracle excel under classic EM — disappears with Viterbi. Furthermore, Viterbi does not degrade with more (complex) data, except with a biased initializer. More than a simple efficiency hack, Viterbi EM actually improves performance. And its benefits to running times are also non-trivial: it not only skips computing the outside charts in every iteration but also converges (sometimes an order of magnitude) 3In a concurrently published related work, Cohen and Smith (2010) prove that the uniform-at-random initializer is a competitive starting M-step for Viterbi EM; our uninformed prior consists of uniform multinomials, seeding the E-step. faster than classic EM (see Figure 3(c,d)).4 4.2 Result #2: Smoothed Models Smoothing rarely helps classic EM and hurts in the case of oracle training (see Figure 4(a)). With Viterbi, supervised initialization suffers much less, the biased initializer is a wash, and the uninformed uniform prior generally gains a few points of accuracy, e.g., up 2.9% (from 42.4% to 45.2%, evaluated against WSJ40) at WSJ15 (see Figure 4(b)). Bab</context>
</contexts>
<marker>Cohen, Smith, 2010</marker>
<rawString>S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="7279" citStr="Collins, 1999" startWordPosition="1171" endWordPosition="1172">as if supervised by the previous best parse trees. And supervised learning from The DMV is traditionally trained and tested on customized subsets of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Following Klein and Manning (2004), we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty sub-trees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic “head-percolation” rules (Collins, 1999) to convert the rest, as is standard practice. We experiment with WSJk (sentences with at most k tokens), for 1 G k G 45, and Section 23 of WSJ&apos; (all sentence lengths). We also evaluate on Brown100, similarly derived from the parsed portion of the Brown corpus (Francis and Kucera, 1979), as our held-out set. Figure 1 shows these corpora’s sentence and token counts. Proposed parse trees are judged on accuracy: a directed score is simply the overall fraction of correctly guessed dependencies. Let S be a set of sentences, with |s |the number of terminals (to1The expected number of trials needed t</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
<author>J Langford</author>
<author>D Marcu</author>
</authors>
<title>Searchbased structured prediction.</title>
<date>2009</date>
<booktitle>Machine Learning, 75(3). C. de Marcken.</booktitle>
<marker>Daum´e, Langford, Marcu, 2009</marker>
<rawString>H. Daum´e, III, J. Langford, and D. Marcu. 2009. Searchbased structured prediction. Machine Learning, 75(3). C. de Marcken. 1995. Lexical heads, phrase structure and the induction of grammar. In WVLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch re-estimation help taggers?</title>
<date>1994</date>
<booktitle>In Proc. ofANLP.</booktitle>
<contexts>
<context position="1950" citStr="Elworthy, 1994" startWordPosition="274" endWordPosition="275">s, and true parses, beyond the wellestablished discrepancies between likelihood, accuracy and extrinsic performance. 1 Introduction Unsupervised learning is hard, often involving difficult objective functions. A typical approach is to attempt maximizing the likelihood of unlabeled data, in accordance with a probabilistic model. Sadly, such functions are riddled with local optima (Charniak, 1993, Ch. 7, inter alia), since their number of peaks grows exponentially with instances of hidden variables. Furthermore, a higher likelihood does not always translate into superior task-specific accuracy (Elworthy, 1994; Merialdo, 1994). Both complications are real, but we will discuss perhaps more significant shortcomings. We prove that learning can be error-prone even in cases when likelihood is an appropriate measure of extrinsic performance and where global optimization is feasible. This is because a key challenge in unsupervised learning is that the desired likelihood is unknown. Its absence renders tasks like structure discovery inherently underconstrained. Search-based algorithms adopt surrogate metrics, gambling on convergence to the “right” regularities in data. Their wrong objectives create cases i</context>
<context position="17751" citStr="Elworthy, 1994" startWordPosition="2849" endWordPosition="2850">niversally degrade with more (complex) training sets, except with a biased initializer. And third, Viterbi learns poorly from small data sets of short sentences (WSJk, k &lt; 5). Viterbi may be better suited to unsupervised grammar induction compared with classic EM, but neither is sufficient, by itself. Both algorithms abandon good solutions and make no guarantees with respect to extrinsic performance. Unfortunately, these two approaches share a deep flaw. 6 Related Work on Improper Objectives It is well-known that maximizing likelihood may, in fact, degrade accuracy (Pereira and Schabes, 1992; Elworthy, 1994; Merialdo, 1994). de Marcken (1995) showed that classic EM suffers from a fatal attraction towards deterministic grammars and suggested a Viterbi training scheme as a remedy. Liang and Klein’s (2008) analysis of errors in unsupervised learning began with the inappropriateness of the likelihood objective (approximation), explored problems of data sparsity (estimation) and focused on EM-specific issues related to non-convexity (identifiability and optimization). Previous literature primarily relied on experimental evidence. de Marcken’s analytical result is an exception but pertains only to EM-</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>D. Elworthy. 1994. Does Baum-Welch re-estimation help taggers? In Proc. ofANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kucera</author>
</authors>
<title>Manual of Information to Accompany a Standard Corpus of Present-Day Edited American English, for use with Digital Computers.</title>
<date>1979</date>
<institution>Department of Linguistic, Brown University.</institution>
<contexts>
<context position="7566" citStr="Francis and Kucera, 1979" startWordPosition="1222" endWordPosition="1225">ference constituent parses and compare against deterministically derived dependencies: after pruning out all empty sub-trees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic “head-percolation” rules (Collins, 1999) to convert the rest, as is standard practice. We experiment with WSJk (sentences with at most k tokens), for 1 G k G 45, and Section 23 of WSJ&apos; (all sentence lengths). We also evaluate on Brown100, similarly derived from the parsed portion of the Brown corpus (Francis and Kucera, 1979), as our held-out set. Figure 1 shows these corpora’s sentence and token counts. Proposed parse trees are judged on accuracy: a directed score is simply the overall fraction of correctly guessed dependencies. Let S be a set of sentences, with |s |the number of terminals (to1The expected number of trials needed to get one Bernoulli(p) success is n - Geometric(p), with n E Z+, P(n) = (1 − p)&amp;quot;−&apos;p and E(n) = p−�; MoM and MLE agree, p� = (# of successes)/(# of trials). 10 70 60 50 40 30 20 10 5 10 15 20 25 30 35 40 WSJk (training on all WSJ sentences up to k tokens in length) (a) %-Accuracy for Ins</context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>W. N. Francis and H. Kucera, 1979. Manual of Information to Accompany a Standard Corpus of Present-Day Edited American English, for use with Digital Computers. Department of Linguistic, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillenwater</author>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>B Taskar</author>
<author>F Pereira</author>
</authors>
<title>Sparsity in grammar induction.</title>
<date>2009</date>
<booktitle>In NIPS: Grammar Induction, Representation of Language and Language Learning.</booktitle>
<marker>Gillenwater, Ganchev, Grac¸a, Taskar, Pereira, 2009</marker>
<rawString>J. Gillenwater, K. Ganchev, J. Grac¸a, B. Taskar, and F. Pereira. 2009. Sparsity in grammar induction. In NIPS: Grammar Induction, Representation of Language and Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P Headden</author>
<author>M Johnson</author>
<author>D McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<journal>Cognitive Science,</journal>
<booktitle>In Proc. ofNAACL-HLT. D. Jurafsky.</booktitle>
<volume>20</volume>
<contexts>
<context position="2997" citStr="Headden et al., 2009" startWordPosition="424" endWordPosition="427">erently underconstrained. Search-based algorithms adopt surrogate metrics, gambling on convergence to the “right” regularities in data. Their wrong objectives create cases in which both efficiency and performance improve when expensive exact learning techniques are replaced by cheap approximations. We propose using Viterbi training (Brown et al., 1993), instead of inside-outside reestimation (Baker, 1979), to induce hierarchical syntactic structure from natural language text. Our experiments with Klein and Manning’s (2004) Dependency Model with Valence (DMV), a popular state-of-the-art model (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2009), beat previous benchmark accuracies by 3.8% (on Section 23 of WSJ) and 7.5% (on parsed Brown). Since objective functions used in unsupervised grammar induction are provably wrong, advantages of exact inference may not apply. It makes sense to try the Viterbi approximation — it is also wrong, only simpler and cheaper than classic EM. As it turns out, Viterbi EM is not only faster but also more accurate, consistent with hypotheses of de Marcken (1995) and Spitkovsky et al. (2009). We begin by reviewing the model, standard data sets and metrics, a</context>
<context position="14993" citStr="Headden et al., 2009" startWordPosition="2423" endWordPosition="2426"> termination as a regularizer. 12 Model Incarnation WSJ10 WSJ20 WSJ° DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100 Less is More (Ad-Hoc* @15) (Spitkovsky et al., 2009) 56.2 48.2 44.1 43.3 A. Smoothed Viterbi Training (@15), Initialized with the Uniform Prior 59.9 50.0 44.8 48.1 B. A Good Initializer (Ad-Hoc*’s @8), Classically Pre-Trained (@15) 63.8 52.3 46.2 49.3 C. Smoothed Viterbi Training (@15), Initialized with B 64.4 53.5 47.8 50.5 D. Smoothed Viterbi Training (@45), Initialized with C 65.3 53.8 47.9 50.8 EVG Smoothed (skip-head), Lexicalized (Headden et al., 2009) 68.8 Table 1: Accuracies on Section 23 of WSJ{10, 20,&apos; } and Brown100 for three recent state-of-the-art systems, our initializer, and smoothed Viterbi-trained runs that employ different initialization strategies. 4.5 WSJ� 5 10 15 20 25 30 35 40 45 Figure 5: Sentence-level cross-entropy on WSJ15 for Ad-Hoc* initializers of WSJ{1, ... , 45}. trees and is therefore still unsupervised. Using the Ad-Hoc* initializer based on WSJ8 to seed classic training at WSJ15 yields a further 1.4% gain in accuracy, scoring 46.2% on WSJ&apos; (see Table 1(B)). This good initializer boosts accuracy attained by smooth</context>
</contexts>
<marker>Headden, Johnson, McClosky, 2009</marker>
<rawString>W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proc. ofNAACL-HLT. D. Jurafsky. 1996. A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science, 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="5818" citStr="Klein and Manning, 2004" startWordPosition="934" endWordPosition="937">TACH(ch, dir, ca) is the fraction of children — those of class ca — attached on the dir side of a head of class ch; �PSTOP(ch, dir, adj = T), the fraction of words of class ch with no children on the dir side; and INTOP(ch, dir, adj = F), the ratio1 of the number of words of class ch having a child on the dir side to their total number of such children. 3 Standard Data Sets and Evaluation |{z } |{z } 1 1 Figure 2: A dependency structure for a short sentence and its probability, as factored by the DMV, after summing out PORDER (Spitkovsky et al., 2009). 2 Dependency Model with Valence The DMV (Klein and Manning, 2004) is a singlestate head automata model (Alshawi, 1996) over lexical word classes {cw} — POS tags. Its generative story for a sub-tree rooted at a head (of class ch) rests on three types of independent decisions: (i) initial direction dir E {L, R} in which to attach children, via probability PORDER(ch); (ii) whether to seal dir, stopping with probability PSTOP(ch, dir, adj), conditioned on adj E {T, F} (true iff considering dir’s first, i.e., adjacent, child); and (iii) attachments (of class ca), according to PATTACH(ch, dir, ca). This produces only projective trees. A root token Q generates the</context>
<context position="18734" citStr="Klein and Manning (2004)" startWordPosition="2991" endWordPosition="2994">estimation) and focused on EM-specific issues related to non-convexity (identifiability and optimization). Previous literature primarily relied on experimental evidence. de Marcken’s analytical result is an exception but pertains only to EM-specific local attractors. Our analysis confirms his intuitions and moreover shows that there can be global preferences for deterministic grammars — problems that would persist with tractable optimization. We prove that there is a fundamental disconnect between objective functions even when likelihood is a reasonable metric and training data are infinite. 6Klein and Manning (2004) originally trained the DMV on WSJ10 and Gillenwater et al. (2009) found it useful to discard data from WSJ3, which is mostly incomplete sentences. x-Entropy h (in bits per token) on WSJ15 lowest cross-entropy (4.32bpt) attained at WSJ8 bpt 5.5 5.0 13 7 Proofs (by Construction) There is a subtle distinction between three different probability distributions that arise in parsing, each of which can be legitimately termed “likelihood” — the mass that a particular model assigns to (i) highest-scoring (Viterbi) parse trees; (ii) the correct (gold) reference trees; and (iii) the sentence strings (su</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Lewis</author>
<author>S Vasishth</author>
</authors>
<title>An activation-based model of sentence processing as skilled memory retrieval.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<contexts>
<context position="29505" citStr="Lewis and Vasishth, 2005" startWordPosition="4930" endWordPosition="4933">t score is the sum of the probabilities over individual derivations, perhaps classic EM Viterbi training is not only faster and more accurecursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featureessentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such by approaches may be quisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human sky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM than inside-outside’s “learning doing” the fully-integrated inside-outside solution. rate but also free of 9 Conclusion 100%, tie ˜θ, or score a low 33.3%, at 1:2:1 odds, spectively —same as ˜θ’s deterministic 66.7% ce. 8 Discussion of Theoretical Results et al. (2009) questioned the benefits of using exact models in approximate inference. In our case, the model already makes strong simplifying assumptions and the objective is also incorrect. It makes sense that Viterbi EM sometimes works, since an approximate wrong could, by chance, be better tha</context>
</contexts>
<marker>Lewis, Vasishth, 2005</marker>
<rawString>R. L. Lewis and S. Vasishth. 2005. An activation-based model of sentence processing as skilled memory retrieval. Cognitive Science, 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>Analyzing the errors of unsupervised learning.</title>
<date>2008</date>
<booktitle>In Proc. of HLT-ACL.</booktitle>
<marker>Liang, Klein, 2008</marker>
<rawString>P. Liang and D. Klein. 2008. Analyzing the errors of unsupervised learning. In Proc. of HLT-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="6887" citStr="Marcus et al., 1993" startWordPosition="1109" endWordPosition="1112">cent, child); and (iii) attachments (of class ca), according to PATTACH(ch, dir, ca). This produces only projective trees. A root token Q generates the head of a sentence as its left (and only) child. Figure 2 displays a simple example. The DMV lends itself to unsupervised learning via inside-outside re-estimation (Baker, 1979). Viterbi training (Brown et al., 1993) re-estimates each next model as if supervised by the previous best parse trees. And supervised learning from The DMV is traditionally trained and tested on customized subsets of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Following Klein and Manning (2004), we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty sub-trees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic “head-percolation” rules (Collins, 1999) to convert the rest, as is standard practice. We experiment with WSJk (sentences with at most k tokens), for 1 G k G 45, and Section 23 of WSJ&apos; (all sentence lengths). We also evaluate on Brown100, similarly</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proc. ofNAACL-HLT.</booktitle>
<contexts>
<context position="29238" citStr="McClosky et al., 2006" startWordPosition="4885" endWordPosition="4888">rned model. He showed that if inference involves an approximation, then using the same approximate method to train the model gives even better performance guarantees than exact training methods. If our task were not parsing but language modeling, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM Viterbi training is not only faster and more accurecursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featureessentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such by approaches may be quisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human sky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM than inside-outside’s “learning doing” the fully-integrated inside-outside solution. rate but also free of 9 Conclusion 100%, tie ˜θ, or score a low 33.3%, at 1:2:1 odds, spectively —same as ˜θ’s deterministic 66.7% ce. 8 Discussion of Theoretical Results et al. (2009) questioned the b</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006a. Effective self-training for parsing. In Proc. ofNAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="29238" citStr="McClosky et al., 2006" startWordPosition="4885" endWordPosition="4888">rned model. He showed that if inference involves an approximation, then using the same approximate method to train the model gives even better performance guarantees than exact training methods. If our task were not parsing but language modeling, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM Viterbi training is not only faster and more accurecursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featureessentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such by approaches may be quisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human sky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM than inside-outside’s “learning doing” the fully-integrated inside-outside solution. rate but also free of 9 Conclusion 100%, tie ˜θ, or score a low 33.3%, at 1:2:1 odds, spectively —same as ˜θ’s deterministic 66.7% ce. 8 Discussion of Theoretical Results et al. (2009) questioned the b</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006b. Reranking and self-training for parser adaptation. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1967" citStr="Merialdo, 1994" startWordPosition="276" endWordPosition="277">es, beyond the wellestablished discrepancies between likelihood, accuracy and extrinsic performance. 1 Introduction Unsupervised learning is hard, often involving difficult objective functions. A typical approach is to attempt maximizing the likelihood of unlabeled data, in accordance with a probabilistic model. Sadly, such functions are riddled with local optima (Charniak, 1993, Ch. 7, inter alia), since their number of peaks grows exponentially with instances of hidden variables. Furthermore, a higher likelihood does not always translate into superior task-specific accuracy (Elworthy, 1994; Merialdo, 1994). Both complications are real, but we will discuss perhaps more significant shortcomings. We prove that learning can be error-prone even in cases when likelihood is an appropriate measure of extrinsic performance and where global optimization is feasible. This is because a key challenge in unsupervised learning is that the desired likelihood is unknown. Its absence renders tasks like structure discovery inherently underconstrained. Search-based algorithms adopt surrogate metrics, gambling on convergence to the “right” regularities in data. Their wrong objectives create cases in which both effi</context>
<context position="17768" citStr="Merialdo, 1994" startWordPosition="2851" endWordPosition="2852">de with more (complex) training sets, except with a biased initializer. And third, Viterbi learns poorly from small data sets of short sentences (WSJk, k &lt; 5). Viterbi may be better suited to unsupervised grammar induction compared with classic EM, but neither is sufficient, by itself. Both algorithms abandon good solutions and make no guarantees with respect to extrinsic performance. Unfortunately, these two approaches share a deep flaw. 6 Related Work on Improper Objectives It is well-known that maximizing likelihood may, in fact, degrade accuracy (Pereira and Schabes, 1992; Elworthy, 1994; Merialdo, 1994). de Marcken (1995) showed that classic EM suffers from a fatal attraction towards deterministic grammars and suggested a Viterbi training scheme as a remedy. Liang and Klein’s (2008) analysis of errors in unsupervised learning began with the inappropriateness of the likelihood objective (approximation), explored problems of data sparsity (estimation) and focused on EM-specific issues related to non-convexity (identifiability and optimization). Previous literature primarily relied on experimental evidence. de Marcken’s analytical result is an exception but pertains only to EM-specific local at</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Weakly supervised natural language learning without redundant views.</title>
<date>2003</date>
<booktitle>In Proc. ofHLTNAACL.</booktitle>
<contexts>
<context position="29215" citStr="Ng and Cardie, 2003" startWordPosition="4881" endWordPosition="4884">predictions for a learned model. He showed that if inference involves an approximation, then using the same approximate method to train the model gives even better performance guarantees than exact training methods. If our task were not parsing but language modeling, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM Viterbi training is not only faster and more accurecursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featureessentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such by approaches may be quisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human sky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM than inside-outside’s “learning doing” the fully-integrated inside-outside solution. rate but also free of 9 Conclusion 100%, tie ˜θ, or score a low 33.3%, at 1:2:1 odds, spectively —same as ˜θ’s deterministic 66.7% ce. 8 Discussion of Theoretical Results et al. </context>
</contexts>
<marker>Ng, Cardie, 2003</marker>
<rawString>V. Ng and C. Cardie. 2003. Weakly supervised natural language learning without redundant views. In Proc. ofHLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>Y Schabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="17735" citStr="Pereira and Schabes, 1992" startWordPosition="2845" endWordPosition="2848"> Second, Viterbi does not universally degrade with more (complex) training sets, except with a biased initializer. And third, Viterbi learns poorly from small data sets of short sentences (WSJk, k &lt; 5). Viterbi may be better suited to unsupervised grammar induction compared with classic EM, but neither is sufficient, by itself. Both algorithms abandon good solutions and make no guarantees with respect to extrinsic performance. Unfortunately, these two approaches share a deep flaw. 6 Related Work on Improper Objectives It is well-known that maximizing likelihood may, in fact, degrade accuracy (Pereira and Schabes, 1992; Elworthy, 1994; Merialdo, 1994). de Marcken (1995) showed that classic EM suffers from a fatal attraction towards deterministic grammars and suggested a Viterbi training scheme as a remedy. Liang and Klein’s (2008) analysis of errors in unsupervised learning began with the inappropriateness of the likelihood objective (approximation), explored problems of data sparsity (estimation) and focused on EM-specific issues related to non-convexity (identifiability and optimization). Previous literature primarily relied on experimental evidence. de Marcken’s analytical result is an exception but pert</context>
<context position="31641" citStr="Pereira and Schabes, 1992" startWordPosition="5257" endWordPosition="5260">to synchronizing approximation methods across learning and inference. Our best algorithm is simpler and an order of magnitude faster than classic EM. It achieves performance: 3.8% higher accuracy than pre state-of-the-art vious published best results on Section 23 (all sentences) of the Wall Street Journal corpus. This improvement generalizes to the Brown corpus, our held-out evaluation set, where the same model registers a 7.5% gain. Unfortunately, approximations alone do not bridge the real gap between objective functions. This deeper issue could be addressed by drawing parsing constraints (Pereira and Schabes, 1992) from specific applications. One example of such an approach, tied to machine translation, is synchronous grammars (Alshawi and Douglas, 2000). An alternative — observing constraints induced by hyper-text mark-up, harvested from the web — is explored in a sister paper (Spitkovsky et al., 2010), published concurrently. Acknowledgments Partially funded by NSF award IIS-0811974 and by the Air Force Research Laboratory (AFRL), under prime contract no. FA8750-09-C-0181; first author supported by the Fannie &amp; John Hertz Foundation Fellowship. We thank Angel X. Chang, Mengqiu Wang and the anonymous r</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>F. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Baby Steps: How “Less is More” in unsupervised dependency parsing.</title>
<date>2009</date>
<booktitle>In NIPS: Grammar Induction, Representation of Language and Language Learning.</booktitle>
<contexts>
<context position="3046" citStr="Spitkovsky et al., 2009" startWordPosition="433" endWordPosition="436">thms adopt surrogate metrics, gambling on convergence to the “right” regularities in data. Their wrong objectives create cases in which both efficiency and performance improve when expensive exact learning techniques are replaced by cheap approximations. We propose using Viterbi training (Brown et al., 1993), instead of inside-outside reestimation (Baker, 1979), to induce hierarchical syntactic structure from natural language text. Our experiments with Klein and Manning’s (2004) Dependency Model with Valence (DMV), a popular state-of-the-art model (Headden et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2009), beat previous benchmark accuracies by 3.8% (on Section 23 of WSJ) and 7.5% (on parsed Brown). Since objective functions used in unsupervised grammar induction are provably wrong, advantages of exact inference may not apply. It makes sense to try the Viterbi approximation — it is also wrong, only simpler and cheaper than classic EM. As it turns out, Viterbi EM is not only faster but also more accurate, consistent with hypotheses of de Marcken (1995) and Spitkovsky et al. (2009). We begin by reviewing the model, standard data sets and metrics, and our experimental results. After relating our c</context>
<context position="4665" citStr="Spitkovsky et al., 2009" startWordPosition="702" endWordPosition="705">100 Corpus Sentences POS Tokens Corpus Sentences POS Tokens WSJ1 159 159 WSJ13 12,270 110,760 WSJ2 499 839 WSJ14 14,095 136,310 WSJ3 876 1,970 WSJ15 15,922 163,715 WSJ4 1,394 4,042 WSJ20 25,523 336,555 WSJ5 2,008 7,112 WSJ25 34,431 540,895 WSJ6 2,745 11,534 WSJ30 41,227 730,099 WSJ7 3,623 17,680 WSJ35 45,191 860,053 WSJ8 4,730 26,536 WSJ40 47,385 942,801 WSJ9 5,938 37,408 WSJ45 48,418 986,830 WSJ10 7,422 52,248 WSJ100 49,206 1,028,054 WSJ11 8,856 68,022 Section 23 2,353 48,201 WSJ12 10,500 87,750 Brown100 24,208 391,796 Figure 1: Sizes of WSJ11, ... , 45,100}, Section 23 of WSJ&apos; and Brown100 (Spitkovsky et al., 2009). NNS VBD IN NN Q Payrolls fell in September . 0 z } |{ P = (1 − PSTOP(0, L, T)) X PATTACH(0, L, VBD) X (1 − PSTOP(VBD, L, T)) X PATTACH(VBD, L, NNS) X (1 − PSTOP(VBD, R, T)) X PATTACH(VBD, R, IN) X (1 − PSTOP(IN, R, T)) X PATTACH(IN, R, NN) X PSTOP(VBD, L, F) X PSTOP(VBD, R, F) X PSTOP(NNS, L, T) X PSTOP(NNS, R, T) X PSTOP(IN, L, T) X PSTOP(IN, R, F) X PSTOP(NN, L, T) X PSTOP(NN, R, T) X PSTOP(0, L, F) X PSTOP(0, R, T) � reference parse trees is straight-forward, since maximum-likelihood estimation reduces to counting: lATTACH(ch, dir, ca) is the fraction of children — those of class ca — att</context>
<context position="8897" citStr="Spitkovsky et al., 2009" startWordPosition="1463" endWordPosition="1466">c) Iterations for Inside-Outside (Soft EM) 5 10 15 20 25 30 35 40 WSJk 70 60 50 40 30 20 10 200 150 100 50 Directed Dependency Accuracy on WSJ40 350 Iterations to Convergence 5 10 15 20 25 30 35 40 WSJk (d) Iterations for Viterbi (Hard EM) Ad-Hoc∗ Uninformed Oracle Iterations to Convergence 200 150 100 50 Directed Dependency Accuracy on WSJ40 (b) %-Accuracy for Viterbi (Hard EM) Oracle Ad-Hoc∗ Uninformed Figure 3: Directed dependency accuracies attained by the DMV, when trained on WSJk, smoothed, then tested against a fixed evaluation set, WSJ40, for three different initialization strategies (Spitkovsky et al., 2009). Red, green and blue graphs represent the supervised (maximum-likelihood oracle) initialization, a linguistically-biased initializer (Ad-Hoc*) and the uninformed (uniform) prior. Panel (b) shows results obtained with Viterbi training instead of classic EM — Panel (a), but is otherwise identical (in both, each of the 45 vertical slices captures five new experimental results and arrows connect starting performance with final accuracy, emphasizing the impact of learning). Panels (c) and (d) show the corresponding numbers of iterations until EM’s convergence. kens) for each s E S. Denote by T (s)</context>
<context position="11204" citStr="Spitkovsky et al., 2009" startWordPosition="1838" endWordPosition="1841"> (2009) variation on Klein and Manning’s (2004) “ad-hoc harmonic” completion. E. sER |s| 11 5 10 15 20 25 30 35 40 WSJ� 5 10 15 20 25 30 35 40 WSJ� (a) %-Accuracy for Inside-Outside (Soft EM) Uninformed Ad-Hoc* Baby Steps Oracle (b) %-Accuracy for Viterbi (Hard EM) Oracle Ad-Hoc* Uninformed Baby Steps 70 60 50 40 30 20 10 Directed Dependency Accuracy on WSJ40 70 60 50 40 30 20 10 Directed Dependency Accuracy on WSJ40 Figure 4: Superimposes directed accuracies attained by DMV models trained with Laplace smoothing (brightly-colored curves) over Figure 3(a,b); violet curves represent Baby Steps (Spitkovsky et al., 2009). 4.1 Result #1: Viterbi-Trained Models The results of Spitkovsky et al. (2009), tested against WSJ40, are re-printed in Figure 3(a); our corresponding Viterbi runs appear in Figure 3(b). We observe crucial differences between the two training modes for each of the three initialization strategies. Both algorithms walk away from the supervised maximum-likelihood solution; however, Viterbi EM loses at most a few points of accuracy (3.7% at WSJ40), whereas classic EM drops nearly twenty points (19.1% at WSJ45). In both cases, the single best unsupervised result is with good initialization, althou</context>
<context position="13298" citStr="Spitkovsky et al., 2009" startWordPosition="2166" endWordPosition="2169">at the uniform-at-random initializer is a competitive starting M-step for Viterbi EM; our uninformed prior consists of uniform multinomials, seeding the E-step. faster than classic EM (see Figure 3(c,d)).4 4.2 Result #2: Smoothed Models Smoothing rarely helps classic EM and hurts in the case of oracle training (see Figure 4(a)). With Viterbi, supervised initialization suffers much less, the biased initializer is a wash, and the uninformed uniform prior generally gains a few points of accuracy, e.g., up 2.9% (from 42.4% to 45.2%, evaluated against WSJ40) at WSJ15 (see Figure 4(b)). Baby Steps (Spitkovsky et al., 2009) — iterative re-training with increasingly more complex data sets, WSJ1, ... ,WSJ45 — using smoothed Viterbi training fails miserably (see Figure 4(b)), due to Viterbi’s poor initial performance at short sentences (possibly because of data sparsity and sensitivity to non-sentences — see examples in §7.3). 4.3 Result #3: State-of-the-Art Models Simply training up smoothed Viterbi at WSJ15, using the uninformed uniform prior, yields 44.8% accuracy on Section 23 of WSJ&apos;, already beating previous state-of-the-art by 0.7% (see Table 1(A)). Since both classic EM and Ad-Hoc* initializers work well wi</context>
<context position="14583" citStr="Spitkovsky et al., 2009" startWordPosition="2360" endWordPosition="2363">heir pre-trained models to initialize Viterbi training, mixing the two strategies. We judged all Ad-Hoc* initializers against WSJ15 and found that the one for WSJ8 minimizes sentence-level cross-entropy (see Figure 5). This approach does not involve reference parse 4For classic EM, the number of iterations to convergence appears sometimes inversely related to performance, giving credence to the notion of early termination as a regularizer. 12 Model Incarnation WSJ10 WSJ20 WSJ° DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100 Less is More (Ad-Hoc* @15) (Spitkovsky et al., 2009) 56.2 48.2 44.1 43.3 A. Smoothed Viterbi Training (@15), Initialized with the Uniform Prior 59.9 50.0 44.8 48.1 B. A Good Initializer (Ad-Hoc*’s @8), Classically Pre-Trained (@15) 63.8 52.3 46.2 49.3 C. Smoothed Viterbi Training (@15), Initialized with B 64.4 53.5 47.8 50.5 D. Smoothed Viterbi Training (@45), Initialized with C 65.3 53.8 47.9 50.8 EVG Smoothed (skip-head), Lexicalized (Headden et al., 2009) 68.8 Table 1: Accuracies on Section 23 of WSJ{10, 20,&apos; } and Brown100 for three recent state-of-the-art systems, our initializer, and smoothed Viterbi-trained runs that employ different ini</context>
<context position="16167" citStr="Spitkovsky et al. (2009)" startWordPosition="2610" endWordPosition="2613">ood initializer boosts accuracy attained by smoothed Viterbi at WSJ15 to 47.8% (see Table 1(C)). Using its solution to re-initialize training at WSJ45 gives a tiny further improvement (0.1%) on Section 23 of WSJ&apos; but bigger gains on WSJ10 (0.9%) and WSJ20 (see Table 1(D)). Our results generalize. Gains due to smoothed Viterbi training and favorable initialization carry over to Brown100 — accuracy improves by 7.5% over previous published numbers (see Table 1).5 5 Discussion of Experimental Results The DMV has no parameters to capture syntactic relationships beyond local trees, e.g., agreement. Spitkovsky et al. (2009) suggest that classic EM breaks down as sentences get longer precisely because the model makes unwarranted independence assumptions. They hypothesize that the DMV reserves too much probability mass for what should be unlikely productions. Since EM faithfully allocates such re-distributions across the possible parse trees, once sentences grow sufficiently long, this process begins to deplete what began as likelier structures. But medium lengths avoid a flood of exponentially-confusing longer sentences (and 5In a sister paper, Spitkovsky et al. (2010) improve performance by incorporating parsing</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2009</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby Steps: How “Less is More” in unsupervised dependency parsing. In NIPS: Grammar Induction, Representation of Language and Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>D Jurafsky</author>
<author>H Alshawi</author>
</authors>
<title>Profiting from mark-up: Hyper-text annotations for guided parsing. In</title>
<date>2010</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="16722" citStr="Spitkovsky et al. (2010)" startWordPosition="2693" endWordPosition="2696">onships beyond local trees, e.g., agreement. Spitkovsky et al. (2009) suggest that classic EM breaks down as sentences get longer precisely because the model makes unwarranted independence assumptions. They hypothesize that the DMV reserves too much probability mass for what should be unlikely productions. Since EM faithfully allocates such re-distributions across the possible parse trees, once sentences grow sufficiently long, this process begins to deplete what began as likelier structures. But medium lengths avoid a flood of exponentially-confusing longer sentences (and 5In a sister paper, Spitkovsky et al. (2010) improve performance by incorporating parsing constraints harvested from the web into Viterbi training; nevertheless, results presented in this paper remain the best of models trained purely on WSJ. the sparseness of unrepresentative shorter ones).6 Our experiments corroborate this hypothesis. First of all, Viterbi manages to hang on to supervised solutions much better than classic EM. Second, Viterbi does not universally degrade with more (complex) training sets, except with a biased initializer. And third, Viterbi learns poorly from small data sets of short sentences (WSJk, k &lt; 5). Viterbi m</context>
</contexts>
<marker>Spitkovsky, Jurafsky, Alshawi, 2010</marker>
<rawString>V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010. Profiting from mark-up: Hyper-text annotations for guided parsing. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Wainwright</author>
</authors>
<title>Estimating the “wrong” graphical model: Benefits in the computation-limited setting.</title>
<date>2006</date>
<journal>Journal ofMachine Learning Research,</journal>
<volume>7</volume>
<contexts>
<context position="28466" citStr="Wainwright (2006)" startWordPosition="4772" endWordPosition="4773">iceably “worse” than ˜θ. Despite leaving zero probability to the truth, θ˜ beats θ* on both surrogate metrics, globally. This seems like an egregious error. Judged by (extrinsic) accuracy, θ˜ still holds its own: it gets four directed edges from predicting parse trees (i) and (ii) completely right, but none of (iii) — a solid 66.7%. Subject to tie-breaking, θ* is equally likely to get (i) and/or (iii) entirely right or totally wrong (they are indistinguishable): it could earn a perfect 16 One reason why Viterbi EM may work well is that its score is used in selecting actual output parse trees. Wainwright (2006) provided strong theoretical and empirical arguments for using the same approximate inference method in training as in performing predictions for a learned model. He showed that if inference involves an approximation, then using the same approximate method to train the model gives even better performance guarantees than exact training methods. If our task were not parsing but language modeling, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM Viterbi training is not only faster and more accurecursion constraints. It therefore invites more</context>
</contexts>
<marker>Wainwright, 2006</marker>
<rawString>M. J. Wainwright. 2006. Estimating the “wrong” graphical model: Benefits in the computation-limited setting. Journal ofMachine Learning Research, 7.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>