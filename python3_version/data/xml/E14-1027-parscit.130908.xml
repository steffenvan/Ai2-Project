<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001594">
<title confidence="0.993863">
Incremental Bayesian Learning of Semantic Categories
</title>
<author confidence="0.991235">
Lea Frermann and Mirella Lapata
</author>
<affiliation confidence="0.9993005">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.991815">
10 Crichton Street, Edinburgh EH8 9AB
</address>
<email confidence="0.997828">
l.frermann@ed.ac.uk, mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.994787" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999730444444445">
Models of category learning have been ex-
tensively studied in cognitive science and
primarily tested on perceptual abstractions
or artificial stimuli. In this paper we focus
on categories acquired from natural lan-
guage stimuli, that is words (e.g., chair is
a member of the FURNITURE category).
We present a Bayesian model which, un-
like previous work, learns both categories
and their features in a single process. Our
model employs particle filters, a sequential
Monte Carlo method commonly used for
approximate probabilistic inference in an
incremental setting. Comparison against
a state-of-the-art graph-based approach re-
veals that our model learns qualitatively
better categories and demonstrates cogni-
tive plausibility during learning.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986393442623">
Considerable psychological research has shown
that people reason about novel objects they en-
counter by identifying the category to which these
objects belong and extrapolating from their past
experiences with other members of that category
(Smith and Medin, 1981). Categorization is a clas-
sic problem in cognitive science, underlying a va-
riety of common mental tasks including percep-
tion, learning, and the use of language.
Given its fundamental nature, categorization
has been extensively studied both experimentally
and in simulations. Indeed, numerous models ex-
ist as to how humans categorize objects ranging
from strict prototypes (categories are represented
by a single idealized member which embodies
their core properties; e.g., Reed 1972) to full ex-
emplar models (categories are represented by a list
of previously encountered members; e.g., Nosof-
sky 1988) and combinations of the two (e.g., Grif-
fiths et al. 2007). A common feature across dif-
ferent studies is the use of stimuli involving real-
world objects (e.g., children’s toys; Starkey 1981),
perceptual abstractions (e.g., photographs of ani-
mals; Quinn and Eimas 1996), or artificial ones
(e.g., binary strings, dot patterns or geometric
shapes; Medin and Schaffer 1978; Posner and
Keele 1968; Bomba and Siqueland 1983). Most
existing models focus on adult categorization, in
which it is assumed that a large number of cate-
gories have already been learnt (but see Anderson
1991 and Griffiths et al. 2007 for exceptions).
In this work we focus on categories acquired
from natural language stimuli (i.e., words) and
investigate how the statistics of the linguistic en-
vironment (as approximated by large corpora) in-
fluence category formation (e.g., chair and ta-
ble are FURNITURE whereas peach and apple are
FRUIT1). The idea of modeling categories using
words as a stand-in for their referents has been
previously used to explore categorization-related
phenomena such as semantic priming (Cree et al.,
1999) and typicality rating (Voorspoels et al.,
2008), to evaluate prototype and exemplar mod-
els (Storms et al., 2000), and to simulate early lan-
guage category acquisition (Fountain and Lapata,
2011). The idea of using naturalistic corpora has
received little attention. Most existing studies use
feature norms as a proxy for people’s representa-
tion of semantic concepts. In a typical procedure,
participants are presented with a word and asked to
generate the most relevant features or attributes for
its referent concept. The most notable collection
of feature norms is probably the multi-year project
of McRae et al. (2005), which obtained features
for a set of 541 common English nouns.
Our approach replaces feature norms with rep-
resentations derived from words’ contexts in cor-
pora. While this is an impoverished view of how
categories are acquired — it is clear that they are
learnt through exposure to the linguistic environ-
ment and the physical world — perceptual infor-
</bodyText>
<footnote confidence="0.9925595">
1Throughout this paper we will use small caps to denote
CATEGORIES and italics for their members.
</footnote>
<page confidence="0.951794">
249
</page>
<note confidence="0.9932205">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 249–258,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99991870212766">
mation relevant for extracting semantic categories
is to a large extent redundantly encoded in linguis-
tic experience (Riordan and Jones, 2011). Besides,
there are known difficulties with feature norms
such as the small number of words for which these
can be obtained, the quality of the attributes, and
variability in the way people generate them (see
Zeigenfuse and Lee 2010 for details). Focusing
on natural language categories allows us to build
categorization models with theoretically unlimited
scope.
To this end, we present a probabilistic Bayesian
model of category acquisition based on the key
idea that learners can adaptively form category
representations that capture the structure ex-
pressed in the observed data. We model category
induction as two interrelated sub-problems: (a) the
acquisition of features that discriminate among
categories, and (b) the grouping of concepts into
categories based on those features. An important
modeling question concerns the exact mechanism
with which categories are learned. To maintain
cognitive plausibility, we develop an incremental
learning algorithm. Incrementality is a central as-
pect of human learning which takes place sequen-
tially and over time. Humans are capable of deal-
ing with a situation even if only partial information
is available. They adaptively learn as new infor-
mation is presented and locally update their inter-
nal knowledge state without systematically revis-
ing everything known about the situation at hand.
Memory and processing limitations also explain
why humans must learn incrementally. It is not
possible to store and have easy access to all the in-
formation one has been exposed to. It seems likely
that people store the most prominent facts and gen-
eralizations, which they modify on they fly when
new facts become available.
Our model learns categories using a particle fil-
ter, a Markov Chain Monte Carlo (MCMC) in-
ference mechanism which sequentially integrates
newly observed data and can be thus viewed as a
plausible proxy for human learning. Experimental
results show that the incremental learner obtains
meaningful categories which outperform the state
of the art whilst at the same time acquiring seman-
tic representations of words and their features.
</bodyText>
<sectionHeader confidence="0.999809" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999980915254238">
The problem of category induction has achieved
much attention in the cognitive science literature.
Incremental category learning was pioneered by
Anderson (1991) who develops a non-parametric
model able to induce categories from abstract
stimuli represented by binary features. Sanborn
et al. (2006) present a fully Bayesian adaptation of
Anderson’s original model, which yields a better
fit with behavioral data. A separate line of work
examines the cognitive characteristics of category
acquisition as well as the processes of generalizing
and generating new categories and exemplars (Jern
and Kemp, 2013; Kemp et al., 2012). The above
models are conceptually similar to ours. How-
ever, they were developed with adult categoriza-
tion in mind, and use rather simplistic categories
representing toy-domains. It is therefore not clear
whether they generalize to arbitrary stimuli and
data sizes. We aim to show that it is possible to ac-
quire natural language categories on a larger scale
purely from linguistic context.
Our model is loosely related to Bayesian mod-
els of word sense induction (Brody and Lapata,
2009; Yao and Durme, 2011). We also assume
that local linguistic context can provide important
cues for word meaning and by extension category
membership. However, the above models focus
on performance optimization and learn in an ideal
batch mode, while incorporating various kinds of
additional features such as part of speech tags or
dependencies. In contrast, we develop a cogni-
tively plausible (early) language learning model
and show that categories can be acquired purely
from context, as well as in an incremental fashion.
From a modeling perspective, we learn cate-
gories incrementally using a particle filtering al-
gorithm (Doucet et al., 2001). Particle filters are
a family of sequential Monte Carlo algorithms
which update the state space of a probabilistic
model with newly encountered information. They
have been successfully applied to natural lan-
guage acquisition tasks such as word segmentation
(Borschinger and Johnson, 2011), or sentence pro-
cessing (Levy et al., 2009). Sanborn et al. (2006)
also use particle filters for small-scale categoriza-
tion experiments with artificial stimuli. To the best
of our knowledge, we present the first particle fil-
tering algorithm for large-scale category acquisi-
tion from natural text.
Our work is closest to Fountain and Lapata
(2011) who also develop a model for inducing nat-
ural language categories. Specifically, they pro-
pose an incremental version of Chinese Whispers
(Biemann, 2006), a randomized graph-clustering
algorithm. The latter takes as input a graph which
is constructed from corpus-based co-occurrence
statistics and produces a hard clustering over the
nodes in the graph. Contrary to our model, they
treat the tasks of inferring a semantic representa-
</bodyText>
<page confidence="0.997368">
250
</page>
<figureCaption confidence="0.993799">
Figure 1: Plate diagram representation of the
BayesCat model.
</figureCaption>
<bodyText confidence="0.999924833333333">
tion for concepts and their class membership as
two separate processes. This allows to experi-
ment with different ways of initializing the co-
occurrence matrix (e.g., from bags of words or
a dependency parsed corpus), however at the ex-
pense of cognitive plausibility. It is unlikely that
humans have two entirely separate mechanisms
for learning the meaning of words and their cat-
egories. We formulate a more expressive model
within a probabilistic framework which captures
the meaning of words, their similarity, and the pre-
dictive power of their linguistic contexts.
</bodyText>
<sectionHeader confidence="0.996232" genericHeader="method">
3 The BayesCat Model
</sectionHeader>
<bodyText confidence="0.999108785714286">
In this section we present our Bayesian model of
category induction (BayesCat for short). The input
to the model is natural language text, and its final
output is a set of clusters representing categories
of semantic concepts found in the input data. Like
many other semantic models, BayesCat is inspired
by the distributional hypothesis which states that
a word’s meaning is predictable from its context
(Harris, 1954). By extension, we also assume that
contextual information can be used to character-
ize general semantic categories. Accordingly, the
input to our model is a corpus of documents, each
defined as a target word t centered in a fixed-length
context window:
</bodyText>
<equation confidence="0.927822">
[c−n ... c−1 t c1 ... cnI (1)
</equation>
<bodyText confidence="0.999897454545455">
We assume that there exists one global distribu-
tion over categories from which all documents are
generated. Each document is assigned a category
label, based on two types of features: the docu-
ment’s target word and its context words, which
are modeled through separate category-specific
distributions. We argue that it is important to dis-
tinguish between these features, since words be-
longing to the same category do not necessarily
co-occur, but tend to occur in the same contexts.
For example, the words polar bear and anteater
</bodyText>
<figure confidence="0.941104">
Draw distribution over categories θ ∼ Dir(α)
for category k do
Draw target word distribution φk ∼ Dir(β)
Draw context word distribution ψk ∼
Dir(γ)
for Document d do
Draw category zd ∼ Mult(θ)
Draw target word wdt ∼ Mult(φzd)
for context position n = {1..N} do
Draw context word wd,n
c ∼ Mult(ψzd)
</figure>
<figureCaption confidence="0.936308">
Figure 2: The generative process of the BayesCat
model.
</figureCaption>
<bodyText confidence="0.998473142857143">
are both members of the category ANIMAL. How-
ever, they rarely co-occur (in fact, a cursory search
using Google yields only three matches for the
query “polar bear * anteater”). Nevertheless, we
would expect to observe both words in similar
contexts since both animals eat, sleep, hunt, have
fur, four legs, and so on. This distinction con-
trasts our category acquisition task from the clas-
sical task of topic inference.
Figure 1 presents a plate diagram of the
BayesCat model; an overview of the generative
process is given in Figure 2. We first draw a global
category distribution θ from the Dirichlet distribu-
tion with parameter α. Next, for each category k,
we draw a distribution over target words φk from a
Dirichlet with parameter β and a distribution over
context words ψk from a Dirichlet with parame-
ter γ. For each document d, we draw a category zd,
then a target word, and N context words from the
category-specific distributions φzd and ψzd, respec-
tively.
</bodyText>
<sectionHeader confidence="0.982856" genericHeader="method">
4 Learning
</sectionHeader>
<bodyText confidence="0.9991398">
Our goal is to infer the joint distribution of
all hidden model parameters, and observable
data W. Since we use conjugate prior distributions
throughout the model, this joint distribution can be
simplified to:
</bodyText>
<equation confidence="0.9975574">
P(W,Z,θ,φ,ψ;α,β,γ) ∝
fir Γ(a4k +βr)
Γ (Er Nk + βr)
∏sΓ(&apos;k+γs) (2)
Γ(∑sV +γs),
</equation>
<bodyText confidence="0.9978565">
where r and s iterate over the target and context
word vocabulary, respectively, and the distribu-
</bodyText>
<figure confidence="0.869347041666667">
α
Dir Mult Mult Dir
θ
z
wt
wc
n
d
Mult
ψ
φ
k
k
Dir
β
γ
∏k Γ(N + αk)
×
Γ(∑kN+αk)
K
∏
k=1
K
× ∏
</figure>
<page confidence="0.6937225">
k=1
251
</page>
<bodyText confidence="0.999980941176471">
tions 0,�, and tV are integrated out and implic-
itly captured by the corresponding co-occurrence
counts N∗∗. I&apos;() denotes the Gamma function, a
generalization of the factorial to real numbers.
Since exact inference of the parameters of the
BayesCat model is intractable, we use sampling-
based approximate inference. Specifically, we
present two learning algorithms, namely a Gibbs
sampler and a particle filter.
The Gibbs Sampler Gibbs sampling is a well-
established approximate learning algorithm, based
on Markov Chain Monte Carlo methods (Geman
and Geman, 1984). It operates in batch-mode by
repeatedly iterating through all data points (doc-
uments in our case) and assigning the currently
sampled document d a category zd conditioned on
the current labelings of all other documents z−d:
</bodyText>
<equation confidence="0.99429">
zd — P(zd|z−d,W−d;a,0,Y), (3)
</equation>
<bodyText confidence="0.975339243243244">
using equation (2) but ignoring information
from the currently sampled document in all co-
occurrence counts.
The Gibbs sampler can be seen as an ideal
learner, which can view and revise any relevant
information at any time during learning. From a
cognitive perspective, this setting is implausible,
since a human language learner encounters train-
ing data incrementally and does not systematically
revisit previous learning decisions. Particle filters
are a class of incremental, or sequential, Monte
Carlo methods which can be used to model aspects
of the language learning process more naturally.
The Particle Filter Intuitively, a particle fil-
ter (henceforth PF) entertains a fixed set of
N weighted hypotheses (particles) based on pre-
vious training examples. Figure 3 shows an
overview of the particle filtering learning proce-
dure. At first, every particle of the PF is initialized
from a base distribution P0 (Initialization). Then a
single iteration over the input data y is performed,
during which the posterior distribution of each
data point yt under all current particles is com-
puted given information from all previously en-
countered data points yt−1 (Sampling/Prediction).
Crucially, each update is conditioned only on the
previous model state zt−1, which results in a con-
stant state space despite an increasing amount of
available data. A common problem with PF al-
gorithms is weight degeneration, i.e., one particle
tends to accumulate most of the weight. To avoid
this problem, at regular intervals the set of parti-
cles is resampled in order to discard particles with
for particle p do &gt; Initialization
Initialize randomly or from z0p — p0(z)
for observation t do
for particle n do ( &gt; Sampling(/&apos;Prediction
</bodyText>
<equation confidence="0.86869">
Pn(ztn|yt) — p(Ztn|ztn 1,a)P(yt|ztn,yt−11
zt « Mult({Pn(ztn)}Ni= 1) &gt; Resampling/
</equation>
<figureCaption confidence="0.999466">
Figure 3: The particle filtering procedure.
</figureCaption>
<bodyText confidence="0.999979333333333">
low probability and to ensure that the sample is
representative of the state space at any time (Re-
sampling).
This general algorithm can be straightforwardly
adapted to our learning problem (Griffiths et al.,
2011; Fearnhead, 2004). Each observation corre-
sponds to a document, which needs to be assigned
a category. To begin with, we assign the first ob-
served document to category 0 in all particles (Ini-
tialization). Then, we iterate once over the remain-
ing documents. For each particle n, we compute
a probability distribution over K categories based
on the simplified posterior distribution as defined
in equation (2) (Sampling/Prediction), with co-
occurrence counts based on the information from
all previously encountered documents. Thus, we
obtain a distribution over N · K possible assign-
ments. From this distribution we sample with
replacement N new particles, assign the current
document to the corresponding category (Resam-
pling), and proceed to the next input document.
</bodyText>
<sectionHeader confidence="0.998649" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9998602">
The goal of our experimental evaluation is to as-
sess the quality of the inferred clusters by compar-
ison to a gold standard and an existing graph-based
model of category acquisition. In addition, we are
interested in the incremental version of the model,
whether it is able to learn meaningful categories
and how these change over time. In the following,
we give details on the corpora we used, describe
how model parameters were selected, and explain
our evaluation procedure.
</bodyText>
<subsectionHeader confidence="0.924643">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999989875">
All our experiments were conducted on a lem-
matized version of the British National Corpus
(BNC). The corpus was further preprocessed by
removing stopwords and infrequent words (occur-
ring less than 800 times in the BNC).
The model output was evaluated against a gold
standard set of categories which was created by
collating the resources developed by Fountain and
</bodyText>
<page confidence="0.984845">
252
</page>
<bodyText confidence="0.99988">
Lapata (2010) and Vinson and Vigliocco (2008).
Both datasets contain a classification of nouns into
(possibly multiple) semantic categories produced
by human participants. We therefore assume that
they represent psychologically salient categories
which the cognitive system is in principle capable
of acquiring. After merging the two resources, and
removing duplicates we obtained 42 semantic cat-
egories for 555 nouns. We split this gold standard
into a development (41 categories, 492 nouns) and
a test set (16 categories, 196 nouns).2
The input to our model consists of short chunks
of text, namely a target word centered in a sym-
metric context window of five words (see (1)).
In our experiments, the set of target words corre-
sponds to the set of nouns in the evaluation dataset.
Target word mentions and their context are ex-
tracted from the BNC.
</bodyText>
<subsectionHeader confidence="0.9225">
5.2 Parameters for the BayesCat Model
</subsectionHeader>
<bodyText confidence="0.999995">
We optimized the hyperparameters of the
BayesCat model on the development set.
For the particle filter, the optimal values are
α = 0.7,b = 0.1,g = 0.1. We used the same
values for the Gibbs Sampler since it proved
insensitive to hyperparameter variations. We run
the Gibbs sampler for 200 iterations3 and report
results averaged over 10 runs. For the PF, we set
the number of particles to 500, and report final
scores averaged over 10 runs. For evaluation,
we take the clustering from the particle with the
highest weight 4.
</bodyText>
<subsectionHeader confidence="0.996291">
5.3 Model Comparison
</subsectionHeader>
<bodyText confidence="0.999767384615385">
Chinese Whispers We compared our approach
with Fountain and Lapata (2011) who present a
non-parametric graph-based model for category
acquisition. Their algorithm incrementally con-
structs a graph from co-occurrence counts of tar-
get words and their contexts (they use a symmetric
context window of five words). Target words con-
stitute the nodes of the graph, their co-occurrences
are transformed into a vector of positive PMI val-
ues, and graph edges correspond to the cosine sim-
ilarity between the PMI-vectors representing any
two nodes. They use Chinese Whispers (Biemann,
2006) to partition a graph into categories.
</bodyText>
<footnote confidence="0.94464725">
2The dataset is available from www.frermann.de/data.
3We checked for convergence on the development set.
4While in theory particles should be averaged, we found
that eventually they became highly similar — a common
problem known as sample impoverishment, which we plan to
tackle in the future. Nevertheless, diversity among particles
is present in the initial learning phase, when uncertainty is
greatest, so the model still benefits from multiple hypotheses.
</footnote>
<bodyText confidence="0.977933103448276">
We replicated the bag-of-words model pre-
sented in Fountain and Lapata (2011) and assessed
its performance on our training corpora and test
sets. The scores we report are averaged over 10
runs.
Chinese Whispers can only make hard cluster-
ing decisions, whereas the BayesCat model re-
turns a soft clustering of target nouns. In order
to be able to compare the two models, we con-
vert the soft clusters to hard clusters by assign-
ing each target word w to category c such that
P(wjc) · P(cjw).
LDA We also compared our model to a standard
topic model, namely Latent Dirichlet Allocation
(LDA; Blei et al. 2003). LDA assumes that a docu-
ment is generated from an individual mixture over
topics, and each topic is associated with one word
distribution. We trained a batch version of LDA
using input identical to our model and the Mallet
toolkit (McCallum, 2002).
Chinese Whispers is a parameter-free algorithm
and thus determines the number of clusters auto-
matically. While the Bayesian models presented
here are parametric in that an upper bound for the
potential number of categories needs to be speci-
fied, the models themselves decide on the specific
value of this number. We set the upper bound of
categories to 100 for LDA as well as the batch and
incremental version of the BayesCat model.
</bodyText>
<subsectionHeader confidence="0.936996">
5.4 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999988476190476">
Our aim is to learn a set of clusters each of which
corresponds to one gold category, i.e., it contains
all and only members of that gold category. We
report evaluation scores based on three metrics
which measure this tradeoff. Since in unsuper-
vised clustering the cluster IDs are meaningless,
all evaluation metrics involve a mapping from in-
duced clusters to gold categories. The first two
metrics described below perform a cluster-based
mapping and are thus not ideal for assessing the
output of soft clustering algorithms. The third
metric performs an item-based mapping and can
be directly used to evaluate soft clusters.
Purity/Collocation are based on member over-
lap between induced clusters and gold classes
(Lang and Lapata, 2011). Purity measures the de-
gree to which each cluster contains instances that
share the same gold class, while collocation mea-
sures the degree to which instances with the same
gold class are assigned to a single cluster. We re-
port the harmonic mean of purity and collocation
</bodyText>
<equation confidence="0.947797">
cat(w) = max
c
</equation>
<page confidence="0.991321">
253
</page>
<bodyText confidence="0.999442956521739">
as a single measure of clustering quality.
V-Measure is the harmonic mean between
homogeneity and collocation (Rosenberg and
Hirschberg, 2007). Like purity, V-Measure
performs cluster-based comparisons but is an
entropy-based method. It measures the condi-
tional entropy of a cluster given a class, and vice
versa.
Cluster-F1 is an item-based evaluation metric
which we propose drawing inspiration from the
supervised metric presented in Agirre and Soroa
(2007). Cluster-F1 maps each target word type to
a gold cluster based on its soft class membership,
and is thus appropriate for evaluation of soft clus-
tering output. We first create a K × G soft map-
ping matrix M from each induced category ki to
gold classes gj from P(gj|ki). We then map each
target word type to a gold class by multiplying
its probability distribution over soft clusters with
the mapping matrix M , and taking the maximum
value. Finally, we compute standard precision, re-
call and F1 between the mapped system categories
and the gold classes.
</bodyText>
<sectionHeader confidence="0.999915" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.999761567901235">
Our experiments are designed to answer three
questions: (1) How do the induced categories
fare against gold standard categories? (2) Are
there performance differences between BayesCat
and Chinese Whispers, given that the two models
adopt distinct mechanisms for representing lexical
meaning and learning semantic categories? (3) Is
our incremental learning mechanism cognitively
plausible? In other words, does the quality of the
induced clusters improve over time and how do the
learnt categories differ from the output of an ideal
batch learner?
Clustering performance for the batch BayesCat
model (BC-Batch), its incremental version
(BC-Inc), Chinese Whispers (CW), and LDA
is shown in Table 1. Comparison of the two
incremental models, namely BC-Inc and CW,
shows that our model outperforms CW under
all evaluation metrics both on the test and the
development set. Our BC models perform at
least as well as LDA, despite the more complex
learning objective. Recall that LDA does not learn
category specific features. BC-Batch performs
best overall, however this is not surprising. The
BayesCat model learnt in batch mode uses a Gibbs
sampler which can be viewed as an ideal learner
with access to the entire training data at any time,
and the ability to systematically revise previous
decisions. This puts the incremental variant at a
disadvantage since the particle filter encounters
the data incrementally and never resamples
previously seen documents. Nevertheless, as
shown in Table 1 BC-Inc’s performance is very
close to BC-Batch. BC-Inc outperforms the Gibbs
sampler in the PC-F1 metric, because it achieves
higher collocation scores. Inspection of the output
reveals that the Gibbs sampler induces larger clus-
ters compared to the particle filter (as well as less
distinct clusters). Although the general pattern of
results is the same on the development and test
sets, absolute scores for all systems are higher on
the test set. This is expected, since the test set
contains less categories with a smaller number of
exemplars and more accurate clusterings can be
thus achieved (on average) more easily.
Figure 4 displays the learning curves produced
by CW and BC-Inc under the PC-F1 (left) and
Cluster-F1 (right) evaluation metrics. Under
PC-F1, CW produces a very steep initial learning
curve which quickly flattens off, whereas no learn-
ing curve emerges for CW under Cluster-F1. The
BayesCat model exhibits more discernible learn-
ing curves under both metrics. We also observe
that learning curves for CW indicate much more
variance during learning compared to BC-Inc, ir-
respectively of the evaluation metric being used.
Figure 4b shows learning curves for BC-Inc when
its output classes are interpreted in two ways,
i.e., as soft or hard clusters. Interestingly, the two
curves have a similar shape which points to the
usefulness of Cluster-F1 as an evaluation metric
for both types of clusters.
In order to better understand the differences in
the learning process between CW and BC-Inc we
tracked the evolution of clusterings over time, as
well as the variance across cluster sizes at each
point in time. The results are plotted in Figure 5.
The top part of the figure compares the number
of clusters learnt by the two models. We see that
the number of clusters inferred by CW drops over
time, but is closer to the number of clusters present
in the gold standard. The final number of clus-
ters inferred by CW is 26, whereas PF-Inc infers
90 clusters (there are 41 gold classes). The mid-
dle plot shows the variance in cluster size induced
at any time by CW which is by orders of magni-
tude higher than the variance observed in the out-
put of BayesCat (bottom plot). More importantly,
the variance in BayesCat resembles the variance
present in the gold standard much more closely.
The clusterings learnt by CW tend to consist of
</bodyText>
<page confidence="0.99566">
254
</page>
<table confidence="0.999457166666667">
Metric LDA Development Set LDA Test Set BC-Batch
CW BC-Inc BC-Batch CW BC-Inc
PC-F1 (Hard) 0.283 0.211 0.283 0.261 0.446 0.380 0.503 0.413
V-Measure (Hard) 0.399 0.143 0.383 0.428 0.572 0.220 0.567 0.606
Cluster-F1 (Hard) 0.416 0.301 0.386 0.447 0.521 0.443 0.671 0.693
Cluster-F1 (Soft) 0.387 — 0.484 0.523 0.665 — 0.644 0.689
</table>
<tableCaption confidence="0.994642333333333">
Table 1: Evaluation of model output against a gold standard. Results are reported for the BayesCat model
trained incrementally (BC-Inc) and in batch mode (BC-Batch), and Chinese Whispers (CW). The type
of clusters being evaluated is shown within parentheses.
</tableCaption>
<figure confidence="0.999593857142857">
PC-F1
0.25
0.15
0.05
0.3
0.2
0.1
CW (Hard)
BC-Inc (Hard)
Cluster-F1
0.55
0.45
0.35
0.25
0.15
0.5
0.4
0.3
0.2
CW (Hard)
BC-Inc (Hard)
BC-Inc (Soft)
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
Number of encountered Documents
(a)
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
Number of encountered Documents
(b)
</figure>
<figureCaption confidence="0.881745">
Figure 4: Learning curves for BC-Inc and CW based on PC-F1 (left), and Cluster-F1 (right). The type
of clusters being evaluated is shown within parentheses. Results are reported on the development set.
</figureCaption>
<bodyText confidence="0.998695941176471">
few very large clusters and a large number of very
small (mostly singleton) clusters. Although some
of the bigger clusters are meaningful, the overall
structure of clusterings does not faithfully repre-
sent the gold standard.
Finally, note that in contrast to CW and LDA,
the BayesCat model learns not only how to in-
duce clusters of target words, but also informa-
tion about their category-specific contexts. Table 2
presents examples of the learnt categories together
with their most likely contexts. For example, one
of the categories our model discovers corresponds
to BUILDINGS. Some of the context words or fea-
tures relating to buildings refer to their location
(e.g., city, road, hill, north, park), architectural
style (e.g., modern, period, estate), and material
(e.g., stone).
</bodyText>
<sectionHeader confidence="0.997599" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999087">
In this paper we have presented a Bayesian model
of category acquisition. Our model learns to group
concepts into categories as well as their features
(i.e., context words associated with them). Cat-
egory learning is performed incrementally, using
a particle filtering algorithm which is a natural
choice for modeling sequential aspects of lan-
guage learning.
We now return to our initial questions and sum-
marize our findings. Firstly, we observe that
our incremental model learns plausible linguistic
categories when compared against the gold stan-
dard. Secondly, these categories are qualitatively
better when evaluated against Chinese Whispers,
a closely related graph-based incremental algo-
rithm. Thirdly, analysis of the model’s output
shows that it simulates category learning in two
important ways, it consistently improves over time
and can additionally acquire category features.
Overall, our model has a more cognitively plau-
sible learning mechanism compared to CW, and
is more expressive, as it can simulate both cat-
egory and feature learning. Although CW ulti-
mately yields some meaningful categories, it does
not acquire any knowledge pertaining to their fea-
tures. This is somewhat unrealistic given that hu-
mans are good at inferring missing features for
</bodyText>
<page confidence="0.992737">
255
</page>
<figure confidence="0.997552285714286">
Number of Clusters
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
Number of encountered Documents
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
Number of encountered Documents
0.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06
Number of encountered Documents
</figure>
<figureCaption confidence="0.961601666666667">
Figure 5: Number of clusters over time (top).
Cluster size variance for CW (middle) and BC-Inc
(bottom). Results shown on the development set.
</figureCaption>
<bodyText confidence="0.978795866666667">
unknown categories (Anderson, 1991). It is also
symptomatic of the nature of the algorithm which
does not have an explicit learning mechanism.
Each node in the graph iteratively adopts (in ran-
dom order) the strongest class in its neighborhood
(i.e., the set of nodes with which it shares an edge).
We also showed that LDA is less appropriate for
the category learning task on account of its for-
mulation which does not allow to simultaneously
acquire clusters and their features.
There are several options for improving our
model. The learning mechanism presented here
is the most basic of particle methods. A common
problem in particle filtering is sample impoverish-
ment, i.e., particles become highly similar after a
few iterations, and do not optimally represent the
sample space. More involved resampling methods
such as stratified sampling or residual resampling,
have been shown to alleviate this problem (Douc,
2005).
From a cognitive perspective, the most obvious
weakness of our algorithm is its strict incremen-
tality. While our model simulates human mem-
BUILDINGS
wall, bridge, building, cottage, gate, house, train,
bus, stone, chapel, brick, cathedral
plan, include, park, city, stone, building, ho-
tel, lead, road, hill, north, modern, visit, main,
period, cathedral, estate, complete, site, owner,
parish
</bodyText>
<sectionHeader confidence="0.787935" genericHeader="evaluation">
WEAPONS
</sectionHeader>
<bodyText confidence="0.9996934">
shotgun, pistol, knife, crowbar, gun, sledgeham-
mer, baton, bullet, motorcycle, van, ambulance
injure, ira, jail, yesterday, arrest, stolen, fire, of-
ficer, gun, police victim, hospital, steal, crash,
murder, incident, driver, accident, hit
</bodyText>
<sectionHeader confidence="0.907861" genericHeader="conclusions">
INSTRUMENTS
</sectionHeader>
<bodyText confidence="0.999104242424243">
tuba, drum, harmonica, bagpipe, harp, violin,
saxophone, rock, piano, banjo, guitar, flute, harp-
sichord, trumpet, rocker, clarinet, stereo, cello,
accordion
amp, orchestra, sound, electric, string, sing,
song, drum, piano, condition, album, instrument,
guitar, band, bass, music
Table 2: Examples of categories induced by the in-
cremental BayesCat model (upper row), together
with their most likely context words (lower row).
ory restrictions and uncertainty by learning based
on a limited number of current knowledge states
(i.e., particles), it never reconsiders past catego-
rization decisions. In many linguistic tasks, how-
ever, learners revisit past decisions (Frazier and
Rayner, 1982) and intuitively we would expect
categories to change based on novel evidence, es-
pecially in the early learning phase. In fixed-lag
smoothing, a particle smoothing variant, model
updates include systematic revision of a fixed set
of previous observations in the light of newly en-
countered evidence (Briers et al., 2010). Based
on this framework, we will investigate different
schemes for informed sequential learning.
Finally, we would like to compare the model’s
predictions against behavioral data, and exam-
ine more thoroughly how categories and features
evolve over time.
Acknowledgments We would like to thank
Charles Sutton and members of the ILCC at the
School of Informatics for their valuable feedback.
We acknowledge the support of EPSRC through
project grant EP/I037415/1.
</bodyText>
<figure confidence="0.998994961538462">
100
80
60
40
20
0
BC-Inc
CW
Gold
Variance
400
300
200
100
0
CW
Gold
Variance
18
15
12
9
6
3
BC-Inc
Gold
</figure>
<page confidence="0.994454">
256
</page>
<sectionHeader confidence="0.974089" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.842112916666667">
Agirre, Eneko and Aitor Soroa. 2007. Semeval-
2007 task 02: Evaluating word sense induc-
tion and discrimination systems. In Proceedings
of the 4th International Workshop on Semantic
Evaluations. Prague, Czech Republic, pages 7–
12.
Anderson, John R. 1991. The adaptive nature of
human categorization. Psychological Review
98:409–429.
Biemann, Chris. 2006. Chinese Whispers - an effi-
cient graph clustering algorithm and its applica-
tion to natural language processing problems. In
</bodyText>
<reference confidence="0.98952211235955">
Proceedings of TextGraphs: the 1st Workshop
on Graph Based Methods for Natural Language
Processing. New York City, pages 73–80.
Blei, David M., Andrew Y. Ng, and Michael I. Jor-
dan. 2003. Latent Dirichlet allocation. Journal
of Machine Learning Research 3:993–1022.
Bomba, Paul C. and Eimas R. Siqueland. 1983.
The nature and structure of infant form cate-
gories. Journal of Experimental Child Psychol-
ogy 35:294–328.
Borschinger, Benjamin and Mark Johnson. 2011.
A particle filter algorithm for Bayesian word
segmentation. In Proceedings of the Aus-
tralasian Language Technology Association
Workshop. Canberra, Australia, pages 10–18.
Briers, Mark, Arnaud Doucet, and Simon Maskell.
2010. Smoothing algorithms for state-space
models. Annals of the Institute of Statistical
Mathematics 62(1):61–89.
Brody, Samuel and Mirella Lapata. 2009.
Bayesian word sense induction. In Proceedings
of the 12th Conference of the European Chapter
of the ACL. Athens, Greece, pages 103–111.
Cree, George S., Ken McRae, and Chris McNor-
gan. 1999. An attractor model of lexical con-
ceptual processing: Simulating semantic prim-
ing. Cognitive Science 23(3):371–414.
Douc, Randal. 2005. Comparison of resampling
schemes for particle filtering. In 4th Interna-
tional Symposium on Image and Signal Pro-
cessing and Analysis. Zagreb, Croatia, pages
64–69.
Doucet, Arnaud, Nando de Freitas, and Neil Gor-
don. 2001. Sequential Monte Carlo Methods in
Practice. Springer, New York.
Fearnhead, Paul. 2004. Particle filters for mix-
ture models with an unknown number of com-
ponents. Statistics and Computing 14(1):11–21.
Fountain, Trevor and Mirella Lapata. 2010. Mean-
ing representation in natural language catego-
rization. In Proceedings of the 32nd Annual
Conference of the Cognitive Science Society.
Portland, Oregon, pages 1916–1921.
Fountain, Trevor and Mirella Lapata. 2011. In-
cremental models of natural language category
acquisition. In Proceedings of the 33nd An-
nual Conference of the Cognitive Science Soci-
ety. Boston, Massachusetts, pages 255–260.
Frazier, Lyn and Keith Rayner. 1982. Making and
correcting errors during sentence comprehen-
sion: Eye movements in the analysis of struc-
turally ambiguous sentences. Cognitive Psy-
chology 14(2):178–210.
Geman, Stuart and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions and the
Bayesian restoration of images. IEEE Trans-
actions on Pattern Analysis and Machine Intel-
ligence 6(6):721–741.
Griffiths, Thomas L., Kevin R. Canini, Adam N.
Sanborn, and Daniel J. Navarro. 2007. Unifying
rational models of categorization via the hierar-
chical Dirichlet process. In Proceedings of the
29th Annual Conference of the Cognitive Sci-
ence Society. Nashville, Tennessee, pages 323–
328.
Griffiths, Thomas L., Adam N. Sanborn, Kevin R.
Canini, John D. Navarro, and Joshua B. Tenen-
baum. 2011. Nonparametric Bayesian mod-
els of categorization. In Emmanuel M. Pothos
and Andy J. Wills, editors, Formal Approaches
in Categorization, Cambridge University Press,
pages 173–198.
Harris, Zellig. 1954. Distributional structure.
Word 10(23):146–162.
Jern, Alan and Charles Kemp. 2013. A proba-
bilistic account of exemplar and category gen-
eration. Cognitive Psychology 66:85–125.
Kemp, Charles, Patrick Shafto, and Joshua B.
Tenenbaum. 2012. An integrated account of
generalization across objects and features. Cog-
nitive Psychology 64:35–75.
Lang, Joel and Mirella Lapata. 2011. Unsuper-
vised semantic role induction with graph par-
titioning. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Edinburgh, Scotland, UK.,
pages 1320–1331.
Levy, Roger P., Florencia Reali, and Thomas L.
Griffiths. 2009. Modeling the effects of mem-
</reference>
<page confidence="0.959113">
257
</page>
<reference confidence="0.998632447368421">
ory on human online sentence processing with
particle filters. In D. Koller, D. Schuurmans,
Y. Bengio, and L. Bottou, editors, Advances
in Neural Information Processing Systems 21,
pages 937–944.
McCallum, Andrew Kachites. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
McRae, Ken, George S. Cree, Mark S. Seidenberg,
and Chris McNorgan. 2005. Semantic feature
production norms for a large set of living and
nonliving things. Behavioral Research Methods
37(4):547–59.
Medin, Douglas L. and Marguerite M. Schaffer.
1978. Context theory of classification learning.
Psychological Review 85(3):207–238.
Nosofsky, Robert M. 1988. Exemplar-based
accounts of relations between classification,
recognition, and typicality. Journal of Exper-
imental Psychology: Learning, Memory, and
Cognition 14:700–708.
Posner, Michael I. and Steven W. Keele. 1968. On
the genesis of abstract ideas. Journal of Exper-
imental Psychology 21:367–379.
Quinn, Paul C. and Peter D. Eimas. 1996. Percep-
tual cues that permit categorical differentiation
of animal species by infants. Journal of Exper-
imental Child Psychology 63:189–211.
Reed, Stephen K. 1972. Pattern recognition and
categorization. Cognitive psychology 3(3):382–
407.
Riordan, Brian and Michael N. Jones. 2011. Re-
dundancy in perceptual and linguistic experi-
ence: Comparing feature-based and distribu-
tional models of semantic representation. Top-
ics in Cognitive Science 3(2):303–345.
Rosenberg, Andrew and Julia Hirschberg. 2007.
V-measure: A conditional entropy-based ex-
ternal cluster evaluation measure. In Proceed-
ings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing
and Computational Natural Language Learn-
ing. Prague, Czech Republic, pages 410–420.
Sanborn, Adam N., Thomas L. Griffiths, and
Daniel J. Navarro. 2006. A more rational model
of categorization. In Proceedings of the 28th
Annual Conference of the Cognitive Science So-
ciety. Vancouver, Canada, pages 726–731.
Smith, Edward E. and Douglas L. Medin. 1981.
Categories and Concepts. Harvard University
Press, Cambridge, MA, USA.
Starkey, David. 1981. The origins of concept for-
mation: Object sorting and object preference in
early infancy. Child Development pages 489–
497.
Storms, Gert, Paul De Boeck, and Wim Ruts.
2000. Prototype and exemplar-based informa-
tion in natural language categories. Journal of
Memory and Language 42:51–73.
Vinson, David and Gabriella Vigliocco. 2008. Se-
mantic feature production norms for a large set
of objects and events. Behavior Research Meth-
ods 40(1):183–190.
Voorspoels, Wouter, Wolf Vanpaemel, and Gert
Storms. 2008. Exemplars and prototypes in
natural language concepts: A typicality-based
evaluation. Psychonomic Bulletin &amp; Review
15(3):630–637.
Yao, Xuchen and Benjamin Van Durme. 2011.
Nonparametric Bayesian word sense induc-
tion. In Proceedings of TextGraphs-6: Graph-
based Methods for Natural Language Process-
ing. Portland, Oregon, pages 10–14.
Zeigenfuse, Matthew D. and Michael D. Lee.
2010. Finding the features that represent stim-
uli. Acta Psychologica 133(3):283–295.
</reference>
<page confidence="0.996097">
258
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.763210">
<title confidence="0.993379">Incremental Bayesian Learning of Semantic Categories</title>
<author confidence="0.803529">Frermann</author>
<affiliation confidence="0.9898355">Institute for Language, Cognition and School of Informatics, University of</affiliation>
<address confidence="0.982804">10 Crichton Street, Edinburgh EH8</address>
<email confidence="0.998621">l.frermann@ed.ac.uk,mlap@inf.ed.ac.uk</email>
<abstract confidence="0.998232578947368">Models of category learning have been extensively studied in cognitive science and primarily tested on perceptual abstractions or artificial stimuli. In this paper we focus categories acquired from lanthat is words (e.g., member of the We present a Bayesian model which, unlike previous work, learns both categories and their features in a single process. Our model employs particle filters, a sequential Monte Carlo method commonly used for approximate probabilistic inference in an incremental setting. Comparison against a state-of-the-art graph-based approach reveals that our model learns qualitatively better categories and demonstrates cognitive plausibility during learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of TextGraphs: the 1st Workshop on Graph Based Methods for Natural Language Processing.</booktitle>
<pages>73--80</pages>
<location>New York City,</location>
<marker></marker>
<rawString>Proceedings of TextGraphs: the 1st Workshop on Graph Based Methods for Natural Language Processing. New York City, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<pages>3--993</pages>
<contexts>
<context position="20692" citStr="Blei et al. 2003" startWordPosition="3276" endWordPosition="3279">e hypotheses. We replicated the bag-of-words model presented in Fountain and Lapata (2011) and assessed its performance on our training corpora and test sets. The scores we report are averaged over 10 runs. Chinese Whispers can only make hard clustering decisions, whereas the BayesCat model returns a soft clustering of target nouns. In order to be able to compare the two models, we convert the soft clusters to hard clusters by assigning each target word w to category c such that P(wjc) · P(cjw). LDA We also compared our model to a standard topic model, namely Latent Dirichlet Allocation (LDA; Blei et al. 2003). LDA assumes that a document is generated from an individual mixture over topics, and each topic is associated with one word distribution. We trained a batch version of LDA using input identical to our model and the Mallet toolkit (McCallum, 2002). Chinese Whispers is a parameter-free algorithm and thus determines the number of clusters automatically. While the Bayesian models presented here are parametric in that an upper bound for the potential number of categories needs to be specified, the models themselves decide on the specific value of this number. We set the upper bound of categories </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul C Bomba</author>
<author>Eimas R Siqueland</author>
</authors>
<title>The nature and structure of infant form categories.</title>
<date>1983</date>
<journal>Journal of Experimental Child Psychology</journal>
<pages>35--294</pages>
<contexts>
<context position="2306" citStr="Bomba and Siqueland 1983" startWordPosition="333" endWordPosition="336">ented by a single idealized member which embodies their core properties; e.g., Reed 1972) to full exemplar models (categories are represented by a list of previously encountered members; e.g., Nosofsky 1988) and combinations of the two (e.g., Griffiths et al. 2007). A common feature across different studies is the use of stimuli involving realworld objects (e.g., children’s toys; Starkey 1981), perceptual abstractions (e.g., photographs of animals; Quinn and Eimas 1996), or artificial ones (e.g., binary strings, dot patterns or geometric shapes; Medin and Schaffer 1978; Posner and Keele 1968; Bomba and Siqueland 1983). Most existing models focus on adult categorization, in which it is assumed that a large number of categories have already been learnt (but see Anderson 1991 and Griffiths et al. 2007 for exceptions). In this work we focus on categories acquired from natural language stimuli (i.e., words) and investigate how the statistics of the linguistic environment (as approximated by large corpora) influence category formation (e.g., chair and table are FURNITURE whereas peach and apple are FRUIT1). The idea of modeling categories using words as a stand-in for their referents has been previously used to </context>
</contexts>
<marker>Bomba, Siqueland, 1983</marker>
<rawString>Bomba, Paul C. and Eimas R. Siqueland. 1983. The nature and structure of infant form categories. Journal of Experimental Child Psychology 35:294–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Borschinger</author>
<author>Mark Johnson</author>
</authors>
<title>A particle filter algorithm for Bayesian word segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop.</booktitle>
<pages>10--18</pages>
<location>Canberra, Australia,</location>
<contexts>
<context position="8580" citStr="Borschinger and Johnson, 2011" startWordPosition="1305" endWordPosition="1308">s part of speech tags or dependencies. In contrast, we develop a cognitively plausible (early) language learning model and show that categories can be acquired purely from context, as well as in an incremental fashion. From a modeling perspective, we learn categories incrementally using a particle filtering algorithm (Doucet et al., 2001). Particle filters are a family of sequential Monte Carlo algorithms which update the state space of a probabilistic model with newly encountered information. They have been successfully applied to natural language acquisition tasks such as word segmentation (Borschinger and Johnson, 2011), or sentence processing (Levy et al., 2009). Sanborn et al. (2006) also use particle filters for small-scale categorization experiments with artificial stimuli. To the best of our knowledge, we present the first particle filtering algorithm for large-scale category acquisition from natural text. Our work is closest to Fountain and Lapata (2011) who also develop a model for inducing natural language categories. Specifically, they propose an incremental version of Chinese Whispers (Biemann, 2006), a randomized graph-clustering algorithm. The latter takes as input a graph which is constructed fr</context>
</contexts>
<marker>Borschinger, Johnson, 2011</marker>
<rawString>Borschinger, Benjamin and Mark Johnson. 2011. A particle filter algorithm for Bayesian word segmentation. In Proceedings of the Australasian Language Technology Association Workshop. Canberra, Australia, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Briers</author>
<author>Arnaud Doucet</author>
<author>Simon Maskell</author>
</authors>
<title>Smoothing algorithms for state-space models.</title>
<date>2010</date>
<journal>Annals of the Institute of Statistical Mathematics</journal>
<volume>62</volume>
<issue>1</issue>
<contexts>
<context position="33407" citStr="Briers et al., 2010" startWordPosition="5285" endWordPosition="5288">ost likely context words (lower row). ory restrictions and uncertainty by learning based on a limited number of current knowledge states (i.e., particles), it never reconsiders past categorization decisions. In many linguistic tasks, however, learners revisit past decisions (Frazier and Rayner, 1982) and intuitively we would expect categories to change based on novel evidence, especially in the early learning phase. In fixed-lag smoothing, a particle smoothing variant, model updates include systematic revision of a fixed set of previous observations in the light of newly encountered evidence (Briers et al., 2010). Based on this framework, we will investigate different schemes for informed sequential learning. Finally, we would like to compare the model’s predictions against behavioral data, and examine more thoroughly how categories and features evolve over time. Acknowledgments We would like to thank Charles Sutton and members of the ILCC at the School of Informatics for their valuable feedback. We acknowledge the support of EPSRC through project grant EP/I037415/1. 100 80 60 40 20 0 BC-Inc CW Gold Variance 400 300 200 100 0 CW Gold Variance 18 15 12 9 6 3 BC-Inc Gold 256 References Agirre, Eneko and</context>
</contexts>
<marker>Briers, Doucet, Maskell, 2010</marker>
<rawString>Briers, Mark, Arnaud Doucet, and Simon Maskell. 2010. Smoothing algorithms for state-space models. Annals of the Institute of Statistical Mathematics 62(1):61–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL.</booktitle>
<pages>103--111</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="7642" citStr="Brody and Lapata, 2009" startWordPosition="1162" endWordPosition="1165">cquisition as well as the processes of generalizing and generating new categories and exemplars (Jern and Kemp, 2013; Kemp et al., 2012). The above models are conceptually similar to ours. However, they were developed with adult categorization in mind, and use rather simplistic categories representing toy-domains. It is therefore not clear whether they generalize to arbitrary stimuli and data sizes. We aim to show that it is possible to acquire natural language categories on a larger scale purely from linguistic context. Our model is loosely related to Bayesian models of word sense induction (Brody and Lapata, 2009; Yao and Durme, 2011). We also assume that local linguistic context can provide important cues for word meaning and by extension category membership. However, the above models focus on performance optimization and learn in an ideal batch mode, while incorporating various kinds of additional features such as part of speech tags or dependencies. In contrast, we develop a cognitively plausible (early) language learning model and show that categories can be acquired purely from context, as well as in an incremental fashion. From a modeling perspective, we learn categories incrementally using a pa</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Brody, Samuel and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference of the European Chapter of the ACL. Athens, Greece, pages 103–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George S Cree</author>
<author>Ken McRae</author>
<author>Chris McNorgan</author>
</authors>
<title>An attractor model of lexical conceptual processing: Simulating semantic priming.</title>
<date>1999</date>
<journal>Cognitive Science</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2991" citStr="Cree et al., 1999" startWordPosition="442" endWordPosition="445">s assumed that a large number of categories have already been learnt (but see Anderson 1991 and Griffiths et al. 2007 for exceptions). In this work we focus on categories acquired from natural language stimuli (i.e., words) and investigate how the statistics of the linguistic environment (as approximated by large corpora) influence category formation (e.g., chair and table are FURNITURE whereas peach and apple are FRUIT1). The idea of modeling categories using words as a stand-in for their referents has been previously used to explore categorization-related phenomena such as semantic priming (Cree et al., 1999) and typicality rating (Voorspoels et al., 2008), to evaluate prototype and exemplar models (Storms et al., 2000), and to simulate early language category acquisition (Fountain and Lapata, 2011). The idea of using naturalistic corpora has received little attention. Most existing studies use feature norms as a proxy for people’s representation of semantic concepts. In a typical procedure, participants are presented with a word and asked to generate the most relevant features or attributes for its referent concept. The most notable collection of feature norms is probably the multi-year project o</context>
</contexts>
<marker>Cree, McRae, McNorgan, 1999</marker>
<rawString>Cree, George S., Ken McRae, and Chris McNorgan. 1999. An attractor model of lexical conceptual processing: Simulating semantic priming. Cognitive Science 23(3):371–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randal Douc</author>
</authors>
<title>Comparison of resampling schemes for particle filtering.</title>
<date>2005</date>
<booktitle>In 4th International Symposium on Image and Signal Processing and Analysis.</booktitle>
<pages>64--69</pages>
<location>Zagreb, Croatia,</location>
<contexts>
<context position="31747" citStr="Douc, 2005" startWordPosition="5049" endWordPosition="5050">d that LDA is less appropriate for the category learning task on account of its formulation which does not allow to simultaneously acquire clusters and their features. There are several options for improving our model. The learning mechanism presented here is the most basic of particle methods. A common problem in particle filtering is sample impoverishment, i.e., particles become highly similar after a few iterations, and do not optimally represent the sample space. More involved resampling methods such as stratified sampling or residual resampling, have been shown to alleviate this problem (Douc, 2005). From a cognitive perspective, the most obvious weakness of our algorithm is its strict incrementality. While our model simulates human memBUILDINGS wall, bridge, building, cottage, gate, house, train, bus, stone, chapel, brick, cathedral plan, include, park, city, stone, building, hotel, lead, road, hill, north, modern, visit, main, period, cathedral, estate, complete, site, owner, parish WEAPONS shotgun, pistol, knife, crowbar, gun, sledgehammer, baton, bullet, motorcycle, van, ambulance injure, ira, jail, yesterday, arrest, stolen, fire, officer, gun, police victim, hospital, steal, crash,</context>
</contexts>
<marker>Douc, 2005</marker>
<rawString>Douc, Randal. 2005. Comparison of resampling schemes for particle filtering. In 4th International Symposium on Image and Signal Processing and Analysis. Zagreb, Croatia, pages 64–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnaud Doucet</author>
<author>Nando de Freitas</author>
<author>Neil Gordon</author>
</authors>
<title>Sequential Monte Carlo Methods in Practice.</title>
<date>2001</date>
<publisher>Springer,</publisher>
<location>New York.</location>
<marker>Doucet, de Freitas, Gordon, 2001</marker>
<rawString>Doucet, Arnaud, Nando de Freitas, and Neil Gordon. 2001. Sequential Monte Carlo Methods in Practice. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Fearnhead</author>
</authors>
<title>Particle filters for mixture models with an unknown number of components.</title>
<date>2004</date>
<journal>Statistics and Computing</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="15955" citStr="Fearnhead, 2004" startWordPosition="2510" endWordPosition="2511">eight. To avoid this problem, at regular intervals the set of particles is resampled in order to discard particles with for particle p do &gt; Initialization Initialize randomly or from z0p — p0(z) for observation t do for particle n do ( &gt; Sampling(/&apos;Prediction Pn(ztn|yt) — p(Ztn|ztn 1,a)P(yt|ztn,yt−11 zt « Mult({Pn(ztn)}Ni= 1) &gt; Resampling/ Figure 3: The particle filtering procedure. low probability and to ensure that the sample is representative of the state space at any time (Resampling). This general algorithm can be straightforwardly adapted to our learning problem (Griffiths et al., 2011; Fearnhead, 2004). Each observation corresponds to a document, which needs to be assigned a category. To begin with, we assign the first observed document to category 0 in all particles (Initialization). Then, we iterate once over the remaining documents. For each particle n, we compute a probability distribution over K categories based on the simplified posterior distribution as defined in equation (2) (Sampling/Prediction), with cooccurrence counts based on the information from all previously encountered documents. Thus, we obtain a distribution over N · K possible assignments. From this distribution we samp</context>
</contexts>
<marker>Fearnhead, 2004</marker>
<rawString>Fearnhead, Paul. 2004. Particle filters for mixture models with an unknown number of components. Statistics and Computing 14(1):11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Fountain</author>
<author>Mirella Lapata</author>
</authors>
<title>Meaning representation in natural language categorization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 32nd Annual Conference of the Cognitive Science Society.</booktitle>
<pages>1916--1921</pages>
<location>Portland, Oregon,</location>
<marker>Fountain, Lapata, 2010</marker>
<rawString>Fountain, Trevor and Mirella Lapata. 2010. Meaning representation in natural language categorization. In Proceedings of the 32nd Annual Conference of the Cognitive Science Society. Portland, Oregon, pages 1916–1921.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Fountain</author>
<author>Mirella Lapata</author>
</authors>
<title>Incremental models of natural language category acquisition.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33nd Annual Conference of the Cognitive Science Society.</booktitle>
<pages>255--260</pages>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="3185" citStr="Fountain and Lapata, 2011" startWordPosition="472" endWordPosition="475">atural language stimuli (i.e., words) and investigate how the statistics of the linguistic environment (as approximated by large corpora) influence category formation (e.g., chair and table are FURNITURE whereas peach and apple are FRUIT1). The idea of modeling categories using words as a stand-in for their referents has been previously used to explore categorization-related phenomena such as semantic priming (Cree et al., 1999) and typicality rating (Voorspoels et al., 2008), to evaluate prototype and exemplar models (Storms et al., 2000), and to simulate early language category acquisition (Fountain and Lapata, 2011). The idea of using naturalistic corpora has received little attention. Most existing studies use feature norms as a proxy for people’s representation of semantic concepts. In a typical procedure, participants are presented with a word and asked to generate the most relevant features or attributes for its referent concept. The most notable collection of feature norms is probably the multi-year project of McRae et al. (2005), which obtained features for a set of 541 common English nouns. Our approach replaces feature norms with representations derived from words’ contexts in corpora. While this</context>
<context position="8927" citStr="Fountain and Lapata (2011)" startWordPosition="1360" endWordPosition="1363">filters are a family of sequential Monte Carlo algorithms which update the state space of a probabilistic model with newly encountered information. They have been successfully applied to natural language acquisition tasks such as word segmentation (Borschinger and Johnson, 2011), or sentence processing (Levy et al., 2009). Sanborn et al. (2006) also use particle filters for small-scale categorization experiments with artificial stimuli. To the best of our knowledge, we present the first particle filtering algorithm for large-scale category acquisition from natural text. Our work is closest to Fountain and Lapata (2011) who also develop a model for inducing natural language categories. Specifically, they propose an incremental version of Chinese Whispers (Biemann, 2006), a randomized graph-clustering algorithm. The latter takes as input a graph which is constructed from corpus-based co-occurrence statistics and produces a hard clustering over the nodes in the graph. Contrary to our model, they treat the tasks of inferring a semantic representa250 Figure 1: Plate diagram representation of the BayesCat model. tion for concepts and their class membership as two separate processes. This allows to experiment with</context>
<context position="19085" citStr="Fountain and Lapata (2011)" startWordPosition="3017" endWordPosition="3020"> Model We optimized the hyperparameters of the BayesCat model on the development set. For the particle filter, the optimal values are α = 0.7,b = 0.1,g = 0.1. We used the same values for the Gibbs Sampler since it proved insensitive to hyperparameter variations. We run the Gibbs sampler for 200 iterations3 and report results averaged over 10 runs. For the PF, we set the number of particles to 500, and report final scores averaged over 10 runs. For evaluation, we take the clustering from the particle with the highest weight 4. 5.3 Model Comparison Chinese Whispers We compared our approach with Fountain and Lapata (2011) who present a non-parametric graph-based model for category acquisition. Their algorithm incrementally constructs a graph from co-occurrence counts of target words and their contexts (they use a symmetric context window of five words). Target words constitute the nodes of the graph, their co-occurrences are transformed into a vector of positive PMI values, and graph edges correspond to the cosine similarity between the PMI-vectors representing any two nodes. They use Chinese Whispers (Biemann, 2006) to partition a graph into categories. 2The dataset is available from www.frermann.de/data. 3We</context>
</contexts>
<marker>Fountain, Lapata, 2011</marker>
<rawString>Fountain, Trevor and Mirella Lapata. 2011. Incremental models of natural language category acquisition. In Proceedings of the 33nd Annual Conference of the Cognitive Science Society. Boston, Massachusetts, pages 255–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
<author>Keith Rayner</author>
</authors>
<title>Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences.</title>
<date>1982</date>
<journal>Cognitive Psychology</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="33088" citStr="Frazier and Rayner, 1982" startWordPosition="5235" endWordPosition="5238">piano, banjo, guitar, flute, harpsichord, trumpet, rocker, clarinet, stereo, cello, accordion amp, orchestra, sound, electric, string, sing, song, drum, piano, condition, album, instrument, guitar, band, bass, music Table 2: Examples of categories induced by the incremental BayesCat model (upper row), together with their most likely context words (lower row). ory restrictions and uncertainty by learning based on a limited number of current knowledge states (i.e., particles), it never reconsiders past categorization decisions. In many linguistic tasks, however, learners revisit past decisions (Frazier and Rayner, 1982) and intuitively we would expect categories to change based on novel evidence, especially in the early learning phase. In fixed-lag smoothing, a particle smoothing variant, model updates include systematic revision of a fixed set of previous observations in the light of newly encountered evidence (Briers et al., 2010). Based on this framework, we will investigate different schemes for informed sequential learning. Finally, we would like to compare the model’s predictions against behavioral data, and examine more thoroughly how categories and features evolve over time. Acknowledgments We would </context>
</contexts>
<marker>Frazier, Rayner, 1982</marker>
<rawString>Frazier, Lyn and Keith Rayner. 1982. Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. Cognitive Psychology 14(2):178–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Donald Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence</journal>
<volume>6</volume>
<issue>6</issue>
<contexts>
<context position="13624" citStr="Geman and Geman, 1984" startWordPosition="2143" endWordPosition="2146">φ k k Dir β γ ∏k Γ(N + αk) × Γ(∑kN+αk) K ∏ k=1 K × ∏ k=1 251 tions 0,�, and tV are integrated out and implicitly captured by the corresponding co-occurrence counts N∗∗. I&apos;() denotes the Gamma function, a generalization of the factorial to real numbers. Since exact inference of the parameters of the BayesCat model is intractable, we use samplingbased approximate inference. Specifically, we present two learning algorithms, namely a Gibbs sampler and a particle filter. The Gibbs Sampler Gibbs sampling is a wellestablished approximate learning algorithm, based on Markov Chain Monte Carlo methods (Geman and Geman, 1984). It operates in batch-mode by repeatedly iterating through all data points (documents in our case) and assigning the currently sampled document d a category zd conditioned on the current labelings of all other documents z−d: zd — P(zd|z−d,W−d;a,0,Y), (3) using equation (2) but ignoring information from the currently sampled document in all cooccurrence counts. The Gibbs sampler can be seen as an ideal learner, which can view and revise any relevant information at any time during learning. From a cognitive perspective, this setting is implausible, since a human language learner encounters trai</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>Geman, Stuart and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence 6(6):721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Kevin R Canini</author>
<author>Adam N Sanborn</author>
<author>Daniel J Navarro</author>
</authors>
<title>Unifying rational models of categorization via the hierarchical Dirichlet process.</title>
<date>2007</date>
<booktitle>In Proceedings of the 29th Annual Conference of the Cognitive Science Society.</booktitle>
<pages>323--328</pages>
<location>Nashville, Tennessee,</location>
<contexts>
<context position="1946" citStr="Griffiths et al. 2007" startWordPosition="277" endWordPosition="281"> in cognitive science, underlying a variety of common mental tasks including perception, learning, and the use of language. Given its fundamental nature, categorization has been extensively studied both experimentally and in simulations. Indeed, numerous models exist as to how humans categorize objects ranging from strict prototypes (categories are represented by a single idealized member which embodies their core properties; e.g., Reed 1972) to full exemplar models (categories are represented by a list of previously encountered members; e.g., Nosofsky 1988) and combinations of the two (e.g., Griffiths et al. 2007). A common feature across different studies is the use of stimuli involving realworld objects (e.g., children’s toys; Starkey 1981), perceptual abstractions (e.g., photographs of animals; Quinn and Eimas 1996), or artificial ones (e.g., binary strings, dot patterns or geometric shapes; Medin and Schaffer 1978; Posner and Keele 1968; Bomba and Siqueland 1983). Most existing models focus on adult categorization, in which it is assumed that a large number of categories have already been learnt (but see Anderson 1991 and Griffiths et al. 2007 for exceptions). In this work we focus on categories ac</context>
</contexts>
<marker>Griffiths, Canini, Sanborn, Navarro, 2007</marker>
<rawString>Griffiths, Thomas L., Kevin R. Canini, Adam N. Sanborn, and Daniel J. Navarro. 2007. Unifying rational models of categorization via the hierarchical Dirichlet process. In Proceedings of the 29th Annual Conference of the Cognitive Science Society. Nashville, Tennessee, pages 323– 328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Adam N Sanborn</author>
<author>Kevin R Canini</author>
<author>John D Navarro</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Nonparametric Bayesian models of categorization.</title>
<date>2011</date>
<pages>173--198</pages>
<editor>In Emmanuel M. Pothos and Andy J. Wills, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="15937" citStr="Griffiths et al., 2011" startWordPosition="2506" endWordPosition="2509">accumulate most of the weight. To avoid this problem, at regular intervals the set of particles is resampled in order to discard particles with for particle p do &gt; Initialization Initialize randomly or from z0p — p0(z) for observation t do for particle n do ( &gt; Sampling(/&apos;Prediction Pn(ztn|yt) — p(Ztn|ztn 1,a)P(yt|ztn,yt−11 zt « Mult({Pn(ztn)}Ni= 1) &gt; Resampling/ Figure 3: The particle filtering procedure. low probability and to ensure that the sample is representative of the state space at any time (Resampling). This general algorithm can be straightforwardly adapted to our learning problem (Griffiths et al., 2011; Fearnhead, 2004). Each observation corresponds to a document, which needs to be assigned a category. To begin with, we assign the first observed document to category 0 in all particles (Initialization). Then, we iterate once over the remaining documents. For each particle n, we compute a probability distribution over K categories based on the simplified posterior distribution as defined in equation (2) (Sampling/Prediction), with cooccurrence counts based on the information from all previously encountered documents. Thus, we obtain a distribution over N · K possible assignments. From this di</context>
</contexts>
<marker>Griffiths, Sanborn, Canini, Navarro, Tenenbaum, 2011</marker>
<rawString>Griffiths, Thomas L., Adam N. Sanborn, Kevin R. Canini, John D. Navarro, and Joshua B. Tenenbaum. 2011. Nonparametric Bayesian models of categorization. In Emmanuel M. Pothos and Andy J. Wills, editors, Formal Approaches in Categorization, Cambridge University Press, pages 173–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1954</date>
<journal>Word</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="10432" citStr="Harris, 1954" startWordPosition="1598" endWordPosition="1599">ulate a more expressive model within a probabilistic framework which captures the meaning of words, their similarity, and the predictive power of their linguistic contexts. 3 The BayesCat Model In this section we present our Bayesian model of category induction (BayesCat for short). The input to the model is natural language text, and its final output is a set of clusters representing categories of semantic concepts found in the input data. Like many other semantic models, BayesCat is inspired by the distributional hypothesis which states that a word’s meaning is predictable from its context (Harris, 1954). By extension, we also assume that contextual information can be used to characterize general semantic categories. Accordingly, the input to our model is a corpus of documents, each defined as a target word t centered in a fixed-length context window: [c−n ... c−1 t c1 ... cnI (1) We assume that there exists one global distribution over categories from which all documents are generated. Each document is assigned a category label, based on two types of features: the document’s target word and its context words, which are modeled through separate category-specific distributions. We argue that i</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Harris, Zellig. 1954. Distributional structure. Word 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Jern</author>
<author>Charles Kemp</author>
</authors>
<title>A probabilistic account of exemplar and category generation.</title>
<date>2013</date>
<journal>Cognitive Psychology</journal>
<pages>66--85</pages>
<contexts>
<context position="7136" citStr="Jern and Kemp, 2013" startWordPosition="1079" endWordPosition="1082">Related Work The problem of category induction has achieved much attention in the cognitive science literature. Incremental category learning was pioneered by Anderson (1991) who develops a non-parametric model able to induce categories from abstract stimuli represented by binary features. Sanborn et al. (2006) present a fully Bayesian adaptation of Anderson’s original model, which yields a better fit with behavioral data. A separate line of work examines the cognitive characteristics of category acquisition as well as the processes of generalizing and generating new categories and exemplars (Jern and Kemp, 2013; Kemp et al., 2012). The above models are conceptually similar to ours. However, they were developed with adult categorization in mind, and use rather simplistic categories representing toy-domains. It is therefore not clear whether they generalize to arbitrary stimuli and data sizes. We aim to show that it is possible to acquire natural language categories on a larger scale purely from linguistic context. Our model is loosely related to Bayesian models of word sense induction (Brody and Lapata, 2009; Yao and Durme, 2011). We also assume that local linguistic context can provide important cue</context>
</contexts>
<marker>Jern, Kemp, 2013</marker>
<rawString>Jern, Alan and Charles Kemp. 2013. A probabilistic account of exemplar and category generation. Cognitive Psychology 66:85–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Kemp</author>
<author>Patrick Shafto</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>An integrated account of generalization across objects and features.</title>
<date>2012</date>
<journal>Cognitive Psychology</journal>
<pages>64--35</pages>
<contexts>
<context position="7156" citStr="Kemp et al., 2012" startWordPosition="1083" endWordPosition="1086">lem of category induction has achieved much attention in the cognitive science literature. Incremental category learning was pioneered by Anderson (1991) who develops a non-parametric model able to induce categories from abstract stimuli represented by binary features. Sanborn et al. (2006) present a fully Bayesian adaptation of Anderson’s original model, which yields a better fit with behavioral data. A separate line of work examines the cognitive characteristics of category acquisition as well as the processes of generalizing and generating new categories and exemplars (Jern and Kemp, 2013; Kemp et al., 2012). The above models are conceptually similar to ours. However, they were developed with adult categorization in mind, and use rather simplistic categories representing toy-domains. It is therefore not clear whether they generalize to arbitrary stimuli and data sizes. We aim to show that it is possible to acquire natural language categories on a larger scale purely from linguistic context. Our model is loosely related to Bayesian models of word sense induction (Brody and Lapata, 2009; Yao and Durme, 2011). We also assume that local linguistic context can provide important cues for word meaning a</context>
</contexts>
<marker>Kemp, Shafto, Tenenbaum, 2012</marker>
<rawString>Kemp, Charles, Patrick Shafto, and Joshua B. Tenenbaum. 2012. An integrated account of generalization across objects and features. Cognitive Psychology 64:35–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised semantic role induction with graph partitioning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>1320--1331</pages>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="22136" citStr="Lang and Lapata, 2011" startWordPosition="3513" endWordPosition="3516"> members of that gold category. We report evaluation scores based on three metrics which measure this tradeoff. Since in unsupervised clustering the cluster IDs are meaningless, all evaluation metrics involve a mapping from induced clusters to gold categories. The first two metrics described below perform a cluster-based mapping and are thus not ideal for assessing the output of soft clustering algorithms. The third metric performs an item-based mapping and can be directly used to evaluate soft clusters. Purity/Collocation are based on member overlap between induced clusters and gold classes (Lang and Lapata, 2011). Purity measures the degree to which each cluster contains instances that share the same gold class, while collocation measures the degree to which instances with the same gold class are assigned to a single cluster. We report the harmonic mean of purity and collocation cat(w) = max c 253 as a single measure of clustering quality. V-Measure is the harmonic mean between homogeneity and collocation (Rosenberg and Hirschberg, 2007). Like purity, V-Measure performs cluster-based comparisons but is an entropy-based method. It measures the conditional entropy of a cluster given a class, and vice ve</context>
</contexts>
<marker>Lang, Lapata, 2011</marker>
<rawString>Lang, Joel and Mirella Lapata. 2011. Unsupervised semantic role induction with graph partitioning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Edinburgh, Scotland, UK., pages 1320–1331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger P Levy</author>
<author>Florencia Reali</author>
<author>Thomas L Griffiths</author>
</authors>
<title>Modeling the effects of memory on human online sentence processing with particle filters.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems 21,</booktitle>
<pages>937--944</pages>
<editor>In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors,</editor>
<contexts>
<context position="8624" citStr="Levy et al., 2009" startWordPosition="1313" endWordPosition="1316">evelop a cognitively plausible (early) language learning model and show that categories can be acquired purely from context, as well as in an incremental fashion. From a modeling perspective, we learn categories incrementally using a particle filtering algorithm (Doucet et al., 2001). Particle filters are a family of sequential Monte Carlo algorithms which update the state space of a probabilistic model with newly encountered information. They have been successfully applied to natural language acquisition tasks such as word segmentation (Borschinger and Johnson, 2011), or sentence processing (Levy et al., 2009). Sanborn et al. (2006) also use particle filters for small-scale categorization experiments with artificial stimuli. To the best of our knowledge, we present the first particle filtering algorithm for large-scale category acquisition from natural text. Our work is closest to Fountain and Lapata (2011) who also develop a model for inducing natural language categories. Specifically, they propose an incremental version of Chinese Whispers (Biemann, 2006), a randomized graph-clustering algorithm. The latter takes as input a graph which is constructed from corpus-based co-occurrence statistics and</context>
</contexts>
<marker>Levy, Reali, Griffiths, 2009</marker>
<rawString>Levy, Roger P., Florencia Reali, and Thomas L. Griffiths. 2009. Modeling the effects of memory on human online sentence processing with particle filters. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 937–944.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="20940" citStr="McCallum, 2002" startWordPosition="3320" endWordPosition="3321">ing decisions, whereas the BayesCat model returns a soft clustering of target nouns. In order to be able to compare the two models, we convert the soft clusters to hard clusters by assigning each target word w to category c such that P(wjc) · P(cjw). LDA We also compared our model to a standard topic model, namely Latent Dirichlet Allocation (LDA; Blei et al. 2003). LDA assumes that a document is generated from an individual mixture over topics, and each topic is associated with one word distribution. We trained a batch version of LDA using input identical to our model and the Mallet toolkit (McCallum, 2002). Chinese Whispers is a parameter-free algorithm and thus determines the number of clusters automatically. While the Bayesian models presented here are parametric in that an upper bound for the potential number of categories needs to be specified, the models themselves decide on the specific value of this number. We set the upper bound of categories to 100 for LDA as well as the batch and incremental version of the BayesCat model. 5.4 Evaluation Metrics Our aim is to learn a set of clusters each of which corresponds to one gold category, i.e., it contains all and only members of that gold cate</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>McCallum, Andrew Kachites. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>George S Cree</author>
<author>Mark S Seidenberg</author>
<author>Chris McNorgan</author>
</authors>
<title>Semantic feature production norms for a large set of living and nonliving things.</title>
<date>2005</date>
<journal>Behavioral Research Methods</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="3612" citStr="McRae et al. (2005)" startWordPosition="539" endWordPosition="542">nd typicality rating (Voorspoels et al., 2008), to evaluate prototype and exemplar models (Storms et al., 2000), and to simulate early language category acquisition (Fountain and Lapata, 2011). The idea of using naturalistic corpora has received little attention. Most existing studies use feature norms as a proxy for people’s representation of semantic concepts. In a typical procedure, participants are presented with a word and asked to generate the most relevant features or attributes for its referent concept. The most notable collection of feature norms is probably the multi-year project of McRae et al. (2005), which obtained features for a set of 541 common English nouns. Our approach replaces feature norms with representations derived from words’ contexts in corpora. While this is an impoverished view of how categories are acquired — it is clear that they are learnt through exposure to the linguistic environment and the physical world — perceptual infor1Throughout this paper we will use small caps to denote CATEGORIES and italics for their members. 249 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 249–258, Gothenburg, Sweden, Ap</context>
</contexts>
<marker>McRae, Cree, Seidenberg, McNorgan, 2005</marker>
<rawString>McRae, Ken, George S. Cree, Mark S. Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavioral Research Methods 37(4):547–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Medin</author>
<author>Marguerite M Schaffer</author>
</authors>
<title>Context theory of classification learning.</title>
<date>1978</date>
<journal>Psychological Review</journal>
<volume>85</volume>
<issue>3</issue>
<contexts>
<context position="2256" citStr="Medin and Schaffer 1978" startWordPosition="325" endWordPosition="328">ng from strict prototypes (categories are represented by a single idealized member which embodies their core properties; e.g., Reed 1972) to full exemplar models (categories are represented by a list of previously encountered members; e.g., Nosofsky 1988) and combinations of the two (e.g., Griffiths et al. 2007). A common feature across different studies is the use of stimuli involving realworld objects (e.g., children’s toys; Starkey 1981), perceptual abstractions (e.g., photographs of animals; Quinn and Eimas 1996), or artificial ones (e.g., binary strings, dot patterns or geometric shapes; Medin and Schaffer 1978; Posner and Keele 1968; Bomba and Siqueland 1983). Most existing models focus on adult categorization, in which it is assumed that a large number of categories have already been learnt (but see Anderson 1991 and Griffiths et al. 2007 for exceptions). In this work we focus on categories acquired from natural language stimuli (i.e., words) and investigate how the statistics of the linguistic environment (as approximated by large corpora) influence category formation (e.g., chair and table are FURNITURE whereas peach and apple are FRUIT1). The idea of modeling categories using words as a stand-i</context>
</contexts>
<marker>Medin, Schaffer, 1978</marker>
<rawString>Medin, Douglas L. and Marguerite M. Schaffer. 1978. Context theory of classification learning. Psychological Review 85(3):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M Nosofsky</author>
</authors>
<title>Exemplar-based accounts of relations between classification, recognition, and typicality.</title>
<date>1988</date>
<journal>Journal of Experimental Psychology: Learning, Memory, and Cognition</journal>
<pages>14--700</pages>
<contexts>
<context position="1888" citStr="Nosofsky 1988" startWordPosition="268" endWordPosition="270"> Medin, 1981). Categorization is a classic problem in cognitive science, underlying a variety of common mental tasks including perception, learning, and the use of language. Given its fundamental nature, categorization has been extensively studied both experimentally and in simulations. Indeed, numerous models exist as to how humans categorize objects ranging from strict prototypes (categories are represented by a single idealized member which embodies their core properties; e.g., Reed 1972) to full exemplar models (categories are represented by a list of previously encountered members; e.g., Nosofsky 1988) and combinations of the two (e.g., Griffiths et al. 2007). A common feature across different studies is the use of stimuli involving realworld objects (e.g., children’s toys; Starkey 1981), perceptual abstractions (e.g., photographs of animals; Quinn and Eimas 1996), or artificial ones (e.g., binary strings, dot patterns or geometric shapes; Medin and Schaffer 1978; Posner and Keele 1968; Bomba and Siqueland 1983). Most existing models focus on adult categorization, in which it is assumed that a large number of categories have already been learnt (but see Anderson 1991 and Griffiths et al. 20</context>
</contexts>
<marker>Nosofsky, 1988</marker>
<rawString>Nosofsky, Robert M. 1988. Exemplar-based accounts of relations between classification, recognition, and typicality. Journal of Experimental Psychology: Learning, Memory, and Cognition 14:700–708.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael I Posner</author>
<author>Steven W Keele</author>
</authors>
<title>On the genesis of abstract ideas.</title>
<date>1968</date>
<journal>Journal of Experimental Psychology</journal>
<pages>21--367</pages>
<contexts>
<context position="2279" citStr="Posner and Keele 1968" startWordPosition="329" endWordPosition="332"> (categories are represented by a single idealized member which embodies their core properties; e.g., Reed 1972) to full exemplar models (categories are represented by a list of previously encountered members; e.g., Nosofsky 1988) and combinations of the two (e.g., Griffiths et al. 2007). A common feature across different studies is the use of stimuli involving realworld objects (e.g., children’s toys; Starkey 1981), perceptual abstractions (e.g., photographs of animals; Quinn and Eimas 1996), or artificial ones (e.g., binary strings, dot patterns or geometric shapes; Medin and Schaffer 1978; Posner and Keele 1968; Bomba and Siqueland 1983). Most existing models focus on adult categorization, in which it is assumed that a large number of categories have already been learnt (but see Anderson 1991 and Griffiths et al. 2007 for exceptions). In this work we focus on categories acquired from natural language stimuli (i.e., words) and investigate how the statistics of the linguistic environment (as approximated by large corpora) influence category formation (e.g., chair and table are FURNITURE whereas peach and apple are FRUIT1). The idea of modeling categories using words as a stand-in for their referents h</context>
</contexts>
<marker>Posner, Keele, 1968</marker>
<rawString>Posner, Michael I. and Steven W. Keele. 1968. On the genesis of abstract ideas. Journal of Experimental Psychology 21:367–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul C Quinn</author>
<author>Peter D Eimas</author>
</authors>
<title>Perceptual cues that permit categorical differentiation of animal species by infants.</title>
<date>1996</date>
<journal>Journal of Experimental Child Psychology</journal>
<pages>63--189</pages>
<contexts>
<context position="2155" citStr="Quinn and Eimas 1996" startWordPosition="310" endWordPosition="313">entally and in simulations. Indeed, numerous models exist as to how humans categorize objects ranging from strict prototypes (categories are represented by a single idealized member which embodies their core properties; e.g., Reed 1972) to full exemplar models (categories are represented by a list of previously encountered members; e.g., Nosofsky 1988) and combinations of the two (e.g., Griffiths et al. 2007). A common feature across different studies is the use of stimuli involving realworld objects (e.g., children’s toys; Starkey 1981), perceptual abstractions (e.g., photographs of animals; Quinn and Eimas 1996), or artificial ones (e.g., binary strings, dot patterns or geometric shapes; Medin and Schaffer 1978; Posner and Keele 1968; Bomba and Siqueland 1983). Most existing models focus on adult categorization, in which it is assumed that a large number of categories have already been learnt (but see Anderson 1991 and Griffiths et al. 2007 for exceptions). In this work we focus on categories acquired from natural language stimuli (i.e., words) and investigate how the statistics of the linguistic environment (as approximated by large corpora) influence category formation (e.g., chair and table are FU</context>
</contexts>
<marker>Quinn, Eimas, 1996</marker>
<rawString>Quinn, Paul C. and Peter D. Eimas. 1996. Perceptual cues that permit categorical differentiation of animal species by infants. Journal of Experimental Child Psychology 63:189–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen K Reed</author>
</authors>
<title>Pattern recognition and categorization.</title>
<date>1972</date>
<journal>Cognitive psychology</journal>
<volume>3</volume>
<issue>3</issue>
<pages>407</pages>
<contexts>
<context position="1770" citStr="Reed 1972" startWordPosition="250" endWordPosition="251"> these objects belong and extrapolating from their past experiences with other members of that category (Smith and Medin, 1981). Categorization is a classic problem in cognitive science, underlying a variety of common mental tasks including perception, learning, and the use of language. Given its fundamental nature, categorization has been extensively studied both experimentally and in simulations. Indeed, numerous models exist as to how humans categorize objects ranging from strict prototypes (categories are represented by a single idealized member which embodies their core properties; e.g., Reed 1972) to full exemplar models (categories are represented by a list of previously encountered members; e.g., Nosofsky 1988) and combinations of the two (e.g., Griffiths et al. 2007). A common feature across different studies is the use of stimuli involving realworld objects (e.g., children’s toys; Starkey 1981), perceptual abstractions (e.g., photographs of animals; Quinn and Eimas 1996), or artificial ones (e.g., binary strings, dot patterns or geometric shapes; Medin and Schaffer 1978; Posner and Keele 1968; Bomba and Siqueland 1983). Most existing models focus on adult categorization, in which i</context>
</contexts>
<marker>Reed, 1972</marker>
<rawString>Reed, Stephen K. 1972. Pattern recognition and categorization. Cognitive psychology 3(3):382– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Riordan</author>
<author>Michael N Jones</author>
</authors>
<title>Redundancy in perceptual and linguistic experience: Comparing feature-based and distributional models of semantic representation.</title>
<date>2011</date>
<journal>Topics in Cognitive Science</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="4419" citStr="Riordan and Jones, 2011" startWordPosition="663" endWordPosition="666">overished view of how categories are acquired — it is clear that they are learnt through exposure to the linguistic environment and the physical world — perceptual infor1Throughout this paper we will use small caps to denote CATEGORIES and italics for their members. 249 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 249–258, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics mation relevant for extracting semantic categories is to a large extent redundantly encoded in linguistic experience (Riordan and Jones, 2011). Besides, there are known difficulties with feature norms such as the small number of words for which these can be obtained, the quality of the attributes, and variability in the way people generate them (see Zeigenfuse and Lee 2010 for details). Focusing on natural language categories allows us to build categorization models with theoretically unlimited scope. To this end, we present a probabilistic Bayesian model of category acquisition based on the key idea that learners can adaptively form category representations that capture the structure expressed in the observed data. We model categor</context>
</contexts>
<marker>Riordan, Jones, 2011</marker>
<rawString>Riordan, Brian and Michael N. Jones. 2011. Redundancy in perceptual and linguistic experience: Comparing feature-based and distributional models of semantic representation. Topics in Cognitive Science 3(2):303–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>V-measure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<pages>410--420</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="22569" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="3585" endWordPosition="3588">ic performs an item-based mapping and can be directly used to evaluate soft clusters. Purity/Collocation are based on member overlap between induced clusters and gold classes (Lang and Lapata, 2011). Purity measures the degree to which each cluster contains instances that share the same gold class, while collocation measures the degree to which instances with the same gold class are assigned to a single cluster. We report the harmonic mean of purity and collocation cat(w) = max c 253 as a single measure of clustering quality. V-Measure is the harmonic mean between homogeneity and collocation (Rosenberg and Hirschberg, 2007). Like purity, V-Measure performs cluster-based comparisons but is an entropy-based method. It measures the conditional entropy of a cluster given a class, and vice versa. Cluster-F1 is an item-based evaluation metric which we propose drawing inspiration from the supervised metric presented in Agirre and Soroa (2007). Cluster-F1 maps each target word type to a gold cluster based on its soft class membership, and is thus appropriate for evaluation of soft clustering output. We first create a K × G soft mapping matrix M from each induced category ki to gold classes gj from P(gj|ki). We then map </context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Rosenberg, Andrew and Julia Hirschberg. 2007. V-measure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Prague, Czech Republic, pages 410–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam N Sanborn</author>
<author>Thomas L Griffiths</author>
<author>Daniel J Navarro</author>
</authors>
<title>A more rational model of categorization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 28th Annual Conference of the Cognitive Science Society.</booktitle>
<pages>726--731</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="6829" citStr="Sanborn et al. (2006)" startWordPosition="1033" endWordPosition="1036"> integrates newly observed data and can be thus viewed as a plausible proxy for human learning. Experimental results show that the incremental learner obtains meaningful categories which outperform the state of the art whilst at the same time acquiring semantic representations of words and their features. 2 Related Work The problem of category induction has achieved much attention in the cognitive science literature. Incremental category learning was pioneered by Anderson (1991) who develops a non-parametric model able to induce categories from abstract stimuli represented by binary features. Sanborn et al. (2006) present a fully Bayesian adaptation of Anderson’s original model, which yields a better fit with behavioral data. A separate line of work examines the cognitive characteristics of category acquisition as well as the processes of generalizing and generating new categories and exemplars (Jern and Kemp, 2013; Kemp et al., 2012). The above models are conceptually similar to ours. However, they were developed with adult categorization in mind, and use rather simplistic categories representing toy-domains. It is therefore not clear whether they generalize to arbitrary stimuli and data sizes. We aim</context>
<context position="8647" citStr="Sanborn et al. (2006)" startWordPosition="1317" endWordPosition="1320"> plausible (early) language learning model and show that categories can be acquired purely from context, as well as in an incremental fashion. From a modeling perspective, we learn categories incrementally using a particle filtering algorithm (Doucet et al., 2001). Particle filters are a family of sequential Monte Carlo algorithms which update the state space of a probabilistic model with newly encountered information. They have been successfully applied to natural language acquisition tasks such as word segmentation (Borschinger and Johnson, 2011), or sentence processing (Levy et al., 2009). Sanborn et al. (2006) also use particle filters for small-scale categorization experiments with artificial stimuli. To the best of our knowledge, we present the first particle filtering algorithm for large-scale category acquisition from natural text. Our work is closest to Fountain and Lapata (2011) who also develop a model for inducing natural language categories. Specifically, they propose an incremental version of Chinese Whispers (Biemann, 2006), a randomized graph-clustering algorithm. The latter takes as input a graph which is constructed from corpus-based co-occurrence statistics and produces a hard cluste</context>
</contexts>
<marker>Sanborn, Griffiths, Navarro, 2006</marker>
<rawString>Sanborn, Adam N., Thomas L. Griffiths, and Daniel J. Navarro. 2006. A more rational model of categorization. In Proceedings of the 28th Annual Conference of the Cognitive Science Society. Vancouver, Canada, pages 726–731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward E Smith</author>
<author>Douglas L Medin</author>
</authors>
<title>Categories and Concepts.</title>
<date>1981</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1287" citStr="Smith and Medin, 1981" startWordPosition="177" endWordPosition="180"> single process. Our model employs particle filters, a sequential Monte Carlo method commonly used for approximate probabilistic inference in an incremental setting. Comparison against a state-of-the-art graph-based approach reveals that our model learns qualitatively better categories and demonstrates cognitive plausibility during learning. 1 Introduction Considerable psychological research has shown that people reason about novel objects they encounter by identifying the category to which these objects belong and extrapolating from their past experiences with other members of that category (Smith and Medin, 1981). Categorization is a classic problem in cognitive science, underlying a variety of common mental tasks including perception, learning, and the use of language. Given its fundamental nature, categorization has been extensively studied both experimentally and in simulations. Indeed, numerous models exist as to how humans categorize objects ranging from strict prototypes (categories are represented by a single idealized member which embodies their core properties; e.g., Reed 1972) to full exemplar models (categories are represented by a list of previously encountered members; e.g., Nosofsky 1988</context>
</contexts>
<marker>Smith, Medin, 1981</marker>
<rawString>Smith, Edward E. and Douglas L. Medin. 1981. Categories and Concepts. Harvard University Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Starkey</author>
</authors>
<title>The origins of concept formation: Object sorting and object preference in early infancy.</title>
<date>1981</date>
<booktitle>Child Development</booktitle>
<pages>489--497</pages>
<contexts>
<context position="2077" citStr="Starkey 1981" startWordPosition="301" endWordPosition="302">ental nature, categorization has been extensively studied both experimentally and in simulations. Indeed, numerous models exist as to how humans categorize objects ranging from strict prototypes (categories are represented by a single idealized member which embodies their core properties; e.g., Reed 1972) to full exemplar models (categories are represented by a list of previously encountered members; e.g., Nosofsky 1988) and combinations of the two (e.g., Griffiths et al. 2007). A common feature across different studies is the use of stimuli involving realworld objects (e.g., children’s toys; Starkey 1981), perceptual abstractions (e.g., photographs of animals; Quinn and Eimas 1996), or artificial ones (e.g., binary strings, dot patterns or geometric shapes; Medin and Schaffer 1978; Posner and Keele 1968; Bomba and Siqueland 1983). Most existing models focus on adult categorization, in which it is assumed that a large number of categories have already been learnt (but see Anderson 1991 and Griffiths et al. 2007 for exceptions). In this work we focus on categories acquired from natural language stimuli (i.e., words) and investigate how the statistics of the linguistic environment (as approximate</context>
</contexts>
<marker>Starkey, 1981</marker>
<rawString>Starkey, David. 1981. The origins of concept formation: Object sorting and object preference in early infancy. Child Development pages 489– 497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gert Storms</author>
<author>Paul De Boeck</author>
<author>Wim Ruts</author>
</authors>
<title>Prototype and exemplar-based information in natural language categories.</title>
<date>2000</date>
<journal>Journal of Memory and Language</journal>
<pages>42--51</pages>
<marker>Storms, De Boeck, Ruts, 2000</marker>
<rawString>Storms, Gert, Paul De Boeck, and Wim Ruts. 2000. Prototype and exemplar-based information in natural language categories. Journal of Memory and Language 42:51–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vinson</author>
<author>Gabriella Vigliocco</author>
</authors>
<title>Semantic feature production norms for a large set of objects and events.</title>
<date>2008</date>
<journal>Behavior Research Methods</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="17624" citStr="Vinson and Vigliocco (2008)" startWordPosition="2776" endWordPosition="2779"> to learn meaningful categories and how these change over time. In the following, we give details on the corpora we used, describe how model parameters were selected, and explain our evaluation procedure. 5.1 Data All our experiments were conducted on a lemmatized version of the British National Corpus (BNC). The corpus was further preprocessed by removing stopwords and infrequent words (occurring less than 800 times in the BNC). The model output was evaluated against a gold standard set of categories which was created by collating the resources developed by Fountain and 252 Lapata (2010) and Vinson and Vigliocco (2008). Both datasets contain a classification of nouns into (possibly multiple) semantic categories produced by human participants. We therefore assume that they represent psychologically salient categories which the cognitive system is in principle capable of acquiring. After merging the two resources, and removing duplicates we obtained 42 semantic categories for 555 nouns. We split this gold standard into a development (41 categories, 492 nouns) and a test set (16 categories, 196 nouns).2 The input to our model consists of short chunks of text, namely a target word centered in a symmetric contex</context>
</contexts>
<marker>Vinson, Vigliocco, 2008</marker>
<rawString>Vinson, David and Gabriella Vigliocco. 2008. Semantic feature production norms for a large set of objects and events. Behavior Research Methods 40(1):183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wouter Voorspoels</author>
<author>Wolf Vanpaemel</author>
<author>Gert Storms</author>
</authors>
<title>Exemplars and prototypes in natural language concepts: A typicality-based evaluation.</title>
<date>2008</date>
<journal>Psychonomic Bulletin &amp; Review</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="3039" citStr="Voorspoels et al., 2008" startWordPosition="449" endWordPosition="452">s have already been learnt (but see Anderson 1991 and Griffiths et al. 2007 for exceptions). In this work we focus on categories acquired from natural language stimuli (i.e., words) and investigate how the statistics of the linguistic environment (as approximated by large corpora) influence category formation (e.g., chair and table are FURNITURE whereas peach and apple are FRUIT1). The idea of modeling categories using words as a stand-in for their referents has been previously used to explore categorization-related phenomena such as semantic priming (Cree et al., 1999) and typicality rating (Voorspoels et al., 2008), to evaluate prototype and exemplar models (Storms et al., 2000), and to simulate early language category acquisition (Fountain and Lapata, 2011). The idea of using naturalistic corpora has received little attention. Most existing studies use feature norms as a proxy for people’s representation of semantic concepts. In a typical procedure, participants are presented with a word and asked to generate the most relevant features or attributes for its referent concept. The most notable collection of feature norms is probably the multi-year project of McRae et al. (2005), which obtained features f</context>
</contexts>
<marker>Voorspoels, Vanpaemel, Storms, 2008</marker>
<rawString>Voorspoels, Wouter, Wolf Vanpaemel, and Gert Storms. 2008. Exemplars and prototypes in natural language concepts: A typicality-based evaluation. Psychonomic Bulletin &amp; Review 15(3):630–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Nonparametric Bayesian word sense induction.</title>
<date>2011</date>
<booktitle>In Proceedings of TextGraphs-6: Graphbased Methods for Natural Language Processing.</booktitle>
<pages>10--14</pages>
<location>Portland, Oregon,</location>
<marker>Yao, Van Durme, 2011</marker>
<rawString>Yao, Xuchen and Benjamin Van Durme. 2011. Nonparametric Bayesian word sense induction. In Proceedings of TextGraphs-6: Graphbased Methods for Natural Language Processing. Portland, Oregon, pages 10–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeigenfuse</author>
<author>Michael D Lee</author>
</authors>
<title>Finding the features that represent stimuli.</title>
<date>2010</date>
<journal>Acta Psychologica</journal>
<volume>133</volume>
<issue>3</issue>
<contexts>
<context position="4652" citStr="Zeigenfuse and Lee 2010" startWordPosition="702" endWordPosition="705">S and italics for their members. 249 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 249–258, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics mation relevant for extracting semantic categories is to a large extent redundantly encoded in linguistic experience (Riordan and Jones, 2011). Besides, there are known difficulties with feature norms such as the small number of words for which these can be obtained, the quality of the attributes, and variability in the way people generate them (see Zeigenfuse and Lee 2010 for details). Focusing on natural language categories allows us to build categorization models with theoretically unlimited scope. To this end, we present a probabilistic Bayesian model of category acquisition based on the key idea that learners can adaptively form category representations that capture the structure expressed in the observed data. We model category induction as two interrelated sub-problems: (a) the acquisition of features that discriminate among categories, and (b) the grouping of concepts into categories based on those features. An important modeling question concerns the e</context>
</contexts>
<marker>Zeigenfuse, Lee, 2010</marker>
<rawString>Zeigenfuse, Matthew D. and Michael D. Lee. 2010. Finding the features that represent stimuli. Acta Psychologica 133(3):283–295.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>