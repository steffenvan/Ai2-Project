<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.905651">
Model Combination for Machine Translation
</title>
<author confidence="0.944817">
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz Och
</author>
<affiliation confidence="0.808073">
UC Berkeley Google, Inc.
</affiliation>
<email confidence="0.990482">
denero@berkeley.edu {shankarkumar,ciprianchelba,och}@google.com
</email>
<sectionHeader confidence="0.994644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998024409090909">
Machine translation benefits from two types
of decoding techniques: consensus decoding
over multiple hypotheses under a single model
and system combination over hypotheses from
different models. We present model combina-
tion, a method that integrates consensus de-
coding and system combination into a uni-
fied, forest-based technique. Our approach
makes few assumptions about the underly-
ing component models, enabling us to com-
bine systems with heterogenous structure. Un-
like most system combination techniques, we
reuse the search space of component models,
which entirely avoids the need to align trans-
lation hypotheses. Despite its relative sim-
plicity, model combination improves trans-
lation quality over a pipelined approach of
first applying consensus decoding to individ-
ual systems, and then applying system combi-
nation to their output. We demonstrate BLEU
improvements across data sets and language
pairs in large-scale experiments.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993054">
Once statistical translation models are trained, a de-
coding approach determines what translations are fi-
nally selected. Two parallel lines of research have
shown consistent improvements over the standard
max-derivation decoding objective, which selects
the highest probability derivation. Consensus de-
coding procedures select translations for a single
system by optimizing for model predictions about
n-grams, motivated either as minimizing Bayes risk
(Kumar and Byrne, 2004), maximizing sentence
similarity (DeNero et al., 2009), or approximating a
max-translation objective (Li et al., 2009b). System
combination procedures, on the other hand, generate
translations from the output of multiple component
systems (Frederking and Nirenburg, 1994). In this
paper, we present model combination, a technique
that unifies these two approaches by learning a con-
sensus model over the n-gram features of multiple
underlying component models.
Model combination operates over the compo-
nent models’ posterior distributions over translation
derivations, encoded as a forest of derivations.1 We
combine these components by constructing a linear
consensus model that includes features from each
component. We then optimize this consensus model
over the space of all translation derivations in the
support of all component models’ posterior distribu-
tions. By reusing the components’ search spaces,
we entirely avoid the hypothesis alignment problem
that is central to standard system combination ap-
proaches (Rosti et al., 2007).
Forest-based consensus decoding techniques dif-
fer in whether they capture model predictions
through n-gram posteriors (Tromble et al., 2008;
Kumar et al., 2009) or expected n-gram counts
(DeNero et al., 2009; Li et al., 2009b). We evaluate
both in controlled experiments, demonstrating their
empirical similarity. We also describe algorithms for
expanding translation forests to ensure that n-grams
are local to a forest’s hyperedges, and for exactly
computing n-gram posteriors efficiently.
Model combination assumes only that each trans-
lation model can produce expectations of n-gram
features; the latent derivation structures of compo-
nent systems can differ arbitrarily. This flexibility
allows us to combine phrase-based, hierarchical, and
syntax-augmented translation models. We evaluate
by combining three large-scale systems on Chinese-
English and Arabic-English NIST data sets, demon-
strating improvements of up to 1.4 BLEU over the
</bodyText>
<footnote confidence="0.996212">
1In this paper, we use the terms translation forest and hyper-
graph interchangeably.
</footnote>
<page confidence="0.942707">
975
</page>
<note confidence="0.832015">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 975–983,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.3567435">
“I saw with the telescope the man”
Yo vi al hombre con el telescopio
</figure>
<figureCaption confidence="0.996824">
Figure 1: An example translation forest encoding two
</figureCaption>
<bodyText confidence="0.861269">
synchronous derivations for a Spanish sentence: one solid
and one dotted. Nodes are annotated with their left and
right unigram contexts, and hyperedges are annotated
with scores B · O(r) and the bigrams they introduce.
best single system max-derivation baseline, and con-
sistent improvements over a more complex multi-
system pipeline that includes independent consensus
R decoding and system combination.
</bodyText>
<sectionHeader confidence="0.977718" genericHeader="method">
2 Model Combination
</sectionHeader>
<bodyText confidence="0.9956015">
Model combination is a model-based approach to se-
lecting translations using information from multiple
component systems. Each system provides its poste-
rior distributions over derivations Pi(d|f), encoded
as a weighted translation forest (i.e., translation hy-
pergraph) in which hyperedges correspond to trans-
lation rule applications r.2 The conditional distribu-
tion over derivations takes the form:
where D(f) is the set of synchronous derivations en-
coded in the forest, r iterates over rule applications
in d, and Oi is the parameter vector for system i. The
feature vector Oi is system specific and includes both
translation model and language model features. Fig-
ure 1 depicts an example forest.
Model combination includes four steps, described
below. The entire sequence is illustrated in Figure 2.
2Phrase-based systems produce phrase lattices, which are in-
stances of forests with arity 1.
</bodyText>
<subsectionHeader confidence="0.997127">
2.1 Computing Combination Features
</subsectionHeader>
<bodyText confidence="0.9998574">
The first step in model combination is to com-
pute n-gram expectations from component system
posteriors—the same quantities found in MBR, con-
sensus, and variational decoding techniques. For an
n-gram g and system i, the expectation
</bodyText>
<equation confidence="0.942334">
vz (g) = F-Pz(d|f) [h(d,g)]
</equation>
<bodyText confidence="0.9996854">
can be either an n-gram expected count, if h(d, g)
is the count of g in d, or the posterior probability
that d contains g, if h(d, g) is an indicator function.
Section 3 describes how to compute these features
efficiently.
</bodyText>
<subsectionHeader confidence="0.999936">
2.2 Constructing a Search Space
</subsectionHeader>
<bodyText confidence="0.999955714285714">
The second step in model combination constructs a
hypothesis space of translation derivations, which
includes all derivations present in the forests con-
tributed by each component system. This search
space D is also a translation forest, and consists of
the conjoined union of the component forests. Let
Ri be the root node of component hypergraph Di.
For all i, we include all of Di in D, along with an
edge from Ri to R, the root of D. D may contain
derivations from different types of translation sys-
tems. However, D only contains derivations (and
therefore translations) that appeared in the hypothe-
sis space of some component system. We do not in-
termingle the component search spaces in any way.
</bodyText>
<subsectionHeader confidence="0.818782">
2.3 Features for the Combination Model
</subsectionHeader>
<bodyText confidence="0.9989828">
The third step defines a new combination model over
all of the derivations in the search space D, and then
annotates D with features that allow for efficient
model inference. We use a linear model over four
types of feature functions of a derivation:
</bodyText>
<listItem confidence="0.999025">
1. Combination feature functions on n-grams
vz (d) = EgENgrams(d) v�i (g) score a deriva-
tion according to the n-grams it contains.
2. Model score feature function b gives the model
score Oi · Oi(d) of a derivation d under the sys-
tem i that d is from.
3. A length feature E computes the word length of
the target-side yield of a derivation.
4. A system indicator feature αi is 1 if the deriva-
tion came from system i, and 0 otherwise.
</listItem>
<figure confidence="0.969164333333333">
“I saw the man with the telescope”
I ... man
I ... telescope
0.4
“telescope the”
0.6
“saw the”
the ... telescope
I ... telescope
0.3
“saw with”
1.0
</figure>
<equation confidence="0.7457438">
“man with”
I ... saw the ... man with ... telescope
exp [ErEd �i · Oi(r)I
Pi(d|f) =
Ed/ED(f) exp [ErEd/ 9i · Oi(r)I
</equation>
<page confidence="0.986541">
976
</page>
<figureCaption confidence="0.332687">
Step 1: Compute Single-Model N-gram Features
</figureCaption>
<bodyText confidence="0.893962642857143">
All of these features are local to rule applications
(hyperedges) in D. The combination features pro-
vide information sharing across the derivations of
Phrasebased system Hierarchcal system
different systems, but are functions of n-grams, and
2 ��)so can be scored on any translation forest. Model
��( score features are already local to rule applications.
The length feature is scored in the standard way.
St 2 Ctruct Sarch Sace
System indicator features are scored only on the hy-
peredges from Ri to R that link each component for-
[ 1 [ 1]
est to the common root.
Scoring the joint search space D with these fea-
</bodyText>
<equation confidence="0.880147">
R R
</equation>
<bodyText confidence="0.9840075">
tures involves annotating each rule application r (i.e.
hyperedge) with the value of each feature.
</bodyText>
<subsectionHeader confidence="0.999869">
2.4 Model Training and Inference
</subsectionHeader>
<bodyText confidence="0.999353333333333">
We have defined the following combination model 07,
sw(d) with weights w over derivations d from I dif-
ferent component models:
</bodyText>
<equation confidence="0.608618">
I 4
E p 4 wz vz (d)n+gwaai (d)n+w&apos;·b(d)+w`·`(d)
</equation>
<bodyText confidence="0.884078">
Because we have assessed all of these features on
local rule applications, we can find the highest scor-
</bodyText>
<equation confidence="0.946123888888889">
i 1 J
=1 n=
w = ag ma BLEU ar max sw(d) ; e
!&amp;quot; # $
f)
ing derivation d* = arg max
dED sw(d) using standard
dED
max-sum (Viterbi) inference over D.
</equation>
<bodyText confidence="0.999996625">
We learn the weights of this consensus model us-
ing hypergraph-based minimum-error-rate training
(Kumar et al., 2009). This procedure maximizes the
translation quality of d* on a held-out set, according
to a corpus-level evaluation metric B(·; e) that com-
pares to a reference set e. We used BLEU, choosing
w to maximize the BLEU score of the set of transla-
tions predicted by the combination model.
</bodyText>
<sectionHeader confidence="0.973243" genericHeader="method">
3 Computing Combination Features
</sectionHeader>
<bodyText confidence="0.999952285714286">
The combination features vni (d) score derivations
from each model with the n-gram predictions of the
others. These predictions sum over all derivations
under a single component model to compute a pos-
terior belief about each n-gram. In this paper, we
compare two kinds of combination features, poste-
rior probabilities and expected counts.3
</bodyText>
<footnote confidence="0.77085125">
3The model combination framework could incorporate ar-
bitrary features on the common output space of the models, but
we focus on features that have previously proven useful for con-
sensus decoding.
</footnote>
<bodyText confidence="0.485624">
Phrase-based model Hierarchical model
</bodyText>
<equation confidence="0.9921028">
w = arg max
w BLEU I arg max sw(d) } ; e$
\ dED(f) JJJ
d* = arg max sw(d)
dED
</equation>
<figureCaption confidence="0.990867375">
Figure 2: Model combination applied to a phrase-based
(pb) and a hierarchical model (h) includes four steps. (1)
shows an excerpt of the bigram feature function for each
component, (2) depicts the result of conjoining a phrase
lattice with a hierarchical forest, (3) shows example hy-
peredge features of the combination model, including bi-
gram features vZ and system indicators αi, and (4) gives
training and decoding objectives.
</figureCaption>
<bodyText confidence="0.999698230769231">
Posterior probabilities represent a model’s be-
lief that the translation will contain a particular n-
gram at least once. They can be expressed as
EP(d|f) [δ(d, g)] for an indicator function δ(d, g)
that is 1 if n-gram g appears in derivation d. These
quantities arise in approximating BLEU for lattice-
based and hypergraph-based minimum Bayes risk
decoding (Tromble et al., 2008; Kumar et al., 2009).
Expected n-gram counts EP(d|f) [c(d, g)] represent
the model’s belief of how many times an n-gram g
will appear in the translation. These quantities ap-
pear in forest-based consensus decoding (DeNero et
al., 2009) and variational decoding (Li et al., 2009b).
</bodyText>
<figure confidence="0.925468571428571">
Step 1: Compute Combination Features
v�pb(“saw the”) = 0.9
...
vh(“saw the”) = 0.7
...
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
</figure>
<figureCaption confidence="0.683654">
Step 4: Model Training and Inference
</figureCaption>
<equation confidence="0.8477932">
Rpb Rh
[αpb = 1]
“saw the”: [v�pb = 0.9, v�� =0.7]
R
[αh = 1]
</equation>
<page confidence="0.979669">
977
</page>
<bodyText confidence="0.999653">
Methods for computing both of these quantities ap-
pear in the literature. However, we address two out-
standing issues below. In Section 5, we also com-
pare the two quantities experimentally.
</bodyText>
<subsectionHeader confidence="0.999776">
3.1 Computing N-gram Posteriors Exactly
</subsectionHeader>
<bodyText confidence="0.959474923076923">
Kumar et al. (2009) describes an efficient approx-
imate algorithm for computing n-gram posterior
probabilities. Algorithm 1 is an exact algorithm that
computes all n-gram posteriors from a forest in a
single inside pass. The algorithm tracks two quanti-
ties at each node n: regular inside scores β(n) and
n-gram inside scores �β(n, g) that sum the scores of
all derivations rooted at n that contain n-gram g.
For each hyperedge, we compute b(g), the sum of
scores for derivations that do not contain g (Lines 8-
11). We then use that quantity to compute the score
of derivations that do contain g (Line 17).
Algorithm 1 Computing n-gram posteriors
</bodyText>
<listItem confidence="0.986472894736842">
1: for n E N in topological order do
2: β(n) +— 0
3: �β(n, g) +— 0, Vg E Ngrams(n)
4: for r E Rules(n) do
5: w exp [θ · φ(r)]
6: b +— w
7: b(g) +— w, Vg E Ngrams(n)
8: for ` E Leaves(r) do
9: b +— b x β(`)
10: for g E Ngrams(n) do
( )
11: �b(g) +— �b(g) x β(`) � �β(`, g)
12: β(n) +— β(n) + b
13: for g E Ngrams(n) do
14: if g E Ngrams(r) then
15: *, g) +— �β(n, g)+b
16: else
17: �β(n, g) +—
18: for g E Ngrams(root) (all g in the HG) do
</listItem>
<equation confidence="0.5458635">
ˆ�(root��)
19: P(g�f) +— β(root)
</equation>
<bodyText confidence="0.999870285714286">
This algorithm can in principle compute the pos-
terior probability of any indicator function on local
features of a derivation. More generally, this algo-
rithm demonstrates how vector-backed inside passes
can compute quantities beyond expectations of local
features (Li and Eisner, 2009).4 Chelba and Maha-
jan (2009) developed a similar algorithm for lattices.
</bodyText>
<footnote confidence="0.897746">
4Indicator functions on derivations are not locally additive
</footnote>
<subsectionHeader confidence="0.997847">
3.2 Ensuring N-gram Locality
</subsectionHeader>
<bodyText confidence="0.989689310344828">
DeNero et al. (2009) describes an efficient algorithm
for computing n-gram expected counts from a trans-
lation forest. This method assumes n-gram local-
ity of the forest, the property that any n-gram intro-
duced by a hyperedge appears in all derivations that
include the hyperedge. However, decoders may re-
combine forest nodes whenever the language model
does not distinguish between n-grams due to back-
off (Li and Khudanpur, 2008). In this case, a forest
encoding of a posterior distribution may not exhibit
n-gram locality in all regions of the search space.
Figure 3 shows a hypergraph which contains non-
local trigrams, along with its local expansion.
Algorithm 2 expands a forest to ensure n-gram lo-
cality while preserving the encoded distribution over
derivations. Let a forest (N, R) consist of nodes N
and hyperedges R, which correspond to rule appli-
cations. Let Rules(n) be the subset of R rooted by
n, and Leaves(r) be the leaf nodes of rule applica-
tion r. The expanded forest (Ne, Re) is constructed
by a function Reapply(r, L) that applies the rule of r
to a new set of leaves L C Ne, forming a pair (r&apos;, n&apos;)
consisting of a new rule application r&apos; rooted by n&apos;.
P is a map from nodes in N to subsets of Ne which
tracks how N projects to Ne. Two nodes in Ne are
identical if they have the same (n-1)-gram left and
right contexts and are projections of the same node
in N. The symbol ® denotes a set cross-product.
Algorithm 2 Expanding for n-gram locality
</bodyText>
<listItem confidence="0.9691612">
1: Ne +— {}; Re +— {}
2: for n E N in topological order do
3: P(n) +— {}
4: for r E Rules(n) do
for L E O
5: tcLeaves(r) [P(`)] do
6: r&apos;, n&apos; +— Reapply(r, L)
7: P(n) +— P(n) U {n&apos;}
8: Ne NeU{n&apos;}
9: Re ReU{r&apos;}
</listItem>
<bodyText confidence="0.850998">
This transformation preserves the original distri-
bution over derivations by splitting states, but main-
taining continuations from those split states by du-
plicating rule applications. The process is analogous
over the rules of a derivation, even if the features they indicate
are local. Therefore, Algorithm 1 is not an instance of an ex-
pectation semiring computation.
</bodyText>
<equation confidence="0.599606">
�β(n, g)+b — b(g)
</equation>
<page confidence="0.967672">
978
</page>
<figureCaption confidence="0.993578">
Figure 3: Hypergraph expansion ensures n-gram locality
without affecting the distribution over derivations. In the
left example, trigrams “green witch was” and “blue witch
was” are non-local due to language model back-off. On
</figureCaption>
<figure confidence="0.8858904">
Fere
the right, states are split to enforce trigram locality.
saw wih the teecope th
h
to expanding bigram lattices tomencode a trigramphis-
tory at each lattice node (Weng et al., 1998).
...
4 Relationship to PriorcWork
Model combination is a multi-system generaliza-
03
I telescope
tion of consensus or minimum Bayes risk decod-
sw wi
ing. When only one component system is included,
Rh
</figure>
<bodyText confidence="0.84936125">
. w t an
model combination is identical to minimum Bayes
risk decoding over hypergraphs as described in Ku-
mar et al. (2009).5
</bodyText>
<subsectionHeader confidence="0.981759">
4.1 System Combination
</subsectionHeader>
<bodyText confidence="0.980317">
System combination techniques in machine trans-
</bodyText>
<subsectionHeader confidence="0.603553">
mbinatin Model
</subsectionHeader>
<bodyText confidence="0.990396">
lation take as input the outputs fel, · · · , ek} of k
translation systems, where ei is a structured transla-
re
tion object (or k-best lists thereof), typically viewed
as a sequence of words. The dominant approach in
the field chooses a primary translation ep as a back-
bone, then finds an alignment ai to the backbone for
</bodyText>
<subsubsectionHeader confidence="0.60996">
Rpb
</subsubsectionHeader>
<bodyText confidence="0.995352523809524">
each ei. A new search space is constructed from
these backbone-aligned outputs, and then a voting
procedure or feature-based model predicts a final
consensus translation (Rosti et al., 2007). Model
combination entirely avoids this alignment problem
by viewing hypotheses as n-gram occurrence vec-
tors rather than word sequences.
Model combination also requires less total com-
putation than applying system combination to
5We do not refer to model combination as a minimum Bayes
risk decoding procedure despite this similarity because risk im-
plies a belief distribution over outputs, and we now have mul-
tiple output distributions that are not necessarily calibrated.
Moreover, our generalized, multi-model objective (Section 2.4)
is motivated by BLEU, but not a direct approximation to it.
consensus-decoded outputs. The best consensus de-
coding methods for individual systems already re-
quire the computation-intensive steps of model com-
bination: producing lattices or forests, computing n-
gram feature expectations, and re-decoding to max-
imize a secondary consensus objective. Hence, to
maximize the performance of system combination,
these steps must be performed for each system,
whereas model combination requires only one for-
est rescoring pass over all systems.
Model combination also leverages aggregate
statistics from the components’ posteriors, whereas
system combiners typically do not. Zhao and He
(2009) showed that n-gram posterior features are
useful in the context of a system combination model,
even when computed from k-best lists.
Despite these advantages, system combination
may bepmore appropriate in some settings. In par-
ticular, model combination is designed primarily for
statistical systems that generate hypergraph outputs.
Model combination can in principle integrate a non-
statisticalesystem that generates either a single hy-
pothesis or an unweighted forest.6 Likewise, the pro-
oould be applied to statistical systems that
only generate k-best lists. However, we would not
expect the same strong performance from model
combination in these constrained settings.
</bodyText>
<subsectionHeader confidence="0.999001">
4.2 Joint Decoding and Collaborative Decoding
</subsectionHeader>
<bodyText confidence="0.9999525">
Liu et al. (2009) describes two techniques for com-
bining multiple synchronous grammars, which the
R authors characterize as joint decoding. Joint de-
coding does not involve a consensus or minimum-
</bodyText>
<equation confidence="0.402563">
h
</equation>
<bodyText confidence="0.999911166666667">
Bayes-risk decoding objective; indeed, their best
results come from standard max-derivation decod-
ing (with a multi-system grammar). More impor-
tantly, their computations rely on a correspondence
between nodes in the hypergraph outputs of differ-
ent systems, and so they can only joint decode over
models with similar search strategies. We combine a
phrase-based model that uses left-to-right decoding
with two hierarchical systems that use bottom-up de-
coding — a scenario to which joint decoding is not
applicable. Though Liu et al. (2009) rightly point
out that most models can be decoded either left-to-
</bodyText>
<footnote confidence="0.8470955">
6A single hypothesis can be represented as a forest, while an
unweighted forest could be assigned a uniform distribution.
</footnote>
<figure confidence="0.997775125">
applied rule
rule leaves
green witch was here
blue witch was here
green witch
blue witch
rule root
was here
n �-4 P(n)
green witch blue witch
green witch was here
blue witch was here
was here
Yo il hb
&apos; el telescopi
cedure c
</figure>
<page confidence="0.996838">
979
</page>
<bodyText confidence="0.999446066666667">
right or bottom-up, such changes can have substan-
tial implications for search efficiency and search er-
ror. We prefer to maintain the flexibility of using dif-
ferent search strategies in each component system.
Li et al. (2009a) is another related technique for
combining translation systems by leveraging model
predictions of n-gram features. K-best lists of par-
tial translations are iteratively reranked using n-
gram features from the predictions of other mod-
els (which are also iteratively updated). Our tech-
nique differs in that we use no k-best approxima-
tions, have fewer parameters to learn (one consensus
weight vector rather than one for each collaborating
decoder) and produce only one output, avoiding an
additional system combination step at the end.
</bodyText>
<sectionHeader confidence="0.999549" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9880916">
We report results on the constrained data track of the
NIST 2008 Arabic-to-English (ar-en) and Chinese-
to-English (zh-en) translation tasks.7 We train on all
parallel and monolingual data allowed in the track.
We use the NIST 2004 eval set (dev) for optimiz-
ing parameters in model combination and test on
the NIST 2008 evaluation set. We report results
using the IBM implementation of the BLEU score
which computes the brevity penalty using the clos-
est reference translation for each segment (Papineni
et al., 2002). We measure statistical significance us-
ing 95% confidence intervals computed using paired
bootstrap resampling. In all table cells (except for
Table 3) systems without statistically significant dif-
ferences are marked with the same superscript.
</bodyText>
<subsectionHeader confidence="0.99973">
5.1 Base Systems
</subsectionHeader>
<bodyText confidence="0.999673818181818">
We combine outputs from three systems. Our
phrase-based system is similar to the alignment tem-
plate system described by Och and Ney (2004).
Translation is performed using a standard left-
to-right beam-search decoder. Our hierarchical
systems consist of a syntax-augmented system
(SAMT) that includes target-language syntactic cat-
egories (Zollmann and Venugopal, 2006) and a
Hiero-style system with a single non-terminal (Chi-
ang, 2007). Each base system yields state-of-the-art
translation performance, summarized in Table 1.
</bodyText>
<footnote confidence="0.937471">
7http://www.nist.gov/speech/tests/mt
</footnote>
<table confidence="0.997370333333333">
Sys Base ar BLEU (%) -en
dev -en zh nist08
nist08 dev
PB MAX 51.6 43.9 37.7 25.4
PB MBR 52.4* 44.6* 38.6* 27.3*
PB CON 52.4* 44.6* 38.7* 27.2*
Hiero MAX 50.9 43.3 40.0 27.2
Hiero MBR 51.4* 43.8* 40.6* 27.8
Hiero CON 51.5* 43.8* 40.5* 28.2
SAMT MAX 51.7 43.8 40.8* 28.4
SAMT MBR 52.7* 44.5* 41.1* 28.8*
SAMT CON 52.6* 44.4* 41.1* 28.7*
</table>
<tableCaption confidence="0.997766">
Table 1: Performance of baseline systems.
</tableCaption>
<table confidence="0.999720333333333">
BLEU (%)
ar-en zh-en
Approach dev nist08 dev nist08
Best MAX system 51.7 43.9 40.8 28.4
Best MBR system 52.7 44.5 41.1 28.8*
MC Conjoin/SI 53.5 45.3 41.6 29.0*
</table>
<tableCaption confidence="0.9274548">
Table 2: Performance from the best single system for
each language pair without consensus decoding (Best
MAX system), the best system with minimum Bayes risk
decoding (Best MBR system), and model combination
across three systems.
</tableCaption>
<bodyText confidence="0.9996672">
For each system, we report the performance of
max-derivation decoding (MAX), hypergraph-based
MBR (Kumar et al., 2009), and a linear version of
forest-based consensus decoding (CON) (DeNero et
al., 2009). MBR and CON differ only in that the first
uses n-gram posteriors, while the second uses ex-
pected n-gram counts. The two consensus decoding
approaches yield comparable performance. Hence,
we report performance for hypergraph-based MBR
in our comparison to model combination below.
</bodyText>
<subsectionHeader confidence="0.999586">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999906181818182">
Table 2 compares model combination (MC) to the
best MAX and MBR systems. Model combination
uses a conjoined search space wherein each hyper-
edge is annotated with 21 features: 12 n-gram poste-
rior features vni computed from the PB/Hiero/SAMT
forests for n &lt; 4; 4 n-gram posterior features vn
computed from the conjoined forest; 1 length fea-
ture E; 1 feature b for the score assigned by the base
model; and 3 system indicator (SI) features αi that
select which base system a derivation came from.
We refer to this model combination approach as MC
</bodyText>
<page confidence="0.990605">
980
</page>
<table confidence="0.999927090909091">
Strategy ar BLEU (%) -en
dev -en zh nist08
nist08 dev
Best MBR system 52.7 44.5 41.1 28.8
MBR Conjoin 52.3 44.5 40.5 28.3
MBR Conjoin/feats-best 52.7 44.9 41.2 28.8
MBR Conjoin/SI 53.1 44.9 41.2 28.9
MC 1-best HG 52.7 44.6 41.1 28.7
MC Conjoin 52.9 44.6 40.3 28.1
MC Conjoin/base/SI 53.5 45.1 41.2 28.9
MC Conjoin/SI 53.5 45.3 41.6 29.0
</table>
<tableCaption confidence="0.997406">
Table 3: Model Combination experiments.
</tableCaption>
<bodyText confidence="0.970534228571429">
Conjoin/SI. Model combination improves over the
single best MAX system by 1.4 BLEU in ar-en and
0.6 BLEU in zh-en, and always improves over MBR.
This improvement could arise due to multiple rea-
sons: a bigger search space, the consensus features
from constituent systems, or the system indicator
features. Table 3 teases apart these contributions.
We first perform MBR on the conjoined hyper-
graph (MBR-Conjoin). In this case, each edge is
tagged with 4 conjoined n-gram features vn, along
with length and base model features. MBR-Conjoin
is worse than MBR on the hypergraph from the
single best system. This could imply that either
the larger search space introduces poor hypotheses
or that the n-gram posteriors obtained are weaker.
When we now restrict the n-gram features to those
from the best system (MBR Conjoin/feats-best),
BLEU scores increase relative to MBR-Conjoin.
This implies that the n-gram features computed over
the conjoined hypergraph are weaker than the corre-
sponding features from the best system.
Adding system indicator features (MBR Con-
join+SI) helps the MBR-Conjoin system consider-
ably; the resulting system is better than the best
MBR system. This could mean that the SI features
guide search towards stronger parts of the larger
search space. In addition, these features provide a
normalization of scores across systems.
We next do several model-combination experi-
ments. We perform model combination using the
search space of only the best MBR system (MC
1best HG). Here, the hypergraph is annotated with
n-gram features from the 3 base systems, as well as
length and base model features. A total of 3 x 4 +
1 + 1 = 14 features are added to each edge. Sur-
</bodyText>
<table confidence="0.99909425">
BLEU (%)
ar-en zh-en
Approach Base dev nist08 dev nist08
Sent-level MAX 51.8* 44.4* 40.8* 28.2*
Word-level MAX 52.0* 44.4* 40.8* 28.1*
Sent-level MBR 52.7+ 44.6* 41.2 28.8+
Word-level MBR 52.5+ 44.7* 40.9 28.8+
MC-conjoin-SI 53.5 45.3 41.6 29.0+
</table>
<tableCaption confidence="0.982598666666667">
Table 4: BLEU performance for different system and
model combination approaches. Sentence-level and
word-level system combination operate over the sentence
output of the base systems, which are either decoded to
maximize derivation score (MAX) or to minimize Bayes
risk (MBR).
</tableCaption>
<bodyText confidence="0.999535227272727">
prisingly, n-gram features from the additional sys-
tems did not help select a better hypothesis within
the search space of a single system.
When we expand the search space to the con-
joined hypergraph (MC Conjoin), it performs worse
relative to MC 1-best. Since these two systems are
identical in their feature set, we hypothesize that
the larger search space has introduced erroneous hy-
potheses. This is similar to the scenario where MBR
Conjoin is worse than MBR 1-best. As in the MBR
case, adding system indicator features helps (MC
Conjoin/base/SI). The result is comparable to MBR
on the conjoined hypergraph with SI features.
We finally add extra n-gram features which are
computed from the conjoined hypergraph (MC Con-
join + SI). This gives the best performance although
the gains over MC Conjoin/base/SI are quite small.
Note that these added features are the same n-gram
features used in MBR Conjoin. Although they are
not strong by themselves, they provide additional
discriminative power by providing a consensus score
across all 3 base systems.
</bodyText>
<subsectionHeader confidence="0.999319">
5.3 Comparison to System Combination
</subsectionHeader>
<bodyText confidence="0.999884">
Table 4 compares model combination to two sys-
tem combination algorithms. The first, which we
call sentence-level combination, chooses among the
base systems’ three translations the sentence that
has the highest consensus score. The second, word-
level combination, builds a “word sausage” from
the outputs of the three systems and chooses a path
through the sausage with the highest score under
a similar model (Macherey and Och, 2007). Nei-
</bodyText>
<page confidence="0.992076">
981
</page>
<table confidence="0.9992344">
BLEU (%)
ar-en zh-en
Approach dev nist08 dev nist08
HG-expand 52.7* 44.5* 41.1* 28.8*
HG-noexpand 52.7* 44.5* 41.1* 28.8*
</table>
<tableCaption confidence="0.9946965">
Table 5: MBR decoding on the syntax augmented system,
with and without hypergraph expansion.
</tableCaption>
<table confidence="0.9997822">
BLEU (%)
ar-en zh-en
Posteriors dev nist08 dev nist08
Exact 52.4* 44.6* 38.6* 27.3*
Approximate 52.5* 44.6* 38.6* 27.2*
</table>
<tableCaption confidence="0.9924535">
Table 6: MBR decoding on the phrase-based system with
either exact or approximate posteriors.
</tableCaption>
<bodyText confidence="0.999582058823529">
ther system combination technique provides much
benefit, presumably because the underlying systems
all share the same data, pre-processing, language
model, alignments, and code base.
Comparing system combination when no consen-
sus (i.e., minimum Bayes risk) decoding is utilized
at all, we find that model combination improves
upon the result by up to 1.1 BLEU points. Model
combination also performs slightly better relative to
system combination over MBR-decoded systems. In
the latter case, system combination actually requires
more computation compared to model combination;
consensus decoding is performed for each system
rather than only once for model combination. This
experiment validates our approach. Model combina-
tion outperforms system combination while avoid-
ing the challenge of aligning translation hypotheses.
</bodyText>
<subsectionHeader confidence="0.95343">
5.4 Algorithmic Improvements
</subsectionHeader>
<bodyText confidence="0.999917142857143">
Section 3 describes two improvements to comput-
ing n-gram posteriors: hypergraph expansion for n-
gram locality and exact posterior computation. Ta-
ble 5 shows MBR decoding with and without expan-
sion (Algorithm 2) in a decoder that collapses nodes
due to language model back-off. These results show
that while expansion is necessary for correctness, it
does not affect performance.
Table 6 compares exact n-gram posterior compu-
tation (Algorithm 1) to the approximation described
by Kumar et al. (2009). Both methods yield identical
results. Again, while the exact method guarantees
correctness of the computation, the approximation
suffices in practice.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999973341463415">
Model combination is a consensus decoding strat-
egy over a collection of forests produced by multi-
ple machine translation systems. These systems can
have varied decoding strategies; we only require that
each system produce a forest (or a lattice) of trans-
lations. This flexibility allows the technique to be
applied quite broadly. For instance, de Gispert et al.
(2009) describe combining systems based on mul-
tiple source representations using minimum Bayes
risk decoding—likewise, they could be combined
via model combination.
Model combination has two significant advan-
tages over current approaches to system combina-
tion. First, it does not rely on hypothesis alignment
between outputs of individual systems. Aligning
translation hypotheses accurately can be challeng-
ing, and has a substantial effect on combination per-
formance (He et al., 2008). Instead of aligning hy-
potheses, we compute expectations of local features
of n-grams. This is analogous to how BLEU score is
computed, which also views sentences as vectors of
n-gram counts (Papineni et al., 2002) . Second, we
do not need to pick a backbone system for combina-
tion. Choosing a backbone system can also be chal-
lenging, and also affects system combination perfor-
mance (He and Toutanova, 2009). Model combina-
tion sidesteps this issue by working with the con-
joined forest produced by the union of the compo-
nent forests, and allows the consensus model to ex-
press system preferences via weights on system in-
dicator features.
Despite its simplicity, model combination pro-
vides strong performance by leveraging existing
consensus, search, and training techniques. The
technique outperforms MBR and consensus decod-
ing on each of the component systems. In addition,
it performs better than standard sentence-based or
word-based system combination techniques applied
to either max-derivation or MBR outputs of the indi-
vidual systems. In sum, it is a natural and effective
model-based approach to multi-system decoding.
</bodyText>
<page confidence="0.996276">
982
</page>
<sectionHeader confidence="0.993858" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999851612903226">
Ciprian Chelba and M. Mahajan. 2009. A dynamic
programming algorithm for computing the posterior
probability of n-gram occurrences in automatic speech
recognition lattices. Personal communication.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.
2009. Minimum bayes risk combination of translation
hypotheses from alternative morphological decompo-
sitions. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of the Association for Computational Lin-
guistics and IJCNLP.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proceedings of the Con-
ference on Applied Natural Language Processing.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Asso-
ciation for Computational Linguistics and IJCNLP.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
Workshop on Syntax and Structure in Statistical Trans-
lation.
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and
Ming Zhou. 2009a. Collaborative decoding: Partial
hypothesis re-ranking using translation consensus be-
tween decoders. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b.
Variational decoding for statistical machine transla-
tion. In Proceedings of the Association for Compu-
tational Linguistics and IJCNLP.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of the Association for Computational Lin-
guistics and IJCNLP.
Wolfgang Macherey and Franz Och. 2007. An empirical
study on computing consensus translations from mul-
tiple machine translation systems. In EMNLP, Prague,
Czech Republic.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Translation.
Computational Linguistics, 30(4):417 – 449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
Association for Computational Linguistics.
Antti-Veikko I. Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie J.
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice minimum Bayes-risk
decoding for statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Fuliang Weng, Andreas Stolcke, and Ananth Sankar.
1998. Efficient lattice representation and generation.
In Intl. Conf. on Spoken Language Processing.
Yong Zhao and Xiaodong He. 2009. Using n-gram based
features for machine translation system combination.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the NAACL 2006 Workshop on statisti-
cal machine translation.
</reference>
<page confidence="0.9991">
983
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.873621">
<title confidence="0.998644">Model Combination for Machine Translation</title>
<author confidence="0.887234">Shankar Kumar DeNero</author>
<author confidence="0.887234">Ciprian Chelba</author>
<author confidence="0.887234">Och</author>
<affiliation confidence="0.9937">UC Berkeley Google, Inc.</affiliation>
<abstract confidence="0.999408260869565">Machine translation benefits from two types of decoding techniques: consensus decoding over multiple hypotheses under a single model and system combination over hypotheses from models. We present combinaa method that integrates consensus decoding and system combination into a unified, forest-based technique. Our approach makes few assumptions about the underlying component models, enabling us to combine systems with heterogenous structure. Unlike most system combination techniques, we reuse the search space of component models, which entirely avoids the need to align translation hypotheses. Despite its relative simplicity, model combination improves translation quality over a pipelined approach of first applying consensus decoding to individual systems, and then applying system combination to their output. We demonstrate BLEU improvements across data sets and language pairs in large-scale experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>M Mahajan</author>
</authors>
<title>A dynamic programming algorithm for computing the posterior probability of n-gram occurrences in automatic speech recognition lattices.</title>
<date>2009</date>
<tech>Personal communication.</tech>
<contexts>
<context position="12881" citStr="Chelba and Mahajan (2009)" startWordPosition="2097" endWordPosition="2101">s(n) 8: for ` E Leaves(r) do 9: b +— b x β(`) 10: for g E Ngrams(n) do ( ) 11: �b(g) +— �b(g) x β(`) � �β(`, g) 12: β(n) +— β(n) + b 13: for g E Ngrams(n) do 14: if g E Ngrams(r) then 15: *, g) +— �β(n, g)+b 16: else 17: �β(n, g) +— 18: for g E Ngrams(root) (all g in the HG) do ˆ�(root��) 19: P(g�f) +— β(root) This algorithm can in principle compute the posterior probability of any indicator function on local features of a derivation. More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009).4 Chelba and Mahajan (2009) developed a similar algorithm for lattices. 4Indicator functions on derivations are not locally additive 3.2 Ensuring N-gram Locality DeNero et al. (2009) describes an efficient algorithm for computing n-gram expected counts from a translation forest. This method assumes n-gram locality of the forest, the property that any n-gram introduced by a hyperedge appears in all derivations that include the hyperedge. However, decoders may recombine forest nodes whenever the language model does not distinguish between n-grams due to backoff (Li and Khudanpur, 2008). In this case, a forest encoding of </context>
</contexts>
<marker>Chelba, Mahajan, 2009</marker>
<rawString>Ciprian Chelba and M. Mahajan. 2009. A dynamic programming algorithm for computing the posterior probability of n-gram occurrences in automatic speech recognition lattices. Personal communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation. Computational Linguistics.</title>
<date>2007</date>
<contexts>
<context position="21480" citStr="Chiang, 2007" startWordPosition="3488" endWordPosition="3490">g paired bootstrap resampling. In all table cells (except for Table 3) systems without statistically significant differences are marked with the same superscript. 5.1 Base Systems We combine outputs from three systems. Our phrase-based system is similar to the alignment template system described by Och and Ney (2004). Translation is performed using a standard leftto-right beam-search decoder. Our hierarchical systems consist of a syntax-augmented system (SAMT) that includes target-language syntactic categories (Zollmann and Venugopal, 2006) and a Hiero-style system with a single non-terminal (Chiang, 2007). Each base system yields state-of-the-art translation performance, summarized in Table 1. 7http://www.nist.gov/speech/tests/mt Sys Base ar BLEU (%) -en dev -en zh nist08 nist08 dev PB MAX 51.6 43.9 37.7 25.4 PB MBR 52.4* 44.6* 38.6* 27.3* PB CON 52.4* 44.6* 38.7* 27.2* Hiero MAX 50.9 43.3 40.0 27.2 Hiero MBR 51.4* 43.8* 40.6* 27.8 Hiero CON 51.5* 43.8* 40.5* 28.2 SAMT MAX 51.7 43.8 40.8* 28.4 SAMT MBR 52.7* 44.5* 41.1* 28.8* SAMT CON 52.6* 44.4* 41.1* 28.7* Table 1: Performance of baseline systems. BLEU (%) ar-en zh-en Approach dev nist08 dev nist08 Best MAX system 51.7 43.9 40.8 28.4 Best MB</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A de Gispert</author>
<author>S Virpioja</author>
<author>M Kurimo</author>
<author>W Byrne</author>
</authors>
<title>Minimum bayes risk combination of translation hypotheses from alternative morphological decompositions.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<marker>de Gispert, Virpioja, Kurimo, Byrne, 2009</marker>
<rawString>A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne. 2009. Minimum bayes risk combination of translation hypotheses from alternative morphological decompositions. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Fast consensus decoding over translation forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics and IJCNLP.</booktitle>
<contexts>
<context position="1679" citStr="DeNero et al., 2009" startWordPosition="228" endWordPosition="231">mprovements across data sets and language pairs in large-scale experiments. 1 Introduction Once statistical translation models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivations.1 We combine these components by constructing a lin</context>
<context position="10906" citStr="DeNero et al., 2009" startWordPosition="1727" endWordPosition="1730">jectives. Posterior probabilities represent a model’s belief that the translation will contain a particular ngram at least once. They can be expressed as EP(d|f) [δ(d, g)] for an indicator function δ(d, g) that is 1 if n-gram g appears in derivation d. These quantities arise in approximating BLEU for latticebased and hypergraph-based minimum Bayes risk decoding (Tromble et al., 2008; Kumar et al., 2009). Expected n-gram counts EP(d|f) [c(d, g)] represent the model’s belief of how many times an n-gram g will appear in the translation. These quantities appear in forest-based consensus decoding (DeNero et al., 2009) and variational decoding (Li et al., 2009b). Step 1: Compute Combination Features v�pb(“saw the”) = 0.9 ... vh(“saw the”) = 0.7 ... Step 2: Construct a Search Space Step 3: Add Features for the Combination Model Step 4: Model Training and Inference Rpb Rh [αpb = 1] “saw the”: [v�pb = 0.9, v�� =0.7] R [αh = 1] 977 Methods for computing both of these quantities appear in the literature. However, we address two outstanding issues below. In Section 5, we also compare the two quantities experimentally. 3.1 Computing N-gram Posteriors Exactly Kumar et al. (2009) describes an efficient approximate a</context>
<context position="13036" citStr="DeNero et al. (2009)" startWordPosition="2120" endWordPosition="2123">: if g E Ngrams(r) then 15: *, g) +— �β(n, g)+b 16: else 17: �β(n, g) +— 18: for g E Ngrams(root) (all g in the HG) do ˆ�(root��) 19: P(g�f) +— β(root) This algorithm can in principle compute the posterior probability of any indicator function on local features of a derivation. More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009).4 Chelba and Mahajan (2009) developed a similar algorithm for lattices. 4Indicator functions on derivations are not locally additive 3.2 Ensuring N-gram Locality DeNero et al. (2009) describes an efficient algorithm for computing n-gram expected counts from a translation forest. This method assumes n-gram locality of the forest, the property that any n-gram introduced by a hyperedge appears in all derivations that include the hyperedge. However, decoders may recombine forest nodes whenever the language model does not distinguish between n-grams due to backoff (Li and Khudanpur, 2008). In this case, a forest encoding of a posterior distribution may not exhibit n-gram locality in all regions of the search space. Figure 3 shows a hypergraph which contains nonlocal trigrams, </context>
<context position="22578" citStr="DeNero et al., 2009" startWordPosition="3665" endWordPosition="3668">mance of baseline systems. BLEU (%) ar-en zh-en Approach dev nist08 dev nist08 Best MAX system 51.7 43.9 40.8 28.4 Best MBR system 52.7 44.5 41.1 28.8* MC Conjoin/SI 53.5 45.3 41.6 29.0* Table 2: Performance from the best single system for each language pair without consensus decoding (Best MAX system), the best system with minimum Bayes risk decoding (Best MBR system), and model combination across three systems. For each system, we report the performance of max-derivation decoding (MAX), hypergraph-based MBR (Kumar et al., 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009). MBR and CON differ only in that the first uses n-gram posteriors, while the second uses expected n-gram counts. The two consensus decoding approaches yield comparable performance. Hence, we report performance for hypergraph-based MBR in our comparison to model combination below. 5.2 Experimental Results Table 2 compares model combination (MC) to the best MAX and MBR systems. Model combination uses a conjoined search space wherein each hyperedge is annotated with 21 features: 12 n-gram posterior features vni computed from the PB/Hiero/SAMT forests for n &lt; 4; 4 n-gram posterior features vn com</context>
</contexts>
<marker>DeNero, Chiang, Knight, 2009</marker>
<rawString>John DeNero, David Chiang, and Kevin Knight. 2009. Fast consensus decoding over translation forests. In Proceedings of the Association for Computational Linguistics and IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Frederking</author>
<author>Sergei Nirenburg</author>
</authors>
<title>Three heads are better than one.</title>
<date>1994</date>
<booktitle>In Proceedings of the Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="1896" citStr="Frederking and Nirenburg, 1994" startWordPosition="257" endWordPosition="260">ted. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component. We then optimize this consensus model over the space of all translation derivations in the support of all component models’ posterior distributions. By r</context>
</contexts>
<marker>Frederking, Nirenburg, 1994</marker>
<rawString>Robert Frederking and Sergei Nirenburg. 1994. Three heads are better than one. In Proceedings of the Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Kristina Toutanova</author>
</authors>
<title>Joint optimization for machine translation system combination.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="30748" citStr="He and Toutanova, 2009" startWordPosition="4956" endWordPosition="4959">t does not rely on hypothesis alignment between outputs of individual systems. Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al., 2008). Instead of aligning hypotheses, we compute expectations of local features of n-grams. This is analogous to how BLEU score is computed, which also views sentences as vectors of n-gram counts (Papineni et al., 2002) . Second, we do not need to pick a backbone system for combination. Choosing a backbone system can also be challenging, and also affects system combination performance (He and Toutanova, 2009). Model combination sidesteps this issue by working with the conjoined forest produced by the union of the component forests, and allows the consensus model to express system preferences via weights on system indicator features. Despite its simplicity, model combination provides strong performance by leveraging existing consensus, search, and training techniques. The technique outperforms MBR and consensus decoding on each of the component systems. In addition, it performs better than standard sentence-based or word-based system combination techniques applied to either max-derivation or MBR ou</context>
</contexts>
<marker>He, Toutanova, 2009</marker>
<rawString>Xiaodong He and Kristina Toutanova. 2009. Joint optimization for machine translation system combination. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-hmm-based hypothesis alignment for combining outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="30340" citStr="He et al., 2008" startWordPosition="4887" endWordPosition="4890"> (or a lattice) of translations. This flexibility allows the technique to be applied quite broadly. For instance, de Gispert et al. (2009) describe combining systems based on multiple source representations using minimum Bayes risk decoding—likewise, they could be combined via model combination. Model combination has two significant advantages over current approaches to system combination. First, it does not rely on hypothesis alignment between outputs of individual systems. Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al., 2008). Instead of aligning hypotheses, we compute expectations of local features of n-grams. This is analogous to how BLEU score is computed, which also views sentences as vectors of n-gram counts (Papineni et al., 2002) . Second, we do not need to pick a backbone system for combination. Choosing a backbone system can also be challenging, and also affects system combination performance (He and Toutanova, 2009). Model combination sidesteps this issue by working with the conjoined forest produced by the union of the component forests, and allows the consensus model to express system preferences via w</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-hmm-based hypothesis alignment for combining outputs from machine translation systems. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1625" citStr="Kumar and Byrne, 2004" startWordPosition="221" endWordPosition="224">ystem combination to their output. We demonstrate BLEU improvements across data sets and language pairs in large-scale experiments. 1 Introduction Once statistical translation models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivatio</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics and IJCNLP.</booktitle>
<contexts>
<context position="2825" citStr="Kumar et al., 2009" startWordPosition="393" endWordPosition="396">forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component. We then optimize this consensus model over the space of all translation derivations in the support of all component models’ posterior distributions. By reusing the components’ search spaces, we entirely avoid the hypothesis alignment problem that is central to standard system combination approaches (Rosti et al., 2007). Forest-based consensus decoding techniques differ in whether they capture model predictions through n-gram posteriors (Tromble et al., 2008; Kumar et al., 2009) or expected n-gram counts (DeNero et al., 2009; Li et al., 2009b). We evaluate both in controlled experiments, demonstrating their empirical similarity. We also describe algorithms for expanding translation forests to ensure that n-grams are local to a forest’s hyperedges, and for exactly computing n-gram posteriors efficiently. Model combination assumes only that each translation model can produce expectations of n-gram features; the latent derivation structures of component systems can differ arbitrarily. This flexibility allows us to combine phrase-based, hierarchical, and syntax-augmented</context>
<context position="8901" citStr="Kumar et al., 2009" startWordPosition="1396" endWordPosition="1399">. hyperedge) with the value of each feature. 2.4 Model Training and Inference We have defined the following combination model 07, sw(d) with weights w over derivations d from I different component models: I 4 E p 4 wz vz (d)n+gwaai (d)n+w&apos;·b(d)+w`·`(d) Because we have assessed all of these features on local rule applications, we can find the highest scori 1 J =1 n= w = ag ma BLEU ar max sw(d) ; e !&amp;quot; # $ f) ing derivation d* = arg max dED sw(d) using standard dED max-sum (Viterbi) inference over D. We learn the weights of this consensus model using hypergraph-based minimum-error-rate training (Kumar et al., 2009). This procedure maximizes the translation quality of d* on a held-out set, according to a corpus-level evaluation metric B(·; e) that compares to a reference set e. We used BLEU, choosing w to maximize the BLEU score of the set of translations predicted by the combination model. 3 Computing Combination Features The combination features vni (d) score derivations from each model with the n-gram predictions of the others. These predictions sum over all derivations under a single component model to compute a posterior belief about each n-gram. In this paper, we compare two kinds of combination fe</context>
<context position="10692" citStr="Kumar et al., 2009" startWordPosition="1693" endWordPosition="1696"> conjoining a phrase lattice with a hierarchical forest, (3) shows example hyperedge features of the combination model, including bigram features vZ and system indicators αi, and (4) gives training and decoding objectives. Posterior probabilities represent a model’s belief that the translation will contain a particular ngram at least once. They can be expressed as EP(d|f) [δ(d, g)] for an indicator function δ(d, g) that is 1 if n-gram g appears in derivation d. These quantities arise in approximating BLEU for latticebased and hypergraph-based minimum Bayes risk decoding (Tromble et al., 2008; Kumar et al., 2009). Expected n-gram counts EP(d|f) [c(d, g)] represent the model’s belief of how many times an n-gram g will appear in the translation. These quantities appear in forest-based consensus decoding (DeNero et al., 2009) and variational decoding (Li et al., 2009b). Step 1: Compute Combination Features v�pb(“saw the”) = 0.9 ... vh(“saw the”) = 0.7 ... Step 2: Construct a Search Space Step 3: Add Features for the Combination Model Step 4: Model Training and Inference Rpb Rh [αpb = 1] “saw the”: [v�pb = 0.9, v�� =0.7] R [αh = 1] 977 Methods for computing both of these quantities appear in the literatur</context>
<context position="15810" citStr="Kumar et al. (2009)" startWordPosition="2610" endWordPosition="2614">e left example, trigrams “green witch was” and “blue witch was” are non-local due to language model back-off. On Fere the right, states are split to enforce trigram locality. saw wih the teecope th h to expanding bigram lattices tomencode a trigramphistory at each lattice node (Weng et al., 1998). ... 4 Relationship to PriorcWork Model combination is a multi-system generaliza03 I telescope tion of consensus or minimum Bayes risk decodsw wi ing. When only one component system is included, Rh . w t an model combination is identical to minimum Bayes risk decoding over hypergraphs as described in Kumar et al. (2009).5 4.1 System Combination System combination techniques in machine transmbinatin Model lation take as input the outputs fel, · · · , ek} of k translation systems, where ei is a structured translare tion object (or k-best lists thereof), typically viewed as a sequence of words. The dominant approach in the field chooses a primary translation ep as a backbone, then finds an alignment ai to the backbone for Rpb each ei. A new search space is constructed from these backbone-aligned outputs, and then a voting procedure or feature-based model predicts a final consensus translation (Rosti et al., 200</context>
<context position="22493" citStr="Kumar et al., 2009" startWordPosition="3652" endWordPosition="3655">.4 SAMT MBR 52.7* 44.5* 41.1* 28.8* SAMT CON 52.6* 44.4* 41.1* 28.7* Table 1: Performance of baseline systems. BLEU (%) ar-en zh-en Approach dev nist08 dev nist08 Best MAX system 51.7 43.9 40.8 28.4 Best MBR system 52.7 44.5 41.1 28.8* MC Conjoin/SI 53.5 45.3 41.6 29.0* Table 2: Performance from the best single system for each language pair without consensus decoding (Best MAX system), the best system with minimum Bayes risk decoding (Best MBR system), and model combination across three systems. For each system, we report the performance of max-derivation decoding (MAX), hypergraph-based MBR (Kumar et al., 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009). MBR and CON differ only in that the first uses n-gram posteriors, while the second uses expected n-gram counts. The two consensus decoding approaches yield comparable performance. Hence, we report performance for hypergraph-based MBR in our comparison to model combination below. 5.2 Experimental Results Table 2 compares model combination (MC) to the best MAX and MBR systems. Model combination uses a conjoined search space wherein each hyperedge is annotated with 21 features: 12 n-gram posterior features vni </context>
<context position="29328" citStr="Kumar et al. (2009)" startWordPosition="4737" endWordPosition="4740">performs system combination while avoiding the challenge of aligning translation hypotheses. 5.4 Algorithmic Improvements Section 3 describes two improvements to computing n-gram posteriors: hypergraph expansion for ngram locality and exact posterior computation. Table 5 shows MBR decoding with and without expansion (Algorithm 2) in a decoder that collapses nodes due to language model back-off. These results show that while expansion is necessary for correctness, it does not affect performance. Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al. (2009). Both methods yield identical results. Again, while the exact method guarantees correctness of the computation, the approximation suffices in practice. 6 Conclusion Model combination is a consensus decoding strategy over a collection of forests produced by multiple machine translation systems. These systems can have varied decoding strategies; we only require that each system produce a forest (or a lattice) of translations. This flexibility allows the technique to be applied quite broadly. For instance, de Gispert et al. (2009) describe combining systems based on multiple source representatio</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proceedings of the Association for Computational Linguistics and IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimumrisk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="12853" citStr="Li and Eisner, 2009" startWordPosition="2093" endWordPosition="2096">: b(g) +— w, Vg E Ngrams(n) 8: for ` E Leaves(r) do 9: b +— b x β(`) 10: for g E Ngrams(n) do ( ) 11: �b(g) +— �b(g) x β(`) � �β(`, g) 12: β(n) +— β(n) + b 13: for g E Ngrams(n) do 14: if g E Ngrams(r) then 15: *, g) +— �β(n, g)+b 16: else 17: �β(n, g) +— 18: for g E Ngrams(root) (all g in the HG) do ˆ�(root��) 19: P(g�f) +— β(root) This algorithm can in principle compute the posterior probability of any indicator function on local features of a derivation. More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009).4 Chelba and Mahajan (2009) developed a similar algorithm for lattices. 4Indicator functions on derivations are not locally additive 3.2 Ensuring N-gram Locality DeNero et al. (2009) describes an efficient algorithm for computing n-gram expected counts from a translation forest. This method assumes n-gram locality of the forest, the property that any n-gram introduced by a hyperedge appears in all derivations that include the hyperedge. However, decoders may recombine forest nodes whenever the language model does not distinguish between n-grams due to backoff (Li and Khudanpur, 2008). In this</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimumrisk training on translation forests. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
<date>2008</date>
<booktitle>In ACL Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<contexts>
<context position="13444" citStr="Li and Khudanpur, 2008" startWordPosition="2185" endWordPosition="2188">l features (Li and Eisner, 2009).4 Chelba and Mahajan (2009) developed a similar algorithm for lattices. 4Indicator functions on derivations are not locally additive 3.2 Ensuring N-gram Locality DeNero et al. (2009) describes an efficient algorithm for computing n-gram expected counts from a translation forest. This method assumes n-gram locality of the forest, the property that any n-gram introduced by a hyperedge appears in all derivations that include the hyperedge. However, decoders may recombine forest nodes whenever the language model does not distinguish between n-grams due to backoff (Li and Khudanpur, 2008). In this case, a forest encoding of a posterior distribution may not exhibit n-gram locality in all regions of the search space. Figure 3 shows a hypergraph which contains nonlocal trigrams, along with its local expansion. Algorithm 2 expands a forest to ensure n-gram locality while preserving the encoded distribution over derivations. Let a forest (N, R) consist of nodes N and hyperedges R, which correspond to rule applications. Let Rules(n) be the subset of R rooted by n, and Leaves(r) be the leaf nodes of rule application r. The expanded forest (Ne, Re) is constructed by a function Reapply</context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. A scalable decoder for parsing-based machine translation with equivalent language model state maintenance. In ACL Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mu Li</author>
<author>Nan Duan</author>
<author>Dongdong Zhang</author>
<author>Chi-Ho Li</author>
<author>Ming Zhou</author>
</authors>
<title>Collaborative decoding: Partial hypothesis re-ranking using translation consensus between decoders.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1742" citStr="Li et al., 2009" startWordPosition="237" endWordPosition="240">iments. 1 Introduction Once statistical translation models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component.</context>
<context position="10948" citStr="Li et al., 2009" startWordPosition="1734" endWordPosition="1737">odel’s belief that the translation will contain a particular ngram at least once. They can be expressed as EP(d|f) [δ(d, g)] for an indicator function δ(d, g) that is 1 if n-gram g appears in derivation d. These quantities arise in approximating BLEU for latticebased and hypergraph-based minimum Bayes risk decoding (Tromble et al., 2008; Kumar et al., 2009). Expected n-gram counts EP(d|f) [c(d, g)] represent the model’s belief of how many times an n-gram g will appear in the translation. These quantities appear in forest-based consensus decoding (DeNero et al., 2009) and variational decoding (Li et al., 2009b). Step 1: Compute Combination Features v�pb(“saw the”) = 0.9 ... vh(“saw the”) = 0.7 ... Step 2: Construct a Search Space Step 3: Add Features for the Combination Model Step 4: Model Training and Inference Rpb Rh [αpb = 1] “saw the”: [v�pb = 0.9, v�� =0.7] R [αh = 1] 977 Methods for computing both of these quantities appear in the literature. However, we address two outstanding issues below. In Section 5, we also compare the two quantities experimentally. 3.1 Computing N-gram Posteriors Exactly Kumar et al. (2009) describes an efficient approximate algorithm for computing n-gram posterior pr</context>
<context position="19722" citStr="Li et al. (2009" startWordPosition="3219" endWordPosition="3222">most models can be decoded either left-to6A single hypothesis can be represented as a forest, while an unweighted forest could be assigned a uniform distribution. applied rule rule leaves green witch was here blue witch was here green witch blue witch rule root was here n �-4 P(n) green witch blue witch green witch was here blue witch was here was here Yo il hb &apos; el telescopi cedure c 979 right or bottom-up, such changes can have substantial implications for search efficiency and search error. We prefer to maintain the flexibility of using different search strategies in each component system. Li et al. (2009a) is another related technique for combining translation systems by leveraging model predictions of n-gram features. K-best lists of partial translations are iteratively reranked using ngram features from the predictions of other models (which are also iteratively updated). Our technique differs in that we use no k-best approximations, have fewer parameters to learn (one consensus weight vector rather than one for each collaborating decoder) and produce only one output, avoiding an additional system combination step at the end. 5 Experiments We report results on the constrained data track of </context>
</contexts>
<marker>Li, Duan, Zhang, Li, Zhou, 2009</marker>
<rawString>Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and Ming Zhou. 2009a. Collaborative decoding: Partial hypothesis re-ranking using translation consensus between decoders. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics and IJCNLP.</booktitle>
<contexts>
<context position="1742" citStr="Li et al., 2009" startWordPosition="237" endWordPosition="240">iments. 1 Introduction Once statistical translation models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component.</context>
<context position="10948" citStr="Li et al., 2009" startWordPosition="1734" endWordPosition="1737">odel’s belief that the translation will contain a particular ngram at least once. They can be expressed as EP(d|f) [δ(d, g)] for an indicator function δ(d, g) that is 1 if n-gram g appears in derivation d. These quantities arise in approximating BLEU for latticebased and hypergraph-based minimum Bayes risk decoding (Tromble et al., 2008; Kumar et al., 2009). Expected n-gram counts EP(d|f) [c(d, g)] represent the model’s belief of how many times an n-gram g will appear in the translation. These quantities appear in forest-based consensus decoding (DeNero et al., 2009) and variational decoding (Li et al., 2009b). Step 1: Compute Combination Features v�pb(“saw the”) = 0.9 ... vh(“saw the”) = 0.7 ... Step 2: Construct a Search Space Step 3: Add Features for the Combination Model Step 4: Model Training and Inference Rpb Rh [αpb = 1] “saw the”: [v�pb = 0.9, v�� =0.7] R [αh = 1] 977 Methods for computing both of these quantities appear in the literature. However, we address two outstanding issues below. In Section 5, we also compare the two quantities experimentally. 3.1 Computing N-gram Posteriors Exactly Kumar et al. (2009) describes an efficient approximate algorithm for computing n-gram posterior pr</context>
<context position="19722" citStr="Li et al. (2009" startWordPosition="3219" endWordPosition="3222">most models can be decoded either left-to6A single hypothesis can be represented as a forest, while an unweighted forest could be assigned a uniform distribution. applied rule rule leaves green witch was here blue witch was here green witch blue witch rule root was here n �-4 P(n) green witch blue witch green witch was here blue witch was here was here Yo il hb &apos; el telescopi cedure c 979 right or bottom-up, such changes can have substantial implications for search efficiency and search error. We prefer to maintain the flexibility of using different search strategies in each component system. Li et al. (2009a) is another related technique for combining translation systems by leveraging model predictions of n-gram features. K-best lists of partial translations are iteratively reranked using ngram features from the predictions of other models (which are also iteratively updated). Our technique differs in that we use no k-best approximations, have fewer parameters to learn (one consensus weight vector rather than one for each collaborating decoder) and produce only one output, avoiding an additional system combination step at the end. 5 Experiments We report results on the constrained data track of </context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b. Variational decoding for statistical machine translation. In Proceedings of the Association for Computational Linguistics and IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Haitao Mi</author>
<author>Yang Feng</author>
<author>Qun Liu</author>
</authors>
<title>Joint decoding with multiple translation models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics and IJCNLP.</booktitle>
<contexts>
<context position="18367" citStr="Liu et al. (2009)" startWordPosition="2996" endWordPosition="2999">m k-best lists. Despite these advantages, system combination may bepmore appropriate in some settings. In particular, model combination is designed primarily for statistical systems that generate hypergraph outputs. Model combination can in principle integrate a nonstatisticalesystem that generates either a single hypothesis or an unweighted forest.6 Likewise, the prooould be applied to statistical systems that only generate k-best lists. However, we would not expect the same strong performance from model combination in these constrained settings. 4.2 Joint Decoding and Collaborative Decoding Liu et al. (2009) describes two techniques for combining multiple synchronous grammars, which the R authors characterize as joint decoding. Joint decoding does not involve a consensus or minimumh Bayes-risk decoding objective; indeed, their best results come from standard max-derivation decoding (with a multi-system grammar). More importantly, their computations rely on a correspondence between nodes in the hypergraph outputs of different systems, and so they can only joint decode over models with similar search strategies. We combine a phrase-based model that uses left-to-right decoding with two hierarchical </context>
</contexts>
<marker>Liu, Mi, Feng, Liu, 2009</marker>
<rawString>Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009. Joint decoding with multiple translation models. In Proceedings of the Association for Computational Linguistics and IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Och</author>
</authors>
<title>An empirical study on computing consensus translations from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In EMNLP,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="27539" citStr="Macherey and Och, 2007" startWordPosition="4472" endWordPosition="4475">join. Although they are not strong by themselves, they provide additional discriminative power by providing a consensus score across all 3 base systems. 5.3 Comparison to System Combination Table 4 compares model combination to two system combination algorithms. The first, which we call sentence-level combination, chooses among the base systems’ three translations the sentence that has the highest consensus score. The second, wordlevel combination, builds a “word sausage” from the outputs of the three systems and chooses a path through the sausage with the highest score under a similar model (Macherey and Och, 2007). Nei981 BLEU (%) ar-en zh-en Approach dev nist08 dev nist08 HG-expand 52.7* 44.5* 41.1* 28.8* HG-noexpand 52.7* 44.5* 41.1* 28.8* Table 5: MBR decoding on the syntax augmented system, with and without hypergraph expansion. BLEU (%) ar-en zh-en Posteriors dev nist08 dev nist08 Exact 52.4* 44.6* 38.6* 27.3* Approximate 52.5* 44.6* 38.6* 27.2* Table 6: MBR decoding on the phrase-based system with either exact or approximate posteriors. ther system combination technique provides much benefit, presumably because the underlying systems all share the same data, pre-processing, language model, alignm</context>
</contexts>
<marker>Macherey, Och, 2007</marker>
<rawString>Wolfgang Macherey and Franz Och. 2007. An empirical study on computing consensus translations from multiple machine translation systems. In EMNLP, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>449</pages>
<contexts>
<context position="21185" citStr="Och and Ney (2004)" startWordPosition="3447" endWordPosition="3450">test on the NIST 2008 evaluation set. We report results using the IBM implementation of the BLEU score which computes the brevity penalty using the closest reference translation for each segment (Papineni et al., 2002). We measure statistical significance using 95% confidence intervals computed using paired bootstrap resampling. In all table cells (except for Table 3) systems without statistically significant differences are marked with the same superscript. 5.1 Base Systems We combine outputs from three systems. Our phrase-based system is similar to the alignment template system described by Och and Ney (2004). Translation is performed using a standard leftto-right beam-search decoder. Our hierarchical systems consist of a syntax-augmented system (SAMT) that includes target-language syntactic categories (Zollmann and Venugopal, 2006) and a Hiero-style system with a single non-terminal (Chiang, 2007). Each base system yields state-of-the-art translation performance, summarized in Table 1. 7http://www.nist.gov/speech/tests/mt Sys Base ar BLEU (%) -en dev -en zh nist08 nist08 dev PB MAX 51.6 43.9 37.7 25.4 PB MBR 52.4* 44.6* 38.6* 27.3* PB CON 52.4* 44.6* 38.7* 27.2* Hiero MAX 50.9 43.3 40.0 27.2 Hier</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, 30(4):417 – 449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="20785" citStr="Papineni et al., 2002" startWordPosition="3387" endWordPosition="3390">oder) and produce only one output, avoiding an additional system combination step at the end. 5 Experiments We report results on the constrained data track of the NIST 2008 Arabic-to-English (ar-en) and Chineseto-English (zh-en) translation tasks.7 We train on all parallel and monolingual data allowed in the track. We use the NIST 2004 eval set (dev) for optimizing parameters in model combination and test on the NIST 2008 evaluation set. We report results using the IBM implementation of the BLEU score which computes the brevity penalty using the closest reference translation for each segment (Papineni et al., 2002). We measure statistical significance using 95% confidence intervals computed using paired bootstrap resampling. In all table cells (except for Table 3) systems without statistically significant differences are marked with the same superscript. 5.1 Base Systems We combine outputs from three systems. Our phrase-based system is similar to the alignment template system described by Och and Ney (2004). Translation is performed using a standard leftto-right beam-search decoder. Our hierarchical systems consist of a syntax-augmented system (SAMT) that includes target-language syntactic categories (Z</context>
<context position="30555" citStr="Papineni et al., 2002" startWordPosition="4922" endWordPosition="4925">ing minimum Bayes risk decoding—likewise, they could be combined via model combination. Model combination has two significant advantages over current approaches to system combination. First, it does not rely on hypothesis alignment between outputs of individual systems. Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al., 2008). Instead of aligning hypotheses, we compute expectations of local features of n-grams. This is analogous to how BLEU score is computed, which also views sentences as vectors of n-gram counts (Papineni et al., 2002) . Second, we do not need to pick a backbone system for combination. Choosing a backbone system can also be challenging, and also affects system combination performance (He and Toutanova, 2009). Model combination sidesteps this issue by working with the conjoined forest produced by the union of the component forests, and allows the consensus model to express system preferences via weights on system indicator features. Despite its simplicity, model combination provides strong performance by leveraging existing consensus, search, and training techniques. The technique outperforms MBR and consens</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Necip Fazil Ayan</author>
<author>Bing Xiang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2663" citStr="Rosti et al., 2007" startWordPosition="370" endWordPosition="373">of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component. We then optimize this consensus model over the space of all translation derivations in the support of all component models’ posterior distributions. By reusing the components’ search spaces, we entirely avoid the hypothesis alignment problem that is central to standard system combination approaches (Rosti et al., 2007). Forest-based consensus decoding techniques differ in whether they capture model predictions through n-gram posteriors (Tromble et al., 2008; Kumar et al., 2009) or expected n-gram counts (DeNero et al., 2009; Li et al., 2009b). We evaluate both in controlled experiments, demonstrating their empirical similarity. We also describe algorithms for expanding translation forests to ensure that n-grams are local to a forest’s hyperedges, and for exactly computing n-gram posteriors efficiently. Model combination assumes only that each translation model can produce expectations of n-gram features; th</context>
<context position="16412" citStr="Rosti et al., 2007" startWordPosition="2712" endWordPosition="2715">mar et al. (2009).5 4.1 System Combination System combination techniques in machine transmbinatin Model lation take as input the outputs fel, · · · , ek} of k translation systems, where ei is a structured translare tion object (or k-best lists thereof), typically viewed as a sequence of words. The dominant approach in the field chooses a primary translation ep as a backbone, then finds an alignment ai to the backbone for Rpb each ei. A new search space is constructed from these backbone-aligned outputs, and then a voting procedure or feature-based model predicts a final consensus translation (Rosti et al., 2007). Model combination entirely avoids this alignment problem by viewing hypotheses as n-gram occurrence vectors rather than word sequences. Model combination also requires less total computation than applying system combination to 5We do not refer to model combination as a minimum Bayes risk decoding procedure despite this similarity because risk implies a belief distribution over outputs, and we now have multiple output distributions that are not necessarily calibrated. Moreover, our generalized, multi-model objective (Section 2.4) is motivated by BLEU, but not a direct approximation to it. con</context>
</contexts>
<marker>Rosti, Ayan, Xiang, Matsoukas, Schwartz, Dorr, 2007</marker>
<rawString>Antti-Veikko I. Rosti, Necip Fazil Ayan, Bing Xiang, Spyros Matsoukas, Richard Schwartz, and Bonnie J. Dorr. 2007. Combining outputs from multiple machine translation systems. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Shankar Kumar</author>
<author>Franz J Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2804" citStr="Tromble et al., 2008" startWordPosition="389" endWordPosition="392">vations, encoded as a forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component. We then optimize this consensus model over the space of all translation derivations in the support of all component models’ posterior distributions. By reusing the components’ search spaces, we entirely avoid the hypothesis alignment problem that is central to standard system combination approaches (Rosti et al., 2007). Forest-based consensus decoding techniques differ in whether they capture model predictions through n-gram posteriors (Tromble et al., 2008; Kumar et al., 2009) or expected n-gram counts (DeNero et al., 2009; Li et al., 2009b). We evaluate both in controlled experiments, demonstrating their empirical similarity. We also describe algorithms for expanding translation forests to ensure that n-grams are local to a forest’s hyperedges, and for exactly computing n-gram posteriors efficiently. Model combination assumes only that each translation model can produce expectations of n-gram features; the latent derivation structures of component systems can differ arbitrarily. This flexibility allows us to combine phrase-based, hierarchical,</context>
<context position="10671" citStr="Tromble et al., 2008" startWordPosition="1689" endWordPosition="1692"> depicts the result of conjoining a phrase lattice with a hierarchical forest, (3) shows example hyperedge features of the combination model, including bigram features vZ and system indicators αi, and (4) gives training and decoding objectives. Posterior probabilities represent a model’s belief that the translation will contain a particular ngram at least once. They can be expressed as EP(d|f) [δ(d, g)] for an indicator function δ(d, g) that is 1 if n-gram g appears in derivation d. These quantities arise in approximating BLEU for latticebased and hypergraph-based minimum Bayes risk decoding (Tromble et al., 2008; Kumar et al., 2009). Expected n-gram counts EP(d|f) [c(d, g)] represent the model’s belief of how many times an n-gram g will appear in the translation. These quantities appear in forest-based consensus decoding (DeNero et al., 2009) and variational decoding (Li et al., 2009b). Step 1: Compute Combination Features v�pb(“saw the”) = 0.9 ... vh(“saw the”) = 0.7 ... Step 2: Construct a Search Space Step 3: Add Features for the Combination Model Step 4: Model Training and Inference Rpb Rh [αpb = 1] “saw the”: [v�pb = 0.9, v�� =0.7] R [αh = 1] 977 Methods for computing both of these quantities ap</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy Tromble, Shankar Kumar, Franz J. Och, and Wolfgang Macherey. 2008. Lattice minimum Bayes-risk decoding for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuliang Weng</author>
<author>Andreas Stolcke</author>
<author>Ananth Sankar</author>
</authors>
<title>Efficient lattice representation and generation.</title>
<date>1998</date>
<booktitle>In Intl. Conf. on Spoken Language Processing.</booktitle>
<contexts>
<context position="15488" citStr="Weng et al., 1998" startWordPosition="2555" endWordPosition="2558"> The process is analogous over the rules of a derivation, even if the features they indicate are local. Therefore, Algorithm 1 is not an instance of an expectation semiring computation. �β(n, g)+b — b(g) 978 Figure 3: Hypergraph expansion ensures n-gram locality without affecting the distribution over derivations. In the left example, trigrams “green witch was” and “blue witch was” are non-local due to language model back-off. On Fere the right, states are split to enforce trigram locality. saw wih the teecope th h to expanding bigram lattices tomencode a trigramphistory at each lattice node (Weng et al., 1998). ... 4 Relationship to PriorcWork Model combination is a multi-system generaliza03 I telescope tion of consensus or minimum Bayes risk decodsw wi ing. When only one component system is included, Rh . w t an model combination is identical to minimum Bayes risk decoding over hypergraphs as described in Kumar et al. (2009).5 4.1 System Combination System combination techniques in machine transmbinatin Model lation take as input the outputs fel, · · · , ek} of k translation systems, where ei is a structured translare tion object (or k-best lists thereof), typically viewed as a sequence of words. </context>
</contexts>
<marker>Weng, Stolcke, Sankar, 1998</marker>
<rawString>Fuliang Weng, Andreas Stolcke, and Ananth Sankar. 1998. Efficient lattice representation and generation. In Intl. Conf. on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yong Zhao</author>
<author>Xiaodong He</author>
</authors>
<title>Using n-gram based features for machine translation system combination.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="17632" citStr="Zhao and He (2009)" startWordPosition="2889" endWordPosition="2892">nsus-decoded outputs. The best consensus decoding methods for individual systems already require the computation-intensive steps of model combination: producing lattices or forests, computing ngram feature expectations, and re-decoding to maximize a secondary consensus objective. Hence, to maximize the performance of system combination, these steps must be performed for each system, whereas model combination requires only one forest rescoring pass over all systems. Model combination also leverages aggregate statistics from the components’ posteriors, whereas system combiners typically do not. Zhao and He (2009) showed that n-gram posterior features are useful in the context of a system combination model, even when computed from k-best lists. Despite these advantages, system combination may bepmore appropriate in some settings. In particular, model combination is designed primarily for statistical systems that generate hypergraph outputs. Model combination can in principle integrate a nonstatisticalesystem that generates either a single hypothesis or an unweighted forest.6 Likewise, the prooould be applied to statistical systems that only generate k-best lists. However, we would not expect the same s</context>
</contexts>
<marker>Zhao, He, 2009</marker>
<rawString>Yong Zhao and Xiaodong He. 2009. Using n-gram based features for machine translation system combination. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the NAACL 2006 Workshop on statistical machine translation.</booktitle>
<contexts>
<context position="21413" citStr="Zollmann and Venugopal, 2006" startWordPosition="3476" endWordPosition="3479">). We measure statistical significance using 95% confidence intervals computed using paired bootstrap resampling. In all table cells (except for Table 3) systems without statistically significant differences are marked with the same superscript. 5.1 Base Systems We combine outputs from three systems. Our phrase-based system is similar to the alignment template system described by Och and Ney (2004). Translation is performed using a standard leftto-right beam-search decoder. Our hierarchical systems consist of a syntax-augmented system (SAMT) that includes target-language syntactic categories (Zollmann and Venugopal, 2006) and a Hiero-style system with a single non-terminal (Chiang, 2007). Each base system yields state-of-the-art translation performance, summarized in Table 1. 7http://www.nist.gov/speech/tests/mt Sys Base ar BLEU (%) -en dev -en zh nist08 nist08 dev PB MAX 51.6 43.9 37.7 25.4 PB MBR 52.4* 44.6* 38.6* 27.3* PB CON 52.4* 44.6* 38.7* 27.2* Hiero MAX 50.9 43.3 40.0 27.2 Hiero MBR 51.4* 43.8* 40.6* 27.8 Hiero CON 51.5* 43.8* 40.5* 28.2 SAMT MAX 51.7 43.8 40.8* 28.4 SAMT MBR 52.7* 44.5* 41.1* 28.8* SAMT CON 52.6* 44.4* 41.1* 28.7* Table 1: Performance of baseline systems. BLEU (%) ar-en zh-en Approac</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the NAACL 2006 Workshop on statistical machine translation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>