<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000140">
<title confidence="0.936643">
Modelling Discourse Relations for Arabic
</title>
<author confidence="0.964741">
Amal Alsaif
</author>
<affiliation confidence="0.992622">
University of Leeds
</affiliation>
<address confidence="0.881468">
Leeds, UK
LS2 9JT
</address>
<email confidence="0.998131">
amalalsaif@yahoo.co.uk
</email>
<author confidence="0.979118">
Katja Markert
</author>
<affiliation confidence="0.990616">
University of Leeds
</affiliation>
<address confidence="0.880162">
Leeds, UK
LS2 9JT
</address>
<email confidence="0.999037">
markert@comp.leeds.ac.uk
</email>
<sectionHeader confidence="0.996664" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999235">
We present the first algorithms to automat-
ically identify explicit discourse connectives
and the relations they signal for Arabic text.
First we show that, for Arabic news, most
adjacent sentences are connected via explicit
connectives in contrast to English, making the
treatment of explicit discourse connectives for
Arabic highly important. We also show that
explicit Arabic discourse connectives are far
more ambiguous than English ones, making
their treatment challenging. In the second
part of the paper, we present supervised al-
gorithms to address automatic discourse con-
nective identification and discourse relation
recognition. Our connective identifier based
on gold standard syntactic features achieves
almost human performance. In addition, an
identifier based solely on simple lexical and
automatically derived morphological and POS
features performs with high reliability, essen-
tial for languages that do not have high-quality
parsers yet. Our algorithm for recognizing dis-
course relations performs significantly better
than a baseline based on the connective sur-
face string alone and therefore reduces the am-
biguity in explicit connective interpretation.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999475178571429">
The automatic detection of discourse relations, such
as causal, contrast or temporal relations, is useful for
many applications such as automatic summarization
(Marcu, 2000), question answering (Girju, 2003),
sentiment analysis (Somasundaran et al., 2008) and
readability assessment (Pitler and Nenkova, 2008).
This task has recently seen renewed interest due to
the growing availability of large-scale corpora anno-
tated for discourse relations, such as the Penn Dis-
course Treebank (Prasad et al., 2008a).
In the Penn Discourse Treebank (PDTB), lo-
cal discourse relations (also called senses) such as
CAUSAL or CONTRAST are annotated. They hold
between two text segments (so-called arguments)
that express abstract entities such as events, facts and
propositions. Annotated discourse relations can be
signalled explicitly by so-called discourse connec-
tives (Marcu, 2000; Webber et al., 1999; Prasad et
al., 2008a) or hold implicitly between adjacent sen-
tences in the same paragraph, i.e. are not signalled
by a specific surface string. In Ex. 1, the connec-
tive while indicates an explicit CONTRAST between
the attitudes of John and Richard. In Ex. 2, the con-
nective while indicates an explicit TEMPORAL rela-
tion. In Ex. 3, an implicit CAUSAL relation between
the first and second sentence holds. We indicate dis-
course connectives and the two arguments they re-
late via annotated square brackets.
</bodyText>
<listItem confidence="0.992875">
(1) [John liked adventure,]Arg2 [ while]DC[Richard
was cautious]Arg2
(2) [The children were crying
loudly]Arg1[while]DC,[their mother was
cooking]Arg2
(3) [I cannot eat any dessert.]Arg1 [I have eaten far
too much already.]Arg2
</listItem>
<bodyText confidence="0.996842">
Although similar corpora for other languages are
being developed such as for Hindi (Prasad et al.,
2008b), Turkish (Zeyrek and Webber, 2008), Chi-
nese (Xue, 2005) and, by ourselves, for Arabic (Al-
</bodyText>
<page confidence="0.973126">
736
</page>
<note confidence="0.9587705">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 736–747,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999075783783784">
Saif and Markert, 2010), efforts in the automated
recognition of discourse connectives, arguments and
relations have so far almost exclusively centered on
English.
In contrast we present the first models for dis-
course relations for Arabic, focusing on explicit con-
nectives. This focus is partially justified by the fact
that this first study for a new language should cen-
ter on the superficially more straightforward case
and that no annotations for implicit relations are yet
available for Arabic. More importantly, however,
we make two essential claims (Section 4). Firstly,
Arabic discourse connectives are more ambiguous
than their English counterparts, i.e cases such as
while which can signal different relations dependent
on context (see Example 1 and 2) are far more fre-
quent. This makes their treatment more challenging.
Secondly, discourse relations between adjacent sen-
tences in Arabic tend to be expressed via an explicit
connective, at least for the news genre, i.e. cases
such as Example 3 are rarer. This makes the treat-
ment of explicit connectives essential.
We tackle two tasks for explicit Arabic connec-
tives in this paper, which are further discussed in
Section 2. Discourse connective recognition needs
to distinguish between discourse usage of potential
connectives and non-discourse usage (such as the
use of while as a noun). We show in Section 5 that
we can distinguish discourse- and non-discourse us-
age for potential connectives in Arabic with very
high reliability, even without parsed data, a fact that
is important for languages with fewer high quality
NLP tools available. We then present an algorithm
for relation identification in Section 6 that shows
small but significant gains over assigning the most
frequent relation for each connective. We discuss
future work and conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.982448" genericHeader="method">
2 The Tasks
</sectionHeader>
<bodyText confidence="0.99992132">
The handling of explicit connectives can be split into
three tasks (Pitler and Nenkova, 2009). The first task
of discourse connective recognition distinguishes
between the discourse usage and non-discourse us-
age of potential connectives. Whereas some poten-
tial connectives such as the Arabic connective A
/lkn/but almost always have discourse usage, this is
not true for all potential connectives.1 Thus, the dis-
course usage of Arabic L. /r˙gbh/desire needs to
be distinguished from its use as a noun. Conjunc-
tions such as 9 /w/and,91 /¯aw/or can have discourse
usage or just conjoin two non-abstract entities as in
o�l.. 9 /qnr w s¯arh/Omar and Sarah.
The second task is discourse connective interpre-
tation where a discourse connective in context is as-
signed a discourse relation. Again, some connec-
tives are largely unambiguous in this respect. For
example, A/lkn/but signals almost always a CON-
TRAST relation. However, there are connectives
where this is not the case, such as .a;A /mnd¯/since
which has a CAUSAL and a TEMPORAL sense.
The third task is argument identification which
identifies the arguments’ position and extent. In this
paper we tackle Task 1 and Task 2 for Arabic in a
supervised machine learning framework.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="method">
3 Related work
</sectionHeader>
<bodyText confidence="0.978444090909091">
Annotated Discourse Corpora and Linguistic
Background. Discourse relations are widely stud-
ied in theoretical linguistics (Halliday and Hasan,
1976; Hobbs, 1985), where also different relation
taxonomies have been derived (Hobbs, 1985; Knott
and Sanders, 1998; Mann and Thompson, 1988;
Marcu, 2000). Different inventories have been used
in English corpora annotated for discourse relations
(Hobbs, 1985; Prasad et al., 2008a; Carlson et al.,
2002) which also differ in other respects (such as
whether they prescribe a tree structure for discourse
annotation). However, the annotation level of ex-
isting Arabic corpora has not yet included the dis-
course layer, making our work the first to address
this problem for Arabic on a larger scale.
Automatic discourse parsing: explicit relations.
There is no work on discourse connective recog-
nition, interpretation and argument assignment for
Arabic, so that we break entirely new ground here.
However, the two tasks we explore (discourse con-
nective recognition and discourse connective disam-
biguation) have been tackled for English.2 (Pitler
</bodyText>
<footnote confidence="0.999557">
1Arabic examples contain in order: the Arabic right-to-left
script, the transliteration (standards ISO/R 233 and DIN 31635)
and the English translation (if possible).
2There is also substantial work on argument identification
(Wellner and Pustejovski, 2007; Elwell and Baldridge, 2008)
</footnote>
<page confidence="0.997019">
737
</page>
<bodyText confidence="0.992700909090909">
and Nenkova, 2009) use gold standard syntactic fea-
tures as well as the connective surface string in a
supervised model for discourse connective recogni-
tion. They achieve very high results with this ap-
proach. We will (i) show that similar features work
well for Arabic (ii) take into account Arabic-specific
morphological properties that improve results fur-
ther and (iii) present a robust version of this ap-
proach that does not rely on full parsing or gold stan-
dard syntactic annotations.
With regard to discourse connective interpreta-
tion, (Miltsakaki et al., 2005) concentrate on disam-
biguating the three connectives since, while, when
only, using a very small set of features indicating
tense and temporal markers in arguments. They
achieve good improvements over a “most frequent
relation per connective” baseline. A more compre-
hensive study on all discourse connectives in the
PDTB (Pitler et al., 2008; Pitler and Nenkova, 2009)
reveals that most connectives are not ambiguous in
English. Using syntactic features of the connec-
tive, they achieve only a very small improvement
over a “most frequent relation per connective base-
line” for which significance tests are not given. We
will show that for Arabic, discourse connectives are
more highly ambiguous with regard to the relations
they convey. We will present a supervised learning
model that uses a wider feature set and that achieves
small but significant improvements over the most
frequent relation per connective baseline.
Automatic discourse parsing: implicit relations.
Implicit relations have excited substantial interest
for English. This includes work in the frame-
work of RST (Soricut and Marcu, 2003; duVerle
and Prendinger, 2009; Marcu and Echihabi, 2002),
SDRT (Baldridge and Lascarides, 2005), Graph-
Bank (Wellner et al., 2006), the PDTB (Blair-
Goldensohn et al., 2007; Pitler et al., 2009; Lin
et al., 2009; Wang et al., 2010; Zhou et al.,
2010; Louis and Nenkova, 2010) or framework-
independent (Sporleder and Lascarides, 2008).3 The
task is challenging as implicits behave substantially
differently from explicits (Sporleder and Lascarides,
but we do not discuss this work in depth here.
</bodyText>
<footnote confidence="0.871713333333333">
3Some work does not make the distinction between implicit
and explicit and/or treats them in a joint framework (Soricut and
Marcu, 2003; Wellner et al., 2006; Wang et al., 2010).
</footnote>
<bodyText confidence="0.999018">
2008) and often need world knowledge (Lin et
al., 2009). However, features/approaches that have
shown improvement over a baseline are word pairs
(Sporleder and Lascarides, 2008), production rules
and syntactic trees (Wang et al., 2010; Lin et al.,
2009) as well as language modelling (Zhou et al.,
2010). As we only deal with explicit connectives
this work is not directly comparable to ours, al-
though we do explore some of the suggested features
for improving explicit connective disambiguation.
</bodyText>
<sectionHeader confidence="0.945354" genericHeader="method">
4 An Arabic Discourse Corpus
</sectionHeader>
<bodyText confidence="0.999956666666667">
We annotate news articles from the Arabic Penn
Treebank (Part 1 v2.0) (Maamouri and Bies, 2004)
for explicitly marked discourse relations. This is the
first discourse-annotated corpus for Arabic, whose
initial development stages we have described in (Al-
Saif and Markert, 2010). We summarize this previ-
ous work and extend it by including agreement stud-
ies for arguments in Sections 4.1 and 4.2. In Sec-
tions 4.3, 4.4 and 4.5. we then present a corpus study
on the corpus which shows our major claim as to the
importance and high levels of ambiguity of Arabic
discourse connectives.
</bodyText>
<subsectionHeader confidence="0.998453">
4.1 Annotation Principles
</subsectionHeader>
<bodyText confidence="0.998271722222222">
We overall follow the annotation principles in the
Penn Discourse Treebank for explicit connectives
(for example, arguments can occur at any distance
from the connectives). The relation set we use is
a more coarse-grained version of the PDTB rela-
tions with two relations added — BACKGROUND
and SIMILARITY — that we found in our Arabic
news texts. The final, hierarchically organized, re-
lation set of 17 discourse relations is shown in Fig 1.
Further adaptations necessary for Arabic are the
inclusion of clitics as connectives such as È /l/for, H.
/b/by,with and ¬ /f/then . In addition, differently to
English, prepositions were included as connectives
as these are frequently used to express discourse re-
lations in Arabic. In these cases, normally argument
2 is the so-called Al-Masdar.4 Typical examples are
Èñ“ð /ws.wl/arrival from the verb É“ð /ws./to ar-
rive and aËðAm× /mh. ¯awlh/attempt from the verb ÈðAg
</bodyText>
<footnote confidence="0.921923666666667">
4The medieval Arabic grammar schools, the Basra and Kufa,
debated whether the noun (almasdar) or the verb is the most
basic element of language (Ryding, 2005).
</footnote>
<page confidence="0.990249">
738
</page>
<figureCaption confidence="0.999746">
Figure 1: Discourse relations for Arabic
</figureCaption>
<bodyText confidence="0.990014454545454">
/h. ¯awl/to try. Al-Masdar is formed using morpho-
logical patterns well-known in the Arabic grammat-
ical tradition: major Arabic grammars list around 60
patterns although some other references also claim
that the patterns are many more as well as more un-
predictable (Abdl al latif et al., 1997; Wright, 2008;
Ryding, 2005). Al-Masdar forms do not fit into one
grammatical or morphological category in English:
they might correspond to a gerund, a nominalization
or a noun which is not a nominalization. Some ex-
amples are listed in Table 1.
</bodyText>
<tableCaption confidence="0.9616295">
Table 1: A list of Al-MaSdar patterns, examples and their
English correspondence
</tableCaption>
<table confidence="0.995325375">
Root Pattern MaSdar Translation
iJ.ƒ /sbh. &lt;ËAªi /f-alh 4kAJ.ƒ /sbah. h swimming
�¯ ÉJ�ª�®�K /tf,yl JJ�oZ /tnfyd execution
Y�®�K /nfd ¯
¯X /df, ÈAª�¯ /f,¯al © defence
¨A�¯X /df¯a,
¨PP� /zr, &lt;ËAªi /f,¯alh 4«@P P� /zr¯a,h agriculture
H. Qk /h. rb Éª&apos; /f,l H. Qk /h. rb war
</table>
<bodyText confidence="0.766542666666667">
An example of Al-MaSdar as argument of a dis-
course relation is Ex. 4, where �©J�ÊJ.�K/tbly˙g/informing
is the Al-MaSdar form of &amp;bl˙g/inform.
</bodyText>
<equation confidence="0.388097333333333">
(4) l)@Y�®i v« �©J�ÊJ.�JË]DC[È] Arg1[Z£Qål J»QÓ úÍ@ A�JJ.ëS]
�
Arg2[ZJ�ÖÞ...QË@ 4»Qå�„Ë@ �‡KA~Kð
</equation>
<bodyText confidence="0.8632">
[d¯ hbn¯a ’l¯a mrkz al-ˇsrt.t.]Arg1[l]DC[ltbly˙g ,n fqd¯an
wt¯ ¯a-iq alˇsrkh alrsmyh]Arg2
[We went to the police station]Arg1 [for]DC [in-
forming about the loss of the company’s official
documents.]Arg2
</bodyText>
<subsectionHeader confidence="0.999844">
4.2 Agreement Studies
</subsectionHeader>
<bodyText confidence="0.999984333333333">
The occurrences of a precompiled list of 107 po-
tential discourse connectives were annotated inde-
pendently by 2 native Arabic speakers on 537 news
texts. Agreement was measured for the distinction
of discourse vs. non-discourse usage, relation as-
signment and argument assignment.
Agreement for the classification tasks of dis-
course connective recognition and relation assign-
ment was measured using kappa (Siegel and Castel-
lan, 1956). Argument agreement was measured by
agr, a directional measure (Wiebe et al., 2005). It
measures the word overlap between the text spans
of two judges (ann1 and ann2). agr(ann1||ann2)
measures the proportion of words ann1 annotated
that were also annotated by ann2.
</bodyText>
<equation confidence="0.991517">
agr(ann1||ann2) = |ann1 matching ann2|
|ann1|
</equation>
<bodyText confidence="0.999914956521739">
Discourse connective recognition proved to be
highly reliable with percentage agreement of 0.95
and a kappa of 0.88 on the 23,331 occurrences of
the 107 potential discourse connectives. 5586 of the
potential connectives were agreed on by both anno-
tators to have discourse usage and agreement for re-
lations and argument assignment was measured on
these. As shown in Table 2, kappa on all 17 relations
was low with 0.57 — it turned out that this was due
to the frequent, almost rhetorical use of the connec-
tive ð /w/and at the beginning of paragraphs, which
is a genre convention for Arabic news that normally
does not convey a specific discourse relation. Disre-
garding such occurrences of ð /w/and, kappa rises to
good agreement: 0.69 for fine-grained relations and
0.75 when measuring agreement between the 4 ma-
jor relations EXPANSION, CONTINGENCY, COM-
PARISON and TEMPORAL.
Argument agreement on the 5586 agreed connec-
tives is shown in Table 3. We report high word over-
lap via agr (over 90%) for Arg2, which is the ar-
gument syntactically attached to the connective, and
lesser but still substantial agreement for Arg1.
</bodyText>
<page confidence="0.999021">
739
</page>
<tableCaption confidence="0.9816975">
Table 2: Inter-annotator reliability for discourse relation
assignment
</tableCaption>
<table confidence="0.999833409090909">
All connectives (5586)
Observed agreement 0.66
Kappa 0.57
Class level
Observed agreement 0.8
Kappa 0.67
Connectives excludingð/w/and at BOP (3500)
Observed agreement 0.74
Kappa 0.69
Class level
Observed agreement 0.71
Kappa 0.75
Agreed disc. conn 5586
Arg1 Arg2
a) exact match
exact match =1 2361 (42%) 3803 (68%)
exact match =0 699 (13%) 18 (0.3%)
partial match 2526 (45%) 1765 (32%)
b) agr metric 78% 93%
agr(ann1||ann2)
agr(ann2||ann1) 74% 93%
Avr (agr) 76% 93%
</table>
<tableCaption confidence="0.976324333333333">
Table 3: Inter-annotator reliability for arguments Arg1
and Arg2, using two different measurements (a) exact
match (b) agr
</tableCaption>
<subsectionHeader confidence="0.996905">
4.3 Gold standard
</subsectionHeader>
<bodyText confidence="0.9999382">
We produced a unified gold standard. First, we auto-
matically corrected easily made annotator mistakes.
With regard to argument extent, we automatically
corrected mistakes such as the erroneous inclusion
of punctuation marks at the end of clauses/sentences
or not including all obligatory complements in a
verb phrase argument. The latter relied on the syn-
tactic annotation in the ATB. Second, with regard to
discourse relation assignment, we automatically as-
signed EXPANSION.CONJUNCTION to all disagreed
instances of ð /w/and at BOP.5 A further disam-
biguation study is necessary for ð /w/and at BOP,
which is beyond the scope of this paper.
Finally, an adjudicator not initially involved in an-
notation reconciled the remaining disagreements at
</bodyText>
<footnote confidence="0.344001">
5Other instances of ð /w/and are not treated this way.
</footnote>
<bodyText confidence="0.9984189375">
all levels and included annotations for 5 new po-
tential discourse connective types not in our initial
connective list but commented on by the annotators
during annotation. 3 news files were removed from
the corpus — they contained no actual news reports
but just a list of headlines.
The final discourse treebank we use has 6328 an-
notated explicit connectives in 534 files. 68 connec-
tive types were found, rising to 80 connective types
if we include all modified forms of a connective as
distinct types such as �Ó Ñ �«QËAK./b¯alr˙gm mn, J Ñ�«P
/r˙gm ¯an as modified forms of Ñ�«P /r˙gm/although.
Most discourse connectives were only annotated
with a single relation but 5% were annotated with
two or more relations (as also allowed in the PDTB).
These statistics are summarised in Table 4.
</bodyText>
<table confidence="0.999400333333333">
Files 534
Total tagged tokens 126,046
(125KB)
Sentences 3607
Paragraphs 3312
Discourse connectives (tokens) 6328
Distinct connective (types) 68
including modifed form con- 80
nectives
Clitic discourse connectives (to- 4779
kens) (76%)
Non-clitic discourse connec- 1549
tives (tokens) (24%)
Relations types (17 single, 38 55
combined)
Single relations (tokens) 6039
(95%)
Combined relations (tokens) 289 (5%)
</table>
<tableCaption confidence="0.999662">
Table 4: Statistics of the final gold standard corpus
</tableCaption>
<subsectionHeader confidence="0.994095">
4.4 Importance of explicitly signalled relations
</subsectionHeader>
<bodyText confidence="0.999838166666667">
We compared the number of relations between 2
adjacent sentences that were explicitly signalled in
English vs. the ones that were explicitly signalled
in Arabic, using the PDTB and our corpus (both
containing texts of the news genre). Out of a total
44,470 adjacent sentence pairs in the PDTB, 5355
</bodyText>
<page confidence="0.976734">
740
</page>
<bodyText confidence="0.999651777777778">
(12%) were linked by an explicit connective.6 In
contrast, out of the 3073 adjacent sentence pairs in
our corpus, 2140 (70%) were linked by an explicit
connective, 948 (30%) were linked via non-wa con-
nectives. Thus, for our corpus, modeling of explicit
connectives is primary: intrasentential relations tend
to be marked by connectives anyway in both English
and Arabic, and our corpus shows that this is true for
most local intersentential relations as well.
</bodyText>
<subsectionHeader confidence="0.907807">
4.5 Ambiguity for Arabic discourse connectives
</subsectionHeader>
<bodyText confidence="0.999974833333333">
We investigate the ambiguity of Arabic connectives
with regard to their sense at class level (4 relations)
as well as the more fine-grained level (all 17 rela-
tions). We restrict our investigation to the connec-
tive occurrences that were annotated with a single
relation (6039 tokens) and also exclude ð /w/and at
the beginning of paragraph, leaving 3813 tokens.7
Of 80 connective types, 52 were unambiguous at the
class level and 47 at the fine-grained level. However,
many of the most frequent connectives are highly
ambiguous. If we just assign the most frequent read-
ing to each of the 3813 connectives, we achieve an
accuracy of 82.7% at the class-level and 74.3% at
the more fine-grained level for relation assignment,
leaving a substantial error margin. This contrasts
with the English PDTB, where at the class-level 92%
can be achieved with this simple method and 85% at
the second-level.8
</bodyText>
<sectionHeader confidence="0.996608" genericHeader="method">
5 Discourse Connective Recognition
</sectionHeader>
<bodyText confidence="0.999915666666667">
We distinguished discourse vs. non-discourse usage
for all potential connectives in the 534 gold stan-
dard files. As headers and footers in the news files
never contained true discourse connectives, we dis-
regarded these, leaving 20,312 potential discourse
connectives of which 6328 are actual connectives.
</bodyText>
<footnote confidence="0.8623584">
6Connections between subclauses or phrases in different,
adjacent sentences were included in the count.
7We automatically assigned CONJUNCTION to many occur-
rences of ð /w/and at BOP (Section 4.3) so that it is not sensible
to include these occurrences in a study of human-assigned am-
biguity.
8The second level in the PDTB with its 16 relations corre-
sponds approximately to our fine-grained inventory. This com-
parison can only be appropriate due to slight differences in the
lower-grained relation inventory.
</footnote>
<subsectionHeader confidence="0.678029">
5.1 Features
</subsectionHeader>
<bodyText confidence="0.999043588235294">
Apart from the surface string of the potential con-
nective Conn, we use the following features. Fea-
tures are either extracted from raw files tokenized
by white space only (M2) or from raw files tok-
enized by white space and tagged by the Stanford
tagger9 (Models M3, M4) or from the Arabic Tree-
bank (ATB) gold standard part-of-speech and parse
annotation (models M5-M9). The syntactic features
(Syn) are inspired by (Pitler and Nenkova, 2009).
Lexical/POS patterns of surrounding words, clitic
features and Al-Masdar are novel.
Surface Features (SConn). These include the po-
sition of the potential connective (sentence-initial,
medial or final). The type of the potential connective
is Simple when the potential connective is a single
token not attached to other tokens, PotClitic when
it is attached. Potential connectives containing more
than one token have MoreThanToken type.
Models where we use ATB or automated tagging
(M3-M9) distinguish further between potential cli-
tics that are assigned a POS and ones that are not.
Models that use ATB annotation also distinguish
between potential connectives that correspond to a
phrase in the ATB (MorethanToken Phrase) and
the ones that do not (MorethanToken NonPhrase).
Lexical features of surrounding words (Lex).
We encode the surface strings of the three words
before and after the connective, recording posi-
tion. These features are especially useful for lan-
guages where no accurate parser or tagger is avail-
able as lexical patterns can capture discourse and
non-discourse usage. For instance, if a potential
connective is followed by 0 /¯an/ it most likely has
a discourse function (see Ex. 5).
</bodyText>
<figure confidence="0.76986">
(5) DC[ ð ] Arg1[ L3AëPBAK. @ñK.A’~
0@] &amp;ºÖß� ÈA�®£B@ 0@
@ñÓA� ÕË @S@ ;ƒ@PYË@ ÈCg� Arg2[€Aª�JËAK. @ðQª J. 0@]
@YJ�k.
[ ¯an ¯al¯at.f¯al ymkn [¯an ys. ¯abw¯a b¯al¯arh¯a-
q]Arg1[w]DC[¯an yˇs,rw¯a b¯aln,¯as]Arg2 hl¯al ¯aldr¯ash
˘
¯ad ¯ a¯ lm yn¯amw¯a ˇgyd¯a
[Children might be tired]Arg1 [and]DC [feel
sleepy]Arg2 during school time if they did not sleep
well
</figure>
<footnote confidence="0.918366">
9http://nlp.stanford.edu/software/tagger.shtml
</footnote>
<page confidence="0.997157">
741
</page>
<bodyText confidence="0.997749042553192">
Part of Speech features (POS). We include
the pos tag of the potential connective via the
ATB/Stanford Tagger. For potential connectives that
consist of more than one token, we combined its
ordered POS tags. Thus, the potential connective
J6. j /fy h. ¯al/in case with its tags (fy PREP)(Hal
NOUN)) will receive the pos PREP#NOUN. If a po-
tential connective does not receive a separate POS
tag in the ATB/tagger, the value ”NONE” is as-
signed. This allows to distinguish clitics from let-
ters at the start of a word. We also record the
POS of the three words before/after the connective
(ATB/Stanford Tagger). Similar to lexical patterns,
these can capture discourse and non-discourse us-
age. For instance, if a potential connective is soon
followed by a modal, it is more likely to have a dis-
course function.
Syntactic category of related phrases (Syn). We
record the syntactic category of the parent of the po-
tential connective in ATB. For example, it is rare
that cases where the parent of the potential connec-
tive is an adjective phrase, correspond to discourse-
usage. A typical example of a non-discourse usage
of9 /w/and ( U_*- 9 SSS a...,���I /¯almdrsh kbyrh w
ˇgmylh/ the school is very large and beautiful) illus-
trates this. Unlike English, parents in Arabic often
are noun phrases as nominalisations are frequent ar-
guments of prepositional connectives. We also en-
code the Left sibling category and right sibling cat-
egory of the connective. For discourse connectives,
the right sibling is normally S, SBAR, VP or an NP
(if the connective is a preposition).
Al-Masdar feature. Potential connectives fol-
lowed by Al-Masdar are more likely to have dis-
course usage (see Section 4.1). Especially preposi-
tions with discourse usage are normally attached to
Al-masdar such as in a;.)lLl /lmh. ¯adt¯h/for contacting
or I��� L /b¯aˇgr¯a6y processing. Al-Masdar informa-
tion is not included in the ATB so we constructed a
binary Al-Masdar feature from (tagged) text by ex-
amining the first noun after the potential connective.
We developed an algorithm to judge such a noun as
Al-Masdar or not. This algorithm uses a stemmer
for Arabic and then determines whether the stem is
al-Masdar by a combination of surface-based rules
to check whether the stem corresponds to one of the
known Al-Masdar patterns.
</bodyText>
<subsectionHeader confidence="0.938657">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.99999569047619">
We used the implementation JRip of the rule-
based classifier Ripper in the machine learning tool
WEKA with its default settings. We used 10-fold
cross-validation throughout. Significance tests are
reported using the McNemar test at the significance
level of 1%. A most frequent category baseline
would assign all potential connectives as not connec-
tive, achieving an accuracy of 68.9% as only 6328 of
our potential 20,312 connectives actually have dis-
course usage. We built several models using differ-
ent features. The results are shown in Table 5.
A simple model M1 that only uses the connective
string improves significantly over the baseline with
75.7% accuracy but a kappa of only 0.48, showing
that this is not a reliable strategy. Models M2-M4
do not rely on gold standard annotation or parsing
(in contrast to the models for English in (Pitler and
Nenkova, 2009)). Using only surface and lexical
features that can be extracted from white-space to-
kenized raw files in addition to the connective string
(M2), gains a substantial improvement over using
the connective string alone. This is further improved
by using POS tags of connectives and surrounding
words with an automatic tagger (M3) and by includ-
ing the Al-Masdar feature (M4), thus making good
use of the morphological properties of Arabic. All
differences are statistically significant (M1 &lt; M2 &lt;
M3 &lt; M4). The final model is reliable (kappa 0.70),
an encouraging result given the absence of parsing
and important for resource-scarce languages.
With ATB gold standard tokenisation, tagging and
parsing, our models (not surprisingly) improve fur-
ther showing the same pattern of (M1 &lt; M5 &lt; M6 &lt;
M7) with all differences being significant. The final
best model achieves highly reliable results (accuracy
92.4%, kappa 0.82). We also conclude that syntac-
tic features are more useful than lexical patterns as
model M8 (syntax with no lexical patterns) achieves
equally good results as M6. Our models also man-
age to generalise well over individual connectives.
If we leave out the connective string (M9), we still
achieve a highly reliable result.
</bodyText>
<sectionHeader confidence="0.996013" genericHeader="method">
6 Discourse Relation Recognition
</sectionHeader>
<bodyText confidence="0.9986675">
When disambiguating the relation that discourse
connectives signal, we assume that the arguments of
</bodyText>
<page confidence="0.993413">
742
</page>
<table confidence="0.999916923076923">
Features Acurr K
Baseline (not conn) 68.9 0
M1 Conn only 75.7 0.48
Tokenization by white space + auto tagger
M2 Conn+ SConn+Lex 85.6 0.62
M3 Conn+ SConn+Lex+POS 87.6 0.69
M4 Conn+SConn+Lex+POS+Masdar 88.5 0.70
ATB-based features
M5 Conn+SConn+Lex 86.2 0.65
M6 Conn+SConn+Lex+Syn/POS 91.2 0.79
M7 Conn+SConn+Lex+Syn/POS+Masdar 92.4 0.82
M8 Conn+SConn+Syn 91.2 0.79
M9 SConn+Lex+Syn+Masdar 91.2 0.79
</table>
<tableCaption confidence="0.999846">
Table 5: Performance of different models for identifying discourse connectives.
</tableCaption>
<bodyText confidence="0.999660090909091">
the connective are known. This is well-established
for PDTB relation recognition (Wang et al., 2010;
Lin et al., 2009; Miltsakaki et al., 2005). Our mod-
els predict single relations on two datasets: (i) all
instances of connectives signalling single relations
(Set All, 6039 instances) (2) all instances apart from
the connective 9 /w/and at beginning of paragraph
as they are affected by the auto-correction process
(Set no-wa-atBOP, 3813 instances). We use 10-fold
cross-validation and JRip as well as a McNemar test
at the 5% level for significance tests.
</bodyText>
<subsectionHeader confidence="0.963496">
6.1 Features
</subsectionHeader>
<bodyText confidence="0.99358994">
Whereas some of the features we use have been used
for English implicit relation recognition (Lin et al.,
2009; Wang et al., 2010; Pitler et al., 2009) , they
are new for Arabic and not widely used for explicit
connectives. All features are extracted from the ATB
gold standard parses.
Connective features. This includes the connec-
tive string Conn. In addition, we also use the sur-
face connective features and POS of connective de-
scribed in Section 5. We also use the syntactic path
to the connective as a novel feature.
Words and POS of arguments. The words and
pos tags of the first three words in Arg1 and
Arg2 are used to catch patterns in arguments.
For example, when the first word of Arg2 is
�� /qd/might/may or j�� /k¯an/had, the relation is
�
likely to be EXPANSION.BACKGROUND or EXPAN-
SION.CONJUNCTION. We also measure word over-
lap between the arguments, hoping to catch relations
such as COMPARISON.SIMILARITY.
Masdar. This feature states whether the first or
second word in Arg 2 is an Al-Masdar. Many prepo-
sitional connectives followed by an Al-Masdar indi-
cate a CONTINGENCY.CAUSE relation (see Ex. 4)
Tense and Negation. Each argument is assigned
its tense as one of perfect, imperfect, future or none.
We also indicate whether the tense of Arg1 or 2 are
the same and whether a negation is part of Arg 1
or 2. Inspired by (Miltsakaki et al., 2005), we stip-
ulate that tense is useful for recognizing temporal
and causal relations. For example, the arguments of
the relation TEMPORAL.SYNCHRONOUS are likely
to have the same tense. In contrast, arg1 tense is
more likely to be prior to arg2 tense for TEMPO-
RAL.ASYNCHRONOUS and CAUSE relations.
Length, Distance and Order Features. We use
the length of arguments (in words), word distance
between a connective and its arguments (-1; for ar-
guments in order Arg1 Conn Arg2 Arg1), tree dis-
tance of connective and arguments (0 if connective
and an argument are in the same tree) and a bi-
nary feature of whether Arg1 and Arg2 are in dif-
ferent sentences. A nominal feature encodes one of
the three orders Arg1 Conn Arg2, Conn Arg2 Arg1
and Arg1 Conn Arg2 Arg1, the latter being fre-
quent in Arabic for TEMPORAL.ASYNCHRONOUS
relations.
Argument Parent. We record the syntactic par-
ent of each Argument. However, not every argu-
</bodyText>
<page confidence="0.996795">
743
</page>
<bodyText confidence="0.999891625">
ment corresponds to a complete tree in the ATB —
in these cases we extract the category of the parent
shared by the first and last word in the argument.
Production Rules. We use all non-lexical produc-
tion rules that occur more than 10 times in the argu-
ments as binary features. This was inspired by (Lin
et al., 2009) who use production rules to good effect
for implicit relations in English.
</bodyText>
<subsectionHeader confidence="0.95187">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999971235294118">
Table 6 shows the results for fine-grained (17 rela-
tions) classification. The baseline of assigning the
most frequent relation EXPANSION.CONJUNCTION
to every connective performs with an accuracy of
52.5% on Set All and 35% on set no-wa-atBOP. If
we use a model that relies on the discourse connec-
tive alone (M1) we achieve results of 77.2%/74.3%,
respectively. As noted in Section 4.5 this is substan-
tially lower than what the same model can achieve
for English. Including connective and argument fea-
tures (apart from production rules) in M2, leads to a
small but significant improvement.10 Further incor-
poration of production rules does not improve the
results (M3). In Table 7, we show the results at the
class-level (4 relations). Here using additional fea-
tures over the connective string does not lead to sig-
nificant improvements.
</bodyText>
<subsectionHeader confidence="0.996486">
6.3 Discussion and Error Analysis
</subsectionHeader>
<bodyText confidence="0.999987666666667">
We concentrate our discussion on fine-grained clas-
sification excluding wa at BOP.
Our improvements in M2 over the connective-
only classifier (M1) are in two main areas. First, our
model performs generalisation, i.e. outputs some
rules that do not use the connective string at all.
These achieve a somewhat surprising improvement
of M2 over M1 for unambiguous connectives which
are too rare to classify via the connective string. In
those cases, they either (i) have not been seen in the
training data before and are therefore not classifiable
when seen first time in the test set or (ii) have been
</bodyText>
<footnote confidence="0.631885">
10Our corpus includes some texts on similar topics where
some sentences are (almost) repeated in different texts. To
investigate whether our improvements are due to this repeti-
tion, we also performed an experiment excluding all repeated
instances of feature vectors from the corpus. The results are
almost the same and, most importantly, M2 again improves sig-
nificantly over M1.
</footnote>
<table confidence="0.998556615384616">
Ref Features Acc K
All connectives (6039)
Baseline (CONJUNCTION) 52.5 0
M1 Conn only (1) 77.2 0.60
M2 Conn+Conn f+ Arg f (37) 78.8 0.66
M3 Conn+Conn f+ Arg f+ Pro- 78.3 0.65
duction rules (1237)
excluding wa at BOP (3813)
Baseline (CONJUNCTION) 35 0
M1 Conn only (1) 74.3 0.65
M2 Conn+Conn f+ Arg f (37) 77 0.69
M3 Conn+Conn f+ Arg f+ Pro- 76.7 0.69
duction rules (1237)
</table>
<tableCaption confidence="0.994073">
Table 6: Performance of different models for identifying
fine-grained discourse relations on two datasets.
</tableCaption>
<table confidence="0.999648666666667">
Ref Features Acc K
All connectives (6039)
Baseline (EXPANSION) 62.4 0
M1 Conn only (1) 88.7 0.78
M2 Conn+Conn f+ Arg f (37) 88.7 0.78
excluding wa at BOP (3813)
Baseline (EXPANSION) 41.8 0
M1 Conn only (1) 82.7 0.74
M2 Conn+Conn f+ Arg f (37) 83.5 0.75
</table>
<tableCaption confidence="0.9572385">
Table 7: Performance of different models for identifying
class-level discourse relations on two datasets.
</tableCaption>
<bodyText confidence="0.9999662">
seen in the training data too rarely for the rule-based
classifier to develop a rule judged to be more re-
liable than the default EXPANSION.CONJUNCTION
classification. Our data includes 47 unambiguous
connective types, accounting for 574 of the 3813
tokens. 30 of these 47 types are so rare that we
found mistakes in the connective-only classification,
including y1 /¯al¯a/except (2), ����� /gb(2),L��L6 /t. ¯a-
lm¯a(2), � jlbr˙gm(1). For 14 of these 30 connec-
tives, model M2 was able to use generalised rules
to improve relation assignment.11 These rules in-
volve mainly connective surface and POS features.
Thus, sentence-start adverbials consisting of more
than one token such as J x, /byd ¯an(6) and J ��� ��
/˙gyr ¯an(6) were correctly classified as CONTRAST.
</bodyText>
<footnote confidence="0.693274">
11For the other 16 connectives neither of the models was able
to classify them correctly.
</footnote>
<page confidence="0.995276">
744
</page>
<bodyText confidence="0.999355676470588">
This advantage of our model over the connective-
only model might disappear if in a larger corpus
more instances of those connectives are found
and are still unambiguous. Therefore, we are
more interested in how our classifier does on
truly ambiguous connectives (33 connective types
accounting for 3239 tokens of 3813 overall to-
kens). We conducted a separate significance test
on ambiguous connectives only and found that
M2 improves over M1 classification significantly
at the 1% level. How well we do on individual
connectives depends on their frequency and on their
level of ambiguity. If connectives are ambiguous
and of low frequency (ñË /lw, AÖ�ß@ /¯anm¯a, ÈAg /h. ¯al/)
both M1 and M2 do perform badly on them. If
connectives are frequent (10 or more occurrences)
and have relatively low ambiguity (majority reading
accounts for more than 70% of instances), the
overall performance of M1 and M2 with regard to
accuracy is also similar, often both using just the
connective string. On the other hand, if connectives
are frequent and have high ambiguity (i.e. no such
clear majority reading), then M2 normally improves
(often substantially) on M1. Examples of such
connectives are AÒ» /km¯a, AÒJ��¯ /fym¯aand Q&amp;quot;K@ /¯at¯r.
Most of the successful rules use tense in some form,
either via part of speech of verbs or via comparing
the tense in the two arguments. This, for example,
led to a successful recognition of all 9 instances of
Similarity in the connective kmA (whose major-
ity relation is Expansion.Conjunction in 40 out
of 65 occurrences). The connective ¬ /f/then is dis-
tinguished into EXPANSION.EXEMPLIFICATION,
CONTINGENCY.CAUSE.RESULT and CONTIN-
GENCY.CAUSE.REASON readings, depending on
the lexemes around it, the parents of its arguments,
and whether its argument 2 is tensed or not. Thus,
nontensed arguments are most often nominalisations
leading to a reason reading, whereas a verb phrase
as argument 2 and a sentence as argument 1 often is
a result reading. However, it is worth reporting that
in cases of very high ambiguity, M2 is still far from
perfect such as for connectives f ¬� /fandQ&amp;quot;K@ /¯at¯r.
Some improvements again come from gener-
alised rules: there are some very high-coverage
and high precision generalised rules that reduce
dependency on the connective string. For example,
clitic prepositions (such as È /l/for) can without
any further information be clearly classified as
Contingency.Cause.Reason.NonPragmatic
covering 494 occurrences with only 26 mistakes.
These are cases where the following argument is
normally Al-Masdar.
Our analysis leads us to the following strategy
for follow-on work. First of all, a larger corpus is
necessary to get more examples for low frequency
connectives. Secondly, experiments with different
classifiers are worthwhile to conduct to see how our
improvements generalise. Third, the most mileage
is in further improvements on frequent, ambiguous
connectives such as ¬ /f, &amp;quot;/mnd¯and ð@ /¯aw. This
can be achieved with, on the one hand, training
connective-specific classifiers on larger data sets but
will, on the other hand, also need a wider feature
base. From our corpus study, we think that lexico-
semantic features such as word pairs and seman-
tic classes of verbal/nominalised arguments are the
most promising.
</bodyText>
<sectionHeader confidence="0.996025" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999978">
We have presented the first study on the automatic
detection and disambiguation of Arabic discourse
connectives. A corpus study showed that these
are highly frequent and more ambiguous than their
English counterparts. Our automatic algorithms
achieve very good results on discourse connective
identification, using Arabic morphological proper-
ties to good effect. It is particular promising that we
do not need parsed data to identify discourse usage
of potential connectives reliably. Our algorithm for
discourse connective interpretation beats the chal-
lenging baseline of assigning the most frequent re-
lation per connective. In future, we will explore fur-
ther features for connective disambiguation as well
as connective-specific classification, combined with
semi-supervised algorithms to alleviate data sparse-
ness. We will also develop algorithms for argument
identification.
</bodyText>
<sectionHeader confidence="0.989508" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9617516">
Amal Al-Saif is supported by a PhD scholarship from
the Imam Muhammad Ibn Saud University, Saudi Arabia.
We thank the British Academy for additional funding for
the annotation study via Grant SG51944. Also thank you
to the 3 anonymous reviewers for their comments.
</bodyText>
<page confidence="0.997477">
745
</page>
<sectionHeader confidence="0.981923" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999832">
M. Abdl al latif, A. Umar, and M. Zahran. 1997. Alnhw
AlAsAsi. Dar Alfker Al-Arabi, Cairo, Egypt.
A. AlSaif and K. Markert. 2010. The leeds arabic dis-
course treebank: Annotating discourse connectives for
arabic. In Language Resources and Evaluation Con-
ference (LREC).
J. Baldridge and A. Lascarides. 2005. Probabilistic head-
driven parsing for discourse structure. In Proc. Of
Conll 2005.
S. Blair-Goldensohn, K McKeown, and O. Rambow.
2007. Building and refining rhetorical-semantic rela-
tion models. In Proc. of HLT-NAACL 2007.
L. Carlson, D. Marcu, and M. Okurewski. 2002. Rst
discourse treebank. Linguistic Data Consortium.
D. duVerle and H. Prendinger. 2009. A novel discourse
parser based on support vector machine classification.
In Proc. of ACL 2009.
R. Elwell and J. Baldridge. 2008. Discourse connec-
tive argument identification with connective specific
rankers. In Proc. of the International Conference on
Semantic Computing.
R. Girju. 2003. Automatic detection of causal relations
for questions answering. In Proc. of the ACL 2003
Workshop on Multilingual Summarisation and Ques-
tion Answering, pages 76–83.
M.A.K. Halliday and R. Hasan. 1976. Cohesion in En-
glish. Longman London.
J.R. Hobbs. 1985. On the coherence and structure of
discourse. Center for the Study of Language and In-
formation, Stanford, Calif.
A. Knott and T. Sanders. 1998. The classification of
coherence relations and their linguistic markers: An
exploration of two languages. Journal of Pragmatics,
30(2):135–175.
Z. Lin, M. Kan, and H.T. Ng. 2009. Recognizing implicit
discourse relations in the penn discourse treebank. In
Proc. of EMNLP 2009, pages 343–351.
A. Louis and A. Nenkova. 2010. Creating local coher-
ence: An empirical assessment. In Proc. of NAACL
2010.
M. Maamouri and A. Bies. 2004. Developing an Ara-
bic treebank: Methods, guidelines, procedures, and
tools. In Proceedings of the Workshop on Computa-
tional Approaches to Arabic Script-based Languages
(COLING), Geneva.
W.C. Mann and S.A. Thompson. 1988. Rhetorical struc-
ture theory: Toward a functional theory of text organi-
zation. Text, 8(3):243–281.
D. Marcu and A. Echihabi. 2002. An unsupervised ap-
proach to recognizing discourse relations. In Proc. of
ACL 2002.
D. Marcu. 2000. The theory and practice of discourse
parsing and summarization. MIT Press.
E. Miltsakaki, N. Dinesh, R. Prasad, A. Joshi, and
B. Webber. 2005. Experiments on sense annotation
and sense disambiguation of discourse connectives. In
Proc. of the Workshop on Treebanks and Linguistic
Theories.
E. Pitler and A. Nenkova. 2008. Revisiting readabil-
ity: A unified framework for predicting text quality. In
Proc. of EMNLP 2008, pages 186–195.
E. Pitler and A. Nenkova. 2009. Using syntax to dis-
ambiguate explicit discourse connectives. In Proc of
ACL-IJCNLP 2009 (Short Papers), pages 13–16.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee,
and A. Joshi. 2008. Easily identifiable discourse
relations. In Proceedings of the 22nd International
Conference on Computational Linguistics (COLING
2008), Manchester, UK, August.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in text.
In Proc. of ACL-IJCNLP 2009, pages 683–691.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008a. The Penn discourse
treebank 2.0. In Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC 2008).
R. Prasad, S. Husain, D.M. Sharma, and A. Joshi. 2008b.
Towards an Annotated Corpus of Discourse Relations
in Hindi. In The Third International Joint Conference
on Natural Language Processing, pages 7–12. Cite-
seer.
K.C. Ryding. 2005. A reference grammar of modern
standard Arabic. Cambridge Univ Pr.
S. Siegel and N.J. Castellan. 1956. Nonparametric
statistics for the behavioral sciences. McGraw-Hill
New York.
S. Somasundaran, J. Wiebe, and J. Ruppenhofer. 2008.
Discourse-level opinion interpretation. In Proc. of
Coling 2008.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical information.
In Proc of HLT-NAACL 2003.
C. Sporleder and A. Lascarides. 2008. Using auto-
matically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineering,
14:369–416.
W. Wang, J. Su, and C. Tan. 2010. Kernel-based dis-
course relation recognition with temporal ordering in-
formation. In Proc. of ACL 2010, pages 710–719.
B. Webber, A. Knott, M. Stone, and A. Joshi. 1999.
Discourse relations: A structural and presuppositional
account using lexicalised TAG. In Proceedings of
</reference>
<page confidence="0.98196">
746
</page>
<reference confidence="0.999343892857143">
the 37th annual meeting of the Association for Com-
putational Linguistics on Computational Linguistics,
page 48. Association for Computational Linguistics.
B. Wellner and J. Pustejovski. 2007. Automatically
identifying the arguments of discourse connectives. In
Proc. of EMNLP 2007, pages 92–101.
B. Wellner, J. Pustejovski, A. Havasi, A. Rumshisky, and
R. Suair. 2006. Classification of discourse coherence
relations: An exploratory study using multiple knowl-
edge sources. In Proc. of SIGDIAL2006.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation.
W. Wright. 2008. A grammar of the Arabic language.
Bibliobazaar.
Nianwen Xue. 2005. Annotating discourse connectives
in the chinese treebank. In CorpusAnno ’05: Proceed-
ings of the Workshop on Frontiers in Corpus Annota-
tions II, pages 84–91, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
D. Zeyrek and B. Webber. 2008. A discourse resource
for turkish: Annotating discourse connectives in the
metu corpus. Proceedings of IJCNLP-2008. Hyder-
abad, India.
Z. Zhou, Y. Xu, Z. Niu, M. Lan, . Su, and Tan. C. 2010.
Predicting discourse connectives for implicit discourse
relation recognition. In Proc. of Coling 2010, pages
1507–1514.
</reference>
<page confidence="0.997634">
747
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.287923">
<title confidence="0.998675">Modelling Discourse Relations for Arabic</title>
<author confidence="0.757465">Amal</author>
<affiliation confidence="0.964627">University of Leeds,</affiliation>
<address confidence="0.929927">LS2 9JT</address>
<email confidence="0.990045">amalalsaif@yahoo.co.uk</email>
<author confidence="0.582394">Katja</author>
<affiliation confidence="0.96113">University of Leeds,</affiliation>
<address confidence="0.848728">LS2 9JT</address>
<email confidence="0.993704">markert@comp.leeds.ac.uk</email>
<abstract confidence="0.998302037037037">We present the first algorithms to automatically identify explicit discourse connectives and the relations they signal for Arabic text. First we show that, for Arabic news, most adjacent sentences are connected via explicit connectives in contrast to English, making the treatment of explicit discourse connectives for Arabic highly important. We also show that explicit Arabic discourse connectives are far more ambiguous than English ones, making their treatment challenging. In the second part of the paper, we present supervised algorithms to address automatic discourse connective identification and discourse relation recognition. Our connective identifier based on gold standard syntactic features achieves almost human performance. In addition, an identifier based solely on simple lexical and automatically derived morphological and POS features performs with high reliability, essential for languages that do not have high-quality parsers yet. Our algorithm for recognizing discourse relations performs significantly better than a baseline based on the connective surface string alone and therefore reduces the ambiguity in explicit connective interpretation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Abdl al latif</author>
<author>A Umar</author>
<author>M Zahran</author>
</authors>
<title>Alnhw AlAsAsi. Dar Alfker Al-Arabi,</title>
<date>1997</date>
<location>Cairo, Egypt.</location>
<contexts>
<context position="12736" citStr="latif et al., 1997" startWordPosition="1961" endWordPosition="1964">ypical examples are Èñ“ð /ws.wl/arrival from the verb É“ð /ws./to arrive and aËðAm× /mh. ¯awlh/attempt from the verb ÈðAg 4The medieval Arabic grammar schools, the Basra and Kufa, debated whether the noun (almasdar) or the verb is the most basic element of language (Ryding, 2005). 738 Figure 1: Discourse relations for Arabic /h. ¯awl/to try. Al-Masdar is formed using morphological patterns well-known in the Arabic grammatical tradition: major Arabic grammars list around 60 patterns although some other references also claim that the patterns are many more as well as more unpredictable (Abdl al latif et al., 1997; Wright, 2008; Ryding, 2005). Al-Masdar forms do not fit into one grammatical or morphological category in English: they might correspond to a gerund, a nominalization or a noun which is not a nominalization. Some examples are listed in Table 1. Table 1: A list of Al-MaSdar patterns, examples and their English correspondence Root Pattern MaSdar Translation iJ.ƒ /sbh. &lt;ËAªi /f-alh 4kAJ.ƒ /sbah. h swimming �¯ ÉJ�ª�®�K /tf,yl JJ�oZ /tnfyd execution Y�®�K /nfd ¯ ¯X /df, ÈAª�¯ /f,¯al © defence ¨A�¯X /df¯a, ¨PP� /zr, &lt;ËAªi /f,¯alh 4«@P P� /zr¯a,h agriculture H. Qk /h. rb Éª&apos; /f,l H. Qk /h. rb war A</context>
</contexts>
<marker>latif, Umar, Zahran, 1997</marker>
<rawString>M. Abdl al latif, A. Umar, and M. Zahran. 1997. Alnhw AlAsAsi. Dar Alfker Al-Arabi, Cairo, Egypt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A AlSaif</author>
<author>K Markert</author>
</authors>
<title>The leeds arabic discourse treebank: Annotating discourse connectives for arabic.</title>
<date>2010</date>
<booktitle>In Language Resources and Evaluation Conference (LREC).</booktitle>
<contexts>
<context position="10997" citStr="AlSaif and Markert, 2010" startWordPosition="1674" endWordPosition="1678">, production rules and syntactic trees (Wang et al., 2010; Lin et al., 2009) as well as language modelling (Zhou et al., 2010). As we only deal with explicit connectives this work is not directly comparable to ours, although we do explore some of the suggested features for improving explicit connective disambiguation. 4 An Arabic Discourse Corpus We annotate news articles from the Arabic Penn Treebank (Part 1 v2.0) (Maamouri and Bies, 2004) for explicitly marked discourse relations. This is the first discourse-annotated corpus for Arabic, whose initial development stages we have described in (AlSaif and Markert, 2010). We summarize this previous work and extend it by including agreement studies for arguments in Sections 4.1 and 4.2. In Sections 4.3, 4.4 and 4.5. we then present a corpus study on the corpus which shows our major claim as to the importance and high levels of ambiguity of Arabic discourse connectives. 4.1 Annotation Principles We overall follow the annotation principles in the Penn Discourse Treebank for explicit connectives (for example, arguments can occur at any distance from the connectives). The relation set we use is a more coarse-grained version of the PDTB relations with two relations</context>
</contexts>
<marker>AlSaif, Markert, 2010</marker>
<rawString>A. AlSaif and K. Markert. 2010. The leeds arabic discourse treebank: Annotating discourse connectives for arabic. In Language Resources and Evaluation Conference (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baldridge</author>
<author>A Lascarides</author>
</authors>
<title>Probabilistic headdriven parsing for discourse structure.</title>
<date>2005</date>
<booktitle>In Proc. Of Conll</booktitle>
<contexts>
<context position="9620" citStr="Baldridge and Lascarides, 2005" startWordPosition="1456" endWordPosition="1459">ine” for which significance tests are not given. We will show that for Arabic, discourse connectives are more highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang et al., 2010). 2008) and often need worl</context>
</contexts>
<marker>Baldridge, Lascarides, 2005</marker>
<rawString>J. Baldridge and A. Lascarides. 2005. Probabilistic headdriven parsing for discourse structure. In Proc. Of Conll 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Blair-Goldensohn</author>
<author>K McKeown</author>
<author>O Rambow</author>
</authors>
<title>Building and refining rhetorical-semantic relation models.</title>
<date>2007</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<marker>Blair-Goldensohn, McKeown, Rambow, 2007</marker>
<rawString>S. Blair-Goldensohn, K McKeown, and O. Rambow. 2007. Building and refining rhetorical-semantic relation models. In Proc. of HLT-NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M Okurewski</author>
</authors>
<title>Rst discourse treebank. Linguistic Data Consortium.</title>
<date>2002</date>
<contexts>
<context position="6929" citStr="Carlson et al., 2002" startWordPosition="1045" endWordPosition="1048">ification which identifies the arguments’ position and extent. In this paper we tackle Task 1 and Task 2 for Arabic in a supervised machine learning framework. 3 Related work Annotated Discourse Corpora and Linguistic Background. Discourse relations are widely studied in theoretical linguistics (Halliday and Hasan, 1976; Hobbs, 1985), where also different relation taxonomies have been derived (Hobbs, 1985; Knott and Sanders, 1998; Mann and Thompson, 1988; Marcu, 2000). Different inventories have been used in English corpora annotated for discourse relations (Hobbs, 1985; Prasad et al., 2008a; Carlson et al., 2002) which also differ in other respects (such as whether they prescribe a tree structure for discourse annotation). However, the annotation level of existing Arabic corpora has not yet included the discourse layer, making our work the first to address this problem for Arabic on a larger scale. Automatic discourse parsing: explicit relations. There is no work on discourse connective recognition, interpretation and argument assignment for Arabic, so that we break entirely new ground here. However, the two tasks we explore (discourse connective recognition and discourse connective disambiguation) ha</context>
</contexts>
<marker>Carlson, Marcu, Okurewski, 2002</marker>
<rawString>L. Carlson, D. Marcu, and M. Okurewski. 2002. Rst discourse treebank. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D duVerle</author>
<author>H Prendinger</author>
</authors>
<title>A novel discourse parser based on support vector machine classification.</title>
<date>2009</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="9554" citStr="duVerle and Prendinger, 2009" startWordPosition="1447" endWordPosition="1450">improvement over a “most frequent relation per connective baseline” for which significance tests are not given. We will show that for Arabic, discourse connectives are more highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; W</context>
</contexts>
<marker>duVerle, Prendinger, 2009</marker>
<rawString>D. duVerle and H. Prendinger. 2009. A novel discourse parser based on support vector machine classification. In Proc. of ACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Elwell</author>
<author>J Baldridge</author>
</authors>
<title>Discourse connective argument identification with connective specific rankers.</title>
<date>2008</date>
<booktitle>In Proc. of the International Conference on Semantic Computing.</booktitle>
<contexts>
<context position="7852" citStr="Elwell and Baldridge, 2008" startWordPosition="1182" endWordPosition="1185">matic discourse parsing: explicit relations. There is no work on discourse connective recognition, interpretation and argument assignment for Arabic, so that we break entirely new ground here. However, the two tasks we explore (discourse connective recognition and discourse connective disambiguation) have been tackled for English.2 (Pitler 1Arabic examples contain in order: the Arabic right-to-left script, the transliteration (standards ISO/R 233 and DIN 31635) and the English translation (if possible). 2There is also substantial work on argument identification (Wellner and Pustejovski, 2007; Elwell and Baldridge, 2008) 737 and Nenkova, 2009) use gold standard syntactic features as well as the connective surface string in a supervised model for discourse connective recognition. They achieve very high results with this approach. We will (i) show that similar features work well for Arabic (ii) take into account Arabic-specific morphological properties that improve results further and (iii) present a robust version of this approach that does not rely on full parsing or gold standard syntactic annotations. With regard to discourse connective interpretation, (Miltsakaki et al., 2005) concentrate on disambiguating</context>
</contexts>
<marker>Elwell, Baldridge, 2008</marker>
<rawString>R. Elwell and J. Baldridge. 2008. Discourse connective argument identification with connective specific rankers. In Proc. of the International Conference on Semantic Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
</authors>
<title>Automatic detection of causal relations for questions answering.</title>
<date>2003</date>
<booktitle>In Proc. of the ACL 2003 Workshop on Multilingual Summarisation and Question Answering,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="1592" citStr="Girju, 2003" startWordPosition="221" endWordPosition="222">on simple lexical and automatically derived morphological and POS features performs with high reliability, essential for languages that do not have high-quality parsers yet. Our algorithm for recognizing discourse relations performs significantly better than a baseline based on the connective surface string alone and therefore reduces the ambiguity in explicit connective interpretation. 1 Introduction The automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such as automatic summarization (Marcu, 2000), question answering (Girju, 2003), sentiment analysis (Somasundaran et al., 2008) and readability assessment (Pitler and Nenkova, 2008). This task has recently seen renewed interest due to the growing availability of large-scale corpora annotated for discourse relations, such as the Penn Discourse Treebank (Prasad et al., 2008a). In the Penn Discourse Treebank (PDTB), local discourse relations (also called senses) such as CAUSAL or CONTRAST are annotated. They hold between two text segments (so-called arguments) that express abstract entities such as events, facts and propositions. Annotated discourse relations can be signall</context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>R. Girju. 2003. Automatic detection of causal relations for questions answering. In Proc. of the ACL 2003 Workshop on Multilingual Summarisation and Question Answering, pages 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>R Hasan</author>
</authors>
<title>Cohesion in English. Longman London.</title>
<date>1976</date>
<contexts>
<context position="6629" citStr="Halliday and Hasan, 1976" startWordPosition="1001" endWordPosition="1004">urse relation. Again, some connectives are largely unambiguous in this respect. For example, A/lkn/but signals almost always a CONTRAST relation. However, there are connectives where this is not the case, such as .a;A /mnd¯/since which has a CAUSAL and a TEMPORAL sense. The third task is argument identification which identifies the arguments’ position and extent. In this paper we tackle Task 1 and Task 2 for Arabic in a supervised machine learning framework. 3 Related work Annotated Discourse Corpora and Linguistic Background. Discourse relations are widely studied in theoretical linguistics (Halliday and Hasan, 1976; Hobbs, 1985), where also different relation taxonomies have been derived (Hobbs, 1985; Knott and Sanders, 1998; Mann and Thompson, 1988; Marcu, 2000). Different inventories have been used in English corpora annotated for discourse relations (Hobbs, 1985; Prasad et al., 2008a; Carlson et al., 2002) which also differ in other respects (such as whether they prescribe a tree structure for discourse annotation). However, the annotation level of existing Arabic corpora has not yet included the discourse layer, making our work the first to address this problem for Arabic on a larger scale. Automati</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M.A.K. Halliday and R. Hasan. 1976. Cohesion in English. Longman London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>On the coherence and structure of discourse. Center for the Study of Language and Information,</title>
<date>1985</date>
<location>Stanford, Calif.</location>
<contexts>
<context position="6643" citStr="Hobbs, 1985" startWordPosition="1005" endWordPosition="1006"> connectives are largely unambiguous in this respect. For example, A/lkn/but signals almost always a CONTRAST relation. However, there are connectives where this is not the case, such as .a;A /mnd¯/since which has a CAUSAL and a TEMPORAL sense. The third task is argument identification which identifies the arguments’ position and extent. In this paper we tackle Task 1 and Task 2 for Arabic in a supervised machine learning framework. 3 Related work Annotated Discourse Corpora and Linguistic Background. Discourse relations are widely studied in theoretical linguistics (Halliday and Hasan, 1976; Hobbs, 1985), where also different relation taxonomies have been derived (Hobbs, 1985; Knott and Sanders, 1998; Mann and Thompson, 1988; Marcu, 2000). Different inventories have been used in English corpora annotated for discourse relations (Hobbs, 1985; Prasad et al., 2008a; Carlson et al., 2002) which also differ in other respects (such as whether they prescribe a tree structure for discourse annotation). However, the annotation level of existing Arabic corpora has not yet included the discourse layer, making our work the first to address this problem for Arabic on a larger scale. Automatic discourse pa</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>J.R. Hobbs. 1985. On the coherence and structure of discourse. Center for the Study of Language and Information, Stanford, Calif.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Knott</author>
<author>T Sanders</author>
</authors>
<title>The classification of coherence relations and their linguistic markers: An exploration of two languages.</title>
<date>1998</date>
<journal>Journal of Pragmatics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="6741" citStr="Knott and Sanders, 1998" startWordPosition="1017" endWordPosition="1020">most always a CONTRAST relation. However, there are connectives where this is not the case, such as .a;A /mnd¯/since which has a CAUSAL and a TEMPORAL sense. The third task is argument identification which identifies the arguments’ position and extent. In this paper we tackle Task 1 and Task 2 for Arabic in a supervised machine learning framework. 3 Related work Annotated Discourse Corpora and Linguistic Background. Discourse relations are widely studied in theoretical linguistics (Halliday and Hasan, 1976; Hobbs, 1985), where also different relation taxonomies have been derived (Hobbs, 1985; Knott and Sanders, 1998; Mann and Thompson, 1988; Marcu, 2000). Different inventories have been used in English corpora annotated for discourse relations (Hobbs, 1985; Prasad et al., 2008a; Carlson et al., 2002) which also differ in other respects (such as whether they prescribe a tree structure for discourse annotation). However, the annotation level of existing Arabic corpora has not yet included the discourse layer, making our work the first to address this problem for Arabic on a larger scale. Automatic discourse parsing: explicit relations. There is no work on discourse connective recognition, interpretation an</context>
</contexts>
<marker>Knott, Sanders, 1998</marker>
<rawString>A. Knott and T. Sanders. 1998. The classification of coherence relations and their linguistic markers: An exploration of two languages. Journal of Pragmatics, 30(2):135–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Lin</author>
<author>M Kan</author>
<author>H T Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the penn discourse treebank.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>343--351</pages>
<contexts>
<context position="9733" citStr="Lin et al., 2009" startWordPosition="1477" endWordPosition="1480">ith regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang et al., 2010). 2008) and often need world knowledge (Lin et al., 2009). However, features/approaches that have shown improvement over a baseline are word</context>
<context position="28216" citStr="Lin et al., 2009" startWordPosition="4419" endWordPosition="4422">the arguments of 742 Features Acurr K Baseline (not conn) 68.9 0 M1 Conn only 75.7 0.48 Tokenization by white space + auto tagger M2 Conn+ SConn+Lex 85.6 0.62 M3 Conn+ SConn+Lex+POS 87.6 0.69 M4 Conn+SConn+Lex+POS+Masdar 88.5 0.70 ATB-based features M5 Conn+SConn+Lex 86.2 0.65 M6 Conn+SConn+Lex+Syn/POS 91.2 0.79 M7 Conn+SConn+Lex+Syn/POS+Masdar 92.4 0.82 M8 Conn+SConn+Syn 91.2 0.79 M9 SConn+Lex+Syn+Masdar 91.2 0.79 Table 5: Performance of different models for identifying discourse connectives. the connective are known. This is well-established for PDTB relation recognition (Wang et al., 2010; Lin et al., 2009; Miltsakaki et al., 2005). Our models predict single relations on two datasets: (i) all instances of connectives signalling single relations (Set All, 6039 instances) (2) all instances apart from the connective 9 /w/and at beginning of paragraph as they are affected by the auto-correction process (Set no-wa-atBOP, 3813 instances). We use 10-fold cross-validation and JRip as well as a McNemar test at the 5% level for significance tests. 6.1 Features Whereas some of the features we use have been used for English implicit relation recognition (Lin et al., 2009; Wang et al., 2010; Pitler et al., </context>
<context position="31267" citStr="Lin et al., 2009" startWordPosition="4944" endWordPosition="4947">are in different sentences. A nominal feature encodes one of the three orders Arg1 Conn Arg2, Conn Arg2 Arg1 and Arg1 Conn Arg2 Arg1, the latter being frequent in Arabic for TEMPORAL.ASYNCHRONOUS relations. Argument Parent. We record the syntactic parent of each Argument. However, not every argu743 ment corresponds to a complete tree in the ATB — in these cases we extract the category of the parent shared by the first and last word in the argument. Production Rules. We use all non-lexical production rules that occur more than 10 times in the arguments as binary features. This was inspired by (Lin et al., 2009) who use production rules to good effect for implicit relations in English. 6.2 Results Table 6 shows the results for fine-grained (17 relations) classification. The baseline of assigning the most frequent relation EXPANSION.CONJUNCTION to every connective performs with an accuracy of 52.5% on Set All and 35% on set no-wa-atBOP. If we use a model that relies on the discourse connective alone (M1) we achieve results of 77.2%/74.3%, respectively. As noted in Section 4.5 this is substantially lower than what the same model can achieve for English. Including connective and argument features (apart</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Z. Lin, M. Kan, and H.T. Ng. 2009. Recognizing implicit discourse relations in the penn discourse treebank. In Proc. of EMNLP 2009, pages 343–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>Creating local coherence: An empirical assessment.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL</booktitle>
<contexts>
<context position="9797" citStr="Louis and Nenkova, 2010" startWordPosition="1489" endWordPosition="1492"> a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang et al., 2010). 2008) and often need world knowledge (Lin et al., 2009). However, features/approaches that have shown improvement over a baseline are word pairs (Sporleder and Lascarides, 2008), production rules and sy</context>
</contexts>
<marker>Louis, Nenkova, 2010</marker>
<rawString>A. Louis and A. Nenkova. 2010. Creating local coherence: An empirical assessment. In Proc. of NAACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>A Bies</author>
</authors>
<title>Developing an Arabic treebank: Methods, guidelines, procedures, and tools.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages (COLING),</booktitle>
<location>Geneva.</location>
<contexts>
<context position="10816" citStr="Maamouri and Bies, 2004" startWordPosition="1649" endWordPosition="1652">). 2008) and often need world knowledge (Lin et al., 2009). However, features/approaches that have shown improvement over a baseline are word pairs (Sporleder and Lascarides, 2008), production rules and syntactic trees (Wang et al., 2010; Lin et al., 2009) as well as language modelling (Zhou et al., 2010). As we only deal with explicit connectives this work is not directly comparable to ours, although we do explore some of the suggested features for improving explicit connective disambiguation. 4 An Arabic Discourse Corpus We annotate news articles from the Arabic Penn Treebank (Part 1 v2.0) (Maamouri and Bies, 2004) for explicitly marked discourse relations. This is the first discourse-annotated corpus for Arabic, whose initial development stages we have described in (AlSaif and Markert, 2010). We summarize this previous work and extend it by including agreement studies for arguments in Sections 4.1 and 4.2. In Sections 4.3, 4.4 and 4.5. we then present a corpus study on the corpus which shows our major claim as to the importance and high levels of ambiguity of Arabic discourse connectives. 4.1 Annotation Principles We overall follow the annotation principles in the Penn Discourse Treebank for explicit c</context>
</contexts>
<marker>Maamouri, Bies, 2004</marker>
<rawString>M. Maamouri and A. Bies. 2004. Developing an Arabic treebank: Methods, guidelines, procedures, and tools. In Proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages (COLING), Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="6766" citStr="Mann and Thompson, 1988" startWordPosition="1021" endWordPosition="1024">lation. However, there are connectives where this is not the case, such as .a;A /mnd¯/since which has a CAUSAL and a TEMPORAL sense. The third task is argument identification which identifies the arguments’ position and extent. In this paper we tackle Task 1 and Task 2 for Arabic in a supervised machine learning framework. 3 Related work Annotated Discourse Corpora and Linguistic Background. Discourse relations are widely studied in theoretical linguistics (Halliday and Hasan, 1976; Hobbs, 1985), where also different relation taxonomies have been derived (Hobbs, 1985; Knott and Sanders, 1998; Mann and Thompson, 1988; Marcu, 2000). Different inventories have been used in English corpora annotated for discourse relations (Hobbs, 1985; Prasad et al., 2008a; Carlson et al., 2002) which also differ in other respects (such as whether they prescribe a tree structure for discourse annotation). However, the annotation level of existing Arabic corpora has not yet included the discourse layer, making our work the first to address this problem for Arabic on a larger scale. Automatic discourse parsing: explicit relations. There is no work on discourse connective recognition, interpretation and argument assignment for</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>W.C. Mann and S.A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>A Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="9581" citStr="Marcu and Echihabi, 2002" startWordPosition="1451" endWordPosition="1454">ent relation per connective baseline” for which significance tests are not given. We will show that for Arabic, discourse connectives are more highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang e</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>D. Marcu and A. Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proc. of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The theory and practice of discourse parsing and summarization.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1558" citStr="Marcu, 2000" startWordPosition="217" endWordPosition="218">ition, an identifier based solely on simple lexical and automatically derived morphological and POS features performs with high reliability, essential for languages that do not have high-quality parsers yet. Our algorithm for recognizing discourse relations performs significantly better than a baseline based on the connective surface string alone and therefore reduces the ambiguity in explicit connective interpretation. 1 Introduction The automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such as automatic summarization (Marcu, 2000), question answering (Girju, 2003), sentiment analysis (Somasundaran et al., 2008) and readability assessment (Pitler and Nenkova, 2008). This task has recently seen renewed interest due to the growing availability of large-scale corpora annotated for discourse relations, such as the Penn Discourse Treebank (Prasad et al., 2008a). In the Penn Discourse Treebank (PDTB), local discourse relations (also called senses) such as CAUSAL or CONTRAST are annotated. They hold between two text segments (so-called arguments) that express abstract entities such as events, facts and propositions. Annotated </context>
<context position="6780" citStr="Marcu, 2000" startWordPosition="1025" endWordPosition="1026">e connectives where this is not the case, such as .a;A /mnd¯/since which has a CAUSAL and a TEMPORAL sense. The third task is argument identification which identifies the arguments’ position and extent. In this paper we tackle Task 1 and Task 2 for Arabic in a supervised machine learning framework. 3 Related work Annotated Discourse Corpora and Linguistic Background. Discourse relations are widely studied in theoretical linguistics (Halliday and Hasan, 1976; Hobbs, 1985), where also different relation taxonomies have been derived (Hobbs, 1985; Knott and Sanders, 1998; Mann and Thompson, 1988; Marcu, 2000). Different inventories have been used in English corpora annotated for discourse relations (Hobbs, 1985; Prasad et al., 2008a; Carlson et al., 2002) which also differ in other respects (such as whether they prescribe a tree structure for discourse annotation). However, the annotation level of existing Arabic corpora has not yet included the discourse layer, making our work the first to address this problem for Arabic on a larger scale. Automatic discourse parsing: explicit relations. There is no work on discourse connective recognition, interpretation and argument assignment for Arabic, so th</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>D. Marcu. 2000. The theory and practice of discourse parsing and summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Miltsakaki</author>
<author>N Dinesh</author>
<author>R Prasad</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>Experiments on sense annotation and sense disambiguation of discourse connectives.</title>
<date>2005</date>
<booktitle>In Proc. of the Workshop on Treebanks and Linguistic Theories.</booktitle>
<contexts>
<context position="8422" citStr="Miltsakaki et al., 2005" startWordPosition="1274" endWordPosition="1277">ner and Pustejovski, 2007; Elwell and Baldridge, 2008) 737 and Nenkova, 2009) use gold standard syntactic features as well as the connective surface string in a supervised model for discourse connective recognition. They achieve very high results with this approach. We will (i) show that similar features work well for Arabic (ii) take into account Arabic-specific morphological properties that improve results further and (iii) present a robust version of this approach that does not rely on full parsing or gold standard syntactic annotations. With regard to discourse connective interpretation, (Miltsakaki et al., 2005) concentrate on disambiguating the three connectives since, while, when only, using a very small set of features indicating tense and temporal markers in arguments. They achieve good improvements over a “most frequent relation per connective” baseline. A more comprehensive study on all discourse connectives in the PDTB (Pitler et al., 2008; Pitler and Nenkova, 2009) reveals that most connectives are not ambiguous in English. Using syntactic features of the connective, they achieve only a very small improvement over a “most frequent relation per connective baseline” for which significance tests</context>
<context position="28242" citStr="Miltsakaki et al., 2005" startWordPosition="4423" endWordPosition="4426">42 Features Acurr K Baseline (not conn) 68.9 0 M1 Conn only 75.7 0.48 Tokenization by white space + auto tagger M2 Conn+ SConn+Lex 85.6 0.62 M3 Conn+ SConn+Lex+POS 87.6 0.69 M4 Conn+SConn+Lex+POS+Masdar 88.5 0.70 ATB-based features M5 Conn+SConn+Lex 86.2 0.65 M6 Conn+SConn+Lex+Syn/POS 91.2 0.79 M7 Conn+SConn+Lex+Syn/POS+Masdar 92.4 0.82 M8 Conn+SConn+Syn 91.2 0.79 M9 SConn+Lex+Syn+Masdar 91.2 0.79 Table 5: Performance of different models for identifying discourse connectives. the connective are known. This is well-established for PDTB relation recognition (Wang et al., 2010; Lin et al., 2009; Miltsakaki et al., 2005). Our models predict single relations on two datasets: (i) all instances of connectives signalling single relations (Set All, 6039 instances) (2) all instances apart from the connective 9 /w/and at beginning of paragraph as they are affected by the auto-correction process (Set no-wa-atBOP, 3813 instances). We use 10-fold cross-validation and JRip as well as a McNemar test at the 5% level for significance tests. 6.1 Features Whereas some of the features we use have been used for English implicit relation recognition (Lin et al., 2009; Wang et al., 2010; Pitler et al., 2009) , they are new for A</context>
<context position="30030" citStr="Miltsakaki et al., 2005" startWordPosition="4727" endWordPosition="4730">e relation is � likely to be EXPANSION.BACKGROUND or EXPANSION.CONJUNCTION. We also measure word overlap between the arguments, hoping to catch relations such as COMPARISON.SIMILARITY. Masdar. This feature states whether the first or second word in Arg 2 is an Al-Masdar. Many prepositional connectives followed by an Al-Masdar indicate a CONTINGENCY.CAUSE relation (see Ex. 4) Tense and Negation. Each argument is assigned its tense as one of perfect, imperfect, future or none. We also indicate whether the tense of Arg1 or 2 are the same and whether a negation is part of Arg 1 or 2. Inspired by (Miltsakaki et al., 2005), we stipulate that tense is useful for recognizing temporal and causal relations. For example, the arguments of the relation TEMPORAL.SYNCHRONOUS are likely to have the same tense. In contrast, arg1 tense is more likely to be prior to arg2 tense for TEMPORAL.ASYNCHRONOUS and CAUSE relations. Length, Distance and Order Features. We use the length of arguments (in words), word distance between a connective and its arguments (-1; for arguments in order Arg1 Conn Arg2 Arg1), tree distance of connective and arguments (0 if connective and an argument are in the same tree) and a binary feature of wh</context>
</contexts>
<marker>Miltsakaki, Dinesh, Prasad, Joshi, Webber, 2005</marker>
<rawString>E. Miltsakaki, N. Dinesh, R. Prasad, A. Joshi, and B. Webber. 2005. Experiments on sense annotation and sense disambiguation of discourse connectives. In Proc. of the Workshop on Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>A Nenkova</author>
</authors>
<title>Revisiting readability: A unified framework for predicting text quality.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>186--195</pages>
<contexts>
<context position="1694" citStr="Pitler and Nenkova, 2008" startWordPosition="232" endWordPosition="235">high reliability, essential for languages that do not have high-quality parsers yet. Our algorithm for recognizing discourse relations performs significantly better than a baseline based on the connective surface string alone and therefore reduces the ambiguity in explicit connective interpretation. 1 Introduction The automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such as automatic summarization (Marcu, 2000), question answering (Girju, 2003), sentiment analysis (Somasundaran et al., 2008) and readability assessment (Pitler and Nenkova, 2008). This task has recently seen renewed interest due to the growing availability of large-scale corpora annotated for discourse relations, such as the Penn Discourse Treebank (Prasad et al., 2008a). In the Penn Discourse Treebank (PDTB), local discourse relations (also called senses) such as CAUSAL or CONTRAST are annotated. They hold between two text segments (so-called arguments) that express abstract entities such as events, facts and propositions. Annotated discourse relations can be signalled explicitly by so-called discourse connectives (Marcu, 2000; Webber et al., 1999; Prasad et al., 200</context>
</contexts>
<marker>Pitler, Nenkova, 2008</marker>
<rawString>E. Pitler and A. Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In Proc. of EMNLP 2008, pages 186–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>A Nenkova</author>
</authors>
<title>Using syntax to disambiguate explicit discourse connectives.</title>
<date>2009</date>
<booktitle>In Proc of ACL-IJCNLP 2009 (Short Papers),</booktitle>
<pages>13--16</pages>
<contexts>
<context position="5331" citStr="Pitler and Nenkova, 2009" startWordPosition="797" endWordPosition="800">usage (such as the use of while as a noun). We show in Section 5 that we can distinguish discourse- and non-discourse usage for potential connectives in Arabic with very high reliability, even without parsed data, a fact that is important for languages with fewer high quality NLP tools available. We then present an algorithm for relation identification in Section 6 that shows small but significant gains over assigning the most frequent relation for each connective. We discuss future work and conclude in Section 7. 2 The Tasks The handling of explicit connectives can be split into three tasks (Pitler and Nenkova, 2009). The first task of discourse connective recognition distinguishes between the discourse usage and non-discourse usage of potential connectives. Whereas some potential connectives such as the Arabic connective A /lkn/but almost always have discourse usage, this is not true for all potential connectives.1 Thus, the discourse usage of Arabic L. /r˙gbh/desire needs to be distinguished from its use as a noun. Conjunctions such as 9 /w/and,91 /¯aw/or can have discourse usage or just conjoin two non-abstract entities as in o�l.. 9 /qnr w s¯arh/Omar and Sarah. The second task is discourse connective </context>
<context position="8790" citStr="Pitler and Nenkova, 2009" startWordPosition="1331" endWordPosition="1334">orphological properties that improve results further and (iii) present a robust version of this approach that does not rely on full parsing or gold standard syntactic annotations. With regard to discourse connective interpretation, (Miltsakaki et al., 2005) concentrate on disambiguating the three connectives since, while, when only, using a very small set of features indicating tense and temporal markers in arguments. They achieve good improvements over a “most frequent relation per connective” baseline. A more comprehensive study on all discourse connectives in the PDTB (Pitler et al., 2008; Pitler and Nenkova, 2009) reveals that most connectives are not ambiguous in English. Using syntactic features of the connective, they achieve only a very small improvement over a “most frequent relation per connective baseline” for which significance tests are not given. We will show that for Arabic, discourse connectives are more highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations.</context>
<context position="21448" citStr="Pitler and Nenkova, 2009" startWordPosition="3344" endWordPosition="3347">lations corresponds approximately to our fine-grained inventory. This comparison can only be appropriate due to slight differences in the lower-grained relation inventory. 5.1 Features Apart from the surface string of the potential connective Conn, we use the following features. Features are either extracted from raw files tokenized by white space only (M2) or from raw files tokenized by white space and tagged by the Stanford tagger9 (Models M3, M4) or from the Arabic Treebank (ATB) gold standard part-of-speech and parse annotation (models M5-M9). The syntactic features (Syn) are inspired by (Pitler and Nenkova, 2009). Lexical/POS patterns of surrounding words, clitic features and Al-Masdar are novel. Surface Features (SConn). These include the position of the potential connective (sentence-initial, medial or final). The type of the potential connective is Simple when the potential connective is a single token not attached to other tokens, PotClitic when it is attached. Potential connectives containing more than one token have MoreThanToken type. Models where we use ATB or automated tagging (M3-M9) distinguish further between potential clitics that are assigned a POS and ones that are not. Models that use </context>
<context position="26247" citStr="Pitler and Nenkova, 2009" startWordPosition="4117" endWordPosition="4120">ance level of 1%. A most frequent category baseline would assign all potential connectives as not connective, achieving an accuracy of 68.9% as only 6328 of our potential 20,312 connectives actually have discourse usage. We built several models using different features. The results are shown in Table 5. A simple model M1 that only uses the connective string improves significantly over the baseline with 75.7% accuracy but a kappa of only 0.48, showing that this is not a reliable strategy. Models M2-M4 do not rely on gold standard annotation or parsing (in contrast to the models for English in (Pitler and Nenkova, 2009)). Using only surface and lexical features that can be extracted from white-space tokenized raw files in addition to the connective string (M2), gains a substantial improvement over using the connective string alone. This is further improved by using POS tags of connectives and surrounding words with an automatic tagger (M3) and by including the Al-Masdar feature (M4), thus making good use of the morphological properties of Arabic. All differences are statistically significant (M1 &lt; M2 &lt; M3 &lt; M4). The final model is reliable (kappa 0.70), an encouraging result given the absence of parsing and </context>
</contexts>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>E. Pitler and A. Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives. In Proc of ACL-IJCNLP 2009 (Short Papers), pages 13–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>M Raghupathy</author>
<author>H Mehta</author>
<author>A Nenkova</author>
<author>A Lee</author>
<author>A Joshi</author>
</authors>
<title>Easily identifiable discourse relations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008),</booktitle>
<location>Manchester, UK,</location>
<contexts>
<context position="8763" citStr="Pitler et al., 2008" startWordPosition="1327" endWordPosition="1330">unt Arabic-specific morphological properties that improve results further and (iii) present a robust version of this approach that does not rely on full parsing or gold standard syntactic annotations. With regard to discourse connective interpretation, (Miltsakaki et al., 2005) concentrate on disambiguating the three connectives since, while, when only, using a very small set of features indicating tense and temporal markers in arguments. They achieve good improvements over a “most frequent relation per connective” baseline. A more comprehensive study on all discourse connectives in the PDTB (Pitler et al., 2008; Pitler and Nenkova, 2009) reveals that most connectives are not ambiguous in English. Using syntactic features of the connective, they achieve only a very small improvement over a “most frequent relation per connective baseline” for which significance tests are not given. We will show that for Arabic, discourse connectives are more highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse p</context>
</contexts>
<marker>Pitler, Raghupathy, Mehta, Nenkova, Lee, Joshi, 2008</marker>
<rawString>E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee, and A. Joshi. 2008. Easily identifiable discourse relations. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008), Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP</booktitle>
<pages>683--691</pages>
<contexts>
<context position="9715" citStr="Pitler et al., 2009" startWordPosition="1473" endWordPosition="1476">re highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang et al., 2010). 2008) and often need world knowledge (Lin et al., 2009). However, features/approaches that have shown improvement over a</context>
<context position="28821" citStr="Pitler et al., 2009" startWordPosition="4517" endWordPosition="4520">in et al., 2009; Miltsakaki et al., 2005). Our models predict single relations on two datasets: (i) all instances of connectives signalling single relations (Set All, 6039 instances) (2) all instances apart from the connective 9 /w/and at beginning of paragraph as they are affected by the auto-correction process (Set no-wa-atBOP, 3813 instances). We use 10-fold cross-validation and JRip as well as a McNemar test at the 5% level for significance tests. 6.1 Features Whereas some of the features we use have been used for English implicit relation recognition (Lin et al., 2009; Wang et al., 2010; Pitler et al., 2009) , they are new for Arabic and not widely used for explicit connectives. All features are extracted from the ATB gold standard parses. Connective features. This includes the connective string Conn. In addition, we also use the surface connective features and POS of connective described in Section 5. We also use the syntactic path to the connective as a novel feature. Words and POS of arguments. The words and pos tags of the first three words in Arg1 and Arg2 are used to catch patterns in arguments. For example, when the first word of Arg2 is �� /qd/might/may or j�� /k¯an/had, the relation is �</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proc. of ACL-IJCNLP 2009, pages 683–691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>L Robaldo</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The Penn discourse treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="1887" citStr="Prasad et al., 2008" startWordPosition="263" endWordPosition="266">ctive surface string alone and therefore reduces the ambiguity in explicit connective interpretation. 1 Introduction The automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such as automatic summarization (Marcu, 2000), question answering (Girju, 2003), sentiment analysis (Somasundaran et al., 2008) and readability assessment (Pitler and Nenkova, 2008). This task has recently seen renewed interest due to the growing availability of large-scale corpora annotated for discourse relations, such as the Penn Discourse Treebank (Prasad et al., 2008a). In the Penn Discourse Treebank (PDTB), local discourse relations (also called senses) such as CAUSAL or CONTRAST are annotated. They hold between two text segments (so-called arguments) that express abstract entities such as events, facts and propositions. Annotated discourse relations can be signalled explicitly by so-called discourse connectives (Marcu, 2000; Webber et al., 1999; Prasad et al., 2008a) or hold implicitly between adjacent sentences in the same paragraph, i.e. are not signalled by a specific surface string. In Ex. 1, the connective while indicates an explicit CONTRAST betwe</context>
<context position="3110" citStr="Prasad et al., 2008" startWordPosition="451" endWordPosition="454">he attitudes of John and Richard. In Ex. 2, the connective while indicates an explicit TEMPORAL relation. In Ex. 3, an implicit CAUSAL relation between the first and second sentence holds. We indicate discourse connectives and the two arguments they relate via annotated square brackets. (1) [John liked adventure,]Arg2 [ while]DC[Richard was cautious]Arg2 (2) [The children were crying loudly]Arg1[while]DC,[their mother was cooking]Arg2 (3) [I cannot eat any dessert.]Arg1 [I have eaten far too much already.]Arg2 Although similar corpora for other languages are being developed such as for Hindi (Prasad et al., 2008b), Turkish (Zeyrek and Webber, 2008), Chinese (Xue, 2005) and, by ourselves, for Arabic (Al736 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 736–747, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Saif and Markert, 2010), efforts in the automated recognition of discourse connectives, arguments and relations have so far almost exclusively centered on English. In contrast we present the first models for discourse relations for Arabic, focusing on explicit connectives. This focus is partially justified b</context>
<context position="6905" citStr="Prasad et al., 2008" startWordPosition="1041" endWordPosition="1044">task is argument identification which identifies the arguments’ position and extent. In this paper we tackle Task 1 and Task 2 for Arabic in a supervised machine learning framework. 3 Related work Annotated Discourse Corpora and Linguistic Background. Discourse relations are widely studied in theoretical linguistics (Halliday and Hasan, 1976; Hobbs, 1985), where also different relation taxonomies have been derived (Hobbs, 1985; Knott and Sanders, 1998; Mann and Thompson, 1988; Marcu, 2000). Different inventories have been used in English corpora annotated for discourse relations (Hobbs, 1985; Prasad et al., 2008a; Carlson et al., 2002) which also differ in other respects (such as whether they prescribe a tree structure for discourse annotation). However, the annotation level of existing Arabic corpora has not yet included the discourse layer, making our work the first to address this problem for Arabic on a larger scale. Automatic discourse parsing: explicit relations. There is no work on discourse connective recognition, interpretation and argument assignment for Arabic, so that we break entirely new ground here. However, the two tasks we explore (discourse connective recognition and discourse conne</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber. 2008a. The Penn discourse treebank 2.0. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>S Husain</author>
<author>D M Sharma</author>
<author>A Joshi</author>
</authors>
<title>Towards an Annotated Corpus of Discourse Relations in Hindi.</title>
<date>2008</date>
<booktitle>In The Third International Joint Conference on Natural Language Processing,</booktitle>
<pages>7--12</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1887" citStr="Prasad et al., 2008" startWordPosition="263" endWordPosition="266">ctive surface string alone and therefore reduces the ambiguity in explicit connective interpretation. 1 Introduction The automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such as automatic summarization (Marcu, 2000), question answering (Girju, 2003), sentiment analysis (Somasundaran et al., 2008) and readability assessment (Pitler and Nenkova, 2008). This task has recently seen renewed interest due to the growing availability of large-scale corpora annotated for discourse relations, such as the Penn Discourse Treebank (Prasad et al., 2008a). In the Penn Discourse Treebank (PDTB), local discourse relations (also called senses) such as CAUSAL or CONTRAST are annotated. They hold between two text segments (so-called arguments) that express abstract entities such as events, facts and propositions. Annotated discourse relations can be signalled explicitly by so-called discourse connectives (Marcu, 2000; Webber et al., 1999; Prasad et al., 2008a) or hold implicitly between adjacent sentences in the same paragraph, i.e. are not signalled by a specific surface string. In Ex. 1, the connective while indicates an explicit CONTRAST betwe</context>
<context position="3110" citStr="Prasad et al., 2008" startWordPosition="451" endWordPosition="454">he attitudes of John and Richard. In Ex. 2, the connective while indicates an explicit TEMPORAL relation. In Ex. 3, an implicit CAUSAL relation between the first and second sentence holds. We indicate discourse connectives and the two arguments they relate via annotated square brackets. (1) [John liked adventure,]Arg2 [ while]DC[Richard was cautious]Arg2 (2) [The children were crying loudly]Arg1[while]DC,[their mother was cooking]Arg2 (3) [I cannot eat any dessert.]Arg1 [I have eaten far too much already.]Arg2 Although similar corpora for other languages are being developed such as for Hindi (Prasad et al., 2008b), Turkish (Zeyrek and Webber, 2008), Chinese (Xue, 2005) and, by ourselves, for Arabic (Al736 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 736–747, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Saif and Markert, 2010), efforts in the automated recognition of discourse connectives, arguments and relations have so far almost exclusively centered on English. In contrast we present the first models for discourse relations for Arabic, focusing on explicit connectives. This focus is partially justified b</context>
<context position="6905" citStr="Prasad et al., 2008" startWordPosition="1041" endWordPosition="1044">task is argument identification which identifies the arguments’ position and extent. In this paper we tackle Task 1 and Task 2 for Arabic in a supervised machine learning framework. 3 Related work Annotated Discourse Corpora and Linguistic Background. Discourse relations are widely studied in theoretical linguistics (Halliday and Hasan, 1976; Hobbs, 1985), where also different relation taxonomies have been derived (Hobbs, 1985; Knott and Sanders, 1998; Mann and Thompson, 1988; Marcu, 2000). Different inventories have been used in English corpora annotated for discourse relations (Hobbs, 1985; Prasad et al., 2008a; Carlson et al., 2002) which also differ in other respects (such as whether they prescribe a tree structure for discourse annotation). However, the annotation level of existing Arabic corpora has not yet included the discourse layer, making our work the first to address this problem for Arabic on a larger scale. Automatic discourse parsing: explicit relations. There is no work on discourse connective recognition, interpretation and argument assignment for Arabic, so that we break entirely new ground here. However, the two tasks we explore (discourse connective recognition and discourse conne</context>
</contexts>
<marker>Prasad, Husain, Sharma, Joshi, 2008</marker>
<rawString>R. Prasad, S. Husain, D.M. Sharma, and A. Joshi. 2008b. Towards an Annotated Corpus of Discourse Relations in Hindi. In The Third International Joint Conference on Natural Language Processing, pages 7–12. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K C Ryding</author>
</authors>
<title>A reference grammar of modern standard Arabic. Cambridge Univ Pr.</title>
<date>2005</date>
<contexts>
<context position="12398" citStr="Ryding, 2005" startWordPosition="1908" endWordPosition="1909">ons necessary for Arabic are the inclusion of clitics as connectives such as È /l/for, H. /b/by,with and ¬ /f/then . In addition, differently to English, prepositions were included as connectives as these are frequently used to express discourse relations in Arabic. In these cases, normally argument 2 is the so-called Al-Masdar.4 Typical examples are Èñ“ð /ws.wl/arrival from the verb É“ð /ws./to arrive and aËðAm× /mh. ¯awlh/attempt from the verb ÈðAg 4The medieval Arabic grammar schools, the Basra and Kufa, debated whether the noun (almasdar) or the verb is the most basic element of language (Ryding, 2005). 738 Figure 1: Discourse relations for Arabic /h. ¯awl/to try. Al-Masdar is formed using morphological patterns well-known in the Arabic grammatical tradition: major Arabic grammars list around 60 patterns although some other references also claim that the patterns are many more as well as more unpredictable (Abdl al latif et al., 1997; Wright, 2008; Ryding, 2005). Al-Masdar forms do not fit into one grammatical or morphological category in English: they might correspond to a gerund, a nominalization or a noun which is not a nominalization. Some examples are listed in Table 1. Table 1: A list</context>
</contexts>
<marker>Ryding, 2005</marker>
<rawString>K.C. Ryding. 2005. A reference grammar of modern standard Arabic. Cambridge Univ Pr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Siegel</author>
<author>N J Castellan</author>
</authors>
<title>Nonparametric statistics for the behavioral sciences.</title>
<date>1956</date>
<publisher>McGraw-Hill</publisher>
<location>New York.</location>
<contexts>
<context position="14229" citStr="Siegel and Castellan, 1956" startWordPosition="2191" endWordPosition="2195">t.]Arg1[l]DC[ltbly˙g ,n fqd¯an wt¯ ¯a-iq alˇsrkh alrsmyh]Arg2 [We went to the police station]Arg1 [for]DC [informing about the loss of the company’s official documents.]Arg2 4.2 Agreement Studies The occurrences of a precompiled list of 107 potential discourse connectives were annotated independently by 2 native Arabic speakers on 537 news texts. Agreement was measured for the distinction of discourse vs. non-discourse usage, relation assignment and argument assignment. Agreement for the classification tasks of discourse connective recognition and relation assignment was measured using kappa (Siegel and Castellan, 1956). Argument agreement was measured by agr, a directional measure (Wiebe et al., 2005). It measures the word overlap between the text spans of two judges (ann1 and ann2). agr(ann1||ann2) measures the proportion of words ann1 annotated that were also annotated by ann2. agr(ann1||ann2) = |ann1 matching ann2| |ann1| Discourse connective recognition proved to be highly reliable with percentage agreement of 0.95 and a kappa of 0.88 on the 23,331 occurrences of the 107 potential discourse connectives. 5586 of the potential connectives were agreed on by both annotators to have discourse usage and agree</context>
</contexts>
<marker>Siegel, Castellan, 1956</marker>
<rawString>S. Siegel and N.J. Castellan. 1956. Nonparametric statistics for the behavioral sciences. McGraw-Hill New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Wiebe</author>
<author>J Ruppenhofer</author>
</authors>
<title>Discourse-level opinion interpretation.</title>
<date>2008</date>
<booktitle>In Proc. of Coling</booktitle>
<contexts>
<context position="1640" citStr="Somasundaran et al., 2008" startWordPosition="225" endWordPosition="228">y derived morphological and POS features performs with high reliability, essential for languages that do not have high-quality parsers yet. Our algorithm for recognizing discourse relations performs significantly better than a baseline based on the connective surface string alone and therefore reduces the ambiguity in explicit connective interpretation. 1 Introduction The automatic detection of discourse relations, such as causal, contrast or temporal relations, is useful for many applications such as automatic summarization (Marcu, 2000), question answering (Girju, 2003), sentiment analysis (Somasundaran et al., 2008) and readability assessment (Pitler and Nenkova, 2008). This task has recently seen renewed interest due to the growing availability of large-scale corpora annotated for discourse relations, such as the Penn Discourse Treebank (Prasad et al., 2008a). In the Penn Discourse Treebank (PDTB), local discourse relations (also called senses) such as CAUSAL or CONTRAST are annotated. They hold between two text segments (so-called arguments) that express abstract entities such as events, facts and propositions. Annotated discourse relations can be signalled explicitly by so-called discourse connectives</context>
</contexts>
<marker>Somasundaran, Wiebe, Ruppenhofer, 2008</marker>
<rawString>S. Somasundaran, J. Wiebe, and J. Ruppenhofer. 2008. Discourse-level opinion interpretation. In Proc. of Coling 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In Proc of HLT-NAACL</booktitle>
<contexts>
<context position="9524" citStr="Soricut and Marcu, 2003" startWordPosition="1443" endWordPosition="1446">chieve only a very small improvement over a “most frequent relation per connective baseline” for which significance tests are not given. We will show that for Arabic, discourse connectives are more highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3Some work does not make the distinction between implicit and explicit and/or treats them in a joint framewo</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>R. Soricut and D. Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In Proc of HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>A Lascarides</author>
</authors>
<title>Using automatically labelled examples to classify rhetorical relations: An assessment.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<pages>14--369</pages>
<contexts>
<context position="9854" citStr="Sporleder and Lascarides, 2008" startWordPosition="1496" endWordPosition="1499">ture set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang et al., 2010). 2008) and often need world knowledge (Lin et al., 2009). However, features/approaches that have shown improvement over a baseline are word pairs (Sporleder and Lascarides, 2008), production rules and syntactic trees (Wang et al., 2010; Lin et al., 2009) as we</context>
</contexts>
<marker>Sporleder, Lascarides, 2008</marker>
<rawString>C. Sporleder and A. Lascarides. 2008. Using automatically labelled examples to classify rhetorical relations: An assessment. Natural Language Engineering, 14:369–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>J Su</author>
<author>C Tan</author>
</authors>
<title>Kernel-based discourse relation recognition with temporal ordering information.</title>
<date>2010</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>710--719</pages>
<contexts>
<context position="9752" citStr="Wang et al., 2010" startWordPosition="1481" endWordPosition="1484">relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang et al., 2010). 2008) and often need world knowledge (Lin et al., 2009). However, features/approaches that have shown improvement over a baseline are word pairs (Sporleder a</context>
<context position="28198" citStr="Wang et al., 2010" startWordPosition="4415" endWordPosition="4418">al, we assume that the arguments of 742 Features Acurr K Baseline (not conn) 68.9 0 M1 Conn only 75.7 0.48 Tokenization by white space + auto tagger M2 Conn+ SConn+Lex 85.6 0.62 M3 Conn+ SConn+Lex+POS 87.6 0.69 M4 Conn+SConn+Lex+POS+Masdar 88.5 0.70 ATB-based features M5 Conn+SConn+Lex 86.2 0.65 M6 Conn+SConn+Lex+Syn/POS 91.2 0.79 M7 Conn+SConn+Lex+Syn/POS+Masdar 92.4 0.82 M8 Conn+SConn+Syn 91.2 0.79 M9 SConn+Lex+Syn+Masdar 91.2 0.79 Table 5: Performance of different models for identifying discourse connectives. the connective are known. This is well-established for PDTB relation recognition (Wang et al., 2010; Lin et al., 2009; Miltsakaki et al., 2005). Our models predict single relations on two datasets: (i) all instances of connectives signalling single relations (Set All, 6039 instances) (2) all instances apart from the connective 9 /w/and at beginning of paragraph as they are affected by the auto-correction process (Set no-wa-atBOP, 3813 instances). We use 10-fold cross-validation and JRip as well as a McNemar test at the 5% level for significance tests. 6.1 Features Whereas some of the features we use have been used for English implicit relation recognition (Lin et al., 2009; Wang et al., 201</context>
</contexts>
<marker>Wang, Su, Tan, 2010</marker>
<rawString>W. Wang, J. Su, and C. Tan. 2010. Kernel-based discourse relation recognition with temporal ordering information. In Proc. of ACL 2010, pages 710–719.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Webber</author>
<author>A Knott</author>
<author>M Stone</author>
<author>A Joshi</author>
</authors>
<title>Discourse relations: A structural and presuppositional account using lexicalised TAG.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>page</pages>
<contexts>
<context position="2274" citStr="Webber et al., 1999" startWordPosition="319" endWordPosition="322">y assessment (Pitler and Nenkova, 2008). This task has recently seen renewed interest due to the growing availability of large-scale corpora annotated for discourse relations, such as the Penn Discourse Treebank (Prasad et al., 2008a). In the Penn Discourse Treebank (PDTB), local discourse relations (also called senses) such as CAUSAL or CONTRAST are annotated. They hold between two text segments (so-called arguments) that express abstract entities such as events, facts and propositions. Annotated discourse relations can be signalled explicitly by so-called discourse connectives (Marcu, 2000; Webber et al., 1999; Prasad et al., 2008a) or hold implicitly between adjacent sentences in the same paragraph, i.e. are not signalled by a specific surface string. In Ex. 1, the connective while indicates an explicit CONTRAST between the attitudes of John and Richard. In Ex. 2, the connective while indicates an explicit TEMPORAL relation. In Ex. 3, an implicit CAUSAL relation between the first and second sentence holds. We indicate discourse connectives and the two arguments they relate via annotated square brackets. (1) [John liked adventure,]Arg2 [ while]DC[Richard was cautious]Arg2 (2) [The children were cry</context>
</contexts>
<marker>Webber, Knott, Stone, Joshi, 1999</marker>
<rawString>B. Webber, A. Knott, M. Stone, and A. Joshi. 1999. Discourse relations: A structural and presuppositional account using lexicalised TAG. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, page 48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wellner</author>
<author>J Pustejovski</author>
</authors>
<title>Automatically identifying the arguments of discourse connectives.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>92--101</pages>
<contexts>
<context position="7823" citStr="Wellner and Pustejovski, 2007" startWordPosition="1178" endWordPosition="1181"> Arabic on a larger scale. Automatic discourse parsing: explicit relations. There is no work on discourse connective recognition, interpretation and argument assignment for Arabic, so that we break entirely new ground here. However, the two tasks we explore (discourse connective recognition and discourse connective disambiguation) have been tackled for English.2 (Pitler 1Arabic examples contain in order: the Arabic right-to-left script, the transliteration (standards ISO/R 233 and DIN 31635) and the English translation (if possible). 2There is also substantial work on argument identification (Wellner and Pustejovski, 2007; Elwell and Baldridge, 2008) 737 and Nenkova, 2009) use gold standard syntactic features as well as the connective surface string in a supervised model for discourse connective recognition. They achieve very high results with this approach. We will (i) show that similar features work well for Arabic (ii) take into account Arabic-specific morphological properties that improve results further and (iii) present a robust version of this approach that does not rely on full parsing or gold standard syntactic annotations. With regard to discourse connective interpretation, (Miltsakaki et al., 2005) </context>
</contexts>
<marker>Wellner, Pustejovski, 2007</marker>
<rawString>B. Wellner and J. Pustejovski. 2007. Automatically identifying the arguments of discourse connectives. In Proc. of EMNLP 2007, pages 92–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wellner</author>
<author>J Pustejovski</author>
<author>A Havasi</author>
<author>A Rumshisky</author>
<author>R Suair</author>
</authors>
<title>Classification of discourse coherence relations: An exploratory study using multiple knowledge sources.</title>
<date>2006</date>
<booktitle>In Proc. of SIGDIAL2006.</booktitle>
<contexts>
<context position="9654" citStr="Wellner et al., 2006" startWordPosition="1462" endWordPosition="1465">ven. We will show that for Arabic, discourse connectives are more highly ambiguous with regard to the relations they convey. We will present a supervised learning model that uses a wider feature set and that achieves small but significant improvements over the most frequent relation per connective baseline. Automatic discourse parsing: implicit relations. Implicit relations have excited substantial interest for English. This includes work in the framework of RST (Soricut and Marcu, 2003; duVerle and Prendinger, 2009; Marcu and Echihabi, 2002), SDRT (Baldridge and Lascarides, 2005), GraphBank (Wellner et al., 2006), the PDTB (BlairGoldensohn et al., 2007; Pitler et al., 2009; Lin et al., 2009; Wang et al., 2010; Zhou et al., 2010; Louis and Nenkova, 2010) or frameworkindependent (Sporleder and Lascarides, 2008).3 The task is challenging as implicits behave substantially differently from explicits (Sporleder and Lascarides, but we do not discuss this work in depth here. 3Some work does not make the distinction between implicit and explicit and/or treats them in a joint framework (Soricut and Marcu, 2003; Wellner et al., 2006; Wang et al., 2010). 2008) and often need world knowledge (Lin et al., 2009). Ho</context>
</contexts>
<marker>Wellner, Pustejovski, Havasi, Rumshisky, Suair, 2006</marker>
<rawString>B. Wellner, J. Pustejovski, A. Havasi, A. Rumshisky, and R. Suair. 2006. Classification of discourse coherence relations: An exploratory study using multiple knowledge sources. In Proc. of SIGDIAL2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation.</title>
<date>2005</date>
<contexts>
<context position="14313" citStr="Wiebe et al., 2005" startWordPosition="2205" endWordPosition="2208">]Arg1 [for]DC [informing about the loss of the company’s official documents.]Arg2 4.2 Agreement Studies The occurrences of a precompiled list of 107 potential discourse connectives were annotated independently by 2 native Arabic speakers on 537 news texts. Agreement was measured for the distinction of discourse vs. non-discourse usage, relation assignment and argument assignment. Agreement for the classification tasks of discourse connective recognition and relation assignment was measured using kappa (Siegel and Castellan, 1956). Argument agreement was measured by agr, a directional measure (Wiebe et al., 2005). It measures the word overlap between the text spans of two judges (ann1 and ann2). agr(ann1||ann2) measures the proportion of words ann1 annotated that were also annotated by ann2. agr(ann1||ann2) = |ann1 matching ann2| |ann1| Discourse connective recognition proved to be highly reliable with percentage agreement of 0.95 and a kappa of 0.88 on the 23,331 occurrences of the 107 potential discourse connectives. 5586 of the potential connectives were agreed on by both annotators to have discourse usage and agreement for relations and argument assignment was measured on these. As shown in Table </context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wright</author>
</authors>
<title>A grammar of the Arabic language.</title>
<date>2008</date>
<journal>Bibliobazaar.</journal>
<contexts>
<context position="12750" citStr="Wright, 2008" startWordPosition="1965" endWordPosition="1966">Èñ“ð /ws.wl/arrival from the verb É“ð /ws./to arrive and aËðAm× /mh. ¯awlh/attempt from the verb ÈðAg 4The medieval Arabic grammar schools, the Basra and Kufa, debated whether the noun (almasdar) or the verb is the most basic element of language (Ryding, 2005). 738 Figure 1: Discourse relations for Arabic /h. ¯awl/to try. Al-Masdar is formed using morphological patterns well-known in the Arabic grammatical tradition: major Arabic grammars list around 60 patterns although some other references also claim that the patterns are many more as well as more unpredictable (Abdl al latif et al., 1997; Wright, 2008; Ryding, 2005). Al-Masdar forms do not fit into one grammatical or morphological category in English: they might correspond to a gerund, a nominalization or a noun which is not a nominalization. Some examples are listed in Table 1. Table 1: A list of Al-MaSdar patterns, examples and their English correspondence Root Pattern MaSdar Translation iJ.ƒ /sbh. &lt;ËAªi /f-alh 4kAJ.ƒ /sbah. h swimming �¯ ÉJ�ª�®�K /tf,yl JJ�oZ /tnfyd execution Y�®�K /nfd ¯ ¯X /df, ÈAª�¯ /f,¯al © defence ¨A�¯X /df¯a, ¨PP� /zr, &lt;ËAªi /f,¯alh 4«@P P� /zr¯a,h agriculture H. Qk /h. rb Éª&apos; /f,l H. Qk /h. rb war An example of A</context>
</contexts>
<marker>Wright, 2008</marker>
<rawString>W. Wright. 2008. A grammar of the Arabic language. Bibliobazaar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Annotating discourse connectives in the chinese treebank.</title>
<date>2005</date>
<booktitle>In CorpusAnno ’05: Proceedings of the Workshop on Frontiers in Corpus Annotations II,</booktitle>
<pages>84--91</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3168" citStr="Xue, 2005" startWordPosition="462" endWordPosition="463">dicates an explicit TEMPORAL relation. In Ex. 3, an implicit CAUSAL relation between the first and second sentence holds. We indicate discourse connectives and the two arguments they relate via annotated square brackets. (1) [John liked adventure,]Arg2 [ while]DC[Richard was cautious]Arg2 (2) [The children were crying loudly]Arg1[while]DC,[their mother was cooking]Arg2 (3) [I cannot eat any dessert.]Arg1 [I have eaten far too much already.]Arg2 Although similar corpora for other languages are being developed such as for Hindi (Prasad et al., 2008b), Turkish (Zeyrek and Webber, 2008), Chinese (Xue, 2005) and, by ourselves, for Arabic (Al736 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 736–747, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Saif and Markert, 2010), efforts in the automated recognition of discourse connectives, arguments and relations have so far almost exclusively centered on English. In contrast we present the first models for discourse relations for Arabic, focusing on explicit connectives. This focus is partially justified by the fact that this first study for a new language should</context>
</contexts>
<marker>Xue, 2005</marker>
<rawString>Nianwen Xue. 2005. Annotating discourse connectives in the chinese treebank. In CorpusAnno ’05: Proceedings of the Workshop on Frontiers in Corpus Annotations II, pages 84–91, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zeyrek</author>
<author>B Webber</author>
</authors>
<title>A discourse resource for turkish: Annotating discourse connectives in the metu corpus.</title>
<date>2008</date>
<booktitle>Proceedings of IJCNLP-2008.</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="3147" citStr="Zeyrek and Webber, 2008" startWordPosition="456" endWordPosition="459">. In Ex. 2, the connective while indicates an explicit TEMPORAL relation. In Ex. 3, an implicit CAUSAL relation between the first and second sentence holds. We indicate discourse connectives and the two arguments they relate via annotated square brackets. (1) [John liked adventure,]Arg2 [ while]DC[Richard was cautious]Arg2 (2) [The children were crying loudly]Arg1[while]DC,[their mother was cooking]Arg2 (3) [I cannot eat any dessert.]Arg1 [I have eaten far too much already.]Arg2 Although similar corpora for other languages are being developed such as for Hindi (Prasad et al., 2008b), Turkish (Zeyrek and Webber, 2008), Chinese (Xue, 2005) and, by ourselves, for Arabic (Al736 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 736–747, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Saif and Markert, 2010), efforts in the automated recognition of discourse connectives, arguments and relations have so far almost exclusively centered on English. In contrast we present the first models for discourse relations for Arabic, focusing on explicit connectives. This focus is partially justified by the fact that this first study for </context>
</contexts>
<marker>Zeyrek, Webber, 2008</marker>
<rawString>D. Zeyrek and B. Webber. 2008. A discourse resource for turkish: Annotating discourse connectives in the metu corpus. Proceedings of IJCNLP-2008. Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C</author>
</authors>
<title>Predicting discourse connectives for implicit discourse relation recognition.</title>
<date>2010</date>
<booktitle>In Proc. of Coling</booktitle>
<pages>1507--1514</pages>
<marker>C, 2010</marker>
<rawString>Z. Zhou, Y. Xu, Z. Niu, M. Lan, . Su, and Tan. C. 2010. Predicting discourse connectives for implicit discourse relation recognition. In Proc. of Coling 2010, pages 1507–1514.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>