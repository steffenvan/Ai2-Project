<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013541">
<title confidence="0.945995">
Using N-Grams to Understand the Nature of Summaries
</title>
<author confidence="0.731285">
Michele Banko and Lucy Vanderwende
</author>
<address confidence="0.714904">
One Microsoft Way
Redmond, WA 98052
</address>
<email confidence="0.998415">
{mbanko, lucyv}@microsoft.com
</email>
<sectionHeader confidence="0.995628" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999837904761905">
Although single-document summarization is a
well-studied task, the nature of multi-
document summarization is only beginning to
be studied in detail. While close attention has
been paid to what technologies are necessary
when moving from single to multi-document
summarization, the properties of human-
written multi-document summaries have not
been quantified. In this paper, we empirically
characterize human-written summaries
provided in a widely used summarization
corpus by attempting to answer the questions:
Can multi-document summaries that are
written by humans be characterized as
extractive or generative? Are multi-document
summaries less extractive than single-
document summaries? Our results suggest that
extraction-based techniques which have been
successful for single-document summarization
may not be sufficient when summarizing
multiple documents.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998568581818182">
The explosion of available online text has made it
necessary to be able to present information in a succinct,
navigable manner. The increased accessibility of
worldwide online news sources and the continually
expanding size of the worldwide web place demands on
users attempting to wade through vast amounts of text.
Document clustering and multi-document summarization
technologies working in tandem promise to ease some of
the burden on users when browsing related documents.
Summarizing a set of documents brings about
challenges that are not present when summarizing a
single document. One might expect that a good multi-
document summary will present a synthesis of multiple
views of the event being described over different
documents, or present a high-level view of an event that
is not explicitly reflected in any single document. A
useful multi-document summary may also indicate the
presence of new or distinct information contained within
a set of documents describing the same topic (McKeown
et. al., 1999, Mani and Bloedorn, 1999). To meet these
expectations, a multi-document summary is required to
generalize, condense and merge information coming
from multiple sources.
Although single-document summarization is a well-
studied task (see Mani and Maybury, 1999 for an
overview), multi-document summarization is only
recently being studied closely (Marcu &amp; Gerber 2001).
While close attention has been paid to multi-document
summarization technologies (Barzilay et al. 2002,
Goldstein et al 2000), the inherent properties of human-
written multi-document summaries have not yet been
quantified. In this paper, we seek to empirically
characterize ideal multi-document summaries in part by
attempting to answer the questions: Can multi-document
summaries that are written by humans be characterized
as extractive or generative? Are multi-document
summaries less extractive than single-document
summaries? Our aim in answering these questions is to
discover how the nature of multi-document summaries
will impact our system requirements.
We have chosen to focus our experiments on the data
provided for summarization evaluation during the
Document Understanding Conference (DUC). While we
recognize that other summarization corpora may exhibit
different properties than what we report, the data
prepared for DUC evaluations is widely used, and
continues to be a powerful force in shaping directions in
summarization research and evaluation.
In the following section we describe previous work
related to investigating the potential for extractive
summaries. Section 3 describes a new approach for
assessing the degree to which a summary can be
described as extractive, and reports our findings for both
single and multiple document summarization tasks. We
conclude with a discussion of our findings in Section 4.
</bodyText>
<sectionHeader confidence="0.999186" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999969689655172">
Jing (2002) previously examined the degree to which
single-document summaries can be characterized as
extractive. Based on a manual inspection of 15 human-
written summaries, she proposes that for the task of
single-document summarization, human summarizers
use a “cut-and-paste” approach in which six main
operations are performed: sentence reduction, sentence
combination, syntactic transformation, reordering,
lexical paraphrasing, and generalization or specification.
The first four operations are reflected in the construction
of an HMM model that can be used to decompose human
summaries. According to this model, 81% of summary
sentences contained in a corpus of 300 human-written
summaries of news articles on telecommunications were
found to fit the cut-and-paste method, with the rest
believed to have been composed from scratch.1
Another recent study (Lin and Hovy, 2003)
investigated the extent to which extractive methods may
be sufficient for summarization in the single-document
case. By computing a performance upper-bound for
pure sentence extraction, they found that state-of-the-art
extraction-based systems are still 15%-24%2 away from
this limit, and 10% away from average human
performance. While this sheds light on how much gain
can be achieved by optimizing sentence extraction
methods for single-document summarization, to our
knowledge, no one has assessed the potential for
extraction-based systems when attempting to summarize
multiple documents.
</bodyText>
<sectionHeader confidence="0.656904" genericHeader="method">
3 Using N-gram Sequences to
Characterize Summaries
</sectionHeader>
<bodyText confidence="0.999969333333333">
Our approach to characterizing summaries is much
simpler than what Jing has described and is based on the
following idea: if human-written summaries are
extractive, then we should expect to see long spans of
text that have been lifted from the source documents to
form a summary.
Note that this holds under the assumptions made by
Jing’s model of operations that are performed by human
summarizers. In the examples of operations given by
Jing, we notice that long n-grams are preserved
(designated by brackets), even in the operations mostly
likely to disrupt the original text:
</bodyText>
<footnote confidence="0.8101992">
1 Jing considers a sentence to have been generated from
scratch if fewer than half of the words were composed of
terms coming from the original document.
2 The range in potential gains is due to possible
variations in summary length.
</footnote>
<subsectionHeader confidence="0.736857">
Sentence Reduction:
</subsectionHeader>
<bodyText confidence="0.998915571428571">
Document sentence: When it arrives sometime next
year in new TV sets, the V-chip will give parents a new
and potentially revolutionary device to block out
programs they don’t want their children to see.
Summary sentence: [The V-chip will give parents a]
[device to block out programs they don’t want their
children to see.]
</bodyText>
<subsectionHeader confidence="0.893519">
Syntactic Transformation:
</subsectionHeader>
<bodyText confidence="0.989341625">
Document sentence: Since annoy.com enables
visitors to send unvarnished opinions to political and
other figures in the news, the company was concerned
that its activities would be banned by the statute.
Summary sentence: [Annoy.com enables visitors to
send unvarnished opinions to political and other figures
in the news] and feared the law could put them out of
business.
</bodyText>
<subsectionHeader confidence="0.671893">
Sentence Combination:
</subsectionHeader>
<bodyText confidence="0.999850272727273">
Document sentence 1: But it also raises serious
questions about the privacy of such highly personal
information wafting about the digital world.
Document sentence 2: The issue thus fits squarely
into the broader debate about privacy and security on the
Internet, whether it involves protecting credit card
numbers or keeping children from offensive information.
Summary sentence: [But it also raises] the issue of
[privacy of such] [personal information] and this issue
hits the nail on the head [in the broader debate about
privacy and security on the Internet.]
</bodyText>
<subsectionHeader confidence="0.996195">
3.1 Data and Experiments
</subsectionHeader>
<bodyText confidence="0.999920511627907">
For our experiments we used data made available from
the 2001 Document Understanding Conference (DUC),
an annual large-scale evaluation of summarization
systems sponsored by the National Institute of Standards
and Technology (NIST). In this corpus, NIST has
gathered documents describing 60 events, taken from the
Associated Press, Wall Street Journal, FBIS San Jose
Mercury, and LA Times newswires. An event is
described by between 3 and 20 separate (but not
necessarily unique) documents; on average a cluster
contains 10 documents. Of the 60 available clusters, we
used the portion specifically designated for training,
which contains a total of 295 documents distributed over
30 clusters.
As part of the DUC 2001 summarization corpus,
NIST also provides four hand-written summaries of
different lengths for every document cluster, as well as
100-word summaries of each document. Since we
wished to collectively compare single-document
summaries against multi-document summaries, we used
the 100-word multi-document summaries for our
analysis. It is important to note that for each cluster, all
summaries (50, 100, 200 and 400-word multi-document
and 100-word per-document) have been written by the
same author. NIST used a total of ten authors, each
providing summaries for 3 of the 30 topics. The
instructions provided did not differ per task; in both
single and multi-document scenarios, the authors were
directed to use complete sentences and told to feel free to
use their own words (Over, 2004).
To compare the text of human-authored multi-
document summaries to the full-text documents
describing the events, we automatically broke the
documents into sentences, and constructed a minimal
tiling of each summary sentence. Specifically, for each
sentence in the summary, we searched for all n-grams
that are present in both the summary and the documents,
placing no restrictions on the potential size of an n-gram.
We then covered each summary sentence with the n-
grams, optimizing to use as few n-grams as possible (i.e.
favoring n-grams that are longer in length). For this
experiment, we normalized the data by converting all
terms to lowercase and removing punctuation.
</bodyText>
<subsectionHeader confidence="0.730902">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999876130434783">
On average, we found the length of a tile to be 4.47 for
single-document summaries, compared with 2.33 for
multi-document summaries. We discovered that 61 out
of all 1667 hand-written single-document summary
sentences exactly matched a sentence in the source
document, however we did not find any sentences for
which this was the case when examining multi-document
summaries.
We also wanted to study how many sentences are
fully tiled by phrases coming from exactly one sentence
in the document corpus, and found that while no
sentences from the multi-document summaries matched
this criteria, 7.6% of sentences in the single-document
summaries could be tiled in this manner. When trying to
tile sentences with tiles coming from only one document
sentence, we found that we could tile, on average, 93%
of a single-document sentence in that manner, compared
to an average of 36% of a multi-document sentence.
This suggests that for multi-document summarization,
we are not seeing any instances of what can be
considered single-sentence compression. Table 1
summarizes the findings we have presented in this
section.
</bodyText>
<table confidence="0.9950713">
SingleDoc MultiDoc
Average Tile Size 4.47 2.33
(words) 38 24
Max Tile Size 3.7% 0%
(words) 7.6% 0%
Exact sentence
matches
Complete tiling
from single
sentence
</table>
<tableCaption confidence="0.99994">
Table 1. Comparison of Summary Tiling
</tableCaption>
<bodyText confidence="0.999630705882353">
Figure 1 shows the relative frequency with which a
summary sentence is optimally tiled using tile-sizes up to
25 words in length in both the single and multi-
document scenarios. The data shows that the relative
frequency with which a single-document summary
sentence is optimally tiled using n-grams containing 3 or
more words is consistently higher compared to the multi-
document case. Not shown on the histogram (due to
insufficient readability) is that we found 379 tiles (of
approximately 86,000) between 25 and 38 words long
covering sentences from single-document summaries.
No tiles longer than 24 words were found for multi-
document summaries.
In order to test whether tile samples coming from
tiling of single-document summaries and multi-
document summaries are likely to have come from the
same underlying population, we performed two one-
tailed unpaired t-tests, in one instance assuming equal
variances, and in the other case asssuming the variances
were unequal. For these statistical significance tests, we
randomly sampled 100 summary sentences from each
task, and extracted the lengths of the n-grams found via
minmal tiling. This resulted in the creation of a sample
of 551 tiles for single-document sentences and 735 tiles
for multi-document sentences.
For both tests (performed with α=0.05), the P-values
were low enough (0.00033 and 0.000858, respectively)
to be able to reject the null hypothesis that the average
tile length coming from single-document summaries is
the same as the average tile length found in multi-
document summaries. We chose to use a one-tailed P-
value because based on our experiments we already
suspected that the single-document tiles had a larger
mean.
</bodyText>
<figureCaption confidence="0.9787115">
Figure 1. Comparison of Relative Frequencies of
Optimal Tile Lengths
</figureCaption>
<figure confidence="0.987861714285714">
0.50
0.45
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Singledoc Multidoc
Tile Length
</figure>
<sectionHeader confidence="0.853638" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999991644444445">
Our experiments show that when writing multi-
document summaries, human summarizers do not appear
to be cutting and pasting phrases in an extractive fashion.
On average, they are borrowing text around the bigram
level, instead of extracting long sequences of words or
full sentences as they tend to do when summarizing a
single document. The extent to which human
summarizers form extractive summaries during single
and multi-document summarization was found to be
different at a level which is statistically significant.
These findings are additionally supported by the fact that
automatic n-gram-based evaluation measures now being
used to assess predominately extractive multi-document
summarization systems correlate strongly with human
judgments when restricted to the usage of unigrams and
bigrams, but correlate weakly when longer n-grams are
factored into the equation (Lin &amp; Hovy, 2003). In the
future, we wish to apply our method to other corpora,
and to explore the extent to which different
summarization goals, such as describing an event or
providing a biography, affect the degree to which
humans employ rewriting as opposed to extraction.
Despite the unique requirements for multi-document
summarization, relatively few systems have crossed over
into employing generation and reformulation (McKeown
&amp; Radev, 1995, Nenkova, et al. 2003). For the most part,
summarization systems continue to be based on sentence
extraction methods. Considering that humans appear to
be generating summary text that differs widely from
sentences in the original documents, we suspect that
approaches which make use of generation and
reformulation techniques may yield the most promise for
multi-document summarization. We would like to
empirically quantify to what extent current
summarization systems reformulate text, by applying the
techniques presented in this paper to system output.
Finally, the potential impact of our findings with
respect to recent evaluation metrics should not be
overlooked. Caution must be given when employing
automatic evaluation metrics based on the overlap of n-
grams between human references and system summaries.
When reference summaries do not contain long n-grams
drawn from the source documents, but are instead
generated in the author’s own words, the use of a large
number of reference summaries becomes more critical.
</bodyText>
<sectionHeader confidence="0.996349" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.982292333333333">
The authors wish to thank Eric Ringger for providing
tools used to construct the tiles, and Bob Moore for
suggestions pertaining to our experiments.
</bodyText>
<reference confidence="0.998594104166667">
Regina Barzilay, Noemie Elhadad, Kathleen McKeown.
2002. &amp;quot;Inferring Strategies for Sentence Ordering in
Multidocument Summarization.&amp;quot; JAIR, 17:35-55.
Jade Goldstein, Vibhu Mittal, Mark Kantrowitz and
Jaime Carbonell, 2000. Multi-Document
Summarization by Sentence Extraction. In the
Proceedings of the ANLP/NAACL Workshop on
Automatic Summarization. Seattle, WA
Hongyan Jing. 2002. Using Hidden Markov Modeling to
Decompose Human-Written Summaries.
Computational Linguistics 28(4): 527-543.
Chin-Yew Lin, and E.H. Hovy 2003. Automatic
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of 2003
Language Technology Conference (HLT-NAACL
2003), Edmonton, Canada.
Chin-Yew Lin and Eduard.H. Hovy. 2003. The Potential
and Limitations of Sentence Extraction for
Summarization. In Proceedings of the Workshop on
Automatic Summarization post-conference workshop
of HLT-NAACL-2003. Edmonton, Canada.
Daniel Marcu and Laurie Gerber. 2001. An Inquiry into
the Nature of Multidocument Abstracts, Extracts, and
Their Evaluation. Proceedings of the NAACL-2001
Workshop on Automatic Summarization
Inderjeet Mani and Eric Bloedorn. 1999. Summarizing
similarities and differences among related documents.
Information Retrieval, 1, pp. 35-67.
Inderjeet Mani and Mark Maybury (Eds.). 1999.
Advances in Automatic Text Summarization. MIT
Press, Cambridge MA.
Kathy McKeown, Judith Klavans, Vasileios
Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin.
1999. Towards multidocument summarization by
reformulation: Progress and prospects. In Proceedings
of AAAI.
Kathleen R. McKeown and Dragomir R. Radev.
Generating summaries of multiple news articles.
1995. In Proceedings, 18th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 74-82.
Ani Nenkova, Barry Schiffman, Andrew Schlaiker,
Sasha Blair-Goldensohn, Regina Barzilay, Sergey
Sigelman, Vasileios Hatzivassiloglou, and Kathleen
McKeown. 2003. Columbia at the Document
Understanding Conference. Document Understanding
Conference 2003.
Paul Over. 2004. Personal communication.
</reference>
<sectionHeader confidence="0.636075" genericHeader="references">
References
</sectionHeader>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.520682">
<title confidence="0.999859">Using N-Grams to Understand the Nature of Summaries</title>
<author confidence="0.990809">Michele Banko</author>
<author confidence="0.990809">Lucy</author>
<affiliation confidence="0.525493">One Microsoft</affiliation>
<address confidence="0.997167">Redmond, WA</address>
<email confidence="0.999572">mbanko@microsoft.com</email>
<email confidence="0.999572">lucyv@microsoft.com</email>
<abstract confidence="0.9997185">Although single-document summarization is a well-studied task, the nature of multidocument summarization is only beginning to be studied in detail. While close attention has been paid to what technologies are necessary when moving from single to multi-document summarization, the properties of humanwritten multi-document summaries have not been quantified. In this paper, we empirically characterize human-written summaries provided in a widely used summarization corpus by attempting to answer the questions: Can multi-document summaries that are written by humans be characterized as extractive or generative? Are multi-document summaries less extractive than singledocument summaries? Our results suggest that extraction-based techniques which have been successful for single-document summarization may not be sufficient when summarizing multiple documents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Kathleen McKeown</author>
</authors>
<title>Inferring Strategies for Sentence Ordering in Multidocument Summarization.&amp;quot;</title>
<date>2002</date>
<journal>JAIR,</journal>
<pages>17--35</pages>
<contexts>
<context position="2523" citStr="Barzilay et al. 2002" startWordPosition="358" endWordPosition="361">mary may also indicate the presence of new or distinct information contained within a set of documents describing the same topic (McKeown et. al., 1999, Mani and Bloedorn, 1999). To meet these expectations, a multi-document summary is required to generalize, condense and merge information coming from multiple sources. Although single-document summarization is a wellstudied task (see Mani and Maybury, 1999 for an overview), multi-document summarization is only recently being studied closely (Marcu &amp; Gerber 2001). While close attention has been paid to multi-document summarization technologies (Barzilay et al. 2002, Goldstein et al 2000), the inherent properties of humanwritten multi-document summaries have not yet been quantified. In this paper, we seek to empirically characterize ideal multi-document summaries in part by attempting to answer the questions: Can multi-document summaries that are written by humans be characterized as extractive or generative? Are multi-document summaries less extractive than single-document summaries? Our aim in answering these questions is to discover how the nature of multi-document summaries will impact our system requirements. We have chosen to focus our experiments </context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>Regina Barzilay, Noemie Elhadad, Kathleen McKeown. 2002. &amp;quot;Inferring Strategies for Sentence Ordering in Multidocument Summarization.&amp;quot; JAIR, 17:35-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Vibhu Mittal</author>
<author>Mark Kantrowitz</author>
<author>Jaime Carbonell</author>
</authors>
<title>Multi-Document Summarization by Sentence Extraction.</title>
<date>2000</date>
<booktitle>In the Proceedings of the ANLP/NAACL Workshop on Automatic Summarization.</booktitle>
<location>Seattle, WA</location>
<contexts>
<context position="2546" citStr="Goldstein et al 2000" startWordPosition="362" endWordPosition="365"> the presence of new or distinct information contained within a set of documents describing the same topic (McKeown et. al., 1999, Mani and Bloedorn, 1999). To meet these expectations, a multi-document summary is required to generalize, condense and merge information coming from multiple sources. Although single-document summarization is a wellstudied task (see Mani and Maybury, 1999 for an overview), multi-document summarization is only recently being studied closely (Marcu &amp; Gerber 2001). While close attention has been paid to multi-document summarization technologies (Barzilay et al. 2002, Goldstein et al 2000), the inherent properties of humanwritten multi-document summaries have not yet been quantified. In this paper, we seek to empirically characterize ideal multi-document summaries in part by attempting to answer the questions: Can multi-document summaries that are written by humans be characterized as extractive or generative? Are multi-document summaries less extractive than single-document summaries? Our aim in answering these questions is to discover how the nature of multi-document summaries will impact our system requirements. We have chosen to focus our experiments on the data provided fo</context>
</contexts>
<marker>Goldstein, Mittal, Kantrowitz, Carbonell, 2000</marker>
<rawString>Jade Goldstein, Vibhu Mittal, Mark Kantrowitz and Jaime Carbonell, 2000. Multi-Document Summarization by Sentence Extraction. In the Proceedings of the ANLP/NAACL Workshop on Automatic Summarization. Seattle, WA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Using Hidden Markov Modeling to Decompose Human-Written Summaries.</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<issue>4</issue>
<pages>527--543</pages>
<contexts>
<context position="3883" citStr="Jing (2002)" startWordPosition="560" endWordPosition="561">pora may exhibit different properties than what we report, the data prepared for DUC evaluations is widely used, and continues to be a powerful force in shaping directions in summarization research and evaluation. In the following section we describe previous work related to investigating the potential for extractive summaries. Section 3 describes a new approach for assessing the degree to which a summary can be described as extractive, and reports our findings for both single and multiple document summarization tasks. We conclude with a discussion of our findings in Section 4. 2 Related Work Jing (2002) previously examined the degree to which single-document summaries can be characterized as extractive. Based on a manual inspection of 15 humanwritten summaries, she proposes that for the task of single-document summarization, human summarizers use a “cut-and-paste” approach in which six main operations are performed: sentence reduction, sentence combination, syntactic transformation, reordering, lexical paraphrasing, and generalization or specification. The first four operations are reflected in the construction of an HMM model that can be used to decompose human summaries. According to this </context>
</contexts>
<marker>Jing, 2002</marker>
<rawString>Hongyan Jing. 2002. Using Hidden Markov Modeling to Decompose Human-Written Summaries. Computational Linguistics 28(4): 527-543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic Evaluation of Summaries Using N-gram Cooccurrence Statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of 2003 Language Technology Conference (HLT-NAACL</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="4752" citStr="Lin and Hovy, 2003" startWordPosition="682" endWordPosition="685"> “cut-and-paste” approach in which six main operations are performed: sentence reduction, sentence combination, syntactic transformation, reordering, lexical paraphrasing, and generalization or specification. The first four operations are reflected in the construction of an HMM model that can be used to decompose human summaries. According to this model, 81% of summary sentences contained in a corpus of 300 human-written summaries of news articles on telecommunications were found to fit the cut-and-paste method, with the rest believed to have been composed from scratch.1 Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. By computing a performance upper-bound for pure sentence extraction, they found that state-of-the-art extraction-based systems are still 15%-24%2 away from this limit, and 10% away from average human performance. While this sheds light on how much gain can be achieved by optimizing sentence extraction methods for single-document summarization, to our knowledge, no one has assessed the potential for extraction-based systems when attempting to summarize multiple documents. 3 Usin</context>
<context position="13904" citStr="Lin &amp; Hovy, 2003" startWordPosition="2116" endWordPosition="2119">nces as they tend to do when summarizing a single document. The extent to which human summarizers form extractive summaries during single and multi-document summarization was found to be different at a level which is statistically significant. These findings are additionally supported by the fact that automatic n-gram-based evaluation measures now being used to assess predominately extractive multi-document summarization systems correlate strongly with human judgments when restricted to the usage of unigrams and bigrams, but correlate weakly when longer n-grams are factored into the equation (Lin &amp; Hovy, 2003). In the future, we wish to apply our method to other corpora, and to explore the extent to which different summarization goals, such as describing an event or providing a biography, affect the degree to which humans employ rewriting as opposed to extraction. Despite the unique requirements for multi-document summarization, relatively few systems have crossed over into employing generation and reformulation (McKeown &amp; Radev, 1995, Nenkova, et al. 2003). For the most part, summarization systems continue to be based on sentence extraction methods. Considering that humans appear to be generating </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin, and E.H. Hovy 2003. Automatic Evaluation of Summaries Using N-gram Cooccurrence Statistics. In Proceedings of 2003 Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard H Hovy</author>
</authors>
<title>The Potential and Limitations of Sentence Extraction for Summarization.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization post-conference workshop of HLT-NAACL-2003.</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="4752" citStr="Lin and Hovy, 2003" startWordPosition="682" endWordPosition="685"> “cut-and-paste” approach in which six main operations are performed: sentence reduction, sentence combination, syntactic transformation, reordering, lexical paraphrasing, and generalization or specification. The first four operations are reflected in the construction of an HMM model that can be used to decompose human summaries. According to this model, 81% of summary sentences contained in a corpus of 300 human-written summaries of news articles on telecommunications were found to fit the cut-and-paste method, with the rest believed to have been composed from scratch.1 Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. By computing a performance upper-bound for pure sentence extraction, they found that state-of-the-art extraction-based systems are still 15%-24%2 away from this limit, and 10% away from average human performance. While this sheds light on how much gain can be achieved by optimizing sentence extraction methods for single-document summarization, to our knowledge, no one has assessed the potential for extraction-based systems when attempting to summarize multiple documents. 3 Usin</context>
<context position="13904" citStr="Lin &amp; Hovy, 2003" startWordPosition="2116" endWordPosition="2119">nces as they tend to do when summarizing a single document. The extent to which human summarizers form extractive summaries during single and multi-document summarization was found to be different at a level which is statistically significant. These findings are additionally supported by the fact that automatic n-gram-based evaluation measures now being used to assess predominately extractive multi-document summarization systems correlate strongly with human judgments when restricted to the usage of unigrams and bigrams, but correlate weakly when longer n-grams are factored into the equation (Lin &amp; Hovy, 2003). In the future, we wish to apply our method to other corpora, and to explore the extent to which different summarization goals, such as describing an event or providing a biography, affect the degree to which humans employ rewriting as opposed to extraction. Despite the unique requirements for multi-document summarization, relatively few systems have crossed over into employing generation and reformulation (McKeown &amp; Radev, 1995, Nenkova, et al. 2003). For the most part, summarization systems continue to be based on sentence extraction methods. Considering that humans appear to be generating </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard.H. Hovy. 2003. The Potential and Limitations of Sentence Extraction for Summarization. In Proceedings of the Workshop on Automatic Summarization post-conference workshop of HLT-NAACL-2003. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Laurie Gerber</author>
</authors>
<title>An Inquiry into the Nature of Multidocument Abstracts, Extracts, and Their Evaluation.</title>
<date>2001</date>
<booktitle>Proceedings of the NAACL-2001 Workshop on Automatic Summarization</booktitle>
<contexts>
<context position="2419" citStr="Marcu &amp; Gerber 2001" startWordPosition="344" endWordPosition="347">el view of an event that is not explicitly reflected in any single document. A useful multi-document summary may also indicate the presence of new or distinct information contained within a set of documents describing the same topic (McKeown et. al., 1999, Mani and Bloedorn, 1999). To meet these expectations, a multi-document summary is required to generalize, condense and merge information coming from multiple sources. Although single-document summarization is a wellstudied task (see Mani and Maybury, 1999 for an overview), multi-document summarization is only recently being studied closely (Marcu &amp; Gerber 2001). While close attention has been paid to multi-document summarization technologies (Barzilay et al. 2002, Goldstein et al 2000), the inherent properties of humanwritten multi-document summaries have not yet been quantified. In this paper, we seek to empirically characterize ideal multi-document summaries in part by attempting to answer the questions: Can multi-document summaries that are written by humans be characterized as extractive or generative? Are multi-document summaries less extractive than single-document summaries? Our aim in answering these questions is to discover how the nature o</context>
</contexts>
<marker>Marcu, Gerber, 2001</marker>
<rawString>Daniel Marcu and Laurie Gerber. 2001. An Inquiry into the Nature of Multidocument Abstracts, Extracts, and Their Evaluation. Proceedings of the NAACL-2001 Workshop on Automatic Summarization</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Eric Bloedorn</author>
</authors>
<title>Summarizing similarities and differences among related documents.</title>
<date>1999</date>
<journal>Information Retrieval,</journal>
<volume>1</volume>
<pages>35--67</pages>
<contexts>
<context position="2080" citStr="Mani and Bloedorn, 1999" startWordPosition="297" endWordPosition="300">some of the burden on users when browsing related documents. Summarizing a set of documents brings about challenges that are not present when summarizing a single document. One might expect that a good multidocument summary will present a synthesis of multiple views of the event being described over different documents, or present a high-level view of an event that is not explicitly reflected in any single document. A useful multi-document summary may also indicate the presence of new or distinct information contained within a set of documents describing the same topic (McKeown et. al., 1999, Mani and Bloedorn, 1999). To meet these expectations, a multi-document summary is required to generalize, condense and merge information coming from multiple sources. Although single-document summarization is a wellstudied task (see Mani and Maybury, 1999 for an overview), multi-document summarization is only recently being studied closely (Marcu &amp; Gerber 2001). While close attention has been paid to multi-document summarization technologies (Barzilay et al. 2002, Goldstein et al 2000), the inherent properties of humanwritten multi-document summaries have not yet been quantified. In this paper, we seek to empirically</context>
</contexts>
<marker>Mani, Bloedorn, 1999</marker>
<rawString>Inderjeet Mani and Eric Bloedorn. 1999. Summarizing similarities and differences among related documents. Information Retrieval, 1, pp. 35-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Mark Maybury</author>
</authors>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge MA.</location>
<contexts>
<context position="2311" citStr="Mani and Maybury, 1999" startWordPosition="329" endWordPosition="332">ent a synthesis of multiple views of the event being described over different documents, or present a high-level view of an event that is not explicitly reflected in any single document. A useful multi-document summary may also indicate the presence of new or distinct information contained within a set of documents describing the same topic (McKeown et. al., 1999, Mani and Bloedorn, 1999). To meet these expectations, a multi-document summary is required to generalize, condense and merge information coming from multiple sources. Although single-document summarization is a wellstudied task (see Mani and Maybury, 1999 for an overview), multi-document summarization is only recently being studied closely (Marcu &amp; Gerber 2001). While close attention has been paid to multi-document summarization technologies (Barzilay et al. 2002, Goldstein et al 2000), the inherent properties of humanwritten multi-document summaries have not yet been quantified. In this paper, we seek to empirically characterize ideal multi-document summaries in part by attempting to answer the questions: Can multi-document summaries that are written by humans be characterized as extractive or generative? Are multi-document summaries less ext</context>
</contexts>
<marker>Mani, Maybury, 1999</marker>
<rawString>Inderjeet Mani and Mark Maybury (Eds.). 1999. Advances in Automatic Text Summarization. MIT Press, Cambridge MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathy McKeown</author>
<author>Judith Klavans</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Regina Barzilay</author>
<author>Eleazar Eskin</author>
</authors>
<title>Towards multidocument summarization by reformulation: Progress and prospects.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<marker>McKeown, Klavans, Hatzivassiloglou, Barzilay, Eskin, 1999</marker>
<rawString>Kathy McKeown, Judith Klavans, Vasileios Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin. 1999. Towards multidocument summarization by reformulation: Progress and prospects. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Dragomir R Radev</author>
</authors>
<title>Generating summaries of multiple news articles.</title>
<date>1995</date>
<booktitle>In Proceedings, 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>74--82</pages>
<contexts>
<context position="14337" citStr="McKeown &amp; Radev, 1995" startWordPosition="2181" endWordPosition="2184">ems correlate strongly with human judgments when restricted to the usage of unigrams and bigrams, but correlate weakly when longer n-grams are factored into the equation (Lin &amp; Hovy, 2003). In the future, we wish to apply our method to other corpora, and to explore the extent to which different summarization goals, such as describing an event or providing a biography, affect the degree to which humans employ rewriting as opposed to extraction. Despite the unique requirements for multi-document summarization, relatively few systems have crossed over into employing generation and reformulation (McKeown &amp; Radev, 1995, Nenkova, et al. 2003). For the most part, summarization systems continue to be based on sentence extraction methods. Considering that humans appear to be generating summary text that differs widely from sentences in the original documents, we suspect that approaches which make use of generation and reformulation techniques may yield the most promise for multi-document summarization. We would like to empirically quantify to what extent current summarization systems reformulate text, by applying the techniques presented in this paper to system output. Finally, the potential impact of our findi</context>
</contexts>
<marker>McKeown, Radev, 1995</marker>
<rawString>Kathleen R. McKeown and Dragomir R. Radev. Generating summaries of multiple news articles. 1995. In Proceedings, 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 74-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Barry Schiffman</author>
<author>Andrew Schlaiker</author>
<author>Sasha Blair-Goldensohn</author>
<author>Regina Barzilay</author>
<author>Sergey Sigelman</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen McKeown</author>
</authors>
<title>Columbia at the Document Understanding Conference. Document Understanding Conference</title>
<date>2003</date>
<contexts>
<context position="14360" citStr="Nenkova, et al. 2003" startWordPosition="2185" endWordPosition="2188">with human judgments when restricted to the usage of unigrams and bigrams, but correlate weakly when longer n-grams are factored into the equation (Lin &amp; Hovy, 2003). In the future, we wish to apply our method to other corpora, and to explore the extent to which different summarization goals, such as describing an event or providing a biography, affect the degree to which humans employ rewriting as opposed to extraction. Despite the unique requirements for multi-document summarization, relatively few systems have crossed over into employing generation and reformulation (McKeown &amp; Radev, 1995, Nenkova, et al. 2003). For the most part, summarization systems continue to be based on sentence extraction methods. Considering that humans appear to be generating summary text that differs widely from sentences in the original documents, we suspect that approaches which make use of generation and reformulation techniques may yield the most promise for multi-document summarization. We would like to empirically quantify to what extent current summarization systems reformulate text, by applying the techniques presented in this paper to system output. Finally, the potential impact of our findings with respect to rec</context>
</contexts>
<marker>Nenkova, Schiffman, Schlaiker, Blair-Goldensohn, Barzilay, Sigelman, Hatzivassiloglou, McKeown, 2003</marker>
<rawString>Ani Nenkova, Barry Schiffman, Andrew Schlaiker, Sasha Blair-Goldensohn, Regina Barzilay, Sergey Sigelman, Vasileios Hatzivassiloglou, and Kathleen McKeown. 2003. Columbia at the Document Understanding Conference. Document Understanding Conference 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Over</author>
</authors>
<date>2004</date>
<tech>Personal communication.</tech>
<contexts>
<context position="9067" citStr="Over, 2004" startWordPosition="1349" endWordPosition="1350">wished to collectively compare single-document summaries against multi-document summaries, we used the 100-word multi-document summaries for our analysis. It is important to note that for each cluster, all summaries (50, 100, 200 and 400-word multi-document and 100-word per-document) have been written by the same author. NIST used a total of ten authors, each providing summaries for 3 of the 30 topics. The instructions provided did not differ per task; in both single and multi-document scenarios, the authors were directed to use complete sentences and told to feel free to use their own words (Over, 2004). To compare the text of human-authored multidocument summaries to the full-text documents describing the events, we automatically broke the documents into sentences, and constructed a minimal tiling of each summary sentence. Specifically, for each sentence in the summary, we searched for all n-grams that are present in both the summary and the documents, placing no restrictions on the potential size of an n-gram. We then covered each summary sentence with the ngrams, optimizing to use as few n-grams as possible (i.e. favoring n-grams that are longer in length). For this experiment, we normali</context>
</contexts>
<marker>Over, 2004</marker>
<rawString>Paul Over. 2004. Personal communication.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>