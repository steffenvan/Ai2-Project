<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9994515">
A Practical Methodology
for the Evaluation of Spoken Language Systems
</title>
<author confidence="0.941561">
Sean Boisen and Madeleine Bates*
</author>
<affiliation confidence="0.858025">
Bolt Beranek and Newman, Inc. (BBN)
</affiliation>
<address confidence="0.936562">
10 Moulton Street, Cambridge MA 02138 USA
</address>
<email confidence="0.997816">
sboisen@bbn.com, bates@bbn.com
</email>
<sectionHeader confidence="0.999822" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999929903225806">
A meaningful evaluation methodology can advance the
state-of-the-art by encouraging mature, practical applications
rather than &amp;quot;toy&amp;quot; implementations. Evaluation is also cru-
cial to assessing competing claims and identifying promis-
ing technical approaches. While work in speech recognition
(SR) has a history of evaluation methodologies that per-
mit comparison among various systems, until recently no
methodology existed for either developers of natural lan-
guage (NL) interfaces or researchers in speech understanding
(SU) to evaluate and compare the systems they developed.
Recently considerable progress has been made by a num-
ber of groups involved in the DARPA Spoken Language
Systems (SLS) program to agree on a methodology for
comparative evaluation of SLS systems, and that method-
ology has been put into practice several times in com-
parative tests of several SLS systems. These evaluations
are probably the only NI, evaluations other than the series
of Message Understanding Conferences (Sundheim, 1989;
Sundheim, 1991) to have been developed and used by a
group of researchers at different sites, although several ex-
cellent workshops have been held to study some of these
problems (Palmer et al., 1989; Neal et al., 1991).
This paper describes a practical &amp;quot;black-box&amp;quot; methodology
for automatic evaluation of question-answering NL systems.
While each new application domain will require some devel-
opment of special resources, the heart of the methodology is
domain-independent, and it can be used with either speech
or text input. The particular characteristics of the approach
are described in the following section: subsequent sections
present its implementation in the DARPA SLS community,
and some problems and directions for future development.
</bodyText>
<sectionHeader confidence="0.990951" genericHeader="method">
2 The Evaluation Framework
</sectionHeader>
<subsectionHeader confidence="0.975635">
2.1 Characteristics of the Methodology
</subsectionHeader>
<bodyText confidence="0.9981075">
The goal of this research has been to produce a well-defined,
meaningful evaluation methodology which is
</bodyText>
<footnote confidence="0.960713142857143">
*The work reported here was supported by the Advanced Re-
search Projects Agency and was monitored by the Office of Naval
Research under Contract No. 00014-89--C--0008. The views and
conclusions contained in this document are those of the authors
and should not be interpreted as necessarily representing the offi-
cial policies, either expressed or implied, of the Defense Advanced
Research Projects Agency or the United States Government.
</footnote>
<listItem confidence="0.999785090909091">
• automatic, to enable evaluation over large quantities of
data
• based on an objective assessment of the understanding
capabilities of a NL system (rather than its user inter-
face, portability, speed, etc.)
• capable of application to a wide variety of NL systems
and approaches
• suitable for blind testing
• as non-intrusive as possible on the system being eval-
uated (to decrease the costs of evaluation)
• domain independent.
</listItem>
<bodyText confidence="0.999456903225806">
The systems are assumed to be front ends to an interactive
database query system, implemented in a particular common
domain.
The methodology can be described as &amp;quot;black box&amp;quot; in that
there is no attempt to evaluate the internal representations
(syntactic, semantic, etc.) of a system. Instead, only the
content of an answer retrieved from the database is evalu-
ated: if the answer is correct, it is assumed that the system
understood the query correctly. Comparing answers has the
practical advantage of being a simple way to give widely var-
ied systems a common basis for comparison. Although some
recent work has suggested promising approaches (Black e,
al., 1991), system-internal representations are hard to com.
pare, or even impossible in some cases where System X ha
no level of representation corresponding to System Y&apos;s. I
is easy, however, to define a simple common language foi
representing answers (see Appendix A), and easy to mar
system-specific representations into this common language.
This methodology has been successfully applied in th(
context of cross-site blind tests, where the evaluation i.
based on input which the system has never seen before
This type of evaluation leaves out many other important as
pects of a system, such as the user interface, or the utilit]
(or speed) of performing a particular task with a system tha
includes a NL component (work by Tennant (1981), Bate
and Rettig (1988), and Neal et al. (1991) addresses some o
these other factors).
Examples below will be taken from the current DARRi
SLS application, the Airline Travel Information Systen
(ATIS). This is a database of flights with information o
the aircraft, stops and connections, meals, etc.
</bodyText>
<page confidence="0.989492">
162
</page>
<figure confidence="0.995400285714286">
T77:77.77777:77T:77&amp;quot;
Score (Right, Wrong, No Answer)
SLS
Answer
Text
text
NL
&amp;quot;meaning&amp;quot;
Application
Interface
DB Commands
Answer
Preparation
Data
</figure>
<figureCaption confidence="0.999941">
Figure 1: The evaluation process
</figureCaption>
<subsectionHeader confidence="0.999264">
2.2 Evaluation Architecture and Common Resources
</subsectionHeader>
<bodyText confidence="0.999911714285714">
We assume an evaluation architecture like that in Figure 1.
The shaded components are common resources of the eval-
uation, and are not part of the system(s) being evaluated.
Specifically, it is assumed there is a common database which
all systems use in producing answers, which defines both the
data tuples (rows in tables) and the data types for elements
of these tuples (string, integer, etc.).
Queries relevant to the database are collected under con-
ditions as realistic as possible (see 2.4). Answers to the
corpus of queries must be provided, expressed in a common
standard format (Common Answer Specification, or CAS):
one such format is exemplified in Appendix A. Some por-
tion of these pairs of queries and answers is then set aside
as a test corpus, and the remainder is provided as training
material.
In practice, it has also proved useful to include in the
training data the database query expression (for example, an
SQL expression) which was used to produce the reference
answer: this often makes it possible for system developers
to understand what was expected for a query, even if the
answer is empty or otherwise limited in content.
</bodyText>
<subsectionHeader confidence="0.96637">
2.2.1 Agreeing on Meaning
</subsectionHeader>
<bodyText confidence="0.993992346153846">
While the pairing of queries with answers provides the
training and test corpora, these must be augmented by com-
mon agreement as to how queries should be answered. In
practice, agreeing on the meaning of queries has been one
of the hardest tasks. The issues are often extremely subtle,
and interact with the structure and content of the database
in sometimes unexpected ways.
As an example of the problem, consider the following
request to an airline information system:
List the direct flights from
Boston to Dallas that serve
meals.
It seems straightforward, but should this include flights
that might stop in Chicago without making a connection
there? Should it include flights that serve a snack, since a
snack is not considered by some people to be a full meal?
Without some common agreement, many systems would
produce very different answers for the same questions, all
of them equally right according to each system&apos;s own defi-
nitions of the terms, but not amenable to automatic inter-
system comparison. To implement this methodology for
such a domain, therefore, it is necessary to stipulate the
meaning of potentially ambiguous terms such as &amp;quot;mid-day&amp;quot;,
&amp;quot;meals&amp;quot; , &amp;quot;the fare of a flight&amp;quot;. The current list of such
&amp;quot;principles of interpretation&amp;quot; for the ATIS domain contains
about 60 specifications, including things like:
</bodyText>
<listItem confidence="0.9982702">
• which tables and fields in the database identify the ma-
jor entities in the domain (flights, aircraft, fares, etc.)
• how to interpret fare expressions like &amp;quot;one-way fare&amp;quot;,
&amp;quot;the cheapest fare&amp;quot;, &amp;quot;excursion fare&amp;quot;, etc.
• which cities are to be considered &amp;quot;near&amp;quot; an airport.
</listItem>
<bodyText confidence="0.8934265">
Some other examples from the current principles of inter-
pretation are given in Appendix B.
</bodyText>
<page confidence="0.998055">
163
</page>
<subsubsectionHeader confidence="0.687587">
2.2.2 Reference Answers
</subsubsectionHeader>
<bodyText confidence="0.999491742857143">
It is not enough to agree on meaning of queries in the
chosen domain. It is also necessary to develop a common
understanding of precisely what is to be produced as the
answer, or part of the answer, to a question.
For example, if a user asks &amp;quot;What is the departure time of
the earliest flight from San Francisco to Atlanta?&amp;quot;, one sys-
tem might reply with a single time and another might reply
with that time plus additional columns containing the carrier
and flight number, a third system might also include the ar-
rival time and the origin and destination airports. None of
these answers could be said to be wrong, although one might
argue about the advantages and disadvantages of terseness
and verbosity.
While it is technically possible to mandate exactly which
columns from the database should be returned for expres-
sions, this is not practical: it requires agreement on a much
larger set of issues, and conflicts with the principle that eval-
uation should be as non-intrusive as possible. Furthermore,
it is not strictly necessary: what matters most is not whether
a system provided exactly the same data as some reference
answer, but whether the correct answer is clearly among the
data provided (as long as no incorrect data was returned).
For the sake of automatic evaluation, then, a canonical
reference answer (the minimum &amp;quot;right answer&amp;quot;) is devel-
oped for each evaluable query in the training set. The con-
tent of this reference answer is determined both by domain-
independent linguistic principles (Boisen et al., 1989) and
domain-specific stipulation. The language used to express
the answers for the ATIS domain is presented in Appendix A.
Evaluation using the minimal answer alone makes it pos-
sible to exploit the fact that extra fields in an answer are not
penalized. For example, the answer
((&amp;quot;AA&amp;quot; 152 0920 1015 &amp;quot;BOS&amp;quot; &amp;quot;CHI&amp;quot;
&amp;quot;SNACK&amp;quot; ) )
could be produced for any of the following queries:
</bodyText>
<listItem confidence="0.998938333333333">
• &amp;quot;When does American Airlines flight 152 leave?&amp;quot;
• &amp;quot;What&apos;s the earliest flight from Boston to Chicago?&amp;quot;
• &amp;quot;Does the 9:20 flight to Chicago serve meals?&amp;quot;
</listItem>
<bodyText confidence="0.999419818181818">
and would be counted correct.
For the ATIS evaluations, it was necessary to rectify this
problem without overly constraining what systems can pro-
duce as an answer. The solution arrived at was to have
two kinds of reference answers for each query: a minimum
answer, which contains the absolute minimum amount of
data that must be included in an answer for it to be correct,
and a maximum answer (that can be automatically derived
from the minimum) containing all the &amp;quot;reasonable&amp;quot; fields
that might be included, but no completely irrelevant ones.
For example, for a question asking about the arrival time of
a flight, the minimum answer would contain the flight ID
and the arrival time. The maximum answer would contain
the airline name and flight number, but not the meal ser-
vice or any fare information. In order to be counted correct,
the answer produced by a system must contain at least the
data in the minimum answer, and no more than the data in
the maximum answer; if additional fields are produced, the
answer is counted as wrong. This successfully reduced the
incentive for systems to overgenerate answers in hope of
getting credit for answering queries that they did not really
understand.
</bodyText>
<subsubsectionHeader confidence="0.631552">
2.23 Comparison Software
</subsubsectionHeader>
<bodyText confidence="0.999978583333334">
Another common resource is software to compare the ref-
erence answers to those produced by various systems. 1
This task is complicated substantially by the fact that the
reference answer is intentionally minimal, but the answer
supplied by a system may contain extra information, and
cannot be assumed to have the columns or rows in the same
order as the reference answer. Some intelligence is there-
fore needed to determine when two answers match: simple
identity tests won&apos;t work.
In the general case, comparing the atomic values in an an-
swer expression just means an identity test. The only excep-
tion is real numbers, for which an epsilon test is performed,
to deal with round-off discrepancies arising from different
hardware precision.2 The number of significant digits that
are required to be the same is a parameter of the comparator.
Answer comparison at the level of tables require more so-
phistication, since column order is ignored, and the answer
may include additional columns that are not in the specifica-
tion. Furthermore, those additional columns can mean that
the answer will include extra whole tuples not present in
the specification. For example, in the ATIS domain, if the
Concorde and Airbus are both aircraft whose type is &amp;quot;JET&amp;quot;,
they would together contribute only one tuple (row) to the
simple list of aircraft types below.
</bodyText>
<sectionHeader confidence="0.473187" genericHeader="method">
( (&amp;quot;JET&amp;quot; )
</sectionHeader>
<bodyText confidence="0.993176857142857">
(&amp;quot;TURBOPROP&amp;quot;)
(&amp;quot;HELICOPTER&amp;quot;)
(&amp;quot;AMPHIBIAN&amp;quot;)
(&amp;quot;PROPELLER&amp;quot; ) )
On the other hand, if aircraft names were included in the
table, they would each appear, producing a larger number of
tuples overall.
</bodyText>
<sectionHeader confidence="0.795722" genericHeader="method">
((&amp;quot;AEROSPATIALE CONCORDE&amp;quot;. &amp;quot;JET&amp;quot;)
(&amp;quot;AIRBUS INDUSTRIE&amp;quot; &amp;quot;JET&amp;quot;)
</sectionHeader>
<bodyText confidence="0.944740611111111">
(&amp;quot;LOCKHEED L188 ELECTRA&amp;quot; &amp;quot;TURBOPROP&amp;quot;)
.)
With answers in the form of tables, the algorithm explores
each possible mapping from the required columns found in
the reference answer (henceforth REF) to the actual columns
found in the answer being evaluated (HYP). (Naturally, there
must be at least as many columns in HYP as in REF, or the
answer is clearly wrong.) For each such mapping, it reduces
HYP according to the mapping, eliminating any duplicate
tuples in the reduced table, and then compares REF against
that reduced table, testing set-equivalence between the two.
Special provision is made for single element answers, Sc
that a scalar REF and a HYP which is a table containing
a single element are judged to be equivalent. That is, a
scalar REF will match either a scalar or a single elemeni
&apos;The first implementation of this software was by Lance
Ramshaw (Boisen et a/., 1989). It has since been re-implementec
and modified by NIST for the ATIS evaluations.
</bodyText>
<footnote confidence="0.94800225">
2For the ATIS evaluations, this identity test has been relaxed
somewhat so that, e.g., strings need not have quotes around their
if they do not contain &amp;quot;white space&amp;quot; characters. See Appendix
for further details.
</footnote>
<page confidence="0.997166">
164
</page>
<bodyText confidence="0.999630636363636">
table for HYP, and a REF which is a single element table
specification will also match either kind of answer.
For the ATIS evaluations, two extensions were made to
this approach. A REF may be ambiguous, containing several
sub expressions each of which is itself a REF: in this case,
if HYP matches any of the answers in REF, the comparison
succeeds. A special answer token (NO_ANSWER) was also
agreed to, so that when a system can detect that it doesn&apos;t
have enough information, it can report that fact rather than
guessing. This is based on the assumption that failing to
answer is less serious than answering incorrectly.
</bodyText>
<subsectionHeader confidence="0.999761">
2.3 Scoring Answers
</subsectionHeader>
<bodyText confidence="0.999957294117647">
Expressing results can be almost as complicated as obtaining
them. Originally it was thought that a simple &amp;quot;X percent
correct&amp;quot; measure would be sufficient, however it became
clear that there was a significant difference between giving
a wrong answer and giving no answer at all, so the results are
now presented as: Number right, Number wrong, Number
not answered, Weighted Error Percentage (weighted so that
wrong answers are twice as bad as no answer at all), and
Score (100 - weighted error).
Whenever numeric measures of understanding are pre-
sented, they should in principle be accompanied by some
measure of the significance and reliability of the metric. Al-
though precise significance tests for this methodology are not
yet known, it is clear that &amp;quot;black box&amp;quot; testing is not a perfect
measure. In particular, it is impossible to tell whether a sys-
tem got a correct answer for the &amp;quot;right&amp;quot; reason, rather than
through chance: this is especially true when the space of
possible answers is small (yes-no questions are an extreme
answer). Since more precise measures are much more costly,
however, the present methodology has been considered ad-
equate for the current state of the art in NL evaluation.
Given that current weighted error rates for the DARPA
ATIS evaluations range from 55%-18%, we can roughly
estimate the confidence interval to be approximately 8%.3
Another source of variation in the scoring metric is the fact
that queries taken from different speakers can vary widely
in terms of how easy it is for systems to understand and
answer them correctly. For example, in the February 1991
ATIS evaluations, the performance of BBN&apos;s Delphi SLS on
text input from individual speakers ranged from 75% to 10%
correct. The word error from speech recognition was also
the highest for those speakers with the highest NL error rates,
suggesting that individual speaker differences can strongly
impact the results.
</bodyText>
<footnote confidence="0.462286">
3Assuming there is some probability of error in each trial
(query), the variance in this error rate can be estimated using the
formula
2Ve(1 e)
</footnote>
<bodyText confidence="0.9982978">
where e is the error rate expressed as a decimal (so 55% error =
0.55), and n is the size of the test set. Taking e = 0.45 (one of the
better scores from the February 91 ATIS evaluation), and n = 145,
differences in scores greater than 0.08 (8%) have a 95% likelihood
of being significant.
</bodyText>
<subsectionHeader confidence="0.996461">
2.4 Evaluation Data
2.4.1 Collecting Data
</subsectionHeader>
<bodyText confidence="0.999690571428572">
The methodology presented above places no a priori re-
strictions on how the data itself should be collected. For
the ATIS evaluations, several different methods of data col-
lection, including a method called &amp;quot;Wizard scenarios&amp;quot;, were
used to collect raw data, both speech and transcribed text
(Hemphill, 1990). This resulted in the collection of a num-
ber of human-machine dialogues. One advantage of this ap-
proach is that it produced both the queries and draft answers
at the same time. It also became clear that the language
obtained is very strongly influenced by the particular task,
the domain and database being used, the amount and form
of data returned to the user, and the type of data collection
methodology used. This is still an area of active research in
the DARPA SLS community.
</bodyText>
<subsectionHeader confidence="0.79821">
2.4.2 Classifying Data
</subsectionHeader>
<bodyText confidence="0.9929615">
Typically, some of the data which is collected is not suit-
able as test data, because:
</bodyText>
<listItem confidence="0.999549833333333">
• the queries fall outside the domain or the database query
application
• the queries require capabilities beyond strict NL under-
standing (for example, very complex inferencing or the
use of large amounts of knowledge outside the domain)
• the queries are overly vague (&amp;quot;Tell me about ...&amp;quot;)
</listItem>
<bodyText confidence="0.999843714285714">
It is also possible that phenomena may arise in test data
which falls outside the agreement on meanings derived from
the training data (the &amp;quot;principles of interpretation&amp;quot;). Such
queries should be excluded from the test corpus, since it is
not possible to make a meaningful comparison on answers
unless there is prior agreement on precisely what the answer
should be.
</bodyText>
<sectionHeader confidence="0.827743" genericHeader="method">
2.43 Discourse Context
</sectionHeader>
<bodyText confidence="0.999678727272727">
The methodology of comparing paired queries and an-
swers assumes the query itself contains all the information
necessary for producing an answer. This is, of course, often
not true in spontaneous goal-directed utterances, since one
query may create a context for another, and the full con-
text is required to answer (e.g., &amp;quot;Show me the flights ...
&amp;quot;Which of THEM ... &amp;quot;). Various means of extending this
methodology for evaluating context-dependent queries have
been proposed, and some of them have been implemented
in the ATIS evaluations (Boisen et al. (1989), Hirschman et
al. (1990), Bates and Ayuso (1991), Pallett (1991)).
</bodyText>
<sectionHeader confidence="0.999109" genericHeader="method">
3 The DARPA SLS Evaluations
</sectionHeader>
<bodyText confidence="0.999967375">
The goal of the DARPA Spoken Language Systems program
is to further research and demonstrate the potential utility of
speech understanding. Currently, at least five major sites
(AT&amp;T, BBN, CMU, MIT, and SRI) are developing com-
plete SLS systems, and another site (Paramax) is integrating
its NL component with other speech systems. Representa-
tives from these and other organizations meet regularly to
discuss program goals and to evaluate progress.
</bodyText>
<page confidence="0.994804">
165
</page>
<bodyText confidence="0.999940730769231">
This DARPA SLS community formed a committee on
evaluation4, chaired by David Pallett of the National Insti-
tute of Standards and Technology (NIST). The committee
was to develop a methodology for data collection, training
data dissemination, and testing for SLS systems under de-
velopment. The first community-wide evaluation using the
first version of this methodology took place in June, 1990,
with subsequent evaluations in February 1991 and February
1992.
The emphasis of the committee&apos;s work has been on au-
tomatic evaluation of queries to an air travel information
system (ATIS). Air travel was chosen as an application that
is easy for everyone to understand. The methodology pre-
sented here was originally developed in the context of the
need for SLS evaluation, and has been extended in important
ways by the community based on the practical experience
of doing evaluations.
As a result of the ATIS evaluations, a body of resources
has now been compiled and is available through NIST. This
includes the ATIS relational database, a corpus of paired
queries and answers, protocols for data collection, soft-
ware for automatic comparison of answers, the &amp;quot;Principles
of Interpretation&amp;quot; specifying domain-specific meanings of
queries, and the CAS format (Appendix A is the current
version). Interested parties should contact David Pallet of
NIST for more information.5
</bodyText>
<sectionHeader confidence="0.994336" genericHeader="method">
4 Advantages and Limitations of the
Methodology
</sectionHeader>
<bodyText confidence="0.896869">
Several benefits come from the use of this methodology:
</bodyText>
<listItem confidence="0.999916727272727">
• It forces advance agreement on the meaning of critical
terms and on some information to be included in the
answer.
• It is objective, to the extent that a method for selecting
testable queries can be defined, and to the extent that
the agreements mentioned above can be reached.
• It requires less human effort (primarily in the creating of
canonical examples and answers) than non-automatic,
more subjective evaluation. It is thus better suited to
large test sets.
• It can be easily extended.
</listItem>
<bodyText confidence="0.999553428571429">
Most of the weaknesses of this methodology arise from the
fact that the answers produced by a database query system
are only an approximation of its understanding capabilities.
As with any black-box approach, it may give undue credit
to a system that gets the right answer for the wrong reason
(i.e., without really understanding the query), although this
should be mitigated by using larger and more varied test
</bodyText>
<footnote confidence="0.9460048">
4The primary members of the original committee are: Lyn
Bates (BBN), Debbie Dahl (UNISYS), Bill Fisher (NIST), Lynette
Hirschman (Mn&apos;), Bob Moore (SRI), and Rich Stern (CMU). Suc-
cessor committees have also included Jared Bernstein (SRI), Kate
Hunike-Smith (SRI), Patti Price (SRI), Alex Rudnicicy (CMU), and
Jay Wilpon (AT&amp;T). Many other people have contributed to the
work of these committees and their subcommittees.
5David Pallet may be contacted at the National Institute of
Standards and Technology, Technology Building, Room A216,
Gaithersburg, MD 20899, (301)975-2944.
</footnote>
<bodyText confidence="0.9985812">
corpora. It does not distinguish between merely acceptable
answers and very good answers.
Another limitation of this approach is that it does not
adequately measure the handling of some phenomena, such
as extended dialogues.
</bodyText>
<sectionHeader confidence="0.982309" genericHeader="method">
5 Other Evaluation Methodologies
</sectionHeader>
<bodyText confidence="0.9999788">
This approach to evaluation shares many characteristics
with the methods used for the DARPA-sponsored Message
Understanding Conferences (Sundheim, 1989; Sundheim,
1991). In particular, both approaches are focused on exter-
nal (black-box) evaluation of the understanding capabilities
of systems using input/output pairs, and there are many sim-
ilar problems in precisely specifying how NL systems are to
satisfy the application task.
Despite these similarities, this methodology probably
comes closer to evaluating the actual understanding capa-
bilities of NL systems. One reason is that the constraints
on both input and output are more rigorous. For database
query tasks, virtually every word must be correctly under-
stood to produce a correct answer: by contrast, much of
the MUC-3 texts is irrelevant to the application task. Since
this methodology focuses on single queries (perhaps with
additional context), a smaller amount of language is being
examined in each individual comparison.
Similarly, for database query, the database itself implicitly
constrains the space of possible answers, and each answer
is scored as either correct or incorrect. This differs from
the MUC evaluations, where an answer template is a com-
posite of many bits of information, and is scored along the
dimensions of recall, precision, and overgeneration.
Rome Laboratory has also sponsored a recent effort to
define another approach to evaluating NL systems (Neal et
a/., 1991; Walter, 1992). This methodology is focussed on
human evaluation of interactive systems, and is a &amp;quot;glass-
box&amp;quot; method which looks at the performance of the linguistic
components of the system under review.
</bodyText>
<sectionHeader confidence="0.998051" genericHeader="method">
6 Future Issues
</sectionHeader>
<bodyText confidence="0.999969166666666">
The hottest topic currently facing the SLS community with
respect to evaluation is what to do about dialogues. Many
of the natural tasks one might do with a database interface
involve extended problem-solving dialogues, but no method-
ology exists for evaluating the capabilities of systems at-
tempting to engage in dialogues with users.
</bodyText>
<sectionHeader confidence="0.8323355" genericHeader="method">
A Common Answer Specification (CAS) for
the ATIS Application
</sectionHeader>
<bodyText confidence="0.9998861">
(Note: this is the official CAS specification for the DARPA
ATIS evaluations, as distributed by NIST. It is domain in-
dependent, but not necessarily complete: for example, it
assumes that the units of any database value are unambigu-
ously determined by the database specification. This would
not be sufficient for applications that allowed unit conver-
sion, e.g. &amp;quot;Show me the weight of ...&amp;quot; where the weight
could be expressed in tons, metric tons, pounds, etc. This
sort of extension should not affect the ease of automatically
comparing answer expressions, however.)
</bodyText>
<page confidence="0.994406">
166
</page>
<figure confidence="0.937494">
Basic Syntax in BNF
answer casl I (casi OR answer)
casl scalar-value I relation 1 NO_ANSWER
I no_answer
scalar-value boolean-value I number-value
string
boolean-value YES 1 yes I TRUE true I NO
I no I FALSE I false
number-value integer I real-number
integer [sign] digit+
sign
digit --4011.121314151601
8 1 9
real-number sign digit+ . digit* I digit-i- . digit*
string char_except_whitespace+ I &amp;quot; char* &amp;quot;
relation --+ ( tuple* )
tuple ( value+ )
value ----■ scalar-value I NIL
</figure>
<bodyText confidence="0.999556625">
Standard BNF notation has been extended to include two
other common devices : &amp;quot;A+&amp;quot; means &amp;quot;one or more A&apos;s&amp;quot;
and &amp;quot;A*&amp;quot; means &amp;quot;zero or more A&apos;s&amp;quot;.
The formulation given above does not define
char_except_whitespace and char. All of the standard ASCII
characters count as members of char, and all but &amp;quot;white
space&amp;quot; are counted as char_except_whitespace. Following
ANSI &amp;quot;C&amp;quot;, blanks, horizontal and vertical tabs, newlines,
formfeeds, and comments are, collectively, &amp;quot;white space&amp;quot;.
The only change in the syntax of CAS itself from the
previous version is that now a string may be represented as
either a sequence of characters not containing white space
or as a sequence of any characters enclosed in quotation
marks. Note that only non-exponential real numbers are
allowed, and that empty tuples are not allowed (but empty
relations are).
</bodyText>
<subsectionHeader confidence="0.982082">
Additional Syntactic Constraints
</subsectionHeader>
<bodyText confidence="0.9612390625">
The syntactic classes boolean-value, string, and number-
value define the types &amp;quot;boolean&amp;quot;, &amp;quot;string&amp;quot;, and &amp;quot;number&amp;quot;,
respectively. All the tuples in a relation must have the same
number of values, and those values must be of the same
respective types (boolean, string, or number).
If a token could represent either a string or a number, it
will be taken to be a number; if it could represent either a
string or a boolean, it will be taken to be a boolean. Inter-
pretation as a string may be forced by enclosing a token in
quotation marks.
In a tuple, NIL as the representation of missing data is
allowed as a special case for any value, so a legal answer
indicating the costs of ground transportation in Boston would
be
((&amp;quot;L&amp;quot; 5.00) (&amp;quot;R&amp;quot; nil)
(&amp;quot;A.&amp;quot; nil) (&amp;quot;R&amp;quot; nil))
</bodyText>
<subsectionHeader confidence="0.567283">
Elementary Rules for CAS Comparisons
</subsectionHeader>
<bodyText confidence="0.999707571428572">
String comparison is case-sensitive, but the distinguished
values (YES, NO, TRUE, FALSE, NO_ANSWER, and NIL)
may be written in either upper or lower case.
Each indexical position for a value in a tuple (say, the ith)
is assumed to represent the same field or variable in all the
tuples in a given relation.
Answer relations must be derived from the existing re-
lations in the database, either by subsetting and combining
relations or by operations like averaging, summation, etc.
In matching an hypothesized (HYP) CAS form with a ref-
erence (REF) one, the order of values in the tuples is not
important; nor is the order of tuples in a relation, nor the
order of alternatives in a CAS form using OR. The scoring
algorithm will use the re-ordering that maximizes the indi-
cated score. Extra values in a tuple are not counted as errors,
but distinct extra tuples in a relation are. A tuple is not dis-
tinct if its values for the fields specified by the REF CAS
are the same as another tuple in the relation; these duplicate
tuples are ignored. CAS forms that include alternate CAS&apos;s
connected with OR are intended to allow a single HYP form
to match any one of several REF CAS forms. If the HYP
CAS form contains alternates, the score is undefined.
In comparing two real number values, a tolerance will
be allowed; the default is ±.01%. No tolerance is allowed
in the comparison of integers. In comparing two strings,
initial and final sub-strings of white space are ignored. In
comparing boolean values, TRUE and YES are equivalent,
as are FALSE and NO.
</bodyText>
<sectionHeader confidence="0.795618" genericHeader="method">
B Some Examples from the Principles of
Interpretation Document for the ATIS
Application
</sectionHeader>
<bodyText confidence="0.999784428571428">
(Note: these are excerpted from the official Principles of In-
terpretation document dated 11/20/91. The entire document
is comprised of about 60 different points, and is available
from David Pallet at NIST.
The term &amp;quot;annotator&amp;quot; below refers to a human prepar-
ing training or test data by reviewing reference answers to
queries.)
</bodyText>
<sectionHeader confidence="0.9958965" genericHeader="method">
INTERPETING ATIS QUERIES RE THE DATABASE
1 General Principles:
</sectionHeader>
<subsectionHeader confidence="0.988425">
1.1 Only reasonable interpretations will be used.
</subsectionHeader>
<bodyText confidence="0.956215928571429">
An annotator or judge must decide if a linguistically
possible interpretation is reasonable or not.
1.2 The context will be used in deciding if an interpretation
is reasonable.
1.3 Each interpretation must be expressible as one SQL
statement.
At present (11/18/91) a few specified exceptions to this
principle are allowed, such as allowing boolean answers
for yes/no questions.
1.4 All interpretations meeting the above rules will be used
by the annotators to generate possible reference an-
swers.
A query is thus ambiguous iff it has two interpretations
that are fairly represented by distinct SQL expressions.
</bodyText>
<page confidence="0.946047">
167
</page>
<bodyText confidence="0.990872388888889">
flight_id, stop_number
The reference SQL expression stands as a semantic rep-
resentation or logical form. If a query has two inter-
pretations that result in the same SQL, it will not be
considered ambiguous. The fact that the two distinct
SQL expressions may yield the same answer given the
database is immaterial.
The annotators must be aware of the usual sources of
ambiguity, such as structural ambiguity, exemplified by
cases like &amp;quot;the prices of flights, first class, from X to
Y&amp;quot;, in which the attachment of a modifier that can ap-
ply to either prices or flights is unclear. (This should
be (ambiguously) interpreted both ways, as both &amp;quot;the
first-class prices on flights from X to Y&amp;quot; and &amp;quot;the prices
on first-class flights from X to Y&amp;quot;.) More generally, if
structural ambiguities like this could result in different
(SQL) interpretations, they must be treated as ambigu-
ous.
</bodyText>
<sectionHeader confidence="0.511686" genericHeader="method">
2 Specific Principles:
</sectionHeader>
<bodyText confidence="0.9996362">
In this arena, certain English expressions have special
meanings, particularly in terms of the database distributed
by TI in the spring of 1990 and revised in November 1990
and May 1991. Here are the ones we have agreed on: (In
the following, &amp;quot;A.B&amp;quot; refers to field B of table A.)
</bodyText>
<sectionHeader confidence="0.808147" genericHeader="conclusions">
2.1 Requests for enumeration.
</sectionHeader>
<bodyText confidence="0.9997306">
A large class of tables in the database have entries that
can be taken as defining things that can be asked for
in a query. In the answer, each of these things will be
identified by giving a value of the primary key of its
table. These tables are:
</bodyText>
<tableCaption confidence="0.522764">
Table Name English Term(s) Primary Key
</tableCaption>
<table confidence="0.291285666666667">
aircraft aircraft, equipment aircraft_code
airline airline airline_code
airport airport airport_code
flight_stop (intermed.) stops
high flight_number
2.2 Flights.
2.2.1 A flight &amp;quot;between X and Y&amp;quot; means a flight &amp;quot;from
X to Y&amp;quot;.
2.2.3 A request for a flight&apos;s stops will be interpreted
</table>
<bodyText confidence="0.6641195">
as asking for the intermediate stops only, from the
flight_stop table.
</bodyText>
<footnote confidence="0.962160454545455">
2.3 Fares.
2.3.1 A &amp;quot;one-way&amp;quot; fare is a fare for which
round_trip_required = &amp;quot;NO&amp;quot;.
2.3.2 A &amp;quot;round-trip&amp;quot; fare is a fare with a non-null value
for faresound_trip_cost.
2.3.3 The &amp;quot;cheapest fare&amp;quot; means the lowest one-
direction fare.
•
2.3.8 Questions about fares will always be treated as
fares for flights in the maximal answer.
2.4 Times.
</footnote>
<note confidence="0.482734375">
2.4.1 The normal answer to otherwise unmodified
&amp;quot;when&amp;quot; queries will be a time of day, not a date
or a duration.
2.4.2 The answer to queries like &amp;quot;On what days does
flight X fly&amp;quot; will be a list of days.day_name fields.
2.4.3 Queries that refer to a time earlier than 1300 hours
without specifying &amp;quot;a.m.&amp;quot; or &amp;quot;p.m.&amp;quot; are ambigu-
ous and may be interpreted as either.
</note>
<subsubsectionHeader confidence="0.795727">
2.4.4 Periods of the day.
</subsubsectionHeader>
<bodyText confidence="0.9874236">
The following table gives precise interpretations
for some vague terms referring to time periods.
The time intervals given do not include the end
points. Items flagged with &amp;quot;*&amp;quot; are in the current
(rdb3.3) database interval table.
</bodyText>
<figure confidence="0.963017714285714">
PERIOD BEGIN TIME END TIME
morning* 0000 1200
afternoon* 1200 1800
evening* 1800 2200
day* 600 1800
night* 1800 600
early morning* 0000 800
</figure>
<subsectionHeader confidence="0.947603">
2.9 Meaning requests.
</subsectionHeader>
<bodyText confidence="0.994159">
2.9.1 With the particular exceptions noted below, re-
quests for the &amp;quot;meaning&amp;quot; of something will only be
interpretable if that thing is a code with a canned
decoding definition in the database. In case the
code field is not the key field of the table, infor-
mation should be returned for all tuples that match
on the code field. Here are the things so defined,
with the fields containing their decoding:
</bodyText>
<tableCaption confidence="0.629878">
Table Code Field Decoding Field
</tableCaption>
<reference confidence="0.670418166666667">
aircraft aircraft_code aircraft_description
airline airline_code airline_name
airport airport_code airport_name
city city_code city_name
class_of_service booking_class class_description
code_description code description
</reference>
<bodyText confidence="0.983267">
2.11 Queries that are literally yes-or-no questions are con-
sidered to be ambiguous between interpretation as a
yes-or-no question and interpretation as the correspond-
ing wh-question. For example, &amp;quot;Are there flights from
Boston to Philly?&amp;quot; may be answered by either a
boolean value (&amp;quot;YES/TRUE/NO/FALSE&amp;quot;) or a table of
flights from Boston to Philadelphia.
2.15 When a query refers to an aircraft type such as &amp;quot;BOE-
ING 767&amp;quot;, the manufacturer (if one is given) must
match the aircraft.manufacturer field and the type may
be matched against either the aircraft.code field or the
aircraft.basic_type field, ambiguously.
2.16 Utterances whose answers require arithmetic computa-
tion are not now considered to be interpretable; this
does not apply to arithmetic comparisons, including
computing the MaXiMUM or minimum value of a field,
or counting elements of a set of tuples.
</bodyText>
<page confidence="0.997654">
168
</page>
<sectionHeader confidence="0.995828" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999678571428571">
B. Ballard. A Methodology for Evaluating Near-Prototype
NL Processors. Technical Report OSU—CISRC—TR-81-4,
Ohio State University, 1981.
M. Bates and D. Ayuso. A proposal for incremental dia-
logue evaluation. In Proceedings of the Speech and Natural
Language Workshop, San Mateo, California, February 1991.
DARPA, Morgan Kaufmann Publishers, Inc.
M. Bates and M. Rettig. How to choose NL software. AI
Expert, July 1988.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman,
P. Harrison, D. Hindle, B. Ingria, F. Jelinek, J. Klavens,
M. Liberman, M. Marcus, S. Roukos, B. Santorini, and
T. Strzalkowski. A procedure for quantitatively comparing
the syntactic coverage of English grammars. In Proceedings
of the Speech and Natural Language Workshop, San Ma-
teo, California, February 1991. DARPA, Morgan Kaufmann
Publishers, Inc.
S. Boisen, L. Rainshaw, D. Ayuso, and M. Bates. A proposal
for SLS evaluation. In Proceedings of the Speech and Nat-
ural Language Workshop, San Mateo, California, October
1989. DARPA, Morgan Kaufmann Publishers, Inc.
DARPA. Proceedings of the Speech and Natural Language
Workshop, San Mateo, California, June 1990. Morgan Kauf-
mann Publishers, Inc.
DARPA. Proceedings of the Speech and Natural Language
Workshop, San Mateo, California, February 1991. Morgan
Kaufmann Publishers, Inc.
DARPA. Proceedings of the Third Message Understand-
ing Conference (MUC-3), San Mateo, California, May 1991.
Morgan Kaufmann Publishers, Inc.
DARPA. Proceedings of the Speech and Natural Language
Workshop, San Mateo, California, February 1992. Morgan
Kaufmann Publishers, Inc.
C. Hemphill. TI implementation of corpus collection. In
Proceedings of the Speech and Natural Language Workshop,
San Mateo, California, June 1990. DARPA, Morgan Kauf-
mann Publishers, Inc.
L. Hirschman, D. Dahl, D. McKay, L. Norton, and
M. Linebarger. A proposal for automatic evaluation of dis-
course. In Proceedings of the Speech and Natural Language
Workshop, San Mateo, California, June 1990. DARPA, Mor-
gan Kaufmann Publishers, Inc.
J. Neal, T. Finin, R. Grishman, C. Montgomery, and S. Wal-
ter. Workshop on the Evaluation of Natural Language Pro-
cessing Systems. Technical Report (to appear), RADC, June
1991.
D. S. Pallett. DARPA Resource Management and ATIS
benchmark test poster session. In Proceedings of the Speech
and Natural Language Workshop, San Mateo, California,
February 1991. DARPA, Morgan Kaufmann Publishers, Inc.
M. Palmer, T. Finin, and S. Walter. Workshop on the Eval-
uation of Natural Language Processing Systems. Technical
Report RADC-TR-89-302, RADC, 1989.
B. M. Sundheim. Plans for a task-oriented evaluation of
natural language understanding systems. In Proceedings of
the Speech and Natural Language Workshop, pages 197-202,
Philadelphia, PA, Februrary 1989.
B. M. Sundheim. Overview of the Third Message Under-
standing Evaluation and Conference. In Proceedings of the
Third Message Understanding Conference (MUC-3), pages
3-16, San Mateo, California, May 1991. DARPA, Morgan
Kaufmann Publishers, Inc.
H. Tennant. Evaluation of Natural Language Processors.
PhD thesis, University of Illinois, 1981.
S. Walter. Neal-Montgomery NLP system evaluation
methodology. In Proceedings of the Speech and Natural
Language Workshop, San Mateo, California, February 1992.
DARPA, Morgan Kaufmann Publishers, Inc.
R. M. Weischedel. Issues and Red Herrings in Evaluating
Natural Language Interfaces. Pergamnon Press, 1986.
</reference>
<page confidence="0.998822">
169
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.497742">
<title confidence="0.9993815">A Practical Methodology for the Evaluation of Spoken Language Systems</title>
<author confidence="0.999558">Sean Boisen</author>
<author confidence="0.999558">Madeleine Bates</author>
<affiliation confidence="0.501257">Bolt Beranek and Newman, Inc. (BBN)</affiliation>
<address confidence="0.997153">10 Moulton Street, Cambridge MA 02138 USA</address>
<email confidence="0.994805">sboisen@bbn.com,bates@bbn.com</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Ballard</author>
</authors>
<title>aircraft aircraft_code aircraft_description airline airline_code airline_name airport airport_code airport_name city city_code city_name class_of_service booking_class class_description code_description code description</title>
<date>1981</date>
<tech>Technical Report OSU—CISRC—TR-81-4,</tech>
<institution>Ohio State University,</institution>
<marker>Ballard, 1981</marker>
<rawString>aircraft aircraft_code aircraft_description airline airline_code airline_name airport airport_code airport_name city city_code city_name class_of_service booking_class class_description code_description code description B. Ballard. A Methodology for Evaluating Near-Prototype NL Processors. Technical Report OSU—CISRC—TR-81-4, Ohio State University, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bates</author>
<author>D Ayuso</author>
</authors>
<title>A proposal for incremental dialogue evaluation.</title>
<date>1991</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<publisher>DARPA, Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo, California,</location>
<contexts>
<context position="19137" citStr="Bates and Ayuso (1991)" startWordPosition="3122" endWordPosition="3125">course Context The methodology of comparing paired queries and answers assumes the query itself contains all the information necessary for producing an answer. This is, of course, often not true in spontaneous goal-directed utterances, since one query may create a context for another, and the full context is required to answer (e.g., &amp;quot;Show me the flights ... &amp;quot;Which of THEM ... &amp;quot;). Various means of extending this methodology for evaluating context-dependent queries have been proposed, and some of them have been implemented in the ATIS evaluations (Boisen et al. (1989), Hirschman et al. (1990), Bates and Ayuso (1991), Pallett (1991)). 3 The DARPA SLS Evaluations The goal of the DARPA Spoken Language Systems program is to further research and demonstrate the potential utility of speech understanding. Currently, at least five major sites (AT&amp;T, BBN, CMU, MIT, and SRI) are developing complete SLS systems, and another site (Paramax) is integrating its NL component with other speech systems. Representatives from these and other organizations meet regularly to discuss program goals and to evaluate progress. 165 This DARPA SLS community formed a committee on evaluation4, chaired by David Pallett of the National </context>
</contexts>
<marker>Bates, Ayuso, 1991</marker>
<rawString>M. Bates and D. Ayuso. A proposal for incremental dialogue evaluation. In Proceedings of the Speech and Natural Language Workshop, San Mateo, California, February 1991. DARPA, Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bates</author>
<author>M Rettig</author>
</authors>
<title>How to choose NL software. AI Expert,</title>
<date>1988</date>
<marker>Bates, Rettig, 1988</marker>
<rawString>M. Bates and M. Rettig. How to choose NL software. AI Expert, July 1988.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickenger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>B Ingria</author>
<author>F Jelinek</author>
<author>J Klavens</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<publisher>DARPA, Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo, California,</location>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavens, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, B. Ingria, F. Jelinek, J. Klavens, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proceedings of the Speech and Natural Language Workshop, San Mateo, California, February 1991. DARPA, Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Boisen</author>
<author>L Rainshaw</author>
<author>D Ayuso</author>
<author>M Bates</author>
</authors>
<title>A proposal for SLS evaluation.</title>
<date>1989</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<publisher>DARPA, Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo, California,</location>
<contexts>
<context position="9365" citStr="Boisen et al., 1989" startWordPosition="1494" endWordPosition="1497"> conflicts with the principle that evaluation should be as non-intrusive as possible. Furthermore, it is not strictly necessary: what matters most is not whether a system provided exactly the same data as some reference answer, but whether the correct answer is clearly among the data provided (as long as no incorrect data was returned). For the sake of automatic evaluation, then, a canonical reference answer (the minimum &amp;quot;right answer&amp;quot;) is developed for each evaluable query in the training set. The content of this reference answer is determined both by domainindependent linguistic principles (Boisen et al., 1989) and domain-specific stipulation. The language used to express the answers for the ATIS domain is presented in Appendix A. Evaluation using the minimal answer alone makes it possible to exploit the fact that extra fields in an answer are not penalized. For example, the answer ((&amp;quot;AA&amp;quot; 152 0920 1015 &amp;quot;BOS&amp;quot; &amp;quot;CHI&amp;quot; &amp;quot;SNACK&amp;quot; ) ) could be produced for any of the following queries: • &amp;quot;When does American Airlines flight 152 leave?&amp;quot; • &amp;quot;What&apos;s the earliest flight from Boston to Chicago?&amp;quot; • &amp;quot;Does the 9:20 flight to Chicago serve meals?&amp;quot; and would be counted correct. For the ATIS evaluations, it was necessary</context>
<context position="19088" citStr="Boisen et al. (1989)" startWordPosition="3114" endWordPosition="3117">n precisely what the answer should be. 2.43 Discourse Context The methodology of comparing paired queries and answers assumes the query itself contains all the information necessary for producing an answer. This is, of course, often not true in spontaneous goal-directed utterances, since one query may create a context for another, and the full context is required to answer (e.g., &amp;quot;Show me the flights ... &amp;quot;Which of THEM ... &amp;quot;). Various means of extending this methodology for evaluating context-dependent queries have been proposed, and some of them have been implemented in the ATIS evaluations (Boisen et al. (1989), Hirschman et al. (1990), Bates and Ayuso (1991), Pallett (1991)). 3 The DARPA SLS Evaluations The goal of the DARPA Spoken Language Systems program is to further research and demonstrate the potential utility of speech understanding. Currently, at least five major sites (AT&amp;T, BBN, CMU, MIT, and SRI) are developing complete SLS systems, and another site (Paramax) is integrating its NL component with other speech systems. Representatives from these and other organizations meet regularly to discuss program goals and to evaluate progress. 165 This DARPA SLS community formed a committee on evalu</context>
</contexts>
<marker>Boisen, Rainshaw, Ayuso, Bates, 1989</marker>
<rawString>S. Boisen, L. Rainshaw, D. Ayuso, and M. Bates. A proposal for SLS evaluation. In Proceedings of the Speech and Natural Language Workshop, San Mateo, California, October 1989. DARPA, Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<date>1990</date>
<booktitle>Proceedings of the Speech and Natural Language Workshop,</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo, California,</location>
<marker>DARPA, 1990</marker>
<rawString>DARPA. Proceedings of the Speech and Natural Language Workshop, San Mateo, California, June 1990. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<date>1991</date>
<booktitle>Proceedings of the Speech and Natural Language Workshop,</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo, California,</location>
<marker>DARPA, 1991</marker>
<rawString>DARPA. Proceedings of the Speech and Natural Language Workshop, San Mateo, California, February 1991. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<date>1991</date>
<booktitle>Proceedings of the Third Message Understanding Conference (MUC-3),</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo, California,</location>
<marker>DARPA, 1991</marker>
<rawString>DARPA. Proceedings of the Third Message Understanding Conference (MUC-3), San Mateo, California, May 1991. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<date>1992</date>
<booktitle>Proceedings of the Speech and Natural Language Workshop,</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo, California,</location>
<marker>DARPA, 1992</marker>
<rawString>DARPA. Proceedings of the Speech and Natural Language Workshop, San Mateo, California, February 1992. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hemphill</author>
</authors>
<title>TI implementation of corpus collection.</title>
<date>1990</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<publisher>DARPA, Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo, California,</location>
<contexts>
<context position="17257" citStr="Hemphill, 1990" startWordPosition="2811" endWordPosition="2812">te expressed as a decimal (so 55% error = 0.55), and n is the size of the test set. Taking e = 0.45 (one of the better scores from the February 91 ATIS evaluation), and n = 145, differences in scores greater than 0.08 (8%) have a 95% likelihood of being significant. 2.4 Evaluation Data 2.4.1 Collecting Data The methodology presented above places no a priori restrictions on how the data itself should be collected. For the ATIS evaluations, several different methods of data collection, including a method called &amp;quot;Wizard scenarios&amp;quot;, were used to collect raw data, both speech and transcribed text (Hemphill, 1990). This resulted in the collection of a number of human-machine dialogues. One advantage of this approach is that it produced both the queries and draft answers at the same time. It also became clear that the language obtained is very strongly influenced by the particular task, the domain and database being used, the amount and form of data returned to the user, and the type of data collection methodology used. This is still an area of active research in the DARPA SLS community. 2.4.2 Classifying Data Typically, some of the data which is collected is not suitable as test data, because: • the qu</context>
</contexts>
<marker>Hemphill, 1990</marker>
<rawString>C. Hemphill. TI implementation of corpus collection. In Proceedings of the Speech and Natural Language Workshop, San Mateo, California, June 1990. DARPA, Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>D Dahl</author>
<author>D McKay</author>
<author>L Norton</author>
<author>M Linebarger</author>
</authors>
<title>A proposal for automatic evaluation of discourse.</title>
<date>1990</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<publisher>DARPA, Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo, California,</location>
<contexts>
<context position="19113" citStr="Hirschman et al. (1990)" startWordPosition="3118" endWordPosition="3121">nswer should be. 2.43 Discourse Context The methodology of comparing paired queries and answers assumes the query itself contains all the information necessary for producing an answer. This is, of course, often not true in spontaneous goal-directed utterances, since one query may create a context for another, and the full context is required to answer (e.g., &amp;quot;Show me the flights ... &amp;quot;Which of THEM ... &amp;quot;). Various means of extending this methodology for evaluating context-dependent queries have been proposed, and some of them have been implemented in the ATIS evaluations (Boisen et al. (1989), Hirschman et al. (1990), Bates and Ayuso (1991), Pallett (1991)). 3 The DARPA SLS Evaluations The goal of the DARPA Spoken Language Systems program is to further research and demonstrate the potential utility of speech understanding. Currently, at least five major sites (AT&amp;T, BBN, CMU, MIT, and SRI) are developing complete SLS systems, and another site (Paramax) is integrating its NL component with other speech systems. Representatives from these and other organizations meet regularly to discuss program goals and to evaluate progress. 165 This DARPA SLS community formed a committee on evaluation4, chaired by David </context>
</contexts>
<marker>Hirschman, Dahl, McKay, Norton, Linebarger, 1990</marker>
<rawString>L. Hirschman, D. Dahl, D. McKay, L. Norton, and M. Linebarger. A proposal for automatic evaluation of discourse. In Proceedings of the Speech and Natural Language Workshop, San Mateo, California, June 1990. DARPA, Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Neal</author>
<author>T Finin</author>
<author>R Grishman</author>
<author>C Montgomery</author>
<author>S Walter</author>
</authors>
<title>Workshop on the Evaluation of Natural Language Processing Systems.</title>
<date>1991</date>
<tech>Technical Report (to appear), RADC,</tech>
<contexts>
<context position="1453" citStr="Neal et al., 1991" startWordPosition="214" endWordPosition="217">been made by a number of groups involved in the DARPA Spoken Language Systems (SLS) program to agree on a methodology for comparative evaluation of SLS systems, and that methodology has been put into practice several times in comparative tests of several SLS systems. These evaluations are probably the only NI, evaluations other than the series of Message Understanding Conferences (Sundheim, 1989; Sundheim, 1991) to have been developed and used by a group of researchers at different sites, although several excellent workshops have been held to study some of these problems (Palmer et al., 1989; Neal et al., 1991). This paper describes a practical &amp;quot;black-box&amp;quot; methodology for automatic evaluation of question-answering NL systems. While each new application domain will require some development of special resources, the heart of the methodology is domain-independent, and it can be used with either speech or text input. The particular characteristics of the approach are described in the following section: subsequent sections present its implementation in the DARPA SLS community, and some problems and directions for future development. 2 The Evaluation Framework 2.1 Characteristics of the Methodology The go</context>
<context position="4461" citStr="Neal et al. (1991)" startWordPosition="688" endWordPosition="691"> I is easy, however, to define a simple common language foi representing answers (see Appendix A), and easy to mar system-specific representations into this common language. This methodology has been successfully applied in th( context of cross-site blind tests, where the evaluation i. based on input which the system has never seen before This type of evaluation leaves out many other important as pects of a system, such as the user interface, or the utilit] (or speed) of performing a particular task with a system tha includes a NL component (work by Tennant (1981), Bate and Rettig (1988), and Neal et al. (1991) addresses some o these other factors). Examples below will be taken from the current DARRi SLS application, the Airline Travel Information Systen (ATIS). This is a database of flights with information o the aircraft, stops and connections, meals, etc. 162 T77:77.77777:77T:77&amp;quot; Score (Right, Wrong, No Answer) SLS Answer Text text NL &amp;quot;meaning&amp;quot; Application Interface DB Commands Answer Preparation Data Figure 1: The evaluation process 2.2 Evaluation Architecture and Common Resources We assume an evaluation architecture like that in Figure 1. The shaded components are common resources of the evalua</context>
</contexts>
<marker>Neal, Finin, Grishman, Montgomery, Walter, 1991</marker>
<rawString>J. Neal, T. Finin, R. Grishman, C. Montgomery, and S. Walter. Workshop on the Evaluation of Natural Language Processing Systems. Technical Report (to appear), RADC, June 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Pallett</author>
</authors>
<title>DARPA Resource Management and ATIS benchmark test poster session.</title>
<date>1991</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<tech>Technical Report RADC-TR-89-302, RADC,</tech>
<publisher>DARPA, Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo, California,</location>
<contexts>
<context position="19153" citStr="Pallett (1991)" startWordPosition="3126" endWordPosition="3127">dology of comparing paired queries and answers assumes the query itself contains all the information necessary for producing an answer. This is, of course, often not true in spontaneous goal-directed utterances, since one query may create a context for another, and the full context is required to answer (e.g., &amp;quot;Show me the flights ... &amp;quot;Which of THEM ... &amp;quot;). Various means of extending this methodology for evaluating context-dependent queries have been proposed, and some of them have been implemented in the ATIS evaluations (Boisen et al. (1989), Hirschman et al. (1990), Bates and Ayuso (1991), Pallett (1991)). 3 The DARPA SLS Evaluations The goal of the DARPA Spoken Language Systems program is to further research and demonstrate the potential utility of speech understanding. Currently, at least five major sites (AT&amp;T, BBN, CMU, MIT, and SRI) are developing complete SLS systems, and another site (Paramax) is integrating its NL component with other speech systems. Representatives from these and other organizations meet regularly to discuss program goals and to evaluate progress. 165 This DARPA SLS community formed a committee on evaluation4, chaired by David Pallett of the National Institute of Sta</context>
</contexts>
<marker>Pallett, 1991</marker>
<rawString>D. S. Pallett. DARPA Resource Management and ATIS benchmark test poster session. In Proceedings of the Speech and Natural Language Workshop, San Mateo, California, February 1991. DARPA, Morgan Kaufmann Publishers, Inc. M. Palmer, T. Finin, and S. Walter. Workshop on the Evaluation of Natural Language Processing Systems. Technical Report RADC-TR-89-302, RADC, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B M Sundheim</author>
</authors>
<title>Plans for a task-oriented evaluation of natural language understanding systems.</title>
<date>1989</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<pages>197--202</pages>
<location>Philadelphia, PA, Februrary</location>
<contexts>
<context position="1233" citStr="Sundheim, 1989" startWordPosition="178" endWordPosition="179">tly no methodology existed for either developers of natural language (NL) interfaces or researchers in speech understanding (SU) to evaluate and compare the systems they developed. Recently considerable progress has been made by a number of groups involved in the DARPA Spoken Language Systems (SLS) program to agree on a methodology for comparative evaluation of SLS systems, and that methodology has been put into practice several times in comparative tests of several SLS systems. These evaluations are probably the only NI, evaluations other than the series of Message Understanding Conferences (Sundheim, 1989; Sundheim, 1991) to have been developed and used by a group of researchers at different sites, although several excellent workshops have been held to study some of these problems (Palmer et al., 1989; Neal et al., 1991). This paper describes a practical &amp;quot;black-box&amp;quot; methodology for automatic evaluation of question-answering NL systems. While each new application domain will require some development of special resources, the heart of the methodology is domain-independent, and it can be used with either speech or text input. The particular characteristics of the approach are described in the fol</context>
<context position="23002" citStr="Sundheim, 1989" startWordPosition="3724" endWordPosition="3725">rk of these committees and their subcommittees. 5David Pallet may be contacted at the National Institute of Standards and Technology, Technology Building, Room A216, Gaithersburg, MD 20899, (301)975-2944. corpora. It does not distinguish between merely acceptable answers and very good answers. Another limitation of this approach is that it does not adequately measure the handling of some phenomena, such as extended dialogues. 5 Other Evaluation Methodologies This approach to evaluation shares many characteristics with the methods used for the DARPA-sponsored Message Understanding Conferences (Sundheim, 1989; Sundheim, 1991). In particular, both approaches are focused on external (black-box) evaluation of the understanding capabilities of systems using input/output pairs, and there are many similar problems in precisely specifying how NL systems are to satisfy the application task. Despite these similarities, this methodology probably comes closer to evaluating the actual understanding capabilities of NL systems. One reason is that the constraints on both input and output are more rigorous. For database query tasks, virtually every word must be correctly understood to produce a correct answer: by</context>
</contexts>
<marker>Sundheim, 1989</marker>
<rawString>B. M. Sundheim. Plans for a task-oriented evaluation of natural language understanding systems. In Proceedings of the Speech and Natural Language Workshop, pages 197-202, Philadelphia, PA, Februrary 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B M Sundheim</author>
</authors>
<title>Overview of the Third Message Understanding Evaluation and Conference.</title>
<date>1991</date>
<booktitle>In Proceedings of the Third Message Understanding Conference (MUC-3),</booktitle>
<pages>3--16</pages>
<publisher>DARPA, Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo, California,</location>
<contexts>
<context position="1250" citStr="Sundheim, 1991" startWordPosition="180" endWordPosition="181">gy existed for either developers of natural language (NL) interfaces or researchers in speech understanding (SU) to evaluate and compare the systems they developed. Recently considerable progress has been made by a number of groups involved in the DARPA Spoken Language Systems (SLS) program to agree on a methodology for comparative evaluation of SLS systems, and that methodology has been put into practice several times in comparative tests of several SLS systems. These evaluations are probably the only NI, evaluations other than the series of Message Understanding Conferences (Sundheim, 1989; Sundheim, 1991) to have been developed and used by a group of researchers at different sites, although several excellent workshops have been held to study some of these problems (Palmer et al., 1989; Neal et al., 1991). This paper describes a practical &amp;quot;black-box&amp;quot; methodology for automatic evaluation of question-answering NL systems. While each new application domain will require some development of special resources, the heart of the methodology is domain-independent, and it can be used with either speech or text input. The particular characteristics of the approach are described in the following section: s</context>
<context position="23019" citStr="Sundheim, 1991" startWordPosition="3726" endWordPosition="3727">ittees and their subcommittees. 5David Pallet may be contacted at the National Institute of Standards and Technology, Technology Building, Room A216, Gaithersburg, MD 20899, (301)975-2944. corpora. It does not distinguish between merely acceptable answers and very good answers. Another limitation of this approach is that it does not adequately measure the handling of some phenomena, such as extended dialogues. 5 Other Evaluation Methodologies This approach to evaluation shares many characteristics with the methods used for the DARPA-sponsored Message Understanding Conferences (Sundheim, 1989; Sundheim, 1991). In particular, both approaches are focused on external (black-box) evaluation of the understanding capabilities of systems using input/output pairs, and there are many similar problems in precisely specifying how NL systems are to satisfy the application task. Despite these similarities, this methodology probably comes closer to evaluating the actual understanding capabilities of NL systems. One reason is that the constraints on both input and output are more rigorous. For database query tasks, virtually every word must be correctly understood to produce a correct answer: by contrast, much o</context>
</contexts>
<marker>Sundheim, 1991</marker>
<rawString>B. M. Sundheim. Overview of the Third Message Understanding Evaluation and Conference. In Proceedings of the Third Message Understanding Conference (MUC-3), pages 3-16, San Mateo, California, May 1991. DARPA, Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tennant</author>
</authors>
<title>Evaluation of Natural Language Processors.</title>
<date>1981</date>
<tech>PhD thesis,</tech>
<institution>University of Illinois,</institution>
<contexts>
<context position="4413" citStr="Tennant (1981)" startWordPosition="681" endWordPosition="682"> representation corresponding to System Y&apos;s. I is easy, however, to define a simple common language foi representing answers (see Appendix A), and easy to mar system-specific representations into this common language. This methodology has been successfully applied in th( context of cross-site blind tests, where the evaluation i. based on input which the system has never seen before This type of evaluation leaves out many other important as pects of a system, such as the user interface, or the utilit] (or speed) of performing a particular task with a system tha includes a NL component (work by Tennant (1981), Bate and Rettig (1988), and Neal et al. (1991) addresses some o these other factors). Examples below will be taken from the current DARRi SLS application, the Airline Travel Information Systen (ATIS). This is a database of flights with information o the aircraft, stops and connections, meals, etc. 162 T77:77.77777:77T:77&amp;quot; Score (Right, Wrong, No Answer) SLS Answer Text text NL &amp;quot;meaning&amp;quot; Application Interface DB Commands Answer Preparation Data Figure 1: The evaluation process 2.2 Evaluation Architecture and Common Resources We assume an evaluation architecture like that in Figure 1. The shad</context>
</contexts>
<marker>Tennant, 1981</marker>
<rawString>H. Tennant. Evaluation of Natural Language Processors. PhD thesis, University of Illinois, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Walter</author>
</authors>
<title>Neal-Montgomery NLP system evaluation methodology.</title>
<date>1992</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<publisher>DARPA, Morgan Kaufmann Publishers, Inc.</publisher>
<location>San Mateo, California,</location>
<contexts>
<context position="24321" citStr="Walter, 1992" startWordPosition="3926" endWordPosition="3927">n single queries (perhaps with additional context), a smaller amount of language is being examined in each individual comparison. Similarly, for database query, the database itself implicitly constrains the space of possible answers, and each answer is scored as either correct or incorrect. This differs from the MUC evaluations, where an answer template is a composite of many bits of information, and is scored along the dimensions of recall, precision, and overgeneration. Rome Laboratory has also sponsored a recent effort to define another approach to evaluating NL systems (Neal et a/., 1991; Walter, 1992). This methodology is focussed on human evaluation of interactive systems, and is a &amp;quot;glassbox&amp;quot; method which looks at the performance of the linguistic components of the system under review. 6 Future Issues The hottest topic currently facing the SLS community with respect to evaluation is what to do about dialogues. Many of the natural tasks one might do with a database interface involve extended problem-solving dialogues, but no methodology exists for evaluating the capabilities of systems attempting to engage in dialogues with users. A Common Answer Specification (CAS) for the ATIS Applicatio</context>
</contexts>
<marker>Walter, 1992</marker>
<rawString>S. Walter. Neal-Montgomery NLP system evaluation methodology. In Proceedings of the Speech and Natural Language Workshop, San Mateo, California, February 1992. DARPA, Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Weischedel</author>
</authors>
<title>Issues and Red Herrings in Evaluating Natural Language Interfaces.</title>
<date>1986</date>
<publisher>Pergamnon Press,</publisher>
<marker>Weischedel, 1986</marker>
<rawString>R. M. Weischedel. Issues and Red Herrings in Evaluating Natural Language Interfaces. Pergamnon Press, 1986.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>