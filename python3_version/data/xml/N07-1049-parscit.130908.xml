<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010354">
<title confidence="0.974852">
Tree Revision Learning for Dependency Parsing
</title>
<author confidence="0.910949">
Giuseppe Attardi
</author>
<affiliation confidence="0.831946333333333">
Dipartimento di Informatica
Universit`a di Pisa
Pisa, Italy
</affiliation>
<email confidence="0.989971">
attardi@di.unipi.it
</email>
<author confidence="0.881242">
Massimiliano Ciaramita
</author>
<affiliation confidence="0.858112">
Yahoo! Research Barcelona
</affiliation>
<address confidence="0.89951">
Barcelona, Spain
</address>
<email confidence="0.915319">
massi@yahoo-inc.com
</email>
<sectionHeader confidence="0.998101" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952461538462">
We present a revision learning model for
improving the accuracy of a dependency
parser. The revision stage corrects the out-
put of the base parser by means of revi-
sion rules learned from the mistakes of
the base parser itself. Revision learning
is performed with a discriminative classi-
fier. The revision stage has linear com-
plexity and preserves the efficiency of the
base parser. We present empirical evalu-
ations on the treebanks of two languages,
which show effectiveness in relative error
reduction and state of the art accuracy.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968098039216">
A dependency parse tree encodes useful semantic in-
formation for several language processing tasks. De-
pendency parsing is a simpler task than constituent
parsing, since dependency trees do not have ex-
tra non-terminal nodes and there is no need for a
grammar to generate them. Approaches to depen-
dency parsing either generate such trees by consid-
ering all possible spanning trees (McDonald et al.,
2005), or build a single tree on the fly by means of
shift-reduce parsing actions (Yamada &amp; Matsumoto,
2003). In particular, Nivre and Scholz (2004) and
Attardi (2006) have developed deterministic depen-
dency parsers with linear complexity, suitable for
processing large amounts of text, as required, for ex-
ample, in information retrieval applications.
We investigate a novel revision approach to
dependency parsing related to re-ranking and
transformation-based methods (Brill, 1993; Brill,
1995; Collins, 2000; Charniak &amp; Johnson, 2005;
Collins &amp; Koo, 2006). Similarly to re-ranking, the
second stage attempts to improve the output of a
base parser. Instead of re-ranking n-best candi-
date parses, our method works by revising a sin-
gle parse tree, either the first-best or the one con-
structed by a deterministic shift-reduce parser, as in
transformation-based learning. Parse trees are re-
vised by applying rules which replace incorrect with
correct dependencies. These rules are learned by
comparing correct parse trees with incorrect trees
produced by the base parser on a training corpus.
We use the same training corpus on which the base
parser was trained, but this need not be the case.
Hence, we define a new learning task whose output
space is a set of revision rules and whose input is
a set of features extracted at each node in the parse
trees produced by the parser on the training corpus.
A statistical classifier is trained to solve this task.
The approach is more suitable for dependency
parsing since trees do not have non-terminal nodes,
therefore revisions do not require adding/removing
nodes. However, the method applies to any parser
since it only analyzes output trees. An intuitive mo-
tivation for this method is the observation that a
dependency parser correctly identifies most of the
dependencies in a tree, and only local corrections
might be necessary to produce a correct tree. Per-
forming several parses in order to generate multiple
trees would often just repeat the same steps. This
could be avoided by focusing on the points where at-
tachments are incorrect. In the experiments reported
below, on average, the revision stage performs 4.28
</bodyText>
<page confidence="0.978601">
388
</page>
<note confidence="0.7984005">
Proceedings of NAACL HLT 2007, pages 388–395,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.997380333333333">
corrections per sentence, or one every 6.25 tokens.
In our implementation we adopt a shift-reduce
parser which minimizes computational costs. The
resulting two-stage parser has complexity O(n), lin-
ear in the length of the sentence. We evaluated our
model on the treebanks of English and Swedish. The
experimental results show a relative error reduction
of, respectively, 16% and 11% with respect to the
base parser, achieving state of accuracy on Swedish.
</bodyText>
<sectionHeader confidence="0.937993" genericHeader="introduction">
2 Dependency parsing
</sectionHeader>
<bodyText confidence="0.999868857142857">
Detection of dependency relations can be useful
in tasks such as information extraction (Culotta &amp;
Sorensen, 2004), lexical acquisition (Snow et al.,
2005), ontology learning (Ciaramita et al., 2005),
and machine translation (Ding &amp; Palmer, 2005).
A dependency parser is trained on a corpus an-
notated with lexical dependencies, which are eas-
ier to produce by annotators without deep linguis-
tic knowledge and are becoming available in many
languages (Buchholz &amp; Marsi, 2006). Recent de-
velopments in dependency parsing show that deter-
ministic parsers can achieve good accuracy (Nivre &amp;
Scholz, 2004), and high performance, in the range of
hundreds of sentences per second (Attardi, 2006).
</bodyText>
<listItem confidence="0.962601714285714">
A dependency parser takes as input a sentence
s and returns a dependency graph G. Let D =
{d1, d2, ..., dm} be the set of permissible depen-
dency types. A dependency graph for a sentence
s = (s1, s2, ..., sn) is a labeled directed graph G =
(s, A), such that:
(a) s is the set of nodes, corresponding to the to-
kens in the input string;
(b) A is a set of labeled arcs (wi, d, wj), wi,j E s,
d E D; wj is called the head, wi the modifier
and d the dependency label;
(c) bwi E s there is at most one arc a E A, such
that a = (wi, d, wj);
(d) there are no cycles;
</listItem>
<bodyText confidence="0.989837">
In statistical parsing a generator (e.g. a
PCFG) is used to produce a number of candidate
trees (Collins, 2000) with associated scores. This
approach has been used also for dependency parsing,
generating spanning trees as candidates and comput-
ing the maximum spanning tree using discriminative
learning algorithms (McDonald et al., 2005).
</bodyText>
<equation confidence="0.9986315">
hS,n|I,T,Ai
Shift (1)
hn|S,I,T,Ai
hs|S,n|I,T,Ai
Right (2)
hS,n|I,T,A∪{(s,r,n)}i
hs|S,n|I,T,Ai
Left (3)
hS,s|I,T,A∪{(n,r,s)}i
hs1|s2|S,n|I,T,Ai
Right2 (4)
hs1|S,n|I,T,A∪{(s2,r,n)}i
hs1|s2|S,n|I,T,Ai
Left2 (5)
hs2|S,s1|I,T,A∪{(n,r,s2)}i
hs1|s2|s3|S,n|I,T,Ai
Right3 (6)
hs1|s2|S,n|I,T,A∪{(s3,r,n)}i
hs1|s2|s3|S,n|I,T,Ai
Left3 hs2|s3|S,s1|I,T,A∪{(n,r,s3)}i (7)
hs1|s2|S,n|I,T,Ai
Extract (8)
hn|s1|S,I,s2|T,Ai
hS,I,s1|T,Ai
Insert (9)
hs1|S,I,T,Ai
</equation>
<tableCaption confidence="0.997504">
Table 1. The set of parsing rules of the base parser.
</tableCaption>
<bodyText confidence="0.999641411764706">
Yamada and Matsumoto (2003) have proposed an
alternative approach, based on deterministic bottom-
up parsing. Instead of learning directly which tree
to assign to a sentence, the parser learns which
Shift/Reduce actions to use for building the tree.
Parsing is cast as a classification problem: at each
step the parser applies a classifier to the features rep-
resenting its current state to predict the next action to
perform. Nivre and Scholz (2004) proposed a vari-
ant of the model of Yamada and Matsumoto that re-
duces the complexity from the worst case quadratic
to linear. Attardi (2006) proposed a variant of the
rules that allows deterministic single-pass parsing
and as well as handling non-projective relations.
Several approaches to dependency parsing on multi-
ple languages have been evaluated in the CoNLL-X
Shared Task (Buchholz &amp; Marsi, 2006).
</bodyText>
<sectionHeader confidence="0.987793" genericHeader="method">
3 A shift-reduce dependency parser
</sectionHeader>
<bodyText confidence="0.999912">
As a base parser we use DeSR, a shift-reduce
parser described in (Attardi, 2006). The parser
constructs dependency trees by scanning input sen-
tences in a single left-to-right pass and performing
Shift/Reduce parsing actions. The parsing algorithm
is fully deterministic and has linear complexity. Its
behavior can be described as repeatedly selecting
and applying some parsing rules to transform its
state.
The state of the parser is represented by a quadru-
</bodyText>
<page confidence="0.995115">
389
</page>
<bodyText confidence="0.995920782608696">
ple (S, I, T, A): S is the stack, I is the list of (re-
maining) input tokens, T is a stack of saved to-
kens and A is the arc relation for the dependency
graph, consisting of a set of labeled arcs (wZ, r, wj),
wZ, wj E W (the set of tokens), and d E D (the
set of dependencies). Given an input sentence s,
the parser is initialized to (0, s, 0, 0), and terminates
when it reaches the configuration (s, 0, 0, A).
Table 1 lists all parsing rules. The Shift rule
advances on the input, while the various Left,
Right variants create links between the next in-
put token and some previous token on the stack.
Extract/Insert generalize the previous rules by
respectively moving one token to the stack T and
reinserting the top of T into S. An essential differ-
ence with respect to the rules of Yamada and Mat-
sumoto (2003) is that the Right rules move back to
the input the top of the stack, allowing some further
processing on it, which would otherwise require a
second pass. The extra Left and Right rules (4-
7, Table 1), and the ExtractInsert rules (8 and
9, Table 1), are new rules added for handling non-
projective trees. The algorithm works as follows:
</bodyText>
<equation confidence="0.84857575">
Algorithm 1: DeSR
input: s = w1, w2, ..., wn
begin
S +— ()
I +— (w1, w2, ..., wn)
T +— ()
A +— ()
while I =� () do
x +— getContext(S, I, T, A)
y +— estimateAction(w, x)
performAction(y, S, I, T, A)
end
</equation>
<bodyText confidence="0.999865428571429">
The function getContext() extracts a vector x
of contextual features around the current token, i.e.,
from a subset of I and S. estimateAction() pre-
dicts a parsing action y given a trained model w and
x. In the experiments presented below, we used as
features the lemma, Part-of-Speech, and dependency
type of the following items:
</bodyText>
<listItem confidence="0.9966115">
• 2 top items from S;
• 4 items from I;
</listItem>
<bodyText confidence="0.9758665">
Step Description
r Up to root node
u Up one parent
−n Left to the n-th token
+n Right to the n-th token
[ Head of previous constituent
] Head of following constituent
&gt; First token of previous constituent
&lt; First token of following constituent
d − − Down to the leftmost child
d + + Down to the rightmost child
d − 1 Down to the first left child
d + 1 Down to the first right child
dP Down to token with POS P
</bodyText>
<tableCaption confidence="0.7630605">
Table 2. Description of the atomic movements allowed on
the graph relatively to a token w.
</tableCaption>
<listItem confidence="0.988853">
• 2 leftmost and 2 rightmost children from the
top of S and I.
</listItem>
<sectionHeader confidence="0.820225" genericHeader="method">
4 Revising parse trees
</sectionHeader>
<bodyText confidence="0.999972368421053">
The base parser is fairly accurate and even when
there are mistakes most sentence chunks are correct.
The full correct parse tree can often be recovered by
performing just a small number of revisions on the
base parse. We propose to learn these revisions and
to apply them to the single best tree output by the
base parser. Such an approach preserves the deter-
ministic nature of the parser, since revising the tree
requires a second sequential step over the whole sen-
tence. The second step may also improve accuracy
by incorporating additional evidence, gathered from
the analysis of the tree which is not available during
the first stage of parsing.
Our approach introduces a second learning task
in which a model is trained to revise parse trees.
Several questions needs to be addressed: which tree
transformations to use in revising the parse tree,
how to determine which transformation to apply, in
which order, and which features to use for learning.
</bodyText>
<subsectionHeader confidence="0.998234">
4.1 Basic graph movements
</subsectionHeader>
<bodyText confidence="0.999894">
We define a revision as a combination of atomic
moves on a graph; e.g., moving a link to the follow-
ing or preceding token in the sentence, up or down
the graph following the directed edges. Table 2 sum-
marizes the set of atomic steps we used.
</bodyText>
<page confidence="0.994452">
390
</page>
<figureCaption confidence="0.993409">
Figure 1. An incorrect dependency tree: the dashed arrow from ”sale” to ”by” should be replaced with the one from
”offered” to ”by”.
</figureCaption>
<subsectionHeader confidence="0.986134">
4.2 Revision rules
</subsectionHeader>
<bodyText confidence="0.99990740625">
A revision rule is a sequence of atomic steps on the
graph which identifies the head of a modifier. As an
example, Figure 1 depicts a tree in which the mod-
ifier “by” is incorrectly attached to the head “sale”
(dashed arrow), rather than to the correct head “of-
fered” (continuous arrow)1. There are several possi-
ble revision rules for this case: “uu”, move up two
nodes; −3, three tokens to the left, etc. To bound
the complexity of feature extraction the maximum
length of a sequence is bound to 4. A revision for
a dependency relation is a link re-direction, which
moves a single link in a tree to a different head. This
is an elementary transformation which preserves the
number of nodes in the tree.
A possible problem with these rules is that they
are not tree-preserving, i.e. a tree may become a
cyclic graph. For instance, rules that create a link
to a descendant introduce cycles, unless the appli-
cation of another rule will link one of the nodes in
the path to the descendant to a node outside the cy-
cle. To address these issues we apply the following
heuristics in selecting the proper combination: rules
that redirect to child nodes are chosen only when
no other rule is applicable (upwards rule are safe),
and shorter rules are preferred over longer ones. In
our experiments we never observed the production
of any cycles.
On Wall Street Journal Penn Treebank section 22
we found that the 20 most frequent rules are suffi-
cient to correct 80% of the errors, see Table 3. This
confirms that the atomic movements produce simple
and effective revision rules.
</bodyText>
<footnote confidence="0.974951">
1Arrows go from head to modifier as agreed among the par-
ticipants to the CoNLL-X shared task.
</footnote>
<table confidence="0.999849904761905">
COUNTS RULE TARGET LOCATION
983 uu Up twice
685 -1 Token to the left
469 +1 Token to the right
265 [ Head of previous constituent
215 uuu Up 3 times
197 +1u Right, up
194 r To root
174 -1u Left, up
116 &gt;u Token after constituent, up
103 ud−− Up down to leftmost child
90 V To 1st child with POS verb
83 d+1 Down to first right child
82 uuuu Up 4 times
74 &lt; Token before constituent
73 ud+1 Up down to 1st right child
71 uV Up, down to 1st verb
61 ud-1 Up, down to last left child
56 ud+1d+1 Up, down to 1st right child twice
55 d+1d+1 Down to 1st right child twice
48 d−− Down to leftmost child
</table>
<tableCaption confidence="0.999667">
Table 3. 20 most frequent revision rules in wsj22.
</tableCaption>
<subsectionHeader confidence="0.930609">
4.3 Tree revision problem
</subsectionHeader>
<bodyText confidence="0.9997079375">
The tree revision problem can be formalized as fol-
lows. Let G = (s, A) be a dependency tree for
sentence s = (w1, w2, ..., wn). A revision rule is
a mapping r : A —* A which, when applied to an
arc a = (wi, d, wj), returns an arc a&apos; = (wi, d, ws).
A revised parse tree is defined as r(G) = (s, A&apos;)
such that A&apos; = Ir(a) : a E Al.
This definition corresponds to applying the revi-
sions to the original tree in a batch, as in (Brill,
1993). Alternatively, one could choose to apply the
transformations incrementally, applying each one to
the tree resulting from previous applications. We
chose the first alternative, since the intermediate
trees created during the transformation process may
not be well-formed dependency graphs, and analyz-
ing them in order to determine features for classifi-
</bodyText>
<page confidence="0.994087">
391
</page>
<bodyText confidence="0.9999492">
cation might incur problems. For instance, the graph
might have abnormal properties that differ from
those of any other graph produced by the parser.
Moreover, there might not be enough cases of such
graphs to form a sufficiently large training set.
</bodyText>
<sectionHeader confidence="0.783591" genericHeader="method">
5 Learning a revision model
</sectionHeader>
<bodyText confidence="0.999953555555556">
We frame the problem of revising a tree as a super-
vised classification task. Given a training set 5 =
(xi, yi)Z_&apos;1, such that xi E W and yi E Y , our goal
is to learn a classifier, i.e., a function F : X —* Y .
The output space represents the revision rules, in
particular we denote with y1 the identity revision
rule. Features represents syntactic and morphologi-
cal properties of the dependency being examined in
its context on the graph.
</bodyText>
<subsectionHeader confidence="0.990859">
5.1 Multiclass perceptron
</subsectionHeader>
<bodyText confidence="0.99982175">
The classifier used in revision is based on the per-
ceptron algorithm (Rosemblatt, 1958), implemented
as a multiclass classifier (Crammer &amp; Singer, 2003).
One introduces a weight vector ai E W for each
yi E Y , in which aij represents the weight associ-
ated with feature j in class i, and learn a with the
perceptron from the training data using a winner-
take-all discriminant function:
</bodyText>
<equation confidence="0.986167">
F(x) = arg max (x, ay) (10)
yEY
</equation>
<bodyText confidence="0.999985">
The only adjustable parameter in this model is the
number of instances T to use for training. We chose
T by means of validation on the development data,
typically with a value around 10 times the size of the
training data. For regularization purposes we adopt
an average perceptron (Collins, 2002) which returns
for each y, ay = T �t 1 ay, the average of all
weight vectors ay posited during training. The per-
ceptron was chosen because outperformed other al-
gorithms we experimented with (MaxEnt, MBL and
SVM), particularly when including feature pairs, as
discussed later.
</bodyText>
<subsectionHeader confidence="0.9248">
5.2 Features
</subsectionHeader>
<bodyText confidence="0.9999885">
We used as features for the revision phase the same
type of features used for training the parser (de-
scribed in Section 3). This does not have to be the
case in general. In fact, one might want to introduce
features that are specific for this task. For example,
global features of the full tree which might be not
possible to represent or extract while parsing, as in
statistical parse re-ranking (Collins &amp; Koo, 2006).
The features used are lemma, Part-of-Speech, and
dependency type of the following items: the current
node, its parent, grandparent, great-grandparent, of
the children thereof and, in addition, the previous
and next tokens of the node. We also add as features
all feature pairs that occurred more than 10 times,
to reduce the size of the feature space. In alternative
one could use a polynomial kernel. We preferred this
option because, given the large size of the training
data, a dual model is often impractical.
</bodyText>
<subsectionHeader confidence="0.941128">
5.3 Revision model
</subsectionHeader>
<bodyText confidence="0.999976935483871">
Given a dependency graph G = (s, A), for a sen-
tence s = (w1, ..., wn), the revised tree is R(G) =
(s, A&apos;), where each dependency az is equal to F(ai).
In other words, the head in ai has been changed, or
not, according to the rule predicted by the classifier.
In particular, we assume that revisions are indepen-
dent of each other and perform a revision of a tree
from left to right. As Table 3 suggests, there are
many revision rules with low frequency. Rather than
learning a huge classifier, for rules with little train-
ing data, we limit the number of classes to a value
k. We experimented with values between 30 and
50, accounting for 98-99% of all rules, and even-
tually used 50, by experimenting with the develop-
ment portion of the data. All rules that fall outside
the threshold are collected in a single class y0 of “un-
resolved” cases. If predicted, y0, similarly to y1, has
no effect on the dependency.
Occasionally, in 59 sentences out of 2416 on
section 23 of the Wall Street Journal Penn Tree-
bank (Marcus et al., 1993), the shift-reduce parser
fails to attach a node to a head, producing a dis-
connected graph. The disconnected node will ap-
pear as a root, having no head. The problem occurs
most often on punctuations (66/84 on WSJ section
23), so it affects only marginally the accuracy scores
(UAS, LAS) as computed in the CoNLL-X evalua-
tion (Buchholz &amp; Marsi, 2006). A final step of the
revision deals with multiple roots, using a heuristic
rule it selects one of the disconnected sub-trees as
root, a verb, and attaches all sub-trees to it.
</bodyText>
<page confidence="0.99703">
392
</page>
<figureCaption confidence="0.996476">
Figure 2. Frequency of the 30 most frequent rules ob-
tained with different parsers on wsj22 and wsj2-21.
</figureCaption>
<subsectionHeader confidence="0.998059">
5.4 Algorithm complexity
</subsectionHeader>
<bodyText confidence="0.999924166666667">
The base dependency parser is deterministic and per-
forms a single scan over the sentence. For each word
it performs feature extraction and invokes the classi-
fier to predict the parsing action. If prediction time is
bound by a constant, as in linear classifiers, parsing
has linear complexity. The revision pass is deter-
ministic and performs similar feature extraction and
prediction on each token. Hence, the complexity of
the overall parser is O(n). In comparison, the com-
plexity of McDonald’s parser (2006) is cubic, while
the parser of Yamada and Matsumoto (2003) has a
worst case quadratic complexity.
</bodyText>
<sectionHeader confidence="0.998717" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999413">
6.1 Data and setup
</subsectionHeader>
<bodyText confidence="0.999734230769231">
We evaluated our method on English using the stan-
dard partitions of the Wall Street Journal Penn Tree-
bank: sections 2-21 for training, section 22 for
development, and section 23 for evaluation. The
constituent trees were transformed into dependency
trees by means of a script implementing rules pro-
posed by Collins and Yamada2. In a second eval-
uation we used the Swedish Treebank (Nilsson et
al., 2005) from CoNLL-X, approximately 11,000
sentences; for development purposes we performed
cross-validation on the training data.
We trained two base parsers on the Penn Tree-
bank: one with our own implementation of Maxi-
</bodyText>
<footnote confidence="0.941115">
2http://w3.msi.vxu.se/%7enivre/research/Penn2Malt.html
</footnote>
<table confidence="0.999571125">
Parser UAS LAS
DeSR-ME 84.96 83.53
DeSR-MBL 88.41 86.85
Revision-MBL 89.11 86.39
Revision-ME 90.27 86.44
N&amp;S 87.3 -
Y&amp;M 90.3 -
MST-2 91.5 -
</table>
<tableCaption confidence="0.984797">
Table 4. Results on the Wall Street Journal Penn Tree-
bank.
</tableCaption>
<bodyText confidence="0.999957">
mum Entropy, one with the TiMBL library for Mem-
ory Based Learning (MBL, (Timbl, 2003)). We
parsed sections 2 to 21 with each parser and pro-
duced two datasets for training the revision model:
“wsj2-21.mbl” and “wsj2-21.me”. Each depen-
dency is represented as a feature vector (cf. Sec-
tion 5.2), the prediction is a revision rule (cf. Sec-
tion 4.2). For the smaller Swedish data we trained
one base parser with MaxEnt and one with the SVM
implementation in libSVM (Chang &amp; Lin, 2001) us-
ing a polynomial kernel with degree 2.
</bodyText>
<subsectionHeader confidence="0.971502">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999986173913043">
On the Penn Treebank, the base parser trained with
MBL (DeSR-MBL) achieves higher accuracy, 88.41
unlabeled accuracy score (UAS), than the same
parser trained with MaxEnt (DeSR-ME), 84.96
UAS. The revision model trained on “wsj2-21.me”
(Revision-ME) increases the accuracy of DeSR-ME
to 88.01 UAS (+3%). The revision model trained
on “wsj2-21.mbl” (DeSR-MBL) improves the accu-
racy of DeSR-MBL from 88.42 to 89.11 (+0.7%).
The difference is mainly due to the fact that DeSR-
MBL is quite accurate on the training data, almost
99%, hence “wsj2-21.mbl” contains less errors on
which to train the revision parser. This is typi-
cal of the memory-based learning algorithm used
in DeSR-MBL. Conversely, DeSR-ME achieves a
score of of 85% on the training data, which is
closer to the actual accuracy of the parser on unseen
data. As an illustration, Figure 2 plots the distri-
butions of revision rules in “wsj2-21.mbl” (DeSR-
MBL), “wsj2-21.me” (DeSR-ME), and “wsj22.mbl”
(DeSR-MBL) which represents the distribution of
correct revision rules on the output of DeSR-MBL
on the development set. The distributions of “wsj2-
</bodyText>
<page confidence="0.997434">
393
</page>
<table confidence="0.999281">
Parser UAS LAS
DeSR-SVM 88.41 83.31
Revision-ME 89.76 83.13
Corston-Oliver&amp; Aue 89.54 82.33
Nivre 89.50 84.58
</table>
<tableCaption confidence="0.999825">
Table 5. Results on the Swedish Treebank.
</tableCaption>
<bodyText confidence="0.999866">
21.me” and “wsj22.mbl” are visibly similar, while
“wsj2-21.mbl” is significantly more skewed towards
not revising. Hence, the less accurate parser DeSR-
ME might be more suitable for producing revision
training data. Applying the revision model trained
on “wsj2-21.me” (Revision-ME) to the output of
DeSR-MBL the result is 90.27% UAS. A relative
error reduction of 16.05% from the previous 88.41
UAS of DeSR-MBL. This finding suggests that it
may be worth while experimenting with all possi-
ble revision-model/base-parser pairs as well as ex-
ploring alternative ways for generating data for the
revision model; e.g., by cross-validation.
Table 4 summarizes the results on the Penn Tree-
bank. Revision models are evaluated on the output
of DeSR-MBL. The table also reports the scores ob-
tained on the same data set by by the shift reduce
parsers of Nivre and Scholz’s (2004) and Yamada
and Matsumoto (2003), and McDonald and Pereira’s
second-order maximum spanning tree parser (Mc-
Donald &amp; Pereira, 2006). However the scores are
not directly comparable, since in our experiments
we used the settings of the CoNLL-X Shared Task,
which provide correct POS tags to the parser.
On the Swedish Treebank collection we trained
a revision model (Revision-ME) on the output of
the MaxEnt base parser. We parsed the evalua-
tion data with the SVM base parser (DeSR-SVM)
which achieves 88.41 UAS. The revision model
achieves 89.76 UAS, with a relative error reduc-
tion of 11.64%. Here we can compare directly with
the best systems for this dataset in CoNLL-X. The
best system (Corston-Oliver &amp; Aue, 2006), a vari-
ant of the MST algorithm, obtained 89.54 UAS,
while the second system (Nivre, 2006) obtained
89.50; cf. Table 5. Parsing the Swedish evalua-
tion set (about 6,000 words) DeSR-SVM processes
1.7 words per second on a Xeon 2.8Ghz machine,
DeSR-ME parses more than one thousand w/sec. In
the revision step Revision-ME processes 61 w/sec.
</bodyText>
<sectionHeader confidence="0.999937" genericHeader="method">
7 Related work
</sectionHeader>
<bodyText confidence="0.999974694444444">
Several authors have proposed to improve parsing
via re-ranking (Collins, 2000; Charniak &amp; Johnson,
2005; Collins &amp; Koo, 2006). The base parser pro-
duces a list of n-best parse trees for a sentence. The
re-ranker is trained on the output trees, using addi-
tional global features, with a discriminative model.
These approaches achieve error reductions up to
13% (Collins &amp; Koo, 2006). In transformation-
based learning (Brill, 1993; Brill, 1995; Satta &amp;
Brill, 1995) the learning algorithm starts with a
baseline assignment, e.g., the most frequent Part-of-
Speech for a word, then repeatedly applies rewriting
rules. Similarly to re-ranking our method aims at
improving the accuracy of the base parser with an
additional learner. However, as in transformation-
based learning, it avoids generating multiple parses
and applies revisions to arcs in the tree which it con-
siders incorrect. This is consistent with the architec-
ture of our base parser, which is deterministic and
builds a single tree, rather than evaluating the best
outcome of a generator.
With respect to transformation-based methods,
our method does not attempt to build a tree but only
to revise it. That is, it defines a different output space
from the base parser’s: the possible revisions on the
graph. The revision model of Nakagawa et al. (2002)
applies a second classifier for deciding whether the
predictions of a base learner are accurate. However,
the model only makes a binary decision, which is
suitable for the simpler problem of POS tagging.
The work of Hall and Novak (Hall &amp; Novak, 2005)
is the closest to ours. Hall and Novak develop a cor-
rective model for constituency parsing in order to
recover non-projective dependencies, which a stan-
dard constituent parser does not handle. The tech-
nique is applied to parsing Czech.
</bodyText>
<sectionHeader confidence="0.999061" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999977">
We presented a novel approach for improving the
accuracy of a dependency parser by applying re-
vision transformations to its parse trees. Experi-
mental results prove that the approach is viable and
promising. The proposed method achieves good ac-
curacy and excellent performance using a determin-
istic shift-reduce base parser. As an issue for further
investigation, we mention that in this framework, as
</bodyText>
<page confidence="0.996326">
394
</page>
<bodyText confidence="0.9864275">
in re-ranking, it is possible to exploit global features
in the revision phase; e.g., semantic features such as
those produced by named-entity detection systems.
A. Culotta and J. Sorensen. 2004. Dependency Tree Ker-
nels for Relation Extraction. In Proceedings of ACL
2004.
</bodyText>
<sectionHeader confidence="0.997732" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999843">
We would like to thank Jordi Atserias and Brian
Roark for useful discussions and comments.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999909939759036">
G. Attardi. 2006. Experiments with a Multilanguage
Non-Projective Dependency Parser. In Proceedings of
CoNNL-X 2006.
S. Buchholz and E. Marsi. 2006. Introduction to
CoNNL-X Shared Task on Multilingual Dependency
Parsing. In Proceedings of CoNNL-X 2006.
E. Brill. 1993. Automatic Grammar Induction and Pars-
ing free Text: A Transformation-Based Approach. In
Proceedings of ACL 1993.
E. Brill. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing. Compu-
tational Linguistics 21(4): pp.543-565.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
In Proceedings of ACL 2005.
C. Chang and C. Lin. 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
M. Ciaramita, A. Gangemi, E. Ratsch, J. Sari´c and I. Ro-
jas. 2005. Unsupervised Learning of Semantic Rela-
tions between Concepts of a Molecular Biology Ontol-
ogy. In Proceedings of IJCAI 2005.
M. Collins. 2000. Discriminative Reranking for Natural
Language Parsing. In Proceedings of ICML 2000.
M. Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings of
EMNLP 2002.
M. Collins and T. Koo. 2006. Discriminative Reranking
for Natural Language Parsing. Computational Lin-
guistics 31(1): pp.25-69.
K. Crammer and Y. Singer. 2003. Ultraconservative On-
line Algorithms for Multiclass Problems. Journal of
Machine Learning Research 3: pp.951-991.
S. Corston-Oliver and A. Aue. 2006. Dependency Pars-
ing with Reference to Slovene, Spanish and Swedish.
In Proceedings of CoNLL-X.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2003. Timbl: Tilburg memory
based learner, version 5.0, reference guide. Technical
Report ILK 03-10, Tilburg University, ILK.
Y. Ding and M. Palmer. 2005. Machine Translation us-
ing Probabilistic Synchronous Dependency Insertion
Grammars. In Proceedings of ACL 2005.
K. Hall and V. Novak. 2005. Corrective Modeling for
Non-Projective Dependency Parsing. In Proceedings
of the 9th International Workshop on Parsing Tech-
nologies.
M. Marcus, B. Santorini and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The
Penn Treebank. Computational Linguistics, 19(2): pp.
313-330.
R. McDonald, F. Pereira, K. Ribarov and J. Haji˘c. 2005.
Non-projective Dependency Parsing using Spanning
Tree Algorithms. In Proceedings of HLT-EMNLP
2005.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proceedings of EACL 2006.
T. Nakagawa, T. Kudo and Y. Matsumoto. 2002. Revi-
sion Learning and its Applications to Part-of-Speech
Tagging. In Proceedings ofACL 2002.
J. Nilsson, J. Hall and J. Nivre. 2005. MAMBA Meets
TIGER: Reconstructing a Swedish Treebank from An-
tiquity. In Proceedings of the NODALIDA.
J. Nivre and M. Scholz. 2004. Deterministic Depen-
dency Parsing of English Text. In Proceedings of
COLING 2004.
J. Nivre. 2006. Labeled Pseudo-Projective Dependency
Parsing with Support Vector Machines. In Proceed-
ings of CoNLL-X.
F. Rosemblatt. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in the
Brain. Psych. Rev., 68: pp. 386-407.
G. Satta and E. Brill. 1995, Efficient Transformation-
Based Parsing. In Proceedings of ACL 1996.
R. Snow, D. Jurafsky and Y. Ng 2005. Learning Syn-
tactic Patterns for Automatic Hypernym Discovery. In
Proceedings of NIPS 17.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proceedings of the 9th International Workshop on
Parsing Technologies.
</reference>
<page confidence="0.999117">
395
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.220176">
<title confidence="0.999677">Tree Revision Learning for Dependency Parsing</title>
<author confidence="0.800877">Giuseppe</author>
<affiliation confidence="0.9394375">Dipartimento di Universit`a di</affiliation>
<address confidence="0.515068">Pisa,</address>
<email confidence="0.996833">attardi@di.unipi.it</email>
<author confidence="0.80928">Massimiliano</author>
<affiliation confidence="0.8414415">Yahoo! Research Barcelona,</affiliation>
<email confidence="0.999817">massi@yahoo-inc.com</email>
<abstract confidence="0.998237428571428">We present a revision learning model for improving the accuracy of a dependency parser. The revision stage corrects the output of the base parser by means of revision rules learned from the mistakes of the base parser itself. Revision learning is performed with a discriminative classifier. The revision stage has linear complexity and preserves the efficiency of the base parser. We present empirical evaluations on the treebanks of two languages, which show effectiveness in relative error reduction and state of the art accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Attardi</author>
</authors>
<title>Experiments with a Multilanguage Non-Projective Dependency Parser.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNNL-X</booktitle>
<contexts>
<context position="1349" citStr="Attardi (2006)" startWordPosition="206" endWordPosition="207">tion and state of the art accuracy. 1 Introduction A dependency parse tree encodes useful semantic information for several language processing tasks. Dependency parsing is a simpler task than constituent parsing, since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them. Approaches to dependency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree on the fly by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). In particular, Nivre and Scholz (2004) and Attardi (2006) have developed deterministic dependency parsers with linear complexity, suitable for processing large amounts of text, as required, for example, in information retrieval applications. We investigate a novel revision approach to dependency parsing related to re-ranking and transformation-based methods (Brill, 1993; Brill, 1995; Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). Similarly to re-ranking, the second stage attempts to improve the output of a base parser. Instead of re-ranking n-best candidate parses, our method works by revising a single parse tree, either the first-be</context>
<context position="4638" citStr="Attardi, 2006" startWordPosition="723" endWordPosition="724">ation extraction (Culotta &amp; Sorensen, 2004), lexical acquisition (Snow et al., 2005), ontology learning (Ciaramita et al., 2005), and machine translation (Ding &amp; Palmer, 2005). A dependency parser is trained on a corpus annotated with lexical dependencies, which are easier to produce by annotators without deep linguistic knowledge and are becoming available in many languages (Buchholz &amp; Marsi, 2006). Recent developments in dependency parsing show that deterministic parsers can achieve good accuracy (Nivre &amp; Scholz, 2004), and high performance, in the range of hundreds of sentences per second (Attardi, 2006). A dependency parser takes as input a sentence s and returns a dependency graph G. Let D = {d1, d2, ..., dm} be the set of permissible dependency types. A dependency graph for a sentence s = (s1, s2, ..., sn) is a labeled directed graph G = (s, A), such that: (a) s is the set of nodes, corresponding to the tokens in the input string; (b) A is a set of labeled arcs (wi, d, wj), wi,j E s, d E D; wj is called the head, wi the modifier and d the dependency label; (c) bwi E s there is at most one arc a E A, such that a = (wi, d, wj); (d) there are no cycles; In statistical parsing a generator (e.g</context>
<context position="6621" citStr="Attardi (2006)" startWordPosition="1042" endWordPosition="1043"> rules of the base parser. Yamada and Matsumoto (2003) have proposed an alternative approach, based on deterministic bottomup parsing. Instead of learning directly which tree to assign to a sentence, the parser learns which Shift/Reduce actions to use for building the tree. Parsing is cast as a classification problem: at each step the parser applies a classifier to the features representing its current state to predict the next action to perform. Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity from the worst case quadratic to linear. Attardi (2006) proposed a variant of the rules that allows deterministic single-pass parsing and as well as handling non-projective relations. Several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL-X Shared Task (Buchholz &amp; Marsi, 2006). 3 A shift-reduce dependency parser As a base parser we use DeSR, a shift-reduce parser described in (Attardi, 2006). The parser constructs dependency trees by scanning input sentences in a single left-to-right pass and performing Shift/Reduce parsing actions. The parsing algorithm is fully deterministic and has linear complexity. Its</context>
</contexts>
<marker>Attardi, 2006</marker>
<rawString>G. Attardi. 2006. Experiments with a Multilanguage Non-Projective Dependency Parser. In Proceedings of CoNNL-X 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>Introduction to CoNNL-X Shared Task on Multilingual Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNNL-X</booktitle>
<contexts>
<context position="4426" citStr="Buchholz &amp; Marsi, 2006" startWordPosition="688" endWordPosition="691"> relative error reduction of, respectively, 16% and 11% with respect to the base parser, achieving state of accuracy on Swedish. 2 Dependency parsing Detection of dependency relations can be useful in tasks such as information extraction (Culotta &amp; Sorensen, 2004), lexical acquisition (Snow et al., 2005), ontology learning (Ciaramita et al., 2005), and machine translation (Ding &amp; Palmer, 2005). A dependency parser is trained on a corpus annotated with lexical dependencies, which are easier to produce by annotators without deep linguistic knowledge and are becoming available in many languages (Buchholz &amp; Marsi, 2006). Recent developments in dependency parsing show that deterministic parsers can achieve good accuracy (Nivre &amp; Scholz, 2004), and high performance, in the range of hundreds of sentences per second (Attardi, 2006). A dependency parser takes as input a sentence s and returns a dependency graph G. Let D = {d1, d2, ..., dm} be the set of permissible dependency types. A dependency graph for a sentence s = (s1, s2, ..., sn) is a labeled directed graph G = (s, A), such that: (a) s is the set of nodes, corresponding to the tokens in the input string; (b) A is a set of labeled arcs (wi, d, wj), wi,j E </context>
<context position="6884" citStr="Buchholz &amp; Marsi, 2006" startWordPosition="1079" endWordPosition="1082">r building the tree. Parsing is cast as a classification problem: at each step the parser applies a classifier to the features representing its current state to predict the next action to perform. Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity from the worst case quadratic to linear. Attardi (2006) proposed a variant of the rules that allows deterministic single-pass parsing and as well as handling non-projective relations. Several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL-X Shared Task (Buchholz &amp; Marsi, 2006). 3 A shift-reduce dependency parser As a base parser we use DeSR, a shift-reduce parser described in (Attardi, 2006). The parser constructs dependency trees by scanning input sentences in a single left-to-right pass and performing Shift/Reduce parsing actions. The parsing algorithm is fully deterministic and has linear complexity. Its behavior can be described as repeatedly selecting and applying some parsing rules to transform its state. The state of the parser is represented by a quadru389 ple (S, I, T, A): S is the stack, I is the list of (remaining) input tokens, T is a stack of saved tok</context>
<context position="18267" citStr="Buchholz &amp; Marsi, 2006" startWordPosition="3149" endWordPosition="3152">at fall outside the threshold are collected in a single class y0 of “unresolved” cases. If predicted, y0, similarly to y1, has no effect on the dependency. Occasionally, in 59 sentences out of 2416 on section 23 of the Wall Street Journal Penn Treebank (Marcus et al., 1993), the shift-reduce parser fails to attach a node to a head, producing a disconnected graph. The disconnected node will appear as a root, having no head. The problem occurs most often on punctuations (66/84 on WSJ section 23), so it affects only marginally the accuracy scores (UAS, LAS) as computed in the CoNLL-X evaluation (Buchholz &amp; Marsi, 2006). A final step of the revision deals with multiple roots, using a heuristic rule it selects one of the disconnected sub-trees as root, a verb, and attaches all sub-trees to it. 392 Figure 2. Frequency of the 30 most frequent rules obtained with different parsers on wsj22 and wsj2-21. 5.4 Algorithm complexity The base dependency parser is deterministic and performs a single scan over the sentence. For each word it performs feature extraction and invokes the classifier to predict the parsing action. If prediction time is bound by a constant, as in linear classifiers, parsing has linear complexit</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. Introduction to CoNNL-X Shared Task on Multilingual Dependency Parsing. In Proceedings of CoNNL-X 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Automatic Grammar Induction and Parsing free Text: A Transformation-Based Approach.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1664" citStr="Brill, 1993" startWordPosition="248" endWordPosition="249">e them. Approaches to dependency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree on the fly by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). In particular, Nivre and Scholz (2004) and Attardi (2006) have developed deterministic dependency parsers with linear complexity, suitable for processing large amounts of text, as required, for example, in information retrieval applications. We investigate a novel revision approach to dependency parsing related to re-ranking and transformation-based methods (Brill, 1993; Brill, 1995; Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). Similarly to re-ranking, the second stage attempts to improve the output of a base parser. Instead of re-ranking n-best candidate parses, our method works by revising a single parse tree, either the first-best or the one constructed by a deterministic shift-reduce parser, as in transformation-based learning. Parse trees are revised by applying rules which replace incorrect with correct dependencies. These rules are learned by comparing correct parse trees with incorrect trees produced by the base parser on a training</context>
<context position="13835" citStr="Brill, 1993" startWordPosition="2379" endWordPosition="2380">down to 1st right child twice 55 d+1d+1 Down to 1st right child twice 48 d−− Down to leftmost child Table 3. 20 most frequent revision rules in wsj22. 4.3 Tree revision problem The tree revision problem can be formalized as follows. Let G = (s, A) be a dependency tree for sentence s = (w1, w2, ..., wn). A revision rule is a mapping r : A —* A which, when applied to an arc a = (wi, d, wj), returns an arc a&apos; = (wi, d, ws). A revised parse tree is defined as r(G) = (s, A&apos;) such that A&apos; = Ir(a) : a E Al. This definition corresponds to applying the revisions to the original tree in a batch, as in (Brill, 1993). Alternatively, one could choose to apply the transformations incrementally, applying each one to the tree resulting from previous applications. We chose the first alternative, since the intermediate trees created during the transformation process may not be well-formed dependency graphs, and analyzing them in order to determine features for classifi391 cation might incur problems. For instance, the graph might have abnormal properties that differ from those of any other graph produced by the parser. Moreover, there might not be enough cases of such graphs to form a sufficiently large trainin</context>
<context position="24244" citStr="Brill, 1993" startWordPosition="4117" endWordPosition="4118">ords) DeSR-SVM processes 1.7 words per second on a Xeon 2.8Ghz machine, DeSR-ME parses more than one thousand w/sec. In the revision step Revision-ME processes 61 w/sec. 7 Related work Several authors have proposed to improve parsing via re-ranking (Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). The base parser produces a list of n-best parse trees for a sentence. The re-ranker is trained on the output trees, using additional global features, with a discriminative model. These approaches achieve error reductions up to 13% (Collins &amp; Koo, 2006). In transformationbased learning (Brill, 1993; Brill, 1995; Satta &amp; Brill, 1995) the learning algorithm starts with a baseline assignment, e.g., the most frequent Part-ofSpeech for a word, then repeatedly applies rewriting rules. Similarly to re-ranking our method aims at improving the accuracy of the base parser with an additional learner. However, as in transformationbased learning, it avoids generating multiple parses and applies revisions to arcs in the tree which it considers incorrect. This is consistent with the architecture of our base parser, which is deterministic and builds a single tree, rather than evaluating the best outcom</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>E. Brill. 1993. Automatic Grammar Induction and Parsing free Text: A Transformation-Based Approach. In Proceedings of ACL 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-Based Error-Driven Learning and Natural Language Processing.</title>
<date>1995</date>
<journal>Computational Linguistics</journal>
<volume>21</volume>
<issue>4</issue>
<pages>543--565</pages>
<contexts>
<context position="1677" citStr="Brill, 1995" startWordPosition="250" endWordPosition="251">aches to dependency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree on the fly by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). In particular, Nivre and Scholz (2004) and Attardi (2006) have developed deterministic dependency parsers with linear complexity, suitable for processing large amounts of text, as required, for example, in information retrieval applications. We investigate a novel revision approach to dependency parsing related to re-ranking and transformation-based methods (Brill, 1993; Brill, 1995; Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). Similarly to re-ranking, the second stage attempts to improve the output of a base parser. Instead of re-ranking n-best candidate parses, our method works by revising a single parse tree, either the first-best or the one constructed by a deterministic shift-reduce parser, as in transformation-based learning. Parse trees are revised by applying rules which replace incorrect with correct dependencies. These rules are learned by comparing correct parse trees with incorrect trees produced by the base parser on a training corpus. We u</context>
<context position="24257" citStr="Brill, 1995" startWordPosition="4119" endWordPosition="4120">M processes 1.7 words per second on a Xeon 2.8Ghz machine, DeSR-ME parses more than one thousand w/sec. In the revision step Revision-ME processes 61 w/sec. 7 Related work Several authors have proposed to improve parsing via re-ranking (Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). The base parser produces a list of n-best parse trees for a sentence. The re-ranker is trained on the output trees, using additional global features, with a discriminative model. These approaches achieve error reductions up to 13% (Collins &amp; Koo, 2006). In transformationbased learning (Brill, 1993; Brill, 1995; Satta &amp; Brill, 1995) the learning algorithm starts with a baseline assignment, e.g., the most frequent Part-ofSpeech for a word, then repeatedly applies rewriting rules. Similarly to re-ranking our method aims at improving the accuracy of the base parser with an additional learner. However, as in transformationbased learning, it avoids generating multiple parses and applies revisions to arcs in the tree which it considers incorrect. This is consistent with the architecture of our base parser, which is deterministic and builds a single tree, rather than evaluating the best outcome of a genera</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-Based Error-Driven Learning and Natural Language Processing. Computational Linguistics 21(4): pp.543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-Fine nBest Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1718" citStr="Charniak &amp; Johnson, 2005" startWordPosition="254" endWordPosition="257">either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree on the fly by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). In particular, Nivre and Scholz (2004) and Attardi (2006) have developed deterministic dependency parsers with linear complexity, suitable for processing large amounts of text, as required, for example, in information retrieval applications. We investigate a novel revision approach to dependency parsing related to re-ranking and transformation-based methods (Brill, 1993; Brill, 1995; Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). Similarly to re-ranking, the second stage attempts to improve the output of a base parser. Instead of re-ranking n-best candidate parses, our method works by revising a single parse tree, either the first-best or the one constructed by a deterministic shift-reduce parser, as in transformation-based learning. Parse trees are revised by applying rules which replace incorrect with correct dependencies. These rules are learned by comparing correct parse trees with incorrect trees produced by the base parser on a training corpus. We use the same training corpus on which the </context>
<context position="23922" citStr="Charniak &amp; Johnson, 2005" startWordPosition="4062" endWordPosition="4065">th a relative error reduction of 11.64%. Here we can compare directly with the best systems for this dataset in CoNLL-X. The best system (Corston-Oliver &amp; Aue, 2006), a variant of the MST algorithm, obtained 89.54 UAS, while the second system (Nivre, 2006) obtained 89.50; cf. Table 5. Parsing the Swedish evaluation set (about 6,000 words) DeSR-SVM processes 1.7 words per second on a Xeon 2.8Ghz machine, DeSR-ME parses more than one thousand w/sec. In the revision step Revision-ME processes 61 w/sec. 7 Related work Several authors have proposed to improve parsing via re-ranking (Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). The base parser produces a list of n-best parse trees for a sentence. The re-ranker is trained on the output trees, using additional global features, with a discriminative model. These approaches achieve error reductions up to 13% (Collins &amp; Koo, 2006). In transformationbased learning (Brill, 1993; Brill, 1995; Satta &amp; Brill, 1995) the learning algorithm starts with a baseline assignment, e.g., the most frequent Part-ofSpeech for a word, then repeatedly applies rewriting rules. Similarly to re-ranking our method aims at improving the accuracy of the base parser with an </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-Fine nBest Parsing and MaxEnt Discriminative Reranking. In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chang</author>
<author>C Lin</author>
</authors>
<title>LIBSVM: A Library for Support Vector Machines. Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="20564" citStr="Chang &amp; Lin, 2001" startWordPosition="3525" endWordPosition="3528">sion-MBL 89.11 86.39 Revision-ME 90.27 86.44 N&amp;S 87.3 - Y&amp;M 90.3 - MST-2 91.5 - Table 4. Results on the Wall Street Journal Penn Treebank. mum Entropy, one with the TiMBL library for Memory Based Learning (MBL, (Timbl, 2003)). We parsed sections 2 to 21 with each parser and produced two datasets for training the revision model: “wsj2-21.mbl” and “wsj2-21.me”. Each dependency is represented as a feature vector (cf. Section 5.2), the prediction is a revision rule (cf. Section 4.2). For the smaller Swedish data we trained one base parser with MaxEnt and one with the SVM implementation in libSVM (Chang &amp; Lin, 2001) using a polynomial kernel with degree 2. 6.2 Results On the Penn Treebank, the base parser trained with MBL (DeSR-MBL) achieves higher accuracy, 88.41 unlabeled accuracy score (UAS), than the same parser trained with MaxEnt (DeSR-ME), 84.96 UAS. The revision model trained on “wsj2-21.me” (Revision-ME) increases the accuracy of DeSR-ME to 88.01 UAS (+3%). The revision model trained on “wsj2-21.mbl” (DeSR-MBL) improves the accuracy of DeSR-MBL from 88.42 to 89.11 (+0.7%). The difference is mainly due to the fact that DeSRMBL is quite accurate on the training data, almost 99%, hence “wsj2-21.mbl</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>C. Chang and C. Lin. 2001. LIBSVM: A Library for Support Vector Machines. Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>A Gangemi</author>
<author>E Ratsch</author>
<author>J Sari´c</author>
<author>I Rojas</author>
</authors>
<title>Unsupervised Learning of Semantic Relations between Concepts of a Molecular Biology Ontology.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI</booktitle>
<marker>Ciaramita, Gangemi, Ratsch, Sari´c, Rojas, 2005</marker>
<rawString>M. Ciaramita, A. Gangemi, E. Ratsch, J. Sari´c and I. Rojas. 2005. Unsupervised Learning of Semantic Relations between Concepts of a Molecular Biology Ontology. In Proceedings of IJCAI 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Reranking for Natural Language Parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="1692" citStr="Collins, 2000" startWordPosition="252" endWordPosition="253">ndency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree on the fly by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). In particular, Nivre and Scholz (2004) and Attardi (2006) have developed deterministic dependency parsers with linear complexity, suitable for processing large amounts of text, as required, for example, in information retrieval applications. We investigate a novel revision approach to dependency parsing related to re-ranking and transformation-based methods (Brill, 1993; Brill, 1995; Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). Similarly to re-ranking, the second stage attempts to improve the output of a base parser. Instead of re-ranking n-best candidate parses, our method works by revising a single parse tree, either the first-best or the one constructed by a deterministic shift-reduce parser, as in transformation-based learning. Parse trees are revised by applying rules which replace incorrect with correct dependencies. These rules are learned by comparing correct parse trees with incorrect trees produced by the base parser on a training corpus. We use the same tra</context>
<context position="5310" citStr="Collins, 2000" startWordPosition="866" endWordPosition="867">rns a dependency graph G. Let D = {d1, d2, ..., dm} be the set of permissible dependency types. A dependency graph for a sentence s = (s1, s2, ..., sn) is a labeled directed graph G = (s, A), such that: (a) s is the set of nodes, corresponding to the tokens in the input string; (b) A is a set of labeled arcs (wi, d, wj), wi,j E s, d E D; wj is called the head, wi the modifier and d the dependency label; (c) bwi E s there is at most one arc a E A, such that a = (wi, d, wj); (d) there are no cycles; In statistical parsing a generator (e.g. a PCFG) is used to produce a number of candidate trees (Collins, 2000) with associated scores. This approach has been used also for dependency parsing, generating spanning trees as candidates and computing the maximum spanning tree using discriminative learning algorithms (McDonald et al., 2005). hS,n|I,T,Ai Shift (1) hn|S,I,T,Ai hs|S,n|I,T,Ai Right (2) hS,n|I,T,A∪{(s,r,n)}i hs|S,n|I,T,Ai Left (3) hS,s|I,T,A∪{(n,r,s)}i hs1|s2|S,n|I,T,Ai Right2 (4) hs1|S,n|I,T,A∪{(s2,r,n)}i hs1|s2|S,n|I,T,Ai Left2 (5) hs2|S,s1|I,T,A∪{(n,r,s2)}i hs1|s2|s3|S,n|I,T,Ai Right3 (6) hs1|s2|S,n|I,T,A∪{(s3,r,n)}i hs1|s2|s3|S,n|I,T,Ai Left3 hs2|s3|S,s1|I,T,A∪{(n,r,s3)}i (7) hs1|s2|S,n|I,T,</context>
<context position="23896" citStr="Collins, 2000" startWordPosition="4060" endWordPosition="4061">s 89.76 UAS, with a relative error reduction of 11.64%. Here we can compare directly with the best systems for this dataset in CoNLL-X. The best system (Corston-Oliver &amp; Aue, 2006), a variant of the MST algorithm, obtained 89.54 UAS, while the second system (Nivre, 2006) obtained 89.50; cf. Table 5. Parsing the Swedish evaluation set (about 6,000 words) DeSR-SVM processes 1.7 words per second on a Xeon 2.8Ghz machine, DeSR-ME parses more than one thousand w/sec. In the revision step Revision-ME processes 61 w/sec. 7 Related work Several authors have proposed to improve parsing via re-ranking (Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). The base parser produces a list of n-best parse trees for a sentence. The re-ranker is trained on the output trees, using additional global features, with a discriminative model. These approaches achieve error reductions up to 13% (Collins &amp; Koo, 2006). In transformationbased learning (Brill, 1993; Brill, 1995; Satta &amp; Brill, 1995) the learning algorithm starts with a baseline assignment, e.g., the most frequent Part-ofSpeech for a word, then repeatedly applies rewriting rules. Similarly to re-ranking our method aims at improving the accuracy o</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>M. Collins. 2000. Discriminative Reranking for Natural Language Parsing. In Proceedings of ICML 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="15649" citStr="Collins, 2002" startWordPosition="2688" endWordPosition="2689">d as a multiclass classifier (Crammer &amp; Singer, 2003). One introduces a weight vector ai E W for each yi E Y , in which aij represents the weight associated with feature j in class i, and learn a with the perceptron from the training data using a winnertake-all discriminant function: F(x) = arg max (x, ay) (10) yEY The only adjustable parameter in this model is the number of instances T to use for training. We chose T by means of validation on the development data, typically with a value around 10 times the size of the training data. For regularization purposes we adopt an average perceptron (Collins, 2002) which returns for each y, ay = T �t 1 ay, the average of all weight vectors ay posited during training. The perceptron was chosen because outperformed other algorithms we experimented with (MaxEnt, MBL and SVM), particularly when including feature pairs, as discussed later. 5.2 Features We used as features for the revision phase the same type of features used for training the parser (described in Section 3). This does not have to be the case in general. In fact, one might want to introduce features that are specific for this task. For example, global features of the full tree which might be n</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Discriminative Reranking for Natural Language Parsing.</title>
<date>2006</date>
<journal>Computational Linguistics</journal>
<volume>31</volume>
<issue>1</issue>
<pages>25--69</pages>
<contexts>
<context position="1740" citStr="Collins &amp; Koo, 2006" startWordPosition="258" endWordPosition="261"> by considering all possible spanning trees (McDonald et al., 2005), or build a single tree on the fly by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). In particular, Nivre and Scholz (2004) and Attardi (2006) have developed deterministic dependency parsers with linear complexity, suitable for processing large amounts of text, as required, for example, in information retrieval applications. We investigate a novel revision approach to dependency parsing related to re-ranking and transformation-based methods (Brill, 1993; Brill, 1995; Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). Similarly to re-ranking, the second stage attempts to improve the output of a base parser. Instead of re-ranking n-best candidate parses, our method works by revising a single parse tree, either the first-best or the one constructed by a deterministic shift-reduce parser, as in transformation-based learning. Parse trees are revised by applying rules which replace incorrect with correct dependencies. These rules are learned by comparing correct parse trees with incorrect trees produced by the base parser on a training corpus. We use the same training corpus on which the base parser was traine</context>
<context position="16356" citStr="Collins &amp; Koo, 2006" startWordPosition="2809" endWordPosition="2812"> during training. The perceptron was chosen because outperformed other algorithms we experimented with (MaxEnt, MBL and SVM), particularly when including feature pairs, as discussed later. 5.2 Features We used as features for the revision phase the same type of features used for training the parser (described in Section 3). This does not have to be the case in general. In fact, one might want to introduce features that are specific for this task. For example, global features of the full tree which might be not possible to represent or extract while parsing, as in statistical parse re-ranking (Collins &amp; Koo, 2006). The features used are lemma, Part-of-Speech, and dependency type of the following items: the current node, its parent, grandparent, great-grandparent, of the children thereof and, in addition, the previous and next tokens of the node. We also add as features all feature pairs that occurred more than 10 times, to reduce the size of the feature space. In alternative one could use a polynomial kernel. We preferred this option because, given the large size of the training data, a dual model is often impractical. 5.3 Revision model Given a dependency graph G = (s, A), for a sentence s = (w1, ...,</context>
<context position="23944" citStr="Collins &amp; Koo, 2006" startWordPosition="4066" endWordPosition="4069">ion of 11.64%. Here we can compare directly with the best systems for this dataset in CoNLL-X. The best system (Corston-Oliver &amp; Aue, 2006), a variant of the MST algorithm, obtained 89.54 UAS, while the second system (Nivre, 2006) obtained 89.50; cf. Table 5. Parsing the Swedish evaluation set (about 6,000 words) DeSR-SVM processes 1.7 words per second on a Xeon 2.8Ghz machine, DeSR-ME parses more than one thousand w/sec. In the revision step Revision-ME processes 61 w/sec. 7 Related work Several authors have proposed to improve parsing via re-ranking (Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). The base parser produces a list of n-best parse trees for a sentence. The re-ranker is trained on the output trees, using additional global features, with a discriminative model. These approaches achieve error reductions up to 13% (Collins &amp; Koo, 2006). In transformationbased learning (Brill, 1993; Brill, 1995; Satta &amp; Brill, 1995) the learning algorithm starts with a baseline assignment, e.g., the most frequent Part-ofSpeech for a word, then repeatedly applies rewriting rules. Similarly to re-ranking our method aims at improving the accuracy of the base parser with an additional learner. Ho</context>
</contexts>
<marker>Collins, Koo, 2006</marker>
<rawString>M. Collins and T. Koo. 2006. Discriminative Reranking for Natural Language Parsing. Computational Linguistics 31(1): pp.25-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative Online Algorithms for Multiclass Problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<volume>3</volume>
<pages>951--991</pages>
<contexts>
<context position="15088" citStr="Crammer &amp; Singer, 2003" startWordPosition="2583" endWordPosition="2586">odel We frame the problem of revising a tree as a supervised classification task. Given a training set 5 = (xi, yi)Z_&apos;1, such that xi E W and yi E Y , our goal is to learn a classifier, i.e., a function F : X —* Y . The output space represents the revision rules, in particular we denote with y1 the identity revision rule. Features represents syntactic and morphological properties of the dependency being examined in its context on the graph. 5.1 Multiclass perceptron The classifier used in revision is based on the perceptron algorithm (Rosemblatt, 1958), implemented as a multiclass classifier (Crammer &amp; Singer, 2003). One introduces a weight vector ai E W for each yi E Y , in which aij represents the weight associated with feature j in class i, and learn a with the perceptron from the training data using a winnertake-all discriminant function: F(x) = arg max (x, ay) (10) yEY The only adjustable parameter in this model is the number of instances T to use for training. We chose T by means of validation on the development data, typically with a value around 10 times the size of the training data. For regularization purposes we adopt an average perceptron (Collins, 2002) which returns for each y, ay = T �t 1 </context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative Online Algorithms for Multiclass Problems. Journal of Machine Learning Research 3: pp.951-991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
<author>A Aue</author>
</authors>
<title>Dependency Parsing with Reference to Slovene, Spanish and Swedish.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X.</booktitle>
<contexts>
<context position="23463" citStr="Corston-Oliver &amp; Aue, 2006" startWordPosition="3988" endWordPosition="3991">ng tree parser (McDonald &amp; Pereira, 2006). However the scores are not directly comparable, since in our experiments we used the settings of the CoNLL-X Shared Task, which provide correct POS tags to the parser. On the Swedish Treebank collection we trained a revision model (Revision-ME) on the output of the MaxEnt base parser. We parsed the evaluation data with the SVM base parser (DeSR-SVM) which achieves 88.41 UAS. The revision model achieves 89.76 UAS, with a relative error reduction of 11.64%. Here we can compare directly with the best systems for this dataset in CoNLL-X. The best system (Corston-Oliver &amp; Aue, 2006), a variant of the MST algorithm, obtained 89.54 UAS, while the second system (Nivre, 2006) obtained 89.50; cf. Table 5. Parsing the Swedish evaluation set (about 6,000 words) DeSR-SVM processes 1.7 words per second on a Xeon 2.8Ghz machine, DeSR-ME parses more than one thousand w/sec. In the revision step Revision-ME processes 61 w/sec. 7 Related work Several authors have proposed to improve parsing via re-ranking (Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). The base parser produces a list of n-best parse trees for a sentence. The re-ranker is trained on the output trees, u</context>
</contexts>
<marker>Corston-Oliver, Aue, 2006</marker>
<rawString>S. Corston-Oliver and A. Aue. 2006. Dependency Parsing with Reference to Slovene, Spanish and Swedish. In Proceedings of CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory based learner, version 5.0, reference guide.</title>
<date>2003</date>
<tech>Technical Report ILK 03-10,</tech>
<institution>Tilburg University, ILK.</institution>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2003</marker>
<rawString>W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den Bosch. 2003. Timbl: Tilburg memory based learner, version 5.0, reference guide. Technical Report ILK 03-10, Tilburg University, ILK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ding</author>
<author>M Palmer</author>
</authors>
<title>Machine Translation using Probabilistic Synchronous Dependency Insertion Grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="4199" citStr="Ding &amp; Palmer, 2005" startWordPosition="651" endWordPosition="654">r which minimizes computational costs. The resulting two-stage parser has complexity O(n), linear in the length of the sentence. We evaluated our model on the treebanks of English and Swedish. The experimental results show a relative error reduction of, respectively, 16% and 11% with respect to the base parser, achieving state of accuracy on Swedish. 2 Dependency parsing Detection of dependency relations can be useful in tasks such as information extraction (Culotta &amp; Sorensen, 2004), lexical acquisition (Snow et al., 2005), ontology learning (Ciaramita et al., 2005), and machine translation (Ding &amp; Palmer, 2005). A dependency parser is trained on a corpus annotated with lexical dependencies, which are easier to produce by annotators without deep linguistic knowledge and are becoming available in many languages (Buchholz &amp; Marsi, 2006). Recent developments in dependency parsing show that deterministic parsers can achieve good accuracy (Nivre &amp; Scholz, 2004), and high performance, in the range of hundreds of sentences per second (Attardi, 2006). A dependency parser takes as input a sentence s and returns a dependency graph G. Let D = {d1, d2, ..., dm} be the set of permissible dependency types. A depen</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Y. Ding and M. Palmer. 2005. Machine Translation using Probabilistic Synchronous Dependency Insertion Grammars. In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hall</author>
<author>V Novak</author>
</authors>
<title>Corrective Modeling for Non-Projective Dependency Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="25377" citStr="Hall &amp; Novak, 2005" startWordPosition="4301" endWordPosition="4304">hich is deterministic and builds a single tree, rather than evaluating the best outcome of a generator. With respect to transformation-based methods, our method does not attempt to build a tree but only to revise it. That is, it defines a different output space from the base parser’s: the possible revisions on the graph. The revision model of Nakagawa et al. (2002) applies a second classifier for deciding whether the predictions of a base learner are accurate. However, the model only makes a binary decision, which is suitable for the simpler problem of POS tagging. The work of Hall and Novak (Hall &amp; Novak, 2005) is the closest to ours. Hall and Novak develop a corrective model for constituency parsing in order to recover non-projective dependencies, which a standard constituent parser does not handle. The technique is applied to parsing Czech. 8 Conclusion We presented a novel approach for improving the accuracy of a dependency parser by applying revision transformations to its parse trees. Experimental results prove that the approach is viable and promising. The proposed method achieves good accuracy and excellent performance using a deterministic shift-reduce base parser. As an issue for further in</context>
</contexts>
<marker>Hall, Novak, 2005</marker>
<rawString>K. Hall and V. Novak. 2005. Corrective Modeling for Non-Projective Dependency Parsing. In Proceedings of the 9th International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<contexts>
<context position="17918" citStr="Marcus et al., 1993" startWordPosition="3088" endWordPosition="3091">e are many revision rules with low frequency. Rather than learning a huge classifier, for rules with little training data, we limit the number of classes to a value k. We experimented with values between 30 and 50, accounting for 98-99% of all rules, and eventually used 50, by experimenting with the development portion of the data. All rules that fall outside the threshold are collected in a single class y0 of “unresolved” cases. If predicted, y0, similarly to y1, has no effect on the dependency. Occasionally, in 59 sentences out of 2416 on section 23 of the Wall Street Journal Penn Treebank (Marcus et al., 1993), the shift-reduce parser fails to attach a node to a head, producing a disconnected graph. The disconnected node will appear as a root, having no head. The problem occurs most often on punctuations (66/84 on WSJ section 23), so it affects only marginally the accuracy scores (UAS, LAS) as computed in the CoNLL-X evaluation (Buchholz &amp; Marsi, 2006). A final step of the revision deals with multiple roots, using a heuristic rule it selects one of the disconnected sub-trees as root, a verb, and attaches all sub-trees to it. 392 Figure 2. Frequency of the 30 most frequent rules obtained with differ</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini and M. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2): pp. 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Haji˘c</author>
</authors>
<title>Non-projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP</booktitle>
<marker>McDonald, Pereira, Ribarov, Haji˘c, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov and J. Haji˘c. 2005. Non-projective Dependency Parsing using Spanning Tree Algorithms. In Proceedings of HLT-EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online Learning of Approximate Dependency Parsing Algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="22877" citStr="McDonald &amp; Pereira, 2006" startWordPosition="3889" endWordPosition="3893">rom the previous 88.41 UAS of DeSR-MBL. This finding suggests that it may be worth while experimenting with all possible revision-model/base-parser pairs as well as exploring alternative ways for generating data for the revision model; e.g., by cross-validation. Table 4 summarizes the results on the Penn Treebank. Revision models are evaluated on the output of DeSR-MBL. The table also reports the scores obtained on the same data set by by the shift reduce parsers of Nivre and Scholz’s (2004) and Yamada and Matsumoto (2003), and McDonald and Pereira’s second-order maximum spanning tree parser (McDonald &amp; Pereira, 2006). However the scores are not directly comparable, since in our experiments we used the settings of the CoNLL-X Shared Task, which provide correct POS tags to the parser. On the Swedish Treebank collection we trained a revision model (Revision-ME) on the output of the MaxEnt base parser. We parsed the evaluation data with the SVM base parser (DeSR-SVM) which achieves 88.41 UAS. The revision model achieves 89.76 UAS, with a relative error reduction of 11.64%. Here we can compare directly with the best systems for this dataset in CoNLL-X. The best system (Corston-Oliver &amp; Aue, 2006), a variant of</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. In Proceedings of EACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nakagawa</author>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Revision Learning and its Applications to Part-of-Speech Tagging.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="25125" citStr="Nakagawa et al. (2002)" startWordPosition="4258" endWordPosition="4261">ase parser with an additional learner. However, as in transformationbased learning, it avoids generating multiple parses and applies revisions to arcs in the tree which it considers incorrect. This is consistent with the architecture of our base parser, which is deterministic and builds a single tree, rather than evaluating the best outcome of a generator. With respect to transformation-based methods, our method does not attempt to build a tree but only to revise it. That is, it defines a different output space from the base parser’s: the possible revisions on the graph. The revision model of Nakagawa et al. (2002) applies a second classifier for deciding whether the predictions of a base learner are accurate. However, the model only makes a binary decision, which is suitable for the simpler problem of POS tagging. The work of Hall and Novak (Hall &amp; Novak, 2005) is the closest to ours. Hall and Novak develop a corrective model for constituency parsing in order to recover non-projective dependencies, which a standard constituent parser does not handle. The technique is applied to parsing Czech. 8 Conclusion We presented a novel approach for improving the accuracy of a dependency parser by applying revisi</context>
</contexts>
<marker>Nakagawa, Kudo, Matsumoto, 2002</marker>
<rawString>T. Nakagawa, T. Kudo and Y. Matsumoto. 2002. Revision Learning and its Applications to Part-of-Speech Tagging. In Proceedings ofACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nilsson</author>
<author>J Hall</author>
<author>J Nivre</author>
</authors>
<title>MAMBA Meets TIGER: Reconstructing a Swedish Treebank from Antiquity.</title>
<date>2005</date>
<booktitle>In Proceedings of the NODALIDA.</booktitle>
<contexts>
<context position="19618" citStr="Nilsson et al., 2005" startWordPosition="3373" endWordPosition="3376">of the overall parser is O(n). In comparison, the complexity of McDonald’s parser (2006) is cubic, while the parser of Yamada and Matsumoto (2003) has a worst case quadratic complexity. 6 Experiments 6.1 Data and setup We evaluated our method on English using the standard partitions of the Wall Street Journal Penn Treebank: sections 2-21 for training, section 22 for development, and section 23 for evaluation. The constituent trees were transformed into dependency trees by means of a script implementing rules proposed by Collins and Yamada2. In a second evaluation we used the Swedish Treebank (Nilsson et al., 2005) from CoNLL-X, approximately 11,000 sentences; for development purposes we performed cross-validation on the training data. We trained two base parsers on the Penn Treebank: one with our own implementation of Maxi2http://w3.msi.vxu.se/%7enivre/research/Penn2Malt.html Parser UAS LAS DeSR-ME 84.96 83.53 DeSR-MBL 88.41 86.85 Revision-MBL 89.11 86.39 Revision-ME 90.27 86.44 N&amp;S 87.3 - Y&amp;M 90.3 - MST-2 91.5 - Table 4. Results on the Wall Street Journal Penn Treebank. mum Entropy, one with the TiMBL library for Memory Based Learning (MBL, (Timbl, 2003)). We parsed sections 2 to 21 with each parser a</context>
</contexts>
<marker>Nilsson, Hall, Nivre, 2005</marker>
<rawString>J. Nilsson, J. Hall and J. Nivre. 2005. MAMBA Meets TIGER: Reconstructing a Swedish Treebank from Antiquity. In Proceedings of the NODALIDA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>M Scholz</author>
</authors>
<title>Deterministic Dependency Parsing of English Text.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="1330" citStr="Nivre and Scholz (2004)" startWordPosition="201" endWordPosition="204">ness in relative error reduction and state of the art accuracy. 1 Introduction A dependency parse tree encodes useful semantic information for several language processing tasks. Dependency parsing is a simpler task than constituent parsing, since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them. Approaches to dependency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree on the fly by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). In particular, Nivre and Scholz (2004) and Attardi (2006) have developed deterministic dependency parsers with linear complexity, suitable for processing large amounts of text, as required, for example, in information retrieval applications. We investigate a novel revision approach to dependency parsing related to re-ranking and transformation-based methods (Brill, 1993; Brill, 1995; Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). Similarly to re-ranking, the second stage attempts to improve the output of a base parser. Instead of re-ranking n-best candidate parses, our method works by revising a single parse tree, </context>
<context position="6481" citStr="Nivre and Scholz (2004)" startWordPosition="1015" endWordPosition="1018">t3 hs2|s3|S,s1|I,T,A∪{(n,r,s3)}i (7) hs1|s2|S,n|I,T,Ai Extract (8) hn|s1|S,I,s2|T,Ai hS,I,s1|T,Ai Insert (9) hs1|S,I,T,Ai Table 1. The set of parsing rules of the base parser. Yamada and Matsumoto (2003) have proposed an alternative approach, based on deterministic bottomup parsing. Instead of learning directly which tree to assign to a sentence, the parser learns which Shift/Reduce actions to use for building the tree. Parsing is cast as a classification problem: at each step the parser applies a classifier to the features representing its current state to predict the next action to perform. Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity from the worst case quadratic to linear. Attardi (2006) proposed a variant of the rules that allows deterministic single-pass parsing and as well as handling non-projective relations. Several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL-X Shared Task (Buchholz &amp; Marsi, 2006). 3 A shift-reduce dependency parser As a base parser we use DeSR, a shift-reduce parser described in (Attardi, 2006). The parser constructs dependency trees by scanning input sentences in a single</context>
<context position="4550" citStr="Nivre &amp; Scholz, 2004" startWordPosition="707" endWordPosition="710">h. 2 Dependency parsing Detection of dependency relations can be useful in tasks such as information extraction (Culotta &amp; Sorensen, 2004), lexical acquisition (Snow et al., 2005), ontology learning (Ciaramita et al., 2005), and machine translation (Ding &amp; Palmer, 2005). A dependency parser is trained on a corpus annotated with lexical dependencies, which are easier to produce by annotators without deep linguistic knowledge and are becoming available in many languages (Buchholz &amp; Marsi, 2006). Recent developments in dependency parsing show that deterministic parsers can achieve good accuracy (Nivre &amp; Scholz, 2004), and high performance, in the range of hundreds of sentences per second (Attardi, 2006). A dependency parser takes as input a sentence s and returns a dependency graph G. Let D = {d1, d2, ..., dm} be the set of permissible dependency types. A dependency graph for a sentence s = (s1, s2, ..., sn) is a labeled directed graph G = (s, A), such that: (a) s is the set of nodes, corresponding to the tokens in the input string; (b) A is a set of labeled arcs (wi, d, wj), wi,j E s, d E D; wj is called the head, wi the modifier and d the dependency label; (c) bwi E s there is at most one arc a E A, suc</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>J. Nivre and M. Scholz. 2004. Deterministic Dependency Parsing of English Text. In Proceedings of COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X.</booktitle>
<contexts>
<context position="23554" citStr="Nivre, 2006" startWordPosition="4006" endWordPosition="4007">periments we used the settings of the CoNLL-X Shared Task, which provide correct POS tags to the parser. On the Swedish Treebank collection we trained a revision model (Revision-ME) on the output of the MaxEnt base parser. We parsed the evaluation data with the SVM base parser (DeSR-SVM) which achieves 88.41 UAS. The revision model achieves 89.76 UAS, with a relative error reduction of 11.64%. Here we can compare directly with the best systems for this dataset in CoNLL-X. The best system (Corston-Oliver &amp; Aue, 2006), a variant of the MST algorithm, obtained 89.54 UAS, while the second system (Nivre, 2006) obtained 89.50; cf. Table 5. Parsing the Swedish evaluation set (about 6,000 words) DeSR-SVM processes 1.7 words per second on a Xeon 2.8Ghz machine, DeSR-ME parses more than one thousand w/sec. In the revision step Revision-ME processes 61 w/sec. 7 Related work Several authors have proposed to improve parsing via re-ranking (Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). The base parser produces a list of n-best parse trees for a sentence. The re-ranker is trained on the output trees, using additional global features, with a discriminative model. These approaches achieve erro</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>J. Nivre. 2006. Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines. In Proceedings of CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rosemblatt</author>
</authors>
<title>The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.</title>
<date>1958</date>
<journal>Psych. Rev.,</journal>
<volume>68</volume>
<pages>386--407</pages>
<contexts>
<context position="15023" citStr="Rosemblatt, 1958" startWordPosition="2576" endWordPosition="2577"> a sufficiently large training set. 5 Learning a revision model We frame the problem of revising a tree as a supervised classification task. Given a training set 5 = (xi, yi)Z_&apos;1, such that xi E W and yi E Y , our goal is to learn a classifier, i.e., a function F : X —* Y . The output space represents the revision rules, in particular we denote with y1 the identity revision rule. Features represents syntactic and morphological properties of the dependency being examined in its context on the graph. 5.1 Multiclass perceptron The classifier used in revision is based on the perceptron algorithm (Rosemblatt, 1958), implemented as a multiclass classifier (Crammer &amp; Singer, 2003). One introduces a weight vector ai E W for each yi E Y , in which aij represents the weight associated with feature j in class i, and learn a with the perceptron from the training data using a winnertake-all discriminant function: F(x) = arg max (x, ay) (10) yEY The only adjustable parameter in this model is the number of instances T to use for training. We chose T by means of validation on the development data, typically with a value around 10 times the size of the training data. For regularization purposes we adopt an average </context>
</contexts>
<marker>Rosemblatt, 1958</marker>
<rawString>F. Rosemblatt. 1958. The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. Psych. Rev., 68: pp. 386-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Satta</author>
<author>E Brill</author>
</authors>
<title>Efficient TransformationBased Parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="24279" citStr="Satta &amp; Brill, 1995" startWordPosition="4121" endWordPosition="4124">.7 words per second on a Xeon 2.8Ghz machine, DeSR-ME parses more than one thousand w/sec. In the revision step Revision-ME processes 61 w/sec. 7 Related work Several authors have proposed to improve parsing via re-ranking (Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). The base parser produces a list of n-best parse trees for a sentence. The re-ranker is trained on the output trees, using additional global features, with a discriminative model. These approaches achieve error reductions up to 13% (Collins &amp; Koo, 2006). In transformationbased learning (Brill, 1993; Brill, 1995; Satta &amp; Brill, 1995) the learning algorithm starts with a baseline assignment, e.g., the most frequent Part-ofSpeech for a word, then repeatedly applies rewriting rules. Similarly to re-ranking our method aims at improving the accuracy of the base parser with an additional learner. However, as in transformationbased learning, it avoids generating multiple parses and applies revisions to arcs in the tree which it considers incorrect. This is consistent with the architecture of our base parser, which is deterministic and builds a single tree, rather than evaluating the best outcome of a generator. With respect to t</context>
</contexts>
<marker>Satta, Brill, 1995</marker>
<rawString>G. Satta and E. Brill. 1995, Efficient TransformationBased Parsing. In Proceedings of ACL 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>Y Ng</author>
</authors>
<title>Learning Syntactic Patterns for Automatic Hypernym Discovery.</title>
<date>2005</date>
<booktitle>In Proceedings of NIPS 17.</booktitle>
<contexts>
<context position="4108" citStr="Snow et al., 2005" startWordPosition="638" endWordPosition="641">r sentence, or one every 6.25 tokens. In our implementation we adopt a shift-reduce parser which minimizes computational costs. The resulting two-stage parser has complexity O(n), linear in the length of the sentence. We evaluated our model on the treebanks of English and Swedish. The experimental results show a relative error reduction of, respectively, 16% and 11% with respect to the base parser, achieving state of accuracy on Swedish. 2 Dependency parsing Detection of dependency relations can be useful in tasks such as information extraction (Culotta &amp; Sorensen, 2004), lexical acquisition (Snow et al., 2005), ontology learning (Ciaramita et al., 2005), and machine translation (Ding &amp; Palmer, 2005). A dependency parser is trained on a corpus annotated with lexical dependencies, which are easier to produce by annotators without deep linguistic knowledge and are becoming available in many languages (Buchholz &amp; Marsi, 2006). Recent developments in dependency parsing show that deterministic parsers can achieve good accuracy (Nivre &amp; Scholz, 2004), and high performance, in the range of hundreds of sentences per second (Attardi, 2006). A dependency parser takes as input a sentence s and returns a depend</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>R. Snow, D. Jurafsky and Y. Ng 2005. Learning Syntactic Patterns for Automatic Hypernym Discovery. In Proceedings of NIPS 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical Dependency Analysis with Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="6061" citStr="Yamada and Matsumoto (2003)" startWordPosition="947" endWordPosition="950">d computing the maximum spanning tree using discriminative learning algorithms (McDonald et al., 2005). hS,n|I,T,Ai Shift (1) hn|S,I,T,Ai hs|S,n|I,T,Ai Right (2) hS,n|I,T,A∪{(s,r,n)}i hs|S,n|I,T,Ai Left (3) hS,s|I,T,A∪{(n,r,s)}i hs1|s2|S,n|I,T,Ai Right2 (4) hs1|S,n|I,T,A∪{(s2,r,n)}i hs1|s2|S,n|I,T,Ai Left2 (5) hs2|S,s1|I,T,A∪{(n,r,s2)}i hs1|s2|s3|S,n|I,T,Ai Right3 (6) hs1|s2|S,n|I,T,A∪{(s3,r,n)}i hs1|s2|s3|S,n|I,T,Ai Left3 hs2|s3|S,s1|I,T,A∪{(n,r,s3)}i (7) hs1|s2|S,n|I,T,Ai Extract (8) hn|s1|S,I,s2|T,Ai hS,I,s1|T,Ai Insert (9) hs1|S,I,T,Ai Table 1. The set of parsing rules of the base parser. Yamada and Matsumoto (2003) have proposed an alternative approach, based on deterministic bottomup parsing. Instead of learning directly which tree to assign to a sentence, the parser learns which Shift/Reduce actions to use for building the tree. Parsing is cast as a classification problem: at each step the parser applies a classifier to the features representing its current state to predict the next action to perform. Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity from the worst case quadratic to linear. Attardi (2006) proposed a variant of the rules that al</context>
<context position="8191" citStr="Yamada and Matsumoto (2003)" startWordPosition="1313" endWordPosition="1317">abeled arcs (wZ, r, wj), wZ, wj E W (the set of tokens), and d E D (the set of dependencies). Given an input sentence s, the parser is initialized to (0, s, 0, 0), and terminates when it reaches the configuration (s, 0, 0, A). Table 1 lists all parsing rules. The Shift rule advances on the input, while the various Left, Right variants create links between the next input token and some previous token on the stack. Extract/Insert generalize the previous rules by respectively moving one token to the stack T and reinserting the top of T into S. An essential difference with respect to the rules of Yamada and Matsumoto (2003) is that the Right rules move back to the input the top of the stack, allowing some further processing on it, which would otherwise require a second pass. The extra Left and Right rules (4- 7, Table 1), and the ExtractInsert rules (8 and 9, Table 1), are new rules added for handling nonprojective trees. The algorithm works as follows: Algorithm 1: DeSR input: s = w1, w2, ..., wn begin S +— () I +— (w1, w2, ..., wn) T +— () A +— () while I =� () do x +— getContext(S, I, T, A) y +— estimateAction(w, x) performAction(y, S, I, T, A) end The function getContext() extracts a vector x of contextual f</context>
<context position="19143" citStr="Yamada and Matsumoto (2003)" startWordPosition="3294" endWordPosition="3297">erent parsers on wsj22 and wsj2-21. 5.4 Algorithm complexity The base dependency parser is deterministic and performs a single scan over the sentence. For each word it performs feature extraction and invokes the classifier to predict the parsing action. If prediction time is bound by a constant, as in linear classifiers, parsing has linear complexity. The revision pass is deterministic and performs similar feature extraction and prediction on each token. Hence, the complexity of the overall parser is O(n). In comparison, the complexity of McDonald’s parser (2006) is cubic, while the parser of Yamada and Matsumoto (2003) has a worst case quadratic complexity. 6 Experiments 6.1 Data and setup We evaluated our method on English using the standard partitions of the Wall Street Journal Penn Treebank: sections 2-21 for training, section 22 for development, and section 23 for evaluation. The constituent trees were transformed into dependency trees by means of a script implementing rules proposed by Collins and Yamada2. In a second evaluation we used the Swedish Treebank (Nilsson et al., 2005) from CoNLL-X, approximately 11,000 sentences; for development purposes we performed cross-validation on the training data. W</context>
<context position="22780" citStr="Yamada and Matsumoto (2003)" startWordPosition="3876" endWordPosition="3879">sion-ME) to the output of DeSR-MBL the result is 90.27% UAS. A relative error reduction of 16.05% from the previous 88.41 UAS of DeSR-MBL. This finding suggests that it may be worth while experimenting with all possible revision-model/base-parser pairs as well as exploring alternative ways for generating data for the revision model; e.g., by cross-validation. Table 4 summarizes the results on the Penn Treebank. Revision models are evaluated on the output of DeSR-MBL. The table also reports the scores obtained on the same data set by by the shift reduce parsers of Nivre and Scholz’s (2004) and Yamada and Matsumoto (2003), and McDonald and Pereira’s second-order maximum spanning tree parser (McDonald &amp; Pereira, 2006). However the scores are not directly comparable, since in our experiments we used the settings of the CoNLL-X Shared Task, which provide correct POS tags to the parser. On the Swedish Treebank collection we trained a revision model (Revision-ME) on the output of the MaxEnt base parser. We parsed the evaluation data with the SVM base parser (DeSR-SVM) which achieves 88.41 UAS. The revision model achieves 89.76 UAS, with a relative error reduction of 11.64%. Here we can compare directly with the bes</context>
<context position="1290" citStr="Yamada &amp; Matsumoto, 2003" startWordPosition="195" endWordPosition="198">nks of two languages, which show effectiveness in relative error reduction and state of the art accuracy. 1 Introduction A dependency parse tree encodes useful semantic information for several language processing tasks. Dependency parsing is a simpler task than constituent parsing, since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them. Approaches to dependency parsing either generate such trees by considering all possible spanning trees (McDonald et al., 2005), or build a single tree on the fly by means of shift-reduce parsing actions (Yamada &amp; Matsumoto, 2003). In particular, Nivre and Scholz (2004) and Attardi (2006) have developed deterministic dependency parsers with linear complexity, suitable for processing large amounts of text, as required, for example, in information retrieval applications. We investigate a novel revision approach to dependency parsing related to re-ranking and transformation-based methods (Brill, 1993; Brill, 1995; Collins, 2000; Charniak &amp; Johnson, 2005; Collins &amp; Koo, 2006). Similarly to re-ranking, the second stage attempts to improve the output of a base parser. Instead of re-ranking n-best candidate parses, our method</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical Dependency Analysis with Support Vector Machines. In Proceedings of the 9th International Workshop on Parsing Technologies.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>