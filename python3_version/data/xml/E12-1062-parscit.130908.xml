<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000219">
<title confidence="0.9767925">
Learning for Microblogs with Distant Supervision:
Political Forecasting with Twitter
</title>
<author confidence="0.971112">
Micol Marchetti-Bowick
</author>
<affiliation confidence="0.974555">
Microsoft Corporation
</affiliation>
<address confidence="0.972719">
475 Brannan Street
San Francisco, CA 94122
</address>
<email confidence="0.999284">
micolmb@microsoft.com
</email>
<author confidence="0.981449">
Nathanael Chambers
</author>
<affiliation confidence="0.997543">
Department of Computer Science
</affiliation>
<address confidence="0.728257">
United States Naval Academy
Annapolis, MD 21409
</address>
<email confidence="0.999276">
nchamber@usna.edu
</email>
<sectionHeader confidence="0.99467" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997333214285714">
Microblogging websites such as Twitter
offer a wealth of insight into a popu-
lation’s current mood. Automated ap-
proaches to identify general sentiment to-
ward a particular topic often perform two
steps: Topic Identification and Sentiment
Analysis. Topic Identification first identi-
fies tweets that are relevant to a desired
topic (e.g., a politician or event), and Sen-
timent Analysis extracts each tweet’s atti-
tude toward the topic. Many techniques for
Topic Identification simply involve select-
ing tweets using a keyword search. Here,
we present an approach that instead uses
distant supervision to train a classifier on
the tweets returned by the search. We show
that distant supervision leads to improved
performance in the Topic Identification task
as well in the downstream Sentiment Anal-
ysis stage. We then use a system that incor-
porates distant supervision into both stages
to analyze the sentiment toward President
Obama expressed in a dataset of tweets.
Our results better correlate with Gallup’s
Presidential Job Approval polls than pre-
vious work. Finally, we discover a sur-
prising baseline that outperforms previous
work without a Topic Identification stage.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977183673469">
Social networks and blogs contain a wealth of
data about how the general public views products,
campaigns, events, and people. Automated algo-
rithms can use this data to provide instant feed-
back on what people are saying about a topic.
Two challenges in building such algorithms are
(1) identifying topic-relevant posts, and (2) iden-
tifying the attitude of each post toward the topic.
This paper studies distant supervision (Mintz et
al., 2009) as a solution to both challenges. We
apply our approach to the problem of predicting
Presidential Job Approval polls from Twitter data,
and we present results that improve on previous
work in this area. We also present a novel base-
line that performs remarkably well without using
topic identification.
Topic identification is the task of identifying
text that discusses a topic of interest. Most pre-
vious work on microblogs uses simple keyword
searches to find topic-relevant tweets on the as-
sumption that short tweets do not need more so-
phisticated processing. For instance, searches for
the name “Obama” have been assumed to return
a representative set of tweets about the U.S. Pres-
ident (O’Connor et al., 2010). One of the main
contributions of this paper is to show that keyword
search can lead to noisy results, and that the same
keywords can instead be used in a distantly super-
vised framework to yield improved performance.
Distant supervision uses noisy signals in text
as positive labels to train classifiers. For in-
stance, the token “Obama” can be used to iden-
tify a series of tweets that discuss U.S. President
Barack Obama. Although searching for token
matches can return false positives, using the re-
sulting tweets as positive training examples pro-
vides supervision from a distance. This paper ex-
periments with several diverse sets of keywords
to train distantly supervised classifiers for topic
identification. We evaluate each classifier on a
hand-labeled dataset of political and apolitical
tweets, and demonstrate an improvement in F1
score over simple keyword search (.39 to .90 in
the best case). We also make available the first la-
beled dataset for topic identification in politics to
encourage future work.
Sentiment analysis encompasses a broad field
of research, but most microblog work focuses
on two moods: positive and negative sentiment.
</bodyText>
<page confidence="0.988078">
603
</page>
<note confidence="0.9766545">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 603–612,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999759433333333">
Algorithms to identify these moods range from
matching words in a sentiment lexicon to training
classifiers with a hand-labeled corpus. Since la-
beling corpora is expensive, recent work on Twit-
ter uses emoticons (i.e., ASCII smiley faces such
as :-( and :-)) as noisy labels in tweets for distant
supervision (Pak and Paroubek, 2010; Davidov et
al., 2010; Kouloumpis et al., 2011). This paper
presents new analysis of the downstream effects
of topic identification on sentiment classifiers and
their application to political forecasting.
Interest in measuring the political mood of
a country has recently grown (O’Connor et al.,
2010; Tumasjan et al., 2010; Gonzalez-Bailon et
al., 2010; Carvalho et al., 2011; Tan et al., 2011).
Here we compare our sentiment results to Presi-
dential Job Approval polls and show that the sen-
timent scores produced by our system are posi-
tively correlated with both the Approval and Dis-
approval job ratings.
In this paper we present a method for cou-
pling two distantly supervised algorithms for
topic identification and sentiment classification on
Twitter. In Section 4, we describe our approach to
topic identification and present a new annotated
corpus of political tweets for future study. In Sec-
tion 5, we apply distant supervision to sentiment
analysis. Finally, Section 6 discusses our sys-
tem’s performance on modeling Presidential Job
Approval ratings from Twitter data.
</bodyText>
<sectionHeader confidence="0.993118" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999958984615385">
The past several years have seen sentiment anal-
ysis grow into a diverse research area. The idea
of sentiment applied to microblogging domains is
relatively new, but there are numerous recent pub-
lications on the subject. Since this paper focuses
on the microblog setting, we concentrate on these
contributions here.
The most straightforward approach to senti-
ment analysis is using a sentiment lexicon to la-
bel tweets based on how many sentiment words
appear. This approach tends to be used by appli-
cations that measure the general mood of a popu-
lation. O’Connor et al. (2010) use a ratio of posi-
tive and negative word counts on Twitter, Kramer
(2010) counts lexicon words on Facebook, and
Thelwall (2011) uses the publicly available Sen-
tiStrength algorithm to make weighted counts of
keywords based on predefined polarity strengths.
In contrast to lexicons, many approaches in-
stead focus on ways to train supervised classi-
fiers. However, labeled data is expensive to cre-
ate, and examples of Twitter classifiers trained on
hand-labeled data are few (Jiang et al., 2011). In-
stead, distant supervision has grown in popular-
ity. These algorithms use emoticons to serve as
semantic indicators for sentiment. For instance,
a sad face (e.g., :-() serves as a noisy label for a
negative mood. Read (2005) was the first to sug-
gest emoticons for UseNet data, followed by Go
et al. (Go et al., 2009) on Twitter, and many others
since (Bifet and Frank, 2010; Pak and Paroubek,
2010; Davidov et al., 2010; Kouloumpis et al.,
2011). Hashtags (e.g., #cool and #happy) have
also been used as noisy sentiment labels (Davi-
dov et al., 2010; Kouloumpis et al., 2011). Fi-
nally, multiple models can be blended into a sin-
gle classifier (Barbosa and Feng, 2010). Here, we
adopt the emoticon algorithm for sentiment analy-
sis, and evaluate it on a specific domain (politics).
Topic identification in Twitter has received
much less attention than sentiment analysis. The
majority of approaches simply select a single
keyword (e.g., “Obama”) to represent their topic
(e.g., “US President”) and retrieve all tweets that
contain the word (O’Connor et al., 2010; Tumas-
jan et al., 2010; Tan et al., 2011). The underlying
assumption is that the keyword is precise, and due
to the vast number of tweets, the search will re-
turn a large enough dataset to measure sentiment
toward that topic. In this work, we instead use
a distantly supervised system similar in spirit to
those recently applied to sentiment analysis.
Finally, we evaluate the approaches presented
in this paper on the domain of politics. Tumasjan
et al. (2010) showed that the results of a recent
German election could be predicted through fre-
quency counts with remarkable accuracy. Most
similar to this paper is that of O’Connor et al.
(2010), in which tweets relating to President
Obama are retrieved with a keyword search and
a sentiment lexicon is used to measure overall
approval. This extracted approval ratio is then
compared to Gallup’s Presidential Job Approval
polling data. We directly compare their results
with various distantly supervised approaches.
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="method">
3 Datasets
</sectionHeader>
<bodyText confidence="0.9679885">
The experiments in this paper use seven months of
tweets from Twitter (www.twitter.com) collected
</bodyText>
<page confidence="0.998307">
604
</page>
<bodyText confidence="0.9994828125">
between June 1, 2009 and December 31, 2009.
The corpus contains over 476 million tweets la-
beled with usernames and timestamps, collected
through Twitter’s ‘spritzer’ API without keyword
filtering. Tweets are aligned with polling data in
Section 6 using their timestamps.
The full system is evaluated against the pub-
licly available daily Presidential Job Approval
polling data from Gallup1. Every day, Gallup asks
1,500 adults in the United States about whether
they approve or disapprove of “the job Presi-
dent Obama is doing as president.” The results
are compiled into two trend lines for Approval
and Disapproval ratings, as shown in Figure 1.
We compare our positive and negative sentiment
scores against these two trends.
</bodyText>
<sectionHeader confidence="0.995746" genericHeader="method">
4 Topic Identification
</sectionHeader>
<bodyText confidence="0.999978909090909">
This section addresses the task of Topic Identi-
fication in the context of microblogs. While the
general field of topic identification is broad, its
use on microblogs has been somewhat limited.
Previous work on the political domain simply uses
keywords to identify topic-specific tweets (e.g.,
O’Connor et al. (2010) use “Obama” to find pres-
idential tweets). This section shows that distant
supervision can use the same keywords to build a
classifier that is much more robust to noise than
approaches that use pure keyword search.
</bodyText>
<subsectionHeader confidence="0.990671">
4.1 Distant Supervision
</subsectionHeader>
<bodyText confidence="0.9999905">
Distant supervision uses noisy signals to identify
positive examples of a topic in the face of unla-
beled data. As described in Section 2, recent sen-
timent analysis work has applied distant supervi-
sion using emoticons as the signals. The approach
extracts tweets with ASCII smiley faces (e.g., :)
and ;)) and builds classifiers trained on these pos-
itive examples. We apply distant supervision to
topic identification and evaluate its effectiveness
on this subtask.
As with sentiment analysis, we need to collect
positive and negative examples of tweets about
the target topic. Instead of emoticons, we extract
positive tweets containing one or more predefined
keywords. Negative tweets are randomly chosen
from the corpus. Examples of positive and neg-
ative tweets that can be used to train a classifier
based on the keyword “Obama” are given here:
</bodyText>
<footnote confidence="0.8379575">
1http://gallup.com/poll/113980/gallup-daily-obama-job-
approval.aspx
</footnote>
<table confidence="0.9990102">
ID Type Keywords
PC-1 Obama obama
PC-2 General republican, democrat, senate,
congress, government
PC-3 Topic health care, economy, tax cuts,
tea party, bailout, sotomayor
PC-4 Politician obama, biden, mccain, reed,
pelosi, clinton, palin
PC-5 Ideology liberal, conservative, progres-
sive, socialist, capitalist
</table>
<tableCaption confidence="0.985259666666667">
Table 1: The keywords used to select positive training
sets for each political classifier (a subset of all PC-3
and PC-5 keywords are shown to conserve space).
</tableCaption>
<construct confidence="0.8565475">
positive: LOL, obama made a bears refer-
ence in green bay. uh oh.
negative: New blog up! It regards the new
iPhone 3G S: &lt;URL&gt;
</construct>
<bodyText confidence="0.999935428571429">
We then use these automatically extracted
datasets to train a multinomial Naive Bayes classi-
fier. Before feature collection, the text is normal-
ized as follows: (a) all links to photos (twitpics)
are replaced with a single generic token, (b) all
non-twitpic URLs are replaced with a token, (c)
all user references (e.g., @MyFriendBob) are col-
lapsed, (d) all numbers are collapsed to INT, (e)
tokens containing the same letter twice or more
in a row are condensed to a two-letter string (e.g.
the word ahhhhh becomes ahh), (f) lowercase the
text and insert spaces between words and punctu-
ation. The text of each tweet is then tokenized,
and the tokens are used to collect unigram and bi-
gram features. All features that occur fewer than
10 times in the training corpus are ignored.
Finally, after training a classifier on this dataset,
every tweet in the corpus is classified as either
positive (i.e., relevant to the topic) or negative
(i.e., irrelevant). The positive tweets are then sent
to the second sentiment analysis stage.
</bodyText>
<subsectionHeader confidence="0.993741">
4.2 Keyword Selection
</subsectionHeader>
<bodyText confidence="0.9998536">
Keywords are the input to our proposed distantly
supervised system, and of course, the input to pre-
vious work that relies on keyword search. We
evaluate classifiers based on different keywords to
measure the effects of keyword selection.
O’Connor et al. (2010) used the keywords
“Obama” and “McCain”, and Tumasjan et al.
(2010) simply extracted tweets containing Ger-
many’s political party names. Both approaches
extracted matching tweets, considered them rele-
</bodyText>
<page confidence="0.995144">
605
</page>
<note confidence="0.436784">
Gallup Daily Obama Job Approval Ratings
</note>
<figureCaption confidence="0.999722">
Figure 1: Gallup presidential job Approval and Disapproval ratings measured between June and Dec 2009.
</figureCaption>
<bodyText confidence="0.999460833333333">
vant (correctly, in many cases), and applied sen-
timent analysis. However, different keywords
may result in very different extractions. We in-
stead attempted to build a generic “political” topic
classifier. To do this, we experimented with the
five different sets of keywords shown in Table 1.
For each set, we extracted all tweets matching
one or more keywords, and created a balanced
positive/negative training set by then selecting
negative examples randomly from non-matching
tweets. A couple examples of ideology (PC-5) ex-
tractions are shown here:
</bodyText>
<construct confidence="0.9698865">
You often hear of deontologist libertarians
and utilitarian liberals but are there any
Aristotelian socialists?
&lt;url&gt; - Then, slather on a liberal amount
of plaster, sand down smooth, and paint
however you want. I hope this helps!
</construct>
<bodyText confidence="0.998829">
The second tweet is an example of the noisy
nature of keyword extraction. Most extractions
are accurate, but different keywords retrieve very
different sets of tweets. Examples for the political
topics (PC-3) are shown here:
</bodyText>
<construct confidence="0.979854166666667">
RT @PoliticalMath: hope the president’s
health care predictions &lt;url&gt; are better
than his stimulus predictions &lt;url&gt;
@adamjschmidt You mean we could have
chosen health care for every man woman
and child in America or the Iraq war?
</construct>
<bodyText confidence="0.8753565">
Each keyword set builds a classifier using the ap-
proach described in Section 4.1.
</bodyText>
<subsectionHeader confidence="0.999184">
4.3 Labeled Datasets
</subsectionHeader>
<bodyText confidence="0.99994662962963">
In order to evaluate distant supervision against
keyword search, we created two new labeled
datasets of political and apolitical tweets.
The Political Dataset is an amalgamation of all
four keyword extractions (PC-1 is a subset of PC-
4) listed in Table 1. It consists of 2,000 tweets ran-
domly chosen from the keyword searches of PC-
2, PC-3, PC-4, and PC-5 with 500 tweets from
each. This combined dataset enables an evalua-
tion of how well each classifier can identify tweets
from other classifiers. The General Dataset con-
tains 2,000 random tweets from the entire corpus.
This dataset allows us to evaluate how well clas-
sifiers identify political tweets in the wild.
This paper’s authors initially annotated the
same 200 tweets in the General Dataset to com-
pute inter-annotator agreement. The Kappa was
0.66, which is typically considered good agree-
ment. Most disagreements occurred over tweets
about money and the economy. We then split the
remaining portions of the two datasets between
the two annotators. The Political Dataset con-
tains 1,691 political and 309 apolitical tweets, and
the General Dataset contains 28 political tweets
and 1,978 apolitical tweets. These two datasets of
2000 tweets each are publicly available for future
evaluation and comparison to this work2.
</bodyText>
<subsectionHeader confidence="0.87608">
4.4 Experiments
</subsectionHeader>
<bodyText confidence="0.9999284375">
Our first experiment addresses the question of
keyword variance. We measure performance on
the Political Dataset, a combination of all of our
proposed political keywords. Each keyword set
contributed to 25% of the dataset, so the eval-
uation measures the extent to which a classifier
identifies other keyword tweets. We classified
the 2000 tweets with the five distantly supervised
classifiers and the one “Obama” keyword extrac-
tor from O’Connor et al. (2010).
Results are shown on the left side of Figure 2.
Precision and recall calculate correct identifica-
tion of the political label. The five distantly super-
vised approaches perform similarly, and show re-
markable robustness despite their different train-
ing sets. In contrast, the keyword extractor only
</bodyText>
<footnote confidence="0.981277">
2http://www.usna.edu/cs/nchamber/data/twitter
</footnote>
<page confidence="0.99254">
606
</page>
<figureCaption confidence="0.996459">
Figure 2: Five distantly supervised classifiers and the Obama keyword classifier. Left panel: the Political Dataset
of political tweets. Right panel: the General Dataset representative of Twitter as a whole.
</figureCaption>
<bodyText confidence="0.999834882352941">
captures about a quarter of the political tweets.
PC-1 is the distantly supervised analog to the
Obama keyword extractor, and we see that dis-
tant supervision increases its F1 score dramati-
cally from 0.39 to 0.90.
The second evaluation addresses the question
of classifier performance on Twitter as a whole,
not just on a political dataset. We evaluate on the
General Dataset just as on the Political Dataset.
Results are shown on the right side of Figure 2.
Most tweets posted to Twitter are not about pol-
itics, so the apolitical label dominates this more
representative dataset. Again, the five distant
supervision classifiers have similar results. The
Obama keyword search has the highest precision,
but drastically sacrifices recall. Four of the five
classifiers outperform keyword search in F1 score.
</bodyText>
<subsectionHeader confidence="0.859992">
4.5 Discussion
</subsectionHeader>
<bodyText confidence="0.9991775">
The Political Dataset results show that distant su-
pervision adds robustness to a keyword search.
The distantly supervised “Obama” classifier (PC-
1) improved the basic “Obama” keyword search
by 0.51 absolute F1 points. Furthermore, dis-
tant supervision doesn’t require additional human
input, but simply adds a trained classifier. Two
example tweets that an Obama keyword search
misses but that its distantly supervised analog
captures are shown here:
</bodyText>
<construct confidence="0.804013285714286">
Why does Congress get to opt out of the
Obummercare and we can’t. A company
gets fined if they don’t comply. Kiss free-
dom goodbye.
I agree with the lady from california, I am
sixty six years old and for the first time in
my life I am ashamed of our government.
</construct>
<bodyText confidence="0.999960652173913">
These results also illustrate that distant supervi-
sion allows for flexibility in construction of the
classifier. Different keywords show little change
in classifier performance.
The General Dataset experiment evaluates clas-
sifier performance in the wild. The keyword ap-
proach again scores below those trained on noisy
labels. It classifies most tweets as apolitical and
thus achieves very low recall for tweets that are
actually about politics. On the other hand, distant
supervision creates classifiers that over-extract
political tweets. This is a result of using balanced
datasets in training; such effects can be mitigated
by changing the training balance. Even so, four
of the five distantly trained classifiers score higher
than the raw keyword approach. The only under-
performer was PC-1, which suggests that when
building a classifier for a relatively broad topic
like politics, a variety of keywords is important.
The next section takes the output from our clas-
sifiers (i.e., our topic-relevant tweets) and eval-
uates a fully automated sentiment analysis algo-
rithm against real-world polling data.
</bodyText>
<sectionHeader confidence="0.9958" genericHeader="method">
5 Targeted Sentiment Analysis
</sectionHeader>
<bodyText confidence="0.999924875">
The previous section evaluated algorithms that
extract topic-relevant tweets. We now evaluate
methods to distill the overall sentiment that they
express. This section compares two common ap-
proaches to sentiment analysis.
We first replicated the technique used in
O’Connor et al. (2010), in which a lexicon of pos-
itive and negative sentiment words called Opin-
</bodyText>
<page confidence="0.992494">
607
</page>
<bodyText confidence="0.9999757">
ionFinder (Wilson and Hoffmann, 2005) is used
to evaluate the sentiment of each tweet (others
have used similar lexicons (Kramer, 2010; Thel-
wall et al., 2010)). We evaluate our full distantly
supervised approach to theirs. We also experi-
mented with SentiStrength, a lexicon-based pro-
gram built to identify sentiment in online com-
ments of the social media website, MySpace.
Though MySpace is close in genre to Twitter, we
did not observe a performance gain. All reported
results thus use OpinionFinder to facilitate a more
accurate comparison with previous work.
Second, we built a distantly supervised system
using tweets containing emoticons as done in pre-
vious work (Read, 2005; Go et al., 2009; Bifet and
Frank, 2010; Pak and Paroubek, 2010; Davidov
et al., 2010; Kouloumpis et al., 2011). Although
distant supervision has previously been shown to
outperform sentiment lexicons, these evaluations
do not consider the extra topic identification step.
</bodyText>
<subsectionHeader confidence="0.998403">
5.1 Sentiment Lexicon
</subsectionHeader>
<bodyText confidence="0.999945461538462">
The OpinionFinder lexicon is a list of 2,304 pos-
itive and 4,151 negative sentiment terms (Wilson
and Hoffmann, 2005). We ignore neutral words
in the lexicon and we do not differentiate between
weak and strong sentiment words. A tweet is la-
beled positive if it contains any positive terms, and
negative if it contains any negative terms. A tweet
can be marked as both positive and negative, and
if a tweet contains words in neither category, it
is marked neutral. This procedure is the same as
used by O’Connor et al. (2010). The sentiment
scores Spos and Sneg for a given set of N tweets
are calculated as follows:
</bodyText>
<equation confidence="0.955783">
E
x 1{xlabel = positive}
Spos = (1) N
S = Ex 1{xlabel = negative} (2)
pos N
</equation>
<bodyText confidence="0.9997818">
where 1{xlabel = positive} is 1 if the tweet x is
labeled positive, and N is the number of tweets in
the corpus. For the sake of comparison, we also
calculate a sentiment ratio as done in O’Connor
et al. (2010):
</bodyText>
<equation confidence="0.98505825">
E
Srati = x 1{xlabel = positive} (3)
o
Ex 1{xlabel = negative}
</equation>
<subsectionHeader confidence="0.986607">
5.2 Distant Supervision
</subsectionHeader>
<bodyText confidence="0.9808843125">
To build a trained classifier, we automatically gen-
erated a positive training set by searching for
tweets that contain at least one positive emoti-
con and no negative emoticons. We generated a
negative training set using an analogous process.
The emoticon symbols used for positive sentiment
were :) =) :-) :] =] :-] :} :o) :D =D :-D :P =P
:-P C:. Negative emoticons were :( =( :-( :[ =[
:-[ :{ :-c :c} D: D= :S :/=/ :-/ :’( : (. Using this
data, we train a multinomial Naive Bayes classi-
fier using the same method used for the political
classifiers described in Section 4.1. This classifier
is then used to label topic-specific tweets as ex-
pressing positive or negative sentiment. Finally,
the three overall sentiment scores Spos, Sneg, and
Sratio are calculated from the results.
</bodyText>
<sectionHeader confidence="0.953538" genericHeader="method">
6 Predicting Approval Polls
</sectionHeader>
<bodyText confidence="0.999985714285714">
This section uses the two-stage Targeted Senti-
ment Analysis system described above in a real-
world setting. We analyze the sentiment of Twit-
ter users toward U.S. President Barack Obama.
This allows us to both evaluate distant supervision
against previous work on the topic, and demon-
strate a practical application of the approach.
</bodyText>
<subsectionHeader confidence="0.997387">
6.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999960294117647">
The following experiments combine both topic
identification and sentiment analysis. The previ-
ous sections described six topic identification ap-
proaches, and two sentiment analysis approaches.
We evaluate all combinations of these systems,
and compare their final sentiment scores for each
day in the nearly seven-month period over which
our dataset spans.
Gallup’s Daily Job Approval reports two num-
bers: Approval and Disapproval. We calculate in-
dividual sentiment scores Spos and Sneg for each
day, and compare the two sets of trends using
Pearson’s correlation coefficient. O’Connor et al.
do not explicitly evaluate these two, but instead
use the ratio Sratio. We also calculate this daily
ratio from Gallup for comparison purposes by di-
viding the Approval by the Disapproval.
</bodyText>
<subsectionHeader confidence="0.969677">
6.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999512166666667">
The first set of results uses the lexicon-based clas-
sifier for sentiment analysis and compares the dif-
ferent topic identification approaches. The first
table in Table 2 reports Pearson’s correlation co-
efficient with Gallup’s Approval and Disapproval
ratings. Regardless of the Topic classifier, all
</bodyText>
<page confidence="0.996881">
608
</page>
<subsectionHeader confidence="0.553106">
Sentiment Lexicon
</subsectionHeader>
<tableCaption confidence="0.9979952">
Table 2: Correlation between Gallup polling data and
the extracted sentiment with a lexicon (trends shown
in Figure 3) and distant supervision (Figure 4).
Table 3: Correlation between Gallup Approval / Dis-
approval ratio and extracted sentiment ratio scores.
</tableCaption>
<bodyText confidence="0.999679589285715">
systems inversely correlate with Presidential Ap-
proval. However, they correlate well with Dis-
approval. Figure 3 graphically shows the trend
lines for the keyword and the distantly supervised
system PC-1. The visualization illustrates how
the keyword-based approach is highly influenced
by day-by-day changes, whereas PC-1 displays a
much smoother trend.
The second set of results uses distant supervi-
sion for sentiment analysis and again varies the
topic identification approach. The second table
in Table 2 gives the correlation numbers and Fig-
ure 4 shows the keyword and PC-1 trend lines.The
results are widely better than when a lexicon is
used for sentiment analysis. Approval is no longer
inversely correlated, and two of the distantly su-
pervised systems strongly correlate (PC-1, PC-5).
The best performing system (PC-1) used dis-
tant supervision for both topic identification and
sentiment analysis. Pearson’s correlation coeffi-
cient for this approach is 0.71 with Approval and
0.73 with Disapproval.
Finally, we compute the ratio S atzo between
the positive and negative sentiment scores (Equa-
tion 3) to compare to O’Connor et al. (2010). Ta-
ble 3 shows the results. The distantly supervised
topic identification algorithms show little change
between a sentiment lexicon or a classifier. How-
ever, O’Connor et al.’s keyword approach im-
proves when used with a distantly supervised sen-
timent classifier (.22 to .40). Merging Approval
and Disapproval into one ratio appears to mask
the sentiment lexicon’s poor correlation with Ap-
proval. The ratio may not be an ideal evalua-
tion metric for this reason. Real-world interest in
Presidential Approval ratings desire separate Ap-
proval and Disapproval scores, as Gallup reports.
Our results (Table 2) show that distant supervi-
sion avoids a negative correlation with Approval,
but the ratio hides this important advantage.
One reason the ratio may mask the negative
Approval correlation is because tweets are often
classified as both positive and negative by a lexi-
con (Section 5.1). This could explain the behav-
ior seen in Figure 3 in which both the positive and
negative sentiment scores rise over time. How-
ever, further experimentation did not rectify this
pattern. We revised Spo3 and S,,,g to make binary
decisions for a lexicon: a tweet is labeled posi-
tive if it strictly contains more positive words than
negative (and vice versa). Correlation showed lit-
tle change. Approval was still negatively corre-
lated, Disapproval positive (although less so in
both), and the ratio scores actually dropped fur-
ther. The sentiment ratio continued to hide the
poor Approval performance by a lexicon.
</bodyText>
<subsectionHeader confidence="0.99947">
6.3 New Baseline: Topic-Neutral Sentiment
</subsectionHeader>
<bodyText confidence="0.999505">
Distant supervision for sentiment analysis outper-
forms that with a sentiment lexicon (Table 2).
Distant supervision for topic identification further
improves the results (PC-1 v. keyword). The
best system uses distant supervision in both stages
(PC-1 with distantly supervised sentiment), out-
performing the purely keyword-based algorithm
of O’Connor et al. (2010). However, the question
of how important topic identification is has not yet
been addressed here or in the literature.
Both O’Connor et al. (2010) and Tumasjan et
al. (2010) created joint systems with two topic
identification and sentiment analysis stages. But
</bodyText>
<figure confidence="0.992645394366197">
0.42
0.71
0.71
0.65
0.60
0.74
-0.22
-0.65
-0.61
-0.51
-0.49
-0.65
keyword
PC-1
PC-2
PC-3
PC-4
PC-5
0.38
0.73
0.46
0.31
0.26
0.62
0.27
0.71
0.33
0.05
0.08
0.54
keyword
PC-1
PC-2
PC-3
PC-4
PC-5
Topic Classifier
Approval
Disapproval
Distantly Supervised Sentiment
Topic Classifier
Approval
Disapproval
.61
.27
.33
.46
.63
.22
.60
.28
.30
.46
.64
.40
Sentiment Lexicon
keyword
PC-1
PC-2
PC-3
PC-4
PC-5
Distantly Supervised Sentiment
keyword
PC-1
PC-2
PC-3
PC-4
PC-5
609
Sentiment Lexicon
</figure>
<figureCaption confidence="0.997012">
Figure 3: Presidential job approval and disapproval calculated using two different topic identification techniques,
and using a sentiment lexicon for sentiment analysis. Gallup polling results are shown in black.
</figureCaption>
<sectionHeader confidence="0.35125" genericHeader="method">
Distantly Supervised Sentiment
</sectionHeader>
<figureCaption confidence="0.9949">
Figure 4: Presidential job approval sentiment scores calculated using two different topic identification techniques,
and using the emoticon classifier for sentiment analysis. Gallup polling results are shown in black.
</figureCaption>
<figure confidence="0.495265">
Topic-Neutral Sentiment
</figure>
<figureCaption confidence="0.841501">
Figure 5: Presidential job approval sentiment scores calculated using the entire twitter corpus, with two different
techniques for sentiment analysis. Gallup polling results are shown in black for comparison.
</figureCaption>
<page confidence="0.985769">
610
</page>
<tableCaption confidence="0.740340666666667">
Topic-Neutral Sentiment
Table 4: Pearson’s correlation coefficient of Sentiment
Analysis without Topic Identification.
</tableCaption>
<bodyText confidence="0.999936615384615">
what if the topic identification step were removed
and sentiment analysis instead run on the entire
Twitter corpus? To answer this question, we
ran the distantly supervised emoticon classifier to
classify all tweets in the 7 months of Twitter data.
For each day, we computed the positive and neg-
ative sentiment scores as above. The evaluation is
identical, except for the removal of topic identifi-
cation. Correlation results are shown in Table 4.
This baseline parallels the results seen when
topic identification is used: the sentiment lexi-
con is again inversely correlated with Approval,
and distant supervision outperforms the lexicon
approach in both ratings. This is not surpris-
ing given previous distantly supervised work on
sentiment analysis (Go et al., 2009; Davidov et
al., 2010; Kouloumpis et al., 2011). However,
our distant supervision also performs as well as
the best performing topic-specific system. The
best performing topic classifier, PC-1, correlated
with Approval with r=0.71 (0.69 here) and Dis-
approval with r=0.73 (0.74 here). Computing
overall sentiment on Twitter performs as well as
political-specific sentiment. This unintuitive re-
sult suggests a new baseline that all topic-based
systems should compute.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999966517857143">
This paper introduces a new methodology for
gleaning topic-specific sentiment information.
We highlight four main contributions here.
First, this work is one of the first to evaluate
distant supervision for topic identification. All
five political classifiers outperformed the lexicon-
driven keyword equivalent that has been widely
used in the past. Our model achieved .90 F1 com-
pared to the keyword .39 F1 on our political tweet
dataset. On twitter as a whole, distant supervision
increased F1 by over 100%. The results also sug-
gest that performance is relatively insensitive to
the specific choice of seed keywords that are used
to select the training set for the political classifier.
Second, the sentiment analysis experiments
build upon what has recently been shown in the
literature: distant supervision with emoticons is
a valuable methodology. We also expand upon
prior work by discovering drastic performance
differences between positive and negative lexi-
con words. The OpinionFinder lexicon failed
to correlate (inversely) with Gallup’s Approval
polls, whereas a distantly trained classifier cor-
related strongly with both Approval and Disap-
proval (Pearson’s .71 and .73). We only tested
OpinionFinder and SentiStrength, so it is possible
that another lexicon might perform better. How-
ever, our results suggest that lexicons vary in their
quality across sentiment, and distant supervision
may provide more robustness.
Third, our results outperform previous work on
Presidential Job Approval prediction (O’Connor
et al., 2010). We presented two novel approaches
to the domain: a coupled distantly supervised sys-
tem, and a topic-neutral baseline, both of which
outperform previous results. In fact, the baseline
surprisingly matches or outperforms the more so-
phisticated approaches that use topic identifica-
tion. The baseline correlates .69 with Approval
and .74 with Disapproval. This suggests a new
baseline that should be used in all topic-specific
sentiment applications.
Fourth, we described and made available two
new annotated datasets of political tweets to facil-
itate future work in this area.
Finally, Twitter users are not a representative
sample of the U.S. population, yet the high corre-
lation between political sentiment on Twitter and
Gallup ratings makes these results all the more
intriguing for polling methodologies. Our spe-
cific 7-month period of time differs from previous
work, and thus we hesitate to draw strong con-
clusions from our comparisons or to extend im-
plications to non-political domains. Future work
should further investigate distant supervision as a
tool to assist topic detection in microblogs.
</bodyText>
<sectionHeader confidence="0.998822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999908666666667">
We thank Jure Leskovec for the Twitter data,
Brendan O’Connor for open and frank correspon-
dence, and the reviewers for helpful suggestions.
</bodyText>
<figure confidence="0.99899">
Algorithm
Approval
Disapproval
Distant Sup.
Keyword Lexicon
0.69
-0.63
0.74
0.69
</figure>
<page confidence="0.989526">
611
</page>
<sectionHeader confidence="0.992742" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999802011764706">
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics (COL-
ING 2010).
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Lecture
Notes in Computer Science, volume 6332, pages 1–
15.
Paula Carvalho, Luis Sarmento, Jorge Teixeira, and
Mario J. Silva. 2011. Liars and saviors in a senti-
ment annotated corpus of comments to political de-
bates. In Proceedings of the Association for Com-
putational Linguistics (ACL-2011), pages 564–568.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING 2010).
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervi-
sion. Technical report.
Sandra Gonzalez-Bailon, Rafael E. Banchs, and An-
dreas Kaltenbrunner. 2010. Emotional reactions
and the pulse of public opinion: Measuring the im-
pact of political events on the sentiment of online
discussions. Technical report.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the Associ-
ation for Computational Linguistics (ACL-2011).
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In Proceedings of the Fifth
International AAAI Conference on Weblogs and So-
cial Media.
Adam D. I. Kramer. 2010. An unobtrusive behavioral
model of ‘gross national happiness’. In Proceed-
ings of the 28th International Conference on Human
Factors in Computing Systems (CHI 2010).
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, ACL
’09, pages 1003–1011.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
AAAI Conference on Weblogs and Social Media.
Alexander Pak and Patrick Paroubek. 2010. Twitter
as a corpus for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh International
Conference On Language Resources and Evalua-
tion (LREC).
Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. In Proceedings of the ACL Stu-
dent Research Workshop (ACL-2005).
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment
analysis incorporating social networks. In Pro-
ceedings of the 17th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment
strength detection in short informal text. Journal of
the American Society for Information Science and
Technology, 61(12):2544–2558.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2011. Sentiment in twitter events. Jour-
nal of the American Society for Information Science
and Technology, 62(2):406–418.
Andranik Tumasjan, Timm O. Sprenger, Philipp G.
Sandner, and Isabell M. Welpe. 2010. Election
forecasts with twitter: How 140 characters reflect
the political landscape. Social Science Computer
Review.
J.; Wilson, T.; Wiebe and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the Conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing.
</reference>
<page confidence="0.997499">
612
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.255972">
<title confidence="0.9727085">Learning for Microblogs with Distant Political Forecasting with Twitter</title>
<author confidence="0.538173">Micol</author>
<affiliation confidence="0.888605">Microsoft</affiliation>
<address confidence="0.904387">475 Brannan San Francisco, CA</address>
<email confidence="0.99947">micolmb@microsoft.com</email>
<author confidence="0.906078">Nathanael</author>
<affiliation confidence="0.977625">Department of Computer United States Naval</affiliation>
<address confidence="0.674814">Annapolis, MD</address>
<email confidence="0.997936">nchamber@usna.edu</email>
<abstract confidence="0.999289137931034">Microblogging websites such as Twitter offer a wealth of insight into a population’s current mood. Automated approaches to identify general sentiment toward a particular topic often perform two Identification Topic Identification first identifies tweets that are relevant to a desired topic (e.g., a politician or event), and Sentiment Analysis extracts each tweet’s attitude toward the topic. Many techniques for Topic Identification simply involve selecting tweets using a keyword search. Here, we present an approach that instead uses distant supervision to train a classifier on the tweets returned by the search. We show that distant supervision leads to improved performance in the Topic Identification task as well in the downstream Sentiment Analysis stage. We then use a system that incorporates distant supervision into both stages to analyze the sentiment toward President Obama expressed in a dataset of tweets. Our results better correlate with Gallup’s Presidential Job Approval polls than previous work. Finally, we discover a surprising baseline that outperforms previous work without a Topic Identification stage.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="7185" citStr="Barbosa and Feng, 2010" startWordPosition="1137" endWordPosition="1140">opularity. These algorithms use emoticons to serve as semantic indicators for sentiment. For instance, a sad face (e.g., :-() serves as a noisy label for a negative mood. Read (2005) was the first to suggest emoticons for UseNet data, followed by Go et al. (Go et al., 2009) on Twitter, and many others since (Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Hashtags (e.g., #cool and #happy) have also been used as noisy sentiment labels (Davidov et al., 2010; Kouloumpis et al., 2011). Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). Here, we adopt the emoticon algorithm for sentiment analysis, and evaluate it on a specific domain (politics). Topic identification in Twitter has received much less attention than sentiment analysis. The majority of approaches simply select a single keyword (e.g., “Obama”) to represent their topic (e.g., “US President”) and retrieve all tweets that contain the word (O’Connor et al., 2010; Tumasjan et al., 2010; Tan et al., 2011). The underlying assumption is that the keyword is precise, and due to the vast number of tweets, the search will return a large enough dataset to measure sentiment </context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bifet</author>
<author>Eibe Frank</author>
</authors>
<title>Sentiment knowledge discovery in twitter streaming data.</title>
<date>2010</date>
<booktitle>In Lecture Notes in Computer Science,</booktitle>
<volume>6332</volume>
<pages>1--15</pages>
<contexts>
<context position="6893" citStr="Bifet and Frank, 2010" startWordPosition="1087" endWordPosition="1090">trengths. In contrast to lexicons, many approaches instead focus on ways to train supervised classifiers. However, labeled data is expensive to create, and examples of Twitter classifiers trained on hand-labeled data are few (Jiang et al., 2011). Instead, distant supervision has grown in popularity. These algorithms use emoticons to serve as semantic indicators for sentiment. For instance, a sad face (e.g., :-() serves as a noisy label for a negative mood. Read (2005) was the first to suggest emoticons for UseNet data, followed by Go et al. (Go et al., 2009) on Twitter, and many others since (Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Hashtags (e.g., #cool and #happy) have also been used as noisy sentiment labels (Davidov et al., 2010; Kouloumpis et al., 2011). Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). Here, we adopt the emoticon algorithm for sentiment analysis, and evaluate it on a specific domain (politics). Topic identification in Twitter has received much less attention than sentiment analysis. The majority of approaches simply select a single keyword (e.g., “Obama”) to represent their topic (e.g.,</context>
<context position="20520" citStr="Bifet and Frank, 2010" startWordPosition="3238" endWordPosition="3241"> used similar lexicons (Kramer, 2010; Thelwall et al., 2010)). We evaluate our full distantly supervised approach to theirs. We also experimented with SentiStrength, a lexicon-based program built to identify sentiment in online comments of the social media website, MySpace. Though MySpace is close in genre to Twitter, we did not observe a performance gain. All reported results thus use OpinionFinder to facilitate a more accurate comparison with previous work. Second, we built a distantly supervised system using tweets containing emoticons as done in previous work (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Although distant supervision has previously been shown to outperform sentiment lexicons, these evaluations do not consider the extra topic identification step. 5.1 Sentiment Lexicon The OpinionFinder lexicon is a list of 2,304 positive and 4,151 negative sentiment terms (Wilson and Hoffmann, 2005). We ignore neutral words in the lexicon and we do not differentiate between weak and strong sentiment words. A tweet is labeled positive if it contains any positive terms, and negative if it contains any negative terms. A tweet</context>
</contexts>
<marker>Bifet, Frank, 2010</marker>
<rawString>Albert Bifet and Eibe Frank. 2010. Sentiment knowledge discovery in twitter streaming data. In Lecture Notes in Computer Science, volume 6332, pages 1– 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula Carvalho</author>
<author>Luis Sarmento</author>
<author>Jorge Teixeira</author>
<author>Mario J Silva</author>
</authors>
<title>Liars and saviors in a sentiment annotated corpus of comments to political debates.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL-2011),</booktitle>
<pages>564--568</pages>
<contexts>
<context position="4734" citStr="Carvalho et al., 2011" startWordPosition="730" endWordPosition="733">training classifiers with a hand-labeled corpus. Since labeling corpora is expensive, recent work on Twitter uses emoticons (i.e., ASCII smiley faces such as :-( and :-)) as noisy labels in tweets for distant supervision (Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). This paper presents new analysis of the downstream effects of topic identification on sentiment classifiers and their application to political forecasting. Interest in measuring the political mood of a country has recently grown (O’Connor et al., 2010; Tumasjan et al., 2010; Gonzalez-Bailon et al., 2010; Carvalho et al., 2011; Tan et al., 2011). Here we compare our sentiment results to Presidential Job Approval polls and show that the sentiment scores produced by our system are positively correlated with both the Approval and Disapproval job ratings. In this paper we present a method for coupling two distantly supervised algorithms for topic identification and sentiment classification on Twitter. In Section 4, we describe our approach to topic identification and present a new annotated corpus of political tweets for future study. In Section 5, we apply distant supervision to sentiment analysis. Finally, Section 6 </context>
</contexts>
<marker>Carvalho, Sarmento, Teixeira, Silva, 2011</marker>
<rawString>Paula Carvalho, Luis Sarmento, Jorge Teixeira, and Mario J. Silva. 2011. Liars and saviors in a sentiment annotated corpus of comments to political debates. In Proceedings of the Association for Computational Linguistics (ACL-2011), pages 564–568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="4379" citStr="Davidov et al., 2010" startWordPosition="677" endWordPosition="680">cuses on two moods: positive and negative sentiment. 603 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 603–612, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics Algorithms to identify these moods range from matching words in a sentiment lexicon to training classifiers with a hand-labeled corpus. Since labeling corpora is expensive, recent work on Twitter uses emoticons (i.e., ASCII smiley faces such as :-( and :-)) as noisy labels in tweets for distant supervision (Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). This paper presents new analysis of the downstream effects of topic identification on sentiment classifiers and their application to political forecasting. Interest in measuring the political mood of a country has recently grown (O’Connor et al., 2010; Tumasjan et al., 2010; Gonzalez-Bailon et al., 2010; Carvalho et al., 2011; Tan et al., 2011). Here we compare our sentiment results to Presidential Job Approval polls and show that the sentiment scores produced by our system are positively correlated with both the Approval and Disapproval job ratings. In this paper w</context>
<context position="6939" citStr="Davidov et al., 2010" startWordPosition="1095" endWordPosition="1098">hes instead focus on ways to train supervised classifiers. However, labeled data is expensive to create, and examples of Twitter classifiers trained on hand-labeled data are few (Jiang et al., 2011). Instead, distant supervision has grown in popularity. These algorithms use emoticons to serve as semantic indicators for sentiment. For instance, a sad face (e.g., :-() serves as a noisy label for a negative mood. Read (2005) was the first to suggest emoticons for UseNet data, followed by Go et al. (Go et al., 2009) on Twitter, and many others since (Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Hashtags (e.g., #cool and #happy) have also been used as noisy sentiment labels (Davidov et al., 2010; Kouloumpis et al., 2011). Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). Here, we adopt the emoticon algorithm for sentiment analysis, and evaluate it on a specific domain (politics). Topic identification in Twitter has received much less attention than sentiment analysis. The majority of approaches simply select a single keyword (e.g., “Obama”) to represent their topic (e.g., “US President”) and retrieve all tweets that </context>
<context position="20566" citStr="Davidov et al., 2010" startWordPosition="3246" endWordPosition="3249">et al., 2010)). We evaluate our full distantly supervised approach to theirs. We also experimented with SentiStrength, a lexicon-based program built to identify sentiment in online comments of the social media website, MySpace. Though MySpace is close in genre to Twitter, we did not observe a performance gain. All reported results thus use OpinionFinder to facilitate a more accurate comparison with previous work. Second, we built a distantly supervised system using tweets containing emoticons as done in previous work (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Although distant supervision has previously been shown to outperform sentiment lexicons, these evaluations do not consider the extra topic identification step. 5.1 Sentiment Lexicon The OpinionFinder lexicon is a list of 2,304 positive and 4,151 negative sentiment terms (Wilson and Hoffmann, 2005). We ignore neutral words in the lexicon and we do not differentiate between weak and strong sentiment words. A tweet is labeled positive if it contains any positive terms, and negative if it contains any negative terms. A tweet can be marked as both positive and negative, </context>
<context position="29707" citStr="Davidov et al., 2010" startWordPosition="4686" endWordPosition="4689">ised emoticon classifier to classify all tweets in the 7 months of Twitter data. For each day, we computed the positive and negative sentiment scores as above. The evaluation is identical, except for the removal of topic identification. Correlation results are shown in Table 4. This baseline parallels the results seen when topic identification is used: the sentiment lexicon is again inversely correlated with Approval, and distant supervision outperforms the lexicon approach in both ratings. This is not surprising given previous distantly supervised work on sentiment analysis (Go et al., 2009; Davidov et al., 2010; Kouloumpis et al., 2011). However, our distant supervision also performs as well as the best performing topic-specific system. The best performing topic classifier, PC-1, correlated with Approval with r=0.71 (0.69 here) and Disapproval with r=0.73 (0.74 here). Computing overall sentiment on Twitter performs as well as political-specific sentiment. This unintuitive result suggests a new baseline that all topic-based systems should compute. 7 Discussion This paper introduces a new methodology for gleaning topic-specific sentiment information. We highlight four main contributions here. First, t</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<tech>Technical report.</tech>
<contexts>
<context position="6836" citStr="Go et al., 2009" startWordPosition="1077" endWordPosition="1080">ed counts of keywords based on predefined polarity strengths. In contrast to lexicons, many approaches instead focus on ways to train supervised classifiers. However, labeled data is expensive to create, and examples of Twitter classifiers trained on hand-labeled data are few (Jiang et al., 2011). Instead, distant supervision has grown in popularity. These algorithms use emoticons to serve as semantic indicators for sentiment. For instance, a sad face (e.g., :-() serves as a noisy label for a negative mood. Read (2005) was the first to suggest emoticons for UseNet data, followed by Go et al. (Go et al., 2009) on Twitter, and many others since (Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Hashtags (e.g., #cool and #happy) have also been used as noisy sentiment labels (Davidov et al., 2010; Kouloumpis et al., 2011). Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). Here, we adopt the emoticon algorithm for sentiment analysis, and evaluate it on a specific domain (politics). Topic identification in Twitter has received much less attention than sentiment analysis. The majority of approaches simply select a singl</context>
<context position="20497" citStr="Go et al., 2009" startWordPosition="3234" endWordPosition="3237">weet (others have used similar lexicons (Kramer, 2010; Thelwall et al., 2010)). We evaluate our full distantly supervised approach to theirs. We also experimented with SentiStrength, a lexicon-based program built to identify sentiment in online comments of the social media website, MySpace. Though MySpace is close in genre to Twitter, we did not observe a performance gain. All reported results thus use OpinionFinder to facilitate a more accurate comparison with previous work. Second, we built a distantly supervised system using tweets containing emoticons as done in previous work (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Although distant supervision has previously been shown to outperform sentiment lexicons, these evaluations do not consider the extra topic identification step. 5.1 Sentiment Lexicon The OpinionFinder lexicon is a list of 2,304 positive and 4,151 negative sentiment terms (Wilson and Hoffmann, 2005). We ignore neutral words in the lexicon and we do not differentiate between weak and strong sentiment words. A tweet is labeled positive if it contains any positive terms, and negative if it contains any </context>
<context position="29685" citStr="Go et al., 2009" startWordPosition="4682" endWordPosition="4685"> distantly supervised emoticon classifier to classify all tweets in the 7 months of Twitter data. For each day, we computed the positive and negative sentiment scores as above. The evaluation is identical, except for the removal of topic identification. Correlation results are shown in Table 4. This baseline parallels the results seen when topic identification is used: the sentiment lexicon is again inversely correlated with Approval, and distant supervision outperforms the lexicon approach in both ratings. This is not surprising given previous distantly supervised work on sentiment analysis (Go et al., 2009; Davidov et al., 2010; Kouloumpis et al., 2011). However, our distant supervision also performs as well as the best performing topic-specific system. The best performing topic classifier, PC-1, correlated with Approval with r=0.71 (0.69 here) and Disapproval with r=0.73 (0.74 here). Computing overall sentiment on Twitter performs as well as political-specific sentiment. This unintuitive result suggests a new baseline that all topic-based systems should compute. 7 Discussion This paper introduces a new methodology for gleaning topic-specific sentiment information. We highlight four main contri</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Gonzalez-Bailon</author>
<author>Rafael E Banchs</author>
<author>Andreas Kaltenbrunner</author>
</authors>
<title>Emotional reactions and the pulse of public opinion: Measuring the impact of political events on the sentiment of online discussions.</title>
<date>2010</date>
<tech>Technical report.</tech>
<contexts>
<context position="4711" citStr="Gonzalez-Bailon et al., 2010" startWordPosition="726" endWordPosition="729">rds in a sentiment lexicon to training classifiers with a hand-labeled corpus. Since labeling corpora is expensive, recent work on Twitter uses emoticons (i.e., ASCII smiley faces such as :-( and :-)) as noisy labels in tweets for distant supervision (Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). This paper presents new analysis of the downstream effects of topic identification on sentiment classifiers and their application to political forecasting. Interest in measuring the political mood of a country has recently grown (O’Connor et al., 2010; Tumasjan et al., 2010; Gonzalez-Bailon et al., 2010; Carvalho et al., 2011; Tan et al., 2011). Here we compare our sentiment results to Presidential Job Approval polls and show that the sentiment scores produced by our system are positively correlated with both the Approval and Disapproval job ratings. In this paper we present a method for coupling two distantly supervised algorithms for topic identification and sentiment classification on Twitter. In Section 4, we describe our approach to topic identification and present a new annotated corpus of political tweets for future study. In Section 5, we apply distant supervision to sentiment analys</context>
</contexts>
<marker>Gonzalez-Bailon, Banchs, Kaltenbrunner, 2010</marker>
<rawString>Sandra Gonzalez-Bailon, Rafael E. Banchs, and Andreas Kaltenbrunner. 2010. Emotional reactions and the pulse of public opinion: Measuring the impact of political events on the sentiment of online discussions. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Mo Yu</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Target-dependent twitter sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL-2011).</booktitle>
<contexts>
<context position="6517" citStr="Jiang et al., 2011" startWordPosition="1020" endWordPosition="1023">s appear. This approach tends to be used by applications that measure the general mood of a population. O’Connor et al. (2010) use a ratio of positive and negative word counts on Twitter, Kramer (2010) counts lexicon words on Facebook, and Thelwall (2011) uses the publicly available SentiStrength algorithm to make weighted counts of keywords based on predefined polarity strengths. In contrast to lexicons, many approaches instead focus on ways to train supervised classifiers. However, labeled data is expensive to create, and examples of Twitter classifiers trained on hand-labeled data are few (Jiang et al., 2011). Instead, distant supervision has grown in popularity. These algorithms use emoticons to serve as semantic indicators for sentiment. For instance, a sad face (e.g., :-() serves as a noisy label for a negative mood. Read (2005) was the first to suggest emoticons for UseNet data, followed by Go et al. (Go et al., 2009) on Twitter, and many others since (Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Hashtags (e.g., #cool and #happy) have also been used as noisy sentiment labels (Davidov et al., 2010; Kouloumpis et al., 2011). Finally, multiple mod</context>
</contexts>
<marker>Jiang, Yu, Zhou, Liu, Zhao, 2011</marker>
<rawString>Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment classification. In Proceedings of the Association for Computational Linguistics (ACL-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the omg!</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="4405" citStr="Kouloumpis et al., 2011" startWordPosition="681" endWordPosition="684">sitive and negative sentiment. 603 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 603–612, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics Algorithms to identify these moods range from matching words in a sentiment lexicon to training classifiers with a hand-labeled corpus. Since labeling corpora is expensive, recent work on Twitter uses emoticons (i.e., ASCII smiley faces such as :-( and :-)) as noisy labels in tweets for distant supervision (Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). This paper presents new analysis of the downstream effects of topic identification on sentiment classifiers and their application to political forecasting. Interest in measuring the political mood of a country has recently grown (O’Connor et al., 2010; Tumasjan et al., 2010; Gonzalez-Bailon et al., 2010; Carvalho et al., 2011; Tan et al., 2011). Here we compare our sentiment results to Presidential Job Approval polls and show that the sentiment scores produced by our system are positively correlated with both the Approval and Disapproval job ratings. In this paper we present a method for cou</context>
<context position="6965" citStr="Kouloumpis et al., 2011" startWordPosition="1099" endWordPosition="1102">ays to train supervised classifiers. However, labeled data is expensive to create, and examples of Twitter classifiers trained on hand-labeled data are few (Jiang et al., 2011). Instead, distant supervision has grown in popularity. These algorithms use emoticons to serve as semantic indicators for sentiment. For instance, a sad face (e.g., :-() serves as a noisy label for a negative mood. Read (2005) was the first to suggest emoticons for UseNet data, followed by Go et al. (Go et al., 2009) on Twitter, and many others since (Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Hashtags (e.g., #cool and #happy) have also been used as noisy sentiment labels (Davidov et al., 2010; Kouloumpis et al., 2011). Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). Here, we adopt the emoticon algorithm for sentiment analysis, and evaluate it on a specific domain (politics). Topic identification in Twitter has received much less attention than sentiment analysis. The majority of approaches simply select a single keyword (e.g., “Obama”) to represent their topic (e.g., “US President”) and retrieve all tweets that contain the word (O’Connor</context>
<context position="20592" citStr="Kouloumpis et al., 2011" startWordPosition="3250" endWordPosition="3253">luate our full distantly supervised approach to theirs. We also experimented with SentiStrength, a lexicon-based program built to identify sentiment in online comments of the social media website, MySpace. Though MySpace is close in genre to Twitter, we did not observe a performance gain. All reported results thus use OpinionFinder to facilitate a more accurate comparison with previous work. Second, we built a distantly supervised system using tweets containing emoticons as done in previous work (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Although distant supervision has previously been shown to outperform sentiment lexicons, these evaluations do not consider the extra topic identification step. 5.1 Sentiment Lexicon The OpinionFinder lexicon is a list of 2,304 positive and 4,151 negative sentiment terms (Wilson and Hoffmann, 2005). We ignore neutral words in the lexicon and we do not differentiate between weak and strong sentiment words. A tweet is labeled positive if it contains any positive terms, and negative if it contains any negative terms. A tweet can be marked as both positive and negative, and if a tweet contains wo</context>
<context position="29733" citStr="Kouloumpis et al., 2011" startWordPosition="4690" endWordPosition="4693">er to classify all tweets in the 7 months of Twitter data. For each day, we computed the positive and negative sentiment scores as above. The evaluation is identical, except for the removal of topic identification. Correlation results are shown in Table 4. This baseline parallels the results seen when topic identification is used: the sentiment lexicon is again inversely correlated with Approval, and distant supervision outperforms the lexicon approach in both ratings. This is not surprising given previous distantly supervised work on sentiment analysis (Go et al., 2009; Davidov et al., 2010; Kouloumpis et al., 2011). However, our distant supervision also performs as well as the best performing topic-specific system. The best performing topic classifier, PC-1, correlated with Approval with r=0.71 (0.69 here) and Disapproval with r=0.73 (0.74 here). Computing overall sentiment on Twitter performs as well as political-specific sentiment. This unintuitive result suggests a new baseline that all topic-based systems should compute. 7 Discussion This paper introduces a new methodology for gleaning topic-specific sentiment information. We highlight four main contributions here. First, this work is one of the fir</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the bad and the omg! In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam D I Kramer</author>
</authors>
<title>An unobtrusive behavioral model of ‘gross national happiness’.</title>
<date>2010</date>
<booktitle>In Proceedings of the 28th International Conference on Human Factors in Computing Systems (CHI</booktitle>
<contexts>
<context position="6099" citStr="Kramer (2010)" startWordPosition="957" endWordPosition="958">timent analysis grow into a diverse research area. The idea of sentiment applied to microblogging domains is relatively new, but there are numerous recent publications on the subject. Since this paper focuses on the microblog setting, we concentrate on these contributions here. The most straightforward approach to sentiment analysis is using a sentiment lexicon to label tweets based on how many sentiment words appear. This approach tends to be used by applications that measure the general mood of a population. O’Connor et al. (2010) use a ratio of positive and negative word counts on Twitter, Kramer (2010) counts lexicon words on Facebook, and Thelwall (2011) uses the publicly available SentiStrength algorithm to make weighted counts of keywords based on predefined polarity strengths. In contrast to lexicons, many approaches instead focus on ways to train supervised classifiers. However, labeled data is expensive to create, and examples of Twitter classifiers trained on hand-labeled data are few (Jiang et al., 2011). Instead, distant supervision has grown in popularity. These algorithms use emoticons to serve as semantic indicators for sentiment. For instance, a sad face (e.g., :-() serves as a</context>
<context position="19935" citStr="Kramer, 2010" startWordPosition="3145" endWordPosition="3146">uates a fully automated sentiment analysis algorithm against real-world polling data. 5 Targeted Sentiment Analysis The previous section evaluated algorithms that extract topic-relevant tweets. We now evaluate methods to distill the overall sentiment that they express. This section compares two common approaches to sentiment analysis. We first replicated the technique used in O’Connor et al. (2010), in which a lexicon of positive and negative sentiment words called Opin607 ionFinder (Wilson and Hoffmann, 2005) is used to evaluate the sentiment of each tweet (others have used similar lexicons (Kramer, 2010; Thelwall et al., 2010)). We evaluate our full distantly supervised approach to theirs. We also experimented with SentiStrength, a lexicon-based program built to identify sentiment in online comments of the social media website, MySpace. Though MySpace is close in genre to Twitter, we did not observe a performance gain. All reported results thus use OpinionFinder to facilitate a more accurate comparison with previous work. Second, we built a distantly supervised system using tweets containing emoticons as done in previous work (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Parou</context>
</contexts>
<marker>Kramer, 2010</marker>
<rawString>Adam D. I. Kramer. 2010. An unobtrusive behavioral model of ‘gross national happiness’. In Proceedings of the 28th International Conference on Human Factors in Computing Systems (CHI 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, ACL ’09,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="1946" citStr="Mintz et al., 2009" startWordPosition="288" endWordPosition="291">sidential Job Approval polls than previous work. Finally, we discover a surprising baseline that outperforms previous work without a Topic Identification stage. 1 Introduction Social networks and blogs contain a wealth of data about how the general public views products, campaigns, events, and people. Automated algorithms can use this data to provide instant feedback on what people are saying about a topic. Two challenges in building such algorithms are (1) identifying topic-relevant posts, and (2) identifying the attitude of each post toward the topic. This paper studies distant supervision (Mintz et al., 2009) as a solution to both challenges. We apply our approach to the problem of predicting Presidential Job Approval polls from Twitter data, and we present results that improve on previous work in this area. We also present a novel baseline that performs remarkably well without using topic identification. Topic identification is the task of identifying text that discusses a topic of interest. Most previous work on microblogs uses simple keyword searches to find topic-relevant tweets on the assumption that short tweets do not need more sophisticated processing. For instance, searches for the name “</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, ACL ’09, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of the AAAI Conference on Weblogs and Social Media.</booktitle>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In Proceedings of the AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference On Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="4357" citStr="Pak and Paroubek, 2010" startWordPosition="673" endWordPosition="676">t most microblog work focuses on two moods: positive and negative sentiment. 603 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 603–612, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics Algorithms to identify these moods range from matching words in a sentiment lexicon to training classifiers with a hand-labeled corpus. Since labeling corpora is expensive, recent work on Twitter uses emoticons (i.e., ASCII smiley faces such as :-( and :-)) as noisy labels in tweets for distant supervision (Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). This paper presents new analysis of the downstream effects of topic identification on sentiment classifiers and their application to political forecasting. Interest in measuring the political mood of a country has recently grown (O’Connor et al., 2010; Tumasjan et al., 2010; Gonzalez-Bailon et al., 2010; Carvalho et al., 2011; Tan et al., 2011). Here we compare our sentiment results to Presidential Job Approval polls and show that the sentiment scores produced by our system are positively correlated with both the Approval and Disapproval job ra</context>
<context position="6917" citStr="Pak and Paroubek, 2010" startWordPosition="1091" endWordPosition="1094">o lexicons, many approaches instead focus on ways to train supervised classifiers. However, labeled data is expensive to create, and examples of Twitter classifiers trained on hand-labeled data are few (Jiang et al., 2011). Instead, distant supervision has grown in popularity. These algorithms use emoticons to serve as semantic indicators for sentiment. For instance, a sad face (e.g., :-() serves as a noisy label for a negative mood. Read (2005) was the first to suggest emoticons for UseNet data, followed by Go et al. (Go et al., 2009) on Twitter, and many others since (Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Hashtags (e.g., #cool and #happy) have also been used as noisy sentiment labels (Davidov et al., 2010; Kouloumpis et al., 2011). Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). Here, we adopt the emoticon algorithm for sentiment analysis, and evaluate it on a specific domain (politics). Topic identification in Twitter has received much less attention than sentiment analysis. The majority of approaches simply select a single keyword (e.g., “Obama”) to represent their topic (e.g., “US President”) and ret</context>
<context position="20544" citStr="Pak and Paroubek, 2010" startWordPosition="3242" endWordPosition="3245">(Kramer, 2010; Thelwall et al., 2010)). We evaluate our full distantly supervised approach to theirs. We also experimented with SentiStrength, a lexicon-based program built to identify sentiment in online comments of the social media website, MySpace. Though MySpace is close in genre to Twitter, we did not observe a performance gain. All reported results thus use OpinionFinder to facilitate a more accurate comparison with previous work. Second, we built a distantly supervised system using tweets containing emoticons as done in previous work (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Although distant supervision has previously been shown to outperform sentiment lexicons, these evaluations do not consider the extra topic identification step. 5.1 Sentiment Lexicon The OpinionFinder lexicon is a list of 2,304 positive and 4,151 negative sentiment terms (Wilson and Hoffmann, 2005). We ignore neutral words in the lexicon and we do not differentiate between weak and strong sentiment words. A tweet is labeled positive if it contains any positive terms, and negative if it contains any negative terms. A tweet can be marked as both p</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of the Seventh International Conference On Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathon Read</author>
</authors>
<title>Using emoticons to reduce dependency in machine learning techniques for sentiment classification.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Student Research Workshop (ACL-2005).</booktitle>
<contexts>
<context position="6744" citStr="Read (2005)" startWordPosition="1060" endWordPosition="1061"> and Thelwall (2011) uses the publicly available SentiStrength algorithm to make weighted counts of keywords based on predefined polarity strengths. In contrast to lexicons, many approaches instead focus on ways to train supervised classifiers. However, labeled data is expensive to create, and examples of Twitter classifiers trained on hand-labeled data are few (Jiang et al., 2011). Instead, distant supervision has grown in popularity. These algorithms use emoticons to serve as semantic indicators for sentiment. For instance, a sad face (e.g., :-() serves as a noisy label for a negative mood. Read (2005) was the first to suggest emoticons for UseNet data, followed by Go et al. (Go et al., 2009) on Twitter, and many others since (Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Hashtags (e.g., #cool and #happy) have also been used as noisy sentiment labels (Davidov et al., 2010; Kouloumpis et al., 2011). Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). Here, we adopt the emoticon algorithm for sentiment analysis, and evaluate it on a specific domain (politics). Topic identification in Twitter has received m</context>
<context position="20480" citStr="Read, 2005" startWordPosition="3232" endWordPosition="3233">nt of each tweet (others have used similar lexicons (Kramer, 2010; Thelwall et al., 2010)). We evaluate our full distantly supervised approach to theirs. We also experimented with SentiStrength, a lexicon-based program built to identify sentiment in online comments of the social media website, MySpace. Though MySpace is close in genre to Twitter, we did not observe a performance gain. All reported results thus use OpinionFinder to facilitate a more accurate comparison with previous work. Second, we built a distantly supervised system using tweets containing emoticons as done in previous work (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Although distant supervision has previously been shown to outperform sentiment lexicons, these evaluations do not consider the extra topic identification step. 5.1 Sentiment Lexicon The OpinionFinder lexicon is a list of 2,304 positive and 4,151 negative sentiment terms (Wilson and Hoffmann, 2005). We ignore neutral words in the lexicon and we do not differentiate between weak and strong sentiment words. A tweet is labeled positive if it contains any positive terms, and negative if</context>
</contexts>
<marker>Read, 2005</marker>
<rawString>Jonathon Read. 2005. Using emoticons to reduce dependency in machine learning techniques for sentiment classification. In Proceedings of the ACL Student Research Workshop (ACL-2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenhao Tan</author>
<author>Lillian Lee</author>
<author>Jie Tang</author>
<author>Long Jiang</author>
<author>Ming Zhou</author>
<author>Ping Li</author>
</authors>
<title>User-level sentiment analysis incorporating social networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="4753" citStr="Tan et al., 2011" startWordPosition="734" endWordPosition="737">th a hand-labeled corpus. Since labeling corpora is expensive, recent work on Twitter uses emoticons (i.e., ASCII smiley faces such as :-( and :-)) as noisy labels in tweets for distant supervision (Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). This paper presents new analysis of the downstream effects of topic identification on sentiment classifiers and their application to political forecasting. Interest in measuring the political mood of a country has recently grown (O’Connor et al., 2010; Tumasjan et al., 2010; Gonzalez-Bailon et al., 2010; Carvalho et al., 2011; Tan et al., 2011). Here we compare our sentiment results to Presidential Job Approval polls and show that the sentiment scores produced by our system are positively correlated with both the Approval and Disapproval job ratings. In this paper we present a method for coupling two distantly supervised algorithms for topic identification and sentiment classification on Twitter. In Section 4, we describe our approach to topic identification and present a new annotated corpus of political tweets for future study. In Section 5, we apply distant supervision to sentiment analysis. Finally, Section 6 discusses our syste</context>
<context position="7620" citStr="Tan et al., 2011" startWordPosition="1206" endWordPosition="1209">ve also been used as noisy sentiment labels (Davidov et al., 2010; Kouloumpis et al., 2011). Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). Here, we adopt the emoticon algorithm for sentiment analysis, and evaluate it on a specific domain (politics). Topic identification in Twitter has received much less attention than sentiment analysis. The majority of approaches simply select a single keyword (e.g., “Obama”) to represent their topic (e.g., “US President”) and retrieve all tweets that contain the word (O’Connor et al., 2010; Tumasjan et al., 2010; Tan et al., 2011). The underlying assumption is that the keyword is precise, and due to the vast number of tweets, the search will return a large enough dataset to measure sentiment toward that topic. In this work, we instead use a distantly supervised system similar in spirit to those recently applied to sentiment analysis. Finally, we evaluate the approaches presented in this paper on the domain of politics. Tumasjan et al. (2010) showed that the results of a recent German election could be predicted through frequency counts with remarkable accuracy. Most similar to this paper is that of O’Connor et al. (201</context>
</contexts>
<marker>Tan, Lee, Tang, Jiang, Zhou, Li, 2011</marker>
<rawString>Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming Zhou, and Ping Li. 2011. User-level sentiment analysis incorporating social networks. In Proceedings of the 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
<author>Georgios Paltoglou</author>
<author>Di Cai</author>
<author>Arvid Kappas</author>
</authors>
<title>Sentiment strength detection in short informal text.</title>
<date>2010</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>61</volume>
<issue>12</issue>
<marker>Thelwall, Buckley, Paltoglou, Di Cai, Kappas, 2010</marker>
<rawString>Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Di Cai, and Arvid Kappas. 2010. Sentiment strength detection in short informal text. Journal of the American Society for Information Science and Technology, 61(12):2544–2558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
<author>Georgios Paltoglou</author>
</authors>
<title>Sentiment in twitter events.</title>
<date>2011</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>62</volume>
<issue>2</issue>
<marker>Thelwall, Buckley, Paltoglou, 2011</marker>
<rawString>Mike Thelwall, Kevan Buckley, and Georgios Paltoglou. 2011. Sentiment in twitter events. Journal of the American Society for Information Science and Technology, 62(2):406–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andranik Tumasjan</author>
<author>Timm O Sprenger</author>
<author>Philipp G Sandner</author>
<author>Isabell M Welpe</author>
</authors>
<title>Election forecasts with twitter: How 140 characters reflect the political landscape.</title>
<date>2010</date>
<journal>Social Science Computer Review.</journal>
<contexts>
<context position="4681" citStr="Tumasjan et al., 2010" startWordPosition="722" endWordPosition="725"> range from matching words in a sentiment lexicon to training classifiers with a hand-labeled corpus. Since labeling corpora is expensive, recent work on Twitter uses emoticons (i.e., ASCII smiley faces such as :-( and :-)) as noisy labels in tweets for distant supervision (Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). This paper presents new analysis of the downstream effects of topic identification on sentiment classifiers and their application to political forecasting. Interest in measuring the political mood of a country has recently grown (O’Connor et al., 2010; Tumasjan et al., 2010; Gonzalez-Bailon et al., 2010; Carvalho et al., 2011; Tan et al., 2011). Here we compare our sentiment results to Presidential Job Approval polls and show that the sentiment scores produced by our system are positively correlated with both the Approval and Disapproval job ratings. In this paper we present a method for coupling two distantly supervised algorithms for topic identification and sentiment classification on Twitter. In Section 4, we describe our approach to topic identification and present a new annotated corpus of political tweets for future study. In Section 5, we apply distant s</context>
<context position="7601" citStr="Tumasjan et al., 2010" startWordPosition="1201" endWordPosition="1205">., #cool and #happy) have also been used as noisy sentiment labels (Davidov et al., 2010; Kouloumpis et al., 2011). Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). Here, we adopt the emoticon algorithm for sentiment analysis, and evaluate it on a specific domain (politics). Topic identification in Twitter has received much less attention than sentiment analysis. The majority of approaches simply select a single keyword (e.g., “Obama”) to represent their topic (e.g., “US President”) and retrieve all tweets that contain the word (O’Connor et al., 2010; Tumasjan et al., 2010; Tan et al., 2011). The underlying assumption is that the keyword is precise, and due to the vast number of tweets, the search will return a large enough dataset to measure sentiment toward that topic. In this work, we instead use a distantly supervised system similar in spirit to those recently applied to sentiment analysis. Finally, we evaluate the approaches presented in this paper on the domain of politics. Tumasjan et al. (2010) showed that the results of a recent German election could be predicted through frequency counts with remarkable accuracy. Most similar to this paper is that of O</context>
<context position="12847" citStr="Tumasjan et al. (2010)" startWordPosition="2036" endWordPosition="2039">ing corpus are ignored. Finally, after training a classifier on this dataset, every tweet in the corpus is classified as either positive (i.e., relevant to the topic) or negative (i.e., irrelevant). The positive tweets are then sent to the second sentiment analysis stage. 4.2 Keyword Selection Keywords are the input to our proposed distantly supervised system, and of course, the input to previous work that relies on keyword search. We evaluate classifiers based on different keywords to measure the effects of keyword selection. O’Connor et al. (2010) used the keywords “Obama” and “McCain”, and Tumasjan et al. (2010) simply extracted tweets containing Germany’s political party names. Both approaches extracted matching tweets, considered them rele605 Gallup Daily Obama Job Approval Ratings Figure 1: Gallup presidential job Approval and Disapproval ratings measured between June and Dec 2009. vant (correctly, in many cases), and applied sentiment analysis. However, different keywords may result in very different extractions. We instead attempted to build a generic “political” topic classifier. To do this, we experimented with the five different sets of keywords shown in Table 1. For each set, we extracted al</context>
<context position="27529" citStr="Tumasjan et al. (2010)" startWordPosition="4364" endWordPosition="4367">the poor Approval performance by a lexicon. 6.3 New Baseline: Topic-Neutral Sentiment Distant supervision for sentiment analysis outperforms that with a sentiment lexicon (Table 2). Distant supervision for topic identification further improves the results (PC-1 v. keyword). The best system uses distant supervision in both stages (PC-1 with distantly supervised sentiment), outperforming the purely keyword-based algorithm of O’Connor et al. (2010). However, the question of how important topic identification is has not yet been addressed here or in the literature. Both O’Connor et al. (2010) and Tumasjan et al. (2010) created joint systems with two topic identification and sentiment analysis stages. But 0.42 0.71 0.71 0.65 0.60 0.74 -0.22 -0.65 -0.61 -0.51 -0.49 -0.65 keyword PC-1 PC-2 PC-3 PC-4 PC-5 0.38 0.73 0.46 0.31 0.26 0.62 0.27 0.71 0.33 0.05 0.08 0.54 keyword PC-1 PC-2 PC-3 PC-4 PC-5 Topic Classifier Approval Disapproval Distantly Supervised Sentiment Topic Classifier Approval Disapproval .61 .27 .33 .46 .63 .22 .60 .28 .30 .46 .64 .40 Sentiment Lexicon keyword PC-1 PC-2 PC-3 PC-4 PC-5 Distantly Supervised Sentiment keyword PC-1 PC-2 PC-3 PC-4 PC-5 609 Sentiment Lexicon Figure 3: Presidential job a</context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sandner, and Isabell M. Welpe. 2010. Election forecasts with twitter: How 140 characters reflect the political landscape. Social Science Computer Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wilson</author>
<author>T Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing.</booktitle>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>J.; Wilson, T.; Wiebe and P. Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>