<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998625">
Parsing Graphs with Hyperedge Replacement Grammars
</title>
<author confidence="0.997592">
David Chiang
</author>
<affiliation confidence="0.9946035">
Information Sciences Institute
University of Southern California
</affiliation>
<author confidence="0.996126">
Karl Moritz Hermann
</author>
<affiliation confidence="0.9984165">
Department of Computer Science
University of Oxford
</affiliation>
<author confidence="0.981188">
Jacob Andreas
</author>
<affiliation confidence="0.9952795">
Columbia University
University of Cambridge
</affiliation>
<author confidence="0.987405">
Bevan Jones
</author>
<affiliation confidence="0.990015">
University of Edinburgh
Macquarie University
</affiliation>
<author confidence="0.988239">
Daniel Bauer
</author>
<affiliation confidence="0.9965215">
Department of Computer Science
Columbia University
</affiliation>
<author confidence="0.995266">
Kevin Knight
</author>
<affiliation confidence="0.998054">
Information Sciences Institute
University of Southern California
</affiliation>
<sectionHeader confidence="0.989337" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999841352941177">
Hyperedge replacement grammar (HRG)
is a formalism for generating and trans-
forming graphs that has potential appli-
cations in natural language understand-
ing and generation. A recognition al-
gorithm due to Lautemann is known to
be polynomial-time for graphs that are
connected and of bounded degree. We
present a more precise characterization of
the algorithm’s complexity, an optimiza-
tion analogous to binarization of context-
free grammars, and some important im-
plementation details, resulting in an algo-
rithm that is practical for natural-language
applications. The algorithm is part of Boli-
nas, a new software toolkit for HRG pro-
cessing.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934769230769">
Hyperedge replacement grammar (HRG) is a
context-free rewriting formalism for generating
graphs (Drewes et al., 1997), and its synchronous
counterpart can be used for transforming graphs
to/from other graphs or trees. As such, it has great
potential for applications in natural language un-
derstanding and generation, and semantics-based
machine translation (Jones et al., 2012). Fig-
ure 1 shows some examples of graphs for natural-
language semantics.
A polynomial-time recognition algorithm for
HRGs was described by Lautemann (1990), build-
ing on the work of Rozenberg and Welzl (1986)
on boundary node label controlled grammars, and
others have presented polynomial-time algorithms
as well (Mazanek and Minas, 2008; Moot, 2008).
Although Lautemann’s algorithm is correct and
tractable, its presentation is prefaced with the re-
mark: “As we are only interested in distinguish-
ing polynomial time from non-polynomial time,
the analysis will be rather crude, and implemen-
tation details will be explicated as little as possi-
ble.” Indeed, the key step of the algorithm, which
matches a rule against the input graph, is described
at a very high level, so that it is not obvious (for a
non-expert in graph algorithms) how to implement
it. More importantly, this step as described leads
to a time complexity that is polynomial, but poten-
tially of very high degree.
In this paper, we describe in detail a more effi-
cient version of this algorithm and its implementa-
tion. We give a more precise complexity analysis
in terms of the grammar and the size and maxi-
mum degree of the input graph, and we show how
to optimize it by a process analogous to binariza-
tion of CFGs, following Gildea (2011). The re-
sulting algorithm is practical and is implemented
as part of the open-source Bolinas toolkit for hy-
peredge replacement grammars.
</bodyText>
<sectionHeader confidence="0.966862" genericHeader="method">
2 Hyperedge replacement grammars
</sectionHeader>
<bodyText confidence="0.9992535">
We give a short example of how HRG works, fol-
lowed by formal definitions.
</bodyText>
<subsectionHeader confidence="0.899022">
2.1 Example
</subsectionHeader>
<bodyText confidence="0.9987356">
Consider a weighted graph language involving just
two types of semantic frames (want and believe),
two types of entities (boy and girl), and two roles
(arg0 and arg1). Figure 1 shows a few graphs from
this language.
Figure 2 shows how to derive one of these
graphs using an HRG. The derivation starts with
a single edge labeled with the nonterminal sym-
bol S. The first rewriting step replaces this edge
with a subgraph, which we might read as “The
</bodyText>
<page confidence="0.965129">
924
</page>
<note confidence="0.9584445">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 924–932,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.975554">
Figure 1: Sample members of a graph language,
</figureCaption>
<bodyText confidence="0.869725363636364">
representing the meanings of (clockwise from up-
per left): “The girl wants the boy,” “The boy is
believed,” and “The boy wants the girl to believe
that he wants her.”
boy wants something (X) involving himself.” The
second rewriting step replaces the X edge with an-
other subgraph, which we might read as “The boy
wants the girl to believe something (Y) involving
both of them.” The derivation continues with a
third rewriting step, after which there are no more
nonterminal-labeled edges.
</bodyText>
<subsectionHeader confidence="0.994169">
2.2 Definitions
</subsectionHeader>
<bodyText confidence="0.8955495">
The graphs we use in this paper have edge labels,
but no node labels; while node labels are intu-
itive for many graphs in NLP, using both node and
edge labels complicates the definition of hyper-
edge grammar and algorithms. All of our graphs
are directed (ordered), as the purpose of most
graph structures in NLP is to model dependencies
between entities.
Definition 1. An edge-labeled, ordered hyper-
graph is a tuple H = (V, E, f), where
</bodyText>
<listItem confidence="0.9034954">
• V is a finite set of nodes
• E c V+ is a finite set of hyperedges, each of
which connects one or more distinct nodes
• f : E → C assigns a label (drawn from the
finite set C) to each edge.
</listItem>
<bodyText confidence="0.994544333333333">
For brevity we use the terms graph and hyper-
graph interchangeably, and similarly for edge and
hyperedge. In the definition of HRGs, we will use
the notion of hypergraph fragments, which are the
elementary structures that the grammar assembles
into hypergraphs.
</bodyText>
<equation confidence="0.5320825">
Definition 2. A hypergraph fragment is a tuple
(V, E, f, X), where (V, E, f) is a hypergraph and
X E V+ is a list of distinct nodes called the ex-
ternal nodes.
</equation>
<bodyText confidence="0.984210166666667">
The function of graph fragments in HRG is
analogous to the right-hand sides of CFG rules
and to elementary trees in tree adjoining gram-
mars (Joshi and Schabes, 1997). The external
nodes indicate how to integrate a graph into an-
other graph during a derivation, and are analogous
</bodyText>
<listItem confidence="0.9081779">
to foot nodes. In diagrams, we draw them with a
black circle ( ).
Definition 3. A hyperedge replacement grammar
(HRG) is a tuple G = (N, T, P, S) where
• N and T are finite disjoint sets of nonterminal
and terminal symbols
• S E N is the start symbol
• P is a finite set of productions of the form
A → R, where A E N and R is a graph frag-
ment over N U T.
</listItem>
<bodyText confidence="0.998501833333333">
We now describe the HRG rewriting mecha-
nism.
Definition 4. Given a HRG G, we define the re-
lation H ⇒G H′ (or, H′ is derived from H in one
step) as follows. Let e = (v1 · · · vk) be an edge in
H with label A. Let (A → R) be a production of G,
where R has external nodes XR = (u1 · · · uk). Then
we write H ⇒G H′ if H′ is the graph formed by
removing e from H, making an isomorphic copy
of R, and identifying vi with (the copy of) ui for
i = 1,...,k.
Let H ⇒∗G H′ (or, H′ is derived from H) be the
reflexive, transitive closure of ⇒G. The graph lan-
guage of a grammar G is the (possibly infinite) set
of graphs H that have no edges with nonterminal
labels such that
When a HRG rule (A → R) is applied to an
edge e, the mapping of external nodes in R to the
</bodyText>
<figure confidence="0.983567882352941">
arg1
arg0
believe′ arg1
want′
girl′
boy′
boy′
want′ arg1
believe′ arg1
arg0
want′
arg0
arg1
girl′
arg0
boy′
S ⇒∗G H.
</figure>
<page confidence="0.905008">
925
</page>
<figureCaption confidence="0.9793085">
Figure 2: Derivation of a hyperedge replacement grammar for a graph representing the meaning of “The
boy wants the girl to believe that he wants her.”
</figureCaption>
<figure confidence="0.998380119047619">
1 2
2
1
boy′
boy′
want′ arg1
believe′ arg1
S
want′
arg1
1
arg0
X
arg1
3
boy′
arg0
Y
arg0
want′
arg0
girl′
arg0
girl′
arg1
arg0
want′ arg1
2 believe′
arg1
arg0
want′
Y
→
believe′
→
arg0
arg1
Y
1
girl′
X
1
</figure>
<bodyText confidence="0.999170304347826">
nodes of e is implied by the ordering of nodes
in e and XR. When writing grammar rules, we
make this ordering explicit by writing the left hand
side of a rule as an edge and indexing the external
nodes of R on both sides, as shown in Figure 2.
HRG derivations are context-free in the sense
that the applicability of each production depends
on the nonterminal label of the replaced edge only.
This allows us to represent a derivation as a deriva-
tion tree, and sets of derivations of a graph as a
derivation forest (which can in turn represented as
hypergraphs). Thus we can apply many of the
methods developed for other context free gram-
mars. For example, it is easy to define weighted
and synchronous versions of HRGs.
Definition 5. If K is a semiring, a K-weighted
HRG is a tuple G = (N, T, P, S, d), where
(N, T, P, S) is a HRG and d : P → K assigns a
weight in K to each production. The weight of a
derivation of G is the product of the weights of the
productions used in the derivation.
We defer a definition of synchronous HRGs un-
til Section 4, where they are discussed in detail.
</bodyText>
<sectionHeader confidence="0.991376" genericHeader="method">
3 Parsing
</sectionHeader>
<bodyText confidence="0.9999535">
Lautemann’s recognition algorithm for HRGs is a
generalization of the CKY algorithm for CFGs.
Its key step is the matching of a rule against the
input graph, analogous to the concatenation of
two spans in CKY. The original description leaves
open how this matching is done, and because it
tries to match the whole rule at once, it has asymp-
totic complexity exponential in the number of non-
terminal edges. In this section, we present a re-
finement that makes the rule-matching procedure
explicit, and because it matches rules little by lit-
tle, similarly to binarization of CFG rules, it does
so more efficiently than the original.
Let H be the input graph. Let n be the number of
nodes in H, and d be the maximum degree of any
node. Let G be a HRG. For simplicity, we assume
that the right-hand sides of rules are connected.
This restriction entails that each graph generated
by G is connected; therefore, we assume that H is
connected as well. Finally, let m be an arbitrary
node of H called the marker node, whose usage
will become clear below.1
</bodyText>
<subsectionHeader confidence="0.999712">
3.1 Representing subgraphs
</subsectionHeader>
<bodyText confidence="0.9995175">
Just as CKY deals with substrings (i, j] of the in-
put, the HRG parsing algorithm deals with edge-
induced subgraphs I of the input. An edge-
induced subgraph of H = (V, E, f) is, for some
</bodyText>
<footnote confidence="0.8538405">
1To handle the more general case where H is not con-
nected, we would need a marker for each component.
</footnote>
<page confidence="0.99612">
926
</page>
<bodyText confidence="0.999651461538462">
subset E′ c E, the smallest subgraph containing
all edges in E′. From now on, we will assume that
all subgraphs are edge-induced subgraphs.
In CKY, the two endpoints i and j com-
pletely specify the recognized part of the input,
wi+1 · · · wj. Likewise, we do not need to store all
of I explicitly.
Definition 6. Let I be a subgraph of H. A bound-
ary node of I is a node in I which is either a node
with an edge in H\I or an external node. A bound-
ary edge of I is an edge in I which has a boundary
node as an endpoint. The boundary representation
of I is the tuple (bn(I), be(I, v), m E I), where
</bodyText>
<listItem confidence="0.98010725">
• bn(I) is the set of boundary nodes of I
• be(I, v) be the set of boundary edges of v in I
• (m E I) is a flag indicating whether the
marker node is in I.
</listItem>
<bodyText confidence="0.8863165">
The boundary representation of I suffices to
specify I compactly.
</bodyText>
<construct confidence="0.997264">
Proposition 1. If I and I′ are two subgraphs of H
with the same boundary representation, then I =
I′.
</construct>
<listItem confidence="0.93513575">
Proof. Case 1: bn(I) is empty. If m E I and m E I′,
then all edges of H must belong to both I and I′,
that is, I = I′ = H. Otherwise, if m V I and m V I′,
then no edges can belong to either I or I′, that is,
</listItem>
<equation confidence="0.939035">
I = I′ = 0.
</equation>
<bodyText confidence="0.985660266666667">
Case 2: bn(I) is nonempty. Suppose I # I′;
without loss of generality, suppose that there is an
edge e that is in I \ I′. Let π be the shortest path
(ignoring edge direction) that begins with e and
ends with a boundary node. All the edges along π
must be in I \I′, or else there would be a boundary
node in the middle of π, and π would not be the
shortest path from e to a boundary node. Then, in
particular, the last edge of π must be in I\I′. Since
it has a boundary node as an endpoint, it must be a
boundary edge of I, but cannot be a boundary edge
of I′, which is a contradiction. ❑
If two subgraphs are disjoint, we can use their
boundary representations to compute the boundary
representation of their union.
</bodyText>
<construct confidence="0.996979636363636">
Proposition 2. Let I and J be two subgraphs
whose edges are disjoint. A node v is a boundary
node of I U J iff one of the following holds:
(i) v is a boundary node of one subgraph but not
the other
(ii) v is a boundary node of both subgraphs, and
has an edge which is not a boundary edge of
either.
An edge is a boundary edge of I U J iff it has a
boundary node of I U J as an endpoint and is a
boundary edge of I or J.
</construct>
<bodyText confidence="0.976644388888889">
Proof. (=&gt;) v has an edge in either I or J and an
edge e outside both I and J. Therefore it must be a
boundary node of either I or J. Moreover, e is not
a boundary edge of either, satisfying condition (ii).
(&lt;-=) Case (i): without loss of generality, assume
v is a boundary node of I. It has an edge e in I, and
therefore in I U J, and an edge e′ outside I, which
must also be outside J. For e V J (because I and
J are disjoint), and if e′ E J, then v would be a
boundary node of J. Therefore, e′ V I U J, so v is
a boundary node of I U J. Case (ii): v has an edge
in I and therefore I U J, and an edge not in either
I or J. ❑
This result leads to Algorithm 1, which runs in
time linear in the number of boundary nodes.
Algorithm 1 Compute the union of two disjoint
subgraphs I and J.
for all v E bn(I) do
</bodyText>
<equation confidence="0.689824">
E (-- be(I, v) U be(J, v)
</equation>
<bodyText confidence="0.509985833333333">
if v V bn(J) or v has an edge not in E then
add v to bn(I U J)
be(I U J, v) (-- E
for all v E bn(J) do
if v V bn(I) then
add v to bn(I U J)
</bodyText>
<equation confidence="0.7205475">
be(I U J, v) (-- be(I, v) U be(J, v)
(m E I U J) (-- (m E I) V (m E J)
</equation>
<bodyText confidence="0.999908333333333">
In practice, for small subgraphs, it may be more
efficient simply to use an explicit set of edges in-
stead of the boundary representation. For the Geo-
Query corpus (Tang and Mooney, 2001), whose
graphs are only 7.4 nodes on average, we gener-
ally find this to be the case.
</bodyText>
<subsectionHeader confidence="0.988725">
3.2 Treewidth
</subsectionHeader>
<bodyText confidence="0.999979">
Lautemann’s algorithm tries to match a rule
against the input graph all at once. But we can op-
timize the algorithm by matching a rule incremen-
tally. This is analogous to the rank-minimization
problem for linear context-free rewriting systems.
Gildea has shown that this problem is related to
</bodyText>
<page confidence="0.985222">
927
</page>
<bodyText confidence="0.8702445">
the notion of treewidth (Gildea, 2011), which we
review briefly here.
</bodyText>
<listItem confidence="0.84272525">
Definition 7. A tree decomposition of a graph
H = (V, E) is a tree T, each of whose nodes η
is associated with sets Vη c V and Eη c E, with
the following properties:
1. Vertex cover: For each v E V, there is a node
η E T such that v E Vη.
2. Edge cover: For each e = (v1 • • • vk) E E,
there is exactly one node η E T such that e E
Eη. We say that η introduces e. Moreover,
v1,...,vk E Vη.
3. Running intersection: For each v E V, the set
1η E T  |v E Vη1 is connected.
</listItem>
<bodyText confidence="0.9998204">
The width of T is max |Vη |−1. The treewidth of H
is the minimal width of any tree decomposition
of H.
A tree decomposition of a graph fragment
(V, E, X) is a tree decomposition of (V, E) that has
the additional property that all the external nodes
belong to Vη for some η. (Without loss of general-
ity, we assume that η is the root.)
For example, Figure 3b shows a graph, and Fig-
ure 3c shows a tree decomposition. This decom-
position has width three, because its largest node
has 4 elements. In general, a tree has width one,
and it can be shown that a graph has treewidth at
most two iff it does not have the following graph
as a minor (Bodlaender, 1997):
</bodyText>
<table confidence="0.682587333333333">
method mean max
exact 1.491 2
approximate 1.494 3
</table>
<tableCaption confidence="0.826026666666667">
Table 1: Mean and maximum treewidths of rules
extracted from the GeoQuery corpus, using exact
and approximate methods.
</tableCaption>
<figure confidence="0.974669857142857">
a
Y
b 1
girl′
0 0
(a) 0
believe′
arg1
a
Y
arg0
b
1
girl′
0 arg1
a
b 1
0
1
believe′
0
b
0
0
0
arg0
K4 = Figure 3: (a) A rule right-hand side, and (b) a nice
tree decomposition.
</figure>
<bodyText confidence="0.999422">
Finding a tree decomposition with minimal
width is in general NP-hard (Arnborg et al., 1987).
However, we find that for the graphs we are inter-
ested in in NLP applications, even a naive algo-
rithm gives tree decompositions of low width in
practice: simply perform a depth-first traversal of
the edges of the graph, forming a tree T. Then,
augment the Vη as necessary to satisfy the running
intersection property.
As a test, we extracted rules from the Geo-
Query corpus (Tang and Mooney, 2001) using the
SYNSEM algorithm (Jones et al., 2012), and com-
puted tree decompositions exactly using a branch-
and-bound method (Gogate and Dechter, 2004)
and this approximate method. Table 1 shows that,
in practice, treewidths are not very high even when
computed only approximately.
Any tree decomposition can be converted into
one which is nice in the following sense (simpli-
fied from Cygan et al. (2011)). Each tree node η
must be one of:
</bodyText>
<listItem confidence="0.99995175">
• A leaf node, such that Vη = 0.
• A unary node, which introduces exactly one
edge e.
• A binary node, which introduces no edges.
</listItem>
<bodyText confidence="0.999752">
The example decomposition in Figure 3c is nice.
This canonical form simplifies the operation of the
parser described in the following section.
Let G be a HRG. For each production (A �
R) E G, find a nice tree decomposition of R and
call it TR. The treewidth of G is the maximum
</bodyText>
<page confidence="0.995879">
928
</page>
<bodyText confidence="0.99882">
treewidth of any right-hand side in G.
The basic idea of the recognition algorithm is
to recognize the right-hand side of each rule incre-
mentally by working bottom-up on its tree decom-
position. The properties of tree decomposition al-
low us to limit the number of boundary nodes of
the partially-recognized rule.
More formally, let R&gt;q be the subgraph of R in-
duced by the union of Eq′ for all q′ equal to or
dominated by q. Then we can show the following.
</bodyText>
<construct confidence="0.679143333333333">
Proposition 3. Let R be a graph fragment, and as-
sume a tree decomposition of R. All the boundary
nodes of R&gt;q belong to Vq n Vparent(q).
</construct>
<bodyText confidence="0.9974702">
Proof. Let v be a boundary node of R&gt;q. Node v
must have an edge in R&gt;q and therefore in Rq′ for
some q′ dominated by or equal to q.
Case 1: v is an external node. Since the root
node contains all the external nodes, by the run-
ning intersection property, both Vq and Vparent(q)
must contain v as well.
Case 2: v has an edge not in R&gt;q. Therefore
there must be a tree node not dominated by or
equal to q that contains this edge, and therefore
v. So by the running intersection property, q and
its parent must contain v as well. ❑
This result, in turn, will allow us to bound the
complexity of the parsing algorithm in terms of the
treewidth of G.
</bodyText>
<subsectionHeader confidence="0.998172">
3.3 Inference rules
</subsectionHeader>
<bodyText confidence="0.987528875">
We present the parsing algorithm as a deductive
system (Shieber et al., 1995). The items have
one of two forms. A passive item has the form
[A, I, X], where X E V+ is an explicit ordering
of the boundary nodes of I. This means that we
have recognized that A =&gt;* I. Thus, the goal
item is [S, H, E]. An active item has the form
[A → R, q, I, 0], where
</bodyText>
<listItem confidence="0.9825696">
• (A → R) is a production of G
• q is a node of TR
• I is a subgraph of H
• 0 is a bijection between the boundary nodes
of R&gt;q and those of I.
</listItem>
<bodyText confidence="0.957974">
The parser must ensure that 0 is a bijection when
it creates a new item. Below, we use the notation
{e H e′1 or {e H X1 for the mapping that sends
each node of e to the corresponding node of e′
or X.
Passive items are generated by the following
rule:
</bodyText>
<listItem confidence="0.993331">
• Root
</listItem>
<equation confidence="0.953718">
[B → Q, θ, J, to]
[B, J, X]
</equation>
<bodyText confidence="0.995552">
where θ is the root of TQ, and Xj = to(XQ,j).
If we assume that the TR are nice, then the in-
ference rules that generate active items follow the
different types of nodes in a nice tree decomposi-
tion:
</bodyText>
<listItem confidence="0.989098">
• Leaf
</listItem>
<equation confidence="0.876838">
[A → R, q, 0, 0]
</equation>
<bodyText confidence="0.919357">
where q is a leaf node of TR.
</bodyText>
<listItem confidence="0.925268">
• (Unary) Nonterminal
</listItem>
<equation confidence="0.990505">
[A → R,q1, I, 0] [B, J, X]
[A → R,q, I U J,0 U {e H X1]
</equation>
<bodyText confidence="0.989959">
where q1 is the only child of q, and e is intro-
duced by q and is labeled with nonterminal B.
</bodyText>
<listItem confidence="0.974108">
• (Unary) Terminal
</listItem>
<equation confidence="0.9917635">
[A → R,q1, I, 0]
[A → R,q, I U {e′1,0 U {e H e′1]
</equation>
<bodyText confidence="0.998304666666667">
where q1 is the only child of q, e is introduced
by q, and e and e′ are both labeled with ter-
minal a.
</bodyText>
<listItem confidence="0.983634">
• Binary
</listItem>
<equation confidence="0.993215">
[A → R,q1, I,01] [A → R,q2, J,02]
[A → R,q, I U J, 01 U 02]
</equation>
<bodyText confidence="0.999897125">
where q1 and q2 are the two children of q.
In the Nonterminal, Terminal, and Binary rules,
we form unions of subgraphs and unions of map-
pings. When forming the union of two subgraphs,
we require that the subgraphs be disjoint (however,
see Section 3.4 below for a relaxation of this con-
dition). When forming the union of two mappings,
we require that the result be a bijection. If either
of these conditions is not met, the inference rule
cannot apply.
For efficiency, it is important to index the items
for fast access. For the Nonterminal inference
rule, passive items [B, J, X] should be indexed by
key (B, |bn(J)|), so that when the next item on the
agenda is an active item [A → R, q1, I, 0], we
know that all possible matching passive items are
</bodyText>
<page confidence="0.996942">
929
</page>
<figureCaption confidence="0.986072">
Figure 4: Illustration of unsoundness in the recog-
</figureCaption>
<bodyText confidence="0.965339055555556">
nition algorithm without the disjointness check.
Using grammar (a), the recognition algorithm
would incorrectly accept the graph (b) by assem-
bling together the three overlapping fragments (c).
under key hℓ(e), |e|i. Similarly, active items should
be indexed by key hℓ(e), |e|i so that they can be
found when the next item on the agenda is a pas-
sive item. For the Binary inference rule, active
items should be indexed by their tree node (η1
or η2).
This procedure can easily be extended to pro-
duce a packed forest of all possible derivations
of the input graph, representable as a hypergraph
just as for other context-free rewriting formalisms.
The Viterbi algorithm can then be applied to
this representation to find the highest-probability
derivation, or the Inside/Outside algorithm to set
weights by Expectation-Maximization.
</bodyText>
<subsectionHeader confidence="0.997335">
3.4 The disjointness check
</subsectionHeader>
<bodyText confidence="0.999972375">
A successful proof using the inference rules above
builds an HRG derivation (comprising all the
rewrites used by the Nonterminal rule) which de-
rives a graph H′, as well as a graph isomorphism
φ : H′ → H (the union of the mappings from all
the items).
During inference, whenever we form the union
of two subgraphs, we require that the subgraphs
be disjoint. This is a rather expensive operation:
it can be done using only their boundary represen-
tations, but the best algorithm we are aware of is
still quadratic in the number of boundary nodes.
Is it possible to drop the disjointness check? If
we did so, it would become possible for the algo-
rithm to recognize the same part of H twice. For
example, Figure 4 shows an example of a grammar
and an input that would be incorrectly recognized.
However, we can replace the disjointness check
with a weaker and faster check such that any
derivation that merges two non-disjoint subgraphs
will ultimately fail, and therefore the derived
graph H′ is isomorphic to the input graph H′ as
desired. This weaker check is to require, when
merging two subgraphs I and J, that:
</bodyText>
<listItem confidence="0.996671">
1. I and J have no boundary edges in common,
and
2. If m belongs to both I and J, it must be a
boundary node of both.
</listItem>
<bodyText confidence="0.989444277777778">
Condition (1) is enough to guarantee that φ is lo-
cally one-to-one in the sense that for all v ∈ H′, φ
restricted to v and its neighbors is one-to-one. This
is easy to show by induction: if φI : I′ → H and
φJ : J′ → H are locally one-to-one, then φI ∪ φJ
must also be, provided condition (1) is met. Intu-
itively, the consequence of this is that we can de-
tect any place where φ changes (say) from being
one-to-one to two-to-one. So if φ is two-to-one,
then it must be two-to-one everywhere (as in the
example of Figure 4).
But condition (2) guarantees that φ maps only
one node to the marker m. We can show this again
by induction: if φI and φJ each map only one node
to m, then φI ∪φJ must map only one node to m, by
a combination of condition (2) and the fact that the
inference rules guarantee that φI, φJ, and φI ∪ φJ
are one-to-one on boundary nodes.
Then we can show that, since m is recognized
exactly once, the whole graph is also recognized
exactly once.
Proposition 4. If H and H′ are connected graphs,
φ : H′ → H is locally one-to-one, and φ−1 is de-
fined for some node of H, then φ is a bijection.
Proof. Suppose that φ is not a bijection. Then
there must be two nodes v′1, v′2 ∈ H′ such that
φ(v′1) = φ(v′2) = v ∈ H. We also know that there
is a node, namely, m, such that m′ = φ−1(m) is de-
fined.2 Choose a path π (ignoring edge direction)
from v to m. Because φ is a local isomorphism,
we can construct a path from v′1 to m′ that maps
to π. Similarly, we can construct a path from v′ 2
to m′ that maps to π. Let u′ be the first node that
these two paths have in common. But u′ must have
two edges that map to the same edge, which is a
contradiction. ❑
</bodyText>
<footnote confidence="0.8007435">
2If H were not connected, we would choose the marker in
the same connected component as v.
</footnote>
<figure confidence="0.9984115625">
X
a
X
→
X
X →
a
(a) (b)
a
a a a
a a
(c)
S
a
a
a
</figure>
<page confidence="0.95778">
930
</page>
<sectionHeader confidence="0.521826" genericHeader="method">
3.5 Complexity
</sectionHeader>
<bodyText confidence="0.999977571428572">
The key to the efficiency of the algorithm is that
the treewidth of G leads to a bound on the number
of boundary nodes we must keep track of at any
time.
Let k be the treewidth of G. The time complex-
ity of the algorithm is the number of ways of in-
stantiating the inference rules. Each inference rule
mentions only boundary nodes of R&gt;η or R&gt;ηi, all
of which belong to Vη (by Proposition 3), so there
are at most |Vη |≤ k + 1 of them. In the Nonter-
minal and Binary inference rules, each boundary
edge could belong to I or J or neither. Therefore,
the number of possible instantiations of any infer-
ence rule is in O((3dn)k+1).
The space complexity of the algorithm is the
number of possible items. For each active item
[A → R, η, I, φ], every boundary node of R&gt;η must
belong to Vη ∩ Vparent(η) (by Proposition 3). There-
fore the number of boundary nodes is at most k +1
(but typically less), and the number of possible
items is in O((2dn)k+1).
</bodyText>
<sectionHeader confidence="0.99559" genericHeader="method">
4 Synchronous Parsing
</sectionHeader>
<bodyText confidence="0.965458285714286">
As mentioned in Section 2.2, because HRGs have
context-free derivation trees, it is easy to define
synchronous HRGs, which define mappings be-
tween languages of graphs.
Definition 8. A synchronous hyperedge re-
placement grammar (SHRG) is a tuple G =
hN, T, T′, P, Si, where
</bodyText>
<listItem confidence="0.9834305">
• N is a finite set of nonterminal symbols
• T and T′ are finite sets of terminal symbols
• S ∈ N is the start symbol
• P is a finite set of productions of the form
</listItem>
<bodyText confidence="0.973835">
(A → hR, R′, ∼i), where R is a graph fragment
over N ∪ T and R′ is a graph fragment over
N ∪ T′. The relation ∼ is a bijection linking
nonterminal mentions in R and R′, such that
if e ∼ e′, then they have the same label. We
call R the source side and R′ the target side.
Some NLP applications (for example, word
alignment) require synchronous parsing: given a
pair of graphs, finding the derivation or forest of
derivations that simultaneously generate both the
source and target. The algorithm to do this is a
straightforward generalization of the HRG parsing
algorithm. For each rule (A → hR, R′, ∼i), we con-
struct a nice tree decomposition of R∪R′ such that:
</bodyText>
<listItem confidence="0.929477">
• All the external nodes of both R and R′ be-
long to Vη for some η. (Without loss of gen-
erality, assume that η is the root.)
• If e ∼ e′, then e and e′ are introduced by the
same tree node.
</listItem>
<bodyText confidence="0.999475">
In the synchronous parsing algorithm, passive
items have the form [A, I, X, I′, X′] and active
items have the form [A → R : R′, η, I, φ, I′, φ′].
For brevity we omit a re-presentation of all the in-
ference rules, as they are very similar to their non-
synchronous counterparts. The main difference is
that in the Nonterminal rule, two linked edges are
rewritten simultaneously:
</bodyText>
<equation confidence="0.992024666666667">
[A → R : R′,η1, I,φ, I′,φ′] [B, J, X, J′, X′]
[A → R : R′,η, I ∪ J,φ ∪ {ej 7→ Xj},
I′ ∪ J′, φ′ ∪ {e′j 7→ X′j}]
</equation>
<bodyText confidence="0.9999672">
where η1 is the only child of η, e and e′ are both
introduced by η and e ∼ e′, and both are labeled
with nonterminal B.
The complexity of the parsing algorithm is
again in O((3dn)k+1), where k is now the max-
imum treewidth of the dependency graph as de-
fined in this section. In general, this treewidth will
be greater than the treewidth of either the source or
target side on its own, so that synchronous parsing
is generally slower than standard parsing.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999698375">
Although Lautemann’s polynomial-time extension
of CKY to HRGs has been known for some time,
the desire to use graph grammars for large-scale
NLP applications introduces some practical con-
siderations not accounted for in Lautemann’s orig-
inal presentation. We have provided a detailed de-
scription of our refinement of his algorithm and its
implementation. It runs in O((3dn)k+1) time and
requires O((2dn)k+1) space, where n is the num-
ber of nodes in the input graph, d is its maximum
degree, and k is the maximum treewidth of the
rule right-hand sides in the grammar. We have
also described how to extend this algorithm to
synchronous parsing. The parsing algorithms de-
scribed in this paper are implemented in the Boli-
nas toolkit.3
</bodyText>
<footnote confidence="0.9846245">
3The Bolinas toolkit can be downloaded from
hhttp://www.isi.edu/licensed-sw/bolinas/i.
</footnote>
<page confidence="0.997213">
931
</page>
<sectionHeader confidence="0.999104" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999917333333333">
We would like to thank the anonymous reviewers
for their helpful comments. This research was sup-
ported in part by ARO grant W911NF-10-1-0533.
</bodyText>
<sectionHeader confidence="0.998475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999931406779661">
Stefan Arnborg, Derek G. Corneil, and Andrzej
Proskurowski. 1987. Complexity of finding embed-
dings in a k-tree. SIAM Journal on Algebraic and
Discrete Methods, 8(2).
Hans L. Bodlaender. 1997. Treewidth: Algorithmic
techniques and results. In Proc. 22nd International
Symposium on Mathematical Foundations of Com-
puter Science (MFCS ’97), pages 29–36, Berlin.
Springer-Verlag.
Marek Cygan, Jesper Nederlof, Marcin Pilipczuk,
Michał Pilipczuk, Johan M. M. van Rooij, and
Jakub Onufry Wojtaszczyk. 2011. Solving connec-
tivity problems parameterized by treewidth in single
exponential time. Computing Research Repository,
abs/1103.0534.
Frank Drewes, Hans-J¨org Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement graph gram-
mars. In Grzegorz Rozenberg, editor, Handbook of
Graph Grammars and Computing by Graph Trans-
formation, pages 95–162. World Scientific.
Daniel Gildea. 2011. Grammar factorization by
tree decomposition. Computational Linguistics,
37(1):231–248.
Vibhav Gogate and Rina Dechter. 2004. A complete
anytime algorithm for treewidth. In Proceedings of
the Conference on Uncertainty in Artificial Intelli-
gence.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In Proc. COLING.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages and Automata, volume 3, pages 69–124.
Springer.
Clemens Lautemann. 1990. The complexity of
graph languages generated by hyperedge replace-
ment. Acta Informatica, 27:399–421.
Steffen Mazanek and Mark Minas. 2008. Parsing of
hyperedge replacement grammars with graph parser
combinators. In Proc. 7th International Work-
shop on Graph Transformation and Visual Modeling
Techniques.
Richard Moot. 2008. Lambek grammars, tree ad-
joining grammars and hyperedge replacement gram-
mars. In Proc. TAG+9, pages 65–72.
Grzegorz Rozenberg and Emo Welzl. 1986. Bound-
ary NLC graph grammars—basic definitions, nor-
mal forms, and complexity. Information and Con-
trol, 69:136–167.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3–36.
Lappoon Tang and Raymond Mooney. 2001. Using
multiple clause constructors in inductive logic pro-
gramming for semantic parsing. In Proc. European
Conference on Machine Learning.
</reference>
<page confidence="0.997125">
932
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.327382">
<title confidence="0.999997">Parsing Graphs with Hyperedge Replacement Grammars</title>
<author confidence="0.999986">David Chiang</author>
<affiliation confidence="0.9981415">Information Sciences University of Southern California</affiliation>
<author confidence="0.998754">Karl Moritz</author>
<affiliation confidence="0.913713125">Department of Computer University of Oxford Jacob Columbia University of Cambridge Bevan University of Macquarie University</affiliation>
<author confidence="0.999839">Daniel Bauer</author>
<affiliation confidence="0.99994">Department of Computer Columbia University</affiliation>
<author confidence="0.999558">Kevin Knight</author>
<affiliation confidence="0.997152">Information Sciences University of Southern California</affiliation>
<abstract confidence="0.987639055555556">Hyperedge replacement grammar (HRG) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation. A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithm’s complexity, an optimization analogous to binarization of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language The algorithm is part of Bolia new software toolkit for HRG processing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefan Arnborg</author>
<author>Derek G Corneil</author>
<author>Andrzej Proskurowski</author>
</authors>
<title>Complexity of finding embeddings in a k-tree.</title>
<date>1987</date>
<journal>SIAM Journal on Algebraic and Discrete Methods,</journal>
<volume>8</volume>
<issue>2</issue>
<contexts>
<context position="15140" citStr="Arnborg et al., 1987" startWordPosition="2901" endWordPosition="2904">de has 4 elements. In general, a tree has width one, and it can be shown that a graph has treewidth at most two iff it does not have the following graph as a minor (Bodlaender, 1997): method mean max exact 1.491 2 approximate 1.494 3 Table 1: Mean and maximum treewidths of rules extracted from the GeoQuery corpus, using exact and approximate methods. a Y b 1 girl′ 0 0 (a) 0 believe′ arg1 a Y arg0 b 1 girl′ 0 arg1 a b 1 0 1 believe′ 0 b 0 0 0 arg0 K4 = Figure 3: (a) A rule right-hand side, and (b) a nice tree decomposition. Finding a tree decomposition with minimal width is in general NP-hard (Arnborg et al., 1987). However, we find that for the graphs we are interested in in NLP applications, even a naive algorithm gives tree decompositions of low width in practice: simply perform a depth-first traversal of the edges of the graph, forming a tree T. Then, augment the Vη as necessary to satisfy the running intersection property. As a test, we extracted rules from the GeoQuery corpus (Tang and Mooney, 2001) using the SYNSEM algorithm (Jones et al., 2012), and computed tree decompositions exactly using a branchand-bound method (Gogate and Dechter, 2004) and this approximate method. Table 1 shows that, in p</context>
</contexts>
<marker>Arnborg, Corneil, Proskurowski, 1987</marker>
<rawString>Stefan Arnborg, Derek G. Corneil, and Andrzej Proskurowski. 1987. Complexity of finding embeddings in a k-tree. SIAM Journal on Algebraic and Discrete Methods, 8(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans L Bodlaender</author>
</authors>
<title>Treewidth: Algorithmic techniques and results.</title>
<date>1997</date>
<booktitle>In Proc. 22nd International Symposium on Mathematical Foundations of Computer Science (MFCS ’97),</booktitle>
<pages>29--36</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin.</location>
<contexts>
<context position="14701" citStr="Bodlaender, 1997" startWordPosition="2815" endWordPosition="2816">eewidth of H is the minimal width of any tree decomposition of H. A tree decomposition of a graph fragment (V, E, X) is a tree decomposition of (V, E) that has the additional property that all the external nodes belong to Vη for some η. (Without loss of generality, we assume that η is the root.) For example, Figure 3b shows a graph, and Figure 3c shows a tree decomposition. This decomposition has width three, because its largest node has 4 elements. In general, a tree has width one, and it can be shown that a graph has treewidth at most two iff it does not have the following graph as a minor (Bodlaender, 1997): method mean max exact 1.491 2 approximate 1.494 3 Table 1: Mean and maximum treewidths of rules extracted from the GeoQuery corpus, using exact and approximate methods. a Y b 1 girl′ 0 0 (a) 0 believe′ arg1 a Y arg0 b 1 girl′ 0 arg1 a b 1 0 1 believe′ 0 b 0 0 0 arg0 K4 = Figure 3: (a) A rule right-hand side, and (b) a nice tree decomposition. Finding a tree decomposition with minimal width is in general NP-hard (Arnborg et al., 1987). However, we find that for the graphs we are interested in in NLP applications, even a naive algorithm gives tree decompositions of low width in practice: simpl</context>
</contexts>
<marker>Bodlaender, 1997</marker>
<rawString>Hans L. Bodlaender. 1997. Treewidth: Algorithmic techniques and results. In Proc. 22nd International Symposium on Mathematical Foundations of Computer Science (MFCS ’97), pages 29–36, Berlin. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marek Cygan</author>
<author>Jesper Nederlof</author>
<author>Marcin Pilipczuk</author>
<author>Michał Pilipczuk</author>
<author>Johan M M van Rooij</author>
<author>Jakub Onufry Wojtaszczyk</author>
</authors>
<title>Solving connectivity problems parameterized by treewidth in single exponential time. Computing Research Repository,</title>
<date>2011</date>
<marker>Cygan, Nederlof, Pilipczuk, Pilipczuk, van Rooij, Wojtaszczyk, 2011</marker>
<rawString>Marek Cygan, Jesper Nederlof, Marcin Pilipczuk, Michał Pilipczuk, Johan M. M. van Rooij, and Jakub Onufry Wojtaszczyk. 2011. Solving connectivity problems parameterized by treewidth in single exponential time. Computing Research Repository, abs/1103.0534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Drewes</author>
<author>Hans-J¨org Kreowski</author>
<author>Annegret Habel</author>
</authors>
<title>Hyperedge replacement graph grammars.</title>
<date>1997</date>
<booktitle>In Grzegorz Rozenberg, editor, Handbook of Graph Grammars and Computing by Graph Transformation,</booktitle>
<pages>95--162</pages>
<publisher>World Scientific.</publisher>
<contexts>
<context position="1236" citStr="Drewes et al., 1997" startWordPosition="169" endWordPosition="172">anding and generation. A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithm’s complexity, an optimization analogous to binarization of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing. 1 Introduction Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes et al., 1997), and its synchronous counterpart can be used for transforming graphs to/from other graphs or trees. As such, it has great potential for applications in natural language understanding and generation, and semantics-based machine translation (Jones et al., 2012). Figure 1 shows some examples of graphs for naturallanguage semantics. A polynomial-time recognition algorithm for HRGs was described by Lautemann (1990), building on the work of Rozenberg and Welzl (1986) on boundary node label controlled grammars, and others have presented polynomial-time algorithms as well (Mazanek and Minas, 2008; Mo</context>
</contexts>
<marker>Drewes, Kreowski, Habel, 1997</marker>
<rawString>Frank Drewes, Hans-J¨org Kreowski, and Annegret Habel. 1997. Hyperedge replacement graph grammars. In Grzegorz Rozenberg, editor, Handbook of Graph Grammars and Computing by Graph Transformation, pages 95–162. World Scientific.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Grammar factorization by tree decomposition.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="2798" citStr="Gildea (2011)" startWordPosition="428" endWordPosition="429">h matches a rule against the input graph, is described at a very high level, so that it is not obvious (for a non-expert in graph algorithms) how to implement it. More importantly, this step as described leads to a time complexity that is polynomial, but potentially of very high degree. In this paper, we describe in detail a more efficient version of this algorithm and its implementation. We give a more precise complexity analysis in terms of the grammar and the size and maximum degree of the input graph, and we show how to optimize it by a process analogous to binarization of CFGs, following Gildea (2011). The resulting algorithm is practical and is implemented as part of the open-source Bolinas toolkit for hyperedge replacement grammars. 2 Hyperedge replacement grammars We give a short example of how HRG works, followed by formal definitions. 2.1 Example Consider a weighted graph language involving just two types of semantic frames (want and believe), two types of entities (boy and girl), and two roles (arg0 and arg1). Figure 1 shows a few graphs from this language. Figure 2 shows how to derive one of these graphs using an HRG. The derivation starts with a single edge labeled with the nonterm</context>
<context position="13546" citStr="Gildea, 2011" startWordPosition="2571" endWordPosition="2572">actice, for small subgraphs, it may be more efficient simply to use an explicit set of edges instead of the boundary representation. For the GeoQuery corpus (Tang and Mooney, 2001), whose graphs are only 7.4 nodes on average, we generally find this to be the case. 3.2 Treewidth Lautemann’s algorithm tries to match a rule against the input graph all at once. But we can optimize the algorithm by matching a rule incrementally. This is analogous to the rank-minimization problem for linear context-free rewriting systems. Gildea has shown that this problem is related to 927 the notion of treewidth (Gildea, 2011), which we review briefly here. Definition 7. A tree decomposition of a graph H = (V, E) is a tree T, each of whose nodes η is associated with sets Vη c V and Eη c E, with the following properties: 1. Vertex cover: For each v E V, there is a node η E T such that v E Vη. 2. Edge cover: For each e = (v1 • • • vk) E E, there is exactly one node η E T such that e E Eη. We say that η introduces e. Moreover, v1,...,vk E Vη. 3. Running intersection: For each v E V, the set 1η E T |v E Vη1 is connected. The width of T is max |Vη |−1. The treewidth of H is the minimal width of any tree decomposition of</context>
</contexts>
<marker>Gildea, 2011</marker>
<rawString>Daniel Gildea. 2011. Grammar factorization by tree decomposition. Computational Linguistics, 37(1):231–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vibhav Gogate</author>
<author>Rina Dechter</author>
</authors>
<title>A complete anytime algorithm for treewidth.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="15686" citStr="Gogate and Dechter, 2004" startWordPosition="2994" endWordPosition="2997">e decomposition with minimal width is in general NP-hard (Arnborg et al., 1987). However, we find that for the graphs we are interested in in NLP applications, even a naive algorithm gives tree decompositions of low width in practice: simply perform a depth-first traversal of the edges of the graph, forming a tree T. Then, augment the Vη as necessary to satisfy the running intersection property. As a test, we extracted rules from the GeoQuery corpus (Tang and Mooney, 2001) using the SYNSEM algorithm (Jones et al., 2012), and computed tree decompositions exactly using a branchand-bound method (Gogate and Dechter, 2004) and this approximate method. Table 1 shows that, in practice, treewidths are not very high even when computed only approximately. Any tree decomposition can be converted into one which is nice in the following sense (simplified from Cygan et al. (2011)). Each tree node η must be one of: • A leaf node, such that Vη = 0. • A unary node, which introduces exactly one edge e. • A binary node, which introduces no edges. The example decomposition in Figure 3c is nice. This canonical form simplifies the operation of the parser described in the following section. Let G be a HRG. For each production (A</context>
</contexts>
<marker>Gogate, Dechter, 2004</marker>
<rawString>Vibhav Gogate and Rina Dechter. 2004. A complete anytime algorithm for treewidth. In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan Jones</author>
<author>Jacob Andreas</author>
<author>Daniel Bauer</author>
<author>Karl Moritz Hermann</author>
<author>Kevin Knight</author>
</authors>
<title>Semantics-based machine translation with hyperedge replacement grammars.</title>
<date>2012</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="1496" citStr="Jones et al., 2012" startWordPosition="207" endWordPosition="210">on of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing. 1 Introduction Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes et al., 1997), and its synchronous counterpart can be used for transforming graphs to/from other graphs or trees. As such, it has great potential for applications in natural language understanding and generation, and semantics-based machine translation (Jones et al., 2012). Figure 1 shows some examples of graphs for naturallanguage semantics. A polynomial-time recognition algorithm for HRGs was described by Lautemann (1990), building on the work of Rozenberg and Welzl (1986) on boundary node label controlled grammars, and others have presented polynomial-time algorithms as well (Mazanek and Minas, 2008; Moot, 2008). Although Lautemann’s algorithm is correct and tractable, its presentation is prefaced with the remark: “As we are only interested in distinguishing polynomial time from non-polynomial time, the analysis will be rather crude, and implementation detai</context>
<context position="15586" citStr="Jones et al., 2012" startWordPosition="2979" endWordPosition="2982">g0 K4 = Figure 3: (a) A rule right-hand side, and (b) a nice tree decomposition. Finding a tree decomposition with minimal width is in general NP-hard (Arnborg et al., 1987). However, we find that for the graphs we are interested in in NLP applications, even a naive algorithm gives tree decompositions of low width in practice: simply perform a depth-first traversal of the edges of the graph, forming a tree T. Then, augment the Vη as necessary to satisfy the running intersection property. As a test, we extracted rules from the GeoQuery corpus (Tang and Mooney, 2001) using the SYNSEM algorithm (Jones et al., 2012), and computed tree decompositions exactly using a branchand-bound method (Gogate and Dechter, 2004) and this approximate method. Table 1 shows that, in practice, treewidths are not very high even when computed only approximately. Any tree decomposition can be converted into one which is nice in the following sense (simplified from Cygan et al. (2011)). Each tree node η must be one of: • A leaf node, such that Vη = 0. • A unary node, which introduces exactly one edge e. • A binary node, which introduces no edges. The example decomposition in Figure 3c is nice. This canonical form simplifies th</context>
</contexts>
<marker>Jones, Andreas, Bauer, Hermann, Knight, 2012</marker>
<rawString>Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012. Semantics-based machine translation with hyperedge replacement grammars. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Treeadjoining grammars.</title>
<date>1997</date>
<booktitle>In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages and Automata,</booktitle>
<volume>3</volume>
<pages>69--124</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5454" citStr="Joshi and Schabes, 1997" startWordPosition="891" endWordPosition="894">n from the finite set C) to each edge. For brevity we use the terms graph and hypergraph interchangeably, and similarly for edge and hyperedge. In the definition of HRGs, we will use the notion of hypergraph fragments, which are the elementary structures that the grammar assembles into hypergraphs. Definition 2. A hypergraph fragment is a tuple (V, E, f, X), where (V, E, f) is a hypergraph and X E V+ is a list of distinct nodes called the external nodes. The function of graph fragments in HRG is analogous to the right-hand sides of CFG rules and to elementary trees in tree adjoining grammars (Joshi and Schabes, 1997). The external nodes indicate how to integrate a graph into another graph during a derivation, and are analogous to foot nodes. In diagrams, we draw them with a black circle ( ). Definition 3. A hyperedge replacement grammar (HRG) is a tuple G = (N, T, P, S) where • N and T are finite disjoint sets of nonterminal and terminal symbols • S E N is the start symbol • P is a finite set of productions of the form A → R, where A E N and R is a graph fragment over N U T. We now describe the HRG rewriting mechanism. Definition 4. Given a HRG G, we define the relation H ⇒G H′ (or, H′ is derived from H i</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind K. Joshi and Yves Schabes. 1997. Treeadjoining grammars. In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages and Automata, volume 3, pages 69–124. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clemens Lautemann</author>
</authors>
<title>The complexity of graph languages generated by hyperedge replacement.</title>
<date>1990</date>
<journal>Acta Informatica,</journal>
<pages>27--399</pages>
<contexts>
<context position="1650" citStr="Lautemann (1990)" startWordPosition="232" endWordPosition="233">orithm is part of Bolinas, a new software toolkit for HRG processing. 1 Introduction Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes et al., 1997), and its synchronous counterpart can be used for transforming graphs to/from other graphs or trees. As such, it has great potential for applications in natural language understanding and generation, and semantics-based machine translation (Jones et al., 2012). Figure 1 shows some examples of graphs for naturallanguage semantics. A polynomial-time recognition algorithm for HRGs was described by Lautemann (1990), building on the work of Rozenberg and Welzl (1986) on boundary node label controlled grammars, and others have presented polynomial-time algorithms as well (Mazanek and Minas, 2008; Moot, 2008). Although Lautemann’s algorithm is correct and tractable, its presentation is prefaced with the remark: “As we are only interested in distinguishing polynomial time from non-polynomial time, the analysis will be rather crude, and implementation details will be explicated as little as possible.” Indeed, the key step of the algorithm, which matches a rule against the input graph, is described at a very </context>
</contexts>
<marker>Lautemann, 1990</marker>
<rawString>Clemens Lautemann. 1990. The complexity of graph languages generated by hyperedge replacement. Acta Informatica, 27:399–421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Mazanek</author>
<author>Mark Minas</author>
</authors>
<title>Parsing of hyperedge replacement grammars with graph parser combinators.</title>
<date>2008</date>
<booktitle>In Proc. 7th International Workshop on Graph Transformation and Visual Modeling Techniques.</booktitle>
<contexts>
<context position="1832" citStr="Mazanek and Minas, 2008" startWordPosition="258" endWordPosition="261">raphs (Drewes et al., 1997), and its synchronous counterpart can be used for transforming graphs to/from other graphs or trees. As such, it has great potential for applications in natural language understanding and generation, and semantics-based machine translation (Jones et al., 2012). Figure 1 shows some examples of graphs for naturallanguage semantics. A polynomial-time recognition algorithm for HRGs was described by Lautemann (1990), building on the work of Rozenberg and Welzl (1986) on boundary node label controlled grammars, and others have presented polynomial-time algorithms as well (Mazanek and Minas, 2008; Moot, 2008). Although Lautemann’s algorithm is correct and tractable, its presentation is prefaced with the remark: “As we are only interested in distinguishing polynomial time from non-polynomial time, the analysis will be rather crude, and implementation details will be explicated as little as possible.” Indeed, the key step of the algorithm, which matches a rule against the input graph, is described at a very high level, so that it is not obvious (for a non-expert in graph algorithms) how to implement it. More importantly, this step as described leads to a time complexity that is polynomi</context>
</contexts>
<marker>Mazanek, Minas, 2008</marker>
<rawString>Steffen Mazanek and Mark Minas. 2008. Parsing of hyperedge replacement grammars with graph parser combinators. In Proc. 7th International Workshop on Graph Transformation and Visual Modeling Techniques.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Moot</author>
</authors>
<title>Lambek grammars, tree adjoining grammars and hyperedge replacement grammars.</title>
<date>2008</date>
<booktitle>In Proc. TAG+9,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="1845" citStr="Moot, 2008" startWordPosition="262" endWordPosition="263">7), and its synchronous counterpart can be used for transforming graphs to/from other graphs or trees. As such, it has great potential for applications in natural language understanding and generation, and semantics-based machine translation (Jones et al., 2012). Figure 1 shows some examples of graphs for naturallanguage semantics. A polynomial-time recognition algorithm for HRGs was described by Lautemann (1990), building on the work of Rozenberg and Welzl (1986) on boundary node label controlled grammars, and others have presented polynomial-time algorithms as well (Mazanek and Minas, 2008; Moot, 2008). Although Lautemann’s algorithm is correct and tractable, its presentation is prefaced with the remark: “As we are only interested in distinguishing polynomial time from non-polynomial time, the analysis will be rather crude, and implementation details will be explicated as little as possible.” Indeed, the key step of the algorithm, which matches a rule against the input graph, is described at a very high level, so that it is not obvious (for a non-expert in graph algorithms) how to implement it. More importantly, this step as described leads to a time complexity that is polynomial, but poten</context>
</contexts>
<marker>Moot, 2008</marker>
<rawString>Richard Moot. 2008. Lambek grammars, tree adjoining grammars and hyperedge replacement grammars. In Proc. TAG+9, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Rozenberg</author>
<author>Emo Welzl</author>
</authors>
<title>Boundary NLC graph grammars—basic definitions, normal forms, and complexity.</title>
<date>1986</date>
<journal>Information and Control,</journal>
<pages>69--136</pages>
<contexts>
<context position="1702" citStr="Rozenberg and Welzl (1986)" startWordPosition="240" endWordPosition="243">toolkit for HRG processing. 1 Introduction Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes et al., 1997), and its synchronous counterpart can be used for transforming graphs to/from other graphs or trees. As such, it has great potential for applications in natural language understanding and generation, and semantics-based machine translation (Jones et al., 2012). Figure 1 shows some examples of graphs for naturallanguage semantics. A polynomial-time recognition algorithm for HRGs was described by Lautemann (1990), building on the work of Rozenberg and Welzl (1986) on boundary node label controlled grammars, and others have presented polynomial-time algorithms as well (Mazanek and Minas, 2008; Moot, 2008). Although Lautemann’s algorithm is correct and tractable, its presentation is prefaced with the remark: “As we are only interested in distinguishing polynomial time from non-polynomial time, the analysis will be rather crude, and implementation details will be explicated as little as possible.” Indeed, the key step of the algorithm, which matches a rule against the input graph, is described at a very high level, so that it is not obvious (for a non-exp</context>
</contexts>
<marker>Rozenberg, Welzl, 1986</marker>
<rawString>Grzegorz Rozenberg and Emo Welzl. 1986. Boundary NLC graph grammars—basic definitions, normal forms, and complexity. Information and Control, 69:136–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--3</pages>
<contexts>
<context position="17722" citStr="Shieber et al., 1995" startWordPosition="3380" endWordPosition="3383">qual to q. Case 1: v is an external node. Since the root node contains all the external nodes, by the running intersection property, both Vq and Vparent(q) must contain v as well. Case 2: v has an edge not in R&gt;q. Therefore there must be a tree node not dominated by or equal to q that contains this edge, and therefore v. So by the running intersection property, q and its parent must contain v as well. ❑ This result, in turn, will allow us to bound the complexity of the parsing algorithm in terms of the treewidth of G. 3.3 Inference rules We present the parsing algorithm as a deductive system (Shieber et al., 1995). The items have one of two forms. A passive item has the form [A, I, X], where X E V+ is an explicit ordering of the boundary nodes of I. This means that we have recognized that A =&gt;* I. Thus, the goal item is [S, H, E]. An active item has the form [A → R, q, I, 0], where • (A → R) is a production of G • q is a node of TR • I is a subgraph of H • 0 is a bijection between the boundary nodes of R&gt;q and those of I. The parser must ensure that 0 is a bijection when it creates a new item. Below, we use the notation {e H e′1 or {e H X1 for the mapping that sends each node of e to the corresponding </context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart M. Shieber, Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24:3–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lappoon Tang</author>
<author>Raymond Mooney</author>
</authors>
<title>Using multiple clause constructors in inductive logic programming for semantic parsing. In</title>
<date>2001</date>
<booktitle>Proc. European Conference on Machine Learning.</booktitle>
<contexts>
<context position="13113" citStr="Tang and Mooney, 2001" startWordPosition="2495" endWordPosition="2498"> in either I or J. ❑ This result leads to Algorithm 1, which runs in time linear in the number of boundary nodes. Algorithm 1 Compute the union of two disjoint subgraphs I and J. for all v E bn(I) do E (-- be(I, v) U be(J, v) if v V bn(J) or v has an edge not in E then add v to bn(I U J) be(I U J, v) (-- E for all v E bn(J) do if v V bn(I) then add v to bn(I U J) be(I U J, v) (-- be(I, v) U be(J, v) (m E I U J) (-- (m E I) V (m E J) In practice, for small subgraphs, it may be more efficient simply to use an explicit set of edges instead of the boundary representation. For the GeoQuery corpus (Tang and Mooney, 2001), whose graphs are only 7.4 nodes on average, we generally find this to be the case. 3.2 Treewidth Lautemann’s algorithm tries to match a rule against the input graph all at once. But we can optimize the algorithm by matching a rule incrementally. This is analogous to the rank-minimization problem for linear context-free rewriting systems. Gildea has shown that this problem is related to 927 the notion of treewidth (Gildea, 2011), which we review briefly here. Definition 7. A tree decomposition of a graph H = (V, E) is a tree T, each of whose nodes η is associated with sets Vη c V and Eη c E, </context>
<context position="15538" citStr="Tang and Mooney, 2001" startWordPosition="2971" endWordPosition="2974">g0 b 1 girl′ 0 arg1 a b 1 0 1 believe′ 0 b 0 0 0 arg0 K4 = Figure 3: (a) A rule right-hand side, and (b) a nice tree decomposition. Finding a tree decomposition with minimal width is in general NP-hard (Arnborg et al., 1987). However, we find that for the graphs we are interested in in NLP applications, even a naive algorithm gives tree decompositions of low width in practice: simply perform a depth-first traversal of the edges of the graph, forming a tree T. Then, augment the Vη as necessary to satisfy the running intersection property. As a test, we extracted rules from the GeoQuery corpus (Tang and Mooney, 2001) using the SYNSEM algorithm (Jones et al., 2012), and computed tree decompositions exactly using a branchand-bound method (Gogate and Dechter, 2004) and this approximate method. Table 1 shows that, in practice, treewidths are not very high even when computed only approximately. Any tree decomposition can be converted into one which is nice in the following sense (simplified from Cygan et al. (2011)). Each tree node η must be one of: • A leaf node, such that Vη = 0. • A unary node, which introduces exactly one edge e. • A binary node, which introduces no edges. The example decomposition in Figu</context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>Lappoon Tang and Raymond Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In Proc. European Conference on Machine Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>