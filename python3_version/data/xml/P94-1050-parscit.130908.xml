<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.039026">
<title confidence="0.8147315">
AN AUTOMATIC METHOD OF FINDING TOPIC
BOUNDARIES
</title>
<author confidence="0.994897">
Jeffrey C. Reynar*
</author>
<affiliation confidence="0.906474666666667">
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, Pennsylvania, USA
</affiliation>
<email confidence="0.997603">
jcreynar@unagi.cis.upenn.edu
</email>
<sectionHeader confidence="0.996653" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999801">
This article outlines a new method of locating discourse
boundaries based on lexical cohesion and a graphical
technique called dotplotting. The application of dot-
plotting to discourse segmentation can be performed ei-
ther manually, by examining a graph, or automatically,
using an optimization algorithm. The results of two ex-
periments involving automatically locating boundaries
between a series of concatenated documents are pre-
sented. Areas of application and future directions for
this work are also outlined.
</bodyText>
<sectionHeader confidence="0.958161" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.99995096">
In general, texts are &amp;quot;about&amp;quot; some topic. That is, the
sentences which compose a document contribute infor-
mation related to the topic in a coherent fashion. In all
but the shortest texts, the topic will be expounded upon
through the discussion of multiple subtopics. Whether
the organization of the text is hierarchical in nature,
as described in (Grosz and Sidner, 1986), or linear, as
examined in (Skorochod&apos;ko, 1972), boundaries between
subtopics will generally exist.
In some cases, these boundaries will be explicit and
will correspond to paragraphs, or in longer texts, sec-
tions or chapters. They can also be implicit. Newspa-
per articles often contain paragraph demarcations, but
less frequently contain section markings, even though
lengthy articles often address the main topic by dis-
cussing subtopics in separate paragraphs or regions of
the article.
Topic boundaries are useful for several different tasks.
Hearst and Plaunt (1993) demonstrated their usefulness
for information retrieval by showing that segmenting
documents and indexing the resulting subdocuments
improves accuracy on an information retrieval task.
Youmans (1991) showed that his text segmentation al-
gorithm could be used to manually find scene bound-
aries in works of literature. Morris and Hirst (1991) at-
</bodyText>
<footnote confidence="0.9850494">
*The author would like to thank Christy Doran, Ja-
son Eisner, Al Kim, Mark Liberman, Mitch Marcus, Mike
Schultz and David Yarowsky for their helpful comments and
acknowledge the support of DARPA grant No. N0014-85-
K0018 and ARO grant No. DAAL 03-89-00031 PRI.
</footnote>
<bodyText confidence="0.995080357142857">
tempted to confirm the theories of discourse structure
outlined in (Grosz and Sidner, 1986) using information
from a thesaurus. In addition, Kozima (1993) specu-
lated that segmenting text along topic boundaries may
be useful for anaphora resolution and text summariza-
tion.
This paper is about an automatic method of finding
discourse boundaries based on the repetition of lexi-
cal items. Halliday and Hasan (1976) and others have
claimed that the repetition of lexical items, and in par-
ticular content-carrying lexical items, provides coher-
ence to a text. This observation has been used implic-
itly in several of the techniques described above, but
the method presented here depends exclusively on it.
</bodyText>
<subsectionHeader confidence="0.542195">
Methodology
</subsectionHeader>
<bodyText confidence="0.999985296296296">
Church (1993) describes a graphical method, called dot-
plotting, for aligning bilingual corpora. This method
has been adapted here for finding discourse boundaries.
The dotplot used for discovering topic boundaries is cre-
ated by enumerating the lexical items in an article and
plotting points which correspond to word repetitions.
For example, if a particular word appears at word po-
sitions x and y in a text, then the four points corre-
sponding to the cartesian product of the set containing
these two positions with itself would be plotted. That
is, (x, x), (x , y), (y, x) and (y , y) would be plotted on
the dotplot.
Prior to creating the dotplot, several filters are ap-
plied to the text. First, since closed-class words carry
little semantic weight, they are removed by filtering
based on part of speech information. Next, the remain-
ing words are lemmatized using the morphological anal-
ysis software described in (Karp et al., 1992). Finally,
the lemmas are filtered to remove a small number of
common words which are regarded as open-class by the
part of speech tag set, but which contribute little to the
meaning of the text. For example, forms of the verbs
BE and HAVE are open class words, but are ubiquitous
in all types of text. Once these steps have been taken,
the dotplot is created in the manner described above. A
sample dotplot of four concatenated Wall Street Jour-
nal articles is shown in figure 1. The real boundaries
</bodyText>
<page confidence="0.987152">
331
</page>
<figure confidence="0.999868947368421">
I 50 2.00
2.50 3.00
0.00 0.50 1.00
Y 104
650.00
60000
550.00
500.00
450.00
400.00
350.00
30000
250.00
200.00
150.00
Y 103
&amp;may
If
x
</figure>
<figureCaption confidence="0.8031965">
Figure 1: The dotplot of four concatenated Wall Street Figure 2: The outside density plot of the same four
Journal articles. articles.
</figureCaption>
<bodyText confidence="0.999113209302326">
between documents are located at word positions 1085,
2206 and 2863.
The word position in the file increases as values in-
crease along both axes of the dotplot. As a result, the
diagonal with slope equal to one is present since each
word in the text is identical to itself. The gaps in this
line correspond to points where words have been re-
moved by one of the filters. Since the repetition of lexi-
cal items occurs more frequently within regions of a text
which are about the same topic or group of topics, the
visually apparent squares along the main diagonal of
the plot correspond to regions of the text. Regions are
delimited by squares because of the symmetry present
in the dotplot.
Although boundaries may be identified visually using
the dotplot, the plot itself is unnecessary for the dis-
covery of boundaries. The reason the regions along the
diagonal are striking to the eye is that they are denser.
This fact leads naturally to an algorithm based on max-
imizing the density of the regions within squares along
the diagonal, which in turn corresponds to minimizing
the density of the regions not contained within these
squares. Once the densities of areas outside these re-
gions have been computed, the algorithm begins by se-
lecting the boundary which results in the lowest outside
density. Additional boundaries are added until either
the outside density increases or a particular number
of boundaries have been added. Potential boundaries
are selected from a list of either sentence boundaries or
paragraph boundaries, depending on the experiment.
More formally, let n be the length of the concatena-
tion of articles; let m be the number of unique tokens
(after lemmatization and removal of words on the stop
list); let B be a list of boundaries, initialized to contain
only the boundary corresponding to the beginning of
the series of articles, 0. Maintain B in ascending order.
Let i be a potential boundary; let P = B U {i}, also
sorted in ascending order; let Vx y be a vector contain-
ing the word counts associated with word positions x
through y in the concatenation. Now, find the i such
that the equation below is minimized. Repeat this min-
imization, inserting i into B, until the desired number
of boundaries have been located.
</bodyText>
<equation confidence="0.655262666666667">
IF&apos;
EiiPi VPjn
j=2 (Pi - Pi -1)(n - Pi)
</equation>
<bodyText confidence="0.9999274">
The dot product in the equation reveals the similar-
ity between this method and Heart and Plaunt&apos;s (1993)
work which was done in a vector-space framework. The
crucial difference lies in the global nature of this equa-
tion. Their algorithm placed boundaries by comparing
neighboring regions only, while this technique compares
each region with all other regions.
A graph depicting the density of the regions not en-
closed in squares along the diagonal is shown in figure
2. The y-coordinate on this graph represents the den-
sity when a boundary is placed at the corresponding
location on the x-axis. These data are derived from
the dotplot shown in figure 1. Actual boundaries corre-
spond to the most extreme minima—those at positions
1085, 2206 and 2863.
</bodyText>
<sectionHeader confidence="0.82232" genericHeader="acknowledgments">
Results
</sectionHeader>
<bodyText confidence="0.999938666666667">
Since determining where topic boundaries belong is a
subjective task, (Passoneau and Litman, 1993), the pre-
liminary experiments conducted using this algorithm
involved discovering boundaries between concatenated
articles. All of the articles were from the Wall Street
Journal and were tagged in conjunction with the Penn
Treebank project, which is described in (Marcus et al.,
1993). The motivation behind this experiment is that
newspaper articles are about sufficiently different top-
ics that discerning the boundaries between them should
serve as a baseline measure of the algorithm&apos;s effective-
ness.
</bodyText>
<page confidence="0.99338">
332
</page>
<table confidence="0.999614222222222">
Expt. 1 Expt. 2
# of exact matches 271 106
# of close matches 196 55
# of extra boundaries 1085 38
# of missed boundaries 43 355
Precision 0.175 0.549
Precision counting close 0.300 0.803
Recall 0.531 0.208
Recall counting close 0.916 0.304
</table>
<tableCaption confidence="0.99995">
Table 1: Results of two experiments.
</tableCaption>
<bodyText confidence="0.999963">
The results of two experiments in which between two
and eight randomly selected Wall Street Journal arti-
cles were concatenated are shown in table 1. Both ex-
periments were performed on the same data set which
consisted of 150 concatenations of articles containing a
total of 660 articles averaging 24.5 sentences in length.
The average sentence length was 24.5 words. The differ-
ence between the two experiments was that in the first
experiment, boundaries were placed only at the ends of
sentences, while in the second experiment, they were
only placed at paragraph boundaries. Tuning the stop-
ping criteria parameters in either method allows im-
provements in precision to be traded for declines in re-
call and vice versa. The first experiment demonstrates
that high recall rates can be achieved and the second
shows that high precision can also be achieved.
In these tests, a minimum separation between bound-
aries was imposed to prevent documents from being
repeatedly subdivided around the location of one ac-
tual boundary. For the purposes of evaluation, an exact
match is one in which the algorithm placed a boundary
at the same position as one existed in the collection of
articles. A missed boundary is one for which the algo-
rithm found no corresponding boundary. If a boundary
was not an exact match, but was within three sentences
of the correct location, the result was considered a close
match. Precision and recall scores were computed both
including and excluding the number of close matches.
The precision and recall scores including close matches
reflect the admission of only one close match per ac-
tual boundary. It should be noted that some of the
extra boundaries found may correspond to actual shifts
in topic and may not be superfluous.
</bodyText>
<subsectionHeader confidence="0.720929">
Future Work
</subsectionHeader>
<bodyText confidence="0.999948947368421">
The current implementation of the algorithm relies on
part of speech information to detect closed class words
and to find sentence boundaries. However, a larger
common word list and a sentence boundary recognition
algorithm could be employed to obviate the need for
tags. Then the method could be easily applied to large
amounts of text. Also, since the task of segmenting
concatenated documents is quite artificial, the approach
should be applied to finding topic boundaries. To this
end, the algorithm&apos;s output should be compared to the
segmentations produced by human judges and the sec-
tion divisions authors insert into some forms of writing,
such as technical writing. Additionally, the segment in-
formation produced by the algorithm should be used
in an information retrieval task as was done in (Hearst
and Plaunt, 1993). Lastly, since this paper only exam-
ined flat segmentations, work needs to be done to see
whether useful hierarchical segmentations can be pro-
duced.
</bodyText>
<sectionHeader confidence="0.998689" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997782560975609">
Church, Kenneth Ward. Char_align: A Program for
Aligning Parallel Texts at the Character Level. Pro-
ceedings of the 31st Annual Meeting of the Associa-
tion for Computational Linguistics, 1993.
Grosz, Barbara J. and Candace L. Sidner. Attention,
Intentions and the Structure of Discourse. Computa-
tional Linguistics, Volume 12, Number 3, 1986.
Halliday, Michael and Ruciaiya Hasan. Cohesion in En-
glish. New York: Longman Group, 1976.
Hearst, Marti A. and Christian Plaunt. Subtopic Struc-
turing for Full-Length Document Access. Proceed-
ings of the Special Interest Group on Information Re-
trieval, 1993.
Karp, Daniel, Yves Schabes, Martin Zaidel and Dania
Egedi. A Freely Available Wide Coverage Morpho-
logical Analyzer for English. Proceedings of the 15th
International Conference on Computational Linguis-
tics, 1992.
Kozima, Hideki. Text Segmentation Based on Similar-
ity Between Words. Proceedings of the 31st Annual
Meeting of the Association for Computational Lin-
guistics, 1993.
Marcus, Mitchell P., Beatrice Santorini and Mary Ann
Markiewicz. Building a Large Annotated Corpus of
English: The Penn Treebank. Computational Lin-
guistics, Volume 19, Number 2, 1993.
Morris, Jane and Graeme Hirst. Lexical Cohesion Com-
puted by Thesaural Relations as an Indicator of the
Structure of Text. Computational Linguistics, Vol-
ume 17, Number 1, 1991.
Passoneau, Rebecca J. and Diane J. Litman. Intention-
Based Segmentation: Human Reliability and Corre-
lation with Linguistic Cues. Proceedings of the 31st
Annual Meeting of the Association for Computational
Linguistics, 1993.
Skorochod&apos;ko, E.F. Adaptive Method of Automatic Ab-
stracting and Indexing. Information Processing, Vol-
ume 71, 1972.
Youmans, Gilbert. A New Tool for Discourse Analy-
sis: The Vocabulary-Management Profile. Language,
Volume 67, Number 4, 1991.
</reference>
<page confidence="0.999411">
333
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.844089">
<title confidence="0.9602155">AN AUTOMATIC METHOD OF FINDING TOPIC BOUNDARIES</title>
<author confidence="0.999979">Jeffrey C Reynar</author>
<affiliation confidence="0.999905">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.999652">Philadelphia, Pennsylvania, USA</address>
<email confidence="0.99986">jcreynar@unagi.cis.upenn.edu</email>
<abstract confidence="0.992254181818182">This article outlines a new method of locating discourse boundaries based on lexical cohesion and a graphical technique called dotplotting. The application of dotplotting to discourse segmentation can be performed either manually, by examining a graph, or automatically, using an optimization algorithm. The results of two experiments involving automatically locating boundaries between a series of concatenated documents are presented. Areas of application and future directions for this work are also outlined.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
</authors>
<title>Char_align: A Program for Aligning Parallel Texts at the Character Level.</title>
<date>1993</date>
<booktitle>Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="3005" citStr="Church (1993)" startWordPosition="450" endWordPosition="451">rom a thesaurus. In addition, Kozima (1993) speculated that segmenting text along topic boundaries may be useful for anaphora resolution and text summarization. This paper is about an automatic method of finding discourse boundaries based on the repetition of lexical items. Halliday and Hasan (1976) and others have claimed that the repetition of lexical items, and in particular content-carrying lexical items, provides coherence to a text. This observation has been used implicitly in several of the techniques described above, but the method presented here depends exclusively on it. Methodology Church (1993) describes a graphical method, called dotplotting, for aligning bilingual corpora. This method has been adapted here for finding discourse boundaries. The dotplot used for discovering topic boundaries is created by enumerating the lexical items in an article and plotting points which correspond to word repetitions. For example, if a particular word appears at word positions x and y in a text, then the four points corresponding to the cartesian product of the set containing these two positions with itself would be plotted. That is, (x, x), (x , y), (y, x) and (y , y) would be plotted on the dot</context>
</contexts>
<marker>Church, 1993</marker>
<rawString>Church, Kenneth Ward. Char_align: A Program for Aligning Parallel Texts at the Character Level. Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<date>1986</date>
<booktitle>Attention, Intentions and the Structure of Discourse. Computational Linguistics, Volume 12, Number 3,</booktitle>
<contexts>
<context position="1108" citStr="Grosz and Sidner, 1986" startWordPosition="157" endWordPosition="160">ng an optimization algorithm. The results of two experiments involving automatically locating boundaries between a series of concatenated documents are presented. Areas of application and future directions for this work are also outlined. Introduction In general, texts are &amp;quot;about&amp;quot; some topic. That is, the sentences which compose a document contribute information related to the topic in a coherent fashion. In all but the shortest texts, the topic will be expounded upon through the discussion of multiple subtopics. Whether the organization of the text is hierarchical in nature, as described in (Grosz and Sidner, 1986), or linear, as examined in (Skorochod&apos;ko, 1972), boundaries between subtopics will generally exist. In some cases, these boundaries will be explicit and will correspond to paragraphs, or in longer texts, sections or chapters. They can also be implicit. Newspaper articles often contain paragraph demarcations, but less frequently contain section markings, even though lengthy articles often address the main topic by discussing subtopics in separate paragraphs or regions of the article. Topic boundaries are useful for several different tasks. Hearst and Plaunt (1993) demonstrated their usefulness</context>
<context position="2372" citStr="Grosz and Sidner, 1986" startWordPosition="349" endWordPosition="352">t segmenting documents and indexing the resulting subdocuments improves accuracy on an information retrieval task. Youmans (1991) showed that his text segmentation algorithm could be used to manually find scene boundaries in works of literature. Morris and Hirst (1991) at*The author would like to thank Christy Doran, Jason Eisner, Al Kim, Mark Liberman, Mitch Marcus, Mike Schultz and David Yarowsky for their helpful comments and acknowledge the support of DARPA grant No. N0014-85- K0018 and ARO grant No. DAAL 03-89-00031 PRI. tempted to confirm the theories of discourse structure outlined in (Grosz and Sidner, 1986) using information from a thesaurus. In addition, Kozima (1993) speculated that segmenting text along topic boundaries may be useful for anaphora resolution and text summarization. This paper is about an automatic method of finding discourse boundaries based on the repetition of lexical items. Halliday and Hasan (1976) and others have claimed that the repetition of lexical items, and in particular content-carrying lexical items, provides coherence to a text. This observation has been used implicitly in several of the techniques described above, but the method presented here depends exclusively</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, Barbara J. and Candace L. Sidner. Attention, Intentions and the Structure of Discourse. Computational Linguistics, Volume 12, Number 3, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Halliday</author>
<author>Ruciaiya Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman Group,</publisher>
<location>New York:</location>
<contexts>
<context position="2692" citStr="Halliday and Hasan (1976)" startWordPosition="399" endWordPosition="402"> Jason Eisner, Al Kim, Mark Liberman, Mitch Marcus, Mike Schultz and David Yarowsky for their helpful comments and acknowledge the support of DARPA grant No. N0014-85- K0018 and ARO grant No. DAAL 03-89-00031 PRI. tempted to confirm the theories of discourse structure outlined in (Grosz and Sidner, 1986) using information from a thesaurus. In addition, Kozima (1993) speculated that segmenting text along topic boundaries may be useful for anaphora resolution and text summarization. This paper is about an automatic method of finding discourse boundaries based on the repetition of lexical items. Halliday and Hasan (1976) and others have claimed that the repetition of lexical items, and in particular content-carrying lexical items, provides coherence to a text. This observation has been used implicitly in several of the techniques described above, but the method presented here depends exclusively on it. Methodology Church (1993) describes a graphical method, called dotplotting, for aligning bilingual corpora. This method has been adapted here for finding discourse boundaries. The dotplot used for discovering topic boundaries is created by enumerating the lexical items in an article and plotting points which co</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday, Michael and Ruciaiya Hasan. Cohesion in English. New York: Longman Group, 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
<author>Christian Plaunt</author>
</authors>
<title>Subtopic Structuring for Full-Length Document Access.</title>
<date>1993</date>
<booktitle>Proceedings of the Special Interest Group on Information Retrieval,</booktitle>
<contexts>
<context position="1678" citStr="Hearst and Plaunt (1993)" startWordPosition="242" endWordPosition="245">al in nature, as described in (Grosz and Sidner, 1986), or linear, as examined in (Skorochod&apos;ko, 1972), boundaries between subtopics will generally exist. In some cases, these boundaries will be explicit and will correspond to paragraphs, or in longer texts, sections or chapters. They can also be implicit. Newspaper articles often contain paragraph demarcations, but less frequently contain section markings, even though lengthy articles often address the main topic by discussing subtopics in separate paragraphs or regions of the article. Topic boundaries are useful for several different tasks. Hearst and Plaunt (1993) demonstrated their usefulness for information retrieval by showing that segmenting documents and indexing the resulting subdocuments improves accuracy on an information retrieval task. Youmans (1991) showed that his text segmentation algorithm could be used to manually find scene boundaries in works of literature. Morris and Hirst (1991) at*The author would like to thank Christy Doran, Jason Eisner, Al Kim, Mark Liberman, Mitch Marcus, Mike Schultz and David Yarowsky for their helpful comments and acknowledge the support of DARPA grant No. N0014-85- K0018 and ARO grant No. DAAL 03-89-00031 PR</context>
</contexts>
<marker>Hearst, Plaunt, 1993</marker>
<rawString>Hearst, Marti A. and Christian Plaunt. Subtopic Structuring for Full-Length Document Access. Proceedings of the Special Interest Group on Information Retrieval, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Karp</author>
<author>Yves Schabes</author>
<author>Martin Zaidel</author>
<author>Dania Egedi</author>
</authors>
<title>A Freely Available Wide Coverage Morphological Analyzer for English.</title>
<date>1992</date>
<booktitle>Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<contexts>
<context position="3927" citStr="Karp et al., 1992" startWordPosition="604" endWordPosition="607">repetitions. For example, if a particular word appears at word positions x and y in a text, then the four points corresponding to the cartesian product of the set containing these two positions with itself would be plotted. That is, (x, x), (x , y), (y, x) and (y , y) would be plotted on the dotplot. Prior to creating the dotplot, several filters are applied to the text. First, since closed-class words carry little semantic weight, they are removed by filtering based on part of speech information. Next, the remaining words are lemmatized using the morphological analysis software described in (Karp et al., 1992). Finally, the lemmas are filtered to remove a small number of common words which are regarded as open-class by the part of speech tag set, but which contribute little to the meaning of the text. For example, forms of the verbs BE and HAVE are open class words, but are ubiquitous in all types of text. Once these steps have been taken, the dotplot is created in the manner described above. A sample dotplot of four concatenated Wall Street Journal articles is shown in figure 1. The real boundaries 331 I 50 2.00 2.50 3.00 0.00 0.50 1.00 Y 104 650.00 60000 550.00 500.00 450.00 400.00 350.00 30000 2</context>
</contexts>
<marker>Karp, Schabes, Zaidel, Egedi, 1992</marker>
<rawString>Karp, Daniel, Yves Schabes, Martin Zaidel and Dania Egedi. A Freely Available Wide Coverage Morphological Analyzer for English. Proceedings of the 15th International Conference on Computational Linguistics, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Kozima</author>
</authors>
<title>Text Segmentation Based on Similarity Between Words.</title>
<date>1993</date>
<booktitle>Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="2435" citStr="Kozima (1993)" startWordPosition="360" endWordPosition="361">ccuracy on an information retrieval task. Youmans (1991) showed that his text segmentation algorithm could be used to manually find scene boundaries in works of literature. Morris and Hirst (1991) at*The author would like to thank Christy Doran, Jason Eisner, Al Kim, Mark Liberman, Mitch Marcus, Mike Schultz and David Yarowsky for their helpful comments and acknowledge the support of DARPA grant No. N0014-85- K0018 and ARO grant No. DAAL 03-89-00031 PRI. tempted to confirm the theories of discourse structure outlined in (Grosz and Sidner, 1986) using information from a thesaurus. In addition, Kozima (1993) speculated that segmenting text along topic boundaries may be useful for anaphora resolution and text summarization. This paper is about an automatic method of finding discourse boundaries based on the repetition of lexical items. Halliday and Hasan (1976) and others have claimed that the repetition of lexical items, and in particular content-carrying lexical items, provides coherence to a text. This observation has been used implicitly in several of the techniques described above, but the method presented here depends exclusively on it. Methodology Church (1993) describes a graphical method,</context>
</contexts>
<marker>Kozima, 1993</marker>
<rawString>Kozima, Hideki. Text Segmentation Based on Similarity Between Words. Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Markiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="8122" citStr="Marcus et al., 1993" startWordPosition="1322" endWordPosition="1325">ity when a boundary is placed at the corresponding location on the x-axis. These data are derived from the dotplot shown in figure 1. Actual boundaries correspond to the most extreme minima—those at positions 1085, 2206 and 2863. Results Since determining where topic boundaries belong is a subjective task, (Passoneau and Litman, 1993), the preliminary experiments conducted using this algorithm involved discovering boundaries between concatenated articles. All of the articles were from the Wall Street Journal and were tagged in conjunction with the Penn Treebank project, which is described in (Marcus et al., 1993). The motivation behind this experiment is that newspaper articles are about sufficiently different topics that discerning the boundaries between them should serve as a baseline measure of the algorithm&apos;s effectiveness. 332 Expt. 1 Expt. 2 # of exact matches 271 106 # of close matches 196 55 # of extra boundaries 1085 38 # of missed boundaries 43 355 Precision 0.175 0.549 Precision counting close 0.300 0.803 Recall 0.531 0.208 Recall counting close 0.916 0.304 Table 1: Results of two experiments. The results of two experiments in which between two and eight randomly selected Wall Street Journa</context>
</contexts>
<marker>Marcus, Santorini, Markiewicz, 1993</marker>
<rawString>Marcus, Mitchell P., Beatrice Santorini and Mary Ann Markiewicz. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, Volume 19, Number 2, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical Cohesion Computed by Thesaural Relations as an</title>
<date>1991</date>
<journal>Indicator of the Structure of Text. Computational Linguistics, Volume</journal>
<volume>17</volume>
<contexts>
<context position="2018" citStr="Morris and Hirst (1991)" startWordPosition="291" endWordPosition="294">paragraph demarcations, but less frequently contain section markings, even though lengthy articles often address the main topic by discussing subtopics in separate paragraphs or regions of the article. Topic boundaries are useful for several different tasks. Hearst and Plaunt (1993) demonstrated their usefulness for information retrieval by showing that segmenting documents and indexing the resulting subdocuments improves accuracy on an information retrieval task. Youmans (1991) showed that his text segmentation algorithm could be used to manually find scene boundaries in works of literature. Morris and Hirst (1991) at*The author would like to thank Christy Doran, Jason Eisner, Al Kim, Mark Liberman, Mitch Marcus, Mike Schultz and David Yarowsky for their helpful comments and acknowledge the support of DARPA grant No. N0014-85- K0018 and ARO grant No. DAAL 03-89-00031 PRI. tempted to confirm the theories of discourse structure outlined in (Grosz and Sidner, 1986) using information from a thesaurus. In addition, Kozima (1993) speculated that segmenting text along topic boundaries may be useful for anaphora resolution and text summarization. This paper is about an automatic method of finding discourse boun</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris, Jane and Graeme Hirst. Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text. Computational Linguistics, Volume 17, Number 1, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passoneau</author>
<author>Diane J Litman</author>
</authors>
<title>IntentionBased Segmentation: Human Reliability and Correlation with Linguistic Cues.</title>
<date>1993</date>
<booktitle>Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="7838" citStr="Passoneau and Litman, 1993" startWordPosition="1280" endWordPosition="1283">gorithm placed boundaries by comparing neighboring regions only, while this technique compares each region with all other regions. A graph depicting the density of the regions not enclosed in squares along the diagonal is shown in figure 2. The y-coordinate on this graph represents the density when a boundary is placed at the corresponding location on the x-axis. These data are derived from the dotplot shown in figure 1. Actual boundaries correspond to the most extreme minima—those at positions 1085, 2206 and 2863. Results Since determining where topic boundaries belong is a subjective task, (Passoneau and Litman, 1993), the preliminary experiments conducted using this algorithm involved discovering boundaries between concatenated articles. All of the articles were from the Wall Street Journal and were tagged in conjunction with the Penn Treebank project, which is described in (Marcus et al., 1993). The motivation behind this experiment is that newspaper articles are about sufficiently different topics that discerning the boundaries between them should serve as a baseline measure of the algorithm&apos;s effectiveness. 332 Expt. 1 Expt. 2 # of exact matches 271 106 # of close matches 196 55 # of extra boundaries 1</context>
</contexts>
<marker>Passoneau, Litman, 1993</marker>
<rawString>Passoneau, Rebecca J. and Diane J. Litman. IntentionBased Segmentation: Human Reliability and Correlation with Linguistic Cues. Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Skorochod&apos;ko</author>
</authors>
<title>Adaptive Method of Automatic Abstracting and Indexing. Information Processing,</title>
<date>1972</date>
<volume>71</volume>
<contexts>
<context position="1156" citStr="Skorochod&apos;ko, 1972" startWordPosition="166" endWordPosition="167">eriments involving automatically locating boundaries between a series of concatenated documents are presented. Areas of application and future directions for this work are also outlined. Introduction In general, texts are &amp;quot;about&amp;quot; some topic. That is, the sentences which compose a document contribute information related to the topic in a coherent fashion. In all but the shortest texts, the topic will be expounded upon through the discussion of multiple subtopics. Whether the organization of the text is hierarchical in nature, as described in (Grosz and Sidner, 1986), or linear, as examined in (Skorochod&apos;ko, 1972), boundaries between subtopics will generally exist. In some cases, these boundaries will be explicit and will correspond to paragraphs, or in longer texts, sections or chapters. They can also be implicit. Newspaper articles often contain paragraph demarcations, but less frequently contain section markings, even though lengthy articles often address the main topic by discussing subtopics in separate paragraphs or regions of the article. Topic boundaries are useful for several different tasks. Hearst and Plaunt (1993) demonstrated their usefulness for information retrieval by showing that segme</context>
</contexts>
<marker>Skorochod&apos;ko, 1972</marker>
<rawString>Skorochod&apos;ko, E.F. Adaptive Method of Automatic Abstracting and Indexing. Information Processing, Volume 71, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilbert Youmans</author>
</authors>
<title>A New Tool for Discourse Analysis: The Vocabulary-Management Profile.</title>
<date>1991</date>
<journal>Language, Volume</journal>
<volume>67</volume>
<contexts>
<context position="1878" citStr="Youmans (1991)" startWordPosition="269" endWordPosition="270">ll correspond to paragraphs, or in longer texts, sections or chapters. They can also be implicit. Newspaper articles often contain paragraph demarcations, but less frequently contain section markings, even though lengthy articles often address the main topic by discussing subtopics in separate paragraphs or regions of the article. Topic boundaries are useful for several different tasks. Hearst and Plaunt (1993) demonstrated their usefulness for information retrieval by showing that segmenting documents and indexing the resulting subdocuments improves accuracy on an information retrieval task. Youmans (1991) showed that his text segmentation algorithm could be used to manually find scene boundaries in works of literature. Morris and Hirst (1991) at*The author would like to thank Christy Doran, Jason Eisner, Al Kim, Mark Liberman, Mitch Marcus, Mike Schultz and David Yarowsky for their helpful comments and acknowledge the support of DARPA grant No. N0014-85- K0018 and ARO grant No. DAAL 03-89-00031 PRI. tempted to confirm the theories of discourse structure outlined in (Grosz and Sidner, 1986) using information from a thesaurus. In addition, Kozima (1993) speculated that segmenting text along topi</context>
</contexts>
<marker>Youmans, 1991</marker>
<rawString>Youmans, Gilbert. A New Tool for Discourse Analysis: The Vocabulary-Management Profile. Language, Volume 67, Number 4, 1991.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>