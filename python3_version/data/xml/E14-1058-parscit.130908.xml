<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.991215666666667">
Weighted Krippendorff&apos;s alpha is a more reliable metrics for multi-
coders ordinal annotations: experimental studies on emotion, opinion
and coreference annotation
</title>
<author confidence="0.599765">
Jean-Yves Antoine
</author>
<affiliation confidence="0.558206">
Université François Rabelais de
</affiliation>
<address confidence="0.6958575">
Tours, LI (EA 6300)
Blois, France
</address>
<email confidence="0.657226">
Jean-Yves.Antoine@univ-tours.fr
</email>
<author confidence="0.568974">
Jeanne Villaneau
</author>
<affiliation confidence="0.391545">
Université Européenne de
</affiliation>
<address confidence="0.3086615">
Bretagne, IRISA
Lorient, France
</address>
<email confidence="0.432119">
Jeanne.Villaneau@univ-ubs.fr
</email>
<author confidence="0.299257">
Anaïs Lefeuvre
</author>
<affiliation confidence="0.266411">
Université François Rabelais
</affiliation>
<address confidence="0.6513645">
de Tours, LI (EA 6300)
Blois, France
</address>
<email confidence="0.814721">
anais.lefeuvre@univ-tours.fr
</email>
<sectionHeader confidence="0.99032" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998403823529412">
The question of data reliability is of first im-
portance to assess the quality of manually an-
notated corpora. Although Cohen’s κ is the
prevailing reliability measure used in NLP, al-
ternative statistics have been proposed. This
paper presents an experimental study with four
measures (Cohen’s κ, Scott’s π, binary and
weighted Krippendorff’ s α) on three tasks:
emotion, opinion and coreference annotation.
The reported studies investigate the factors of
influence (annotator bias, category prevalence,
number of coders, number of categories) that
should affect reliability estimation. Results
show that the use of a weighted measure re-
stricts this influence on ordinal annotations.
They suggest that weighted α is the most reli-
able metrics for such an annotation scheme.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999805953488372">
The newly intensive use of machine learning
techniques as well as the need of evaluation data
has led Natural Language Processing (NLP) to
develop large annotated corpora. The interest for
such enriched language resources has reached
domains (semantics, pragmatics, affective com-
puting) where the annotation process is highly
affected by the coders subjectivity. The reliabil-
ity of the resulting annotations must be trusted by
measures that assess the inter-coders agreement.
While medecine, psychology, and more gener-
ally content analysis, have considered for years
the issue of data reliability, NLP has only inves-
tigated this question from the mid 1990s. The
influential work of Carletta (1996) has led the κ
statistic (Cohen, 1960) to become the prevailing
standard for measuring the reliability of corpus
annotation. Many studies have however ques-
tioned the limitations of the κ statistic and have
proposed alternative measures of reliability.
Krippendorff claims that “popularity of κ not-
withstanding, Cohen’s κ is simply unsuitable as
a measure of the reliability of data” in a paper
presenting his α coefficient (Krippendorff,
2008).
Except for some rare but noticeable studies
(Arstein and Poesio, 2005), most of these critical
works restrict to theoretical issues about chance
agreement estimation or limitations due to vari-
ous statistical biases (Arstein and Poesio, 2008).
On the opposite, this paper investigates experi-
mentally these questions on three different tasks:
emotion, opinion and coreference annotation.
Four measures of reliability will be considered:
Cohen’s κ (Cohen, 1960), Scott’s π (Scott, 1955)
and two measures of Krippendorff’s α (Krippen-
dorff, 2004) with different distance.
Section 2 gives a comprehensive presentation
of these metrics. Section 3 details the potential
methodological biases that should affect the reli-
ability estimation. In section 4, we explain the
methodology we followed for this study. Lastly,
experimental results are presented in section 5.
</bodyText>
<sectionHeader confidence="0.99445" genericHeader="method">
2 Reliability measures
</sectionHeader>
<bodyText confidence="0.9995489375">
Any reliability measure considers the most perti-
nent criterion to estimate data reliability to be
reproducibility. Reproducibility can be estimated
by observing the agreement among independent
annotators (Krippendorff, 2004): the more the
coders agree on the data they have produced, the
more their annotations are likely to be repro-
duced by any other set of coders.
Pure observed agreement is not considered as
a good estimator since it does not give any ac-
count to the amount of chance that yields to this
agreement. For instance, a restricted number of
coding categories should favor chance agree-
ment. What must be estimated is the proportion
of observed agreement beyond the one that is
expected by chance:
</bodyText>
<figure confidence="0.404296">
(1) Measure =
−
A A
o − e
1
A
e
</figure>
<page confidence="0.965657">
550
</page>
<note confidence="0.9929855">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 550–559,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999938171428571">
where Ao is the observed agreement between
coders and Ae is an estimation of the possible
chance agreement. Reliability metrics differ by
the way they estimate this chance agreement.
Cohen’s x (Cohen, 1960) defines chance as
the statistical independence of the use of coding
categories by the annotators. It postulates that
chance annotation is governed by prior distribu-
tions that are specific to each coder (annotator
bias). x was originally developed for two coders
and nominal data. (Davies and Fleiss, 1982) has
proposed a generalization to any number of cod-
ers, while (Cohen, 1968) has defined a weighted
version of the x measure that fulfils better the
need of reliability estimation for ordinal annota-
tions: the disagreement between two ordinal an-
notations is no more binary, but depends on a
Euclidian distance. This weighted generalization
restricts however to a two coders scheme (Art-
stein and Poesio, 2008): a weighted version of
the multi-coders x statistics is still missing.
Unlike Cohen’s x, Scott’s n (Scott, 1955)
does not aim at modelling annotator bias. It de-
fines chance as the statistical independence of
the data and the set of coding categories, inde-
pendently from the coders. It considers therefore
the annotation process and not the behaviour of
the annotators. Scott’s original proposal con-
cerned only two coders. (Fleiss 1971) gave a
generalisation of the statistics to any number of
coders through a measure of pairwise agreement.
Krippendorff‘s a (Krippendorff, 2004) con-
siders chance independently from coders like
Scott’s n, but data reliability is estimated de-
pending on disagreement instead of agreement:
</bodyText>
<listItem confidence="0.823329">
(2) Alpha =
</listItem>
<bodyText confidence="0.999853583333333">
where Do is the observed disagreement be-
tween coders and De is an estimation of the pos-
sible chance disagreement. Another original as-
pect of this metrics is to allow disagreement es-
timation between two categories through any
distance measure. This implies that a handles
directly any number of coders and any kind of
annotation (nominal or ordinal coding scheme).
In this paper, we will consider the a statistics
with a binary as well as a Euclidian distance, in
order to assess separately the influence of the
distance measure and the metrics by itself.
</bodyText>
<sectionHeader confidence="0.921877" genericHeader="method">
3 Quality criteria for reliability metrics
</sectionHeader>
<bodyText confidence="0.8444225">
There is an abundant literature about the criteria
of quality a reliability measure should satisfy
(Hayes, 2007). These works emphasize on two
important points:
</bodyText>
<listItem confidence="0.999170857142857">
• A trustworthy measure should provide sta-
ble results: measures must be reasonably
independent of any factor of influence.
• The magnitude of the measure must be in-
terpreted in terms of absolute level of reli-
ability: the statistics must come up with
trustworthy reliability thresholds.
</listItem>
<bodyText confidence="0.99352725">
These questions have mainly been investigated
from a theoretical point of view. This section
summarizes the main conclusions that should be
drawn from these critical studies.
</bodyText>
<subsectionHeader confidence="0.999779">
3.1 Annotator bias and number of coders
</subsectionHeader>
<bodyText confidence="0.999793352941177">
Annotator bias refers to the influence of the idio-
syncratic behavior of the coders. It can be esti-
mated by a bias index which measures the extent
to which the distribution of categories differs
from one coder’s annotation to another (Sim and
Wright, 2005). Annotator bias has an influence
on the magnitude of the reliability measures
(Feinstein and Cicchetti,1990). Besides, it con-
cerns the invariance of the measures to the per-
mutation or selection of annotators but also to the
number of coders. A review of the literature
shows that theoretical studies on annotator bias
are not convergent. In particular, opposite argu-
ments have been proposed concerning Cohen’s x
(Di Eugenio and Glass 2004, Arstein and Poesio
2008, Hayes, 2007). This is why we have carried
on experiments that investigate:
</bodyText>
<listItem confidence="0.793455636363636">
• to what extent measures depend on the se-
lection of a specific set of coders (§ 5.3),
• to what extent the stability of the measures
depends on the number of coders (§ 5.4).
Arstein and Poesio (2005) have shown
that the greater the number of coders is,
the lower the annotator bias decreases.
Our aim is to go further this conclusion:
we will study whether one measure needs
fewer coders than another one to converge
towards an acceptable annotator bias.
</listItem>
<subsectionHeader confidence="0.999622">
3.2 Category prevalence
</subsectionHeader>
<bodyText confidence="0.999423142857143">
Prevalence refers to the influence on reliability
estimation of a coding category under which a
disproportionate amount of annotated data falls.
It can be estimated by a prevalence index which
measures the frequency differences of categories
on cases where the coders agree (Sim and
Wright, 2005). When the prevalence index is
</bodyText>
<equation confidence="0.5816528">
D
o
D −
e
De
</equation>
<page confidence="0.989482">
551
</page>
<bodyText confidence="0.9998557">
high, chance-corrected measures are spuriously
reduced since chance agreement is higher in this
situation (Brennan and Sliman, 1992; Di Eugenio
and Glass, 2004). This yields some authors to
propose corrected coefficients like the PABAK
measure (Byrt and al., 1993), which is a preva-
lence adjusted and annotator bias adjusted ver-
sion of Cohen’s κ. The influence of prevalence
will not be investigated here, since no category is
significantly prevalent in our data.
</bodyText>
<subsectionHeader confidence="0.99981">
3.3 Number of coding categories
</subsectionHeader>
<bodyText confidence="0.999989545454545">
The number of coding categories has an influ-
ence on the reliability measures magnitude: the
larger the number of categories is, the less the
coders have a chance to agree. Even if this de-
crease should concern chance agreement too,
lower reliability estimations are observed with
high numbers of categories (Brenner and
Kliebsch, 1996). This paper investigates this in-
fluence by comparing reliability values obtained
with a 3-categories and a 5-categories coding
scheme applied on the same data (see § 5.1).
</bodyText>
<subsectionHeader confidence="0.976566">
3.4 Interpreting the magnitude of meas-
</subsectionHeader>
<bodyText confidence="0.9929006">
ures in terms of effective reliability
One last question concerns the interpretation of
the reliability measures magnitude. It has been
particularly investigated with Cohen’s κ. Carletta
(1996) advocates 0.8 to be a threshold of good
reliability, while a value between 0.67 and 0.8 is
considered sufficient to allow tentative conclu-
sion to be drawn. On the opposite, Krippendorff
(2004b) claims that this 0.67 cutoff is a pretty
low standard while Neuendorf (2002) supports
an even more restrictive interpretation.
Thus, the definition of relevant levels of reli-
ability remains an open problem. We will see
how our experiments should draw a methodo-
logical framework to answer this crucial issue.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="method">
4 Experiments: methodology
</sectionHeader>
<subsectionHeader confidence="0.98777">
4.1 Introduction
</subsectionHeader>
<bodyText confidence="0.999313764705882">
We have conducted experiments on three dif-
ferent annotation tasks in order to guarantee an
appreciable generality of our findings. The first
two experiments correspond to an ordinal anno-
tation. They concern the affective dimension of
language (emotion and opinion annotation). They
have been conducted with naïve coders to pre-
serve the spontaneity of judgment which is
searched for in affective computing.
The third experiment concerns coreference
annotation. It is a nominal annotation that has
been designed to be used as a comparison with
the previous ordinal annotations tasks.
The corresponding annotated corpora are
available (TestAccord database) on the french
Parole_Publique1 corpus repository under a CC-
BY-SA Creative Commons licence.
</bodyText>
<subsectionHeader confidence="0.993964">
4.2 Emotion corpus
</subsectionHeader>
<bodyText confidence="0.999982688888889">
Emotion annotation consists in adding emo-
tional information to written messages or speech
transcripts. There is no real consensus about how
an emotion has to be described in an annotation
scheme. Two main approaches can be found in
the literature. On the one hand, emotions are
coded by affective modalities (Scherer, 2005),
among which sadness, disgust, enjoyment, fear,
surprise and anger are the most usual (Ekman,
1999; Cowie and Cornelius, 2003). On the other
hand, an ordinal classification in a multidimen-
sional space is considered. Several dimensions
have been proposed among which three are pre-
vailing (Russell, 1980): valence, intensity and
activation. Activation distinguishes passive from
active emotional states. Valence describes
whether the emotional state conveyed by the text
is positive, negative or neutral. Lastly, intensity
describes the level of emotion conveyed.
Whatever the approach, low to moderate inter-
annotator agreements are observed, what ex-
plains that reference annotation must be achieved
through a majority vote with a significant num-
ber of coders (Schuller and al. 2009). Inter-coder
agreement is particularly low when emotions are
coded into modalities (Devillers and al., 2005;
Callejas and Lopez-Cozar, 2008). This is why
this study focuses on an ordinal annotation.
Our works on emotion detection (Le Tallec
and al., 2011) deal with a specific context: affec-
tive robotics. We consider an affective multimo-
dal interaction between hospitalized children and
a companion robot. Consequently, this experi-
ment will concern a child-dedicated corpus. Al-
though many works already focused on child
language (MacWhinney, 2000), no emotional
child corpus is currently available in French, our
studied language. We have decided to create a
little corpus (230 sentences) of fairy tales, which
are regularly used in works related to child affect
analysis (Alm and al., 2005; Volkova and al.,
2010). The selected texts come from modern
fairy tales (Vassallo, 2004; Vanderheyden, 1995)
which present the interest of being quite confi-
dential. This guarantees that the coders discover
</bodyText>
<footnote confidence="0.96912">
1 www.info.univ-tours.fr/~antoine/parole_publique
</footnote>
<page confidence="0.995101">
552
</page>
<bodyText confidence="0.999587285714286">
the text during the annotation. We asked 25 sub-
jects to characterize the emotional value con-
veyed by every sentence through a 5-items scale
of values, ranging from very negative to very
positive.
As shown on Table 1, this affective scale en-
compasses valence and intensity dimensions. It
enables to compare without methodological bias
an annotation with 3 coding categories (valence:
negative, positive, neutral) and the original 5-
categories (valence+intensity) annotation.
A preliminary experiment showed us that
children meet difficulties to handle a 5-values
emotional scale. This is why the annotation was
conducted on the fairy tales corpus with adults
(11 men/14 women; average age: 31.6 years). All
the coders have a superior level of education (at
least, high-school diploma), they did not know
each other and worked separately during the an-
notation task. Only four of them had a prior ex-
perience in corpus annotation.
</bodyText>
<table confidence="0.984924888888889">
Value Meaning Valence / Intensity /
Polarity Strength
-2 very negative negative strong
-1 moderately negative moderate
negative
0 no emotion neutral none
1 moderately positive moderate
positive
2 very positive positive strong
</table>
<tableCaption confidence="0.999867">
Table 1. emotion or opinion annotation schemes
</tableCaption>
<bodyText confidence="0.999993388888889">
The coders were not trained but were given
precise annotation guidelines providing some
explanations and examples on the emotional val-
ues they had to use. They achieved the annota-
tion once, without any restriction on time. They
had to rely on their own judgment, without con-
sidering any additional information. Sentences
were given in a random order to investigate an
out-of-context perception of emotion. We con-
ducted a second experiment where the order of
the sentences followed the original fairy tale, in
order to study the influence of the discourse con-
text. The criterion of data significance – at least
five chance agreements per category – proposed
by (Krippendorff, 2004) is greatly satisfied for
the valence annotation (3 categories). It is ap-
proached on the complete annotation where we
can assure 4 chance agreements per category.
</bodyText>
<subsectionHeader confidence="0.991216">
4.3 Opinion corpus
</subsectionHeader>
<bodyText confidence="0.999943605263158">
The second experiment concerns opinion an-
notation. Emotion detection can be related to a
certain extent, with opinion mining (or sentiment
analysis), whose aim is to detect the attitude of
people in the texts they produce. A basic task in
opinion mining consists in classifying the polar-
ity of a given text, which should be either a sen-
tence (Wilson and al., 2005), a speech turn or a
complete document (Turney, 2002). Polarity
plays the same role as valence does for affect
analysis: it describes whether the expressed
judgment is positive, negative, or neutral. One
should also characterize the sentiment strength
(Thelwall and al., 2010). This feature can be re-
lated to the notion of intensity used in emotional
annotation. Both polarity and sentiment strength
are considered in our annotation task.
This experiment has been carried out on a cor-
pus of film reviews. The reviews were relatively
short texts written by ordinary people on dedi-
cated French websites (www.senscritique.com
and www.allocine.fr). They concerned the same
French movie. The corpus contains 183 sen-
tences. Its annotation was conducted by the 25
previous subjects. The methodology is identical
to the emotion annotation task. The subjects were
asked to qualify the opinion that was conveyed
by every sentence of the reviews by means of
the same scale of values (Table 1). This scale
encompasses this time the polarity and sentiment
strength dimensions. Once again, the sentences
were given in a random order and contextual or-
der respectively. The criterion of data signifi-
cance is satisfied here too.
On both annotations, experiments with the
random or the contextual order give similar re-
sults. Results from the contextual annotation will
be given only when necessary.
</bodyText>
<subsectionHeader confidence="0.996814">
4.4 Coreference corpus
</subsectionHeader>
<bodyText confidence="0.9994555">
The last experiment concerns coreference an-
notation. We have developed an annotated cor-
pus (ANCOR) which clusters various types of
spontaneous and conversational speech. With a
total of 488,000 lexical units, it is one of the
largest coreference corpora dedicated to spoken
language (Muzerelle and al. 2014). Its annotation
was split into three successive phases:
</bodyText>
<listItem confidence="0.999913666666667">
• Entity mentions marking,
• Referential relations marking,
• Referential relations characterization
</listItem>
<bodyText confidence="0.999646">
The experiment described in this paper con-
cerns the characterization of the referential rela-
tions. This nominal annotation consists in classi-
fying relations among five different types:
</bodyText>
<page confidence="0.989869">
553
</page>
<listItem confidence="0.9709768">
• Direct coreference (DIR) – Coreferent
mentions are NPs with same lexical heads.
• Indirect coreference (IND) – These men-
tions are NPs with distinct lexical heads.
• Pronominal anaphora (PRO) – The subse-
quent coreferent mention is a pronoun.
• Bridging anaphora (BRI) – The subse-
quent mention does not refer to its antece-
dent but depends on it for its referential in-
terpretation (example: meronymy).
• Bridging pronominal anaphora (BPA) –
Bridging anaphora where the subsequent
mention is a pronoun. This type empha-
sizes metonymies (example: Avoid Cen-
tral Hostel... they are unpleasant)
</listItem>
<bodyText confidence="0.999958545454545">
The subjects (3 men / 6 women) were adult
people (average age: 41.2 years) with a high pro-
ficiency in linguistics (researchers in NLP or cor-
pus linguistics). They know each other but
worked separately during the annotation, without
any restriction on time. They are considered as
experts since they participated to the definition
of the annotation guide. The study was con-
ducted on an extract of 10 dialogues, represent-
ing 384 relations. Krippendorff’s (2004) criterion
of significance is therefore satisfied here too.
</bodyText>
<subsectionHeader confidence="0.947883">
4.5 Reliability measures
</subsectionHeader>
<bodyText confidence="0.9950725">
The experiments have been conducted with four
chance-balanced reliability measures2 :
</bodyText>
<listItem confidence="0.995012">
• Multi-κ : multiple coders/binary distance
Cohen’s κ (Davies and Fleiss, 1982),
• Multi-π : multiple coders/binary distance
Scott’s π (Fleiss, 1971),
• αb : Krippendorff’s α with binary distance,
• α : standard Krippendorff’s α with a 1-
dimension Euclidian distance.
</listItem>
<bodyText confidence="0.999681692307692">
The use of Euclidian distance is unfounded on
coreference which handles a nominal annotation.
Thus, α will not be computed on this last corpus.
2 Experiments were also conducted with Cronbach’αc
(Cronbach, 1951). This metrics is based on a correlation
measure. Krippendorff (2009) considers soundly that corre-
lation coefficients are inappropriate to estimate reliability.
Our results show that αc is systematically outperformed by
the other metrics. In particular, it is highly dependent to
coder bias. For instance we observed a relative standard
deviation of αc measures higher than 22% when measuring
the influence of coders set permuation (§ 5.3, table 5). This
observation discards Cronbach’αc �as a trustworthy measure.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.990572">
5.1 Influence of the number of categories
</subsectionHeader>
<bodyText confidence="0.999838">
Our affective coding scheme enables a direct
comparison between a 3-classes (valence or po-
larity) and a 5-classes annotation. The 3-classes
scheme clusters the coding categories with the
same valence or polarity. For instance {-2,-1}
negative values are clustered in the same cate-
gory which receive the index 1. For the computa-
tion of the weighted α, the distance between
negative (-1) and positive (1) classes will be
equal to 2. Table 2 presents the reliability meas-
ures observed on all of the corpora.
</bodyText>
<table confidence="0.999651769230769">
Corpus Emotion (fairy tales)
Metric M-κ M-π αb α
3-classes 0.41 0.41 0.41 0.57
5-classes 0.29 0.29 0.29 0.57
Abs. diff. 0.12 0.12 0.12 0.0
Corpus Opinion (film reviews)
Metric M-κ M-π αb α
3-classes 0.58 0.58 0.58 0.75
5-classes 0.45 0.45 0.45 0.80
Abs. diff. 0.13 0.13 0.13 0.05
Corpus Coreference (spoken dialogues)
Metric M-κ M-π αb α
5-classes 0.69 0.69 0.69 n.s.
</table>
<tableCaption confidence="0.9920325">
Table 2. Reliability measures: emotion and opinion
random annotation as well as coreference annotation
</tableCaption>
<bodyText confidence="0.999887130434783">
Several general conclusions can be drawn
from these figures. At first, low inter-coder
agreements are observed on affective annotation,
which is coherent with many other studies (Dev-
illers and al., 2005; Callejas and Lopez-Cozar,
2008). Non-weighted metrics (multi-κ, multi-π,
αb) range from 0.29 to 0.58, depending on the
annotation scheme. This confirms that these an-
notation tasks are prone to high subjectivity.
Higher levels of agreement may have been ob-
tained if the annotators were trained with super-
vision. As said before, this would have reduced
the spontaneity of judgment. Furthermore, a
comprehensive meta-analysis (Bayerl and Paul,
2011) has shown that no difference may be found
on data reliability between experts and novices.
The reliability measures given by the weighted
version of Krippendorff’s α on the two affective
tasks are significantly higher: α values range
from 0.57 to 0.80, which suggests a rather suffi-
cient reliability. These results are not an artifact.
They come from better disagreement estimation.
For instance, the difference between a positive
</bodyText>
<page confidence="0.996821">
554
</page>
<bodyText confidence="0.999879153846154">
and a negative annotation is more serious than
between the positive and the neutral emotion,
what a weighted metrics accounts for.
Satisfactory measures are found on the con-
trary on the coreference task (0.69 with every
metric). This result was expected, since a large
part of the annotation decisions are based on ob-
jective (syntactic or semantic) considerations.
Whatever the experiment you consider, multi-
x, multi-n and ab coefficients present very close
values (identical until the 3rd decimal). A similar
observation was made by (Arstein and Poesio,
2005) with 18 coders. This validates the theoreti-
cal hypothesis on the convergence of individual-
distribution and single-distribution measures
when the number of coders increases. Our ex-
periments show that annotator bias is moderate
with 25 coders when inter-coders agreement is
rather low (affective tasks), while 9 coders are
enough to guarantee a low annotator bias when
data reliability is higher (coreference task).
Lastly, the comparison between the two anno-
tation schemes (3 or 5 classes) in affective tasks
provides some indications on the influence of the
number of coding categories on reliability esti-
mation3. As expected (see § 3.3), multi-x, multi-n
and ab values increase significantly when the
number of classes decreases.
On the contrary, weighted a is significantly
less affected by the increase of the number of
categories. The a value remains unchanged on
the emotional corpus and its variation restricts to
0.05 on the opinion task. It seems that the use of
a Euclidian distance counterbalances the higher
risk of disagreement when the number of catego-
ries grows. Such an independence of the number
of coding categories is an interesting property for
a reliability measure, which has never been re-
ported as far as we know.
</bodyText>
<table confidence="0.99515875">
Metric M-x M-n ab a
3-classes 0.61 0.61 0.61 0.78
5-classes 0.49 0.49 0.49 0.83
Abs. diff. 0.12 0.12 0.12 0.05
</table>
<tableCaption confidence="0.992597">
Table 3. Reliability measures with 3 and 5 annotation
classes: opinion contextual annotation (film reviews).
</tableCaption>
<bodyText confidence="0.976385">
Finally, Table 3 presents as an illustration the
reliabilities measures we obtained with the con-
textual annotation of the opinion corpus. These
3 The 3-classes coding scheme is a semantic reduction of the
5-classes one. One should wonder whether the same results
can be observed with unrelated categories. (Chu-Ren and
al., 2002) shows indeed that expanding PoS tags with sub-
categories does not increase categorical ambiguity.
results are fully coherent with the previous ones.
One should note in addition that reliability meas-
ures are significantly higher on these contextual
annotations: the context of discourse helps the
coders to qualify opinions more objectively.
</bodyText>
<subsectionHeader confidence="0.997921">
5.2 Influence of prevalence
</subsectionHeader>
<bodyText confidence="0.9999035625">
Table 4 presents the distribution of the annota-
tions on the three corpora. (Devillers and al.,
2005; Callejas and Lopez-Cozar, 2008) reported
that more than 80% of the speech turns are clas-
sified as neutral in their emotional corpora. This
prevalence was not found on our affective cor-
pora. Positive annotations are nearly as frequent
as the neutral ones on the emotion task. This ob-
servation is due to the deliberate emotional na-
ture of fairy tales. Likewise, the neutral opinion
is minority among the film reviews, which aim
frequently at expressing pronounced judgments.
Positive opinions are slightly majority on the
opinion corpus but this prevalence is limited: it
represents an increase of only 50% of frequency,
by comparison with a uniform distribution.
</bodyText>
<table confidence="0.999711615384616">
Corpus Emotion (fairy tales)
5-classes −2 −1 0 1 2
Distribution 8% 17% 38% 23% 14%
3-classes Negative neutral Positive
Distribution 25% 38% 37%
Corpus Opinion (film reviews)
5-classes -2 -1 0 1 2
Distribution 15% 21% 14% 26% 25%
3-classes negative neutral positive
Distribution 36% 14% 51%
Corpus Coreference (spoken dialogues)
5-classes DIR IND PRO BRI BPA
Distribution 40% 7% 42% 10% 1%
</table>
<tableCaption confidence="0.999591">
Table 4. Distribution of the coding categories
</tableCaption>
<bodyText confidence="0.999932555555555">
In the coreference corpus, two classes are
highly dominant, but they are not prevalent
alone. There is no indication in the literature that
the prevalence of two balanced categories has a
bias on data reliability measure. For all these rea-
sons, we didn&apos;t investigate the influence of preva-
lence. Besides, relevant works are questioning
the importance of the influence of prevalence on
inter-coders agreement measures (Vach, 2005).
</bodyText>
<subsectionHeader confidence="0.999331">
5.3 Influence of coders set permutation
</subsectionHeader>
<bodyText confidence="0.999914166666667">
“a coefficient for assessing the reliability of data
must treat coders as interchangeable (Krippen-
dorff, 2004b). We have studied the stability of
reliability measures computed on any combina-
tion of 10 coders (among 25) on the affective
corpora, and 4 coders (among 9) on the corefer-
</bodyText>
<page confidence="0.996144">
555
</page>
<bodyText confidence="0.99130125">
ence corpus. The influence of permutation is
quantified by a measure of relative standard de-
viation (e.g. related to the average value) among
the sets of coders (Table 5).
</bodyText>
<table confidence="0.999711111111111">
Corpus Emotion (fairy tales)
Metric M-κ M-π αb α
3-classes 7.4% 7.7% 7.6% 6.2%
5-classes 9.0% 9.1% 9.1% 6.1%
Corpus Opinion (film reviews)
3-classes 3.4% 3.3% 3.3% 2.6%
5-classes 4.0% 4.0% 4.1% 1.7%
Corpus Coreference (spoken dialogues)
5-classes 4.6% 4.6% 4.6% n.c.
</table>
<tableCaption confidence="0.988637">
Table 5. Relative standard deviation of measures on
any independent sets of coders
</tableCaption>
<bodyText confidence="0.9990726">
Binary metrics do not differ on this criterion:
multi-κ, multi-π and αb present very similar re-
sults. On the opposite, the benefit of a Euclidian
distance of agreement is clear: α is significantly
less influenced by coders set permutation.
</bodyText>
<subsectionHeader confidence="0.979805">
5.4 Influence of the number of coders
</subsectionHeader>
<bodyText confidence="0.999895933333333">
A good way to limit annotator bias is to enroll an
important number of annotators. This need is
unfortunately contradictory with a restriction of
annotation costs. The estimation of data reliabil-
ity must thereby remain trustworthy with a
minimal number of coders. As far as we know,
there is no clear indication in the literature about
the definition of such a minimal size.
We have conducted an experiment which in-
vestigates the influence of the number of coders
on the relevancy of reliability estimation. Con-
sidering N annotations (N=25 for affective anno-
tation and N=9 for coreference annotation), we
compute all the possible reliability values with
any subsets of S coders, S varying from 2 to N.
As an estimation of the trustworthiness of the
coefficients, the relative standard deviation of the
reliability values is computed for every size S
(Figures 1 to 3). The influence of the number of
coders is obvious: detrimental standard devia-
tions are found with small coders set sizes. This
finding concerns above all multi-κ, multi-π and
αb, which present very close behaviors on all
annotations. One the opposite, the weighted
α coefficient converges significantly faster to a
trustworthy reliability measure The comparison
between αb and α is enlightening. It shows again
that the main benefit of Krippendorff’s proposal
results from its accounting for a weighted dis-
tance in a multi-coders ordinal annotation.
</bodyText>
<figureCaption confidence="0.9997575">
Figure 1. Relative standard deviation on any set of
coders of a given size. 5-classes coding scheme. Emo-
tion (top) and opinion (bottom) random annotation.
Figure 2. Relative standard deviation on any set of
coders of a given size. 3-classes coding scheme. Emo-
tion (top) and opinion (bottom) random annotation.
</figureCaption>
<figure confidence="0.999790884615385">
Relative stddev (%)
Relative stddev (%)
40%
20%
30%
10%
0%
20%
15%
10%
5%
0%
Number of coders
Number of coders
Relative stddev (%)
Relative stddev (%)
15%
10%
20%
30%
10%
5%
0%
0%
Number of coders
Number of coders
</figure>
<page confidence="0.90011">
556
</page>
<figureCaption confidence="0.972546">
Figure 3. Relative std deviation of measures on any
sets of coders for a given coders set size: coreference
</figureCaption>
<sectionHeader confidence="0.882534" genericHeader="conclusions">
6 Conclusion and perspectives
</sectionHeader>
<bodyText confidence="0.999715">
Our experiments were conducted on various an-
notation tasks which assure a certain representa-
tiveness of our conclusions:
</bodyText>
<listItem confidence="0.999331">
• Cohen’s κ, Krippendorff‘s α �and Scott’s π�
provide close values when they use the
same measure of disagreement.
• A convergence of these measures has been
</listItem>
<bodyText confidence="0.842552866666667">
noticed in the literature when the number
of coders is high. We observed it even on
very restricted sets of annotators.
• The use of a weighted measure (Euclidian
distance) has several benefits on ordinal
data. It restricts the influence on reliability
measure of both the number of categories
and the number of coders. Unfortunately,
Cohen’s κ statistics cannot consider a
weighted distance in a multi-coders
framework contrary to Krippendorff’s α.
• There is no benefit of using Krippendorff‘s
α on nominal data, since a binary distance
is mandatory on this situation.
To conclude, the main interest of Krippen-
dorff’s α is thus its ability to integrate any kind
of distance. In light of our results, the weighted
version of this coefficient must be preferred
every time an ordinal annotation with multiple
coders is considered.
Our experiments leave open an essential ques-
tion: the objective definition of trustworthy
thresholds of reliability. We propose to investi-
gate this question in terms of expected modifica-
tions of the reference annotation. A majority vote
is generally used as a gold standard to create this
reference with multiple coders. As a preliminary
experiment, we have compared our reference
affective annotations (25 coders) with those ob-
tained on any other included set of coders.
</bodyText>
<figureCaption confidence="0.981690666666667">
Figure 4. Average modifications of the reference ac-
cording to the number of coders. Emotion annotation
(top) and opinion annotation (bottom)
</figureCaption>
<bodyText confidence="0.995618689655172">
Figure 4 presents the average percentage of
modifications of the reference according to the
number of coders. We wonder to what extent
these curves can be related to reliability meas-
ures. It seems indeed that the higher the meas-
ures are, the lower the modifications are too. For
instance, almost all of the coefficients present
higher or equal reliability values with 3 coding
categories (Tables 2 &amp; 3), which corresponds to
lower levels of modifications on Figure 3. Like-
wise, reliability measures are higher on the opin-
ion annotation, where we observe lower modifi-
cations of the reference.
As a result, we expect results like those pre-
sented on figure 4 to enable a direct interpreta-
tion of reliability measures. For instance, with a
multi-κ values of 0.41, or a αb value of 0.57 (Ta-
ble 2, 3-classes emotion annotation), one should
expect around 8% of errors on our reference an-
notation if 10 coders are considered. We plan to
extend these experiments with simultated syn-
thetic data to characterize precisely the relations
between absolute reliability measures and ex-
pected confidence in the reference annotation.
We expect to obtain with simulated annotation a
sufficient variety of agreement to establish sound
recommendations on data reliability thresholds.
We intend to modify randomly human annota-
tions to conduct this simulation.
</bodyText>
<figure confidence="0.999694483870968">
Relative stddev (%)
10%
5%
0%
2 3 4 5 6 7 8
Number of coders
multi-pi
multi-k
binary alpha
50%
40%
30%
20%
10%
0%
number of coders
% of modifications
1 3 5 7 9 11 13 15 17 19 21 23
3 classes
5 classes
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
50,0%
% of modifications
40,0%
30,0%
20,0%
10,0%
0,0%
HC 3 classes
HC 5 classes
number of coders
</figure>
<page confidence="0.979627">
557
</page>
<sectionHeader confidence="0.988257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999625421568627">
Cecilia Alm, Dan Roth, Richard Sproat. 2005. Emo-
tions from Text: Machine Learning for Text-based
Emotion Prediction, In Proc. HLT&amp;EMNLP’2005.
Vancouver, Canada. 579-586
Ron Arstein and Masimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Compu-
tational Linguistics. 34(4):555-596.
Ron Artstein and Massimo Poesio. 2005. Bias de-
creases in proportion to the number of annotators.
In Proceedings FG-MoL’2005, 141:150, Edin-
burgh, UK.
Petra Saskia Bayerl and Karsten Ingmar Paul, 2011.
What Determines Inter-Coder Agreement in Man-
ual Annotations? A Meta-Analytic Investigation .
Computational Linguistics. 37(4), 699:725.
Paul Brennan and Alan Silman. 1992. Statistical
methods for assessing observer variability in clini-
cal measures. BMJ, 304:1491-1494.
Ted Byrt, Janet Bishop, John Carlin. 1993. Bias,
prevalence and kappa. Journal of Clinical Epide-
miology, 46:423-429.
Hermann Brenner and Ulrike Kliebsch. 1996. Depen-
dance of weighted kappa coefficients on the num-
ber of categories. Epidemiology. 7:199-202.
Zoraida Callejas and Ramon Lopez-Cozar. 2008. In-
fluence of contextual information in emotion anno-
tation for spoken dialogue systems, Speech Com-
munication, 50:416-433
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the Kappa statistic. Computational
Linguistics, 22(2):249-254
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. Educational and Psychological
Measurement, 20:37-46.
Jacob Cohen. 1968. Weighted kappa: nominal scale
agreement with provision for scaled disagreement
or partial credit. Psychol. Bulletin, 70(4):213–220
Roddy Cowie and Randolph Cornelius. 2003. De-
scribing the emotional states that are expressed in
speech. Speech Communication. 40 :5-32.
Lee J. Cronbach. 1951. Coefficient alpha and the in-
ternal structure of tests. Psychometrica. 16:297-334
Laurence Devillers, Laurence Vidrascu, Lori Lamel.
2005. Emotion detection in real-life spoken dialogs
recorded in call center. Journal of Neural Net-
works, 18(4):407-422.
Paul Ekman. 1999. Patterns of emotions: New Analy-
sis of Anxiety and Emotion. Plenum Press, New-
York, NY.
Barbara Di Eugenio and Michael Glass. 2004. The
kappa statistic: A second look. Computational Lin-
guistics, 30(1):95–101
Mark Davies and Joseph Fleiss. 1982. Measuring
agreement for multinomial data. Biometrics,
38(4):1047-1051.
Alvan Feinstein and Domenic Cicchetti. 1990. High
agreement but low Kappa : the problem of two
paradoxes. J. of Clinical Epidemiology, 43:543-549
Joseph L. Fleiss. 1971 Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5): 378–382
Andrew Hayes. 2007. Answering the call for a stan-
dard reliability measure for coding data. Communi-
cation Methods and Measures 1, 1:77-89.
Klaus Krippendorff. 2004. Content Analysis: an In-
troduction to its Methodology. Chapter 11. Sage:
Thousand Oaks, CA.
Klaus Krippendorff. 2004b. Reliability in Content
Analysis: Some Common Misconceptions and
Recommendations. Human Communication Re-
search, 30(3): 411-433, 2004
Klaus Krippendorff. 2008. Testing the reliability of
content analysis data: what is involved and why. In
Klaus Krippendorff, Mark Angela Bloch (Eds) The
content analysis reader. Sage Publications. Thou-
sand Oaks, CA.
Klaus Krippendorff. 2009. Testing the reliability of
content analysis data: what is involved and why. In
Klaus Krippendorff , Mary Angela Bock. The Con-
tent Analysis Reader. Sage: Thousand Oaks, CA
Marc Le Tallec, Jeanne Villaneau, Jean-Yves An-
toine, Dominique Duhaut. 2011 Affective Interac-
tion with a Companion Robot for vulnerable Chil-
dren: a Linguistically based Model for Emotion
Detection. In Proc. Language Technology Confer-
ence 2011, Poznan, Poland, 445-450.
Brian MacWhinney. 2000. The CHILDES project :
Tools for analyzing talk. 3rd edition. Lawrence Erl-
baum associates Mahwah, NJ.
Judith Muzerelle, Anaïs Lefeuvre, Emmanuel Schang,
Jean-Yves Antoine, Aurore Pelletier, Denis Mau-
rel, Iris Eshkol, Jeanne Villaneau. 2014. AN-
COR_Centre, a large free spoken French corefer-
ence corpus: description of the resource and reli-
ability measures. In Proc. LREC’2014 (submitted).
Kimberly Neuendorf. 2002. The Content Analysis
Guidebook. Sage Publications, Thousand Oaks, CA
James Russell. 1980. A Circumplex Model of Affect,
J. Personality and Social Psy., 39(6): 1161-1178.
Klaus Scherer. 2005. What are emotions? and how
can they be measured? Social Science Information,
44 (4):694–729.
</reference>
<page confidence="0.97658">
558
</page>
<reference confidence="0.999740135135135">
Björn Schuller, Stefan Steidl, Anto Batliner. 2009.
The Interspeech&apos;2009 emotion challenge. In Pro-
ceedings Interspeech&apos;2009, Brighton, UK. 312:315.
William Scott. 1955. Reliability of content analysis:
the case of nominal scale coding. Public Opinions
Quaterly, 19:321-325.
Julius Sim and Chris Wright. 2005. The Kappa Statis-
tic in Reliability Studies: Use, Interpretation, and
Sample Size Requirements. Physical Therapy,
85(3):257:268.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, Arvid Kappas. 2010. Sentiment strength
detection in short informal text. Journal of the
American Society for Information Science and
Technology, 61 (12): 2544–2558.
Peter Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised clas-
sification of reviews, In Proceedings ACL’02,
Philadelphia, Pennsylvania, 417-424.
Werner Vach, 2005. The dependence of Cohen’s
kappa on the prevalence does not matter, Journal
of Clinical Epidemiology, 58, 655-661).
Rose-Marie Vassallo. 2004. Comment le Grand Nord
découvrit l’été. Flammarion, Paris, France.
Kees Vanderheyden. 1995. Le Noel des animaux de la
montagne. Fairy tale available at the URL :
http://www.momes.net/histoiresillustrees/contesde
montagne/noelanimaux.html
Ekaterina Volkova, Betty Mohler, Detmar Meurers,
Dale Gerdemann and Heinrich Bülthoff. 2010.
Emotional perception of fairy tales: achieving
agreement in emotion annotation of text, In Pro-
ceedings NAACL HLT 2010. Los Angeles, CA.
Theresa Wilson, Janyce Wiebe, Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level
sentiment analysis. In Proc. of HLT-EMNLP’2005.
347-354.
</reference>
<page confidence="0.998627">
559
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.027965">
<title confidence="0.6172752">Krippendorff&apos;s alpha is a more reliable metrics for coders ordinal annotations: experimental studies on emotion, and coreference annotation Jean-Yves Université François Rabelais</title>
<address confidence="0.865507">Tours, LI (EA Blois, France</address>
<email confidence="0.808492">Jean-Yves.Antoine@univ-tours.fr</email>
<affiliation confidence="0.762462666666667">Jeanne Université Européenne Bretagne,</affiliation>
<address confidence="0.995376">Lorient, France</address>
<email confidence="0.870247">Jeanne.Villaneau@univ-ubs.fr</email>
<affiliation confidence="0.726396">Anaïs Université François de Tours, LI (EA</affiliation>
<address confidence="0.951988">Blois, France</address>
<email confidence="0.994933">anais.lefeuvre@univ-tours.fr</email>
<abstract confidence="0.999641555555555">The question of data reliability is of first importance to assess the quality of manually ancorpora. Although Cohen’s the prevailing reliability measure used in NLP, alternative statistics have been proposed. This paper presents an experimental study with four (Cohen’s Scott’s binary and Krippendorff’ s on three tasks: emotion, opinion and coreference annotation. The reported studies investigate the factors of influence (annotator bias, category prevalence, number of coders, number of categories) that should affect reliability estimation. Results show that the use of a weighted measure restricts this influence on ordinal annotations. suggest that weighted the most reliable metrics for such an annotation scheme.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cecilia Alm</author>
<author>Dan Roth</author>
<author>Richard Sproat</author>
</authors>
<title>Emotions from Text: Machine Learning for Text-based Emotion Prediction,</title>
<date>2005</date>
<booktitle>In Proc. HLT&amp;EMNLP’2005.</booktitle>
<pages>579--586</pages>
<location>Vancouver,</location>
<marker>Alm, Roth, Sproat, 2005</marker>
<rawString>Cecilia Alm, Dan Roth, Richard Sproat. 2005. Emotions from Text: Machine Learning for Text-based Emotion Prediction, In Proc. HLT&amp;EMNLP’2005. Vancouver, Canada. 579-586</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Arstein</author>
<author>Masimo Poesio</author>
</authors>
<title>Inter-Coder Agreement for Computational Linguistics. Computational Linguistics.</title>
<date>2008</date>
<pages>34--4</pages>
<contexts>
<context position="2669" citStr="Arstein and Poesio, 2008" startWordPosition="383" endWordPosition="386">ng standard for measuring the reliability of corpus annotation. Many studies have however questioned the limitations of the κ statistic and have proposed alternative measures of reliability. Krippendorff claims that “popularity of κ notwithstanding, Cohen’s κ is simply unsuitable as a measure of the reliability of data” in a paper presenting his α coefficient (Krippendorff, 2008). Except for some rare but noticeable studies (Arstein and Poesio, 2005), most of these critical works restrict to theoretical issues about chance agreement estimation or limitations due to various statistical biases (Arstein and Poesio, 2008). On the opposite, this paper investigates experimentally these questions on three different tasks: emotion, opinion and coreference annotation. Four measures of reliability will be considered: Cohen’s κ (Cohen, 1960), Scott’s π (Scott, 1955) and two measures of Krippendorff’s α (Krippendorff, 2004) with different distance. Section 2 gives a comprehensive presentation of these metrics. Section 3 details the potential methodological biases that should affect the reliability estimation. In section 4, we explain the methodology we followed for this study. Lastly, experimental results are presente</context>
<context position="7896" citStr="Arstein and Poesio 2008" startWordPosition="1213" endWordPosition="1216">timated by a bias index which measures the extent to which the distribution of categories differs from one coder’s annotation to another (Sim and Wright, 2005). Annotator bias has an influence on the magnitude of the reliability measures (Feinstein and Cicchetti,1990). Besides, it concerns the invariance of the measures to the permutation or selection of annotators but also to the number of coders. A review of the literature shows that theoretical studies on annotator bias are not convergent. In particular, opposite arguments have been proposed concerning Cohen’s x (Di Eugenio and Glass 2004, Arstein and Poesio 2008, Hayes, 2007). This is why we have carried on experiments that investigate: • to what extent measures depend on the selection of a specific set of coders (§ 5.3), • to what extent the stability of the measures depends on the number of coders (§ 5.4). Arstein and Poesio (2005) have shown that the greater the number of coders is, the lower the annotator bias decreases. Our aim is to go further this conclusion: we will study whether one measure needs fewer coders than another one to converge towards an acceptable annotator bias. 3.2 Category prevalence Prevalence refers to the influence on relia</context>
</contexts>
<marker>Arstein, Poesio, 2008</marker>
<rawString>Ron Arstein and Masimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. Computational Linguistics. 34(4):555-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Bias decreases in proportion to the number of annotators.</title>
<date>2005</date>
<booktitle>In Proceedings FG-MoL’2005, 141:150,</booktitle>
<location>Edinburgh, UK.</location>
<marker>Artstein, Poesio, 2005</marker>
<rawString>Ron Artstein and Massimo Poesio. 2005. Bias decreases in proportion to the number of annotators. In Proceedings FG-MoL’2005, 141:150, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petra Saskia Bayerl</author>
<author>Karsten Ingmar Paul</author>
</authors>
<title>What Determines Inter-Coder Agreement in Manual Annotations? A Meta-Analytic Investigation .</title>
<date>2011</date>
<journal>Computational Linguistics.</journal>
<volume>37</volume>
<issue>4</issue>
<pages>699--725</pages>
<contexts>
<context position="21877" citStr="Bayerl and Paul, 2011" startWordPosition="3396" endWordPosition="3399">can be drawn from these figures. At first, low inter-coder agreements are observed on affective annotation, which is coherent with many other studies (Devillers and al., 2005; Callejas and Lopez-Cozar, 2008). Non-weighted metrics (multi-κ, multi-π, αb) range from 0.29 to 0.58, depending on the annotation scheme. This confirms that these annotation tasks are prone to high subjectivity. Higher levels of agreement may have been obtained if the annotators were trained with supervision. As said before, this would have reduced the spontaneity of judgment. Furthermore, a comprehensive meta-analysis (Bayerl and Paul, 2011) has shown that no difference may be found on data reliability between experts and novices. The reliability measures given by the weighted version of Krippendorff’s α on the two affective tasks are significantly higher: α values range from 0.57 to 0.80, which suggests a rather sufficient reliability. These results are not an artifact. They come from better disagreement estimation. For instance, the difference between a positive 554 and a negative annotation is more serious than between the positive and the neutral emotion, what a weighted metrics accounts for. Satisfactory measures are found o</context>
</contexts>
<marker>Bayerl, Paul, 2011</marker>
<rawString>Petra Saskia Bayerl and Karsten Ingmar Paul, 2011. What Determines Inter-Coder Agreement in Manual Annotations? A Meta-Analytic Investigation . Computational Linguistics. 37(4), 699:725.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Brennan</author>
<author>Alan Silman</author>
</authors>
<title>Statistical methods for assessing observer variability in clinical measures.</title>
<date>1992</date>
<journal>BMJ,</journal>
<pages>304--1491</pages>
<marker>Brennan, Silman, 1992</marker>
<rawString>Paul Brennan and Alan Silman. 1992. Statistical methods for assessing observer variability in clinical measures. BMJ, 304:1491-1494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Byrt</author>
<author>Janet Bishop</author>
<author>John Carlin</author>
</authors>
<title>Bias, prevalence and kappa.</title>
<date>1993</date>
<journal>Journal of Clinical Epidemiology,</journal>
<pages>46--423</pages>
<marker>Byrt, Bishop, Carlin, 1993</marker>
<rawString>Ted Byrt, Janet Bishop, John Carlin. 1993. Bias, prevalence and kappa. Journal of Clinical Epidemiology, 46:423-429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Brenner</author>
<author>Ulrike Kliebsch</author>
</authors>
<title>Dependance of weighted kappa coefficients on the number of categories.</title>
<date>1996</date>
<journal>Epidemiology.</journal>
<pages>7--199</pages>
<contexts>
<context position="9627" citStr="Brenner and Kliebsch, 1996" startWordPosition="1499" endWordPosition="1502">cted coefficients like the PABAK measure (Byrt and al., 1993), which is a prevalence adjusted and annotator bias adjusted version of Cohen’s κ. The influence of prevalence will not be investigated here, since no category is significantly prevalent in our data. 3.3 Number of coding categories The number of coding categories has an influence on the reliability measures magnitude: the larger the number of categories is, the less the coders have a chance to agree. Even if this decrease should concern chance agreement too, lower reliability estimations are observed with high numbers of categories (Brenner and Kliebsch, 1996). This paper investigates this influence by comparing reliability values obtained with a 3-categories and a 5-categories coding scheme applied on the same data (see § 5.1). 3.4 Interpreting the magnitude of measures in terms of effective reliability One last question concerns the interpretation of the reliability measures magnitude. It has been particularly investigated with Cohen’s κ. Carletta (1996) advocates 0.8 to be a threshold of good reliability, while a value between 0.67 and 0.8 is considered sufficient to allow tentative conclusion to be drawn. On the opposite, Krippendorff (2004b) c</context>
</contexts>
<marker>Brenner, Kliebsch, 1996</marker>
<rawString>Hermann Brenner and Ulrike Kliebsch. 1996. Dependance of weighted kappa coefficients on the number of categories. Epidemiology. 7:199-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoraida Callejas</author>
<author>Ramon Lopez-Cozar</author>
</authors>
<title>Influence of contextual information in emotion annotation for spoken dialogue systems,</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<pages>50--416</pages>
<contexts>
<context position="12588" citStr="Callejas and Lopez-Cozar, 2008" startWordPosition="1944" endWordPosition="1947">sell, 1980): valence, intensity and activation. Activation distinguishes passive from active emotional states. Valence describes whether the emotional state conveyed by the text is positive, negative or neutral. Lastly, intensity describes the level of emotion conveyed. Whatever the approach, low to moderate interannotator agreements are observed, what explains that reference annotation must be achieved through a majority vote with a significant number of coders (Schuller and al. 2009). Inter-coder agreement is particularly low when emotions are coded into modalities (Devillers and al., 2005; Callejas and Lopez-Cozar, 2008). This is why this study focuses on an ordinal annotation. Our works on emotion detection (Le Tallec and al., 2011) deal with a specific context: affective robotics. We consider an affective multimodal interaction between hospitalized children and a companion robot. Consequently, this experiment will concern a child-dedicated corpus. Although many works already focused on child language (MacWhinney, 2000), no emotional child corpus is currently available in French, our studied language. We have decided to create a little corpus (230 sentences) of fairy tales, which are regularly used in works </context>
<context position="21462" citStr="Callejas and Lopez-Cozar, 2008" startWordPosition="3333" endWordPosition="3336">.57 5-classes 0.29 0.29 0.29 0.57 Abs. diff. 0.12 0.12 0.12 0.0 Corpus Opinion (film reviews) Metric M-κ M-π αb α 3-classes 0.58 0.58 0.58 0.75 5-classes 0.45 0.45 0.45 0.80 Abs. diff. 0.13 0.13 0.13 0.05 Corpus Coreference (spoken dialogues) Metric M-κ M-π αb α 5-classes 0.69 0.69 0.69 n.s. Table 2. Reliability measures: emotion and opinion random annotation as well as coreference annotation Several general conclusions can be drawn from these figures. At first, low inter-coder agreements are observed on affective annotation, which is coherent with many other studies (Devillers and al., 2005; Callejas and Lopez-Cozar, 2008). Non-weighted metrics (multi-κ, multi-π, αb) range from 0.29 to 0.58, depending on the annotation scheme. This confirms that these annotation tasks are prone to high subjectivity. Higher levels of agreement may have been obtained if the annotators were trained with supervision. As said before, this would have reduced the spontaneity of judgment. Furthermore, a comprehensive meta-analysis (Bayerl and Paul, 2011) has shown that no difference may be found on data reliability between experts and novices. The reliability measures given by the weighted version of Krippendorff’s α on the two affecti</context>
<context position="25160" citStr="Callejas and Lopez-Cozar, 2008" startWordPosition="3913" endWordPosition="3916">f the 5-classes one. One should wonder whether the same results can be observed with unrelated categories. (Chu-Ren and al., 2002) shows indeed that expanding PoS tags with subcategories does not increase categorical ambiguity. results are fully coherent with the previous ones. One should note in addition that reliability measures are significantly higher on these contextual annotations: the context of discourse helps the coders to qualify opinions more objectively. 5.2 Influence of prevalence Table 4 presents the distribution of the annotations on the three corpora. (Devillers and al., 2005; Callejas and Lopez-Cozar, 2008) reported that more than 80% of the speech turns are classified as neutral in their emotional corpora. This prevalence was not found on our affective corpora. Positive annotations are nearly as frequent as the neutral ones on the emotion task. This observation is due to the deliberate emotional nature of fairy tales. Likewise, the neutral opinion is minority among the film reviews, which aim frequently at expressing pronounced judgments. Positive opinions are slightly majority on the opinion corpus but this prevalence is limited: it represents an increase of only 50% of frequency, by compariso</context>
</contexts>
<marker>Callejas, Lopez-Cozar, 2008</marker>
<rawString>Zoraida Callejas and Ramon Lopez-Cozar. 2008. Influence of contextual information in emotion annotation for spoken dialogue systems, Speech Communication, 50:416-433</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the Kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--2</pages>
<contexts>
<context position="1983" citStr="Carletta (1996)" startWordPosition="281" endWordPosition="282">uation data has led Natural Language Processing (NLP) to develop large annotated corpora. The interest for such enriched language resources has reached domains (semantics, pragmatics, affective computing) where the annotation process is highly affected by the coders subjectivity. The reliability of the resulting annotations must be trusted by measures that assess the inter-coders agreement. While medecine, psychology, and more generally content analysis, have considered for years the issue of data reliability, NLP has only investigated this question from the mid 1990s. The influential work of Carletta (1996) has led the κ statistic (Cohen, 1960) to become the prevailing standard for measuring the reliability of corpus annotation. Many studies have however questioned the limitations of the κ statistic and have proposed alternative measures of reliability. Krippendorff claims that “popularity of κ notwithstanding, Cohen’s κ is simply unsuitable as a measure of the reliability of data” in a paper presenting his α coefficient (Krippendorff, 2008). Except for some rare but noticeable studies (Arstein and Poesio, 2005), most of these critical works restrict to theoretical issues about chance agreement </context>
<context position="10031" citStr="Carletta (1996)" startWordPosition="1561" endWordPosition="1562"> is, the less the coders have a chance to agree. Even if this decrease should concern chance agreement too, lower reliability estimations are observed with high numbers of categories (Brenner and Kliebsch, 1996). This paper investigates this influence by comparing reliability values obtained with a 3-categories and a 5-categories coding scheme applied on the same data (see § 5.1). 3.4 Interpreting the magnitude of measures in terms of effective reliability One last question concerns the interpretation of the reliability measures magnitude. It has been particularly investigated with Cohen’s κ. Carletta (1996) advocates 0.8 to be a threshold of good reliability, while a value between 0.67 and 0.8 is considered sufficient to allow tentative conclusion to be drawn. On the opposite, Krippendorff (2004b) claims that this 0.67 cutoff is a pretty low standard while Neuendorf (2002) supports an even more restrictive interpretation. Thus, the definition of relevant levels of reliability remains an open problem. We will see how our experiments should draw a methodological framework to answer this crucial issue. 4 Experiments: methodology 4.1 Introduction We have conducted experiments on three different anno</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the Kappa statistic. Computational Linguistics, 22(2):249-254</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="2021" citStr="Cohen, 1960" startWordPosition="288" endWordPosition="289">essing (NLP) to develop large annotated corpora. The interest for such enriched language resources has reached domains (semantics, pragmatics, affective computing) where the annotation process is highly affected by the coders subjectivity. The reliability of the resulting annotations must be trusted by measures that assess the inter-coders agreement. While medecine, psychology, and more generally content analysis, have considered for years the issue of data reliability, NLP has only investigated this question from the mid 1990s. The influential work of Carletta (1996) has led the κ statistic (Cohen, 1960) to become the prevailing standard for measuring the reliability of corpus annotation. Many studies have however questioned the limitations of the κ statistic and have proposed alternative measures of reliability. Krippendorff claims that “popularity of κ notwithstanding, Cohen’s κ is simply unsuitable as a measure of the reliability of data” in a paper presenting his α coefficient (Krippendorff, 2008). Except for some rare but noticeable studies (Arstein and Poesio, 2005), most of these critical works restrict to theoretical issues about chance agreement estimation or limitations due to vario</context>
<context position="4472" citStr="Cohen, 1960" startWordPosition="665" endWordPosition="666"> of coding categories should favor chance agreement. What must be estimated is the proportion of observed agreement beyond the one that is expected by chance: (1) Measure = − A A o − e 1 A e 550 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 550–559, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics where Ao is the observed agreement between coders and Ae is an estimation of the possible chance agreement. Reliability metrics differ by the way they estimate this chance agreement. Cohen’s x (Cohen, 1960) defines chance as the statistical independence of the use of coding categories by the annotators. It postulates that chance annotation is governed by prior distributions that are specific to each coder (annotator bias). x was originally developed for two coders and nominal data. (Davies and Fleiss, 1982) has proposed a generalization to any number of coders, while (Cohen, 1968) has defined a weighted version of the x measure that fulfils better the need of reliability estimation for ordinal annotations: the disagreement between two ordinal annotations is no more binary, but depends on a Eucli</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>Weighted kappa: nominal scale agreement with provision for scaled disagreement or partial credit.</title>
<date>1968</date>
<journal>Psychol. Bulletin,</journal>
<volume>70</volume>
<issue>4</issue>
<contexts>
<context position="4853" citStr="Cohen, 1968" startWordPosition="726" endWordPosition="727">omputational Linguistics where Ao is the observed agreement between coders and Ae is an estimation of the possible chance agreement. Reliability metrics differ by the way they estimate this chance agreement. Cohen’s x (Cohen, 1960) defines chance as the statistical independence of the use of coding categories by the annotators. It postulates that chance annotation is governed by prior distributions that are specific to each coder (annotator bias). x was originally developed for two coders and nominal data. (Davies and Fleiss, 1982) has proposed a generalization to any number of coders, while (Cohen, 1968) has defined a weighted version of the x measure that fulfils better the need of reliability estimation for ordinal annotations: the disagreement between two ordinal annotations is no more binary, but depends on a Euclidian distance. This weighted generalization restricts however to a two coders scheme (Artstein and Poesio, 2008): a weighted version of the multi-coders x statistics is still missing. Unlike Cohen’s x, Scott’s n (Scott, 1955) does not aim at modelling annotator bias. It defines chance as the statistical independence of the data and the set of coding categories, independently fro</context>
</contexts>
<marker>Cohen, 1968</marker>
<rawString>Jacob Cohen. 1968. Weighted kappa: nominal scale agreement with provision for scaled disagreement or partial credit. Psychol. Bulletin, 70(4):213–220</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roddy Cowie</author>
<author>Randolph Cornelius</author>
</authors>
<title>Describing the emotional states that are expressed in speech.</title>
<date>2003</date>
<journal>Speech Communication.</journal>
<volume>40</volume>
<pages>5--32</pages>
<contexts>
<context position="11792" citStr="Cowie and Cornelius, 2003" startWordPosition="1829" endWordPosition="1832">rresponding annotated corpora are available (TestAccord database) on the french Parole_Publique1 corpus repository under a CCBY-SA Creative Commons licence. 4.2 Emotion corpus Emotion annotation consists in adding emotional information to written messages or speech transcripts. There is no real consensus about how an emotion has to be described in an annotation scheme. Two main approaches can be found in the literature. On the one hand, emotions are coded by affective modalities (Scherer, 2005), among which sadness, disgust, enjoyment, fear, surprise and anger are the most usual (Ekman, 1999; Cowie and Cornelius, 2003). On the other hand, an ordinal classification in a multidimensional space is considered. Several dimensions have been proposed among which three are prevailing (Russell, 1980): valence, intensity and activation. Activation distinguishes passive from active emotional states. Valence describes whether the emotional state conveyed by the text is positive, negative or neutral. Lastly, intensity describes the level of emotion conveyed. Whatever the approach, low to moderate interannotator agreements are observed, what explains that reference annotation must be achieved through a majority vote with</context>
</contexts>
<marker>Cowie, Cornelius, 2003</marker>
<rawString>Roddy Cowie and Randolph Cornelius. 2003. Describing the emotional states that are expressed in speech. Speech Communication. 40 :5-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee J Cronbach</author>
</authors>
<title>Coefficient alpha and the internal structure of tests.</title>
<date>1951</date>
<journal>Psychometrica.</journal>
<pages>16--297</pages>
<contexts>
<context position="19684" citStr="Cronbach, 1951" startWordPosition="3053" endWordPosition="3054">ore satisfied here too. 4.5 Reliability measures The experiments have been conducted with four chance-balanced reliability measures2 : • Multi-κ : multiple coders/binary distance Cohen’s κ (Davies and Fleiss, 1982), • Multi-π : multiple coders/binary distance Scott’s π (Fleiss, 1971), • αb : Krippendorff’s α with binary distance, • α : standard Krippendorff’s α with a 1- dimension Euclidian distance. The use of Euclidian distance is unfounded on coreference which handles a nominal annotation. Thus, α will not be computed on this last corpus. 2 Experiments were also conducted with Cronbach’αc (Cronbach, 1951). This metrics is based on a correlation measure. Krippendorff (2009) considers soundly that correlation coefficients are inappropriate to estimate reliability. Our results show that αc is systematically outperformed by the other metrics. In particular, it is highly dependent to coder bias. For instance we observed a relative standard deviation of αc measures higher than 22% when measuring the influence of coders set permuation (§ 5.3, table 5). This observation discards Cronbach’αc �as a trustworthy measure. 5 Results 5.1 Influence of the number of categories Our affective coding scheme enabl</context>
</contexts>
<marker>Cronbach, 1951</marker>
<rawString>Lee J. Cronbach. 1951. Coefficient alpha and the internal structure of tests. Psychometrica. 16:297-334</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence Devillers</author>
<author>Laurence Vidrascu</author>
<author>Lori Lamel</author>
</authors>
<title>Emotion detection in real-life spoken dialogs recorded in call center.</title>
<date>2005</date>
<journal>Journal of Neural Networks,</journal>
<pages>18--4</pages>
<marker>Devillers, Vidrascu, Lamel, 2005</marker>
<rawString>Laurence Devillers, Laurence Vidrascu, Lori Lamel. 2005. Emotion detection in real-life spoken dialogs recorded in call center. Journal of Neural Networks, 18(4):407-422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Ekman</author>
</authors>
<title>Patterns of emotions: New Analysis of Anxiety and Emotion.</title>
<date>1999</date>
<publisher>Plenum Press,</publisher>
<location>NewYork, NY.</location>
<contexts>
<context position="11764" citStr="Ekman, 1999" startWordPosition="1827" endWordPosition="1828">tasks. The corresponding annotated corpora are available (TestAccord database) on the french Parole_Publique1 corpus repository under a CCBY-SA Creative Commons licence. 4.2 Emotion corpus Emotion annotation consists in adding emotional information to written messages or speech transcripts. There is no real consensus about how an emotion has to be described in an annotation scheme. Two main approaches can be found in the literature. On the one hand, emotions are coded by affective modalities (Scherer, 2005), among which sadness, disgust, enjoyment, fear, surprise and anger are the most usual (Ekman, 1999; Cowie and Cornelius, 2003). On the other hand, an ordinal classification in a multidimensional space is considered. Several dimensions have been proposed among which three are prevailing (Russell, 1980): valence, intensity and activation. Activation distinguishes passive from active emotional states. Valence describes whether the emotional state conveyed by the text is positive, negative or neutral. Lastly, intensity describes the level of emotion conveyed. Whatever the approach, low to moderate interannotator agreements are observed, what explains that reference annotation must be achieved </context>
</contexts>
<marker>Ekman, 1999</marker>
<rawString>Paul Ekman. 1999. Patterns of emotions: New Analysis of Anxiety and Emotion. Plenum Press, NewYork, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Michael Glass</author>
</authors>
<title>The kappa statistic: A second look.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<marker>Di Eugenio, Glass, 2004</marker>
<rawString>Barbara Di Eugenio and Michael Glass. 2004. The kappa statistic: A second look. Computational Linguistics, 30(1):95–101</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Davies</author>
<author>Joseph Fleiss</author>
</authors>
<title>Measuring agreement for multinomial data.</title>
<date>1982</date>
<journal>Biometrics,</journal>
<pages>38--4</pages>
<contexts>
<context position="4778" citStr="Davies and Fleiss, 1982" startWordPosition="711" endWordPosition="714">guistics, pages 550–559, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics where Ao is the observed agreement between coders and Ae is an estimation of the possible chance agreement. Reliability metrics differ by the way they estimate this chance agreement. Cohen’s x (Cohen, 1960) defines chance as the statistical independence of the use of coding categories by the annotators. It postulates that chance annotation is governed by prior distributions that are specific to each coder (annotator bias). x was originally developed for two coders and nominal data. (Davies and Fleiss, 1982) has proposed a generalization to any number of coders, while (Cohen, 1968) has defined a weighted version of the x measure that fulfils better the need of reliability estimation for ordinal annotations: the disagreement between two ordinal annotations is no more binary, but depends on a Euclidian distance. This weighted generalization restricts however to a two coders scheme (Artstein and Poesio, 2008): a weighted version of the multi-coders x statistics is still missing. Unlike Cohen’s x, Scott’s n (Scott, 1955) does not aim at modelling annotator bias. It defines chance as the statistical i</context>
<context position="19283" citStr="Davies and Fleiss, 1982" startWordPosition="2988" endWordPosition="2991">h proficiency in linguistics (researchers in NLP or corpus linguistics). They know each other but worked separately during the annotation, without any restriction on time. They are considered as experts since they participated to the definition of the annotation guide. The study was conducted on an extract of 10 dialogues, representing 384 relations. Krippendorff’s (2004) criterion of significance is therefore satisfied here too. 4.5 Reliability measures The experiments have been conducted with four chance-balanced reliability measures2 : • Multi-κ : multiple coders/binary distance Cohen’s κ (Davies and Fleiss, 1982), • Multi-π : multiple coders/binary distance Scott’s π (Fleiss, 1971), • αb : Krippendorff’s α with binary distance, • α : standard Krippendorff’s α with a 1- dimension Euclidian distance. The use of Euclidian distance is unfounded on coreference which handles a nominal annotation. Thus, α will not be computed on this last corpus. 2 Experiments were also conducted with Cronbach’αc (Cronbach, 1951). This metrics is based on a correlation measure. Krippendorff (2009) considers soundly that correlation coefficients are inappropriate to estimate reliability. Our results show that αc is systematic</context>
</contexts>
<marker>Davies, Fleiss, 1982</marker>
<rawString>Mark Davies and Joseph Fleiss. 1982. Measuring agreement for multinomial data. Biometrics, 38(4):1047-1051.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvan Feinstein</author>
<author>Domenic Cicchetti</author>
</authors>
<title>High agreement but low Kappa : the problem of two paradoxes.</title>
<date>1990</date>
<journal>J. of Clinical Epidemiology,</journal>
<pages>43--543</pages>
<marker>Feinstein, Cicchetti, 1990</marker>
<rawString>Alvan Feinstein and Domenic Cicchetti. 1990. High agreement but low Kappa : the problem of two paradoxes. J. of Clinical Epidemiology, 43:543-549</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<pages>378--382</pages>
<contexts>
<context position="5620" citStr="Fleiss 1971" startWordPosition="849" endWordPosition="850">wo ordinal annotations is no more binary, but depends on a Euclidian distance. This weighted generalization restricts however to a two coders scheme (Artstein and Poesio, 2008): a weighted version of the multi-coders x statistics is still missing. Unlike Cohen’s x, Scott’s n (Scott, 1955) does not aim at modelling annotator bias. It defines chance as the statistical independence of the data and the set of coding categories, independently from the coders. It considers therefore the annotation process and not the behaviour of the annotators. Scott’s original proposal concerned only two coders. (Fleiss 1971) gave a generalisation of the statistics to any number of coders through a measure of pairwise agreement. Krippendorff‘s a (Krippendorff, 2004) considers chance independently from coders like Scott’s n, but data reliability is estimated depending on disagreement instead of agreement: (2) Alpha = where Do is the observed disagreement between coders and De is an estimation of the possible chance disagreement. Another original aspect of this metrics is to allow disagreement estimation between two categories through any distance measure. This implies that a handles directly any number of coders an</context>
<context position="19353" citStr="Fleiss, 1971" startWordPosition="3000" endWordPosition="3001">w each other but worked separately during the annotation, without any restriction on time. They are considered as experts since they participated to the definition of the annotation guide. The study was conducted on an extract of 10 dialogues, representing 384 relations. Krippendorff’s (2004) criterion of significance is therefore satisfied here too. 4.5 Reliability measures The experiments have been conducted with four chance-balanced reliability measures2 : • Multi-κ : multiple coders/binary distance Cohen’s κ (Davies and Fleiss, 1982), • Multi-π : multiple coders/binary distance Scott’s π (Fleiss, 1971), • αb : Krippendorff’s α with binary distance, • α : standard Krippendorff’s α with a 1- dimension Euclidian distance. The use of Euclidian distance is unfounded on coreference which handles a nominal annotation. Thus, α will not be computed on this last corpus. 2 Experiments were also conducted with Cronbach’αc (Cronbach, 1951). This metrics is based on a correlation measure. Krippendorff (2009) considers soundly that correlation coefficients are inappropriate to estimate reliability. Our results show that αc is systematically outperformed by the other metrics. In particular, it is highly de</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L. Fleiss. 1971 Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5): 378–382</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hayes</author>
</authors>
<title>Answering the call for a standard reliability measure for coding data.</title>
<date>2007</date>
<journal>Communication Methods and Measures</journal>
<volume>1</volume>
<pages>1--77</pages>
<contexts>
<context position="6627" citStr="Hayes, 2007" startWordPosition="1011" endWordPosition="1012">e disagreement. Another original aspect of this metrics is to allow disagreement estimation between two categories through any distance measure. This implies that a handles directly any number of coders and any kind of annotation (nominal or ordinal coding scheme). In this paper, we will consider the a statistics with a binary as well as a Euclidian distance, in order to assess separately the influence of the distance measure and the metrics by itself. 3 Quality criteria for reliability metrics There is an abundant literature about the criteria of quality a reliability measure should satisfy (Hayes, 2007). These works emphasize on two important points: • A trustworthy measure should provide stable results: measures must be reasonably independent of any factor of influence. • The magnitude of the measure must be interpreted in terms of absolute level of reliability: the statistics must come up with trustworthy reliability thresholds. These questions have mainly been investigated from a theoretical point of view. This section summarizes the main conclusions that should be drawn from these critical studies. 3.1 Annotator bias and number of coders Annotator bias refers to the influence of the idio</context>
<context position="7910" citStr="Hayes, 2007" startWordPosition="1217" endWordPosition="1218">hich measures the extent to which the distribution of categories differs from one coder’s annotation to another (Sim and Wright, 2005). Annotator bias has an influence on the magnitude of the reliability measures (Feinstein and Cicchetti,1990). Besides, it concerns the invariance of the measures to the permutation or selection of annotators but also to the number of coders. A review of the literature shows that theoretical studies on annotator bias are not convergent. In particular, opposite arguments have been proposed concerning Cohen’s x (Di Eugenio and Glass 2004, Arstein and Poesio 2008, Hayes, 2007). This is why we have carried on experiments that investigate: • to what extent measures depend on the selection of a specific set of coders (§ 5.3), • to what extent the stability of the measures depends on the number of coders (§ 5.4). Arstein and Poesio (2005) have shown that the greater the number of coders is, the lower the annotator bias decreases. Our aim is to go further this conclusion: we will study whether one measure needs fewer coders than another one to converge towards an acceptable annotator bias. 3.2 Category prevalence Prevalence refers to the influence on reliability estimat</context>
</contexts>
<marker>Hayes, 2007</marker>
<rawString>Andrew Hayes. 2007. Answering the call for a standard reliability measure for coding data. Communication Methods and Measures 1, 1:77-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: an Introduction to its Methodology. Chapter 11. Sage: Thousand Oaks,</title>
<date>2004</date>
<location>CA.</location>
<contexts>
<context position="2969" citStr="Krippendorff, 2004" startWordPosition="427" endWordPosition="429">bility of data” in a paper presenting his α coefficient (Krippendorff, 2008). Except for some rare but noticeable studies (Arstein and Poesio, 2005), most of these critical works restrict to theoretical issues about chance agreement estimation or limitations due to various statistical biases (Arstein and Poesio, 2008). On the opposite, this paper investigates experimentally these questions on three different tasks: emotion, opinion and coreference annotation. Four measures of reliability will be considered: Cohen’s κ (Cohen, 1960), Scott’s π (Scott, 1955) and two measures of Krippendorff’s α (Krippendorff, 2004) with different distance. Section 2 gives a comprehensive presentation of these metrics. Section 3 details the potential methodological biases that should affect the reliability estimation. In section 4, we explain the methodology we followed for this study. Lastly, experimental results are presented in section 5. 2 Reliability measures Any reliability measure considers the most pertinent criterion to estimate data reliability to be reproducibility. Reproducibility can be estimated by observing the agreement among independent annotators (Krippendorff, 2004): the more the coders agree on the da</context>
<context position="5763" citStr="Krippendorff, 2004" startWordPosition="870" endWordPosition="871">oders scheme (Artstein and Poesio, 2008): a weighted version of the multi-coders x statistics is still missing. Unlike Cohen’s x, Scott’s n (Scott, 1955) does not aim at modelling annotator bias. It defines chance as the statistical independence of the data and the set of coding categories, independently from the coders. It considers therefore the annotation process and not the behaviour of the annotators. Scott’s original proposal concerned only two coders. (Fleiss 1971) gave a generalisation of the statistics to any number of coders through a measure of pairwise agreement. Krippendorff‘s a (Krippendorff, 2004) considers chance independently from coders like Scott’s n, but data reliability is estimated depending on disagreement instead of agreement: (2) Alpha = where Do is the observed disagreement between coders and De is an estimation of the possible chance disagreement. Another original aspect of this metrics is to allow disagreement estimation between two categories through any distance measure. This implies that a handles directly any number of coders and any kind of annotation (nominal or ordinal coding scheme). In this paper, we will consider the a statistics with a binary as well as a Euclid</context>
<context position="10223" citStr="Krippendorff (2004" startWordPosition="1593" endWordPosition="1594">er and Kliebsch, 1996). This paper investigates this influence by comparing reliability values obtained with a 3-categories and a 5-categories coding scheme applied on the same data (see § 5.1). 3.4 Interpreting the magnitude of measures in terms of effective reliability One last question concerns the interpretation of the reliability measures magnitude. It has been particularly investigated with Cohen’s κ. Carletta (1996) advocates 0.8 to be a threshold of good reliability, while a value between 0.67 and 0.8 is considered sufficient to allow tentative conclusion to be drawn. On the opposite, Krippendorff (2004b) claims that this 0.67 cutoff is a pretty low standard while Neuendorf (2002) supports an even more restrictive interpretation. Thus, the definition of relevant levels of reliability remains an open problem. We will see how our experiments should draw a methodological framework to answer this crucial issue. 4 Experiments: methodology 4.1 Introduction We have conducted experiments on three different annotation tasks in order to guarantee an appreciable generality of our findings. The first two experiments correspond to an ordinal annotation. They concern the affective dimension of language (e</context>
<context position="15382" citStr="Krippendorff, 2004" startWordPosition="2378" endWordPosition="2379"> guidelines providing some explanations and examples on the emotional values they had to use. They achieved the annotation once, without any restriction on time. They had to rely on their own judgment, without considering any additional information. Sentences were given in a random order to investigate an out-of-context perception of emotion. We conducted a second experiment where the order of the sentences followed the original fairy tale, in order to study the influence of the discourse context. The criterion of data significance – at least five chance agreements per category – proposed by (Krippendorff, 2004) is greatly satisfied for the valence annotation (3 categories). It is approached on the complete annotation where we can assure 4 chance agreements per category. 4.3 Opinion corpus The second experiment concerns opinion annotation. Emotion detection can be related to a certain extent, with opinion mining (or sentiment analysis), whose aim is to detect the attitude of people in the texts they produce. A basic task in opinion mining consists in classifying the polarity of a given text, which should be either a sentence (Wilson and al., 2005), a speech turn or a complete document (Turney, 2002).</context>
<context position="26807" citStr="Krippendorff, 2004" startWordPosition="4176" endWordPosition="4178">Distribution of the coding categories In the coreference corpus, two classes are highly dominant, but they are not prevalent alone. There is no indication in the literature that the prevalence of two balanced categories has a bias on data reliability measure. For all these reasons, we didn&apos;t investigate the influence of prevalence. Besides, relevant works are questioning the importance of the influence of prevalence on inter-coders agreement measures (Vach, 2005). 5.3 Influence of coders set permutation “a coefficient for assessing the reliability of data must treat coders as interchangeable (Krippendorff, 2004b). We have studied the stability of reliability measures computed on any combination of 10 coders (among 25) on the affective corpora, and 4 coders (among 9) on the corefer555 ence corpus. The influence of permutation is quantified by a measure of relative standard deviation (e.g. related to the average value) among the sets of coders (Table 5). Corpus Emotion (fairy tales) Metric M-κ M-π αb α 3-classes 7.4% 7.7% 7.6% 6.2% 5-classes 9.0% 9.1% 9.1% 6.1% Corpus Opinion (film reviews) 3-classes 3.4% 3.3% 3.3% 2.6% 5-classes 4.0% 4.0% 4.1% 1.7% Corpus Coreference (spoken dialogues) 5-classes 4.6%</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus Krippendorff. 2004. Content Analysis: an Introduction to its Methodology. Chapter 11. Sage: Thousand Oaks, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Reliability in Content Analysis: Some Common Misconceptions and Recommendations.</title>
<date>2004</date>
<journal>Human Communication Research,</journal>
<volume>30</volume>
<issue>3</issue>
<pages>411--433</pages>
<contexts>
<context position="2969" citStr="Krippendorff, 2004" startWordPosition="427" endWordPosition="429">bility of data” in a paper presenting his α coefficient (Krippendorff, 2008). Except for some rare but noticeable studies (Arstein and Poesio, 2005), most of these critical works restrict to theoretical issues about chance agreement estimation or limitations due to various statistical biases (Arstein and Poesio, 2008). On the opposite, this paper investigates experimentally these questions on three different tasks: emotion, opinion and coreference annotation. Four measures of reliability will be considered: Cohen’s κ (Cohen, 1960), Scott’s π (Scott, 1955) and two measures of Krippendorff’s α (Krippendorff, 2004) with different distance. Section 2 gives a comprehensive presentation of these metrics. Section 3 details the potential methodological biases that should affect the reliability estimation. In section 4, we explain the methodology we followed for this study. Lastly, experimental results are presented in section 5. 2 Reliability measures Any reliability measure considers the most pertinent criterion to estimate data reliability to be reproducibility. Reproducibility can be estimated by observing the agreement among independent annotators (Krippendorff, 2004): the more the coders agree on the da</context>
<context position="5763" citStr="Krippendorff, 2004" startWordPosition="870" endWordPosition="871">oders scheme (Artstein and Poesio, 2008): a weighted version of the multi-coders x statistics is still missing. Unlike Cohen’s x, Scott’s n (Scott, 1955) does not aim at modelling annotator bias. It defines chance as the statistical independence of the data and the set of coding categories, independently from the coders. It considers therefore the annotation process and not the behaviour of the annotators. Scott’s original proposal concerned only two coders. (Fleiss 1971) gave a generalisation of the statistics to any number of coders through a measure of pairwise agreement. Krippendorff‘s a (Krippendorff, 2004) considers chance independently from coders like Scott’s n, but data reliability is estimated depending on disagreement instead of agreement: (2) Alpha = where Do is the observed disagreement between coders and De is an estimation of the possible chance disagreement. Another original aspect of this metrics is to allow disagreement estimation between two categories through any distance measure. This implies that a handles directly any number of coders and any kind of annotation (nominal or ordinal coding scheme). In this paper, we will consider the a statistics with a binary as well as a Euclid</context>
<context position="10223" citStr="Krippendorff (2004" startWordPosition="1593" endWordPosition="1594">er and Kliebsch, 1996). This paper investigates this influence by comparing reliability values obtained with a 3-categories and a 5-categories coding scheme applied on the same data (see § 5.1). 3.4 Interpreting the magnitude of measures in terms of effective reliability One last question concerns the interpretation of the reliability measures magnitude. It has been particularly investigated with Cohen’s κ. Carletta (1996) advocates 0.8 to be a threshold of good reliability, while a value between 0.67 and 0.8 is considered sufficient to allow tentative conclusion to be drawn. On the opposite, Krippendorff (2004b) claims that this 0.67 cutoff is a pretty low standard while Neuendorf (2002) supports an even more restrictive interpretation. Thus, the definition of relevant levels of reliability remains an open problem. We will see how our experiments should draw a methodological framework to answer this crucial issue. 4 Experiments: methodology 4.1 Introduction We have conducted experiments on three different annotation tasks in order to guarantee an appreciable generality of our findings. The first two experiments correspond to an ordinal annotation. They concern the affective dimension of language (e</context>
<context position="15382" citStr="Krippendorff, 2004" startWordPosition="2378" endWordPosition="2379"> guidelines providing some explanations and examples on the emotional values they had to use. They achieved the annotation once, without any restriction on time. They had to rely on their own judgment, without considering any additional information. Sentences were given in a random order to investigate an out-of-context perception of emotion. We conducted a second experiment where the order of the sentences followed the original fairy tale, in order to study the influence of the discourse context. The criterion of data significance – at least five chance agreements per category – proposed by (Krippendorff, 2004) is greatly satisfied for the valence annotation (3 categories). It is approached on the complete annotation where we can assure 4 chance agreements per category. 4.3 Opinion corpus The second experiment concerns opinion annotation. Emotion detection can be related to a certain extent, with opinion mining (or sentiment analysis), whose aim is to detect the attitude of people in the texts they produce. A basic task in opinion mining consists in classifying the polarity of a given text, which should be either a sentence (Wilson and al., 2005), a speech turn or a complete document (Turney, 2002).</context>
<context position="26807" citStr="Krippendorff, 2004" startWordPosition="4176" endWordPosition="4178">Distribution of the coding categories In the coreference corpus, two classes are highly dominant, but they are not prevalent alone. There is no indication in the literature that the prevalence of two balanced categories has a bias on data reliability measure. For all these reasons, we didn&apos;t investigate the influence of prevalence. Besides, relevant works are questioning the importance of the influence of prevalence on inter-coders agreement measures (Vach, 2005). 5.3 Influence of coders set permutation “a coefficient for assessing the reliability of data must treat coders as interchangeable (Krippendorff, 2004b). We have studied the stability of reliability measures computed on any combination of 10 coders (among 25) on the affective corpora, and 4 coders (among 9) on the corefer555 ence corpus. The influence of permutation is quantified by a measure of relative standard deviation (e.g. related to the average value) among the sets of coders (Table 5). Corpus Emotion (fairy tales) Metric M-κ M-π αb α 3-classes 7.4% 7.7% 7.6% 6.2% 5-classes 9.0% 9.1% 9.1% 6.1% Corpus Opinion (film reviews) 3-classes 3.4% 3.3% 3.3% 2.6% 5-classes 4.0% 4.0% 4.1% 1.7% Corpus Coreference (spoken dialogues) 5-classes 4.6%</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus Krippendorff. 2004b. Reliability in Content Analysis: Some Common Misconceptions and Recommendations. Human Communication Research, 30(3): 411-433, 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Testing the reliability of content analysis data: what is involved and why. In Klaus Krippendorff, Mark Angela Bloch (Eds) The content analysis reader. Sage Publications. Thousand Oaks,</title>
<date>2008</date>
<location>CA.</location>
<contexts>
<context position="2426" citStr="Krippendorff, 2008" startWordPosition="349" endWordPosition="350">y content analysis, have considered for years the issue of data reliability, NLP has only investigated this question from the mid 1990s. The influential work of Carletta (1996) has led the κ statistic (Cohen, 1960) to become the prevailing standard for measuring the reliability of corpus annotation. Many studies have however questioned the limitations of the κ statistic and have proposed alternative measures of reliability. Krippendorff claims that “popularity of κ notwithstanding, Cohen’s κ is simply unsuitable as a measure of the reliability of data” in a paper presenting his α coefficient (Krippendorff, 2008). Except for some rare but noticeable studies (Arstein and Poesio, 2005), most of these critical works restrict to theoretical issues about chance agreement estimation or limitations due to various statistical biases (Arstein and Poesio, 2008). On the opposite, this paper investigates experimentally these questions on three different tasks: emotion, opinion and coreference annotation. Four measures of reliability will be considered: Cohen’s κ (Cohen, 1960), Scott’s π (Scott, 1955) and two measures of Krippendorff’s α (Krippendorff, 2004) with different distance. Section 2 gives a comprehensive</context>
</contexts>
<marker>Krippendorff, 2008</marker>
<rawString>Klaus Krippendorff. 2008. Testing the reliability of content analysis data: what is involved and why. In Klaus Krippendorff, Mark Angela Bloch (Eds) The content analysis reader. Sage Publications. Thousand Oaks, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Testing the reliability of content analysis data: what is involved and why. In Klaus Krippendorff , Mary Angela Bock. The Content Analysis Reader. Sage: Thousand Oaks,</title>
<date>2009</date>
<location>CA</location>
<contexts>
<context position="19753" citStr="Krippendorff (2009)" startWordPosition="3063" endWordPosition="3064">have been conducted with four chance-balanced reliability measures2 : • Multi-κ : multiple coders/binary distance Cohen’s κ (Davies and Fleiss, 1982), • Multi-π : multiple coders/binary distance Scott’s π (Fleiss, 1971), • αb : Krippendorff’s α with binary distance, • α : standard Krippendorff’s α with a 1- dimension Euclidian distance. The use of Euclidian distance is unfounded on coreference which handles a nominal annotation. Thus, α will not be computed on this last corpus. 2 Experiments were also conducted with Cronbach’αc (Cronbach, 1951). This metrics is based on a correlation measure. Krippendorff (2009) considers soundly that correlation coefficients are inappropriate to estimate reliability. Our results show that αc is systematically outperformed by the other metrics. In particular, it is highly dependent to coder bias. For instance we observed a relative standard deviation of αc measures higher than 22% when measuring the influence of coders set permuation (§ 5.3, table 5). This observation discards Cronbach’αc �as a trustworthy measure. 5 Results 5.1 Influence of the number of categories Our affective coding scheme enables a direct comparison between a 3-classes (valence or polarity) and </context>
</contexts>
<marker>Krippendorff, 2009</marker>
<rawString>Klaus Krippendorff. 2009. Testing the reliability of content analysis data: what is involved and why. In Klaus Krippendorff , Mary Angela Bock. The Content Analysis Reader. Sage: Thousand Oaks, CA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Le Tallec</author>
<author>Jeanne Villaneau</author>
<author>Jean-Yves Antoine</author>
<author>Dominique Duhaut</author>
</authors>
<title>Affective Interaction with a Companion Robot for vulnerable Children: a Linguistically based Model for Emotion Detection.</title>
<date>2011</date>
<booktitle>In Proc. Language Technology Conference 2011,</booktitle>
<pages>445--450</pages>
<location>Poznan,</location>
<marker>Le Tallec, Villaneau, Antoine, Duhaut, 2011</marker>
<rawString>Marc Le Tallec, Jeanne Villaneau, Jean-Yves Antoine, Dominique Duhaut. 2011 Affective Interaction with a Companion Robot for vulnerable Children: a Linguistically based Model for Emotion Detection. In Proc. Language Technology Conference 2011, Poznan, Poland, 445-450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
</authors>
<title>The CHILDES project : Tools for analyzing talk. 3rd edition. Lawrence Erlbaum associates Mahwah,</title>
<date>2000</date>
<location>NJ.</location>
<contexts>
<context position="12996" citStr="MacWhinney, 2000" startWordPosition="2008" endWordPosition="2009">ity vote with a significant number of coders (Schuller and al. 2009). Inter-coder agreement is particularly low when emotions are coded into modalities (Devillers and al., 2005; Callejas and Lopez-Cozar, 2008). This is why this study focuses on an ordinal annotation. Our works on emotion detection (Le Tallec and al., 2011) deal with a specific context: affective robotics. We consider an affective multimodal interaction between hospitalized children and a companion robot. Consequently, this experiment will concern a child-dedicated corpus. Although many works already focused on child language (MacWhinney, 2000), no emotional child corpus is currently available in French, our studied language. We have decided to create a little corpus (230 sentences) of fairy tales, which are regularly used in works related to child affect analysis (Alm and al., 2005; Volkova and al., 2010). The selected texts come from modern fairy tales (Vassallo, 2004; Vanderheyden, 1995) which present the interest of being quite confidential. This guarantees that the coders discover 1 www.info.univ-tours.fr/~antoine/parole_publique 552 the text during the annotation. We asked 25 subjects to characterize the emotional value convey</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>Brian MacWhinney. 2000. The CHILDES project : Tools for analyzing talk. 3rd edition. Lawrence Erlbaum associates Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Muzerelle</author>
</authors>
<title>Anaïs Lefeuvre, Emmanuel Schang, Jean-Yves Antoine, Aurore Pelletier, Denis Maurel, Iris Eshkol, Jeanne Villaneau.</title>
<date>2014</date>
<booktitle>In Proc. LREC’2014 (submitted).</booktitle>
<marker>Muzerelle, 2014</marker>
<rawString>Judith Muzerelle, Anaïs Lefeuvre, Emmanuel Schang, Jean-Yves Antoine, Aurore Pelletier, Denis Maurel, Iris Eshkol, Jeanne Villaneau. 2014. ANCOR_Centre, a large free spoken French coreference corpus: description of the resource and reliability measures. In Proc. LREC’2014 (submitted).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimberly Neuendorf</author>
</authors>
<title>The Content Analysis Guidebook. Sage Publications,</title>
<date>2002</date>
<location>Thousand Oaks, CA</location>
<contexts>
<context position="10302" citStr="Neuendorf (2002)" startWordPosition="1606" endWordPosition="1607">bility values obtained with a 3-categories and a 5-categories coding scheme applied on the same data (see § 5.1). 3.4 Interpreting the magnitude of measures in terms of effective reliability One last question concerns the interpretation of the reliability measures magnitude. It has been particularly investigated with Cohen’s κ. Carletta (1996) advocates 0.8 to be a threshold of good reliability, while a value between 0.67 and 0.8 is considered sufficient to allow tentative conclusion to be drawn. On the opposite, Krippendorff (2004b) claims that this 0.67 cutoff is a pretty low standard while Neuendorf (2002) supports an even more restrictive interpretation. Thus, the definition of relevant levels of reliability remains an open problem. We will see how our experiments should draw a methodological framework to answer this crucial issue. 4 Experiments: methodology 4.1 Introduction We have conducted experiments on three different annotation tasks in order to guarantee an appreciable generality of our findings. The first two experiments correspond to an ordinal annotation. They concern the affective dimension of language (emotion and opinion annotation). They have been conducted with naïve coders to p</context>
</contexts>
<marker>Neuendorf, 2002</marker>
<rawString>Kimberly Neuendorf. 2002. The Content Analysis Guidebook. Sage Publications, Thousand Oaks, CA</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Russell</author>
</authors>
<title>A Circumplex Model of Affect,</title>
<date>1980</date>
<journal>J. Personality and Social Psy.,</journal>
<volume>39</volume>
<issue>6</issue>
<pages>1161--1178</pages>
<contexts>
<context position="11968" citStr="Russell, 1980" startWordPosition="1858" endWordPosition="1859">tation consists in adding emotional information to written messages or speech transcripts. There is no real consensus about how an emotion has to be described in an annotation scheme. Two main approaches can be found in the literature. On the one hand, emotions are coded by affective modalities (Scherer, 2005), among which sadness, disgust, enjoyment, fear, surprise and anger are the most usual (Ekman, 1999; Cowie and Cornelius, 2003). On the other hand, an ordinal classification in a multidimensional space is considered. Several dimensions have been proposed among which three are prevailing (Russell, 1980): valence, intensity and activation. Activation distinguishes passive from active emotional states. Valence describes whether the emotional state conveyed by the text is positive, negative or neutral. Lastly, intensity describes the level of emotion conveyed. Whatever the approach, low to moderate interannotator agreements are observed, what explains that reference annotation must be achieved through a majority vote with a significant number of coders (Schuller and al. 2009). Inter-coder agreement is particularly low when emotions are coded into modalities (Devillers and al., 2005; Callejas an</context>
</contexts>
<marker>Russell, 1980</marker>
<rawString>James Russell. 1980. A Circumplex Model of Affect, J. Personality and Social Psy., 39(6): 1161-1178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Scherer</author>
</authors>
<title>What are emotions? and how can they be measured?</title>
<date>2005</date>
<journal>Social Science Information,</journal>
<volume>44</volume>
<pages>4--694</pages>
<contexts>
<context position="11665" citStr="Scherer, 2005" startWordPosition="1812" endWordPosition="1813">al annotation that has been designed to be used as a comparison with the previous ordinal annotations tasks. The corresponding annotated corpora are available (TestAccord database) on the french Parole_Publique1 corpus repository under a CCBY-SA Creative Commons licence. 4.2 Emotion corpus Emotion annotation consists in adding emotional information to written messages or speech transcripts. There is no real consensus about how an emotion has to be described in an annotation scheme. Two main approaches can be found in the literature. On the one hand, emotions are coded by affective modalities (Scherer, 2005), among which sadness, disgust, enjoyment, fear, surprise and anger are the most usual (Ekman, 1999; Cowie and Cornelius, 2003). On the other hand, an ordinal classification in a multidimensional space is considered. Several dimensions have been proposed among which three are prevailing (Russell, 1980): valence, intensity and activation. Activation distinguishes passive from active emotional states. Valence describes whether the emotional state conveyed by the text is positive, negative or neutral. Lastly, intensity describes the level of emotion conveyed. Whatever the approach, low to moderat</context>
</contexts>
<marker>Scherer, 2005</marker>
<rawString>Klaus Scherer. 2005. What are emotions? and how can they be measured? Social Science Information, 44 (4):694–729.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Björn Schuller</author>
<author>Stefan Steidl</author>
<author>Anto Batliner</author>
</authors>
<title>The Interspeech&apos;2009 emotion challenge.</title>
<date>2009</date>
<booktitle>In Proceedings Interspeech&apos;2009,</booktitle>
<pages>312--315</pages>
<location>Brighton, UK.</location>
<marker>Schuller, Steidl, Batliner, 2009</marker>
<rawString>Björn Schuller, Stefan Steidl, Anto Batliner. 2009. The Interspeech&apos;2009 emotion challenge. In Proceedings Interspeech&apos;2009, Brighton, UK. 312:315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Scott</author>
</authors>
<title>Reliability of content analysis: the case of nominal scale coding. Public Opinions Quaterly,</title>
<date>1955</date>
<contexts>
<context position="2911" citStr="Scott, 1955" startWordPosition="419" endWordPosition="420">’s κ is simply unsuitable as a measure of the reliability of data” in a paper presenting his α coefficient (Krippendorff, 2008). Except for some rare but noticeable studies (Arstein and Poesio, 2005), most of these critical works restrict to theoretical issues about chance agreement estimation or limitations due to various statistical biases (Arstein and Poesio, 2008). On the opposite, this paper investigates experimentally these questions on three different tasks: emotion, opinion and coreference annotation. Four measures of reliability will be considered: Cohen’s κ (Cohen, 1960), Scott’s π (Scott, 1955) and two measures of Krippendorff’s α (Krippendorff, 2004) with different distance. Section 2 gives a comprehensive presentation of these metrics. Section 3 details the potential methodological biases that should affect the reliability estimation. In section 4, we explain the methodology we followed for this study. Lastly, experimental results are presented in section 5. 2 Reliability measures Any reliability measure considers the most pertinent criterion to estimate data reliability to be reproducibility. Reproducibility can be estimated by observing the agreement among independent annotators</context>
<context position="5297" citStr="Scott, 1955" startWordPosition="797" endWordPosition="798"> bias). x was originally developed for two coders and nominal data. (Davies and Fleiss, 1982) has proposed a generalization to any number of coders, while (Cohen, 1968) has defined a weighted version of the x measure that fulfils better the need of reliability estimation for ordinal annotations: the disagreement between two ordinal annotations is no more binary, but depends on a Euclidian distance. This weighted generalization restricts however to a two coders scheme (Artstein and Poesio, 2008): a weighted version of the multi-coders x statistics is still missing. Unlike Cohen’s x, Scott’s n (Scott, 1955) does not aim at modelling annotator bias. It defines chance as the statistical independence of the data and the set of coding categories, independently from the coders. It considers therefore the annotation process and not the behaviour of the annotators. Scott’s original proposal concerned only two coders. (Fleiss 1971) gave a generalisation of the statistics to any number of coders through a measure of pairwise agreement. Krippendorff‘s a (Krippendorff, 2004) considers chance independently from coders like Scott’s n, but data reliability is estimated depending on disagreement instead of agr</context>
</contexts>
<marker>Scott, 1955</marker>
<rawString>William Scott. 1955. Reliability of content analysis: the case of nominal scale coding. Public Opinions Quaterly, 19:321-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julius Sim</author>
<author>Chris Wright</author>
</authors>
<title>The Kappa Statistic in Reliability Studies: Use, Interpretation, and Sample Size Requirements. Physical Therapy,</title>
<date>2005</date>
<contexts>
<context position="7432" citStr="Sim and Wright, 2005" startWordPosition="1139" endWordPosition="1142">ude of the measure must be interpreted in terms of absolute level of reliability: the statistics must come up with trustworthy reliability thresholds. These questions have mainly been investigated from a theoretical point of view. This section summarizes the main conclusions that should be drawn from these critical studies. 3.1 Annotator bias and number of coders Annotator bias refers to the influence of the idiosyncratic behavior of the coders. It can be estimated by a bias index which measures the extent to which the distribution of categories differs from one coder’s annotation to another (Sim and Wright, 2005). Annotator bias has an influence on the magnitude of the reliability measures (Feinstein and Cicchetti,1990). Besides, it concerns the invariance of the measures to the permutation or selection of annotators but also to the number of coders. A review of the literature shows that theoretical studies on annotator bias are not convergent. In particular, opposite arguments have been proposed concerning Cohen’s x (Di Eugenio and Glass 2004, Arstein and Poesio 2008, Hayes, 2007). This is why we have carried on experiments that investigate: • to what extent measures depend on the selection of a spec</context>
<context position="8749" citStr="Sim and Wright, 2005" startWordPosition="1356" endWordPosition="1359"> of coders (§ 5.4). Arstein and Poesio (2005) have shown that the greater the number of coders is, the lower the annotator bias decreases. Our aim is to go further this conclusion: we will study whether one measure needs fewer coders than another one to converge towards an acceptable annotator bias. 3.2 Category prevalence Prevalence refers to the influence on reliability estimation of a coding category under which a disproportionate amount of annotated data falls. It can be estimated by a prevalence index which measures the frequency differences of categories on cases where the coders agree (Sim and Wright, 2005). When the prevalence index is D o D − e De 551 high, chance-corrected measures are spuriously reduced since chance agreement is higher in this situation (Brennan and Sliman, 1992; Di Eugenio and Glass, 2004). This yields some authors to propose corrected coefficients like the PABAK measure (Byrt and al., 1993), which is a prevalence adjusted and annotator bias adjusted version of Cohen’s κ. The influence of prevalence will not be investigated here, since no category is significantly prevalent in our data. 3.3 Number of coding categories The number of coding categories has an influence on the </context>
</contexts>
<marker>Sim, Wright, 2005</marker>
<rawString>Julius Sim and Chris Wright. 2005. The Kappa Statistic in Reliability Studies: Use, Interpretation, and Sample Size Requirements. Physical Therapy, 85(3):257:268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
</authors>
<title>Georgios Paltoglou, Di Cai, Arvid Kappas.</title>
<date>2010</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>61</volume>
<issue>12</issue>
<pages>2544--2558</pages>
<marker>Thelwall, Buckley, 2010</marker>
<rawString>Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Di Cai, Arvid Kappas. 2010. Sentiment strength detection in short informal text. Journal of the American Society for Information Science and Technology, 61 (12): 2544–2558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews,</title>
<date>2002</date>
<booktitle>In Proceedings ACL’02,</booktitle>
<pages>417--424</pages>
<location>Philadelphia, Pennsylvania,</location>
<contexts>
<context position="15981" citStr="Turney, 2002" startWordPosition="2479" endWordPosition="2480">endorff, 2004) is greatly satisfied for the valence annotation (3 categories). It is approached on the complete annotation where we can assure 4 chance agreements per category. 4.3 Opinion corpus The second experiment concerns opinion annotation. Emotion detection can be related to a certain extent, with opinion mining (or sentiment analysis), whose aim is to detect the attitude of people in the texts they produce. A basic task in opinion mining consists in classifying the polarity of a given text, which should be either a sentence (Wilson and al., 2005), a speech turn or a complete document (Turney, 2002). Polarity plays the same role as valence does for affect analysis: it describes whether the expressed judgment is positive, negative, or neutral. One should also characterize the sentiment strength (Thelwall and al., 2010). This feature can be related to the notion of intensity used in emotional annotation. Both polarity and sentiment strength are considered in our annotation task. This experiment has been carried out on a corpus of film reviews. The reviews were relatively short texts written by ordinary people on dedicated French websites (www.senscritique.com and www.allocine.fr). They con</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews, In Proceedings ACL’02, Philadelphia, Pennsylvania, 417-424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Werner Vach</author>
</authors>
<title>The dependence of Cohen’s kappa on the prevalence does not matter,</title>
<date>2005</date>
<journal>Journal of Clinical Epidemiology,</journal>
<volume>58</volume>
<pages>655--661</pages>
<contexts>
<context position="26656" citStr="Vach, 2005" startWordPosition="4155" endWordPosition="4156">l positive Distribution 36% 14% 51% Corpus Coreference (spoken dialogues) 5-classes DIR IND PRO BRI BPA Distribution 40% 7% 42% 10% 1% Table 4. Distribution of the coding categories In the coreference corpus, two classes are highly dominant, but they are not prevalent alone. There is no indication in the literature that the prevalence of two balanced categories has a bias on data reliability measure. For all these reasons, we didn&apos;t investigate the influence of prevalence. Besides, relevant works are questioning the importance of the influence of prevalence on inter-coders agreement measures (Vach, 2005). 5.3 Influence of coders set permutation “a coefficient for assessing the reliability of data must treat coders as interchangeable (Krippendorff, 2004b). We have studied the stability of reliability measures computed on any combination of 10 coders (among 25) on the affective corpora, and 4 coders (among 9) on the corefer555 ence corpus. The influence of permutation is quantified by a measure of relative standard deviation (e.g. related to the average value) among the sets of coders (Table 5). Corpus Emotion (fairy tales) Metric M-κ M-π αb α 3-classes 7.4% 7.7% 7.6% 6.2% 5-classes 9.0% 9.1% 9</context>
</contexts>
<marker>Vach, 2005</marker>
<rawString>Werner Vach, 2005. The dependence of Cohen’s kappa on the prevalence does not matter, Journal of Clinical Epidemiology, 58, 655-661).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rose-Marie Vassallo</author>
</authors>
<title>Comment le Grand Nord découvrit l’été. Flammarion,</title>
<date>2004</date>
<location>Paris, France.</location>
<contexts>
<context position="13328" citStr="Vassallo, 2004" startWordPosition="2062" endWordPosition="2063">a specific context: affective robotics. We consider an affective multimodal interaction between hospitalized children and a companion robot. Consequently, this experiment will concern a child-dedicated corpus. Although many works already focused on child language (MacWhinney, 2000), no emotional child corpus is currently available in French, our studied language. We have decided to create a little corpus (230 sentences) of fairy tales, which are regularly used in works related to child affect analysis (Alm and al., 2005; Volkova and al., 2010). The selected texts come from modern fairy tales (Vassallo, 2004; Vanderheyden, 1995) which present the interest of being quite confidential. This guarantees that the coders discover 1 www.info.univ-tours.fr/~antoine/parole_publique 552 the text during the annotation. We asked 25 subjects to characterize the emotional value conveyed by every sentence through a 5-items scale of values, ranging from very negative to very positive. As shown on Table 1, this affective scale encompasses valence and intensity dimensions. It enables to compare without methodological bias an annotation with 3 coding categories (valence: negative, positive, neutral) and the origina</context>
</contexts>
<marker>Vassallo, 2004</marker>
<rawString>Rose-Marie Vassallo. 2004. Comment le Grand Nord découvrit l’été. Flammarion, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees Vanderheyden</author>
</authors>
<title>Le Noel des animaux de la montagne. Fairy tale available at the URL :</title>
<date>1995</date>
<note>http://www.momes.net/histoiresillustrees/contesde montagne/noelanimaux.html</note>
<contexts>
<context position="13349" citStr="Vanderheyden, 1995" startWordPosition="2064" endWordPosition="2065">xt: affective robotics. We consider an affective multimodal interaction between hospitalized children and a companion robot. Consequently, this experiment will concern a child-dedicated corpus. Although many works already focused on child language (MacWhinney, 2000), no emotional child corpus is currently available in French, our studied language. We have decided to create a little corpus (230 sentences) of fairy tales, which are regularly used in works related to child affect analysis (Alm and al., 2005; Volkova and al., 2010). The selected texts come from modern fairy tales (Vassallo, 2004; Vanderheyden, 1995) which present the interest of being quite confidential. This guarantees that the coders discover 1 www.info.univ-tours.fr/~antoine/parole_publique 552 the text during the annotation. We asked 25 subjects to characterize the emotional value conveyed by every sentence through a 5-items scale of values, ranging from very negative to very positive. As shown on Table 1, this affective scale encompasses valence and intensity dimensions. It enables to compare without methodological bias an annotation with 3 coding categories (valence: negative, positive, neutral) and the original 5- categories (vale</context>
</contexts>
<marker>Vanderheyden, 1995</marker>
<rawString>Kees Vanderheyden. 1995. Le Noel des animaux de la montagne. Fairy tale available at the URL : http://www.momes.net/histoiresillustrees/contesde montagne/noelanimaux.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Volkova</author>
<author>Betty Mohler</author>
<author>Detmar Meurers</author>
<author>Dale Gerdemann</author>
<author>Heinrich Bülthoff</author>
</authors>
<title>Emotional perception of fairy tales: achieving agreement in emotion annotation of text,</title>
<date>2010</date>
<booktitle>In Proceedings NAACL HLT</booktitle>
<location>Los Angeles, CA.</location>
<marker>Volkova, Mohler, Meurers, Gerdemann, Bülthoff, 2010</marker>
<rawString>Ekaterina Volkova, Betty Mohler, Detmar Meurers, Dale Gerdemann and Heinrich Bülthoff. 2010. Emotional perception of fairy tales: achieving agreement in emotion annotation of text, In Proceedings NAACL HLT 2010. Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP’2005.</booktitle>
<pages>347--354</pages>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proc. of HLT-EMNLP’2005. 347-354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>