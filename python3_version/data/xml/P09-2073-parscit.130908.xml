<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.033071">
<title confidence="0.9981135">
Automatic Cost Estimation for Tree Edit Distance Using Particle Swarm
Optimization
</title>
<author confidence="0.997216">
Yashar Mehdad
</author>
<affiliation confidence="0.999084">
University of Trento and FBK - Irst
</affiliation>
<address confidence="0.606129">
Trento, Italy
</address>
<email confidence="0.998302">
mehdad@fbk.eu
</email>
<sectionHeader confidence="0.993882" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919">
Recently, there is a growing interest in
working with tree-structured data in differ-
ent applications and domains such as com-
putational biology and natural language
processing. Moreover, many applications
in computational linguistics require the
computation of similarities over pair of
syntactic or semantic trees. In this context,
Tree Edit Distance (TED) has been widely
used for many years. However, one of the
main constraints of this method is to tune
the cost of edit operations, which makes
it difficult or sometimes very challenging
in dealing with complex problems. In this
paper, we propose an original method to
estimate and optimize the operation costs
in TED, applying the Particle Swarm Op-
timization algorithm. Our experiments on
Recognizing Textual Entailment show the
success of this method in automatic esti-
mation, rather than manual assignment of
edit costs.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986782608696">
Among many tree-based algorithms, Tree Edit
Distance (TED) has offered many solutions for
various NLP applications such as information re-
trieval, information extraction, similarity estima-
tion and textual entailment. Tree edit distance is
defined as the minimum costly set of basic oper-
ations transforming one tree to another. In com-
mon, TED approaches use an initial fixed cost for
each operation.
Generally, the initial assigned cost to each edit
operation depends on the nature of nodes, appli-
cations and dataset. For example the probabil-
ity of deleting a function word from a string is
not the same as deleting a symbol in RNA struc-
ture. According to this fact, tree comparison may
be affected by application and dataset. A solu-
tion to this problem is assigning the cost to each
edit operation empirically or based on the expert
knowledge and recommendation. These methods
emerge a critical problem when the domain, field
or application is new and the level of expertise and
empirical knowledge is very limited.
Other approaches towards this problem tried to
learn a generative or discriminative probabilistic
model (Bernard et al., 2008) from the data. One
of the drawbacks of those approaches is that the
cost values of edit operations are hidden behind
the probabilistic model. Additionally, the cost can
not be weighted or varied according to the tree
context and node location.
In order to overcome these drawbacks, we are
proposing a stochastic method based on Particle
Swarm Optimization (PSO) to estimate the cost of
each edit operation based on the user defined ap-
plication and dataset. A further advantage of the
method, besides automatic learning of the opera-
tion costs, is to investigate the cost values in order
to better understand how TED approaches the ap-
plication and data in different domains.
As for the experiments, we learn a model for
recognizing textual entailment, based on TED,
where the input is a pair of strings represented as
syntactic dependency trees. Our results illustrate
that optimizing the cost of each operation can dra-
matically affect the accuracy and achieve a better
model for recognizing textual entailment.
</bodyText>
<sectionHeader confidence="0.976508" genericHeader="method">
2 Tree Edit Distance
</sectionHeader>
<bodyText confidence="0.998395625">
Tree edit distance measure is a similarity metric
for rooted ordered trees. Assuming that we have
two rooted and ordered trees, it means that one
node in each tree is assigned as a root and the
children of each node are ordered. The edit op-
erations on the nodes a and b between trees are
defined as: Insertion (A —* a), Deletion (a —* A)
and Substitution (a —* b). Each edit operation has
</bodyText>
<page confidence="0.979684">
289
</page>
<note confidence="0.925434">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 289–292,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9999643">
an associated cost (denoted as -y(a —* b)). An
edit script on two trees is a sequence of edit op-
erations changing a tree to another. Consequently,
the cost of an edit script is the sum of the costs of
its edit operations. Based on the main definition
of this approach, TED is the cost of minimum cost
edit script between two trees (Zhang and Shasha,
1989).
In the classic TED, a cost value is assigned to
each operation initially, and the distance is com-
puted based on the initial cost values. Considering
that the distance can vary in different domains and
datasets, converging to an optimal set of values for
operations is almost empirically impossible. In
the following sections, we propose a method for
estimating the optimum set of values for opera-
tion costs in TED algorithm. Our method is built
on adapting the PSO optimization approach as a
search process to automate the procedure of cost
estimation.
</bodyText>
<sectionHeader confidence="0.982693" genericHeader="method">
3 Particle Swarm Optimization
</sectionHeader>
<bodyText confidence="0.9997669375">
PSO is a stochastic optimization technique which
was introduced recently based on the social be-
haviour of bird flocking and fish schooling (Eber-
hart et al., 2001). PSO is one of the population-
based search methods which takes advantage of
the concept of social sharing of information. In
this algorithm each particle can learn from the ex-
perience of other particles in the same population
(called swarm). In other words, each particle in
the iterative search process would adjust its fly-
ing velocity as well as position not only based on
its own acquaintance but also other particles’ fly-
ing experience in the swarm. This algorithm has
found efficient in solving a number of engineering
problems. PSO is mainly built on the following
equations.
</bodyText>
<equation confidence="0.998856">
Xi = Xi + VZ (1)
Vi = wV + c1r1(Xbi − Xi)
+ c2r2(Xgi − Xi) (2)
</equation>
<bodyText confidence="0.999977">
To be concise, for each particle at each itera-
tion, the position Xi (Equation 1) and velocity Vi
(Equation 2) is updated. Xbi is the best position
of the particle during its past routes and Xgi is
the best global position over all routes travelled
by the particles of the swarm. r1 and r2 are ran-
dom variables drawn from a uniform distribution
in the range [0,1], while c1 and c2 are two accel-
eration constants regulating the relative velocities
with respect to the best local and global positions.
The weight w is used as a tradeoff between the
global and local best positions. It is usually se-
lected slightly less than 1 for better global explo-
ration (Melgani and Bazi, 2008). Position opti-
mally is computed based on the fitness function
defined in association with the related problem.
Both position and velocity are updated during the
iterations until convergence is reached or iterations
attain the maximum number defined by the user.
</bodyText>
<sectionHeader confidence="0.996641" genericHeader="method">
4 Automatic Cost Optimization for TED
</sectionHeader>
<bodyText confidence="0.999934">
In this section we proposed a system for estimat-
ing and optimizing the cost of each edit operation
for TED. As mentioned earlier, the aim of this sys-
tem is to find the optimal set of operation costs to:
1) improve the performance of TED in different
applications, and 2) provide some information on
how different operations in TED approach an ap-
plication or dataset. In order to obtain this, the
system is developed using an optimization frame-
work based on PSO.
</bodyText>
<subsectionHeader confidence="0.966664">
4.1 PSO Setup
</subsectionHeader>
<bodyText confidence="0.999946166666667">
One of the most important steps in applying PSO
is to define a fitness function, which could lead
the swarm to the optimized particles based on the
application and data. The choice of this function
is very crucial since, based on this, PSO evalu-
ates the quality of each candidate particle for driv-
ing the solution space to optimization. Moreover,
this function should be, possibly, application and
data independent, as well as flexible enough to be
adapted to the TED based problems. With the in-
tention of accomplishing these goals, we define
two main fitness functions as follows:
</bodyText>
<listItem confidence="0.871612333333333">
1) Bhattacharyya Distance: This statistical
measure determines the similarity of two discrete
probability distributions (Bhattacharyya, 1943).
In classification, this method is used to mea-
sure the distance between two different classes.
Put it differently, maximizing the Bhattacharyya
distance would increase the separability of two
classes.
2) Accuracy: By maximizing the accuracy ob-
tained from 10 fold cross-validation on the devel-
opment set, as the fitness function, we estimate the
optimized cost of the edit operations.
</listItem>
<page confidence="0.982574">
290
</page>
<subsectionHeader confidence="0.753765">
4.2 Integrating TED with PSO
</subsectionHeader>
<bodyText confidence="0.999631666666667">
The procedure to estimate and optimize the cost
of edit operations in TED applying the PSO algo-
rithm, is as follows.
</bodyText>
<listItem confidence="0.983053611111111">
a) Initialization
1) Generate a random swarm of size n (cost of
edit operations).
2) For each position of the particle from the
swarm, obtain the fitness function value.
3) Set the best position of each particle with its
initial position (Xbi).
b) Search
4) Detect the best global position (Xgi) in the
swarm based on maximum value of the fit-
ness function over all explored routes.
5) Update the velocity of each particle (VZ).
6) Update the position of each particle (Xi).
7) For each candidate particle calculate the fit-
ness function.
8) Update the best position of each particle if
the current position has a larger value.
c) Convergence
</listItem>
<bodyText confidence="0.683174666666667">
9) Run till the maximum number of iteration
(in our case set to 10) is reached or start the
search process.
</bodyText>
<sectionHeader confidence="0.997264" genericHeader="method">
5 Experimental Design
</sectionHeader>
<bodyText confidence="0.999946444444444">
Our experiments were conducted on the basis of
Recognizing Textual Entailment (RTE) datasets1.
Textual Entailment can be explained as an associ-
ation between a coherent text(T) and a language
expression, called hypothesis(H). The entailment
function for the pair T-H returns the true value
when the meaning of H can be inferred from the
meaning of T and false otherwise. In another
word, Textual Entailment can be defined as hu-
man reading comprehension task. One of the ap-
proaches to textual entailment problem is based on
the distance between T and H.
In this approach, the entailment score for a pair
is calculated on the minimal set of edit operations
that transform T into H. An entailment relation is
assigned to a T-H pair in the case that overall cost
of the transformations is below a certain thresh-
old. The threshold, which corresponds to tree edit
</bodyText>
<page confidence="0.729453">
1http://www.pascal-network.org/Challenges/RTE1-4
</page>
<bodyText confidence="0.999911638297872">
distace, is empirically estimated over the dataset.
This method was implemented by (Kouylekov and
Magnini, 2005), based on TED algorithm (Zhang
and Shasha, 1989). Each RTE dataset includes
its own development and test set, however, RTE-4
was released only as a test set and the data from
RTE-1 to RTE-3 were exploited as development
set for evaluating RTE-4 data.
In order to deal with TED approach to textual
entailment, we used EDITS2 package (Edit Dis-
tance Textual Entailment Suite) (Magnini et al.,
2009). In addition, We partially exploit JSwarm-
PSO3 package with some adaptations as an im-
plementation of PSO algorithm. Each pair in the
datasets converted to two syntactic dependency
trees using Stanford statistical parser4, developed
in the Stanford university NLP group by (Klein
and Manning, 2003).
We conducted six different experiments in two
sets on each RTE dataset. The costs were esti-
mated on the training set, then we evaluate the es-
timated costs on the test set. In the first set of ex-
periments, we set a simple cost scheme based on
three operations. Implementing this cost scheme,
we expect to optimize the cost of each edit opera-
tion without considering that the operation costs
may vary based on different characteristics of a
node, such as size, location or content. The results
were obtained using: 1) The random cost assign-
ment, 2) Assigning the cost based on the exper-
tise knowledge and intuition (So called Intuitive),
and 3) Automatic estimated and optimized cost for
each operation. In the second case, we applied the
same cost values which was used in EDITS by its
developers (Magnini et al., 2009).
In the second set of experiments, we tried to
take advantage of an advanced cost scheme with
more fine-grained operations to assign a weight to
the edit operations based on the characteristics of
the nodes (Magnini et al., 2009). For example if a
node is in the list of stop-words, the deletion cost
should be different from the cost of deleting a con-
tent word. By this intuition, we tried to optimize 9
specialized costs for edit operations (A swarm of
size 9). At each experiment, both fitness functions
were applied and the best results were chosen for
presentation.
</bodyText>
<footnote confidence="0.999703333333333">
2http://edits.fbk.eu/
3http://jswarm-pso.sourceforge.net/
4http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<page confidence="0.987673">
291
</page>
<table confidence="0.9910072">
Data set
Model RTE4 RTE3 RTE2 RTE1
Random 49.6 53.62 50.37 50.5
Simple Intuitive 51.3 59.6 56.5 49.8
Optimized 56.5 61.62 58 58.12
Random 53.60 52.0 54.62 53.5
Adv. Intuitive 57.6 59.37 57.75 55.5
Optimized 59.5 62.4 59.87 58.62
Baseline 57.19
RTE-4 Challenge 57.0
</table>
<tableCaption confidence="0.762368">
Table 1: Comparison of accuracy on all RTE
datasets based on optimized and unoptimized cost
schemes.
</tableCaption>
<sectionHeader confidence="0.99988" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999974166666667">
Our results are summarized in Table 1. We show
the accuracy gained by a distance-based base-
line for textual entailment (Mehdad and Magnini,
2009) in compare with the results achieved by the
random, intuitive and optimized cost schemes us-
ing EDITS system. For the better comparison,
we also present the results of the EDITS system
(Cabrio et al., 2008) in RTE-4 challenge using
combination of different distances as features for
classification (Cabrio et al., 2008).
Table 1 shows that, in all datasets, accuracy im-
proved up to 9% by optimizing the cost of each
edit operation. Results prove that, the optimized
cost scheme enhances the quality of the system
performance even more than the cost scheme used
by the experts (Intuitive cost scheme). Further-
more, using the fine-grained and weighted cost
scheme for edit operations we could achieve the
highest results in accuracy. Moreover, by explor-
ing the estimated optimal cost of each operation,
we could find even some linguistics phenomena
which exists in the dataset. For instance, in most
of the cases, the cost of deletion was estimated
zero, which shows that deleting the words from
the text does not effect the distance in the entail-
ment pairs. In addition, the optimized model can
reflect more consistency and stability (from 58 to
62 in accuracy) than other models, while in unop-
timized models the result varies more, on different
datasets (from 50 in RTE-1 to 59 in RTE-3).
</bodyText>
<sectionHeader confidence="0.998934" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999555">
In this paper, we proposed a novel approach for es-
timating the cost of edit operations in TED. This
model has the advantage of being efficient and
more transparent than probabilistic approaches as
well as having less complexity. The easy imple-
mentation of this approach, besides its flexibility,
makes it suitable to be applied in real world appli-
cations. The experimental results on textual entail-
ment, as one of the challenging problems in NLP,
confirm our claim.
</bodyText>
<sectionHeader confidence="0.996676" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.962669833333333">
Besides my special thanks to F. Melgani, B.
Magnini and M. Kouylekov for their academic and
technical support, I acknowledge the reviewers for
their comments. The EDITS system has been sup-
ported by the EU-funded project QALL-ME (FP6
IST-033860).
</bodyText>
<sectionHeader confidence="0.996758" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999898657142857">
M. Bernard, L. Boyer, A. Habrard, and M. Sebban.
2008. Learning probabilistic models of tree edit dis-
tance. Pattern Recogn., 41(8):2611–2629.
A. Bhattacharyya. 1943. On a measure of diver-
gence between two statistical populations defined by
probability distributions. Bull. Calcutta Math. Soc.,
35:99109.
E. Cabrio, M. Kouylekovand, and B. Magnini. 2008.
Combining specialized entailment engines for rte-4.
In Proceedings of TAC08, 4th PASCAL Challenges
Workshop on Recognising Textual Entailment.
R. C. Eberhart, Y. Shi, and J. Kennedy. 2001. Swarm
Intelligence. The Morgan Kaufmann Series in Arti-
ficial Intelligence.
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Advances in Neural Information Pro-
cessing Systems 15, Cambridge, MA. MIT Press.
M. Kouylekov and B. Magnini. 2005. Recognizing
textual entailment with tree edit distance algorithms.
In PASCAL Challenges on RTE, pages 17–20.
B. Magnini, M. Kouylekov, and E. Cabrio. 2009. Edits
- Edit Distance Textual Entailment Suite User Man-
ual. Available at http://edits.fbk.eu/.
Y. Mehdad and B. Magnini. 2009. A word overlap
baseline for the recognizing textual entailment task.
Available at http://edits.fbk.eu/.
F. Melgani and Y. Bazi. 2008. Classification of elec-
trocardiogram signals with support vector machines
and particle swarm optimization. IEEE Transac-
tions on Information Technology in Biomedicine,
12(5):667–677.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245–1262.
</reference>
<page confidence="0.997489">
292
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.730212">
<title confidence="0.9997635">Automatic Cost Estimation for Tree Edit Distance Using Particle Swarm Optimization</title>
<author confidence="0.978279">Yashar Mehdad</author>
<affiliation confidence="0.999544">University of Trento and FBK - Irst</affiliation>
<address confidence="0.76505">Trento, Italy</address>
<email confidence="0.996922">mehdad@fbk.eu</email>
<abstract confidence="0.999011782608696">Recently, there is a growing interest in working with tree-structured data in different applications and domains such as computational biology and natural language processing. Moreover, many applications in computational linguistics require the computation of similarities over pair of syntactic or semantic trees. In this context, Tree Edit Distance (TED) has been widely used for many years. However, one of the main constraints of this method is to tune the cost of edit operations, which makes it difficult or sometimes very challenging in dealing with complex problems. In this paper, we propose an original method to estimate and optimize the operation costs in TED, applying the Particle Swarm Optimization algorithm. Our experiments on Recognizing Textual Entailment show the success of this method in automatic estimation, rather than manual assignment of edit costs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Bernard</author>
<author>L Boyer</author>
<author>A Habrard</author>
<author>M Sebban</author>
</authors>
<title>Learning probabilistic models of tree edit distance.</title>
<date>2008</date>
<journal>Pattern Recogn.,</journal>
<volume>41</volume>
<issue>8</issue>
<contexts>
<context position="2203" citStr="Bernard et al., 2008" startWordPosition="341" endWordPosition="344">example the probability of deleting a function word from a string is not the same as deleting a symbol in RNA structure. According to this fact, tree comparison may be affected by application and dataset. A solution to this problem is assigning the cost to each edit operation empirically or based on the expert knowledge and recommendation. These methods emerge a critical problem when the domain, field or application is new and the level of expertise and empirical knowledge is very limited. Other approaches towards this problem tried to learn a generative or discriminative probabilistic model (Bernard et al., 2008) from the data. One of the drawbacks of those approaches is that the cost values of edit operations are hidden behind the probabilistic model. Additionally, the cost can not be weighted or varied according to the tree context and node location. In order to overcome these drawbacks, we are proposing a stochastic method based on Particle Swarm Optimization (PSO) to estimate the cost of each edit operation based on the user defined application and dataset. A further advantage of the method, besides automatic learning of the operation costs, is to investigate the cost values in order to better und</context>
</contexts>
<marker>Bernard, Boyer, Habrard, Sebban, 2008</marker>
<rawString>M. Bernard, L. Boyer, A. Habrard, and M. Sebban. 2008. Learning probabilistic models of tree edit distance. Pattern Recogn., 41(8):2611–2629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bhattacharyya</author>
</authors>
<title>On a measure of divergence between two statistical populations defined by probability distributions.</title>
<date>1943</date>
<journal>Bull. Calcutta Math. Soc.,</journal>
<pages>35--99109</pages>
<contexts>
<context position="7682" citStr="Bhattacharyya, 1943" startWordPosition="1278" endWordPosition="1279">warm to the optimized particles based on the application and data. The choice of this function is very crucial since, based on this, PSO evaluates the quality of each candidate particle for driving the solution space to optimization. Moreover, this function should be, possibly, application and data independent, as well as flexible enough to be adapted to the TED based problems. With the intention of accomplishing these goals, we define two main fitness functions as follows: 1) Bhattacharyya Distance: This statistical measure determines the similarity of two discrete probability distributions (Bhattacharyya, 1943). In classification, this method is used to measure the distance between two different classes. Put it differently, maximizing the Bhattacharyya distance would increase the separability of two classes. 2) Accuracy: By maximizing the accuracy obtained from 10 fold cross-validation on the development set, as the fitness function, we estimate the optimized cost of the edit operations. 290 4.2 Integrating TED with PSO The procedure to estimate and optimize the cost of edit operations in TED applying the PSO algorithm, is as follows. a) Initialization 1) Generate a random swarm of size n (cost of e</context>
</contexts>
<marker>Bhattacharyya, 1943</marker>
<rawString>A. Bhattacharyya. 1943. On a measure of divergence between two statistical populations defined by probability distributions. Bull. Calcutta Math. Soc., 35:99109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Cabrio</author>
<author>M Kouylekovand</author>
<author>B Magnini</author>
</authors>
<title>Combining specialized entailment engines for rte-4.</title>
<date>2008</date>
<booktitle>In Proceedings of TAC08, 4th PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="12931" citStr="Cabrio et al., 2008" startWordPosition="2138" endWordPosition="2141">zed 56.5 61.62 58 58.12 Random 53.60 52.0 54.62 53.5 Adv. Intuitive 57.6 59.37 57.75 55.5 Optimized 59.5 62.4 59.87 58.62 Baseline 57.19 RTE-4 Challenge 57.0 Table 1: Comparison of accuracy on all RTE datasets based on optimized and unoptimized cost schemes. 6 Results Our results are summarized in Table 1. We show the accuracy gained by a distance-based baseline for textual entailment (Mehdad and Magnini, 2009) in compare with the results achieved by the random, intuitive and optimized cost schemes using EDITS system. For the better comparison, we also present the results of the EDITS system (Cabrio et al., 2008) in RTE-4 challenge using combination of different distances as features for classification (Cabrio et al., 2008). Table 1 shows that, in all datasets, accuracy improved up to 9% by optimizing the cost of each edit operation. Results prove that, the optimized cost scheme enhances the quality of the system performance even more than the cost scheme used by the experts (Intuitive cost scheme). Furthermore, using the fine-grained and weighted cost scheme for edit operations we could achieve the highest results in accuracy. Moreover, by exploring the estimated optimal cost of each operation, we co</context>
</contexts>
<marker>Cabrio, Kouylekovand, Magnini, 2008</marker>
<rawString>E. Cabrio, M. Kouylekovand, and B. Magnini. 2008. Combining specialized entailment engines for rte-4. In Proceedings of TAC08, 4th PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Eberhart</author>
<author>Y Shi</author>
<author>J Kennedy</author>
</authors>
<title>Swarm Intelligence.</title>
<date>2001</date>
<booktitle>Series in Artificial Intelligence.</booktitle>
<publisher>The Morgan Kaufmann</publisher>
<contexts>
<context position="4860" citStr="Eberhart et al., 2001" startWordPosition="793" endWordPosition="797"> cost values. Considering that the distance can vary in different domains and datasets, converging to an optimal set of values for operations is almost empirically impossible. In the following sections, we propose a method for estimating the optimum set of values for operation costs in TED algorithm. Our method is built on adapting the PSO optimization approach as a search process to automate the procedure of cost estimation. 3 Particle Swarm Optimization PSO is a stochastic optimization technique which was introduced recently based on the social behaviour of bird flocking and fish schooling (Eberhart et al., 2001). PSO is one of the populationbased search methods which takes advantage of the concept of social sharing of information. In this algorithm each particle can learn from the experience of other particles in the same population (called swarm). In other words, each particle in the iterative search process would adjust its flying velocity as well as position not only based on its own acquaintance but also other particles’ flying experience in the swarm. This algorithm has found efficient in solving a number of engineering problems. PSO is mainly built on the following equations. Xi = Xi + VZ (1) V</context>
</contexts>
<marker>Eberhart, Shi, Kennedy, 2001</marker>
<rawString>R. C. Eberhart, Y. Shi, and J. Kennedy. 2001. Swarm Intelligence. The Morgan Kaufmann Series in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15,</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="10699" citStr="Klein and Manning, 2003" startWordPosition="1771" endWordPosition="1774">ncludes its own development and test set, however, RTE-4 was released only as a test set and the data from RTE-1 to RTE-3 were exploited as development set for evaluating RTE-4 data. In order to deal with TED approach to textual entailment, we used EDITS2 package (Edit Distance Textual Entailment Suite) (Magnini et al., 2009). In addition, We partially exploit JSwarmPSO3 package with some adaptations as an implementation of PSO algorithm. Each pair in the datasets converted to two syntactic dependency trees using Stanford statistical parser4, developed in the Stanford university NLP group by (Klein and Manning, 2003). We conducted six different experiments in two sets on each RTE dataset. The costs were estimated on the training set, then we evaluate the estimated costs on the test set. In the first set of experiments, we set a simple cost scheme based on three operations. Implementing this cost scheme, we expect to optimize the cost of each edit operation without considering that the operation costs may vary based on different characteristics of a node, such as size, location or content. The results were obtained using: 1) The random cost assignment, 2) Assigning the cost based on the expertise knowledge</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems 15, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kouylekov</author>
<author>B Magnini</author>
</authors>
<title>Recognizing textual entailment with tree edit distance algorithms.</title>
<date>2005</date>
<booktitle>In PASCAL Challenges on RTE,</booktitle>
<pages>17--20</pages>
<contexts>
<context position="10006" citStr="Kouylekov and Magnini, 2005" startWordPosition="1659" endWordPosition="1662">tual Entailment can be defined as human reading comprehension task. One of the approaches to textual entailment problem is based on the distance between T and H. In this approach, the entailment score for a pair is calculated on the minimal set of edit operations that transform T into H. An entailment relation is assigned to a T-H pair in the case that overall cost of the transformations is below a certain threshold. The threshold, which corresponds to tree edit 1http://www.pascal-network.org/Challenges/RTE1-4 distace, is empirically estimated over the dataset. This method was implemented by (Kouylekov and Magnini, 2005), based on TED algorithm (Zhang and Shasha, 1989). Each RTE dataset includes its own development and test set, however, RTE-4 was released only as a test set and the data from RTE-1 to RTE-3 were exploited as development set for evaluating RTE-4 data. In order to deal with TED approach to textual entailment, we used EDITS2 package (Edit Distance Textual Entailment Suite) (Magnini et al., 2009). In addition, We partially exploit JSwarmPSO3 package with some adaptations as an implementation of PSO algorithm. Each pair in the datasets converted to two syntactic dependency trees using Stanford sta</context>
</contexts>
<marker>Kouylekov, Magnini, 2005</marker>
<rawString>M. Kouylekov and B. Magnini. 2005. Recognizing textual entailment with tree edit distance algorithms. In PASCAL Challenges on RTE, pages 17–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>M Kouylekov</author>
<author>E Cabrio</author>
</authors>
<title>Edits - Edit Distance Textual Entailment Suite User Manual. Available at http://edits.fbk.eu/.</title>
<date>2009</date>
<contexts>
<context position="10402" citStr="Magnini et al., 2009" startWordPosition="1726" endWordPosition="1729">ertain threshold. The threshold, which corresponds to tree edit 1http://www.pascal-network.org/Challenges/RTE1-4 distace, is empirically estimated over the dataset. This method was implemented by (Kouylekov and Magnini, 2005), based on TED algorithm (Zhang and Shasha, 1989). Each RTE dataset includes its own development and test set, however, RTE-4 was released only as a test set and the data from RTE-1 to RTE-3 were exploited as development set for evaluating RTE-4 data. In order to deal with TED approach to textual entailment, we used EDITS2 package (Edit Distance Textual Entailment Suite) (Magnini et al., 2009). In addition, We partially exploit JSwarmPSO3 package with some adaptations as an implementation of PSO algorithm. Each pair in the datasets converted to two syntactic dependency trees using Stanford statistical parser4, developed in the Stanford university NLP group by (Klein and Manning, 2003). We conducted six different experiments in two sets on each RTE dataset. The costs were estimated on the training set, then we evaluate the estimated costs on the test set. In the first set of experiments, we set a simple cost scheme based on three operations. Implementing this cost scheme, we expect </context>
<context position="11749" citStr="Magnini et al., 2009" startWordPosition="1953" endWordPosition="1956">istics of a node, such as size, location or content. The results were obtained using: 1) The random cost assignment, 2) Assigning the cost based on the expertise knowledge and intuition (So called Intuitive), and 3) Automatic estimated and optimized cost for each operation. In the second case, we applied the same cost values which was used in EDITS by its developers (Magnini et al., 2009). In the second set of experiments, we tried to take advantage of an advanced cost scheme with more fine-grained operations to assign a weight to the edit operations based on the characteristics of the nodes (Magnini et al., 2009). For example if a node is in the list of stop-words, the deletion cost should be different from the cost of deleting a content word. By this intuition, we tried to optimize 9 specialized costs for edit operations (A swarm of size 9). At each experiment, both fitness functions were applied and the best results were chosen for presentation. 2http://edits.fbk.eu/ 3http://jswarm-pso.sourceforge.net/ 4http://nlp.stanford.edu/software/lex-parser.shtml 291 Data set Model RTE4 RTE3 RTE2 RTE1 Random 49.6 53.62 50.37 50.5 Simple Intuitive 51.3 59.6 56.5 49.8 Optimized 56.5 61.62 58 58.12 Random 53.60 5</context>
</contexts>
<marker>Magnini, Kouylekov, Cabrio, 2009</marker>
<rawString>B. Magnini, M. Kouylekov, and E. Cabrio. 2009. Edits - Edit Distance Textual Entailment Suite User Manual. Available at http://edits.fbk.eu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Mehdad</author>
<author>B Magnini</author>
</authors>
<title>A word overlap baseline for the recognizing textual entailment task. Available at http://edits.fbk.eu/.</title>
<date>2009</date>
<contexts>
<context position="12725" citStr="Mehdad and Magnini, 2009" startWordPosition="2103" endWordPosition="2106">dits.fbk.eu/ 3http://jswarm-pso.sourceforge.net/ 4http://nlp.stanford.edu/software/lex-parser.shtml 291 Data set Model RTE4 RTE3 RTE2 RTE1 Random 49.6 53.62 50.37 50.5 Simple Intuitive 51.3 59.6 56.5 49.8 Optimized 56.5 61.62 58 58.12 Random 53.60 52.0 54.62 53.5 Adv. Intuitive 57.6 59.37 57.75 55.5 Optimized 59.5 62.4 59.87 58.62 Baseline 57.19 RTE-4 Challenge 57.0 Table 1: Comparison of accuracy on all RTE datasets based on optimized and unoptimized cost schemes. 6 Results Our results are summarized in Table 1. We show the accuracy gained by a distance-based baseline for textual entailment (Mehdad and Magnini, 2009) in compare with the results achieved by the random, intuitive and optimized cost schemes using EDITS system. For the better comparison, we also present the results of the EDITS system (Cabrio et al., 2008) in RTE-4 challenge using combination of different distances as features for classification (Cabrio et al., 2008). Table 1 shows that, in all datasets, accuracy improved up to 9% by optimizing the cost of each edit operation. Results prove that, the optimized cost scheme enhances the quality of the system performance even more than the cost scheme used by the experts (Intuitive cost scheme).</context>
</contexts>
<marker>Mehdad, Magnini, 2009</marker>
<rawString>Y. Mehdad and B. Magnini. 2009. A word overlap baseline for the recognizing textual entailment task. Available at http://edits.fbk.eu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Melgani</author>
<author>Y Bazi</author>
</authors>
<title>Classification of electrocardiogram signals with support vector machines and particle swarm optimization.</title>
<date>2008</date>
<journal>IEEE Transactions on Information Technology in Biomedicine,</journal>
<volume>12</volume>
<issue>5</issue>
<contexts>
<context position="6182" citStr="Melgani and Bazi, 2008" startWordPosition="1031" endWordPosition="1034">e position Xi (Equation 1) and velocity Vi (Equation 2) is updated. Xbi is the best position of the particle during its past routes and Xgi is the best global position over all routes travelled by the particles of the swarm. r1 and r2 are random variables drawn from a uniform distribution in the range [0,1], while c1 and c2 are two acceleration constants regulating the relative velocities with respect to the best local and global positions. The weight w is used as a tradeoff between the global and local best positions. It is usually selected slightly less than 1 for better global exploration (Melgani and Bazi, 2008). Position optimally is computed based on the fitness function defined in association with the related problem. Both position and velocity are updated during the iterations until convergence is reached or iterations attain the maximum number defined by the user. 4 Automatic Cost Optimization for TED In this section we proposed a system for estimating and optimizing the cost of each edit operation for TED. As mentioned earlier, the aim of this system is to find the optimal set of operation costs to: 1) improve the performance of TED in different applications, and 2) provide some information on </context>
</contexts>
<marker>Melgani, Bazi, 2008</marker>
<rawString>F. Melgani and Y. Bazi. 2008. Classification of electrocardiogram signals with support vector machines and particle swarm optimization. IEEE Transactions on Information Technology in Biomedicine, 12(5):667–677.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zhang</author>
<author>D Shasha</author>
</authors>
<title>Simple fast algorithms for the editing distance between trees and related problems.</title>
<date>1989</date>
<journal>SIAM J. Comput.,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="4113" citStr="Zhang and Shasha, 1989" startWordPosition="671" endWordPosition="674"> nodes a and b between trees are defined as: Insertion (A —* a), Deletion (a —* A) and Substitution (a —* b). Each edit operation has 289 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 289–292, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP an associated cost (denoted as -y(a —* b)). An edit script on two trees is a sequence of edit operations changing a tree to another. Consequently, the cost of an edit script is the sum of the costs of its edit operations. Based on the main definition of this approach, TED is the cost of minimum cost edit script between two trees (Zhang and Shasha, 1989). In the classic TED, a cost value is assigned to each operation initially, and the distance is computed based on the initial cost values. Considering that the distance can vary in different domains and datasets, converging to an optimal set of values for operations is almost empirically impossible. In the following sections, we propose a method for estimating the optimum set of values for operation costs in TED algorithm. Our method is built on adapting the PSO optimization approach as a search process to automate the procedure of cost estimation. 3 Particle Swarm Optimization PSO is a stocha</context>
<context position="10055" citStr="Zhang and Shasha, 1989" startWordPosition="1667" endWordPosition="1670">hension task. One of the approaches to textual entailment problem is based on the distance between T and H. In this approach, the entailment score for a pair is calculated on the minimal set of edit operations that transform T into H. An entailment relation is assigned to a T-H pair in the case that overall cost of the transformations is below a certain threshold. The threshold, which corresponds to tree edit 1http://www.pascal-network.org/Challenges/RTE1-4 distace, is empirically estimated over the dataset. This method was implemented by (Kouylekov and Magnini, 2005), based on TED algorithm (Zhang and Shasha, 1989). Each RTE dataset includes its own development and test set, however, RTE-4 was released only as a test set and the data from RTE-1 to RTE-3 were exploited as development set for evaluating RTE-4 data. In order to deal with TED approach to textual entailment, we used EDITS2 package (Edit Distance Textual Entailment Suite) (Magnini et al., 2009). In addition, We partially exploit JSwarmPSO3 package with some adaptations as an implementation of PSO algorithm. Each pair in the datasets converted to two syntactic dependency trees using Stanford statistical parser4, developed in the Stanford unive</context>
</contexts>
<marker>Zhang, Shasha, 1989</marker>
<rawString>K. Zhang and D. Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. SIAM J. Comput., 18(6):1245–1262.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>