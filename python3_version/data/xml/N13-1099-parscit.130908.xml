<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.051272">
<title confidence="0.989786">
Improving the Quality of Minority Class Identification in Dialog Act Tagging
</title>
<author confidence="0.969962">
Adinoyi Omuya Vinodkumar Prabhakaran Owen Rambow
</author>
<affiliation confidence="0.954808">
10gen CS, Columbia University CCLS, Columbia University
</affiliation>
<address confidence="0.887931">
New York, NY, USA New York, NY, USA New York, NY, USA
</address>
<email confidence="0.997165">
wisdom@10gen.com vinod@cs.columbia.edu rambow@ccls.columbia.edu
</email>
<sectionHeader confidence="0.995607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971888888889">
We present a method of improving the perfor-
mance of dialog act tagging in identifying mi-
nority classes by using per-class feature opti-
mization and a method of choosing the class
based not on confidence, but on a cascade of
classifiers. We show that it gives a minor-
ity class F-measure error reduction of 22.8%,
while also reducing the error for other classes
and the overall error by about 10%.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960725">
In this paper, we discuss dialog act tagging, the
task of assigning a dialog act to an utterance, where
a dialog act (DA) is a high-level categorization of
the pragmatic meaning of the utterance. Our data is
email. Our starting point is the tagger described in
(Hu et al., 2009), which uses a standard multi-class
classifier based on support vector machines (SVMs).
While the performance of this system is pretty good
as measured by accuracy, it performs badly on the
DA REQUEST-ACTION, which is a rare class. Multi-
class SVMs are typically implemented as a set of
SVMs, one per class, with the overall choice of class
being determined by the SVM with the highest con-
fidence (“one-against-all”). Multi-class SVMs are
typically packaged as a single system, whose inner
workings are ignored by the NLP researcher. In this
paper we show that, for our problem of DA classi-
fication, we can boost the performance of the rare
classes (while maintaining the overall performance)
by performing feature optimization separately for
each individual classifier. But we also show that we
can achieve an all-around error reduction by alter-
ing the method by which the multi-class classifier
combines the individual SVMs. This new method
of combination is a simple cascade: we run the in-
dividual classifiers in ascending order of frequency
of the classes in the training corpus; the first classi-
fier to classify the data point positively determines
the choice of the overall classifier. If no classifier
classifies the data point positively, we use the usual
confidence-based method. This new method obtains
a 22.8% error reduction for the minority class, and
around 10% error reduction for the other classes and
for the overall classifier.
This paper is structured as follows. We start out
by discussing related work (Section 2). We then
present our data in Section 3, and in Section 4 we
present the experiments with our systems and the re-
sults. We report the results of an extrinsic evaluation
in Section 5, and conclude.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999950461538462">
Dialog act (DA) annotations and tagging, inspired
by the speech act theory of Austin (1975) and Searle
(1976), have been used in the NLP community to un-
derstand and model dialog. Initial work was done on
spoken interactions (see for example (Stolcke et al.,
2000)). Recently, studies have explored dialog act
tagging in written interactions such as emails (Co-
hen et al., 2004), forums (Kim et al., 2006; Kim et
al., 2010b), instant messaging (Kim et al., 2010a)
and Twitter (Zhang et al., 2012). Most DA tagging
systems for written interactions use a message/post
level tagging scheme, and allow multiple tags for
each message/post. In such a tagging scheme, indi-
</bodyText>
<page confidence="0.972493">
802
</page>
<subsectionHeader confidence="0.296181">
Proceedings of NAACL-HLT 2013, pages 802–807,
</subsectionHeader>
<bodyText confidence="0.964363">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
vidual binary classifiers for each tag are independent
of one another. However, recent studies have found
merit in segmenting each message into functional
units and assigning a single DA to each segment (Hu
et al., 2009). Our work falls in this paradigm (we
choose a single DA for smaller textual units). We
build on the work by (Hu et al., 2009); we improve
their dialog act predicting performance on minority
classes using per-class feature optimization.
</bodyText>
<sectionHeader confidence="0.996156" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999769727272727">
In this study, we use the email corpus presented in
(Hu et al., 2009), which is manually annotated for
DA tags. The corpus contains 122 email threads
with a total of 360 messages and 20,740 word to-
kens. This set of email threads is chosen from a ver-
sion of the Enron email corpus with some missing
messages restored from other emails in which they
were quoted (Yeh and Harnly, 2006; Agarwal et al.,
2012). Most emails are concerned with exchanging
information, scheduling meetings, or solving prob-
lems, but there are also purely social emails.
</bodyText>
<table confidence="0.999012">
Dialog Act Tag Count (%)
REQUEST-ACTION (R-A) 35 (2.5%)
REQUEST-INFORMATION (R-I) 151 (10.7%)
CONVENTIONAL (CONV) 357 (25.4%)
INFORM (INF) 853 (60.7%)
Total # of DFUs 1406
</table>
<tableCaption confidence="0.999948">
Table 1: Annotation statistics
</tableCaption>
<bodyText confidence="0.999884555555556">
Each message in the thread is segmented into Di-
alog Functional Units (DFUs). A DFU is a con-
tiguous span within an email message which has
a coherent communicative intention. Each DFU
is assigned a single DA label which is one of the
following: REQUEST-ACTION (R-A), REQUEST-
INFORMATION (R-I), CONVENTIONAL (CONV)
and INFORM (INF). There are three other DA labels
— INFORM-OFFLINE, COMMIT, and NODA for no
dialog act — which occurred 5 or fewer times in the
corpus. We ignore these DA labels in this paper. The
corpus also contains links between the DFUs, but we
do not use those annotations in this study. Table 1
presents the distribution of DA labels in our corpus.
We now describe each of the DAs we consider in our
experiments.
In a REQUEST-ACTION, the writer signals
her desire that the reader perform some non-
communicative act, i.e., an act that cannot in itself
be part of the dialogue. For example, a writer can
ask the reader to write a report or make coffee.
In a REQUEST-INFORMATION, the writer signals
her desire that the reader perform a specific com-
municative act, namely that he provide information
(either facts or opinion).
In an INFORM, the writer conveys information, or
more precisely, the writer signals that her desire that
the reader adopt a certain belief. It covers many dif-
ferent types of information that can be conveyed in-
cluding answers to questions, beliefs (committed or
not), attitudes, and elaborations on prior DAs.
A CONVENTIONAL dialog act does not signal any
specific communicative intention on the part of the
writer, but rather it helps structure and thus facilitate
the communication. Examples include greetings, in-
troductions, expressions of gratitude, etc.
</bodyText>
<sectionHeader confidence="0.996154" genericHeader="method">
4 System
</sectionHeader>
<bodyText confidence="0.9999085">
We developed four systems for our experiments: a
baseline (BAS) system which is close to the system
described in (Hu et al., 2009), and three variants of
our novel divide and conquer (DAC) system. Fea-
tures used in both systems are extracted as explained
in Section 4.2. Section 4.3 describes the baseline
system, the basic DAC system, and two variations
of the DAC system.
</bodyText>
<subsectionHeader confidence="0.971266">
4.1 Experimental Framework
</subsectionHeader>
<bodyText confidence="0.9999251">
In all our experiments, we use linear kernel Sup-
port Vector Machines (SVM). However, across the
systems, there are differences in how we use them.
Our framework was built with the ClearTK toolkit
(Ogren et al., 2008) with its wrapper for SVMLight
(Joachims, 1999). The ClearTK wrapper internally
shifts the prediction threshold based on posterior
probabilistic scores calculated using the algorithm
of Lin et al. (2007). We report results from 5-fold
cross validation performed on the entire corpus.
</bodyText>
<subsectionHeader confidence="0.994783">
4.2 Feature Engineering
</subsectionHeader>
<bodyText confidence="0.9998685">
In developing our system, we classified our features
into three categories: lexical, verbal and message-
</bodyText>
<page confidence="0.99255">
803
</page>
<bodyText confidence="0.999972625">
level. Lexical features consists of n-grams of words,
n-grams of POS tags, mixed n-grams of closed class
words and POS tags (Prabhakaran et al., 2012), as
well as a small set of specialized features — Start-
POS/Lemma (POS tag and lemma of the first word),
LastPOS/Lemma (POS tag and lemma of the last
word), MDCount (number of modal verbs in the
DFU) and QuestionMark (is there a question mark
in the DFU). We used the POS tags produced by the
OpenNLP POS tagger. Verbal features capture the
position and identity of the first verb in the DFU. Fi-
nally, message-level features capture aspects of the
location of the DFU in the message and of the mes-
sage in the thread (relative position and size). In
optimizing each system, we first performed an ex-
haustive search across all combinations of features
within each category. For the lexical n-gram fea-
tures we varied the n-gram window from 1 to 5. This
step gave us the best performing feature combination
within each category. In a second step, we found the
best combination of categories, using the previously
determined features for each category. In this pa-
per, we do not report best performing feature sets
for each configuration, due to lack of space.
</bodyText>
<subsectionHeader confidence="0.986392">
4.3 Experiments
</subsectionHeader>
<bodyText confidence="0.994675">
Baseline (BAS) System This system uses the
ClearTK built-in one-versus-all multiclass SVM in
prediction. Internally, the multi-class SVM builds
a set of binary classifiers, one for each dialog act.
For a given test instance, the classifier that obtains
the highest probability score determines the overall
prediction. We performed feature optimization on
the whole multiclass classifier (as described in Sec-
tion 4.2), i.e., the same set of features was available
to all component classifiers. We optimized for sys-
tem accuracy. Table 2 shows results using this sys-
tem. In this and all tables, we give the performance
of the system on the four DAs, using precision, re-
call, and F-measure. The DAs are listed in ascend-
ing order of frequency in the corpus (least frequent
DA first). We also give an overall accuracy evalua-
tion. As we can see, detecting REQUEST-ACTION is
much harder than detecting the other DAs.
Basic Divide and Conquer (DAC) System Like
the BAS system, the DAC system also builds a bi-
nary classifier for each dialog act separately, and the
</bodyText>
<table confidence="0.999682666666667">
Prec. Rec. F-meas.
R-A 57.9 31.4 40.7
R-I 91.5 78.2 84.3
CONV 92.0 95.8 93.8
INF 91.6 95.1 93.3
Accuracy 91.3
</table>
<tableCaption confidence="0.985074">
Table 2: Results for baseline (BAS) system (standard
multiclass SVM)
</tableCaption>
<bodyText confidence="0.997989285714286">
component classifier with highest probability score
determines the overall prediction. The crucial dif-
ference in the DAC system is that the feature opti-
mization is performed for each component classifier
separately. Each component classifier is optimized
for F-measure. Table 3 shows results using this sys-
tem.
</bodyText>
<table confidence="0.999579666666667">
Prec. Recall F-meas. ER
R-A 66.7 40.0 50.0 15.6
R-I 91.5 78.2 84.3 0.0
CONV 93.9 94.1 94.0 2.6
INF 91.4 96.1 93.7 5.7
Accuracy 91.7 4.9
</table>
<tableCaption confidence="0.907236">
Table 3: Results for basic DAC system (per-class feature
optimization followed by maximum confidence based
choice); “ER” refers to error reduction in percent over
standard multiclass SVM (Table 2)
</tableCaption>
<bodyText confidence="0.964409117647059">
Minority Preference (DACMP) System This sys-
tem is exactly the same as the basic DAC system
except for one crucial difference: overall classifica-
tion is biased towards a specified minority class. If
the minority class binary classifier predicts true, this
system chooses the minority class as the predicted
class. In cases where the minority class classifier
predicts false, it backs off to the basic DAC system
after removing the minority class classifier from the
confidence tally. Table 4 shows our results using
REQUEST-ACTION as the minority class.
Cascading Minority Preference (DACCMP) System
This system is similar to the Minority Preference
System; however, instead of a single supplied mi-
nority class, the system accepts an ordered list of
classes. The classifier then works, in order, through
this list; whenever any classifier in the list predicts
</bodyText>
<page confidence="0.996898">
804
</page>
<table confidence="0.999298333333333">
Prec. Recall F-meas. ER
R-A 66.7 45.7 54.2 22.8
R-I 91.5 78.2 84.3 0.0
CONV 93.9 94.1 94.0 2.6
INF 91.6 96.0 93.8 6.5
Accuracy 91.8 5.7
</table>
<tableCaption confidence="0.9997162">
Table 4: Results for minority-preference DAC system —
DACMP (first consult REQUEST-ACTION tagger, then de-
fault to choice by maximum confidence); “ER” refers to
error reduction in percent over standard multiclass SVM
(Table 2)
</tableCaption>
<bodyText confidence="0.998237083333333">
true, for a given instance, it then assigns this class
as the predicted class. The subsequent classifiers in
the list are not run. If all classifiers predict false, we
back off to the basic DAC system, i.e., the compo-
nent classifier with highest probability score deter-
mines the overall prediction. We ordered the list of
classes in the ascending order of their frequencies in
the training data. This ordering is driven by the ob-
servation that the less frequent classes are also hard
to predict correctly. Table 5 shows our results using
the ordered list: (REQUEST-ACTION, REQUEST-
INFORMATION, CONVENTIONAL, INFORM).
</bodyText>
<table confidence="0.999200833333333">
Prec. Recall F-meas. ER
R-A 66.7 45.7 54.2 22.8
R-I 91.0 80.8 85.6 8.4
CONV 93.7 95.3 94.5 10.1
INF 92.4 95.8 94.0 10.0
Accuracy 92.2 10.6
</table>
<tableCaption confidence="0.99419475">
Table 5: Results for cascading minority-preference DAC
system — DACCMP (consult classifiers in reverse order
of frequency of class); “ER” refers to error reduction in
percent over standard multiclass SVM (Table 2)
</tableCaption>
<subsectionHeader confidence="0.967371">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999970785714286">
As shown in Table 3, the basic DAC system obtained
a 15.6% F-measure error reduction for the minor-
ity class REQUEST-ACTION over the BAS system.
It also improves performance of two other classes
— CONVENTIONAL and INFORM, and obtaines a
4.9% error reduction on overall accuracy. Recall
here that the only difference between the DAC sys-
tem and the BAS system is the per-class feature op-
timization and therefore this must be the reason for
this boost in performance. When we turn to DACMP,
we see that the performance on the minority class
REQUEST-ACTION is further enhanced, with an F-
measure error reduction of 22.8%; the overall ac-
curacy improves slightly with an error reduction of
5.7%. Finally, DACCMP further improves the perfor-
mance. Since the method of choosing the minor-
ity class REQUEST-ACTION does not change over
DACMP, the F-measure error reduction remains the
same. However, now all three other classes also im-
prove their performance, and we obtain a 10.6% er-
ror reduction on overall accuracy over the baseline
system.
Following (Guyon et al., 2002), we performed a
post-hoc analysis by inspecting the feature weights
of the best performing models created for each in-
dividual classifier in the DAC system. Table 6 lists
some interesting features chosen during feature opti-
mization for the individual SVMs. We selected them
from the top 25 features in terms of absolute value
of feature weights.
Some features help distinguish different DA cat-
egories. For example, the feature QuestionMark
is the feature with the highest negative weight for
INFORM, but has the highest positive weight for
REQUEST-INFORMATION. Features like fyi and pe-
riod (.) have high positive weights for INFORM
and high negative weights for CONVENTIONAL.
Some other features are important only for certain
classes. For e.g., please and VB NN are important
for REQUEST-ACTION, but not so for other classes.
Overall, the most discriminating features for both
INFORM and CONVENTIONAL are mostly word
ngrams, while those for REQUEST-ACTION and
REQUEST-INFORMATION are mostly POS ngrams.
This shows why our approach of per-class feature
optimization is important to boost the classification
performance.
Another interesting observation is that the least
frequent category, REQUEST-ACTION, has the least
strong indicators (as measured by feature weights).
Presumably this is because there is much less train-
ing data for this class. This explains why our cascad-
ing classifiers approach giving priority to the least
frequent categories worked better than a simple con-
fidence based approach, since the simple approach
drowns out the less confident classifiers.
</bodyText>
<page confidence="0.995736">
805
</page>
<table confidence="0.999453857142857">
REQUEST-ACTION REQUEST-INFORMATION CONVENTIONAL INFORM
please (0.9) QuestionMark (6.6) StartPOS NNP (2.7) QuestionMark (-3.0)
VB NN (0.7) BOS PRP (-1.2) thanks (2.3) thanks (-2.2)
you VB (0.3) WRB (1.0) . (-2.0) . (2.2)
PRP (-0.3) PRP VBP (-0.9) fyi (-2.0) fyi (1.9)
MD PRP VB (0.3) BOS MD (0.8) , (0.9) you (-1.0)
will (-0.2) BOS DT (-0.7) QuestionMark (-0.8) can you (-0.9)
</table>
<tableCaption confidence="0.968964333333333">
Table 6: Post-hoc analysis on the models built by the DAC system: some of the top features with corresponding
feature weights in parentheses, for each individual tagger. (POS tags are capitalized; BOS stands for Beginning Of
Sentence)
</tableCaption>
<sectionHeader confidence="0.97161" genericHeader="method">
5 Extrinsic Evaluation
</sectionHeader>
<bodyText confidence="0.999200181818182">
In this section, we perform an extrinsic evaluation
for the dialog act tagger presented in Section 4 by
applying it to the task of identifying Overt Displays
of Power (ODP) in emails, proposed by Prabhakaran
et al. (2012). The task is to identify utterances where
the linguistic form introduces additional constraints
on its responses, beyond those introduced by the
general dialog act. The dialog act features were
found to be useful and the best performing system
obtained an F-measure of 65.8 using gold dialog
act tags. For our extrinsic evaluation, we retrained
the ODP tagger using dialog act tags predicted by
our BAS and DACCMP systems instead of gold dia-
log acts. ODP tagger uses the same dataset as ours
for training. In the cross validation step, we made
sure that the test folds for ODP were excluded from
training the taggers to obtain DA tags. At each ODP
cross validation step, we trained a BAS or DACCMP
tagger using ODP’s training folds for that step and
used tags produced by that tagger for both training
and testing the ODP tagger for that step. Table 7 lists
the results obtained.
</bodyText>
<table confidence="0.9995322">
Prec. Rec. F-meas.
No-DA 55.7 45.4 50.0
Gold-DA 75.8 58.1 65.8
BAS-DA 60.6 46.5 52.6
DACCMP-DA 67.2 45.4 54.2
</table>
<tableCaption confidence="0.9584065">
Table 7: Results for ODP system using various sources
of DA tags
</tableCaption>
<bodyText confidence="0.999786833333333">
Using BAS tagged DA, the F-measure of ODP
system reduced by 13.2 points to 52.6 from using
gold dialog acts (F=65.8). Using DACCMP, the F-
measure improved over BAS by 1.6 points to 54.2.
This constitutes an error reduction of 12.1%, tak-
ing the system using gold DA tags as the reference.
This improvement is noteworthy, given the fact that
the overall error reduction obtained by DACCMP over
BAS in the DA tagging was around 10.6%. Also, the
DACCMP-based ODP system obtained an error reduc-
tion of about 26.6% over a system that does not use
the DA features at all (F=50.0).
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999984166666667">
We presented a method of improving the perfor-
mance of dialog act tagging in identifying minority
classes by using per-class feature optimization and
choosing the class based on a cascade of classifiers.
We showed that it gives a minority class F-measure
error reduction of 22.8% while also reducing the er-
ror on other classes and the overall error by around
10%. We also presented an extrinsic evaluation of
this technique on detecting Overt Displays of Power
in dialog, where we achieve an error reduction of
12.1% over using the standard multiclass SVM to
generate dialog act tags.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999884">
This work is supported, in part, by the Johns Hop-
kins Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect the
views of the sponsor. While working on this project,
the first author Adinoyi Omuya was affiliated with
the Center for Computational Learning Systems at
Columbia University. We thank several anonymous
reviewers for their constructive feedback.
</bodyText>
<page confidence="0.997614">
806
</page>
<sectionHeader confidence="0.989905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999809766233767">
Apoorv Agarwal, Adinoyi Omuya, Aaron Harnly, and
Owen Rambow. 2012. A Comprehensive Gold Stan-
dard for the Enron Organizational Hierarchy. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 161–165, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
J. L. Austin. 1975. How to Do Things with Words. Har-
vard University Press, Cambridge, Mass.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to Classify Email into
“Speech Acts” . In Dekang Lin and Dekai Wu, ed-
itors, Proceedings of EMNLP 2004, pages 309–316,
Barcelona, Spain, July. Association for Computational
Linguistics.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene Selection for Cancer
Classification using Support Vector Machines. Mach.
Learn., 46:389–422, March.
Jun Hu, Rebecca Passonneau, and Owen Rambow. 2009.
Contrasting the Interaction Structure of an Email and
a Telephone Corpus: A Machine Learning Approach
to Annotation of Dialogue Function Units. In Pro-
ceedings of the SIGDIAL 2009 Conference, London,
UK, September. Association for Computational Lin-
guistics.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In Bernhard Sch¨olkopf, Christo-
pher J.C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning, Cam-
bridge, MA, USA. MIT Press.
J. Kim, G. Chern, D. Feng, E. Shaw, and E. Hovy.
2006. Mining and Assessing Discussions on the Web
Through Speech Act Analysis. In Proceedings of the
Workshop on Web Content Mining with Human Lan-
guage Technologies at the 5th International Semantic
Web Conference.
S.N. Kim, L. Cavedon, and T. Baldwin. 2010a. Classify-
ing Dialogue Acts in One-on-one Live Chats. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 862–871.
Association for Computational Linguistics.
S.N. Kim, L. Wang, and T. Baldwin. 2010b. Tagging
and Linking Web Forum Posts. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 192–202. Association for
Computational Linguistics.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A Note on Platt’s Probabilistic Outputs for Support
Vector Machines. Mach. Learn., 68:267–276, Octo-
ber.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA toolkit for statistical natural
language processing. In Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP work-
shop at Language Resources and Evaluation Confer-
ence (LREC).
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting Overt Display of Power in
Written Dialogs. In Human Language Technologies:
The 2012 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Montreal, Canada, June. Association for Compu-
tational Linguistics.
J.R. Searle. 1976. A Classification of Illocutionary Acts.
Language in society, 5(01):1–23.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C.V. Ess-Dykema,
and M. Meteer. 2000. Dialogue Act Modeling for
Automatic Tagging and Recognition of Conversational
Speech. Computational linguistics, 26(3):339–373.
J.Y. Yeh and A. Harnly. 2006. Email Thread Reassembly
Using Similarity Matching. In Third Conference on
Email and Anti-Spam (CEAS), pages 27–28.
R. Zhang, D. Gao, and W. Li. 2012. Towards Scalable
Speech Act Recognition in Twitter: Tackling Insuffi-
cient Training Data. EACL 2012, page 18.
</reference>
<page confidence="0.99787">
807
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.944480">
<title confidence="0.999901">Improving the Quality of Minority Class Identification in Dialog Act Tagging</title>
<author confidence="0.998216">Adinoyi Omuya Vinodkumar Prabhakaran Owen Rambow</author>
<affiliation confidence="0.954752">10gen CS, Columbia University CCLS, Columbia University</affiliation>
<address confidence="0.999396">New York, NY, USA New York, NY, USA New York, NY, USA</address>
<email confidence="0.999516">wisdom@10gen.comvinod@cs.columbia.edurambow@ccls.columbia.edu</email>
<abstract confidence="0.9990558">We present a method of improving the performance of dialog act tagging in identifying minority classes by using per-class feature optimization and a method of choosing the class based not on confidence, but on a cascade of classifiers. We show that it gives a minority class F-measure error reduction of 22.8%, while also reducing the error for other classes and the overall error by about 10%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Adinoyi Omuya</author>
<author>Aaron Harnly</author>
<author>Owen Rambow</author>
</authors>
<title>A Comprehensive Gold Standard for the Enron Organizational Hierarchy.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>161--165</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="4405" citStr="Agarwal et al., 2012" startWordPosition="723" endWordPosition="726">radigm (we choose a single DA for smaller textual units). We build on the work by (Hu et al., 2009); we improve their dialog act predicting performance on minority classes using per-class feature optimization. 3 Data In this study, we use the email corpus presented in (Hu et al., 2009), which is manually annotated for DA tags. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006; Agarwal et al., 2012). Most emails are concerned with exchanging information, scheduling meetings, or solving problems, but there are also purely social emails. Dialog Act Tag Count (%) REQUEST-ACTION (R-A) 35 (2.5%) REQUEST-INFORMATION (R-I) 151 (10.7%) CONVENTIONAL (CONV) 357 (25.4%) INFORM (INF) 853 (60.7%) Total # of DFUs 1406 Table 1: Annotation statistics Each message in the thread is segmented into Dialog Functional Units (DFUs). A DFU is a contiguous span within an email message which has a coherent communicative intention. Each DFU is assigned a single DA label which is one of the following: REQUEST-ACTIO</context>
</contexts>
<marker>Agarwal, Omuya, Harnly, Rambow, 2012</marker>
<rawString>Apoorv Agarwal, Adinoyi Omuya, Aaron Harnly, and Owen Rambow. 2012. A Comprehensive Gold Standard for the Enron Organizational Hierarchy. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 161–165, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Austin</author>
</authors>
<title>How to Do Things with Words.</title>
<date>1975</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="2832" citStr="Austin (1975)" startWordPosition="461" endWordPosition="462"> data point positively, we use the usual confidence-based method. This new method obtains a 22.8% error reduction for the minority class, and around 10% error reduction for the other classes and for the overall classifier. This paper is structured as follows. We start out by discussing related work (Section 2). We then present our data in Section 3, and in Section 4 we present the experiments with our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT</context>
</contexts>
<marker>Austin, 1975</marker>
<rawString>J. L. Austin. 1975. How to Do Things with Words. Harvard University Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Vitor R Carvalho</author>
<author>Tom M Mitchell</author>
</authors>
<title>Learning to Classify Email into “Speech Acts” .</title>
<date>2004</date>
<booktitle>Proceedings of EMNLP 2004,</booktitle>
<pages>309--316</pages>
<editor>In Dekang Lin and Dekai Wu, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3117" citStr="Cohen et al., 2004" startWordPosition="506" endWordPosition="510">cussing related work (Section 2). We then present our data in Section 3, and in Section 4 we present the experiments with our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one another. However, recent studies have found merit in segmenting each message into functional units and assigning a sin</context>
</contexts>
<marker>Cohen, Carvalho, Mitchell, 2004</marker>
<rawString>William W. Cohen, Vitor R. Carvalho, and Tom M. Mitchell. 2004. Learning to Classify Email into “Speech Acts” . In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 309–316, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Guyon</author>
<author>Jason Weston</author>
<author>Stephen Barnhill</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Gene Selection for Cancer Classification using Support Vector Machines.</title>
<date>2002</date>
<location>Mach. Learn., 46:389–422,</location>
<contexts>
<context position="13853" citStr="Guyon et al., 2002" startWordPosition="2274" endWordPosition="2277"> boost in performance. When we turn to DACMP, we see that the performance on the minority class REQUEST-ACTION is further enhanced, with an Fmeasure error reduction of 22.8%; the overall accuracy improves slightly with an error reduction of 5.7%. Finally, DACCMP further improves the performance. Since the method of choosing the minority class REQUEST-ACTION does not change over DACMP, the F-measure error reduction remains the same. However, now all three other classes also improve their performance, and we obtain a 10.6% error reduction on overall accuracy over the baseline system. Following (Guyon et al., 2002), we performed a post-hoc analysis by inspecting the feature weights of the best performing models created for each individual classifier in the DAC system. Table 6 lists some interesting features chosen during feature optimization for the individual SVMs. We selected them from the top 25 features in terms of absolute value of feature weights. Some features help distinguish different DA categories. For example, the feature QuestionMark is the feature with the highest negative weight for INFORM, but has the highest positive weight for REQUEST-INFORMATION. Features like fyi and period (.) have h</context>
</contexts>
<marker>Guyon, Weston, Barnhill, Vapnik, 2002</marker>
<rawString>Isabelle Guyon, Jason Weston, Stephen Barnhill, and Vladimir Vapnik. 2002. Gene Selection for Cancer Classification using Support Vector Machines. Mach. Learn., 46:389–422, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Hu</author>
<author>Rebecca Passonneau</author>
<author>Owen Rambow</author>
</authors>
<title>Contrasting the Interaction Structure of an Email and a Telephone Corpus: A Machine Learning Approach to Annotation of Dialogue Function Units.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009 Conference,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>London, UK,</location>
<contexts>
<context position="997" citStr="Hu et al., 2009" startWordPosition="158" endWordPosition="161">n identifying minority classes by using per-class feature optimization and a method of choosing the class based not on confidence, but on a cascade of classifiers. We show that it gives a minority class F-measure error reduction of 22.8%, while also reducing the error for other classes and the overall error by about 10%. 1 Introduction In this paper, we discuss dialog act tagging, the task of assigning a dialog act to an utterance, where a dialog act (DA) is a high-level categorization of the pragmatic meaning of the utterance. Our data is email. Our starting point is the tagger described in (Hu et al., 2009), which uses a standard multi-class classifier based on support vector machines (SVMs). While the performance of this system is pretty good as measured by accuracy, it performs badly on the DA REQUEST-ACTION, which is a rare class. Multiclass SVMs are typically implemented as a set of SVMs, one per class, with the overall choice of class being determined by the SVM with the highest confidence (“one-against-all”). Multi-class SVMs are typically packaged as a single system, whose inner workings are ignored by the NLP researcher. In this paper we show that, for our problem of DA classification, w</context>
<context position="3757" citStr="Hu et al., 2009" startWordPosition="607" endWordPosition="610">06; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one another. However, recent studies have found merit in segmenting each message into functional units and assigning a single DA to each segment (Hu et al., 2009). Our work falls in this paradigm (we choose a single DA for smaller textual units). We build on the work by (Hu et al., 2009); we improve their dialog act predicting performance on minority classes using per-class feature optimization. 3 Data In this study, we use the email corpus presented in (Hu et al., 2009), which is manually annotated for DA tags. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in which they were quo</context>
<context position="6585" citStr="Hu et al., 2009" startWordPosition="1081" endWordPosition="1084">ls that her desire that the reader adopt a certain belief. It covers many different types of information that can be conveyed including answers to questions, beliefs (committed or not), attitudes, and elaborations on prior DAs. A CONVENTIONAL dialog act does not signal any specific communicative intention on the part of the writer, but rather it helps structure and thus facilitate the communication. Examples include greetings, introductions, expressions of gratitude, etc. 4 System We developed four systems for our experiments: a baseline (BAS) system which is close to the system described in (Hu et al., 2009), and three variants of our novel divide and conquer (DAC) system. Features used in both systems are extracted as explained in Section 4.2. Section 4.3 describes the baseline system, the basic DAC system, and two variations of the DAC system. 4.1 Experimental Framework In all our experiments, we use linear kernel Support Vector Machines (SVM). However, across the systems, there are differences in how we use them. Our framework was built with the ClearTK toolkit (Ogren et al., 2008) with its wrapper for SVMLight (Joachims, 1999). The ClearTK wrapper internally shifts the prediction threshold ba</context>
</contexts>
<marker>Hu, Passonneau, Rambow, 2009</marker>
<rawString>Jun Hu, Rebecca Passonneau, and Owen Rambow. 2009. Contrasting the Interaction Structure of an Email and a Telephone Corpus: A Machine Learning Approach to Annotation of Dialogue Function Units. In Proceedings of the SIGDIAL 2009 Conference, London, UK, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making Large-Scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning,</booktitle>
<editor>In Bernhard Sch¨olkopf, Christopher J.C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="7118" citStr="Joachims, 1999" startWordPosition="1171" endWordPosition="1172"> baseline (BAS) system which is close to the system described in (Hu et al., 2009), and three variants of our novel divide and conquer (DAC) system. Features used in both systems are extracted as explained in Section 4.2. Section 4.3 describes the baseline system, the basic DAC system, and two variations of the DAC system. 4.1 Experimental Framework In all our experiments, we use linear kernel Support Vector Machines (SVM). However, across the systems, there are differences in how we use them. Our framework was built with the ClearTK toolkit (Ogren et al., 2008) with its wrapper for SVMLight (Joachims, 1999). The ClearTK wrapper internally shifts the prediction threshold based on posterior probabilistic scores calculated using the algorithm of Lin et al. (2007). We report results from 5-fold cross validation performed on the entire corpus. 4.2 Feature Engineering In developing our system, we classified our features into three categories: lexical, verbal and message803 level. Lexical features consists of n-grams of words, n-grams of POS tags, mixed n-grams of closed class words and POS tags (Prabhakaran et al., 2012), as well as a small set of specialized features — StartPOS/Lemma (POS tag and lem</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making Large-Scale SVM Learning Practical. In Bernhard Sch¨olkopf, Christopher J.C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning, Cambridge, MA, USA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>G Chern</author>
<author>D Feng</author>
<author>E Shaw</author>
<author>E Hovy</author>
</authors>
<title>Mining and Assessing Discussions on the Web Through Speech Act Analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Web Content Mining with Human Language Technologies at the 5th International Semantic Web Conference.</booktitle>
<contexts>
<context position="3143" citStr="Kim et al., 2006" startWordPosition="512" endWordPosition="515"> 2). We then present our data in Section 3, and in Section 4 we present the experiments with our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one another. However, recent studies have found merit in segmenting each message into functional units and assigning a single DA to each segment (Hu</context>
</contexts>
<marker>Kim, Chern, Feng, Shaw, Hovy, 2006</marker>
<rawString>J. Kim, G. Chern, D. Feng, E. Shaw, and E. Hovy. 2006. Mining and Assessing Discussions on the Web Through Speech Act Analysis. In Proceedings of the Workshop on Web Content Mining with Human Language Technologies at the 5th International Semantic Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S N Kim</author>
<author>L Cavedon</author>
<author>T Baldwin</author>
</authors>
<title>Classifying Dialogue Acts in One-on-one Live Chats.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>862--871</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3161" citStr="Kim et al., 2010" startWordPosition="516" endWordPosition="519">nt our data in Section 3, and in Section 4 we present the experiments with our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one another. However, recent studies have found merit in segmenting each message into functional units and assigning a single DA to each segment (Hu et al., 2009). Ou</context>
</contexts>
<marker>Kim, Cavedon, Baldwin, 2010</marker>
<rawString>S.N. Kim, L. Cavedon, and T. Baldwin. 2010a. Classifying Dialogue Acts in One-on-one Live Chats. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862–871. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S N Kim</author>
<author>L Wang</author>
<author>T Baldwin</author>
</authors>
<title>Tagging and Linking Web Forum Posts.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>192--202</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3161" citStr="Kim et al., 2010" startWordPosition="516" endWordPosition="519">nt our data in Section 3, and in Section 4 we present the experiments with our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one another. However, recent studies have found merit in segmenting each message into functional units and assigning a single DA to each segment (Hu et al., 2009). Ou</context>
</contexts>
<marker>Kim, Wang, Baldwin, 2010</marker>
<rawString>S.N. Kim, L. Wang, and T. Baldwin. 2010b. Tagging and Linking Web Forum Posts. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 192–202. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsuan-Tien Lin</author>
<author>Chih-Jen Lin</author>
<author>Ruby C Weng</author>
</authors>
<title>A Note on Platt’s Probabilistic Outputs for Support Vector Machines.</title>
<date>2007</date>
<location>Mach. Learn., 68:267–276,</location>
<contexts>
<context position="7274" citStr="Lin et al. (2007)" startWordPosition="1191" endWordPosition="1194">res used in both systems are extracted as explained in Section 4.2. Section 4.3 describes the baseline system, the basic DAC system, and two variations of the DAC system. 4.1 Experimental Framework In all our experiments, we use linear kernel Support Vector Machines (SVM). However, across the systems, there are differences in how we use them. Our framework was built with the ClearTK toolkit (Ogren et al., 2008) with its wrapper for SVMLight (Joachims, 1999). The ClearTK wrapper internally shifts the prediction threshold based on posterior probabilistic scores calculated using the algorithm of Lin et al. (2007). We report results from 5-fold cross validation performed on the entire corpus. 4.2 Feature Engineering In developing our system, we classified our features into three categories: lexical, verbal and message803 level. Lexical features consists of n-grams of words, n-grams of POS tags, mixed n-grams of closed class words and POS tags (Prabhakaran et al., 2012), as well as a small set of specialized features — StartPOS/Lemma (POS tag and lemma of the first word), LastPOS/Lemma (POS tag and lemma of the last word), MDCount (number of modal verbs in the DFU) and QuestionMark (is there a question </context>
</contexts>
<marker>Lin, Lin, Weng, 2007</marker>
<rawString>Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007. A Note on Platt’s Probabilistic Outputs for Support Vector Machines. Mach. Learn., 68:267–276, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip V Ogren</author>
<author>Philipp G Wetzler</author>
<author>Steven Bethard</author>
</authors>
<title>ClearTK: A UIMA toolkit for statistical natural language processing.</title>
<date>2008</date>
<booktitle>In Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP workshop at Language Resources and Evaluation Conference (LREC).</booktitle>
<contexts>
<context position="7071" citStr="Ogren et al., 2008" startWordPosition="1162" endWordPosition="1165">em We developed four systems for our experiments: a baseline (BAS) system which is close to the system described in (Hu et al., 2009), and three variants of our novel divide and conquer (DAC) system. Features used in both systems are extracted as explained in Section 4.2. Section 4.3 describes the baseline system, the basic DAC system, and two variations of the DAC system. 4.1 Experimental Framework In all our experiments, we use linear kernel Support Vector Machines (SVM). However, across the systems, there are differences in how we use them. Our framework was built with the ClearTK toolkit (Ogren et al., 2008) with its wrapper for SVMLight (Joachims, 1999). The ClearTK wrapper internally shifts the prediction threshold based on posterior probabilistic scores calculated using the algorithm of Lin et al. (2007). We report results from 5-fold cross validation performed on the entire corpus. 4.2 Feature Engineering In developing our system, we classified our features into three categories: lexical, verbal and message803 level. Lexical features consists of n-grams of words, n-grams of POS tags, mixed n-grams of closed class words and POS tags (Prabhakaran et al., 2012), as well as a small set of special</context>
</contexts>
<marker>Ogren, Wetzler, Bethard, 2008</marker>
<rawString>Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard. 2008. ClearTK: A UIMA toolkit for statistical natural language processing. In Towards Enhanced Interoperability for Large HLT Systems: UIMA for NLP workshop at Language Resources and Evaluation Conference (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
<author>Mona Diab</author>
</authors>
<title>Predicting Overt Display of Power in Written Dialogs.</title>
<date>2012</date>
<booktitle>In Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Canada,</location>
<contexts>
<context position="7636" citStr="Prabhakaran et al., 2012" startWordPosition="1247" endWordPosition="1250">work was built with the ClearTK toolkit (Ogren et al., 2008) with its wrapper for SVMLight (Joachims, 1999). The ClearTK wrapper internally shifts the prediction threshold based on posterior probabilistic scores calculated using the algorithm of Lin et al. (2007). We report results from 5-fold cross validation performed on the entire corpus. 4.2 Feature Engineering In developing our system, we classified our features into three categories: lexical, verbal and message803 level. Lexical features consists of n-grams of words, n-grams of POS tags, mixed n-grams of closed class words and POS tags (Prabhakaran et al., 2012), as well as a small set of specialized features — StartPOS/Lemma (POS tag and lemma of the first word), LastPOS/Lemma (POS tag and lemma of the last word), MDCount (number of modal verbs in the DFU) and QuestionMark (is there a question mark in the DFU). We used the POS tags produced by the OpenNLP POS tagger. Verbal features capture the position and identity of the first verb in the DFU. Finally, message-level features capture aspects of the location of the DFU in the message and of the message in the thread (relative position and size). In optimizing each system, we first performed an exhau</context>
<context position="16285" citStr="Prabhakaran et al. (2012)" startWordPosition="2654" endWordPosition="2657"> PRP VBP (-0.9) fyi (-2.0) fyi (1.9) MD PRP VB (0.3) BOS MD (0.8) , (0.9) you (-1.0) will (-0.2) BOS DT (-0.7) QuestionMark (-0.8) can you (-0.9) Table 6: Post-hoc analysis on the models built by the DAC system: some of the top features with corresponding feature weights in parentheses, for each individual tagger. (POS tags are capitalized; BOS stands for Beginning Of Sentence) 5 Extrinsic Evaluation In this section, we perform an extrinsic evaluation for the dialog act tagger presented in Section 4 by applying it to the task of identifying Overt Displays of Power (ODP) in emails, proposed by Prabhakaran et al. (2012). The task is to identify utterances where the linguistic form introduces additional constraints on its responses, beyond those introduced by the general dialog act. The dialog act features were found to be useful and the best performing system obtained an F-measure of 65.8 using gold dialog act tags. For our extrinsic evaluation, we retrained the ODP tagger using dialog act tags predicted by our BAS and DACCMP systems instead of gold dialog acts. ODP tagger uses the same dataset as ours for training. In the cross validation step, we made sure that the test folds for ODP were excluded from tra</context>
</contexts>
<marker>Prabhakaran, Rambow, Diab, 2012</marker>
<rawString>Vinodkumar Prabhakaran, Owen Rambow, and Mona Diab. 2012. Predicting Overt Display of Power in Written Dialogs. In Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Montreal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Searle</author>
</authors>
<title>A Classification of Illocutionary Acts. Language in society,</title>
<date>1976</date>
<pages>5--01</pages>
<contexts>
<context position="2850" citStr="Searle (1976)" startWordPosition="464" endWordPosition="465">vely, we use the usual confidence-based method. This new method obtains a 22.8% error reduction for the minority class, and around 10% error reduction for the other classes and for the overall classifier. This paper is structured as follows. We start out by discussing related work (Section 2). We then present our data in Section 3, and in Section 4 we present the experiments with our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–8</context>
</contexts>
<marker>Searle, 1976</marker>
<rawString>J.R. Searle. 1976. A Classification of Illocutionary Acts. Language in society, 5(01):1–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>K Ries</author>
<author>N Coccaro</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>D Jurafsky</author>
<author>P Taylor</author>
<author>R Martin</author>
<author>C V Ess-Dykema</author>
<author>M Meteer</author>
</authors>
<title>Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech.</title>
<date>2000</date>
<journal>Computational linguistics,</journal>
<pages>26--3</pages>
<contexts>
<context position="3004" citStr="Stolcke et al., 2000" startWordPosition="489" endWordPosition="492">tion for the other classes and for the overall classifier. This paper is structured as follows. We start out by discussing related work (Section 2). We then present our data in Section 3, and in Section 4 we present the experiments with our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one anoth</context>
</contexts>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Ess-Dykema, Meteer, 2000</marker>
<rawString>A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, P. Taylor, R. Martin, C.V. Ess-Dykema, and M. Meteer. 2000. Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech. Computational linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Y Yeh</author>
<author>A Harnly</author>
</authors>
<title>Email Thread Reassembly Using Similarity Matching.</title>
<date>2006</date>
<booktitle>In Third Conference on Email and Anti-Spam (CEAS),</booktitle>
<pages>27--28</pages>
<contexts>
<context position="4382" citStr="Yeh and Harnly, 2006" startWordPosition="719" endWordPosition="722"> work falls in this paradigm (we choose a single DA for smaller textual units). We build on the work by (Hu et al., 2009); we improve their dialog act predicting performance on minority classes using per-class feature optimization. 3 Data In this study, we use the email corpus presented in (Hu et al., 2009), which is manually annotated for DA tags. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006; Agarwal et al., 2012). Most emails are concerned with exchanging information, scheduling meetings, or solving problems, but there are also purely social emails. Dialog Act Tag Count (%) REQUEST-ACTION (R-A) 35 (2.5%) REQUEST-INFORMATION (R-I) 151 (10.7%) CONVENTIONAL (CONV) 357 (25.4%) INFORM (INF) 853 (60.7%) Total # of DFUs 1406 Table 1: Annotation statistics Each message in the thread is segmented into Dialog Functional Units (DFUs). A DFU is a contiguous span within an email message which has a coherent communicative intention. Each DFU is assigned a single DA label which is one of the f</context>
</contexts>
<marker>Yeh, Harnly, 2006</marker>
<rawString>J.Y. Yeh and A. Harnly. 2006. Email Thread Reassembly Using Similarity Matching. In Third Conference on Email and Anti-Spam (CEAS), pages 27–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zhang</author>
<author>D Gao</author>
<author>W Li</author>
</authors>
<title>Towards Scalable Speech Act Recognition in Twitter: Tackling Insufficient Training Data.</title>
<date>2012</date>
<booktitle>EACL 2012,</booktitle>
<pages>18</pages>
<contexts>
<context position="3235" citStr="Zhang et al., 2012" startWordPosition="528" endWordPosition="531">ith our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one another. However, recent studies have found merit in segmenting each message into functional units and assigning a single DA to each segment (Hu et al., 2009). Our work falls in this paradigm (we choose a single DA for smaller textual u</context>
</contexts>
<marker>Zhang, Gao, Li, 2012</marker>
<rawString>R. Zhang, D. Gao, and W. Li. 2012. Towards Scalable Speech Act Recognition in Twitter: Tackling Insufficient Training Data. EACL 2012, page 18.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>