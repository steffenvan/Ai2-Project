<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001364">
<title confidence="0.977924">
Automatic Collection of Related Terms from the Web
</title>
<author confidence="0.989091">
Satoshi Sato and Yasuhiro Sasaki
</author>
<affiliation confidence="0.992398">
Graduate School of Informatics
Kyoto University
</affiliation>
<address confidence="0.568717">
Sakyo, Kyoto, 606-8501
Japan
</address>
<email confidence="0.956932">
sato@i.kyoto-u.ac.jp, sasaki@pine.kuee.kyoto-u.ac.jp
</email>
<figure confidence="0.823606111111111">
✲
Abstract
ra seed term
S
Compiling
corpus
✓ ✏
✲corpus
✒ C3 ✑
</figure>
<bodyText confidence="0.9996389375">
This paper proposes a method of collect-
ing a dozen terms that are closely re-
lated to a given seed term. The proposed
method consists of three steps. The first
step, compiling corpus step, collects texts
that contain the given seed term by us-
ing search engines. The second step, au-
tomatic term recognition, extracts impor-
tant terms from the corpus by using Naka-
gawa’s method. These extracted terms be-
come the candidates for the final step. The
final step, filtering step, removes inappro-
priate terms from the candidates based on
search engine hits. An evaluation result
shows that the precision of the method is
85%.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9990730625">
This study aims to realize an automatic method of
collecting technical terms that are related to a given
seed term. In case “natural language processing” is
given as a seed term, the method is expected to col-
lect technical terms that are related to natural lan-
guage processing, such as morphological analysis,
parsing, information retrieval, and machine transla-
tion. The target application of the method is auto-
matic or semi-automatic compilation of a glossary or
technical-term dictionary for a certain domain. Re-
cursive application of the method enables to collect a
list of terms that are used in a certain domain: the list
becomes a glossary of the domain. A technical-term
dictionary can be compiled by adding an explanation
for every term in the glossary, which is performed by
term explainer (Sato, 2001).
</bodyText>
<figureCaption confidence="0.999443">
Figure 1: System configuration
</figureCaption>
<bodyText confidence="0.9998638">
Automatic acquisition of technical terms in a cer-
tain domain has been studied as automatic term
recognition (Kageura and Umino, 1996; Kageura
and Koyama, 2000), and the methods require a large
corpus that are manually prepared for a target do-
main. In contrast, our system, which is proposed in
this paper, requires only a seed word; from this seed
word, the system compiles a corpus from the Web by
using search engines and produces a dozen technical
terms that are closely related to the seed word.
</bodyText>
<sectionHeader confidence="0.991615" genericHeader="method">
2 System
</sectionHeader>
<bodyText confidence="0.94995925">
Figure 1 shows the configuration of the system. The
system consists of three steps: compiling corpus, au-
tomatic term recognition (ATR), and filtering. This
system is implemented for Japanese language.
</bodyText>
<subsectionHeader confidence="0.996614">
2.1 Compiling corpus
</subsectionHeader>
<bodyText confidence="0.999453333333333">
The first step, compiling corpus, produces a corpus
C3 for a seed term s. In general, compiling corpus is
to select the appropriate passages from a document
set. We use the Web for the document set and se-
lect the passages that describe s for the corpus. The
actual procedure of compiling corpus is:
</bodyText>
<figure confidence="0.946964785714286">
❄
ATR
the Web
✻
❄
✓ ✏
related terms ✛
✒ T ✑
❄
Filtering
✓ ✏
✛ candidates
✒ X ✑
1. Web page collection
</figure>
<bodyText confidence="0.999768111111111">
For a given seed term s, the system first makes
four queries: “s toha”, “s toiu”, “s ha”, and
“s”, where toha, ha, and toiu are Japanese
functional words that are often used for defin-
ing or explaining a term. Then, the system col-
lects the top K (= 100) pages at maximum for
each query by using a search engine. If a col-
lected page has a link whose anchor string is s,
the system collects the linked page too.
</bodyText>
<sectionHeader confidence="0.530056" genericHeader="method">
2. Sentence extraction
</sectionHeader>
<bodyText confidence="0.999953545454545">
The system decomposes each page into sen-
tences, and extracts the sentences that contain
the seed term s.
The reason why we use the additional three queries
is that they work efficiently for collecting web pages
that contain a definition or an explanation of s. We
use two search engines, Goo1 and Infoseek2. We
send all four queries to Goo but only the query “s” to
Infoseek, because Infoseek usually returns the same
result for the four queries. A typical corpus size is
about 500 sentences.
</bodyText>
<subsectionHeader confidence="0.998586">
2.2 Automatic term recognition
</subsectionHeader>
<bodyText confidence="0.9997886">
The second step, automatic term recognition (ATR),
extracts important terms from the compiled cor-
pus. We use Nakagawa’s ATR method (Nakagawa,
2000), which works well for Japanese text, with
some modifications. The procedure is as follows.
</bodyText>
<listItem confidence="0.985002">
1. Generation of term list
</listItem>
<bodyText confidence="0.916517333333334">
To make the term list L by extracting every
term that is a noun or a compound noun from
the compiled corpus.
</bodyText>
<listItem confidence="0.659609">
2. Selection by scoring
</listItem>
<bodyText confidence="0.996631">
To select the top N (= 30) terms from the list L
by using a scoring function.
For the scoring function of a term x, we use
the following function, which is multiplying Nak-
agawa’s Imp, by a frequency factor F(x, L)α.
</bodyText>
<equation confidence="0.9784165">
score(x, L) = Imp,(x, L) × F(x, L)α
�
1 if x is a single noun
F(x, L) = “frequency of x in L” otherwise
</equation>
<footnote confidence="0.9950345">
1www.goo.ne.jp
2www.infoseek.co.jp
</footnote>
<bodyText confidence="0.993392142857143">
While Nakagawa’s Imp, does not consider term fre-
quency, this function does: α is a parameter that con-
trols how strongly the frequency is considered. We
use α = 0.5 in experiments.
The result of automatic term recognition for “
(natural language processing)” is shown in
the column candidate in Table 1.
</bodyText>
<subsectionHeader confidence="0.995016">
2.3 Filtering
</subsectionHeader>
<bodyText confidence="0.999941">
The filtering step is necessary because the obtained
candidates are noisy due to the small corpus size.
This step consists of two tests: technical-term test
and relation test.
</bodyText>
<subsubsectionHeader confidence="0.442048">
2.3.1 Technical-term test
</subsubsectionHeader>
<bodyText confidence="0.9995665">
The technical-term test removes the terms that do
not satisfy conditions of technical terms. We employ
the following four conditions that a technical term
should satisfy.
</bodyText>
<listItem confidence="0.9897995">
1. The term is sometimes or frequently used in a
certain domain.
2. The term is not a general term.
3. There is a definition or explanation of the term.
4. There are several technical terms that are re-
lated to the term.
</listItem>
<bodyText confidence="0.9965335">
We have implemented the checking program of the
first two conditions in the system: the third condition
can be checked by integrating the system with term
explainer (Sato, 2001), which produces a definition
or explanation of a given term; the fourth condition
can be checked by using the system recursively.
There are several choices for implementing the
checking program. Our choice is to use the Web via
a search engine. A search engine returns a number,
hit, which is an estimated number of pages that sat-
isfy a given query. In case the query is a term, its hit
is the number of pages that contain the term on the
Web. We use the following notation.
H(x) = “the number of pages that contain
the term x”
The number H(x) can be used as an estimated
frequency of the term x on the Web, i.e., on the
hugest set of documents. Based on this number, we
can infer whether a term is a technical term or not:
in case the number is very small, the term is not a
</bodyText>
<tableCaption confidence="0.990477">
Table 1: Result for “natural language processing”
</tableCaption>
<table confidence="0.971398459459459">
candidate Tech. Rel.
(natural langauge pro- - -
cessing; NLP) √ √
(NLP technology)
(NLP system) √ √
(NLP research)
(NLP study) √ √
(processing) √ √
(text processing) √
(research and development)
(Information Processing
Society of Japan; IPSJ) √ √
(semantic processing)
(speech processing) √
(speech information pro- √ √
cessing)
(information processing)
(NLP domain)
(research field) √ √*
(parsing) √ √
(information retrieval) √ √
(SIGNLP) √ √
(speech recognition) √ √
(machine translation) √ √
(morphological analysis) √ √
(information pro- √ √
cessing system) √
(research)
(semantic analysis)
(chair of NLP) √ √*
(NLP sym- √
posium)
(application system)
(knowledge information √ √
processing)
(language)
(information)
</table>
<bodyText confidence="0.998865333333333">
technical term because it does not satisfy the first
condition; in case the number is large enough, the
term is probably a general term so that it is not a
technical term. Two parameters, Min and Max, are
necessary here. We have decided that we use search
engine Goo for H(x), and determined Min = 100
and Max = 100, 000, based on preliminary experi-
ments.
In summary, our technical-term test is:
</bodyText>
<equation confidence="0.536352">
If 100 ≤ H(x) ≤ 100, 000
then x is a technical term.
</equation>
<subsubsectionHeader confidence="0.40762">
2.3.2 Relation test
</subsubsectionHeader>
<bodyText confidence="0.999715103448276">
The relation test removes the terms that are not
closely related to the seed term from the candidates.
Our conditions of “x is closely related to s” is: (1)
x is a broader or narrower term of s; or (2) relation
degree between x and s is high enough, i.e., above a
given threshold.
The candidate terms can be classified from the
viewpoint of term composition. Under a given seed
term, we introduce the following five types for clas-
sification.
Type 0 the given seed term s: e.g.,
(natural language processing)
Type 1 a term that contains s: e.g.,
(natural language processing system)
Type 2 a term that is a subsequence of s: e.g.,
(natural language)
Type 3 a term that contains at least a component of
s: e.g., (language analysis)
Type 4 others: e.g., (parsing)
The reason why we introduce these types is that
the following rules are true with a few exception: (1)
A type-1 term is a narrower term of the seed term
s; (2) A type-2 term is a broader term of the seed
term s. We assume that these rules are always true:
they are used to determine whether x is a broader or
narrower term of s.
To measure the relation degree, we use con-
ditional probabilities, which are calculated from
search engine hits.
</bodyText>
<equation confidence="0.998608">
H(s ∧ x)
P(s|x) = H(x)
H(s ∧ x)
P(x|s) = H(s)
</equation>
<bodyText confidence="0.961250384615385">
where
H(s ∧ x) = “the number of pages that contain
both s and x”
One of two probabilities is equal to or greater than
a given threshold Z, the system decides that x is
closely related to s. We use Z = 0.05 as the thresh-
old.
In summary, our relation test is:
If x is type-1 or type-2; or
P(s|x) ≥ 0.05 or P(x|s) ≥ 0.05
then x is closely related to s.
The result of the filtering step for “
(natural language processing)” is in Table 1; a
</bodyText>
<tableCaption confidence="0.99159">
Table 2: Experimental Result
</tableCaption>
<table confidence="0.996201125">
domain Evaluation I Evaluation II
correct incorrect total S F A C R total
natural language processing 101 (93%) 8 ( 7%) 109 6 3 14 11 8 43
Japanese language 71 (81%) 17(19%) 88 7 0 19 5 1 32
information technology 113 (88%) 15 (12%) 128 10 5 27 13 0 55
current topics 106 (91%) 10 ( 9%) 116 2 0 13 19 5 39
persons in Japanese history 128 (76%) 41 (24%) 169 18 0 23 1 0 42
Total 519 (85%) 91(15%) 610 43 8 96 49 14 210
</table>
<bodyText confidence="0.992749666666667">
check mark ‘√’ indidates that the term passed the
test. Twenty terms out of the thrity candidate terms
passed the first techinical-term test (Tech.) and six-
teen terms out of the twenty terms passed the second
relation test (Rel.). The final result includes two in-
appropriate terms, which are indicated by ‘*’.
</bodyText>
<sectionHeader confidence="0.999507" genericHeader="conclusions">
3 Experiments and Disucssion
</sectionHeader>
<bodyText confidence="0.999260468085107">
First, we examined the precision of the system. We
prepared fifty seed terms in total: ten terms for
each of five genres; natural language processing,
Japanese language, information technology, current
topics, and persons in Japanese history. From these
fifty terms, the system collected 610 terms in total;
the average number of output terms per input is 12.2
terms. We checked whether each of the 610 terms
is a correct related term of the original seed term by
hand. The result is shown in the left half (Evaluation
I) of Table 2. In this evaluation, 519 terms out of 610
terms were correct: the precision is 85%. From this
high value, we conclude that the system can be used
as a tool that helps us compile a glossary.
Second, we tried to examine the recall of the
system. It is impossible to calculate the actual re-
call value, because the ideal output is not clear and
cannot be defined. To estimate the recall, we first
prepared three to five target terms that should be
collected from each seed word, and then checked
whether each of the target terms was included in
the system output. We counted the number of tar-
get terms in the following five cases. The right half
(Evaluation II) in Table 2 shows the result.
S: the target term was collected by the system.
F: the target term was removed in the filtering step.
A: the target term existed in the compiled corpus,
but was not extracted by automatic term extrac-
tion.
C: the target term existed in the collected web
pages, but did not exist in the compiled corpus.
R: the target term did not exist on the collected web
pages.
Only 43 terms (20%) out of 210 terms were col-
lected by the system. This low recall primarily
comes from the failure of automatic term recogni-
tion (case A in the above classification). Improve-
ment of this step is necessary.
We also examined whether each of the 210 target
terms passes the filtering step. The result was that
133 (63%) terms passed; 44 terms did not satisfy
the condition H(x) ≥ 100; 15 terms did not satisfy
the condition H(x) ≤ 100, 000; and 18 terms did
not pass the relation test. These experimental results
suggest that the ATR step may be replaced with a
simple and exhaustive term collector from a corpus.
We have a plan to examine this possibility next.
</bodyText>
<sectionHeader confidence="0.999028" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994025833333333">
Kyo Kageura and Teruo Koyama. 2000. Special issue:
Japanese term extraction. Terminolgy, 6(2).
Kyo Kageura and Bin Umino. 1996. Methods of au-
tomatic term recognition: A review. Terminology,
3(2):259–289.
Hiroshi Nakagawa. 2000. Automatic term recognition
based on statistics of compound nouns. Terminology,
6(2):195–210.
Satoshi Sato. 2001. Automated editing of hypertext
r´esum´e from the world wide web. In Proceedings
of 2001 Symposium on Applications and the Internet
(SAINT 2001), pages 15–22.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.277996">
<title confidence="0.999408">Automatic Collection of Related Terms from the Web</title>
<author confidence="0.999936">Sato Sasaki</author>
<affiliation confidence="0.9998685">Graduate School of Informatics Kyoto University</affiliation>
<address confidence="0.689714">Sakyo, Kyoto, 606-8501 Japan</address>
<email confidence="0.814231">✲</email>
<abstract confidence="0.990384818181818">seed S Compiling corpus ✓ ✏ This paper proposes a method of collecting a dozen terms that are closely reto a given The proposed method consists of three steps. The first step, compiling corpus step, collects texts that contain the given seed term by using search engines. The second step, automatic term recognition, extracts important terms from the corpus by using Nakagawa’s method. These extracted terms become the candidates for the final step. The final step, filtering step, removes inappropriate terms from the candidates based on search engine hits. An evaluation result shows that the precision of the method is 85%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kyo Kageura</author>
<author>Teruo Koyama</author>
</authors>
<title>Special issue: Japanese term extraction.</title>
<date>2000</date>
<journal>Terminolgy,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="1910" citStr="Kageura and Koyama, 2000" startWordPosition="305" endWordPosition="308">application of the method is automatic or semi-automatic compilation of a glossary or technical-term dictionary for a certain domain. Recursive application of the method enables to collect a list of terms that are used in a certain domain: the list becomes a glossary of the domain. A technical-term dictionary can be compiled by adding an explanation for every term in the glossary, which is performed by term explainer (Sato, 2001). Figure 1: System configuration Automatic acquisition of technical terms in a certain domain has been studied as automatic term recognition (Kageura and Umino, 1996; Kageura and Koyama, 2000), and the methods require a large corpus that are manually prepared for a target domain. In contrast, our system, which is proposed in this paper, requires only a seed word; from this seed word, the system compiles a corpus from the Web by using search engines and produces a dozen technical terms that are closely related to the seed word. 2 System Figure 1 shows the configuration of the system. The system consists of three steps: compiling corpus, automatic term recognition (ATR), and filtering. This system is implemented for Japanese language. 2.1 Compiling corpus The first step, compiling co</context>
</contexts>
<marker>Kageura, Koyama, 2000</marker>
<rawString>Kyo Kageura and Teruo Koyama. 2000. Special issue: Japanese term extraction. Terminolgy, 6(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyo Kageura</author>
<author>Bin Umino</author>
</authors>
<title>Methods of automatic term recognition: A review.</title>
<date>1996</date>
<journal>Terminology,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="1883" citStr="Kageura and Umino, 1996" startWordPosition="301" endWordPosition="304"> translation. The target application of the method is automatic or semi-automatic compilation of a glossary or technical-term dictionary for a certain domain. Recursive application of the method enables to collect a list of terms that are used in a certain domain: the list becomes a glossary of the domain. A technical-term dictionary can be compiled by adding an explanation for every term in the glossary, which is performed by term explainer (Sato, 2001). Figure 1: System configuration Automatic acquisition of technical terms in a certain domain has been studied as automatic term recognition (Kageura and Umino, 1996; Kageura and Koyama, 2000), and the methods require a large corpus that are manually prepared for a target domain. In contrast, our system, which is proposed in this paper, requires only a seed word; from this seed word, the system compiles a corpus from the Web by using search engines and produces a dozen technical terms that are closely related to the seed word. 2 System Figure 1 shows the configuration of the system. The system consists of three steps: compiling corpus, automatic term recognition (ATR), and filtering. This system is implemented for Japanese language. 2.1 Compiling corpus T</context>
</contexts>
<marker>Kageura, Umino, 1996</marker>
<rawString>Kyo Kageura and Bin Umino. 1996. Methods of automatic term recognition: A review. Terminology, 3(2):259–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Automatic term recognition based on statistics of compound nouns.</title>
<date>2000</date>
<journal>Terminology,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="3986" citStr="Nakagawa, 2000" startWordPosition="680" endWordPosition="681">acts the sentences that contain the seed term s. The reason why we use the additional three queries is that they work efficiently for collecting web pages that contain a definition or an explanation of s. We use two search engines, Goo1 and Infoseek2. We send all four queries to Goo but only the query “s” to Infoseek, because Infoseek usually returns the same result for the four queries. A typical corpus size is about 500 sentences. 2.2 Automatic term recognition The second step, automatic term recognition (ATR), extracts important terms from the compiled corpus. We use Nakagawa’s ATR method (Nakagawa, 2000), which works well for Japanese text, with some modifications. The procedure is as follows. 1. Generation of term list To make the term list L by extracting every term that is a noun or a compound noun from the compiled corpus. 2. Selection by scoring To select the top N (= 30) terms from the list L by using a scoring function. For the scoring function of a term x, we use the following function, which is multiplying Nakagawa’s Imp, by a frequency factor F(x, L)α. score(x, L) = Imp,(x, L) × F(x, L)α � 1 if x is a single noun F(x, L) = “frequency of x in L” otherwise 1www.goo.ne.jp 2www.infoseek</context>
</contexts>
<marker>Nakagawa, 2000</marker>
<rawString>Hiroshi Nakagawa. 2000. Automatic term recognition based on statistics of compound nouns. Terminology, 6(2):195–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sato</author>
</authors>
<title>Automated editing of hypertext r´esum´e from the world wide web.</title>
<date>2001</date>
<booktitle>In Proceedings of 2001 Symposium on Applications and the Internet</booktitle>
<pages>15--22</pages>
<location>SAINT</location>
<contexts>
<context position="1718" citStr="Sato, 2001" startWordPosition="278" endWordPosition="279">d to collect technical terms that are related to natural language processing, such as morphological analysis, parsing, information retrieval, and machine translation. The target application of the method is automatic or semi-automatic compilation of a glossary or technical-term dictionary for a certain domain. Recursive application of the method enables to collect a list of terms that are used in a certain domain: the list becomes a glossary of the domain. A technical-term dictionary can be compiled by adding an explanation for every term in the glossary, which is performed by term explainer (Sato, 2001). Figure 1: System configuration Automatic acquisition of technical terms in a certain domain has been studied as automatic term recognition (Kageura and Umino, 1996; Kageura and Koyama, 2000), and the methods require a large corpus that are manually prepared for a target domain. In contrast, our system, which is proposed in this paper, requires only a seed word; from this seed word, the system compiles a corpus from the Web by using search engines and produces a dozen technical terms that are closely related to the seed word. 2 System Figure 1 shows the configuration of the system. The system</context>
<context position="5680" citStr="Sato, 2001" startWordPosition="977" endWordPosition="978">d relation test. 2.3.1 Technical-term test The technical-term test removes the terms that do not satisfy conditions of technical terms. We employ the following four conditions that a technical term should satisfy. 1. The term is sometimes or frequently used in a certain domain. 2. The term is not a general term. 3. There is a definition or explanation of the term. 4. There are several technical terms that are related to the term. We have implemented the checking program of the first two conditions in the system: the third condition can be checked by integrating the system with term explainer (Sato, 2001), which produces a definition or explanation of a given term; the fourth condition can be checked by using the system recursively. There are several choices for implementing the checking program. Our choice is to use the Web via a search engine. A search engine returns a number, hit, which is an estimated number of pages that satisfy a given query. In case the query is a term, its hit is the number of pages that contain the term on the Web. We use the following notation. H(x) = “the number of pages that contain the term x” The number H(x) can be used as an estimated frequency of the term x on </context>
</contexts>
<marker>Sato, 2001</marker>
<rawString>Satoshi Sato. 2001. Automated editing of hypertext r´esum´e from the world wide web. In Proceedings of 2001 Symposium on Applications and the Internet (SAINT 2001), pages 15–22.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>