<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002456">
<title confidence="0.9968075">
Automatic Labelling of Topic Models Learned from Twitter by
Summarisation
</title>
<author confidence="0.977878">
Amparo Elizabeth Cano Basavet Yulan Het Ruifeng Xu§
</author>
<affiliation confidence="0.9855805">
t Knowledge Media Institute, Open University, UK
t School of Engineering and Applied Science, Aston University, UK
§ Key Laboratory of Network Oriented Intelligent Computation
Shenzhen Graduate School, Harbin Institute of Technology, China
</affiliation>
<email confidence="0.997313">
amparo.cano@open.ac.uk, y.he@cantab.net, xuruifeng@hitsz.edu.cn
</email>
<sectionHeader confidence="0.993896" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979896551724">
Latent topics derived by topic models such
as Latent Dirichlet Allocation (LDA) are
the result of hidden thematic structures
which provide further insights into the
data. The automatic labelling of such
topics derived from social media poses
however new challenges since topics may
characterise novel events happening in the
real world. Existing automatic topic la-
belling approaches which depend on exter-
nal knowledge sources become less appli-
cable here since relevant articles/concepts
of the extracted topics may not exist in ex-
ternal sources. In this paper we propose
to address the problem of automatic la-
belling of latent topics learned from Twit-
ter as a summarisation problem. We in-
troduce a framework which apply sum-
marisation algorithms to generate topic la-
bels. These algorithms are independent
of external sources and only rely on the
identification of dominant terms in doc-
uments related to the latent topic. We
compare the efficiency of existing state
of the art summarisation algorithms. Our
results suggest that summarisation algo-
rithms generate better topic labels which
capture event-related context compared to
the top-n terms returned by LDA.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99984048">
Topic model based algorithms applied to social
media data have become a mainstream technique
in performing various tasks including sentiment
analysis (He, 2012) and event detection (Zhao et
al., 2012; Diao et al., 2012). However, one of
the main challenges is the task of understanding
the semantics of a topic. This task has been ap-
proached by investigating methodologies for iden-
tifying meaningful topics through semantic coher-
ence (Aletras and Stevenson, 2013; Mimno et al.,
2011; Newman et al., 2010) and for characterising
the semantic content of a topic through automatic
labelling techniques (Hulpus et al., 2013; Lau et
al., 2011; Mei et al., 2007). In this paper we focus
on the latter.
Our research task of automatic labelling a topic
consists on selecting a set of words that best de-
scribes the semantics of the terms involved in this
topic. The most generic approach to automatic la-
belling has been to use as primitive labels the top-
n words in a topic distribution learned by a topic
model such as LDA (Griffiths and Steyvers, 2004;
Blei et al., 2003). Such top words are usually
ranked using the marginal probabilities P(wiltj)
associated with each word wi for a given topic tj.
This task can be illustrated by considering the fol-
lowing topic derived from social media related to
Education:
school protest student fee choic motherlod
tuition teacher anger polic
where the top 10 words ranked by P(wi tj) for
this topic are listed. Therefore the task is to find
the top-n terms which are more representative of
the given topic. In this example, the topic certainly
relates to a student protest as revealed by the top
3 terms which can be used as a good label for this
topic.
However previous work has shown that top
terms are not enough for interpreting the coherent
meaning of a topic (Mei et al., 2007). More re-
cent approaches have explored the use of external
sources (e.g. Wikipedia, WordNet) for supporting
the automatic labelling of topics by deriving can-
didate labels by means of lexical (Lau et al., 2011;
Magatti et al., 2009; Mei et al., 2007) or graph-
based (Hulpus et al., 2013) algorithms applied on
these sources.
Mei et al. (2007) proposed an unsupervised
probabilistic methodology to automatically assign
a label to a topic model. Their proposed approach
</bodyText>
<page confidence="0.964123">
618
</page>
<bodyText confidence="0.993329948275862">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 618–624,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
was defined as an optimisation problem involving
the minimisation of the KL divergence between a
given topic and the candidate labels while max-
imising the mutual information between these two
word distributions. Lau et al. (2010) proposed to
label topics by selecting top-n terms to label the
overall topic based on different ranking mecha-
nisms including pointwise mutual information and
conditional probabilities.
Methods relying on external sources for auto-
matic labelling of topics include the work by Ma-
gatti et al. (2009) which derived candidate topic
labels for topics induced by LDA using the hi-
erarchy obtained from the Google Directory ser-
vice and expanded through the use of the OpenOf-
fice English Thesaurus. Lau et al. (2011) gen-
erated label candidates for a topic based on top-
ranking topic terms and titles of Wikipedia arti-
cles. They then built a Support Vector Regres-
sion (SVR) model for ranking the label candidates.
More recently, Hulpus et al. (2013) proposed to
make use of a structured data source (DBpedia)
and employed graph centrality measures to gener-
ate semantic concept labels which can characterise
the content of a topic.
Most previous topic labelling approaches focus
on topics derived from well formatted and static
documents. However in contrast to this type of
content, the labelling of topics derived from tweets
presents different challenges. In nature microp-
ost content is sparse and present ill-formed words.
Moreover, the use of Twitter as the “what’s-
happening-right now” tool, introduces new event-
dependent relations between words which might
not have a counter part in existing knowledge
sources (e.g. Wikipedia). Our original interest in
labelling topics stems from work in topic model
based event extraction from social media, in par-
ticular from tweets (Shen et al., 2013; Diao et
al., 2012). As opposed to previous approaches,
the research presented in this paper addresses the
labelling of topics exposing event-related content
that might not have a counter part on existing ex-
ternal sources. Based on the observation that a
short summary of a collection of documents can
serve as a label characterising the collection, we
propose to generate topic label candidates based
on the summarisation of a topic’s relevant docu-
ments. Our contributions are two-fold:
- We propose a novel approach for topics la-
belling that relies on term relevance of documents
relating to a topic; and
- We show that summarisation algorithms,
which are independent of extenal sources, can be
used with success to label topics, presenting a
higher perfomance than the top-n terms baseline.
</bodyText>
<sectionHeader confidence="0.989348" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.99997525">
We propose to approach the topic labelling prob-
lem as a multi-document summarisation task. The
following describes our proposed framework to
characterise documents relevant to a topic.
</bodyText>
<subsectionHeader confidence="0.970631">
2.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.998443055555556">
Given a set of documents the problem to be solved
by topic modelling is the posterior inference of the
variables, which determine the hidden thematic
structures that best explain an observed set of doc-
uments. Focusing on the Latent Dirichlet Alloca-
tion (LDA) model (Blei et al., 2003; Griffiths and
Steyvers, 2004), let D be a corpus of documents
denoted as D = {d1, d2,.., dD}; where each doc-
ument consists of a sequence of Nd words denoted
by d = (w1, w2,.., wNd); and each word in a
document is an item from a vocabulary index of
V different terms denoted by {1, 2,.., V }. Given
D documents containing K topics expressed over
V unique words, LDA generative process is de-
scribed as follows:
- For each topic k ∈ {1, ...K} draw Ok ∼
Dirichlet(β),
-For each document d ∈ {1..D}:
</bodyText>
<listItem confidence="0.971168375">
• draw Bd ∼ Dirichlet(α);
• For each word n ∈ {1..Nd} in document d:
◦ draw a topic zd,n ∼ Multinomial(Bd);
◦ draw a word wd,n ∼ Multinomial(ϕzd,n).
where Ok is the word distribution for topic k,
and Bd is the distribution of topics in document
d. Topics are interpreted using the top N terms
ranked based on the marginal probability p(wi|tj).
</listItem>
<subsectionHeader confidence="0.999386">
2.2 Automatic Labelling of Topic Models
</subsectionHeader>
<bodyText confidence="0.999974777777778">
Given K topics over the document collection D,
the topic labelling task consists on discovering a
sequence of words for each topic k ∈ K. We pro-
pose to generate topic label candidates by sum-
marising topic relevant documents. Such docu-
ments can be derived using both the observed data
from the corpus D and the inferred topic model
variables. In particular, the prominent topic of a
document d can be found by
</bodyText>
<equation confidence="0.9220275">
kd = arg max p(k|d) (1)
k∈K
</equation>
<page confidence="0.976188">
619
</page>
<bodyText confidence="0.998916">
Therefore given a topic k, a set of C documents
related to this topic can be obtained via equation
1.
Given the set of documents C relevant to topic k,
we proposed to generate a label of a desired length
x from the summarisation of C.
</bodyText>
<subsectionHeader confidence="0.99654">
2.3 Topic Labelling by Summarisation
</subsectionHeader>
<bodyText confidence="0.99995864">
We compare different summarisation algorithms
based on their ability to provide a good label to a
given topic. In particular we investigate the use of
lexical features by comparing three different well-
known multi-document summarisation algorithms
against the top-n topic terms baseline. These al-
gorithms include:
Sum Basic (SB) This is a frequency based sum-
marisation algorithm (Nenkova and Vanderwende,
2005), which computes initial word probabilities
for words in a text. It then weights each sen-
tence in the text (in our case a micropost) by
computing the average probability of the words in
the sentence. In each iteration it picks the high-
est weighted document and from it the highest
weighted word. It uses an update function which
penalises words which have already been picked.
Hybrid TFIDF (TFIDF) It is similar to SB,
however rather than computing the initial word
probabilities based on word frequencies it weights
terms based on TFIDF. In this case the document
frequency is computed as the number of times a
word appears in a micropost from the collection
C. Following the same procedure as SB it returns
the top x weighted terms.
Maximal Marginal Relevance (MMR) This is a
relevance based ranking algorithm (Carbonell and
Goldstein, 1998), which avoids redundancy in the
documents used for generating a summary. It mea-
sures the degree of dissimilarity between the docu-
ments considered and previously selected ones al-
ready in the ranked list.
Text Rank (TR) This is a graph-based sum-
mariser method (Mihalcea and Tarau, 2004) where
each word is a vertex. The relevance of a vertex
(term) to the graph is computed based on global
information recursively drawn from the whole
graph. It uses the PageRank algorithm (Brin and
Page, 1998) to recursively change the weight of
the vertices. The final score of a word is there-
fore not only dependent on the terms immediately
connected to it but also on how these terms con-
nect to others. To assign the weight of an edge
between two terms, TextRank computes word co-
occurrence in windows of N words (in our case
N = 10). Once a final score is calculated for each
vertex of the graph, TextRank sorts the terms in
a reverse order and provided the top T vertices in
the ranking. Each of these algorithms produces a
label of a desired length x for a given topic k.
</bodyText>
<sectionHeader confidence="0.994231" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.984579">
3.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999979684210526">
Our Twitter Corpus (TW) was collected between
November 2010 and January 2011. TW comprises
over 1 million tweets. We used the OpenCalais’
document categorisation service1 to generate cate-
gorical sets. In particular, we considered four dif-
ferent categories which contain many real-world
events, namely: War and Conflict (War), Disaster
and Accident (DisAc), Education (Edu) and Law
and Crime (LawCri). The final TW dataset after
removing retweets and short microposts (less than
5 words after removing stopwords) contains 7000
tweets in each category.
We preprocessed TW by first removing: punc-
tuation, numbers, non-alphabet characters, stop
words, user mentions, and URL links. We then
performed Porter stemming (Porter, 1980) in order
to reduce the vocabulary size. Finally to address
the issue of data sparseness in the TW dataset, we
removed words with a frequency lower than 5.
</bodyText>
<subsectionHeader confidence="0.998515">
3.2 Generating the Gold Standard
</subsectionHeader>
<bodyText confidence="0.999934833333333">
Evaluation of automatic topic labelling often re-
lied on human assessment which requires heavy
manual effort (Lau et al., 2011; Hulpus et al.,
2013). However performing human evaluations of
Social Media test sets comprising thousands of in-
puts become a difficult task. This is due to both
the corpus size, the diversity of event-related top-
ics and the limited availability of domain experts.
To alleviate this issue here, we followed the distri-
bution similarity approach, which has been widely
applied in the automatic generation of gold stan-
dards (GSs) for summary evaluations (Donaway et
al., 2000; Lin et al., 2006; Louis and Nenkova,
2009; Louis and Nenkova, 2013). This approach
compares two corpora, one for which no GS labels
exist, against a reference corpus for which a GS
exists. In our case these corpora correspond to the
TW and a Newswire dataset (NW). Since previous
</bodyText>
<footnote confidence="0.998502">
1OpenCalais service, http://www.opencalais.com
</footnote>
<page confidence="0.994031">
620
</page>
<bodyText confidence="0.892630961538462">
research has shown that headlines are good indi-
cators of the main focus of a text, both in struc-
ture and content, and that they can act as a human
produced abstract (Nenkova, 2005), we used head-
lines as the GS labels of NW.
The News Corpus (NW) was collected during
the same period of time as the TW corpus. NW
consists of a collection of news articles crawled
from traditional news media (BBC, CNN, and
New York Times) comprising over 77,000 articles
which include supplemental metadata (e.g. head-
line, author, publishing date). We also used the
OpenCalais’ document categorisation service to
automatically label news articles and considered
the same four topical categories, (War, DisAc,
Edu and LawCri). The same preprocessing steps
were performed on NW.
Therefore, following a similarity alignment ap-
proach we performed the steps oulined in Algo-
rithm 1 for generating the GS topic labels of a topic
in TW.
Algorithm 1 GS for Topic Labels
Input: LDA topics for TW, and the LDA topics for NW for
category c.
Output: Gold standard topic label for each of the LDA top-
ics for TW.
</bodyText>
<listItem confidence="0.9513185">
1: for each topic i E {1, 2, ..., 100} from TW do
2: for each topic j E {1, 2..., 100} from NW do
3: Compute the Cosine similarity between word dis-
tributions of topic ti and topic tj.
4: end for
5: Select topic j which has the highest similarity to i and
whose similarity measure is greater than a threshold
(in this case 0.7)
6: end for
7: for each of the extracted topic pairs (ti − tj) do
8: Collect relevant news articles CjNW of topic tj from
the NW set.
9: Extract the headlines of news articles from CjNW and
select the top x most frequent words as the gold stan-
dard label for topic ti in the TW set
10: end for
</listItem>
<bodyText confidence="0.999980133333333">
These steps can be outlined as follows:1) We
ran LDA on TW and NW separately for each cate-
gory with the number of topics set to 100; 2) We
then aligned the Twitter topics and Newswire top-
ics by the similarity measurement of word distri-
butions of these topics (Ercan and Cicekli, 2008;
Haghighi and Vanderwende, 2009; Wang et al.,
2009; Delort and Alfonseca, 2012); 3) Finally to
generate the GS label for each aligned topic pair
(ti − tj), we extracted the headlines of the news
articles relevant to tj and selected the top x most
frequent words (after stop word removal and stem-
ming). The generated label was used as the gold
standard label for the corresponding Twitter topic
ti in the topic pair.
</bodyText>
<sectionHeader confidence="0.997758" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999293696969697">
We compared the results of the summarisation
techniques with the top terms (TT) of a topic as
our baseline. These TT set corresponds to the
top x terms ranked based on the probability of
the word given the topic (p(w|k)) from the topic
model. We evaluated these summarisation ap-
proaches with the ROUGE-1 method (Lin, 2004),
a widely used summarisation evaluation metric
that correlates well with human evaluation (Liu
and Liu, 2008). This method measures the over-
lap of words between the generated summary and
a reference, in our case the GS generated from the
NW dataset.
The evaluation was performed at x =
{1, ..,10}. Figure 1 presents the ROUGE-1 per-
formance of the summarisation approaches as the
lengthx of the generated topic label increases. We
can see in all four categories that the SB and
TFIDF approaches provide a better summarisa-
tion coverage as the length of the topic label in-
creases. In particular, in both the Education
and Law &amp; Crime categories, both SB and
TFIDF outperforms TT and TR by a large margin.
The obtained ROUGE-1 performance is within the
same range of performance previously reported on
Social Media summarisation (Inouye and Kalita,
2011; Nichols et al., 2012; Ren et al., 2013).
Table 1 presents average results for ROUGE-
1 in the four categories. Particularly the SB
and TFIDF summarisation techniques consis-
tently outperform the TT baseline across all four
categories. SB gives the best results in three cate-
gories except War.
</bodyText>
<table confidence="0.99603">
TT SB
War 0.162 0.184
DisAc 0.134 0.194
Edu 0.106 0.240
LawCri 0.035 0.159
</table>
<tableCaption confidence="0.912318">
Table 1: Average ROUGE-1 for topic labels at x =
{1..10}, generated from the TW dataset.
</tableCaption>
<bodyText confidence="0.9912052">
The generated labels with summarisation at x =
5 are presented in Table 2, where GS represents the
label generated from the Newswire headlines.
Different summarisation techniques reveal
words which do not appear in the top terms but
</bodyText>
<figure confidence="0.968932585365853">
ROUGE-1
TFIDF MMR TR
0.192
0.160 0.132 0.124
0.187 0.104 0.023
0.149 0.034 0.115
0.154 0.141
621
Twitter Topics
Rouge
0.25
0.20
0.15
0.10
0.05
War_Conflict
Rouge
0.25
0.20
0.15
0.10
Disaster_Accident
Rouge
0.2
0.1
Education
Rouge
0.20
0.15
0.10
0.05
0.00
Law_Crime
variable
TT
SB
TFIDF
TR
MMR
2.5 5.0 7.5 10.0 2.5 5.0 7.5 10.0 2.5 5.0 7.5 10.0 2.5 5.0 7.5 10.0
x x x x
</figure>
<figureCaption confidence="0.9759845">
Figure 1: Performance in ROUGE for Twitter-derived topic labels, where x is the number of terms in the
generated label
</figureCaption>
<bodyText confidence="0.99993975862069">
which are relevant to the information clustered
by the topic. In this way, the labels generated for
topics belonging to different categories generally
extend the information provided by the top terms.
For example in Table 2, the DisAc headline is
characteristic of the New Zealand’s Pike River’s
coal mine blast accident, which is an event
occurred in November 2010.
Although the top 5 terms set from the LDA topic
extracted from TW (listed under TT) does capture
relevant information related to the event, it does
not provide information regarding the blast. In this
sense the topic label generated by SB more accu-
rately describes this event.
We can also notice that the GS labels generated
from Newswire media presented in Table 2 appear
on their own, to be good labels for the TW topics.
However as we described in the introduction we
want to avoid relaying on external sources for the
derivation of topic labels.
This experiment shows that frequency based
summarisation techniques outperform graph-
based and relevance based summarisation
techniques for generating topic labels that im-
prove upon the top-terms baseline, without relying
on external sources. This is an attractive property
for automatically generating topic labels for
tweets where their event-related content might not
have a counter part on existing external sources.
</bodyText>
<sectionHeader confidence="0.99297" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9971675">
In this paper we proposed a novel alternative to
topic labelling which do not rely on external data
sources. To the best of out knowledge no existing
work has been formally studied for automatic la-
belling through summarisation. This experiment
shows that existing summarisation techniques can
be exploited to provide a better label of a topic,
extending in this way a topic’s information by pro-
</bodyText>
<figure confidence="0.576510785714286">
War DisAc
GS protest brief polic mine zealand rescu miner
afghanistan attack world coal fire blast kill man dis-
leader bomb obama ast
pakistan
TT polic offic milit recent mine coal pike river
mosqu zealand
SB terror war polic arrest offic mine coal explos river pike
TFIDF polic war arrest offic terror mine coal pike safeti
zealand
mmR recent milit arrest attack trap zealand coal mine ex-
target plos
TR war world peac terror hope mine zealand plan fire fda
Edu LawCri
</figure>
<figureCaption confidence="0.268048">
GS school protest student fee man charg murder arrest
choic motherlod tuition polic brief woman attack
</figureCaption>
<bodyText confidence="0.9288446">
teacher anger polic inquiri found
TT student univers protest oc- man law child deal jail
cupi plan
SB student univers school man arrest law kill judg
protest educ
TFIDF student univers protest man arrest law judg kill
plan colleg
mmR nation colleg protest stu- found kid wife student jail
dent occupi
TR student tuition fee group man law child deal jail
hit
Table 2: Labelling examples for topics generated
from the TW Dataset. GS represents the gold-
standard generated from the relevant Newswire
dataset. All terms are Porter stemmed as described
in subsection 3.1
viding a richer context than top-terms. These re-
sults show that there is room to further improve
upon existing summarisation techniques to cater
for generating candidate labels.
</bodyText>
<sectionHeader confidence="0.989775" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999369">
This work was supported by the EPRSC grant
EP/J020427/1, the EU-FP7 project SENSE4US
(grant no. 611242), and the Shenzhen Interna-
tional Cooperation Research Funding (grant num-
ber GJHZ20120613110641217).
</bodyText>
<page confidence="0.997262">
622
</page>
<sectionHeader confidence="0.924257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997269089285714">
Nikolaos Aletras and Mark Stevenson. 2013. Evaluat-
ing topic coherence using distributional semantics.
In Proceedings of the 10th International Conference
on Computational Semantics (IWCS 2013) – Long
Papers, pages 13–22, Potsdam, Germany, March.
Association for Computational Linguistics.
David Meir Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. In J. Mach. Learn.
Res. 3, pages 993–1022.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine*
1. In Computer networks and ISDN systems, vol-
ume 30, pages 107–117.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ’98, pages 335–336, New York,
NY, USA. ACM.
Jean-Yves Delort and Enrique Alfonseca. 2012. Dual-
sum: A topic-model based approach for update sum-
marization. In Proceedings of the 13th Conference
of the European Chapter of the Association for Com-
putational Linguistics, EACL ’12, pages 214–223,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng Lim.
2012. Finding bursty topics from microblogs. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 536–544, Jeju Island, Korea,
July. Association for Computational Linguistics.
Robert L. Donaway, Kevin W. Drummey, and Laura A.
Mather. 2000. A comparison of rankings produced
by summarization evaluation measures. In Proceed-
ings of the 2000 NAACL-ANLP Workshop on Au-
tomatic Summarization, NAACL-ANLP-AutoSum
’00, pages 69–78, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Gonenc Ercan and Ilyas Cicekli. 2008. Lexical co-
hesion based topic modeling for summarization. In
Proceedings of the 9th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, CICLing’08, pages 582–592, Berlin, Hei-
delberg. Springer-Verlag.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228–5235.
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, NAACL ’09, pages 362–370,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Yulan He. 2012. Incorporating sentiment prior
knowledge for weakly supervised sentiment analy-
sis. ACM Transactions on Asian Language Infor-
mation Processing, 11(2):4:1–4:19, June.
Ioana Hulpus, Conor Hayes, Marcel Karnstedt, and
Derek Greene. 2013. Unsupervised graph-based
topic labelling using dbpedia. In Proceedings of the
sixth ACM international conference on Web search
and data mining, WSDM ’13, pages 465–474, New
York, NY, USA. ACM.
David Inouye and Jugal K. Kalita. 2011. Comparing
twitter summarization algorithms for multiple post
summaries. In SocialCom/PASSAT, pages 298–306.
IEEE.
Jey Han Lau, David Newman, Karimi Sarvnaz, and
Timothy Baldwin. 2010. Best Topic Word Selec-
tion for Topic Labelling. CoLing.
Jey Han Lau, Karl Grieser, David Newman, and Tim-
othy Baldwin. 2011. Automatic labelling of topic
models. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ’11, pages 1536–1545, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and
Jian-Yun Nie. 2006. An information-theoretic
approach to automatic evaluation of summaries.
In Proceedings of the Main Conference on Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, HLT-NAACL ’06, pages 463–
470, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Feifan Liu and Yang Liu. 2008. Correlation between
rouge and human evaluation of extractive meeting
summaries. In Proceedings of the 46th Annual
Meeting of the Association for Computational Lin-
guistics on Human Language Technologies: Short
Papers, HLT-Short ’08, pages 201–204, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Annie Louis and Ani Nenkova. 2009. Automatically
evaluating content selection in summarization with-
out human models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1 - Volume 1, EMNLP
’09, pages 306–314, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Annie Louis and Ani Nenkova. 2013. Automatically
assessing machine summary content without a gold
</reference>
<page confidence="0.989261">
623
</page>
<reference confidence="0.999946328947368">
standard. Computational Linguistics, 39(2):267–
300.
Davide Magatti, Silvia Calegari, Davide Ciucci, and
Fabio Stella. 2009. Automatic labeling of top-
ics. In Proceedings of the 2009 Ninth International
Conference on Intelligent Systems Design and Appli-
cations, ISDA ’09, pages 1227–1232, Washington,
DC, USA. IEEE Computer Society.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In Proceedings of the 13th ACM SIGKDD
international conference on Knowledge discovery
and data mining, KDD ’07, pages 490–499, New
York, NY, USA. ACM.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing Order into Texts. In Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’04, pages 404–411, Barcelona, Spain. As-
sociation for Computational Linguistics.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’11,
pages 262–272, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization. Microsoft Re-
search, Redmond, Washington, Tech. Rep. MSR-TR-
2005-101.
Ani Nenkova. 2005. Automatic text summarization of
newswire: Lessons learned from the document un-
derstanding conference. In Proceedings of the 20th
National Conference on Artificial Intelligence - Vol-
ume 3, AAAI’05, pages 1436–1441. AAAI Press.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ’10, pages 100–108, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jeffrey Nichols, Jalal Mahmud, and Clemens Drews.
2012. Summarizing sporting events using twitter. In
Proceedings of the 2012 ACM International Confer-
ence on Intelligent User Interfaces, IUI ’12, pages
189–198, New York, NY, USA. ACM.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Zhaochun Ren, Shangsong Liang, Edgar Meij, and
Maarten de Rijke. 2013. Personalized time-aware
tweets summarization. In Proceedings of the 36th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR
’13, pages 513–522, New York, NY, USA. ACM.
Chao Shen, Fei Liu, Fuliang Weng, and Tao Li. 2013.
A participant-based approach for event summariza-
tion using twitter streams. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ’13, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-document summarization us-
ing sentence-based topic models. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ’09, pages 297–300, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Xin Zhao, Baihan Shu, Jing Jiang, Yang Song, Hongfei
Yan, and Xiaoming Li. 2012. Identifying event-
related bursts via social media activities. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1466–1477, Jeju Island, Korea, July. Association for
Computational Linguistics.
</reference>
<page confidence="0.998646">
624
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.649852">
<title confidence="0.99874">Automatic Labelling of Topic Models Learned from Twitter Summarisation</title>
<author confidence="0.999387">Elizabeth Cano</author>
<affiliation confidence="0.999423">Media Institute, Open University, of Engineering and Applied Science, Aston University, Laboratory of Network Oriented Intelligent Shenzhen Graduate School, Harbin Institute of Technology,</affiliation>
<email confidence="0.826385">amparo.cano@open.ac.uk,y.he@cantab.net,xuruifeng@hitsz.edu.cn</email>
<abstract confidence="0.992889466666667">Latent topics derived by topic models such as Latent Dirichlet Allocation (LDA) are the result of hidden thematic structures which provide further insights into the data. The automatic labelling of such topics derived from social media poses however new challenges since topics may characterise novel events happening in the real world. Existing automatic topic labelling approaches which depend on external knowledge sources become less applicable here since relevant articles/concepts of the extracted topics may not exist in external sources. In this paper we propose to address the problem of automatic labelling of latent topics learned from Twitter as a summarisation problem. We introduce a framework which apply summarisation algorithms to generate topic labels. These algorithms are independent of external sources and only rely on the identification of dominant terms in documents related to the latent topic. We compare the efficiency of existing state of the art summarisation algorithms. Our results suggest that summarisation algorithms generate better topic labels which capture event-related context compared to returned by LDA.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nikolaos Aletras</author>
<author>Mark Stevenson</author>
</authors>
<title>Evaluating topic coherence using distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers,</booktitle>
<pages>13--22</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Potsdam, Germany,</location>
<contexts>
<context position="2077" citStr="Aletras and Stevenson, 2013" startWordPosition="306" endWordPosition="309">s suggest that summarisation algorithms generate better topic labels which capture event-related context compared to the top-n terms returned by LDA. 1 Introduction Topic model based algorithms applied to social media data have become a mainstream technique in performing various tasks including sentiment analysis (He, 2012) and event detection (Zhao et al., 2012; Diao et al., 2012). However, one of the main challenges is the task of understanding the semantics of a topic. This task has been approached by investigating methodologies for identifying meaningful topics through semantic coherence (Aletras and Stevenson, 2013; Mimno et al., 2011; Newman et al., 2010) and for characterising the semantic content of a topic through automatic labelling techniques (Hulpus et al., 2013; Lau et al., 2011; Mei et al., 2007). In this paper we focus on the latter. Our research task of automatic labelling a topic consists on selecting a set of words that best describes the semantics of the terms involved in this topic. The most generic approach to automatic labelling has been to use as primitive labels the topn words in a topic distribution learned by a topic model such as LDA (Griffiths and Steyvers, 2004; Blei et al., 2003</context>
</contexts>
<marker>Aletras, Stevenson, 2013</marker>
<rawString>Nikolaos Aletras and Mark Stevenson. 2013. Evaluating topic coherence using distributional semantics. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers, pages 13–22, Potsdam, Germany, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Meir Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>In J. Mach. Learn. Res.</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="2678" citStr="Blei et al., 2003" startWordPosition="414" endWordPosition="417">d Stevenson, 2013; Mimno et al., 2011; Newman et al., 2010) and for characterising the semantic content of a topic through automatic labelling techniques (Hulpus et al., 2013; Lau et al., 2011; Mei et al., 2007). In this paper we focus on the latter. Our research task of automatic labelling a topic consists on selecting a set of words that best describes the semantics of the terms involved in this topic. The most generic approach to automatic labelling has been to use as primitive labels the topn words in a topic distribution learned by a topic model such as LDA (Griffiths and Steyvers, 2004; Blei et al., 2003). Such top words are usually ranked using the marginal probabilities P(wiltj) associated with each word wi for a given topic tj. This task can be illustrated by considering the following topic derived from social media related to Education: school protest student fee choic motherlod tuition teacher anger polic where the top 10 words ranked by P(wi tj) for this topic are listed. Therefore the task is to find the top-n terms which are more representative of the given topic. In this example, the topic certainly relates to a student protest as revealed by the top 3 terms which can be used as a goo</context>
<context position="7218" citStr="Blei et al., 2003" startWordPosition="1145" endWordPosition="1148">nal sources, can be used with success to label topics, presenting a higher perfomance than the top-n terms baseline. 2 Methodology We propose to approach the topic labelling problem as a multi-document summarisation task. The following describes our proposed framework to characterise documents relevant to a topic. 2.1 Preliminaries Given a set of documents the problem to be solved by topic modelling is the posterior inference of the variables, which determine the hidden thematic structures that best explain an observed set of documents. Focusing on the Latent Dirichlet Allocation (LDA) model (Blei et al., 2003; Griffiths and Steyvers, 2004), let D be a corpus of documents denoted as D = {d1, d2,.., dD}; where each document consists of a sequence of Nd words denoted by d = (w1, w2,.., wNd); and each word in a document is an item from a vocabulary index of V different terms denoted by {1, 2,.., V }. Given D documents containing K topics expressed over V unique words, LDA generative process is described as follows: - For each topic k ∈ {1, ...K} draw Ok ∼ Dirichlet(β), -For each document d ∈ {1..D}: • draw Bd ∼ Dirichlet(α); • For each word n ∈ {1..Nd} in document d: ◦ draw a topic zd,n ∼ Multinomial(</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Meir Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. In J. Mach. Learn. Res. 3, pages 993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine* 1.</title>
<date>1998</date>
<booktitle>In Computer networks and ISDN systems,</booktitle>
<volume>30</volume>
<pages>107--117</pages>
<contexts>
<context position="10557" citStr="Brin and Page, 1998" startWordPosition="1728" endWordPosition="1731"> x weighted terms. Maximal Marginal Relevance (MMR) This is a relevance based ranking algorithm (Carbonell and Goldstein, 1998), which avoids redundancy in the documents used for generating a summary. It measures the degree of dissimilarity between the documents considered and previously selected ones already in the ranked list. Text Rank (TR) This is a graph-based summariser method (Mihalcea and Tarau, 2004) where each word is a vertex. The relevance of a vertex (term) to the graph is computed based on global information recursively drawn from the whole graph. It uses the PageRank algorithm (Brin and Page, 1998) to recursively change the weight of the vertices. The final score of a word is therefore not only dependent on the terms immediately connected to it but also on how these terms connect to others. To assign the weight of an edge between two terms, TextRank computes word cooccurrence in windows of N words (in our case N = 10). Once a final score is calculated for each vertex of the graph, TextRank sorts the terms in a reverse order and provided the top T vertices in the ranking. Each of these algorithms produces a label of a desired length x for a given topic k. 3 Experimental Setup 3.1 Dataset</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine* 1. In Computer networks and ISDN systems, volume 30, pages 107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of mmr, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’98,</booktitle>
<pages>335--336</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10064" citStr="Carbonell and Goldstein, 1998" startWordPosition="1645" endWordPosition="1648">tion it picks the highest weighted document and from it the highest weighted word. It uses an update function which penalises words which have already been picked. Hybrid TFIDF (TFIDF) It is similar to SB, however rather than computing the initial word probabilities based on word frequencies it weights terms based on TFIDF. In this case the document frequency is computed as the number of times a word appears in a micropost from the collection C. Following the same procedure as SB it returns the top x weighted terms. Maximal Marginal Relevance (MMR) This is a relevance based ranking algorithm (Carbonell and Goldstein, 1998), which avoids redundancy in the documents used for generating a summary. It measures the degree of dissimilarity between the documents considered and previously selected ones already in the ranked list. Text Rank (TR) This is a graph-based summariser method (Mihalcea and Tarau, 2004) where each word is a vertex. The relevance of a vertex (term) to the graph is computed based on global information recursively drawn from the whole graph. It uses the PageRank algorithm (Brin and Page, 1998) to recursively change the weight of the vertices. The final score of a word is therefore not only dependen</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’98, pages 335–336, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Yves Delort</author>
<author>Enrique Alfonseca</author>
</authors>
<title>Dualsum: A topic-model based approach for update summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12,</booktitle>
<pages>214--223</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="15065" citStr="Delort and Alfonseca, 2012" startWordPosition="2511" endWordPosition="2514">e extracted topic pairs (ti − tj) do 8: Collect relevant news articles CjNW of topic tj from the NW set. 9: Extract the headlines of news articles from CjNW and select the top x most frequent words as the gold standard label for topic ti in the TW set 10: end for These steps can be outlined as follows:1) We ran LDA on TW and NW separately for each category with the number of topics set to 100; 2) We then aligned the Twitter topics and Newswire topics by the similarity measurement of word distributions of these topics (Ercan and Cicekli, 2008; Haghighi and Vanderwende, 2009; Wang et al., 2009; Delort and Alfonseca, 2012); 3) Finally to generate the GS label for each aligned topic pair (ti − tj), we extracted the headlines of the news articles relevant to tj and selected the top x most frequent words (after stop word removal and stemming). The generated label was used as the gold standard label for the corresponding Twitter topic ti in the topic pair. 4 Experimental Results We compared the results of the summarisation techniques with the top terms (TT) of a topic as our baseline. These TT set corresponds to the top x terms ranked based on the probability of the word given the topic (p(w|k)) from the topic mode</context>
</contexts>
<marker>Delort, Alfonseca, 2012</marker>
<rawString>Jean-Yves Delort and Enrique Alfonseca. 2012. Dualsum: A topic-model based approach for update summarization. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12, pages 214–223, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiming Diao</author>
<author>Jing Jiang</author>
<author>Feida Zhu</author>
<author>Ee-Peng Lim</author>
</authors>
<title>Finding bursty topics from microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>536--544</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="1834" citStr="Diao et al., 2012" startWordPosition="268" endWordPosition="271">algorithms are independent of external sources and only rely on the identification of dominant terms in documents related to the latent topic. We compare the efficiency of existing state of the art summarisation algorithms. Our results suggest that summarisation algorithms generate better topic labels which capture event-related context compared to the top-n terms returned by LDA. 1 Introduction Topic model based algorithms applied to social media data have become a mainstream technique in performing various tasks including sentiment analysis (He, 2012) and event detection (Zhao et al., 2012; Diao et al., 2012). However, one of the main challenges is the task of understanding the semantics of a topic. This task has been approached by investigating methodologies for identifying meaningful topics through semantic coherence (Aletras and Stevenson, 2013; Mimno et al., 2011; Newman et al., 2010) and for characterising the semantic content of a topic through automatic labelling techniques (Hulpus et al., 2013; Lau et al., 2011; Mei et al., 2007). In this paper we focus on the latter. Our research task of automatic labelling a topic consists on selecting a set of words that best describes the semantics of </context>
<context position="5943" citStr="Diao et al., 2012" startWordPosition="942" endWordPosition="945"> from well formatted and static documents. However in contrast to this type of content, the labelling of topics derived from tweets presents different challenges. In nature micropost content is sparse and present ill-formed words. Moreover, the use of Twitter as the “what’shappening-right now” tool, introduces new eventdependent relations between words which might not have a counter part in existing knowledge sources (e.g. Wikipedia). Our original interest in labelling topics stems from work in topic model based event extraction from social media, in particular from tweets (Shen et al., 2013; Diao et al., 2012). As opposed to previous approaches, the research presented in this paper addresses the labelling of topics exposing event-related content that might not have a counter part on existing external sources. Based on the observation that a short summary of a collection of documents can serve as a label characterising the collection, we propose to generate topic label candidates based on the summarisation of a topic’s relevant documents. Our contributions are two-fold: - We propose a novel approach for topics labelling that relies on term relevance of documents relating to a topic; and - We show th</context>
</contexts>
<marker>Diao, Jiang, Zhu, Lim, 2012</marker>
<rawString>Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng Lim. 2012. Finding bursty topics from microblogs. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 536–544, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert L Donaway</author>
<author>Kevin W Drummey</author>
<author>Laura A Mather</author>
</authors>
<title>A comparison of rankings produced by summarization evaluation measures.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization, NAACL-ANLP-AutoSum ’00,</booktitle>
<pages>69--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12671" citStr="Donaway et al., 2000" startWordPosition="2078" endWordPosition="2081">ing the Gold Standard Evaluation of automatic topic labelling often relied on human assessment which requires heavy manual effort (Lau et al., 2011; Hulpus et al., 2013). However performing human evaluations of Social Media test sets comprising thousands of inputs become a difficult task. This is due to both the corpus size, the diversity of event-related topics and the limited availability of domain experts. To alleviate this issue here, we followed the distribution similarity approach, which has been widely applied in the automatic generation of gold standards (GSs) for summary evaluations (Donaway et al., 2000; Lin et al., 2006; Louis and Nenkova, 2009; Louis and Nenkova, 2013). This approach compares two corpora, one for which no GS labels exist, against a reference corpus for which a GS exists. In our case these corpora correspond to the TW and a Newswire dataset (NW). Since previous 1OpenCalais service, http://www.opencalais.com 620 research has shown that headlines are good indicators of the main focus of a text, both in structure and content, and that they can act as a human produced abstract (Nenkova, 2005), we used headlines as the GS labels of NW. The News Corpus (NW) was collected during t</context>
</contexts>
<marker>Donaway, Drummey, Mather, 2000</marker>
<rawString>Robert L. Donaway, Kevin W. Drummey, and Laura A. Mather. 2000. A comparison of rankings produced by summarization evaluation measures. In Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization, NAACL-ANLP-AutoSum ’00, pages 69–78, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonenc Ercan</author>
<author>Ilyas Cicekli</author>
</authors>
<title>Lexical cohesion based topic modeling for summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing’08,</booktitle>
<pages>582--592</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="14985" citStr="Ercan and Cicekli, 2008" startWordPosition="2499" endWordPosition="2502"> is greater than a threshold (in this case 0.7) 6: end for 7: for each of the extracted topic pairs (ti − tj) do 8: Collect relevant news articles CjNW of topic tj from the NW set. 9: Extract the headlines of news articles from CjNW and select the top x most frequent words as the gold standard label for topic ti in the TW set 10: end for These steps can be outlined as follows:1) We ran LDA on TW and NW separately for each category with the number of topics set to 100; 2) We then aligned the Twitter topics and Newswire topics by the similarity measurement of word distributions of these topics (Ercan and Cicekli, 2008; Haghighi and Vanderwende, 2009; Wang et al., 2009; Delort and Alfonseca, 2012); 3) Finally to generate the GS label for each aligned topic pair (ti − tj), we extracted the headlines of the news articles relevant to tj and selected the top x most frequent words (after stop word removal and stemming). The generated label was used as the gold standard label for the corresponding Twitter topic ti in the topic pair. 4 Experimental Results We compared the results of the summarisation techniques with the top terms (TT) of a topic as our baseline. These TT set corresponds to the top x terms ranked b</context>
</contexts>
<marker>Ercan, Cicekli, 2008</marker>
<rawString>Gonenc Ercan and Ilyas Cicekli. 2008. Lexical cohesion based topic modeling for summarization. In Proceedings of the 9th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing’08, pages 582–592, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<journal>PNAS,</journal>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="2658" citStr="Griffiths and Steyvers, 2004" startWordPosition="410" endWordPosition="413">semantic coherence (Aletras and Stevenson, 2013; Mimno et al., 2011; Newman et al., 2010) and for characterising the semantic content of a topic through automatic labelling techniques (Hulpus et al., 2013; Lau et al., 2011; Mei et al., 2007). In this paper we focus on the latter. Our research task of automatic labelling a topic consists on selecting a set of words that best describes the semantics of the terms involved in this topic. The most generic approach to automatic labelling has been to use as primitive labels the topn words in a topic distribution learned by a topic model such as LDA (Griffiths and Steyvers, 2004; Blei et al., 2003). Such top words are usually ranked using the marginal probabilities P(wiltj) associated with each word wi for a given topic tj. This task can be illustrated by considering the following topic derived from social media related to Education: school protest student fee choic motherlod tuition teacher anger polic where the top 10 words ranked by P(wi tj) for this topic are listed. Therefore the task is to find the top-n terms which are more representative of the given topic. In this example, the topic certainly relates to a student protest as revealed by the top 3 terms which </context>
<context position="7249" citStr="Griffiths and Steyvers, 2004" startWordPosition="1149" endWordPosition="1152"> used with success to label topics, presenting a higher perfomance than the top-n terms baseline. 2 Methodology We propose to approach the topic labelling problem as a multi-document summarisation task. The following describes our proposed framework to characterise documents relevant to a topic. 2.1 Preliminaries Given a set of documents the problem to be solved by topic modelling is the posterior inference of the variables, which determine the hidden thematic structures that best explain an observed set of documents. Focusing on the Latent Dirichlet Allocation (LDA) model (Blei et al., 2003; Griffiths and Steyvers, 2004), let D be a corpus of documents denoted as D = {d1, d2,.., dD}; where each document consists of a sequence of Nd words denoted by d = (w1, w2,.., wNd); and each word in a document is an item from a vocabulary index of V different terms denoted by {1, 2,.., V }. Given D documents containing K topics expressed over V unique words, LDA generative process is described as follows: - For each topic k ∈ {1, ...K} draw Ok ∼ Dirichlet(β), -For each document d ∈ {1..D}: • draw Bd ∼ Dirichlet(α); • For each word n ∈ {1..Nd} in document d: ◦ draw a topic zd,n ∼ Multinomial(Bd); ◦ draw a word wd,n ∼ Multi</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. PNAS, 101(suppl. 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="15017" citStr="Haghighi and Vanderwende, 2009" startWordPosition="2503" endWordPosition="2506">old (in this case 0.7) 6: end for 7: for each of the extracted topic pairs (ti − tj) do 8: Collect relevant news articles CjNW of topic tj from the NW set. 9: Extract the headlines of news articles from CjNW and select the top x most frequent words as the gold standard label for topic ti in the TW set 10: end for These steps can be outlined as follows:1) We ran LDA on TW and NW separately for each category with the number of topics set to 100; 2) We then aligned the Twitter topics and Newswire topics by the similarity measurement of word distributions of these topics (Ercan and Cicekli, 2008; Haghighi and Vanderwende, 2009; Wang et al., 2009; Delort and Alfonseca, 2012); 3) Finally to generate the GS label for each aligned topic pair (ti − tj), we extracted the headlines of the news articles relevant to tj and selected the top x most frequent words (after stop word removal and stemming). The generated label was used as the gold standard label for the corresponding Twitter topic ti in the topic pair. 4 Experimental Results We compared the results of the summarisation techniques with the top terms (TT) of a topic as our baseline. These TT set corresponds to the top x terms ranked based on the probability of the w</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 362–370, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
</authors>
<title>Incorporating sentiment prior knowledge for weakly supervised sentiment analysis.</title>
<date>2012</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="1775" citStr="He, 2012" startWordPosition="259" endWordPosition="260">sation algorithms to generate topic labels. These algorithms are independent of external sources and only rely on the identification of dominant terms in documents related to the latent topic. We compare the efficiency of existing state of the art summarisation algorithms. Our results suggest that summarisation algorithms generate better topic labels which capture event-related context compared to the top-n terms returned by LDA. 1 Introduction Topic model based algorithms applied to social media data have become a mainstream technique in performing various tasks including sentiment analysis (He, 2012) and event detection (Zhao et al., 2012; Diao et al., 2012). However, one of the main challenges is the task of understanding the semantics of a topic. This task has been approached by investigating methodologies for identifying meaningful topics through semantic coherence (Aletras and Stevenson, 2013; Mimno et al., 2011; Newman et al., 2010) and for characterising the semantic content of a topic through automatic labelling techniques (Hulpus et al., 2013; Lau et al., 2011; Mei et al., 2007). In this paper we focus on the latter. Our research task of automatic labelling a topic consists on sel</context>
</contexts>
<marker>He, 2012</marker>
<rawString>Yulan He. 2012. Incorporating sentiment prior knowledge for weakly supervised sentiment analysis. ACM Transactions on Asian Language Information Processing, 11(2):4:1–4:19, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioana Hulpus</author>
<author>Conor Hayes</author>
<author>Marcel Karnstedt</author>
<author>Derek Greene</author>
</authors>
<title>Unsupervised graph-based topic labelling using dbpedia.</title>
<date>2013</date>
<booktitle>In Proceedings of the sixth ACM international conference on Web search and data mining, WSDM ’13,</booktitle>
<pages>465--474</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2234" citStr="Hulpus et al., 2013" startWordPosition="331" endWordPosition="334">Topic model based algorithms applied to social media data have become a mainstream technique in performing various tasks including sentiment analysis (He, 2012) and event detection (Zhao et al., 2012; Diao et al., 2012). However, one of the main challenges is the task of understanding the semantics of a topic. This task has been approached by investigating methodologies for identifying meaningful topics through semantic coherence (Aletras and Stevenson, 2013; Mimno et al., 2011; Newman et al., 2010) and for characterising the semantic content of a topic through automatic labelling techniques (Hulpus et al., 2013; Lau et al., 2011; Mei et al., 2007). In this paper we focus on the latter. Our research task of automatic labelling a topic consists on selecting a set of words that best describes the semantics of the terms involved in this topic. The most generic approach to automatic labelling has been to use as primitive labels the topn words in a topic distribution learned by a topic model such as LDA (Griffiths and Steyvers, 2004; Blei et al., 2003). Such top words are usually ranked using the marginal probabilities P(wiltj) associated with each word wi for a given topic tj. This task can be illustrate</context>
<context position="3716" citStr="Hulpus et al., 2013" startWordPosition="595" endWordPosition="598">-n terms which are more representative of the given topic. In this example, the topic certainly relates to a student protest as revealed by the top 3 terms which can be used as a good label for this topic. However previous work has shown that top terms are not enough for interpreting the coherent meaning of a topic (Mei et al., 2007). More recent approaches have explored the use of external sources (e.g. Wikipedia, WordNet) for supporting the automatic labelling of topics by deriving candidate labels by means of lexical (Lau et al., 2011; Magatti et al., 2009; Mei et al., 2007) or graphbased (Hulpus et al., 2013) algorithms applied on these sources. Mei et al. (2007) proposed an unsupervised probabilistic methodology to automatically assign a label to a topic model. Their proposed approach 618 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 618–624, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics was defined as an optimisation problem involving the minimisation of the KL divergence between a given topic and the candidate labels while maximising the mutual information between these two word distrib</context>
<context position="5079" citStr="Hulpus et al. (2013)" startWordPosition="807" endWordPosition="810">cluding pointwise mutual information and conditional probabilities. Methods relying on external sources for automatic labelling of topics include the work by Magatti et al. (2009) which derived candidate topic labels for topics induced by LDA using the hierarchy obtained from the Google Directory service and expanded through the use of the OpenOffice English Thesaurus. Lau et al. (2011) generated label candidates for a topic based on topranking topic terms and titles of Wikipedia articles. They then built a Support Vector Regression (SVR) model for ranking the label candidates. More recently, Hulpus et al. (2013) proposed to make use of a structured data source (DBpedia) and employed graph centrality measures to generate semantic concept labels which can characterise the content of a topic. Most previous topic labelling approaches focus on topics derived from well formatted and static documents. However in contrast to this type of content, the labelling of topics derived from tweets presents different challenges. In nature micropost content is sparse and present ill-formed words. Moreover, the use of Twitter as the “what’shappening-right now” tool, introduces new eventdependent relations between words</context>
<context position="12220" citStr="Hulpus et al., 2013" startWordPosition="2006" endWordPosition="2009">ets and short microposts (less than 5 words after removing stopwords) contains 7000 tweets in each category. We preprocessed TW by first removing: punctuation, numbers, non-alphabet characters, stop words, user mentions, and URL links. We then performed Porter stemming (Porter, 1980) in order to reduce the vocabulary size. Finally to address the issue of data sparseness in the TW dataset, we removed words with a frequency lower than 5. 3.2 Generating the Gold Standard Evaluation of automatic topic labelling often relied on human assessment which requires heavy manual effort (Lau et al., 2011; Hulpus et al., 2013). However performing human evaluations of Social Media test sets comprising thousands of inputs become a difficult task. This is due to both the corpus size, the diversity of event-related topics and the limited availability of domain experts. To alleviate this issue here, we followed the distribution similarity approach, which has been widely applied in the automatic generation of gold standards (GSs) for summary evaluations (Donaway et al., 2000; Lin et al., 2006; Louis and Nenkova, 2009; Louis and Nenkova, 2013). This approach compares two corpora, one for which no GS labels exist, against </context>
</contexts>
<marker>Hulpus, Hayes, Karnstedt, Greene, 2013</marker>
<rawString>Ioana Hulpus, Conor Hayes, Marcel Karnstedt, and Derek Greene. 2013. Unsupervised graph-based topic labelling using dbpedia. In Proceedings of the sixth ACM international conference on Web search and data mining, WSDM ’13, pages 465–474, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Inouye</author>
<author>Jugal K Kalita</author>
</authors>
<title>Comparing twitter summarization algorithms for multiple post summaries.</title>
<date>2011</date>
<booktitle>In SocialCom/PASSAT,</booktitle>
<pages>298--306</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="16597" citStr="Inouye and Kalita, 2011" startWordPosition="2773" endWordPosition="2776">ed from the NW dataset. The evaluation was performed at x = {1, ..,10}. Figure 1 presents the ROUGE-1 performance of the summarisation approaches as the lengthx of the generated topic label increases. We can see in all four categories that the SB and TFIDF approaches provide a better summarisation coverage as the length of the topic label increases. In particular, in both the Education and Law &amp; Crime categories, both SB and TFIDF outperforms TT and TR by a large margin. The obtained ROUGE-1 performance is within the same range of performance previously reported on Social Media summarisation (Inouye and Kalita, 2011; Nichols et al., 2012; Ren et al., 2013). Table 1 presents average results for ROUGE1 in the four categories. Particularly the SB and TFIDF summarisation techniques consistently outperform the TT baseline across all four categories. SB gives the best results in three categories except War. TT SB War 0.162 0.184 DisAc 0.134 0.194 Edu 0.106 0.240 LawCri 0.035 0.159 Table 1: Average ROUGE-1 for topic labels at x = {1..10}, generated from the TW dataset. The generated labels with summarisation at x = 5 are presented in Table 2, where GS represents the label generated from the Newswire headlines. </context>
</contexts>
<marker>Inouye, Kalita, 2011</marker>
<rawString>David Inouye and Jugal K. Kalita. 2011. Comparing twitter summarization algorithms for multiple post summaries. In SocialCom/PASSAT, pages 298–306. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>David Newman</author>
<author>Karimi Sarvnaz</author>
<author>Timothy Baldwin</author>
</authors>
<title>Best Topic Word Selection for Topic Labelling.</title>
<date>2010</date>
<publisher>CoLing.</publisher>
<contexts>
<context position="4341" citStr="Lau et al. (2010)" startWordPosition="685" endWordPosition="688">hms applied on these sources. Mei et al. (2007) proposed an unsupervised probabilistic methodology to automatically assign a label to a topic model. Their proposed approach 618 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 618–624, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics was defined as an optimisation problem involving the minimisation of the KL divergence between a given topic and the candidate labels while maximising the mutual information between these two word distributions. Lau et al. (2010) proposed to label topics by selecting top-n terms to label the overall topic based on different ranking mechanisms including pointwise mutual information and conditional probabilities. Methods relying on external sources for automatic labelling of topics include the work by Magatti et al. (2009) which derived candidate topic labels for topics induced by LDA using the hierarchy obtained from the Google Directory service and expanded through the use of the OpenOffice English Thesaurus. Lau et al. (2011) generated label candidates for a topic based on topranking topic terms and titles of Wikiped</context>
</contexts>
<marker>Lau, Newman, Sarvnaz, Baldwin, 2010</marker>
<rawString>Jey Han Lau, David Newman, Karimi Sarvnaz, and Timothy Baldwin. 2010. Best Topic Word Selection for Topic Labelling. CoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic labelling of topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>1536--1545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2252" citStr="Lau et al., 2011" startWordPosition="335" endWordPosition="338">orithms applied to social media data have become a mainstream technique in performing various tasks including sentiment analysis (He, 2012) and event detection (Zhao et al., 2012; Diao et al., 2012). However, one of the main challenges is the task of understanding the semantics of a topic. This task has been approached by investigating methodologies for identifying meaningful topics through semantic coherence (Aletras and Stevenson, 2013; Mimno et al., 2011; Newman et al., 2010) and for characterising the semantic content of a topic through automatic labelling techniques (Hulpus et al., 2013; Lau et al., 2011; Mei et al., 2007). In this paper we focus on the latter. Our research task of automatic labelling a topic consists on selecting a set of words that best describes the semantics of the terms involved in this topic. The most generic approach to automatic labelling has been to use as primitive labels the topn words in a topic distribution learned by a topic model such as LDA (Griffiths and Steyvers, 2004; Blei et al., 2003). Such top words are usually ranked using the marginal probabilities P(wiltj) associated with each word wi for a given topic tj. This task can be illustrated by considering t</context>
<context position="3639" citStr="Lau et al., 2011" startWordPosition="580" endWordPosition="583">P(wi tj) for this topic are listed. Therefore the task is to find the top-n terms which are more representative of the given topic. In this example, the topic certainly relates to a student protest as revealed by the top 3 terms which can be used as a good label for this topic. However previous work has shown that top terms are not enough for interpreting the coherent meaning of a topic (Mei et al., 2007). More recent approaches have explored the use of external sources (e.g. Wikipedia, WordNet) for supporting the automatic labelling of topics by deriving candidate labels by means of lexical (Lau et al., 2011; Magatti et al., 2009; Mei et al., 2007) or graphbased (Hulpus et al., 2013) algorithms applied on these sources. Mei et al. (2007) proposed an unsupervised probabilistic methodology to automatically assign a label to a topic model. Their proposed approach 618 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 618–624, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics was defined as an optimisation problem involving the minimisation of the KL divergence between a given topic and the candidate </context>
<context position="12198" citStr="Lau et al., 2011" startWordPosition="2002" endWordPosition="2005">ter removing retweets and short microposts (less than 5 words after removing stopwords) contains 7000 tweets in each category. We preprocessed TW by first removing: punctuation, numbers, non-alphabet characters, stop words, user mentions, and URL links. We then performed Porter stemming (Porter, 1980) in order to reduce the vocabulary size. Finally to address the issue of data sparseness in the TW dataset, we removed words with a frequency lower than 5. 3.2 Generating the Gold Standard Evaluation of automatic topic labelling often relied on human assessment which requires heavy manual effort (Lau et al., 2011; Hulpus et al., 2013). However performing human evaluations of Social Media test sets comprising thousands of inputs become a difficult task. This is due to both the corpus size, the diversity of event-related topics and the limited availability of domain experts. To alleviate this issue here, we followed the distribution similarity approach, which has been widely applied in the automatic generation of gold standards (GSs) for summary evaluations (Donaway et al., 2000; Lin et al., 2006; Louis and Nenkova, 2009; Louis and Nenkova, 2013). This approach compares two corpora, one for which no GS </context>
</contexts>
<marker>Lau, Grieser, Newman, Baldwin, 2011</marker>
<rawString>Jey Han Lau, Karl Grieser, David Newman, and Timothy Baldwin. 2011. Automatic labelling of topic models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 1536–1545, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Guihong Cao</author>
<author>Jianfeng Gao</author>
<author>Jian-Yun Nie</author>
</authors>
<title>An information-theoretic approach to automatic evaluation of summaries.</title>
<date>2006</date>
<booktitle>In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06,</booktitle>
<pages>463--470</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12689" citStr="Lin et al., 2006" startWordPosition="2082" endWordPosition="2085">Evaluation of automatic topic labelling often relied on human assessment which requires heavy manual effort (Lau et al., 2011; Hulpus et al., 2013). However performing human evaluations of Social Media test sets comprising thousands of inputs become a difficult task. This is due to both the corpus size, the diversity of event-related topics and the limited availability of domain experts. To alleviate this issue here, we followed the distribution similarity approach, which has been widely applied in the automatic generation of gold standards (GSs) for summary evaluations (Donaway et al., 2000; Lin et al., 2006; Louis and Nenkova, 2009; Louis and Nenkova, 2013). This approach compares two corpora, one for which no GS labels exist, against a reference corpus for which a GS exists. In our case these corpora correspond to the TW and a Newswire dataset (NW). Since previous 1OpenCalais service, http://www.opencalais.com 620 research has shown that headlines are good indicators of the main focus of a text, both in structure and content, and that they can act as a human produced abstract (Nenkova, 2005), we used headlines as the GS labels of NW. The News Corpus (NW) was collected during the same period of </context>
</contexts>
<marker>Lin, Cao, Gao, Nie, 2006</marker>
<rawString>Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-Yun Nie. 2006. An information-theoretic approach to automatic evaluation of summaries. In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06, pages 463– 470, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="15747" citStr="Lin, 2004" startWordPosition="2633" endWordPosition="2634">tj), we extracted the headlines of the news articles relevant to tj and selected the top x most frequent words (after stop word removal and stemming). The generated label was used as the gold standard label for the corresponding Twitter topic ti in the topic pair. 4 Experimental Results We compared the results of the summarisation techniques with the top terms (TT) of a topic as our baseline. These TT set corresponds to the top x terms ranked based on the probability of the word given the topic (p(w|k)) from the topic model. We evaluated these summarisation approaches with the ROUGE-1 method (Lin, 2004), a widely used summarisation evaluation metric that correlates well with human evaluation (Liu and Liu, 2008). This method measures the overlap of words between the generated summary and a reference, in our case the GS generated from the NW dataset. The evaluation was performed at x = {1, ..,10}. Figure 1 presents the ROUGE-1 performance of the summarisation approaches as the lengthx of the generated topic label increases. We can see in all four categories that the SB and TFIDF approaches provide a better summarisation coverage as the length of the topic label increases. In particular, in bot</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifan Liu</author>
<author>Yang Liu</author>
</authors>
<title>Correlation between rouge and human evaluation of extractive meeting summaries.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, HLT-Short ’08,</booktitle>
<pages>201--204</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="15857" citStr="Liu and Liu, 2008" startWordPosition="2647" endWordPosition="2650">nt words (after stop word removal and stemming). The generated label was used as the gold standard label for the corresponding Twitter topic ti in the topic pair. 4 Experimental Results We compared the results of the summarisation techniques with the top terms (TT) of a topic as our baseline. These TT set corresponds to the top x terms ranked based on the probability of the word given the topic (p(w|k)) from the topic model. We evaluated these summarisation approaches with the ROUGE-1 method (Lin, 2004), a widely used summarisation evaluation metric that correlates well with human evaluation (Liu and Liu, 2008). This method measures the overlap of words between the generated summary and a reference, in our case the GS generated from the NW dataset. The evaluation was performed at x = {1, ..,10}. Figure 1 presents the ROUGE-1 performance of the summarisation approaches as the lengthx of the generated topic label increases. We can see in all four categories that the SB and TFIDF approaches provide a better summarisation coverage as the length of the topic label increases. In particular, in both the Education and Law &amp; Crime categories, both SB and TFIDF outperforms TT and TR by a large margin. The obt</context>
</contexts>
<marker>Liu, Liu, 2008</marker>
<rawString>Feifan Liu and Yang Liu. 2008. Correlation between rouge and human evaluation of extractive meeting summaries. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, HLT-Short ’08, pages 201–204, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatically evaluating content selection in summarization without human models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09,</booktitle>
<pages>306--314</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12714" citStr="Louis and Nenkova, 2009" startWordPosition="2086" endWordPosition="2089">matic topic labelling often relied on human assessment which requires heavy manual effort (Lau et al., 2011; Hulpus et al., 2013). However performing human evaluations of Social Media test sets comprising thousands of inputs become a difficult task. This is due to both the corpus size, the diversity of event-related topics and the limited availability of domain experts. To alleviate this issue here, we followed the distribution similarity approach, which has been widely applied in the automatic generation of gold standards (GSs) for summary evaluations (Donaway et al., 2000; Lin et al., 2006; Louis and Nenkova, 2009; Louis and Nenkova, 2013). This approach compares two corpora, one for which no GS labels exist, against a reference corpus for which a GS exists. In our case these corpora correspond to the TW and a Newswire dataset (NW). Since previous 1OpenCalais service, http://www.opencalais.com 620 research has shown that headlines are good indicators of the main focus of a text, both in structure and content, and that they can act as a human produced abstract (Nenkova, 2005), we used headlines as the GS labels of NW. The News Corpus (NW) was collected during the same period of time as the TW corpus. NW</context>
</contexts>
<marker>Louis, Nenkova, 2009</marker>
<rawString>Annie Louis and Ani Nenkova. 2009. Automatically evaluating content selection in summarization without human models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09, pages 306–314, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatically assessing machine summary content without a gold standard.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>2</issue>
<pages>300</pages>
<contexts>
<context position="12740" citStr="Louis and Nenkova, 2013" startWordPosition="2090" endWordPosition="2093">en relied on human assessment which requires heavy manual effort (Lau et al., 2011; Hulpus et al., 2013). However performing human evaluations of Social Media test sets comprising thousands of inputs become a difficult task. This is due to both the corpus size, the diversity of event-related topics and the limited availability of domain experts. To alleviate this issue here, we followed the distribution similarity approach, which has been widely applied in the automatic generation of gold standards (GSs) for summary evaluations (Donaway et al., 2000; Lin et al., 2006; Louis and Nenkova, 2009; Louis and Nenkova, 2013). This approach compares two corpora, one for which no GS labels exist, against a reference corpus for which a GS exists. In our case these corpora correspond to the TW and a Newswire dataset (NW). Since previous 1OpenCalais service, http://www.opencalais.com 620 research has shown that headlines are good indicators of the main focus of a text, both in structure and content, and that they can act as a human produced abstract (Nenkova, 2005), we used headlines as the GS labels of NW. The News Corpus (NW) was collected during the same period of time as the TW corpus. NW consists of a collection </context>
</contexts>
<marker>Louis, Nenkova, 2013</marker>
<rawString>Annie Louis and Ani Nenkova. 2013. Automatically assessing machine summary content without a gold standard. Computational Linguistics, 39(2):267– 300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Magatti</author>
<author>Silvia Calegari</author>
<author>Davide Ciucci</author>
<author>Fabio Stella</author>
</authors>
<title>Automatic labeling of topics.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Ninth International Conference on Intelligent Systems Design and Applications, ISDA ’09,</booktitle>
<pages>1227--1232</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="3661" citStr="Magatti et al., 2009" startWordPosition="584" endWordPosition="587">topic are listed. Therefore the task is to find the top-n terms which are more representative of the given topic. In this example, the topic certainly relates to a student protest as revealed by the top 3 terms which can be used as a good label for this topic. However previous work has shown that top terms are not enough for interpreting the coherent meaning of a topic (Mei et al., 2007). More recent approaches have explored the use of external sources (e.g. Wikipedia, WordNet) for supporting the automatic labelling of topics by deriving candidate labels by means of lexical (Lau et al., 2011; Magatti et al., 2009; Mei et al., 2007) or graphbased (Hulpus et al., 2013) algorithms applied on these sources. Mei et al. (2007) proposed an unsupervised probabilistic methodology to automatically assign a label to a topic model. Their proposed approach 618 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 618–624, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics was defined as an optimisation problem involving the minimisation of the KL divergence between a given topic and the candidate labels while maximisin</context>
</contexts>
<marker>Magatti, Calegari, Ciucci, Stella, 2009</marker>
<rawString>Davide Magatti, Silvia Calegari, Davide Ciucci, and Fabio Stella. 2009. Automatic labeling of topics. In Proceedings of the 2009 Ninth International Conference on Intelligent Systems Design and Applications, ISDA ’09, pages 1227–1232, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xuehua Shen</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Automatic labeling of multinomial topic models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’07,</booktitle>
<pages>490--499</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2271" citStr="Mei et al., 2007" startWordPosition="339" endWordPosition="342"> social media data have become a mainstream technique in performing various tasks including sentiment analysis (He, 2012) and event detection (Zhao et al., 2012; Diao et al., 2012). However, one of the main challenges is the task of understanding the semantics of a topic. This task has been approached by investigating methodologies for identifying meaningful topics through semantic coherence (Aletras and Stevenson, 2013; Mimno et al., 2011; Newman et al., 2010) and for characterising the semantic content of a topic through automatic labelling techniques (Hulpus et al., 2013; Lau et al., 2011; Mei et al., 2007). In this paper we focus on the latter. Our research task of automatic labelling a topic consists on selecting a set of words that best describes the semantics of the terms involved in this topic. The most generic approach to automatic labelling has been to use as primitive labels the topn words in a topic distribution learned by a topic model such as LDA (Griffiths and Steyvers, 2004; Blei et al., 2003). Such top words are usually ranked using the marginal probabilities P(wiltj) associated with each word wi for a given topic tj. This task can be illustrated by considering the following topic </context>
<context position="3680" citStr="Mei et al., 2007" startWordPosition="588" endWordPosition="591">efore the task is to find the top-n terms which are more representative of the given topic. In this example, the topic certainly relates to a student protest as revealed by the top 3 terms which can be used as a good label for this topic. However previous work has shown that top terms are not enough for interpreting the coherent meaning of a topic (Mei et al., 2007). More recent approaches have explored the use of external sources (e.g. Wikipedia, WordNet) for supporting the automatic labelling of topics by deriving candidate labels by means of lexical (Lau et al., 2011; Magatti et al., 2009; Mei et al., 2007) or graphbased (Hulpus et al., 2013) algorithms applied on these sources. Mei et al. (2007) proposed an unsupervised probabilistic methodology to automatically assign a label to a topic model. Their proposed approach 618 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 618–624, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics was defined as an optimisation problem involving the minimisation of the KL divergence between a given topic and the candidate labels while maximising the mutual inform</context>
</contexts>
<marker>Mei, Shen, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. 2007. Automatic labeling of multinomial topic models. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’07, pages 490–499, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing Order into Texts.</title>
<date>2004</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing, EMNLP ’04,</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Barcelona,</location>
<contexts>
<context position="10349" citStr="Mihalcea and Tarau, 2004" startWordPosition="1692" endWordPosition="1695">es it weights terms based on TFIDF. In this case the document frequency is computed as the number of times a word appears in a micropost from the collection C. Following the same procedure as SB it returns the top x weighted terms. Maximal Marginal Relevance (MMR) This is a relevance based ranking algorithm (Carbonell and Goldstein, 1998), which avoids redundancy in the documents used for generating a summary. It measures the degree of dissimilarity between the documents considered and previously selected ones already in the ranked list. Text Rank (TR) This is a graph-based summariser method (Mihalcea and Tarau, 2004) where each word is a vertex. The relevance of a vertex (term) to the graph is computed based on global information recursively drawn from the whole graph. It uses the PageRank algorithm (Brin and Page, 1998) to recursively change the weight of the vertices. The final score of a word is therefore not only dependent on the terms immediately connected to it but also on how these terms connect to others. To assign the weight of an edge between two terms, TextRank computes word cooccurrence in windows of N words (in our case N = 10). Once a final score is calculated for each vertex of the graph, T</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing Order into Texts. In Conference on Empirical Methods in Natural Language Processing, EMNLP ’04, pages 404–411, Barcelona, Spain. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Edmund Talley</author>
<author>Miriam Leenders</author>
<author>Andrew McCallum</author>
</authors>
<title>Optimizing semantic coherence in topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>262--272</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2097" citStr="Mimno et al., 2011" startWordPosition="310" endWordPosition="313">algorithms generate better topic labels which capture event-related context compared to the top-n terms returned by LDA. 1 Introduction Topic model based algorithms applied to social media data have become a mainstream technique in performing various tasks including sentiment analysis (He, 2012) and event detection (Zhao et al., 2012; Diao et al., 2012). However, one of the main challenges is the task of understanding the semantics of a topic. This task has been approached by investigating methodologies for identifying meaningful topics through semantic coherence (Aletras and Stevenson, 2013; Mimno et al., 2011; Newman et al., 2010) and for characterising the semantic content of a topic through automatic labelling techniques (Hulpus et al., 2013; Lau et al., 2011; Mei et al., 2007). In this paper we focus on the latter. Our research task of automatic labelling a topic consists on selecting a set of words that best describes the semantics of the terms involved in this topic. The most generic approach to automatic labelling has been to use as primitive labels the topn words in a topic distribution learned by a topic model such as LDA (Griffiths and Steyvers, 2004; Blei et al., 2003). Such top words ar</context>
</contexts>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>David Mimno, Hanna M. Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 262–272, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
</authors>
<title>The impact of frequency on summarization. Microsoft Research,</title>
<date>2005</date>
<pages>2005--101</pages>
<location>Redmond, Washington, Tech. Rep.</location>
<contexts>
<context position="9221" citStr="Nenkova and Vanderwende, 2005" startWordPosition="1503" endWordPosition="1506">ated to this topic can be obtained via equation 1. Given the set of documents C relevant to topic k, we proposed to generate a label of a desired length x from the summarisation of C. 2.3 Topic Labelling by Summarisation We compare different summarisation algorithms based on their ability to provide a good label to a given topic. In particular we investigate the use of lexical features by comparing three different wellknown multi-document summarisation algorithms against the top-n topic terms baseline. These algorithms include: Sum Basic (SB) This is a frequency based summarisation algorithm (Nenkova and Vanderwende, 2005), which computes initial word probabilities for words in a text. It then weights each sentence in the text (in our case a micropost) by computing the average probability of the words in the sentence. In each iteration it picks the highest weighted document and from it the highest weighted word. It uses an update function which penalises words which have already been picked. Hybrid TFIDF (TFIDF) It is similar to SB, however rather than computing the initial word probabilities based on word frequencies it weights terms based on TFIDF. In this case the document frequency is computed as the number</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Microsoft Research, Redmond, Washington, Tech. Rep. MSR-TR2005-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
</authors>
<title>Automatic text summarization of newswire: Lessons learned from the document understanding conference.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th National Conference on Artificial Intelligence - Volume 3, AAAI’05,</booktitle>
<pages>1436--1441</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="13184" citStr="Nenkova, 2005" startWordPosition="2167" endWordPosition="2168">ied in the automatic generation of gold standards (GSs) for summary evaluations (Donaway et al., 2000; Lin et al., 2006; Louis and Nenkova, 2009; Louis and Nenkova, 2013). This approach compares two corpora, one for which no GS labels exist, against a reference corpus for which a GS exists. In our case these corpora correspond to the TW and a Newswire dataset (NW). Since previous 1OpenCalais service, http://www.opencalais.com 620 research has shown that headlines are good indicators of the main focus of a text, both in structure and content, and that they can act as a human produced abstract (Nenkova, 2005), we used headlines as the GS labels of NW. The News Corpus (NW) was collected during the same period of time as the TW corpus. NW consists of a collection of news articles crawled from traditional news media (BBC, CNN, and New York Times) comprising over 77,000 articles which include supplemental metadata (e.g. headline, author, publishing date). We also used the OpenCalais’ document categorisation service to automatically label news articles and considered the same four topical categories, (War, DisAc, Edu and LawCri). The same preprocessing steps were performed on NW. Therefore, following a</context>
</contexts>
<marker>Nenkova, 2005</marker>
<rawString>Ani Nenkova. 2005. Automatic text summarization of newswire: Lessons learned from the document understanding conference. In Proceedings of the 20th National Conference on Artificial Intelligence - Volume 3, AAAI’05, pages 1436–1441. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>100--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2119" citStr="Newman et al., 2010" startWordPosition="314" endWordPosition="317">better topic labels which capture event-related context compared to the top-n terms returned by LDA. 1 Introduction Topic model based algorithms applied to social media data have become a mainstream technique in performing various tasks including sentiment analysis (He, 2012) and event detection (Zhao et al., 2012; Diao et al., 2012). However, one of the main challenges is the task of understanding the semantics of a topic. This task has been approached by investigating methodologies for identifying meaningful topics through semantic coherence (Aletras and Stevenson, 2013; Mimno et al., 2011; Newman et al., 2010) and for characterising the semantic content of a topic through automatic labelling techniques (Hulpus et al., 2013; Lau et al., 2011; Mei et al., 2007). In this paper we focus on the latter. Our research task of automatic labelling a topic consists on selecting a set of words that best describes the semantics of the terms involved in this topic. The most generic approach to automatic labelling has been to use as primitive labels the topn words in a topic distribution learned by a topic model such as LDA (Griffiths and Steyvers, 2004; Blei et al., 2003). Such top words are usually ranked using</context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 100–108, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Nichols</author>
<author>Jalal Mahmud</author>
<author>Clemens Drews</author>
</authors>
<title>Summarizing sporting events using twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 ACM International Conference on Intelligent User Interfaces, IUI ’12,</booktitle>
<pages>189--198</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="16619" citStr="Nichols et al., 2012" startWordPosition="2777" endWordPosition="2780">he evaluation was performed at x = {1, ..,10}. Figure 1 presents the ROUGE-1 performance of the summarisation approaches as the lengthx of the generated topic label increases. We can see in all four categories that the SB and TFIDF approaches provide a better summarisation coverage as the length of the topic label increases. In particular, in both the Education and Law &amp; Crime categories, both SB and TFIDF outperforms TT and TR by a large margin. The obtained ROUGE-1 performance is within the same range of performance previously reported on Social Media summarisation (Inouye and Kalita, 2011; Nichols et al., 2012; Ren et al., 2013). Table 1 presents average results for ROUGE1 in the four categories. Particularly the SB and TFIDF summarisation techniques consistently outperform the TT baseline across all four categories. SB gives the best results in three categories except War. TT SB War 0.162 0.184 DisAc 0.134 0.194 Edu 0.106 0.240 LawCri 0.035 0.159 Table 1: Average ROUGE-1 for topic labels at x = {1..10}, generated from the TW dataset. The generated labels with summarisation at x = 5 are presented in Table 2, where GS represents the label generated from the Newswire headlines. Different summarisatio</context>
</contexts>
<marker>Nichols, Mahmud, Drews, 2012</marker>
<rawString>Jeffrey Nichols, Jalal Mahmud, and Clemens Drews. 2012. Summarizing sporting events using twitter. In Proceedings of the 2012 ACM International Conference on Intelligent User Interfaces, IUI ’12, pages 189–198, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="11884" citStr="Porter, 1980" startWordPosition="1951" endWordPosition="1952"> We used the OpenCalais’ document categorisation service1 to generate categorical sets. In particular, we considered four different categories which contain many real-world events, namely: War and Conflict (War), Disaster and Accident (DisAc), Education (Edu) and Law and Crime (LawCri). The final TW dataset after removing retweets and short microposts (less than 5 words after removing stopwords) contains 7000 tweets in each category. We preprocessed TW by first removing: punctuation, numbers, non-alphabet characters, stop words, user mentions, and URL links. We then performed Porter stemming (Porter, 1980) in order to reduce the vocabulary size. Finally to address the issue of data sparseness in the TW dataset, we removed words with a frequency lower than 5. 3.2 Generating the Gold Standard Evaluation of automatic topic labelling often relied on human assessment which requires heavy manual effort (Lau et al., 2011; Hulpus et al., 2013). However performing human evaluations of Social Media test sets comprising thousands of inputs become a difficult task. This is due to both the corpus size, the diversity of event-related topics and the limited availability of domain experts. To alleviate this is</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaochun Ren</author>
<author>Shangsong Liang</author>
<author>Edgar Meij</author>
<author>Maarten de Rijke</author>
</authors>
<title>Personalized time-aware tweets summarization.</title>
<date>2013</date>
<booktitle>In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’13,</booktitle>
<pages>513--522</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Ren, Liang, Meij, de Rijke, 2013</marker>
<rawString>Zhaochun Ren, Shangsong Liang, Edgar Meij, and Maarten de Rijke. 2013. Personalized time-aware tweets summarization. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’13, pages 513–522, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Shen</author>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Tao Li</author>
</authors>
<title>A participant-based approach for event summarization using twitter streams.</title>
<date>2013</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’13,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5923" citStr="Shen et al., 2013" startWordPosition="938" endWordPosition="941">s on topics derived from well formatted and static documents. However in contrast to this type of content, the labelling of topics derived from tweets presents different challenges. In nature micropost content is sparse and present ill-formed words. Moreover, the use of Twitter as the “what’shappening-right now” tool, introduces new eventdependent relations between words which might not have a counter part in existing knowledge sources (e.g. Wikipedia). Our original interest in labelling topics stems from work in topic model based event extraction from social media, in particular from tweets (Shen et al., 2013; Diao et al., 2012). As opposed to previous approaches, the research presented in this paper addresses the labelling of topics exposing event-related content that might not have a counter part on existing external sources. Based on the observation that a short summary of a collection of documents can serve as a label characterising the collection, we propose to generate topic label candidates based on the summarisation of a topic’s relevant documents. Our contributions are two-fold: - We propose a novel approach for topics labelling that relies on term relevance of documents relating to a top</context>
</contexts>
<marker>Shen, Liu, Weng, Li, 2013</marker>
<rawString>Chao Shen, Fei Liu, Fuliang Weng, and Tao Li. 2013. A participant-based approach for event summarization using twitter streams. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’13, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Shenghuo Zhu</author>
<author>Tao Li</author>
<author>Yihong Gong</author>
</authors>
<title>Multi-document summarization using sentence-based topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09,</booktitle>
<pages>297--300</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="15036" citStr="Wang et al., 2009" startWordPosition="2507" endWordPosition="2510">r 7: for each of the extracted topic pairs (ti − tj) do 8: Collect relevant news articles CjNW of topic tj from the NW set. 9: Extract the headlines of news articles from CjNW and select the top x most frequent words as the gold standard label for topic ti in the TW set 10: end for These steps can be outlined as follows:1) We ran LDA on TW and NW separately for each category with the number of topics set to 100; 2) We then aligned the Twitter topics and Newswire topics by the similarity measurement of word distributions of these topics (Ercan and Cicekli, 2008; Haghighi and Vanderwende, 2009; Wang et al., 2009; Delort and Alfonseca, 2012); 3) Finally to generate the GS label for each aligned topic pair (ti − tj), we extracted the headlines of the news articles relevant to tj and selected the top x most frequent words (after stop word removal and stemming). The generated label was used as the gold standard label for the corresponding Twitter topic ti in the topic pair. 4 Experimental Results We compared the results of the summarisation techniques with the top terms (TT) of a topic as our baseline. These TT set corresponds to the top x terms ranked based on the probability of the word given the topic</context>
</contexts>
<marker>Wang, Zhu, Li, Gong, 2009</marker>
<rawString>Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong Gong. 2009. Multi-document summarization using sentence-based topic models. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09, pages 297–300, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Zhao</author>
<author>Baihan Shu</author>
<author>Jing Jiang</author>
<author>Yang Song</author>
<author>Hongfei Yan</author>
<author>Xiaoming Li</author>
</authors>
<title>Identifying eventrelated bursts via social media activities.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1466--1477</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="1814" citStr="Zhao et al., 2012" startWordPosition="264" endWordPosition="267">opic labels. These algorithms are independent of external sources and only rely on the identification of dominant terms in documents related to the latent topic. We compare the efficiency of existing state of the art summarisation algorithms. Our results suggest that summarisation algorithms generate better topic labels which capture event-related context compared to the top-n terms returned by LDA. 1 Introduction Topic model based algorithms applied to social media data have become a mainstream technique in performing various tasks including sentiment analysis (He, 2012) and event detection (Zhao et al., 2012; Diao et al., 2012). However, one of the main challenges is the task of understanding the semantics of a topic. This task has been approached by investigating methodologies for identifying meaningful topics through semantic coherence (Aletras and Stevenson, 2013; Mimno et al., 2011; Newman et al., 2010) and for characterising the semantic content of a topic through automatic labelling techniques (Hulpus et al., 2013; Lau et al., 2011; Mei et al., 2007). In this paper we focus on the latter. Our research task of automatic labelling a topic consists on selecting a set of words that best describ</context>
</contexts>
<marker>Zhao, Shu, Jiang, Song, Yan, Li, 2012</marker>
<rawString>Xin Zhao, Baihan Shu, Jing Jiang, Yang Song, Hongfei Yan, and Xiaoming Li. 2012. Identifying eventrelated bursts via social media activities. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1466–1477, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>