<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9987085">
An Alternative Conception of
Tree-Adjoining Derivation
</title>
<author confidence="0.991835">
Yves Schabes*
</author>
<affiliation confidence="0.914556">
Mitsubishi Electric Research Laboratory
</affiliation>
<author confidence="0.947995">
Stuart M. Shiebert
</author>
<affiliation confidence="0.820677">
Harvard University
</affiliation>
<bodyText confidence="0.998337125">
The precise formulation of derivation for tree-adjoining grammars has important ramifications
for a wide variety of uses of the formalism, from syntactic analysis to semantic interpretation and
statistical language modeling. We argue that the definition of tree-adjoining derivation must be
reformulated in order to manifest the proper linguistic dependencies in derivations. The particular
proposal is both precisely characterizable through a definition of TAG derivations as equivalence
classes of ordered derivation trees, and computationally operational, by virtue of a compilation
to linear indexed grammars together with an efficient algorithm for recognition and parsing
according to the compiled grammar.
</bodyText>
<sectionHeader confidence="0.990285" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999975863636364">
In a context-free grammar, the derivation of a string in the rewriting sense can be cap-
tured in a single canonical tree structure that abstracts all possible derivation orders.
As it turns out, this derivation tree also corresponds exactly to the hierarchical structure
that the derivation imposes on the string, the derived tree structure of the string. The
formalism of tree-adjoining grammars (TAG), on the other hand, decouples these two
notions of derivation tree and derived tree. Intuitively, the derivation tree is a more
finely grained structure than the derived tree, and as such can serve as a substrate
on which to pursue further analysis of the string. This intuitive possibility is made
manifest in several ways. Fine-grained syntactic analysis can be pursued by imposing
on the derivation tree further combinatorial constraints, for instance, selective adjoin-
ing constraints or equational constraints over feature structures. Statistical analysis
can be explored through the specification of derivational probabilities as formalized
in stochastic tree-adjoining grammars. Semantic analysis can be overlaid through the
synchronous derivations of two TAGs.
All of these methods rely on the derivation tree as the source of the important
primitive relationships among trees. The decoupling of derivation trees from derived
trees thus makes possible a more flexible ability to pursue these types of analyses. At
the same time, the exact definition of derivation becomes of paramount importance.
In this paper, we argue that previous definitions of tree-adjoining derivation have not
taken full advantage of this decoupling, and are not as appropriate as they might be
for the kind of further analysis that tree-adjoining analyses could make possible. In
particular, the standard definition of derivation, attributable to Vijay-Shanker (1987),
</bodyText>
<note confidence="0.7164165">
* Cambridge, MA 02139
t Division of Applied Sciences, Cambridge, MA 02138
© 1994 Association for Computational Linguistics
Computational Linguistics Volume 20, Number 1
</note>
<bodyText confidence="0.999845730769231">
requires that auxiliary trees be adjoined at distinct nodes in elementary trees. However,
in certain cases, especially cases characterized as linguistic modification, it is more
appropriate to allow multiple adjunctions at a single node.
In this paper we propose a redefinition of TAG derivation along these lines,
whereby multiple auxiliary trees of modification can be adjoined at a single node,
whereas only a single auxiliary tree of predication can. The redefinition constitutes a
new definition of derivation for TAG that we will refer to as extended derivation. For
such a redefinition to be serviceable, however, it is necessary that it be both precise
and operational. In service of the former, we provide a formal definition of extended
derivation using a new approach to representing derivations as equivalence classes of
ordered derivation trees. With respect to the latter, we provide a method of compi-
lation of TAGs into corresponding linear indexed grammars (LIG), which makes the
derivation structure explicit; and show how the generated LIG can drive a parsing
algorithm that recovers, either implicitly or explicitly, the extended derivations of the
string.
The paper is organized as follows. First we review Vijay-Shanker&apos;s standard defi-
nition of TAG derivation and introduce the motivation for extended derivations. Then
we present the extended notion of derivation and its formal definition. The original
compilation of TAGs to LIGs provided by Vijay-Shanker and Weir and our variant for
extended derivations are both described. Finally, we discuss a parsing algorithm for
TAG that operates by a variant of Earley parsing on the corresponding LIG. The set
of extended derivations can subsequently be recovered from the set of Earley items
generated by the algorithm. The resultant algorithm is further modified so as to build
an explicit derivation tree incrementally as parsing proceeds; this modification, which
is a novel result in its own right, allows the parsing algorithm to be used by systems
that require incremental processing with respect to tree-adjoining grammars.
</bodyText>
<sectionHeader confidence="0.91962" genericHeader="method">
2. The Standard Definition of Derivation
</sectionHeader>
<bodyText confidence="0.996862727272727">
To exemplify the distinction between standard and extended derivations, we exhibit
the TAG of Figure 1.1 This grammar derives some simple noun phrases such as
&amp;quot;roasted red pepper&amp;quot; and &amp;quot;baked red potato.&amp;quot; The former, for instance, is associated
with the derived tree in Figure 2(a). The tree can be viewed as being derived in two
ways:2
Dependent: The auxiliary tre€0,0 is adjoined at the root node (address Er of Ore-
The resultant tree is adjoined at the N node (address 1) of initial tree ape.
This derivation is depicted as the derivation tree in Figure 3(a).
Independent: The auxiliary trees Oro and Ore are adjoined at the N node (address
1) of the initial tree ape. This derivation is depicted as the derivation tree
in Figure 3(b).
</bodyText>
<footnote confidence="0.8844763">
1 Here and elsewhere, we conventionally use the Greek letter a and its subscripted and primed variants
for initial trees, 0 and its variants for auxiliary trees, and -y and its variants for elementary trees in
general. The foot node of an auxiliary tree is marked with an asterisk (&apos;*&apos;).
2 We ignore here the possibility of another dependent derivation wherein adjunction occurs at the foot
node of an auxiliary tree. Because this introduces yet another systematic ambiguity, it is typically
disallowed by stipulation in the literature on linguistic analyses using TAGs.
3 The address of a node in a tree is taken to be its Gorn number, that sequence of integers specifying
which branches to traverse in order starting from the root of the tree to reach the node. The address of
the root of the tree is therefore the empty sequence, notated e. See the appendix for a more complete
discussion of notation.
</footnote>
<page confidence="0.96268">
92
</page>
<note confidence="0.795957">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<figure confidence="0.993539666666667">
NP NP N N N
1 I
N N Ad] N* Ad] N* Ad] N*
I I 1 I I
potato pepper roasted red baked
(apo) (ape ) ro) (P re) (Pb)
</figure>
<figureCaption confidence="0.660109">
Figure 1
</figureCaption>
<figure confidence="0.9975889">
A sample tree-adjoining grammar.
NP NP
1 I
N N
ZN ZX
Adj N Adj N
roasted Ad] N red Ad] N
I I I I
red pepper roasted pepper
(a) (b)
</figure>
<figureCaption confidence="0.749782">
Figure 2
</figureCaption>
<figure confidence="0.9527725">
Two trees derived by the grammar of Figure 1.
a Pe
1 I
Pre ape
13ro Pro Pre
(a) (b)
</figure>
<figureCaption confidence="0.821222">
Figure 3
</figureCaption>
<bodyText confidence="0.401753">
Derivation trees for the derived tree of Figure 2(a) according to the grammar of Figure 1.
</bodyText>
<page confidence="0.993282">
93
</page>
<note confidence="0.925177">
Computational Linguistics Volume 20, Number 1
</note>
<bodyText confidence="0.999563761904762">
In the independent derivation, two trees are separately adjoined at one and the same
node in the initial tree. In the dependent derivation, on the other hand, one auxiliary
tree is adjoined to the other, the latter only being adjoined to the initial tree. We will
use this informal terminology uniformly in the sequel to distinguish the two general
topologies of derivation trees.
The standard definition of derivation, as codified by Vijay-Shanker, restricts deriva-
tions so that two adjunctions cannot occur at the same node in the same elementary tree. The
dependent notion of derivation (Figure 3(a)) is therefore the only sanctioned derivation
for the desired tree in Figure 2(a); the independent derivation (Figure 3(b)) is disal-
lowed. Vijay-Shanker&apos;s definition is appropriate because for any independent deriva-
tion, there is a dependent derivation of the same derived tree. This can be easily seen
in that any adjunction of 02 at a node at which an adjunction of )31 occurs could instead
be replaced by an adjunction of 02 at the root of 01.
The advantage of this standard definition of derivation is that a derivation tree in
this normal form unambiguously specifies a derived tree. The independent derivation
tree, on the other hand, is ambiguous as to the derived tree it specifies in that a
notion of precedence of the adjunctions at the same node is unspecified, but crucial to
the derived tree specified. This follows from the fact that the independent derivation
tree is symmetric with respect to the roles of the two auxiliary trees (by inspection),
whereas the derived tree is not. By symmetry, therefore, it must be the case that the
same independent derivation tree specifies the alternative derived tree in Figure 2(b).
</bodyText>
<sectionHeader confidence="0.922746" genericHeader="method">
3. Motivation for an Extended Definition of Derivation
</sectionHeader>
<bodyText confidence="0.9998515">
In the absence of some further interpretation of the derivation tree nothing hinges on
the choice of derivation definition, so that the standard definition disallowing inde-
pendent derivations is as reasonable as any other. However, tree-adjoining grammars
are almost universally extended with augmentations that make the issue apposite.
We discuss three such variations here, all of which argue for the use of independent
derivations under certain circumstances.&apos;
</bodyText>
<subsectionHeader confidence="0.999478">
3.1 Adding Adjoining Constraints
</subsectionHeader>
<bodyText confidence="0.994288052631579">
Already in very early work on tree-adjoining grammars (Joshi, Levy, and Takahashi
1975) constraints were allowed to be specified as to whether a particular auxiliary
tree may or may not be adjoined at a particular node in a particular tree. The idea
is formulated in its modern variant as selective-adjoining constraints (Vijay-Shanker and
Joshi 1985). As an application of this capability, we consider the traditional grammatical
view that directional adjuncts can be used only with certain verbs.&apos; This would account
4 The formulation of derivation for tree-adjoining grammars is also of significance for other grammatical
formalisms based on weaker forms of adjunction such as lexicalized context-free grammar (Schabes
and Waters 1993a) and its stochastic extension (Schabes and Waters 1993b), though we do not discuss
these arguments here.
5 For instance, Quirk, Greenbaum, Leech, and Svartvik (1985, page 517) remark that &amp;quot;direction adjuncts
of both goal and source can normally be used only with verbs of motion.&amp;quot; Although the restriction is
undoubtedly a semantic one, we will examine the modeling of it in a TAG deriving syntactic trees for
two reasons. First, the problematic nature of independent derivation is more easily seen in this way.
Second, much of the intuition behind TAG analyses is based on a tight relationship between syntactic
and semantic structure. Thus, whatever scheme for semantics is to be used with TAGs will require
appropriate derivations to model these data. For example, an analysis of this phenomenon by adjoining
constraints on the semantic half of a synchronous TAG would be subject to the identical argument. See
Section 3.3.
</bodyText>
<page confidence="0.997621">
94
</page>
<note confidence="0.964691">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<bodyText confidence="0.688977">
for the felicity distinctions between the following sentences:
</bodyText>
<listItem confidence="0.3912535">
1. a. Brockway walked his Labrador towards the yacht club.
b. # Brockway resembled his Labrador towards the yacht club.
</listItem>
<bodyText confidence="0.99934225">
This could be modeled by disallowing through selective adjoining constraints the
adjunction of the elementary tree corresponding to a towards adverbial at the VP node
of the elementary tree corresponding to the verb resembles.6 However, the restriction
applies even with intervening (and otherwise acceptable) adverbials.
</bodyText>
<listItem confidence="0.863573">
2. a. Brockway walked his Labrador yesterday.
b. Brockway walked his Labrador yesterday towards the yacht club.
3. a. Brockway resembled his Labrador yesterday.
b. # Brockway resembled his Labrador yesterday towards the yacht club.
</listItem>
<bodyText confidence="0.999301125">
Under the standard definition of derivation, there is no direct adjunction in the latter
sentence of the towards tree into the resembles tree. Rather, it is dependently adjoined
at the root of the elementary tree that heads the adverbial yesterday, the latter directly
adjoining into the main verb tree. To restrict both of the ill-formed sentences, then,
a restriction must be placed not only on adjoining the goal adverbial in a resembles
context, but also in the yesterday adverbial context. But this constraint is too strong, as
it disallows sentence (2b) above as well.
The problem is that the standard derivation does not correctly reflect the syn-
tactic relation between the adverbial modifier and the phrase it modifies when there
are multiple modifications in a single clause. In such a case, each of the adverbials
independently modifies the verb, and this should be reflected in their independent
adjunction at the same point. But this is specifically disallowed in a standard deriva-
tion.
Another example along the same lines follows from the requirement that tense
as manifested in a verb group be consistent with temporal adjuncts. For instance,
consider the following examples:
</bodyText>
<listItem confidence="0.979955">
4. a. Brockway walked his Labrador yesterday.
b. # Brockway will walk his Labrador yesterday.
5. a. # Brockway walked his Labrador tomorrow.
b. Brockway will walk his Labrador tomorrow.
Again, the relationship is independent of other intervening adjuncts.
6. a. Brockway walked his Labrador towards the yacht club yesterday.
b. # Brockway will walk his Labrador towards the yacht club yesterday.
7. a. # Brockway walked his Labrador towards the yacht club tomorrow.
b. Brockway will walk his Labrador towards the yacht club tomorrow.
</listItem>
<bodyText confidence="0.812219">
It is important to note that these arguments apply specifically to auxiliary trees that
correspond to a modification relationship. Auxiliary trees are used in TAG typically
</bodyText>
<footnote confidence="0.603705">
6 Whether the adjunction occurs at the VP node or the S node is immaterial to the argument.
</footnote>
<page confidence="0.995473">
95
</page>
<note confidence="0.564917">
Computational Linguistics Volume 20, Number 1
</note>
<bodyText confidence="0.993389666666667">
for predication relations as well,&apos; as in the case of raising and sentential complement
constructions.&apos; Consider the following sentences. (The brackets mark the leaves of the
pertinent trees to be combined by adjunction in the assumed analysis.)
</bodyText>
<listItem confidence="0.973889125">
8. a. Brockway assumed that Harrison wanted to walk his Labrador.
b. [Brockway assumed that] [Harrison wanted] [to walk his Labrador]
9. a. Brockway wanted to try to walk his Labrador.
b. [Brockway wanted] [to try] [to walk his Labrador]
10. a. * Harrison wanted Brockway tried to walk his Labrador.
b. * [Harrison wanted] [Brockway tried] [to walk his Labrador]
11. a. Harrison wanted to assume that Brockway walked his Labrador.
b. [Harrison wanted] [to assume that] [Brockway walked his Labrador]
</listItem>
<bodyText confidence="0.999683379310345">
Assume (following, for instance, the analysis of Kroch and Joshi [1985]) that the trees
associated with the various forms of the verbs try, want, and assume all take senten-
tial complements, certain of which are tensed with overt subjects and others untensed
with empty subjects. The auxiliary trees for these verbs specify by adjoining constraints
which type of sentential complement they take: assume requires tensed complements,
want and try untensed. Under this analysis the auxiliary trees must not be allowed to
independently adjoin at the same node. For instance, if trees corresponding to &amp;quot;Harri-
son wanted&amp;quot; and &amp;quot;Brockway tried&amp;quot; (which both require untensed complements) were
both adjoined at the root of the tree for &amp;quot;to walk his Labrador,&amp;quot; the selective adjoin-
ing constraints would be satisfied, yet the generated sentence (10a) is ungrammatical.
Conversely, under independent adjunction, sentence (11a) would be deemed ungram-
matical, although it is in fact grammatical. Thus, the case of predicative trees is entirely
unlike that of modifier trees. Here, the standard notion of derivation is exactly what
is needed as far as interpretation of adjoining constraints is concerned.
An alternative would be to modify the way in which adjoining constraints are
updated upon adjunction. If after adjoining a modifier tree at a node, the adjoining
constraints of the original node, rather than those of the root and foot of the modifier
tree, are manifest in the corresponding nodes in the derived tree, the adjoining con-
straints would propagate appropriately to handle the examples above. This alternative
leads, however, to a formalism for which derivation trees are no longer context-free,
with concomitant difficulties in designing parsing algorithms. Instead, the extended
definition of derivation effectively allows use of a Kleene-* in the &amp;quot;grammar&amp;quot; of deriva-
tion trees.
Adjoining constraints can also be implemented using feature structure equations
(Vijay-Shanker and Joshi 1988). It is possible that judicious use of such techniques
might prevent the particular problems noted here. Such an encoding of a solution
requires consideration of constraints that pass among many trees just to limit the co-
occurrence of a pair of trees. However, it more closely follows the spirit of TAGs to
state such intuitively local limitations locally.
</bodyText>
<footnote confidence="0.812049222222222">
7 We use the term &apos;predication&apos; in its logical sense, that is, for auxiliary trees that serve as logical
predicates over the trees into which they adjoin, in contrast to the term&apos;s linguistic sub-sense in which
the argument of the predicate is a linguistic subject.
8 The distinction between predicative and modifier trees has been proposed previously for purely
linguistic reasons by Kroch (1989), who refers to them as complement and athematic trees, respectively.
The arguments presented here can be seen as providing further evidence for differentiating the two
kinds of auxiliary trees. A precursor to this idea can perhaps be seen in the distinction between
repeatable and nonrepeatable adjunction in the formalism of string adjunct grammars, a precursor of
TAGs (Joshi, Kosaraju, and Yamada 1972b, pages 253-254).
</footnote>
<page confidence="0.997112">
96
</page>
<note confidence="0.829158">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<bodyText confidence="0.9999394">
In summary, the interpretation of adjoining constraints in TAG is sensitive to the
particular notion of derivation that is used. Therefore, it can be used as a litmus
test for an appropriate definition of derivation. As such, it argues for a nonstandard
independent notion of derivation for modifier auxiliary trees and a standard dependent
notion for predicative trees.
</bodyText>
<subsectionHeader confidence="0.999999">
3.2 Adding Statistical Parameters
</subsectionHeader>
<bodyText confidence="0.999993214285714">
In a similar vein, the statistical parameters of a stochastic lexicalized TAG (SLTAG)
(Resnik 1992; Schabes 1992) specify the probability of adjunction of a given auxiliary
tree at a specific node in another tree. This specification may again be interpreted
with regard to differing derivations, obviously with differing impact on the resulting
probabilities assigned to derivation trees. (In the extreme case, a constraint prohibiting
adjoining corresponds to a zero probability in an SLTAG. The relation to the argument
in the previous section follows thereby.) Consider a case in which linguistic modifi-
cation of noun phrases by adjectives is modeled by adjunction of a modifying tree.
Under the standard definition of derivation, multiple modifications of a single NP
would lead to dependent adjunctions in which a first modifier adjoins at the root of
a second. As an example, we consider again the grammar given in Figure 1, which
admits of derivations for the strings &amp;quot;baked red potato&amp;quot; and &amp;quot;baked red pepper.&amp;quot;
Specifying adjunction probabilities on standard derivations, the distinction between
the overall probabilities for these two strings depends solely on the adjunction proba-
bilities of Ore (the tree for red) into ap, and ape (those for potato and pepper, respectively),
as the tree 13b for the word baked is adjoined in both cases at the root of 0, in both
standard derivations. In the extended derivations, on the other hand, both modifying
trees are adjoined independently into the noun trees. Thus, the overall probabilities
are determined as well by the probabilities of adjunction of the trees for baked into the
nominal trees. It seems intuitively plausible that the most important relationships to
characterize statistically are those between modifier and modified, rather than between
two modifiers.&apos; In the case at hand, the fact that one typically refers to the process
of cooking potatoes as &amp;quot;baking,&amp;quot; whereas the appropriate term for the corresponding
cooking process applied to peppers is &amp;quot;roasting,&amp;quot; would be more determining of the
expected overall probabilities.
Note again that the distinction between modifier and predicative trees is important.
The standard definition of derivation is entirely appropriate for adjunction probabili-
ties for predicative trees, but not for modifier trees.
</bodyText>
<subsectionHeader confidence="0.999994">
3.3 Adding Semantics
</subsectionHeader>
<bodyText confidence="0.9999625">
Finally, the formation of synchronous TAGs has been proposed to allow use of TAGs
in semantic interpretation, natural language generation, and machine translation. In
previous work (Shieber and Schabes 1990), the definition of synchronous TAG deriva-
tion is given in a manner that requires multiple adjunctions at a single node. The need
for such derivations follows from the fact that synchronous derivations are intended
to model semantic relationships. In cases of multiple adjunction of modifier trees at
</bodyText>
<footnote confidence="0.599505375">
9 Intuition is an appropriate guide in the design of the SLTAG framework, as the idea is to set up a
linguistically plausible infrastructure on top of which a lexically based statistical model can be built. In
addition, suggestive (though certainly not conclusive) evidence along these lines can be gleaned from
corpora analyses. For instance, in a simple experiment in which medium frequency triples of exactly
the discussed form &amp;quot;(adjective) (adjective) (noun)&amp;quot; were examined, the mean mutual information
between the first adjective and the noun was found to be larger than that between the two adjectives.
The statistical assumptions behind this particular experiment do not allow very robust conclusions to
be drawn, and more work is needed along these lines.
</footnote>
<page confidence="0.998719">
97
</page>
<note confidence="0.565305">
Computational Linguistics Volume 20, Number 1
</note>
<bodyText confidence="0.968568777777778">
a single node, the appropriate semantic relationships comprise separate modifications
rather than cascaded ones, and this is reflected in the definition of synchronous TAG
derivation.10 Because of this, a parser for synchronous TAGs must recover, at least
implicitly, the extended derivations of TAG-derived trees. Shieber (in press) provides
a more complete discussion of the relationship between synchronous TAGs and the
extended definition of derivation with special emphasis on the ramifications for formal
expressivity.
Note that the independence of the adjunction of modifiers in the syntax does not
imply that semantically there is no precedence or scoping relation between them. As
exemplified in Figure 5, the derived tree generated by multiple independent adjunc-
tions at a single node still manifests nesting relationships among the adjoined trees.
This fact may be used to advantage in the semantic half of a synchronous tree-adjoining
grammar to specify the semantic distinction between, for example, the following two
sentences:11
12. a. Brockway ran over his polo mallet twice intentionally.
b. Brockway ran over his polo mallet intentionally twice.
We hope to address this issue in greater detail in future work on synchronous tree-
adjoining grammars.
</bodyText>
<subsectionHeader confidence="0.951641">
3.4 Desired Properties of Extended Derivations
</subsectionHeader>
<bodyText confidence="0.998761">
We have presented several arguments that the standard notion of derivation does not
allow for an appropriate specification of dependencies to be captured. An extended
notion of derivation is needed that
</bodyText>
<listItem confidence="0.9995155">
1. differentiates predicative and modifier auxiliary trees;
2. requires dependent derivations for predicative trees;
3. allows independent derivations for modifier trees; and
4. unambiguously and nonredundantly specifies a derived tree.
</listItem>
<bodyText confidence="0.894931333333333">
Furthermore, following from considerations of the role of modifier trees in a grammar
as essentially optional and freely applicable elements, we would like the following
criterion to hold of extended derivations:
</bodyText>
<listItem confidence="0.985118">
5. If a node can be modified at all, it can be modified any number of times,
including zero times.
</listItem>
<bodyText confidence="0.9972178">
Recall that a derivation tree (as traditionally conceived) is a tree with unordered
arcs where each node is labeled by an elementary tree of a TAG and each arc is labeled
by a tree address specifying a node in the parent tree. In a standard derivation tree
no two sibling arcs can be labeled with the same address. In an extended derivation
tree, however, the condition is relaxed: No two sibling arcs to predicative trees can be
</bodyText>
<footnote confidence="0.98981">
10 The importance of the distinction between predicative and modifier trees with respect to how
derivations are defined was not appreciated in the earlier work; derivations were taken to be of the
independent variety in all cases. In future work, we plan to remedy this flaw.
11 We are indebted to an anonymous reviewer of an earlier version of this paper for raising this issue
crisply through examples similar to those given here.
</footnote>
<page confidence="0.98028">
98
</page>
<note confidence="0.831335">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<bodyText confidence="0.999809833333333">
labeled with the same address. Thus, for any given address there can be at most one
predicative tree and several modifier trees adjoined at that node. As we have seen, this
relaxed definition violates the fourth desideratum above; for instance, the derivation
tree in Figure 3(b) ambiguously specifies both derived trees in Figure 2. In the next
section we provide a formal definition of extended derivations that satisfies all of the
criteria above.
</bodyText>
<sectionHeader confidence="0.941941" genericHeader="method">
4. Formal Definition of Extended Derivations
</sectionHeader>
<bodyText confidence="0.9999548">
In this section we introduce a new framework for describing TAG derivation trees that
allows for a natural expression of both standard and extended derivations, and makes
available even more fine-grained restrictions on derivation trees. First, we define or-
dered derivation trees and show that they unambiguously but redundantly specify
derivations.12 We characterize the redundant trees as those related by a sibling swap-
ping operation. Derivation trees proper are then taken to be the equivalence classes of
ordered derivation trees in which the equivalence relation is generated by the sibling
swapping. By limiting the underlying set of ordered derivation trees in various ways,
Vijay-Shanker&apos;s definition of derivation tree, a precise form of the extended definition,
and many other definitions of derivation can be characterized in this way.
</bodyText>
<subsectionHeader confidence="0.999206">
4.1 Ordered Derivation Trees
</subsectionHeader>
<bodyText confidence="0.979299592592592">
Ordered derivation trees, like the traditional derivation trees, are trees with nodes
labeled by elementary trees where each arc is labeled with an address in the tree for
the parent node of the arc. However, the arcs are taken to be ordered with respect to
each other.
An ordered derivation tree is well-formed if for each of its arcs, linking parent
node labeled -y to child node labeled -y&apos; and itself labeled with address t, the tree -y&apos;
is an auxiliary tree that can be adjoined at the node t in the tree -y. (Alternatively, if
substitution is allowed, -y&apos; may be an initial tree that can be substituted at the node t
in -y. Later definitions ignore this possibility, but are easily generalized.)
We define the function D from ordered derivation trees to the derived trees they
specify, according to the following recursive definition:
-y if D is a trivial tree of one node labeled with the elementary tree -y
-y[D(Di)Iti,D(D2)1t2,... ,D(Dk)Itk]
if D is a tree with root node labeled with the elementary tree
and with k child subtrees D1, • • • , Dk
whose arcs are labeled with addresses t1, . • • , tk•
D(D) =
Here -y[Ai /t1,., Ak I tk] specifies the simultaneous adjunction of trees A1 through Ak
at t1 through tk, respectively, in -y . It is defined as the iterative adjunction of the A, in
order at their respective addresses, with appropriate updating of the tree addresses of
any later adjunction to reflect the effect of earlier adjunctions that occur at addresses
dominating the address of the later adjunction.
12 Historical precedent for independent derivation and the associated ordered derivation trees can be
found in the derivation trees postulated for string adjunct grammars (Joshi, Kosaraju, and Yamada
1972a, 99-100). In this system, siblings in derivation trees are viewed as totally, not partially, ordered.
The systematic ambiguity introduced thereby is eliminated by stipulating that the sibling order be
consistent with an arbitrary ordering on adjunction sites.
</bodyText>
<page confidence="0.988481">
99
</page>
<note confidence="0.55876">
Computational Linguistics Volume 20, Number 1
</note>
<subsectionHeader confidence="0.998229">
4.2 Derivation Trees
</subsectionHeader>
<bodyText confidence="0.998089837837838">
It is easy to see that the derived tree specified by a given ordered derivation tree is
unchanged if adjacent siblings whose arcs are labeled with different tree addresses are
swapped. (This is not true of adjacent siblings whose arcs are labeled with the same
address.) That is, if t t&apos; then 7[.. , Alt,B/ t&apos; , .1 = 7[. ,B/t&apos; , A/ t, .1. A graphical
&amp;quot;proof&amp;quot; of this intuitive fact is given in Figure 4. A formal proof, although tedious and
unenlightening, is possible as well. We provide it in an appendix, primarily because
the definitional aspects of the TAG formulation may be of some interest.
This fact about the swapping of adjacent siblings shows that ordered derivation
trees possess an inherent redundancy. The order of adjacent sibling subtrees labeled
with different tree addresses is immaterial. Consequently, we can define true derivation
trees to be the equivalence classes of the base set of ordered derivation trees under the
equivalence relation generated by the sibling subtree swapping operation above. This
is a well-formed definition by virtue of the proposition argued informally above.
This definition generalizes the traditional definition in not restricting the tree ad-
dress labels in any way. It therefore satisfies criterion (3) of Section 3.4. Furthermore, by
virtue of the explicit quotient with respect to sibling swapping, a derivation tree under
this definition unambiguously and nonredundantly specifies a derived tree (criterion
4). It does not, however, differentiate predicative from modifier trees (criterion (1)), nor
can it therefore mandate dependent derivations for predicative trees (criterion (2)).
This general approach can, however, be specialized to correspond to several pre-
vious definitions of derivation tree. For instance, if we further restrict the base set
of ordered derivation trees so that no two siblings are labeled with the same tree
address, then the equivalence relation over these ordered derivation trees allows for
full reordering of all siblings. Clearly, these equivalence classes are isomorphic to the
unordered trees, and we have reconstructed Vijay-Shanker&apos;s standard definition of
derivation tree.
If we instead restrict ordered derivation trees so that no two siblings corresponding
to predicative trees are labeled with the same tree address, then we have reconstructed
a version of the extended definition argued for in this paper. Under this restriction,
criteria (1) and (2) are satisfied, while maintaining (3) and (4).
By careful selection of other constraints on the base set, other linguistic restrictions
might be imposed on derivation trees, still using the same definition of derivation trees
as equivalence classes over ordered derivation trees. In the next section, we show that
the definition of the previous paragraph should be further restricted to disallow the
reordering of predicative and modifier trees. We also describe other potential linguistic
applications of the ability to finely control the notion of derivation through the use of
ordered derivation trees.
</bodyText>
<subsectionHeader confidence="0.999708">
4.3 Further Restrictions on Extended Derivations
</subsectionHeader>
<bodyText confidence="0.999983363636364">
The extended definition of derivation tree given in the previous section effectively
specifies the output derived tree by adding a partial ordering on sibling arcs that
correspond to modifier trees adjoined at the same address. All other arcs are effectively
unordered (in the sense that all relative orderings of them exist in the equivalence
class).
Assume that in a given tree 7 at a particular address t, the k modifier trees , . . . ,
are directly adjoined in that order. Associated with the subtrees rooted at the k ele-
mentary auxiliary trees in this derivation are k derived auxiliary trees (A1,.. • , Akt
respectively). The derived tree specified by this derivation tree, according to the def-
inition of D given above, would have the derived tree A1 directly below A2 and so
forth, with Ak at the top. Now suppose that in addition, a predicative tree 7r is also
</bodyText>
<page confidence="0.974058">
100
</page>
<figureCaption confidence="0.962257">
Figure 4
</figureCaption>
<bodyText confidence="0.984263285714286">
A graphical proof of the irrelevance of adjacent sibling swapping.
These diagrams show the effect of performing two adjunctions (of auxiliary trees depicted,
one as dark-shaded and one light-shaded), presumed to be specified by adjacent siblings in an
ordered derivation tree. The adjunctions are to occur at two addresses (referred to in this
caption as t and t&apos;, respectively). The two addresses must be such that either (a) they are
distinct but neither dominates the other, (b) t dominates t&apos; (or vice versa), or (c) they are
identical. In case (a) the diagram shows that either order of adjunction yields the same
derived tree. Adjunction at t and then t&apos; corresponds to the upper arrows, adjunction at t&apos; and
then t the lower arrows. Similarly, in case (b), adjunction at t followed by adjunction at an
appropriately updated t&apos; yields the same result as adjunction first at t&apos; and then at t. Clearly,
adjunctions occurring before these two or after do not affect the interchangeability. Thus, if
two adjacent siblings in a derivation tree specify adjunctions at distinct addresses t and t&apos;, the
adjunctions can occur in either order. Diagram (c) demonstrates that this is not the case when
t and t&apos; are the same.
</bodyText>
<note confidence="0.664775">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<page confidence="0.844196">
101
</page>
<figure confidence="0.999018">
Computational Linguistics Volume 20, Number 1
(a) (b)
</figure>
<figureCaption confidence="0.987924">
Figure 5
</figureCaption>
<bodyText confidence="0.995667">
Schematic extended derivation tree and associated derived tree.
In a derived tree, the predicative tree adjoined at an address t is required to follow all
modifier trees adjoined at the same address, as in (a). The derived tree therefore appears as
depicted in (b) with the predicative tree outermost.
adjoined at address t. It must be ordered with respect to the tt, in the derivation tree,
and its relative order determines where in the bottom-to-top order in the derived tree
the tree /17, associated with the subderivation rooted at 7 goes.
The question that we raise here is whether all k +1 possible placements of the tree
7r relative to the ,a, are linguistically reasonable. We might allow all k + 1 orderings
(as in the definition of the previous section), or we might restrict them by requiring,
say, that the predicative tree always be adjoined before, or perhaps after, any modifier
trees at a given address. We emphasize that this is a linguistic question, in the sense
that the definition of extended derivation is well formed whatever decision is made
on this question.
Henceforth, we will assume that predicative trees are always adjoined after any
modifier trees at the same address, so that they appear above the modifier trees in the
derived tree. We call this &amp;quot;outermost predication&amp;quot; because a predicative tree appears
wrapped around the outside of the modifier trees adjoined at the same address. (See
Figure 5.) If we were to mandate innermost predication, in which a predicative tree
is always adjoined before the modifier trees at the same address, the predicative tree
would appear within all of the modifier trees, innermost in the derived tree.
Linguistically, the outermost method specifies that if both a predicative tree and a
modifier tree are adjoined at a single node, then the predicative tree attaches higher
than the modifier tree; in terms of the derived tree, it is as if the predicative tree
were adjoined at the root of the modifier tree. This accords with the semantic intuition
that in such a case (for English at least), the modifier is modifying the original tree,
not the predicative one. (The alternate &amp;quot;reading,&amp;quot; in which the modifier modifies the
predicative tree, is still obtainable under an outermost-predication standard by having
the modifier auxiliary tree adjoin dependently at the root node of the predicative tree.)
</bodyText>
<page confidence="0.996431">
102
</page>
<note confidence="0.852684">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<bodyText confidence="0.98927">
In contrast, the innermost-predication method specifies that the modifier tree attaches
higher, as if the modifier tree adjoined at the root of the predicative tree and was
therefore modifying the predicative tree, contra semantic intuitions.
For this reason, we specify that outermost predication is mandated. This is easily
done by further limiting the base set of ordered derivation trees to those in which
predicative trees are ordered after modifier tree siblings.
(From a technical standpoint, by the way, the outermost-predication method has
the advantage that it requires no changes to the parsing rules to be presented later,
but only a single addition. The innermost-predication method induces some subtle
interactions between the original parsing rules and the additional one, necessitating
a much more complicated set of modifications to the original algorithm. In fact, the
complexities in generating such an algorithm constituted the precipitating factor that
led us to revise our original innermost-predication attempt at redefining tree-adjoining
derivation. The linguistic argument, although commanding, became clear to us only
later.)
Another possibility, which we mention but do not pursue here, is to allow for
language-particular precedence constraints to restrict the possible orderings of deriva-
tion-tree siblings, in a manner similar to the linear precedence constraints of ID/LP
format (Gazdar, Klein, Pullum, and Sag 1985) but at the level of derivation trees.
These might be interpreted as hard constraints or soft orderings depending on the
application. This more fine-grained approach to the issue of ordering has several ap-
plications. Soft orderings might be used to account for ordering preferences among
modifiers, such as the default ordering of English adjectives that accounts for the typ-
ical preference for &amp;quot;a large red ball&amp;quot; over &amp;quot;? a red large ball&amp;quot; and the typical ordering
of temporal before spatial adverbial phrases in German.
Similarly, hard constraints might allow for the handling of an apparent counter-
example to the outermost-predication rule.&amp;quot; One natural analysis of the sentence
13. At what time did Brockway say Harrison arrived?
would involve adjunction of a predicative tree for the phrase &amp;quot;did Brockway say&amp;quot; at
the root of the tree for &amp;quot;Harrison arrived.&amp;quot; A Wh modifier tree &amp;quot;at what time&amp;quot; must
be adjoined in as well. The example question is ambiguous, of course, as to whether
it questions the time of the saying or of the arriving. In the former case, the modifier
tree presumably adjoins at the root of the predicative tree for &amp;quot;did Brockway say&amp;quot; that
it modifies. In the latter case, which is of primary interest here, it must adjoin at the
root of the tree for &amp;quot;Harrison arrived.&amp;quot; Thus, both trees would be adjoined at the same
address, and the outermost-predication rule would predict the derived sentence to be
&amp;quot;Did Brockway say at what time Harrison arrived.&amp;quot; To get around this problem, we
might specify hard ordering constraints for English that place all VVh modifier trees
after all predicative trees, which in turn come after all non-Wh modifier trees. This
would place the VVh modifier outermost as required.
Although we find this extra flexibility to be an attractive aspect of this approach,
we stay with the more stringent outermost-predication restriction in the material that
follows.
13 Other solutions are possible that do not require extended derivations or linear precedence constraints.
For instance, we might postulate an elementary tree for the verb arrived that includes a substitution
node for a fronted adverbial VVh phrase.
</bodyText>
<page confidence="0.995441">
103
</page>
<note confidence="0.544857">
Computational Linguistics Volume 20, Number 1
</note>
<subsectionHeader confidence="0.425172">
5. Compilation of TAGs to Linear Indexed Grammars
</subsectionHeader>
<bodyText confidence="0.99997">
In this section we present a technique for compiling tree-adjoining grammars into
linear indexed grammars such that the linear indexed grammar makes explicit the
extended derivations of the TAG. This compilation plays two roles. First, it provides
for a simple proof of the generative equivalence of TAGs under the standard and
extended definitions of derivation, as described at the end of this section. Second, it
can be used as the basis for a parsing algorithm that recovers the extended derivations
for strings. The design of such an algorithm is the topic of Section 6.
Linear indexed grammars (LIG) constitute a grammatical framework based, like
context-free, context-sensitive, and unrestricted rewriting systems, on rewriting strings
of nonterminal and terminal symbols. Unlike these systems, linear indexed grammars,
like the indexed grammars from which they are restricted, allow stacks of marker
symbols, called indices, to be associated with the nonterminal symbols being rewritten.
The linear version of the formalism allows the full index information from the parent
to be used to specify the index information for only one of the child constituents.
Thus, a linear indexed production can be given schematically as:
</bodyText>
<equation confidence="0.974081">
N0[..130] —&gt; Ni[/311 Ns[..0s] 1\15-4-1[05+1] • • • Nk[Oki
</equation>
<bodyText confidence="0.999362032258065">
The Ni are nonterrninals, the )3i, strings of indices. The &amp;quot;..&amp;quot; notation stands for the
remainder of the stack below the given string of indices. Note that only one element
on the right-hand side, N„ inherits the remainder of the stack from the parent. (This
schematic rule is intended to be indicative, not definitive. We ignore issues such as
the optionality of the inherited stack, how terminal symbols fit in, and so forth. Vijay-
Shanker and Weir [1990] present a complete discussion.)
Vijay-Shanker and Weir (1990) present a way of specifying any TAG as a linear
indexed grammar. The LIG version makes explicit the standard notion of derivation
being presumed. Also, the LIG version of a TAG grammar can be used for recognition
and parsing. Because the LIG formalism is based on augmented rewriting, the parsing
algorithms can be much simpler to understand and easier to modify, and no loss of
generality is incurred. For these reasons, we use the technique in this work.
The compilation process that manifests the standard definition of derivation can
be most easily understood by viewing nodes in a TAG elementary tree as having
both a top and bottom component, identically marked for nonterminal category, that
dominate (but may not immediately dominate) each other. (See Figure 6.) The rewrite
rules of the corresponding linear indexed grammar capture the immediate domination
between a bottom node and its child top nodes directly, and capture the domination
between top and bottom parts of the same node by optionally allowing rewriting from
the top of a node to an appropriate auxiliary tree, and from the foot of the auxiliary
tree back to the bottom of the node. The index stack keeps track of the nodes on which
adjunction has occurred so that the recognition to the left and the right of the foot
node will occur under identical assumption of derivation structure.
The TAG grammar is encoded as a LIG with two nonterminal symbols t and b cor-
responding to the top and bottom components, respectively, of each node. The stack
indices correspond to the individual nodes of the elementary trees of the TAG gram-
mar. Thus, there are as many stack index symbols as there are nodes in the elementary
trees of the grammar, and each such index (i.e., node) corresponds unambiguously to
a single address in a single elementary tree. (In fact, the symbols can be thought of as
pairs of an elementary tree identifier and an address within that tree, and our imple-
mentation encodes them in just that way.) The index at the top of the stack corresponds
</bodyText>
<page confidence="0.996673">
104
</page>
<note confidence="0.932671">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<figureCaption confidence="0.894295333333333">
Figure 6
Schematic structure of adjunction with top and bottom of each node separated.
Figure 7
</figureCaption>
<bodyText confidence="0.974256375">
A stack of indices [1)11173] captures the adjunction history that led to the reaching of the node
773 in the parsing process.
Parsing of an elementary tree a proceeded to node m in that tree, at which point
adjunction of the tree containing 712 was pursued by the parser. When the node 7/2 was
reached, the tree containing 773 was implicitly adjoined. Once this latter tree is completely
parsed, the remainder of the tree containing 772 can be parsed from that point, and so on.
to the node being rewritten. Thus, a LIG nonterminal with stack t[n] corresponds to
the top component of node n, and bEnin2n3] corresponds to the bottom component of
773. The indices m and 7/2 capture the history of adjunctions that are pending comple-
tion of the tree in which 713 is a node. Figure 7 depicts the interpretation of a stack of
indices.
In summary, given a tree-adjoining grammar, the following LIG rules are gener-
ated:
I. Immediate domination dominating foot: For each auxiliary tree node n that
dominates the foot node, with children 77s, . . 71n, where 71s is the
child that also dominates the foot node, include a production
</bodyText>
<equation confidence="0.649691">
b[-.771 t[m] • • • th-ilt[..m]t[m+1 • • • tinnl-
</equation>
<page confidence="0.988563">
105
</page>
<note confidence="0.202539">
Computational Linguistics Volume 20, Number 1
</note>
<listItem confidence="0.968488153846154">
2. Immediate domination not including foot: For each elementary tree node n
that does not dominate a foot node, with children m • • • 771n. include a
production
b[n] t[m] • • • tkin].
3. No adjunction: For each elementary tree node n that is not marked for
substitution or obligatory adjunction, include a production
4. Start root of adjunction: For each elementary tree node n on which the
auxiliary tree )3 with root node yr can be adjoined, include the following
production:
t[••711 t[••ipid•
5. Start foot of adjunction: For each elementary tree node n on which the
auxiliary tree /3 with foot node rif can be adjoined, include the following
production:
</listItem>
<equation confidence="0.59682">
b[-nnf] —&gt; b[••n]•
</equation>
<listItem confidence="0.965895666666667">
6. Start substitution: For each elementary tree node Ti marked for
substitution on which the initial tree a with root node yr can be
substituted, include the production
</listItem>
<equation confidence="0.542514">
*11 —&gt; tkiri•
</equation>
<bodyText confidence="0.997837642857143">
We will refer to productions generated by Rule i above as Type i productions. For
example, Type 3 productions are of the form t{. .i —&gt; b[..77]. For further information
concerning the compilation see Vijay-Shanker and Weir (1990). For present purposes, it
is sufficient to note that the method directly embeds the standard notion of derivation
in the rewriting process. To perform an adjunction, we move (by Rule 4) from the
node adjoined at to the top of the root of the auxiliary tree. At the root, additional
adjunctions might be performed. When returning from the foot of the auxiliary tree
back to the node where adjunction occurred, rewriting continues at the bottom of the
node (see Rule 5), not the top, so that no more adjunctions can be started at that node.
Thus, the dependent nature of predicative adjunction is enforced because only a single
adjunction can occur at any given node.
In order to permit extended derivations, we must allow for multiple modifier tree
adjunctions at a single node. There are two natural ways this might be accomplished,
as depicted in Figure 8.
</bodyText>
<listItem confidence="0.980799166666667">
1. Modified start foot of adjunction rule: Allow moving from the bottom of the
foot of a modifier auxiliary tree to the top (rather than the bottom) of the
node at which it adjoined (Figure 8b).
2. Modified start root of adjunction rule: Allow moving from the bottom (rather
than the top) of a node to the top of the root of a modifier auxiliary tree
(Figure 8c).
</listItem>
<bodyText confidence="0.9697225">
As can be seen from the figures, both of these methods allow recursion at a node,
unlike the original method depicted in Figure 8a. Thus multiple modifier trees are
allowed to adjoin at a single node. Note that since predicative trees fall under the
original rules, at most a single predicative tree can be adjoined at a node. The two
</bodyText>
<page confidence="0.973021">
106
</page>
<figure confidence="0.9167685">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
(a)
</figure>
<figureCaption confidence="0.991545">
Figure 8
</figureCaption>
<bodyText confidence="0.946028">
Schematic structure of possible predicative and modifier adjunctions with top and bottom of
each node separated.
methods correspond exactly to the innermost- and outermost-predication methods
discussed in Section 4.3. For the reasons described there, the latter is preferred.&apos;
In summary, independent derivation structures can be allowed for modifier aux-
iliary trees by starting the adjunction process from the bottom, rather than the top of
a node for those trees. Thus, we split Type 4 LIG productions into two subtypes for
predicative and modifier trees, respectively.
</bodyText>
<listItem confidence="0.698855428571429">
4a. Start root of predicative adjunction: For each elementary tree node n on
which the predicative auxiliary tree 3 with root node &apos;tir can be adjoined,
include the following production:
t[..77]
4b. Start root of modifier adjunction: For each elementary tree node n on which
the modifier auxiliary tree 3 with root node nr can be adjoined, include
the following production:
</listItem>
<bodyText confidence="0.773869">
t[..nnr].
Once this augmentation has been made, we no longer need to allow for adjunctions at
the root nodes of modifier auxiliary trees, as repeated adjunction is now allowed for
14 The more general definition allowing predicative trees to occur anywhere within a sequence of
modifier adjunctions would be achieved by adding both types of rules.
</bodyText>
<page confidence="0.988221">
107
</page>
<note confidence="0.738999">
Computational Linguistics Volume 20, Number 1
</note>
<bodyText confidence="0.999959388888889">
by the new rule 4b. Consequently, grammars should forbid adjunction of a modifier
tree 131 at the root of a modifier tree 02 except where /31 is intended to modify 02
directly.
This simple modification to the compilation process from TAG to LIG fully spec-
ifies the modified notion of derivation. Note that the extra criterion (5) noted in Sec-
tion 3.4 is satisfied by this definition: modifier adjunctions are inherently repeatable
and eliminable as the movement through the adjunction &amp;quot;loop&amp;quot; ends up at the same
point that it begins. The recognition algorithms for TAG based on this compilation,
however, must be adjusted to allow for the new rule types.
This compilation makes possible a simple proof of the weak-generative equiva-
lence of TAGs under the standard and extended derivations!&apos; Call the set of languages
generable by a TAG under the standard definition of derivation TAL, and under the
extended definition TAL,. Clearly, TAL, C TAL, since the standard definition can be
mimicked by making all auxiliary trees predicative. The compilation above provides
the inclusion TAL, c LIL, where LIL is the set of linear indexed languages. The final
inclusion LIL C TAL, has been shown indirectly by Vijay-Shanker (1987) using em-
bedded push-down automata and modified head grammars as intermediaries. From
these inclusions, we can conclude that TAL, = TAL,.
</bodyText>
<sectionHeader confidence="0.932075" genericHeader="method">
6. Recognition and Parsing
</sectionHeader>
<bodyText confidence="0.999984333333333">
A recognition algorithm for TAGs can be constructed based on the above translation
into corresponding LIGs as specified by Rules 1 through 6 in the previous section. The
algorithm is not a full recognition algorithm for LIGs, but rather, is tuned for exactly
the types of rules generated as output of this compilation process. In this section, we
present the recognition algorithm and modify it to work with the extended derivation
compilation.
We will use the following notations in this and later sections. The symbol P will
serve as a variable over the two LIG grammar nonterminals t and b. The substring of
the string w1 • • • wn being parsed between indices i and j will be notated as wi+1 • • • wp
which we take to be the empty string when i is greater than or equal to j. We will use
F, A, and e for sequences containing terminals and LIG nonterminals with their stack
specifications. For instance, F might be t[mit[..772]t[7/3].
The parsing algorithm can be seen as a tabular parsing method based on deduction
of items, as in Earley deduction (Pereira and Warren 1983). We will so describe it, by
presenting inference rules over items of the form
</bodyText>
<equation confidence="0.582793">
(P[ii] F • A, i, j , k , 1) .
</equation>
<bodyText confidence="0.938207272727273">
Such items play the role of the items of Earley&apos;s algorithm. Unlike the items of Earley&apos;s
algorithm, however, an item of this form does not embed a grammar rule proper; that
is, P[n] -4 FA is not necessarily a rule of the grammar. Rather, it is what we will call
a reduced rule; for reasons described below, the nonterminals in I&apos; and A as well as
the nonterminal P[n] record only the top element of each stack of indices. We will use
the notation P[n] —4 FA for the unreduced form of the rule whose reduced form is
P[n] —&gt; FA. For instance, the rule specified by the notation t[m] 07721 might be the
rule t[..nii t[../11772]. The reader can easily verify that the TAG to LIG compilation is
such that there is a one-to-one correspondence between the generated rules and their
reduced form. Consequently, this notation is well defined.
15 We are grateful to K. Vijay-Shanker for bringing this point to our attention.
</bodyText>
<page confidence="0.996452">
108
</page>
<note confidence="0.944135">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<bodyText confidence="0.999867857142857">
The dot in the items is analogous to that found in Earley and LR items as well. It
serves as a marker for how far recognition has proceeded in identifying the subcon-
stituents for this rule. The indices i, j, k, and 1 specify the portion of the string w1• • • wn
covered by the recognition of the item. The substring between i and / (i.e., wi-F-1 • • • wt)
has been recognized, perhaps with a region between j and k where the foot of the tree
below the node n has been recognized. (If the foot node is not dominated by r, we
take the values of j and k to be the dummy value &apos;—&apos;.)
</bodyText>
<subsectionHeader confidence="0.995396">
6.1 The Inference Rules
</subsectionHeader>
<bodyText confidence="0.99260775">
In this section, we specify several inference rules for parsing a LIG generated from a
TAG, which we recall in this section. One explanatory comment is in order, however,
before the rules are presented. The rules of a LIG associate with each constituent a
nonterminal and a stack of indices. It seems natural for a parsing algorithm to maintain
this association by building items that specify for each constituent the full information
of nonterminal and index stack. However, this would necessitate storing an unbounded
amount of information for each potential constituent, resulting in a parsing algorithm
that is potentially quite inefficient when nondeterminism arises during the parsing
process, and perhaps noneffective if the grammar is infinitely ambiguous. Instead, the
parse items manipulated by the inference rules that we present do not keep all of
this information for each constituent. Rather, the items keep only the single top stack
element for each constituent (in addition to the nonterminal symbol). This drastically
decreases the number of possible items and accounts for the polynomial character of
the resultant algorithm.&apos; Side conditions make up for some of the loss of information,
thereby maintaining correctness. For instance, the Type 4 Completor rule specifies a
relation between n and i that takes the place of popping an element off of the stack
associated with 77. However, the side conditions are strictly weaker than maintaining
full stack information. Consequently, the algorithm, though correct, does not maintain
the valid prefix property. See Schabes (1991) for further discussion and alternatives.
Scanning and prediction work much as in Earley&apos;s original algorithm.
</bodyText>
<listItem confidence="0.989169">
• Scanner:
</listItem>
<equation confidence="0.9564755">
(b[77] F • aA, i,j,k,l)
(b[n] —&gt; Fa • A, i, j,k,1 + 1)
</equation>
<bodyText confidence="0.9929226">
Note that the only rules that need be considered are those where the
parent is a bottom node, as terminal symbols occur on the right-hand
side only of Type 1 or 2 productions. Otherwise, the rule is exactly as
that for Earley&apos;s algorithm except that the extra foot indices (j and k) are
carried along.
</bodyText>
<listItem confidence="0.991955">
• Predictor:
</listItem>
<equation confidence="0.9817515">
(P[n] --+ F • P&apos;[71] i, j,k,l) [711 e
(pi [771 • 0,1, /)
</equation>
<bodyText confidence="0.95688">
This rule serves to form predictions for any type production in the
grammar, as the variables P and P&apos; range over the values t and b. In the
16 Vijay-Shanker and Weir (1990) first proposed the recording of only the top stack element in order to
achieve efficient parsing. The algorithm they presented is a bottom-up general LIG parsing algorithm.
Schabes (1991) sketches a proof of an 0(n6) bound for an Earley-style algorithm for TAG parsing that
is more closely related to the algorithm proposed here.
</bodyText>
<equation confidence="0.758683">
a = wi+1
</equation>
<page confidence="0.950043">
109
</page>
<note confidence="0.715869">
Computational Linguistics Volume 20, Number 1
</note>
<bodyText confidence="0.999599666666667">
predicted item, the foot is not dominated by the (empty) recognized
input, so that the dummy value &apos;—&apos; is used for the foot indices. Note that
the predicted item records the reduced form of an unreduced rule
of the grammar.
Completion of items (moving of the dot from left to right over a nonterminal)
breaks up into several cases, depending on which production type is being completed.
This is because the addition of the extra indices and the separate interpretations for
top and bottom productions require differing index manipulations to be performed.
We will list the various steps, organized by what type of production they participate
in the completion of. ,
Productions that specify immediate domination (from Rules 1 and 2) are completed
whenever the top of the child node is fully recognized.
</bodyText>
<listItem confidence="0.968186">
• Type 1 and 2 Cornpletor:
</listItem>
<equation confidence="0.95007">
(b[771] —&gt; I&apos; • t[MA,m,j&apos;, i) (t[q] 0 • , j , k, I)
—&gt; rt[n] • A,m,jU j&apos; ,k U k&apos; , I)
</equation>
<bodyText confidence="0.997925769230769">
Here, t[i1] has been fully recognized as the substring between i and 1. The
item expecting t[n] can be completed. One of the two antecedent items
might also dominate the foot node of the tree to which n and Th belong,
and would therefore have indices for the foot substring. The operations
j U j&apos; and k U k&apos; are used to specify whichever of j or j&apos; (and respectively
for k or k&apos;) contain foot substring indices. The formal definition of U is as
follows:
j, if j&apos; = —
j U =
if j&apos; = j
undefined otherwise
The remaining rules (3 through 6) are each completed by a particular completion
instance.
</bodyText>
<listItem confidence="0.873223">
• Type 3 Cornpletor:
</listItem>
<equation confidence="0.9502035">
—&gt; • b[n], — , i) (b[n] 0 • , j, k,l)
(t[i7] b[q] • ,i, j,k,l)
</equation>
<bodyText confidence="0.99944575">
This rule is used to complete a prediction that no (predicative)
adjunction occurs at node n. Once the part of the string dominated by
b[ni has been found, as evidenced by the second antecedent item, the
prediction of no adjunction can be completed.
</bodyText>
<figure confidence="0.674589666666667">
• Type 4 Cornpletor:
(tH —&gt; • *id , — — , i) t[..771 —&gt; t[..Trrirl
(t[Thl 0 • , i,j, k, 1)
(b[71] A • , j,p, q,k)
(t[77] t[q7] • ,i,p,q,1)
(t[n]
</figure>
<page confidence="0.987687">
110
</page>
<note confidence="0.930925">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<bodyText confidence="0.9997355">
Here, an adjunction has been predicted at 7/, and the adjoined derived
tree (between gni and b[n]) and the derived material that ri itself
dominates (below b[77]) have both been completed. Thus t[n] is
completely recognized. Note that the side condition (the unreduced form
of the reduced rule in the first antecedent item) is placed merely to
guarantee that rir is the root node of an adjoinable auxiliary tree.
</bodyText>
<figure confidence="0.900274333333333">
• Type 5 Cornpletor:
(MT] -&gt; • b[n] , i, -, -, i) (b[] -÷ • , j, k, 1) b{..nrifi b[..]
(b[T] b[n] • , 1, 1)
</figure>
<bodyText confidence="0.737636">
When adjunction has been performed and recognition up to the foot
node rr has been performed, it is necessary to recognize all the material
under the foot node. When that is done, the foot node prediction can be
completed. Note that it must be possible to have adjoined the auxiliary
tree at node n as specified in the production in the side condition.
</bodyText>
<listItem confidence="0.492155">
• Type 6 Completor:
</listItem>
<equation confidence="0.497384666666667">
(t[n] -4 • t[yd, i) (*id • , -M
t[ii] t[m]
(t[n] tind • , /)
</equation>
<bodyText confidence="0.9934275625">
Completion of the material below the root node nr of an initial tree
allows for the completion of the node at which substitution occurred.
The recognition process for a string w1 • • • wn starts with some items that serve as
axioms for these inference rules. For each rule t[m] F where i is the root node of
an initial tree whose node is labeled with the start nonterminal, the item (t[ii] • F,
0, 0) is an axiom. If from these axioms an item of the form (t[i] r • , o, n) can
be proved according to the rules of inference above, the string is accepted; otherwise
it is rejected.
Alternatively, the axioms can be stated as if there were extra rules S tird for
each ys a start-nonterminal-labeled root node of an initial tree. In this case, the axioms
are items of the form • t[ils], 0, 0) and the string is accepted upon proving
(S t[ris] • , 0,-,-, n). In this case, an extra prediction and completion rule is needed
just for these rules, since the normal rules do not allow S on the left-hand side. This
point is taken up further in Section 6.4.
Generation of items can be cached in the standard way for inference-based parsing
algorithms (Shieber 1992); this leads to a tabular or chart-based parsing algorithm.
</bodyText>
<subsectionHeader confidence="0.999713">
6.2 The Algorithm Invariant
</subsectionHeader>
<bodyText confidence="0.996396714285714">
The algorithm maintains an invariant that holds of all items added to the chart. We
will describe the invariant using some additional notational conventions. Recall that
P[n] F is the LIG production in the grammar whose reduced form is P[n] F. The
notation 11-y] where -y is a sequence of stack symbols (i.e., nodes), specifies the sequence
F with -y replacing the occurrence of .. in the stack specifications. For example, if r
is the sequence t[ni I t[..r/2] t[m], then F{-y} = t[m]t[-y772] t[m]. A single LIG derivation step
will be notated with and its reflexive transitive closure with z.
</bodyText>
<page confidence="0.996738">
111
</page>
<subsectionHeader confidence="0.326792">
Computational Linguistics Volume 20, Number 1
</subsectionHeader>
<bodyText confidence="0.934625">
The invariant specifies that (P[77] --+ F • A, i,j,k,l) is in the chart only if17
</bodyText>
<listItem confidence="0.957807666666667">
1. If node n dominates the foot node Tif of the tree to which it belongs, then
there exists a string of stack symbols (i.e., nodes) 7 such that
(a) P[n] -4 FA is a LIG rule in the grammar, where r is the
unreduced form of F.
(b) r[777] wi+1. • •wib[77/Awk+1.• •WI
(c) b[ytif] • •wk
2. If node 71 does not dominate the foot node Tif of the tree to which it
belongs or there is no foot node in the tree, then
(a) PH —&gt; FA is a LIG rule in the grammar, where r is the
unreduced form of F.
(b) -f wi-Fi • • • wi
(c) j and k are not bound.
</listItem>
<bodyText confidence="0.958949">
According to this invariant, for a node ris that is the root of an initial tree, the item
(t[m] r • , 0, r0 is in the chart only if OW F w1 • • • wn. Thus, soundness of
the algorithm as a recognizer follows.
</bodyText>
<subsectionHeader confidence="0.997664">
6.3 Modifications for Extended Derivations
</subsectionHeader>
<bodyText confidence="0.998458666666667">
Extending the algorithm to allow for the new types of production (specifically, as
derived by Rule 4b) requires adding a completion rule for Type 4b productions. For
the new type of production, a completion rule of the following form is required:
</bodyText>
<figure confidence="0.8628472">
• Type 4h Cornpletor:
(b[n] • t[m], , i)
(t[77,1 e • , j, k, 1)
(b[n] —* • , j, p, q ,k)
(b[q] —&gt; t[Ir] • , , p, q , 1)
</figure>
<bodyText confidence="0.981086375">
In addition to being able to complete Type 4b items, we must also be able to
complete other items using completed Type 4b items. This is an issue in particular for
completor rules that might move their dot over a b[n] constituent; in particular, the
Type 3 and 5 Completors. However, these rules have been stated so that the antecedent
item with right-hand side b[n] already matches Type 4b items. Furthermore, the general
statement, including index manipulation is still appropriate in the context of Type 4b
productions. Thus, no further changes to the recognition inference rules are needed
for this purpose.
17 The invariant is not stated as a biconditional because this would require strengthening of the
antecedent condition. The natural strengthening, following the standard for Earley&apos;s algorithm, would
be to add a requirement that the item be consistent with left context, as
(d) zyi • • • wiP[11/]
but this is too strong. This condition implies that the algorithm possesses the valid prefix property,
which it does not. The exact statement of the invariant condition that would allow for exact
specifications of the item semantics is the topic of ongoing research. However, the current specification
is sufficient for proving soundness of the algorithm.
</bodyText>
<page confidence="0.997146">
112
</page>
<note confidence="0.939774">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<bodyText confidence="0.997980555555556">
However, a bit of care must be taken in the interpretation of the Type 1/2 Com-
pletor. Type 4b items that require completion bear a superficial resemblance to Type 1
and 2 items, in that both have a constituent of the form tH after the dot. In Type 4b
items, the constituent is t[nr], in Type 4a items t[i]. But it is crucial that the Type 1/2
Completor not be used to complete Type 4b items. A simple distinguishing character-
istic is that in Type 1 and 2 items to be completed, the node n after the dot is never a
root node (as it is immediately dominated by ih), whereas in Type 4b items, the node
rir after the dot is always a root node (of a modifier tree). Simple side conditions can
distinguish the cases.
</bodyText>
<figureCaption confidence="0.576841">
Figure 9 contains the final versions of the inference rules for recognition of LIGs
corresponding to extended TAG derivations.
</figureCaption>
<subsectionHeader confidence="0.999414">
6.4 Maintaining Derivation Structures
</subsectionHeader>
<bodyText confidence="0.999976921052631">
One of the intended applications for extended derivation TAG parsing is the parsing
of synchronous TAGs. Especially important in this application is the ability to generate
the derivation trees while parsing proceeds.
A synchronous TAG is composed of two base TAGs (which we will call the source
TAG and the target TAG) whose elementary trees have been paired one-to-one. A syn-
chronous TAG whose source TAG is a grammar for a fragment of English and whose
target TAG is a grammar for a logical form language may be used to generate logical
forms for each sentence of English that the source grammar admits (Shieber and Sch-
abes 1990). Similarly, with source and target swapped, the synchronized grammar may
be used to generate English sentences corresponding to logical forms (Shieber and Sch-
abes 1991). If the source and target grammars specify fragments of natural languages,
an automatic translation system is specified (Abeille, Schabes, and Joshi 1990).
Abstractly viewed, the processing of a synchronous grammar proceeds by parsing
an input string according to the source grammar, thereby generating a derivation
tree for the string; mapping the derivation tree into a derivation tree for the target
grammar; and generating a derived tree (hence, derived string) according to the target
grammar.
One frequent worry about synchronous TAGs as used in their semantic interpreta-
tion mode is whether it is possible to perform incremental interpretation. The abstract
view of processing just presented seems to require that a full derivation tree be de-
veloped before interpretation into the logical form language can proceed. Incremental
interpretation, on the other hand, would allow partial interpretation results to guide
the parsing process on-line, thereby decreasing the nondeterminism in the parsing
process. Whether incremental interpretation is possible depends precisely on the ex-
tent to which the three abstract phases of synchronous TAG processing can in fact be
interleaved. In previous work we left this issue open. In this section, we allay these
worries by showing how the extended TAG parser just presented can build derivation
trees incrementally as parsing proceeds. Once this has been demonstrated, it should
be obvious that these derivation trees could be transferred to target derivation trees
during the parsing process and immediately generated from. Thus, incremental inter-
pretation is demonstrated to be possible in the synchronous TAG framework. In fact,
the technique presented in this section has allowed for the first implementation of syn-
chronous TAG processing, by Onnig Dombalagian. This implementation was directly
based on the inference-based TAG parser mentioned in Section 6.5 and presented in
full elsewhere (Schabes and Shieber 1992).
We associate with each item a set of operations that have been implicitly carried
out by the parser in recognizing the substring covered by the item. An operation can
be characterized by a derivation tree and a tree address at which the derivation tree is
</bodyText>
<page confidence="0.99652">
113
</page>
<figure confidence="0.881990071428571">
Computational Linguistics Volume 20, Number 1
• Scanner:
(b[n] F • aA, i,j, k, 1)
(b[n] ra • A, ,j , k, 1 + 1) a = w1+1
• Predictor: (P[771 F • P&apos; [OA, j , k, 1)
• 6,1, , 1)
• Type 1 and 2 Completor: 13&apos; [Ti&apos;l —&gt; e
(b[m] —&gt; F • On] , m, , , i) (t[n] —&gt; e • , i,j, k, 1) 97 not a root node
rt[n] • A, m, j U j&apos; , k u k&apos; , 1)
• Type 3 Cornpletor:
(t[n] --&gt; • b[], i, , — , i) (b[n] —&gt; 8 • , i,j, k, 1)
(t[n] --&gt; b[n] • , j , k, 1)
• Type 4a Completor:
• t[77„1, i)
(t[nr] e • , i,j, k, 1)
(b[n] —&gt; A • , j,p, q, k)
(t[n] --&gt; 1[7]r] • , i ,p, q, 1)
• Type 4b Completor:
(b[n] —&gt; • t[nr], , — , i)
(t[r] —&gt; 0 • , j , k, 1)
(b[n] —&gt; A • , j, p, q, k)
(b[n] —&gt; t[nr] • , p, q , 1)
• Type 5 Completor:
(b[nfl ---&gt; • b[n], , — , — , i) (b[77] —&gt; • , j , k, 1)
(b[T] —&gt; b[n] • , i , 1 , I)
• Type 6 Completor:
(t[n] —&gt; • t[nr], — , — , i) (t[lir] —+0 • ,i, — , 1)
(t[n] —&gt; t[nr] •
</figure>
<figureCaption confidence="0.664618">
Figure 9
</figureCaption>
<figure confidence="0.8599596">
Inference rules for extended derivation TAG recognition.
(t[til
--&gt; t[..mir]
b[..771
t{171 t [77r1
</figure>
<bodyText confidence="0.917659555555556">
to be placed; it corresponds roughly to a branch of a derivation tree. Prediction items
have the empty set of operations. Type 4 and 6 completion steps build new elements
of the sets as they correspond to actually carrying out adjunction and substitution
operations, respectively. Other completion steps merely pool the operations from their
constituent parts.
In describing the building of derivation trees, we will use normal set notation for
the sets of derivation trees. We will assume that for each node ?I, there are functions
tree(n) and addr(q) that specify, respectively, the initial tree that n occurs in and its
address in that tree. Finally, we will use a constructor function for derivation trees
</bodyText>
<page confidence="0.994738">
114
</page>
<note confidence="0.848266">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<bodyText confidence="0.988159444444444">
deriv(-y, S), where 1, specifies an elementary tree and S specifies a set of operations on
it. An operation is built with op(t,D) where t is a tree address and D is a derivation
tree to be operated at that address.
Figure 10 lists the previously presented recognition rules augmented to build
derivation structures as the final component of each item. The axioms for this in-
ference system are items of the form (S -4 • t[ns], 0, -, -, 0, {}), where we assume as in
Section 6.1 that there are extra rules S --&gt; t[rid for each ii, a start-nonterminal-labeled
root node of an initial tree. We require an extra rule for prediction and completion to
handle this new type of rule. The predictor rule is the obvious analog:
</bodyText>
<listItem confidence="0.997589">
• Start Rule Predictor:
</listItem>
<equation confidence="0.998787">
(S --&gt; F • P&apos;fril A, i, j,k,l, S)
(11711 ---* • e, 1,-,-,!, -, 1, {})
</equation>
<bodyText confidence="0.9972402">
In fact, the existing predictor rule could have been easily generalized to handle this
case.
The completor for these start rules is the obvious analog to a Type 6 completor,
except in the handling of the derivation. It delivers, instead of a set of derivation
operations, a single derivation tree.
</bodyText>
<listItem confidence="0.997445">
• Start Rule Cornpletor:
</listItem>
<equation confidence="0.992867">
(s —, • *id, i, —,—, i, {}) (t[ys] -- 6 • , i, --, -, /, S)
(S -4 t[m] • , i,-,-, 1, deriv(tree(ris), 5))
</equation>
<bodyText confidence="0.999788">
The string is accepted upon proving (S .- t[ns] • , 0, -, -, n, D), where D is the
derivation developed during the parse.
</bodyText>
<subsectionHeader confidence="0.999404">
6.5 Complexity Considerations
</subsectionHeader>
<bodyText confidence="0.9999533">
The inference system of Section 6.3 essentially specifies a parsing algorithm with com-
plexity of 0(n6) in the length of the string. Adding explicit derivation structures to the
items, as in the inference system of the previous section, eliminates the polynomial
character of the algorithm in that there may be an unbounded number of derivations
corresponding to any given item of the original sort. Even for finitely ambiguous
grammars, the number of derivations may be exponential. Nonetheless, this fact does
not vitiate the usefulness of the second algorithm, which maintains derivations ex-
plicitly. The point of this augmentation is to allow for incremental interpretation—for
interleaved processing of a post-syntactic sort—so as to guide the parsing process in
making choices on-line. By using the extra derivation information, the parser should
be able to eliminate certain nondeterministic paths of computation; otherwise, there
is no reason to do the interpretation incrementally. But this determinization of choice
presumably decreases the complexity. Thus, the extra information is designed for use
in cases where the full search space is not intended to be explored.
Of course, a polynomial shared-forest representation of the exponential number
of derivations could have been maintained (by maintaining back pointers among the
items in the standard fashion). For performing incremental interpretation for the pur-
pose of determinization of parsing, however, the non-shared representation is suffi-
cient, and preferable on grounds of ease of implementation and expository conve-
nience.
</bodyText>
<page confidence="0.992735">
115
</page>
<figure confidence="0.978656689655172">
Computational Linguistics Volume 20, Number 1
• Scanner:
(b[77] F • a A, j , k, 1, S)
(b[n] -&gt; Fa • A, j , k, 1 + 1, s) a = w 1+1
• Predictor: (P[n] • P&apos; tri/jA, i , j , k, I, S)
(P1[771 • e, 1, {})
PTY1 e
• Type 1 and 2 Completor: n not a root node
(b[in] -&gt; r • t[nl n1,1 , , si) (t[n] e • , I, S2)
(b[m] -&gt; Ft[n] • A,m,j ,ku k&apos; , I, U S2)
• Type 3 Completor:
(t[n] -4 • b[ri], -, - , {}) (b[ n] e • , i,j,k, 1, S)
(t[n] -4 b[n] • , j , k, 1, S)
• Type 4a Completor:
(t[ii] • Om}, 1, -, -, 1, {})
(t[77,1 e • , j,k,l, Si)
(b[n] -4 A • , j, p, q,k, S2)
- t[i7] • , p, q , 1, fop(addr(n), deriv(tree(nr), SIM U S2)
• Type 4b Completor: b[..771 t[. .rin]
(NM — • t[ibl, —, — {})
(*b.] • , i,j,k, 1, S1)
(b[n] -+ A • ,j, p, q,k, S2)
(t[n] t[nr] • , i,p, q , 1, fop(addr(n), deriv(tree(nr), Si))} U S2)
• Type 5 Cornpletor:
(b[n11 • b[n] , -, - , {}) (b[n] e • , i,j,k, I, S) b[..rmf]
(Mil/1 b[n] • ,i, i, , 1, 1, S)
• Type 6 Completor: *I] -&gt; t[77,1
(t[n] -&gt; • tind, - 0) (t[nr] e • , 1, s)
(t[n] tkirj • , 1, fop(addr(n), deriv(tree(rir), S))})
</figure>
<figureCaption confidence="0.702993">
Figure 10
Inference rules for extended derivation TAG parsing.
</figureCaption>
<bodyText confidence="0.999894333333333">
As a proof of concept, the parsing algorithm just described was implemented in
Prolog on top of a simple, general-purpose, agenda-based inference engine. Encod-
ings of explicit inference rules are essentially interpreted by the inference engine. The
Prolog database is used as the chart; items not already subsumed by a previously gen-
erated item are asserted to the database as the parser runs. An agenda of potential new
items is maintained. Items are added to the agenda as inference rules are triggered by
items added to the chart. Because the inference rules are stated explicitly, the relation
between the abstract inference rules described in this paper and the implementation
is extremely transparent. As a meta-interpreter, the prototype is not particularly effi-
</bodyText>
<equation confidence="0.676207">
t{.7/1
(tEril
</equation>
<page confidence="0.998046">
116
</page>
<note confidence="0.951103">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<bodyText confidence="0.991659">
cient. (In particular, the implementation does not achieve the theoretical 0(n6) bound
on complexity, because of a lack of appropriate indexing.) Code for the prototype
implementation is available for distribution electronically from the authors.
</bodyText>
<sectionHeader confidence="0.513416" genericHeader="method">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.9999162">
The precise formulation of derivation for tree-adjoining grammars has important rami-
fications for a wide variety of uses of the formalism, from syntactic analysis to semantic
interpretation and statistical language modeling. We have argued that the definition of
tree-adjoining derivation must be reformulated in order to take greatest advantage of
the decoupling of derivation tree and derived tree by manifesting the proper linguistic
dependencies in derivations. The particular proposal is both precisely characterizable
through a definition of TAG derivations as equivalence classes of ordered derivation
trees and computationally operational by virtue of a compilation to linear indexed
grammars together with an efficient algorithm for recognition and parsing according
to the compiled grammar.
</bodyText>
<sectionHeader confidence="0.820529" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.91828832">
Order of authors is not intended as an
indication of precedence of authorship.
Much of the work reported in this paper
was performed while the first author was at
the Department of Computer and
Information Science, University of
Pennsylvania, Philadelphia, PA. The first
author was supported in part by DARPA
Grant N0014-90-31863, ARO Grant
DAAL03-89-C-0031, and NSF Grant
IRI-90-16592. The second author was
supported in part by Presidential Young
Investigator award IRI-91-57996 from the
National Science Foundation and a
matching grant from Xerox Corporation.
The authors wish to thank Aravind Joshi for
his support of the research, and Aravind
Joshi, Judith Klavans, Anthony Kroch,
Shalom Lappin, Kathy McCoy, Fernando
Pereira, James Pustejovsky, and
K. Vijay-Shanker for their helpful
discussions of the issues involved. We are
indebted to David Yarowsky for aid in the
design of the experiment mentioned in
footnote 9 and for its execution.
</bodyText>
<sectionHeader confidence="0.960634" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.998953274509804">
Abeille, Anne; Schabes, Yves; and Joshi,
Aravind K. (1990). &amp;quot;Using lexicalized tree
adjoining grammars for machine
translation.&amp;quot; In Proceedings, 13th
International Conference on Computational
Linguistics, Volume 3,1-6, Helsinki,
Finland.
Gazdar, Gerald; Klein, Ewan; Pullum,
Geoffrey K.; and Sag, Ivan A. (1985).
Generalized Phrase Structure Grammar.
Blackwell.
Joshi, A. K.; Kosaraju, S. R.; and Yamada,
H. M. (1972a). &amp;quot;String adjunct grammars:
I. Local and distributed adjunction.&amp;quot;
Information and Control, 21(2), 93-116.
Joshi, A. K.; Kosaraju, S. R.; and Yamada,
H. M. (1972b). &amp;quot;String adjunct grammars:
H. Equational representation, null
symbols, and linguistic relevance.&amp;quot;
Information and Control, 21(3), 235-260.
Joshi, Aravind K.; Levy, L. S.; and
Takahashi, M. (1975). &amp;quot;Tree adjunct
grammars.&amp;quot; Journal of Computer and System
Sciences, 10(1), 136-163.
Kroch, Anthony S. (1989). &amp;quot;Asymmetries in
long distance extraction in a TAG
grammar.&amp;quot; In Alternative Conceptions of
Phrase Structure, edited by M. Baltin and
A. Kroch, 66-98. University of Chicago
Press.
Kroch, Anthony S., and Joshi, Aravind K.
(1985). &amp;quot;The linguistic relevance of tree
adjoining grammar.&amp;quot; Technical Report
MS-CIS-85-18, Department of Computer
and Information Science, University of
Pennsylvania, Philadelphia, PA.
Pereira, Fernando C. N., and Warren, David
H. D. (1983). &amp;quot;Parsing as deduction.&amp;quot; In
Proceedings, 21st Annual Meeting of the
Association for Computational Linguistics,
137-144. Cambridge, MA.
Quirk, Randolph; Greenbaum, Sidney;
Leech, Geoffrey; and Svartvik, Jan (1985).
A Comprehensive Grammar of the English
Language. Longman.
Resnik, Philip (1992). &amp;quot;Probabilistic
tree-adjoining grammar as a framework
for statistical natural language
processing.&amp;quot; In Proceedings, 14th
International Conference on Computational
Linguistics, 418-424. Nantes, France.
</reference>
<page confidence="0.99387">
117
</page>
<note confidence="0.785857">
Computational Linguistics Volume 20, Number 1
</note>
<reference confidence="0.981521365079365">
Schabes, Yves (1991). &amp;quot;The valid prefix
property and left to right parsing of
tree-adjoining grammar.&amp;quot; In Proceedings,
Second International Workshop on Parsing
Technologies, 21-30. Cancun, Mexico.
Schabes, Yves (1992). &amp;quot;Stochastic lexicalized
tree-adjoining grammars.&amp;quot; In Proceedings,
14th International Conference on
Computational Linguistics, 426-432. Nantes,
France.
Schabes, Yves, and Shieber, Stuart M. (1992).
&amp;quot;An alternative conception of
tree-adjoining derivation.&amp;quot; Technical
Report 08-92, Harvard University,
Cambridge, MA.
Schabes, Yves, and Waters, Richard C.
(1993a). &amp;quot;Lexicalized context-free
grammars.&amp;quot; In Proceedings, 31st Annual
Meeting of the Association for Computational
Linguistics, 121-129. Columbus, OH.
Schabes, Yves, and Waters, Richard C.
(1993b). &amp;quot;Stochastic lexicalized
context-free grammars.&amp;quot; In Proceedings,
Third International Workshop on Parsing
Technologies, 257-266. Tilburg, The
Netherlands and Durbuy, Belgium.
Shieber, Stuart M. (1992). Constraint-Based
Grammar Formalisms. MIT Press.
Shieber, Stuart M. (in press). &amp;quot;Restricting the
weak-generative capacity of synchronous
tree-adjoining grammars.&amp;quot; Computational
Intelligence.
Shieber, Stuart M., and Schabes, Yves (1990).
&amp;quot;Synchronous tree-adjoining grammars.&amp;quot;
In Proceedings, 13th International Conference
on Computational Linguistics, Volume 3,
253-258. Helsinki, Finland.
Shieber, Stuart M., and Schabes, Yves (1991).
&amp;quot;Generation and synchronous tree
adjoining grammars.&amp;quot; Computational
Intelligence, 4(7), 220-228.
Vijay-Shanker, K. (1987). A Study of Tree
Adjoining Grammars. Doctoral dissertation,
Department of Computer and
Information Science, University of
Pennsylvania, Philadelphia, PA.
Vijay-Shanker, K., and Joshi, Aravind K.
(1985). &amp;quot;Some computational properties of
tree adjoining grammars.&amp;quot; In Proceedings,
23rd Annual Meeting of the Association for
Computational Linguistics, 82-93. Chicago,
IL.
Vijay-Shanker, K., and Joshi, Aravind K.
(1988). &amp;quot;Feature structure based tree
adjoining grammars.&amp;quot; In Proceedings, 12th
International Conference on Computational
Linguistics, 714-719. Budapest, Hungary.
Vijay-Shanker, K., and Weir, David J. (1990).
&amp;quot;Polynomial parsing of extensions of
context-free grammars.&amp;quot; In Current Issues
in Parsing Technologies, edited by Masaru
Tomita, 191-206. Kluwer Academic
Publishers.
</reference>
<page confidence="0.998159">
118
</page>
<note confidence="0.974022">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<sectionHeader confidence="0.803458" genericHeader="method">
Appendix A: Proof of Redundancy of Adjacent Sibling Swapping
</sectionHeader>
<subsectionHeader confidence="0.731753">
A.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.968725833333333">
A.1.1 Tree Addresses. We define tree addresses (variables over which are convention-
ally notated p,q,...,t,u,v and their subscripted and primed variants) as the finite,
possibly empty, sequences of positive integers (conventionally i, j ,k), with _ • _ as the
sequence concatenation operator. We uniformly abuse notation by conflating the dis-
tinction between singleton sequences and their one element.
We use the notation p q to notate that tree address p is a proper prefix of q,
and p --&lt; q for improper prefix. When p q, we write q – p for the (possibly empty)
sequence obtained from q by removing p from the front, e.g., 1 2 • 3 • 4 – 1 • 2 = 3 • 4.
A.1.2 Trees. We will take trees (conventionally A,B,E,T; also a, ,(3, &apos;y in the prior text)
to be finite partial functions from tree addresses to symbols, such that the functions
are
Prefix closed: For any tree T, if T(p • i) is defined then T(p) is defined.
Left closed: For any tree T, if T(p • i) is defined and i &gt; 1 then T(p • (i – 1)) is
defined.
We will refer to the domain of a tree T, the tree addresses for which T is defined,
as the nodes of T. A node p of T is a frontier node if T(p • i) is undefined for all i. A node
of T is an interior node if it is not a frontier node. We say that a node p of T is labeled
with a symbol s if T(p) = s.
</bodyText>
<subsectionHeader confidence="0.984086">
A.2 Tree-Adjoining Grammars and Derivations
</subsectionHeader>
<bodyText confidence="0.95656775">
A.2.1 Tree-Adjoining Grammars. In the following definitions, we restrict attention to
tree-adjoining grammars in which adjunction is the only operation; substitution is not
allowed. The definitions are, however, easily augmented to include substitution. We
define a tree-adjoining grammar to be given by a quintuple N, I, A, S) where
</bodyText>
<listItem confidence="0.9977586">
• E is a finite set of terminal symbols.
• N is a finite set of nonterminal symbols disjoint from E.
• (V = E U N is the vocabulary of the grammar.)
• S is a distinguished nonterminal symbol, the start symbol.
• I is a finite set of trees, the initial trees, where
</listItem>
<bodyText confidence="0.9976555">
—interior nodes are labeled by nonterminal symbols, and
—frontier nodes are labeled by terminal symbols or the special
symbol E. (We require that E V, as € intuitively specifies the
empty string.)
</bodyText>
<listItem confidence="0.937037">
• A is a finite set of trees, the auxiliary trees, where
</listItem>
<bodyText confidence="0.974215">
—interior nodes are labeled by nonterminal symbols, and
—frontier nodes are labeled by terminal symbols or E, except for
one node, called the foot node, which is labeled with a
nonterminal symbol.
</bodyText>
<listItem confidence="0.8928915">
• (S = I U A is the set of elementary trees of the grammar.)
By convention, the address of the foot node of a tree A is notated fA•
</listItem>
<page confidence="0.994946">
119
</page>
<note confidence="0.845693">
Computational Linguistics Volume 20, Number 1
</note>
<footnote confidence="0.8804195">
A.2.2 Adjunction. The adjunction of an auxiliary tree A at address tin tree E notated
E[A/t1 is defined to be the smallest (least defined) tree T such that
</footnote>
<equation confidence="0.979473666666667">
E(r) if t r
{ (1)
T(r) = A(u) if r = t • u and fA u (2)
E(t - u) if r = t • fA • u (3)
These cases are disjoint except at addresses t and t • A. We have
T(t) = E(t)
</equation>
<bodyText confidence="0.830019">
by clause (1), and
</bodyText>
<figure confidence="0.3216772">
T(t) A(t)
by clause (2). Similarly, we have
T(t • fA)=A(fA)
by clause (2) and
T(t • fA) = E(t)
</figure>
<bodyText confidence="0.520561">
by clause (3). So for an adjunction to be well defined, it must be the case that
</bodyText>
<equation confidence="0.989913">
E(t) = A(t) = A(fA)
</equation>
<bodyText confidence="0.99985575">
that is, the node at which adjunction occurs must have the same label as the root and
foot of the auxiliary tree adjoined. This is, of course, standard in definitions of TAG.
Alternatively, this constraint can be added as a stipulation and the definition mod-
ified as follows:
</bodyText>
<equation confidence="0.994619">
E(r) if t r
T(r) = A(u) if r = t • u and fA u
E(t • u) if r = t • fA • u
</equation>
<bodyText confidence="0.987112409090909">
We will use this latter definition below.
A.2.3 Ordered Derivation Trees. Ordered derivation trees are ordered trees composed
of nodes, conventionally notated as n, possibly in its subscripted and primed variants.
(For ordered derivation trees, we will be less formal as to their mathematical structure.
In particular, the formalization of the previous section need not apply; the definitions
that follow define all of the structure that we will need.) The parent of a node 7/
in a derivation tree will be written parent(n), and the tree in £ that the node marks
adjunction of will be notated tree(n). The tree tree(n) is to be adjoined into its parent
tree(parent(n)) at an address specified on the arc in the tree linking the two; this address
is notated addr(n). (Of course, the root node has no parent or address; the parent and
addr functions are partial.)
An ordered derivation tree is well formed if for each arc in the derivation tree
from n to parent(n) labeled with addr(n), the tree tree(n) is an auxiliary tree that can be
adjoined at the node addr(n) in tree(parent(n)).
We repeat from Section 4.1 the definition of the function D from derivation trees
to the derived trees they specify, in the notation of this appendix:
D(D) -= tree(n) if D is a trivial tree of one node n
treeW[D(Di)/ti,D(D2)/t2, • • • ,D(Dk)/tki
if D is a tree with root node n
and with k child subtrees D1,.. • , Dk
1
whose arcs are labeled with addresses t1 , • • • , tk•
</bodyText>
<page confidence="0.882043">
120
</page>
<subsectionHeader confidence="0.399954">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</subsectionHeader>
<bodyText confidence="0.995820833333333">
As in Section 4.1, E[Ai/ti, , Ak/tk] specifies the simultaneous adjunction of trees
A1 through Ak at t1 through tk, respectively, in E. It is defined as the iterative adjunction
of the A, in order at their respective addresses, with appropriate updating of the tree
addresses of later adjunctions to reflect the effect of earlier adjunctions. In particular,
the following inductive definition suffices; the base case holds for the adjunction of
zero auxiliary trees.
</bodyText>
<equation confidence="0.991877666666667">
E[] =E
E[Ai /ti, A2/ t2, . . . , Ak/tk]
= (E[A1 / tip[A2/update(t2, A1, t1), . . , Ak/update(tk, A1, t1)]
</equation>
<bodyText confidence="0.867925">
where
</bodyText>
<equation confidence="0.996565666666667">
Is if t s
update(s, A, t) =
t • fA • (s — t) if t s
</equation>
<bodyText confidence="0.997730666666667">
In the following section, we leave out parentheses in specifying sequential ad-
junctions such as (E[A1/ t1D[A2/ t2] under a convention of left associativity of the [_/_]
operator.
</bodyText>
<subsectionHeader confidence="0.633767">
A.3 Effect of Sibling Swaps
</subsectionHeader>
<bodyText confidence="0.999531333333333">
In this section, we show that the derived tree specified by a given ordered deriva-
tion tree is unchanged if adjacent siblings whose arcs are labeled with different tree
addresses are swapped. This will be shown as the following proposition.
</bodyText>
<sectionHeader confidence="0.459663" genericHeader="method">
Proposition
</sectionHeader>
<bodyText confidence="0.729518428571429">
If t t&apos; then E[. ,A/t,B/P, .] = . ,B/t&apos; ,A/t, .1.
We start with a lemma, the case for only two adjunctions.
Lemma
If t t&apos; then E[A/t, B/C] = E[B /t&apos; , A/ t].
Proof
There are three major cases, depending on the relationship of t and t&apos;:
Case t -&lt; t&apos;: Let s = t&apos; t. Then
</bodyText>
<figure confidence="0.916888371428571">
E[A/ t,B / t&apos;](r) E[A/ t][B / update(t&apos; , A, t)](r)
= E[A/t][B / t • fA • s] (r)
E[A / t](r) if t • fA • s r
B(u) if r = t • fA • s • u and fB u
E[A/ t](t • fA • s • u) if r = t • fA • s • fB • u
E(r) if t • fA • s r and t r
A(v) if t fA • s r and r = t v
E(t v) ift•fA•sZrandr=t•fA.v
B(u) if r = t • fA • s • u and fB
E(t • s u) if r = t • fA • s fB • u
E(r) if t
A(v) if r = t • v
E(t v) if s v and r t • fA • v
B(u) if r = t • fA • s • u and fB u
E(t • s • u) if r = t • fA • s•fB u
121
Computational Linguistics Volume 20, Number 1
If siblings are swapped,
E[B / t&apos; , A/ t](r) = E[B / t&apos;][A/ update(t , B, t&apos;)](r)
= E[B/ t&apos;][A / t](r)
E[B / t • s](r) if t r
A(v) ifr=-t•vandfAv
E[B / t • s](t v) if r =- t • fA v
E(r) iftr
A(v) if r = t v
E(t • v) if r = t • fA • v and t•s t-v
B(u) ifr=t•fA •vandt-v=t•s•uandfB u
E(t•s•u) ifr-=t•fA •vandt•v-=t•fB• u
E(r) iftZr
A(v) if r = t • v
E(t v) ifs v and r=t•fA -v
B(u) if r = t • fA • s • u and fB
E(t s • u) if r = t • fA • s • fB • u
Case t&apos; t: Analogously.
Case t tl and tf t:
</figure>
<equation confidence="0.9516805">
E[A/ t,B / t&apos;](r) = E[A/ t][B /update(e , A, t)](r)
= E[A/ t][B / t&apos;](r)
E[A/ t] (r) if t&apos; r
B(u) if r = t&apos; • u and fB u
E[A/ t](t&apos; • u) if r = t&apos; • fB • u
E(r) if t&apos; r and t r
A(v) if t&apos; r and r = t • v and fA v
E(t v) if t&apos; rand r= t •fA -v
B(u) if r = t&apos; • u and fB u
E(t&apos; u) if r = t&apos; • fB • u
</equation>
<bodyText confidence="0.968131666666667">
Note that this is unchanged (up to variable renaming) under swapping of
A for B and t for t&apos;. That is E[A/ t,B / e1(r) E[B /t&apos; , A/ t](r).
We now return to the main proposition.
</bodyText>
<subsectionHeader confidence="0.796874">
Proposition
</subsectionHeader>
<bodyText confidence="0.960947">
If t t&apos; then E[. , A/ t,B/t&apos; , .1 = E[. ,B/t&apos; ,A/t, .].
</bodyText>
<subsectionHeader confidence="0.789019">
Proof
</subsectionHeader>
<bodyText confidence="0.951644">
The effect of the adjunctions before the two specified in the swap is obviously the
same on all following adjunctions, so we need only show that
</bodyText>
<equation confidence="0.780834">
E[A/t,B/t/,C/t] = E[B/t/ ,A/t,Cilti, • • • ,Ckitki
</equation>
<reference confidence="0.564498666666667">
without loss of generality. We examine the effect of the A and B adjunctions on the
tree address t, for each C, separately. In the case of the former adjunction order
E[A/ t,B/ t&apos; , , Ci/ t„. .1
E[A/ t][B / update (t&apos; ,A, t), . . , C, / update(t„ A, t), . . .]
E[A/ t][B /update(t&apos; , A, t)}[.. , Ci / update(update(t„ A, t), B, update(t&apos; ,A, t)), . . .]
E[A / t,B / t&apos;][. , C, / update(update (t„ A, t), B, update (t&apos; ,A, t)), . . .]
</reference>
<page confidence="0.994869">
122
</page>
<note confidence="0.881237">
Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation
</note>
<bodyText confidence="0.918841818181818">
and for the latter adjunction order:
E[B/t/ , A/t, ,C,/ti, .
= E[B t&apos;][A/ update(t , B, t&apos;),. , C,/ update(t„ B, t&apos;), . . .1
= E[B / t&apos;][A/ update(t, B, t&apos;)][. . . , Ci / update(update(t„ B, t&apos;), A, update (t, B, t&apos;)), . . .]
= E[B / t&apos; , A/ t][. , Ci /update(update(t„ B, t&apos;), A, update (t, B, t&apos;)), .]
= E[A/ t , B / t&apos;][. , C,/ update(update(t„ B, t&apos;), A, update(t, B, t&apos;)), . . .]
This last step holds by virtue of the lemma.
Thus, it suffices to show that
update(update(t„ A, t), B, update(/&apos; , A, t)) -= update(update(t„ B, t&apos;), A, update(t, B, t&apos;))
Again, we perform a case analysis depending on the prefix relationships of t, t&apos;,
and t,. Note that we make use of the fact that if t t&apos; then (t&apos; — t) • s = t&apos; • s — t.
</bodyText>
<equation confidence="0.26932775">
Case t t&apos;:
Subcase t,:
update(update(t , A, t), B, update(t&apos; , A, t))
update(t • fA • (t, — t), B, t • fA • (t&apos; — t))
= t • fA • (t&apos; — t) fa • (t, — t&apos;)
t fA • (t&apos; • • (t, - t&apos;) - t)
- update(t&apos; • fa • (t, — t&apos;), A, t)
update(update(t„ B, t&apos;), A, update(t, B, t&apos;))
</equation>
<bodyText confidence="0.617261">
Subcase t&apos; t, and t t,:
update (update(t„ A, t), B, update(t&apos; , A, t))
= update(t • fA • (t, — t),B, t• f A • (t&apos; — t))
▪ t • fA • (t, — t)
= update(t„ A, t)
= update(update(t„ B, t&apos;), A, update(t, B, t&apos;))
Subcase tf ti and t t,:
</bodyText>
<equation confidence="0.836454333333333">
update(update(t„ A, t), B, update (t&apos; ,A, t))
update(t„ B, t • fA • (t&apos; — t))
= t,
</equation>
<bodyText confidence="0.772643166666667">
= update(t„ A, t • fa • (t&apos; — t))
= update(update(t„ B, t&apos;), A, update(t, B, t&apos;))
Case t&apos; t: The proof is as for the previous case with t for t&apos; and vice versa.
Case t tf and tt t:
Subcase t t,: We can conclude from the assumptions that tt t.
Then
</bodyText>
<construct confidence="0.9288228">
update(update(t„ A, t), B, update (t&apos; ,A, t))
-= update(t fA (t, — t),B, t&apos;)
= t • fA • (t, t)
= update(t,, A, t)
-= update(update(t„ B, t&apos;), A, update(t, B, t&apos;))
</construct>
<page confidence="0.991687">
123
</page>
<figure confidence="0.8281352">
Computational Linguistics Volume 20, Number 1
Subcase t 7g ti and t&apos; -&lt; ti: The proof is as for the previous subcase
with t for t&apos; and vice versa.
Subcase t 7g ti and tf ti:
update(update(t„ A, t),B , update(t&apos; ,A, t))
= update (t„ B , t1)
=ti
= update(t„ A, t)
= update(update(ti,B, t&apos;), A, update (t, B, t&apos;))
0
</figure>
<page confidence="0.964811">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.829767">
<title confidence="0.998969">An Alternative Conception of Tree-Adjoining Derivation</title>
<author confidence="0.985067">Yves Schabes</author>
<affiliation confidence="0.965778">Mitsubishi Electric Research Laboratory</affiliation>
<author confidence="0.97415">Stuart M Shiebert</author>
<affiliation confidence="0.999987">Harvard University</affiliation>
<abstract confidence="0.98512075">The precise formulation of derivation for tree-adjoining grammars has important ramifications for a wide variety of uses of the formalism, from syntactic analysis to semantic interpretation and statistical language modeling. We argue that the definition of tree-adjoining derivation must be reformulated in order to manifest the proper linguistic dependencies in derivations. The particular proposal is both precisely characterizable through a definition of TAG derivations as equivalence classes of ordered derivation trees, and computationally operational, by virtue of a compilation to linear indexed grammars together with an efficient algorithm for recognition and parsing according to the compiled grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeille</author>
<author>Yves Schabes</author>
<author>Aravind K Joshi</author>
</authors>
<title>Using lexicalized tree adjoining grammars for machine translation.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 13th International Conference on Computational Linguistics, Volume 3,1-6,</booktitle>
<location>Helsinki, Finland.</location>
<marker>Abeille, Schabes, Joshi, 1990</marker>
<rawString>Abeille, Anne; Schabes, Yves; and Joshi, Aravind K. (1990). &amp;quot;Using lexicalized tree adjoining grammars for machine translation.&amp;quot; In Proceedings, 13th International Conference on Computational Linguistics, Volume 3,1-6, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey K Pullum</author>
<author>Ivan A Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Blackwell.</publisher>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey K.; and Sag, Ivan A. (1985). Generalized Phrase Structure Grammar. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>S R Kosaraju</author>
<author>H M Yamada</author>
</authors>
<title>String adjunct grammars: I. Local and distributed adjunction.&amp;quot;</title>
<date>1972</date>
<journal>Information and Control,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>93--116</pages>
<marker>Joshi, Kosaraju, Yamada, 1972</marker>
<rawString>Joshi, A. K.; Kosaraju, S. R.; and Yamada, H. M. (1972a). &amp;quot;String adjunct grammars: I. Local and distributed adjunction.&amp;quot; Information and Control, 21(2), 93-116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>S R Kosaraju</author>
<author>H M Yamada</author>
</authors>
<title>String adjunct grammars: H. Equational representation, null symbols, and linguistic relevance.&amp;quot;</title>
<date>1972</date>
<journal>Information and Control,</journal>
<volume>21</volume>
<issue>3</issue>
<pages>235--260</pages>
<marker>Joshi, Kosaraju, Yamada, 1972</marker>
<rawString>Joshi, A. K.; Kosaraju, S. R.; and Yamada, H. M. (1972b). &amp;quot;String adjunct grammars: H. Equational representation, null symbols, and linguistic relevance.&amp;quot; Information and Control, 21(3), 235-260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.&amp;quot;</title>
<date>1975</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>10</volume>
<issue>1</issue>
<pages>136--163</pages>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Joshi, Aravind K.; Levy, L. S.; and Takahashi, M. (1975). &amp;quot;Tree adjunct grammars.&amp;quot; Journal of Computer and System Sciences, 10(1), 136-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony S Kroch</author>
</authors>
<title>Asymmetries in long distance extraction in a TAG grammar.&amp;quot; In Alternative Conceptions of Phrase Structure,</title>
<date>1989</date>
<pages>66--98</pages>
<publisher>University of Chicago Press.</publisher>
<note>edited by</note>
<contexts>
<context position="17526" citStr="Kroch (1989)" startWordPosition="2762" endWordPosition="2763">es consideration of constraints that pass among many trees just to limit the cooccurrence of a pair of trees. However, it more closely follows the spirit of TAGs to state such intuitively local limitations locally. 7 We use the term &apos;predication&apos; in its logical sense, that is, for auxiliary trees that serve as logical predicates over the trees into which they adjoin, in contrast to the term&apos;s linguistic sub-sense in which the argument of the predicate is a linguistic subject. 8 The distinction between predicative and modifier trees has been proposed previously for purely linguistic reasons by Kroch (1989), who refers to them as complement and athematic trees, respectively. The arguments presented here can be seen as providing further evidence for differentiating the two kinds of auxiliary trees. A precursor to this idea can perhaps be seen in the distinction between repeatable and nonrepeatable adjunction in the formalism of string adjunct grammars, a precursor of TAGs (Joshi, Kosaraju, and Yamada 1972b, pages 253-254). 96 Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation In summary, the interpretation of adjoining constraints in TAG is sensitive to the particular notion of derivati</context>
</contexts>
<marker>Kroch, 1989</marker>
<rawString>Kroch, Anthony S. (1989). &amp;quot;Asymmetries in long distance extraction in a TAG grammar.&amp;quot; In Alternative Conceptions of Phrase Structure, edited by M. Baltin and A. Kroch, 66-98. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony S Kroch</author>
<author>Aravind K Joshi</author>
</authors>
<title>The linguistic relevance of tree adjoining grammar.&amp;quot;</title>
<date>1985</date>
<tech>Technical Report MS-CIS-85-18,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<marker>Kroch, Joshi, 1985</marker>
<rawString>Kroch, Anthony S., and Joshi, Aravind K. (1985). &amp;quot;The linguistic relevance of tree adjoining grammar.&amp;quot; Technical Report MS-CIS-85-18, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>David H D Warren</author>
</authors>
<title>Parsing as deduction.&amp;quot;</title>
<date>1983</date>
<booktitle>In Proceedings, 21st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>137--144</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="51742" citStr="Pereira and Warren 1983" startWordPosition="8284" endWordPosition="8287"> use the following notations in this and later sections. The symbol P will serve as a variable over the two LIG grammar nonterminals t and b. The substring of the string w1 • • • wn being parsed between indices i and j will be notated as wi+1 • • • wp which we take to be the empty string when i is greater than or equal to j. We will use F, A, and e for sequences containing terminals and LIG nonterminals with their stack specifications. For instance, F might be t[mit[..772]t[7/3]. The parsing algorithm can be seen as a tabular parsing method based on deduction of items, as in Earley deduction (Pereira and Warren 1983). We will so describe it, by presenting inference rules over items of the form (P[ii] F • A, i, j , k , 1) . Such items play the role of the items of Earley&apos;s algorithm. Unlike the items of Earley&apos;s algorithm, however, an item of this form does not embed a grammar rule proper; that is, P[n] -4 FA is not necessarily a rule of the grammar. Rather, it is what we will call a reduced rule; for reasons described below, the nonterminals in I&apos; and A as well as the nonterminal P[n] record only the top element of each stack of indices. We will use the notation P[n] —4 FA for the unreduced form of the ru</context>
</contexts>
<marker>Pereira, Warren, 1983</marker>
<rawString>Pereira, Fernando C. N., and Warren, David H. D. (1983). &amp;quot;Parsing as deduction.&amp;quot; In Proceedings, 21st Annual Meeting of the Association for Computational Linguistics, 137-144. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Svartvik</author>
</authors>
<date>1985</date>
<journal>A Comprehensive Grammar of the English Language. Longman.</journal>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Quirk, Randolph; Greenbaum, Sidney; Leech, Geoffrey; and Svartvik, Jan (1985). A Comprehensive Grammar of the English Language. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Probabilistic tree-adjoining grammar as a framework for statistical natural language processing.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, 14th International Conference on Computational Linguistics,</booktitle>
<pages>418--424</pages>
<location>Nantes, France.</location>
<contexts>
<context position="18517" citStr="Resnik 1992" startWordPosition="2912" endWordPosition="2913">Kosaraju, and Yamada 1972b, pages 253-254). 96 Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation In summary, the interpretation of adjoining constraints in TAG is sensitive to the particular notion of derivation that is used. Therefore, it can be used as a litmus test for an appropriate definition of derivation. As such, it argues for a nonstandard independent notion of derivation for modifier auxiliary trees and a standard dependent notion for predicative trees. 3.2 Adding Statistical Parameters In a similar vein, the statistical parameters of a stochastic lexicalized TAG (SLTAG) (Resnik 1992; Schabes 1992) specify the probability of adjunction of a given auxiliary tree at a specific node in another tree. This specification may again be interpreted with regard to differing derivations, obviously with differing impact on the resulting probabilities assigned to derivation trees. (In the extreme case, a constraint prohibiting adjoining corresponds to a zero probability in an SLTAG. The relation to the argument in the previous section follows thereby.) Consider a case in which linguistic modification of noun phrases by adjectives is modeled by adjunction of a modifying tree. Under the</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Resnik, Philip (1992). &amp;quot;Probabilistic tree-adjoining grammar as a framework for statistical natural language processing.&amp;quot; In Proceedings, 14th International Conference on Computational Linguistics, 418-424. Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>The valid prefix property and left to right parsing of tree-adjoining grammar.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, Second International Workshop on Parsing Technologies,</booktitle>
<pages>21--30</pages>
<location>Cancun, Mexico.</location>
<contexts>
<context position="55029" citStr="Schabes (1991)" startWordPosition="8853" endWordPosition="8854"> addition to the nonterminal symbol). This drastically decreases the number of possible items and accounts for the polynomial character of the resultant algorithm.&apos; Side conditions make up for some of the loss of information, thereby maintaining correctness. For instance, the Type 4 Completor rule specifies a relation between n and i that takes the place of popping an element off of the stack associated with 77. However, the side conditions are strictly weaker than maintaining full stack information. Consequently, the algorithm, though correct, does not maintain the valid prefix property. See Schabes (1991) for further discussion and alternatives. Scanning and prediction work much as in Earley&apos;s original algorithm. • Scanner: (b[77] F • aA, i,j,k,l) (b[n] —&gt; Fa • A, i, j,k,1 + 1) Note that the only rules that need be considered are those where the parent is a bottom node, as terminal symbols occur on the right-hand side only of Type 1 or 2 productions. Otherwise, the rule is exactly as that for Earley&apos;s algorithm except that the extra foot indices (j and k) are carried along. • Predictor: (P[n] --+ F • P&apos;[71] i, j,k,l) [711 e (pi [771 • 0,1, /) This rule serves to form predictions for any type p</context>
<context position="65278" citStr="Schabes 1991" startWordPosition="10719" endWordPosition="10721">trees while parsing proceeds. A synchronous TAG is composed of two base TAGs (which we will call the source TAG and the target TAG) whose elementary trees have been paired one-to-one. A synchronous TAG whose source TAG is a grammar for a fragment of English and whose target TAG is a grammar for a logical form language may be used to generate logical forms for each sentence of English that the source grammar admits (Shieber and Schabes 1990). Similarly, with source and target swapped, the synchronized grammar may be used to generate English sentences corresponding to logical forms (Shieber and Schabes 1991). If the source and target grammars specify fragments of natural languages, an automatic translation system is specified (Abeille, Schabes, and Joshi 1990). Abstractly viewed, the processing of a synchronous grammar proceeds by parsing an input string according to the source grammar, thereby generating a derivation tree for the string; mapping the derivation tree into a derivation tree for the target grammar; and generating a derived tree (hence, derived string) according to the target grammar. One frequent worry about synchronous TAGs as used in their semantic interpretation mode is whether i</context>
</contexts>
<marker>Schabes, 1991</marker>
<rawString>Schabes, Yves (1991). &amp;quot;The valid prefix property and left to right parsing of tree-adjoining grammar.&amp;quot; In Proceedings, Second International Workshop on Parsing Technologies, 21-30. Cancun, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Stochastic lexicalized tree-adjoining grammars.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, 14th International Conference on Computational Linguistics,</booktitle>
<pages>426--432</pages>
<location>Nantes, France.</location>
<contexts>
<context position="18532" citStr="Schabes 1992" startWordPosition="2914" endWordPosition="2915"> Yamada 1972b, pages 253-254). 96 Yves Schabes and Stuart M. Shieber Tree-Adjoining Derivation In summary, the interpretation of adjoining constraints in TAG is sensitive to the particular notion of derivation that is used. Therefore, it can be used as a litmus test for an appropriate definition of derivation. As such, it argues for a nonstandard independent notion of derivation for modifier auxiliary trees and a standard dependent notion for predicative trees. 3.2 Adding Statistical Parameters In a similar vein, the statistical parameters of a stochastic lexicalized TAG (SLTAG) (Resnik 1992; Schabes 1992) specify the probability of adjunction of a given auxiliary tree at a specific node in another tree. This specification may again be interpreted with regard to differing derivations, obviously with differing impact on the resulting probabilities assigned to derivation trees. (In the extreme case, a constraint prohibiting adjoining corresponds to a zero probability in an SLTAG. The relation to the argument in the previous section follows thereby.) Consider a case in which linguistic modification of noun phrases by adjectives is modeled by adjunction of a modifying tree. Under the standard defin</context>
</contexts>
<marker>Schabes, 1992</marker>
<rawString>Schabes, Yves (1992). &amp;quot;Stochastic lexicalized tree-adjoining grammars.&amp;quot; In Proceedings, 14th International Conference on Computational Linguistics, 426-432. Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Stuart M Shieber</author>
</authors>
<title>An alternative conception of tree-adjoining derivation.&amp;quot;</title>
<date>1992</date>
<tech>Technical Report 08-92,</tech>
<institution>Harvard University,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="67248" citStr="Schabes and Shieber 1992" startWordPosition="11012" endWordPosition="11015">ees incrementally as parsing proceeds. Once this has been demonstrated, it should be obvious that these derivation trees could be transferred to target derivation trees during the parsing process and immediately generated from. Thus, incremental interpretation is demonstrated to be possible in the synchronous TAG framework. In fact, the technique presented in this section has allowed for the first implementation of synchronous TAG processing, by Onnig Dombalagian. This implementation was directly based on the inference-based TAG parser mentioned in Section 6.5 and presented in full elsewhere (Schabes and Shieber 1992). We associate with each item a set of operations that have been implicitly carried out by the parser in recognizing the substring covered by the item. An operation can be characterized by a derivation tree and a tree address at which the derivation tree is 113 Computational Linguistics Volume 20, Number 1 • Scanner: (b[n] F • aA, i,j, k, 1) (b[n] ra • A, ,j , k, 1 + 1) a = w1+1 • Predictor: (P[771 F • P&apos; [OA, j , k, 1) • 6,1, , 1) • Type 1 and 2 Completor: 13&apos; [Ti&apos;l —&gt; e (b[m] —&gt; F • On] , m, , , i) (t[n] —&gt; e • , i,j, k, 1) 97 not a root node rt[n] • A, m, j U j&apos; , k u k&apos; , 1) • Type 3 Cornp</context>
</contexts>
<marker>Schabes, Shieber, 1992</marker>
<rawString>Schabes, Yves, and Shieber, Stuart M. (1992). &amp;quot;An alternative conception of tree-adjoining derivation.&amp;quot; Technical Report 08-92, Harvard University, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Richard C Waters</author>
</authors>
<title>Lexicalized context-free grammars.&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings, 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>121--129</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="10267" citStr="Schabes and Waters 1993" startWordPosition="1622" endWordPosition="1625">o be specified as to whether a particular auxiliary tree may or may not be adjoined at a particular node in a particular tree. The idea is formulated in its modern variant as selective-adjoining constraints (Vijay-Shanker and Joshi 1985). As an application of this capability, we consider the traditional grammatical view that directional adjuncts can be used only with certain verbs.&apos; This would account 4 The formulation of derivation for tree-adjoining grammars is also of significance for other grammatical formalisms based on weaker forms of adjunction such as lexicalized context-free grammar (Schabes and Waters 1993a) and its stochastic extension (Schabes and Waters 1993b), though we do not discuss these arguments here. 5 For instance, Quirk, Greenbaum, Leech, and Svartvik (1985, page 517) remark that &amp;quot;direction adjuncts of both goal and source can normally be used only with verbs of motion.&amp;quot; Although the restriction is undoubtedly a semantic one, we will examine the modeling of it in a TAG deriving syntactic trees for two reasons. First, the problematic nature of independent derivation is more easily seen in this way. Second, much of the intuition behind TAG analyses is based on a tight relationship bet</context>
</contexts>
<marker>Schabes, Waters, 1993</marker>
<rawString>Schabes, Yves, and Waters, Richard C. (1993a). &amp;quot;Lexicalized context-free grammars.&amp;quot; In Proceedings, 31st Annual Meeting of the Association for Computational Linguistics, 121-129. Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Richard C Waters</author>
</authors>
<title>Stochastic lexicalized context-free grammars.&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings, Third International Workshop on Parsing Technologies,</booktitle>
<pages>257--266</pages>
<institution>Tilburg, The Netherlands and Durbuy, Belgium.</institution>
<contexts>
<context position="10267" citStr="Schabes and Waters 1993" startWordPosition="1622" endWordPosition="1625">o be specified as to whether a particular auxiliary tree may or may not be adjoined at a particular node in a particular tree. The idea is formulated in its modern variant as selective-adjoining constraints (Vijay-Shanker and Joshi 1985). As an application of this capability, we consider the traditional grammatical view that directional adjuncts can be used only with certain verbs.&apos; This would account 4 The formulation of derivation for tree-adjoining grammars is also of significance for other grammatical formalisms based on weaker forms of adjunction such as lexicalized context-free grammar (Schabes and Waters 1993a) and its stochastic extension (Schabes and Waters 1993b), though we do not discuss these arguments here. 5 For instance, Quirk, Greenbaum, Leech, and Svartvik (1985, page 517) remark that &amp;quot;direction adjuncts of both goal and source can normally be used only with verbs of motion.&amp;quot; Although the restriction is undoubtedly a semantic one, we will examine the modeling of it in a TAG deriving syntactic trees for two reasons. First, the problematic nature of independent derivation is more easily seen in this way. Second, much of the intuition behind TAG analyses is based on a tight relationship bet</context>
</contexts>
<marker>Schabes, Waters, 1993</marker>
<rawString>Schabes, Yves, and Waters, Richard C. (1993b). &amp;quot;Stochastic lexicalized context-free grammars.&amp;quot; In Proceedings, Third International Workshop on Parsing Technologies, 257-266. Tilburg, The Netherlands and Durbuy, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Constraint-Based Grammar Formalisms.</title>
<date>1992</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="60304" citStr="Shieber 1992" startWordPosition="9832" endWordPosition="9833">s accepted; otherwise it is rejected. Alternatively, the axioms can be stated as if there were extra rules S tird for each ys a start-nonterminal-labeled root node of an initial tree. In this case, the axioms are items of the form • t[ils], 0, 0) and the string is accepted upon proving (S t[ris] • , 0,-,-, n). In this case, an extra prediction and completion rule is needed just for these rules, since the normal rules do not allow S on the left-hand side. This point is taken up further in Section 6.4. Generation of items can be cached in the standard way for inference-based parsing algorithms (Shieber 1992); this leads to a tabular or chart-based parsing algorithm. 6.2 The Algorithm Invariant The algorithm maintains an invariant that holds of all items added to the chart. We will describe the invariant using some additional notational conventions. Recall that P[n] F is the LIG production in the grammar whose reduced form is P[n] F. The notation 11-y] where -y is a sequence of stack symbols (i.e., nodes), specifies the sequence F with -y replacing the occurrence of .. in the stack specifications. For example, if r is the sequence t[ni I t[..r/2] t[m], then F{-y} = t[m]t[-y772] t[m]. A single LIG </context>
<context position="67248" citStr="Shieber 1992" startWordPosition="11014" endWordPosition="11015">tally as parsing proceeds. Once this has been demonstrated, it should be obvious that these derivation trees could be transferred to target derivation trees during the parsing process and immediately generated from. Thus, incremental interpretation is demonstrated to be possible in the synchronous TAG framework. In fact, the technique presented in this section has allowed for the first implementation of synchronous TAG processing, by Onnig Dombalagian. This implementation was directly based on the inference-based TAG parser mentioned in Section 6.5 and presented in full elsewhere (Schabes and Shieber 1992). We associate with each item a set of operations that have been implicitly carried out by the parser in recognizing the substring covered by the item. An operation can be characterized by a derivation tree and a tree address at which the derivation tree is 113 Computational Linguistics Volume 20, Number 1 • Scanner: (b[n] F • aA, i,j, k, 1) (b[n] ra • A, ,j , k, 1 + 1) a = w1+1 • Predictor: (P[771 F • P&apos; [OA, j , k, 1) • 6,1, , 1) • Type 1 and 2 Completor: 13&apos; [Ti&apos;l —&gt; e (b[m] —&gt; F • On] , m, , , i) (t[n] —&gt; e • , i,j, k, 1) 97 not a root node rt[n] • A, m, j U j&apos; , k u k&apos; , 1) • Type 3 Cornp</context>
</contexts>
<marker>Shieber, 1992</marker>
<rawString>Shieber, Stuart M. (1992). Constraint-Based Grammar Formalisms. MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Restricting the weak-generative capacity of synchronous tree-adjoining grammars.&amp;quot;</title>
<journal>Computational Intelligence.</journal>
<marker>Shieber, </marker>
<rawString>Shieber, Stuart M. (in press). &amp;quot;Restricting the weak-generative capacity of synchronous tree-adjoining grammars.&amp;quot; Computational Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
</authors>
<title>Synchronous tree-adjoining grammars.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 13th International Conference on Computational Linguistics,</booktitle>
<volume>3</volume>
<pages>253--258</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="20969" citStr="Shieber and Schabes 1990" startWordPosition="3286" endWordPosition="3289">g,&amp;quot; whereas the appropriate term for the corresponding cooking process applied to peppers is &amp;quot;roasting,&amp;quot; would be more determining of the expected overall probabilities. Note again that the distinction between modifier and predicative trees is important. The standard definition of derivation is entirely appropriate for adjunction probabilities for predicative trees, but not for modifier trees. 3.3 Adding Semantics Finally, the formation of synchronous TAGs has been proposed to allow use of TAGs in semantic interpretation, natural language generation, and machine translation. In previous work (Shieber and Schabes 1990), the definition of synchronous TAG derivation is given in a manner that requires multiple adjunctions at a single node. The need for such derivations follows from the fact that synchronous derivations are intended to model semantic relationships. In cases of multiple adjunction of modifier trees at 9 Intuition is an appropriate guide in the design of the SLTAG framework, as the idea is to set up a linguistically plausible infrastructure on top of which a lexically based statistical model can be built. In addition, suggestive (though certainly not conclusive) evidence along these lines can be </context>
<context position="65109" citStr="Shieber and Schabes 1990" startWordPosition="10692" endWordPosition="10696">the intended applications for extended derivation TAG parsing is the parsing of synchronous TAGs. Especially important in this application is the ability to generate the derivation trees while parsing proceeds. A synchronous TAG is composed of two base TAGs (which we will call the source TAG and the target TAG) whose elementary trees have been paired one-to-one. A synchronous TAG whose source TAG is a grammar for a fragment of English and whose target TAG is a grammar for a logical form language may be used to generate logical forms for each sentence of English that the source grammar admits (Shieber and Schabes 1990). Similarly, with source and target swapped, the synchronized grammar may be used to generate English sentences corresponding to logical forms (Shieber and Schabes 1991). If the source and target grammars specify fragments of natural languages, an automatic translation system is specified (Abeille, Schabes, and Joshi 1990). Abstractly viewed, the processing of a synchronous grammar proceeds by parsing an input string according to the source grammar, thereby generating a derivation tree for the string; mapping the derivation tree into a derivation tree for the target grammar; and generating a d</context>
</contexts>
<marker>Shieber, Schabes, 1990</marker>
<rawString>Shieber, Stuart M., and Schabes, Yves (1990). &amp;quot;Synchronous tree-adjoining grammars.&amp;quot; In Proceedings, 13th International Conference on Computational Linguistics, Volume 3, 253-258. Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
</authors>
<title>Generation and synchronous tree adjoining grammars.&amp;quot;</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<volume>4</volume>
<issue>7</issue>
<pages>220--228</pages>
<contexts>
<context position="65278" citStr="Shieber and Schabes 1991" startWordPosition="10717" endWordPosition="10721"> derivation trees while parsing proceeds. A synchronous TAG is composed of two base TAGs (which we will call the source TAG and the target TAG) whose elementary trees have been paired one-to-one. A synchronous TAG whose source TAG is a grammar for a fragment of English and whose target TAG is a grammar for a logical form language may be used to generate logical forms for each sentence of English that the source grammar admits (Shieber and Schabes 1990). Similarly, with source and target swapped, the synchronized grammar may be used to generate English sentences corresponding to logical forms (Shieber and Schabes 1991). If the source and target grammars specify fragments of natural languages, an automatic translation system is specified (Abeille, Schabes, and Joshi 1990). Abstractly viewed, the processing of a synchronous grammar proceeds by parsing an input string according to the source grammar, thereby generating a derivation tree for the string; mapping the derivation tree into a derivation tree for the target grammar; and generating a derived tree (hence, derived string) according to the target grammar. One frequent worry about synchronous TAGs as used in their semantic interpretation mode is whether i</context>
</contexts>
<marker>Shieber, Schabes, 1991</marker>
<rawString>Shieber, Stuart M., and Schabes, Yves (1991). &amp;quot;Generation and synchronous tree adjoining grammars.&amp;quot; Computational Intelligence, 4(7), 220-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
</authors>
<title>A Study of Tree Adjoining Grammars. Doctoral dissertation,</title>
<date>1987</date>
<institution>Department of Computer and Information Science, University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2728" citStr="Vijay-Shanker (1987)" startWordPosition="391" endWordPosition="392"> of the important primitive relationships among trees. The decoupling of derivation trees from derived trees thus makes possible a more flexible ability to pursue these types of analyses. At the same time, the exact definition of derivation becomes of paramount importance. In this paper, we argue that previous definitions of tree-adjoining derivation have not taken full advantage of this decoupling, and are not as appropriate as they might be for the kind of further analysis that tree-adjoining analyses could make possible. In particular, the standard definition of derivation, attributable to Vijay-Shanker (1987), * Cambridge, MA 02139 t Division of Applied Sciences, Cambridge, MA 02138 © 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 1 requires that auxiliary trees be adjoined at distinct nodes in elementary trees. However, in certain cases, especially cases characterized as linguistic modification, it is more appropriate to allow multiple adjunctions at a single node. In this paper we propose a redefinition of TAG derivation along these lines, whereby multiple auxiliary trees of modification can be adjoined at a single node, whereas only a single auxiliary</context>
<context position="50501" citStr="Vijay-Shanker (1987)" startWordPosition="8070" endWordPosition="8071">r, must be adjusted to allow for the new rule types. This compilation makes possible a simple proof of the weak-generative equivalence of TAGs under the standard and extended derivations!&apos; Call the set of languages generable by a TAG under the standard definition of derivation TAL, and under the extended definition TAL,. Clearly, TAL, C TAL, since the standard definition can be mimicked by making all auxiliary trees predicative. The compilation above provides the inclusion TAL, c LIL, where LIL is the set of linear indexed languages. The final inclusion LIL C TAL, has been shown indirectly by Vijay-Shanker (1987) using embedded push-down automata and modified head grammars as intermediaries. From these inclusions, we can conclude that TAL, = TAL,. 6. Recognition and Parsing A recognition algorithm for TAGs can be constructed based on the above translation into corresponding LIGs as specified by Rules 1 through 6 in the previous section. The algorithm is not a full recognition algorithm for LIGs, but rather, is tuned for exactly the types of rules generated as output of this compilation process. In this section, we present the recognition algorithm and modify it to work with the extended derivation com</context>
</contexts>
<marker>Vijay-Shanker, 1987</marker>
<rawString>Vijay-Shanker, K. (1987). A Study of Tree Adjoining Grammars. Doctoral dissertation, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>Aravind K Joshi</author>
</authors>
<title>Some computational properties of tree adjoining grammars.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>82--93</pages>
<location>Chicago, IL.</location>
<contexts>
<context position="9881" citStr="Vijay-Shanker and Joshi 1985" startWordPosition="1566" endWordPosition="1569">er. However, tree-adjoining grammars are almost universally extended with augmentations that make the issue apposite. We discuss three such variations here, all of which argue for the use of independent derivations under certain circumstances.&apos; 3.1 Adding Adjoining Constraints Already in very early work on tree-adjoining grammars (Joshi, Levy, and Takahashi 1975) constraints were allowed to be specified as to whether a particular auxiliary tree may or may not be adjoined at a particular node in a particular tree. The idea is formulated in its modern variant as selective-adjoining constraints (Vijay-Shanker and Joshi 1985). As an application of this capability, we consider the traditional grammatical view that directional adjuncts can be used only with certain verbs.&apos; This would account 4 The formulation of derivation for tree-adjoining grammars is also of significance for other grammatical formalisms based on weaker forms of adjunction such as lexicalized context-free grammar (Schabes and Waters 1993a) and its stochastic extension (Schabes and Waters 1993b), though we do not discuss these arguments here. 5 For instance, Quirk, Greenbaum, Leech, and Svartvik (1985, page 517) remark that &amp;quot;direction adjuncts of b</context>
</contexts>
<marker>Vijay-Shanker, Joshi, 1985</marker>
<rawString>Vijay-Shanker, K., and Joshi, Aravind K. (1985). &amp;quot;Some computational properties of tree adjoining grammars.&amp;quot; In Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics, 82-93. Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>Aravind K Joshi</author>
</authors>
<title>Feature structure based tree adjoining grammars.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, 12th International Conference on Computational Linguistics,</booktitle>
<pages>714--719</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="16772" citStr="Vijay-Shanker and Joshi 1988" startWordPosition="2638" endWordPosition="2641"> of the original node, rather than those of the root and foot of the modifier tree, are manifest in the corresponding nodes in the derived tree, the adjoining constraints would propagate appropriately to handle the examples above. This alternative leads, however, to a formalism for which derivation trees are no longer context-free, with concomitant difficulties in designing parsing algorithms. Instead, the extended definition of derivation effectively allows use of a Kleene-* in the &amp;quot;grammar&amp;quot; of derivation trees. Adjoining constraints can also be implemented using feature structure equations (Vijay-Shanker and Joshi 1988). It is possible that judicious use of such techniques might prevent the particular problems noted here. Such an encoding of a solution requires consideration of constraints that pass among many trees just to limit the cooccurrence of a pair of trees. However, it more closely follows the spirit of TAGs to state such intuitively local limitations locally. 7 We use the term &apos;predication&apos; in its logical sense, that is, for auxiliary trees that serve as logical predicates over the trees into which they adjoin, in contrast to the term&apos;s linguistic sub-sense in which the argument of the predicate is</context>
</contexts>
<marker>Vijay-Shanker, Joshi, 1988</marker>
<rawString>Vijay-Shanker, K., and Joshi, Aravind K. (1988). &amp;quot;Feature structure based tree adjoining grammars.&amp;quot; In Proceedings, 12th International Conference on Computational Linguistics, 714-719. Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
</authors>
<title>Polynomial parsing of extensions of context-free grammars.&amp;quot;</title>
<date>1990</date>
<booktitle>In Current Issues in Parsing Technologies, edited by Masaru Tomita,</booktitle>
<pages>191--206</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="41796" citStr="Vijay-Shanker and Weir (1990)" startWordPosition="6600" endWordPosition="6603">ar indexed production can be given schematically as: N0[..130] —&gt; Ni[/311 Ns[..0s] 1\15-4-1[05+1] • • • Nk[Oki The Ni are nonterrninals, the )3i, strings of indices. The &amp;quot;..&amp;quot; notation stands for the remainder of the stack below the given string of indices. Note that only one element on the right-hand side, N„ inherits the remainder of the stack from the parent. (This schematic rule is intended to be indicative, not definitive. We ignore issues such as the optionality of the inherited stack, how terminal symbols fit in, and so forth. VijayShanker and Weir [1990] present a complete discussion.) Vijay-Shanker and Weir (1990) present a way of specifying any TAG as a linear indexed grammar. The LIG version makes explicit the standard notion of derivation being presumed. Also, the LIG version of a TAG grammar can be used for recognition and parsing. Because the LIG formalism is based on augmented rewriting, the parsing algorithms can be much simpler to understand and easier to modify, and no loss of generality is incurred. For these reasons, we use the technique in this work. The compilation process that manifests the standard definition of derivation can be most easily understood by viewing nodes in a TAG elementar</context>
<context position="46300" citStr="Vijay-Shanker and Weir (1990)" startWordPosition="7372" endWordPosition="7375">owing production: t[••711 t[••ipid• 5. Start foot of adjunction: For each elementary tree node n on which the auxiliary tree /3 with foot node rif can be adjoined, include the following production: b[-nnf] —&gt; b[••n]• 6. Start substitution: For each elementary tree node Ti marked for substitution on which the initial tree a with root node yr can be substituted, include the production *11 —&gt; tkiri• We will refer to productions generated by Rule i above as Type i productions. For example, Type 3 productions are of the form t{. .i —&gt; b[..77]. For further information concerning the compilation see Vijay-Shanker and Weir (1990). For present purposes, it is sufficient to note that the method directly embeds the standard notion of derivation in the rewriting process. To perform an adjunction, we move (by Rule 4) from the node adjoined at to the top of the root of the auxiliary tree. At the root, additional adjunctions might be performed. When returning from the foot of the auxiliary tree back to the node where adjunction occurred, rewriting continues at the bottom of the node (see Rule 5), not the top, so that no more adjunctions can be started at that node. Thus, the dependent nature of predicative adjunction is enfo</context>
<context position="55751" citStr="Vijay-Shanker and Weir (1990)" startWordPosition="8986" endWordPosition="8989">iginal algorithm. • Scanner: (b[77] F • aA, i,j,k,l) (b[n] —&gt; Fa • A, i, j,k,1 + 1) Note that the only rules that need be considered are those where the parent is a bottom node, as terminal symbols occur on the right-hand side only of Type 1 or 2 productions. Otherwise, the rule is exactly as that for Earley&apos;s algorithm except that the extra foot indices (j and k) are carried along. • Predictor: (P[n] --+ F • P&apos;[71] i, j,k,l) [711 e (pi [771 • 0,1, /) This rule serves to form predictions for any type production in the grammar, as the variables P and P&apos; range over the values t and b. In the 16 Vijay-Shanker and Weir (1990) first proposed the recording of only the top stack element in order to achieve efficient parsing. The algorithm they presented is a bottom-up general LIG parsing algorithm. Schabes (1991) sketches a proof of an 0(n6) bound for an Earley-style algorithm for TAG parsing that is more closely related to the algorithm proposed here. a = wi+1 109 Computational Linguistics Volume 20, Number 1 predicted item, the foot is not dominated by the (empty) recognized input, so that the dummy value &apos;—&apos; is used for the foot indices. Note that the predicted item records the reduced form of an unreduced rule of</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1990</marker>
<rawString>Vijay-Shanker, K., and Weir, David J. (1990). &amp;quot;Polynomial parsing of extensions of context-free grammars.&amp;quot; In Current Issues in Parsing Technologies, edited by Masaru Tomita, 191-206. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="false">
<title>without loss of generality. We examine the effect of the A and B adjunctions on the tree address t, for each C, separately. In the case of the former adjunction order E[A/ t,B/ t&apos;</title>
<journal>Ci/ t„. .1 E[A/ t][B / update (t&apos; ,A, t), . . , C, / update(t„ A, t), . . .] E[A/ t][B /update(t&apos; , A, t)}[.. , Ci / update(update(t„ A, t), B, update(t&apos; ,A, t)), . . .] E[A / t,B / t&apos;][. , C, / update(update (t„ A, t), B, update (t&apos; ,A, t)), . . .</journal>
<marker></marker>
<rawString>without loss of generality. We examine the effect of the A and B adjunctions on the tree address t, for each C, separately. In the case of the former adjunction order E[A/ t,B/ t&apos; , , Ci/ t„. .1 E[A/ t][B / update (t&apos; ,A, t), . . , C, / update(t„ A, t), . . .] E[A/ t][B /update(t&apos; , A, t)}[.. , Ci / update(update(t„ A, t), B, update(t&apos; ,A, t)), . . .] E[A / t,B / t&apos;][. , C, / update(update (t„ A, t), B, update (t&apos; ,A, t)), . . .]</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>