<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000229">
<title confidence="0.996963">
Relation Extraction with Matrix Factorization and Universal Schemas
</title>
<author confidence="0.999177">
Sebastian Riedel Limin Yao, Andrew McCallum, Benjamin M. Marlin
</author>
<affiliation confidence="0.9994845">
Department of Computer Science Department of Computer Science
University College London University of Massachusetts at Amherst
</affiliation>
<email confidence="0.998286">
s.riedel@ucl.ac.uk {lmyao,mccallum,marlin}@cs.umass.edu
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999694555555556">
Traditional relation extraction predicts rela-
tions within some fixed and finite target
schema. Machine learning approaches to this
task require either manual annotation or, in
the case of distant supervision, existing struc-
tured sources of the same schema. The need
for existing datasets can be avoided by us-
ing a universal schema: the union of all in-
volved schemas (surface form predicates as in
OpenIE, and relations in the schemas of pre-
existing databases). This schema has an al-
most unlimited set of relations (due to surface
forms), and supports integration with existing
structured data (through the relation types of
existing databases). To populate a database of
such schema we present matrix factorization
models that learn latent feature vectors for en-
tity tuples and relations. We show that such
latent models achieve substantially higher ac-
curacy than a traditional classification ap-
proach. More importantly, by operating simul-
taneously on relations observed in text and in
pre-existing structured DBs such as Freebase,
we are able to reason about unstructured and
structured data in mutually-supporting ways.
By doing so our approach outperforms state-
of-the-art distant supervision.
</bodyText>
<sectionHeader confidence="0.999631" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999732714285714">
Most previous work in relation extraction uses a pre-
defined, finite and fixed schema of relation types
(such as born-in or employed-by). Usually some tex-
tual data is labeled according to this schema, and
this labeling is then used in supervised training of
an automated relation extractor, e.g. Culotta and
Sorensen (2004). However, labeling textual rela-
</bodyText>
<page confidence="0.97906">
74
</page>
<bodyText confidence="0.999918083333334">
tions is time-consuming and difficult, leading to sig-
nificant recent interest in distantly-supervised learn-
ing. Here one aligns existing database records with
the sentences in which these records have been “ren-
dered”––effectively labeling the text—and from this
labeling we can train a machine learning system as
before (Craven and Kumlien, 1999; Mintz et al.,
2009; Bunescu and Mooney, 2007; Riedel et al.,
2010). However, this method relies on the availabil-
ity of a large database that has the desired schema.
The need for pre-existing datasets can be avoided
by using language itself as the source of the schema.
This is the approach taken by OpenIE (Etzioni et al.,
2008). Here surface patterns between mentions of
concepts serve as relations. This approach requires
no supervision and has tremendous flexibility, but
lacks the ability to generalize. For example, Ope-
nIE may find FERGUSON–historian-at–HARVARD
but does not know FERGUSON–is-a-professor-at–
HARVARD. OpenIE has traditionally relied on a
large diversity of textual expressions to provide good
coverage. But this diversity is not always available,
and, in any case, the lack of generalization greatly
inhibits the ability to support reasoning.
One way to gain generalization is to cluster tex-
tual surface forms that have similar meaning (Lin
and Pantel, 2001; Pantel et al., 2007; Yates and
Etzioni, 2009; Yao et al., 2011). While the clus-
ters discovered by all these methods usually contain
semantically related items, closer inspection invari-
ably shows that they do not provide reliable impli-
cature. For example, a typical representative clus-
ter may include historian-at, professor-at, scientist-
at, worked-at. Although these relation types are in-
deed semantically related, note that scientist-at does
not necessarily imply professor-at, and worked-at
</bodyText>
<note confidence="0.4482335">
Proceedings of NAACL-HLT 2013, pages 74–84,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999971159090909">
certainly does not imply scientist-at. In fact, we
contend that any relational schema would inherently
be brittle and ill-defined––having ambiguities, prob-
lematic boundary cases, and incompleteness.1 For
example, Freebase, in spite of its extensive effort to-
wards high coverage, has no critized nor scientist-at
relation.
In response to this problem, we present a new ap-
proach: implicature with universal schemas. Here
we embrace the diversity and ambiguity of original
inputs; we avoid forcing textual meaning into pre-
defined boxes. This is accomplished by defining
our schema to be the union of all source schemas:
original input forms, e.g. variants of surface pat-
terns similarly to OpenIE, as well as relations in
the schemas of many available pre-existing struc-
tured databases. But then, unlike OpenIE, our fo-
cus lies on learning asymmetric implicature among
relations. This allows us to probabilistically “fill
in” inferred unobserved entity-entity relations in
this union. For example, after observing FERGU-
SON–historian-at–HARVARD our system infers that
FERGUSON–professor-at–HARVARD, but not vice
versa.
At the heart of our approach is the hypothesis that
we should concentrate on predicting source data––a
relatively well defined task that can be evaluated and
optimized––as opposed to modeling semantic equiv-
alence, which we believe will always be illusive.
Note that by operating simultaneously on rela-
tions observed in text and in pre-existing structured
databases such as Freebase, we are able to reason
about unstructured and structured data in mutually-
supporting ways. For example, we can predict sur-
face pattern relations that effectively serve as addi-
tional features when predicting Freebase relations,
hence improving generalization. Also notice that
users of our system will not have to study and un-
derstand the complexities of a particular schema in
order to issue queries; they can ask in whatever form
naturally occurs to them, and our system will likely
already have that relation in our universal schema.
Our technical approach is based on extensions
to probabilistic models of matrix factorization and
</bodyText>
<footnote confidence="0.90581775">
1At NAACL 2012 Lucy Vanderwende asked “Where do the
relation types come from?” There was no satisfying answer. At
the same meeting, and in line with Brachman (1983), Ed Hovy
stated “We don’t even know what is-a means.”
</footnote>
<bodyText confidence="0.999558928571429">
collaborative filtering (Collins et al., 2001; Koren,
2008; Rendle et al., 2009). We represent the prob-
abilistic knowledge base as a matrix with entity-
entity pairs in the rows and relations in the columns
(see figure 1). The rows come from running cross-
document entity resolution across pre-existing struc-
tured databases and textual corpora. The columns
come from the union of surface forms and DB rela-
tions. We present a series of models that learn lower
dimensional manifolds for tuples, relations and enti-
ties, and a set of weights that capture direct correla-
tions between relations. Weights and lower dimen-
sional representations act, through dot products, as
the natural parameters of a single log-linear model
to derive per-cell probabilities.
In experiments we show that our models can ac-
curately predict surface patterns relationships which
do not appear explicitly in text, and that learning la-
tent representations of entities, tuples and relations
substantially improves results over a traditional clas-
sifier approach. Moreover, we can improve accu-
racy by simultaneously operating on relations ob-
served in the New York Times corpus and in Free-
base. In particular, our model outperforms the cur-
rent state-of-the-art distant supervision method (Sur-
deanu et al., 2012) by 10% points Mean Average
Precision through joint implicature among surface
patterns and Freebase relations.
</bodyText>
<sectionHeader confidence="0.986992" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.98633">
Before we present our approach in more detail, we
briefly introduce some notation. We use R to de-
note the set of relations we seek to predict (such as
works-written in Freebase, or the X–historian-at–Y
pattern), and T to denote the set of input tuples. For
simplicity we assume each relation to be binary, al-
though our approach can be easily generalized to the
n-ary case. Given a relation r E R and a tuple t E T
the pair (r, t) is a fact, or relation instance. The in-
put to our model is a set of observed facts O, and
the observed facts for a given tuple is denoted by
Ot := {(r,t) E O1.
Our goal is a model that can estimate, for a
given relation r (such as X–historian-at–Y) and a
given tuple t (such as &lt;FERGUSON,HARVARD&gt;),
the probability p (yr,t = 1) where yr,t is a binary
random variable that is true iff t is in relation r. We
</bodyText>
<page confidence="0.999025">
75
</page>
<figureCaption confidence="0.9488996">
Figure 1: Filling up a database of universal schema.
Dark circles are observed facts, shaded circles are in-
ferred facts. Relation Extraction (RE) maps surface pat-
tern relations (and other features) to structured relations.
Surface form clustering models correlations between pat-
terns, and can be fed into RE (Yao et al., 2011). Database
alignment and integration models correlations between
structured relations (not done in this work). Reasoning
with the universal schema incorporates these tasks in a
joint fashion.
</figureCaption>
<bodyText confidence="0.999921642857143">
introduce a series of exponential family models that
estimate this probability using a natural parameter
θr,t and the logistic function:
We will first describe our models through differ-
ent definitions of the natural parameter θr,t. In each
case θr,t will be a function of r, t and a set of weights
and/or latent feature vectors. In section 2.5 we will
then show how these weights and vectors can be es-
timated based on the observed facts O.
Notice that we can interpret p (yr,t = 1) as the
probability that a customer t likes product r. This
analogy allows us to draw from a large body of work
in collaborative filtering, such as work in probabilis-
tic matrix factorization and implicit feedback.
</bodyText>
<subsectionHeader confidence="0.994669">
2.1 Latent Feature Model
</subsectionHeader>
<bodyText confidence="0.985360470588235">
One way to define θr,t is through a latent feature
model F. Here we measure compatibility between
relation r and tuple t as dot product of two latent
feature representations of size KF: ar for relation r,
and vt for tuple t. This gives:
This corresponds to generalized PCA (Collins et al.,
2001), a model were the matrix O = (θr,t) of natural
parameters is defined as the low rank factorization
AV.
Notice that we intentionally omit any per-relation
bias-terms. In section 4 we evaluate ranked answers
to queries on a per-relation basis, and a per-relation
bias term will have no effect on ranking facts of the
same relation. Also consider that such latent feature
models can capture asymmetry by assigning more
peaked vectors to specific relations, and more uni-
form vectors to general relations.
</bodyText>
<subsectionHeader confidence="0.97458">
2.2 Neighborhood Model
</subsectionHeader>
<bodyText confidence="0.999985272727273">
We can interpolate the confidence for a given tuple
and relation based on the trueness of other similar
relations for the same tuple. In collaborative filter-
ing this is referred to as a neighborhood-based ap-
proach (Koren, 2008). In terms of our natural pa-
rameter, we implement a neighborhood model N via
a set of weights wr,ri, where each corresponds to a
directed association strength between relations r and
r&apos;. For a given tuple t and relation r we then sum
up the weights corresponding to all relations r&apos; that
have been observed for tuple t:
</bodyText>
<equation confidence="0.9830275">
θN r,t := � wr,ri.
(ri,t)EO\{(r,t)}
</equation>
<bodyText confidence="0.996069857142857">
Notice that the neighborhood model amounts to
a collection of local log-linear classifiers, one for
each relation r with feature functions fr,r, (t) =
ff [r&apos; =� r n (r&apos;, t) E O] and weights wr. This means
that in contrast to model F, this model cannot har-
ness any synergies between textual and pre-existing
DB relations.
</bodyText>
<figure confidence="0.975983333333333">
X-pmfwsw�t-Y X-hist�da t-Y employee(X,Y) member(X,Y)
Ferguson,Harvard
Oman,Oxford
Firth,Oxford
Gödel,Princeton
Surface Patterns KB Relations
0.95
Cluster Rel. Extraction Align
1
1
Reasoning with Universal Schema
0.05
1
1
1
0.97
0.93 0.97
1
0.95
1
Train
Test
1
p (yr,t = 1|θr,t) := σ (θr,t) =
1 + exp (−θr,t ).
� KF
k
ar,kvt,k.
θF
r,t :=
</figure>
<page confidence="0.686438">
76
</page>
<subsectionHeader confidence="0.940059">
2.3 Entity Model
</subsectionHeader>
<bodyText confidence="0.999720789473685">
Relations have selectional preferences: they allow
only certain types in their argument slots. While
knowledge bases such as Freebase or DBPedia have
extensive ontologies of types of entities, these are of-
ten not sufficiently fine to allow relations to discrim-
inate (Yao et al., 2012b). Hence, instead of using a
predetermined set of entity types, in our entity model
E we learn a latent entity representation from data.
More concretely, for each entity a we introduce a la-
tent feature vector te of dimension KE. In addition,
for each relation r and argument slot i we introduce
a feature vector di of the same dimension. For ex-
ample, binary relations have feature representations
d1 for argument 1, and d2 for argument 2. Mea-
suring compatibility of an entity tuple and relation
amounts to measuring, and summing up, compati-
bility between each argument slot representation and
the corresponding entity representation. This leads
to:
</bodyText>
<equation confidence="0.944362333333333">
BE . arity(r) X KE di,kttz,k.
r,t •_ X k
i=1
</equation>
<bodyText confidence="0.999858">
Note that due to entity resolution, tuples may
share entities, and hence parameters are shared
across rows.
</bodyText>
<subsectionHeader confidence="0.956497">
2.4 Combined Model
</subsectionHeader>
<bodyText confidence="0.999742333333333">
In practice all the above models can capture impor-
tant aspects of the data. Hence we also use various
combinations, such as:
</bodyText>
<equation confidence="0.8160695">
NFE N F E
Br,t := BN + BF + Br,t.
</equation>
<subsectionHeader confidence="0.989357">
2.5 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999984655172414">
Our models are parametrized through weights and
latent component vectors. We could estimate these
parameters by maximizing the loglikelihood of the
observed data akin to Collins et al. (2001). How-
ever, as we do not have access to negative facts, the
model would simply learn to predict all facts to be
true. In our initial attempt to overcome this issue
we sampled a set of unobserved facts as designated
negative facts, as is done in related distant supervi-
sion approaches. However, we found that (a) our
results were sensitive to the choice of negative data
and (b) runtime was increased substantially because
of a large number of required negative facts.
In collaborative filtering positive-only data is also
known as implicit feedback. This type of feedback
arises, for example, when users buy but not rate
items. One successful approach to learning with im-
plicit feedback is based on the observation that the
actual task is not necessarily one of prediction (here:
to predict a number between 0 and 1) but one of
(generally simpler) ranking: to give true “user-item”
cells higher scores than false ones. Bayesian Person-
alized Ranking (BPR) uses a variant of this ranking:
giving observed true facts higher scores than unob-
served (true or false) facts (Rendle et al., 2009). This
relaxed constraint is to be contrasted with the log-
likelihood setting that essentially requires (randomly
sampled) negative facts to score below a globally de-
fined threshold.
</bodyText>
<subsectionHeader confidence="0.577134">
2.5.1 Objective
</subsectionHeader>
<bodyText confidence="0.999279555555555">
We first create a dataset of ranked pairs: for each
relation r and each observed fact f+ := (r, t+) E O
we choose all tuples t− such that f− := (r, t−) E�
O—that is, tuples we have not observed to be in
relation r. For each pair of facts f+ and f− we
want p (f+) &gt; p (f−) and hence Bf+ &gt; Bf−. In
BPR this is achieved by maximizing a sum terms of
the form Objf+,f− := log (Q (Bf+ − Bf− one for
each ranked pair:
</bodyText>
<equation confidence="0.9981445">
Obj := X X Obj(r,t+),(r,t−). (1)
(r,t+)EO (r,t−)/EO
</equation>
<bodyText confidence="0.999821454545455">
Notice that this objective differs slightly from the
one used by Rendle et al. (2009). Consider tuples
as users and items as relations. We rank different
users with respect to the same item, while BPR ranks
items with respect to the same user. Also notice that
the BPR objective is an approximation to the per-
relation AUC (area under the ROC curve), and hence
directly correlated to what we want to achieve: well-
ranked tuples per relation.
Note that all parameters are regularized with
quadratic penalty which we omit here for brevity.
</bodyText>
<subsectionHeader confidence="0.831836">
2.5.2 Optimization
</subsectionHeader>
<bodyText confidence="0.990326333333333">
To maximize the objective2 in equation 1 we fol-
low Rendle et al. (2009) and employ Stochastic Gra-
dient Descent (SGD). In particular, in each epoch
</bodyText>
<footnote confidence="0.968706">
2This objective is non-convex for all models excluding the
N model.
</footnote>
<page confidence="0.998458">
77
</page>
<bodyText confidence="0.999894777777778">
we sample |O |facts with replacement from O. For
each sampled fact (r, t+) we then sample a tuple
t− E T such that (r, t−) E� O is not an observed
fact. This gives us |O |fact pairs (f+, f−), and for
each pair we do an SGD update using the corre-
sponding gradients of Objf+�f−. For the F model
the gradients correspond to those presented by Ren-
dle et al. (2009). The remaining gradients are easy
to derive; we omit details for brevity.
</bodyText>
<sectionHeader confidence="0.999978" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999878231707317">
This work extends a previous workshop paper (Yao
et al., 2012a) by introducing the neighborhood and
entity model, by working with the BPR objective,
and by more extensive experiments.
Relational Clustering There is a large body of
work aiming to discover latent relations by clus-
tering surface patterns (Hasegawa et al., 2004;
Shinyama and Sekine, 2006; Kok and Domingos,
2008; Yao et al., 2011; Takamatsu et al., 2011), or
by inducing synonymy relationships between pat-
terns independently of the entities (Yates and Et-
zioni, 2009; Pantel et al., 2007; Lin and Pantel,
2001). Our approach has a fundamentally different
objective: we are not (primarily) interested in clus-
ters of patterns or their semantic representation, but
in predicting patterns where they are not observed.
Moreover, these related methods rely on a symmetric
notion of synonymy in which clustered patterns are
assumed to have the same meaning. Our approach
rejects this assumption in favor of a model which
learns that certain patterns, or combinations thereof,
entail others in one direction, but not necessarily the
other. This is similar in spirit to work on learning
entailment rules (Szpektor et al., 2004; Zanzotto et
al., 2006; Szpektor and Dagan, 2008). However, for
us even entailment rules are just a by-product of our
goal to improve prediction, and it is this goal we di-
rectly optimize for and evaluate.
Matrix Factorization Our approach is also re-
lated to work on factorizing YAGO to predict new
links (Nickel et al., 2012). The primary differences
are that we include surface patterns in our schema,
use a ranking objective, and learn latent vectors for
entities and tuples. Likewise, matrix factorization in
various flavors has received significant attention in
the lexical semantics community, from LSA to re-
cent work on non-negative sparse embeddings (Mur-
phy et al., 2012). In our problem columns corre-
spond to relations, and rows correspond to entity tu-
ples. By contrast, there columns are words, and rows
are contextual features such as “words in a local win-
dow.” Consequently, our objective is to complete
the matrix, whereas their objective is to learn better
latent embeddings of words (which by themselves
again cannot capture any sense of asymmetry).
OpenIE Open IE (Etzioni et al., 2008) extracts
facts mentioned in text, but does not predict poten-
tial facts not mentioned in text. Finding answers
requires explicit mentions, and hence suffers from
lower recall for not-so-frequently mentioned facts.
Methods that learn rules between textual patterns in
OpenIE aim at a similar goal as our proposed ap-
proach (Schoenmackers et al., 2008; Schoenmack-
ers et al., 2010). However, their approach is sub-
stantially more complex, requires a categorization
of entities into fine grained entity types, and needs
inference in high tree-width Markov Networks. By
contrast, our approach is based on a single unified
model, requires no entity types, and for us inferring
a fact amounts to not more than a few dot products.
In addition, in our Universal Schema approach Ope-
nIE surface patterns are just one kind of relations,
and our aim is populate relations of all kinds. In the
future we may even include relations between enti-
ties and continuous attributes (say, gene expression
measurements).
Distant Supervision In Distant Supervision (DS)
a set of facts from pre-existing structured sources
is aligned with surface patterns mentioned in
text (Bunescu and Mooney, 2007; Mintz et al., 2009;
Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu
et al., 2012), and this alignment is then used to train
a relation extractor. A core difference to our ap-
proach is the number of target relations: In DS it
is the relatively small schema size of the knowledge
base, while we also include surface patterns. This
allows us to answer more expressive queries. More-
over, by learning from surface-pattern correlations,
our latent models induce feature representations for
patterns that do not appear in the DS training set. As
we will see in section 4, this allows us to outperform
state-of-the-art DS models.
</bodyText>
<page confidence="0.991282">
78
</page>
<bodyText confidence="0.991633692307692">
Never-Ending Learning and Bootstrapping Our
latent feature models are capable of never-ending
learning (Carlson et al., 2010). That is, we can con-
tinue to train these models with incoming data, even
if no structured annotation is available. In bootstrap-
ping approaches the current model is used to predict
new relations, and these hypothesized relations are
used as new supervision targets (i.e. self-training).
By contrast, our model only strengthens the correla-
tions between incoming co-occurring observations.
This has the advantage that wrong predictions are
less likely be reinforced, hence reducing the risk of
semantic drift.
</bodyText>
<sectionHeader confidence="0.999885" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999672">
How accurately can we fill a database of Universal
Schema, and does reasoning jointly across a uni-
versal schema help to improve over more isolated
approaches? In the following we seek to answer
this question empirically. To this end we train our
models on observed facts in a newswire corpus and
Freebase, and then manually evaluate ranked predic-
tions: first for structured relations and then for sur-
face form relations.
</bodyText>
<subsectionHeader confidence="0.961728">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999921130434783">
Following previous work (Riedel et al., 2010),
our documents are taken from the NYTimes cor-
pus (Sandhaus, 2008). Articles after 2000 are used
as training corpus, articles from 1990 to 1999 as
test corpus. We also split Freebase facts 50/50 into
train and test facts, and their corresponding tuples
into train and test tuples. Then we align training tu-
ples with the training corpus, and test tuples with the
test corpus. This alignment relies on a preprocessing
step that links NER mentions in text with entities in
Freebase. In our case we use a simple string-match
heuristic to find this linking. Now we align an entity
tuple (t1, t2) with a pair of mentions (m1, m2) in
the same sentence if m1 is linked to t1 and m2 to t2.
Based on this alignment we filter out all relations for
which we find fewer than 10 tuples with mentions in
text.
The above alignment and filtering process reduces
the total number of tuples related according to Free-
base to 16k: approximately 8k tuples with facts
mentioned in the training set, and approximately 8k
such tuples for the test set. In addition we have a
set of approximately 200k training tuples for which
both arguments appear in the same sentence and
both can be linked to Freebase entities, but for which
no Freebase fact is recorded. This can either be be-
cause they are not related, or simply because Free-
base does not contain the relationship yet. We also
have about 200k such tuples in the test set. To sim-
plify evaluation, we create a subsampled test set by
randomly choosing 10k of the original test set tuples.
The above alignment allows us to determine, for
each tuple t, the observed facts Ot as follows. To
find the surface pattern facts OPAT
t for the tuple t =
(t1, t2) we extract, for each mention m = (m1, m2)
of t, the lexicalized dependency path p between m1
and m2. Then we add (p, t) to OPAT
t . For example,
we get “&lt;-subj&lt;-head-&gt;obj-&gt;” for “M1 heads M2.”
Filtering out patterns with fewer than 10 mentions
in text yields approximately 4k patterns. For train-
ing tuples we add as Freebase facts OFB
t all facts
(r, t) that appear in Freebase, and for which r has
not been filtered out beforehand. For the test set OFB
</bodyText>
<equation confidence="0.714822666666667">
t
remains empty. The total set of observed facts Ot is
OPBUOPAT
</equation>
<bodyText confidence="0.7665195">
t , and their union over all tuples forms the
set of observed facts O.
</bodyText>
<subsectionHeader confidence="0.86163">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999987666666667">
For evaluation we use collections of relations: sur-
face patterns in one experiment and Freebase re-
lations in the other. In either case we compare
the competing systems with respect to their ranked
results for each relation in the collection. Given
this ranking task, our evaluation is inspired by the
TREC competitions and work in information re-
trieval (Manning et al., 2008). That is, we treat
each relation as query and receive the top 1000 (run
depth) entity pairs from each system. Then we pool
the top 100 (pool depth) answers from each system
and manually judge their relevance or “truth.” This
gives a set of relevant results that we can use to cal-
culate recall and precision measures. In particular,
we can use these annotations to measure an average
precision across the precision-recall curve, and an
aggregate mean average precision (MAP) across all
relations. This metric has shown to be very robust
and stable (Manning et al., 2008). In addition we
also present a weighted version of MAP (weighted
MAP) in which the average precision for each re-
</bodyText>
<page confidence="0.996817">
79
</page>
<table confidence="0.999961681818182">
Relation # MI09 YA11 SU12 N F NF NFE
person/company 103 0.67 0.64 0.70 0.73 0.75 0.76 0.79
location/containedby 74 0.48 0.51 0.54 0.43 0.68 0.67 0.69
author/works_written 29 0.50 0.51 0.52 0.45 0.61 0.63 0.69
person/nationality 28 0.14 0.40 0.13 0.13 0.19 0.18 0.21
parent/child 19 0.14 0.25 0.62 0.46 0.76 0.78 0.76
person/place_of_death 19 0.79 0.79 0.86 0.89 0.83 0.85 0.86
person/place_of_birth 18 0.78 0.75 0.82 0.50 0.83 0.81 0.89
neighborhood/neighborhood_of 12 0.00 0.00 0.08 0.43 0.65 0.66 0.72
person/parents 7 0.24 0.27 0.58 0.56 0.53 0.58 0.39
company/founders 4 0.25 0.25 0.53 0.24 0.77 0.80 0.68
film/directed_by 4 0.06 0.15 0.25 0.09 0.26 0.26 0.30
sports_team/league 4 0.00 0.43 0.18 0.21 0.59 0.70 0.63
team/arena_stadium 3 0.00 0.06 0.06 0.03 0.08 0.09 0.08
team_owner/teams_owned 2 0.00 0.50 0.70 0.55 0.38 0.61 0.75
roadcast/area_served 2 1.00 0.50 1.00 0.58 0.58 0.83 1.00
structure/architect 2 0.00 0.00 1.00 0.27 1.00 1.00 1.00
composer/compositions 2 0.00 0.00 0.00 0.50 0.67 0.83 0.12
person/religion 1 0.00 1.00 1.00 0.50 1.00 1.00 1.00
film/produced_by 1 1.00 1.00 1.00 1.00 0.50 0.50 0.33
MAP 0.32 0.42 0.56 0.45 0.61 0.66 0.63
Weighted MAP 0.48 0.52 0.57 0.52 0.66 0.67 0.69
</table>
<tableCaption confidence="0.813653666666667">
Table 1: Average and (weighted) Mean Average Precisions for Freebase relations based on pooled results. The #
column shows the number of true facts in the pool. NFE is statistically different to all but NF and F according to the
sign test. Bold faced are winners per relation, italics indicate ties.
</tableCaption>
<bodyText confidence="0.999913826086956">
lation is weighted by the relation’s number of true
facts.
Notice that we deviate from previous work in dis-
tant supervision that (a) combines the results from
several relations in a single precision recall curve,
and (b) uses held-out evaluation to measure how
well the predictions match existing Freebase facts.
This has several benefits. First, when aggregating
across relations results are often dominated by a few
very frequent relations, such as containedby, provid-
ing little information about how the models perform
across the board. Second, evaluating with Freebase
held-out data is biased. For example, we find that
frequently mentioned entity pairs are more likely to
have relations in Freebase. Systems that rank such
tuples higher receives higher precision than those
that do not have such bias, regardless of how cor-
rect their predictions are. Third, we can aggregate
per-relation comparisons to establish statistical sig-
nificance, for example via the sign test.
Also note that while we run our models on the
complete training and test set, evaluation is re-
stricted to the subsampled test set.
</bodyText>
<subsectionHeader confidence="0.999737">
4.3 Predicting Freebase Relations
</subsectionHeader>
<bodyText confidence="0.9998733125">
Table 1 shows our results for Freebase relations,
omitting those for which none of the systems can
find any relevant facts. Our first baseline is MI09,
a distantly supervised classifier based on the work
of Mintz et al. (2009). This classifier only learns
from observed pattern-relation pairs in the training
set (of which we only have about 8k). By contrast,
our latent feature models can learn pattern-pattern
correlations both on the unlabeled training and test
set (comparable to bootstrapping). We hence also
compare against YA11, a version of MI09 that uses
preprocessed cluster features according to Yao et al.
(2011). The third baseline is SU12, the state-of-the-
art Multi-Instance Multi-Label system by Surdeanu
et al. (2012).
The remaining systems are our neighborhood
</bodyText>
<page confidence="0.97668">
80
</page>
<figure confidence="0.981773333333333">
Averaged 11-point Precision/Recall
0 0.2 0.4 0.6 0.8 1
Recall
</figure>
<figureCaption confidence="0.99394">
Figure 2: Averaged 11-point precision recall curve for
Freebase relations in table 1.
</figureCaption>
<bodyText confidence="0.999946060606061">
model (N), the factorized model (F), their combi-
nation (NF) and the combined model with a latent
entity representation (NFE). For all our models we
use the same number of components when applica-
ble (KF = KE = 100), 1000 epochs, and 0.01 as
regularizer for component weights and 0.1 for neigh-
borhood weights.
Table 1 shows that adding pattern cluster features
(and hence incorporating more data) helps YA11
to improve over MI09. Likewise, we see that the
factorized model F improves over N, again learn-
ing from unlabeled data. This improvement is big-
ger than the corresponding change between MI09
and YA11, possibly indicating that our latent rep-
resentations are optimized directly towards improv-
ing prediction performance. The combination of N,
F and E outperforms all other models in terms of
weighted MAP, indicating the power of selectional
preferences learned from data. Note that NFE is
significantly different (p « 0.05 in sign test) to all
but the NF and F models. In terms of MAP the NF
model outperforms NFE, indicating that it does not
do as well for frequent relations, but better for infre-
quent ones.
Figure 2 shows an averaged 11-point precision re-
call graph (Manning et al., 2008) for Freebase re-
lations. We notice that our latent models outper-
form all remaining models across all recall levels,
and that combining neighborhood and latent models
is helpful. This finding is consistent with our MAP
results. Figure 3 shows the recall-precision curve for
the works_written relation with respect to our three
baselines and the NFE model. Observe how preci-
</bodyText>
<figure confidence="0.861135333333333">
Recall/Precision
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
</figure>
<figureCaption confidence="0.998513">
Figure 3: Precision and recall for works_written(X,Y).
</figureCaption>
<table confidence="0.999866538461538">
Relation # N F NF NFE
visit 80 0.19 0.68 0.49 0.42
attend 69 0.23 0.10 0.07 0.10
base 61 0.46 0.87 0.81 0.68
head 38 0.47 0.67 0.70 0.68
scientist 36 0.25 0.84 0.79 0.73
support 18 0.16 0.29 0.32 0.38
adviser 11 0.19 0.15 0.19 0.28
criticize 9 0.09 0.60 0.67 0.64
praise 4 0.01 0.03 0.05 0.10
vote 3 0.18 0.18 0.34 0.34
MAP 0.22 0.44 0.44 0.43
Weighted MAP 0.28 0.56 0.50 0.46
</table>
<tableCaption confidence="0.9980815">
Table 2: Average and (weighted) Mean Average Preci-
sions for surface patterns.2
</tableCaption>
<bodyText confidence="0.999910071428571">
sion drops for both MI09 and SU12 at about 50%
recall. At this point the remaining unretrieved facts
have patterns that have not been seen together with
works_written in the training set. By using cluster
features, YA11 can overcome this problem partly,
but not as dramatically as NFE—a pattern we ob-
serve for many relations.
All our models are fast to train. The slowest
model trains in just 45 minutes. By contrast, training
the topic model in YA11 alone takes 4 hours. Train-
ing SU12 takes two hours (on less data). Also notice
that our models not only learn to predict Freebase
relations, but also approximately 4k surface pattern
relations.
</bodyText>
<subsectionHeader confidence="0.999697">
4.4 Predicting Surface Patterns
</subsectionHeader>
<bodyText confidence="0.9637865">
Table 2 presents a comparison of our models with re-
spect to 10 surface pattern relations. These relations
</bodyText>
<figure confidence="0.985859258064516">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
1
MI09
YA11
SU12
N
F
NF
NFE
0.8
0.6
0.4
0.2
0
1
MI09
YA11
SU12
NFE
81
Averaged 11-point Precision/Recall
0 0.2 0.4 0.6 0.8 1
Recall
</figure>
<figureCaption confidence="0.998374">
Figure 4: Averaged 11-point precision recall curve for
surface pattern relations in table 2.
</figureCaption>
<bodyText confidence="0.999905260869565">
were chosen according to what we believe are inter-
esting questions not currently captured in Freebase.
We again see that learning a latent representation (F,
NF and NFE) from additional data helps quite sub-
stantially over the N model. For in the weighted
MAP metric we note that incorporating entity rep-
resentations (in the NFE model) in fact hurts total
performance.3 One reason may be the fact that Free-
base relations are typed—they require very specific
types of entities as arguments. By contrast, for a
surface pattern like “X visits Y” X could be a person
or organization, and Y could be a location, organi-
zation or person. However, in terms of MAP score
this time there is no obvious winner among the la-
tent models. This is also confirmed by the averaged
11-point precision recall curve in figure 4.
Notice that we can accurately predict the X–
scientist-at–Y surface pattern relation in table 2,
as well as the more general person/company (em-
ployedBy) relation in table 1. This indicates that
our models can capture asymmetry—a symmetric
model would either over-predict X–scientist-at–Y
or under-predict person/company.
</bodyText>
<sectionHeader confidence="0.9993" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999902666666667">
We present relation extraction into universal
schemas. Such schemas contain surface patterns
as relations, as well as relations from structured
sources. By predicting missing tuples for surface
pattern relations we can populate a database with-
out any labelled data, and answer questions not sup-
</bodyText>
<footnote confidence="0.5707895">
3Due to the small set of relations only N is significantly dif-
ferent to F, NF and NFE (p « 0.05 in sign test).
</footnote>
<bodyText confidence="0.999375862068965">
ported by the structured schema alone. By predict-
ing missing tuples in the structured schema we can
expand a knowledge base of fixed schema, and only
require a set of existing facts from this schema. Cru-
cially, by predicting and modeling both surface pat-
terns and structured relations simultaneously we can
improve performance. We show this experimentally
by contrasting a series of the popular weakly super-
vised models to our collaborative filtering models
that learn latent feature representations across sur-
face patterns and structured relations. Moreover, our
models are computationally efficient, requiring less
time than comparable methods, while learning more
relations.
Reasoning with universal schemas is not merely a
tool for information extraction. It can also serve as
a framework for various data integration tasks. For
example, we could integrate facts from one schema
(say, Freebase) into another (say, the TAC KBP
schema) by adding both sets of relations to the set
of surface patterns. Reasoning with this schema
will mean populating each database with facts from
the other, and would leverage information in surface
patterns to improve integration. In future work we
also plan to integrate universal entity types and at-
tributes into the model.
The source code of our system, its output, and
all data annotations are available at http://www.
riedelcastro.org/uschema.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999996181818182">
We thank the reviewers for very helpful comments.
This work was supported in part by the Center for In-
telligent Information Retrieval and the University of
Massachusetts, in part by UPenn NSF medium IIS-
0803847, in part by DARPA under agreement num-
ber FA8750-13-2-0020 and FA8750-09-C-0181, and
in part by an award from Google. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the view of DARPA,
AFRL, or the US government.
</bodyText>
<sectionHeader confidence="0.998869" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.972243333333333">
Ronald J. Brachman. 1983. What is-a is and isn t:
An analysis of taxonomic links in semantic networks.
IEEE Computer, 16(10):30–36.
</reference>
<figure confidence="0.995889">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
N
F
NF
NFE
</figure>
<page confidence="0.982917">
82
</page>
<note confidence="0.7527695">
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using min-
</note>
<reference confidence="0.996132552380952">
imal supervision. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ’07).
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka, and Tom M. Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In Proceedings of the 25th AAAI Con-
ference on Artificial Intelligence (AAAI ’10).
Michael Collins, Sanjoy Dasgupta, and Robert E.
Schapire. 2001. A generalization of principal com-
ponent analysis to the exponential family. In Proceed-
ings of NIPS.
M. Craven and J. Kumlien. 1999. Constructing biolog-
ical knowledge-bases by extracting information from
text sources. In Proceedings of the Seventh Interna-
tional Conference on Intelligent Systems for Molecular
Biology, pages 77–86, Germany.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, Barcelona, Spain.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68–74.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Dis-
covering Relations among Named Entities from Large
Corpora. Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL
’04), pages 415–422.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of ACL.
Stanley Kok and Pedro Domingos. 2008. Extracting Se-
mantic Networks from Text Via Relational Clustering.
In ECML.
Yehuda Koren. 2008. Factorization meets the neighbor-
hood: a multifaceted collaborative filtering model. In
Proceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ’08, pages 426–434, New York, NY, USA.
ACM.
Dekang Lin and Patrick Pantel. 2001. DIRT - discovery
of inference rules from text. In Knowledge Discovery
and Data Mining, pages 323–328.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schütze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, UK.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP (ACL ’09),
pages 1003–1011. Association for Computational Lin-
guistics.
Brian Murphy, Partha Pratim Talukdar, and Tom
Mitchell. 2012. Learning effective and interpretable
semantic models using non-negative sparse embed-
ding. In COLING, pages 1933–1950.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing yago: scalable machine
learning for linked data. In Proceedings of the 21st
international conference on World Wide Web, WWW
’12, pages 271–280, New York, NY, USA. ACM.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning Inferential Selectional Preferences. In Pro-
ceedings of NAACL HLT.
Steffen Rendle, Christoph Freudenthaler, Zeno Gantner,
and Lars Schmidt-Thieme. 2009. Bpr: Bayesian per-
sonalized ranking from implicit feedback. In Proceed-
ings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence, UAI ’09, pages 452–461, Ar-
lington, Virginia, United States. AUAI Press.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ’10).
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling textual inference to the web.
In EMNLP ’08: Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 79–88, Morristown, NJ, USA. Association for
Computational Linguistics.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’10, pages 1088–1098,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, HLT-NAACL ’06, pages 304–
311, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and
Christopher D. Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings
</reference>
<page confidence="0.986747">
83
</page>
<reference confidence="0.997571717948718">
of the Conference on Empirical methods in natural
language processing (EMNLP ’12), pages 455–465.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, COLING ’08, pages 849–856,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2011. Probabilistic matrix factorization leveraging
contexts for unsupervised relation discovery. In Pro-
ceedings of PAKDD.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ’11), July.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012a. Probabilistic databases of universal schema.
In Proceedings of the AKBC-WEKEX Workshop at
NAACL 2012, June.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012b. Unsupervised relation discovery with sense
disambiguation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ’12), July.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal ofArtificial Intelligence Research,
34:255–296.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using selec-
tional preferences. In Proceedings of the 44th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ’06).
</reference>
<page confidence="0.99923">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953432">
<title confidence="0.999989">Relation Extraction with Matrix Factorization and Universal Schemas</title>
<author confidence="0.999963">Sebastian Riedel Limin Yao</author>
<author confidence="0.999963">Andrew McCallum</author>
<author confidence="0.999963">Benjamin M Marlin</author>
<affiliation confidence="0.9921615">Department of Computer Science Department of Computer Science University College London University of Massachusetts at Amherst</affiliation>
<email confidence="0.998424">s.riedel@ucl.ac.uk{lmyao,mccallum,marlin}@cs.umass.edu</email>
<abstract confidence="0.998883571428572">Traditional relation extraction predicts relations within some fixed and finite target schema. Machine learning approaches to this task require either manual annotation or, in the case of distant supervision, existing structured sources of the same schema. The need for existing datasets can be avoided by usa the union of all involved schemas (surface form predicates as in OpenIE, and relations in the schemas of preexisting databases). This schema has an almost unlimited set of relations (due to surface forms), and supports integration with existing structured data (through the relation types of existing databases). To populate a database of such schema we present matrix factorization models that learn latent feature vectors for entity tuples and relations. We show that such latent models achieve substantially higher accuracy than a traditional classification approach. More importantly, by operating simultaneously on relations observed in text and in pre-existing structured DBs such as Freebase, we are able to reason about unstructured and structured data in mutually-supporting ways. By doing so our approach outperforms stateof-the-art distant supervision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ronald J Brachman</author>
</authors>
<title>What is-a is and isn t: An analysis of taxonomic links in semantic networks.</title>
<date>1983</date>
<journal>IEEE Computer,</journal>
<volume>16</volume>
<issue>10</issue>
<contexts>
<context position="6131" citStr="Brachman (1983)" startWordPosition="919" endWordPosition="920">eatures when predicting Freebase relations, hence improving generalization. Also notice that users of our system will not have to study and understand the complexities of a particular schema in order to issue queries; they can ask in whatever form naturally occurs to them, and our system will likely already have that relation in our universal schema. Our technical approach is based on extensions to probabilistic models of matrix factorization and 1At NAACL 2012 Lucy Vanderwende asked “Where do the relation types come from?” There was no satisfying answer. At the same meeting, and in line with Brachman (1983), Ed Hovy stated “We don’t even know what is-a means.” collaborative filtering (Collins et al., 2001; Koren, 2008; Rendle et al., 2009). We represent the probabilistic knowledge base as a matrix with entityentity pairs in the rows and relations in the columns (see figure 1). The rows come from running crossdocument entity resolution across pre-existing structured databases and textual corpora. The columns come from the union of surface forms and DB relations. We present a series of models that learn lower dimensional manifolds for tuples, relations and entities, and a set of weights that captu</context>
</contexts>
<marker>Brachman, 1983</marker>
<rawString>Ronald J. Brachman. 1983. What is-a is and isn t: An analysis of taxonomic links in semantic networks. IEEE Computer, 16(10):30–36.</rawString>
</citation>
<citation valid="false">
<title>imal supervision.</title>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL ’07).</booktitle>
<marker></marker>
<rawString>imal supervision. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL ’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for never-ending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI ’10).</booktitle>
<contexts>
<context position="20398" citStr="Carlson et al., 2010" startWordPosition="3328" endWordPosition="3331">lation extractor. A core difference to our approach is the number of target relations: In DS it is the relatively small schema size of the knowledge base, while we also include surface patterns. This allows us to answer more expressive queries. Moreover, by learning from surface-pattern correlations, our latent models induce feature representations for patterns that do not appear in the DS training set. As we will see in section 4, this allows us to outperform state-of-the-art DS models. 78 Never-Ending Learning and Bootstrapping Our latent feature models are capable of never-ending learning (Carlson et al., 2010). That is, we can continue to train these models with incoming data, even if no structured annotation is available. In bootstrapping approaches the current model is used to predict new relations, and these hypothesized relations are used as new supervision targets (i.e. self-training). By contrast, our model only strengthens the correlations between incoming co-occurring observations. This has the advantage that wrong predictions are less likely be reinforced, hence reducing the risk of semantic drift. 4 Experiments How accurately can we fill a database of Universal Schema, and does reasoning </context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Hruschka, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka, and Tom M. Mitchell. 2010. Toward an architecture for never-ending language learning. In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Sanjoy Dasgupta</author>
<author>Robert E Schapire</author>
</authors>
<title>A generalization of principal component analysis to the exponential family.</title>
<date>2001</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="6231" citStr="Collins et al., 2001" startWordPosition="933" endWordPosition="936">ers of our system will not have to study and understand the complexities of a particular schema in order to issue queries; they can ask in whatever form naturally occurs to them, and our system will likely already have that relation in our universal schema. Our technical approach is based on extensions to probabilistic models of matrix factorization and 1At NAACL 2012 Lucy Vanderwende asked “Where do the relation types come from?” There was no satisfying answer. At the same meeting, and in line with Brachman (1983), Ed Hovy stated “We don’t even know what is-a means.” collaborative filtering (Collins et al., 2001; Koren, 2008; Rendle et al., 2009). We represent the probabilistic knowledge base as a matrix with entityentity pairs in the rows and relations in the columns (see figure 1). The rows come from running crossdocument entity resolution across pre-existing structured databases and textual corpora. The columns come from the union of surface forms and DB relations. We present a series of models that learn lower dimensional manifolds for tuples, relations and entities, and a set of weights that capture direct correlations between relations. Weights and lower dimensional representations act, through</context>
<context position="9952" citStr="Collins et al., 2001" startWordPosition="1566" endWordPosition="1569">n be estimated based on the observed facts O. Notice that we can interpret p (yr,t = 1) as the probability that a customer t likes product r. This analogy allows us to draw from a large body of work in collaborative filtering, such as work in probabilistic matrix factorization and implicit feedback. 2.1 Latent Feature Model One way to define θr,t is through a latent feature model F. Here we measure compatibility between relation r and tuple t as dot product of two latent feature representations of size KF: ar for relation r, and vt for tuple t. This gives: This corresponds to generalized PCA (Collins et al., 2001), a model were the matrix O = (θr,t) of natural parameters is defined as the low rank factorization AV. Notice that we intentionally omit any per-relation bias-terms. In section 4 we evaluate ranked answers to queries on a per-relation basis, and a per-relation bias term will have no effect on ranking facts of the same relation. Also consider that such latent feature models can capture asymmetry by assigning more peaked vectors to specific relations, and more uniform vectors to general relations. 2.2 Neighborhood Model We can interpolate the confidence for a given tuple and relation based on t</context>
<context position="13218" citStr="Collins et al. (2001)" startWordPosition="2119" endWordPosition="2122">epresentation and the corresponding entity representation. This leads to: BE . arity(r) X KE di,kttz,k. r,t •_ X k i=1 Note that due to entity resolution, tuples may share entities, and hence parameters are shared across rows. 2.4 Combined Model In practice all the above models can capture important aspects of the data. Hence we also use various combinations, such as: NFE N F E Br,t := BN + BF + Br,t. 2.5 Parameter Estimation Our models are parametrized through weights and latent component vectors. We could estimate these parameters by maximizing the loglikelihood of the observed data akin to Collins et al. (2001). However, as we do not have access to negative facts, the model would simply learn to predict all facts to be true. In our initial attempt to overcome this issue we sampled a set of unobserved facts as designated negative facts, as is done in related distant supervision approaches. However, we found that (a) our results were sensitive to the choice of negative data and (b) runtime was increased substantially because of a large number of required negative facts. In collaborative filtering positive-only data is also known as implicit feedback. This type of feedback arises, for example, when use</context>
</contexts>
<marker>Collins, Dasgupta, Schapire, 2001</marker>
<rawString>Michael Collins, Sanjoy Dasgupta, and Robert E. Schapire. 2001. A generalization of principal component analysis to the exponential family. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Craven</author>
<author>J Kumlien</author>
</authors>
<title>Constructing biological knowledge-bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology,</booktitle>
<pages>77--86</pages>
<contexts>
<context position="2237" citStr="Craven and Kumlien, 1999" startWordPosition="326" endWordPosition="329">schema of relation types (such as born-in or employed-by). Usually some textual data is labeled according to this schema, and this labeling is then used in supervised training of an automated relation extractor, e.g. Culotta and Sorensen (2004). However, labeling textual rela74 tions is time-consuming and difficult, leading to significant recent interest in distantly-supervised learning. Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). However, this method relies on the availability of a large database that has the desired schema. The need for pre-existing datasets can be avoided by using language itself as the source of the schema. This is the approach taken by OpenIE (Etzioni et al., 2008). Here surface patterns between mentions of concepts serve as relations. This approach requires no supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find FERGUSON–historian-at–HARVARD but does not know FERGUSON–is-a</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>M. Craven and J. Kumlien. 1999. Constructing biological knowledge-bases by extracting information from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology, pages 77–86, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffery Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1857" citStr="Culotta and Sorensen (2004)" startWordPosition="270" endWordPosition="273">e importantly, by operating simultaneously on relations observed in text and in pre-existing structured DBs such as Freebase, we are able to reason about unstructured and structured data in mutually-supporting ways. By doing so our approach outperforms stateof-the-art distant supervision. 1 Introduction Most previous work in relation extraction uses a predefined, finite and fixed schema of relation types (such as born-in or employed-by). Usually some textual data is labeled according to this schema, and this labeling is then used in supervised training of an automated relation extractor, e.g. Culotta and Sorensen (2004). However, labeling textual rela74 tions is time-consuming and difficult, leading to significant recent interest in distantly-supervised learning. Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). However, this method relies on the availability of a large database that has the desired schema. The need for pre-existing datasets can be avoided by </context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffery Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of ACL, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
</authors>
<date>2008</date>
<booktitle>Open information extraction from the web. Commun. ACM,</booktitle>
<pages>51--12</pages>
<contexts>
<context position="2567" citStr="Etzioni et al., 2008" startWordPosition="384" endWordPosition="387">ificant recent interest in distantly-supervised learning. Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). However, this method relies on the availability of a large database that has the desired schema. The need for pre-existing datasets can be avoided by using language itself as the source of the schema. This is the approach taken by OpenIE (Etzioni et al., 2008). Here surface patterns between mentions of concepts serve as relations. This approach requires no supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find FERGUSON–historian-at–HARVARD but does not know FERGUSON–is-a-professor-at– HARVARD. OpenIE has traditionally relied on a large diversity of textual expressions to provide good coverage. But this diversity is not always available, and, in any case, the lack of generalization greatly inhibits the ability to support reasoning. One way to gain generalization is to cluster textual surface for</context>
<context position="18471" citStr="Etzioni et al., 2008" startWordPosition="3019" endWordPosition="3022">kewise, matrix factorization in various flavors has received significant attention in the lexical semantics community, from LSA to recent work on non-negative sparse embeddings (Murphy et al., 2012). In our problem columns correspond to relations, and rows correspond to entity tuples. By contrast, there columns are words, and rows are contextual features such as “words in a local window.” Consequently, our objective is to complete the matrix, whereas their objective is to learn better latent embeddings of words (which by themselves again cannot capture any sense of asymmetry). OpenIE Open IE (Etzioni et al., 2008) extracts facts mentioned in text, but does not predict potential facts not mentioned in text. Finding answers requires explicit mentions, and hence suffers from lower recall for not-so-frequently mentioned facts. Methods that learn rules between textual patterns in OpenIE aim at a similar goal as our proposed approach (Schoenmackers et al., 2008; Schoenmackers et al., 2010). However, their approach is substantially more complex, requires a categorization of entities into fine grained entity types, and needs inference in high tree-width Markov Networks. By contrast, our approach is based on a </context>
</contexts>
<marker>Etzioni, Banko, Soderland, Weld, 2008</marker>
<rawString>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S. Weld. 2008. Open information extraction from the web. Commun. ACM, 51(12):68–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hasegawa</author>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>Discovering Relations among Named Entities from Large Corpora.</title>
<date>2004</date>
<booktitle>Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL ’04),</booktitle>
<pages>415--422</pages>
<contexts>
<context position="16513" citStr="Hasegawa et al., 2004" startWordPosition="2702" endWordPosition="2705">his gives us |O |fact pairs (f+, f−), and for each pair we do an SGD update using the corresponding gradients of Objf+�f−. For the F model the gradients correspond to those presented by Rendle et al. (2009). The remaining gradients are easy to derive; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approa</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discovering Relations among Named Entities from Large Corpora. Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL ’04), pages 415–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="19706" citStr="Hoffmann et al., 2011" startWordPosition="3216" endWordPosition="3219"> model, requires no entity types, and for us inferring a fact amounts to not more than a few dot products. In addition, in our Universal Schema approach OpenIE surface patterns are just one kind of relations, and our aim is populate relations of all kinds. In the future we may even include relations between entities and continuous attributes (say, gene expression measurements). Distant Supervision In Distant Supervision (DS) a set of facts from pre-existing structured sources is aligned with surface patterns mentioned in text (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), and this alignment is then used to train a relation extractor. A core difference to our approach is the number of target relations: In DS it is the relatively small schema size of the knowledge base, while we also include surface patterns. This allows us to answer more expressive queries. Moreover, by learning from surface-pattern correlations, our latent models induce feature representations for patterns that do not appear in the DS training set. As we will see in section 4, this allows us to outperform state-of-the-art DS models. 78 Never-Ending Learning and Bootstr</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kok</author>
<author>Pedro Domingos</author>
</authors>
<title>Extracting Semantic Networks from Text Via Relational Clustering.</title>
<date>2008</date>
<booktitle>In ECML.</booktitle>
<contexts>
<context position="16564" citStr="Kok and Domingos, 2008" startWordPosition="2710" endWordPosition="2713"> pair we do an SGD update using the corresponding gradients of Objf+�f−. For the F model the gradients correspond to those presented by Rendle et al. (2009). The remaining gradients are easy to derive; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model whic</context>
</contexts>
<marker>Kok, Domingos, 2008</marker>
<rawString>Stanley Kok and Pedro Domingos. 2008. Extracting Semantic Networks from Text Via Relational Clustering. In ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yehuda Koren</author>
</authors>
<title>Factorization meets the neighborhood: a multifaceted collaborative filtering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08,</booktitle>
<pages>426--434</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6244" citStr="Koren, 2008" startWordPosition="937" endWordPosition="938"> not have to study and understand the complexities of a particular schema in order to issue queries; they can ask in whatever form naturally occurs to them, and our system will likely already have that relation in our universal schema. Our technical approach is based on extensions to probabilistic models of matrix factorization and 1At NAACL 2012 Lucy Vanderwende asked “Where do the relation types come from?” There was no satisfying answer. At the same meeting, and in line with Brachman (1983), Ed Hovy stated “We don’t even know what is-a means.” collaborative filtering (Collins et al., 2001; Koren, 2008; Rendle et al., 2009). We represent the probabilistic knowledge base as a matrix with entityentity pairs in the rows and relations in the columns (see figure 1). The rows come from running crossdocument entity resolution across pre-existing structured databases and textual corpora. The columns come from the union of surface forms and DB relations. We present a series of models that learn lower dimensional manifolds for tuples, relations and entities, and a set of weights that capture direct correlations between relations. Weights and lower dimensional representations act, through dot products</context>
<context position="10704" citStr="Koren, 2008" startWordPosition="1691" endWordPosition="1692">y per-relation bias-terms. In section 4 we evaluate ranked answers to queries on a per-relation basis, and a per-relation bias term will have no effect on ranking facts of the same relation. Also consider that such latent feature models can capture asymmetry by assigning more peaked vectors to specific relations, and more uniform vectors to general relations. 2.2 Neighborhood Model We can interpolate the confidence for a given tuple and relation based on the trueness of other similar relations for the same tuple. In collaborative filtering this is referred to as a neighborhood-based approach (Koren, 2008). In terms of our natural parameter, we implement a neighborhood model N via a set of weights wr,ri, where each corresponds to a directed association strength between relations r and r&apos;. For a given tuple t and relation r we then sum up the weights corresponding to all relations r&apos; that have been observed for tuple t: θN r,t := � wr,ri. (ri,t)EO\{(r,t)} Notice that the neighborhood model amounts to a collection of local log-linear classifiers, one for each relation r with feature functions fr,r, (t) = ff [r&apos; =� r n (r&apos;, t) E O] and weights wr. This means that in contrast to model F, this model</context>
</contexts>
<marker>Koren, 2008</marker>
<rawString>Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08, pages 426–434, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT - discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Knowledge Discovery and Data Mining,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="3217" citStr="Lin and Pantel, 2001" startWordPosition="479" endWordPosition="482">n mentions of concepts serve as relations. This approach requires no supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find FERGUSON–historian-at–HARVARD but does not know FERGUSON–is-a-professor-at– HARVARD. OpenIE has traditionally relied on a large diversity of textual expressions to provide good coverage. But this diversity is not always available, and, in any case, the lack of generalization greatly inhibits the ability to support reasoning. One way to gain generalization is to cluster textual surface forms that have similar meaning (Lin and Pantel, 2001; Pantel et al., 2007; Yates and Etzioni, 2009; Yao et al., 2011). While the clusters discovered by all these methods usually contain semantically related items, closer inspection invariably shows that they do not provide reliable implicature. For example, a typical representative cluster may include historian-at, professor-at, scientistat, worked-at. Although these relation types are indeed semantically related, note that scientist-at does not necessarily imply professor-at, and worked-at Proceedings of NAACL-HLT 2013, pages 74–84, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Comp</context>
<context position="16762" citStr="Lin and Pantel, 2001" startWordPosition="2743" endWordPosition="2746">e; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. This is similar in spirit to work on learning entailment rules (Szpektor et al</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT - discovery of inference rules from text. In Knowledge Discovery and Data Mining, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Schütze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="24053" citStr="Manning et al., 2008" startWordPosition="3965" endWordPosition="3968"> appear in Freebase, and for which r has not been filtered out beforehand. For the test set OFB t remains empty. The total set of observed facts Ot is OPBUOPAT t , and their union over all tuples forms the set of observed facts O. 4.2 Evaluation For evaluation we use collections of relations: surface patterns in one experiment and Freebase relations in the other. In either case we compare the competing systems with respect to their ranked results for each relation in the collection. Given this ranking task, our evaluation is inspired by the TREC competitions and work in information retrieval (Manning et al., 2008). That is, we treat each relation as query and receive the top 1000 (run depth) entity pairs from each system. Then we pool the top 100 (pool depth) answers from each system and manually judge their relevance or “truth.” This gives a set of relevant results that we can use to calculate recall and precision measures. In particular, we can use these annotations to measure an average precision across the precision-recall curve, and an aggregate mean average precision (MAP) across all relations. This metric has shown to be very robust and stable (Manning et al., 2008). In addition we also present </context>
<context position="29506" citStr="Manning et al., 2008" startWordPosition="4859" endWordPosition="4862">ng change between MI09 and YA11, possibly indicating that our latent representations are optimized directly towards improving prediction performance. The combination of N, F and E outperforms all other models in terms of weighted MAP, indicating the power of selectional preferences learned from data. Note that NFE is significantly different (p « 0.05 in sign test) to all but the NF and F models. In terms of MAP the NF model outperforms NFE, indicating that it does not do as well for frequent relations, but better for infrequent ones. Figure 2 shows an averaged 11-point precision recall graph (Manning et al., 2008) for Freebase relations. We notice that our latent models outperform all remaining models across all recall levels, and that combining neighborhood and latent models is helpful. This finding is consistent with our MAP results. Figure 3 shows the recall-precision curve for the works_written relation with respect to our three baselines and the NFE model. Observe how preciRecall/Precision 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Figure 3: Precision and recall for works_written(X,Y). Relation # N F NF NFE visit 80 0.19 0.68 0.49 0.42 attend 69 0.23 0.10 0.07 0.10 base 61 0.46 0.87 0.81 0.68 </context>
</contexts>
<marker>Manning, Raghavan, Schütze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL ’09),</booktitle>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2257" citStr="Mintz et al., 2009" startWordPosition="330" endWordPosition="333">such as born-in or employed-by). Usually some textual data is labeled according to this schema, and this labeling is then used in supervised training of an automated relation extractor, e.g. Culotta and Sorensen (2004). However, labeling textual rela74 tions is time-consuming and difficult, leading to significant recent interest in distantly-supervised learning. Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). However, this method relies on the availability of a large database that has the desired schema. The need for pre-existing datasets can be avoided by using language itself as the source of the schema. This is the approach taken by OpenIE (Etzioni et al., 2008). Here surface patterns between mentions of concepts serve as relations. This approach requires no supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find FERGUSON–historian-at–HARVARD but does not know FERGUSON–is-a-professor-at– HARVA</context>
<context position="19662" citStr="Mintz et al., 2009" startWordPosition="3208" endWordPosition="3211">our approach is based on a single unified model, requires no entity types, and for us inferring a fact amounts to not more than a few dot products. In addition, in our Universal Schema approach OpenIE surface patterns are just one kind of relations, and our aim is populate relations of all kinds. In the future we may even include relations between entities and continuous attributes (say, gene expression measurements). Distant Supervision In Distant Supervision (DS) a set of facts from pre-existing structured sources is aligned with surface patterns mentioned in text (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), and this alignment is then used to train a relation extractor. A core difference to our approach is the number of target relations: In DS it is the relatively small schema size of the knowledge base, while we also include surface patterns. This allows us to answer more expressive queries. Moreover, by learning from surface-pattern correlations, our latent models induce feature representations for patterns that do not appear in the DS training set. As we will see in section 4, this allows us to outperform state-of-the-art DS </context>
<context position="27609" citStr="Mintz et al. (2009)" startWordPosition="4549" endWordPosition="4552">ves higher precision than those that do not have such bias, regardless of how correct their predictions are. Third, we can aggregate per-relation comparisons to establish statistical significance, for example via the sign test. Also note that while we run our models on the complete training and test set, evaluation is restricted to the subsampled test set. 4.3 Predicting Freebase Relations Table 1 shows our results for Freebase relations, omitting those for which none of the systems can find any relevant facts. Our first baseline is MI09, a distantly supervised classifier based on the work of Mintz et al. (2009). This classifier only learns from observed pattern-relation pairs in the training set (of which we only have about 8k). By contrast, our latent feature models can learn pattern-pattern correlations both on the unlabeled training and test set (comparable to bootstrapping). We hence also compare against YA11, a version of MI09 that uses preprocessed cluster features according to Yao et al. (2011). The third baseline is SU12, the state-of-theart Multi-Instance Multi-Label system by Surdeanu et al. (2012). The remaining systems are our neighborhood 80 Averaged 11-point Precision/Recall 0 0.2 0.4 </context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL ’09), pages 1003–1011. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Murphy</author>
<author>Partha Pratim Talukdar</author>
<author>Tom Mitchell</author>
</authors>
<title>Learning effective and interpretable semantic models using non-negative sparse embedding.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>1933--1950</pages>
<contexts>
<context position="18048" citStr="Murphy et al., 2012" startWordPosition="2948" endWordPosition="2952">, for us even entailment rules are just a by-product of our goal to improve prediction, and it is this goal we directly optimize for and evaluate. Matrix Factorization Our approach is also related to work on factorizing YAGO to predict new links (Nickel et al., 2012). The primary differences are that we include surface patterns in our schema, use a ranking objective, and learn latent vectors for entities and tuples. Likewise, matrix factorization in various flavors has received significant attention in the lexical semantics community, from LSA to recent work on non-negative sparse embeddings (Murphy et al., 2012). In our problem columns correspond to relations, and rows correspond to entity tuples. By contrast, there columns are words, and rows are contextual features such as “words in a local window.” Consequently, our objective is to complete the matrix, whereas their objective is to learn better latent embeddings of words (which by themselves again cannot capture any sense of asymmetry). OpenIE Open IE (Etzioni et al., 2008) extracts facts mentioned in text, but does not predict potential facts not mentioned in text. Finding answers requires explicit mentions, and hence suffers from lower recall fo</context>
</contexts>
<marker>Murphy, Talukdar, Mitchell, 2012</marker>
<rawString>Brian Murphy, Partha Pratim Talukdar, and Tom Mitchell. 2012. Learning effective and interpretable semantic models using non-negative sparse embedding. In COLING, pages 1933–1950.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Nickel</author>
<author>Volker Tresp</author>
<author>Hans-Peter Kriegel</author>
</authors>
<title>Factorizing yago: scalable machine learning for linked data.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web, WWW ’12,</booktitle>
<pages>271--280</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="17695" citStr="Nickel et al., 2012" startWordPosition="2895" endWordPosition="2898">d to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. This is similar in spirit to work on learning entailment rules (Szpektor et al., 2004; Zanzotto et al., 2006; Szpektor and Dagan, 2008). However, for us even entailment rules are just a by-product of our goal to improve prediction, and it is this goal we directly optimize for and evaluate. Matrix Factorization Our approach is also related to work on factorizing YAGO to predict new links (Nickel et al., 2012). The primary differences are that we include surface patterns in our schema, use a ranking objective, and learn latent vectors for entities and tuples. Likewise, matrix factorization in various flavors has received significant attention in the lexical semantics community, from LSA to recent work on non-negative sparse embeddings (Murphy et al., 2012). In our problem columns correspond to relations, and rows correspond to entity tuples. By contrast, there columns are words, and rows are contextual features such as “words in a local window.” Consequently, our objective is to complete the matrix</context>
</contexts>
<marker>Nickel, Tresp, Kriegel, 2012</marker>
<rawString>Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2012. Factorizing yago: scalable machine learning for linked data. In Proceedings of the 21st international conference on World Wide Web, WWW ’12, pages 271–280, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Rahul Bhagat</author>
<author>Bonaventura Coppola</author>
<author>Timothy Chklovski</author>
<author>Eduard Hovy</author>
</authors>
<title>ISP: Learning Inferential Selectional Preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT.</booktitle>
<contexts>
<context position="3238" citStr="Pantel et al., 2007" startWordPosition="483" endWordPosition="486"> serve as relations. This approach requires no supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find FERGUSON–historian-at–HARVARD but does not know FERGUSON–is-a-professor-at– HARVARD. OpenIE has traditionally relied on a large diversity of textual expressions to provide good coverage. But this diversity is not always available, and, in any case, the lack of generalization greatly inhibits the ability to support reasoning. One way to gain generalization is to cluster textual surface forms that have similar meaning (Lin and Pantel, 2001; Pantel et al., 2007; Yates and Etzioni, 2009; Yao et al., 2011). While the clusters discovered by all these methods usually contain semantically related items, closer inspection invariably shows that they do not provide reliable implicature. For example, a typical representative cluster may include historian-at, professor-at, scientistat, worked-at. Although these relation types are indeed semantically related, note that scientist-at does not necessarily imply professor-at, and worked-at Proceedings of NAACL-HLT 2013, pages 74–84, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics</context>
<context position="16739" citStr="Pantel et al., 2007" startWordPosition="2739" endWordPosition="2742">nts are easy to derive; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. This is similar in spirit to work on learning entailmen</context>
</contexts>
<marker>Pantel, Bhagat, Coppola, Chklovski, Hovy, 2007</marker>
<rawString>Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard Hovy. 2007. ISP: Learning Inferential Selectional Preferences. In Proceedings of NAACL HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Rendle</author>
<author>Christoph Freudenthaler</author>
<author>Zeno Gantner</author>
<author>Lars Schmidt-Thieme</author>
</authors>
<title>Bpr: Bayesian personalized ranking from implicit feedback.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09,</booktitle>
<pages>452--461</pages>
<publisher>AUAI Press.</publisher>
<location>Arlington, Virginia, United States.</location>
<contexts>
<context position="6266" citStr="Rendle et al., 2009" startWordPosition="939" endWordPosition="942">study and understand the complexities of a particular schema in order to issue queries; they can ask in whatever form naturally occurs to them, and our system will likely already have that relation in our universal schema. Our technical approach is based on extensions to probabilistic models of matrix factorization and 1At NAACL 2012 Lucy Vanderwende asked “Where do the relation types come from?” There was no satisfying answer. At the same meeting, and in line with Brachman (1983), Ed Hovy stated “We don’t even know what is-a means.” collaborative filtering (Collins et al., 2001; Koren, 2008; Rendle et al., 2009). We represent the probabilistic knowledge base as a matrix with entityentity pairs in the rows and relations in the columns (see figure 1). The rows come from running crossdocument entity resolution across pre-existing structured databases and textual corpora. The columns come from the union of surface forms and DB relations. We present a series of models that learn lower dimensional manifolds for tuples, relations and entities, and a set of weights that capture direct correlations between relations. Weights and lower dimensional representations act, through dot products, as the natural param</context>
<context position="14304" citStr="Rendle et al., 2009" startWordPosition="2302" endWordPosition="2305"> In collaborative filtering positive-only data is also known as implicit feedback. This type of feedback arises, for example, when users buy but not rate items. One successful approach to learning with implicit feedback is based on the observation that the actual task is not necessarily one of prediction (here: to predict a number between 0 and 1) but one of (generally simpler) ranking: to give true “user-item” cells higher scores than false ones. Bayesian Personalized Ranking (BPR) uses a variant of this ranking: giving observed true facts higher scores than unobserved (true or false) facts (Rendle et al., 2009). This relaxed constraint is to be contrasted with the loglikelihood setting that essentially requires (randomly sampled) negative facts to score below a globally defined threshold. 2.5.1 Objective We first create a dataset of ranked pairs: for each relation r and each observed fact f+ := (r, t+) E O we choose all tuples t− such that f− := (r, t−) E� O—that is, tuples we have not observed to be in relation r. For each pair of facts f+ and f− we want p (f+) &gt; p (f−) and hence Bf+ &gt; Bf−. In BPR this is achieved by maximizing a sum terms of the form Objf+,f− := log (Q (Bf+ − Bf− one for each rank</context>
<context position="15591" citStr="Rendle et al. (2009)" startWordPosition="2538" endWordPosition="2541"> that this objective differs slightly from the one used by Rendle et al. (2009). Consider tuples as users and items as relations. We rank different users with respect to the same item, while BPR ranks items with respect to the same user. Also notice that the BPR objective is an approximation to the perrelation AUC (area under the ROC curve), and hence directly correlated to what we want to achieve: wellranked tuples per relation. Note that all parameters are regularized with quadratic penalty which we omit here for brevity. 2.5.2 Optimization To maximize the objective2 in equation 1 we follow Rendle et al. (2009) and employ Stochastic Gradient Descent (SGD). In particular, in each epoch 2This objective is non-convex for all models excluding the N model. 77 we sample |O |facts with replacement from O. For each sampled fact (r, t+) we then sample a tuple t− E T such that (r, t−) E� O is not an observed fact. This gives us |O |fact pairs (f+, f−), and for each pair we do an SGD update using the corresponding gradients of Objf+�f−. For the F model the gradients correspond to those presented by Rendle et al. (2009). The remaining gradients are easy to derive; we omit details for brevity. 3 Related Work Thi</context>
</contexts>
<marker>Rendle, Freudenthaler, Gantner, Schmidt-Thieme, 2009</marker>
<rawString>Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. Bpr: Bayesian personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09, pages 452–461, Arlington, Virginia, United States. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD ’10).</booktitle>
<contexts>
<context position="2305" citStr="Riedel et al., 2010" startWordPosition="338" endWordPosition="341">textual data is labeled according to this schema, and this labeling is then used in supervised training of an automated relation extractor, e.g. Culotta and Sorensen (2004). However, labeling textual rela74 tions is time-consuming and difficult, leading to significant recent interest in distantly-supervised learning. Here one aligns existing database records with the sentences in which these records have been “rendered”––effectively labeling the text—and from this labeling we can train a machine learning system as before (Craven and Kumlien, 1999; Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). However, this method relies on the availability of a large database that has the desired schema. The need for pre-existing datasets can be avoided by using language itself as the source of the schema. This is the approach taken by OpenIE (Etzioni et al., 2008). Here surface patterns between mentions of concepts serve as relations. This approach requires no supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find FERGUSON–historian-at–HARVARD but does not know FERGUSON–is-a-professor-at– HARVARD. OpenIE has traditionally relied on a large d</context>
<context position="19683" citStr="Riedel et al., 2010" startWordPosition="3212" endWordPosition="3215">d on a single unified model, requires no entity types, and for us inferring a fact amounts to not more than a few dot products. In addition, in our Universal Schema approach OpenIE surface patterns are just one kind of relations, and our aim is populate relations of all kinds. In the future we may even include relations between entities and continuous attributes (say, gene expression measurements). Distant Supervision In Distant Supervision (DS) a set of facts from pre-existing structured sources is aligned with surface patterns mentioned in text (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), and this alignment is then used to train a relation extractor. A core difference to our approach is the number of target relations: In DS it is the relatively small schema size of the knowledge base, while we also include surface patterns. This allows us to answer more expressive queries. Moreover, by learning from surface-pattern correlations, our latent models induce feature representations for patterns that do not appear in the DS training set. As we will see in section 4, this allows us to outperform state-of-the-art DS models. 78 Never-Endi</context>
<context position="21395" citStr="Riedel et al., 2010" startWordPosition="3486" endWordPosition="3489">ervations. This has the advantage that wrong predictions are less likely be reinforced, hence reducing the risk of semantic drift. 4 Experiments How accurately can we fill a database of Universal Schema, and does reasoning jointly across a universal schema help to improve over more isolated approaches? In the following we seek to answer this question empirically. To this end we train our models on observed facts in a newswire corpus and Freebase, and then manually evaluate ranked predictions: first for structured relations and then for surface form relations. 4.1 Data Following previous work (Riedel et al., 2010), our documents are taken from the NYTimes corpus (Sandhaus, 2008). Articles after 2000 are used as training corpus, articles from 1990 to 1999 as test corpus. We also split Freebase facts 50/50 into train and test facts, and their corresponding tuples into train and test tuples. Then we align training tuples with the training corpus, and test tuples with the test corpus. This alignment relies on a preprocessing step that links NER mentions in text with entities in Freebase. In our case we use a simple string-match heuristic to find this linking. Now we align an entity tuple (t1, t2) with a pa</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The New York Times Annotated Corpus. Linguistic Data Consortium,</title>
<date>2008</date>
<location>Philadelphia.</location>
<contexts>
<context position="21461" citStr="Sandhaus, 2008" startWordPosition="3499" endWordPosition="3500">y be reinforced, hence reducing the risk of semantic drift. 4 Experiments How accurately can we fill a database of Universal Schema, and does reasoning jointly across a universal schema help to improve over more isolated approaches? In the following we seek to answer this question empirically. To this end we train our models on observed facts in a newswire corpus and Freebase, and then manually evaluate ranked predictions: first for structured relations and then for surface form relations. 4.1 Data Following previous work (Riedel et al., 2010), our documents are taken from the NYTimes corpus (Sandhaus, 2008). Articles after 2000 are used as training corpus, articles from 1990 to 1999 as test corpus. We also split Freebase facts 50/50 into train and test facts, and their corresponding tuples into train and test tuples. Then we align training tuples with the training corpus, and test tuples with the test corpus. This alignment relies on a preprocessing step that links NER mentions in text with entities in Freebase. In our case we use a simple string-match heuristic to find this linking. Now we align an entity tuple (t1, t2) with a pair of mentions (m1, m2) in the same sentence if m1 is linked to t1</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus, 2008. The New York Times Annotated Corpus. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
</authors>
<title>Scaling textual inference to the web. In</title>
<date>2008</date>
<booktitle>EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="18819" citStr="Schoenmackers et al., 2008" startWordPosition="3073" endWordPosition="3076">tual features such as “words in a local window.” Consequently, our objective is to complete the matrix, whereas their objective is to learn better latent embeddings of words (which by themselves again cannot capture any sense of asymmetry). OpenIE Open IE (Etzioni et al., 2008) extracts facts mentioned in text, but does not predict potential facts not mentioned in text. Finding answers requires explicit mentions, and hence suffers from lower recall for not-so-frequently mentioned facts. Methods that learn rules between textual patterns in OpenIE aim at a similar goal as our proposed approach (Schoenmackers et al., 2008; Schoenmackers et al., 2010). However, their approach is substantially more complex, requires a categorization of entities into fine grained entity types, and needs inference in high tree-width Markov Networks. By contrast, our approach is based on a single unified model, requires no entity types, and for us inferring a fact amounts to not more than a few dot products. In addition, in our Universal Schema approach OpenIE surface patterns are just one kind of relations, and our aim is populate relations of all kinds. In the future we may even include relations between entities and continuous a</context>
</contexts>
<marker>Schoenmackers, Etzioni, Weld, 2008</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni, and Daniel S. Weld. 2008. Scaling textual inference to the web. In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 79–88, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
<author>Jesse Davis</author>
</authors>
<title>Learning first-order horn clauses from web text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1088--1098</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18848" citStr="Schoenmackers et al., 2010" startWordPosition="3077" endWordPosition="3081"> in a local window.” Consequently, our objective is to complete the matrix, whereas their objective is to learn better latent embeddings of words (which by themselves again cannot capture any sense of asymmetry). OpenIE Open IE (Etzioni et al., 2008) extracts facts mentioned in text, but does not predict potential facts not mentioned in text. Finding answers requires explicit mentions, and hence suffers from lower recall for not-so-frequently mentioned facts. Methods that learn rules between textual patterns in OpenIE aim at a similar goal as our proposed approach (Schoenmackers et al., 2008; Schoenmackers et al., 2010). However, their approach is substantially more complex, requires a categorization of entities into fine grained entity types, and needs inference in high tree-width Markov Networks. By contrast, our approach is based on a single unified model, requires no entity types, and for us inferring a fact amounts to not more than a few dot products. In addition, in our Universal Schema approach OpenIE surface patterns are just one kind of relations, and our aim is populate relations of all kinds. In the future we may even include relations between entities and continuous attributes (say, gene expressi</context>
</contexts>
<marker>Schoenmackers, Etzioni, Weld, Davis, 2010</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld, and Jesse Davis. 2010. Learning first-order horn clauses from web text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1088–1098, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06,</booktitle>
<pages>304--311</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16540" citStr="Shinyama and Sekine, 2006" startWordPosition="2706" endWordPosition="2709">airs (f+, f−), and for each pair we do an SGD update using the corresponding gradients of Objf+�f−. For the F model the gradients correspond to those presented by Rendle et al. (2009). The remaining gradients are easy to derive; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption </context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06, pages 304– 311, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance multilabel learning for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’12),</booktitle>
<pages>455--465</pages>
<contexts>
<context position="7460" citStr="Surdeanu et al., 2012" startWordPosition="1126" endWordPosition="1130">oducts, as the natural parameters of a single log-linear model to derive per-cell probabilities. In experiments we show that our models can accurately predict surface patterns relationships which do not appear explicitly in text, and that learning latent representations of entities, tuples and relations substantially improves results over a traditional classifier approach. Moreover, we can improve accuracy by simultaneously operating on relations observed in the New York Times corpus and in Freebase. In particular, our model outperforms the current state-of-the-art distant supervision method (Surdeanu et al., 2012) by 10% points Mean Average Precision through joint implicature among surface patterns and Freebase relations. 2 Model Before we present our approach in more detail, we briefly introduce some notation. We use R to denote the set of relations we seek to predict (such as works-written in Freebase, or the X–historian-at–Y pattern), and T to denote the set of input tuples. For simplicity we assume each relation to be binary, although our approach can be easily generalized to the n-ary case. Given a relation r E R and a tuple t E T the pair (r, t) is a fact, or relation instance. The input to our m</context>
<context position="19730" citStr="Surdeanu et al., 2012" startWordPosition="3220" endWordPosition="3223">ity types, and for us inferring a fact amounts to not more than a few dot products. In addition, in our Universal Schema approach OpenIE surface patterns are just one kind of relations, and our aim is populate relations of all kinds. In the future we may even include relations between entities and continuous attributes (say, gene expression measurements). Distant Supervision In Distant Supervision (DS) a set of facts from pre-existing structured sources is aligned with surface patterns mentioned in text (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), and this alignment is then used to train a relation extractor. A core difference to our approach is the number of target relations: In DS it is the relatively small schema size of the knowledge base, while we also include surface patterns. This allows us to answer more expressive queries. Moreover, by learning from surface-pattern correlations, our latent models induce feature representations for patterns that do not appear in the DS training set. As we will see in section 4, this allows us to outperform state-of-the-art DS models. 78 Never-Ending Learning and Bootstrapping Our latent featur</context>
<context position="28116" citStr="Surdeanu et al. (2012)" startWordPosition="4626" endWordPosition="4629">evant facts. Our first baseline is MI09, a distantly supervised classifier based on the work of Mintz et al. (2009). This classifier only learns from observed pattern-relation pairs in the training set (of which we only have about 8k). By contrast, our latent feature models can learn pattern-pattern correlations both on the unlabeled training and test set (comparable to bootstrapping). We hence also compare against YA11, a version of MI09 that uses preprocessed cluster features according to Yao et al. (2011). The third baseline is SU12, the state-of-theart Multi-Instance Multi-Label system by Surdeanu et al. (2012). The remaining systems are our neighborhood 80 Averaged 11-point Precision/Recall 0 0.2 0.4 0.6 0.8 1 Recall Figure 2: Averaged 11-point precision recall curve for Freebase relations in table 1. model (N), the factorized model (F), their combination (NF) and the combined model with a latent entity representation (NFE). For all our models we use the same number of components when applicable (KF = KE = 100), 1000 epochs, and 0.01 as regularizer for component weights and 0.1 for neighborhood weights. Table 1 shows that adding pattern cluster features (and hence incorporating more data) helps YA1</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance multilabel learning for relation extraction. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’12), pages 455–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08,</booktitle>
<pages>849--856</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17419" citStr="Szpektor and Dagan, 2008" startWordPosition="2846" endWordPosition="2849">ly different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. This is similar in spirit to work on learning entailment rules (Szpektor et al., 2004; Zanzotto et al., 2006; Szpektor and Dagan, 2008). However, for us even entailment rules are just a by-product of our goal to improve prediction, and it is this goal we directly optimize for and evaluate. Matrix Factorization Our approach is also related to work on factorizing YAGO to predict new links (Nickel et al., 2012). The primary differences are that we include surface patterns in our schema, use a ranking objective, and learn latent vectors for entities and tuples. Likewise, matrix factorization in various flavors has received significant attention in the lexical semantics community, from LSA to recent work on non-negative sparse emb</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08, pages 849–856, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Hristo Tanev</author>
<author>Ido Dagan</author>
<author>Bonaventura Coppola</author>
</authors>
<title>Scaling web-based acquisition of entailment relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="17369" citStr="Szpektor et al., 2004" startWordPosition="2838" endWordPosition="2841"> Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. This is similar in spirit to work on learning entailment rules (Szpektor et al., 2004; Zanzotto et al., 2006; Szpektor and Dagan, 2008). However, for us even entailment rules are just a by-product of our goal to improve prediction, and it is this goal we directly optimize for and evaluate. Matrix Factorization Our approach is also related to work on factorizing YAGO to predict new links (Nickel et al., 2012). The primary differences are that we include surface patterns in our schema, use a ranking objective, and learn latent vectors for entities and tuples. Likewise, matrix factorization in various flavors has received significant attention in the lexical semantics community, </context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura Coppola. 2004. Scaling web-based acquisition of entailment relations. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shingo Takamatsu</author>
<author>Issei Sato</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Probabilistic matrix factorization leveraging contexts for unsupervised relation discovery.</title>
<date>2011</date>
<booktitle>In Proceedings of PAKDD.</booktitle>
<contexts>
<context position="16607" citStr="Takamatsu et al., 2011" startWordPosition="2718" endWordPosition="2721">ponding gradients of Objf+�f−. For the F model the gradients correspond to those presented by Rendle et al. (2009). The remaining gradients are easy to derive; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinat</context>
</contexts>
<marker>Takamatsu, Sato, Nakagawa, 2011</marker>
<rawString>Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2011. Probabilistic matrix factorization leveraging contexts for unsupervised relation discovery. In Proceedings of PAKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Aria Haghighi</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Structured relation discovery using generative models.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’11),</booktitle>
<contexts>
<context position="3282" citStr="Yao et al., 2011" startWordPosition="491" endWordPosition="494"> supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find FERGUSON–historian-at–HARVARD but does not know FERGUSON–is-a-professor-at– HARVARD. OpenIE has traditionally relied on a large diversity of textual expressions to provide good coverage. But this diversity is not always available, and, in any case, the lack of generalization greatly inhibits the ability to support reasoning. One way to gain generalization is to cluster textual surface forms that have similar meaning (Lin and Pantel, 2001; Pantel et al., 2007; Yates and Etzioni, 2009; Yao et al., 2011). While the clusters discovered by all these methods usually contain semantically related items, closer inspection invariably shows that they do not provide reliable implicature. For example, a typical representative cluster may include historian-at, professor-at, scientistat, worked-at. Although these relation types are indeed semantically related, note that scientist-at does not necessarily imply professor-at, and worked-at Proceedings of NAACL-HLT 2013, pages 74–84, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics certainly does not imply scientist-at. In f</context>
<context position="8745" citStr="Yao et al., 2011" startWordPosition="1361" endWordPosition="1364">n tuple is denoted by Ot := {(r,t) E O1. Our goal is a model that can estimate, for a given relation r (such as X–historian-at–Y) and a given tuple t (such as &lt;FERGUSON,HARVARD&gt;), the probability p (yr,t = 1) where yr,t is a binary random variable that is true iff t is in relation r. We 75 Figure 1: Filling up a database of universal schema. Dark circles are observed facts, shaded circles are inferred facts. Relation Extraction (RE) maps surface pattern relations (and other features) to structured relations. Surface form clustering models correlations between patterns, and can be fed into RE (Yao et al., 2011). Database alignment and integration models correlations between structured relations (not done in this work). Reasoning with the universal schema incorporates these tasks in a joint fashion. introduce a series of exponential family models that estimate this probability using a natural parameter θr,t and the logistic function: We will first describe our models through different definitions of the natural parameter θr,t. In each case θr,t will be a function of r, t and a set of weights and/or latent feature vectors. In section 2.5 we will then show how these weights and vectors can be estimated</context>
<context position="16582" citStr="Yao et al., 2011" startWordPosition="2714" endWordPosition="2717">e using the corresponding gradients of Objf+�f−. For the F model the gradients correspond to those presented by Rendle et al. (2009). The remaining gradients are easy to derive; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that cert</context>
<context position="28007" citStr="Yao et al. (2011)" startWordPosition="4610" endWordPosition="4613"> shows our results for Freebase relations, omitting those for which none of the systems can find any relevant facts. Our first baseline is MI09, a distantly supervised classifier based on the work of Mintz et al. (2009). This classifier only learns from observed pattern-relation pairs in the training set (of which we only have about 8k). By contrast, our latent feature models can learn pattern-pattern correlations both on the unlabeled training and test set (comparable to bootstrapping). We hence also compare against YA11, a version of MI09 that uses preprocessed cluster features according to Yao et al. (2011). The third baseline is SU12, the state-of-theart Multi-Instance Multi-Label system by Surdeanu et al. (2012). The remaining systems are our neighborhood 80 Averaged 11-point Precision/Recall 0 0.2 0.4 0.6 0.8 1 Recall Figure 2: Averaged 11-point precision recall curve for Freebase relations in table 1. model (N), the factorized model (F), their combination (NF) and the combined model with a latent entity representation (NFE). For all our models we use the same number of components when applicable (KF = KE = 100), 1000 epochs, and 0.01 as regularizer for component weights and 0.1 for neighborh</context>
</contexts>
<marker>Yao, Haghighi, Riedel, McCallum, 2011</marker>
<rawString>Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew McCallum. 2011. Structured relation discovery using generative models. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’11), July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Probabilistic databases of universal schema.</title>
<date>2012</date>
<booktitle>In Proceedings of the AKBC-WEKEX Workshop at NAACL 2012,</booktitle>
<contexts>
<context position="12021" citStr="Yao et al., 2012" startWordPosition="1916" endWordPosition="1919">-Y employee(X,Y) member(X,Y) Ferguson,Harvard Oman,Oxford Firth,Oxford Gödel,Princeton Surface Patterns KB Relations 0.95 Cluster Rel. Extraction Align 1 1 Reasoning with Universal Schema 0.05 1 1 1 0.97 0.93 0.97 1 0.95 1 Train Test 1 p (yr,t = 1|θr,t) := σ (θr,t) = 1 + exp (−θr,t ). � KF k ar,kvt,k. θF r,t := 76 2.3 Entity Model Relations have selectional preferences: they allow only certain types in their argument slots. While knowledge bases such as Freebase or DBPedia have extensive ontologies of types of entities, these are often not sufficiently fine to allow relations to discriminate (Yao et al., 2012b). Hence, instead of using a predetermined set of entity types, in our entity model E we learn a latent entity representation from data. More concretely, for each entity a we introduce a latent feature vector te of dimension KE. In addition, for each relation r and argument slot i we introduce a feature vector di of the same dimension. For example, binary relations have feature representations d1 for argument 1, and d2 for argument 2. Measuring compatibility of an entity tuple and relation amounts to measuring, and summing up, compatibility between each argument slot representation and the co</context>
<context position="16249" citStr="Yao et al., 2012" startWordPosition="2661" endWordPosition="2664">GD). In particular, in each epoch 2This objective is non-convex for all models excluding the N model. 77 we sample |O |facts with replacement from O. For each sampled fact (r, t+) we then sample a tuple t− E T such that (r, t−) E� O is not an observed fact. This gives us |O |fact pairs (f+, f−), and for each pair we do an SGD update using the corresponding gradients of Objf+�f−. For the F model the gradients correspond to those presented by Rendle et al. (2009). The remaining gradients are easy to derive; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interest</context>
</contexts>
<marker>Yao, Riedel, McCallum, 2012</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2012a. Probabilistic databases of universal schema. In Proceedings of the AKBC-WEKEX Workshop at NAACL 2012, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Unsupervised relation discovery with sense disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL ’12),</booktitle>
<contexts>
<context position="12021" citStr="Yao et al., 2012" startWordPosition="1916" endWordPosition="1919">-Y employee(X,Y) member(X,Y) Ferguson,Harvard Oman,Oxford Firth,Oxford Gödel,Princeton Surface Patterns KB Relations 0.95 Cluster Rel. Extraction Align 1 1 Reasoning with Universal Schema 0.05 1 1 1 0.97 0.93 0.97 1 0.95 1 Train Test 1 p (yr,t = 1|θr,t) := σ (θr,t) = 1 + exp (−θr,t ). � KF k ar,kvt,k. θF r,t := 76 2.3 Entity Model Relations have selectional preferences: they allow only certain types in their argument slots. While knowledge bases such as Freebase or DBPedia have extensive ontologies of types of entities, these are often not sufficiently fine to allow relations to discriminate (Yao et al., 2012b). Hence, instead of using a predetermined set of entity types, in our entity model E we learn a latent entity representation from data. More concretely, for each entity a we introduce a latent feature vector te of dimension KE. In addition, for each relation r and argument slot i we introduce a feature vector di of the same dimension. For example, binary relations have feature representations d1 for argument 1, and d2 for argument 2. Measuring compatibility of an entity tuple and relation amounts to measuring, and summing up, compatibility between each argument slot representation and the co</context>
<context position="16249" citStr="Yao et al., 2012" startWordPosition="2661" endWordPosition="2664">GD). In particular, in each epoch 2This objective is non-convex for all models excluding the N model. 77 we sample |O |facts with replacement from O. For each sampled fact (r, t+) we then sample a tuple t− E T such that (r, t−) E� O is not an observed fact. This gives us |O |fact pairs (f+, f−), and for each pair we do an SGD update using the corresponding gradients of Objf+�f−. For the F model the gradients correspond to those presented by Rendle et al. (2009). The remaining gradients are easy to derive; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interest</context>
</contexts>
<marker>Yao, Riedel, McCallum, 2012</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2012b. Unsupervised relation discovery with sense disambiguation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL ’12), July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised methods for determining object and relation synonyms on the web.</title>
<date>2009</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>34--255</pages>
<contexts>
<context position="3263" citStr="Yates and Etzioni, 2009" startWordPosition="487" endWordPosition="490">This approach requires no supervision and has tremendous flexibility, but lacks the ability to generalize. For example, OpenIE may find FERGUSON–historian-at–HARVARD but does not know FERGUSON–is-a-professor-at– HARVARD. OpenIE has traditionally relied on a large diversity of textual expressions to provide good coverage. But this diversity is not always available, and, in any case, the lack of generalization greatly inhibits the ability to support reasoning. One way to gain generalization is to cluster textual surface forms that have similar meaning (Lin and Pantel, 2001; Pantel et al., 2007; Yates and Etzioni, 2009; Yao et al., 2011). While the clusters discovered by all these methods usually contain semantically related items, closer inspection invariably shows that they do not provide reliable implicature. For example, a typical representative cluster may include historian-at, professor-at, scientistat, worked-at. Although these relation types are indeed semantically related, note that scientist-at does not necessarily imply professor-at, and worked-at Proceedings of NAACL-HLT 2013, pages 74–84, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics certainly does not imply</context>
<context position="16718" citStr="Yates and Etzioni, 2009" startWordPosition="2734" endWordPosition="2738">09). The remaining gradients are easy to derive; we omit details for brevity. 3 Related Work This work extends a previous workshop paper (Yao et al., 2012a) by introducing the neighborhood and entity model, by working with the BPR objective, and by more extensive experiments. Relational Clustering There is a large body of work aiming to discover latent relations by clustering surface patterns (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Kok and Domingos, 2008; Yao et al., 2011; Takamatsu et al., 2011), or by inducing synonymy relationships between patterns independently of the entities (Yates and Etzioni, 2009; Pantel et al., 2007; Lin and Pantel, 2001). Our approach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. This is similar in spirit to work </context>
</contexts>
<marker>Yates, Etzioni, 2009</marker>
<rawString>Alexander Yates and Oren Etzioni. 2009. Unsupervised methods for determining object and relation synonyms on the web. Journal ofArtificial Intelligence Research, 34:255–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Maria Teresa Pazienza</author>
</authors>
<title>Discovering asymmetric entailment relations between verbs using selectional preferences.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL ’06).</booktitle>
<contexts>
<context position="17392" citStr="Zanzotto et al., 2006" startWordPosition="2842" endWordPosition="2845">roach has a fundamentally different objective: we are not (primarily) interested in clusters of patterns or their semantic representation, but in predicting patterns where they are not observed. Moreover, these related methods rely on a symmetric notion of synonymy in which clustered patterns are assumed to have the same meaning. Our approach rejects this assumption in favor of a model which learns that certain patterns, or combinations thereof, entail others in one direction, but not necessarily the other. This is similar in spirit to work on learning entailment rules (Szpektor et al., 2004; Zanzotto et al., 2006; Szpektor and Dagan, 2008). However, for us even entailment rules are just a by-product of our goal to improve prediction, and it is this goal we directly optimize for and evaluate. Matrix Factorization Our approach is also related to work on factorizing YAGO to predict new links (Nickel et al., 2012). The primary differences are that we include surface patterns in our schema, use a ranking objective, and learn latent vectors for entities and tuples. Likewise, matrix factorization in various flavors has received significant attention in the lexical semantics community, from LSA to recent work</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Pazienza, 2006</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Maria Teresa Pazienza. 2006. Discovering asymmetric entailment relations between verbs using selectional preferences. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL ’06).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>