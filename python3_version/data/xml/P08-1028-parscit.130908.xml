<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003255">
<title confidence="0.984883">
Vector-based Models of Semantic Composition
</title>
<author confidence="0.997818">
Jeff Mitchell and Mirella Lapata
</author>
<affiliation confidence="0.83854">
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, UK
</affiliation>
<email confidence="0.9952">
jeff.mitchell@ed.ac.uk,mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.995589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999537833333333">
This paper proposes a framework for repre-
senting the meaning of phrases and sentences
in vector space. Central to our approach is
vector composition which we operationalize
in terms of additive and multiplicative func-
tions. Under this framework, we introduce a
wide range of composition models which we
evaluate empirically on a sentence similarity
task. Experimental results demonstrate that
the multiplicative models are superior to the
additive alternatives when compared against
human judgments.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997512563636364">
Vector-based models of word meaning (Lund and
Burgess, 1996; Landauer and Dumais, 1997) have
become increasingly popular in natural language
processing (NLP) and cognitive science. The ap-
peal of these models lies in their ability to rep-
resent meaning simply by using distributional in-
formation under the assumption that words occur-
ring within similar contexts are semantically similar
(Harris, 1968).
A variety of NLP tasks have made good use
of vector-based models. Examples include au-
tomatic thesaurus extraction (Grefenstette, 1994),
word sense discrimination (Sch¨utze, 1998) and dis-
ambiguation (McCarthy et al., 2004), collocation ex-
traction (Schone and Jurafsky, 2001), text segmen-
tation (Choi et al., 2001) , and notably information
retrieval (Salton et al., 1975). In cognitive science
vector-based models have been successful in simu-
lating semantic priming (Lund and Burgess, 1996;
Landauer and Dumais, 1997) and text comprehen-
sion (Landauer and Dumais, 1997; Foltz et al.,
1998). Moreover, the vector similarities within such
semantic spaces have been shown to substantially
correlate with human similarity judgments (McDon-
ald, 2000) and word association norms (Denhire and
Lemaire, 2004).
Despite their widespread use, vector-based mod-
els are typically directed at representing words in
isolation and methods for constructing representa-
tions for phrases or sentences have received little
attention in the literature. In fact, the common-
est method for combining the vectors is to average
them. Vector averaging is unfortunately insensitive
to word order, and more generally syntactic struc-
ture, giving the same representation to any construc-
tions that happen to share the same vocabulary. This
is illustrated in the example below taken from Lan-
dauer et al. (1997). Sentences (1-a) and (1-b) con-
tain exactly the same set of words but their meaning
is entirely different.
(1) a. It was not the sales manager who hit the
bottle that day, but the office worker with
the serious drinking problem.
b. That day the office manager, who was
drinking, hit the problem sales worker with
a bottle, but it was not serious.
While vector addition has been effective in some
applications such as essay grading (Landauer and
Dumais, 1997) and coherence assessment (Foltz
et al., 1998), there is ample empirical evidence
that syntactic relations across and within sentences
are crucial for sentence and discourse processing
(Neville et al., 1991; West and Stanovich, 1986)
and modulate cognitive behavior in sentence prim-
ing (Till et al., 1988) and inference tasks (Heit and
</bodyText>
<page confidence="0.978692">
236
</page>
<note confidence="0.704001">
Proceedings of ACL-08: HLT, pages 236–244,
</note>
<page confidence="0.455">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.98833904">
Rubinstein, 1994).
Computational models of semantics which use
symbolic logic representations (Montague, 1974)
can account naturally for the meaning of phrases or
sentences. Central in these models is the notion of
compositionality — the meaning of complex expres-
sions is determined by the meanings of their con-
stituent expressions and the rules used to combine
them. Here, semantic analysis is guided by syntactic
structure, and therefore sentences (1-a) and (1-b) re-
ceive distinct representations. The downside of this
approach is that differences in meaning are qualita-
tive rather than quantitative, and degrees of similar-
ity cannot be expressed easily.
In this paper we examine models of semantic
composition that are empirically grounded and can
represent similarity relations. We present a gen-
eral framework for vector-based composition which
allows us to consider different classes of models.
Specifically, we present both additive and multi-
plicative models of vector combination and assess
their performance on a sentence similarity rating ex-
periment. Our results show that the multiplicative
models are superior and correlate significantly with
behavioral data.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999406534246576">
The problem of vector composition has received
some attention in the connectionist literature, partic-
ularly in response to criticisms of the ability of con-
nectionist representations to handle complex struc-
tures (Fodor and Pylyshyn, 1988). While neural net-
works can readily represent single distinct objects,
in the case of multiple objects there are fundamen-
tal difficulties in keeping track of which features are
bound to which objects. For the hierarchical struc-
ture of natural language this binding problem be-
comes particularly acute. For example, simplistic
approaches to handling sentences such as John loves
Mary and Mary loves John typically fail to make
valid representations in one of two ways. Either
there is a failure to distinguish between these two
structures, because the network fails to keep track
of the fact that John is subject in one and object
in the other, or there is a failure to recognize that
both structures involve the same participants, be-
cause John as a subject has a distinct representation
from John as an object. In contrast, symbolic repre-
sentations can naturally handle the binding of con-
stituents to their roles, in a systematic manner that
avoids both these problems.
Smolensky (1990) proposed the use of tensor
products as a means of binding one vector to an-
other. The tensor product u ® v is a matrix whose
components are all the possible products uivj of the
components of vectors u and v. A major difficulty
with tensor products is their dimensionality which is
higher than the dimensionality of the original vec-
tors (precisely, the tensor product has dimensional-
ity m x n). To overcome this problem, other tech-
niques have been proposed in which the binding of
two vectors results in a vector which has the same
dimensionality as its components. Holographic re-
duced representations (Plate, 1991) are one imple-
mentation of this idea where the tensor product is
projected back onto the space of its components.
The projection is defined in terms of circular con-
volution a mathematical function that compresses
the tensor product of two vectors. The compression
is achieved by summing along the transdiagonal el-
ements of the tensor product. Noisy versions of the
original vectors can be recovered by means of cir-
cular correlation which is the approximate inverse
of circular convolution. The success of circular cor-
relation crucially depends on the components of the
n-dimensional vectors u and v being randomly dis-
tributed with mean 0 and variance 1n. This poses
problems for modeling linguistic data which is typi-
cally represented by vectors with non-random struc-
ture.
Vector addition is by far the most common
method for representing the meaning of linguistic
sequences. For example, assuming that individual
words are represented by vectors, we can compute
the meaning of a sentence by taking their mean
(Foltz et al., 1998; Landauer and Dumais, 1997).
Vector addition does not increase the dimensional-
ity of the resulting vector. However, since it is order
independent, it cannot capture meaning differences
that are modulated by differences in syntactic struc-
ture. Kintsch (2001) proposes a variation on the vec-
tor addition theme in an attempt to model how the
meaning of a predicate (e.g., run) varies depending
on the arguments it operates upon (e.g, the horse ran
vs. the color ran). The idea is to add not only the
vectors representing the predicate and its argument
but also the neighbors associated with both of them.
The neighbors, Kintsch argues, can ‘strengthen fea-
tures of the predicate that are appropriate for the ar-
gument of the predication’.
</bodyText>
<page confidence="0.943948">
237
</page>
<bodyText confidence="0.884071666666667">
animal stable village gallop jokey
horse 0 6 2 10 4
run 1 8 4 4 0
</bodyText>
<figureCaption confidence="0.969352">
Figure 1: A hypothetical semantic space for horse and
run
</figureCaption>
<bodyText confidence="0.999920066666667">
Unfortunately, comparisons across vector compo-
sition models have been few and far between in the
literature. The merits of different approaches are il-
lustrated with a few hand picked examples and pa-
rameter values and large scale evaluations are uni-
formly absent (see Frank et al. (2007) for a criticism
of Kintsch’s (2001) evaluation standards). Our work
proposes a framework for vector composition which
allows the derivation of different types of models
and licenses two fundamental composition opera-
tions, multiplication and addition (and their combi-
nation). Under this framework, we introduce novel
composition models which we compare empirically
against previous work using a rigorous evaluation
methodology.
</bodyText>
<sectionHeader confidence="0.993304" genericHeader="method">
3 Composition Models
</sectionHeader>
<bodyText confidence="0.999859260869565">
We formulate semantic composition as a function
of two vectors, u and v. We assume that indi-
vidual words are represented by vectors acquired
from a corpus following any of the parametrisa-
tions that have been suggested in the literature.1 We
briefly note here that a word’s vector typically rep-
resents its co-occurrence with neighboring words.
The construction of the semantic space depends on
the definition of linguistic context (e.g., neighbour-
ing words can be documents or collocations), the
number of components used (e.g., the k most fre-
quent words in a corpus), and their values (e.g., as
raw co-occurrence frequencies or ratios of probabil-
ities). A hypothetical semantic space is illustrated in
Figure 1. Here, the space has only five dimensions,
and the matrix cells denote the co-occurrence of the
target words (horse and run) with the context words
animal, stable, and so on.
Let p denote the composition of two vectors u
and v, representing a pair of constituents which
stand in some syntactic relation R. Let K stand for
any additional knowledge or information which is
needed to construct the semantics of their composi-
</bodyText>
<footnote confidence="0.790748333333333">
1A detailed treatment of existing semantic space models is
outside the scope of the present paper. We refer the interested
reader to Pad´o and Lapata (2007) for a comprehensive overview.
</footnote>
<bodyText confidence="0.994968">
tion. We define a general class of models for this
process of composition as:
</bodyText>
<equation confidence="0.990715">
p = f(u,v,R,K) (1)
</equation>
<bodyText confidence="0.999884352941177">
The expression above allows us to derive models for
which p is constructed in a distinct space from u
and v, as is the case for tensor products. It also
allows us to derive models in which composition
makes use of background knowledge K and mod-
els in which composition has a dependence, via the
argument R, on syntax.
To derive specific models from this general frame-
work requires the identification of appropriate con-
straints to narrow the space of functions being con-
sidered. One particularly useful constraint is to
hold R fixed by focusing on a single well defined
linguistic structure, for example the verb-subject re-
lation. Another simplification concerns K which can
be ignored so as to explore what can be achieved in
the absence of additional knowledge. This reduces
the class of models to:
</bodyText>
<equation confidence="0.985953">
p = f(u,v) (2)
</equation>
<bodyText confidence="0.999965571428571">
However, this still leaves the particular form of the
function f unspecified. Now, if we assume that p
lies in the same space as u and v, avoiding the issues
of dimensionality associated with tensor products,
and that f is a linear function, for simplicity, of the
cartesian product of u and v, then we generate a class
of additive models:
</bodyText>
<equation confidence="0.953518">
p = Au +Bv (3)
</equation>
<bodyText confidence="0.9992732">
where A and B are matrices which determine the
contributions made by u and v to the product p. In
contrast, if we assume that f is a linear function of
the tensor product of u and v, then we obtain multi-
plicative models:
</bodyText>
<equation confidence="0.94872">
p = Cuv (4)
</equation>
<bodyText confidence="0.9999475">
where C is a tensor of rank 3, which projects the
tensor product of u and v onto the space of p.
Further constraints can be introduced to reduce
the free parameters in these models. So, if we as-
sume that only the ith components of u and v con-
tribute to the ith component of p, that these com-
ponents are not dependent on i, and that the func-
tion is symmetric with regard to the interchange of u
</bodyText>
<page confidence="0.98493">
238
</page>
<bodyText confidence="0.9778215">
and v, we obtain a simpler instantiation of an addi-
tive model:
</bodyText>
<equation confidence="0.944656">
pi = ui +vi (5)
</equation>
<bodyText confidence="0.999644">
Analogously, under the same assumptions, we ob-
tain the following simpler multiplicative model:
only the ith components of u and v contribute to the
ith component of p. Another class of models can be
derived by relaxing this constraint. To give a con-
crete example, circular convolution is an instance of
the general multiplicative model which breaks this
constraint by allowing uj to contribute to pi:
</bodyText>
<equation confidence="0.975639">
pi = ui · vi (6)
</equation>
<bodyText confidence="0.999969285714286">
For example, according to (5), the addition of the
two vectors representing horse and run in Fig-
ure 1 would yield horse + run = [1 14 6 14 4].
Whereas their product, as given by (6), is
horse · run = [0 48 8 40 0].
Although the composition model in (5) is com-
monly used in the literature, from a linguistic per-
spective, the model in (6) is more appealing. Sim-
ply adding the vectors u and v lumps their contents
together rather than allowing the content of one vec-
tor to pick out the relevant content of the other. In-
stead, it could be argued that the contribution of the
ith component of u should be scaled according to its
relevance to v, and vice versa. In effect, this is what
model (6) achieves.
As a result of the assumption of symmetry, both
these models are ‘bag of words’ models and word
order insensitive. Relaxing the assumption of sym-
metry in the case of the simple additive model pro-
duces a model which weighs the contribution of the
two components differently:
</bodyText>
<equation confidence="0.657181">
pi = αui +βvi (7)
</equation>
<bodyText confidence="0.9999343">
This allows additive models to become more
syntax aware, since semantically important con-
stituents can participate more actively in the com-
position. As an example if we set α to 0.4
and β to 0.6, then horse= [0 2.4 0.8 4 1.6]
and run = [0.6 4.8 2.4 2.4 0], and their sum
horse + run = [0.6 5.6 3.2 6.4 1.6].
An extreme form of this differential in the contri-
bution of constituents is where one of the vectors,
say u, contributes nothing at all to the combination:
</bodyText>
<equation confidence="0.878824">
pi = vj (8)
</equation>
<bodyText confidence="0.9997355">
Admittedly the model in (8) is impoverished and
rather simplistic, however it can serve as a simple
baseline against which to compare more sophisti-
cated models.
The models considered so far assume that com-
ponents do not ‘interfere’ with each other, i.e., that
</bodyText>
<equation confidence="0.979678">
pi =∑ uj · vi−j (9)
j
</equation>
<bodyText confidence="0.999952555555556">
It is also possible to re-introduce the dependence
on K into the model of vector composition. For ad-
ditive models, a natural way to achieve this is to in-
clude further vectors into the summation. These vec-
tors are not arbitrary and ideally they must exhibit
some relation to the words of the construction under
consideration. When modeling predicate-argument
structures, Kintsch (2001) proposes including one or
more distributional neighbors, n, of the predicate:
</bodyText>
<equation confidence="0.977856">
p = u+v+∑ n (10)
</equation>
<bodyText confidence="0.991112538461538">
Note that considerable latitude is allowed in select-
ing the appropriate neighbors. Kintsch (2001) con-
siders only the m most similar neighbors to the pred-
icate, from which he subsequently selects k, those
most similar to its argument. So, if in the composi-
tion of horse with run, the chosen neighbor is ride,
ride = [2 15 7 9 1], then this produces the repre-
sentation horse + run + ride = [3 29 13 23 5]. In
contrast to the simple additive model, this extended
model is sensitive to syntactic structure, since n is
chosen from among the neighbors of the predicate,
distinguishing it from the argument.
Although we have presented multiplicative and
additive models separately, there is nothing inherent
in our formulation that disallows their combination.
The proposal is not merely notational. One poten-
tial drawback of multiplicative models is the effect
of components with value zero. Since the product
of zero with any number is itself zero, the presence
of zeroes in either of the vectors leads to informa-
tion being essentially thrown away. Combining the
multiplicative model with an additive model, which
does not suffer from this problem, could mitigate
this problem:
pi = αui +βvi +γuivi (11)
where α, β, and γ are weighting constants.
</bodyText>
<page confidence="0.996603">
239
</page>
<sectionHeader confidence="0.998854" genericHeader="method">
4 Evaluation Set-up
</sectionHeader>
<bodyText confidence="0.987213425531915">
We evaluated the models presented in Section 3
on a sentence similarity task initially proposed by
Kintsch (2001). In his study, Kintsch builds a model
of how a verb’s meaning is modified in the context of
its subject. He argues that the subjects of ran in The
color ran and The horse ran select different senses
of ran. This change in the verb’s sense is equated to
a shift in its position in semantic space. To quantify
this shift, Kintsch proposes measuring similarity rel-
ative to other verbs acting as landmarks, for example
gallop and dissolve. The idea here is that an appro-
priate composition model when applied to horse and
ran will yield a vector closer to the landmark gallop
than dissolve. Conversely, when color is combined
with ran, the resulting vector will be closer to dis-
solve than gallop.
Focusing on a single compositional structure,
namely intransitive verbs and their subjects, is a
good point of departure for studying vector combi-
nation. Any adequate model of composition must be
able to represent argument-verb meaning. Moreover
by using a minimal structure we factor out inessen-
tial degrees of freedom and are able to assess the
merits of different models on an equal footing. Un-
fortunately, Kintsch (2001) demonstrates how his
own composition algorithm works intuitively on a
few hand selected examples but does not provide a
comprehensive test set. In order to establish an inde-
pendent measure of sentence similarity, we assem-
bled a set of experimental materials and elicited sim-
ilarity ratings from human subjects. In the following
we describe our data collection procedure and give
details on how our composition models were con-
structed and evaluated.
Materials and Design Our materials consisted
of sentences with an an intransitive verb and its sub-
ject. We first compiled a list of intransitive verbs
from CELEX2. All occurrences of these verbs with
a subject noun were next extracted from a RASP
parsed (Briscoe and Carroll, 2002) version of the
British National Corpus (BNC). Verbs and nouns
that were attested less than fifty times in the BNC
were removed as they would result in unreliable vec-
tors. Each reference subject-verb tuple (e.g., horse
ran) was paired with two landmarks, each a syn-
onym of the verb. The landmarks were chosen so
as to represent distinct verb senses, one compatible
</bodyText>
<footnote confidence="0.843261">
2http://www.ru.nl/celex/
</footnote>
<bodyText confidence="0.994463863636364">
with the reference (e.g., horse galloped) and one in-
compatible (e.g., horse dissolved). Landmarks were
taken from WordNet (Fellbaum, 1998). Specifically,
they belonged to different synsets and were maxi-
mally dissimilar as measured by the Jiang and Con-
rath (1997) measure.3
Our initial set of candidate materials consisted
of 20 verbs, each paired with 10 nouns, and 2 land-
marks (400 pairs of sentences in total). These were
further pretested to allow the selection of a subset
of items showing clear variations in sense as we
wanted to have a balanced set of similar and dis-
similar sentences. In the pretest, subjects saw a
reference sentence containing a subject-verb tuple
and its landmarks and were asked to choose which
landmark was most similar to the reference or nei-
ther. Our items were converted into simple sentences
(all in past tense) by adding articles where appropri-
ate. The stimuli were administered to four separate
groups; each group saw one set of 100 sentences.
The pretest was completed by 53 participants.
For each reference verb, the subjects’ responses
were entered into a contingency table, whose rows
corresponded to nouns and columns to each possi-
ble answer (i.e., one of the two landmarks). Each
cell recorded the number of times our subjects se-
lected the landmark as compatible with the noun or
not. We used Fisher’s exact test to determine which
verbs and nouns showed the greatest variation in
landmark preference and items with p-values greater
than 0.001 were discarded. This yielded a reduced
set of experimental items (120 in total) consisting of
15 reference verbs, each with 4 nouns, and 2 land-
marks.
Procedure and Subjects Participants first saw
a set of instructions that explained the sentence sim-
ilarity task and provided several examples. Then
the experimental items were presented; each con-
tained two sentences, one with the reference verb
and one with its landmark. Examples of our items
are given in Table 1. Here, burn is a high similarity
landmark (High) for the reference The fire glowed,
whereas beam is a low similarity landmark (Low).
The opposite is the case for the reference The face
</bodyText>
<footnote confidence="0.995127166666667">
3We assessed a wide range of semantic similarity measures
using the WordNet similarity package (Pedersen et al., 2004).
Most of them yielded similar results. We selected Jiang and
Conrath’s measure since it has been shown to perform consis-
tently well across several cognitive and NLP tasks (Budanitsky
and Hirst, 2001).
</footnote>
<page confidence="0.987715">
240
</page>
<table confidence="0.715182857142857">
Noun Reference High Low
The fire glowed burned beamed
The face glowed beamed burned
The child strayed roamed digressed
The discussion strayed digressed roamed
The sales slumped declined slouched
The shoulders slumped slouched declined
</table>
<tableCaption confidence="0.981834">
Table 1: Example Stimuli with High and Low similarity
landmarks
</tableCaption>
<bodyText confidence="0.99917">
glowed. Sentence pairs were presented serially in
random order. Participants were asked to rate how
similar the two sentences were on a scale of one
to seven. The study was conducted remotely over
the Internet using Webexp4, a software package de-
signed for conducting psycholinguistic studies over
the web. 49 unpaid volunteers completed the exper-
iment, all native speakers of English.
Analysis of Similarity Ratings The reliability
of the collected judgments is important for our eval-
uation experiments; we therefore performed several
tests to validate the quality of the ratings. First, we
examined whether participants gave high ratings to
high similarity sentence pairs and low ratings to low
similarity ones. Figure 2 presents a box-and-whisker
plot of the distribution of the ratings. As we can see
sentences with high similarity landmarks are per-
ceived as more similar to the reference sentence. A
Wilcoxon rank sum test confirmed that the differ-
ence is statistically significant (p &lt; 0.01). We also
measured how well humans agree in their ratings.
We employed leave-one-out resampling (Weiss and
Kulikowski, 1991), by correlating the data obtained
from each participant with the ratings obtained from
all other participants. We used Spearman’s ρ, a non
parametric correlation coefficient, to avoid making
any assumptions about the distribution of the simi-
larity ratings. The average inter-subject agreement5
was ρ = 0.40. We believe that this level of agree-
ment is satisfactory given that naive subjects are
asked to provide judgments on fine-grained seman-
tic distinctions (see Table 1). More evidence that
this is not an easy task comes from Figure 2 where
we observe some overlap in the ratings for High and
Low similarity items.
</bodyText>
<footnote confidence="0.993848666666667">
4http://www.webexp.info/
5Note that Spearman’s rho tends to yield lower coefficients
compared to parametric alternatives such as Pearson’s r.
</footnote>
<figureCaption confidence="0.987188">
Figure 2: Distribution of elicited ratings for High and
Low similarity items
</figureCaption>
<bodyText confidence="0.995399617647059">
Model Parameters Irrespectively of their form,
all composition models discussed here are based on
a semantic space for representing the meanings of
individual words. The semantic space we used in
our experiments was built on a lemmatised version
of the BNC. Following previous work (Bullinaria
and Levy, 2007), we optimized its parameters on a
word-based semantic similarity task. The task in-
volves examining the degree of linear relationship
between the human judgments for two individual
words and vector-based similarity values. We ex-
perimented with a variety of dimensions (ranging
from 50 to 500,000), vector component definitions
(e.g., pointwise mutual information or log likelihood
ratio) and similarity measures (e.g., cosine or confu-
sion probability). We used WordSim353, a bench-
mark dataset (Finkelstein et al., 2002), consisting of
relatedness judgments (on a scale of 0 to 10) for 353
word pairs.
We obtained best results with a model using a
context window of five words on either side of the
target word, the cosine measure, and 2,000 vector
components. The latter were the most common con-
text words (excluding a stop list of function words).
These components were set to the ratio of the proba-
bility of the context word given the target word to
the probability of the context word overall. This
configuration gave high correlations with the Word-
Sim353 similarity judgments using the cosine mea-
sure. In addition, Bullinaria and Levy (2007) found
that these parameters perform well on a number of
other tasks such as the synonymy task from the Test
ofEnglish as a Foreign Language (TOEFL).
Our composition models have no additional pa-
</bodyText>
<figure confidence="0.996446111111111">
High Low
7
6
5
4
3
2
1
0
</figure>
<page confidence="0.995309">
241
</page>
<bodyText confidence="0.999873194444444">
rameters beyond the semantic space just described,
with three exceptions. First, the additive model
in (7) weighs differentially the contribution of the
two constituents. In our case, these are the sub-
ject noun and the intransitive verb. To this end,
we optimized the weights on a small held-out set.
Specifically, we considered eleven models, varying
in their weightings, in steps of 10%, from 100%
noun through 50% of both verb and noun to 100%
verb. For the best performing model the weight
for the verb was 80% and for the noun 20%. Sec-
ondly, we optimized the weightings in the combined
model (11) with a similar grid search over its three
parameters. This yielded a weighted sum consisting
of 95% verb, 0% noun and 5% of their multiplica-
tive combination. Finally, Kintsch’s (2001) additive
model has two extra parameters. The m neighbors
most similar to the predicate, and the k of m neigh-
bors closest to its argument. In our experiments we
selected parameters that Kintsch reports as optimal.
Specifically, m was set to 20 and m to 1.
Evaluation Methodology We evaluated the
proposed composition models in two ways. First,
we used the models to estimate the cosine simi-
larity between the reference sentence and its land-
marks. We expect better models to yield a pattern of
similarity scores like those observed in the human
ratings (see Figure 2). A more scrupulous evalua-
tion requires directly correlating all the individual
participants’ similarity judgments with those of the
models.6 We used Spearman’s p for our correlation
analyses. Again, better models should correlate bet-
ter with the experimental data. We assume that the
inter-subject agreement can serve as an upper bound
for comparing the fit of our models against the hu-
man judgments.
</bodyText>
<sectionHeader confidence="0.999821" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99981">
Our experiments assessed the performance of seven
composition models. These included three additive
models, i.e., simple addition (equation (5), Add),
weighted addition (equation (7), WeightAdd), and
Kintsch’s (2001) model (equation (10), Kintsch), a
multiplicative model (equation (6), Multiply), and
also a model which combines multiplication with
</bodyText>
<footnote confidence="0.997464">
6We avoided correlating the model predictions with aver-
aged participant judgments as this is inappropriate given the or-
dinal nature of the scale of these judgments and also leads to a
dependence between the number of participants and the magni-
tude of the correlation coefficient.
</footnote>
<table confidence="0.985549625">
Model High Low p
NonComp 0.27 0.26 0.08**
Add 0.59 0.59 0.04*
WeightAdd 0.35 0.34 0.09**
Kintsch 0.47 0.45 0.09**
Multiply 0.42 0.28 0.17**
Combined 0.38 0.28 0.19**
UpperBound 4.94 3.25 0.40**
</table>
<tableCaption confidence="0.992317">
Table 2: Model means for High and Low similarity
items and correlation coefficients with human judgments
(*: p &lt; 0.05, **: p &lt; 0.01)
</tableCaption>
<bodyText confidence="0.999739055555555">
addition (equation (11), Combined). As a baseline
we simply estimated the similarity between the ref-
erence verb and its landmarks without taking the
subject noun into account (equation (8), NonComp).
Table 2 shows the average model ratings for High
and Low similarity items. For comparison, we also
show the human ratings for these items (Upper-
Bound). Here, we are interested in relative dif-
ferences, since the two types of ratings correspond
to different scales. Model similarities have been
estimated using cosine which ranges from 0 to 1,
whereas our subjects rated the sentences on a scale
from 1 to 7.
The simple additive model fails to distinguish be-
tween High and Low Similarity items. We observe
a similar pattern for the non compositional base-
line model, the weighted additive model and Kintsch
(2001). The multiplicative and combined models
yield means closer to the human ratings. The dif-
ference between High and Low similarity values es-
timated by these models are statistically significant
(p &lt; 0.01 using the Wilcoxon rank sum test). Fig-
ure 3 shows the distribution of estimated similarities
under the multiplicative model.
The results of our correlation analysis are also
given in Table 2. As can be seen, all models are sig-
nificantly correlated with the human ratings. In or-
der to establish which ones fit our data better, we ex-
amined whether the correlation coefficients achieved
differ significantly using a t-test (Cohen and Cohen,
1983). The lowest correlation (p = 0.04) is observed
for the simple additive model which is not signif-
icantly different from the non-compositional base-
line model. The weighted additive model (p = 0.09)
is not significantly different from the baseline either
or Kintsch (2001) (p = 0.09). Given that the basis
</bodyText>
<page confidence="0.980059">
242
</page>
<figure confidence="0.997909714285714">
1
0.8
0.6
0.4
0.2
0
High Low
</figure>
<figureCaption confidence="0.994320333333333">
Figure 3: Distribution of predicted similarities for the
vector multiplication model on High and Low similarity
items
</figureCaption>
<bodyText confidence="0.999958583333333">
of Kintsch’s model is the summation of the verb, a
neighbor close to the verb and the noun, it is not
surprising that it produces results similar to a sum-
mation which weights the verb more heavily than
the noun. The multiplicative model yields a better
fit with the experimental data, ρ = 0.17. The com-
bined model is best overall with ρ = 0.19. However,
the difference between the two models is not statis-
tically significant. Also note that in contrast to the
combined model, the multiplicative model does not
have any free parameters and hence does not require
optimization for this particular task.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9998622">
In this paper we presented a general framework for
vector-based semantic composition. We formulated
composition as a function of two vectors and intro-
duced several models based on addition and multi-
plication. Despite the popularity of additive mod-
els, our experimental results showed the superior-
ity of models utilizing multiplicative combinations,
at least for the sentence similarity task attempted
here. We conjecture that the additive models are
not sensitive to the fine-grained meaning distinc-
tions involved in our materials. Previous applica-
tions of vector addition to document indexing (Deer-
wester et al., 1990) or essay grading (Landauer et al.,
1997) were more concerned with modeling the gist
of a document rather than the meaning of its sen-
tences. Importantly, additive models capture com-
position by considering all vector components rep-
resenting the meaning of the verb and its subject,
whereas multiplicative models consider a subset,
namely non-zero components. The resulting vector
is sparser but expresses more succinctly the meaning
of the predicate-argument structure, and thus allows
semantic similarity to be modelled more accurately.
Further research is needed to gain a deeper un-
derstanding of vector composition, both in terms of
modeling a wider range of structures (e.g., adjective-
noun, noun-noun) and also in terms of exploring the
space of models more fully. We anticipate that more
substantial correlations can be achieved by imple-
menting more sophisticated models from within the
framework outlined here. In particular, the general
class of multiplicative models (see equation (4)) ap-
pears to be a fruitful area to explore. Future direc-
tions include constraining the number of free param-
eters in linguistically plausible ways and scaling to
larger datasets.
The applications of the framework discussed here
are many and varied both for cognitive science and
NLP. We intend to assess the potential of our com-
position models on context sensitive semantic prim-
ing (Till et al., 1988) and inductive inference (Heit
and Rubinstein, 1994). NLP tasks that could benefit
from composition models include paraphrase iden-
tification and context-dependent language modeling
(Coccaro and Jurafsky, 1998).
</bodyText>
<sectionHeader confidence="0.998602" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999187636363636">
E. Briscoe, J. Carroll. 2002. Robust accurate statistical
annotation of general text. In Proceedings of the 3rd
International Conference on Language Resources and
Evaluation, 1499–1504, Las Palmas, Canary Islands.
A. Budanitsky, G. Hirst. 2001. Semantic distance in
WordNet: An experimental, application-oriented eval-
uation of five measures. In Proceedings ofACL Work-
shop on WordNet and Other Lexical Resources, Pitts-
burgh, PA.
J. Bullinaria, J. Levy. 2007. Extracting semantic rep-
resentations from word co-occurrence statistics: A
computational study. Behavior Research Methods,
39:510–526.
F. Choi, P. Wiemer-Hastings, J. Moore. 2001. Latent se-
mantic analysis for text segmentation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, 109–117, Pittsburgh, PA.
N. Coccaro, D. Jurafsky. 1998. Towards better integra-
tion of semantic predictors in statistical language mod-
eling. In Proceedings of the 5th International Confer-
ence on Spoken Language Processsing, Sydney, Aus-
tralia.
</reference>
<page confidence="0.992874">
243
</page>
<reference confidence="0.999893584905661">
J. Cohen, P. Cohen. 1983. Applied Multiple Regres-
sion/Correlation Analysis for the Behavioral Sciences.
Hillsdale, NJ: Erlbaum.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391–407.
G. Denhire, B. Lemaire. 2004. A computational model
of children’s semantic memory. In Proceedings of the
26th Annual Meeting of the Cognitive Science Society,
297–302, Chicago, IL.
C. Fellbaum, ed. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, E. Ruppin. 2002. Placing
search in context: The concept revisited. ACM Trans-
actions on Information Systems, 20(1):116–131.
J. Fodor, Z. Pylyshyn. 1988. Connectionism and cogni-
tive architecture: A critical analysis. Cognition, 28:3–
71.
P. W. Foltz, W. Kintsch, T. K. Landauer. 1998. The
measurement of textual coherence with latent semantic
analysis. Discourse Process, 15:285–307.
S. Frank, M. Koppen, L. Noordman, W. Vonk. 2007.
World knowledge in computational models of dis-
course comprehension. Discourse Processes. In press.
G. Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley, New York.
E. Heit, J. Rubinstein. 1994. Similarity and property ef-
fects in inductive reasoning. Journal of Experimen-
tal Psychology: Learning, Memory, and Cognition,
20:411–422.
J. J. Jiang, D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of International Conference on Research
in Computational Linguistics, Taiwan.
W. Kintsch. 2001. Predication. Cognitive Science,
25(2):173–202.
T. K. Landauer, S. T. Dumais. 1997. A solution to Plato’s
problem: the latent semantic analysis theory of ac-
quisition, induction and representation of knowledge.
Psychological Review, 104(2):211–240.
T. K. Landauer, D. Laham, B. Rehder, M. E. Schreiner.
1997. How well can passage meaning be derived with-
out using word order: A comparison of latent semantic
analysis and humans. In Proceedings of 19th Annual
Conference of the Cognitive Science Society, 412–417,
Stanford, CA.
K. Lund, C. Burgess. 1996. Producing high-dimensional
semantic spaces from lexical co-occurrence. Be-
havior Research Methods, Instruments &amp; Computers,
28:203–208.
D. McCarthy, R. Koeling, J. Weeds, J. Carroll. 2004.
Finding predominant senses in untagged text. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, 280–287,
Barcelona, Spain.
S. McDonald. 2000. Environmental Determinants of
Lexical Processing Effort. Ph.D. thesis, University of
Edinburgh.
R. Montague. 1974. English as a formal language. In
R. Montague, ed., Formal Philosophy. Yale University
Press, New Haven, CT.
H. Neville, J. L. Nichol, A. Barss, K. I. Forster, M. F. Gar-
rett. 1991. Syntactically based sentence prosessing
classes: evidence form event-related brain potentials.
Journal of Congitive Neuroscience, 3:151–165.
S. Pad´o, M. Lapata. 2007. Dependency-based construc-
tion of semantic space models. Computational Lin-
guistics, 33(2):161–199.
T. Pedersen, S. Patwardhan, J. Michelizzi. 2004. Word-
Net::similarity - measuring the relatedness of con-
cepts. In Proceedings of the 5th Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, 38–41, Boston, MA.
T. A. Plate. 1991. Holographic reduced representations:
Convolution algebra for compositional distributed rep-
resentations. In Proceedings of the 12th Interna-
tional Joint Conference on Artificial Intelligence, 30–
35, Sydney, Australia.
G. Salton, A. Wong, C. S. Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18(11):613–620.
P. Schone, D. Jurafsky. 2001. Is knowledge-free induc-
tion of multiword unit dictionary headwords a solved
problem? In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, 100–
108, Pittsburgh, PA.
H. Sch¨utze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97–124.
P. Smolensky. 1990. Tensor product variable binding and
the representation of symbolic structures in connec-
tionist systems. Artificial Intelligence, 46:159–216.
R. E. Till, E. F. Mross, W. Kintsch. 1988. Time course of
priming for associate and inference words in discourse
context. Memory and Cognition, 16:283–299.
S. M. Weiss, C. A. Kulikowski. 1991. Computer Sys-
tems that Learn: Classification and Prediction Meth-
ods from Statistics, Neural Nets, Machine Learning,
and Expert Systems. Morgan Kaufmann, San Mateo,
CA.
R. F. West, K. E. Stanovich. 1986. Robust effects of
syntactic structure on visual word processing. Journal
ofMemory and Cognition, 14:104–112.
</reference>
<page confidence="0.998536">
244
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.949692">
<title confidence="0.999313">Vector-based Models of Semantic Composition</title>
<author confidence="0.999816">Mitchell Lapata</author>
<affiliation confidence="0.99996">School of Informatics, University of Edinburgh</affiliation>
<address confidence="0.964717">2 Buccleuch Place, Edinburgh EH8 9LW, UK</address>
<abstract confidence="0.998865153846154">This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation, 1499–1504, Las</booktitle>
<location>Palmas, Canary Islands.</location>
<contexts>
<context position="18246" citStr="Briscoe and Carroll, 2002" startWordPosition="3024" endWordPosition="3027">s not provide a comprehensive test set. In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects. In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated. Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject. We first compiled a list of intransitive verbs from CELEX2. All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll, 2002) version of the British National Corpus (BNC). Verbs and nouns that were attested less than fifty times in the BNC were removed as they would result in unreliable vectors. Each reference subject-verb tuple (e.g., horse ran) was paired with two landmarks, each a synonym of the verb. The landmarks were chosen so as to represent distinct verb senses, one compatible 2http://www.ru.nl/celex/ with the reference (e.g., horse galloped) and one incompatible (e.g., horse dissolved). Landmarks were taken from WordNet (Fellbaum, 1998). Specifically, they belonged to different synsets and were maximally di</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>E. Briscoe, J. Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, 1499–1504, Las Palmas, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL Workshop on WordNet and Other Lexical Resources,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="21090" citStr="Budanitsky and Hirst, 2001" startWordPosition="3491" endWordPosition="3494"> contained two sentences, one with the reference verb and one with its landmark. Examples of our items are given in Table 1. Here, burn is a high similarity landmark (High) for the reference The fire glowed, whereas beam is a low similarity landmark (Low). The opposite is the case for the reference The face 3We assessed a wide range of semantic similarity measures using the WordNet similarity package (Pedersen et al., 2004). Most of them yielded similar results. We selected Jiang and Conrath’s measure since it has been shown to perform consistently well across several cognitive and NLP tasks (Budanitsky and Hirst, 2001). 240 Noun Reference High Low The fire glowed burned beamed The face glowed beamed burned The child strayed roamed digressed The discussion strayed digressed roamed The sales slumped declined slouched The shoulders slumped slouched declined Table 1: Example Stimuli with High and Low similarity landmarks glowed. Sentence pairs were presented serially in random order. Participants were asked to rate how similar the two sentences were on a scale of one to seven. The study was conducted remotely over the Internet using Webexp4, a software package designed for conducting psycholinguistic studies ov</context>
</contexts>
<marker>Budanitsky, Hirst, 2001</marker>
<rawString>A. Budanitsky, G. Hirst. 2001. Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures. In Proceedings ofACL Workshop on WordNet and Other Lexical Resources, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bullinaria</author>
<author>J Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<pages>39--510</pages>
<contexts>
<context position="23663" citStr="Bullinaria and Levy, 2007" startWordPosition="3889" endWordPosition="3892">mes from Figure 2 where we observe some overlap in the ratings for High and Low similarity items. 4http://www.webexp.info/ 5Note that Spearman’s rho tends to yield lower coefficients compared to parametric alternatives such as Pearson’s r. Figure 2: Distribution of elicited ratings for High and Low similarity items Model Parameters Irrespectively of their form, all composition models discussed here are based on a semantic space for representing the meanings of individual words. The semantic space we used in our experiments was built on a lemmatised version of the BNC. Following previous work (Bullinaria and Levy, 2007), we optimized its parameters on a word-based semantic similarity task. The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values. We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability). We used WordSim353, a benchmark dataset (Finkelstein et al., 2002), consisting of relatedness judgments (on a scale of 0 to 10) for 353 word pairs.</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>J. Bullinaria, J. Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Choi</author>
<author>P Wiemer-Hastings</author>
<author>J Moore</author>
</authors>
<title>Latent semantic analysis for text segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>109--117</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1443" citStr="Choi et al., 2001" startWordPosition="202" endWordPosition="205">come increasingly popular in natural language processing (NLP) and cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968). A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975). In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998). Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004). Despite their widespread use, vector-based models are typically directed at representing words in isolation and m</context>
</contexts>
<marker>Choi, Wiemer-Hastings, Moore, 2001</marker>
<rawString>F. Choi, P. Wiemer-Hastings, J. Moore. 2001. Latent semantic analysis for text segmentation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 109–117, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Coccaro</author>
<author>D Jurafsky</author>
</authors>
<title>Towards better integration of semantic predictors in statistical language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the 5th International Conference on Spoken Language Processsing,</booktitle>
<location>Sydney, Australia.</location>
<marker>Coccaro, Jurafsky, 1998</marker>
<rawString>N. Coccaro, D. Jurafsky. 1998. Towards better integration of semantic predictors in statistical language modeling. In Proceedings of the 5th International Conference on Spoken Language Processsing, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
<author>P Cohen</author>
</authors>
<title>Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences.</title>
<date>1983</date>
<publisher>Erlbaum.</publisher>
<location>Hillsdale, NJ:</location>
<contexts>
<context position="29206" citStr="Cohen and Cohen, 1983" startWordPosition="4796" endWordPosition="4799">d combined models yield means closer to the human ratings. The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test). Figure 3 shows the distribution of estimated similarities under the multiplicative model. The results of our correlation analysis are also given in Table 2. As can be seen, all models are significantly correlated with the human ratings. In order to establish which ones fit our data better, we examined whether the correlation coefficients achieved differ significantly using a t-test (Cohen and Cohen, 1983). The lowest correlation (p = 0.04) is observed for the simple additive model which is not significantly different from the non-compositional baseline model. The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09). Given that the basis 242 1 0.8 0.6 0.4 0.2 0 High Low Figure 3: Distribution of predicted similarities for the vector multiplication model on High and Low similarity items of Kintsch’s model is the summation of the verb, a neighbor close to the verb and the noun, it is not surprising that it produces results similar</context>
</contexts>
<marker>Cohen, Cohen, 1983</marker>
<rawString>J. Cohen, P. Cohen. 1983. Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences. Hillsdale, NJ: Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Deerwester</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
<author>G W Furnas</author>
<author>R A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="30894" citStr="Deerwester et al., 1990" startWordPosition="5071" endWordPosition="5075">sion In this paper we presented a general framework for vector-based semantic composition. We formulated composition as a function of two vectors and introduced several models based on addition and multiplication. Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here. We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials. Previous applications of vector addition to document indexing (Deerwester et al., 1990) or essay grading (Landauer et al., 1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences. Importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components. The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately. Further research is needed to gain a deeper understanding of ve</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, R. A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Denhire</author>
<author>B Lemaire</author>
</authors>
<title>A computational model of children’s semantic memory.</title>
<date>2004</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Cognitive Science Society,</booktitle>
<location>297–302, Chicago, IL.</location>
<contexts>
<context position="1928" citStr="Denhire and Lemaire, 2004" startWordPosition="273" endWordPosition="276">, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975). In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998). Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004). Despite their widespread use, vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature. In fact, the commonest method for combining the vectors is to average them. Vector averaging is unfortunately insensitive to word order, and more generally syntactic structure, giving the same representation to any constructions that happen to share the same vocabulary. This is illustrated in the example below taken from Landauer et al. (1997). Sentences (1-a) and (1-</context>
</contexts>
<marker>Denhire, Lemaire, 2004</marker>
<rawString>G. Denhire, B. Lemaire. 2004. A computational model of children’s semantic memory. In Proceedings of the 26th Annual Meeting of the Cognitive Science Society, 297–302, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
<author>ed</author>
</authors>
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Fellbaum, ed, 1998</marker>
<rawString>C. Fellbaum, ed. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="24182" citStr="Finkelstein et al., 2002" startWordPosition="3965" endWordPosition="3968">eriments was built on a lemmatised version of the BNC. Following previous work (Bullinaria and Levy, 2007), we optimized its parameters on a word-based semantic similarity task. The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values. We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability). We used WordSim353, a benchmark dataset (Finkelstein et al., 2002), consisting of relatedness judgments (on a scale of 0 to 10) for 353 word pairs. We obtained best results with a model using a context window of five words on either side of the target word, the cosine measure, and 2,000 vector components. The latter were the most common context words (excluding a stop list of function words). These components were set to the ratio of the probability of the context word given the target word to the probability of the context word overall. This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure. In addition, </context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, E. Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fodor</author>
<author>Z Pylyshyn</author>
</authors>
<title>Connectionism and cognitive architecture: A critical analysis.</title>
<date>1988</date>
<journal>Cognition,</journal>
<volume>28</volume>
<pages>71</pages>
<contexts>
<context position="4848" citStr="Fodor and Pylyshyn, 1988" startWordPosition="721" endWordPosition="724">neral framework for vector-based composition which allows us to consider different classes of models. Specifically, we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment. Our results show that the multiplicative models are superior and correlate significantly with behavioral data. 2 Related Work The problem of vector composition has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Fodor and Pylyshyn, 1988). While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects. For the hierarchical structure of natural language this binding problem becomes particularly acute. For example, simplistic approaches to handling sentences such as John loves Mary and Mary loves John typically fail to make valid representations in one of two ways. Either there is a failure to distinguish between these two structures, because the network fails to keep track of the fact that John </context>
</contexts>
<marker>Fodor, Pylyshyn, 1988</marker>
<rawString>J. Fodor, Z. Pylyshyn. 1988. Connectionism and cognitive architecture: A critical analysis. Cognition, 28:3– 71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Foltz</author>
<author>W Kintsch</author>
<author>T K Landauer</author>
</authors>
<title>The measurement of textual coherence with latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Process,</booktitle>
<pages>15--285</pages>
<contexts>
<context position="1718" citStr="Foltz et al., 1998" startWordPosition="244" endWordPosition="247">tically similar (Harris, 1968). A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975). In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998). Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004). Despite their widespread use, vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature. In fact, the commonest method for combining the vectors is to average them. Vector averaging is unfortunately insensitive to word order, and more generally synt</context>
<context position="3007" citStr="Foltz et al., 1998" startWordPosition="449" endWordPosition="452">hat happen to share the same vocabulary. This is illustrated in the example below taken from Landauer et al. (1997). Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different. (1) a. It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b. That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious. While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998), there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and 236 Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Rubinstein, 1994). Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences. Central </context>
<context position="7477" citStr="Foltz et al., 1998" startWordPosition="1156" endWordPosition="1159">circular correlation which is the approximate inverse of circular convolution. The success of circular correlation crucially depends on the components of the n-dimensional vectors u and v being randomly distributed with mean 0 and variance 1n. This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure. Vector addition is by far the most common method for representing the meaning of linguistic sequences. For example, assuming that individual words are represented by vectors, we can compute the meaning of a sentence by taking their mean (Foltz et al., 1998; Landauer and Dumais, 1997). Vector addition does not increase the dimensionality of the resulting vector. However, since it is order independent, it cannot capture meaning differences that are modulated by differences in syntactic structure. Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g., run) varies depending on the arguments it operates upon (e.g, the horse ran vs. the color ran). The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of the</context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>P. W. Foltz, W. Kintsch, T. K. Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse Process, 15:285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Frank</author>
<author>M Koppen</author>
<author>L Noordman</author>
<author>W Vonk</author>
</authors>
<title>World knowledge in computational models of discourse comprehension. Discourse Processes.</title>
<date>2007</date>
<note>In press.</note>
<contexts>
<context position="8626" citStr="Frank et al. (2007)" startWordPosition="1352" endWordPosition="1355">te and its argument but also the neighbors associated with both of them. The neighbors, Kintsch argues, can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. 237 animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Figure 1: A hypothetical semantic space for horse and run Unfortunately, comparisons across vector composition models have been few and far between in the literature. The merits of different approaches are illustrated with a few hand picked examples and parameter values and large scale evaluations are uniformly absent (see Frank et al. (2007) for a criticism of Kintsch’s (2001) evaluation standards). Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination). Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology. 3 Composition Models We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a cor</context>
</contexts>
<marker>Frank, Koppen, Noordman, Vonk, 2007</marker>
<rawString>S. Frank, M. Koppen, L. Noordman, W. Vonk. 2007. World knowledge in computational models of discourse comprehension. Discourse Processes. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1265" citStr="Grefenstette, 1994" startWordPosition="178" endWordPosition="179"> to the additive alternatives when compared against human judgments. 1 Introduction Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968). A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975). In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998). Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonal</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>G. Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="1129" citStr="Harris, 1968" startWordPosition="158" endWordPosition="159">e evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments. 1 Introduction Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968). A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975). In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998). Moreover,</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Z. Harris. 1968. Mathematical Structures of Language. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Heit</author>
<author>J Rubinstein</author>
</authors>
<title>Similarity and property effects in inductive reasoning.</title>
<date>1994</date>
<journal>Journal of Experimental Psychology: Learning, Memory, and Cognition,</journal>
<pages>20--411</pages>
<marker>Heit, Rubinstein, 1994</marker>
<rawString>E. Heit, J. Rubinstein. 1994. Similarity and property effects in inductive reasoning. Journal of Experimental Psychology: Learning, Memory, and Cognition, 20:411–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of International Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="18898" citStr="Jiang and Conrath (1997)" startWordPosition="3126" endWordPosition="3130">onal Corpus (BNC). Verbs and nouns that were attested less than fifty times in the BNC were removed as they would result in unreliable vectors. Each reference subject-verb tuple (e.g., horse ran) was paired with two landmarks, each a synonym of the verb. The landmarks were chosen so as to represent distinct verb senses, one compatible 2http://www.ru.nl/celex/ with the reference (e.g., horse galloped) and one incompatible (e.g., horse dissolved). Landmarks were taken from WordNet (Fellbaum, 1998). Specifically, they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs, each paired with 10 nouns, and 2 landmarks (400 pairs of sentences in total). These were further pretested to allow the selection of a subset of items showing clear variations in sense as we wanted to have a balanced set of similar and dissimilar sentences. In the pretest, subjects saw a reference sentence containing a subject-verb tuple and its landmarks and were asked to choose which landmark was most similar to the reference or neither. Our items were converted into simple sentences (all in past tense) by adding articl</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. J. Jiang, D. W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="7735" citStr="Kintsch (2001)" startWordPosition="1196" endWordPosition="1197">s for modeling linguistic data which is typically represented by vectors with non-random structure. Vector addition is by far the most common method for representing the meaning of linguistic sequences. For example, assuming that individual words are represented by vectors, we can compute the meaning of a sentence by taking their mean (Foltz et al., 1998; Landauer and Dumais, 1997). Vector addition does not increase the dimensionality of the resulting vector. However, since it is order independent, it cannot capture meaning differences that are modulated by differences in syntactic structure. Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g., run) varies depending on the arguments it operates upon (e.g, the horse ran vs. the color ran). The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them. The neighbors, Kintsch argues, can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. 237 animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Figure 1: A hypothetical semantic space for horse and</context>
<context position="14923" citStr="Kintsch (2001)" startWordPosition="2470" endWordPosition="2471">er simplistic, however it can serve as a simple baseline against which to compare more sophisticated models. The models considered so far assume that components do not ‘interfere’ with each other, i.e., that pi =∑ uj · vi−j (9) j It is also possible to re-introduce the dependence on K into the model of vector composition. For additive models, a natural way to achieve this is to include further vectors into the summation. These vectors are not arbitrary and ideally they must exhibit some relation to the words of the construction under consideration. When modeling predicate-argument structures, Kintsch (2001) proposes including one or more distributional neighbors, n, of the predicate: p = u+v+∑ n (10) Note that considerable latitude is allowed in selecting the appropriate neighbors. Kintsch (2001) considers only the m most similar neighbors to the predicate, from which he subsequently selects k, those most similar to its argument. So, if in the composition of horse with run, the chosen neighbor is ride, ride = [2 15 7 9 1], then this produces the representation horse + run + ride = [3 29 13 23 5]. In contrast to the simple additive model, this extended model is sensitive to syntactic structure, s</context>
<context position="16398" citStr="Kintsch (2001)" startWordPosition="2718" endWordPosition="2719">l is not merely notational. One potential drawback of multiplicative models is the effect of components with value zero. Since the product of zero with any number is itself zero, the presence of zeroes in either of the vectors leads to information being essentially thrown away. Combining the multiplicative model with an additive model, which does not suffer from this problem, could mitigate this problem: pi = αui +βvi +γuivi (11) where α, β, and γ are weighting constants. 239 4 Evaluation Set-up We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001). In his study, Kintsch builds a model of how a verb’s meaning is modified in the context of its subject. He argues that the subjects of ran in The color ran and The horse ran select different senses of ran. This change in the verb’s sense is equated to a shift in its position in semantic space. To quantify this shift, Kintsch proposes measuring similarity relative to other verbs acting as landmarks, for example gallop and dissolve. The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve. Conversely, w</context>
<context position="28561" citStr="Kintsch (2001)" startWordPosition="4693" endWordPosition="4694">p). Table 2 shows the average model ratings for High and Low similarity items. For comparison, we also show the human ratings for these items (UpperBound). Here, we are interested in relative differences, since the two types of ratings correspond to different scales. Model similarities have been estimated using cosine which ranges from 0 to 1, whereas our subjects rated the sentences on a scale from 1 to 7. The simple additive model fails to distinguish between High and Low Similarity items. We observe a similar pattern for the non compositional baseline model, the weighted additive model and Kintsch (2001). The multiplicative and combined models yield means closer to the human ratings. The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test). Figure 3 shows the distribution of estimated similarities under the multiplicative model. The results of our correlation analysis are also given in Table 2. As can be seen, all models are significantly correlated with the human ratings. In order to establish which ones fit our data better, we examined whether the correlation coefficients achieved differ signifi</context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>W. Kintsch. 2001. Predication. Cognitive Science, 25(2):173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="817" citStr="Landauer and Dumais, 1997" startWordPosition="109" endWordPosition="112">p@inf.ed.ac.uk Abstract This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments. 1 Introduction Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968). A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmen</context>
<context position="2961" citStr="Landauer and Dumais, 1997" startWordPosition="442" endWordPosition="445">giving the same representation to any constructions that happen to share the same vocabulary. This is illustrated in the example below taken from Landauer et al. (1997). Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different. (1) a. It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b. That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious. While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998), there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and 236 Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Rubinstein, 1994). Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for</context>
<context position="7505" citStr="Landauer and Dumais, 1997" startWordPosition="1160" endWordPosition="1163"> which is the approximate inverse of circular convolution. The success of circular correlation crucially depends on the components of the n-dimensional vectors u and v being randomly distributed with mean 0 and variance 1n. This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure. Vector addition is by far the most common method for representing the meaning of linguistic sequences. For example, assuming that individual words are represented by vectors, we can compute the meaning of a sentence by taking their mean (Foltz et al., 1998; Landauer and Dumais, 1997). Vector addition does not increase the dimensionality of the resulting vector. However, since it is order independent, it cannot capture meaning differences that are modulated by differences in syntactic structure. Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g., run) varies depending on the arguments it operates upon (e.g, the horse ran vs. the color ran). The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them. The neighbors, Kintsch ar</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. K. Landauer, S. T. Dumais. 1997. A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>D Laham</author>
<author>B Rehder</author>
<author>M E Schreiner</author>
</authors>
<title>How well can passage meaning be derived without using word order: A comparison of latent semantic analysis and humans.</title>
<date>1997</date>
<booktitle>In Proceedings of 19th Annual Conference of the Cognitive Science Society,</booktitle>
<location>412–417, Stanford, CA.</location>
<contexts>
<context position="2503" citStr="Landauer et al. (1997)" startWordPosition="361" endWordPosition="365">d association norms (Denhire and Lemaire, 2004). Despite their widespread use, vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature. In fact, the commonest method for combining the vectors is to average them. Vector averaging is unfortunately insensitive to word order, and more generally syntactic structure, giving the same representation to any constructions that happen to share the same vocabulary. This is illustrated in the example below taken from Landauer et al. (1997). Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different. (1) a. It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b. That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious. While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998), there is ample empirical evidence that syntactic relations across and within sentences are cru</context>
<context position="30935" citStr="Landauer et al., 1997" startWordPosition="5079" endWordPosition="5082">ramework for vector-based semantic composition. We formulated composition as a function of two vectors and introduced several models based on addition and multiplication. Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here. We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials. Previous applications of vector addition to document indexing (Deerwester et al., 1990) or essay grading (Landauer et al., 1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences. Importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components. The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately. Further research is needed to gain a deeper understanding of vector composition, both in terms of modeli</context>
</contexts>
<marker>Landauer, Laham, Rehder, Schreiner, 1997</marker>
<rawString>T. K. Landauer, D. Laham, B. Rehder, M. E. Schreiner. 1997. How well can passage meaning be derived without using word order: A comparison of latent semantic analysis and humans. In Proceedings of 19th Annual Conference of the Cognitive Science Society, 412–417, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical co-occurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments &amp; Computers,</journal>
<pages>28--203</pages>
<contexts>
<context position="789" citStr="Lund and Burgess, 1996" startWordPosition="105" endWordPosition="108">ff.mitchell@ed.ac.uk,mlap@inf.ed.ac.uk Abstract This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments. 1 Introduction Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968). A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and </context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>K. Lund, C. Burgess. 1996. Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments &amp; Computers, 28:203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Koeling</author>
<author>J Weeds</author>
<author>J Carroll</author>
</authors>
<title>Finding predominant senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>280--287</pages>
<location>Barcelona,</location>
<contexts>
<context position="1352" citStr="McCarthy et al., 2004" startWordPosition="188" endWordPosition="191">Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968). A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975). In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998). Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004). Despite their widespre</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>D. McCarthy, R. Koeling, J. Weeds, J. Carroll. 2004. Finding predominant senses in untagged text. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, 280–287, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S McDonald</author>
</authors>
<title>Environmental Determinants of Lexical Processing Effort.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1873" citStr="McDonald, 2000" startWordPosition="266" endWordPosition="268">, 1994), word sense discrimination (Sch¨utze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975). In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998). Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004). Despite their widespread use, vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature. In fact, the commonest method for combining the vectors is to average them. Vector averaging is unfortunately insensitive to word order, and more generally syntactic structure, giving the same representation to any constructions that happen to share the same vocabulary. This is illustrated in the example below tak</context>
</contexts>
<marker>McDonald, 2000</marker>
<rawString>S. McDonald. 2000. Environmental Determinants of Lexical Processing Effort. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>English as a formal language. In</title>
<date>1974</date>
<editor>R. Montague, ed., Formal Philosophy.</editor>
<publisher>Yale University Press,</publisher>
<location>New Haven, CT.</location>
<contexts>
<context position="3535" citStr="Montague, 1974" startWordPosition="525" endWordPosition="526">essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998), there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and 236 Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Rubinstein, 1994). Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences. Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them. Here, semantic analysis is guided by syntactic structure, and therefore sentences (1-a) and (1-b) receive distinct representations. The downside of this approach is that differences in meaning are qualitative rather than quantitative, and degrees of similarity cannot be expressed easily. In this paper we examine models of semantic compositi</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>R. Montague. 1974. English as a formal language. In R. Montague, ed., Formal Philosophy. Yale University Press, New Haven, CT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Neville</author>
<author>J L Nichol</author>
<author>A Barss</author>
<author>K I Forster</author>
<author>M F Garrett</author>
</authors>
<title>Syntactically based sentence prosessing classes: evidence form event-related brain potentials.</title>
<date>1991</date>
<journal>Journal of Congitive Neuroscience,</journal>
<pages>3--151</pages>
<contexts>
<context position="3167" citStr="Neville et al., 1991" startWordPosition="472" endWordPosition="475">the same set of words but their meaning is entirely different. (1) a. It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b. That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious. While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998), there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and 236 Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Rubinstein, 1994). Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences. Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the ru</context>
</contexts>
<marker>Neville, Nichol, Barss, Forster, Garrett, 1991</marker>
<rawString>H. Neville, J. L. Nichol, A. Barss, K. I. Forster, M. F. Garrett. 1991. Syntactically based sentence prosessing classes: evidence form event-related brain potentials. Journal of Congitive Neuroscience, 3:151–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pad´o</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>S. Pad´o, M. Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>WordNet::similarity - measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 5th Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>38–41, Boston, MA.</location>
<contexts>
<context position="20890" citStr="Pedersen et al., 2004" startWordPosition="3459" endWordPosition="3462">ks. Procedure and Subjects Participants first saw a set of instructions that explained the sentence similarity task and provided several examples. Then the experimental items were presented; each contained two sentences, one with the reference verb and one with its landmark. Examples of our items are given in Table 1. Here, burn is a high similarity landmark (High) for the reference The fire glowed, whereas beam is a low similarity landmark (Low). The opposite is the case for the reference The face 3We assessed a wide range of semantic similarity measures using the WordNet similarity package (Pedersen et al., 2004). Most of them yielded similar results. We selected Jiang and Conrath’s measure since it has been shown to perform consistently well across several cognitive and NLP tasks (Budanitsky and Hirst, 2001). 240 Noun Reference High Low The fire glowed burned beamed The face glowed beamed burned The child strayed roamed digressed The discussion strayed digressed roamed The sales slumped declined slouched The shoulders slumped slouched declined Table 1: Example Stimuli with High and Low similarity landmarks glowed. Sentence pairs were presented serially in random order. Participants were asked to rate</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>T. Pedersen, S. Patwardhan, J. Michelizzi. 2004. WordNet::similarity - measuring the relatedness of concepts. In Proceedings of the 5th Annual Meeting of the North American Chapter of the Association for Computational Linguistics, 38–41, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A Plate</author>
</authors>
<title>Holographic reduced representations: Convolution algebra for compositional distributed representations.</title>
<date>1991</date>
<booktitle>In Proceedings of the 12th International Joint Conference on Artificial Intelligence, 30– 35,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="6448" citStr="Plate, 1991" startWordPosition="991" endWordPosition="992">oposed the use of tensor products as a means of binding one vector to another. The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely, the tensor product has dimensionality m x n). To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components. Holographic reduced representations (Plate, 1991) are one implementation of this idea where the tensor product is projected back onto the space of its components. The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors. The compression is achieved by summing along the transdiagonal elements of the tensor product. Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution. The success of circular correlation crucially depends on the components of the n-dimensional vectors u and v b</context>
</contexts>
<marker>Plate, 1991</marker>
<rawString>T. A. Plate. 1991. Holographic reduced representations: Convolution algebra for compositional distributed representations. In Proceedings of the 12th International Joint Conference on Artificial Intelligence, 30– 35, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="1501" citStr="Salton et al., 1975" startWordPosition="211" endWordPosition="214"> (NLP) and cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968). A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975). In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998). Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004). Despite their widespread use, vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sen</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, C. S. Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Schone</author>
<author>D Jurafsky</author>
</authors>
<title>Is knowledge-free induction of multiword unit dictionary headwords a solved problem?</title>
<date>2001</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 100– 108,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1404" citStr="Schone and Jurafsky, 2001" startWordPosition="195" endWordPosition="198">rgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968). A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975). In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998). Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004). Despite their widespread use, vector-based models are typically directed a</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>P. Schone, D. Jurafsky. 2001. Is knowledge-free induction of multiword unit dictionary headwords a solved problem? In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 100– 108, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>H. Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--159</pages>
<contexts>
<context position="5833" citStr="Smolensky (1990)" startWordPosition="885" endWordPosition="886">n loves Mary and Mary loves John typically fail to make valid representations in one of two ways. Either there is a failure to distinguish between these two structures, because the network fails to keep track of the fact that John is subject in one and object in the other, or there is a failure to recognize that both structures involve the same participants, because John as a subject has a distinct representation from John as an object. In contrast, symbolic representations can naturally handle the binding of constituents to their roles, in a systematic manner that avoids both these problems. Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another. The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely, the tensor product has dimensionality m x n). To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components. Holographic reduced representation</context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>P. Smolensky. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence, 46:159–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Till</author>
<author>E F Mross</author>
<author>W Kintsch</author>
</authors>
<title>Time course of priming for associate and inference words in discourse context. Memory and Cognition,</title>
<date>1988</date>
<pages>16--283</pages>
<contexts>
<context position="3266" citStr="Till et al., 1988" startWordPosition="488" endWordPosition="491">o hit the bottle that day, but the office worker with the serious drinking problem. b. That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious. While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998), there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and 236 Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Rubinstein, 1994). Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences. Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them. Here, semantic analysis is guided by syntactic structure, and therefore s</context>
</contexts>
<marker>Till, Mross, Kintsch, 1988</marker>
<rawString>R. E. Till, E. F. Mross, W. Kintsch. 1988. Time course of priming for associate and inference words in discourse context. Memory and Cognition, 16:283–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Weiss</author>
<author>C A Kulikowski</author>
</authors>
<date>1991</date>
<booktitle>Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="22516" citStr="Weiss and Kulikowski, 1991" startWordPosition="3711" endWordPosition="3714">periments; we therefore performed several tests to validate the quality of the ratings. First, we examined whether participants gave high ratings to high similarity sentence pairs and low ratings to low similarity ones. Figure 2 presents a box-and-whisker plot of the distribution of the ratings. As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence. A Wilcoxon rank sum test confirmed that the difference is statistically significant (p &lt; 0.01). We also measured how well humans agree in their ratings. We employed leave-one-out resampling (Weiss and Kulikowski, 1991), by correlating the data obtained from each participant with the ratings obtained from all other participants. We used Spearman’s ρ, a non parametric correlation coefficient, to avoid making any assumptions about the distribution of the similarity ratings. The average inter-subject agreement5 was ρ = 0.40. We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1). More evidence that this is not an easy task comes from Figure 2 where we observe some overlap in the ratings for High and Low</context>
</contexts>
<marker>Weiss, Kulikowski, 1991</marker>
<rawString>S. M. Weiss, C. A. Kulikowski. 1991. Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R F West</author>
<author>K E Stanovich</author>
</authors>
<title>Robust effects of syntactic structure on visual word processing.</title>
<date>1986</date>
<journal>Journal ofMemory and Cognition,</journal>
<pages>14--104</pages>
<contexts>
<context position="3194" citStr="West and Stanovich, 1986" startWordPosition="476" endWordPosition="479">but their meaning is entirely different. (1) a. It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b. That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious. While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998), there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and 236 Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Rubinstein, 1994). Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences. Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them. H</context>
</contexts>
<marker>West, Stanovich, 1986</marker>
<rawString>R. F. West, K. E. Stanovich. 1986. Robust effects of syntactic structure on visual word processing. Journal ofMemory and Cognition, 14:104–112.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>