<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.940361">
Generating Referring Expressions Involving Relations
</title>
<author confidence="0.942788">
Robert Dale
</author>
<affiliation confidence="0.998255333333333">
Department of Artificial Intelligence
and Centre for Cognitive Science
University of Edinburgh
</affiliation>
<address confidence="0.857151222222222">
Edinburgh EH8 914w
Scotland
R.DaleOuk.ac.edinburgh
Nicholas Haddock
Hewlett Packard Laboratories
Filton Road
Stoke Gifford
Bristol BS 12 Nz
England
</address>
<email confidence="0.821885">
njhOcom.hp.hpl.hplb
</email>
<sectionHeader confidence="0.960925" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99977525">
In this paper, we review Dale&apos;s [1989] algorithm
for determining the content of a referring expres-
sion. The algorithm, which only permits the use
of one-place predicates, is revised and extended
to deal with n-ary predicates. We investigate the
problem of blocking &apos;recursion&apos; in complex noun
phrases and propose a solution in the context of
our algorithm.
</bodyText>
<sectionHeader confidence="0.896076" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.996975975">
In very simple language generation systems, there
is typically a one-to-one relationship between en-
tities known to the system and the linguistic forms
available for describing those entities; in effect,
each entity has a canonical name. In such sys-
tems, deciding upon the form of reference required
in a given context requires at most choosing be-
tween a pronoun and the canonical name.&apos;
As soon as a generation system has access to a
knowledge base which contains richer knowledge
about the entities in the domain, the system has
to face the problem of deciding what particular
properties of an entity should be used in describ-
ing it in a given context.&apos; Producing a descrip-
tion which includes all of the known properties
of the entity is likely to be both inefficient and
INVe do not mean to imply, of course, that the de-
cision as to whether or not to use a pronoun is simple.
2This problem exists quite independently of any
considerations of the different perspectives that
might be taken upon an entity, where, for example,
one entity can be viewed from the perspective of be-
ing a father, a bicyclist and a teacher, with separate
clusters of properties in each case. Even if the system
is restricted to a single perspective upon each entity
(as almost all language generation systems are), in any
sophisticated knowledge base there will still be more
information available about the entity than it is sen-
sible to include in a description.
misleading.
The core of the problem is finding a way of de-
scribing the intended referent that distinguishes
it from other potential referents with which it
might be confused. We refer to this problem as
the content determination task. In this paper,
we point out some limitations in an earlier solu-
tion proposed in Dale [1988, 1989], and discuss
the possibilites of extending this solution by in-
corporating a use of constraints motivated by the
work of Haddock [1987, 1988].
</bodyText>
<sectionHeader confidence="0.7535245" genericHeader="method">
Generating Referring
Expressions
</sectionHeader>
<subsectionHeader confidence="0.971105">
The Principles of Reference
</subsectionHeader>
<bodyText confidence="0.998900833333333">
Dale [1988, 1989] presents a solution to the con-
tent determination task which is motivated by
three principles of reference. These are essen-
tially Gricean conversational maxims rephrased
from the perspective of generating referring ex-
pressions:
</bodyText>
<listItem confidence="0.825824">
1. The principle of sensitivity states that the
referring expression chosen should take account
of the state of the hearer&apos;s knowledge.
2. The principle of adequacy states that the
referring expression chosen should be sufficient
to identify the intended referent.
3. The principle of efficiency states that the
</listItem>
<bodyText confidence="0.956047428571429">
referring expression chosen should provide no
more information than is necessary for the iden-
tification of the intended referent.
The solution proposed in Dale [1988, 19891 fo-
cuses on the second and third of these principles
of reference as constraints on the content deter-
mination task.
</bodyText>
<page confidence="0.817829">
1
</page>
<listItem confidence="0.859188">
• 161 -
</listItem>
<subsectionHeader confidence="0.860708">
Distinguishing Descriptions
</subsectionHeader>
<bodyText confidence="0.999981227272728">
Other researchers (see, for example, [Davey 1978;
Appelt 1985aD have suggested that the process
of determining the content of a referring expres-
sion should be governed by principles like those
just described. Detailed algorithms for satisfying
these requirements are rarely provided, however.
Suppose that we have a set of entities C (called
the context set) such that C = {al, az, • • an}
and our task is to distinguish from this context set
some intended referent r where r E C. Suppose,
also, that each entity ak is described in the sys-
tem&apos;s knowledge base by means of a set of prop-
erties, pk, , Pk,&apos; • • • , Pk„, -
In order to distinguish our intended referent r
from the other entities in C, we need to find some
set of properties which are together true of r, but
of no other entity in C.3 The linguistic realisa-
tion of this set of properties constitutes a distin-
guishing description (up) of r with respect to
the context C. A minimal distinguishing de-
scription is then the linguistic realisation of the
smallest such set of properties.
</bodyText>
<subsectionHeader confidence="0.971514">
An Algorithm to Compute
Distinguishing Descriptions
</subsectionHeader>
<bodyText confidence="0.9988304">
I,et l„ be the set of properties to be realised in
our description; and let Pr be the set of proper-
ties known to be true of our intended referent r
(we assume that Pr is non-empty). The initial
conditions are thus as follows:
</bodyText>
<listItem confidence="0.999111">
• Cr = {(all entities in the knowledge base)};
• Pr = {(all properties true of r)};
• Lr
</listItem>
<bodyText confidence="0.6475695">
In order to describe the intended referent r with
respect to the context set Cr, we do the following:
</bodyText>
<listItem confidence="0.874779">
1. Check Success
</listItem>
<bodyText confidence="0.903265375">
if ICr I = 1 then return Lr as a DD
elseif Pr = then return Lr as a non-DD
else goto Step 2.
Choose Property
for each pi E Pr do: Cr, 4-- Cr n {X1Pi(x)}
Chosen property is pj, where Cr; is the small-
est set.&apos;
goto Step 3.
</bodyText>
<footnote confidence="0.9661548">
3A similar approach is being pursued by Leavitt
(personal communication) at cm.
41n the terminology of Dale [1988, 1989], this is
equivalent to finding the property with the greatest
discriminatory power.
</footnote>
<listItem confidence="0.61381">
3. Extend Description (wrt the chosen p3)
</listItem>
<equation confidence="0.85866525">
L, 4-- L, U {pi}
Cr 4-
Pr 4— Pr — {P3}
goto Step 1.
</equation>
<bodyText confidence="0.999907967741935">
If we have a distinguishing description, a definite
determiner can be used, since the intended refer-
ent is described uniquely in context. If the result
is a non-distinguishing description, all is not lost:
we can realise the description by means of a noun
phrase of the form one of the Xs, where X is the
realisation of the properties in Li-5 For simplic-
ity, the remainder of this paper concentrates on
the generation of distinguishing descriptions only;
the extended algorithm presented later will sim-
ply fail if it is not possible to produce a DD.
The abstract process described above requires
some slight modifications before it can be used
effectively for noun phrase generation. In partic-
ular, we should note that, in noun phrases, the
head noun typically appears even in cases where
it does not have any discriminatory power. For
example, suppose there are six entities on a table,
all of which are cups although only one is red: we
are then likely to describe that particular cup as
as the red cup rather than simply the red or the
red thing. Thus, in order to implement the above
algorithm, we always first add to L that property
of the entity that would typically be denoted by
a head noun.6 In many cases, this means that no
further properties need be added.
Note also that Step 2 of our algorithm is non-
deterministic, in that several properties may inde-
pendently yield a context set of the same minimal
size. For simplicity, we assume that one of these
equally viable properties is chosen at random.
</bodyText>
<subsectionHeader confidence="0.960435">
Some Problems
</subsectionHeader>
<bodyText confidence="0.999855555555555">
There are some problems with the algorithm just
described.
As Reiter [1990:139] has pointed out, the algo-
rithm does not guarantee to find a minimal dis-
tinguishing description: this is equivalent to the
minimal set cover problem and is thus intractable
as stated.
Second, the mechanism doesn&apos;t necessarily pro-
duce a useful description: consider the example
</bodyText>
<footnote confidence="0.951568">
60ne might be tempted to suggest that a straight-
forward indefinite, as in an X, could be used in such
cases; this is typically not what people do, however.
6For simplicity, we can assume that this is that
property of the entity that would be denoted by what
Rosch [19781 calls the entity&apos;s basic category.
</footnote>
<bodyText confidence="0.975276538461538">
- 162 -
offered by Appelt [1985b:6], where a speaker tells
a hearer (whom she has just met on the bus)
which bus stop to get off at by saying Get off one
stop before I do. This may be a uniquely iden-
tifying description of the intended referent, but
it is of little use without a supplementary offer
to indicate the stop; ultimately, we require some
computational treatment of the Principle of Sen-
sitivity here.
Third, as has been demonstrated by work in
psycholinguistics (for a recent summary, see Lev-
cit [1989:129-134D, the algorithm does not rep-
resent what people seem to do when construct-
ing a referring expression: in particular, people
typically produce referring expressions which are
redundant (over and above the inclusion of the
head noun as discussed above). This fact can, of
course, be taken to nullify the impact of the first
problem described above.
We do not intend to address any of these prob-
lems in the present paper. Instead, we consider an
extension of our basic algorithm to deal with rela-
tions, and focus on an orthogonal problem which
besets any algorithm for generating DDS involving
relations.
</bodyText>
<sectionHeader confidence="0.644978" genericHeader="method">
Relations and the Problem of
&apos;Recursion&apos;
</sectionHeader>
<bodyText confidence="0.997322915254237">
Suppose that our knowledge base consists of a set
of facts, as follows:
{cup(ci),cuP(c2),cup(c3), bowl(bi),bowl(b2),
table(ti),table(t2),floor(h),in(ci, b1),
in (c2, b2) , on (c3, ), on (bi on(b2,4),
on(ti,h),on(t2,fi)}
Thus we have three cups, two bowls, two tables
and a floor. Cup c1 is in bowl b1, and bowl bt
is on the floor, as are the tables and cup c3; and
so on. The algorithm described above deals only
with one-place predicates, and says nothing about
using relations such as on(bi, ft) as part of a
distinguishing description. How can we extend
the basic algorithm to handle relations? It turns
out that this is not as simple as it might seem:
problems arise because of the potential for infinite
regress in the construction of the description.
A natural strategy to adopt for generating ex-
pressions with relations is that used by Appelt
11985a:108-1121. For example, to describe the
entity e3, our planner might determine that the
predicate to be realized in our referring expres-
sion is the abstraction Ax[cup(x)Aon(x, Mb since
this complex predicate is true of only one entity,
namely c3. In Appelt&apos;s TELEGRAM, this results
first in the choice of the head noun cup, followed
by a recursive call to the planner to determine
how h should be described. The resulting noun
phrase is then the cup on the floor.
In many cases this approach will do what is
required. However, in certain situations, it will
attempt to describe a referent in terms of itself
and generate an infinite description.
For example, consider a very specific instance
of the problem, which arises in a scenario of the
kind discussed in Haddock [1987, 1988] from the
perspective of interpretation. Such a scenario is
characterised in the above knowledge base: we
have two bowls and two tables, and one of the
bowls is on one of the tables. Given this situa-
tion, it is felicitous to refer to b2 as the bowl on
the table. However, the use of the definite arti-
cle in the embedded NP the table poses a problem
for purely compositional approaches to interpre-
tation, which would expect the embedded NP to
refer uniquely in isolation.
Naturally, this same scenario will be problem-
atic for a purely compositional approach to gen-
eration of the kind alluded to at the beginning of
this section. Taken literally, this algorithm could
generate an infinite NP, such a.s:7
the bowl on the table which supports the bowl
on the table which supports
Below, we present an algorithm for generating
relational descriptions which deals with this spe-
cific instance of the problem of repetition. Had-
dock [1988] observes the problem can be solved
by giving both determiners scope over the entire
NP, thus:
</bodyText>
<equation confidence="0.670372">
(J!x)(3!y)bowl(x) A on(x, y) A table(y)
</equation>
<bodyText confidence="0.999967285714286">
In Haddock&apos;s model of interpretation, this treat-
ment falls out of a scheme of incremental, left-to-
right reference evaluation based on an incremen-
tal accumulation of constraints. Our generation
algorithm follows Haddock [1988], and Mellish
[1985], in using constraint-network consistency to
determine the entities relating to a description
(see Mackworth [1977]). This is not strictly nec-
essary, since any evaluation procedure such as
generate-and-test or backtracking, can produce
the desired result; however, using network consis-
tency provides a natural evolution of the existing
algorithm, since this already models the problem
in terms of incremental refinement of context sets.
</bodyText>
<footnote confidence="0.774981666666667">
7We ignore the question of determiner choice in the
present paper, and assume for simplicity that definite
determiners are chosen here.
</footnote>
<equation confidence="0.266952">
- 163 -
</equation>
<bodyText confidence="0.99563375">
initially associated with a context set containing
all entities in the knowledge base.
In addition, we use the notation [r \yip to sig-
nify the result of replacing every occurence of the
constant r in p by the variable v. For instance,
We conclude the paper by investigating the im-
plications of our approach for the more general
problem of recursive repetition.
</bodyText>
<figure confidence="0.736335">
A Constraint-Based Algorithm
Ec3\sion(e3, fi) = on(x, fi)
Data Structures
</figure>
<bodyText confidence="0.969147">
The initial conditions are as follows:
We assume three global kinds of data structure.
I. The Referent Stack is a stack of referents we
are trying to describe. Initially this stack is set
to contain just the top-level referent:8
</bodyText>
<equation confidence="0.829684">
[Describe (62 x)1
This means that the goal is to describe the ref-
erent b2 in terms of predicates over the variable
x.
</equation>
<listItem confidence="0.489436111111111">
2. The Property Set for the intended referent
r is the set of facts, or predications, in the
knowledge base relating to r; we will notate
this as Pr. For example, given the knowledge
base introduced in the previous section, the
floor f1 has the following Property Set:
Pi, = {floor(fi ), on(c3 a on (N., ),
on(ti, 10, on(t2, ,fi )1
3. A Constraint Network N will be viewed ab-
</listItem>
<bodyText confidence="0.797135857142857">
stractly as a pair consisting of (a) a set of con-
straints, which corresponds to our description
L, and (b) the context sets for the variables
mentioned in L. The following is an example
of a constraint network, viewed in these terms:
({cup(x),in(s, y)},
[C. = {ci , c2} , Cy = {b1 1b2}])
</bodyText>
<subsectionHeader confidence="0.568655">
The Algorithm
</subsectionHeader>
<bodyText confidence="0.9803715">
For brevity, our algorithm uses the notation Nep
to signify the result of adding the constraint p
to the network N. Whenever a constraint p is
added to a network, assume the following actions
occur: (a) p is added to the set of constraints
L; and (b) the context sets for variables in L are
refined until their values are consistent with the
new constraint.8 Assume that every variable is
8We represent the stack here as a list, with the top
of the stack being the left-most item in the list.
8We do not address the degree of network consis-
tency required by our algorithm. However, for the
examples treated in this paper, a node and arc con-
sistency algorithm, such as Mackworth&apos;s [19771 AC-
3, will suffice. (Haddock [1991] investigates the suffi-
ciency of such low-power techniques for noun phrase
interpretation.) We assume that our algorithm han-
dles constants as well as variables within constraints.
</bodyText>
<listItem confidence="0.999807">
• Stack = [Describe(r, v)]
• Pr = {(all facts true of r)}
• N =({},[Co = {(all entities)}])
</listItem>
<bodyText confidence="0.95747675">
Thus, initially there are no properties in L. As
before, the problem of finding a description L in-
volves three steps which are repeated until a suc-
cessful description has been constructed:
</bodyText>
<listItem confidence="0.9799659">
1. We first check whether the description we have
constructed so far is successful in picking out
the intended referent.
2. If the description is not sufficient to pick out
the intended referent, we choose the most use-
ful fact that will contribute to the description.
3. We then extend the description with a con-
straint representing this fact, and add Describe
goals for any constants relating to the con-
straint.
</listItem>
<bodyText confidence="0.998357333333333">
The essential use of constraints occurs in Step 2
and 3; the detail of the revised algorithm is shown
in Figure 1.
</bodyText>
<subsectionHeader confidence="0.833322">
An Example
</subsectionHeader>
<bodyText confidence="0.965448666666667">
There is insufficient space to go through an exam-
ple in detail here; however, we summarise some
steps for the problematic case of referring to b2 as
the the bowl on the table.10 For simplicity here,
we assume our algorithm will always choose the
head category first. Thus, we have the following
constraint network after one iteration through the
algorithm:
N = ({bowl(X)} , [Cx = {b1 ,b2}])
Let us suppose that the second iteration chooses
on(b2, t1) as the predication with which to extend
our description. When integrated into the con-
straint network, we have
°Again, we ignore the question of determiner choice
and assume definites are chosen.
</bodyText>
<table confidence="0.35650475">
- 164 -
Note that in Steps I, 2 and 3, r and v relate to the
current Describe(r,v) on top of the stack.
I. Check Success
</table>
<construct confidence="0.453817333333333">
if Stack is empty then return L as a DD
elseif IGI = 1 then pop Stack St goto Step 1
elseif P, = 0 then fail
</construct>
<figure confidence="0.923419611111111">
else goto Step 2
Choose Property
for each property Pj E Pr do
(r\v)p,
N, N (I)
Chosen predication is p3, where N contains
the smallest set C„ for v.
goto Step 3
3. Extend Description (w.r.t. the chosen p)
,-- — {p}
p *— [r\v[p
For every other constant r&apos; in p do
associate r&apos; with a new, unique variable v&apos;
p [ri \v&apos;lp
push Describe(r&apos;,1/) onto Stack
initialise a set of facts true of r&apos;
N N (Dp
goto Step I
</figure>
<figureCaption confidence="0.998376">
Figure I: A Constraint-Based Algorithm
</figureCaption>
<equation confidence="0.9676235">
N = ({bowl(x),on(x,y)},
[C. {b1,b2},q, = {h,ti}i)
</equation>
<bodyText confidence="0.962809857142857">
Note that the network has determined a set for
Y which does not include the second table t2 be-
cause it is not known to support anything.
Given our head-category-first strategy, the third
iteration through the algorithm adds table(t1) as
a constraint to N, to form the new network
A&apos; = ({bowl(x),on(x, y),table(y)},
Alter adding this new constraint, fi is eliminated
from C. This leads to the revision of to C.,
which must remove every value which is not on
11.
On the fourth iteration, we exit with the first
component of this network, L, as our description;
we can then realize this content as the bowl on
</bodyText>
<subsectionHeader confidence="0.937802">
The Problem Revisited
</subsectionHeader>
<bodyText confidence="0.999967181818182">
The task of referring to b2 in our knowledge base
is something of a special case, and does not illus-
trate the nature of the general problem of recur-
sion. Consider the task of referring to cl. Due
to the non-determinism in Step 2, our algorithm
might either generate the Di.) corresponding to the
cup in the bowl on the floor, or it might instead
get into an infinite loop corresponding to the cup
in the bowl containing the cup in the bowl con-
taining ... The initial state of the referent stack
and el&apos;s property set will be:
</bodyText>
<equation confidence="0.5762255">
Stack = 1Describe(ci x)]
= {cup(ci.), in(ci,b1)}
</equation>
<bodyText confidence="0.9998995">
At the beginning of the fourth iteration the al-
gorithm will have produced a partial description
corresponding to the cup in the bowl, with the
top-level goal to uniquely distinguish b1:
</bodyText>
<equation confidence="0.9992808">
Stack = [Describe(b1,y),Describe(ci,x)1
Pc, = 0
= {in(c1,b1),on(b1,f1))
N = ({cup(x), in(x, y), bowl(y)},
[C. = {ci,c2},Cy =
</equation>
<bodyText confidence="0.999863">
Step 2 of the fourth iteration computes two net-
works, for the two facts in /11:
</bodyText>
<equation confidence="0.864537666666667">
= N ED M(ct, Y)
({cup(x), in(x, y), bowl(y), in(ci y)}
[Cx =
N2 = N Ei) on(y, )
({cup(x), in(x, y), bowl(y), on(y, f1 )1,
[C. = {ci }, Cy = POD
</equation>
<bodyText confidence="0.922581222222222">
Since both networks yield singleton sets for Ci.„
the algorithm might choose the property in(ci ibt).
This means extending the current description with
a constraint in(z,y), and stacking an additional
commitment to describe ci in terms of the vari-
able z. Hence at the end of the fourth iteration,
the algorithm is in the state
Stack Pescribe(ci , z), Describe(bi y),
Describe(ci, x)]
</bodyText>
<listItem confidence="0.977281">
• {on(bl, fi)}
• Icup(ci),in(ci,b1)}
▪ ({cup(x), in(x, y), bowl(y), in(z , y)} ,
</listItem>
<equation confidence="0.619062">
I. • .1)
</equation>
<bodyText confidence="0.994825829268292">
and may continue to loop in this manner.
The general problem of infinite repetition has
been noted before in the generation literature.
For example, Novak [1988:83] suggests that
- 165 -
HI a two-place predicate is used to generate the
restrictive relative clause, the second object of
this predicate is characterized simply by its prop-
erties to avoid recursive reference as in the car
which was overtaken by the truck which overtook
the car.
Davey [1979], on the other hand, introduces
the notion of a CANLIST (the Currently Active
Node List) for those entities which have already
been mentioned in the noun phrase currently un-
der construction. The generator is then prohib-
ited from describing an entity in terms of entities
already in the CAN LIST.
In the general case, these proposals appear to
be too strong. Davey&apos;s restriction would seem
to be the weaker of the two, but if taken liter-
ally, it will nevertheless prevent legitimate cases
or bound-variable anaphora within an NP, such as
the mani who ate the cake which poisoned him.
We suggest the following, possibly more general
heuristic: do not express a given piece of infor-
mation more than once within the same NP. For
our simplified representation of contextual knowl-
edge, exemplified above, we could encode this
heuristic by stipulating that any fact in the knowl-
edge base can only be chosen once within a given
call to the algorithm. So in the above example,
once the relation in(ci, b1) has been chosen from
the initial set Pc,—in order to constrain the vari-
able x--it is no longer available as a viable con-
textual constraint to distinguish bi later on. This
heuristic will therefore block the infinite descrip-
tion of cl. But as desired, it will admit the bound-
variable anaphora mentioned above, since this NP
is not based on repeated information; the phrase
is merely self-referential.
</bodyText>
<sectionHeader confidence="0.81283" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999992428571429">
We have shown how the referring expression gen-
eration algorithm presented in Dale [1988, 1989]
can be extended to encompass the use of rela-
tions, by making use of constraint network con-
sistency. In the context of this revised genera-
tion procedure we have investigated the problem
of blocking the production of infinitely recursive
noun phrases, and suggested an improvement on
sonic existing approaches to the problem. Ar-
eas for further research include the relationship
of our approach to existing algorithms in other
fields, such as machine learning, and also its re-
lationship to observed characteristics of human
discourse production.
</bodyText>
<sectionHeader confidence="0.966953" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997368">
The work reported here was prompted by a con-
versation with Breck Baldwin. Both authors would
like to thank colleagues at each of their institu-
tions for numerous comments that have improved
this paper.
</bodyText>
<sectionHeader confidence="0.998274" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992351863636364">
Appelt, Douglas E 11985a1 Planning English Sentences.
Cambridge: Cambridge University Press.
Appelt, Douglas E 11985b1 Planning English Referring
Expressions. Artificial Intelligence, 26, 1-33.
Dale, Robert 119881 Generating Referring Expressions
in a Domain of Objects and Processes. PhD The-
sis, Centre for Cognitive Science, University of Ed-
inburgh.
Dale, Robert [19891 Cooking up Referring Expressions.
In Proceedings of the 27th Annual Meeting of the
Association for Computational Linguistics, Vancou-
ver BC, pp68-75.
Davey, Anthony 119781 Discourse Production. Edin-
burgh: Edinburgh University Press.
Haddock, Nicholas 119871 Incremental Interpretation
and Combinatory Categorial Grammar. In Proceed-
ings of the Tenth International Joint Conference on
Artificial Intelligence, Milan, Italy, pp.
Haddock, Nicholas J 119881 Incremental Semantics and
Interactive Syntactic Processing. PhD Thesis, Cen-
tre for Cognitive Science, University of Edinburgh.
Haddock, Nicholas J [19911 Linear-Time Reference Eval-
uation. Technical Report, Hewlett Packard Labora-
tories, Bristol.
Levelt, Willem .1 M 119891 Speaking: From Intention to
Articulation. Cambridge, Mass.: MIT Press.
Mackworth, Alan K 119771 Consistency in Networks of
Relations. Artificial Intelligence, 8, 99-118.
Mellish, Christopher S 119851 Computer Interpretation
of Natural Language Descriptions. Chichester: Ellis
Horwood.
Novak, Hans-Joachim [19881 Generating Referring Phrases
in a Dynamic Environment. Chapter 5 in M Zock
and G Sabah (eds), Advances in Natural Language
Generation, Volume 2, pp76-85. London: Pinter
Publishers.
Reiter, Ehud 119901 Generating Appropriate Natural Lan-
guage Object Descriptions. PhD thesis, Aiken Com-
putation Laboratory, Harvard University.
Bosch, Eleanor 119781 Principles of Categorization. In
E Rosch and B Lloyd (eds), Cognition and Catego-
rization, pp27-48. Hillsdale, NJ: Lawrence Erlbaum
Associates.
- 166 -
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.105102">
<title confidence="0.999985">Generating Referring Expressions Involving Relations</title>
<author confidence="0.999994">Robert Dale</author>
<affiliation confidence="0.999363">Department of Artificial Intelligence and Centre for Cognitive Science University of Edinburgh</affiliation>
<address confidence="0.850805">Edinburgh EH8 914w Scotland</address>
<email confidence="0.943058">R.DaleOuk.ac.edinburgh</email>
<author confidence="0.99831">Nicholas Haddock</author>
<affiliation confidence="0.502505">Hewlett Packard Laboratories</affiliation>
<address confidence="0.47082775">Filton Road Stoke Gifford Bristol BS 12 Nz England</address>
<email confidence="0.98027">njhOcom.hp.hpl.hplb</email>
<abstract confidence="0.999412666666667">In this paper, we review Dale&apos;s [1989] algorithm for determining the content of a referring expression. The algorithm, which only permits the use of one-place predicates, is revised and extended to deal with n-ary predicates. We investigate the problem of blocking &apos;recursion&apos; in complex noun phrases and propose a solution in the context of our algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Douglas Appelt</author>
</authors>
<title>E 11985a1 Planning English Sentences. Cambridge:</title>
<publisher>Cambridge University Press.</publisher>
<marker>Appelt, </marker>
<rawString>Appelt, Douglas E 11985a1 Planning English Sentences. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Douglas Appelt</author>
</authors>
<title>E 11985b1 Planning English Referring Expressions.</title>
<journal>Artificial Intelligence,</journal>
<volume>26</volume>
<pages>1--33</pages>
<marker>Appelt, </marker>
<rawString>Appelt, Douglas E 11985b1 Planning English Referring Expressions. Artificial Intelligence, 26, 1-33.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robert Dale</author>
</authors>
<title>119881 Generating Referring Expressions in a Domain of Objects and Processes.</title>
<tech>PhD Thesis,</tech>
<institution>Centre for Cognitive Science, University of Edinburgh.</institution>
<marker>Dale, </marker>
<rawString>Dale, Robert 119881 Generating Referring Expressions in a Domain of Objects and Processes. PhD Thesis, Centre for Cognitive Science, University of Edinburgh.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robert Dale</author>
</authors>
<title>19891 Cooking up Referring Expressions.</title>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, Vancouver BC,</booktitle>
<pages>68--75</pages>
<marker>Dale, </marker>
<rawString>Dale, Robert [19891 Cooking up Referring Expressions. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, Vancouver BC, pp68-75.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Davey</author>
</authors>
<title>Anthony 119781 Discourse Production. Edinburgh:</title>
<publisher>Edinburgh University Press.</publisher>
<marker>Davey, </marker>
<rawString>Davey, Anthony 119781 Discourse Production. Edinburgh: Edinburgh University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nicholas Haddock</author>
</authors>
<title>119871 Incremental Interpretation and Combinatory Categorial Grammar.</title>
<booktitle>In Proceedings of the Tenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>pp.</pages>
<location>Milan, Italy,</location>
<marker>Haddock, </marker>
<rawString>Haddock, Nicholas 119871 Incremental Interpretation and Combinatory Categorial Grammar. In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, Milan, Italy, pp.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nicholas J Haddock</author>
</authors>
<title>119881 Incremental Semantics and Interactive Syntactic Processing.</title>
<tech>PhD Thesis,</tech>
<institution>Centre for Cognitive Science, University of Edinburgh.</institution>
<marker>Haddock, </marker>
<rawString>Haddock, Nicholas J 119881 Incremental Semantics and Interactive Syntactic Processing. PhD Thesis, Centre for Cognitive Science, University of Edinburgh.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nicholas J Haddock</author>
</authors>
<title>19911 Linear-Time Reference Evaluation.</title>
<tech>Technical Report,</tech>
<location>Hewlett Packard Laboratories, Bristol.</location>
<marker>Haddock, </marker>
<rawString>Haddock, Nicholas J [19911 Linear-Time Reference Evaluation. Technical Report, Hewlett Packard Laboratories, Bristol.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Willem Levelt</author>
</authors>
<title>1 M 119891 Speaking: From Intention to Articulation.</title>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.:</location>
<marker>Levelt, </marker>
<rawString>Levelt, Willem .1 M 119891 Speaking: From Intention to Articulation. Cambridge, Mass.: MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alan Mackworth</author>
</authors>
<title>K 119771 Consistency in Networks of Relations.</title>
<journal>Artificial Intelligence,</journal>
<volume>8</volume>
<pages>99--118</pages>
<marker>Mackworth, </marker>
<rawString>Mackworth, Alan K 119771 Consistency in Networks of Relations. Artificial Intelligence, 8, 99-118.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christopher Mellish</author>
</authors>
<title>S 119851 Computer Interpretation of Natural Language Descriptions.</title>
<publisher>Ellis Horwood.</publisher>
<location>Chichester:</location>
<marker>Mellish, </marker>
<rawString>Mellish, Christopher S 119851 Computer Interpretation of Natural Language Descriptions. Chichester: Ellis Horwood.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Novak</author>
</authors>
<title>Hans-Joachim [19881 Generating Referring Phrases in a Dynamic Environment.</title>
<booktitle>Chapter 5 in M Zock and G Sabah (eds), Advances in Natural Language Generation,</booktitle>
<volume>2</volume>
<pages>76--85</pages>
<publisher>Pinter Publishers.</publisher>
<location>London:</location>
<marker>Novak, </marker>
<rawString>Novak, Hans-Joachim [19881 Generating Referring Phrases in a Dynamic Environment. Chapter 5 in M Zock and G Sabah (eds), Advances in Natural Language Generation, Volume 2, pp76-85. London: Pinter Publishers.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Reiter</author>
</authors>
<title>Ehud 119901 Generating Appropriate Natural Language Object Descriptions.</title>
<tech>PhD thesis,</tech>
<institution>Aiken Computation Laboratory, Harvard University.</institution>
<marker>Reiter, </marker>
<rawString>Reiter, Ehud 119901 Generating Appropriate Natural Language Object Descriptions. PhD thesis, Aiken Computation Laboratory, Harvard University.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bosch</author>
</authors>
<title>Eleanor 119781 Principles of Categorization.</title>
<booktitle>In E Rosch and B Lloyd (eds), Cognition and Categorization,</booktitle>
<pages>27--48</pages>
<institution>Lawrence Erlbaum Associates.</institution>
<location>Hillsdale, NJ:</location>
<marker>Bosch, </marker>
<rawString>Bosch, Eleanor 119781 Principles of Categorization. In E Rosch and B Lloyd (eds), Cognition and Categorization, pp27-48. Hillsdale, NJ: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="false">
<pages>166</pages>
<marker></marker>
<rawString>- 166 -</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>