<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.821105">
An Analysis of Active Learning Strategies for Sequence Labeling Tasks
</title>
<author confidence="0.990189">
Burr Settles*†
</author>
<affiliation confidence="0.856545666666667">
*Dept. of Computer Sciences
University of Wisconsin
Madison, WI 53706, USA
</affiliation>
<email confidence="0.989405">
bsettles@cs.wisc.edu
</email>
<author confidence="0.964485">
Mark Craven†*
</author>
<affiliation confidence="0.853541666666667">
†Dept. of Biostatistics &amp; Medical Informatics
University of Wisconsin
Madison, WI 53706, USA
</affiliation>
<email confidence="0.997428">
craven@biostat.wisc.edu
</email>
<sectionHeader confidence="0.998588" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999842714285714">
Active learning is well-suited to many prob-
lems in natural language processing, where
unlabeled data may be abundant but annota-
tion is slow and expensive. This paper aims
to shed light on the best active learning ap-
proaches for sequence labeling tasks such as
information extraction and document segmen-
tation. We survey previously used query selec-
tion strategies for sequence models, and pro-
pose several novel algorithms to address their
shortcomings. We also conduct a large-scale
empirical comparison using multiple corpora,
which demonstrates that our proposed meth-
ods advance the state of the art.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935450980392">
Traditional supervised learning algorithms use
whatever labeled data is provided to induce a model.
By contrast, active learning gives the learner a de-
gree of control by allowing it to select which in-
stances are labeled and added to the training set. A
typical active learner begins with a small labeled set
L, selects one or more informative query instances
from a large unlabeled pool U, learns from these la-
beled queries (which are then added to L), and re-
peats. In this way, the learner aims to achieve high
accuracy with as little labeling effort as possible.
Thus, active learning can be valuable in domains
where unlabeled data are readily available, but ob-
taining training labels is expensive.
Such is the case with many sequence labeling
tasks in natural language domains. For example,
part-of-speech tagging (Seung et al., 1992; Lafferty
et al., 2001), information extraction (Scheffer et al.,
2001; Sang and DeMeulder, 2003; Kim et al., 2004),
and document segmentation (Carvalho and Cohen,
2004) are all typically treated as sequence labeling
problems. The source data for these tasks (i.e., text
documents in electronic form) are often easily ob-
tained. However, due to the nature of sequence la-
beling tasks, annotating these texts can be rather te-
dious and time-consuming, making active learning
an attractive technique.
While there has been much work on active learn-
ing for classification (Cohn et al., 1994; McCallum
and Nigam, 1998; Zhang and Oles, 2000; Zhu et
al., 2003), active learning for sequence labeling has
received considerably less attention. A few meth-
ods have been proposed, based mostly on the con-
ventions of uncertainty sampling, where the learner
queries the instance about which it has the least cer-
tainty (Scheffer et al., 2001; Culotta and McCallum,
2005; Kim et al., 2006), or query-by-committee,
where a “committee” of models selects the instance
about which its members most disagree (Dagan and
Engelson, 1995). We provide more detail on these
and the new strategies we propose in Section 3.
The comparative effectiveness of these ap-
proaches, however, has not been studied. Further-
more, it has been suggested that uncertainty sam-
pling and query-by-committee fail on occasion (Roy
and McCallum, 2001; Zhu et al., 2003) by query-
ing outliers, e.g., instances considered informative
in isolation by the learner, but containing little infor-
mation about the rest of the distribution of instances.
Proposed methods for dealing with these shortcom-
ings have so far only considered classification tasks.
</bodyText>
<page confidence="0.953741">
1070
</page>
<note confidence="0.8806195">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1070–1079,
Honolulu, October 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999879857142857">
This paper presents two major contributions for
active learning and sequence labeling tasks. First,
we motivate and introduce several new query strate-
gies for probabilistic sequence models. Second, we
conduct a thorough empirical analysis of previously
proposed methods with our algorithms on a variety
of benchmark corpora. The remainder of this pa-
per is organized as follows. Section 2 provides a
brief introduction to sequence labeling and condi-
tional random fields (the sequence model used in
our experiments). Section 3 describes in detail all
the query selection strategies we consider. Section 4
presents the results of our empirical study. Section 5
concludes with a summary of our findings.
</bodyText>
<sectionHeader confidence="0.6308" genericHeader="method">
2 Sequence Labeling and CRFs
</sectionHeader>
<bodyText confidence="0.9999876">
In this paper, we are concerned with active learn-
ing for sequence labeling. Figure 1 illustrates
how, for example, an information extraction prob-
lem can be viewed as a sequence labeling task.
Let x = (x1,... , xT) be an observation sequence
of length T with a corresponding label sequence
y = (y1, ... , yT). Words in a sentence corre-
spond to tokens in the input sequence x, which are
mapped to labels in y. These labels indicate whether
the word belongs to a particular entity class of inter-
est (in this case, org and loc) or not (null). These
labels can be assigned by a sequence model based
on a finite state machine, such as the one shown to
the right in Figure 1.
We focus our discussion of active learning for
sequence labeling on conditional random fields, or
CRFs (Lafferty et al., 2001). The rest of this sec-
tion serves as a brief introduction. CRFs are sta-
tistical graphical models which have demonstrated
state-of-the-art accuracy on virtually all of the se-
quence labeling tasks mentioned in Section 1. We
use linear-chain CRFs, which correspond to condi-
tionally trained probabilistic finite state machines.
A linear-chain CRF model with parameters θ de-
fines the posterior probability of y given x to be1:
</bodyText>
<equation confidence="0.998285">
T K
P(yJ x; θ) = Z(x) exp !
X X θkfk(yt−1, yt, xt) .
t�1 k�1
(1)
</equation>
<footnote confidence="0.999263666666667">
1Our discussion assumes, without loss of generality, that
each label is uniquely represented by one state, thus each label
sequence y corresponds to exactly one path through the model.
</footnote>
<figureCaption confidence="0.998005">
Figure 1: An information extraction example treated as
a sequence labeling task. Also shown is a corresponding
sequence model represented as a finite state machine.
</figureCaption>
<bodyText confidence="0.999953214285714">
Here Z(x) is a normalization factor over all pos-
sible labelings of x, and θk is one of K model
parameter weights corresponding to some feature
fk(yt−1, yt, xt). Each feature fk describes the se-
quence x at position t with label yt, observed along
a transition from label states yt−1 to yt in the finite
state machine. Consider the example text from Fig-
ure 1. Here, fk might be the feature WORD=ACME
and have the value fk = 1 along a transition from
the null state to the org state (and 0 elsewhere).
Other features set to 1 here might be ALLCAPS and
NEXTWORD=Inc. The weights in θ are set to max-
imize the conditional log likelihood ` of training se-
quences in the labeled data set L:
</bodyText>
<equation confidence="0.910938666666667">
`(L; θ) = XL log P(y(l)Jx(l); θ) − XK θ2
l=1 k=1 k
2σ2,
</equation>
<bodyText confidence="0.999697076923077">
where L is the size of the labeled set L, and the sec-
ond term is a Gaussian regularization penalty on 11θ11
to prevent over-fitting. After training, labels can be
predicted for new sequences using the Viterbi algo-
rithm. For more details on CRFs and their training
procedures, see Sutton and McCallum (2006).
Note that, while we describe the active learning
algorithms in the next section in terms of linear-
chain CRFs, they have analogs for other kinds of
sequence models, such as hidden Markov models,
or HMMs (Rabiner, 1989), probabilistic context-
free grammars (Lari and Young, 1990), and general
CRFs (Sutton and McCallum, 2006).
</bodyText>
<sectionHeader confidence="0.993803" genericHeader="method">
3 Active Learning with Sequence Models
</sectionHeader>
<bodyText confidence="0.9992586">
In order to select queries, an active learner must have
a way of assessing how informative each instance is.
Let x* be the most informative instance according to
some query strategy φ(x), which is a function used
to evaluate each instance x in the unlabeled pool U.
</bodyText>
<figure confidence="0.998564428571428">
ACME
Inc.
org
offices
org null
the
null
null
loc
null org
in
Chicago
loc
...
</figure>
<page confidence="0.984689">
1071
</page>
<bodyText confidence="0.952848">
Given: Labeled set L, unlabeled pool U, query
strategy φ(·), query batch size B
</bodyText>
<equation confidence="0.808095">
repeat
// learn a model using the current L
θ = train(L) ;
forb= 1toBdo
//query the most informative instance
x* � = arg maxXEU φ(x) ;
//move the labeled query from U to L
L = L U (x*, label(x*)) ;
U = U − x* � ;
end
until some stopping criterion;
Algorithm 1: Pool-based active learning.
</equation>
<bodyText confidence="0.990099625">
Algorithm 1 provides a sketch of the generic pool-
based active learning scenario.
In the remainder of this section, we describe var-
ious query strategy formulations of φ(·) that have
been used for active learning with sequence mod-
els. We also point out where we think these ap-
proaches may be flawed, and propose several novel
query strategies to address these issues.
</bodyText>
<subsectionHeader confidence="0.999751">
3.1 Uncertainty Sampling
</subsectionHeader>
<bodyText confidence="0.999434285714286">
One of the most common general frameworks for
measuring informativeness is uncertainty sampling
(Lewis and Catlett, 1994), where a learner queries
the instance that it is most uncertain how to la-
bel. Culotta and McCallum (2005) employ a sim-
ple uncertainty-based strategy for sequence models
called least confidence (LC):
</bodyText>
<equation confidence="0.991669">
φLC(x) = 1 − P(y*|x; θ).
</equation>
<bodyText confidence="0.9763524">
Here, y* is the most likely label sequence, i.e., the
Viterbi parse. This approach queries the instance
for which the current model has the least confidence
in its most likely labeling. For CRFs, this confi-
dence can be calculated using the posterior proba-
bility given by Equation (1).
Scheffer et al. (2001) propose another uncertainty
strategy, which queries the instance with the smallest
margin between the posteriors for its two most likely
labelings. We call this approach margin (M):
</bodyText>
<equation confidence="0.998494">
φM(x) = −(P(y*�|x; θ) − P(y*�|x; θ)).
</equation>
<bodyText confidence="0.999837055555556">
Here, y*1 and y*2 are the first and second best la-
bel sequences, respectively. These can be efficiently
computed using the N-best algorithm (Schwartz
and Chow, 1990), a beam-search generalization of
Viterbi, with N = 2. The minus sign in front is sim-
ply to ensure that φM acts as a maximizer for use
with Algorithm 1.
Another uncertainty-based measure of informa-
tiveness is entropy (Shannon, 1948). For a dis-
crete random variable Y , the entropy is given by
H(Y ) = − Ez P(yz) log P(yz), and represents the
information needed to “encode” the distribution of
outcomes for Y . As such, is it often thought of as
a measure of uncertainty in machine learning. In
active learning, we wish to use the entropy of our
model’s posteriors over its labelings. One way this
has been done with probabilistic sequence models is
by computing what we call token entropy (TE):
</bodyText>
<equation confidence="0.870511">
Pθ(yt = m) log Pθ(yt = m),
(2)
</equation>
<bodyText confidence="0.9982245">
where T is the length of x, m ranges over all pos-
sible token labels, and PB(yt = m) is shorthand
for the marginal probability that m is the label at
position t in the sequence, according to the model.
For CRFs and HMMs, these marginals can be effi-
ciently computed using the forward and backward
algorithms (Rabiner, 1989). The summed token en-
tropies have typically been normalized by sequence
length T, to avoid simply querying longer sequences
(Baldridge and Osborne, 2004; Hwa, 2004). How-
ever, we argue that querying long sequences should
not be explicitly discouraged, if in fact they contain
more information. Thus, we also propose the total
token entropy (TTE) measure:
</bodyText>
<equation confidence="0.994607">
φTTE(x) = T X φTE(x).
</equation>
<bodyText confidence="0.9995514">
For most sequence labeling tasks, however, it is
more appropriate to consider the entropy of the la-
bel sequence y as a whole, rather than some aggre-
gate of individual token entropies. Thus an alternate
query strategy is sequence entropy (SE):
</bodyText>
<equation confidence="0.8434095">
φSE(x) = − � P(�y|x; θ) log P(�y|x; θ), (3)
J&apos;
</equation>
<bodyText confidence="0.999966">
where y� ranges over all possible label sequences for
input sequence x. Note, however, that the number
</bodyText>
<equation confidence="0.997074428571429">
1 T
φTE(x) = −
M
E
m��
T
t��
</equation>
<page confidence="0.888626">
1072
</page>
<bodyText confidence="0.999874666666667">
McCallum and Nigam (1998) propose a QBC
strategy for classification based on Kullback-Leibler
(KL) divergence, an information-theoretic measure
of the difference between two probability distribu-
tions. The most informative query is considered to
be the one with the largest average KL divergence
between a committee member’s posterior label dis-
tribution and the consensus. We modify this ap-
proach for sequence models by summing the average
KL scores using the marginals at each token position
and, as with vote entropy, normalizing for length.
We call this approach Kullback-Leibler (KL):
</bodyText>
<equation confidence="0.995331666666667">
T
φKL 1
(x) =
</equation>
<bodyText confidence="0.909116">
where (using shorthand again):
</bodyText>
<equation confidence="0.896827">
D(θ(c)kC),
T t=1
PC
</equation>
<bodyText confidence="0.923613666666667">
Here PC(yt = m) = 1 c=1 Pθ(c)(yt = m), or the
C
“consensus” marginal probability that m is the label
at position t in the sequence.
Both of these disagreement measures are normal-
ized for sequence length T. As with token en-
tropy (2), this may bias the learner toward query-
ing shorter sequences. To study the effects of nor-
malization, we also conduct experiments with non-
normalized variants φTV E and φTKL.
Additionally, we argue that these token-level dis-
agreement measures may be less appropriate for
most tasks than measuring the committee’s disagree-
ment about the label sequence y as a whole. There-
fore, we propose sequence vote entropy (SVE):
</bodyText>
<equation confidence="0.9835935">
XφSV E(x) = − P(ˆy|x; C) log P(ˆy|x; C),
ˆ�ENC
</equation>
<bodyText confidence="0.99858675">
where NC is the union of the N-best parses from
all models in the committee C, and P(ˆy|x; C) =
C PC 1 P(ˆy|x; θ(c)), or the “consensus” posterior
probability for some label sequence ˆy. This can be
thought of as a QBC generalization of N-best en-
tropy, where each committee member casts a vote
for the posterior label distribution. We also explore
a sequence Kullback-Leibler (SKL) variant:
</bodyText>
<equation confidence="0.990811333333333">
1
C
XC
c=1
D(θ(c)kC) =
Pθ(-)(yt = m)
Pθ(-)(yt = m) log
M
X
m=1
PC(yt = m) .
φSKL(x) = 1 X
Cc=1
P(ˆy|x;θ(c)) log P (ˆy|x;θ(c))
P(ˆy|x; C) .
C
X
ˆ�ENC
</equation>
<bodyText confidence="0.990277">
of possible labelings grows exponentially with the
length of x. To make this feasible, previous work
(Kim et al., 2006) has employed an approximation
we call N-best sequence entropy (NSE):
</bodyText>
<equation confidence="0.9967995">
XφNSE(x) = − P(ˆy|x; θ) log P(ˆy|x; θ),
ˆ�EN
</equation>
<bodyText confidence="0.999921642857143">
where N = {yi, ... , y*}, the set of the N most
likely parses, and the posteriors are re-normalized
(i.e., Z(x) in Equation (1) only ranges over N). For
N = 2, this approximation is equivalent to φM, thus
N-best sequence entropy can be thought of as a gen-
eralization of the margin approach.
Recently, an efficient entropy calculation via dy-
namic programming was proposed for CRFs in the
context of semi-supervised learning (Mann and Mc-
Callum, 2007). We use this algorithm to compute
the true sequence entropy (3) for active learning in
a constant-time factor of Viterbi’s complexity. Hwa
(2004) employed a similar approach for active learn-
ing with probabilistic context-free grammars.
</bodyText>
<subsectionHeader confidence="0.999778">
3.2 Query-By-Committee
</subsectionHeader>
<bodyText confidence="0.999412">
Another general active learning framework is the
query-by-committee (QBC) approach (Seung et al.,
1992). In this setting, we use a committee of models
C = {θ(1),...,θ(C)} to represent C different hy-
potheses that are consistent with the labeled set L.
The most informative query, then, is the instance
over which the committee is in most disagreement
about how to label.
In particular, we use the query-by-bagging ap-
proach (Abe and Mamitsuka, 1998) to learn a com-
mittee of CRFs. In each round of active learning,
L is sampled (with replacement) L times to create
a unique, modified labeled set L(c). Each model
θ(c) ∈ C is then trained using its own corresponding
labeled set L(c). To measure disagreement among
committee members, we consider two alternatives.
Dagan and Engelson (1995) introduced QBC with
HMMs for part-of-speech tagging using a measure
called vote entropy (VE):
</bodyText>
<equation confidence="0.9960725">
V (yt, m) V (yt, m)
C log C ,
</equation>
<bodyText confidence="0.984630333333333">
where V (yt, m) is the number of “votes” label m re-
ceives from all the committee member’s Viterbi la-
belings at sequence position t.
</bodyText>
<equation confidence="0.9785766">
φV E (x) = − T
1 T
t=1
XM
m=1
</equation>
<page confidence="0.955414">
1073
</page>
<subsectionHeader confidence="0.964966">
3.3 Expected Gradient Length
</subsectionHeader>
<bodyText confidence="0.999925277777778">
A third general active learning framework we con-
sider is to query the instance that would impart the
greatest change to the current model if we knew its
label. Since we train discriminative models like
CRFs using gradient-based optimization, this in-
volves querying the instance which, if labeled and
added to the training set, would create the greatest
change in the gradient of the objective function (i.e.,
the largest gradient vector used to re-estimate pa-
rameter values).
Let V`(L; θ) be the gradient of the log-
likelihood ` with respect to the model parameters θ,
as given by Sutton and McCallum (2006). Now let
V`(L+(x,y); θ) be the new gradient that would be
obtained by adding the training tuple (x, y) to L.
Since the query algorithm does not know the true la-
bel sequence y in advance, we instead calculate the
expected gradient length (EGL):
</bodyText>
<equation confidence="0.998615666666667">
� �
� �
P(�y|x; θ) �V`(L+(x,ˆy); θ) � ,
</equation>
<bodyText confidence="0.999962545454545">
approximated as an expectation over the N-best la-
belings, where 11 · 11 is the Euclidean norm of each
resulting gradient vector. We first introduced this ap-
proach in previous work on multiple-instance active
learning (Settles et al., 2008), and adapt it to query
selection with sequences here. Note that, at query
time, V`(L; θ) should be nearly zero since ` con-
verged at the previous round of training. Thus, we
can approximate V`(L+(x,ˆy); θ) Pz� V`((x, y); θ)
for computational efficiency, because the training in-
stances are assumed to be independent.
</bodyText>
<subsectionHeader confidence="0.851948">
3.4 Information Density
</subsectionHeader>
<bodyText confidence="0.999741166666667">
It has been suggested that uncertainty sampling and
QBC are prone to querying outliers (Roy and Mc-
Callum, 2001; Zhu et al., 2003). Figure 2 illus-
trates this problem for a binary linear classifier us-
ing uncertainty sampling. The least certain instance
lies on the classification boundary, but is not “rep-
resentative” of other instances in the distribution, so
knowing its label is unlikely to improve accuracy on
the data as a whole. QBC and EGL exhibit similar
behavior, by spending time querying possible out-
liers simply because they are controversial, or are
expected to impart significant change in the model.
</bodyText>
<figureCaption confidence="0.953621428571429">
Figure 2: An illustration of when uncertainty sampling
can be a poor strategy for classification. Shaded poly-
gons represent labeled instances (L), and circles repre-
sent unlabeled instances (U). Since A is on the decision
boundary, it will be queried as the most uncertain. How-
ever, querying B is likely to result in more information
about the data as a whole.
</figureCaption>
<bodyText confidence="0.9688345">
We argue that this phenomenon can occur with se-
quence labeling tasks as well as with classification.
To address this, we propose a new active learning
approach called information density (ID):
</bodyText>
<equation confidence="0.998796">
U
φID (x) = φSE (x) X 1 1
U u=1
</equation>
<bodyText confidence="0.9963094">
That is, the informativeness of x is weighted by its
average similarity to all other sequences in U, sub-
ject to a parameter β that controls the relative im-
portance of the density term. In the formulation pre-
sented above, sequence entropy φSE measures the
“base” informativeness, but we could just as easily
use any of the instance-level strategies presented in
the previous sections.
This density measure requires us to compute the
similarity of two sequences. To do this, we first
</bodyText>
<equation confidence="0.788532">
transform each x, which is a sequence of feature
vectors (tokens), into a single kernel vector ~x:
x~ = &amp;quot; X f1 (xt), ... , X fJ(xt)# ,
t=1 t=1
</equation>
<bodyText confidence="0.820921384615385">
where fj(xt) is the value of feature fj for token xt,
and J is the number of features in the input represen-
tation2. In other words, sequence x is compressed
into afixed-length feature vector ~x, for which each
element is the sum of the corresponding feature’s
values across all tokens. We can then use cosine
that J # K, and
here differs slightly from the
feature definition given in Section 2. Since the labels
and
yt are unknown before querying, the K features used for model
training are reduced down to the J input features here, which
factor out an
</bodyText>
<figure confidence="0.611507818181818">
2Note
fj(xt)
yt−1
y label dependencies.
B
A
XφEGL(x) =
ˆyEN
!β
sim(x, x(u)) .
T T
</figure>
<page confidence="0.974469">
1074
</page>
<bodyText confidence="0.99875205">
similarity on this simplified representation:
simcos(x, x(u)) =
We have also investigated similarity functions
based on exponentiated Euclidean distance and KL-
divergence, the latter of which was also employed by
McCallum and Nigam (1998) for density-weighting
QBC in text classification. However, these measures
show no improvement over cosine similarity, and re-
quire setting additional hyper-parameters.
One potential drawback of information density is
that the number of required similarity calculations
grows quadratically with the number of instances
in U. For pool-based active learning, we often as-
sume that the size of U is very large. However,
these densities only need to be computed once, and
are independent of the base information measure.
Thus, when employing information density in a real-
world interactive learning setting, the density scores
can simply be pre-computed and cached for efficient
lookup during the actual active learning process.
</bodyText>
<subsectionHeader confidence="0.7479">
3.5 Fisher Information
</subsectionHeader>
<bodyText confidence="0.998097666666667">
We also introduce a query selection strategy for se-
quence models based on Fisher information, build-
ing on the theoretical framework of Zhang and Oles
(2000). Fisher information I(θ) represents the over-
all uncertainty about the estimated model parame-
ters θ, as given by:
</bodyText>
<equation confidence="0.999424">
Z Z P(y|x; θ) ∂2
I(θ) = − P(x) ∂θ2 log P(y|x; θ).
x y
</equation>
<bodyText confidence="0.999970333333333">
For a model with K parameters, the Fisher infor-
mation takes the form of a K × K covariance ma-
trix. Our goal in active learning is to select the query
that most efficiently minimizes the model variance
reflected in I(θ). This can be accomplished by op-
timizing the Fisher information ratio (FIR):
</bodyText>
<equation confidence="0.998243">
φFIR(x) = −tr (Ix(θ)−1IU(θ)) , (4)
</equation>
<bodyText confidence="0.998754789473684">
where Ix(θ) and IU(θ) are Fisher information ma-
trices for sequence x and the unlabeled pool U, re-
spectively. The leading minus sign again ensures
that φFIR is a maximizer for use with Algorithm 1.
Previously, Fisher information for active learning
has only been investigated in the context of simple
binary classification. When employing FIR with se-
quence models like CRFs, there are two additional
computational challenges. First, we must integrate
over all possible labelings y, which can, as we have
seen, be approximated as an expectation over the N-
best labelings. Second, the inner product in the ratio
calculation (4) requires inverting a K × K matrix
for each x. In most interesting natural language ap-
plications, K is very large, making this algorithm
intractable. However, it is common in similar situ-
ations to approximate the Fisher information matrix
with its diagonal (Nyffenegger et al., 2006). Thus
we estimate Ix(θ) using:
</bodyText>
<equation confidence="0.990646727272727">
X &amp;quot; �∂ log P(ˆy|x; θ)�2
IX(θ) = P(ˆy|x; θ) + δ, ... ,
yEN ∂θ1
C∂ log P(ˆy|x; θ) l2 #
+ δ ,
∂θK
and IU(θ) using:
XU
1
IU(θ) = U
u=1
</equation>
<bodyText confidence="0.999045">
For CRFs, the partial derivative at the root of each
element in the diagonal vector is given by:
</bodyText>
<equation confidence="0.967328333333333">
∂ log P(ˆy|x; θ)
∂θk
P(y, y�|x)fk(y, y&apos;, xt),
</equation>
<bodyText confidence="0.999963181818182">
which is similar to the equation used to compute the
training gradient, but without a regularization term.
A smoothing parameter δ « 1 is added to prevent
division by zero when computing the ratio.
Notice that this method implicitly selects repre-
sentative instances by favoring queries with Fisher
information Ix(θ) that is not only high, but similar
to that of the overall data distribution IU(θ). This
is in contrast to information density, which tries to
query representative instances by explicitly model-
ing the distribution with a density weight.
</bodyText>
<equation confidence="0.997994928571429">
x~ · ~x(u)
k~xk × k~x(u)k.
Ix(U)(θ).
fk(ˆyt−1, ˆyt, xt)
=
T
X
t=1
−
T
X
t=1
X
y,y&apos;
</equation>
<page confidence="0.978147">
1075
</page>
<table confidence="0.999889888888889">
Corpus Entities Features Instances
CoNLL-03 4 78,644 19,959
NLPBA 5 128,401 18,854
BioCreative 1 175,331 10,000
FlySlip 1 31,353 1,220
CORA:Headers 15 22,077 935
CORA:References 13 4,208 500
Sig+Reply 2 25 617
SigIE 12 10,600 250
</table>
<tableCaption confidence="0.999873">
Table 1: Properties of the different evaluation corpora.
</tableCaption>
<sectionHeader confidence="0.994495" genericHeader="method">
4 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.9998144">
In this section we present a large-scale empirical
analysis of the query strategies described in Sec-
tion 3 on eight benchmark information extraction
and document segmentation corpora. The data sets
are summarized in Table 1.
</bodyText>
<subsectionHeader confidence="0.958504">
4.1 Data and Methodology
</subsectionHeader>
<bodyText confidence="0.999951372549019">
CoNLL-03 (Sang and DeMeulder, 2003) is a col-
lection of newswire articles annotated with four en-
tities: person, organization, location, and misc.
NLPBA (Kim et al., 2004) is a large collection
of biomedical abstracts annotated with five entities
of interest, such as protein, RNA, and cell-type.
BioCreative (Yeh et al., 2005) and FlySlip (Vla-
chos, 2007) also comprise texts in the biomedical
domain, annotated for gene entity mentions in arti-
cles from the human and fruit fly literature, respec-
tively. CORA (Peng and McCallum, 2004) consists
of two collections: a set of research paper headers
annotated for entities such as title, author, and insti-
tution; and a collection of references annotated with
BibTeX fields such as journal, year, and publisher.
The Sig+Reply corpus (Carvalho and Cohen, 2004)
is a set of email messages annotated for signature
and quoted reply line segments. SigIE is a subset of
the signature blocks from Sig+Reply which we have
enhanced with several address book fields such as
name, email, and phone. All corpora are format-
ted in the “IOB” sequence representation (Ramshaw
and Marcus, 1995).
We implement all fifteen query selection strate-
gies described in Section 3 for use with CRFs, and
evaluate them on all eight data sets. We also com-
pare against two baseline strategies: random in-
stance selection (i.e., passive learning), and naively
querying the longest sequence in terms of tokens.
We use a typical feature set for each corpus based on
the cited literature (including words, orthographic
patterns, part-of-speech, lexicons, etc.). Where the
N-best approximation is used N = 15, and for all
QBC methods C = 3; these figures exhibited a good
balance of accuracy and training speed in prelimi-
nary work. For information density, we arbitrarily
set Q = 1 (i.e., the information and density terms
have equal weight). In each experiment, L is ini-
tialized with five random labeled instances, and up
to 150 queries are subsequently selected from U in
batches of size B = 5. All results are averaged
across five folds using cross-validation.
We evaluate each query strategy by constructing
learning curves that plot the overall F1 measure (for
all entities or segments) as a function of the num-
ber of instances queried. Due to lack of space, we
cannot show learning curves for every experiment.
Instead, Table 2 summarizes our results by reporting
the area under the learning curve for all strategies
on all data. Figure 3 presents a few representative
learning curves for six of the corpora.
</bodyText>
<subsectionHeader confidence="0.99924">
4.2 Discussion of Learning Curves
</subsectionHeader>
<bodyText confidence="0.999639875">
The first conclusion we can draw from these results
is that there is no single clear winner. However, in-
formation density (ID), which we introduce in this
paper, stands out. It usually improves upon the base
sequence entropy measure, never performs poorly,
and has the highest average area under the learning
curve across all tasks. It seems particularly effective
on large corpora, which is a typical assumption for
the active learning setting. Sequence vote entropy
(SVE), a QBC method we propose here, is also note-
worthy in that it is fairly consistently among the top
three strategies, although never the best.
Second, the top uncertainty sampling strategies
are least confidence (LC) and sequence entropy
(SE), the latter being the dominant entropy-based
method. Among the QBC strategies, sequence vote
entropy (SVE) is the clear winner. We conclude that
these three methods are the best base information
measures for use with information density.
Third, query strategies that evaluate the en-
tire sequence (SE, SVE, SKL) are generally su-
perior to those which aggregate token-level infor-
mation. Furthermore, the total token-level strate-
gies (TTE, TVE, TKL) outperform their length-
</bodyText>
<page confidence="0.972932">
1076
</page>
<table confidence="0.999389727272727">
Corpus Baselines LC Uncertainty Sampling NSE VE Query-By-Committee SKL EGL Other FIR
Rand Long M TE TTE SE KL TVE TKL SVE ID
CoNLL-03 78.8 79.4 89.4 84.5 38.9 89.7 90.1 89.1 45.9 62.0 86.7 81.7 89.0 87.9 87.3 89.6 81.7
NLPBA 59.9 67.6 71.0 62.9 53.4 70.9 71.5 68.9 52.4 53.1 66.9 63.5 71.8 68.5 69.3 73.1 73.6
BioCreative 34.6 26.9 54.8 46.8 37.8 53.0 56.0 50.5 35.2 37.4 49.2 45.1 56.6 50.8 51.5 59.1 58.8
FlySlip 112.1 121.0 125.1 119.5 110.3 124.9 125.4 124.1 113.3 109.4 124.1 119.5 122.7 120.7 125.9 126.8 118.2
Headers 76.0 78.2 81.4 78.6 78.5 78.5 80.8 80.4 72.8 78.5 79.7 78.5 80.7 78.4 79.6 80.2 79.1
References 90.0 86.0 89.8 91.5 84.4 88.6 88.4 89.4 85.1 89.1 88.7 88.2 89.9 86.9 88.2 88.7 87.1
Sig+Reply 129.1 129.6 132.1 132.3 131.7 131.6 131.4 133.1 131.4 130.7 132.1 130.6 132.8 132.3 130.5 131.5 133.2
SigIE 84.3 82.7 88.8 87.3 89.3 88.3 87.6 89.1 89.8 85.5 89.7 85.1 89.5 89.7 87.7 88.5 88.5
Average 83.1 83.9 91.6 87.9 78.0 90.7 91.4 90.6 78.2 80.7 89.6 86.5 91.6 89.4 90.0 92.2 90.0
</table>
<tableCaption confidence="0.9975966">
Table 2: Detailed results for all query strategies on all evaluation corpora. Reported is the area under the Fl learning
curve for each strategy after 150 queries (maximum possible score is 150). For each row, the best method is shown
boxed in bold, the second best is shown underlined in bold, and the third best is shown in bold. The last row summa-
rizes the results across all eight tasks by reporting the average area for each strategy. Query strategy formulations for
sequence models introduced in this paper are indicated with italics along the top.
</tableCaption>
<bodyText confidence="0.999378076923077">
normalized counterparts (TE, VE, KL) in nearly all
cases. In fact, the normalized variants are often in-
ferior even to the baselines. While an argument can
be made that these shorter sequences might be eas-
ier to label from a human annotator’s perspective,
our ongoing work indicates that the relationship be-
tween instance length and actual labeling costs (e.g.,
elapsed annotation time) is not a simple one. Anal-
ysis of our experiment logs also shows that length-
normalized methods are occasionally biased toward
short sequences with little intuitive value (e.g., sen-
tences with few or no entities to label). In addition,
vote entropy appears to be a better disagreement
measure for QBC strategies than KL divergence.
Finally, Fisher information (FIR), while theoreti-
cally sound, exhibits behavior that is difficult to in-
terpret. It is sometimes the winning strategy, but oc-
casionally only on par with the baselines. When it
does show significant gains over the other strategies,
these gains appear to be only for the first several
queries (e.g., NLPBA and BioCreative in Figure 3).
This inconsistent performance may be a result of the
approximations made for computational efficiency.
Expected gradient length (EGL) also appears to ex-
hibit mediocre performance, and is likely not worth
its additional computational expense.
</bodyText>
<subsectionHeader confidence="0.999769">
4.3 Discussion of Run Times
</subsectionHeader>
<bodyText confidence="0.99999115625">
Here we discuss the execution times for each query
strategy using current hardware. The uncertainty
sampling methods are roughly comparable in run
time (token-based methods run slightly faster), each
routinely evaluating tens of thousands of sequences
in under a minute. The QBC methods, on the other
hand, must re-train multiple models with each query,
resulting in a lag of three to four minutes per query
batch (and up to 20 minutes for corpora with more
entity labels).
The expected gradient length and Fisher informa-
tion methods are the most computationally expen-
sive, because they must first perform inference over
the possible labelings and then calculate gradients
for each candidate label sequence. As a result, they
take eight to ten minutes (upwards of a half hour on
the larger corpora) for each query. Unlike the other
strategies, their time complexities also scale linearly
with the number of model parameters K which, in
turn, increases as new sequences are added to L.
As noted in Section 3.4, information density in-
curs a large computational cost to estimate the den-
sity weights, but these can be pre-computed and
cached for efficient lookup. In our experiments, this
pre-processing step takes less than a minute for the
smaller corpora, about a half hour for CoNLL-03
and BioCreative, and under two hours for NLPBA.
The density lookup causes no significant change in
the run time of the base information measure. Given
these results, we advocate information density with
an uncertainty sampling base measure in practice,
particularly for active learning with large corpora.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99987325">
In this paper, we have presented a detailed analy-
sis of active learning for sequence labeling tasks.
In particular, we have described and criticized the
query selection strategies used with probabilistic se-
</bodyText>
<page confidence="0.98105">
1077
</page>
<figure confidence="0.999740074074074">
F1 measure
F1 measure
CoNLL-03
information density (ID)
Fisher information (FIR)
query-by-committee (SVE)
random
0 20 40 60 80 100 120 140
0 20 40 60 80 100 120 140
FlySlip
information density (ID)
Fisher information (FIR)
query-by-committee (SVE)
random
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0.8
0.6
0.4
0.2
0
0 20 40 60 80 100 120 140
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
NLPBA
information density (ID)
Fisher information (FIR)
query-by-committee (SVE)
random
0.7
0.6
0.5
0.4
0.3
information density (ID)
0.2
Fisher information (FIR)
0.1 query-by-committee (SVE)
random
0
BioCreative
0 20 40 60 80 100 120 140
0.8
0.6
0.4
0.2
0
1
Sig+Reply
information density (ID)
Fisher information (FIR)
query-by-committee (SVE)
random
0.7
0.6
0.5
0.4
0.3
information density (ID)
0.2
Fisher information (FIR)
0.1 query-by-committee (SVE)
random
0
SigIE
0 20 40 60 80 100 120 140
0 20 40 60 80 100 120 140
number of instances queried number of instances queried number of instances queried
</figure>
<figureCaption confidence="0.999868">
Figure 3: Learning curves for selected query strategies on six of the evaluation corpora.
</figureCaption>
<bodyText confidence="0.999923625">
quence models to date, and proposed several novel
strategies to address some of their shortcomings.
Our large-scale empirical evaluation demonstrates
that some of these newly proposed methods advance
the state of the art in active learning with sequence
models. These methods include information density
(which we recommend in practice), sequence vote
entropy, and sometimes Fisher information.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999891333333333">
We would like to thank the anonymous reviewers for
their helpful feedback. This work was supported by
NIH grants T15-LM07359 and R01-LM07050.
</bodyText>
<sectionHeader confidence="0.998898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997144217391304">
N. Abe and H. Mamitsuka. 1998. Query learning strate-
gies using boosting and bagging. In Proceedings of
the International Conference on Machine Learning
(ICML), pages 1–9. Morgan Kaufmann.
J. Baldridge and M. Osborne. 2004. Active learning and
the total cost of annotation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 9–16. ACL Press.
V.R. Carvalho and W. Cohen. 2004. Learning to extract
signature and reply lines from email. In Proceedings
of the Conference on Email and Anti-Spam (CEAS).
D. Cohn, L. Atlas, and R. Ladner. 1994. Improving gen-
eralization with active learning. Machine Learning,
15(2):201–221.
A. Culotta and A. McCallum. 2005. Reducing labeling
effort for stuctured prediction tasks. In Proceedings
of the National Conference on Artificial Intelligence
(AAAI), pages 746–751. AAAI Press.
I. Dagan and S. Engelson. 1995. Committee-based
sampling for training probabilistic classifiers. In Pro-
ceedings of the International Conference on Machine
Learning (ICML), pages 150–157. Morgan Kaufmann.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30(3):73–77.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recognition
task at JNLPBA. In Proceedings of the International
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (NLPBA), pages 70–
75.
S. Kim, Y. Song, K. Kim, J.W. Cha, and G.G. Lee.
2006. MMR-based active machine learning for bio
named entity recognition. In Proceedings of Human
Language Technology and the North American Asso-
ciation for Computational Linguistics (HLT-NAACL),
pages 69–72. ACL Press.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning
(ICML), pages 282–289. Morgan Kaufmann.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language, 4:35–56.
D. Lewis and J. Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In Pro-
</reference>
<page confidence="0.875684">
1078
</page>
<reference confidence="0.9992945125">
ceedings of the International Conference on Machine
Learning (ICML), pages 148–156. Morgan Kaufmann.
G. Mann and A. McCallum. 2007. Efficient computation
of entropy gradient for semi-supervised conditional
random fields. In Proceedings of the North American
Association for Computational Linguistics (NAACL),
pages 109–112. ACL Press.
A. McCallum and K. Nigam. 1998. Employing EM
in pool-based active learning for text classification.
In Proceedings of the International Conference on
Machine Learning (ICML), pages 359–367. Morgan
Kaufmann.
M. Nyffenegger, J.C. Chappelier, and E. Gaussier. 2006.
Revisiting Fisher kernels for document similarities. In
Proceedings of the European Conference on Machine
Learning (ECML), pages 727–734. Springer.
F. Peng and A. McCallum. 2004. Accurate information
extraction from research papers using conditional ran-
dom fields. In Proceedings of Human Language Tech-
nology and the North American Association for Com-
putational Linguistics (HLT-NAACL). ACL Press.
L. R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257–286.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the ACL Workshop on Very Large Corpora.
N. Roy and A. McCallum. 2001. Toward optimal active
learning through sampling estimation of error reduc-
tion. In Proceedings of the International Conference
on Machine Learning (ICML), pages 441–448. Mor-
gan Kaufmann.
E.F.T.K. Sang and F. DeMeulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent
named entity recognition. In Proceedings of the
Conference on Natural Language Learning (CoNLL),
pages 142–147.
T. Scheffer, C. Decomain, and S. Wrobel. 2001. Ac-
tive hidden Markov models for information extraction.
In Proceedings of the International Conference on Ad-
vances in Intelligent Data Analysis (CAIDA), pages
309–318. Springer-Verlag.
R. Schwartz and Y.-L. Chow. 1990. The N-best algo-
rithm: an efficient and exact procedure for finding the
N most likely sentence hypotheses. In Proceedings
of the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 81–83. IEEE
Press.
B. Settles, M. Craven, and S. Ray. 2008. Multiple-
instance active learning. In Advances in Neural Infor-
mation Processing Systems (NIPS), volume 20, pages
1289–1296. MIT Press.
H.S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proceedings of the ACM
Workshop on Computational Learning Theory, pages
287–294.
C. E. Shannon. 1948. A mathematical theory of com-
munication. Bell System Technical Journal, 27:379–
423,623–656.
C. Sutton and A. McCallum. 2006. An introduction to
conditional random fields for relational learning. In
L. Getoor and B. Taskar, editors, Introduction to Sta-
tistical Relational Learning. MIT Press.
A. Vlachos. 2007. Evaluating and combining biomedical
named entity recognition systems. In BioNLP 2007:
Biological, translational, and clinical language pro-
cessing, pages 199–206.
A. Yeh, A. Morgan, M. Colosimo, and L. Hirschman.
2005. Biocreative task 1a: gene mention finding eval-
uation. BMC Bioinformatics, 6(Suppl 1):S2.
T. Zhang and F.J. Oles. 2000. A probability analysis
on the value of unlabeled data for classification prob-
lems. In Proceedings of the International Conference
on Machine Learning (ICML), pages 1191–1198. Mor-
gan Kaufmann.
X. Zhu, J. Lafferty, and Z. Ghahramani. 2003. Combin-
ing active learning and semi-supervised learning using
gaussian fields and harmonic functions. In Proceed-
ings of the ICML Workshop on the Continuum from
Labeled to Unlabeled Data, pages 58–65.
</reference>
<page confidence="0.997358">
1079
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.741149">
<title confidence="0.985785">An Analysis of Active Learning Strategies for Sequence Labeling Tasks</title>
<affiliation confidence="0.936037">of Computer University of</affiliation>
<address confidence="0.997386">Madison, WI 53706,</address>
<email confidence="0.999028">bsettles@cs.wisc.edu</email>
<affiliation confidence="0.9920785">of Biostatistics &amp; Medical University of</affiliation>
<address confidence="0.997335">Madison, WI 53706,</address>
<email confidence="0.99979">craven@biostat.wisc.edu</email>
<abstract confidence="0.992097733333333">Active learning is well-suited to many problems in natural language processing, where unlabeled data may be abundant but annotation is slow and expensive. This paper aims to shed light on the best active learning approaches for sequence labeling tasks such as information extraction and document segmentation. We survey previously used query selection strategies for sequence models, and propose several novel algorithms to address their shortcomings. We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed methods advance the state of the art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Abe</author>
<author>H Mamitsuka</author>
</authors>
<title>Query learning strategies using boosting and bagging.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>1--9</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="14695" citStr="Abe and Mamitsuka, 1998" startWordPosition="2470" endWordPosition="2473">g in a constant-time factor of Viterbi’s complexity. Hwa (2004) employed a similar approach for active learning with probabilistic context-free grammars. 3.2 Query-By-Committee Another general active learning framework is the query-by-committee (QBC) approach (Seung et al., 1992). In this setting, we use a committee of models C = {θ(1),...,θ(C)} to represent C different hypotheses that are consistent with the labeled set L. The most informative query, then, is the instance over which the committee is in most disagreement about how to label. In particular, we use the query-by-bagging approach (Abe and Mamitsuka, 1998) to learn a committee of CRFs. In each round of active learning, L is sampled (with replacement) L times to create a unique, modified labeled set L(c). Each model θ(c) ∈ C is then trained using its own corresponding labeled set L(c). To measure disagreement among committee members, we consider two alternatives. Dagan and Engelson (1995) introduced QBC with HMMs for part-of-speech tagging using a measure called vote entropy (VE): V (yt, m) V (yt, m) C log C , where V (yt, m) is the number of “votes” label m receives from all the committee member’s Viterbi labelings at sequence position t. φV E </context>
</contexts>
<marker>Abe, Mamitsuka, 1998</marker>
<rawString>N. Abe and H. Mamitsuka. 1998. Query learning strategies using boosting and bagging. In Proceedings of the International Conference on Machine Learning (ICML), pages 1–9. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baldridge</author>
<author>M Osborne</author>
</authors>
<title>Active learning and the total cost of annotation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>9--16</pages>
<publisher>ACL Press.</publisher>
<contexts>
<context position="10769" citStr="Baldridge and Osborne, 2004" startWordPosition="1800" endWordPosition="1803"> labelings. One way this has been done with probabilistic sequence models is by computing what we call token entropy (TE): Pθ(yt = m) log Pθ(yt = m), (2) where T is the length of x, m ranges over all possible token labels, and PB(yt = m) is shorthand for the marginal probability that m is the label at position t in the sequence, according to the model. For CRFs and HMMs, these marginals can be efficiently computed using the forward and backward algorithms (Rabiner, 1989). The summed token entropies have typically been normalized by sequence length T, to avoid simply querying longer sequences (Baldridge and Osborne, 2004; Hwa, 2004). However, we argue that querying long sequences should not be explicitly discouraged, if in fact they contain more information. Thus, we also propose the total token entropy (TTE) measure: φTTE(x) = T X φTE(x). For most sequence labeling tasks, however, it is more appropriate to consider the entropy of the label sequence y as a whole, rather than some aggregate of individual token entropies. Thus an alternate query strategy is sequence entropy (SE): φSE(x) = − � P(�y|x; θ) log P(�y|x; θ), (3) J&apos; where y� ranges over all possible label sequences for input sequence x. Note, however,</context>
</contexts>
<marker>Baldridge, Osborne, 2004</marker>
<rawString>J. Baldridge and M. Osborne. 2004. Active learning and the total cost of annotation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9–16. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V R Carvalho</author>
<author>W Cohen</author>
</authors>
<title>Learning to extract signature and reply lines from email.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Email and Anti-Spam (CEAS).</booktitle>
<contexts>
<context position="1945" citStr="Carvalho and Cohen, 2004" startWordPosition="298" endWordPosition="301">pool U, learns from these labeled queries (which are then added to L), and repeats. In this way, the learner aims to achieve high accuracy with as little labeling effort as possible. Thus, active learning can be valuable in domains where unlabeled data are readily available, but obtaining training labels is expensive. Such is the case with many sequence labeling tasks in natural language domains. For example, part-of-speech tagging (Seung et al., 1992; Lafferty et al., 2001), information extraction (Scheffer et al., 2001; Sang and DeMeulder, 2003; Kim et al., 2004), and document segmentation (Carvalho and Cohen, 2004) are all typically treated as sequence labeling problems. The source data for these tasks (i.e., text documents in electronic form) are often easily obtained. However, due to the nature of sequence labeling tasks, annotating these texts can be rather tedious and time-consuming, making active learning an attractive technique. While there has been much work on active learning for classification (Cohn et al., 1994; McCallum and Nigam, 1998; Zhang and Oles, 2000; Zhu et al., 2003), active learning for sequence labeling has received considerably less attention. A few methods have been proposed, bas</context>
<context position="24138" citStr="Carvalho and Cohen, 2004" startWordPosition="4074" endWordPosition="4077">rge collection of biomedical abstracts annotated with five entities of interest, such as protein, RNA, and cell-type. BioCreative (Yeh et al., 2005) and FlySlip (Vlachos, 2007) also comprise texts in the biomedical domain, annotated for gene entity mentions in articles from the human and fruit fly literature, respectively. CORA (Peng and McCallum, 2004) consists of two collections: a set of research paper headers annotated for entities such as title, author, and institution; and a collection of references annotated with BibTeX fields such as journal, year, and publisher. The Sig+Reply corpus (Carvalho and Cohen, 2004) is a set of email messages annotated for signature and quoted reply line segments. SigIE is a subset of the signature blocks from Sig+Reply which we have enhanced with several address book fields such as name, email, and phone. All corpora are formatted in the “IOB” sequence representation (Ramshaw and Marcus, 1995). We implement all fifteen query selection strategies described in Section 3 for use with CRFs, and evaluate them on all eight data sets. We also compare against two baseline strategies: random instance selection (i.e., passive learning), and naively querying the longest sequence i</context>
</contexts>
<marker>Carvalho, Cohen, 2004</marker>
<rawString>V.R. Carvalho and W. Cohen. 2004. Learning to extract signature and reply lines from email. In Proceedings of the Conference on Email and Anti-Spam (CEAS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cohn</author>
<author>L Atlas</author>
<author>R Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="2359" citStr="Cohn et al., 1994" startWordPosition="365" endWordPosition="368">peech tagging (Seung et al., 1992; Lafferty et al., 2001), information extraction (Scheffer et al., 2001; Sang and DeMeulder, 2003; Kim et al., 2004), and document segmentation (Carvalho and Cohen, 2004) are all typically treated as sequence labeling problems. The source data for these tasks (i.e., text documents in electronic form) are often easily obtained. However, due to the nature of sequence labeling tasks, annotating these texts can be rather tedious and time-consuming, making active learning an attractive technique. While there has been much work on active learning for classification (Cohn et al., 1994; McCallum and Nigam, 1998; Zhang and Oles, 2000; Zhu et al., 2003), active learning for sequence labeling has received considerably less attention. A few methods have been proposed, based mostly on the conventions of uncertainty sampling, where the learner queries the instance about which it has the least certainty (Scheffer et al., 2001; Culotta and McCallum, 2005; Kim et al., 2006), or query-by-committee, where a “committee” of models selects the instance about which its members most disagree (Dagan and Engelson, 1995). We provide more detail on these and the new strategies we propose in Se</context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>D. Cohn, L. Atlas, and R. Ladner. 1994. Improving generalization with active learning. Machine Learning, 15(2):201–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>A McCallum</author>
</authors>
<title>Reducing labeling effort for stuctured prediction tasks.</title>
<date>2005</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>746--751</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="2727" citStr="Culotta and McCallum, 2005" startWordPosition="425" endWordPosition="428">d. However, due to the nature of sequence labeling tasks, annotating these texts can be rather tedious and time-consuming, making active learning an attractive technique. While there has been much work on active learning for classification (Cohn et al., 1994; McCallum and Nigam, 1998; Zhang and Oles, 2000; Zhu et al., 2003), active learning for sequence labeling has received considerably less attention. A few methods have been proposed, based mostly on the conventions of uncertainty sampling, where the learner queries the instance about which it has the least certainty (Scheffer et al., 2001; Culotta and McCallum, 2005; Kim et al., 2006), or query-by-committee, where a “committee” of models selects the instance about which its members most disagree (Dagan and Engelson, 1995). We provide more detail on these and the new strategies we propose in Section 3. The comparative effectiveness of these approaches, however, has not been studied. Furthermore, it has been suggested that uncertainty sampling and query-by-committee fail on occasion (Roy and McCallum, 2001; Zhu et al., 2003) by querying outliers, e.g., instances considered informative in isolation by the learner, but containing little information about the</context>
<context position="8758" citStr="Culotta and McCallum (2005)" startWordPosition="1453" endWordPosition="1456">earning. Algorithm 1 provides a sketch of the generic poolbased active learning scenario. In the remainder of this section, we describe various query strategy formulations of φ(·) that have been used for active learning with sequence models. We also point out where we think these approaches may be flawed, and propose several novel query strategies to address these issues. 3.1 Uncertainty Sampling One of the most common general frameworks for measuring informativeness is uncertainty sampling (Lewis and Catlett, 1994), where a learner queries the instance that it is most uncertain how to label. Culotta and McCallum (2005) employ a simple uncertainty-based strategy for sequence models called least confidence (LC): φLC(x) = 1 − P(y*|x; θ). Here, y* is the most likely label sequence, i.e., the Viterbi parse. This approach queries the instance for which the current model has the least confidence in its most likely labeling. For CRFs, this confidence can be calculated using the posterior probability given by Equation (1). Scheffer et al. (2001) propose another uncertainty strategy, which queries the instance with the smallest margin between the posteriors for its two most likely labelings. We call this approach mar</context>
</contexts>
<marker>Culotta, McCallum, 2005</marker>
<rawString>A. Culotta and A. McCallum. 2005. Reducing labeling effort for stuctured prediction tasks. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 746–751. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>S Engelson</author>
</authors>
<title>Committee-based sampling for training probabilistic classifiers.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>150--157</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="2886" citStr="Dagan and Engelson, 1995" startWordPosition="449" endWordPosition="452">technique. While there has been much work on active learning for classification (Cohn et al., 1994; McCallum and Nigam, 1998; Zhang and Oles, 2000; Zhu et al., 2003), active learning for sequence labeling has received considerably less attention. A few methods have been proposed, based mostly on the conventions of uncertainty sampling, where the learner queries the instance about which it has the least certainty (Scheffer et al., 2001; Culotta and McCallum, 2005; Kim et al., 2006), or query-by-committee, where a “committee” of models selects the instance about which its members most disagree (Dagan and Engelson, 1995). We provide more detail on these and the new strategies we propose in Section 3. The comparative effectiveness of these approaches, however, has not been studied. Furthermore, it has been suggested that uncertainty sampling and query-by-committee fail on occasion (Roy and McCallum, 2001; Zhu et al., 2003) by querying outliers, e.g., instances considered informative in isolation by the learner, but containing little information about the rest of the distribution of instances. Proposed methods for dealing with these shortcomings have so far only considered classification tasks. 1070 Proceedings</context>
<context position="15033" citStr="Dagan and Engelson (1995)" startWordPosition="2527" endWordPosition="2530">,...,θ(C)} to represent C different hypotheses that are consistent with the labeled set L. The most informative query, then, is the instance over which the committee is in most disagreement about how to label. In particular, we use the query-by-bagging approach (Abe and Mamitsuka, 1998) to learn a committee of CRFs. In each round of active learning, L is sampled (with replacement) L times to create a unique, modified labeled set L(c). Each model θ(c) ∈ C is then trained using its own corresponding labeled set L(c). To measure disagreement among committee members, we consider two alternatives. Dagan and Engelson (1995) introduced QBC with HMMs for part-of-speech tagging using a measure called vote entropy (VE): V (yt, m) V (yt, m) C log C , where V (yt, m) is the number of “votes” label m receives from all the committee member’s Viterbi labelings at sequence position t. φV E (x) = − T 1 T t=1 XM m=1 1073 3.3 Expected Gradient Length A third general active learning framework we consider is to query the instance that would impart the greatest change to the current model if we knew its label. Since we train discriminative models like CRFs using gradient-based optimization, this involves querying the instance w</context>
</contexts>
<marker>Dagan, Engelson, 1995</marker>
<rawString>I. Dagan and S. Engelson. 1995. Committee-based sampling for training probabilistic classifiers. In Proceedings of the International Conference on Machine Learning (ICML), pages 150–157. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
</authors>
<title>Sample selection for statistical parsing.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="10781" citStr="Hwa, 2004" startWordPosition="1804" endWordPosition="1805">been done with probabilistic sequence models is by computing what we call token entropy (TE): Pθ(yt = m) log Pθ(yt = m), (2) where T is the length of x, m ranges over all possible token labels, and PB(yt = m) is shorthand for the marginal probability that m is the label at position t in the sequence, according to the model. For CRFs and HMMs, these marginals can be efficiently computed using the forward and backward algorithms (Rabiner, 1989). The summed token entropies have typically been normalized by sequence length T, to avoid simply querying longer sequences (Baldridge and Osborne, 2004; Hwa, 2004). However, we argue that querying long sequences should not be explicitly discouraged, if in fact they contain more information. Thus, we also propose the total token entropy (TTE) measure: φTTE(x) = T X φTE(x). For most sequence labeling tasks, however, it is more appropriate to consider the entropy of the label sequence y as a whole, rather than some aggregate of individual token entropies. Thus an alternate query strategy is sequence entropy (SE): φSE(x) = − � P(�y|x; θ) log P(�y|x; θ), (3) J&apos; where y� ranges over all possible label sequences for input sequence x. Note, however, that the nu</context>
<context position="14134" citStr="Hwa (2004)" startWordPosition="2386" endWordPosition="2387">(ˆy|x; θ), ˆ�EN where N = {yi, ... , y*}, the set of the N most likely parses, and the posteriors are re-normalized (i.e., Z(x) in Equation (1) only ranges over N). For N = 2, this approximation is equivalent to φM, thus N-best sequence entropy can be thought of as a generalization of the margin approach. Recently, an efficient entropy calculation via dynamic programming was proposed for CRFs in the context of semi-supervised learning (Mann and McCallum, 2007). We use this algorithm to compute the true sequence entropy (3) for active learning in a constant-time factor of Viterbi’s complexity. Hwa (2004) employed a similar approach for active learning with probabilistic context-free grammars. 3.2 Query-By-Committee Another general active learning framework is the query-by-committee (QBC) approach (Seung et al., 1992). In this setting, we use a committee of models C = {θ(1),...,θ(C)} to represent C different hypotheses that are consistent with the labeled set L. The most informative query, then, is the instance over which the committee is in most disagreement about how to label. In particular, we use the query-by-bagging approach (Abe and Mamitsuka, 1998) to learn a committee of CRFs. In each </context>
</contexts>
<marker>Hwa, 2004</marker>
<rawString>R. Hwa. 2004. Sample selection for statistical parsing. Computational Linguistics, 30(3):73–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>T Ohta</author>
<author>Y Tsuruoka</author>
<author>Y Tateisi</author>
<author>N Collier</author>
</authors>
<title>Introduction to the bio-entity recognition task at JNLPBA.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA),</booktitle>
<pages>70--75</pages>
<contexts>
<context position="1891" citStr="Kim et al., 2004" startWordPosition="291" endWordPosition="294">mative query instances from a large unlabeled pool U, learns from these labeled queries (which are then added to L), and repeats. In this way, the learner aims to achieve high accuracy with as little labeling effort as possible. Thus, active learning can be valuable in domains where unlabeled data are readily available, but obtaining training labels is expensive. Such is the case with many sequence labeling tasks in natural language domains. For example, part-of-speech tagging (Seung et al., 1992; Lafferty et al., 2001), information extraction (Scheffer et al., 2001; Sang and DeMeulder, 2003; Kim et al., 2004), and document segmentation (Carvalho and Cohen, 2004) are all typically treated as sequence labeling problems. The source data for these tasks (i.e., text documents in electronic form) are often easily obtained. However, due to the nature of sequence labeling tasks, annotating these texts can be rather tedious and time-consuming, making active learning an attractive technique. While there has been much work on active learning for classification (Cohn et al., 1994; McCallum and Nigam, 1998; Zhang and Oles, 2000; Zhu et al., 2003), active learning for sequence labeling has received considerably</context>
<context position="23505" citStr="Kim et al., 2004" startWordPosition="3974" endWordPosition="3977">FlySlip 1 31,353 1,220 CORA:Headers 15 22,077 935 CORA:References 13 4,208 500 Sig+Reply 2 25 617 SigIE 12 10,600 250 Table 1: Properties of the different evaluation corpora. 4 Empirical Evaluation In this section we present a large-scale empirical analysis of the query strategies described in Section 3 on eight benchmark information extraction and document segmentation corpora. The data sets are summarized in Table 1. 4.1 Data and Methodology CoNLL-03 (Sang and DeMeulder, 2003) is a collection of newswire articles annotated with four entities: person, organization, location, and misc. NLPBA (Kim et al., 2004) is a large collection of biomedical abstracts annotated with five entities of interest, such as protein, RNA, and cell-type. BioCreative (Yeh et al., 2005) and FlySlip (Vlachos, 2007) also comprise texts in the biomedical domain, annotated for gene entity mentions in articles from the human and fruit fly literature, respectively. CORA (Peng and McCallum, 2004) consists of two collections: a set of research paper headers annotated for entities such as title, author, and institution; and a collection of references annotated with BibTeX fields such as journal, year, and publisher. The Sig+Reply </context>
</contexts>
<marker>Kim, Ohta, Tsuruoka, Tateisi, Collier, 2004</marker>
<rawString>J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier. 2004. Introduction to the bio-entity recognition task at JNLPBA. In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA), pages 70– 75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>Y Song</author>
<author>K Kim</author>
<author>J W Cha</author>
<author>G G Lee</author>
</authors>
<title>MMR-based active machine learning for bio named entity recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of Human Language Technology and the North American Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>69--72</pages>
<publisher>ACL Press.</publisher>
<contexts>
<context position="2746" citStr="Kim et al., 2006" startWordPosition="429" endWordPosition="432">e of sequence labeling tasks, annotating these texts can be rather tedious and time-consuming, making active learning an attractive technique. While there has been much work on active learning for classification (Cohn et al., 1994; McCallum and Nigam, 1998; Zhang and Oles, 2000; Zhu et al., 2003), active learning for sequence labeling has received considerably less attention. A few methods have been proposed, based mostly on the conventions of uncertainty sampling, where the learner queries the instance about which it has the least certainty (Scheffer et al., 2001; Culotta and McCallum, 2005; Kim et al., 2006), or query-by-committee, where a “committee” of models selects the instance about which its members most disagree (Dagan and Engelson, 1995). We provide more detail on these and the new strategies we propose in Section 3. The comparative effectiveness of these approaches, however, has not been studied. Furthermore, it has been suggested that uncertainty sampling and query-by-committee fail on occasion (Roy and McCallum, 2001; Zhu et al., 2003) by querying outliers, e.g., instances considered informative in isolation by the learner, but containing little information about the rest of the distri</context>
<context position="13425" citStr="Kim et al., 2006" startWordPosition="2263" endWordPosition="2266"> from all models in the committee C, and P(ˆy|x; C) = C PC 1 P(ˆy|x; θ(c)), or the “consensus” posterior probability for some label sequence ˆy. This can be thought of as a QBC generalization of N-best entropy, where each committee member casts a vote for the posterior label distribution. We also explore a sequence Kullback-Leibler (SKL) variant: 1 C XC c=1 D(θ(c)kC) = Pθ(-)(yt = m) Pθ(-)(yt = m) log M X m=1 PC(yt = m) . φSKL(x) = 1 X Cc=1 P(ˆy|x;θ(c)) log P (ˆy|x;θ(c)) P(ˆy|x; C) . C X ˆ�ENC of possible labelings grows exponentially with the length of x. To make this feasible, previous work (Kim et al., 2006) has employed an approximation we call N-best sequence entropy (NSE): XφNSE(x) = − P(ˆy|x; θ) log P(ˆy|x; θ), ˆ�EN where N = {yi, ... , y*}, the set of the N most likely parses, and the posteriors are re-normalized (i.e., Z(x) in Equation (1) only ranges over N). For N = 2, this approximation is equivalent to φM, thus N-best sequence entropy can be thought of as a generalization of the margin approach. Recently, an efficient entropy calculation via dynamic programming was proposed for CRFs in the context of semi-supervised learning (Mann and McCallum, 2007). We use this algorithm to compute th</context>
</contexts>
<marker>Kim, Song, Kim, Cha, Lee, 2006</marker>
<rawString>S. Kim, Y. Song, K. Kim, J.W. Cha, and G.G. Lee. 2006. MMR-based active machine learning for bio named entity recognition. In Proceedings of Human Language Technology and the North American Association for Computational Linguistics (HLT-NAACL), pages 69–72. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="1799" citStr="Lafferty et al., 2001" startWordPosition="277" endWordPosition="280">aining set. A typical active learner begins with a small labeled set L, selects one or more informative query instances from a large unlabeled pool U, learns from these labeled queries (which are then added to L), and repeats. In this way, the learner aims to achieve high accuracy with as little labeling effort as possible. Thus, active learning can be valuable in domains where unlabeled data are readily available, but obtaining training labels is expensive. Such is the case with many sequence labeling tasks in natural language domains. For example, part-of-speech tagging (Seung et al., 1992; Lafferty et al., 2001), information extraction (Scheffer et al., 2001; Sang and DeMeulder, 2003; Kim et al., 2004), and document segmentation (Carvalho and Cohen, 2004) are all typically treated as sequence labeling problems. The source data for these tasks (i.e., text documents in electronic form) are often easily obtained. However, due to the nature of sequence labeling tasks, annotating these texts can be rather tedious and time-consuming, making active learning an attractive technique. While there has been much work on active learning for classification (Cohn et al., 1994; McCallum and Nigam, 1998; Zhang and Ol</context>
<context position="5177" citStr="Lafferty et al., 2001" startWordPosition="822" endWordPosition="825">nce labeling task. Let x = (x1,... , xT) be an observation sequence of length T with a corresponding label sequence y = (y1, ... , yT). Words in a sentence correspond to tokens in the input sequence x, which are mapped to labels in y. These labels indicate whether the word belongs to a particular entity class of interest (in this case, org and loc) or not (null). These labels can be assigned by a sequence model based on a finite state machine, such as the one shown to the right in Figure 1. We focus our discussion of active learning for sequence labeling on conditional random fields, or CRFs (Lafferty et al., 2001). The rest of this section serves as a brief introduction. CRFs are statistical graphical models which have demonstrated state-of-the-art accuracy on virtually all of the sequence labeling tasks mentioned in Section 1. We use linear-chain CRFs, which correspond to conditionally trained probabilistic finite state machines. A linear-chain CRF model with parameters θ defines the posterior probability of y given x to be1: T K P(yJ x; θ) = Z(x) exp ! X X θkfk(yt−1, yt, xt) . t�1 k�1 (1) 1Our discussion assumes, without loss of generality, that each label is uniquely represented by one state, thus e</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning (ICML), pages 282–289. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="7336" citStr="Lari and Young, 1990" startWordPosition="1202" endWordPosition="1205">y(l)Jx(l); θ) − XK θ2 l=1 k=1 k 2σ2, where L is the size of the labeled set L, and the second term is a Gaussian regularization penalty on 11θ11 to prevent over-fitting. After training, labels can be predicted for new sequences using the Viterbi algorithm. For more details on CRFs and their training procedures, see Sutton and McCallum (2006). Note that, while we describe the active learning algorithms in the next section in terms of linearchain CRFs, they have analogs for other kinds of sequence models, such as hidden Markov models, or HMMs (Rabiner, 1989), probabilistic contextfree grammars (Lari and Young, 1990), and general CRFs (Sutton and McCallum, 2006). 3 Active Learning with Sequence Models In order to select queries, an active learner must have a way of assessing how informative each instance is. Let x* be the most informative instance according to some query strategy φ(x), which is a function used to evaluate each instance x in the unlabeled pool U. ACME Inc. org offices org null the null null loc null org in Chicago loc ... 1071 Given: Labeled set L, unlabeled pool U, query strategy φ(·), query batch size B repeat // learn a model using the current L θ = train(L) ; forb= 1toBdo //query the m</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>J Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>148--156</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="8652" citStr="Lewis and Catlett, 1994" startWordPosition="1434" endWordPosition="1437"> U (x*, label(x*)) ; U = U − x* � ; end until some stopping criterion; Algorithm 1: Pool-based active learning. Algorithm 1 provides a sketch of the generic poolbased active learning scenario. In the remainder of this section, we describe various query strategy formulations of φ(·) that have been used for active learning with sequence models. We also point out where we think these approaches may be flawed, and propose several novel query strategies to address these issues. 3.1 Uncertainty Sampling One of the most common general frameworks for measuring informativeness is uncertainty sampling (Lewis and Catlett, 1994), where a learner queries the instance that it is most uncertain how to label. Culotta and McCallum (2005) employ a simple uncertainty-based strategy for sequence models called least confidence (LC): φLC(x) = 1 − P(y*|x; θ). Here, y* is the most likely label sequence, i.e., the Viterbi parse. This approach queries the instance for which the current model has the least confidence in its most likely labeling. For CRFs, this confidence can be calculated using the posterior probability given by Equation (1). Scheffer et al. (2001) propose another uncertainty strategy, which queries the instance wi</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>D. Lewis and J. Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proceedings of the International Conference on Machine Learning (ICML), pages 148–156. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mann</author>
<author>A McCallum</author>
</authors>
<title>Efficient computation of entropy gradient for semi-supervised conditional random fields.</title>
<date>2007</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics (NAACL),</booktitle>
<pages>109--112</pages>
<publisher>ACL Press.</publisher>
<contexts>
<context position="13988" citStr="Mann and McCallum, 2007" startWordPosition="2360" endWordPosition="2364">of x. To make this feasible, previous work (Kim et al., 2006) has employed an approximation we call N-best sequence entropy (NSE): XφNSE(x) = − P(ˆy|x; θ) log P(ˆy|x; θ), ˆ�EN where N = {yi, ... , y*}, the set of the N most likely parses, and the posteriors are re-normalized (i.e., Z(x) in Equation (1) only ranges over N). For N = 2, this approximation is equivalent to φM, thus N-best sequence entropy can be thought of as a generalization of the margin approach. Recently, an efficient entropy calculation via dynamic programming was proposed for CRFs in the context of semi-supervised learning (Mann and McCallum, 2007). We use this algorithm to compute the true sequence entropy (3) for active learning in a constant-time factor of Viterbi’s complexity. Hwa (2004) employed a similar approach for active learning with probabilistic context-free grammars. 3.2 Query-By-Committee Another general active learning framework is the query-by-committee (QBC) approach (Seung et al., 1992). In this setting, we use a committee of models C = {θ(1),...,θ(C)} to represent C different hypotheses that are consistent with the labeled set L. The most informative query, then, is the instance over which the committee is in most dis</context>
</contexts>
<marker>Mann, McCallum, 2007</marker>
<rawString>G. Mann and A. McCallum. 2007. Efficient computation of entropy gradient for semi-supervised conditional random fields. In Proceedings of the North American Association for Computational Linguistics (NAACL), pages 109–112. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
</authors>
<title>Employing EM in pool-based active learning for text classification.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>359--367</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="2385" citStr="McCallum and Nigam, 1998" startWordPosition="369" endWordPosition="372">g et al., 1992; Lafferty et al., 2001), information extraction (Scheffer et al., 2001; Sang and DeMeulder, 2003; Kim et al., 2004), and document segmentation (Carvalho and Cohen, 2004) are all typically treated as sequence labeling problems. The source data for these tasks (i.e., text documents in electronic form) are often easily obtained. However, due to the nature of sequence labeling tasks, annotating these texts can be rather tedious and time-consuming, making active learning an attractive technique. While there has been much work on active learning for classification (Cohn et al., 1994; McCallum and Nigam, 1998; Zhang and Oles, 2000; Zhu et al., 2003), active learning for sequence labeling has received considerably less attention. A few methods have been proposed, based mostly on the conventions of uncertainty sampling, where the learner queries the instance about which it has the least certainty (Scheffer et al., 2001; Culotta and McCallum, 2005; Kim et al., 2006), or query-by-committee, where a “committee” of models selects the instance about which its members most disagree (Dagan and Engelson, 1995). We provide more detail on these and the new strategies we propose in Section 3. The comparative e</context>
<context position="11445" citStr="McCallum and Nigam (1998)" startWordPosition="1921" endWordPosition="1924">sequences should not be explicitly discouraged, if in fact they contain more information. Thus, we also propose the total token entropy (TTE) measure: φTTE(x) = T X φTE(x). For most sequence labeling tasks, however, it is more appropriate to consider the entropy of the label sequence y as a whole, rather than some aggregate of individual token entropies. Thus an alternate query strategy is sequence entropy (SE): φSE(x) = − � P(�y|x; θ) log P(�y|x; θ), (3) J&apos; where y� ranges over all possible label sequences for input sequence x. Note, however, that the number 1 T φTE(x) = − M E m�� T t�� 1072 McCallum and Nigam (1998) propose a QBC strategy for classification based on Kullback-Leibler (KL) divergence, an information-theoretic measure of the difference between two probability distributions. The most informative query is considered to be the one with the largest average KL divergence between a committee member’s posterior label distribution and the consensus. We modify this approach for sequence models by summing the average KL scores using the marginals at each token position and, as with vote entropy, normalizing for length. We call this approach Kullback-Leibler (KL): T φKL 1 (x) = where (using shorthand </context>
<context position="19525" citStr="McCallum and Nigam (1998)" startWordPosition="3310" endWordPosition="3313">ture’s values across all tokens. We can then use cosine that J # K, and here differs slightly from the feature definition given in Section 2. Since the labels and yt are unknown before querying, the K features used for model training are reduced down to the J input features here, which factor out an 2Note fj(xt) yt−1 y label dependencies. B A XφEGL(x) = ˆyEN !β sim(x, x(u)) . T T 1074 similarity on this simplified representation: simcos(x, x(u)) = We have also investigated similarity functions based on exponentiated Euclidean distance and KLdivergence, the latter of which was also employed by McCallum and Nigam (1998) for density-weighting QBC in text classification. However, these measures show no improvement over cosine similarity, and require setting additional hyper-parameters. One potential drawback of information density is that the number of required similarity calculations grows quadratically with the number of instances in U. For pool-based active learning, we often assume that the size of U is very large. However, these densities only need to be computed once, and are independent of the base information measure. Thus, when employing information density in a realworld interactive learning setting,</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>A. McCallum and K. Nigam. 1998. Employing EM in pool-based active learning for text classification. In Proceedings of the International Conference on Machine Learning (ICML), pages 359–367. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nyffenegger</author>
<author>J C Chappelier</author>
<author>E Gaussier</author>
</authors>
<title>Revisiting Fisher kernels for document similarities.</title>
<date>2006</date>
<booktitle>In Proceedings of the European Conference on Machine Learning (ECML),</booktitle>
<pages>727--734</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="21830" citStr="Nyffenegger et al., 2006" startWordPosition="3685" endWordPosition="3688"> context of simple binary classification. When employing FIR with sequence models like CRFs, there are two additional computational challenges. First, we must integrate over all possible labelings y, which can, as we have seen, be approximated as an expectation over the Nbest labelings. Second, the inner product in the ratio calculation (4) requires inverting a K × K matrix for each x. In most interesting natural language applications, K is very large, making this algorithm intractable. However, it is common in similar situations to approximate the Fisher information matrix with its diagonal (Nyffenegger et al., 2006). Thus we estimate Ix(θ) using: X &amp;quot; �∂ log P(ˆy|x; θ)�2 IX(θ) = P(ˆy|x; θ) + δ, ... , yEN ∂θ1 C∂ log P(ˆy|x; θ) l2 # + δ , ∂θK and IU(θ) using: XU 1 IU(θ) = U u=1 For CRFs, the partial derivative at the root of each element in the diagonal vector is given by: ∂ log P(ˆy|x; θ) ∂θk P(y, y�|x)fk(y, y&apos;, xt), which is similar to the equation used to compute the training gradient, but without a regularization term. A smoothing parameter δ « 1 is added to prevent division by zero when computing the ratio. Notice that this method implicitly selects representative instances by favoring queries with Fis</context>
</contexts>
<marker>Nyffenegger, Chappelier, Gaussier, 2006</marker>
<rawString>M. Nyffenegger, J.C. Chappelier, and E. Gaussier. 2006. Revisiting Fisher kernels for document similarities. In Proceedings of the European Conference on Machine Learning (ECML), pages 727–734. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>A McCallum</author>
</authors>
<title>Accurate information extraction from research papers using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of Human Language Technology and the North American Association for Computational Linguistics (HLT-NAACL).</booktitle>
<publisher>ACL Press.</publisher>
<contexts>
<context position="23868" citStr="Peng and McCallum, 2004" startWordPosition="4032" endWordPosition="4035">ent segmentation corpora. The data sets are summarized in Table 1. 4.1 Data and Methodology CoNLL-03 (Sang and DeMeulder, 2003) is a collection of newswire articles annotated with four entities: person, organization, location, and misc. NLPBA (Kim et al., 2004) is a large collection of biomedical abstracts annotated with five entities of interest, such as protein, RNA, and cell-type. BioCreative (Yeh et al., 2005) and FlySlip (Vlachos, 2007) also comprise texts in the biomedical domain, annotated for gene entity mentions in articles from the human and fruit fly literature, respectively. CORA (Peng and McCallum, 2004) consists of two collections: a set of research paper headers annotated for entities such as title, author, and institution; and a collection of references annotated with BibTeX fields such as journal, year, and publisher. The Sig+Reply corpus (Carvalho and Cohen, 2004) is a set of email messages annotated for signature and quoted reply line segments. SigIE is a subset of the signature blocks from Sig+Reply which we have enhanced with several address book fields such as name, email, and phone. All corpora are formatted in the “IOB” sequence representation (Ramshaw and Marcus, 1995). We impleme</context>
</contexts>
<marker>Peng, McCallum, 2004</marker>
<rawString>F. Peng and A. McCallum. 2004. Accurate information extraction from research papers using conditional random fields. In Proceedings of Human Language Technology and the North American Association for Computational Linguistics (HLT-NAACL). ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="7277" citStr="Rabiner, 1989" startWordPosition="1196" endWordPosition="1197">ences in the labeled data set L: `(L; θ) = XL log P(y(l)Jx(l); θ) − XK θ2 l=1 k=1 k 2σ2, where L is the size of the labeled set L, and the second term is a Gaussian regularization penalty on 11θ11 to prevent over-fitting. After training, labels can be predicted for new sequences using the Viterbi algorithm. For more details on CRFs and their training procedures, see Sutton and McCallum (2006). Note that, while we describe the active learning algorithms in the next section in terms of linearchain CRFs, they have analogs for other kinds of sequence models, such as hidden Markov models, or HMMs (Rabiner, 1989), probabilistic contextfree grammars (Lari and Young, 1990), and general CRFs (Sutton and McCallum, 2006). 3 Active Learning with Sequence Models In order to select queries, an active learner must have a way of assessing how informative each instance is. Let x* be the most informative instance according to some query strategy φ(x), which is a function used to evaluate each instance x in the unlabeled pool U. ACME Inc. org offices org null the null null loc null org in Chicago loc ... 1071 Given: Labeled set L, unlabeled pool U, query strategy φ(·), query batch size B repeat // learn a model us</context>
<context position="10617" citStr="Rabiner, 1989" startWordPosition="1779" endWordPosition="1780">ought of as a measure of uncertainty in machine learning. In active learning, we wish to use the entropy of our model’s posteriors over its labelings. One way this has been done with probabilistic sequence models is by computing what we call token entropy (TE): Pθ(yt = m) log Pθ(yt = m), (2) where T is the length of x, m ranges over all possible token labels, and PB(yt = m) is shorthand for the marginal probability that m is the label at position t in the sequence, according to the model. For CRFs and HMMs, these marginals can be efficiently computed using the forward and backward algorithms (Rabiner, 1989). The summed token entropies have typically been normalized by sequence length T, to avoid simply querying longer sequences (Baldridge and Osborne, 2004; Hwa, 2004). However, we argue that querying long sequences should not be explicitly discouraged, if in fact they contain more information. Thus, we also propose the total token entropy (TTE) measure: φTTE(x) = T X φTE(x). For most sequence labeling tasks, however, it is more appropriate to consider the entropy of the label sequence y as a whole, rather than some aggregate of individual token entropies. Thus an alternate query strategy is sequ</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACL Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="24456" citStr="Ramshaw and Marcus, 1995" startWordPosition="4127" endWordPosition="4130">ively. CORA (Peng and McCallum, 2004) consists of two collections: a set of research paper headers annotated for entities such as title, author, and institution; and a collection of references annotated with BibTeX fields such as journal, year, and publisher. The Sig+Reply corpus (Carvalho and Cohen, 2004) is a set of email messages annotated for signature and quoted reply line segments. SigIE is a subset of the signature blocks from Sig+Reply which we have enhanced with several address book fields such as name, email, and phone. All corpora are formatted in the “IOB” sequence representation (Ramshaw and Marcus, 1995). We implement all fifteen query selection strategies described in Section 3 for use with CRFs, and evaluate them on all eight data sets. We also compare against two baseline strategies: random instance selection (i.e., passive learning), and naively querying the longest sequence in terms of tokens. We use a typical feature set for each corpus based on the cited literature (including words, orthographic patterns, part-of-speech, lexicons, etc.). Where the N-best approximation is used N = 15, and for all QBC methods C = 3; these figures exhibited a good balance of accuracy and training speed in</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L.A. Ramshaw and M.P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the ACL Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Roy</author>
<author>A McCallum</author>
</authors>
<title>Toward optimal active learning through sampling estimation of error reduction.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>441--448</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="3174" citStr="Roy and McCallum, 2001" startWordPosition="495" endWordPosition="498">ly on the conventions of uncertainty sampling, where the learner queries the instance about which it has the least certainty (Scheffer et al., 2001; Culotta and McCallum, 2005; Kim et al., 2006), or query-by-committee, where a “committee” of models selects the instance about which its members most disagree (Dagan and Engelson, 1995). We provide more detail on these and the new strategies we propose in Section 3. The comparative effectiveness of these approaches, however, has not been studied. Furthermore, it has been suggested that uncertainty sampling and query-by-committee fail on occasion (Roy and McCallum, 2001; Zhu et al., 2003) by querying outliers, e.g., instances considered informative in isolation by the learner, but containing little information about the rest of the distribution of instances. Proposed methods for dealing with these shortcomings have so far only considered classification tasks. 1070 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1070–1079, Honolulu, October 2008. c�2008 Association for Computational Linguistics This paper presents two major contributions for active learning and sequence labeling tasks. First, we motivate and intro</context>
<context position="16934" citStr="Roy and McCallum, 2001" startWordPosition="2861" endWordPosition="2865">· 11 is the Euclidean norm of each resulting gradient vector. We first introduced this approach in previous work on multiple-instance active learning (Settles et al., 2008), and adapt it to query selection with sequences here. Note that, at query time, V`(L; θ) should be nearly zero since ` converged at the previous round of training. Thus, we can approximate V`(L+(x,ˆy); θ) Pz� V`((x, y); θ) for computational efficiency, because the training instances are assumed to be independent. 3.4 Information Density It has been suggested that uncertainty sampling and QBC are prone to querying outliers (Roy and McCallum, 2001; Zhu et al., 2003). Figure 2 illustrates this problem for a binary linear classifier using uncertainty sampling. The least certain instance lies on the classification boundary, but is not “representative” of other instances in the distribution, so knowing its label is unlikely to improve accuracy on the data as a whole. QBC and EGL exhibit similar behavior, by spending time querying possible outliers simply because they are controversial, or are expected to impart significant change in the model. Figure 2: An illustration of when uncertainty sampling can be a poor strategy for classification.</context>
</contexts>
<marker>Roy, McCallum, 2001</marker>
<rawString>N. Roy and A. McCallum. 2001. Toward optimal active learning through sampling estimation of error reduction. In Proceedings of the International Conference on Machine Learning (ICML), pages 441–448. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F T K Sang</author>
<author>F DeMeulder</author>
</authors>
<title>Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning (CoNLL),</booktitle>
<pages>142--147</pages>
<contexts>
<context position="1872" citStr="Sang and DeMeulder, 2003" startWordPosition="287" endWordPosition="290"> selects one or more informative query instances from a large unlabeled pool U, learns from these labeled queries (which are then added to L), and repeats. In this way, the learner aims to achieve high accuracy with as little labeling effort as possible. Thus, active learning can be valuable in domains where unlabeled data are readily available, but obtaining training labels is expensive. Such is the case with many sequence labeling tasks in natural language domains. For example, part-of-speech tagging (Seung et al., 1992; Lafferty et al., 2001), information extraction (Scheffer et al., 2001; Sang and DeMeulder, 2003; Kim et al., 2004), and document segmentation (Carvalho and Cohen, 2004) are all typically treated as sequence labeling problems. The source data for these tasks (i.e., text documents in electronic form) are often easily obtained. However, due to the nature of sequence labeling tasks, annotating these texts can be rather tedious and time-consuming, making active learning an attractive technique. While there has been much work on active learning for classification (Cohn et al., 1994; McCallum and Nigam, 1998; Zhang and Oles, 2000; Zhu et al., 2003), active learning for sequence labeling has re</context>
<context position="23371" citStr="Sang and DeMeulder, 2003" startWordPosition="3952" endWordPosition="3955">T X t=1 − T X t=1 X y,y&apos; 1075 Corpus Entities Features Instances CoNLL-03 4 78,644 19,959 NLPBA 5 128,401 18,854 BioCreative 1 175,331 10,000 FlySlip 1 31,353 1,220 CORA:Headers 15 22,077 935 CORA:References 13 4,208 500 Sig+Reply 2 25 617 SigIE 12 10,600 250 Table 1: Properties of the different evaluation corpora. 4 Empirical Evaluation In this section we present a large-scale empirical analysis of the query strategies described in Section 3 on eight benchmark information extraction and document segmentation corpora. The data sets are summarized in Table 1. 4.1 Data and Methodology CoNLL-03 (Sang and DeMeulder, 2003) is a collection of newswire articles annotated with four entities: person, organization, location, and misc. NLPBA (Kim et al., 2004) is a large collection of biomedical abstracts annotated with five entities of interest, such as protein, RNA, and cell-type. BioCreative (Yeh et al., 2005) and FlySlip (Vlachos, 2007) also comprise texts in the biomedical domain, annotated for gene entity mentions in articles from the human and fruit fly literature, respectively. CORA (Peng and McCallum, 2004) consists of two collections: a set of research paper headers annotated for entities such as title, aut</context>
</contexts>
<marker>Sang, DeMeulder, 2003</marker>
<rawString>E.F.T.K. Sang and F. DeMeulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Conference on Natural Language Learning (CoNLL), pages 142–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Scheffer</author>
<author>C Decomain</author>
<author>S Wrobel</author>
</authors>
<title>Active hidden Markov models for information extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Advances in Intelligent Data Analysis (CAIDA),</booktitle>
<pages>309--318</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1846" citStr="Scheffer et al., 2001" startWordPosition="283" endWordPosition="286"> a small labeled set L, selects one or more informative query instances from a large unlabeled pool U, learns from these labeled queries (which are then added to L), and repeats. In this way, the learner aims to achieve high accuracy with as little labeling effort as possible. Thus, active learning can be valuable in domains where unlabeled data are readily available, but obtaining training labels is expensive. Such is the case with many sequence labeling tasks in natural language domains. For example, part-of-speech tagging (Seung et al., 1992; Lafferty et al., 2001), information extraction (Scheffer et al., 2001; Sang and DeMeulder, 2003; Kim et al., 2004), and document segmentation (Carvalho and Cohen, 2004) are all typically treated as sequence labeling problems. The source data for these tasks (i.e., text documents in electronic form) are often easily obtained. However, due to the nature of sequence labeling tasks, annotating these texts can be rather tedious and time-consuming, making active learning an attractive technique. While there has been much work on active learning for classification (Cohn et al., 1994; McCallum and Nigam, 1998; Zhang and Oles, 2000; Zhu et al., 2003), active learning fo</context>
<context position="9184" citStr="Scheffer et al. (2001)" startWordPosition="1524" endWordPosition="1527">frameworks for measuring informativeness is uncertainty sampling (Lewis and Catlett, 1994), where a learner queries the instance that it is most uncertain how to label. Culotta and McCallum (2005) employ a simple uncertainty-based strategy for sequence models called least confidence (LC): φLC(x) = 1 − P(y*|x; θ). Here, y* is the most likely label sequence, i.e., the Viterbi parse. This approach queries the instance for which the current model has the least confidence in its most likely labeling. For CRFs, this confidence can be calculated using the posterior probability given by Equation (1). Scheffer et al. (2001) propose another uncertainty strategy, which queries the instance with the smallest margin between the posteriors for its two most likely labelings. We call this approach margin (M): φM(x) = −(P(y*�|x; θ) − P(y*�|x; θ)). Here, y*1 and y*2 are the first and second best label sequences, respectively. These can be efficiently computed using the N-best algorithm (Schwartz and Chow, 1990), a beam-search generalization of Viterbi, with N = 2. The minus sign in front is simply to ensure that φM acts as a maximizer for use with Algorithm 1. Another uncertainty-based measure of informativeness is entro</context>
</contexts>
<marker>Scheffer, Decomain, Wrobel, 2001</marker>
<rawString>T. Scheffer, C. Decomain, and S. Wrobel. 2001. Active hidden Markov models for information extraction. In Proceedings of the International Conference on Advances in Intelligent Data Analysis (CAIDA), pages 309–318. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schwartz</author>
<author>Y-L Chow</author>
</authors>
<title>The N-best algorithm: an efficient and exact procedure for finding the N most likely sentence hypotheses.</title>
<date>1990</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>81--83</pages>
<publisher>IEEE Press.</publisher>
<contexts>
<context position="9570" citStr="Schwartz and Chow, 1990" startWordPosition="1585" endWordPosition="1588">This approach queries the instance for which the current model has the least confidence in its most likely labeling. For CRFs, this confidence can be calculated using the posterior probability given by Equation (1). Scheffer et al. (2001) propose another uncertainty strategy, which queries the instance with the smallest margin between the posteriors for its two most likely labelings. We call this approach margin (M): φM(x) = −(P(y*�|x; θ) − P(y*�|x; θ)). Here, y*1 and y*2 are the first and second best label sequences, respectively. These can be efficiently computed using the N-best algorithm (Schwartz and Chow, 1990), a beam-search generalization of Viterbi, with N = 2. The minus sign in front is simply to ensure that φM acts as a maximizer for use with Algorithm 1. Another uncertainty-based measure of informativeness is entropy (Shannon, 1948). For a discrete random variable Y , the entropy is given by H(Y ) = − Ez P(yz) log P(yz), and represents the information needed to “encode” the distribution of outcomes for Y . As such, is it often thought of as a measure of uncertainty in machine learning. In active learning, we wish to use the entropy of our model’s posteriors over its labelings. One way this has</context>
</contexts>
<marker>Schwartz, Chow, 1990</marker>
<rawString>R. Schwartz and Y.-L. Chow. 1990. The N-best algorithm: an efficient and exact procedure for finding the N most likely sentence hypotheses. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 81–83. IEEE Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
<author>M Craven</author>
<author>S Ray</author>
</authors>
<title>Multipleinstance active learning.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<volume>20</volume>
<pages>1289--1296</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16484" citStr="Settles et al., 2008" startWordPosition="2787" endWordPosition="2790">he loglikelihood ` with respect to the model parameters θ, as given by Sutton and McCallum (2006). Now let V`(L+(x,y); θ) be the new gradient that would be obtained by adding the training tuple (x, y) to L. Since the query algorithm does not know the true label sequence y in advance, we instead calculate the expected gradient length (EGL): � � � � P(�y|x; θ) �V`(L+(x,ˆy); θ) � , approximated as an expectation over the N-best labelings, where 11 · 11 is the Euclidean norm of each resulting gradient vector. We first introduced this approach in previous work on multiple-instance active learning (Settles et al., 2008), and adapt it to query selection with sequences here. Note that, at query time, V`(L; θ) should be nearly zero since ` converged at the previous round of training. Thus, we can approximate V`(L+(x,ˆy); θ) Pz� V`((x, y); θ) for computational efficiency, because the training instances are assumed to be independent. 3.4 Information Density It has been suggested that uncertainty sampling and QBC are prone to querying outliers (Roy and McCallum, 2001; Zhu et al., 2003). Figure 2 illustrates this problem for a binary linear classifier using uncertainty sampling. The least certain instance lies on t</context>
</contexts>
<marker>Settles, Craven, Ray, 2008</marker>
<rawString>B. Settles, M. Craven, and S. Ray. 2008. Multipleinstance active learning. In Advances in Neural Information Processing Systems (NIPS), volume 20, pages 1289–1296. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Seung</author>
<author>M Opper</author>
<author>H Sompolinsky</author>
</authors>
<title>Query by committee.</title>
<date>1992</date>
<booktitle>In Proceedings of the ACM Workshop on Computational Learning Theory,</booktitle>
<pages>287--294</pages>
<contexts>
<context position="1775" citStr="Seung et al., 1992" startWordPosition="273" endWordPosition="276"> and added to the training set. A typical active learner begins with a small labeled set L, selects one or more informative query instances from a large unlabeled pool U, learns from these labeled queries (which are then added to L), and repeats. In this way, the learner aims to achieve high accuracy with as little labeling effort as possible. Thus, active learning can be valuable in domains where unlabeled data are readily available, but obtaining training labels is expensive. Such is the case with many sequence labeling tasks in natural language domains. For example, part-of-speech tagging (Seung et al., 1992; Lafferty et al., 2001), information extraction (Scheffer et al., 2001; Sang and DeMeulder, 2003; Kim et al., 2004), and document segmentation (Carvalho and Cohen, 2004) are all typically treated as sequence labeling problems. The source data for these tasks (i.e., text documents in electronic form) are often easily obtained. However, due to the nature of sequence labeling tasks, annotating these texts can be rather tedious and time-consuming, making active learning an attractive technique. While there has been much work on active learning for classification (Cohn et al., 1994; McCallum and N</context>
<context position="14351" citStr="Seung et al., 1992" startWordPosition="2412" endWordPosition="2415"> to φM, thus N-best sequence entropy can be thought of as a generalization of the margin approach. Recently, an efficient entropy calculation via dynamic programming was proposed for CRFs in the context of semi-supervised learning (Mann and McCallum, 2007). We use this algorithm to compute the true sequence entropy (3) for active learning in a constant-time factor of Viterbi’s complexity. Hwa (2004) employed a similar approach for active learning with probabilistic context-free grammars. 3.2 Query-By-Committee Another general active learning framework is the query-by-committee (QBC) approach (Seung et al., 1992). In this setting, we use a committee of models C = {θ(1),...,θ(C)} to represent C different hypotheses that are consistent with the labeled set L. The most informative query, then, is the instance over which the committee is in most disagreement about how to label. In particular, we use the query-by-bagging approach (Abe and Mamitsuka, 1998) to learn a committee of CRFs. In each round of active learning, L is sampled (with replacement) L times to create a unique, modified labeled set L(c). Each model θ(c) ∈ C is then trained using its own corresponding labeled set L(c). To measure disagreemen</context>
</contexts>
<marker>Seung, Opper, Sompolinsky, 1992</marker>
<rawString>H.S. Seung, M. Opper, and H. Sompolinsky. 1992. Query by committee. In Proceedings of the ACM Workshop on Computational Learning Theory, pages 287–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<volume>27</volume>
<pages>423--623</pages>
<contexts>
<context position="9802" citStr="Shannon, 1948" startWordPosition="1627" endWordPosition="1628">pose another uncertainty strategy, which queries the instance with the smallest margin between the posteriors for its two most likely labelings. We call this approach margin (M): φM(x) = −(P(y*�|x; θ) − P(y*�|x; θ)). Here, y*1 and y*2 are the first and second best label sequences, respectively. These can be efficiently computed using the N-best algorithm (Schwartz and Chow, 1990), a beam-search generalization of Viterbi, with N = 2. The minus sign in front is simply to ensure that φM acts as a maximizer for use with Algorithm 1. Another uncertainty-based measure of informativeness is entropy (Shannon, 1948). For a discrete random variable Y , the entropy is given by H(Y ) = − Ez P(yz) log P(yz), and represents the information needed to “encode” the distribution of outcomes for Y . As such, is it often thought of as a measure of uncertainty in machine learning. In active learning, we wish to use the entropy of our model’s posteriors over its labelings. One way this has been done with probabilistic sequence models is by computing what we call token entropy (TE): Pθ(yt = m) log Pθ(yt = m), (2) where T is the length of x, m ranges over all possible token labels, and PB(yt = m) is shorthand for the m</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>C. E. Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal, 27:379– 423,623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
</authors>
<title>An introduction to conditional random fields for relational learning.</title>
<date>2006</date>
<editor>In L. Getoor and B. Taskar, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7058" citStr="Sutton and McCallum (2006)" startWordPosition="1157" endWordPosition="1160">k = 1 along a transition from the null state to the org state (and 0 elsewhere). Other features set to 1 here might be ALLCAPS and NEXTWORD=Inc. The weights in θ are set to maximize the conditional log likelihood ` of training sequences in the labeled data set L: `(L; θ) = XL log P(y(l)Jx(l); θ) − XK θ2 l=1 k=1 k 2σ2, where L is the size of the labeled set L, and the second term is a Gaussian regularization penalty on 11θ11 to prevent over-fitting. After training, labels can be predicted for new sequences using the Viterbi algorithm. For more details on CRFs and their training procedures, see Sutton and McCallum (2006). Note that, while we describe the active learning algorithms in the next section in terms of linearchain CRFs, they have analogs for other kinds of sequence models, such as hidden Markov models, or HMMs (Rabiner, 1989), probabilistic contextfree grammars (Lari and Young, 1990), and general CRFs (Sutton and McCallum, 2006). 3 Active Learning with Sequence Models In order to select queries, an active learner must have a way of assessing how informative each instance is. Let x* be the most informative instance according to some query strategy φ(x), which is a function used to evaluate each insta</context>
<context position="15960" citStr="Sutton and McCallum (2006)" startWordPosition="2694" endWordPosition="2697">ted Gradient Length A third general active learning framework we consider is to query the instance that would impart the greatest change to the current model if we knew its label. Since we train discriminative models like CRFs using gradient-based optimization, this involves querying the instance which, if labeled and added to the training set, would create the greatest change in the gradient of the objective function (i.e., the largest gradient vector used to re-estimate parameter values). Let V`(L; θ) be the gradient of the loglikelihood ` with respect to the model parameters θ, as given by Sutton and McCallum (2006). Now let V`(L+(x,y); θ) be the new gradient that would be obtained by adding the training tuple (x, y) to L. Since the query algorithm does not know the true label sequence y in advance, we instead calculate the expected gradient length (EGL): � � � � P(�y|x; θ) �V`(L+(x,ˆy); θ) � , approximated as an expectation over the N-best labelings, where 11 · 11 is the Euclidean norm of each resulting gradient vector. We first introduced this approach in previous work on multiple-instance active learning (Settles et al., 2008), and adapt it to query selection with sequences here. Note that, at query t</context>
</contexts>
<marker>Sutton, McCallum, 2006</marker>
<rawString>C. Sutton and A. McCallum. 2006. An introduction to conditional random fields for relational learning. In L. Getoor and B. Taskar, editors, Introduction to Statistical Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vlachos</author>
</authors>
<title>Evaluating and combining biomedical named entity recognition systems.</title>
<date>2007</date>
<journal>In BioNLP</journal>
<pages>199--206</pages>
<contexts>
<context position="23689" citStr="Vlachos, 2007" startWordPosition="4004" endWordPosition="4006">Evaluation In this section we present a large-scale empirical analysis of the query strategies described in Section 3 on eight benchmark information extraction and document segmentation corpora. The data sets are summarized in Table 1. 4.1 Data and Methodology CoNLL-03 (Sang and DeMeulder, 2003) is a collection of newswire articles annotated with four entities: person, organization, location, and misc. NLPBA (Kim et al., 2004) is a large collection of biomedical abstracts annotated with five entities of interest, such as protein, RNA, and cell-type. BioCreative (Yeh et al., 2005) and FlySlip (Vlachos, 2007) also comprise texts in the biomedical domain, annotated for gene entity mentions in articles from the human and fruit fly literature, respectively. CORA (Peng and McCallum, 2004) consists of two collections: a set of research paper headers annotated for entities such as title, author, and institution; and a collection of references annotated with BibTeX fields such as journal, year, and publisher. The Sig+Reply corpus (Carvalho and Cohen, 2004) is a set of email messages annotated for signature and quoted reply line segments. SigIE is a subset of the signature blocks from Sig+Reply which we h</context>
</contexts>
<marker>Vlachos, 2007</marker>
<rawString>A. Vlachos. 2007. Evaluating and combining biomedical named entity recognition systems. In BioNLP 2007: Biological, translational, and clinical language processing, pages 199–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yeh</author>
<author>A Morgan</author>
<author>M Colosimo</author>
<author>L Hirschman</author>
</authors>
<title>Biocreative task 1a: gene mention finding evaluation.</title>
<date>2005</date>
<journal>BMC Bioinformatics,</journal>
<volume>6</volume>
<pages>1--2</pages>
<contexts>
<context position="23661" citStr="Yeh et al., 2005" startWordPosition="3998" endWordPosition="4001">valuation corpora. 4 Empirical Evaluation In this section we present a large-scale empirical analysis of the query strategies described in Section 3 on eight benchmark information extraction and document segmentation corpora. The data sets are summarized in Table 1. 4.1 Data and Methodology CoNLL-03 (Sang and DeMeulder, 2003) is a collection of newswire articles annotated with four entities: person, organization, location, and misc. NLPBA (Kim et al., 2004) is a large collection of biomedical abstracts annotated with five entities of interest, such as protein, RNA, and cell-type. BioCreative (Yeh et al., 2005) and FlySlip (Vlachos, 2007) also comprise texts in the biomedical domain, annotated for gene entity mentions in articles from the human and fruit fly literature, respectively. CORA (Peng and McCallum, 2004) consists of two collections: a set of research paper headers annotated for entities such as title, author, and institution; and a collection of references annotated with BibTeX fields such as journal, year, and publisher. The Sig+Reply corpus (Carvalho and Cohen, 2004) is a set of email messages annotated for signature and quoted reply line segments. SigIE is a subset of the signature bloc</context>
</contexts>
<marker>Yeh, Morgan, Colosimo, Hirschman, 2005</marker>
<rawString>A. Yeh, A. Morgan, M. Colosimo, and L. Hirschman. 2005. Biocreative task 1a: gene mention finding evaluation. BMC Bioinformatics, 6(Suppl 1):S2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
<author>F J Oles</author>
</authors>
<title>A probability analysis on the value of unlabeled data for classification problems.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>1191--1198</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="2407" citStr="Zhang and Oles, 2000" startWordPosition="373" endWordPosition="376">t al., 2001), information extraction (Scheffer et al., 2001; Sang and DeMeulder, 2003; Kim et al., 2004), and document segmentation (Carvalho and Cohen, 2004) are all typically treated as sequence labeling problems. The source data for these tasks (i.e., text documents in electronic form) are often easily obtained. However, due to the nature of sequence labeling tasks, annotating these texts can be rather tedious and time-consuming, making active learning an attractive technique. While there has been much work on active learning for classification (Cohn et al., 1994; McCallum and Nigam, 1998; Zhang and Oles, 2000; Zhu et al., 2003), active learning for sequence labeling has received considerably less attention. A few methods have been proposed, based mostly on the conventions of uncertainty sampling, where the learner queries the instance about which it has the least certainty (Scheffer et al., 2001; Culotta and McCallum, 2005; Kim et al., 2006), or query-by-committee, where a “committee” of models selects the instance about which its members most disagree (Dagan and Engelson, 1995). We provide more detail on these and the new strategies we propose in Section 3. The comparative effectiveness of these </context>
<context position="20426" citStr="Zhang and Oles (2000)" startWordPosition="3446" endWordPosition="3449">atically with the number of instances in U. For pool-based active learning, we often assume that the size of U is very large. However, these densities only need to be computed once, and are independent of the base information measure. Thus, when employing information density in a realworld interactive learning setting, the density scores can simply be pre-computed and cached for efficient lookup during the actual active learning process. 3.5 Fisher Information We also introduce a query selection strategy for sequence models based on Fisher information, building on the theoretical framework of Zhang and Oles (2000). Fisher information I(θ) represents the overall uncertainty about the estimated model parameters θ, as given by: Z Z P(y|x; θ) ∂2 I(θ) = − P(x) ∂θ2 log P(y|x; θ). x y For a model with K parameters, the Fisher information takes the form of a K × K covariance matrix. Our goal in active learning is to select the query that most efficiently minimizes the model variance reflected in I(θ). This can be accomplished by optimizing the Fisher information ratio (FIR): φFIR(x) = −tr (Ix(θ)−1IU(θ)) , (4) where Ix(θ) and IU(θ) are Fisher information matrices for sequence x and the unlabeled pool U, respect</context>
</contexts>
<marker>Zhang, Oles, 2000</marker>
<rawString>T. Zhang and F.J. Oles. 2000. A probability analysis on the value of unlabeled data for classification problems. In Proceedings of the International Conference on Machine Learning (ICML), pages 1191–1198. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>J Lafferty</author>
<author>Z Ghahramani</author>
</authors>
<title>Combining active learning and semi-supervised learning using gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceedings of the ICML Workshop on the Continuum from Labeled to Unlabeled Data,</booktitle>
<pages>58--65</pages>
<contexts>
<context position="2426" citStr="Zhu et al., 2003" startWordPosition="377" endWordPosition="380">ion extraction (Scheffer et al., 2001; Sang and DeMeulder, 2003; Kim et al., 2004), and document segmentation (Carvalho and Cohen, 2004) are all typically treated as sequence labeling problems. The source data for these tasks (i.e., text documents in electronic form) are often easily obtained. However, due to the nature of sequence labeling tasks, annotating these texts can be rather tedious and time-consuming, making active learning an attractive technique. While there has been much work on active learning for classification (Cohn et al., 1994; McCallum and Nigam, 1998; Zhang and Oles, 2000; Zhu et al., 2003), active learning for sequence labeling has received considerably less attention. A few methods have been proposed, based mostly on the conventions of uncertainty sampling, where the learner queries the instance about which it has the least certainty (Scheffer et al., 2001; Culotta and McCallum, 2005; Kim et al., 2006), or query-by-committee, where a “committee” of models selects the instance about which its members most disagree (Dagan and Engelson, 1995). We provide more detail on these and the new strategies we propose in Section 3. The comparative effectiveness of these approaches, however</context>
<context position="16953" citStr="Zhu et al., 2003" startWordPosition="2866" endWordPosition="2869">rm of each resulting gradient vector. We first introduced this approach in previous work on multiple-instance active learning (Settles et al., 2008), and adapt it to query selection with sequences here. Note that, at query time, V`(L; θ) should be nearly zero since ` converged at the previous round of training. Thus, we can approximate V`(L+(x,ˆy); θ) Pz� V`((x, y); θ) for computational efficiency, because the training instances are assumed to be independent. 3.4 Information Density It has been suggested that uncertainty sampling and QBC are prone to querying outliers (Roy and McCallum, 2001; Zhu et al., 2003). Figure 2 illustrates this problem for a binary linear classifier using uncertainty sampling. The least certain instance lies on the classification boundary, but is not “representative” of other instances in the distribution, so knowing its label is unlikely to improve accuracy on the data as a whole. QBC and EGL exhibit similar behavior, by spending time querying possible outliers simply because they are controversial, or are expected to impart significant change in the model. Figure 2: An illustration of when uncertainty sampling can be a poor strategy for classification. Shaded polygons re</context>
</contexts>
<marker>Zhu, Lafferty, Ghahramani, 2003</marker>
<rawString>X. Zhu, J. Lafferty, and Z. Ghahramani. 2003. Combining active learning and semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the ICML Workshop on the Continuum from Labeled to Unlabeled Data, pages 58–65.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>