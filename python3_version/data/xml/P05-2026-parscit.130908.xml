<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005082">
<title confidence="0.996625">
A Domain-Specific Statistical Surface Realizer
</title>
<author confidence="0.995547">
Jeffrey T. Russell
</author>
<affiliation confidence="0.9787365">
Center for the Study of Language and Information
Stanford University
</affiliation>
<email confidence="0.999182">
jefe@stanford.edu
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929846153846">
We present a search-based approach to au-
tomatic surface realization given a cor-
pus of domain sentences. Using heuris-
tic search based on a statistical language
model and a structure we introduce called
an inheritance table we overgenerate a
set of complete syntactic-semantic trees
that are consistent with the given seman-
tic structure and have high likelihood rela-
tive to the language model. These trees are
then lexicalized, linearized, scored, and
ranked. This model is being developed to
generate real-time navigation instructions.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999731117647059">
The target application for this work is real-time, in-
teractive navigation instructions. Good direction-
givers respond actively to a driver’s actions and
questions, and express instructions relative to a large
variety of landmarks, times, and distances. These
traits require robust, real-time natural language gen-
eration. This can be broken into three steps: (1) gen-
erating a route plan, (2) reasoning about the route
and the user to produce an abstract representation
of individual instructions, and (3) realizing these in-
structions as sentences in natural language (in our
case, English). We focus on the last of these steps:
given a structure that represents the semantic content
of a sentence, we want to produce an English sen-
tence that expresses this content. According to the
traditional division of content determination, sen-
tence planning, and surface realization, our work
</bodyText>
<page confidence="0.980281">
151
</page>
<bodyText confidence="0.999950970588235">
is primarily concerned with surface realization, but
also includes aspects of sentence planning. Our
application requires robust flexibility within a re-
stricted domain that is not well represented in the
traditional corpora or tools. These requirements sug-
gest using trainable stochastic generation.
A number of statistical surface realizers have been
described, notably the FERGUS (Bangalore and
Rambow, 2000) and HALogen systems (Langkilde-
Geary, 2002), as well as experiments in (Rat-
naparkhi, 2000). FERGUS (Flexible Empiri-
cist/Rationalist Generation Using Syntax) takes as
input a dependency tree whose nodes are marked
with lexemes only. The generator automatically “su-
pertags” each input node with a TAG tree, then pro-
duces a lattice of all possible linearizations consis-
tent with the supertagged dependency tree. Finally
it selects the most likely traversal of this lattice,
conditioned on a domain-trained language model.
The HALogen system is a broad-coverage genera-
tor that uses a combination of statistical and sym-
bolic techniques. The input, a structure of feature-
value pairs (see Section 3.1), is symbolically trans-
formed into a forest of possible expressions, which
are then ranked using a corpus-trained statistical lan-
guage model. Ratnaparkhi also uses an overgener-
ation approach, using search to generate candidate
sentences which are then scored and ranked. His
paper outlines experiments with an n-gram model,
a trained dependency grammar, and finally a hand-
built grammar including content-driven conditions
for applying rules. The last of these systems outper-
formed the n-gram and trained grammar in testing
based on human judgments.
</bodyText>
<note confidence="0.386808">
Proceedings of the ACL Student Research Workshop, pages 151–156,
</note>
<page confidence="0.39442">
Ann Arbor, Michigan, June 2005. c�2005 Association for Computational Linguistics
</page>
<bodyText confidence="0.9999809">
The basic idea of our system fits in the
overgenerate-and-rank paradigm. Our approach is
partly motivated by the idea of ‘softening’ Ratna-
parkhi’s third system, replacing the hand-built gram-
mar rules with a combination of a trained statistical
language model and a structure called an inheritance
table, which captures long-run dependency informa-
tion. This allows us to overgenerate based on rules
that are sensitive to structured content without incur-
ring the cost of designing such rules by hand.
</bodyText>
<sectionHeader confidence="0.989947" genericHeader="introduction">
2 Algorithm
</sectionHeader>
<bodyText confidence="0.999981714285714">
We use dependency tree representations for both the
semantics and syntax of a sentence; we introduce
the syntactic-semantic (SS) tree to combine infor-
mation from both of these structures. An SS tree
is constructed by “attaching” some of the nodes of a
sentence’s semantic tree to the nodes of its syntactic
tree, obeying two rules:
</bodyText>
<listItem confidence="0.999330333333333">
• Each node in the semantic tree is attached to
at most one node of the syntactic tree.
• Semantic and syntactic hierarchical order-
</listItem>
<bodyText confidence="0.971234428571428">
ings are consistent. That is to say, if two se-
mantic nodes x1 and x2 are attached to two syn-
tactic nodes y1 and y2, respectively, then x1 is
a descendant of x2 in the semantic tree if and
only if y1 is a descendant of y2 in the syntactic
tree.
The nodes of an SS tree are either unattached se-
mantic or syntactic nodes, or else pairs of attached
nodes. The SS tree’s hierarchy is consistent with the
hierarchies in the syntactic and semantic trees. We
say that an SS tree T satisfies a semantic structure
S if S is embedded in T. This serves as formaliza-
tion of the idea of a sentence expressing a certain
content.
</bodyText>
<subsectionHeader confidence="0.557082">
2.1 Outline
</subsectionHeader>
<bodyText confidence="0.9986925">
The core of our method is a heuristic search of the
space of possible SS trees. Our search goal is to find
the N best complete SS trees that express the given
semantic structure. We take ‘best’ here to be the
trees which have the highest conditional likelihood
given that they express the right semantic structure.
If S is our semantic structure and LM is our statis-
tical language model, we want to find syntactic trees
T that maximize PLM(T|S).
In order to search the space of trees, we build
up trees by expanding one node at a time. During
the search, then, we deal with incomplete trees; that
is, trees with some nodes not fully expanded. This
means that we need a way to determine how promis-
ing an incomplete tree T is: i.e., how good the best
complete trees are that can be built up by expanding
T. As it turns out (Section 2.2), we can efficiently
approximate the function 1 PLM (T |S) for an incom-
plete tree, and this function is a good heuristic for
the maximum likelihood of a complete tree extended
from T.
Here is an outline of the algorithm:
</bodyText>
<listItem confidence="0.985510090909091">
• Start with a root tree.
– Take the top N trees and expand one node
in each.
– Score each expanded tree for PLM (T |S),
and put in the search order accordingly.
– Repeat until we find enough trees that sat-
isfy S.
• Complete the trees.
• Linearize and lexicalize the trees.
• Rank the complete trees according to some
scoring function.
</listItem>
<subsectionHeader confidence="0.991337">
2.2 Heuristic
</subsectionHeader>
<bodyText confidence="0.99994">
Our search goal is to maximize PLM (T |S). (Hence-
forth we abbreviate PLM as just P.) Ideally,
then, we would at each step expand the incomplete
tree that can be extended to the highest-likelihood
complete tree, i.e. that has the highest value of
maxT&apos; P(T&apos;|S) over all complete trees T&apos; that ex-
tend T. We use the notation T&apos; &gt; T when T&apos; is a
complete tree that extends an incomplete tree T, and
the notation T&apos; &gt; S when T&apos; satisfies S. Then the
“goodness” of a tree T is given by
</bodyText>
<equation confidence="0.801279857142857">
P(T�|S) = max
T&apos;&gt;T;T&apos;DS
1This probability is defined to be the sum of the probabilities
PLM (TIT)PLM (T�|S) for all complete trees T&apos;
max
T&apos;&gt;T
P(TWP(S) (1)
</equation>
<page confidence="0.984222">
152
</page>
<bodyText confidence="0.999293285714286">
Since finding this maximum explicitly is not fea-
sible, we use the heuristic P(T|S). By Bayes’ rule,
P(T|S) = P(S|T)P(T)/P(S), where P(S) is a
normalizing factor, P(T) can be easily calculated
using the language model (as the product of the
probabilities of the node expansions that appear in
T), and
</bodyText>
<equation confidence="0.9953991">
P(S|T) = � P(S|T�)P(T�|T) = � P(T�|T)
T&apos; T&apos;DS
Since P(T&apos;|T) = P(T|T&apos;)P(T&apos;)/P(T), and since
P(T |T&apos;) is 1 if T&apos; &gt; T and 0 otherwise, we have
1 P(T|S) = P(S) E P(T|T&apos;)P(T&apos;)
T
1P(T&apos;)
P(S) TI&gt;� &apos;oS
Together with Equation 1 this shows that
P(T|S) &gt; maxT,&gt;T P(T&apos;|S), since the maximum
</equation>
<bodyText confidence="0.999297236842105">
is one of the terms in the sum. This fact is analogous
to showing that P(T|S) is an admissible heuristic
(in the sense of A* search).
We can see how to calculate P(T |S) in practice
by decomposing the structure of a tree T~ such that
T&apos; &gt; T and T�&gt;S. Since T&apos; extends T, the top of T&apos;
is identical to T. The semantic tree S will have some
of its nodes in T, and some in the part of T~ that
extends beyond T. Let α(S, T) be the set containing
the highest nodes in S that are not in T. Each node
s E α(S, T) is the root node of a subtree in T~. Each
of these subtrees can be considered separately.
First we consider how the these subtrees are
joined to the nodes in T. The condition of consis-
tent ordering requires that each node in α(S,T) be
a descendant in T&apos; of its parent in S, and moreover
it should not be a descendant of any of its siblings
in S. Let sib be a set of siblings in α(S, T), and let
p be their semantic parent. Then p is the root node
of a subtree of T, called Tp. We will designate the
T-set of sib as the set of leaves of Tp that are not
descended from any nodes in S below p–in particu-
lar, that are not descended from any other siblings of
the nodes in sib. Then in T&apos; all of the nodes in sib
must descend from the T-set of sib. In other words,
there is a set of subtrees of T~ which are rooted at
the nodes in the T-set of sib, and all of the nodes in
sib appear in these subtrees such that none of them
are descended from each other.
This analysis sets us up to rewrite P(T|S) in
terms of sums over these various subtrees. We
use the notation P({x1, ..., xk} —* {y1, ..., yl})
to denote the probability that the nodes y1, ..., yl
eventually descend from x1, ..., xk without domi-
nating each other; this probability is the sum of
P(T1, ..., Tk) over all sets of trees T1 &gt; x1, ..., Tk &gt;
xk such that each node y1, ..., yl appears in some Ti
and no yi descends from any yj. Then we can rewrite
</bodyText>
<equation confidence="0.92292525">
P(T|S) as
P(T) fl �
P(T-set(sib) —* sib) P(x —* Sx)
P(S) sib x∈α(S,T)
</equation>
<bodyText confidence="0.955364384615385">
Sx denotes the subtree of S whose root node is x.
P(x —* Sx) is 1 if Sx contains only the node x, and
otherwise is
⎠
Rather than calculating the value of formula 2 ex-
actly, we now introduce an approximation to our
heuristic function. For sets X, Y , we approximate
P(X —* Y ) with 11y∈Y P(X —* y). This amounts
to two simplifications: first, we drop the restriction
that no node be descended from its semantic sib-
ling; second, we assume that the probabilities of
each node descending from X are independent from
one another.
</bodyText>
<equation confidence="0.973504833333333">
P(X —* y) is the probability that at least one
x E X has y as a descendant, i.e. P(X —* y) =
AL1x∈XP(x —* y), where AL1 is the ‘At-least-
one’ function.2 This means that we can approximate
P(T|S) as
AL1x∈T-set(y)P(x —* y)P(y —* Sy)
</equation>
<bodyText confidence="0.8013795">
2That is, given the probabilities of a set of events, the At-
least-one function gives the probability of at least one of the
events occuring. For independent events, AL1{} = 0 and
AL1{p1,..., p.} = p. + (1 − p.)AL1{p1,..., p.−1}.
</bodyText>
<equation confidence="0.99979175">
P(x —* childrenS(x)) H P(y —* Sy)
(Y∈childrenS (x)
P(T) �
P(S) y∈α(S,T)
</equation>
<page confidence="0.990649">
153
</page>
<bodyText confidence="0.998516909090909">
The calculation of P(T|S) has been reduced to
finding P(x → y) for individual nodes. These
values are retrieved from the inheritance table, de-
scribed below.
Note that when we expand a single node of an
incomplete tree, only a few factors in Equation 3
change. Rather than recalculating each tree’s score
from scratch, then, by caching intermediate results
we can recompute only the terms that change. This
allows for efficient calculation of the heuristic func-
tion.
</bodyText>
<subsectionHeader confidence="0.996497">
2.3 Inheritance Table
</subsectionHeader>
<bodyText confidence="0.999931083333333">
The inheritance table (IT) allows us to predict the
potential descendants of an incomplete tree. For
each pair of SS nodes x and y, the IT stores P(x →
y), the probability that y will eventually appear as
a descendant of x. The IT is precomputed once
from the language model; the same IT is used for
all queries.
We can compute the IT using an iterative process.
Consider the transformation T that takes a distribu-
tion Q(x → y) to a new distribution T(Q) such that
T(Q)(x → y) is equal to 1 when x = y, and other-
wise is equal to
</bodyText>
<equation confidence="0.998691">
E PLM((|x)AL1z∈ζQ(z → y) (4)
ζ∈Exp(x)
</equation>
<bodyText confidence="0.999101">
Here Exp(x) is the set of possible expansions of x,
and PLM((|x) is the probability of the expansion (
according to the language model.
The defining property of the IT’s distribution P
is that T(P) = P. We can use this property
to compute the table iteratively. Begin by setting
P0(x → y) to 1 when x = y and 0 otherwise. Then
at each step let Pk+1 = T(Pk). When this process
converges, the limiting function is the correct inher-
itance distribution.
</bodyText>
<subsectionHeader confidence="0.997265">
2.4 Completing Trees
</subsectionHeader>
<bodyText confidence="0.999970476190476">
A final important issue is termination. Ordinarily, it
would be sensible to remove a tree from the search
order only when it is a goal state—that is, if it is a
complete tree that satisfies S. However, this turns
out to be not the best approach in this case due to a
quirk of our heuristic. P(T |S) has two non-constant
factors, P(S|T) and P(T). Once all of the nodes
in S appear in an incomplete tree T, P(S|T) = 1,
and so it won’t increase as the tree is expanded fur-
ther. Moreover, with each node expanded, P(T) de-
creases. This means that we are unlikely to make
progress beyond the point where all of the semantic
content appears in a tree.
An effective way to deal with this is to remove
trees from the search order as soon as P(S|T)
reaches 1. When the search terminates by finding
enough of these ‘almost complete’ trees, these trees
are completed: we find the optimal complete trees
by repeatedly expanding the N most likely almost-
complete trees (ranked by P(T)) until sufficiently
many complete trees are found.
</bodyText>
<sectionHeader confidence="0.99883" genericHeader="method">
3 Implementation
</sectionHeader>
<subsectionHeader confidence="0.96292">
3.1 Representation
</subsectionHeader>
<bodyText confidence="0.998679107142857">
Our semantic representation is based on the HALo-
gen input structure (Langkilde-Geary, 2002). The
meaning of a sentence is represented by a tree whose
nodes are each marked with a concept and a seman-
tic role. For example, the meaning of the sentence
“Turn left at the second traffic light” is represented
by the following structure:
(maketurn
:direction (left)
:spatial-locating
(trafficlight
:modifier (second)))
The syntax model we use is statistical dependency
grammar. As we outlined in Section 2, the semantic
and syntactic structures are attached to one another
in an SS tree. In order to accomodate the require-
ment that each semantic node is attached to no more
than one syntactic node, collocations like “traffic
light” or “John Hancock Tower”, are treated as sin-
gle syntactic nodes. It can also be convenient to ex-
tend this idea, treating phrases like “turn around”
or “thank you very much” as atomic. In the case
where a concept attaches to multi-word expression,
but where it is inconvenient to treat the expression
as a syntactic atom, we adopt the convention of at-
taching the concept to the hierarchically dominant
word in the expression. For instance, the concept of
turning can be attached to the expression “make a
</bodyText>
<page confidence="0.99949">
154
</page>
<bodyText confidence="0.999938833333333">
turn”; in this case we attach the concept to the word
“make”, and not to “turn”.
The nodes of an SS tree are (word, part of speech,
concept, semantic role) 4-tuples, where the concept
and role are left empty for function words, and the
word and part of speech are left empty for concepts
with no direct syntactic correlate. Generally we omit
the word itself from the tree in order to mitigate spar-
sity issues; these are added to the final full tree by a
lexical choice module.
We use a domain-trained language model based
on the same dependency structure as our syntactic-
semantic representations. The currently imple-
mented model calculates the probability of expan-
sions given a parent node based on an explicit tabu-
lar representation of the distribution P(ζ|x) for each
x. This language model is also used to score and
rank generated sentences.
</bodyText>
<subsectionHeader confidence="0.999674">
3.2 Corpus and Annotation
</subsectionHeader>
<bodyText confidence="0.999388458333333">
Training this language model requires an annotated
corpus of in-domain text. Our main corpus comes
from transcripts of direction-giving in a simulation
context, collected using the “Wizard of Oz” set-up
described in (Cheng et al., 2004). For development
and testing, we extracted approximately 600 instruc-
tions, divided into training and test sets. The training
set was used to train the language model used for
search, the lexical choice module, and the scoring
function. Both sets both underwent four partially-
automated stages of annotation.
First we tag words with their part of speech, using
the Brill tagger with manually modified lexicon and
transformation rules for our domain (Brill, 1995).
Second, the words are disambiguated and assigned
a concept tag. For this we construct a domain on-
tology, which is used to automatically tag the unam-
biguous words and prompt for human disambigua-
tion in the remaining cases. The third step is to as-
sign semantic roles. This is accomplished by using
a list of contextual rules, similar to the rules used by
the Brill tagger. For example, the rule
CON intersection PREV1OR2OR3WD at
: spatial-locating
assigns the role “spatial-locating” to a word whose
concept is “intersection” if the word “at” appears
one, two, or three words before it. A segment of
the corpus was automatically annotated using such
rules, then a human annotater made corrections and
added new rules, repeating these steps until the cor-
pus was fully annotated with semantic roles.
After the first three stages, the sentence, “Turn left
at the next intersection” is annotated as follows:
turn/VB/maketurn left/RB/
$leftright/direction at/IN the/
DT next/JJ/first/modifier
intersection/NN/intersection/
spatial-locating
The final annotation step is parsing. For this
we use an approach similar to Pereira and Sch-
abes’ grammar induction from partially bracketed
text (Pereira and Schabes, 1992). First we annotate
a segment of the corpus. Then we use the inside-
outside algorithm to simultaneously train a depen-
dency grammar and complete the annotation. We
then manually correct a further segment of the an-
notation, and repeat until acceptable parses are ob-
tained.
</bodyText>
<subsectionHeader confidence="0.991885">
3.3 Rendering
</subsectionHeader>
<bodyText confidence="0.99997615">
Linearizing an SS tree amounts to deciding the or-
der of the branches and whether each appears on the
left or the right side of the head. We built this infor-
mation into our language model, so a grammar rule
for expanding a node includes full ordering informa-
tion. This makes the linearization step trivial at the
cost of adding sparsity to the language model.
Lexicalization could be relegated to the language
model in the same way, by including lexemes in the
representation of each node, but again this would in-
cur sparsity costs. The other option is to delegate
lexical choice to a separate module, which takes a
SS tree and assigns a word to each node. We use a
hybrid approach: content words are assigned using
a lexical choice module, while most function words
are included explicitly in the language model. The
current lexical choice module simply assigns each
unlabeled node the most likely word conditioned on
its (POS, concept, role) triple, as observed in the
training corpus.
</bodyText>
<sectionHeader confidence="0.999703" genericHeader="method">
4 Example
</sectionHeader>
<bodyText confidence="0.9963215">
We take the semantic structure presented in Sec-
tion 3.1 as an example generation query. The search
</bodyText>
<page confidence="0.997084">
155
</page>
<bodyText confidence="0.933095538461539">
stage terminates when 100 trees that embed this se-
mantic structure have been found. The best-scoring
sentence has the following lexicalized tree:
turn/VB/maketurn
+left/RB/$leftright/direction
+at/IN
+traffic_light/NN/
trafficlight/
spatial-locating
-the/DT
+next/JJ/first/modifier
This is finally rendered thus:
turn left at the second traffic light.
</bodyText>
<sectionHeader confidence="0.997023" genericHeader="method">
5 Preliminary Results
</sectionHeader>
<bodyText confidence="0.999950857142857">
For initial testing, we separated the annotated corpus
into a 565-sentence training set and a 57-sentence
test set. We automatically extracted semantic struc-
tures from the test set, then used these structures
as generation queries, returning only the highest-
ranked sentence for each query. The generated re-
sults were then evaluated by three independent hu-
man annotaters along two dimensions: (1) Is the
generated sentence grammatical? (2) Does the gen-
erated sentence have the same meaning as the origi-
nal sentence?
For 11 of the 57 sentences (19%), the query ex-
traction failed due to inadequate grammar cover-
age.3 Of the 46 instances where a query was suc-
cessfully extracted, 3 queries (7%) timed out with-
out producing output. Averaging the annotaters’
judgments, 1 generated sentence (2%) was ungram-
matical, and 3 generated sentences (7%) had dif-
ferent meanings from their originals. 39 queries
(85%) produced output that was both grammatical
and faithful to the original sentence’s meaning.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="discussions">
6 Future Work
</sectionHeader>
<bodyText confidence="0.9999525">
Statistically-driven search offers a means of effi-
ciently overgenerating sentences to express a given
semantic structure. This is well-suited not only to
our navigation domain, but also to other domains
</bodyText>
<footnote confidence="0.81157575">
3The corpus was partially annotated for parse data, the full
parses being automatically generated from the domain-trained
language model. It was at this step that query extraction some-
times failed.
</footnote>
<bodyText confidence="0.999941045454545">
with a relatively small vocabulary but variable and
complex content structure. Our implementation of
the idea of this paper is under development in a num-
ber of directions.
A better option for robust language modeling
is to use maximum entropy techniques to train a
feature-based model. For instance, we can deter-
mine the probability of each child using such fea-
tures as the POS, concept, and role of the parent and
previous siblings. It may also be more effective to
isolate linear precedence from the language model,
introducing a non-trivial linearization step. Simi-
larly, the lexicalization module can be improved on
by using a more context-sensitive model.
Using only a tree-based scoring function is likely
to produce inferior results to one that incorporates a
linear score. A weighted average of the dependency
score with an n-gram model would already offer im-
provement. To further improve fluency, these could
also be combined with a scoring function that takes
longer-range dependencies into account, as well as
penalizing extraneous content.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999556681818182">
Srinivas Bangalore and O. Rambow. 2000. Using TAG,
a Tree Model, and a Language Model for Genera-
tion. 5th Int’l Workshop on Tree-Adjoining Grammars
(TAG+), TALANA, Paris.
Eric Brill. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing: A Case
Study in Part of Speech Tagging. Computational Lin-
guistics, 21 (4).
Hua Cheng, H. Bratt, R. Mishra, E. Shriberg, S. Upson, J.
Chen, F. Weng, S. Peters, L. Cavedon and J. Niekrasz.
2004. A Wizard Of OZ Framework for Collecting
Spoken Human-Computer Dialogs. Proc. 8th ICSLP,
Jeju Island, Korea.
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. Proc. 2nd INLG, Harriman, NY.
Fernando Pereira and Y. Schabes. 1992. Inside-outside
reestimation from partially bracketed corpora. Proc.
30th ACL, p.128-135, Newark.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. Proc. 1st NAACL,
Seattle.
</reference>
<page confidence="0.998788">
156
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.993356">
<title confidence="0.998748">A Domain-Specific Statistical Surface Realizer</title>
<author confidence="0.999923">Jeffrey T Russell</author>
<affiliation confidence="0.9993455">Center for the Study of Language and Information Stanford University</affiliation>
<email confidence="0.999405">jefe@stanford.edu</email>
<abstract confidence="0.999742071428571">We present a search-based approach to automatic surface realization given a corpus of domain sentences. Using heuristic search based on a statistical language model and a structure we introduce called table overgenerate a set of complete syntactic-semantic trees that are consistent with the given semantic structure and have high likelihood relative to the language model. These trees are then lexicalized, linearized, scored, and ranked. This model is being developed to generate real-time navigation instructions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>O Rambow</author>
</authors>
<title>Using TAG, a Tree Model, and a Language Model for Generation.</title>
<date>2000</date>
<booktitle>5th Int’l Workshop on Tree-Adjoining Grammars (TAG+), TALANA,</booktitle>
<location>Paris.</location>
<contexts>
<context position="2007" citStr="Bangalore and Rambow, 2000" startWordPosition="295" endWordPosition="298">ntic content of a sentence, we want to produce an English sentence that expresses this content. According to the traditional division of content determination, sentence planning, and surface realization, our work 151 is primarily concerned with surface realization, but also includes aspects of sentence planning. Our application requires robust flexibility within a restricted domain that is not well represented in the traditional corpora or tools. These requirements suggest using trainable stochastic generation. A number of statistical surface realizers have been described, notably the FERGUS (Bangalore and Rambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). FERGUS (Flexible Empiricist/Rationalist Generation Using Syntax) takes as input a dependency tree whose nodes are marked with lexemes only. The generator automatically “supertags” each input node with a TAG tree, then produces a lattice of all possible linearizations consistent with the supertagged dependency tree. Finally it selects the most likely traversal of this lattice, conditioned on a domain-trained language model. The HALogen system is a broad-coverage generator that uses a combination of stati</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and O. Rambow. 2000. Using TAG, a Tree Model, and a Language Model for Generation. 5th Int’l Workshop on Tree-Adjoining Grammars (TAG+), TALANA, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="16049" citStr="Brill, 1995" startWordPosition="2812" endWordPosition="2813"> corpus comes from transcripts of direction-giving in a simulation context, collected using the “Wizard of Oz” set-up described in (Cheng et al., 2004). For development and testing, we extracted approximately 600 instructions, divided into training and test sets. The training set was used to train the language model used for search, the lexical choice module, and the scoring function. Both sets both underwent four partiallyautomated stages of annotation. First we tag words with their part of speech, using the Brill tagger with manually modified lexicon and transformation rules for our domain (Brill, 1995). Second, the words are disambiguated and assigned a concept tag. For this we construct a domain ontology, which is used to automatically tag the unambiguous words and prompt for human disambiguation in the remaining cases. The third step is to assign semantic roles. This is accomplished by using a list of contextual rules, similar to the rules used by the Brill tagger. For example, the rule CON intersection PREV1OR2OR3WD at : spatial-locating assigns the role “spatial-locating” to a word whose concept is “intersection” if the word “at” appears one, two, or three words before it. A segment of </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging. Computational Linguistics, 21 (4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Cheng</author>
<author>H Bratt</author>
<author>R Mishra</author>
<author>E Shriberg</author>
<author>S Upson</author>
<author>J Chen</author>
<author>F Weng</author>
<author>S Peters</author>
<author>L Cavedon</author>
<author>J Niekrasz</author>
</authors>
<title>A Wizard Of OZ Framework for Collecting Spoken Human-Computer Dialogs.</title>
<date>2004</date>
<booktitle>Proc. 8th ICSLP,</booktitle>
<location>Jeju Island,</location>
<contexts>
<context position="15588" citStr="Cheng et al., 2004" startWordPosition="2738" endWordPosition="2741">main-trained language model based on the same dependency structure as our syntacticsemantic representations. The currently implemented model calculates the probability of expansions given a parent node based on an explicit tabular representation of the distribution P(ζ|x) for each x. This language model is also used to score and rank generated sentences. 3.2 Corpus and Annotation Training this language model requires an annotated corpus of in-domain text. Our main corpus comes from transcripts of direction-giving in a simulation context, collected using the “Wizard of Oz” set-up described in (Cheng et al., 2004). For development and testing, we extracted approximately 600 instructions, divided into training and test sets. The training set was used to train the language model used for search, the lexical choice module, and the scoring function. Both sets both underwent four partiallyautomated stages of annotation. First we tag words with their part of speech, using the Brill tagger with manually modified lexicon and transformation rules for our domain (Brill, 1995). Second, the words are disambiguated and assigned a concept tag. For this we construct a domain ontology, which is used to automatically t</context>
</contexts>
<marker>Cheng, Bratt, Mishra, Shriberg, Upson, Chen, Weng, Peters, Cavedon, Niekrasz, 2004</marker>
<rawString>Hua Cheng, H. Bratt, R. Mishra, E. Shriberg, S. Upson, J. Chen, F. Weng, S. Peters, L. Cavedon and J. Niekrasz. 2004. A Wizard Of OZ Framework for Collecting Spoken Human-Computer Dialogs. Proc. 8th ICSLP, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde-Geary</author>
</authors>
<title>An empirical verification of coverage and correctness for a general-purpose sentence generator.</title>
<date>2002</date>
<booktitle>Proc. 2nd INLG,</booktitle>
<location>Harriman, NY.</location>
<contexts>
<context position="13338" citStr="Langkilde-Geary, 2002" startWordPosition="2363" endWordPosition="2364">This means that we are unlikely to make progress beyond the point where all of the semantic content appears in a tree. An effective way to deal with this is to remove trees from the search order as soon as P(S|T) reaches 1. When the search terminates by finding enough of these ‘almost complete’ trees, these trees are completed: we find the optimal complete trees by repeatedly expanding the N most likely almostcomplete trees (ranked by P(T)) until sufficiently many complete trees are found. 3 Implementation 3.1 Representation Our semantic representation is based on the HALogen input structure (Langkilde-Geary, 2002). The meaning of a sentence is represented by a tree whose nodes are each marked with a concept and a semantic role. For example, the meaning of the sentence “Turn left at the second traffic light” is represented by the following structure: (maketurn :direction (left) :spatial-locating (trafficlight :modifier (second))) The syntax model we use is statistical dependency grammar. As we outlined in Section 2, the semantic and syntactic structures are attached to one another in an SS tree. In order to accomodate the requirement that each semantic node is attached to no more than one syntactic node</context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>Irene Langkilde-Geary. 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator. Proc. 2nd INLG, Harriman, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Y Schabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>Proc. 30th ACL,</booktitle>
<pages>128--135</pages>
<location>Newark.</location>
<contexts>
<context position="17257" citStr="Pereira and Schabes, 1992" startWordPosition="2997" endWordPosition="3000"> it. A segment of the corpus was automatically annotated using such rules, then a human annotater made corrections and added new rules, repeating these steps until the corpus was fully annotated with semantic roles. After the first three stages, the sentence, “Turn left at the next intersection” is annotated as follows: turn/VB/maketurn left/RB/ $leftright/direction at/IN the/ DT next/JJ/first/modifier intersection/NN/intersection/ spatial-locating The final annotation step is parsing. For this we use an approach similar to Pereira and Schabes’ grammar induction from partially bracketed text (Pereira and Schabes, 1992). First we annotate a segment of the corpus. Then we use the insideoutside algorithm to simultaneously train a dependency grammar and complete the annotation. We then manually correct a further segment of the annotation, and repeat until acceptable parses are obtained. 3.3 Rendering Linearizing an SS tree amounts to deciding the order of the branches and whether each appears on the left or the right side of the head. We built this information into our language model, so a grammar rule for expanding a node includes full ordering information. This makes the linearization step trivial at the cost</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. Proc. 30th ACL, p.128-135, Newark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Trainable methods for surface natural language generation.</title>
<date>2000</date>
<booktitle>Proc. 1st NAACL,</booktitle>
<location>Seattle.</location>
<contexts>
<context position="2097" citStr="Ratnaparkhi, 2000" startWordPosition="310" endWordPosition="312">ording to the traditional division of content determination, sentence planning, and surface realization, our work 151 is primarily concerned with surface realization, but also includes aspects of sentence planning. Our application requires robust flexibility within a restricted domain that is not well represented in the traditional corpora or tools. These requirements suggest using trainable stochastic generation. A number of statistical surface realizers have been described, notably the FERGUS (Bangalore and Rambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). FERGUS (Flexible Empiricist/Rationalist Generation Using Syntax) takes as input a dependency tree whose nodes are marked with lexemes only. The generator automatically “supertags” each input node with a TAG tree, then produces a lattice of all possible linearizations consistent with the supertagged dependency tree. Finally it selects the most likely traversal of this lattice, conditioned on a domain-trained language model. The HALogen system is a broad-coverage generator that uses a combination of statistical and symbolic techniques. The input, a structure of featurevalue pairs (see Section </context>
</contexts>
<marker>Ratnaparkhi, 2000</marker>
<rawString>Adwait Ratnaparkhi. 2000. Trainable methods for surface natural language generation. Proc. 1st NAACL, Seattle.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>