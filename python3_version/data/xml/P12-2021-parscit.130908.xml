<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002314">
<title confidence="0.979604">
Robust Conversion of CCG Derivations to Phrase Structure Trees
</title>
<author confidence="0.999356">
Jonathan K. Kummerfeldt Dan Kleint James R. Curran�
</author>
<affiliation confidence="0.873936666666667">
tComputer Science Division t-lab, School of IT
University of California, Berkeley University of Sydney
Berkeley, CA 94720, USA Sydney, NSW 2006, Australia
</affiliation>
<email confidence="0.997689">
{jkk,klein}@cs.berkeley.edu james@it.usyd.edu.au
</email>
<sectionHeader confidence="0.993866" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99956875">
We propose an improved, bottom-up method
for converting CCG derivations into PTB-style
phrase structure trees. In contrast with past
work (Clark and Curran, 2009), which used
simple transductions on category pairs, our ap-
proach uses richer transductions attached to
single categories. Our conversion preserves
more sentences under round-trip conversion
(51.1% vs. 39.6%) and is more robust. In par-
ticular, unlike past methods, ours does not re-
quire ad-hoc rules over non-local features, and
so can be easily integrated into a parser.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993415">
Converting the Penn Treebank (PTB, Marcus et al.,
1993) to other formalisms, such as HPSG (Miyao
et al., 2004), LFG (Cahill et al., 2008), LTAG (Xia,
1999), and CCG (Hockenmaier, 2003), is a com-
plex process that renders linguistic phenomena in
formalism-specific ways. Tools for reversing these
conversions are desirable for downstream parser use
and parser comparison. However, reversing conver-
sions is difficult, as corpus conversions may lose in-
formation or smooth over PTB inconsistencies.
Clark and Curran (2009) developed a CCG to PTB
conversion that treats the CCG derivation as a phrase
structure tree and applies hand-crafted rules to ev-
ery pair of categories that combine in the derivation.
Because their approach does not exploit the gener-
alisations inherent in the CCG formalism, they must
resort to ad-hoc rules over non-local features of the
CCG constituents being combined (when a fixed pair
of CCG categories correspond to multiple PTB struc-
tures). Even with such rules, they correctly convert
only 39.6% of gold CCGbank derivations.
Our conversion assigns a set of bracket instruc-
tions to each word based on its CCG category, then
follows the CCG derivation, applying and combin-
ing instructions at each combinatory step to build a
phrase structure tree. This requires specific instruc-
tions for each category (not all pairs), and generic
operations for each combinator. We cover all cate-
gories in the development set and correctly convert
51.1% of sentences. Unlike Clark and Curran’s ap-
proach, we require no rules that consider non-local
features of constituents, which enables the possibil-
ity of simple integration with a CKY-based parser.
The most common errors our approach makes in-
volve nodes for clauses and rare spans such as QPs,
NXs, and NACs. Many of these errors are inconsis-
tencies in the original PTB annotations that are not
recoverable. These issues make evaluating parser
output difficult, but our method does enable an im-
proved comparison of CCG and PTB parsers.
</bodyText>
<sectionHeader confidence="0.97109" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99988">
There has been extensive work on converting parser
output for evaluation, e.g. Lin (1998) and Briscoe et
al. (2002) proposed using underlying dependencies
for evaluation. There has also been work on conver-
sion to phrase structure, from dependencies (Xia and
Palmer, 2001; Xia et al., 2009) and from lexicalised
formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008)
and TAG (Chiang, 2000; Sarkar, 2001). Our focus is
on CCG to PTB conversion (Clark and Curran, 2009).
</bodyText>
<subsectionHeader confidence="0.964314">
2.1 Combinatory Categorial Grammar (CCG)
</subsectionHeader>
<bodyText confidence="0.9993745">
The lower half of Figure 1 shows a CCG derivation
(Steedman, 2000) in which each word is assigned a
category, and combinatory rules are applied to ad-
jacent categories until only one remains. Categories
</bodyText>
<page confidence="0.989215">
105
</page>
<note confidence="0.7653275">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<equation confidence="0.738439222222222">
Italian magistrates labeled his death a suicide
N/N N ((S[dcl]\NP)/NP)/NP NP[nb]/N N NP[nb]/N N
� � �
N NP NP
�
NP (S[dcl]\NP)/NP
�
S[dcl]\NP
S[dcl]
</equation>
<figureCaption confidence="0.9876135">
Figure 1: A crossing constituents example: his... suicide
(PTB) crosses labeled... death (CCGbank).
</figureCaption>
<subsectionHeader confidence="0.340814">
Categories Schema
</subsectionHeader>
<tableCaption confidence="0.996671">
Table 1: Example C&amp;C-CONV lexical and rule schemas.
</tableCaption>
<bodyText confidence="0.9997024375">
can be atomic, e.g. the N assigned to magistrates,
or complex functions of the form result/ arg, where
result and arg are categories and the slash indicates
the argument’s directionality. Combinators define
how adjacent categories can combine. Figure 1 uses
function application, where a complex category con-
sumes an adjacent argument to form its result, e.g.
S[dcl]\NP combines with the NP to its left to form
an S[dcl]. More powerful combinators allow cate-
gories to combine with greater flexibility.
We cannot form a PTB tree by simply relabeling
the categories in a CCG derivation because the map-
ping to phrase labels is many-to-many, CCG deriva-
tions contain extra brackets due to binarisation, and
there are cases where the constituents in the PTB tree
and the CCG derivation cross (e.g. in Figure 1).
</bodyText>
<subsectionHeader confidence="0.999657">
2.2 Clark and Curran (2009)
</subsectionHeader>
<bodyText confidence="0.9999814">
Clark and Curran (2009), hereafter C&amp;C-CONV, as-
sign a schema to each leaf (lexical category) and rule
(pair of combining categories) in the CCG derivation.
The PTB tree is constructed from the CCG bottom-
up, creating leaves with lexical schemas, then merg-
ing/adding sub-trees using rule schemas at each step.
The schemas for Figure 1 are shown in Table 1.
These apply to create NPs over magistrates, death,
and suicide, and a VP over labeled, and then com-
bine the trees by placing one under the other at each
step, and finally create an S node at the root.
C&amp;C-CONV has sparsity problems, requiring
schemas for all valid pairs of categories — at a
minimum, the 2853 unique category combinations
found in CCGbank. Clark and Curran (2009) create
schemas for only 776 of these, handling the remain-
der with approximate catch-all rules.
C&amp;C-CONV only specifies one simple schema for
each rule (pair of categories). This appears reason-
able at first, but frequently causes problems, e.g.:
</bodyText>
<equation confidence="0.999605333333333">
(N/N)/(N/N) + N/N
“more than” + “30” (1)
“relatively” + “small” (2)
</equation>
<bodyText confidence="0.999935666666667">
Here either a QP bracket (1) or an ADJP bracket
(2) should be created. Since both examples involve
the same rule schema, C&amp;C-CONV would incorrectly
process them in the same way. To combat the most
glaring errors, C&amp;C-CONV manipulates the PTB tree
with ad-hoc rules based on non-local features over
the CCG nodes being combined — an approach that
cannot be easily integrated into a parser.
These disadvantages are a consequence of failing
to exploit the generalisations that CCG combinators
define. We return to this example below to show how
our approach handles both cases correctly.
</bodyText>
<sectionHeader confidence="0.979355" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.999987277777778">
Our conversion assigns a set of instructions to each
lexical category and defines generic operations for
each combinator that combine instructions. Figure 2
shows a typical instruction, which specifies the node
to create and where to place the PTB trees associated
with the two categories combining. More complex
operations are shown in Table 2. Categories with
multiple arguments are assigned one instruction per
argument, e.g. labeled has three. These are applied
one at a time, as each combinatory step occurs.
For the example from the previous section we be-
gin by assigning the instructions shown in Table 3.
Some of these can apply immediately as they do not
involve an argument, e.g. magistrates has (NP f).
One of the more complex cases in the example is
Italian, which is assigned (NP f {a}). This creates
a new bracket, inserts the functor’s tree, and flattens
and inserts the argument’s tree, producing:
</bodyText>
<figure confidence="0.979393590909091">
(NP (JJ Italian) (NNS magistrates))
S
NP VP
VBD S
NP NP
PRP$ NN DT NN
JJ NNS
N
((S[dcl]\NP)/NP)/NP
create an NP
create a VP
N/N + N
NP[nb]/N + N
((S[dcl]\NP)/NP)/NP + NP
(S[dcl]\NP)/NP + NP
place left under right
place left under right
place right under left
place right under left
NP + S[dcl]\NP
place both under S
�
</figure>
<page confidence="0.652448">
106
</page>
<figureCaption confidence="0.8527665">
Figure 2: An example function application. Top row:
CCG rule. Bottom row: applying instruction (VP f a).
</figureCaption>
<table confidence="0.981183857142857">
Symbol Meaning Example
(X f a) Add an X bracket around (VP f a)
functor and argument
{ } Flatten enclosed node (N f {a})
X* Use same label as arg. (S* f {a})
or default to X
fi Place subtrees (PP fo (S fl..k a))
</table>
<tableCaption confidence="0.999412">
Table 2: Types of operations in instructions.
</tableCaption>
<bodyText confidence="0.999935730769231">
For the complete example the final tree is almost
correct but omits the S bracket around the final two
NPs. To fix our example we could have modified our
instructions to use the final symbol in Table 2. The
subscripts indicate which subtrees to place where.
However, for this particular construction the PTB an-
notations are inconsistent, and so we cannot recover
without introducing more errors elsewhere.
For combinators other than function application,
we combine the instructions in various ways. Ad-
ditionally, we vary the instructions assigned based
on the POS tag in 32 cases, and for the word not,
to recover distinctions not captured by CCGbank
categories alone. In 52 cases the later instruc-
tions depend on the structure of the argument being
picked up. We have sixteen special cases for non-
combinatory binary rules and twelve special cases
for non-combinatory unary rules.
Our approach naturally handles our QP vs. ADJP
example because the two cases have different lexical
categories: ((N/N)/(N/N))\(S[adj]\NP) on than
and (N/N)/(N/N) on relatively. This lexical dif-
ference means we can assign different instructions to
correctly recover the QP and ADJP nodes, whereas
C&amp;C-CONV applies the same schema in both cases
as the categories combining are the same.
</bodyText>
<sectionHeader confidence="0.99942" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999417666666667">
Using sections 00-21 of the treebanks, we hand-
crafted instructions for 527 lexical categories, a pro-
cess that took under 100 hours, and includes all the
categories used by the C&amp;C parser. There are 647
further categories and 35 non-combinatory binary
rules in sections 00-21 that we did not annotate. For
</bodyText>
<table confidence="0.446147">
Category Instruction set
N (NP f)
N /N1 (NP f {a})
NP[nb]/N1 (NP f {a})
((S[dcl]\NP,q)/NP2)/NP1 (VP f a)
(VP {f} a)
(S a f)
</table>
<tableCaption confidence="0.971446">
Table 3: Instruction sets for the categories in Figure 1.
</tableCaption>
<table confidence="0.999661">
System Data P R F Sent.
00 (all) 95.37 93.67 94.51 39.6
C&amp;C 00 (len &lt; 40) 95.85 94.39 95.12 42.1
CONV 23 (all) 95.33 93.95 94.64 39.7
23 (len &lt; 40) 95.44 94.04 94.73 41.9
00 (all) 96.69 96.58 96.63 51.1
This 00 (len &lt; 40) 96.98 96.77 96.87 53.6
Work 23 (all) 96.49 96.11 96.30 51.4
23 (len &lt; 40) 96.57 96.21 96.39 53.8
</table>
<tableCaption confidence="0.9987235">
Table 4: PARSEVAL Precision, Recall, F-Score, and exact
sentence match for converted gold CCG derivations.
</tableCaption>
<bodyText confidence="0.9992333125">
unannotated categories, we use the instructions of
the result category with an added instruction.
Table 4 compares our approach with C&amp;C-CONV
on gold CCG derivations. The results shown are as
reported by EVALB (Abney et al., 1991) using the
Collins (1997) parameters. Our approach leads to in-
creases on all metrics of at least 1.1%, and increases
exact sentence match by over 11% (both absolute).
Many of the remaining errors relate to missing
and extra clause nodes and a range of rare structures,
such as QPs, NACs, and NXs. The only other promi-
nent errors are single word spans, e.g. extra or miss-
ing ADVPs. Many of these errors are unrecover-
able from CCGbank, either because inconsistencies
in the PTB have been smoothed over or because they
are genuine but rare constructions that were lost.
</bodyText>
<subsectionHeader confidence="0.990633">
4.1 Parser Comparison
</subsectionHeader>
<bodyText confidence="0.999918545454546">
When we convert the output of a CCG parser, the PTB
trees that are produced will contain errors created by
our conversion as well as by the parser. In this sec-
tion we are interested in comparing parsers, so we
need to factor out errors created by our conversion.
One way to do this is to calculate a projected score
(PROJ), as the parser result over the oracle result, but
this is a very rough approximation. Another way is
to evaluate only on the 51% of sentences for which
our conversion from gold CCG derivations is perfect
(CLEAN). However, even on this set our conversion
</bodyText>
<figure confidence="0.9971946">
((S\NP)/NP)/NP NP
f a
(S\NP)/NP
VP
f a
</figure>
<page confidence="0.729913">
107
</page>
<figureCaption confidence="0.986195888888889">
Figure 3: For each sentence in the treebank, we plot
the converted parser output against gold conversion (left),
and the original parser evaluation against gold conversion
(right). Left: Most points lie below the diagonal, indicat-
ing that the quality of converted parser output (y) is upper
bounded by the quality of conversion on gold parses (x).
Right: No clear correlation is present, indicating that the
set of sentences that are converted best (on the far right),
are not necessarily easy to parse.
</figureCaption>
<bodyText confidence="0.99958225">
introduces errors, as the parser output may contain
categories that are harder to convert.
Parser F-scores are generally higher on CLEAN,
which could mean that this set is easier to parse, or it
could mean that these sentences don’t contain anno-
tation inconsistencies, and so the parsers aren’t in-
correct for returning the true parse (as opposed to
the one in the PTB). To test this distinction we look
for correlation between conversion quality and parse
difficulty on another metric. In particular, Figure 3
(right) shows CCG labeled dependency performance
for the C&amp;C parser vs. CCGbank conversion PARSE-
VAL scores. The lack of a strong correlation, and the
spread on the line x = 100, supports the theory that
these sentences are not necessarily easier to parse,
but rather have fewer annotation inconsistencies.
In the left plot, the y-axis is PARSEVAL on con-
verted C&amp;C parser output. Conversion quality essen-
tially bounds the performance of the parser. The few
points above the diagonal are mostly short sentences
on which the C&amp;C parser uses categories that lead
to one extra correct node. The main constructions
on which parse errors occur, e.g. PP attachment, are
rarely converted incorrectly, and so we expect the
number of errors to be cumulative. Some sentences
are higher in the right plot than the left because there
are distinctions in CCG that are not always present in
the PTB, e.g. the argument-adjunct distinction.
Table 5 presents F-scores for three PTB parsers
and three CCG parsers (with their output converted
by our method). One interesting comparison is be-
tween the PTB parser of Petrov and Klein (2007) and
</bodyText>
<table confidence="0.998352272727273">
Sentences CLEAN ALL PROJ
Converted gold CCG
CCGbank 100.0 96.3 –
Converted CCG
Clark and Curran (2007) 90.9 85.5 88.8
Fowler and Penn (2010) 90.9 86.0 89.3
Auli and Lopez (2011) 91.7 86.2 89.5
Native PTB
Klein and Manning (2003) 89.8 85.8 –
Petrov and Klein (2007) 93.6 90.1 –
Charniak and Johnson (2005) 94.8 91.5 –
</table>
<tableCaption confidence="0.998608">
Table 5: F-scores on section 23 for PTB parsers and
CCG parsers with their output converted by our method.
CLEAN is only on sentences that are converted perfectly
from gold CCG (51%). ALL is over all sentences. PROJ is
a projected F-score (ALL result / CCGbank ALL result).
</tableCaption>
<bodyText confidence="0.999746928571429">
the CCG parser of Fowler and Penn (2010), which
use the same underlying parser. The performance
gap is partly due to structures in the PTB that are not
recoverable from CCGbank, but probably also indi-
cates that the split-merge model is less effective in
CCG, which has far more symbols than the PTB.
It is difficult to make conclusive claims about
the performance of the parsers. As shown earlier,
CLEAN does not completely factor out the errors in-
troduced by our conversion, as the parser output may
be more difficult to convert, and the calculation of
PROJ only roughly factors out the errors. However,
the results do suggest that the performance of the
CCG parsers is approaching that of the Petrov parser.
</bodyText>
<sectionHeader confidence="0.994531" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999968714285714">
By exploiting the generalised combinators of the
CCG formalism, we have developed a new method
of converting CCG derivations into PTB-style trees.
Our system, which is publicly available1, is more
effective than previous work, increasing exact sen-
tence match by more than 11% (absolute), and can
be directly integrated with a CCG parser.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999838666666667">
We would like to thank the anonymous reviewers
for their helpful suggestions. This research was
supported by a General Sir John Monash Fellow-
ship, the Office of Naval Research under MURI
Grant No. N000140911081, ARC Discovery grant
DP1097291, and the Capital Markets CRC.
</bodyText>
<footnote confidence="0.776479">
1http://code.google.com/p/berkeley-ccg2pst/
</footnote>
<figure confidence="0.996239277777778">
100
80
60
40
20
0
0 20 40 60 80 100
Converted Gold, EVALB
100
80
60
40
20
0
0 20 40 60 80 100
Converted Gold, EVALB
Converted C&amp;C, EVALB
Native C&amp;C, ldeps
</figure>
<page confidence="0.99043">
108
</page>
<sectionHeader confidence="0.9891" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999652457831326">
S. Abney, S. Flickenger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M. Marcus, S. Roukos, B. San-
torini, and T. Strzalkowski. 1991. Procedure for quan-
titatively comparing the syntactic coverage of english
grammars. In Proceedings of the workshop on Speech
and Natural Language, pages 306–311.
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated ccg supertagging and parsing. In Proceed-
ings ofACL, pages 470–480.
Ted Briscoe, John Carroll, Jonathan Graham, and Ann
Copestake. 2002. Relational evaluation schemes. In
Proceedings of the Beyond PARSEVAL Workshop at
LREC, pages 4–8.
Aoife Cahill, Michael Burke, Ruth O’Donovan, Stefan
Riezler, Josef van Genabith, and Andy Way. 2008.
Wide-coverage deep statistical parsing using auto-
matic dependency structure annotation. Computa-
tional Linguistics, 34(1):81–124.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings ofACL, pages 173–180.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings ofACL, pages 456–463.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493–552.
Stephen Clark and James R. Curran. 2009. Comparing
the accuracy of CCG and penn treebank parsers. In
Proceedings ofACL, pages 53–56.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings ofACL,
pages 16–23.
Timothy A. D. Fowler and Gerald Penn. 2010. Accu-
rate context-free parsing with combinatory categorial
grammar. In Proceedings ofACL, pages 335–344.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with Combinatory Categorial Grammar.
Ph.D. thesis, School of Informatics, The University of
Edinburgh.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423–430.
Dekang Lin. 1998. A dependency-based method for
evaluating broad-coverage parsers. Natural Language
Engineering, 4(2):97–114.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313–330.
Takuya Matsuzaki and Jun’ichi Tsujii. 2008. Com-
parative parser performance analysis across grammar
frameworks through automatic tree conversion using
synchronous grammars. In Proceedings of Coling,
pages 545–552.
Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii.
2004. Corpus-oriented grammar development for ac-
quiring a head-driven phrase structure grammar from
the penn treebank. In Proceedings of IJCNLP, pages
684–693.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL,
pages 404–411.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL, pages
1–8.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In Proceedings
ofHLT, pages 1–5.
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,
and Dipti Misra Sharma. 2009. Towards a multi-
representational treebank. In Proceedings of the 7th
International Workshop on Treebanks and Linguistic
Theories, pages 159–170.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of the Natural Lan-
guage Processing Pacific Rim Symposium, pages 398–
403.
</reference>
<page confidence="0.998956">
109
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978278">
<title confidence="0.99984">Conversion of to Phrase Structure Trees</title>
<author confidence="0.998969">K R</author>
<affiliation confidence="0.9997755">Science Division School of IT University of California, Berkeley University of Sydney</affiliation>
<address confidence="0.99966">Berkeley, CA 94720, USA Sydney, NSW 2006, Australia</address>
<email confidence="0.99892">james@it.usyd.edu.au</email>
<abstract confidence="0.998497076923077">We propose an improved, bottom-up method converting into phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>S Flickenger</author>
<author>C Gdaniec</author>
<author>C Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>Procedure for quantitatively comparing the syntactic coverage of english grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language,</booktitle>
<pages>306--311</pages>
<contexts>
<context position="10615" citStr="Abney et al., 1991" startWordPosition="1736" endWordPosition="1739">7 93.67 94.51 39.6 C&amp;C 00 (len &lt; 40) 95.85 94.39 95.12 42.1 CONV 23 (all) 95.33 93.95 94.64 39.7 23 (len &lt; 40) 95.44 94.04 94.73 41.9 00 (all) 96.69 96.58 96.63 51.1 This 00 (len &lt; 40) 96.98 96.77 96.87 53.6 Work 23 (all) 96.49 96.11 96.30 51.4 23 (len &lt; 40) 96.57 96.21 96.39 53.8 Table 4: PARSEVAL Precision, Recall, F-Score, and exact sentence match for converted gold CCG derivations. unannotated categories, we use the instructions of the result category with an added instruction. Table 4 compares our approach with C&amp;C-CONV on gold CCG derivations. The results shown are as reported by EVALB (Abney et al., 1991) using the Collins (1997) parameters. Our approach leads to increases on all metrics of at least 1.1%, and increases exact sentence match by over 11% (both absolute). Many of the remaining errors relate to missing and extra clause nodes and a range of rare structures, such as QPs, NACs, and NXs. The only other prominent errors are single word spans, e.g. extra or missing ADVPs. Many of these errors are unrecoverable from CCGbank, either because inconsistencies in the PTB have been smoothed over or because they are genuine but rare constructions that were lost. 4.1 Parser Comparison When we con</context>
</contexts>
<marker>Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>S. Abney, S. Flickenger, C. Gdaniec, C. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. Procedure for quantitatively comparing the syntactic coverage of english grammars. In Proceedings of the workshop on Speech and Natural Language, pages 306–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
</authors>
<title>A comparison of loopy belief propagation and dual decomposition for integrated ccg supertagging and parsing.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>470--480</pages>
<contexts>
<context position="14137" citStr="Auli and Lopez (2011)" startWordPosition="2334" endWordPosition="2337"> incorrectly, and so we expect the number of errors to be cumulative. Some sentences are higher in the right plot than the left because there are distinctions in CCG that are not always present in the PTB, e.g. the argument-adjunct distinction. Table 5 presents F-scores for three PTB parsers and three CCG parsers (with their output converted by our method). One interesting comparison is between the PTB parser of Petrov and Klein (2007) and Sentences CLEAN ALL PROJ Converted gold CCG CCGbank 100.0 96.3 – Converted CCG Clark and Curran (2007) 90.9 85.5 88.8 Fowler and Penn (2010) 90.9 86.0 89.3 Auli and Lopez (2011) 91.7 86.2 89.5 Native PTB Klein and Manning (2003) 89.8 85.8 – Petrov and Klein (2007) 93.6 90.1 – Charniak and Johnson (2005) 94.8 91.5 – Table 5: F-scores on section 23 for PTB parsers and CCG parsers with their output converted by our method. CLEAN is only on sentences that are converted perfectly from gold CCG (51%). ALL is over all sentences. PROJ is a projected F-score (ALL result / CCGbank ALL result). the CCG parser of Fowler and Penn (2010), which use the same underlying parser. The performance gap is partly due to structures in the PTB that are not recoverable from CCGbank, but prob</context>
</contexts>
<marker>Auli, Lopez, 2011</marker>
<rawString>Michael Auli and Adam Lopez. 2011. A comparison of loopy belief propagation and dual decomposition for integrated ccg supertagging and parsing. In Proceedings ofACL, pages 470–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Jonathan Graham</author>
<author>Ann Copestake</author>
</authors>
<title>Relational evaluation schemes.</title>
<date>2002</date>
<booktitle>In Proceedings of the Beyond PARSEVAL Workshop at LREC,</booktitle>
<pages>4--8</pages>
<contexts>
<context position="3001" citStr="Briscoe et al. (2002)" startWordPosition="462" endWordPosition="465">proach, we require no rules that consider non-local features of constituents, which enables the possibility of simple integration with a CKY-based parser. The most common errors our approach makes involve nodes for clauses and rare spans such as QPs, NXs, and NACs. Many of these errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proc</context>
</contexts>
<marker>Briscoe, Carroll, Graham, Copestake, 2002</marker>
<rawString>Ted Briscoe, John Carroll, Jonathan Graham, and Ann Copestake. 2002. Relational evaluation schemes. In Proceedings of the Beyond PARSEVAL Workshop at LREC, pages 4–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Michael Burke</author>
<author>Ruth O’Donovan</author>
<author>Stefan Riezler</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Wide-coverage deep statistical parsing using automatic dependency structure annotation.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<marker>Cahill, Burke, O’Donovan, Riezler, van Genabith, Way, 2008</marker>
<rawString>Aoife Cahill, Michael Burke, Ruth O’Donovan, Stefan Riezler, Josef van Genabith, and Andy Way. 2008. Wide-coverage deep statistical parsing using automatic dependency structure annotation. Computational Linguistics, 34(1):81–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="14264" citStr="Charniak and Johnson (2005)" startWordPosition="2357" endWordPosition="2360">e left because there are distinctions in CCG that are not always present in the PTB, e.g. the argument-adjunct distinction. Table 5 presents F-scores for three PTB parsers and three CCG parsers (with their output converted by our method). One interesting comparison is between the PTB parser of Petrov and Klein (2007) and Sentences CLEAN ALL PROJ Converted gold CCG CCGbank 100.0 96.3 – Converted CCG Clark and Curran (2007) 90.9 85.5 88.8 Fowler and Penn (2010) 90.9 86.0 89.3 Auli and Lopez (2011) 91.7 86.2 89.5 Native PTB Klein and Manning (2003) 89.8 85.8 – Petrov and Klein (2007) 93.6 90.1 – Charniak and Johnson (2005) 94.8 91.5 – Table 5: F-scores on section 23 for PTB parsers and CCG parsers with their output converted by our method. CLEAN is only on sentences that are converted perfectly from gold CCG (51%). ALL is over all sentences. PROJ is a projected F-score (ALL result / CCGbank ALL result). the CCG parser of Fowler and Penn (2010), which use the same underlying parser. The performance gap is partly due to structures in the PTB that are not recoverable from CCGbank, but probably also indicates that the split-merge model is less effective in CCG, which has far more symbols than the PTB. It is difficu</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings ofACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>456--463</pages>
<contexts>
<context position="3269" citStr="Chiang, 2000" startWordPosition="505" endWordPosition="506">se errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Italian magistrates labeled his death a suicide N/N N ((S[dcl]\NP)/NP)/NP NP[n</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In Proceedings ofACL, pages 456–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="14062" citStr="Clark and Curran (2007)" startWordPosition="2320" endWordPosition="2323">uctions on which parse errors occur, e.g. PP attachment, are rarely converted incorrectly, and so we expect the number of errors to be cumulative. Some sentences are higher in the right plot than the left because there are distinctions in CCG that are not always present in the PTB, e.g. the argument-adjunct distinction. Table 5 presents F-scores for three PTB parsers and three CCG parsers (with their output converted by our method). One interesting comparison is between the PTB parser of Petrov and Klein (2007) and Sentences CLEAN ALL PROJ Converted gold CCG CCGbank 100.0 96.3 – Converted CCG Clark and Curran (2007) 90.9 85.5 88.8 Fowler and Penn (2010) 90.9 86.0 89.3 Auli and Lopez (2011) 91.7 86.2 89.5 Native PTB Klein and Manning (2003) 89.8 85.8 – Petrov and Klein (2007) 93.6 90.1 – Charniak and Johnson (2005) 94.8 91.5 – Table 5: F-scores on section 23 for PTB parsers and CCG parsers with their output converted by our method. CLEAN is only on sentences that are converted perfectly from gold CCG (51%). ALL is over all sentences. PROJ is a projected F-score (ALL result / CCGbank ALL result). the CCG parser of Fowler and Penn (2010), which use the same underlying parser. The performance gap is partly d</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Comparing the accuracy of CCG and penn treebank parsers.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>53--56</pages>
<contexts>
<context position="1394" citStr="Clark and Curran (2009)" startWordPosition="200" endWordPosition="203">quire ad-hoc rules over non-local features, and so can be easily integrated into a parser. 1 Introduction Converting the Penn Treebank (PTB, Marcus et al., 1993) to other formalisms, such as HPSG (Miyao et al., 2004), LFG (Cahill et al., 2008), LTAG (Xia, 1999), and CCG (Hockenmaier, 2003), is a complex process that renders linguistic phenomena in formalism-specific ways. Tools for reversing these conversions are desirable for downstream parser use and parser comparison. However, reversing conversions is difficult, as corpus conversions may lose information or smooth over PTB inconsistencies. Clark and Curran (2009) developed a CCG to PTB conversion that treats the CCG derivation as a phrase structure tree and applies hand-crafted rules to every pair of categories that combine in the derivation. Because their approach does not exploit the generalisations inherent in the CCG formalism, they must resort to ad-hoc rules over non-local features of the CCG constituents being combined (when a fixed pair of CCG categories correspond to multiple PTB structures). Even with such rules, they correctly convert only 39.6% of gold CCGbank derivations. Our conversion assigns a set of bracket instructions to each word b</context>
<context position="3348" citStr="Clark and Curran, 2009" startWordPosition="517" endWordPosition="520">re not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Italian magistrates labeled his death a suicide N/N N ((S[dcl]\NP)/NP)/NP NP[nb]/N N NP[nb]/N N � � � N NP NP � NP (S[dcl]\NP)/NP � S[dcl]\NP S[dcl] Figure 1</context>
<context position="4943" citStr="Clark and Curran (2009)" startWordPosition="769" endWordPosition="772"> adjacent categories can combine. Figure 1 uses function application, where a complex category consumes an adjacent argument to form its result, e.g. S[dcl]\NP combines with the NP to its left to form an S[dcl]. More powerful combinators allow categories to combine with greater flexibility. We cannot form a PTB tree by simply relabeling the categories in a CCG derivation because the mapping to phrase labels is many-to-many, CCG derivations contain extra brackets due to binarisation, and there are cases where the constituents in the PTB tree and the CCG derivation cross (e.g. in Figure 1). 2.2 Clark and Curran (2009) Clark and Curran (2009), hereafter C&amp;C-CONV, assign a schema to each leaf (lexical category) and rule (pair of combining categories) in the CCG derivation. The PTB tree is constructed from the CCG bottomup, creating leaves with lexical schemas, then merging/adding sub-trees using rule schemas at each step. The schemas for Figure 1 are shown in Table 1. These apply to create NPs over magistrates, death, and suicide, and a VP over labeled, and then combine the trees by placing one under the other at each step, and finally create an S node at the root. C&amp;C-CONV has sparsity problems, requiring s</context>
</contexts>
<marker>Clark, Curran, 2009</marker>
<rawString>Stephen Clark and James R. Curran. 2009. Comparing the accuracy of CCG and penn treebank parsers. In Proceedings ofACL, pages 53–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="10640" citStr="Collins (1997)" startWordPosition="1742" endWordPosition="1743"> &lt; 40) 95.85 94.39 95.12 42.1 CONV 23 (all) 95.33 93.95 94.64 39.7 23 (len &lt; 40) 95.44 94.04 94.73 41.9 00 (all) 96.69 96.58 96.63 51.1 This 00 (len &lt; 40) 96.98 96.77 96.87 53.6 Work 23 (all) 96.49 96.11 96.30 51.4 23 (len &lt; 40) 96.57 96.21 96.39 53.8 Table 4: PARSEVAL Precision, Recall, F-Score, and exact sentence match for converted gold CCG derivations. unannotated categories, we use the instructions of the result category with an added instruction. Table 4 compares our approach with C&amp;C-CONV on gold CCG derivations. The results shown are as reported by EVALB (Abney et al., 1991) using the Collins (1997) parameters. Our approach leads to increases on all metrics of at least 1.1%, and increases exact sentence match by over 11% (both absolute). Many of the remaining errors relate to missing and extra clause nodes and a range of rare structures, such as QPs, NACs, and NXs. The only other prominent errors are single word spans, e.g. extra or missing ADVPs. Many of these errors are unrecoverable from CCGbank, either because inconsistencies in the PTB have been smoothed over or because they are genuine but rare constructions that were lost. 4.1 Parser Comparison When we convert the output of a CCG </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings ofACL, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy A D Fowler</author>
<author>Gerald Penn</author>
</authors>
<title>Accurate context-free parsing with combinatory categorial grammar.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>335--344</pages>
<contexts>
<context position="14100" citStr="Fowler and Penn (2010)" startWordPosition="2327" endWordPosition="2330">g. PP attachment, are rarely converted incorrectly, and so we expect the number of errors to be cumulative. Some sentences are higher in the right plot than the left because there are distinctions in CCG that are not always present in the PTB, e.g. the argument-adjunct distinction. Table 5 presents F-scores for three PTB parsers and three CCG parsers (with their output converted by our method). One interesting comparison is between the PTB parser of Petrov and Klein (2007) and Sentences CLEAN ALL PROJ Converted gold CCG CCGbank 100.0 96.3 – Converted CCG Clark and Curran (2007) 90.9 85.5 88.8 Fowler and Penn (2010) 90.9 86.0 89.3 Auli and Lopez (2011) 91.7 86.2 89.5 Native PTB Klein and Manning (2003) 89.8 85.8 – Petrov and Klein (2007) 93.6 90.1 – Charniak and Johnson (2005) 94.8 91.5 – Table 5: F-scores on section 23 for PTB parsers and CCG parsers with their output converted by our method. CLEAN is only on sentences that are converted perfectly from gold CCG (51%). ALL is over all sentences. PROJ is a projected F-score (ALL result / CCGbank ALL result). the CCG parser of Fowler and Penn (2010), which use the same underlying parser. The performance gap is partly due to structures in the PTB that are n</context>
</contexts>
<marker>Fowler, Penn, 2010</marker>
<rawString>Timothy A. D. Fowler and Gerald Penn. 2010. Accurate context-free parsing with combinatory categorial grammar. In Proceedings ofACL, pages 335–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, The University of Edinburgh.</institution>
<contexts>
<context position="1061" citStr="Hockenmaier, 2003" startWordPosition="154" endWordPosition="155">ontrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser. 1 Introduction Converting the Penn Treebank (PTB, Marcus et al., 1993) to other formalisms, such as HPSG (Miyao et al., 2004), LFG (Cahill et al., 2008), LTAG (Xia, 1999), and CCG (Hockenmaier, 2003), is a complex process that renders linguistic phenomena in formalism-specific ways. Tools for reversing these conversions are desirable for downstream parser use and parser comparison. However, reversing conversions is difficult, as corpus conversions may lose information or smooth over PTB inconsistencies. Clark and Curran (2009) developed a CCG to PTB conversion that treats the CCG derivation as a phrase structure tree and applies hand-crafted rules to every pair of categories that combine in the derivation. Because their approach does not exploit the generalisations inherent in the CCG for</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and models for statistical parsing with Combinatory Categorial Grammar. Ph.D. thesis, School of Informatics, The University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="14188" citStr="Klein and Manning (2003)" startWordPosition="2343" endWordPosition="2346">ors to be cumulative. Some sentences are higher in the right plot than the left because there are distinctions in CCG that are not always present in the PTB, e.g. the argument-adjunct distinction. Table 5 presents F-scores for three PTB parsers and three CCG parsers (with their output converted by our method). One interesting comparison is between the PTB parser of Petrov and Klein (2007) and Sentences CLEAN ALL PROJ Converted gold CCG CCGbank 100.0 96.3 – Converted CCG Clark and Curran (2007) 90.9 85.5 88.8 Fowler and Penn (2010) 90.9 86.0 89.3 Auli and Lopez (2011) 91.7 86.2 89.5 Native PTB Klein and Manning (2003) 89.8 85.8 – Petrov and Klein (2007) 93.6 90.1 – Charniak and Johnson (2005) 94.8 91.5 – Table 5: F-scores on section 23 for PTB parsers and CCG parsers with their output converted by our method. CLEAN is only on sentences that are converted perfectly from gold CCG (51%). ALL is over all sentences. PROJ is a projected F-score (ALL result / CCGbank ALL result). the CCG parser of Fowler and Penn (2010), which use the same underlying parser. The performance gap is partly due to structures in the PTB that are not recoverable from CCGbank, but probably also indicates that the split-merge model is l</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A dependency-based method for evaluating broad-coverage parsers.</title>
<date>1998</date>
<journal>Natural Language Engineering,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="2975" citStr="Lin (1998)" startWordPosition="459" endWordPosition="460">and Curran’s approach, we require no rules that consider non-local features of constituents, which enables the possibility of simple integration with a CKY-based parser. The most common errors our approach makes involve nodes for clauses and rare spans such as QPs, NXs, and NACs. Many of these errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one re</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. A dependency-based method for evaluating broad-coverage parsers. Natural Language Engineering, 4(2):97–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="932" citStr="Marcus et al., 1993" startWordPosition="130" endWordPosition="133">edu.au Abstract We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser. 1 Introduction Converting the Penn Treebank (PTB, Marcus et al., 1993) to other formalisms, such as HPSG (Miyao et al., 2004), LFG (Cahill et al., 2008), LTAG (Xia, 1999), and CCG (Hockenmaier, 2003), is a complex process that renders linguistic phenomena in formalism-specific ways. Tools for reversing these conversions are desirable for downstream parser use and parser comparison. However, reversing conversions is difficult, as corpus conversions may lose information or smooth over PTB inconsistencies. Clark and Curran (2009) developed a CCG to PTB conversion that treats the CCG derivation as a phrase structure tree and applies hand-crafted rules to every pair </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Comparative parser performance analysis across grammar frameworks through automatic tree conversion using synchronous grammars.</title>
<date>2008</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>545--552</pages>
<contexts>
<context position="3247" citStr="Matsuzaki and Tsujii, 2008" startWordPosition="499" endWordPosition="502">ch as QPs, NXs, and NACs. Many of these errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Italian magistrates labeled his death a suicide N/N N ((</context>
</contexts>
<marker>Matsuzaki, Tsujii, 2008</marker>
<rawString>Takuya Matsuzaki and Jun’ichi Tsujii. 2008. Comparative parser performance analysis across grammar frameworks through automatic tree conversion using synchronous grammars. In Proceedings of Coling, pages 545–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the penn treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>684--693</pages>
<contexts>
<context position="987" citStr="Miyao et al., 2004" startWordPosition="140" endWordPosition="143"> for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser. 1 Introduction Converting the Penn Treebank (PTB, Marcus et al., 1993) to other formalisms, such as HPSG (Miyao et al., 2004), LFG (Cahill et al., 2008), LTAG (Xia, 1999), and CCG (Hockenmaier, 2003), is a complex process that renders linguistic phenomena in formalism-specific ways. Tools for reversing these conversions are desirable for downstream parser use and parser comparison. However, reversing conversions is difficult, as corpus conversions may lose information or smooth over PTB inconsistencies. Clark and Curran (2009) developed a CCG to PTB conversion that treats the CCG derivation as a phrase structure tree and applies hand-crafted rules to every pair of categories that combine in the derivation. Because t</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2004</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii. 2004. Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the penn treebank. In Proceedings of IJCNLP, pages 684–693.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="13955" citStr="Petrov and Klein (2007)" startWordPosition="2302" endWordPosition="2305">hort sentences on which the C&amp;C parser uses categories that lead to one extra correct node. The main constructions on which parse errors occur, e.g. PP attachment, are rarely converted incorrectly, and so we expect the number of errors to be cumulative. Some sentences are higher in the right plot than the left because there are distinctions in CCG that are not always present in the PTB, e.g. the argument-adjunct distinction. Table 5 presents F-scores for three PTB parsers and three CCG parsers (with their output converted by our method). One interesting comparison is between the PTB parser of Petrov and Klein (2007) and Sentences CLEAN ALL PROJ Converted gold CCG CCGbank 100.0 96.3 – Converted CCG Clark and Curran (2007) 90.9 85.5 88.8 Fowler and Penn (2010) 90.9 86.0 89.3 Auli and Lopez (2011) 91.7 86.2 89.5 Native PTB Klein and Manning (2003) 89.8 85.8 – Petrov and Klein (2007) 93.6 90.1 – Charniak and Johnson (2005) 94.8 91.5 – Table 5: F-scores on section 23 for PTB parsers and CCG parsers with their output converted by our method. CLEAN is only on sentences that are converted perfectly from gold CCG (51%). ALL is over all sentences. PROJ is a projected F-score (ALL result / CCGbank ALL result). the </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACL, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3284" citStr="Sarkar, 2001" startWordPosition="507" endWordPosition="508">inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Italian magistrates labeled his death a suicide N/N N ((S[dcl]\NP)/NP)/NP NP[nb]/N N NP[nb]/N</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings of NAACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3457" citStr="Steedman, 2000" startWordPosition="536" endWordPosition="537">arison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Italian magistrates labeled his death a suicide N/N N ((S[dcl]\NP)/NP)/NP NP[nb]/N N NP[nb]/N N � � � N NP NP � NP (S[dcl]\NP)/NP � S[dcl]\NP S[dcl] Figure 1: A crossing constituents example: his... suicide (PTB) crosses labeled... death (CCGbank). Categories Schema</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
</authors>
<title>Converting dependency structures to phrase structures.</title>
<date>2001</date>
<booktitle>In Proceedings ofHLT,</booktitle>
<pages>1--5</pages>
<contexts>
<context position="3156" citStr="Xia and Palmer, 2001" startWordPosition="485" endWordPosition="488">he most common errors our approach makes involve nodes for clauses and rare spans such as QPs, NXs, and NACs. Many of these errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Associa</context>
</contexts>
<marker>Xia, Palmer, 2001</marker>
<rawString>Fei Xia and Martha Palmer. 2001. Converting dependency structures to phrase structures. In Proceedings ofHLT, pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Owen Rambow</author>
<author>Rajesh Bhatt</author>
<author>Martha Palmer</author>
<author>Dipti Misra Sharma</author>
</authors>
<title>Towards a multirepresentational treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 7th International Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>159--170</pages>
<contexts>
<context position="3175" citStr="Xia et al., 2009" startWordPosition="489" endWordPosition="492">our approach makes involve nodes for clauses and rare spans such as QPs, NXs, and NACs. Many of these errors are inconsistencies in the original PTB annotations that are not recoverable. These issues make evaluating parser output difficult, but our method does enable an improved comparison of CCG and PTB parsers. 2 Background There has been extensive work on converting parser output for evaluation, e.g. Lin (1998) and Briscoe et al. (2002) proposed using underlying dependencies for evaluation. There has also been work on conversion to phrase structure, from dependencies (Xia and Palmer, 2001; Xia et al., 2009) and from lexicalised formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008) and TAG (Chiang, 2000; Sarkar, 2001). Our focus is on CCG to PTB conversion (Clark and Curran, 2009). 2.1 Combinatory Categorial Grammar (CCG) The lower half of Figure 1 shows a CCG derivation (Steedman, 2000) in which each word is assigned a category, and combinatory rules are applied to adjacent categories until only one remains. Categories 105 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105–109, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computatio</context>
</contexts>
<marker>Xia, Rambow, Bhatt, Palmer, Sharma, 2009</marker>
<rawString>Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer, and Dipti Misra Sharma. 2009. Towards a multirepresentational treebank. In Proceedings of the 7th International Workshop on Treebanks and Linguistic Theories, pages 159–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Extracting tree adjoining grammars from bracketed corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>398--403</pages>
<contexts>
<context position="1032" citStr="Xia, 1999" startWordPosition="150" endWordPosition="151">structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser. 1 Introduction Converting the Penn Treebank (PTB, Marcus et al., 1993) to other formalisms, such as HPSG (Miyao et al., 2004), LFG (Cahill et al., 2008), LTAG (Xia, 1999), and CCG (Hockenmaier, 2003), is a complex process that renders linguistic phenomena in formalism-specific ways. Tools for reversing these conversions are desirable for downstream parser use and parser comparison. However, reversing conversions is difficult, as corpus conversions may lose information or smooth over PTB inconsistencies. Clark and Curran (2009) developed a CCG to PTB conversion that treats the CCG derivation as a phrase structure tree and applies hand-crafted rules to every pair of categories that combine in the derivation. Because their approach does not exploit the generalisa</context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>Fei Xia. 1999. Extracting tree adjoining grammars from bracketed corpora. In Proceedings of the Natural Language Processing Pacific Rim Symposium, pages 398– 403.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>