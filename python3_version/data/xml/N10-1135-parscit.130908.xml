<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000213">
<title confidence="0.998578">
Cross-lingual Induction of Selectional Preferences
with Bilingual Vector Spaces
</title>
<author confidence="0.992277">
Yves Peirsman Sebastian Padó
</author>
<affiliation confidence="0.995517">
QLVL, University of Leuven IMS, University of Stuttgart
</affiliation>
<email confidence="0.6591765">
Research Foundation – Flanders (FWO) pado@ims.uni-stuttgart.de
yves.peirsman@arts.kuleuven.be
</email>
<sectionHeader confidence="0.993655" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9975482">
We describe a cross-lingual method for the in-
duction of selectional preferences for resource-
poor languages, where no accurate monolin-
gual models are available. The method uses
bilingual vector spaces to “translate” foreign
language predicate-argument structures into
a resource-rich language like English. The
only prerequisite for constructing the bilin-
gual vector space is a large unparsed corpus
in the resource-poor language, although the
model can profit from (even noisy) syntactic
knowledge. Our experiments show that the
cross-lingual predictions correlate well with
human ratings, clearly outperforming monolin-
gual baseline models.
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999746470588235">
Selectional preferences capture the empirical observa-
tion that not all words are equally good arguments to
a given verb in a particular argument position (Wilks,
1975; Resnik, 1996). For instance, the subjects of
the English verb to shoot are generally people, while
the direct objects can be people or animals. This is
reflected in speakers’ intuitions. Table 1 shows that
the combination the hunter shot the deer is judged
more plausible than the deer shot the hunter. Selec-
tional preferences do not only play an important role
in human sentence processing (McRae et al., 1998),
but are also helpful for NLP tasks like word sense
disambiguation (McCarthy and Carroll, 2003) and
semantic role labeling (Gildea and Jurafsky, 2002).
Computational models of selectional preferences
predict such plausibilities for triples of a predicate p,
an argument position a, and a head word h, such as
</bodyText>
<table confidence="0.926415">
Predicate Relation Noun Plausibility
shoot subject hunter 6.9
shoot object hunter 2.8
shoot subject deer 1.0
shoot object deer 6.4
</table>
<tableCaption confidence="0.9735105">
Table 1: Predicate-relation-noun triples with human plau-
sibility judgments on a 7-point scale (McRae et al., 1998)
</tableCaption>
<bodyText confidence="0.999828296296296">
(shoot,object,hunter). All recent models take a two-
step approach: (1), they extract all triples (p, a, h)
from a large corpus; (2), they apply some type of
generalization to make predictions for unseen items.
Clearly, the accuracy of these models relies crucially
on the quality and coverage of the extracted triples,
and thus on the syntactic analysis of the corpus. Un-
fortunately, corpora that are both large enough and
have a very good syntactic analysis are only available
for a handful of Western and Asian languages, which
leaves all other languages without reliable selectional
preference models.
In this paper, we propose a cross-lingual knowl-
edge transfer approach to this problem: We automat-
ically translate triples (p, a, h) from resource-poor
languages into English, where large and high-quality
parsed corpora are available and we can compute a
reliable plausibility estimate. The translations are
extracted from a bilingual semantic space, which can
be constructed via bootstrapping from large unparsed
corpora in the two languages, without the need for
parallel corpora or bilingual lexical resources.
Structure of the paper. Section 2 reviews models
for selectional preferences. In Section 3, we describe
our approach. Section 4 introduces our experimental
setup, and Sections 5 and 6 present and discuss our
experiments. Section 7 wraps up.
</bodyText>
<page confidence="0.962926">
921
</page>
<note confidence="0.7870665">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 921–929,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.954611" genericHeader="method">
2 Selectional Preferences
</sectionHeader>
<bodyText confidence="0.999399">
The first broad-coverage model of selectional prefer-
ences was developed by Resnik (1996). To estimate
the plausibility of a triple (p, a, h), Resnik first ex-
tracted all head words seen with predicate p in posi-
tion a, Seena(p), from a corpus. He then used the
WordNet hierarchy to generalize over the head words
and to create predictions for unseen ones. A number
of studies has followed the same approach, exploring
different ways of using the structure of WordNet (Abe
and Li, 1996; Clark and Weir, 2002). While these
approaches show good results, they can only make
predictions for argument heads that are covered by
WordNet. This is already a problem for English, and
much more so in other languages, where comparable
resources are often much smaller or entirely absent.
A promising alternative approach is to derive
the generalizations from distributional informa-
tion (Prescher et al., 2000; Padó et al., 2007; Bergsma
et al., 2008). For example, the Padó et al. (2007)
model computes vector space representations for all
head words h and defines the plausibility of the triple
(p, a, h) as a weighted mean of the vector space simi-
larities between h and all h&apos; in Seena(p):
</bodyText>
<equation confidence="0.608022666666667">
� w(h&apos;)· sim(h, h&apos;) (1)
Pl(p, a, h) = Ehl w(h&apos;)
h&apos;ESee%(p)
</equation>
<bodyText confidence="0.999755666666667">
where w(h&apos;) is a weight, typically frequency.
In this model, the generalization is provided by dis-
tributional similarity, which can be computed from a
large corpus, without the need for additional lexical
resources. Padó et al. found it to outperform Resnik’s
approach in an evaluation against human plausibility
judgments. However, note that competitive results
are only obtained by representing the head words in
“syntactic” vector spaces whose dimensions consist
of context words with their syntactic relation to the
target rather than just context words. This is not sur-
prising: Presumably, hunter and deer share a domain
and are likely to have similar word-based context
distributions, even though they differ with regard to
their plausibility for particular predicate-argument
positions. Only when the vector space can capture
their different syntactic co-occurrence patterns can
the model predict different plausibilities.
</bodyText>
<figureCaption confidence="0.966382333333333">
Figure 1: Predicting selectional preferences for a source
language (e.g. German) by translating into a target lan-
guage (e.g. English) with a bilingual vector space.
</figureCaption>
<sectionHeader confidence="0.798532" genericHeader="method">
3 Cross-lingual selectional preferences
</sectionHeader>
<bodyText confidence="0.999845285714286">
In order to compute reliable selectional preference
representations, distributional models need to see
at least some head words for each (p, a) combina-
tion. Manually annotated treebank corpora, which
are becoming available for an increasing number of
languages, are too small for this task. We therefore
explore the idea of predicting the selectional pref-
erences for such languages by taking advantage of
large corpora with high-quality syntactic analyses in
resource-rich languages like English. This idea falls
into the general approach of cross-lingual knowledge
transfer (see e.g. Hwa et al., 2005). The application
to selectional preferences was suggested by Agirre et
al. (2003), who demonstrated its feasibility by man-
ual translation between Basque and English. We
extend their experiments to an automatic model that
predicts plausibility judgments in a resource-poor
language (source language) by exploiting a model in
a resource-rich language (target language).
Figure 1 sketches our method. We assume that
there is not enough high-quality data to build a mono-
lingual selectional preference model for the source
language (shown by dotted lines). However, we can
use a bilingual vector space, that is, a semantic space
in which words of both the source and the target
language are represented, to translate each source
language word s into the target language by identify-
ing its nearest (most similar) target word tr(s):
</bodyText>
<equation confidence="0.995971">
tr(s) = argmaxt sim(s, t) (2)
</equation>
<bodyText confidence="0.9743235">
Now we can use a target language selectional prefer-
ence model to obtain plausibilities for source triples:
</bodyText>
<equation confidence="0.980515">
Pls(p, a, h) = Plt(tr(p), a, tr(h)) (3)
</equation>
<bodyText confidence="0.996318">
where the superscript indicates the language.
</bodyText>
<figure confidence="0.994727444444444">
schie§en
shoot
deer
Hirsch
bilingual
vector space
monolingual
selectional
preference
model
(schie§en,obj,Hirsch)
German triple
monolingual
selectional
preference
model
(shoot,obj,deer)
English triple
</figure>
<page confidence="0.99333">
922
</page>
<bodyText confidence="0.982888285714286">
Eq. (3) gives rise to three questions: (1), How can
we construct the bilingual space to model tr? (2), Is
translating actually the appropriate way of transfer-
ring selectional preferences? (3), Is it reasonable to
retain the source language argument positions like
subject or object? The following subsections discuss
(1) and (2); we will address (3) in Sections 5 and 6.
</bodyText>
<subsectionHeader confidence="0.999845">
3.1 Bilingual Vector Spaces
</subsectionHeader>
<bodyText confidence="0.999920333333333">
Bilingual vector spaces are vector spaces in which
words from two languages are represented (cf. Fig. 2).
The dimensions of this space are labeled with bilin-
gual context word pairs (like secretly/heimlich and
rifle/Gewehr for German–English) that are mutual
translations. By treating such context word pairs as
single dimensions, the vector space can represent tar-
get words from both languages, counting the target
words’ co-occurrences with the context words from
the respective language. In other words, a source-
target word pair (s, t) will be assigned similar vectors
in the semantic space if the context words of s are
translations of the context words of t. Cross-lingual
semantic similarity between words can be measured
using standard vector space similarity (Lee, 1999).
Importantly, bilingual vector spaces can be built
on the basis of co-occurrences drawn from two un-
related corpora for the source and target languages.
Their construction does not require resources such
as parallel corpora or bilingual translation lexicons,
which might not be available for resource-poor source
languages. Where parallel corpora exist, they often
cover specific domains (e.g., politics), while many
bilingual lexicons are prone to ambiguity problems.
The main challenge in constructing bilingual vec-
tor spaces is determining the set of dimensions,
i.e., bilingual word pairs, using as little knowledge as
possible. Most often, such pairs are extracted from
small bilingual lexicons (Fung and McKeown, 1997;
Rapp, 1999; Chiao and Zweigenbaum, 2002). As
mentioned above, such resources might not be avail-
able. We thus follow an alternative approach by using
frequent cognates, words that are shared between the
two languages (Markó et al., 2005). Cognates can
be extracted by simple string matching between the
corpora, and mostly share their meaning (Koehn and
Knight, 2002). However, they account for (at most) a
small percentage of all interesting translation pairs.
To extend the set of dimensions available for the
</bodyText>
<figureCaption confidence="0.9977055">
Figure 2: Sketch of a bilingual vector space for English
(solid dots) and German (empty circles).
</figureCaption>
<bodyText confidence="0.999511818181818">
bilingual space, we use these cognates merely as a
starting point for a bootstrapping process: We build
a bilingual vector space with the initial word pairs as
dimensions, and identify nearest neighbors between
the two languages in the space. These are added as
dimensions of the bilingual space, and the process
is repeated. Since the focus is on identifying reli-
able source-target word pairs rather than complete
coverage as in Eq. (2), we adopt a symmetrical defi-
nition of translation that pairs up only mutual nearest
neighbors, and allows words to remain untranslated:1
</bodyText>
<equation confidence="0.992651">
trsym(s) = t iff tr(s) = t and tr(t) = s (4)
</equation>
<bodyText confidence="0.999916666666667">
From the second iteration onward, this process intro-
duces dimensions that are not identical graphemes,
such as Kind–child and Geschwindigkeit–speed, and
is iterated until convergence. Since each word of
either language can only participate in at most one
dimension, dimensions acquired in later steps can cor-
rect wrong pairs from previous steps, like the “false
friend” German Kind ‘child’ – English kind, which
is part of the initial set of cognates.
</bodyText>
<subsectionHeader confidence="0.999457">
3.2 Translation and Selectional Preferences
</subsectionHeader>
<bodyText confidence="0.999958727272727">
As Figure 1 shows, the easiest way of exploiting a
bilingual semantic space is to identify for each source
word the target language word with the highest se-
mantic similarity. For example, in Figure 2, the best
translation of German schießen is its English nearest
neighbor, shoot. However, it is risky to rely on the
single nearest neighbor – it might simply be wrong.
Even if it is correct, data sparsity is an issue: The
translations may be infrequent in the target language,
or the two translations of p and h may form unlikely
collocates for target language-internal reasons (like
</bodyText>
<footnote confidence="0.570226">
1To avoid unreliable vectors, we also adopt only the 50%
most frequent of the tr,,. pairs. Frequency is defined as the
geometric mean of the two words’ monolingual frequencies.
</footnote>
<figure confidence="0.997391888888889">
secretly/
heimlich
anschleichen
stalk
schie§en
shoot
hit
rifle/
Gewehr
</figure>
<page confidence="0.995435">
923
</page>
<bodyText confidence="0.999984205128205">
difference in register) that do not reflect plausibility.
A third issue are monolingual semantic phenomena
like polysemy and idioms: The implausible German
triple (schief3en,obj,Brise) will be judged as very plau-
sible due to the English idiom to shoot the breeze.
A look at the broader neighborhood of schief3en
suggests that its second and third-best English neigh-
bors, hit, and stalk, can be used to smooth plausibility
estimates for schief3en. Instead of translating source
language words by their single nearest neighbor, we
will take its k nearest neighbors into account. This
is defensible also from a more fundamental point of
view, which suggests that the cross-lingual transfer of
selectional preferences does not require literal trans-
lation in order to work. First, ontological models
like Resnik’s assume that synonymous words behave
similarly with respect to selectional preferences. Sec-
ond, recent work by Chambers and Jurafsky (2009)
has induced “narrative chains”, i.e., likely sequences
of events, by their use of similar head words. Thus,
we expect that all k nearest neighbors of a source
predicate s are informative for the selectional prefer-
ences of s (like schief3en) as long as they are either
synonyms of its literal translation (shoot/hit) or come
from the same narrative chain (stalk/kill/. .. ).
It is also clear that smoothing does not always
equate better predictions. Closeness in a word-based
vector space can also just reflect semantic association.
For example, Spanish tenista ‘tennis player’ is highly
associated with English tennis, but is a bad translation
in terms of selectional preferences. We assume that
this problem is more acute for nouns than for verbs:
The context of verbs is dominated by their arguments,
which is not true for nouns. Consequently, close
nouns in vector space can differ widely in ontological
type, while close verbs generally have one or more
similar argument slots. In our model, we will thus
consider several verb translations, but just the best
head word translation. For details, see Section 5.
</bodyText>
<sectionHeader confidence="0.998212" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99833812">
Our evaluation uses English as the target language
and two source languages: German (as a very close
neighbor of English) and Spanish (as a more distant
one). Neither of these languages are really resource-
poor, but they allow us to compare our cross-lingual
model against monolingual models, to emulate dif-
ferent levels of “resource poorness” and to examine
the model’s learning curve.
Plausibility Data. For German, we used the plau-
sibility judgments collected by Brockmann (2002).
The dataset contains human judgments for ninety
triples sampled from the manually annotated 1 mil-
lion word TiGer corpus (Brants et al., 2002): ten
verbs with three argument positions (subject [SUBJ],
direct object [DOBJ], and oblique (prepositional) ob-
ject [POBJ]) combined with three head words. Mod-
els are evaluated against such datasets by correlating
predicted plausibilities with the (not normally dis-
tributed) human judgments using Spearman’s p, a
non-parametric rank-order correlation coefficient.
We constructed a similar 90-triple data set for
Spanish by sampling triples from two Spanish cor-
pora (see below) using Brockmann’s (2002) crite-
ria. Human judgments for the triples were collected
through the Amazon Mechanical Turk (AMT) crowd-
sourcing platform (Snow et al., 2008). We asked
native speakers of Spanish to rate the plausibility of
a simple sentence with the relevant verb-argument
combination on a five-point Likert scale, obtaining
between 12 and 17 judgments for each triple. For
each datapoint, we removed the single lowest and
highest judgments and computed the mean. We as-
sessed the reliability of our data by replicating Brock-
mann’s experiment for German with our AMT setup.
With a Spearman p of almost .90, our own judgments
correlate very well with Brockmann’s original data.
Monolingual Prior Work and Baselines. For
German, Brockmann and Lapata (2003) evaluated
ontology-based models trained on TiGer triples and
the GermaNet ontology. The results in Table 2 show
that while both models are able to predict the data
significantly, neither of the models can predict all of
the data. We attribute this to the small size of TiGer.2
To gauge the limits of monolingual knowledge-
lean approaches, we constructed two monolingual
distributional models for German and Spanish ac-
cording to the Padó et al. (2007) model (Eq. (1)).
Recall that this model performs generalization in a
syntax-based vector space model. We computed vec-
tor spaces from dependency-parsed corpora for the
</bodyText>
<footnote confidence="0.814903">
2For each of the three argument positions and “all”, Brock-
mann and Lapata report the results for the best parametrization
of the models, which explains the apparently inconsistent results.
</footnote>
<page confidence="0.962598">
924
</page>
<table confidence="0.9994178">
Resnik Clark &amp; Weir
SUBJ .408* .268
DOBJ .430* .611***
POBJ .330 .597***
all .374*** .232*
</table>
<tableCaption confidence="0.989297">
Table 2: Monolingual baselines 1. Spearman correla-
tions for ontology-based models in German as reported by
Brockmann and Lapata (2003). *: p &lt; .05; ***: p &lt; .001
</tableCaption>
<table confidence="0.999766714285714">
Lang. German Spanish
Corpus Schulte’s HGC AnCora Encarta
P Cov. p Cov. p Cov.
SUBJ .34† 90% .44* 80% .14 100%
DOBJ .51** 97% .29 83% -.05 100%
POBJ .41* 93% -.03 100% — —3
all .33** 93% .16 88% .11 67%
</table>
<tableCaption confidence="0.995968333333333">
Table 3: Monolingual baselines 2. Spearman correlation
and coverage for distributional models. † : p &lt; .1; *: p &lt;
.05; **: p &lt; .01.
</tableCaption>
<bodyText confidence="0.999970407407408">
two languages, using the 2,000 most frequent lemma-
dependency relation pairs as dimensions and adopt-
ing the popular pointwise mutual information metric
as co-occurrence statistic. For German, we used
Schulte im Walde’s verb frame resource (Schulte im
Walde et al., 2001), which contains the frequency of
triples calculated from probabilistic parses of 30M
words from the Huge German Corpus (HGC) of
newswire. For Spanish, we consulted two syntac-
tically analyzed corpora: the AnCora (Taulé et al.,
2008) and the Encarta corpus (Calvo et al., 2005). At
0.5M words, the AnCora corpus is small, but man-
ually annotated, whereas the larger, automatically
parsed Encarta corpus amounts to over 18M tokens.
Table 3 shows the results for the distributional
monolingual models. For German, we get significant
correlations for DOBJ and POBJ, an almost signif-
icant correlation for SUBJs, and high significance
for the complete dataset (p &lt; 0.01). These figures
rival the performance of the ontological models (cf.
Table 2), without using ontological information. For
Spanish, the only significant correlation with human
judgments is obtained for subjects, the most frequent
argument position, with the clean AnCora data. An-
Cora is presumably too sparse for the other argument
positions. The large Encarta corpus, in turn, is very
noisy, supporting our concerns from Section 2.
</bodyText>
<footnote confidence="0.733519">
3Since the Encarta data consists of individual dependency
</footnote>
<table confidence="0.968155">
n noun adj verb all
German 7340 .61 .57 .43 .56
Spanish 4143 .62 .67 .41 .58
</table>
<tableCaption confidence="0.9899825">
Table 4: First-translation accuracy for German-English
and Spanish-English translation (n: size of gold standard).
</tableCaption>
<bodyText confidence="0.997019763157895">
Cross-lingual Selectional Preferences. Our archi-
tecture for the cross-lingual prediction of selectional
preferences shown in Figure 1 consists of two com-
ponents, namely the bilingual vector space and a
selectional preference model in the target language.
As our English selectional preference model, we
again use the Padó et al. (2007) model, trained on
a version of the BNC parsed with MINIPAR (Lin,
1993). The parameters of the syntactic vector space
were the same as for the monolingual baseline mod-
els. The bilingual vector spaces were constructed
from three large, unparsed, comparable monolin-
gual corpora. For German, we used the HGC de-
scribed above. For Spanish, we obtained a corpus
with around 100M words, consisting of 2.5 years of
crawled text from two major Spanish newspapers.
For English, we used the BNC.
We first constructed initial sets of bilingual labels.
For German–English, we identified 1064 graphem-
ically identical word pairs that occurred more than
4 times per million words. Due to the larger lex-
ical distance between Spanish and English, there
are fewer graphemically identical tokens for this lan-
guage pair. We therefore applied a Porter stemmer
and found 2104 identical stems, at a higher risk of
“false friends”. We then applied the bootstrapping
cycle from Section 3.1. The set of dimensions con-
verged after around five iterations.
We evaluated the (asymmetric) nearest neighbor
pairs from the final spaces, (s, tr(s)), against two
online dictionaries.4 Table 4 shows that 55% to 60%
of the pairs are listed in the dictionaries, with parallel
tendencies for both language pairs. The bilingual
space performs fairly well for nouns and adjectives,
but badly for verbs, which is a well-known weakness
of distributional models (Peirsman et al., 2008).
Even taking into account the incompleteness of
dictionaries, this looks like a negative result: more
</bodyText>
<footnote confidence="0.949387333333333">
relations rather than trees, we could not model the POBJ data.
4DE-EN: www.dict.cc; ES-EN: www.freelang.net.
Pairs (s, tr(s)) were only evaluated if the dictionary listed s.
</footnote>
<page confidence="0.997944">
925
</page>
<bodyText confidence="0.983558574468085">
than half of all verb translations are incorrect. How-
ever, following up on our intuitions from Section 3.2,
we performed an analysis of the “incorrect” transla-
tions. It revealed that many of the errors in Table 4
are informative, semantically related words. Near-
est neighbor target language verbs in particular tend
to represent the same event type and take the same
kinds of arguments as the source verb. Examples
are German gefährden ‘threaten’ – English affect,
and German Neugier ‘curiosity’ – English enthusi-
asm. We concluded that literal translation quality is
a misleading figure of merit for our task.
Experimental rationale. Section 3 introduced one
major design decision of our model: the question of
how to treat the argument position, which cannot
be translated by the bilingual vector space, in the
cross-lingual transfer. We present two experiments
that investigate the model’s behavior in the absence
and presence of knowledge about argument positions.
Experiment 1 uses no syntactic knowledge about the
source language whatsoever. In this situation, the
best we can do is to assume that source language
argument positions like SUBJ will correspond to the
same argument position in the target language. Exper-
iment 2 attempts to identify, for each source language
argument position, the “best fit” position in the target
language. This results in better plausibility estimates,
but also means that we need at least some syntac-
tic information about the source language. In both
experiments, we vary the number of translations we
consider for each verb.
5 Exp. 1: Induction without syntactic
knowledge in the source language
This experiment assumes that argument positions
simply carry over between languages. While this
assumption clearly simplifies linguistic reality, it has
the advantage of not needing any syntactic informa-
tion about the source language. We thus model Ger-
man and Spanish SUBJ relations by English SUBJ
relations and DOBJs by DOBJs. In the case of (lex-
icalized) POBJs, where we cannot assume identity,
we compute plausibility scores for all English POBJs
that account for at least 10% of the predicate’s ar-
gument tokens, and select the PP with the highest
plausibility estimate. The k best “translations” of the
predicate p, trk(p), are turned into a single prediction
using maximization, yielding the final model:
</bodyText>
<equation confidence="0.9880675">
Plnosyn(p, a, h) = max Plt(pt, a, tr(h)) (5)
ptEtrk(p)
</equation>
<bodyText confidence="0.979517904761905">
Note that this model does not use any source lan-
guage information, except the bilingual vector space.
The results of Experiment 1 are given in Table 5
(coverage always 100%). For German, all predictions
correlate significantly with human ratings, and most
even at p &lt; 0.01, despite our naive assumption about
the cross-lingual argument position identity. The
results exceed both monolingual model types (onto-
logical, Tab. 2, and distributional, Tab. 3), notably
without the use of syntactic data. In particular, the
results for the POBJs, notoriously difficult to model
monolingually, are higher than for SUBJs or DOBJs.
We attribute this to the cross-lingual generalization
which takes all prepositional arguments into account.
The Spanish dataset is harder to model overall.
We obtain significantly high correlations for SUBJ,
but non-significant results for DOBJ and POBJ. This
corresponds well to the patterns for the monolingual
AnCora corpus (Table 3). However, we outperform
AnCora on the complete dataset, where it did not
achieve significance, while the cross-lingual model
does at p &lt; 0.01 — again, even without the use of
syntactic analyses. We attribute the overall lower
results compared to German to systematic syntactic
differences between English and Spanish. For exam-
ple, animate direct objects in Spanish are realized
as POBJs headed by the preposition a. Estimating
the plausibility of such objects by looking at English
POBJs is unlikely to yield good results. The use of
a larger number of verb translations yields a clear
increase in correlation for the German data, but in-
conclusive results for Spanish.
6 Exp. 2: Induction with syntactic
knowledge in the source language
As discussed in Section 3.2, verbs that are semanti-
cally similar in the bilingual vector space may very
well realize their (semantic) argument positions dif-
ferently in the surface syntax. For example, German
teilnehmen is correctly translated to English attend,
but the crucial event argument is realized differently,
namely as a POBJ headed by an in German and as
a DOBJ in English. To address this problem, we
</bodyText>
<page confidence="0.994309">
926
</page>
<table confidence="0.9998973">
DE 1-best 2-best 3-best 4-best 5-best
SUBJ .44* .47** .45* .47** .54**
DOBJ .39* .39* .52** .54** .55**
POBJ .58** .61** .61** .61** .62**
all .35** .37** .37** .38** .40**
ES 1-best 2-best 3-best 4-best 5-best
SUBJ .58** .64** .64** .58** .58**
DOBJ .13 .16 .11 .07 .07
POBJ .13 .13 .09 .14 .14
all .34** .36** .34** .32** .32**
</table>
<tableCaption confidence="0.9813605">
Table 5: Exp.1: Spearman correlation between syntaxless
cross-lingual model and human judgments for k best verb
translations. Best k for each argument position marked in
boldface. Coverage of all models: 100%.
</tableCaption>
<bodyText confidence="0.970312230769231">
learn a mapping function m that identifies the argu-
ment position at of a target language predicate pt
that corresponds best to an argument position a of a
predicate p in the source language. Our simple model
is in the same spirit as the cross-lingual plausibility
model itself: It returns the argument position at of
pt for which the seen head words of (p, a) are most
plausible when translated into the target language:5
Parallel to Eq. (5), the cross-lingual model is now:
Plt(pt, m(p, a, pt), tr(h))
(6)
This model can recover English argument positions
that correspond better to the original ones than the
identity mapping. For example, on our data, it discov-
ers the mapping for teilnehmen an/attend discussed
above. A second example concerns the incorrect, but
informative translation of stagnieren ‘stagnate’ as
boost. Here the model recognizes that the SUBJ of
stagnieren (the stagnating entity) corresponds to the
DOBJ of boost.
Establishing m requires syntactic information in
the source language, in order to obtain the set of
seen head words Seena3(ps). For this reason, Exp. 2
uses the parsed subset of the HGC (German), and the
AnCora and Encarta corpora (Spanish). The results
are shown in Table 6. We generally improve over
</bodyText>
<footnote confidence="0.984">
5To alleviate sparse data, we ignore argument positions of
English verbs that represent less than 10% of its argument tokens.
</footnote>
<table confidence="0.999911">
DE 1-best 2-best 3-best 4-best 5-best
SUBJ .55** .59** .49** .52** .54**
DOBJ .52** .52** .66** .66** .68**
POBJ .61** .68** .70** .69** .70**
all .41** .44** .44* .46** .48**
ES-A 1-best 2-best 3-best 4-best 5-best
SUBJ .52** .47* .42* .41* .42*
DOBJ .52*c .64**c .54*c .42*c .42*c
POBJ .32† .18 .13 .13 .24
all .47** .41** .36** .33** .37**
ES-E 1-best 2-best 3-best 4-best 5-best
SUBJ .40* .42* .39* .39* .41*
DOBJ .21 .02 .06 .13 .20
</table>
<tableCaption confidence="0.8222744">
Table 6: Exp.2: Spearman correlation between syntax-
aware cross-lingual model and human judgments for k
best verb translations. ES-A: AnCora corpus, ES-E: En-
carta corpus. Best k for each argument position in bold-
face. Coverage of all models: 100%, except c: 60%.
</tableCaption>
<bodyText confidence="0.9964662">
Exp. 1. For German, every single model now corre-
lates highly significantly with human judgments (p
&lt; 0.01), and the correlation for the complete dataset
increases from .40 to .48. For Spanish, we see very
good results for the AnCora corpus. Compared to
Exp. 1, we see a slight degradation for the SUBJs;
however, the correlations remain significant for all
values of k. Conversely, all predictions for DOBJs
are now significant,6 and the POBJs have improved at
least numerically, which validates our analysis of the
problems in Exp. 1. The best correlation for the com-
plete dataset improves from .36 to .47. The results
for the Encarta corpus disappoint, though. SUBJs
are significant, but worse than for AnCora, and the
DOBJs remain non-significant throughout. With re-
gard to increasing the number of verb translations,
Exp. 2 shows an almost universal benefit for Ger-
man, but still mixed results for Spanish, which may
indicate that verb translations for Spanish are still
“looser” than the German ones.
In fact, most remaining poor judgments are the
result of problematic translations, which stem from
three main sources. The first one is sparse data. Infre-
quent German and Spanish words often receive unre-
liable vector representations. Some examples are the
</bodyText>
<footnote confidence="0.528108666666667">
6Note, however, that AnCora has an imperfect coverage for
DOBJs (60%). This is because our Spanish dataset contains
verbs sampled from Encarta that do not occur in AnCora.
</footnote>
<equation confidence="0.984349">
m(p, a, pt) = argmax
at hESee%(p)
Plt(pt, at, tr(h))
P ls (p, a, h) = max
synptEtrk(p)
</equation>
<page confidence="0.981296">
927
</page>
<bodyText confidence="0.999928162162162">
German Tau (‘dew’, frequency of 180 in the HGC),
translated as alley, and Reifeprüfung (German SAT,
frequency 120), translated as affiliation. Both of these
may also be due to the difference in genre between
the HGC and the BNC. A second problem is formed
by nearest neighbors that are ontologically dissimi-
lar, as in the tenista ‘tennis player’/tennis example
from above. A final issue relates to limitations of the
Padó et al. (2007) model, whose architecture is sus-
ceptible to polysemy-related problems. For instance,
the Spanish combination (excavar, obj, terreno) was
judged by speakers as very plausible, but its English
equivalent (excavate, obj, land) is assigned a very
low score by the model. This might be due to the
fact that in the BNC, land occurs often in its political
meaning, and forms an outlier among the head words
for (excavate,obj).
How much syntactic information is necessary?
The syntax-aware model requires syntactic infor-
mation about the source language, which seems to
run counter to our original motivation of developing
methods for resource-poor languages. To address this
point, we analyzed the behavior of the syntax-aware
model for small syntactically analyzed corpora that
contained only at most m occurrences for each pred-
icate. We obtained the m occurrences by sampling
from the syntactically analyzed part of the HGC; if
fewer than m occurrences were present in the corpus,
we simply used these. Figure 3 shows the training
curve with 1 verb translation, averaged over n rounds
(n = 10 for 5 arguments, n = 5 for 10 arguments,
n = 4 for 20, 50 and 100 arguments). The general
picture is clear: most of the benefit of the syntactic
data is drawn form the first five occurrences for each
argument position. This shows that a small amount of
targeted syntactic annotation can improve the cross-
lingual model substantially.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99935375">
In this article, we have presented a first unsuper-
vised cross-lingual model of selectional preferences.
Our model proceeds by automatically translating
(predicate, argument position, head word) triples for
resource-poor source languages into a resource-rich
target language, where accurate selectional prefer-
ence models are available. The translation is based on
a bilingual vector space, which can be bootstrapped
</bodyText>
<figure confidence="0.9678865">
0 5 10 20 50 100
number of observed heads
</figure>
<figureCaption confidence="0.933564666666667">
Figure 3: Training curve for the bilingual German–English
model as a function of the number of observed head words
per argument position in the source language.
</figureCaption>
<bodyText confidence="0.9970031875">
from large unparsed corpora in the two languages.
Our results indicate that bilingual methods can go
a long way towards the modeling of selectional pref-
erences in resource-poor languages, where bilingual
lexicons, parallel corpora, or ontologies might not be
available. Our experiments have looked at German
and Spanish, where the cross-lingual models rival
and even exceed monolingual methods that typically
have to rely on small, clean “treebank”-style corpora
or large, very noisy, automatically parsed corpora.
We have also demonstrated that noisy syntactic data
from the source language can be integrated in our
model, where it helps improve the cross-lingual han-
dling of argument positions. The linguistic distance
between the languages can impact (1) the ability to
find accurate translations and (2) the degree of syntac-
tic overlap; nevertheless, as Agirre et al. (2003) show,
the transfer is possible even for unrelated languages.
In this paper, we have instantiated the selectional
preference model in the target language (English)
with the distributional model by Padó et al. (2007).
However, our approach is modular and can be com-
bined with any other selectional preference model.
We see two main avenues for future work: (1), The
construction of properly bilingual models where
source language information can also help to fur-
ther improve the target language model (Diab and
Resnik, 2002); (2), The extension of our cross-lingual
mapping for the argument position to mappings that
hold across multiple predicates as well as argument-
dependent mappings like the Spanish direct objects,
whose realization depends on their animacy.
</bodyText>
<figure confidence="0.996111181818182">
● SUBJ
DOBJ
POBJ
all
● ●
●
●
●
●
Spearman&apos;s rho
0.3 0.4 0.5 0.6 0.7
</figure>
<page confidence="0.982194">
928
</page>
<sectionHeader confidence="0.988573" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999899847619047">
Naoki Abe and Hang Li. 1996. Learning word association
norms using tree cut pair models. In Proc. ICML, pages
3–11, Bari, Italy.
Eneko Agirre, Izaskun Aldezabal, and Eli Pociello. 2003.
A pilot study of English selectional preferences and
their cross-lingual compatibility with Basque. In Proc.
TSD, pages 12–19, Brno, Czech Republic.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference from
unlabeled text. In Proc. EMNLP, pages 59–68, Hon-
olulu, HI.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proc. Workshop on Treebanks and Linguistic
Theories, Sozopol, Bulgaria.
Carsten Brockmann and Mirella Lapata. 2003. Evaluating
and combining approaches to selectional preference
acquisition. In Proc. EACL, pages 27–34, Budapest,
Hungary.
Carsten Brockmann. 2002. Evaluating and combining ap-
proaches to selectional preference acquisition. Master’s
thesis, Universität des Saarlandes, Saarbr icken.
Hiram Calvo, Alexander Gelbukh, and Adam Kilgarriff.
2005. Distributional thesaurus vs. wordnet: A compari-
son of backoff techniques for unsupervised PP attach-
ment. In Proc. CICLing, pages 177–188, Mexico City,
Mexico.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proc. ACL, pages 602–610, Singapore.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proc. COLING, pages
1–5, Taipei, Taiwan.
Stephen Clark and David Weir. 2002. Class-based proba-
bility estimation using a semantic hierarchy. Computa-
tional Linguistics, 28(2):187–206.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proc. ACL, pages 255–262, Philadelphia, PA.
Pascale Fung and Kathleen McKeown. 1997. Finding
terminology translations from non-parallel corpora. In
Proc. 3rd Annual Workshop on Very Large Corpora,
pages 192–202, Hong Kong.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics,
28(3):245–288.
Rebecca Hwa, Philipp Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311–325.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Proc.
ACL-02 Workshop on Unsupervised Lexical Acquisi-
tion, pages 9–16, Philadelphia, PA.
Lillian Lee. 1999. Measures of distributional similarity.
In Proc. ACL, pages 25–32, College Park, MD.
Dekang Lin. 1993. Principle-based parsing without over-
generation. In Proc. ACL, pages 112–120.
Kornél Markó, Stefan Schulz, Olena Medelyan, and Udo
Hahn. 2005. Bootstrapping dictionaries for cross-
language information retrieval. In Proc. SIGIR, pages
528–535, Seattle, WA.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically
acquired selectional preferences. Computational Lin-
guistics, 29(4):639–654.
Ken McRae, Michael Spivey-Knowlton, and Michael
Tanenhaus. 1998. Modeling the influence of thematic
fit (and other constraints) in on-line sentence compre-
hension. Journal of Memory and Language, 38:283–
312.
Sebastian Padó, Ulrike Padó, and Katrin Erk. 2007. Flex-
ible, corpus-based modelling of human plausibility
judgements. In Proc. EMNLP-CoNLL, pages 400–409,
Prague, Czech Republic.
Yves Peirsman, Kris Heylen, and Dirk Geeraerts. 2008.
Size matters. Tight and loose context definitions in
English word space models. In Proc. ESSLLI Workshop
on Lexical Semantics, pages 9–16, Hamburg, Germany.
Detlef Prescher, Stefan Riezler, and Mats Rooth. 2000.
Using a probabilistic class-based lexicon for lexical
ambiguity resolution. In Proc. COLING, pages 649–
655, Saarbr icken, Germany.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proc. ACL, pages 519–526, College Park, MD.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational real-
ization. Cognition, 61:127–159.
Sabine Schulte im Walde, Helmut Schmid, Mats Rooth,
Stefan Riezler, and Detlef Prescher. 2001. Statistical
Grammar Models and Lexicon Acquisition. In Linguis-
tic Form and its Computation, pages 389–440. CSLI
Publications, Stanford, CA.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast – but is it good?
Evaluating non-expert annotations for natural language
tasks. In Proc. EMNLP, pages 254–263, Honolulu, HI.
Mariona Taulé, M. Antònia Martí, and Marta Recasens.
2008. Ancora: Multilevel annotated corpora for Cata-
lan and Spanish. In Proc. LREC, Marrakech, Morocco.
Yorick Wilks. 1975. Preference semantics. In E. Keenan,
editor, Formal Semantics of Natural Language. Cam-
bridge University Press.
</reference>
<page confidence="0.998553">
929
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.334735">
<title confidence="0.996277">Cross-lingual Induction of Selectional with Bilingual Vector Spaces</title>
<author confidence="0.993462">Yves Peirsman Sebastian Padó</author>
<affiliation confidence="0.929067">QLVL, University of Leuven IMS, University of</affiliation>
<note confidence="0.553642">Foundation – Flanders (FWO)</note>
<email confidence="0.774206">yves.peirsman@arts.kuleuven.be</email>
<abstract confidence="0.9980979375">We describe a cross-lingual method for the induction of selectional preferences for resourcepoor languages, where no accurate monolingual models are available. The method uses bilingual vector spaces to “translate” foreign language predicate-argument structures into a resource-rich language like English. The only prerequisite for constructing the bilingual vector space is a large unparsed corpus in the resource-poor language, although the model can profit from (even noisy) syntactic knowledge. Our experiments show that the cross-lingual predictions correlate well with human ratings, clearly outperforming monolingual baseline models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Naoki Abe</author>
<author>Hang Li</author>
</authors>
<title>Learning word association norms using tree cut pair models.</title>
<date>1996</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>3--11</pages>
<location>Bari, Italy.</location>
<contexts>
<context position="4127" citStr="Abe and Li, 1996" startWordPosition="620" endWordPosition="623">he ACL, pages 921–929, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics 2 Selectional Preferences The first broad-coverage model of selectional preferences was developed by Resnik (1996). To estimate the plausibility of a triple (p, a, h), Resnik first extracted all head words seen with predicate p in position a, Seena(p), from a corpus. He then used the WordNet hierarchy to generalize over the head words and to create predictions for unseen ones. A number of studies has followed the same approach, exploring different ways of using the structure of WordNet (Abe and Li, 1996; Clark and Weir, 2002). While these approaches show good results, they can only make predictions for argument heads that are covered by WordNet. This is already a problem for English, and much more so in other languages, where comparable resources are often much smaller or entirely absent. A promising alternative approach is to derive the generalizations from distributional information (Prescher et al., 2000; Padó et al., 2007; Bergsma et al., 2008). For example, the Padó et al. (2007) model computes vector space representations for all head words h and defines the plausibility of the triple </context>
</contexts>
<marker>Abe, Li, 1996</marker>
<rawString>Naoki Abe and Hang Li. 1996. Learning word association norms using tree cut pair models. In Proc. ICML, pages 3–11, Bari, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Izaskun Aldezabal</author>
<author>Eli Pociello</author>
</authors>
<title>A pilot study of English selectional preferences and their cross-lingual compatibility with Basque. In</title>
<date>2003</date>
<booktitle>Proc. TSD,</booktitle>
<pages>12--19</pages>
<location>Brno, Czech Republic.</location>
<contexts>
<context position="6701" citStr="Agirre et al. (2003)" startWordPosition="1015" endWordPosition="1018">ons, distributional models need to see at least some head words for each (p, a) combination. Manually annotated treebank corpora, which are becoming available for an increasing number of languages, are too small for this task. We therefore explore the idea of predicting the selectional preferences for such languages by taking advantage of large corpora with high-quality syntactic analyses in resource-rich languages like English. This idea falls into the general approach of cross-lingual knowledge transfer (see e.g. Hwa et al., 2005). The application to selectional preferences was suggested by Agirre et al. (2003), who demonstrated its feasibility by manual translation between Basque and English. We extend their experiments to an automatic model that predicts plausibility judgments in a resource-poor language (source language) by exploiting a model in a resource-rich language (target language). Figure 1 sketches our method. We assume that there is not enough high-quality data to build a monolingual selectional preference model for the source language (shown by dotted lines). However, we can use a bilingual vector space, that is, a semantic space in which words of both the source and the target language</context>
<context position="33360" citStr="Agirre et al. (2003)" startWordPosition="5266" endWordPosition="5269"> available. Our experiments have looked at German and Spanish, where the cross-lingual models rival and even exceed monolingual methods that typically have to rely on small, clean “treebank”-style corpora or large, very noisy, automatically parsed corpora. We have also demonstrated that noisy syntactic data from the source language can be integrated in our model, where it helps improve the cross-lingual handling of argument positions. The linguistic distance between the languages can impact (1) the ability to find accurate translations and (2) the degree of syntactic overlap; nevertheless, as Agirre et al. (2003) show, the transfer is possible even for unrelated languages. In this paper, we have instantiated the selectional preference model in the target language (English) with the distributional model by Padó et al. (2007). However, our approach is modular and can be combined with any other selectional preference model. We see two main avenues for future work: (1), The construction of properly bilingual models where source language information can also help to further improve the target language model (Diab and Resnik, 2002); (2), The extension of our cross-lingual mapping for the argument position t</context>
</contexts>
<marker>Agirre, Aldezabal, Pociello, 2003</marker>
<rawString>Eneko Agirre, Izaskun Aldezabal, and Eli Pociello. 2003. A pilot study of English selectional preferences and their cross-lingual compatibility with Basque. In Proc. TSD, pages 12–19, Brno, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Discriminative learning of selectional preference from unlabeled text.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>59--68</pages>
<location>Honolulu, HI.</location>
<contexts>
<context position="4581" citStr="Bergsma et al., 2008" startWordPosition="692" endWordPosition="695">to create predictions for unseen ones. A number of studies has followed the same approach, exploring different ways of using the structure of WordNet (Abe and Li, 1996; Clark and Weir, 2002). While these approaches show good results, they can only make predictions for argument heads that are covered by WordNet. This is already a problem for English, and much more so in other languages, where comparable resources are often much smaller or entirely absent. A promising alternative approach is to derive the generalizations from distributional information (Prescher et al., 2000; Padó et al., 2007; Bergsma et al., 2008). For example, the Padó et al. (2007) model computes vector space representations for all head words h and defines the plausibility of the triple (p, a, h) as a weighted mean of the vector space similarities between h and all h&apos; in Seena(p): � w(h&apos;)· sim(h, h&apos;) (1) Pl(p, a, h) = Ehl w(h&apos;) h&apos;ESee%(p) where w(h&apos;) is a weight, typically frequency. In this model, the generalization is provided by distributional similarity, which can be computed from a large corpus, without the need for additional lexical resources. Padó et al. found it to outperform Resnik’s approach in an evaluation against human</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>Shane Bergsma, Dekang Lin, and Randy Goebel. 2008. Discriminative learning of selectional preference from unlabeled text. In Proc. EMNLP, pages 59–68, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proc. Workshop on Treebanks and Linguistic Theories,</booktitle>
<location>Sozopol, Bulgaria.</location>
<contexts>
<context position="15043" citStr="Brants et al., 2002" startWordPosition="2327" endWordPosition="2330">evaluation uses English as the target language and two source languages: German (as a very close neighbor of English) and Spanish (as a more distant one). Neither of these languages are really resourcepoor, but they allow us to compare our cross-lingual model against monolingual models, to emulate different levels of “resource poorness” and to examine the model’s learning curve. Plausibility Data. For German, we used the plausibility judgments collected by Brockmann (2002). The dataset contains human judgments for ninety triples sampled from the manually annotated 1 million word TiGer corpus (Brants et al., 2002): ten verbs with three argument positions (subject [SUBJ], direct object [DOBJ], and oblique (prepositional) object [POBJ]) combined with three head words. Models are evaluated against such datasets by correlating predicted plausibilities with the (not normally distributed) human judgments using Spearman’s p, a non-parametric rank-order correlation coefficient. We constructed a similar 90-triple data set for Spanish by sampling triples from two Spanish corpora (see below) using Brockmann’s (2002) criteria. Human judgments for the triples were collected through the Amazon Mechanical Turk (AMT) </context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank. In Proc. Workshop on Treebanks and Linguistic Theories, Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carsten Brockmann</author>
<author>Mirella Lapata</author>
</authors>
<title>Evaluating and combining approaches to selectional preference acquisition.</title>
<date>2003</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>27--34</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="16281" citStr="Brockmann and Lapata (2003)" startWordPosition="2515" endWordPosition="2518">ng platform (Snow et al., 2008). We asked native speakers of Spanish to rate the plausibility of a simple sentence with the relevant verb-argument combination on a five-point Likert scale, obtaining between 12 and 17 judgments for each triple. For each datapoint, we removed the single lowest and highest judgments and computed the mean. We assessed the reliability of our data by replicating Brockmann’s experiment for German with our AMT setup. With a Spearman p of almost .90, our own judgments correlate very well with Brockmann’s original data. Monolingual Prior Work and Baselines. For German, Brockmann and Lapata (2003) evaluated ontology-based models trained on TiGer triples and the GermaNet ontology. The results in Table 2 show that while both models are able to predict the data significantly, neither of the models can predict all of the data. We attribute this to the small size of TiGer.2 To gauge the limits of monolingual knowledgelean approaches, we constructed two monolingual distributional models for German and Spanish according to the Padó et al. (2007) model (Eq. (1)). Recall that this model performs generalization in a syntax-based vector space model. We computed vector spaces from dependency-parse</context>
</contexts>
<marker>Brockmann, Lapata, 2003</marker>
<rawString>Carsten Brockmann and Mirella Lapata. 2003. Evaluating and combining approaches to selectional preference acquisition. In Proc. EACL, pages 27–34, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carsten Brockmann</author>
</authors>
<title>Evaluating and combining approaches to selectional preference acquisition.</title>
<date>2002</date>
<booktitle>Master’s thesis, Universität des Saarlandes, Saarbr icken.</booktitle>
<contexts>
<context position="14900" citStr="Brockmann (2002)" startWordPosition="2306" endWordPosition="2307">ill thus consider several verb translations, but just the best head word translation. For details, see Section 5. 4 Experimental Setup Our evaluation uses English as the target language and two source languages: German (as a very close neighbor of English) and Spanish (as a more distant one). Neither of these languages are really resourcepoor, but they allow us to compare our cross-lingual model against monolingual models, to emulate different levels of “resource poorness” and to examine the model’s learning curve. Plausibility Data. For German, we used the plausibility judgments collected by Brockmann (2002). The dataset contains human judgments for ninety triples sampled from the manually annotated 1 million word TiGer corpus (Brants et al., 2002): ten verbs with three argument positions (subject [SUBJ], direct object [DOBJ], and oblique (prepositional) object [POBJ]) combined with three head words. Models are evaluated against such datasets by correlating predicted plausibilities with the (not normally distributed) human judgments using Spearman’s p, a non-parametric rank-order correlation coefficient. We constructed a similar 90-triple data set for Spanish by sampling triples from two Spanish </context>
</contexts>
<marker>Brockmann, 2002</marker>
<rawString>Carsten Brockmann. 2002. Evaluating and combining approaches to selectional preference acquisition. Master’s thesis, Universität des Saarlandes, Saarbr icken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiram Calvo</author>
<author>Alexander Gelbukh</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Distributional thesaurus vs. wordnet: A comparison of backoff techniques for unsupervised PP attachment.</title>
<date>2005</date>
<booktitle>In Proc. CICLing,</booktitle>
<pages>177--188</pages>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="18224" citStr="Calvo et al., 2005" startWordPosition="2840" endWordPosition="2843"> coverage for distributional models. † : p &lt; .1; *: p &lt; .05; **: p &lt; .01. two languages, using the 2,000 most frequent lemmadependency relation pairs as dimensions and adopting the popular pointwise mutual information metric as co-occurrence statistic. For German, we used Schulte im Walde’s verb frame resource (Schulte im Walde et al., 2001), which contains the frequency of triples calculated from probabilistic parses of 30M words from the Huge German Corpus (HGC) of newswire. For Spanish, we consulted two syntactically analyzed corpora: the AnCora (Taulé et al., 2008) and the Encarta corpus (Calvo et al., 2005). At 0.5M words, the AnCora corpus is small, but manually annotated, whereas the larger, automatically parsed Encarta corpus amounts to over 18M tokens. Table 3 shows the results for the distributional monolingual models. For German, we get significant correlations for DOBJ and POBJ, an almost significant correlation for SUBJs, and high significance for the complete dataset (p &lt; 0.01). These figures rival the performance of the ontological models (cf. Table 2), without using ontological information. For Spanish, the only significant correlation with human judgments is obtained for subjects, th</context>
</contexts>
<marker>Calvo, Gelbukh, Kilgarriff, 2005</marker>
<rawString>Hiram Calvo, Alexander Gelbukh, and Adam Kilgarriff. 2005. Distributional thesaurus vs. wordnet: A comparison of backoff techniques for unsupervised PP attachment. In Proc. CICLing, pages 177–188, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>602--610</pages>
<contexts>
<context position="13283" citStr="Chambers and Jurafsky (2009)" startWordPosition="2044" endWordPosition="2047">s that its second and third-best English neighbors, hit, and stalk, can be used to smooth plausibility estimates for schief3en. Instead of translating source language words by their single nearest neighbor, we will take its k nearest neighbors into account. This is defensible also from a more fundamental point of view, which suggests that the cross-lingual transfer of selectional preferences does not require literal translation in order to work. First, ontological models like Resnik’s assume that synonymous words behave similarly with respect to selectional preferences. Second, recent work by Chambers and Jurafsky (2009) has induced “narrative chains”, i.e., likely sequences of events, by their use of similar head words. Thus, we expect that all k nearest neighbors of a source predicate s are informative for the selectional preferences of s (like schief3en) as long as they are either synonyms of its literal translation (shoot/hit) or come from the same narrative chain (stalk/kill/. .. ). It is also clear that smoothing does not always equate better predictions. Closeness in a word-based vector space can also just reflect semantic association. For example, Spanish tenista ‘tennis player’ is highly associated w</context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proc. ACL, pages 602–610, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Chuang Chiao</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Looking for candidate translational equivalents in specialized, comparable corpora.</title>
<date>2002</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>1--5</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="9814" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="1489" endWordPosition="1492">rget languages. Their construction does not require resources such as parallel corpora or bilingual translation lexicons, which might not be available for resource-poor source languages. Where parallel corpora exist, they often cover specific domains (e.g., politics), while many bilingual lexicons are prone to ambiguity problems. The main challenge in constructing bilingual vector spaces is determining the set of dimensions, i.e., bilingual word pairs, using as little knowledge as possible. Most often, such pairs are extracted from small bilingual lexicons (Fung and McKeown, 1997; Rapp, 1999; Chiao and Zweigenbaum, 2002). As mentioned above, such resources might not be available. We thus follow an alternative approach by using frequent cognates, words that are shared between the two languages (Markó et al., 2005). Cognates can be extracted by simple string matching between the corpora, and mostly share their meaning (Koehn and Knight, 2002). However, they account for (at most) a small percentage of all interesting translation pairs. To extend the set of dimensions available for the Figure 2: Sketch of a bilingual vector space for English (solid dots) and German (empty circles). bilingual space, we use these c</context>
</contexts>
<marker>Chiao, Zweigenbaum, 2002</marker>
<rawString>Yun-Chuang Chiao and Pierre Zweigenbaum. 2002. Looking for candidate translational equivalents in specialized, comparable corpora. In Proc. COLING, pages 1–5, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="4150" citStr="Clark and Weir, 2002" startWordPosition="624" endWordPosition="627">929, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics 2 Selectional Preferences The first broad-coverage model of selectional preferences was developed by Resnik (1996). To estimate the plausibility of a triple (p, a, h), Resnik first extracted all head words seen with predicate p in position a, Seena(p), from a corpus. He then used the WordNet hierarchy to generalize over the head words and to create predictions for unseen ones. A number of studies has followed the same approach, exploring different ways of using the structure of WordNet (Abe and Li, 1996; Clark and Weir, 2002). While these approaches show good results, they can only make predictions for argument heads that are covered by WordNet. This is already a problem for English, and much more so in other languages, where comparable resources are often much smaller or entirely absent. A promising alternative approach is to derive the generalizations from distributional information (Prescher et al., 2000; Padó et al., 2007; Bergsma et al., 2008). For example, the Padó et al. (2007) model computes vector space representations for all head words h and defines the plausibility of the triple (p, a, h) as a weighted</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Stephen Clark and David Weir. 2002. Class-based probability estimation using a semantic hierarchy. Computational Linguistics, 28(2):187–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>255--262</pages>
<location>Philadelphia, PA.</location>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proc. ACL, pages 255–262, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Kathleen McKeown</author>
</authors>
<title>Finding terminology translations from non-parallel corpora.</title>
<date>1997</date>
<booktitle>In Proc. 3rd Annual Workshop on Very Large Corpora,</booktitle>
<pages>192--202</pages>
<location>Hong Kong.</location>
<contexts>
<context position="9772" citStr="Fung and McKeown, 1997" startWordPosition="1483" endWordPosition="1486">elated corpora for the source and target languages. Their construction does not require resources such as parallel corpora or bilingual translation lexicons, which might not be available for resource-poor source languages. Where parallel corpora exist, they often cover specific domains (e.g., politics), while many bilingual lexicons are prone to ambiguity problems. The main challenge in constructing bilingual vector spaces is determining the set of dimensions, i.e., bilingual word pairs, using as little knowledge as possible. Most often, such pairs are extracted from small bilingual lexicons (Fung and McKeown, 1997; Rapp, 1999; Chiao and Zweigenbaum, 2002). As mentioned above, such resources might not be available. We thus follow an alternative approach by using frequent cognates, words that are shared between the two languages (Markó et al., 2005). Cognates can be extracted by simple string matching between the corpora, and mostly share their meaning (Koehn and Knight, 2002). However, they account for (at most) a small percentage of all interesting translation pairs. To extend the set of dimensions available for the Figure 2: Sketch of a bilingual vector space for English (solid dots) and German (empty</context>
</contexts>
<marker>Fung, McKeown, 1997</marker>
<rawString>Pascale Fung and Kathleen McKeown. 1997. Finding terminology translations from non-parallel corpora. In Proc. 3rd Annual Workshop on Very Large Corpora, pages 192–202, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1654" citStr="Gildea and Jurafsky, 2002" startWordPosition="234" endWordPosition="237">to a given verb in a particular argument position (Wilks, 1975; Resnik, 1996). For instance, the subjects of the English verb to shoot are generally people, while the direct objects can be people or animals. This is reflected in speakers’ intuitions. Table 1 shows that the combination the hunter shot the deer is judged more plausible than the deer shot the hunter. Selectional preferences do not only play an important role in human sentence processing (McRae et al., 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Computational models of selectional preferences predict such plausibilities for triples of a predicate p, an argument position a, and a head word h, such as Predicate Relation Noun Plausibility shoot subject hunter 6.9 shoot object hunter 2.8 shoot subject deer 1.0 shoot object deer 6.4 Table 1: Predicate-relation-noun triples with human plausibility judgments on a 7-point scale (McRae et al., 1998) (shoot,object,hunter). All recent models take a twostep approach: (1), they extract all triples (p, a, h) from a large corpus; (2), they apply some type of generalization to make predictions for </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philipp Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="6619" citStr="Hwa et al., 2005" startWordPosition="1003" endWordPosition="1006">al preferences In order to compute reliable selectional preference representations, distributional models need to see at least some head words for each (p, a) combination. Manually annotated treebank corpora, which are becoming available for an increasing number of languages, are too small for this task. We therefore explore the idea of predicting the selectional preferences for such languages by taking advantage of large corpora with high-quality syntactic analyses in resource-rich languages like English. This idea falls into the general approach of cross-lingual knowledge transfer (see e.g. Hwa et al., 2005). The application to selectional preferences was suggested by Agirre et al. (2003), who demonstrated its feasibility by manual translation between Basque and English. We extend their experiments to an automatic model that predicts plausibility judgments in a resource-poor language (source language) by exploiting a model in a resource-rich language (target language). Figure 1 sketches our method. We assume that there is not enough high-quality data to build a monolingual selectional preference model for the source language (shown by dotted lines). However, we can use a bilingual vector space, t</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philipp Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering, 11(3):311–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In Proc. ACL-02 Workshop on Unsupervised Lexical Acquisition,</booktitle>
<pages>9--16</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="10140" citStr="Koehn and Knight, 2002" startWordPosition="1541" endWordPosition="1544"> The main challenge in constructing bilingual vector spaces is determining the set of dimensions, i.e., bilingual word pairs, using as little knowledge as possible. Most often, such pairs are extracted from small bilingual lexicons (Fung and McKeown, 1997; Rapp, 1999; Chiao and Zweigenbaum, 2002). As mentioned above, such resources might not be available. We thus follow an alternative approach by using frequent cognates, words that are shared between the two languages (Markó et al., 2005). Cognates can be extracted by simple string matching between the corpora, and mostly share their meaning (Koehn and Knight, 2002). However, they account for (at most) a small percentage of all interesting translation pairs. To extend the set of dimensions available for the Figure 2: Sketch of a bilingual vector space for English (solid dots) and German (empty circles). bilingual space, we use these cognates merely as a starting point for a bootstrapping process: We build a bilingual vector space with the initial word pairs as dimensions, and identify nearest neighbors between the two languages in the space. These are added as dimensions of the bilingual space, and the process is repeated. Since the focus is on identifyi</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In Proc. ACL-02 Workshop on Unsupervised Lexical Acquisition, pages 9–16, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>25--32</pages>
<location>College</location>
<contexts>
<context position="9049" citStr="Lee, 1999" startWordPosition="1379" endWordPosition="1380">xt word pairs (like secretly/heimlich and rifle/Gewehr for German–English) that are mutual translations. By treating such context word pairs as single dimensions, the vector space can represent target words from both languages, counting the target words’ co-occurrences with the context words from the respective language. In other words, a sourcetarget word pair (s, t) will be assigned similar vectors in the semantic space if the context words of s are translations of the context words of t. Cross-lingual semantic similarity between words can be measured using standard vector space similarity (Lee, 1999). Importantly, bilingual vector spaces can be built on the basis of co-occurrences drawn from two unrelated corpora for the source and target languages. Their construction does not require resources such as parallel corpora or bilingual translation lexicons, which might not be available for resource-poor source languages. Where parallel corpora exist, they often cover specific domains (e.g., politics), while many bilingual lexicons are prone to ambiguity problems. The main challenge in constructing bilingual vector spaces is determining the set of dimensions, i.e., bilingual word pairs, using </context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In Proc. ACL, pages 25–32, College Park, MD. Dekang Lin. 1993. Principle-based parsing without overgeneration. In Proc. ACL, pages 112–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kornél Markó</author>
<author>Stefan Schulz</author>
<author>Olena Medelyan</author>
<author>Udo Hahn</author>
</authors>
<title>Bootstrapping dictionaries for crosslanguage information retrieval.</title>
<date>2005</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>528--535</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="10010" citStr="Markó et al., 2005" startWordPosition="1521" endWordPosition="1524">pora exist, they often cover specific domains (e.g., politics), while many bilingual lexicons are prone to ambiguity problems. The main challenge in constructing bilingual vector spaces is determining the set of dimensions, i.e., bilingual word pairs, using as little knowledge as possible. Most often, such pairs are extracted from small bilingual lexicons (Fung and McKeown, 1997; Rapp, 1999; Chiao and Zweigenbaum, 2002). As mentioned above, such resources might not be available. We thus follow an alternative approach by using frequent cognates, words that are shared between the two languages (Markó et al., 2005). Cognates can be extracted by simple string matching between the corpora, and mostly share their meaning (Koehn and Knight, 2002). However, they account for (at most) a small percentage of all interesting translation pairs. To extend the set of dimensions available for the Figure 2: Sketch of a bilingual vector space for English (solid dots) and German (empty circles). bilingual space, we use these cognates merely as a starting point for a bootstrapping process: We build a bilingual vector space with the initial word pairs as dimensions, and identify nearest neighbors between the two language</context>
</contexts>
<marker>Markó, Schulz, Medelyan, Hahn, 2005</marker>
<rawString>Kornél Markó, Stefan Schulz, Olena Medelyan, and Udo Hahn. 2005. Bootstrapping dictionaries for crosslanguage information retrieval. In Proc. SIGIR, pages 528–535, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Disambiguating nouns, verbs and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="1599" citStr="McCarthy and Carroll, 2003" startWordPosition="226" endWordPosition="229">servation that not all words are equally good arguments to a given verb in a particular argument position (Wilks, 1975; Resnik, 1996). For instance, the subjects of the English verb to shoot are generally people, while the direct objects can be people or animals. This is reflected in speakers’ intuitions. Table 1 shows that the combination the hunter shot the deer is judged more plausible than the deer shot the hunter. Selectional preferences do not only play an important role in human sentence processing (McRae et al., 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Computational models of selectional preferences predict such plausibilities for triples of a predicate p, an argument position a, and a head word h, such as Predicate Relation Noun Plausibility shoot subject hunter 6.9 shoot object hunter 2.8 shoot subject deer 1.0 shoot object deer 6.4 Table 1: Predicate-relation-noun triples with human plausibility judgments on a 7-point scale (McRae et al., 1998) (shoot,object,hunter). All recent models take a twostep approach: (1), they extract all triples (p, a, h) from a large corpus; (2), they app</context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>Diana McCarthy and John Carroll. 2003. Disambiguating nouns, verbs and adjectives using automatically acquired selectional preferences. Computational Linguistics, 29(4):639–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>Michael Spivey-Knowlton</author>
<author>Michael Tanenhaus</author>
</authors>
<title>Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension.</title>
<date>1998</date>
<journal>Journal of Memory and Language,</journal>
<volume>38</volume>
<pages>312</pages>
<contexts>
<context position="1503" citStr="McRae et al., 1998" startWordPosition="211" endWordPosition="214">lingual baseline models. 1 Introduction Selectional preferences capture the empirical observation that not all words are equally good arguments to a given verb in a particular argument position (Wilks, 1975; Resnik, 1996). For instance, the subjects of the English verb to shoot are generally people, while the direct objects can be people or animals. This is reflected in speakers’ intuitions. Table 1 shows that the combination the hunter shot the deer is judged more plausible than the deer shot the hunter. Selectional preferences do not only play an important role in human sentence processing (McRae et al., 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Computational models of selectional preferences predict such plausibilities for triples of a predicate p, an argument position a, and a head word h, such as Predicate Relation Noun Plausibility shoot subject hunter 6.9 shoot object hunter 2.8 shoot subject deer 1.0 shoot object deer 6.4 Table 1: Predicate-relation-noun triples with human plausibility judgments on a 7-point scale (McRae et al., 1998) (shoot,object,hunter). All recent models tak</context>
</contexts>
<marker>McRae, Spivey-Knowlton, Tanenhaus, 1998</marker>
<rawString>Ken McRae, Michael Spivey-Knowlton, and Michael Tanenhaus. 1998. Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension. Journal of Memory and Language, 38:283– 312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Padó</author>
<author>Ulrike Padó</author>
<author>Katrin Erk</author>
</authors>
<title>Flexible, corpus-based modelling of human plausibility judgements.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL,</booktitle>
<pages>400--409</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4558" citStr="Padó et al., 2007" startWordPosition="688" endWordPosition="691">the head words and to create predictions for unseen ones. A number of studies has followed the same approach, exploring different ways of using the structure of WordNet (Abe and Li, 1996; Clark and Weir, 2002). While these approaches show good results, they can only make predictions for argument heads that are covered by WordNet. This is already a problem for English, and much more so in other languages, where comparable resources are often much smaller or entirely absent. A promising alternative approach is to derive the generalizations from distributional information (Prescher et al., 2000; Padó et al., 2007; Bergsma et al., 2008). For example, the Padó et al. (2007) model computes vector space representations for all head words h and defines the plausibility of the triple (p, a, h) as a weighted mean of the vector space similarities between h and all h&apos; in Seena(p): � w(h&apos;)· sim(h, h&apos;) (1) Pl(p, a, h) = Ehl w(h&apos;) h&apos;ESee%(p) where w(h&apos;) is a weight, typically frequency. In this model, the generalization is provided by distributional similarity, which can be computed from a large corpus, without the need for additional lexical resources. Padó et al. found it to outperform Resnik’s approach in an e</context>
<context position="16731" citStr="Padó et al. (2007)" startWordPosition="2589" endWordPosition="2592">man p of almost .90, our own judgments correlate very well with Brockmann’s original data. Monolingual Prior Work and Baselines. For German, Brockmann and Lapata (2003) evaluated ontology-based models trained on TiGer triples and the GermaNet ontology. The results in Table 2 show that while both models are able to predict the data significantly, neither of the models can predict all of the data. We attribute this to the small size of TiGer.2 To gauge the limits of monolingual knowledgelean approaches, we constructed two monolingual distributional models for German and Spanish according to the Padó et al. (2007) model (Eq. (1)). Recall that this model performs generalization in a syntax-based vector space model. We computed vector spaces from dependency-parsed corpora for the 2For each of the three argument positions and “all”, Brockmann and Lapata report the results for the best parametrization of the models, which explains the apparently inconsistent results. 924 Resnik Clark &amp; Weir SUBJ .408* .268 DOBJ .430* .611*** POBJ .330 .597*** all .374*** .232* Table 2: Monolingual baselines 1. Spearman correlations for ontology-based models in German as reported by Brockmann and Lapata (2003). *: p &lt; .05; </context>
<context position="19628" citStr="Padó et al. (2007)" startWordPosition="3055" endWordPosition="3058">ing our concerns from Section 2. 3Since the Encarta data consists of individual dependency n noun adj verb all German 7340 .61 .57 .43 .56 Spanish 4143 .62 .67 .41 .58 Table 4: First-translation accuracy for German-English and Spanish-English translation (n: size of gold standard). Cross-lingual Selectional Preferences. Our architecture for the cross-lingual prediction of selectional preferences shown in Figure 1 consists of two components, namely the bilingual vector space and a selectional preference model in the target language. As our English selectional preference model, we again use the Padó et al. (2007) model, trained on a version of the BNC parsed with MINIPAR (Lin, 1993). The parameters of the syntactic vector space were the same as for the monolingual baseline models. The bilingual vector spaces were constructed from three large, unparsed, comparable monolingual corpora. For German, we used the HGC described above. For Spanish, we obtained a corpus with around 100M words, consisting of 2.5 years of crawled text from two major Spanish newspapers. For English, we used the BNC. We first constructed initial sets of bilingual labels. For German–English, we identified 1064 graphemically identic</context>
<context position="30429" citStr="Padó et al. (2007)" startWordPosition="4805" endWordPosition="4808">ecause our Spanish dataset contains verbs sampled from Encarta that do not occur in AnCora. m(p, a, pt) = argmax at hESee%(p) Plt(pt, at, tr(h)) P ls (p, a, h) = max synptEtrk(p) 927 German Tau (‘dew’, frequency of 180 in the HGC), translated as alley, and Reifeprüfung (German SAT, frequency 120), translated as affiliation. Both of these may also be due to the difference in genre between the HGC and the BNC. A second problem is formed by nearest neighbors that are ontologically dissimilar, as in the tenista ‘tennis player’/tennis example from above. A final issue relates to limitations of the Padó et al. (2007) model, whose architecture is susceptible to polysemy-related problems. For instance, the Spanish combination (excavar, obj, terreno) was judged by speakers as very plausible, but its English equivalent (excavate, obj, land) is assigned a very low score by the model. This might be due to the fact that in the BNC, land occurs often in its political meaning, and forms an outlier among the head words for (excavate,obj). How much syntactic information is necessary? The syntax-aware model requires syntactic information about the source language, which seems to run counter to our original motivation</context>
<context position="33575" citStr="Padó et al. (2007)" startWordPosition="5299" endWordPosition="5302">ry noisy, automatically parsed corpora. We have also demonstrated that noisy syntactic data from the source language can be integrated in our model, where it helps improve the cross-lingual handling of argument positions. The linguistic distance between the languages can impact (1) the ability to find accurate translations and (2) the degree of syntactic overlap; nevertheless, as Agirre et al. (2003) show, the transfer is possible even for unrelated languages. In this paper, we have instantiated the selectional preference model in the target language (English) with the distributional model by Padó et al. (2007). However, our approach is modular and can be combined with any other selectional preference model. We see two main avenues for future work: (1), The construction of properly bilingual models where source language information can also help to further improve the target language model (Diab and Resnik, 2002); (2), The extension of our cross-lingual mapping for the argument position to mappings that hold across multiple predicates as well as argumentdependent mappings like the Spanish direct objects, whose realization depends on their animacy. ● SUBJ DOBJ POBJ all ● ● ● ● ● ● Spearman&apos;s rho 0.3 </context>
</contexts>
<marker>Padó, Padó, Erk, 2007</marker>
<rawString>Sebastian Padó, Ulrike Padó, and Katrin Erk. 2007. Flexible, corpus-based modelling of human plausibility judgements. In Proc. EMNLP-CoNLL, pages 400–409, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Peirsman</author>
<author>Kris Heylen</author>
<author>Dirk Geeraerts</author>
</authors>
<title>Size matters. Tight and loose context definitions in English word space models.</title>
<date>2008</date>
<booktitle>In Proc. ESSLLI Workshop on Lexical Semantics,</booktitle>
<pages>9--16</pages>
<location>Hamburg, Germany.</location>
<contexts>
<context position="21069" citStr="Peirsman et al., 2008" startWordPosition="3287" endWordPosition="3290">d a Porter stemmer and found 2104 identical stems, at a higher risk of “false friends”. We then applied the bootstrapping cycle from Section 3.1. The set of dimensions converged after around five iterations. We evaluated the (asymmetric) nearest neighbor pairs from the final spaces, (s, tr(s)), against two online dictionaries.4 Table 4 shows that 55% to 60% of the pairs are listed in the dictionaries, with parallel tendencies for both language pairs. The bilingual space performs fairly well for nouns and adjectives, but badly for verbs, which is a well-known weakness of distributional models (Peirsman et al., 2008). Even taking into account the incompleteness of dictionaries, this looks like a negative result: more relations rather than trees, we could not model the POBJ data. 4DE-EN: www.dict.cc; ES-EN: www.freelang.net. Pairs (s, tr(s)) were only evaluated if the dictionary listed s. 925 than half of all verb translations are incorrect. However, following up on our intuitions from Section 3.2, we performed an analysis of the “incorrect” translations. It revealed that many of the errors in Table 4 are informative, semantically related words. Nearest neighbor target language verbs in particular tend to </context>
</contexts>
<marker>Peirsman, Heylen, Geeraerts, 2008</marker>
<rawString>Yves Peirsman, Kris Heylen, and Dirk Geeraerts. 2008. Size matters. Tight and loose context definitions in English word space models. In Proc. ESSLLI Workshop on Lexical Semantics, pages 9–16, Hamburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detlef Prescher</author>
<author>Stefan Riezler</author>
<author>Mats Rooth</author>
</authors>
<title>Using a probabilistic class-based lexicon for lexical ambiguity resolution.</title>
<date>2000</date>
<booktitle>In Proc. COLING, pages 649– 655, Saarbr icken,</booktitle>
<location>Germany.</location>
<contexts>
<context position="4539" citStr="Prescher et al., 2000" startWordPosition="684" endWordPosition="687">chy to generalize over the head words and to create predictions for unseen ones. A number of studies has followed the same approach, exploring different ways of using the structure of WordNet (Abe and Li, 1996; Clark and Weir, 2002). While these approaches show good results, they can only make predictions for argument heads that are covered by WordNet. This is already a problem for English, and much more so in other languages, where comparable resources are often much smaller or entirely absent. A promising alternative approach is to derive the generalizations from distributional information (Prescher et al., 2000; Padó et al., 2007; Bergsma et al., 2008). For example, the Padó et al. (2007) model computes vector space representations for all head words h and defines the plausibility of the triple (p, a, h) as a weighted mean of the vector space similarities between h and all h&apos; in Seena(p): � w(h&apos;)· sim(h, h&apos;) (1) Pl(p, a, h) = Ehl w(h&apos;) h&apos;ESee%(p) where w(h&apos;) is a weight, typically frequency. In this model, the generalization is provided by distributional similarity, which can be computed from a large corpus, without the need for additional lexical resources. Padó et al. found it to outperform Resnik</context>
</contexts>
<marker>Prescher, Riezler, Rooth, 2000</marker>
<rawString>Detlef Prescher, Stefan Riezler, and Mats Rooth. 2000. Using a probabilistic class-based lexicon for lexical ambiguity resolution. In Proc. COLING, pages 649– 655, Saarbr icken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>519--526</pages>
<location>College Park, MD.</location>
<contexts>
<context position="9784" citStr="Rapp, 1999" startWordPosition="1487" endWordPosition="1488">ource and target languages. Their construction does not require resources such as parallel corpora or bilingual translation lexicons, which might not be available for resource-poor source languages. Where parallel corpora exist, they often cover specific domains (e.g., politics), while many bilingual lexicons are prone to ambiguity problems. The main challenge in constructing bilingual vector spaces is determining the set of dimensions, i.e., bilingual word pairs, using as little knowledge as possible. Most often, such pairs are extracted from small bilingual lexicons (Fung and McKeown, 1997; Rapp, 1999; Chiao and Zweigenbaum, 2002). As mentioned above, such resources might not be available. We thus follow an alternative approach by using frequent cognates, words that are shared between the two languages (Markó et al., 2005). Cognates can be extracted by simple string matching between the corpora, and mostly share their meaning (Koehn and Knight, 2002). However, they account for (at most) a small percentage of all interesting translation pairs. To extend the set of dimensions available for the Figure 2: Sketch of a bilingual vector space for English (solid dots) and German (empty circles). b</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. In Proc. ACL, pages 519–526, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional constraints: An information-theoretic model and its computational realization.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--127</pages>
<contexts>
<context position="1105" citStr="Resnik, 1996" startWordPosition="146" endWordPosition="147">anguage predicate-argument structures into a resource-rich language like English. The only prerequisite for constructing the bilingual vector space is a large unparsed corpus in the resource-poor language, although the model can profit from (even noisy) syntactic knowledge. Our experiments show that the cross-lingual predictions correlate well with human ratings, clearly outperforming monolingual baseline models. 1 Introduction Selectional preferences capture the empirical observation that not all words are equally good arguments to a given verb in a particular argument position (Wilks, 1975; Resnik, 1996). For instance, the subjects of the English verb to shoot are generally people, while the direct objects can be people or animals. This is reflected in speakers’ intuitions. Table 1 shows that the combination the hunter shot the deer is judged more plausible than the deer shot the hunter. Selectional preferences do not only play an important role in human sentence processing (McRae et al., 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Computational models of selectional preferences p</context>
<context position="3733" citStr="Resnik (1996)" startWordPosition="552" endWordPosition="553">ed for parallel corpora or bilingual lexical resources. Structure of the paper. Section 2 reviews models for selectional preferences. In Section 3, we describe our approach. Section 4 introduces our experimental setup, and Sections 5 and 6 present and discuss our experiments. Section 7 wraps up. 921 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 921–929, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics 2 Selectional Preferences The first broad-coverage model of selectional preferences was developed by Resnik (1996). To estimate the plausibility of a triple (p, a, h), Resnik first extracted all head words seen with predicate p in position a, Seena(p), from a corpus. He then used the WordNet hierarchy to generalize over the head words and to create predictions for unseen ones. A number of studies has followed the same approach, exploring different ways of using the structure of WordNet (Abe and Li, 1996; Clark and Weir, 2002). While these approaches show good results, they can only make predictions for argument heads that are covered by WordNet. This is already a problem for English, and much more so in o</context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>Philip Resnik. 1996. Selectional constraints: An information-theoretic model and its computational realization. Cognition, 61:127–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Helmut Schmid</author>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
</authors>
<title>Statistical Grammar Models and Lexicon Acquisition.</title>
<date>2001</date>
<booktitle>In Linguistic Form and its Computation,</booktitle>
<pages>389--440</pages>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="17948" citStr="Walde et al., 2001" startWordPosition="2796" endWordPosition="2799">; ***: p &lt; .001 Lang. German Spanish Corpus Schulte’s HGC AnCora Encarta P Cov. p Cov. p Cov. SUBJ .34† 90% .44* 80% .14 100% DOBJ .51** 97% .29 83% -.05 100% POBJ .41* 93% -.03 100% — —3 all .33** 93% .16 88% .11 67% Table 3: Monolingual baselines 2. Spearman correlation and coverage for distributional models. † : p &lt; .1; *: p &lt; .05; **: p &lt; .01. two languages, using the 2,000 most frequent lemmadependency relation pairs as dimensions and adopting the popular pointwise mutual information metric as co-occurrence statistic. For German, we used Schulte im Walde’s verb frame resource (Schulte im Walde et al., 2001), which contains the frequency of triples calculated from probabilistic parses of 30M words from the Huge German Corpus (HGC) of newswire. For Spanish, we consulted two syntactically analyzed corpora: the AnCora (Taulé et al., 2008) and the Encarta corpus (Calvo et al., 2005). At 0.5M words, the AnCora corpus is small, but manually annotated, whereas the larger, automatically parsed Encarta corpus amounts to over 18M tokens. Table 3 shows the results for the distributional monolingual models. For German, we get significant correlations for DOBJ and POBJ, an almost significant correlation for S</context>
</contexts>
<marker>Walde, Schmid, Rooth, Riezler, Prescher, 2001</marker>
<rawString>Sabine Schulte im Walde, Helmut Schmid, Mats Rooth, Stefan Riezler, and Detlef Prescher. 2001. Statistical Grammar Models and Lexicon Acquisition. In Linguistic Form and its Computation, pages 389–440. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Cheap and fast – but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>254--263</pages>
<location>Honolulu, HI.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast – but is it good? Evaluating non-expert annotations for natural language tasks. In Proc. EMNLP, pages 254–263, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taulé</author>
<author>M Antònia Martí</author>
<author>Marta Recasens</author>
</authors>
<title>Ancora: Multilevel annotated corpora for Catalan and Spanish.</title>
<date>2008</date>
<booktitle>In Proc. LREC,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="18180" citStr="Taulé et al., 2008" startWordPosition="2832" endWordPosition="2835">ingual baselines 2. Spearman correlation and coverage for distributional models. † : p &lt; .1; *: p &lt; .05; **: p &lt; .01. two languages, using the 2,000 most frequent lemmadependency relation pairs as dimensions and adopting the popular pointwise mutual information metric as co-occurrence statistic. For German, we used Schulte im Walde’s verb frame resource (Schulte im Walde et al., 2001), which contains the frequency of triples calculated from probabilistic parses of 30M words from the Huge German Corpus (HGC) of newswire. For Spanish, we consulted two syntactically analyzed corpora: the AnCora (Taulé et al., 2008) and the Encarta corpus (Calvo et al., 2005). At 0.5M words, the AnCora corpus is small, but manually annotated, whereas the larger, automatically parsed Encarta corpus amounts to over 18M tokens. Table 3 shows the results for the distributional monolingual models. For German, we get significant correlations for DOBJ and POBJ, an almost significant correlation for SUBJs, and high significance for the complete dataset (p &lt; 0.01). These figures rival the performance of the ontological models (cf. Table 2), without using ontological information. For Spanish, the only significant correlation with </context>
</contexts>
<marker>Taulé, Martí, Recasens, 2008</marker>
<rawString>Mariona Taulé, M. Antònia Martí, and Marta Recasens. 2008. Ancora: Multilevel annotated corpora for Catalan and Spanish. In Proc. LREC, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>Preference semantics.</title>
<date>1975</date>
<booktitle>Formal Semantics of Natural Language.</booktitle>
<editor>In E. Keenan, editor,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1090" citStr="Wilks, 1975" startWordPosition="144" endWordPosition="145">te” foreign language predicate-argument structures into a resource-rich language like English. The only prerequisite for constructing the bilingual vector space is a large unparsed corpus in the resource-poor language, although the model can profit from (even noisy) syntactic knowledge. Our experiments show that the cross-lingual predictions correlate well with human ratings, clearly outperforming monolingual baseline models. 1 Introduction Selectional preferences capture the empirical observation that not all words are equally good arguments to a given verb in a particular argument position (Wilks, 1975; Resnik, 1996). For instance, the subjects of the English verb to shoot are generally people, while the direct objects can be people or animals. This is reflected in speakers’ intuitions. Table 1 shows that the combination the hunter shot the deer is judged more plausible than the deer shot the hunter. Selectional preferences do not only play an important role in human sentence processing (McRae et al., 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Computational models of selectiona</context>
</contexts>
<marker>Wilks, 1975</marker>
<rawString>Yorick Wilks. 1975. Preference semantics. In E. Keenan, editor, Formal Semantics of Natural Language. Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>