<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000680">
<title confidence="0.9930065">
Meerkat Mafia: Multilingual and Cross-Level
Semantic Textual Similarity Systems
</title>
<author confidence="0.976311">
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer Sleeman,
Taneeya Satyapanich, Sunil Gandhi and Tim Finin
</author>
<affiliation confidence="0.999329">
University of Maryland, Baltimore County
</affiliation>
<address confidence="0.881308">
Baltimore, MD 21250 USA
</address>
<email confidence="0.999686">
{abhay1,lushan1,ryus,jsleem1,taneeya1,sunilga1,finin}@umbc.edu
</email>
<sectionHeader confidence="0.994803" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994146142857143">
We describe UMBC’s systems developed
for the SemEval 2014 tasks on Multi-
lingual Semantic Textual Similarity (Task
10) and Cross-Level Semantic Similarity
(Task 3). Our best submission in the
Multilingual task ranked second in both
English and Spanish subtasks using an
unsupervised approach. Our best sys-
tems for Cross-Level task ranked second
in Paragraph-Sentence and first in both
Sentence-Phrase and Word-Sense subtask.
The system ranked first for the Phrase-
Word subtask but was not included in the
official results due to a late submission.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.943848388888889">
We describe the semantic text similarity systems
we developed for two of the SemEval tasks for the
2014 International Workshop on Semantic Evalu-
ation. We developed systems for task 3, Cross-
Level Semantic Similarity (Jurgens et al., 2014),
and task 10, Multilingual Semantic Textual Simi-
larity (Agirre et al., 2014). A key component in
all the systems was an enhanced version of the
word similarity system used in our entry (Han et
al., 2013b) in the 2013 SemEval Semantic Textual
Similarity task.
Our best system in the Multilingual Semantic
Textual Similarity task used an unsupervised ap-
proach and ranked second in both the English and
Spanish subtasks. In the Cross-Level Semantic
Similarity task we developed a number of new al-
gorithms and used new linguistic data resources.
In this task, our best systems ranked second in
the Paragraph-Sentence task, first in the Sentence-
Phrase task and first in the Word-Sense task. The
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails:http://creativecommons.org/licenses/by/4.0/
system ranked first for the Phrase-Word task but
was not included in the official results due to a late
submission.
The remainder of the paper proceeds as follows.
Section 2 describes our word similarity model and
it’s wrapper to deal with named entities and out
of vocabulary words. Sections 3 and 4 describe
how we extended the word similarity model for
the specific tasks. Section 5 presents the results
we achieved on these tasks along with instances
where the system failed. Section 6 highlights our
future plans for improving the system.
</bodyText>
<sectionHeader confidence="0.963" genericHeader="introduction">
2 Semantic Word Similarity Model
</sectionHeader>
<subsectionHeader confidence="0.819254">
2.1 LSA Word Similarity Model
</subsectionHeader>
<bodyText confidence="0.99978676">
Our word similarity model is a revised version of
the one we used in the 2013 *SEM semantic text
similarity task. This was in turn derived from
a system developed for the Graph of Relations
project (UMBC, 2013b). For SemEval, we wanted
a measure that considered a word’s semantics but
not its lexical category, e.g., the verb “marry”
should be semantically similar to the noun “wife”.
An online demonstration of a similar model de-
veloped for the GOR project is available (UMBC,
2013a), but it lacks some of this version’s features.
LSA-based word similarity. LSA Word Simi-
larity relies on the distributional hypothesis that
words occurring in the same context tend to have
similar meanings (Harris, 1968). LSA relies on the
fact that words that are semantically similar (e.g.,
cat and feline or nurse and doctor) are more likely
to occur near one another in text. Thus evidence
for word similarity can be computed from a statis-
tical analysis of a large text corpus.
We extracted raw word co-occurrence statis-
tics from a portion of the 2007 crawl of the Web
corpus from the Stanford WebBase project (Stan-
ford, 2001). We processed the collection to re-
move some undesirable elements (text duplica-
</bodyText>
<page confidence="0.982112">
416
</page>
<note confidence="0.9802055">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 416–423,
Dublin, Ireland, August 23-24, 2014.
</note>
<figure confidence="0.77118">
Word pair ±4 model ±1 model
1 doctor NN, physician NN 0.775 0.726
2 car NN, vehicle NN 0.748 0.802
3 person NN, car NN 0.038 0.024
4 car NN, country NN 0.000 0.016
5 person NN, country NN 0.031 0.069
6 child NN, marry VB 0.098 0.000
7 wife NN, marry VB 0.548 0.274
8 author NN, write VB 0.364 0.128
9 doctor NN, hospital NN 0.473 0.347
10 car NN, driver NN 0.497 0.281
</figure>
<tableCaption confidence="0.995421">
Table 1: Examples from the LSA similarity model.
</tableCaption>
<bodyText confidence="0.999313542857143">
tions, truncated text, non-English text and strange
characters) and produced a three billion word cor-
pus of high quality English, which is available on-
line (Han and Finin, 2013).
We performed POS tagging and lemmatiza-
tion on the corpus using the Stanford POS tag-
ger (Toutanova et al., 2000). Word/term co-
occurrences were counted in a moving window
of a fixed size that scans the entire corpus. We
generated two co-occurrence models using win-
dow sizes f1 and f4 because we observed differ-
ent natures of the models. f1 window produces
a context similar to the dependency context used
in (Lin, 1998). It provides a more precise con-
text but is only good for comparing words within
the same POS. This is because words of different
POS are typically surrounded by words in differ-
ent syntactic forms. In contrast, a context window
of f4 words allows us to compute semantic simi-
larity between words with different POS.
Examples from our LSA similarity model are
given in Table 1. Pairs 1 to 6 illustrate that the
measure has a good property of differentiating
similar words from non-similar words. Examples
7 and 8 show that the f4 model can detect se-
mantically similar words even with different POS
while the f1 model yields poor results. The pairs
in 9 and 10 show that highly related, but not sub-
stitutable, words may have a strong similarity and
that the f1 model is better at detecting them.
Our word co-occurrence models were based on
a predefined vocabulary of more than 22,000 com-
mon English words and noun phrases. We also
added to it more than 2,000 verb phrases extracted
from WordNet. The final dimensions of our word
co-occurrence matrices are 29,000 x 29,000 when
words are POS tagged. Our vocabulary includes
only open-class words, i.e., nouns, verbs, adjec-
tives and adverbs. There are no proper nouns in
the vocabulary with the only exception of country
names.
Singular Value Decomposition (SVD) has been
found to be effective in improving word similar-
ity measures (Landauer and Dumais, 1997). SVD
is typically applied to a word by document matrix,
yielding the familiar LSA technique. In our case,
we apply it to our word by word matrix (Burgess et
al., 1998). Before performing SVD, we transform
the raw word co-occurrence count fij to its log fre-
quency log(fij +1). We select the 300 largest sin-
gular values and reduce the 29K word vectors to
300 dimensions. The LSA similarity between two
words is defined as the cosine similarity of their
corresponding word vectors after the SVD trans-
formation. See (Han et al., 2013b; Lushan Han,
2014) for examples and more information on the
LSA model.
Statistical word similarity measures have limi-
tations. Related words can have similarity scores
as high as what similar words get, e.g., “doctor”
and “hospital”. Word similarity is typically low
for synonyms that have many word senses since
information about different senses are mashed to-
gether (Han et al., 2013a). To address these issues,
we augment the similarity between two words us-
ing knowledge from WordNet, for example, in-
creasing the score if they are in the same WordNet
synset or if one is a direct or two link hypernym
of the other. See (Han et al., 2013b) for further
details.
</bodyText>
<subsectionHeader confidence="0.999498">
2.2 Word Similarity Wrapper
</subsectionHeader>
<bodyText confidence="0.999937769230769">
Our word similarity model is restricted to the vo-
cabulary size which only comprises open class
words. For words outside of the vocabulary, we
can only rely on their lexical features and deter-
mine equivalence (which we score as 0 or 1, since
a continuous scale makes little sense in this sce-
nario). An analysis of the previous STS datasets
show that out-of-vocabulary words account for
about 25 − 45% of the total words. Datasets like
MSRpar and headlines lie on the higher end of this
spectrum due to the high volume of proper nouns.
In the previous version, we computed a charac-
ter bigram overlap score given by
</bodyText>
<sectionHeader confidence="0.4270955" genericHeader="method">
characterBigramScore =jAnBj
jA U Bj
</sectionHeader>
<bodyText confidence="0.997087">
where A and B are the set of bigrams from the first
and second word respectively. We compare this
</bodyText>
<page confidence="0.990577">
417
</page>
<bodyText confidence="0.999802472222222">
against a preset threshold (0.8) to determine equiv-
alence. While this is reasonable for named enti-
ties, it is not the best approach for other classes.
Named Entities. The wrapper is extended to
handle all classes of named entities that are in-
cluded in Stanford CoreNLP (Finkel et al., 2005).
We use heuristic rules to compute the similarity
between two numbers or two dates. To handle
named entity mentions of people, locations and or-
ganizations, we supplement our character bigram
overlap method with the DBpedia Lookup service
(Mendes et al., 2011). For each entity mention, we
select the DBpedia entity with the most inlinks,
which serves as a good estimate of popularity or
significance (Syed et al., 2010). If the two named
entity mentions map to identical DBpedia entities,
we lower our character bigram overlap threshold
to 0.6.
OOV words. As mentioned earlier, when deal-
ing with out-of-vocabulary words, we only have
its lexical features. A straightforward approach is
to simply get more context for the word. Since
our vocabulary is limited, we need to use external
dictionaries to find the word. For our system, we
use Wordnik (Davidson, 2013), which is a compi-
lation of several dictionaries including The Amer-
ican Heritage Dictionary, Wikitionary and Word-
Net. Wordnik provides a REST API to access sev-
eral attributes for a given word such as it’s defini-
tions, examples, related words etc. For out of vo-
cabulary words, we simply retrieve the word pair’s
top definitions and supply it to our existing STS
system (UMBC, 2013a) to compute its similarity.
As a fallback, in case the word is absent even in
Wordnik, we resort to our character bigram over-
lap measure.
</bodyText>
<sectionHeader confidence="0.968204" genericHeader="method">
3 Multilingual Semantic Text Similarity
</sectionHeader>
<subsectionHeader confidence="0.995594">
3.1 English STS
</subsectionHeader>
<bodyText confidence="0.999936096774193">
For the 2014 STS-English subtask we submitted
three runs. They all used a simple term alignment
strategy to compute sentence similarities. The first
run was an unsupervised approach that used the
basic word-similarity model for term alignment.
The next two used a supervised approach to com-
bine the scores from the first run with alignment
scores using the enhanced word-similarity wrap-
per. The two runs differed in their training.
Align and Penalize Approach. The pairingWord
run was produced by the same Align-and-Penalize
system (Han et al., 2013b) that we used in the
2013 STS task with only minor changes. The
biggest change is that we included a small list
of disjoint concepts (Han et al., 2013b) that are
used in the penalization phase, such as {piano, vi-
olin} and {dog, cat}. The disjoint concepts were
manually collected from the MSRvid dataset pro-
vided by the 2012 STS task because we still lack a
reliable general method to automatically produce
them. The list only contains 23 pairs, which can
be downloaded at (dis, 2014).
We also slightly adjusted our stopword list.
We removed a few words that appear in the trial
datasets of 2013 STS task (e.g., frame) but we did
not add any new stopwords for this year’s task. All
the changes are small and we made them only in
the hope that they can slightly improve our system.
Unlike machine learning methods that require
manually selecting an appropriate trained model
for a particular test dataset, our unsupervised
Align-and-Penalize system is applied uniformly
to all six test datasets in 2014 STS task, namely,
deft-forum, deft-news, headlines, images, OnWN
and tweet-news. It achieves the second best rank
among all submitted runs.
Supervised Machine Learning. Our second and
third runs used machine learning approaches sim-
ilar to those we developed for the 2013 STS task
but with significant changes in both pre-processing
and the features extracted.
The most significant pre-processing change was
the use of Stanford coreNLP (Finkel et al., 2005)
tool for tokenization, part-of-speech tagging and
identifying named entity mentions. For the tweet-
news dataset we also removed the hashtag symbol
(‘#’) prior to applying the Stanford tools. We use
only open class words and named entity mentions
and remove all other tokens.
We align tokens between two sentences based
on the updated word similarity wrapper that was
described in Section 2.2. We use information
content from Google word frequencies for word
weights similar to our approach last year. The
alignment process is a many-to-one mapping sim-
ilar to the Align and Penalize approach and two
tokens are only aligned if their similarity is greater
than 0.1. The sentence similarity score is then
computed as the average of the scores of their
aligned tokens. This score, along with the Align
and Penalize approach score, are used as features
to train support vector regression (SVR) models.
</bodyText>
<page confidence="0.993258">
418
</page>
<bodyText confidence="0.999860285714286">
We use an epsilon SVR with a radial basis kernel
function and use a grid search to get the optimal
parameter values for cost, gamma and epsilon. We
use datasets from the previous STS tasks as train-
ing data and the two submitted runs differ in the
choice of their training data.
The first approach, named Hulk, is an attempt
to use a generic model trained on a large data set.
The SVR model uses a total of 3750 sentence pairs
(1500 from MSRvid, 1500 from MSRpar and 750
from headlines) for training. Datasets like SMT
were excluded due to poor quality.
The second approach, named Super Saiyan,
is an attempt at domain specific training. For
OnWN, we used 1361 sentence pairs from previ-
ous OnWN dataset. For Images, we used 1500
sentence pairs from MSRvid dataset. The others
lacked any domain specific training data so we
used a generic training dataset comprising 5111
sentence pairs from MSRvid, MSRpar, headlines
and OnWN datasets.
</bodyText>
<subsectionHeader confidence="0.998739">
3.2 Spanish STS
</subsectionHeader>
<bodyText confidence="0.999970777777778">
As a base-line for this task we first considered
translating the Spanish sentences to English and
running the same systems explained for the En-
glish Subtask (i.e., pairingWord and Hulk). The
results obtained applying this approach to the pro-
vided training data gave a correlation of 0.777 so,
we selected this approach (with some improve-
ments) for the competition.
Translating the sentences. For the automatic
translation of the sentences from Spanish to En-
glish we used the Google Translate API1, a
free, multilingual machine-translation product by
Google. Google Translate presents very accurate
translations for European languages by using sta-
tistical machine translation (Brown et al., 1990)
where the translations are generated on the basis of
statistical models derived from bilingual text cor-
pora. In fact, Google used as part of this corpora
200 billion words from United Nations documents
that are typically published in all six official UN
languages, including English and Spanish.
In the experiments performed with the trial data
we manually evaluated the quality of the trans-
lations (one of the authors is a native Spanish
speaker). The overall translation was very accu-
rate but some statistical anomalies, incorrect trans-
lations due to the abundance of a specific sense of
</bodyText>
<footnote confidence="0.994957">
1http://translate.google.com
</footnote>
<construct confidence="0.299204">
I1: Las costas o costa de un mar, lago o extenso rio es la
tierra a lo largo del borde de estos.
</construct>
<bodyText confidence="0.9715505">
T11: Costs or the cost of a sea, lake or wide river is the
land along the edge of these.
T12: Coasts or the coast of a sea, lake or wide river is the
land along the edge of these.
T13: Coasts or the coast of a sea, lake or wide river is the
land along the border of these.
</bodyText>
<figure confidence="0.867257">
...
</figure>
<figureCaption confidence="0.965986">
Figure 1: Three of the English translations for the
Spanish sentence I1.
</figureCaption>
<bodyText confidence="0.995263891891892">
a word in the training set, appeared.
On one hand, some homonym words are
wrongly translated. For example, the Spanish sen-
tence “Las costas o costa de un mar [...]” was
translated to “Costs or the cost of a sea [...]”.
The Spanish word costa has two different senses:
“coast” (the shore of a sea or ocean) and “‘cost”
(the property of having material worth). On the
other hand, some words are translated preserving
their semantics but with a slightly different mean-
ing. For example, the Spanish sentence “Un cojin
es una funda de tela [...]” was correctly translated
to “A cushion is a fabric cover [...]”. However,
the Spanish sentence “Una almohada es un cojin
en forma rectangular [...]” was translated to “A
pillow is a rectangularpad [...]”2.
Dealing with statistical anomalies. The afore-
mentioned problem of statistical machine transla-
tion caused a slightly adverse effect when comput-
ing the similarity of two English (translated from
Spanish) sentences with the systems explained in
Section 3.1. Therefore, we improved the direct
translation approach by taking into account the
different possible translations for each word in a
Spanish sentence. For that, our system used the in-
formation provided by the Google Translate API,
that is, all the possible translations for every word
of the sentence along with a popularity value. For
each Spanish sentence the system generates all its
possible translations by combining the different
possible translations of each word. For example,
Figure 1 shows three of the English sentences gen-
erated for a given Spanish sentence from the trial
data.
As a way of controlling the combinatorial ex-
plosion of this step, especially for long sentences,
we limited the maximum number of generated
</bodyText>
<footnote confidence="0.994724">
2Notice that both Spanish sentences used the term cojin
that should be translated as cushion (the Spanish word for
pad is almohadilla).
</footnote>
<page confidence="0.998507">
419
</page>
<bodyText confidence="0.998241272727273">
sentences for each Spanish sentence to 20 and
we only selected words with a popularity greater
than 65. We arrived at the popularity threshold
through experimentation on every sentence in the
trial data set. After this filtering, our input for
the “news” and “wikipedia” tests went from 480
and 324 pairs of sentences to 5756 and 1776 pairs,
respectively.
Given a pair of Spanish sentences, I1
and I2, and the set of possible translations
generated by our system for each sentence,
</bodyText>
<equation confidence="0.8926896">
TI1 = {T11, T12, T13, ... , T1n} and TI2 =
{T21, T22, . . . , T2m}, we compute the similarity
between them by using the following formula:
SimENG(T1i, T2j)
n ∗ m
</equation>
<bodyText confidence="0.99992675">
where SimENG(x, y) computes the similarity of
two English sentences using our existing STS sys-
tem (Han et al., 2013b).
For the final competition we submitted three
runs. The first (Pairing in Table 3) used the
pairingWord system with the direct translation of
the Spanish sentences to English. The second
run (PairingAvg in Table 3) used the formula for
SimSPA(x, y) based on SimENG(x, y) with
the pairingWord system. Finally, the third one
(Hulk in Table 3) used the Hulk system with the
direct translation.
</bodyText>
<sectionHeader confidence="0.993403" genericHeader="method">
4 Cross Level Similarity
</sectionHeader>
<subsectionHeader confidence="0.999924">
4.1 Sentence to Paragraph/Phrase
</subsectionHeader>
<bodyText confidence="0.999978181818182">
We used the three systems developed for the En-
glish sentence similarity subtask and described in
Section 3.1 for both the sentence to paragraph and
sentence to phrase subtasks, producing three runs.
The model for Hulk remained the same (trained
on 3750 sentence pairs from MSRvid, MSRpar
and headlines dataset) but the SuperSaiyan sys-
tem, which is the domain specific approach, used
the given train and trial text pairs (about 530) for
the respective subtasks as training to generate task
specific models.
</bodyText>
<subsectionHeader confidence="0.996877">
4.2 Phrase to Word
</subsectionHeader>
<bodyText confidence="0.999989538461539">
In our initial experiments, we directly computed
the phrase-word pair similarity using our English
STS. This yielded a very low correlation of 0.239
for the training set, primarily due to the absence of
these phrases and words in our vocabulary. To ad-
dress this issue, we used external sources to obtain
more contextual information and extracted several
features.
Dictionary features. We used Wordnik as a dic-
tionary resource and retrieved definitions and us-
age examples for the word. We then used our
English STS system to measure the similarity be-
tween these and the given phrase to extract two
features.
Web search features. These features were based
on the hypothesis that if a word and phrase have
similar meanings, then a web search that combines
the word and phrase should return similar docu-
ments when compared to a web search for each
individually.
We implemented this idea by comparing results
of three search queries: the word alone, the phrase
alone, and the word and phrase together.
Using the Bing Search API (BIN, 2014), we re-
trieved the top five results for each search, indexed
them with Lucene (Hatcher et al., 2004), and ex-
tracted term frequency vectors for each of the three
search result document sets. For the phrase ’spill
the beans’ and word ’confess’, for example, we
built a Lucene index for the set of documents re-
trieved by a Bing search for ’spill the beans’, ’con-
fess’, and ’spill the beans confess’. We calculated
the similarity of pairs of search result sets using
the cosine similarity (1) of their term frequency
vectors.
We calculated the mean and minimum sim-
ilarity of pairs of results for the phrase and
phrase+word searches. These features were ex-
tracted from the provided training set and used in
conjunction with the dictionary features to train
an SVM regression model to predict similarity
scores.
We observed this method can be problematic
when a word or phrase has multiple meanings.
For example, ’spill the beans’ relates to ’confess-
ing’ but it is also the name of a coffee shop and
a soup shop. A mix of these pages do get re-
turned by Bing and reduces the accuracy of our re-
sults. However, we found that this technique often
strengthens evidence of similarity enough that it
improves our overall accuracy when used in com-
bination with our dictionary features.
</bodyText>
<equation confidence="0.999619384615385">
SimSPA(I1, I2) =
Pn
i=1
M
P
j=1
Pn
r i=1 in
Pn(1)
(V 1i)2 × r E(V 2i)2
i=1
CosineSimilarity =
V 1i × V2i
</equation>
<page confidence="0.997612">
420
</page>
<tableCaption confidence="0.816829555555555">
Dante#n#1: an Italian poet famous for writing the
Divine Comedy that describes a journey through Hell and
purgatory and paradise guided by Virgil and his idealized
Beatrice
writer#n#1: writes books or stories or articles or the like
professionally for pay
generator#n#3: someone who originates or causes or
initiates something, “he was the generator of several
complaints”
</tableCaption>
<note confidence="0.643131">
author#v#1: be the author of, “She authored this play”
</note>
<figureCaption confidence="0.9992745">
Figure 2: The WordNet sense for Dante#n#1 and
the three author#n senses.
</figureCaption>
<subsectionHeader confidence="0.773994">
4.3 Word to Sense
</subsectionHeader>
<table confidence="0.999790875">
Dataset Pairing Hulk SuperSaiyan
deft-forum 0.4711 (9) 0.4495 (15) 0.4918 (4)
deft-news 0.7628 (8) 0.7850 (1) 0.7712 (3)
headlines 0.7597 (8) 0.7571 0.7666 (2)
images 0.8013 (7) 0.7896 0.7676 (18)
OnWN 0.8745 0.7872 (18) 0.8022 (12)
tweet-news 0.7793 0.7571 (7) 0.7651 (4)
Weighted Mean 0.7605 (2) 0.7349 (6) 0.7410 (5)
</table>
<tableCaption confidence="0.9410485">
Table 2: Performance of our three systems on the
six English test sets.
</tableCaption>
<table confidence="0.99988775">
Dataset Pairing PairingAvg Hulk
Wikipedia 0.6682 (12) 0.7431 (6) 0.7382 (8)
News 0.7852 (12) 0.8454 (1) 0.8225 (6)
Weighted Mean 0.7380 (13) 0.8042 (2) 0.7885 (5)
</table>
<tableCaption confidence="0.994366">
Table 3: Performance of our three systems on the
two Spanish test sets.
</tableCaption>
<bodyText confidence="0.999858303030303">
For this subtask, we used external resources to re-
trieve more contextual information. For a given
word, we retrieved its synonym set from WordNet
along with their corresponding definitions. We re-
trieved the WordNet definition for the word sense
as well. For example, given a word-sense pair
(author#n, Dante#n#1), we retrieved the synset of
author#n (writer.n.01, generator.n.03, author.v.01)
along with their WordNet definitions and the sense
definition of Dante#n#1. Figure 2 shows the
WordNet data for this example.
By pairing every combination of the word’s
synset and their corresponding definitions with the
sense’s surface form and definition, we created
four features. For each feature, we used our En-
glish STS system to compare their semantic sim-
ilarity and kept the maximum score as feature’s
value.
We found that about 10% of the training
dataset’s words fell outside of WordNet’s vocab-
ulary. Examples of missing words included many
informal or “slang” words like kegger, crackberry
and post-season. To address this, we used Word-
nik to retrieve the word’s top definition and com-
puted its similarity with the sense. This reduced
the out-of-vocabulary words to about 2% for the
training data. Wordnik thus gave us two addi-
tional features: the maximum semantic similarity
score of word-sense using Wordnik’s additional
definitions for all words and for just the out-of-
vocabulary words. We used these features to train
an SVM regression model with the provided train-
ing set to predict similarity scores.
</bodyText>
<sectionHeader confidence="0.999905" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.982599482758621">
Multilingual Semantic Text Similarity. Table
2 shows the system performance for the English
STS task. Our best performing system ranked
second 3, behind first place by only 0.0005.
It employs an unsupervised approach with no
training data required. The supervised systems
that handled named entity recognition and out-
of-vocabulary words performed slightly better on
datasets in the news domain but still suffered from
noise due to diverse training datasets.
Table 3 shows the performance for the Spanish
subtask. The best run achieved a weighted correla-
tion of 0.804, behind first place by only 0.003. The
Hulk system was similar to the Pairing run and
used only one translation per sentence. The per-
formance boost could be attributed to large num-
ber of named entities in the News and Wikipedia
datasets.
Cross Level Similarity. Table 4 shows our per-
formance in the Cross Level Similarity tasks. The
Paragraph-Sentence and Sentence-Phrase yielded
good results (ranked second and first respectively)
with our English STS system because of sufficient
amount of textual information. The correlation
scores dropped as the granularity level of the text
got finer.
The Phrase-Word run achieved a correlation of
0.457, the highest for the subtask. However, an
incorrect file was submitted prior to the deadline
</bodyText>
<footnote confidence="0.956137666666667">
3An incorrect file for ‘deft-forum’ dataset was submitted.
The correct version had a correlation of 0.4896 instead of
0.4710. This would have placed it at rank 1 overall.
</footnote>
<page confidence="0.993127">
421
</page>
<note confidence="0.975243777777778">
Wordnik BingSim Score
ID S1 S2 Baseline Definitions Example Sim Avg Min SVM GS Error
Idiomatic-212 spill the beans confess 0 0 0 0.0282 0.1516 0.1266 0.5998 4.0 3.4002
Idiomatic-292 screw the pooch mess up 0 0.04553 0.0176 0.0873 0.4238 0.0687 0.7185 4.0 3.2815
Idiomatic-273 on a shoogly peg insecure 0 0.0793 0 0.0846 0.3115 0.1412 0.8830 4.0 3.1170
Slang-115 wacky tabaccy cannabis 0 0 0 0.0639 0.4960 0.1201 0.5490 4.0 3.4510
Slang-26 pray to the porcelain god vomiting 0 0 0 0.0934 0.5275 0.0999 0.6452 4.0 3.3548
Slang-79 rock and roll commence 0 0.2068 0.0720 0.0467 0.5106 0.0560 0.8820 4.0 3.1180
Newswire-160 exercising rights under canon law lawyer 0.0044 0.6864 0.0046 0.3642 0.4990 0.2402 3.5562 0.5 3.0562
</note>
<tableCaption confidence="0.998583">
Table 5: Examples where our algorithm performed poorly and the scores for individual features.
</tableCaption>
<table confidence="0.999726">
Dataset Pairing Hulk SuperSaiyan WordExpand
Para.-Sent. 0.794 (10) 0.826 (4) 0.834 (2)
Sent.-Phrase 0.704 (14) 0.705 (13) 0.777 (1)
Phrase-Word 0.457 (1)
Word-Sense 0.389 (1)
</table>
<tableCaption confidence="0.9777655">
Table 4: Performance of our systems on the four
Cross-Level Subtasks.
</tableCaption>
<figureCaption confidence="0.99705">
Figure 3: Average error with respect to category.
</figureCaption>
<bodyText confidence="0.999926304347826">
which meant that this was not included in the of-
ficial results. Figure 3 shows the average error
(measured as the average deviation from the gold
standard) across different categories for phrase to
word subtask. Our performance is slightly worse
for slang and idiomatic categories when compared
to others which is due to two reasons: (i) the se-
mantics of idioms is not compositional, reducing
the effectiveness of a distributional similarity mea-
sure and (ii) dictionary-based features often failed
to find definitions and/or examples of idioms. Ta-
ble 5 shows some of the words where our algo-
rithm performed poorly and their scores for indi-
vidual features.
The Word-Sense run ranked first in the sub-
task with a correlation score of 0.389. Table 6
shows some of the word-sense pairs where the
system performed poorly. Our system only used
Wordnik’s top definition which was not always the
right one to use to detect the similarity. For ex-
ample, the first definition of cheese#n is “a solid
food prepared from the pressed curd of milk” but
there is a latter, less prominent one, which is
</bodyText>
<table confidence="0.987413">
ID word sense key sense number predicted gold
80 cheese#n moolah%1:21:00:: moolah#n#1 0.78 4
377 bone#n chalk%1:07:00:: chalk#n#2 1.52 4
441 wasteoid#n drug user%1:18:00:: drug user#n#1 0.78 3
</table>
<tableCaption confidence="0.8433">
Table 6: Examples where our system performed
poorly.
</tableCaption>
<bodyText confidence="0.9927719">
“money”. A second problem is that some words,
like wasteoid#n, were absent even in Wordnik.
Using additional online lexical resources to in-
clude more slangs and idioms, like the Urban Dic-
tionary (Urb, 2014), could address these issues.
However, care must be taken since the quality of
some content is questionable. For example, the
Urban Dictionary’s first definition of “program-
mer” is “An organism capable of converting caf-
feine into code”.
</bodyText>
<sectionHeader confidence="0.999199" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998113">
We described our submissions to the Multilingual
Semantic Textual Similarity (Task 10) and Cross-
Level Semantic Similarity (Task 3) tasks for the
2014 International Workshop on Semantic Eval-
uation. Our best runs ranked second in both En-
glish and Spanish subtasks for Task 10 while rank-
ing first in Sentence-Phrase, Phrase-Word, Word-
Sense tasks and second in Paragraph-Sentence
subtasks for Task 3. Our success is attributed to
a powerful word similarity model based on LSA
word similarity and WordNet knowledge. We
used new linguistic resources like Wordnik to im-
prove our existing system for the Phrase-Word and
Word-Sense tasks and plan to include other re-
sources like “Urban dictionary” in the future.
</bodyText>
<sectionHeader confidence="0.997669" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.970803333333333">
This research was supported by awards 1228198,
1250627 and 0910838 from the U.S. National Sci-
ence Foundation.
</bodyText>
<page confidence="0.997795">
422
</page>
<sectionHeader confidence="0.993852" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999895666666667">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
2014. BING search API. http://bing.com/developers-
/s/APIBasics.html.
Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Fredrick Jelinek, John D Laf-
ferty, Robert L Mercer, and Paul S Roossin. 1990.
A statistical approach to machine translation. Com-
putational linguistics, 16(2):79–85.
Curt Burgess, Kay Livesay, and Kevin Lund. 1998.
Explorations in context space: Words, sentences,
discourse. Discourse Processes, 25(2-3):211–257.
Sara Davidson. 2013. Wordnik. The Charleston Advi-
sor, 15(2):54–58.
2014. Disjoint concept pairs. http://semanticweb-
archive.cs.umbc.edu/disjointConcepts.txt.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2005. Incorporating non-local
information into information extraction systems by
gibbs sampling. In 43rd Annual Meeting of the ACL,
pages 363–370.
Lushan Han and Tim Finin. 2013. UMBC webbase
corpus. http://ebiq.org/r/351.
Lushan Han, Tim Finin, Paul McNamee, Anupam
Joshi, and Yelena Yesha. 2013a. Improving Word
Similarity by Augmenting PMI with Estimates of
Word Polysemy. IEEE Trans. on Knowledge and
Data Engineering, 25(6):1307–1322.
Lushan Han, Abhay L. Kashyap, Tim Finin,
James Mayfield, and Johnathan Weese. 2013b.
UMBC EBIQUITY-CORE: Semantic Textual
Similarity Systems. In 2nd Joint Conf. on Lexical
and Computational Semantics. ACL, June.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York, USA.
Erik Hatcher, Otis Gospodnetic, and Michael McCand-
less. 2004. Lucene in action. Manning Publications
Greenwich, CT.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
104(2):211.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. 17th Int. Conf. on Compu-
tational Linguistics, pages 768–774, Montreal, CN.
Lushan Han. 2014. Schema Free Querying of Seman-
tic Data. Ph.D. thesis, University of Maryland, Bal-
timore County.
Pablo N Mendes, Max Jakob, Andr´es Garc´ıa-Silva, and
Christian Bizer. 2011. Dbpedia spotlight: shedding
light on the web of documents. In 7th Int. Conf. on
Semantic Systems, pages 1–8. ACM.
Stanford. 2001. Stanford WebBase project.
http://bit.ly/WebBase.
Zareen Syed, Tim Finin, Varish Mulwad, and Anupam
Joshi. 2010. Exploiting a Web of Semantic Data for
Interpreting Tables. In Proceedings of the Second
Web Science Conference, April.
Kristina Toutanova, Dan Klein, Christopher Manning,
William Morgan, Anna Rafferty, and Michel Gal-
ley. 2000. Stanford log-linearpart-of-speech tagger.
http://nlp.stanford.edu/software/tagger.shtml.
UMBC. 2013a. Semantic similarity demonstration.
http://swoogle.umbc.edu/SimService/.
UMBC. 2013b. Umbc graph of relations project.
http://ebiq.org/j/95.
2014. Urban dictionary. http://urbandictionary.com/.
</reference>
<page confidence="0.999421">
423
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.623521">
<title confidence="0.9982045">Meerkat Mafia: Multilingual and Semantic Textual Similarity Systems</title>
<author confidence="0.876361">Abhay Kashyap</author>
<author confidence="0.876361">Lushan Han</author>
<author confidence="0.876361">Roberto Yus</author>
<author confidence="0.876361">Jennifer Taneeya Satyapanich</author>
<author confidence="0.876361">Sunil Gandhi</author>
<author confidence="0.876361">Tim</author>
<affiliation confidence="0.993663">University of Maryland, Baltimore</affiliation>
<address confidence="0.999789">Baltimore, MD 21250</address>
<abstract confidence="0.9741952">We describe UMBC’s systems developed the SemEval 2014 tasks on Multilingual Semantic Textual Similarity (Task Semantic Similarity Our best submission in the Multilingual task ranked second in both English and Spanish subtasks using an unsupervised approach. Our best systems for Cross-Level task ranked second in Paragraph-Sentence and first in both Sentence-Phrase and Word-Sense subtask. The system ranked first for the Phrase- Word subtask but was not included in the official results due to a late submission.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Wiebe</author>
</authors>
<title>SemEval-2014 Task 10: Multilingual semantic textual similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1198" citStr="Agirre et al., 2014" startWordPosition="167" endWordPosition="170">asks using an unsupervised approach. Our best systems for Cross-Level task ranked second in Paragraph-Sentence and first in both Sentence-Phrase and Word-Sense subtask. The system ranked first for the PhraseWord subtask but was not included in the official results due to a late submission. 1 Introduction We describe the semantic text similarity systems we developed for two of the SemEval tasks for the 2014 International Workshop on Semantic Evaluation. We developed systems for task 3, CrossLevel Semantic Similarity (Jurgens et al., 2014), and task 10, Multilingual Semantic Textual Similarity (Agirre et al., 2014). A key component in all the systems was an enhanced version of the word similarity system used in our entry (Han et al., 2013b) in the 2013 SemEval Semantic Textual Similarity task. Our best system in the Multilingual Semantic Textual Similarity task used an unsupervised approach and ranked second in both the English and Spanish subtasks. In the Cross-Level Semantic Similarity task we developed a number of new algorithms and used new linguistic data resources. In this task, our best systems ranked second in the Paragraph-Sentence task, first in the SentencePhrase task and first in the Word-Se</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Wiebe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 Task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland.</rawString>
</citation>
<citation valid="false">
<date>2014</date>
<note>BING search API. http://bing.com/developers/s/APIBasics.html.</note>
<marker>2014</marker>
<rawString>2014. BING search API. http://bing.com/developers/s/APIBasics.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Fredrick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational linguistics,</journal>
<pages>16--2</pages>
<contexts>
<context position="14639" citStr="Brown et al., 1990" startWordPosition="2422" endWordPosition="2425">tences to English and running the same systems explained for the English Subtask (i.e., pairingWord and Hulk). The results obtained applying this approach to the provided training data gave a correlation of 0.777 so, we selected this approach (with some improvements) for the competition. Translating the sentences. For the automatic translation of the sentences from Spanish to English we used the Google Translate API1, a free, multilingual machine-translation product by Google. Google Translate presents very accurate translations for European languages by using statistical machine translation (Brown et al., 1990) where the translations are generated on the basis of statistical models derived from bilingual text corpora. In fact, Google used as part of this corpora 200 billion words from United Nations documents that are typically published in all six official UN languages, including English and Spanish. In the experiments performed with the trial data we manually evaluated the quality of the translations (one of the authors is a native Spanish speaker). The overall translation was very accurate but some statistical anomalies, incorrect translations due to the abundance of a specific sense of 1http://t</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Fredrick Jelinek, John D Lafferty, Robert L Mercer, and Paul S Roossin. 1990. A statistical approach to machine translation. Computational linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Curt Burgess</author>
<author>Kay Livesay</author>
<author>Kevin Lund</author>
</authors>
<date>1998</date>
<booktitle>Explorations in context space: Words, sentences, discourse. Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="6550" citStr="Burgess et al., 1998" startWordPosition="1075" endWordPosition="1078">,000 verb phrases extracted from WordNet. The final dimensions of our word co-occurrence matrices are 29,000 x 29,000 when words are POS tagged. Our vocabulary includes only open-class words, i.e., nouns, verbs, adjectives and adverbs. There are no proper nouns in the vocabulary with the only exception of country names. Singular Value Decomposition (SVD) has been found to be effective in improving word similarity measures (Landauer and Dumais, 1997). SVD is typically applied to a word by document matrix, yielding the familiar LSA technique. In our case, we apply it to our word by word matrix (Burgess et al., 1998). Before performing SVD, we transform the raw word co-occurrence count fij to its log frequency log(fij +1). We select the 300 largest singular values and reduce the 29K word vectors to 300 dimensions. The LSA similarity between two words is defined as the cosine similarity of their corresponding word vectors after the SVD transformation. See (Han et al., 2013b; Lushan Han, 2014) for examples and more information on the LSA model. Statistical word similarity measures have limitations. Related words can have similarity scores as high as what similar words get, e.g., “doctor” and “hospital”. Wor</context>
</contexts>
<marker>Burgess, Livesay, Lund, 1998</marker>
<rawString>Curt Burgess, Kay Livesay, and Kevin Lund. 1998. Explorations in context space: Words, sentences, discourse. Discourse Processes, 25(2-3):211–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Davidson</author>
</authors>
<date>2013</date>
<journal>Wordnik. The Charleston Advisor,</journal>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="9508" citStr="Davidson, 2013" startWordPosition="1579" endWordPosition="1580">t al., 2011). For each entity mention, we select the DBpedia entity with the most inlinks, which serves as a good estimate of popularity or significance (Syed et al., 2010). If the two named entity mentions map to identical DBpedia entities, we lower our character bigram overlap threshold to 0.6. OOV words. As mentioned earlier, when dealing with out-of-vocabulary words, we only have its lexical features. A straightforward approach is to simply get more context for the word. Since our vocabulary is limited, we need to use external dictionaries to find the word. For our system, we use Wordnik (Davidson, 2013), which is a compilation of several dictionaries including The American Heritage Dictionary, Wikitionary and WordNet. Wordnik provides a REST API to access several attributes for a given word such as it’s definitions, examples, related words etc. For out of vocabulary words, we simply retrieve the word pair’s top definitions and supply it to our existing STS system (UMBC, 2013a) to compute its similarity. As a fallback, in case the word is absent even in Wordnik, we resort to our character bigram overlap measure. 3 Multilingual Semantic Text Similarity 3.1 English STS For the 2014 STS-English </context>
</contexts>
<marker>Davidson, 2013</marker>
<rawString>Sara Davidson. 2013. Wordnik. The Charleston Advisor, 15(2):54–58.</rawString>
</citation>
<citation valid="true">
<title>Disjoint concept pairs.</title>
<date>2014</date>
<note>http://semanticwebarchive.cs.umbc.edu/disjointConcepts.txt.</note>
<marker>2014</marker>
<rawString>2014. Disjoint concept pairs. http://semanticwebarchive.cs.umbc.edu/disjointConcepts.txt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In 43rd Annual Meeting of the ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="8645" citStr="Finkel et al., 2005" startWordPosition="1437" endWordPosition="1440">atasets like MSRpar and headlines lie on the higher end of this spectrum due to the high volume of proper nouns. In the previous version, we computed a character bigram overlap score given by characterBigramScore =jAnBj jA U Bj where A and B are the set of bigrams from the first and second word respectively. We compare this 417 against a preset threshold (0.8) to determine equivalence. While this is reasonable for named entities, it is not the best approach for other classes. Named Entities. The wrapper is extended to handle all classes of named entities that are included in Stanford CoreNLP (Finkel et al., 2005). We use heuristic rules to compute the similarity between two numbers or two dates. To handle named entity mentions of people, locations and organizations, we supplement our character bigram overlap method with the DBpedia Lookup service (Mendes et al., 2011). For each entity mention, we select the DBpedia entity with the most inlinks, which serves as a good estimate of popularity or significance (Syed et al., 2010). If the two named entity mentions map to identical DBpedia entities, we lower our character bigram overlap threshold to 0.6. OOV words. As mentioned earlier, when dealing with out</context>
<context position="12092" citStr="Finkel et al., 2005" startWordPosition="2004" endWordPosition="2007">ecting an appropriate trained model for a particular test dataset, our unsupervised Align-and-Penalize system is applied uniformly to all six test datasets in 2014 STS task, namely, deft-forum, deft-news, headlines, images, OnWN and tweet-news. It achieves the second best rank among all submitted runs. Supervised Machine Learning. Our second and third runs used machine learning approaches similar to those we developed for the 2013 STS task but with significant changes in both pre-processing and the features extracted. The most significant pre-processing change was the use of Stanford coreNLP (Finkel et al., 2005) tool for tokenization, part-of-speech tagging and identifying named entity mentions. For the tweetnews dataset we also removed the hashtag symbol (‘#’) prior to applying the Stanford tools. We use only open class words and named entity mentions and remove all other tokens. We align tokens between two sentences based on the updated word similarity wrapper that was described in Section 2.2. We use information content from Google word frequencies for word weights similar to our approach last year. The alignment process is a many-to-one mapping similar to the Align and Penalize approach and two t</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In 43rd Annual Meeting of the ACL, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Tim Finin</author>
</authors>
<date>2013</date>
<note>UMBC webbase corpus. http://ebiq.org/r/351.</note>
<contexts>
<context position="4559" citStr="Han and Finin, 2013" startWordPosition="731" endWordPosition="734">and, August 23-24, 2014. Word pair ±4 model ±1 model 1 doctor NN, physician NN 0.775 0.726 2 car NN, vehicle NN 0.748 0.802 3 person NN, car NN 0.038 0.024 4 car NN, country NN 0.000 0.016 5 person NN, country NN 0.031 0.069 6 child NN, marry VB 0.098 0.000 7 wife NN, marry VB 0.548 0.274 8 author NN, write VB 0.364 0.128 9 doctor NN, hospital NN 0.473 0.347 10 car NN, driver NN 0.497 0.281 Table 1: Examples from the LSA similarity model. tions, truncated text, non-English text and strange characters) and produced a three billion word corpus of high quality English, which is available online (Han and Finin, 2013). We performed POS tagging and lemmatization on the corpus using the Stanford POS tagger (Toutanova et al., 2000). Word/term cooccurrences were counted in a moving window of a fixed size that scans the entire corpus. We generated two co-occurrence models using window sizes f1 and f4 because we observed different natures of the models. f1 window produces a context similar to the dependency context used in (Lin, 1998). It provides a more precise context but is only good for comparing words within the same POS. This is because words of different POS are typically surrounded by words in different </context>
</contexts>
<marker>Han, Finin, 2013</marker>
<rawString>Lushan Han and Tim Finin. 2013. UMBC webbase corpus. http://ebiq.org/r/351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Tim Finin</author>
<author>Paul McNamee</author>
<author>Anupam Joshi</author>
<author>Yelena Yesha</author>
</authors>
<title>Improving Word Similarity by Augmenting PMI with Estimates of Word Polysemy.</title>
<date>2013</date>
<journal>IEEE Trans. on Knowledge and Data Engineering,</journal>
<volume>25</volume>
<issue>6</issue>
<contexts>
<context position="1324" citStr="Han et al., 2013" startWordPosition="191" endWordPosition="194">entence-Phrase and Word-Sense subtask. The system ranked first for the PhraseWord subtask but was not included in the official results due to a late submission. 1 Introduction We describe the semantic text similarity systems we developed for two of the SemEval tasks for the 2014 International Workshop on Semantic Evaluation. We developed systems for task 3, CrossLevel Semantic Similarity (Jurgens et al., 2014), and task 10, Multilingual Semantic Textual Similarity (Agirre et al., 2014). A key component in all the systems was an enhanced version of the word similarity system used in our entry (Han et al., 2013b) in the 2013 SemEval Semantic Textual Similarity task. Our best system in the Multilingual Semantic Textual Similarity task used an unsupervised approach and ranked second in both the English and Spanish subtasks. In the Cross-Level Semantic Similarity task we developed a number of new algorithms and used new linguistic data resources. In this task, our best systems ranked second in the Paragraph-Sentence task, first in the SentencePhrase task and first in the Word-Sense task. The This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedin</context>
<context position="6912" citStr="Han et al., 2013" startWordPosition="1137" endWordPosition="1140">ound to be effective in improving word similarity measures (Landauer and Dumais, 1997). SVD is typically applied to a word by document matrix, yielding the familiar LSA technique. In our case, we apply it to our word by word matrix (Burgess et al., 1998). Before performing SVD, we transform the raw word co-occurrence count fij to its log frequency log(fij +1). We select the 300 largest singular values and reduce the 29K word vectors to 300 dimensions. The LSA similarity between two words is defined as the cosine similarity of their corresponding word vectors after the SVD transformation. See (Han et al., 2013b; Lushan Han, 2014) for examples and more information on the LSA model. Statistical word similarity measures have limitations. Related words can have similarity scores as high as what similar words get, e.g., “doctor” and “hospital”. Word similarity is typically low for synonyms that have many word senses since information about different senses are mashed together (Han et al., 2013a). To address these issues, we augment the similarity between two words using knowledge from WordNet, for example, increasing the score if they are in the same WordNet synset or if one is a direct or two link hype</context>
<context position="10632" citStr="Han et al., 2013" startWordPosition="1762" endWordPosition="1765">p measure. 3 Multilingual Semantic Text Similarity 3.1 English STS For the 2014 STS-English subtask we submitted three runs. They all used a simple term alignment strategy to compute sentence similarities. The first run was an unsupervised approach that used the basic word-similarity model for term alignment. The next two used a supervised approach to combine the scores from the first run with alignment scores using the enhanced word-similarity wrapper. The two runs differed in their training. Align and Penalize Approach. The pairingWord run was produced by the same Align-and-Penalize system (Han et al., 2013b) that we used in the 2013 STS task with only minor changes. The biggest change is that we included a small list of disjoint concepts (Han et al., 2013b) that are used in the penalization phase, such as {piano, violin} and {dog, cat}. The disjoint concepts were manually collected from the MSRvid dataset provided by the 2012 STS task because we still lack a reliable general method to automatically produce them. The list only contains 23 pairs, which can be downloaded at (dis, 2014). We also slightly adjusted our stopword list. We removed a few words that appear in the trial datasets of 2013 ST</context>
<context position="18339" citStr="Han et al., 2013" startWordPosition="3055" endWordPosition="3058">through experimentation on every sentence in the trial data set. After this filtering, our input for the “news” and “wikipedia” tests went from 480 and 324 pairs of sentences to 5756 and 1776 pairs, respectively. Given a pair of Spanish sentences, I1 and I2, and the set of possible translations generated by our system for each sentence, TI1 = {T11, T12, T13, ... , T1n} and TI2 = {T21, T22, . . . , T2m}, we compute the similarity between them by using the following formula: SimENG(T1i, T2j) n ∗ m where SimENG(x, y) computes the similarity of two English sentences using our existing STS system (Han et al., 2013b). For the final competition we submitted three runs. The first (Pairing in Table 3) used the pairingWord system with the direct translation of the Spanish sentences to English. The second run (PairingAvg in Table 3) used the formula for SimSPA(x, y) based on SimENG(x, y) with the pairingWord system. Finally, the third one (Hulk in Table 3) used the Hulk system with the direct translation. 4 Cross Level Similarity 4.1 Sentence to Paragraph/Phrase We used the three systems developed for the English sentence similarity subtask and described in Section 3.1 for both the sentence to paragraph and </context>
</contexts>
<marker>Han, Finin, McNamee, Joshi, Yesha, 2013</marker>
<rawString>Lushan Han, Tim Finin, Paul McNamee, Anupam Joshi, and Yelena Yesha. 2013a. Improving Word Similarity by Augmenting PMI with Estimates of Word Polysemy. IEEE Trans. on Knowledge and Data Engineering, 25(6):1307–1322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay L Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Johnathan Weese</author>
</authors>
<title>UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems.</title>
<date>2013</date>
<booktitle>In 2nd Joint Conf. on Lexical and Computational Semantics. ACL,</booktitle>
<contexts>
<context position="1324" citStr="Han et al., 2013" startWordPosition="191" endWordPosition="194">entence-Phrase and Word-Sense subtask. The system ranked first for the PhraseWord subtask but was not included in the official results due to a late submission. 1 Introduction We describe the semantic text similarity systems we developed for two of the SemEval tasks for the 2014 International Workshop on Semantic Evaluation. We developed systems for task 3, CrossLevel Semantic Similarity (Jurgens et al., 2014), and task 10, Multilingual Semantic Textual Similarity (Agirre et al., 2014). A key component in all the systems was an enhanced version of the word similarity system used in our entry (Han et al., 2013b) in the 2013 SemEval Semantic Textual Similarity task. Our best system in the Multilingual Semantic Textual Similarity task used an unsupervised approach and ranked second in both the English and Spanish subtasks. In the Cross-Level Semantic Similarity task we developed a number of new algorithms and used new linguistic data resources. In this task, our best systems ranked second in the Paragraph-Sentence task, first in the SentencePhrase task and first in the Word-Sense task. The This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedin</context>
<context position="6912" citStr="Han et al., 2013" startWordPosition="1137" endWordPosition="1140">ound to be effective in improving word similarity measures (Landauer and Dumais, 1997). SVD is typically applied to a word by document matrix, yielding the familiar LSA technique. In our case, we apply it to our word by word matrix (Burgess et al., 1998). Before performing SVD, we transform the raw word co-occurrence count fij to its log frequency log(fij +1). We select the 300 largest singular values and reduce the 29K word vectors to 300 dimensions. The LSA similarity between two words is defined as the cosine similarity of their corresponding word vectors after the SVD transformation. See (Han et al., 2013b; Lushan Han, 2014) for examples and more information on the LSA model. Statistical word similarity measures have limitations. Related words can have similarity scores as high as what similar words get, e.g., “doctor” and “hospital”. Word similarity is typically low for synonyms that have many word senses since information about different senses are mashed together (Han et al., 2013a). To address these issues, we augment the similarity between two words using knowledge from WordNet, for example, increasing the score if they are in the same WordNet synset or if one is a direct or two link hype</context>
<context position="10632" citStr="Han et al., 2013" startWordPosition="1762" endWordPosition="1765">p measure. 3 Multilingual Semantic Text Similarity 3.1 English STS For the 2014 STS-English subtask we submitted three runs. They all used a simple term alignment strategy to compute sentence similarities. The first run was an unsupervised approach that used the basic word-similarity model for term alignment. The next two used a supervised approach to combine the scores from the first run with alignment scores using the enhanced word-similarity wrapper. The two runs differed in their training. Align and Penalize Approach. The pairingWord run was produced by the same Align-and-Penalize system (Han et al., 2013b) that we used in the 2013 STS task with only minor changes. The biggest change is that we included a small list of disjoint concepts (Han et al., 2013b) that are used in the penalization phase, such as {piano, violin} and {dog, cat}. The disjoint concepts were manually collected from the MSRvid dataset provided by the 2012 STS task because we still lack a reliable general method to automatically produce them. The list only contains 23 pairs, which can be downloaded at (dis, 2014). We also slightly adjusted our stopword list. We removed a few words that appear in the trial datasets of 2013 ST</context>
<context position="18339" citStr="Han et al., 2013" startWordPosition="3055" endWordPosition="3058">through experimentation on every sentence in the trial data set. After this filtering, our input for the “news” and “wikipedia” tests went from 480 and 324 pairs of sentences to 5756 and 1776 pairs, respectively. Given a pair of Spanish sentences, I1 and I2, and the set of possible translations generated by our system for each sentence, TI1 = {T11, T12, T13, ... , T1n} and TI2 = {T21, T22, . . . , T2m}, we compute the similarity between them by using the following formula: SimENG(T1i, T2j) n ∗ m where SimENG(x, y) computes the similarity of two English sentences using our existing STS system (Han et al., 2013b). For the final competition we submitted three runs. The first (Pairing in Table 3) used the pairingWord system with the direct translation of the Spanish sentences to English. The second run (PairingAvg in Table 3) used the formula for SimSPA(x, y) based on SimENG(x, y) with the pairingWord system. Finally, the third one (Hulk in Table 3) used the Hulk system with the direct translation. 4 Cross Level Similarity 4.1 Sentence to Paragraph/Phrase We used the three systems developed for the English sentence similarity subtask and described in Section 3.1 for both the sentence to paragraph and </context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay L. Kashyap, Tim Finin, James Mayfield, and Johnathan Weese. 2013b. UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems. In 2nd Joint Conf. on Lexical and Computational Semantics. ACL, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley,</publisher>
<location>New York, USA.</location>
<contexts>
<context position="3334" citStr="Harris, 1968" startWordPosition="516" endWordPosition="517"> text similarity task. This was in turn derived from a system developed for the Graph of Relations project (UMBC, 2013b). For SemEval, we wanted a measure that considered a word’s semantics but not its lexical category, e.g., the verb “marry” should be semantically similar to the noun “wife”. An online demonstration of a similar model developed for the GOR project is available (UMBC, 2013a), but it lacks some of this version’s features. LSA-based word similarity. LSA Word Similarity relies on the distributional hypothesis that words occurring in the same context tend to have similar meanings (Harris, 1968). LSA relies on the fact that words that are semantically similar (e.g., cat and feline or nurse and doctor) are more likely to occur near one another in text. Thus evidence for word similarity can be computed from a statistical analysis of a large text corpus. We extracted raw word co-occurrence statistics from a portion of the 2007 crawl of the Web corpus from the Stanford WebBase project (Stanford, 2001). We processed the collection to remove some undesirable elements (text duplica416 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 416–423, Dublin,</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Zellig Harris. 1968. Mathematical Structures of Language. Wiley, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Hatcher</author>
<author>Otis Gospodnetic</author>
<author>Michael McCandless</author>
</authors>
<title>Lucene in action.</title>
<date>2004</date>
<publisher>Manning Publications</publisher>
<location>Greenwich, CT.</location>
<contexts>
<context position="20450" citStr="Hatcher et al., 2004" startWordPosition="3405" endWordPosition="3408"> system to measure the similarity between these and the given phrase to extract two features. Web search features. These features were based on the hypothesis that if a word and phrase have similar meanings, then a web search that combines the word and phrase should return similar documents when compared to a web search for each individually. We implemented this idea by comparing results of three search queries: the word alone, the phrase alone, and the word and phrase together. Using the Bing Search API (BIN, 2014), we retrieved the top five results for each search, indexed them with Lucene (Hatcher et al., 2004), and extracted term frequency vectors for each of the three search result document sets. For the phrase ’spill the beans’ and word ’confess’, for example, we built a Lucene index for the set of documents retrieved by a Bing search for ’spill the beans’, ’confess’, and ’spill the beans confess’. We calculated the similarity of pairs of search result sets using the cosine similarity (1) of their term frequency vectors. We calculated the mean and minimum similarity of pairs of results for the phrase and phrase+word searches. These features were extracted from the provided training set and used i</context>
</contexts>
<marker>Hatcher, Gospodnetic, McCandless, 2004</marker>
<rawString>Erik Hatcher, Otis Gospodnetic, and Michael McCandless. 2004. Lucene in action. Manning Publications Greenwich, CT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval-2014 Task 3: Cross-Level Semantic Similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1121" citStr="Jurgens et al., 2014" startWordPosition="155" endWordPosition="158">ission in the Multilingual task ranked second in both English and Spanish subtasks using an unsupervised approach. Our best systems for Cross-Level task ranked second in Paragraph-Sentence and first in both Sentence-Phrase and Word-Sense subtask. The system ranked first for the PhraseWord subtask but was not included in the official results due to a late submission. 1 Introduction We describe the semantic text similarity systems we developed for two of the SemEval tasks for the 2014 International Workshop on Semantic Evaluation. We developed systems for task 3, CrossLevel Semantic Similarity (Jurgens et al., 2014), and task 10, Multilingual Semantic Textual Similarity (Agirre et al., 2014). A key component in all the systems was an enhanced version of the word similarity system used in our entry (Han et al., 2013b) in the 2013 SemEval Semantic Textual Similarity task. Our best system in the Multilingual Semantic Textual Similarity task used an unsupervised approach and ranked second in both the English and Spanish subtasks. In the Cross-Level Semantic Similarity task we developed a number of new algorithms and used new linguistic data resources. In this task, our best systems ranked second in the Parag</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. SemEval-2014 Task 3: Cross-Level Semantic Similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="6382" citStr="Landauer and Dumais, 1997" startWordPosition="1044" endWordPosition="1047">etecting them. Our word co-occurrence models were based on a predefined vocabulary of more than 22,000 common English words and noun phrases. We also added to it more than 2,000 verb phrases extracted from WordNet. The final dimensions of our word co-occurrence matrices are 29,000 x 29,000 when words are POS tagged. Our vocabulary includes only open-class words, i.e., nouns, verbs, adjectives and adverbs. There are no proper nouns in the vocabulary with the only exception of country names. Singular Value Decomposition (SVD) has been found to be effective in improving word similarity measures (Landauer and Dumais, 1997). SVD is typically applied to a word by document matrix, yielding the familiar LSA technique. In our case, we apply it to our word by word matrix (Burgess et al., 1998). Before performing SVD, we transform the raw word co-occurrence count fij to its log frequency log(fij +1). We select the 300 largest singular values and reduce the 29K word vectors to 300 dimensions. The LSA similarity between two words is defined as the cosine similarity of their corresponding word vectors after the SVD transformation. See (Han et al., 2013b; Lushan Han, 2014) for examples and more information on the LSA mode</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. 17th Int. Conf. on Computational Linguistics,</booktitle>
<pages>768--774</pages>
<location>Montreal, CN.</location>
<contexts>
<context position="4978" citStr="Lin, 1998" startWordPosition="806" endWordPosition="807">larity model. tions, truncated text, non-English text and strange characters) and produced a three billion word corpus of high quality English, which is available online (Han and Finin, 2013). We performed POS tagging and lemmatization on the corpus using the Stanford POS tagger (Toutanova et al., 2000). Word/term cooccurrences were counted in a moving window of a fixed size that scans the entire corpus. We generated two co-occurrence models using window sizes f1 and f4 because we observed different natures of the models. f1 window produces a context similar to the dependency context used in (Lin, 1998). It provides a more precise context but is only good for comparing words within the same POS. This is because words of different POS are typically surrounded by words in different syntactic forms. In contrast, a context window of f4 words allows us to compute semantic similarity between words with different POS. Examples from our LSA similarity model are given in Table 1. Pairs 1 to 6 illustrate that the measure has a good property of differentiating similar words from non-similar words. Examples 7 and 8 show that the f4 model can detect semantically similar words even with different POS whil</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proc. 17th Int. Conf. on Computational Linguistics, pages 768–774, Montreal, CN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
</authors>
<title>Schema Free Querying of Semantic Data.</title>
<date>2014</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland, Baltimore County.</institution>
<contexts>
<context position="6932" citStr="Han, 2014" startWordPosition="1142" endWordPosition="1143">proving word similarity measures (Landauer and Dumais, 1997). SVD is typically applied to a word by document matrix, yielding the familiar LSA technique. In our case, we apply it to our word by word matrix (Burgess et al., 1998). Before performing SVD, we transform the raw word co-occurrence count fij to its log frequency log(fij +1). We select the 300 largest singular values and reduce the 29K word vectors to 300 dimensions. The LSA similarity between two words is defined as the cosine similarity of their corresponding word vectors after the SVD transformation. See (Han et al., 2013b; Lushan Han, 2014) for examples and more information on the LSA model. Statistical word similarity measures have limitations. Related words can have similarity scores as high as what similar words get, e.g., “doctor” and “hospital”. Word similarity is typically low for synonyms that have many word senses since information about different senses are mashed together (Han et al., 2013a). To address these issues, we augment the similarity between two words using knowledge from WordNet, for example, increasing the score if they are in the same WordNet synset or if one is a direct or two link hypernym of the other. S</context>
</contexts>
<marker>Han, 2014</marker>
<rawString>Lushan Han. 2014. Schema Free Querying of Semantic Data. Ph.D. thesis, University of Maryland, Baltimore County.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo N Mendes</author>
<author>Max Jakob</author>
<author>Andr´es Garc´ıa-Silva</author>
<author>Christian Bizer</author>
</authors>
<title>Dbpedia spotlight: shedding light on the web of documents.</title>
<date>2011</date>
<booktitle>In 7th Int. Conf. on Semantic Systems,</booktitle>
<pages>1--8</pages>
<publisher>ACM.</publisher>
<marker>Mendes, Jakob, Garc´ıa-Silva, Bizer, 2011</marker>
<rawString>Pablo N Mendes, Max Jakob, Andr´es Garc´ıa-Silva, and Christian Bizer. 2011. Dbpedia spotlight: shedding light on the web of documents. In 7th Int. Conf. on Semantic Systems, pages 1–8. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanford</author>
</authors>
<date>2001</date>
<note>Stanford WebBase project. http://bit.ly/WebBase.</note>
<contexts>
<context position="3744" citStr="Stanford, 2001" startWordPosition="588" endWordPosition="590">some of this version’s features. LSA-based word similarity. LSA Word Similarity relies on the distributional hypothesis that words occurring in the same context tend to have similar meanings (Harris, 1968). LSA relies on the fact that words that are semantically similar (e.g., cat and feline or nurse and doctor) are more likely to occur near one another in text. Thus evidence for word similarity can be computed from a statistical analysis of a large text corpus. We extracted raw word co-occurrence statistics from a portion of the 2007 crawl of the Web corpus from the Stanford WebBase project (Stanford, 2001). We processed the collection to remove some undesirable elements (text duplica416 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 416–423, Dublin, Ireland, August 23-24, 2014. Word pair ±4 model ±1 model 1 doctor NN, physician NN 0.775 0.726 2 car NN, vehicle NN 0.748 0.802 3 person NN, car NN 0.038 0.024 4 car NN, country NN 0.000 0.016 5 person NN, country NN 0.031 0.069 6 child NN, marry VB 0.098 0.000 7 wife NN, marry VB 0.548 0.274 8 author NN, write VB 0.364 0.128 9 doctor NN, hospital NN 0.473 0.347 10 car NN, driver NN 0.497 0.281 Table 1: Ex</context>
</contexts>
<marker>Stanford, 2001</marker>
<rawString>Stanford. 2001. Stanford WebBase project. http://bit.ly/WebBase.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zareen Syed</author>
<author>Tim Finin</author>
<author>Varish Mulwad</author>
<author>Anupam Joshi</author>
</authors>
<title>Exploiting a Web of Semantic Data for Interpreting Tables.</title>
<date>2010</date>
<booktitle>In Proceedings of the Second Web Science Conference,</booktitle>
<contexts>
<context position="9065" citStr="Syed et al., 2010" startWordPosition="1505" endWordPosition="1508">d entities, it is not the best approach for other classes. Named Entities. The wrapper is extended to handle all classes of named entities that are included in Stanford CoreNLP (Finkel et al., 2005). We use heuristic rules to compute the similarity between two numbers or two dates. To handle named entity mentions of people, locations and organizations, we supplement our character bigram overlap method with the DBpedia Lookup service (Mendes et al., 2011). For each entity mention, we select the DBpedia entity with the most inlinks, which serves as a good estimate of popularity or significance (Syed et al., 2010). If the two named entity mentions map to identical DBpedia entities, we lower our character bigram overlap threshold to 0.6. OOV words. As mentioned earlier, when dealing with out-of-vocabulary words, we only have its lexical features. A straightforward approach is to simply get more context for the word. Since our vocabulary is limited, we need to use external dictionaries to find the word. For our system, we use Wordnik (Davidson, 2013), which is a compilation of several dictionaries including The American Heritage Dictionary, Wikitionary and WordNet. Wordnik provides a REST API to access s</context>
</contexts>
<marker>Syed, Finin, Mulwad, Joshi, 2010</marker>
<rawString>Zareen Syed, Tim Finin, Varish Mulwad, and Anupam Joshi. 2010. Exploiting a Web of Semantic Data for Interpreting Tables. In Proceedings of the Second Web Science Conference, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>William Morgan</author>
<author>Anna Rafferty</author>
<author>Michel Galley</author>
</authors>
<date>2000</date>
<note>Stanford log-linearpart-of-speech tagger. http://nlp.stanford.edu/software/tagger.shtml.</note>
<contexts>
<context position="4672" citStr="Toutanova et al., 2000" startWordPosition="751" endWordPosition="754">N 0.748 0.802 3 person NN, car NN 0.038 0.024 4 car NN, country NN 0.000 0.016 5 person NN, country NN 0.031 0.069 6 child NN, marry VB 0.098 0.000 7 wife NN, marry VB 0.548 0.274 8 author NN, write VB 0.364 0.128 9 doctor NN, hospital NN 0.473 0.347 10 car NN, driver NN 0.497 0.281 Table 1: Examples from the LSA similarity model. tions, truncated text, non-English text and strange characters) and produced a three billion word corpus of high quality English, which is available online (Han and Finin, 2013). We performed POS tagging and lemmatization on the corpus using the Stanford POS tagger (Toutanova et al., 2000). Word/term cooccurrences were counted in a moving window of a fixed size that scans the entire corpus. We generated two co-occurrence models using window sizes f1 and f4 because we observed different natures of the models. f1 window produces a context similar to the dependency context used in (Lin, 1998). It provides a more precise context but is only good for comparing words within the same POS. This is because words of different POS are typically surrounded by words in different syntactic forms. In contrast, a context window of f4 words allows us to compute semantic similarity between words</context>
</contexts>
<marker>Toutanova, Klein, Manning, Morgan, Rafferty, Galley, 2000</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, William Morgan, Anna Rafferty, and Michel Galley. 2000. Stanford log-linearpart-of-speech tagger. http://nlp.stanford.edu/software/tagger.shtml.</rawString>
</citation>
<citation valid="false">
<authors>
<author>2013a</author>
</authors>
<title>Semantic similarity demonstration.</title>
<note>http://swoogle.umbc.edu/SimService/.</note>
<marker>2013a, </marker>
<rawString>UMBC. 2013a. Semantic similarity demonstration. http://swoogle.umbc.edu/SimService/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>2013b</author>
</authors>
<title>Umbc graph of relations project.</title>
<note>http://ebiq.org/j/95.</note>
<marker>2013b, </marker>
<rawString>UMBC. 2013b. Umbc graph of relations project. http://ebiq.org/j/95.</rawString>
</citation>
<citation valid="false">
<date>2014</date>
<note>Urban dictionary. http://urbandictionary.com/.</note>
<marker>2014</marker>
<rawString>2014. Urban dictionary. http://urbandictionary.com/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>