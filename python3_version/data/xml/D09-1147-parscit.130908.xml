<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.958822">
Consensus Training for Consensus Decoding in Machine Translation
</title>
<author confidence="0.99225">
Adam Pauls, John DeNero and Dan Klein
</author>
<affiliation confidence="0.9956095">
Computer Science Division
University of California at Berkeley
</affiliation>
<email confidence="0.995968">
{adpauls,denero,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.994737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999843368421053">
We propose a novel objective function for dis-
criminatively tuning log-linear machine trans-
lation models. Our objective explicitly op-
timizes the BLEU score of expected n-gram
counts, the same quantities that arise in forest-
based consensus and minimum Bayes risk de-
coding methods. Our continuous objective
can be optimized using simple gradient as-
cent. However, computing critical quantities
in the gradient necessitates a novel dynamic
program, which we also present here. As-
suming BLEU as an evaluation measure, our
objective function has two principle advan-
tages over standard max BLEU tuning. First,
it specifically optimizes model weights for
downstream consensus decoding procedures.
An unexpected second benefit is that it reduces
overfitting, which can improve test set BLEU
scores when using standard Viterbi decoding.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998463474576271">
Increasing evidence suggests that machine trans-
lation decoders should not search for a single
top scoring Viterbi derivation, but should instead
choose a translation that is sensitive to the model’s
entire predictive distribution. Several recent con-
sensus decoding methods leverage compact repre-
sentations of this distribution by choosing transla-
tions according to n-gram posteriors and expected
counts (Tromble et al., 2008; DeNero et al., 2009;
Li et al., 2009; Kumar et al., 2009). This change
in decoding objective suggests a complementary
change in tuning objective, to one that optimizes
expected n-gram counts directly. The ubiquitous
minimum error rate training (MERT) approach op-
timizes Viterbi predictions, but does not explicitly
boost the aggregated posterior probability of de-
sirable n-grams (Och, 2003).
We therefore propose an alternative objective
function for parameter tuning, which we call con-
sensus BLEU or CoBLEU, that is designed to
maximize the expected counts of the n-grams that
appear in reference translations. To maintain con-
sistency across the translation pipeline, we for-
mulate CoBLEU to share the functional form of
BLEU used for evaluation. As a result, CoBLEU
optimizes exactly the quantities that drive efficient
consensus decoding techniques and precisely mir-
rors the objective used for fast consensus decoding
in DeNero et al. (2009).
CoBLEU is a continuous and (mostly) differ-
entiable function that we optimize using gradient
ascent. We show that this function and its gradient
are efficiently computable over packed forests of
translations generated by machine translation sys-
tems. The gradient includes expectations of prod-
ucts of features and n-gram counts, a quantity that
has not appeared in previous work. We present a
new dynamic program which allows the efficient
computation of these quantities over translation
forests. The resulting gradient ascent procedure
does not require any k-best approximations. Op-
timizing over translation forests gives similar sta-
bility benefits to recent work on lattice-based min-
imum error rate training (Macherey et al., 2008)
and large-margin training (Chiang et al., 2008).
We developed CoBLEU primarily to comple-
ment consensus decoding, which it does; it pro-
duces higher BLEU scores than coupling MERT
with consensus decoding. However, we found
an additional empirical benefit: CoBLEU is less
prone to overfitting than MERT, even when using
Viterbi decoding. In experiments, models trained
to maximize tuning set BLEU using MERT con-
sistently degraded in performance from tuning to
test set, while CoBLEU-trained models general-
ized more robustly. As a result, we found that op-
timizing CoBLEU improved test set performance
reliably using consensus decoding and occasion-
ally using Viterbi decoding.
</bodyText>
<page confidence="0.941341">
1418
</page>
<note confidence="0.997108">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1418–1427,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.905347875">
(a) Hypotheses ranked by OTM = OLM = 1
(b) Computing Consensus Bigram Precision
Eθ[c( n e p n ,d)|f] = 0. 4 + 0.0 = 0.
Eθ[c( p n ,d)|f] = 0. 4 + 0.0 = 0.
Eθ[c( me , d)|f] = 0. + 0. 4 = 0. 1
Eθ[c(g, d)|f] = [0. + 0. 4 + 0.0 ]
E9 min{Eθ[c(g, d)|f], c(g, r)}
E9 Eθ[c(g, d)|f]
</figure>
<figureCaption confidence="0.9951905">
Figure 1: (a) A simple hypothesis space of translations
for a single sentence containing three alternatives, each
with two features. The hypotheses are scored under a
log-linear model with parameters θ equal to the identity
vector. (b) The expected counts of all bigrams that ap-
pear in the computation of consensus bigram precision.
</figureCaption>
<sectionHeader confidence="0.923024" genericHeader="method">
2 Consensus Objective Functions
</sectionHeader>
<bodyText confidence="0.9997824">
Our proposed objective function maximizes n-
gram precision by adapting the BLEU evaluation
metric as a tuning objective (Papineni et al., 2002).
To simplify exposition, we begin by adapting a
simpler metric: bigram precision.
</bodyText>
<subsectionHeader confidence="0.899174">
2.1 Bigram Precision Tuning
</subsectionHeader>
<bodyText confidence="0.936808">
Let the tuning corpus consist of source sentences
</bodyText>
<equation confidence="0.9690595">
F = f1 ... fm and human-generated references
R = r1 ... rm, one reference for each source
sentence. Let ei be a translation of fi, and let
E = e1 ... em be a corpus of translations, one for
each source sentence. A simple evaluation score
for E is its bigram precision BP(R, E):
BP(R, E) = Pm Pg2 c(g2, ei)
i=1
</equation>
<bodyText confidence="0.998989153846154">
where g2 iterates over the set of bigrams in the tar-
get language, and c(g2, e) is the count of bigram
g2 in translation e. As in BLEU, we “clip” the bi-
gram counts of e in the numerator using counts of
bigrams in the reference sentence.
Modern machine translation systems are typi-
cally tuned to maximize the evaluation score of
Viterbi derivations1 under a log-linear model with
parameters θ. Let d∗θ(fi) = arg maxdPθ(d|fi) be
the highest scoring derivation d of fi. For a system
employing Viterbi decoding and evaluated by bi-
gram precision, we would want to select θ to max-
imize MaxBP(R, F, θ):
</bodyText>
<equation confidence="0.995262">
Pm Pgg min{c(g2, d∗ θ(fi)), c(g2, ri)}
i=1
P&apos;1Pgg c(g2, d∗θ(fi))
</equation>
<bodyText confidence="0.99986">
On the other hand, for a system that uses ex-
pected bigram counts for decoding, we would pre-
fer to choose θ such that expected bigram counts
match bigrams in the reference sentence. To this
end, we can evaluate an entire posterior distri-
bution over derivations by computing the same
clipped precision for expected bigram counts us-
</bodyText>
<equation confidence="0.757122">
H
</equation>
<bodyText confidence="0.658826">
ing CoBP(R, F, θ):
where
</bodyText>
<equation confidence="0.632903666666667">
O IF-θ [c(g2, d)|fi] =
d
e
</equation>
<bodyText confidence="0.998631">
is the expected count of bigram g2 in all deriva-
tions d of fi. We define the precise parametric
form of PθV(d |fi) in Section 3. Figure 1 shows pro-
posed translations for a single sentence along with
the bigram expectations needed to compute CoBP.
Equation 1 constitutes an objective function for
tuning the parameters of a machine translation
model. Figure 2 contrasts the properties of CoBP
and MaxBP as tuning objectives, using the simple
example from Figure 1.
Consensus bigram precision is an instance of a
general recipe for converting n-gram based eval-
uation metrics into consensus objective functions
for model tuning. For the remainder of this pa-
per, we focus on consensus BLEU. However, the
techniques herein, including the optimization ap-
proach of Section 3, are applicable to many differ-
entiable functions of expected n-gram counts.
</bodyText>
<footnote confidence="0.95184475">
1By derivation, we mean a translation of a foreign sen-
tence along with any latent structure assumed by the model.
Each derivation corresponds to a particular English transla-
tion, but many derivations may yield the same translation.
</footnote>
<figure confidence="0.9270752">
Sentence f: Il était une rime
Reference r: Once upon a rhyme
TM LM Pr
H1) Once on a rhyme
H2) Once upon a rhyme
H3) Once upon a time
-3
-7
0.67
-5
-6
0.24
-9
-3
0.09
</figure>
<equation confidence="0.94906">
Pm Pgg min{IF-θ[c(g2, d)|fi], c(g2, ri)}
i=1 0 Parameter �M 2
Pim-1 Egg IF-θ [c(g2, d)|ffi] (1)
0. + 0. + 0. 1
=
Pi=1 Pg2 min{c(g2, ei), c(g2, ri
)}
Pθ(d|fi)c(g2,d)
&amp;
</equation>
<page confidence="0.979798">
1419
</page>
<figure confidence="0.999306733333333">
0 2 4 6 8 10
0.0 0.2 0.4 0.6 0.8 1.0
Value of Objective
H,
H2
H3
Log Model Score
-16 -14 -12 -10
1.0 1.5 2.0 2.5 3.0
H, H2 H3
CoBP
MaxBP
H, H3
θLM θLM
(a) (b)
</figure>
<figureCaption confidence="0.905307">
Figure 2: These plots illustrate two properties of the objectives max bigram precision (MaxBP) and consensus
bigram precision (CoBP) on the simple example from Figure 1. (a) MaxBP is only sensitive to the convex hull (the
</figureCaption>
<bodyText confidence="0.630484428571429">
solid line) of model scores. When varying the single parameter θLM, it entirely disregards the correct translation
H2 because H2 never attains a maximal model score. (b) A plot of both objectives shows their differing characteris-
tics. The horizontal segmented line at the top of the plot indicates the range over which consensus decoding would
select each hypothesis, while the segmented line at the bottom indicates the same for Viterbi decoding. MaxBP
is only sensitive to the single point of discontinuity between H1 and H3, and disregards H2 entirely. CoBP peaks
when the distribution most heavily favors H2 while suppressing H1. Though H2 never has a maximal model score,
if θLM is in the indicated range, consensus decoding would select H2, the desired translation.
</bodyText>
<subsectionHeader confidence="0.995944">
2.2 CoBLEU
</subsectionHeader>
<bodyText confidence="0.991244">
The logarithm of the single-reference2 BLEU met-
ric (Papineni et al., 2002) has the following form:
</bodyText>
<equation confidence="0.975824714285714">
!lnBLEU(R,E) = 1 − m |R |i= −
PPm Pgn min{c(gn, ei), c(gn, ri)}
i=1
ln
m Pgn c(gn, ei)
i=1
Above, |R |denotes the number of words in the
</equation>
<bodyText confidence="0.999776571428571">
reference corpus. The notation (·)− is shorthand
for min(·, 0). In the inner sums, gn iterates over
all n-grams of order n. In order to adapt BLEU
to be a consensus tuning objective, we follow the
recipe of Section 2.1: we replace n-gram counts
from a candidate translation with expected n-gram
counts under the model.
</bodyText>
<equation confidence="0.999319333333333">
CoBLEU(R, F, θ) = 1− |R|
Pm P g1Eθ[c(g1, d)|fi]
i=1
</equation>
<bodyText confidence="0.8888455">
The brevity penalty term in BLEU is calculated
using the expected length of the corpus, which
</bodyText>
<footnote confidence="0.925386">
2Throughout this paper, we use only a single reference,
but our objective readily extends to multiple references.
</footnote>
<bodyText confidence="0.987727333333333">
equals the sum of all expected unigram counts.
We call this objective function consensus BLEU,
or CoBLEU for short.
</bodyText>
<sectionHeader confidence="0.982414" genericHeader="method">
3 Optimizing CoBLEU
</sectionHeader>
<bodyText confidence="0.999986916666667">
Unlike the more common MaxBLEU tuning ob-
jective optimized by MERT, CoBLEU is con-
tinuous. For distributions Pθ(d|fi) that factor
over synchronous grammar rules and n-grams, we
show below that it is also analytically differen-
tiable, permitting a straightforward gradient ascent
optimization procedure.3 In order to perform gra-
dient ascent, we require methods for efficiently
computing the gradient of the objective function
for a given parameter setting θ. Once we have the
gradient, we can perform an update at iteration t
of the form
</bodyText>
<equation confidence="0.849026">
θ(t+1) , θ(t) + ηtVθCoBLEU(R, F, θ(t))
</equation>
<bodyText confidence="0.881685888888889">
where ηt is an adaptive step size.4
3Technically, CoBLEU is non-differentiable at some
points because of clipping. At these points, we must com-
pute a sub-gradient, and so our optimization is formally sub-
gradient ascent. See the Appendix for details.
4After each successful step, we grow the step size by a
constant factor. Whenever the objective does not decrease
after a step, we shrink the step size by a constant factor and
try again until a decrease is attained.
</bodyText>
<figure confidence="0.9919245">
1
+4
X
n=1
4
ln Pm Pgn Eθ[c(gn, d)|fi]
i=1
Pm Pgn min{Eθ[c(gn,d)|fi],c(gn, ri)}
i=1
4
1
+4
X
n=1
1420
tail(h)
</figure>
<figureCaption confidence="0.643932">
Figure 3: A hyperedge h represents a “rule” used in
syntactic machine translation. tail(h) refers to the “chil-
dren” of the rule, while head(h) refers to the “head” or
“parent”. A forest of translations is built by combining
</figureCaption>
<bodyText confidence="0.955601944444445">
the nodes vi using h to form a new node u = head(h).
Each forest node consists of a grammar symbol and tar-
get language boundary words used to track n-grams. In
the above, we keep one boundary word for each node,
which allows us to track bigrams.
lowing expectations:
Eo [c(Ok,d)|fi]
Eo [En(d)|fi]
Eo [c(Ok, d) · En(d)|fi]
where En(d) = Es, c(gn, d) is the sum of all n-
grams on derivation d (its “length”). The first ex-
pectation is an expected count of the kth feature
Ok over all derivations of fi. The second is an ex-
pected length, the total expected count of all n-
grams in derivations of fi. We call the final ex-
pectation an expected product of counts. We now
present the computation of each of these expecta-
tions in turn.
</bodyText>
<subsectionHeader confidence="0.999764">
3.2 Computing Feature Expectations
</subsectionHeader>
<bodyText confidence="0.998763">
The expected feature counts Eo[c(Ok, d)|fi] can be
written as
</bodyText>
<equation confidence="0.6997475">
head(h)
u=OnceSrhyme
c(“Once upon”, h)
c(“upon a”, h)
12(h) =
v1=OnceRBOnce
v2=uponINupon
v3=aNPrhyme
</equation>
<bodyText confidence="0.918023428571429">
= 1
= 1
In this section, we develop an analytical expres- � Po(d|fi)c(Ok, d)
sion for the gradient of CoBLEU, then discuss Eo[c(Ok, d)|fi] = Po(h|fi)c(Ok, h)
how to efficiently compute the value of the objec- d
tive function and gradient. �=
h
</bodyText>
<subsectionHeader confidence="0.984008">
3.1 Translation Model Form
</subsectionHeader>
<bodyText confidence="0.99998175">
We first assume the general hypergraph setting of
Huang and Chiang (2007), namely, that deriva-
tions under our translation model form a hyper-
graph. This framework allows us to speak about
both phrase-based and syntax-based translation in
a unified framework.
We define a probability distribution over deriva-
tions d via B as:
</bodyText>
<equation confidence="0.974378166666667">
w(d)
Po(d|fi) =
Z(fi)
with
Z(fi) = � w(d&apos;)
d&apos;
</equation>
<bodyText confidence="0.999164117647059">
where w(d) = exp(BTb(d, fi)) is the weight of a
derivation and b(d, fi) is a featurized representa-
tion of the derivation d of fi. We further assume
that these features decompose over hyperedges in
the hypergraph, like the one in Figure 3. That is,
b(d, fi) = EhEd b(h, fi).
In this setting, we can analytically compute the
gradient of CoBLEU. We provide a sketch of the
derivation of this gradient in the Appendix. In
computing this gradient, we must calculate the fol-
We can justify the second step since fea-
ture counts are local to hyperedges, i.e.
c(Ok, d) = EhEd c(Ok, h). The posterior
probability Po(h|fi) can be efficiently computed
with inside-outside scores. Let I(u) and O(u) be
the standard inside and outside scores for a node
u in the forest.5
</bodyText>
<equation confidence="0.994084">
1 � I(v)
Po(h|fi) = Z(f) w(h) O(head(h))
vEtail(h)
</equation>
<bodyText confidence="0.9997788">
where w(h) is the weight of hyperedge h, given
by exp(BTb(h)), and Z(f) = I(root) is the in-
side score of the root of the forest. Computing
these inside-outside quantities takes time linear in
the number of hyperedges in the forest.
</bodyText>
<subsectionHeader confidence="0.999781">
3.3 Computing n-gram Expectations
</subsectionHeader>
<bodyText confidence="0.999649857142857">
We can compute the expectations of any specific
n-grams, or of total n-gram counts E, in the same
way as feature expectations, provided that target-
side n-grams are also localized to hyperedges (e.g.
consider E to be a feature of a hyperedge whose
value is the number of n-grams on h). If the
nodes in our forests are annotated with target-side
</bodyText>
<footnote confidence="0.963314">
5Appendix Figure 7 gives recursions for I(u) and O(u).
</footnote>
<page confidence="0.988049">
1421
</page>
<bodyText confidence="0.9979505">
boundary words as in Figure 3, then this will be the
case. Note that this is the same approach used by
decoders which integrate a target language model
(e.g. Chiang (2007)). Other work has computed
n-gram expectations in the same way (DeNero et
al., 2009; Li et al., 2009).
</bodyText>
<subsectionHeader confidence="0.879731">
3.4 Computing Expectations of Products of
Counts
</subsectionHeader>
<bodyText confidence="0.999935307692308">
While the previous two expectations can be com-
puted using techniques known in the literature, the
expected product of counts EB[c(φk, d) · `n(d)|fi]
is a novel quantity. Fortunately, an efficient dy-
namic program exists for computing this expec-
tation as well. We present this dynamic program
here as one of the contributions of this paper,
though we omit a full derivation due to space re-
strictions.
To see why this expectation cannot be computed
in the same way as the expected feature or n-gram
counts, we expand the definition of the expectation
above to get
</bodyText>
<equation confidence="0.8204085">
X PB(d|fi) [c(φk, d)`n(d)]
d
</equation>
<bodyText confidence="0.998389833333333">
Unlike feature and n-gram counts, the product of
counts in brackets above does not decompose over
hyperedges, at least not in an obvious way. We
can, however, still decompose the feature counts
c(φk, d) over hyperedges. After this decomposi-
tion and a little re-arranging, we get
</bodyText>
<equation confidence="0.997281545454545">
X= c(φk, h) X Pθ(d|fi)`n(d)
h d:h∈d
&amp;quot; X
1 X
= c(φk,h) w(d)`n(d)
Z(fi)
h d:h∈d
1 X
= c(φk, h)ˆDn θ (h|fi)
Z(fi)
h
</equation>
<bodyText confidence="0.9906724">
The quantity ˆDnB (h|fi) = Pd:h∈d w(d)`n(d) is the
sum of the weight-length products of all deriva-
tions d containing hyperedge h. In the same
way that PB(h|fi) can be efficiently computed
from inside and outside probabilities, this quan-
tity ˆDnB (h|fi) can be efficiently computed with two
new inside and outside quantities, which we call
ˆIn(u) and ˆOn(u). We provide recursions for these
quantities in Figure 4. Like the standard inside and
outside computations, these recursions run in time
linear in the number of hyperedges in the forest.
While a full exposition of the algorithm is not
possible in the available space, we give some brief
intuition behind this dynamic program. We first
define ˆIn(u):
</bodyText>
<equation confidence="0.974441">
ˆIn(u) = X w(du)`n(d)
du
</equation>
<bodyText confidence="0.999965666666667">
where du is a derivation rooted at node u. This is
a sum of weight-length products similar to ˆD. To
give a recurrence for ˆI, we rewrite it:
</bodyText>
<equation confidence="0.8955115">
ˆIn(u) = X X [w(du)`n(h)]
du h∈du
</equation>
<bodyText confidence="0.999987551724138">
Here, we have broken up the total value of `n(d)
across hyperedges in d. The bracketed quantity
is a score of a marked derivation pair (d, h) where
the edge h is some specific element of d. The score
of a marked derivation includes the weight of the
derivation and the factor `n(h) for the marked hy-
peredge.
This sum over marked derivations gives the in-
side recurrence in Figure 4 by the following de-
composition. For ˆIn(u) to sum over all marked
derivation pairs rooted at u, we must consider two
cases. First, the marked hyperedge could be at the
root, in which case we must choose child deriva-
tions from regular inside scores and multiply in the
local `n, giving the first summand of ˆIn(u). Alter-
natively, the marked hyperedge is in exactly one
of the children; for each possibility we recursively
choose a marked derivation for one child, while
the other children choose regular derivations. The
second summand of ˆIn(u) compactly expresses
a sum over instances of this case. ˆOn(u) de-
composes similarly: the marked hyperedge could
be local (first summand), under a sibling (second
summand), or higher in the tree (third summand).
Once we have these new inside-outside quanti-
ties, we can compute Dˆ as in Figure 5. This com-
bination states that marked derivations containing
h are either marked at h, below h, or above h.
As a final detail, computing the gradient
</bodyText>
<equation confidence="0.5500755">
VCclip
n (θ) (see the Appendix) involves a clipped
</equation>
<bodyText confidence="0.9999012">
version of the expected product of counts, for
which a clipped Dˆ is required. This quantity can
be computed with the same dynamic program with
a slight modification. In Figure 4, we show the dif-
ference as a choice point when computing `n(h).
</bodyText>
<subsectionHeader confidence="0.897843">
3.5 Implementation Details
</subsectionHeader>
<bodyText confidence="0.999959666666667">
As stated, the runtime of computing the required
expectations for the objective and gradient is lin-
ear in the number of hyperedges in the forest. The
</bodyText>
<page confidence="0.667544">
1422
</page>
<equation confidence="0.70492412">
X ⎡ ⎤
ˆIn(u) = w(h) `n(h) Y I(v) + X ˆIn (v)Y I(w) ⎦
hEIN(u) vEtail(h) vEtail(h) w#v
X
ˆOn(u) =
⎢ Y
w(h) ⎢ ⎢ `n(h) O(head(h))
X
I(v) + O(head(h))
vEtail(h)
v#u
Y
I(w) + ˆOn(head(h))
wEtail(h) wEtail(h)
w#v w#u
w#u
⎤
⎦⎥⎥⎥⎥
hEOUT(u) vEtail(h)
v#u
YˆIn(v)
I(w)
`n (h) = r E9n c(gn, h) computing unclipped counts
Sl P
9n c(gn, h)✶ [Eθ[c(gn, d)] ≤ c(gn, ri)] computing clipped counts
</equation>
<figureCaption confidence="0.999964">
Figure 4: Inside and Outside recursions for ˆIn(u) and ˆOn(u). IN(u) and OUT(u) refer to the incoming and
outgoing hyperedges of u, respectively. I(·) and O(·) refer to standard inside and outside quantities, defined in
Appendix Figure 7. We initialize with ˆIn(u) = 0 for all terminal forest nodes u and ˆOn(root) = 0 for the root
node. `n(h) computes the sum of all n-grams of order n on a hyperedge h.
Figure 5: Calculation of ˆDnθ (h|fi) after ˆIn(u) and ˆOn(u) have been computed.
</figureCaption>
<equation confidence="0.961087571428571">
ˆDnθ (h|fi) =
⎡
w(h)⎢`n(h)O(head(h)) Y I(v) + O(head(h)) X ˆIn(v) Y I(w) + ˆOn(head(h)) Y I(w)
vEtail(h) vEtail(h) vEtail(h) wEtail(h)
w=,4v
⎤
⎦⎥⎥
</equation>
<bodyText confidence="0.999982615384616">
number of hyperedges is very large, however, be-
cause we must track n-gram contexts in the nodes,
just as we would in an integrated language model
decoder. These contexts are required both to cor-
rectly compute the model score of derivations and
to compute clipped n-gram counts. To speed our
computations, we use the cube pruning method of
Huang and Chiang (2007) with a fixed beam size.
For regularization, we added an L2 penalty on
the size of θ to the CoBLEU objective, a simple
addition for gradient ascent. We did not find that
our performance varied very much for moderate
levels of regularization.
</bodyText>
<sectionHeader confidence="0.777108" genericHeader="method">
3.6 Related Work
</sectionHeader>
<bodyText confidence="0.999934416666666">
The calculation of expected counts can be for-
mulated using the expectation semiring frame-
work of Eisner (2002), though that work does
not show how to compute expected products of
counts which are needed for our gradient calcu-
lations. Concurrently with this work, Li and Eis-
ner (2009) have generalized Eisner (2002) to com-
pute expected products of counts on translation
forests. The training algorithm of Kakade et al.
(2002) makes use of a dynamic program similar to
ours, though specialized to the case of sequence
models.
</bodyText>
<sectionHeader confidence="0.986489" genericHeader="method">
4 Consensus Decoding
</sectionHeader>
<bodyText confidence="0.995896333333333">
Once model parameters θ are learned, we must
select an appropriate decoding objective. Sev-
eral new decoding approaches have been proposed
recently that leverage some notion of consensus
over the many weighted derivations in a transla-
tion forest. In this paper, we adopt the fast consen-
sus decoding procedure of DeNero et al. (2009),
which directly complements CoBLEU tuning. For
a source sentence f, we first build a translation
forest, then compute the expected count of each
n-gram in the translation of f under the model.
We extract a k-best list from the forest, then select
the translation that yields the highest BLEU score
relative to the forest’s expected n-gram counts.
Specifically, let BLEU(e; r) compute the simi-
larity of a sentence e to a reference r based on
the n-gram counts of each. When training with
CoBLEU, we replace e with expected counts and
maximize θ. In consensus decoding, we replace r
with expected counts and maximize e.
Several other efficient consensus decoding pro-
</bodyText>
<page confidence="0.93505">
1423
</page>
<bodyText confidence="0.971208769230769">
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of Value at Convergence
cedures would similarly benefit from a tuning pro-
cedure that aggregates over derivations. For in-
stance, Blunsom and Osborne (2008) select the
translation sentence with highest posterior proba-
bility under the model, summing over derivations.
Li et al. (2009) propose a variational approxima-
tion maximizing sentence probability that decom-
poses over n-grams. Tromble et al. (2008) min-
imize risk under a loss function based on the lin-
ear Taylor approximation to BLEU, which decom-
poses over n-gram posterior probabilities.
</bodyText>
<sectionHeader confidence="0.999275" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999856">
We compared CoBLEU training with an imple-
mentation of minimum error rate training on two
language pairs.
</bodyText>
<subsectionHeader confidence="0.9839">
5.1 Model
</subsectionHeader>
<bodyText confidence="0.999922473684211">
Our optimization procedure is in principle
tractable for any syntactic translation system. For
simplicity, we evaluate the objective using an In-
version Transduction Grammar (ITG) (Wu, 1997)
that emits phrases as terminal productions, as in
(Cherry and Lin, 2007). Phrasal ITG models have
been shown to perform comparably to the state-of-
the art phrase-based system Moses (Koehn et al.,
2007) when using the same phrase table (Petrov et
al., 2008).
We extract a phrase table using the Moses
pipeline, based on Model 4 word alignments gen-
erated from GIZA++ (Och and Ney, 2003). Our fi-
nal ITG grammar includes the five standard Moses
features, an n-gram language model, a length fea-
ture that counts the number of target words, a fea-
ture that counts the number of monotonic ITG
rewrites, and a feature that counts the number of
inverted ITG rewrites.
</bodyText>
<subsectionHeader confidence="0.997003">
5.2 Data
</subsectionHeader>
<bodyText confidence="0.999956909090909">
We extracted phrase tables from the Spanish-
English and French-English sections of the Eu-
roparl corpus, which include approximately 8.5
million words of bitext for each of the language
pairs (Koehn, 2002). We used a trigram lan-
guage model trained on the entire corpus of En-
glish parliamentary proceedings provided with the
Europarl distribution and generated according to
the ACL 2008 SMT shared task specifications.6
For tuning, we used all sentences from the 2007
SMT shared task up to length 25 (880 sentences
</bodyText>
<footnote confidence="0.996546">
6See http://www.statmt.org/wmt08 for details.
</footnote>
<figure confidence="0.673459">
2 4 6 8 10
Iterations
</figure>
<figureCaption confidence="0.963422">
Figure 6: Trajectories of MERT and CoBLEU dur-
</figureCaption>
<bodyText confidence="0.793404388888889">
ing optimization show that MERT is initially unstable,
while CoBLEU training follows a smooth path to con-
vergence. Because these two training procedures op-
timize different functions, we have normalized each
trajectory by the final objective value at convergence.
Therefore, the absolute values of this plot do not re-
flect the performance of either objective, but rather
the smoothness with which the final objective is ap-
proached. The rates of convergence shown in this plot
are not directly comparable. Each iteration for MERT
above includes 10 iterations of coordinate ascent, fol-
lowed by a decoding pass through the training set. Each
iteration of CoBLEU training involves only one gradi-
ent step.
for Spanish and 923 for French), and we tested on
the subset of the first 1000 development set sen-
tences which had length at most 25 words (447
sentences for Spanish and 512 for French).
</bodyText>
<subsectionHeader confidence="0.99954">
5.3 Tuning Optimization
</subsectionHeader>
<bodyText confidence="0.99982395">
We compared two techniques for tuning the nine
log-linear model parameters of our ITG grammar.
We maximized CoBLEU using gradient ascent, as
described above. As a baseline, we maximized
BLEU of the Viterbi translation derivations using
minimum error rate training. To improve opti-
mization stability, MERT used a cumulative k-best
list that included all translations generated during
the tuning process.
One of the benefits of CoBLEU training is that
we compute expectations efficiently over an entire
forest of translations. This has substantial stabil-
ity benefits over methods based on k-best lists. In
Figure 6, we show the progress of CoBLEU as
compared to MERT. Both models are initialized
from 0 and use the same features. This plot ex-
hibits a known issue with MERT training: because
new k-best lists are generated at each iteration,
the objective function can change drastically be-
tween iterations. In contrast, CoBLEU converges
</bodyText>
<figure confidence="0.486011">
COBLEU
MERT
</figure>
<page confidence="0.822199">
1424
</page>
<table confidence="0.963300454545454">
Consensus Decoding
Spanish
Tune Test 0 Br.
MERT 32.5 30.2 -2.3 0.992
CoBLEU 31.4 30.4 -1.0 0.992
MERT→CoBLEU 31.7 30.8 -0.9 0.992
French
Tune Test 0 Br.
MERT 32.5 31.1* -1.4 0.972
CoBLEU 31.9 30.9 -1.0 0.954
MERT→CoBLEU 32.4 31.2* -0.8 0.953
</table>
<tableCaption confidence="0.979130125">
Table 1: Performance measured by BLEU using a con-
sensus decoding method over translation forests shows
an improvement over MERT when using CoBLEU
training. The first two conditions were initialized by
0 vectors. The third condition was initialized by the
final parameters of MERT training. Br. indicates the
brevity penalty on the test set. The * indicates differ-
ences which are not statistically significant.
</tableCaption>
<bodyText confidence="0.9993894">
smoothly to its final objective because the forests
do not change substantially between iterations, de-
spite the pruning needed to track n-grams. Similar
stability benefits have been observed for lattice-
based MERT (Macherey et al., 2008).
</bodyText>
<subsectionHeader confidence="0.628036">
5.4 Results
</subsectionHeader>
<bodyText confidence="0.999805384615385">
We performed experiments from both French and
Spanish into English under three conditions. In the
first two, we initialized both MERT and CoBLEU
training uniformly with zero weights and trained
until convergence. In the third condition, we ini-
tialized CoBLEU with the final parameters from
MERT training, denoted MERT→CoBLEU in the
results tables. We evaluated each of these condi-
tions on both the tuning and test sets using the con-
sensus decoding method of DeNero et al. (2009).
The results appear in Table 1.
In Spanish-English, CoBLEU slightly outper-
formed MERT under the same initialization, while
the opposite pattern appears for French-English.
The best test set performance in both language
pairs was the third condition, in which CoBLEU
training was initialized with MERT. This con-
dition also gave the highest CoBLEU objective
value. This pattern indicates that CoBLEU is a
useful objective for translation with consensus de-
coding, but that the gradient ascent optimization is
getting stuck in local maxima during tuning. This
issue can likely be addressed with annealing, as
described in (Smith and Eisner, 2006).
Interestingly, the brevity penatly results in
French indicate that, even though CoBLEU did
</bodyText>
<subsectionHeader confidence="0.746498">
Viterbi Decoding
</subsectionHeader>
<table confidence="0.998752">
Spanish
Tune Test 0
MERT 32.5 30.2 -2.3
MERT→CoBLEU 30.5 30.9 +0.4
French
Tune Test 0
MERT 32.0 31.0 -1.0
MERT→CoBLEU 31.7 30.9 -0.8
</table>
<tableCaption confidence="0.994814666666667">
Table 2: Performance measured by BLEU using Viterbi
decoding indicates that CoBLEU is less prone to over-
fitting than MERT.
</tableCaption>
<bodyText confidence="0.996045230769231">
not outperform MERT in a statistically significant
way, CoBLEU tends to find shorter sentences with
higher n-gram precision than MERT.
Table 1 displays a second benefit of CoBLEU
training: compared to MERT training, CoBLEU
performance degrades less from tuning to test
set. In Spanish, initializing with MERT-trained
weights and then training with CoBLEU actually
decreases BLEU on the tuning set by 0.8 points.
However, this drop in tuning performance comes
with a corresponding increase of 0.6 on the test
set, relative to MERT training. We see the same
pattern in French, albeit to a smaller degree.
While CoBLEU ought to outperform MERT us-
ing consensus decoding, we expected that MERT
would give better performance under Viterbi de-
coding. Surprisingly, we found that CoBLEU
training actually outperformed MERT in Spanish-
English and performed equally well in French-
English. Table 2 shows the results. In these ex-
periments, we again see that CoBLEU overfit the
training set to a lesser degree than MERT, as evi-
denced by a smaller drop in performance from tun-
ing to test set. In fact, test set performance actually
improved for Spanish-English CoBLEU training
while dropping by 2.3 BLEU for MERT.
</bodyText>
<sectionHeader confidence="0.999003" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9997484">
CoBLEU takes a fundamental quantity used in
consensus decoding, expected n-grams, and trains
to optimize a function of those expectations.
While CoBLEU can therefore be expected to in-
crease test set BLEU under consensus decoding, it
is more surprising that it seems to better regularize
learning even for the Viterbi decoding condition.
It is also worth emphasizing that the CoBLEU ap-
proach is applicable to functions of expected n-
gram counts other than BLEU.
</bodyText>
<page confidence="0.978187">
1425
</page>
<bodyText confidence="0.7717285">
Appendix: The Gradient of CoBLEU
We would like to compute the gradient of
</bodyText>
<figure confidence="0.55753175">
|R|
1 − Pm 1 Pg1 Eθ [c(g1, d) |fi]!−
To simplify notation, we introduce the functions
Eθ[c(gn, e)|fi]
</figure>
<figureCaption confidence="0.616587">
Figure 7: Standard Inside-Outside recursions which
compute I(u) and O(u). IN(u) and OUT(u) refer to the
incoming and outgoing hyperedges of u, respectively.
We initialize with I(u) = 1 for all terminal forest nodes
u and O(root) = 1 for the root node. These quantities
are referenced in Figure 4.
</figureCaption>
<figure confidence="0.963267576923077">
⎡ ⎤
X ⎣Y
I(u) = w(h) I(v) ⎦
hEIN(u) vEtail(h)
X ⎡ I(v) ⎤
O(u) = ⎢ Y ⎦⎥⎥
hEOUT(u) w(h) ⎢ ⎣O(head(h))
vEtail(h)
v#u
1
+4
X
n=1
4
ln Pm P gn Eθ[c(gn, d)|fi]
i=1
Pm Pgn min{Eθ[c(gn, d)|fi], c(gn, ri)}
i=1
Xm
i=1
Cn(θ) =
X
gn
Cclip m X min{Eθ[c(gn, d)|fi], c(r, gn)} and the gradient ∇Cclip
n (θ) = X gn n (θ) is given by
i=1
</figure>
<bodyText confidence="0.948019333333333">
Cn(θ) represents the sum of the expected counts
of all n-grams or order n in all translations of
the source corpus F, while Cclip
</bodyText>
<equation confidence="0.753371">
n (θ) represents the
</equation>
<bodyText confidence="0.99903275">
sum of the same expected counts, but clipped with
reference counts c(gn, ri).
With this notation, we can write our objective
function CoBLEU(R, F, θ) in three terms:
</bodyText>
<equation confidence="0.976423666666667">
� �
1 − |R|
C1(θ) −
</equation>
<bodyText confidence="0.923328">
We first state an identity:
</bodyText>
<equation confidence="0.97286925">
∂ Eθ[c(gn, d)|fi] =
∂θk
Eθ [c(φk, d) · `n(d)|fi]
−Eθ [`n(d)|fi] · Eθ[c(φk, d)|fi]
</equation>
<bodyText confidence="0.730193">
which can be derived by expanding the expectation on
the left-hand side
∂ Pθ(d|fi)c(gn,d)
∂θk
Using this identity and some basic calculus, the
gradient ∇Cn(θ) is
</bodyText>
<equation confidence="0.985456166666667">
Xm Eθ [c(φk, d) · `n(d)|fi] − Cn(θ)Eθ[c(φk, d)|fi]
i=1
&amp;quot;Eθ [c(gn, d) · c(φk, d)|fi]
✶ hEθ [c(gn, d)  |fi] ≤ c(gn, ri)i
−Cclip
n (θ)Eθ[c(φk, d) + fi]
</equation>
<bodyText confidence="0.9981632">
where ✶ denotes an indicator function. At the top
level, the gradient of the first term (the brevity
penalty) is
 |R |∇C1(θ) ✶ hC1(θ) ≤ |R|C1(θ)2
The gradient of the second term is
</bodyText>
<equation confidence="0.92397425">
∇Cclip
n (θ)
Cclip
n (θ)
</equation>
<bodyText confidence="0.984032">
and the gradient of the third term is
</bodyText>
<equation confidence="0.7842215">
∇Cn(θ)
Cn(θ)
</equation>
<bodyText confidence="0.999942285714286">
Note that, because of the indicator func-
tions, CoBLEU is non-differentiable when
Eθ[c(gn, d)|fi] = c(gn, ri) or Cn(θ) = |R|.
Formally, we must compute a sub-gradient at
these points. In practice, we can choose between
the gradients calculated assuming the indicator
function is 0 or 1; we always choose the latter.
</bodyText>
<figure confidence="0.996091763157895">
X X4
ln Cclip
n (θ) − 1
4
n=1 n=1
4
ln Cn(θ)
1
4
+
X
gn
X
d
X
gn
and substituting
∂ Pθ(d|fi) =
∂θk
X
Pθ(d|fi)c(φk,d) − Pθ(d|fi)
d&apos;
Pθ(d&apos;|fi)c(φk, d&apos;)
Xm
i=1
X
gn
4
1
4
X
n=1
4
1
−
4
X
n=1
</figure>
<page confidence="0.983651">
1426
</page>
<sectionHeader confidence="0.992115" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999914414893617">
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proceedings
of the Conference on Emprical Methods for Natural
Language Processing.
Colin Cherry and Dekang Lin. 2007. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In The Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics Workshop on Syntax and Structure in
Statistical Translation.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In The Conference on Em-
pirical Methods in Natural Language Processing.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
The Annual Conference of the Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for prob-
abilistic finite-state transducers. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In The Annual Conference of the Association for
Computational Linguistics.
Sham Kakade, Yee Whye Teh, and Sam T. Roweis.
2002. An alternate objective function for markovian
fields. In Proceedings of ICML.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
The Annual Conference of the Association for Com-
putational Linguistics.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In The Annual
Conference of the Association for Computational
Linguistics.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In The Annual Conference of the Association
for Computational Linguistics.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008. Lattice-based minimum error rate training for
statistical machine translation. In In Proceedings of
Empirical Methods in Natural Language Process-
ing.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29:19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics (ACL), pages 160–167, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In The Annual
Conference of the Association for Computational
Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation us-
ing language projections. In Proceedings of the
2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 108–116, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
David Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In In Pro-
ceedings of the Association for Computational Lin-
guistics.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice minimum Bayes-risk
decoding for statistical machine translation. In The
Conference on Empirical Methods in Natural Lan-
guage Processing.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377–404.
</reference>
<page confidence="0.9936">
1427
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.937691">
<title confidence="0.999653">Consensus Training for Consensus Decoding in Machine Translation</title>
<author confidence="0.996343">John DeNero Pauls</author>
<affiliation confidence="0.992136">Computer Science University of California at</affiliation>
<abstract confidence="0.99774405">We propose a novel objective function for discriminatively tuning log-linear machine translation models. Our objective explicitly opthe BLEU score of counts, the same quantities that arise in forestbased consensus and minimum Bayes risk decoding methods. Our continuous objective can be optimized using simple gradient ascent. However, computing critical quantities in the gradient necessitates a novel dynamic program, which we also present here. Assuming BLEU as an evaluation measure, our objective function has two principle advantages over standard max BLEU tuning. First, it specifically optimizes model weights for downstream consensus decoding procedures. An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Miles Osborne</author>
</authors>
<title>Probabilistic inference for machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Emprical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="21652" citStr="Blunsom and Osborne (2008)" startWordPosition="3674" endWordPosition="3677">est, then select the translation that yields the highest BLEU score relative to the forest’s expected n-gram counts. Specifically, let BLEU(e; r) compute the similarity of a sentence e to a reference r based on the n-gram counts of each. When training with CoBLEU, we replace e with expected counts and maximize θ. In consensus decoding, we replace r with expected counts and maximize e. Several other efficient consensus decoding pro1423 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of Value at Convergence cedures would similarly benefit from a tuning procedure that aggregates over derivations. For instance, Blunsom and Osborne (2008) select the translation sentence with highest posterior probability under the model, summing over derivations. Li et al. (2009) propose a variational approximation maximizing sentence probability that decomposes over n-grams. Tromble et al. (2008) minimize risk under a loss function based on the linear Taylor approximation to BLEU, which decomposes over n-gram posterior probabilities. 5 Experiments We compared CoBLEU training with an implementation of minimum error rate training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation </context>
</contexts>
<marker>Blunsom, Osborne, 2008</marker>
<rawString>Phil Blunsom and Miles Osborne. 2008. Probabilistic inference for machine translation. In Proceedings of the Conference on Emprical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In The Annual Conference of the North American Chapter of the Association for Computational Linguistics Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<contexts>
<context position="22431" citStr="Cherry and Lin, 2007" startWordPosition="3793" endWordPosition="3796">ation maximizing sentence probability that decomposes over n-grams. Tromble et al. (2008) minimize risk under a loss function based on the linear Taylor approximation to BLEU, which decomposes over n-gram posterior probabilities. 5 Experiments We compared CoBLEU training with an implementation of minimum error rate training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al., 2007) when using the same phrase table (Petrov et al., 2008). We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a feature that counts the number of inverted ITG rewrites. 5.2 Data We extr</context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekang Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In The Annual Conference of the North American Chapter of the Association for Computational Linguistics Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In The Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="3206" citStr="Chiang et al., 2008" startWordPosition="469" endWordPosition="472">computable over packed forests of translations generated by machine translation systems. The gradient includes expectations of products of features and n-gram counts, a quantity that has not appeared in previous work. We present a new dynamic program which allows the efficient computation of these quantities over translation forests. The resulting gradient ascent procedure does not require any k-best approximations. Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al., 2008) and large-margin training (Chiang et al., 2008). We developed CoBLEU primarily to complement consensus decoding, which it does; it produces higher BLEU scores than coupling MERT with consensus decoding. However, we found an additional empirical benefit: CoBLEU is less prone to overfitting than MERT, even when using Viterbi decoding. In experiments, models trained to maximize tuning set BLEU using MERT consistently degraded in performance from tuning to test set, while CoBLEU-trained models generalized more robustly. As a result, we found that optimizing CoBLEU improved test set performance reliably using consensus decoding and occasionally</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In The Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation. Computational Linguistics.</title>
<date>2007</date>
<contexts>
<context position="12455" citStr="Chiang (2007)" startWordPosition="2085" endWordPosition="2086">We now present the computation of each of these expectations in turn. 3.2 Computing Feature Expectations The expected feature counts Eo[c(Ok, d)|fi] can be written as head(h) u=OnceSrhyme c(“Once upon”, h) c(“upon a”, h) 12(h) = v1=OnceRBOnce v2=uponINupon v3=aNPrhyme = 1 = 1 In this section, we develop an analytical expres- � Po(d|fi)c(Ok, d) sion for the gradient of CoBLEU, then discuss Eo[c(Ok, d)|fi] = Po(h|fi)c(Ok, h) how to efficiently compute the value of the objec- d tive function and gradient. �= h 3.1 Translation Model Form We first assume the general hypergraph setting of Huang and Chiang (2007), namely, that derivations under our translation model form a hypergraph. This framework allows us to speak about both phrase-based and syntax-based translation in a unified framework. We define a probability distribution over derivations d via B as: w(d) Po(d|fi) = Z(fi) with Z(fi) = � w(d&apos;) d&apos; where w(d) = exp(BTb(d, fi)) is the weight of a derivation and b(d, fi) is a featurized representation of the derivation d of fi. We further assume that these features decompose over hyperedges in the hypergraph, like the one in Figure 3. That is, b(d, fi) = EhEd b(h, fi). In this setting, we can analy</context>
<context position="14400" citStr="Chiang (2007)" startWordPosition="2425" endWordPosition="2426">st. 3.3 Computing n-gram Expectations We can compute the expectations of any specific n-grams, or of total n-gram counts E, in the same way as feature expectations, provided that targetside n-grams are also localized to hyperedges (e.g. consider E to be a feature of a hyperedge whose value is the number of n-grams on h). If the nodes in our forests are annotated with target-side 5Appendix Figure 7 gives recursions for I(u) and O(u). 1421 boundary words as in Figure 3, then this will be the case. Note that this is the same approach used by decoders which integrate a target language model (e.g. Chiang (2007)). Other work has computed n-gram expectations in the same way (DeNero et al., 2009; Li et al., 2009). 3.4 Computing Expectations of Products of Counts While the previous two expectations can be computed using techniques known in the literature, the expected product of counts EB[c(φk, d) · `n(d)|fi] is a novel quantity. Fortunately, an efficient dynamic program exists for computing this expectation as well. We present this dynamic program here as one of the contributions of this paper, though we omit a full derivation due to space restrictions. To see why this expectation cannot be computed in</context>
<context position="19660" citStr="Chiang (2007)" startWordPosition="3344" endWordPosition="3345">er n on a hyperedge h. Figure 5: Calculation of ˆDnθ (h|fi) after ˆIn(u) and ˆOn(u) have been computed. ˆDnθ (h|fi) = ⎡ w(h)⎢`n(h)O(head(h)) Y I(v) + O(head(h)) X ˆIn(v) Y I(w) + ˆOn(head(h)) Y I(w) vEtail(h) vEtail(h) vEtail(h) wEtail(h) w=,4v ⎤ ⎦⎥⎥ number of hyperedges is very large, however, because we must track n-gram contexts in the nodes, just as we would in an integrated language model decoder. These contexts are required both to correctly compute the model score of derivations and to compute clipped n-gram counts. To speed our computations, we use the cube pruning method of Huang and Chiang (2007) with a fixed beam size. For regularization, we added an L2 penalty on the size of θ to the CoBLEU objective, a simple addition for gradient ascent. We did not find that our performance varied very much for moderate levels of regularization. 3.6 Related Work The calculation of expected counts can be formulated using the expectation semiring framework of Eisner (2002), though that work does not show how to compute expected products of counts which are needed for our gradient calculations. Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected produc</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Fast consensus decoding over translation forests.</title>
<date>2009</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1500" citStr="DeNero et al., 2009" startWordPosition="211" endWordPosition="214">decoding procedures. An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding. 1 Introduction Increasing evidence suggests that machine translation decoders should not search for a single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model’s entire predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations. To main</context>
<context position="14483" citStr="DeNero et al., 2009" startWordPosition="2437" endWordPosition="2440">specific n-grams, or of total n-gram counts E, in the same way as feature expectations, provided that targetside n-grams are also localized to hyperedges (e.g. consider E to be a feature of a hyperedge whose value is the number of n-grams on h). If the nodes in our forests are annotated with target-side 5Appendix Figure 7 gives recursions for I(u) and O(u). 1421 boundary words as in Figure 3, then this will be the case. Note that this is the same approach used by decoders which integrate a target language model (e.g. Chiang (2007)). Other work has computed n-gram expectations in the same way (DeNero et al., 2009; Li et al., 2009). 3.4 Computing Expectations of Products of Counts While the previous two expectations can be computed using techniques known in the literature, the expected product of counts EB[c(φk, d) · `n(d)|fi] is a novel quantity. Fortunately, an efficient dynamic program exists for computing this expectation as well. We present this dynamic program here as one of the contributions of this paper, though we omit a full derivation due to space restrictions. To see why this expectation cannot be computed in the same way as the expected feature or n-gram counts, we expand the definition of</context>
<context position="20795" citStr="DeNero et al. (2009)" startWordPosition="3531" endWordPosition="3534"> work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected products of counts on translation forests. The training algorithm of Kakade et al. (2002) makes use of a dynamic program similar to ours, though specialized to the case of sequence models. 4 Consensus Decoding Once model parameters θ are learned, we must select an appropriate decoding objective. Several new decoding approaches have been proposed recently that leverage some notion of consensus over the many weighted derivations in a translation forest. In this paper, we adopt the fast consensus decoding procedure of DeNero et al. (2009), which directly complements CoBLEU tuning. For a source sentence f, we first build a translation forest, then compute the expected count of each n-gram in the translation of f under the model. We extract a k-best list from the forest, then select the translation that yields the highest BLEU score relative to the forest’s expected n-gram counts. Specifically, let BLEU(e; r) compute the similarity of a sentence e to a reference r based on the n-gram counts of each. When training with CoBLEU, we replace e with expected counts and maximize θ. In consensus decoding, we replace r with expected coun</context>
<context position="26892" citStr="DeNero et al. (2009)" startWordPosition="4519" endWordPosition="4522">the pruning needed to track n-grams. Similar stability benefits have been observed for latticebased MERT (Macherey et al., 2008). 5.4 Results We performed experiments from both French and Spanish into English under three conditions. In the first two, we initialized both MERT and CoBLEU training uniformly with zero weights and trained until convergence. In the third condition, we initialized CoBLEU with the final parameters from MERT training, denoted MERT→CoBLEU in the results tables. We evaluated each of these conditions on both the tuning and test sets using the consensus decoding method of DeNero et al. (2009). The results appear in Table 1. In Spanish-English, CoBLEU slightly outperformed MERT under the same initialization, while the opposite pattern appears for French-English. The best test set performance in both language pairs was the third condition, in which CoBLEU training was initialized with MERT. This condition also gave the highest CoBLEU objective value. This pattern indicates that CoBLEU is a useful objective for translation with consensus decoding, but that the gradient ascent optimization is getting stuck in local maxima during tuning. This issue can likely be addressed with annealin</context>
</contexts>
<marker>DeNero, Chiang, Knight, 2009</marker>
<rawString>John DeNero, David Chiang, and Kevin Knight. 2009. Fast consensus decoding over translation forests. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Parameter estimation for probabilistic finite-state transducers.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="20029" citStr="Eisner (2002)" startWordPosition="3407" endWordPosition="3408">n integrated language model decoder. These contexts are required both to correctly compute the model score of derivations and to compute clipped n-gram counts. To speed our computations, we use the cube pruning method of Huang and Chiang (2007) with a fixed beam size. For regularization, we added an L2 penalty on the size of θ to the CoBLEU objective, a simple addition for gradient ascent. We did not find that our performance varied very much for moderate levels of regularization. 3.6 Related Work The calculation of expected counts can be formulated using the expectation semiring framework of Eisner (2002), though that work does not show how to compute expected products of counts which are needed for our gradient calculations. Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected products of counts on translation forests. The training algorithm of Kakade et al. (2002) makes use of a dynamic program similar to ours, though specialized to the case of sequence models. 4 Consensus Decoding Once model parameters θ are learned, we must select an appropriate decoding objective. Several new decoding approaches have been proposed recently that leverage some</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002. Parameter estimation for probabilistic finite-state transducers. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="12455" citStr="Huang and Chiang (2007)" startWordPosition="2083" endWordPosition="2086">f counts. We now present the computation of each of these expectations in turn. 3.2 Computing Feature Expectations The expected feature counts Eo[c(Ok, d)|fi] can be written as head(h) u=OnceSrhyme c(“Once upon”, h) c(“upon a”, h) 12(h) = v1=OnceRBOnce v2=uponINupon v3=aNPrhyme = 1 = 1 In this section, we develop an analytical expres- � Po(d|fi)c(Ok, d) sion for the gradient of CoBLEU, then discuss Eo[c(Ok, d)|fi] = Po(h|fi)c(Ok, h) how to efficiently compute the value of the objec- d tive function and gradient. �= h 3.1 Translation Model Form We first assume the general hypergraph setting of Huang and Chiang (2007), namely, that derivations under our translation model form a hypergraph. This framework allows us to speak about both phrase-based and syntax-based translation in a unified framework. We define a probability distribution over derivations d via B as: w(d) Po(d|fi) = Z(fi) with Z(fi) = � w(d&apos;) d&apos; where w(d) = exp(BTb(d, fi)) is the weight of a derivation and b(d, fi) is a featurized representation of the derivation d of fi. We further assume that these features decompose over hyperedges in the hypergraph, like the one in Figure 3. That is, b(d, fi) = EhEd b(h, fi). In this setting, we can analy</context>
<context position="19660" citStr="Huang and Chiang (2007)" startWordPosition="3342" endWordPosition="3345">ams of order n on a hyperedge h. Figure 5: Calculation of ˆDnθ (h|fi) after ˆIn(u) and ˆOn(u) have been computed. ˆDnθ (h|fi) = ⎡ w(h)⎢`n(h)O(head(h)) Y I(v) + O(head(h)) X ˆIn(v) Y I(w) + ˆOn(head(h)) Y I(w) vEtail(h) vEtail(h) vEtail(h) wEtail(h) w=,4v ⎤ ⎦⎥⎥ number of hyperedges is very large, however, because we must track n-gram contexts in the nodes, just as we would in an integrated language model decoder. These contexts are required both to correctly compute the model score of derivations and to compute clipped n-gram counts. To speed our computations, we use the cube pruning method of Huang and Chiang (2007) with a fixed beam size. For regularization, we added an L2 penalty on the size of θ to the CoBLEU objective, a simple addition for gradient ascent. We did not find that our performance varied very much for moderate levels of regularization. 3.6 Related Work The calculation of expected counts can be formulated using the expectation semiring framework of Eisner (2002), though that work does not show how to compute expected products of counts which are needed for our gradient calculations. Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected produc</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sham Kakade</author>
<author>Yee Whye Teh</author>
<author>Sam T Roweis</author>
</authors>
<title>An alternate objective function for markovian fields.</title>
<date>2002</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="20343" citStr="Kakade et al. (2002)" startWordPosition="3457" endWordPosition="3460">alty on the size of θ to the CoBLEU objective, a simple addition for gradient ascent. We did not find that our performance varied very much for moderate levels of regularization. 3.6 Related Work The calculation of expected counts can be formulated using the expectation semiring framework of Eisner (2002), though that work does not show how to compute expected products of counts which are needed for our gradient calculations. Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected products of counts on translation forests. The training algorithm of Kakade et al. (2002) makes use of a dynamic program similar to ours, though specialized to the case of sequence models. 4 Consensus Decoding Once model parameters θ are learned, we must select an appropriate decoding objective. Several new decoding approaches have been proposed recently that leverage some notion of consensus over the many weighted derivations in a translation forest. In this paper, we adopt the fast consensus decoding procedure of DeNero et al. (2009), which directly complements CoBLEU tuning. For a source sentence f, we first build a translation forest, then compute the expected count of each n-</context>
</contexts>
<marker>Kakade, Teh, Roweis, 2002</marker>
<rawString>Sham Kakade, Yee Whye Teh, and Sam T. Roweis. 2002. An alternate objective function for markovian fields. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="22559" citStr="Koehn et al., 2007" startWordPosition="3814" endWordPosition="3817">d on the linear Taylor approximation to BLEU, which decomposes over n-gram posterior probabilities. 5 Experiments We compared CoBLEU training with an implementation of minimum error rate training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al., 2007) when using the same phrase table (Petrov et al., 2008). We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a feature that counts the number of inverted ITG rewrites. 5.2 Data We extracted phrase tables from the SpanishEnglish and French-English sections of the Europarl corpus, which include approximately 8.5 </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A multilingual corpus for evaluation of machine translation.</title>
<date>2002</date>
<contexts>
<context position="23227" citStr="Koehn, 2002" startWordPosition="3931" endWordPosition="3932"> We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a feature that counts the number of inverted ITG rewrites. 5.2 Data We extracted phrase tables from the SpanishEnglish and French-English sections of the Europarl corpus, which include approximately 8.5 million words of bitext for each of the language pairs (Koehn, 2002). We used a trigram language model trained on the entire corpus of English parliamentary proceedings provided with the Europarl distribution and generated according to the ACL 2008 SMT shared task specifications.6 For tuning, we used all sentences from the 2007 SMT shared task up to length 25 (880 sentences 6See http://www.statmt.org/wmt08 for details. 2 4 6 8 10 Iterations Figure 6: Trajectories of MERT and CoBLEU during optimization show that MERT is initially unstable, while CoBLEU training follows a smooth path to convergence. Because these two training procedures optimize different functi</context>
</contexts>
<marker>Koehn, 2002</marker>
<rawString>Philipp Koehn. 2002. Europarl: A multilingual corpus for evaluation of machine translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1538" citStr="Kumar et al., 2009" startWordPosition="219" endWordPosition="222">ond benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding. 1 Introduction Increasing evidence suggests that machine translation decoders should not search for a single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model’s entire predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations. To maintain consistency across the translatio</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and secondorder expectation semirings with applications to minimum-risk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="20202" citStr="Li and Eisner (2009)" startWordPosition="3434" endWordPosition="3438">ed our computations, we use the cube pruning method of Huang and Chiang (2007) with a fixed beam size. For regularization, we added an L2 penalty on the size of θ to the CoBLEU objective, a simple addition for gradient ascent. We did not find that our performance varied very much for moderate levels of regularization. 3.6 Related Work The calculation of expected counts can be formulated using the expectation semiring framework of Eisner (2002), though that work does not show how to compute expected products of counts which are needed for our gradient calculations. Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected products of counts on translation forests. The training algorithm of Kakade et al. (2002) makes use of a dynamic program similar to ours, though specialized to the case of sequence models. 4 Consensus Decoding Once model parameters θ are learned, we must select an appropriate decoding objective. Several new decoding approaches have been proposed recently that leverage some notion of consensus over the many weighted derivations in a translation forest. In this paper, we adopt the fast consensus decoding procedure of DeNero et al. (2009), which</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and secondorder expectation semirings with applications to minimum-risk training on translation forests. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1517" citStr="Li et al., 2009" startWordPosition="215" endWordPosition="218">An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding. 1 Introduction Increasing evidence suggests that machine translation decoders should not search for a single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model’s entire predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations. To maintain consistency </context>
<context position="14501" citStr="Li et al., 2009" startWordPosition="2441" endWordPosition="2444">of total n-gram counts E, in the same way as feature expectations, provided that targetside n-grams are also localized to hyperedges (e.g. consider E to be a feature of a hyperedge whose value is the number of n-grams on h). If the nodes in our forests are annotated with target-side 5Appendix Figure 7 gives recursions for I(u) and O(u). 1421 boundary words as in Figure 3, then this will be the case. Note that this is the same approach used by decoders which integrate a target language model (e.g. Chiang (2007)). Other work has computed n-gram expectations in the same way (DeNero et al., 2009; Li et al., 2009). 3.4 Computing Expectations of Products of Counts While the previous two expectations can be computed using techniques known in the literature, the expected product of counts EB[c(φk, d) · `n(d)|fi] is a novel quantity. Fortunately, an efficient dynamic program exists for computing this expectation as well. We present this dynamic program here as one of the contributions of this paper, though we omit a full derivation due to space restrictions. To see why this expectation cannot be computed in the same way as the expected feature or n-gram counts, we expand the definition of the expectation a</context>
<context position="21779" citStr="Li et al. (2009)" startWordPosition="3693" endWordPosition="3696">EU(e; r) compute the similarity of a sentence e to a reference r based on the n-gram counts of each. When training with CoBLEU, we replace e with expected counts and maximize θ. In consensus decoding, we replace r with expected counts and maximize e. Several other efficient consensus decoding pro1423 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of Value at Convergence cedures would similarly benefit from a tuning procedure that aggregates over derivations. For instance, Blunsom and Osborne (2008) select the translation sentence with highest posterior probability under the model, summing over derivations. Li et al. (2009) propose a variational approximation maximizing sentence probability that decomposes over n-grams. Tromble et al. (2008) minimize risk under a loss function based on the linear Taylor approximation to BLEU, which decomposes over n-gram posterior probabilities. 5 Experiments We compared CoBLEU training with an implementation of minimum error rate training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases a</context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009. Variational decoding for statistical machine translation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Macherey</author>
<author>F Och</author>
<author>I Thayer</author>
<author>J Uszkoreit</author>
</authors>
<title>Lattice-based minimum error rate training for statistical machine translation. In</title>
<date>2008</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="3158" citStr="Macherey et al., 2008" startWordPosition="462" endWordPosition="465">at this function and its gradient are efficiently computable over packed forests of translations generated by machine translation systems. The gradient includes expectations of products of features and n-gram counts, a quantity that has not appeared in previous work. We present a new dynamic program which allows the efficient computation of these quantities over translation forests. The resulting gradient ascent procedure does not require any k-best approximations. Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al., 2008) and large-margin training (Chiang et al., 2008). We developed CoBLEU primarily to complement consensus decoding, which it does; it produces higher BLEU scores than coupling MERT with consensus decoding. However, we found an additional empirical benefit: CoBLEU is less prone to overfitting than MERT, even when using Viterbi decoding. In experiments, models trained to maximize tuning set BLEU using MERT consistently degraded in performance from tuning to test set, while CoBLEU-trained models generalized more robustly. As a result, we found that optimizing CoBLEU improved test set performance re</context>
<context position="26400" citStr="Macherey et al., 2008" startWordPosition="4439" endWordPosition="4442">sured by BLEU using a consensus decoding method over translation forests shows an improvement over MERT when using CoBLEU training. The first two conditions were initialized by 0 vectors. The third condition was initialized by the final parameters of MERT training. Br. indicates the brevity penalty on the test set. The * indicates differences which are not statistically significant. smoothly to its final objective because the forests do not change substantially between iterations, despite the pruning needed to track n-grams. Similar stability benefits have been observed for latticebased MERT (Macherey et al., 2008). 5.4 Results We performed experiments from both French and Spanish into English under three conditions. In the first two, we initialized both MERT and CoBLEU training uniformly with zero weights and trained until convergence. In the third condition, we initialized CoBLEU with the final parameters from MERT training, denoted MERT→CoBLEU in the results tables. We evaluated each of these conditions on both the tuning and test sets using the consensus decoding method of DeNero et al. (2009). The results appear in Table 1. In Spanish-English, CoBLEU slightly outperformed MERT under the same initia</context>
</contexts>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="22742" citStr="Och and Ney, 2003" startWordPosition="3847" endWordPosition="3850">te training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al., 2007) when using the same phrase table (Petrov et al., 2008). We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a feature that counts the number of inverted ITG rewrites. 5.2 Data We extracted phrase tables from the SpanishEnglish and French-English sections of the Europarl corpus, which include approximately 8.5 million words of bitext for each of the language pairs (Koehn, 2002). We used a trigram language model trained on the entire corpus of English parliamentary proceedings provided with </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1871" citStr="Och, 2003" startWordPosition="268" endWordPosition="269">predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations. To maintain consistency across the translation pipeline, we formulate CoBLEU to share the functional form of BLEU used for evaluation. As a result, CoBLEU optimizes exactly the quantities that drive efficient consensus decoding techniques and precisely mirrors the objective used for fast consensus decoding in DeNero et al. (2009). CoBLEU is a continuous and (mostly) different</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL), pages 160–167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In The Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4770" citStr="Papineni et al., 2002" startWordPosition="734" endWordPosition="737">( me , d)|f] = 0. + 0. 4 = 0. 1 Eθ[c(g, d)|f] = [0. + 0. 4 + 0.0 ] E9 min{Eθ[c(g, d)|f], c(g, r)} E9 Eθ[c(g, d)|f] Figure 1: (a) A simple hypothesis space of translations for a single sentence containing three alternatives, each with two features. The hypotheses are scored under a log-linear model with parameters θ equal to the identity vector. (b) The expected counts of all bigrams that appear in the computation of consensus bigram precision. 2 Consensus Objective Functions Our proposed objective function maximizes ngram precision by adapting the BLEU evaluation metric as a tuning objective (Papineni et al., 2002). To simplify exposition, we begin by adapting a simpler metric: bigram precision. 2.1 Bigram Precision Tuning Let the tuning corpus consist of source sentences F = f1 ... fm and human-generated references R = r1 ... rm, one reference for each source sentence. Let ei be a translation of fi, and let E = e1 ... em be a corpus of translations, one for each source sentence. A simple evaluation score for E is its bigram precision BP(R, E): BP(R, E) = Pm Pg2 c(g2, ei) i=1 where g2 iterates over the set of bigrams in the target language, and c(g2, e) is the count of bigram g2 in translation e. As in </context>
<context position="8936" citStr="Papineni et al., 2002" startWordPosition="1472" endWordPosition="1475">The horizontal segmented line at the top of the plot indicates the range over which consensus decoding would select each hypothesis, while the segmented line at the bottom indicates the same for Viterbi decoding. MaxBP is only sensitive to the single point of discontinuity between H1 and H3, and disregards H2 entirely. CoBP peaks when the distribution most heavily favors H2 while suppressing H1. Though H2 never has a maximal model score, if θLM is in the indicated range, consensus decoding would select H2, the desired translation. 2.2 CoBLEU The logarithm of the single-reference2 BLEU metric (Papineni et al., 2002) has the following form: !lnBLEU(R,E) = 1 − m |R |i= − PPm Pgn min{c(gn, ei), c(gn, ri)} i=1 ln m Pgn c(gn, ei) i=1 Above, |R |denotes the number of words in the reference corpus. The notation (·)− is shorthand for min(·, 0). In the inner sums, gn iterates over all n-grams of order n. In order to adapt BLEU to be a consensus tuning objective, we follow the recipe of Section 2.1: we replace n-gram counts from a candidate translation with expected n-gram counts under the model. CoBLEU(R, F, θ) = 1− |R| Pm P g1Eθ[c(g1, d)|fi] i=1 The brevity penalty term in BLEU is calculated using the expected l</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In The Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coarse-to-fine syntactic machine translation using language projections.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>108--116</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="22614" citStr="Petrov et al., 2008" startWordPosition="3824" endWordPosition="3827">composes over n-gram posterior probabilities. 5 Experiments We compared CoBLEU training with an implementation of minimum error rate training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al., 2007) when using the same phrase table (Petrov et al., 2008). We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a feature that counts the number of inverted ITG rewrites. 5.2 Data We extracted phrase tables from the SpanishEnglish and French-English sections of the Europarl corpus, which include approximately 8.5 million words of bitext for each of the language pairs </context>
</contexts>
<marker>Petrov, Haghighi, Klein, 2008</marker>
<rawString>Slav Petrov, Aria Haghighi, and Dan Klein. 2008. Coarse-to-fine syntactic machine translation using language projections. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 108–116, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models. In</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27535" citStr="Smith and Eisner, 2006" startWordPosition="4618" endWordPosition="4621"> in Table 1. In Spanish-English, CoBLEU slightly outperformed MERT under the same initialization, while the opposite pattern appears for French-English. The best test set performance in both language pairs was the third condition, in which CoBLEU training was initialized with MERT. This condition also gave the highest CoBLEU objective value. This pattern indicates that CoBLEU is a useful objective for translation with consensus decoding, but that the gradient ascent optimization is getting stuck in local maxima during tuning. This issue can likely be addressed with annealing, as described in (Smith and Eisner, 2006). Interestingly, the brevity penatly results in French indicate that, even though CoBLEU did Viterbi Decoding Spanish Tune Test 0 MERT 32.5 30.2 -2.3 MERT→CoBLEU 30.5 30.9 +0.4 French Tune Test 0 MERT 32.0 31.0 -1.0 MERT→CoBLEU 31.7 30.9 -0.8 Table 2: Performance measured by BLEU using Viterbi decoding indicates that CoBLEU is less prone to overfitting than MERT. not outperform MERT in a statistically significant way, CoBLEU tends to find shorter sentences with higher n-gram precision than MERT. Table 1 displays a second benefit of CoBLEU training: compared to MERT training, CoBLEU performance</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Shankar Kumar</author>
<author>Franz Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In The Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1479" citStr="Tromble et al., 2008" startWordPosition="207" endWordPosition="210"> downstream consensus decoding procedures. An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding. 1 Introduction Increasing evidence suggests that machine translation decoders should not search for a single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model’s entire predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference </context>
<context position="21899" citStr="Tromble et al. (2008)" startWordPosition="3710" endWordPosition="3713">with CoBLEU, we replace e with expected counts and maximize θ. In consensus decoding, we replace r with expected counts and maximize e. Several other efficient consensus decoding pro1423 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of Value at Convergence cedures would similarly benefit from a tuning procedure that aggregates over derivations. For instance, Blunsom and Osborne (2008) select the translation sentence with highest posterior probability under the model, summing over derivations. Li et al. (2009) propose a variational approximation maximizing sentence probability that decomposes over n-grams. Tromble et al. (2008) minimize risk under a loss function based on the linear Taylor approximation to BLEU, which decomposes over n-gram posterior probabilities. 5 Experiments We compared CoBLEU training with an implementation of minimum error rate training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the st</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice minimum Bayes-risk decoding for statistical machine translation. In The Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--377</pages>
<contexts>
<context position="22358" citStr="Wu, 1997" startWordPosition="3783" endWordPosition="3784"> derivations. Li et al. (2009) propose a variational approximation maximizing sentence probability that decomposes over n-grams. Tromble et al. (2008) minimize risk under a loss function based on the linear Taylor approximation to BLEU, which decomposes over n-gram posterior probabilities. 5 Experiments We compared CoBLEU training with an implementation of minimum error rate training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al., 2007) when using the same phrase table (Petrov et al., 2008). We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377–404.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>